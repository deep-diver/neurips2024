[{"type": "text", "text": "A Learning-based Capacitated Arc Routing Problem Solver Comparable to Metaheuristics While with Far Less Runtimes ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Recently, neural networks (NN) have made great strides in combinatorial optimiza  \n2 tion problems (COPs). However, they face challenges in solving the capacitated   \n3 arc routing problem (CARP) which is to find the minimum-cost tour that covers all   \n4 required edges on a graph, while within capacity constraints. Actually, NN-based   \n5 approaches tend to lag behind advanced metaheuristics due to complexities caused   \n6 by non-Euclidean graph, traversal direction and capacity constraints. In this   \n7 paper, we introduce an NN-based solver tailored for these complexities, which   \n8 significantly narrows the gap with advanced metaheuristics while with far less   \n9 runtimes. First, we propose the direction-aware attention model (DaAM) to in  \n10 corporate directionality into the embedding process, facilitating more effective   \n11 one-stage decision-making. Second, we design a supervised reinforcement learning   \n12 scheme that involves supervised pre-training to establish a robust initial policy for   \n3 subsequent reinforcement fine-tuning. It proves particularly valuable for solving   \n14 CARP that has a higher complexity than the node routing problems (NRPs). Finally,   \n15 a path optimization method is introduced to adjust the depot return positions within   \n16 the path generated by DaAM. Experiments show that DaAM surpasses heuristics   \n17 and achieves decision quality comparable to state-of-the-art metaheuristics for the   \n18 first time while maintaining superior efficiency, even in large-scale CARP instances.   \n19 The code and datasets are provided in the Appendix. ", "page_idx": 0}, {"type": "text", "text": "20 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "21 The capacitated arc routing problem (CARP) [7] is a combinatorial optimization problem, frequently   \n22 arising in domains such as inspection and search-rescue operations. Theoretically, the CARP is   \n23 established on an undirected connected graph ${\\bf G}\\,=\\,({{\\bf V}},\\bar{{\\bf E}},{\\bf E}_{R})$ that includes a set of nodes $\\mathbf{V}$   \n24 connected by a set of edges $\\mathbf{E}$ , and an edge subset $\\mathbf{E}_{R}\\subseteq\\mathbf{E}$ that needs to be served, called required   \n25 edges. Each required edge has a demand value which spends the capacity of the vehicle when it is   \n26 served. In this context, all vehicles start their routes from the depot node $d e p o t\\in\\mathbf{V}$ and conclude   \n27 their journey by returning to the same depot. The main goal of a CARP solver is to serve all required   \n28 edges with the lowest total path cost, while ensuring that the vehicle does not exceed its capacity $Q$ .   \n29 According to [7], the CARP is recognized as an NP-Hard problem. Among all solver solutions, the   \n30 Memetic Algorithms (MA) [15, 25], first proposed in 2005, have still maintained unrivaled results in   \n31 solving the CARP challenge to this day. However, they have struggled with high time costs and the   \n32 exponential growth of the search space as the problem scale increases. Compared to the traditional   \n33 methods, NN-based solvers [16, 10, 20] are faster with the assistance of GPU, have therefore gained   \n34 increasing attention in recent years. However, unlike the decent performance in solving NRP, such as   \n35 vehicle routing problem (VRP) and travelling salesman problem (TSP), or ARP defined in Euclidean   \n36 graphs, such as rural postman problem (RPP) and Chinese postman problem (CPP), NN-based   \n37 methods usually lags far behind the traditional ones in solving CARP. This discrepancy is attributed   \n38 to the inability of existing methods to effectively reduce the high complexity of solving CARP:   \n39 \u2022 Lack of edge direction in embedding learning: ARP solvers need to determine the edges   \n40 to be traversed along with the direction of traversal, easy for humans to achieve in one step   \n41 but extremely challenging for computers. Existing methods didn\u2019t encode edge directionality   \n42 in embedding, making them have to build edge sequences and determine edges\u2019 directions   \n43 separately and leading to path generation without sufficient consideration.   \n44 \u2022 Ineffective learning for solving CARP: CARP is more complex than NRPs and Euclidean   \n45 ARPs owing to the non-Euclidean input, edge direction, and capacity constraints. Thus,   \n46 learning methods for NRPs and Euclidean ARPs cannot be directly transferred to solve   \n47 CARP or work well even if adapted, leaving a lack of effective learning strategies for CARP.   \n48 In this paper, we aim to address both above issues and propose an NN-based solver for CARP that   \n49 competes with the state-of-the-art MA [25] while with far less runtimes. Firstly, we propose the   \n50 direction-aware attention model (DaAM). It computes embeddings for directed arcs decomposed   \n51 from undirected edges to align with the nature of ARP, thus avoiding missing direction information   \n52 and enabling concise one-stage decision-making. Secondly, we design a supervised reinforcement   \n53 learning method to learn effective heuristics for solving CARP. DaAM is pre-trained to learn an initial   \n54 policy by minimizing the difference from the decisions made by experts, and fine-tuned on larger  \n55 scale CARP instances by Proximal Policy Optimization with self-critical strategy. Finally, to further   \n56 boost the path quality, we propose a path optimizer (PO) to re-decide the vehicles\u2019 optimal return   \n57 positions by dynamic programming. In the experiments, our method approaches the state-of-the-art   \n58 MA with an average gap of $5\\%$ and is $4\\%$ better than the latest heuristics and gains high efficiency. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "59 2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "60 2.1 Graph Embedding Learning ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "61 Graph embedding [3] aims to map nodes or edges in a graph to a low-dimensional vector space.   \n62 This process is commonly achieved via graph neural networks (GNNs) [31]. Kipf et al. [13]   \n63 introduced graph convolutional operations to aggregate information from neighboring nodes for   \n64 updating node representations. Unlike GCN, GAT [27] allowed dynamic node attention during   \n65 information propagation by attention mechanisms. Other GNN variants [9, 30] exhibited a similar   \n66 information aggregation pattern but with different computational approaches. In this paper, since   \n67 an arc is related to the outgoing arc of its endpoint but irrelevant to the incoming arc of that, we use   \n68 attention mechanisms to capture the intricate relationships between arcs for arc embedding learning. ", "page_idx": 1}, {"type": "text", "text": "69 2.2 Learning for Routing Problems ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "70 The routing problem is one of the most classic COPs, and it is mainly categorized into two types   \n71 according to the decision element: node routing problems and arc routing problems.   \n72 Node routing problems (NRPs), such as TSP and VRP, aim to determine the optimal paths that   \n73 traverse all nodes in the Euclidean space or graphs. As the solutions to these problems are context  \n74 dependent sequences of variable size, they cannot be directly modeled by the Seq2Seq model [24].   \n75 To address this problem, Vinyals et al. [28] proposed the Pointer network (PN) to solving Euclidean   \n76 TSP, which achieves variable-size output dictionaries by neural attention. Due to the scarcity of labels   \n77 for supervised learning, Bello et al. [2] modeled the TSP as a single-step reinforcement learning   \n78 problem and trained the PN using policy gradient [29] within Advantage Actor-Critic (A3C) [17]   \n79 framework. Nazari et al. [19] replaced the LSTM encoder in PN with an element-wise projection   \n80 layer and proposed the first NN-based method to solve the Euclidean VRP and its variants. To better   \n81 extract correlations between inputs, Kool et al. [14] utilized multi-head attention for embedding   \n82 learning and trained the model using REINFORCE [29] with a greedy baseline. To solve COPs   \n83 defined on graphs, Khalil et al. [11] proposed S2V-DQN to learn heuristics, employing structure2vec   \n84 [5] for graph embedding learning and n-step DQN [18] for training. While the mentioned NN-based   \n85 approaches have achieved comparable performance to metaheuristics, they cannot be directly applied   \n86 to solve ARP due to the modeling differences between ARP and NRP.   \n87 Arc routing problems (ARPs) involve determining optimal paths for traversing arcs or edges in   \n88 graphs, with variants like RPP, CPP, and CARP. Truong et al. [26] proposed a DRL framework to   \n89 address the CPP with load-dependent costs on Euclidean graphs and achieved better solution quality   \n90 than metaheuristics. However, CARPs are defined on non-Euclidean graphs. Unlike Euclidean graphs   \n91 with given node coordinates, non-Euclidean graphs require manually extracting and aggregating the   \n92 node representations, a task that is typically learnable. Although several NN-based algorithms have   \n93 been proposed, they still lag significantly behind traditional methods. Li et al. [16] pioneered the use   \n94 of the NN-based approach in solving the CARP by transforming it into an NRP. They first determined   \n95 the sequence of edges and then decided the traversal direction for each edge. Hong et al. [10] trained a   \n96 PN in a supervised manner to select undirected edges in each time step, and also determined the edge   \n97 traversal direction as post-processing. Ramamoorthy et al. [20] proposed to generate an initial tour   \n98 based on edge embeddings and then split it into routes within capacity constraint. These approaches   \n99 lack edge directionality encoding, leading to edge selection without sufficient consideration and   \n100 necessitating a two-stage decision process or an additional splitting procedure. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "101 3 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "102 The attention model (AM) [14] exhibits superior effectiveness in solving classic Euclidean COPs due   \n103 to its attention mechanisms for extracting correlations between inputs. Therefore, we use the AM   \n104 as the backbone and give a brief review in terms of the TSP. Given an Euclidean graph ${\\bf G}\\mathrm{=}({\\bf V},{\\bf E})$ ,   \n105 the AM defines a stochastic policy, denoted as $\\pi({\\pmb x}|S)$ , where $\\pmb{x}=(x_{0},...,x_{|\\mathbf{V}|-1})$ represents a   \n106 permutation of the node indexes in $\\mathbf{V}$ , and $\\boldsymbol{S}$ is the problem instance expressing $\\mathbf{G}$ . The AM is   \n107 parameterized by $\\pmb{\\theta}$ as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pi_{\\boldsymbol{\\theta}}(\\mathbf{x}|S)=\\prod_{t=1}^{|\\mathbf{V}|}\\pi_{\\boldsymbol{\\theta}}(x_{t}|S,x_{0:t-1})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "108 where $t$ denotes the time step. Specifically, the AM comprises an encoder and a decoder. The   \n109 encoder first computes initial $d_{h}$ -dimensional embeddings for each node in $\\mathbf{V}$ as $h_{i}^{0}$ through a learned   \n110 linear projection. It then captures the embeddings of $\\bar{h_{i}^{0}}$ using multiple attention layers, with each   \n111 comprising a multi-head attention (MHA) sublayer and a node-wise feed-forward (FF) sublayer.   \n112 Both types of sublayers include a skip connection and batch normalization (BN). Assuming that   \n113 $l\\in\\{1,...,N\\}$ denotes the attention layer, the $l^{\\mathrm{th}}$ layer can be formulated as $h_{i}^{l}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{i}^{l}=\\;\\mathbf{B}\\mathbf{N}^{l}(\\hat{h_{i}}+\\mathrm{FF}^{l}(\\hat{h_{i}}));\\qquad\\hat{h_{i}}=\\;\\mathbf{B}\\mathbf{N}^{l}(h_{i}^{l-1}+\\mathrm{MHA}_{i}^{l}(h_{0}^{l-1},\\cdot\\cdot\\cdot,h_{|\\mathbf{V}|-1}^{l-1}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "114 The decoder aims to append a node to the sequence $\\textbf{\\em x}$ at each time step. Specifically, a context   \n115 embedding $h_{(c)}$ is computed to represent the state at the time step $t$ . Then a single attention head is   \n116 used to calculate the probabilities for each node based on $h_{(c)}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{u_{(c)j}=\\left\\{C\\cdot\\operatorname{tanh}\\Big(d_{h}^{-\\frac{1}{2}}[\\mathbf{W}^{Q}h_{(c)}]^{T}\\mathbf{W}^{K}h_{j}^{N}\\Big)\\quad\\mathrm{if~}j\\neq x_{t^{\\prime}}(\\forall t^{\\prime}<t)\\right.}\\\\ &{}&{\\mathrm{otherwise},\\quad\\quad}\\\\ &{}&{p_{i}=\\pi_{\\theta}(x_{t}=i|S,x_{0:t-1})=u_{(c)i}/\\sum_{j}u_{(c)j}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "117 where $\\mathbf{W}^{Q}$ and $\\mathbf{W}^{K}$ are the learnable parameters of the last attention layer. $u_{(c)j}$ is an unnormalized   \n118 log probability with $(c)$ indicating the context node. $C$ is a constant, and $p_{i}$ is the probability   \n119 distribution computed by the softmax function based on $u_{(c)j}$ . ", "page_idx": 2}, {"type": "text", "text": "120 4 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "121 4.1 Direction-aware Attention Model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "122 In this section, we propose the direction-aware attention model (DaAM). Unlike previous methods   \n123 that separately learn edge embeddings and determine edge directions, our model encodes direction   \n124 information directly into the embedding, enabling one-stage decision-making. As shown in Fig. 1,   \n125 the DaAM makes sequential decisions in two phases to select arcs. The first phase is a one-time   \n126 transformation process, in which the arcs of the input graph are represented as nodes in the new   \n127 directed complete graph. The second phase is executed at each time step, in which GAT is used to   \n128 aggregate the inter-arc weights. Subsequently, AM is used to select the arc of the next action. ", "page_idx": 2}, {"type": "text", "text": "129 4.1.1 Arc Feature Formulation via Graph Transformation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "130 Graph Transformation Motivated by the need to consider direction when traversing edges, we   \n131 explicitly encode the edge direction by edge-to-arc decomposition. Let $\\mathbf{G}{=}(\\mathbf{V},\\mathbf{E},\\bar{\\mathbf{E}_{R}})$ denotes   \n132 the undirected connected graph as input, where $\\mathbf{V}$ is the node set of $\\mathbf{G}$ , $\\mathbf{E}$ is the edge set of $\\mathbf{G}$ ,   \n133 and $\\mathbf{E}_{R}\\ \\subseteq\\ \\mathbf{E}$ is the required edge set. Firstly, given that an edge has two potential traversal   \n134 directions, we decompose each edge $\\mathbf{e}_{n m}\\!=\\!\\left(c o s t_{n m},d e m a n d_{n m},a\\overline{{l}}l o w_{-}s e r v e_{n m}^{\\circ}\\right)\\!\\in\\!\\mathbf{E}_{R}$ into two   \n135 arcs $\\{a r c_{n m},a r c_{m n}\\}$ with opposite directions but the same cost, demand and serving state. Here   \n136 $n,m$ are the indexes of node in $\\mathbf{V}$ . To simplify the representation below, we replace nm and mn   \n137 with single-word symbols, such as $i$ and $j$ . In this way of edge decomposition, we obtain a set of arcs   \n138 denoted as $A_{R}$ . Secondly, we build a new graph $G=(A_{R},E)$ . Specifically, each arc in $A_{R}$ serves   \n139 as a node in $G$ , and directed edge set $E$ is created, with $e_{i j}\\in E$ representing the edge from node   \n140 $a r c_{i}$ to $a r c_{j}$ . The weight $|e_{i j}|$ represents the total cost of the shortest path from the end node of $a r c_{i}$   \n141 to the start node of $a r c_{j}$ . In addition, we treat the depot as a self-loop zero-demand arc that allows   \n142 for repeated serving, denoted as $a r c_{0}$ . Consequently, we transform the input graph $\\mathbf{G}$ into a directed   \n143 complete graph $G$ . By decomposing all edges in $\\mathbf{E}_{R}$ into arcs, it is natural to directly select the arcs   \n144 from $G$ during the decision-making. Given that the Floyd-Warshall algorithm is used to calculate the   \n145 shortest path cost between any pair of nodes in $\\mathbf{G}$ , the time complexity of our graph transformation is   \n146 $\\operatorname*{max}(\\mathcal{O}(|\\mathbf{E}_{R}|^{2}),\\mathcal{O}(|\\mathbf{V}|^{3}))$ .   \n147 Arc Feature Formulation To establish a foundation for decision-making regarding arc selection,   \n148 the features of the arcs are constructed as input for the subsequent model. Specifically, multi  \n149 dimensional scaling (MDS) is used to project the input graph $\\mathbf{G}$ into a $d$ -dimensional Euclidean   \n150 space. The Euclidean coordinates of $a r c_{i}$ \u2019s start and end nodes, denoted as $m d s_{\\mathrm{start}(i)}$ and $m d s_{\\mathrm{end}(i)}$ ,   \n151 are then taken as the features of $a r c_{i}$ to indicate its direction. As shown in Table 1, at time step $t$ ,   \n152 $a r c_{i}$ can be featured as: ", "page_idx": 2}, {"type": "image", "img_path": "7ug4oSmN7l/tmp/c21d936aca26bd61b863a2902a0fd7abf85310bac60b8572981ecc5884bc2ed5.jpg", "img_caption": ["Figure 1: DaAM Pipeline consists of two parts. The first part transforms the input graph $\\mathbf{G}$ by treating the arcs on $\\mathbf{G}$ as nodes of a new directed graph $G$ , only executing once. The second part leverages the GAT and AM to update arc embeddings and select arcs, executing at each time step. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "table", "img_path": "7ug4oSmN7l/tmp/b7c9ea6a71bbb33e18497dd432073bffcfd76e78b21d7b478d5a40e088828272.jpg", "table_caption": ["Table 1: Feature Detail of $a r c_{i}$ at time step $t$ for CARP. "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nF_{t}^{(i)}=(i s\\_d e p o t_{i},c o s t_{i},d e m a n d_{i},|e_{x_{t-1}i}|,a l l o w\\_s e r v e_{t}^{(i)},m d s_{\\mathrm{start}(i)},m d s_{\\mathrm{end}(i)})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "153 where $x_{t-1}$ is the index of the selected arc at the last time step, and $t\\in[1,+\\infty)$ . Our feature models   \n154 arcs rather than edges and encodes the direction attribute of arcs through MDS. Therefore, it is more   \n155 suitable than previous methods [10, 16] for ARPs that need to consider the direction of traversing. ", "page_idx": 3}, {"type": "text", "text": "156 4.1.2 Arc Relation Encoding via Graph Attention Network ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "157 Although AM is efficient in decision-making, according to Eq. (2), it cannot encode the edge weights   \n158 between nodes in $G$ , an important context feature, during learning. Therefore, we use graph attention   \n159 network (GAT) [27] to encode such weights. At each time step $t$ , for each arc $a r c_{i}$ , we integrate the ", "page_idx": 3}, {"type": "text", "text": "160 weights between $a r c_{i}$ and all arcs in $A_{R}$ along with their features into the initial embedding of $a r c_{i}$ . ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{c_{i j}=s o f t m a x\\big(\\alpha(\\mathbf{W}[F_{t}^{(i)}\\,|\\,|\\,F_{t}^{(j)}\\,|\\,|\\,|e_{j i}|\\,])\\big);}&{}&{h_{i}^{0}=\\,\\sigma\\big(\\sum_{j=0}^{|A_{R}|-1}c_{i j}\\mathbf{W}F_{t}^{(j)}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "161 where W is a shared learnable parameter, $[\\cdot||\\cdot]$ is the horizontal concatenation operator, $\\alpha(\\cdot)$ is a   \n162 mapping from the input to a scalar, and $\\sigma(\\cdot)$ denotes the activation function. $h_{i}^{0}$ denotes the initial   \n163 feature embedding of $a r c_{i}$ , which is taken as the input of subsequent AM. Since $G$ is a complete   \n164 graph, we use one graph attention layer to avoid over-smoothing [4]. ", "page_idx": 4}, {"type": "text", "text": "165 4.1.3 Arc Selection via Attention Model ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "166 After aggregating the edge weights of $G$ into the initial embeddings, we utilize AM to learn the final   \n167 arc embeddings and make arc selection decisions. In the encoding phase described by Eq.2, for each   \n168 arc $\\{a r c_{i}\\}$ , we leverage $N$ attention layers to process the initial embeddings $\\{h_{i}^{0}\\}$ and obtain the   \n169 output embeddings of the $N^{\\mathrm{th}}$ layer, i.e., $\\{h_{i}^{N}\\}$ . In the decoding phase, we define the context node   \n170 applicable to CARP: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{(c)}^{N}\\!=\\!\\Big[\\!|A_{R}|^{-1}\\!\\sum_{i=0}^{|A_{R}|-1}\\!h_{i}^{N},h_{x_{t-1}}^{N},\\delta_{t},\\Delta_{t}\\Big],t\\in[1,+\\infty)}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "171 where $x_{t-1}$ indicates the chosen arc index at time step $t-1$ and $x_{0}$ is $a r c_{0}$ . $\\delta_{t}$ is the remaining   \n172 capacity at time step $t$ , $\\begin{array}{r}{\\Delta_{t}=\\Delta(\\delta_{t}>\\frac{Q}{2})}\\end{array}$ is a variable to indicate whether the vehicle\u2019s remaining   \n173 capacity exceeds half. Finally, according to Eq.(3), the decoder of AM takes the context node $h_{(c)}^{N}$   \n174 and arc embeddings $\\{h_{i}^{N}\\}$ as inputs and calculates the probabilities for all arcs, denoted as $p_{i}$ . The   \n175 serviceable arc selected at time step $t$ , i.e., $a r c_{x_{t}}$ , is determined by sampling or greedy decoding. ", "page_idx": 4}, {"type": "text", "text": "176 4.2 Supervised Reinforcement Learning for CARP ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "177 The decision-making of selecting arcs can be modeled as a Markov decision process with the   \n178 following symbols regarding reinforcement learning:   \n179 \u2022 State $s_{t}$ is the newest path of arcs selected from $G$ : $(a r c_{x_{0}},...,a r c_{x_{t-1}})$ , while the terminal state   \n180 is $s_{T}$ with $T$ indicating the final time step.   \n181 \u2022 Action $a_{t}$ is the selected arc at time step $t$ , i.e., $a r c_{x_{t}}$ . Selecting the action $a_{t}$ would add $a r c_{x_{t}}$ to the   \n182 end of the current path $s_{t}$ and tag the corresponding arcs of $a r c_{x_{t}}$ with their features allow_serve   \n183 changed to 0. Notably, $a r c_{0}$ can be selected repeatedly but not consecutively.   \n184 \u2022 Reward $r_{t}$ is obtained after taking action $a_{t}$ at state $s_{t}$ , which equals the negative shortest path   \n185 cost from the last arc arcxt\u22121 to the selected arc arcxt .   \n186 \u2022 Stochastic policy $\\pi(a_{t}|s_{t})$ specifies the probability distribution over all actions at state $s_{t}$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "187 We parameterize the stochastic policy of DaAM with $\\theta$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pi(x_{t}|\\,S,{x_{0:t-1}})=\\pi_{\\theta}(a_{t}|s_{t})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "188 where $\\boldsymbol{S}$ is a CARP instance. Starting from initial state $s_{0}$ , we get a trajectory $\\tau\\quad=$   \n189 $\\left({{s_{0}},{a_{0}},{r_{0}},...,{r_{T-1}},{s_{T}}}\\right)$ using $\\pi_{\\theta}$ . The goal of learning is to maximize the cumulative reward:   \n190 R(\u03c4) = tT =\u221201 r . However, due to the high complexity of CARP, vanilla deep reinforcement learn  \n191 ing methods learn feasible strategies inefficiently. A natural solution is to minimize the difference   \n192 between the model\u2019s decisions and expert decisions. To achieve this, we employ supervised learning   \n193 to learn an initial policy based on labeled data and then fine-tune the model through reinforcement   \n194 learning. ", "page_idx": 4}, {"type": "text", "text": "195 4.2.1 Supervised Pre-training via Multi-class Classification ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "196 In the pre-training stage, we consider arc-selection at each time step as a multi-class classification   \n197 task, and employ the state-of-the-art CARP method MAENS to obtain high-quality paths as the label.   \n198 Assuming that $y_{t}\\,\\in\\,\\mathbb{R}^{|A_{R}|}$ denotes the one-hot label vector at time step $t$ of any path, with $y_{t}^{(k)}$   \n199 indicating each element. We utilize the cross-entropy loss to train the policy represented in Eq. (7): ", "page_idx": 4}, {"type": "equation", "text": "$$\nL=-\\sum_{t=0}^{T-1}\\sum_{k=0}^{|A_{R}|-1}y_{t}^{(k)}\\log\\left(\\pi_{\\theta}(a r c_{k}|s_{t})\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "200 We use the policy optimized by cross-entropy, denoted as $\\pi_{s}$ , to initialize the policy network $\\pi_{\\theta}$ and   \n201 as the baseline policy $\\pi_{b}$ in reinforcement learning.   \n203 During the fine-tuning phase, we use Proximal Policy Optimization (PPO) to optimize our model   \n204 $\\pi_{\\theta}\\big(a_{t}|s_{t}\\big)$ due to its outstanding stability in policy updates. Considering the low sample efficiency in   \n205 reinforcement learning, we employ a training approach similar to self-critical training [21] to reduce   \n206 gradient variance and expedite convergence. Specifically, We use another policy $\\pi_{b}$ to generate a   \n207 trajectory and calculate its cumulative reward, serving as a baseline function. Our optimization   \n208 objective is based on PPO-Clip [23]: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}_{(s,a)\\sim\\pi_{b}}\\left[\\operatorname*{min}\\left(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{b}(a|s)}\\big(R(\\tau_{s}^{\\theta})-R(\\tau_{s}^{b})\\big),\\mathrm{clip}\\left(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{b}(a|s)},1\\!-\\!\\epsilon,1\\!+\\!\\epsilon\\right)\\left(R(\\tau_{s}^{\\theta})-R(\\tau_{s}^{b})\\right)\\right)\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "209 where $s$ is used to replace current state $s_{t}$ for symbol simplification, and $a$ for $a_{t}$ . $\\mathrm{clip}(w,v_{\\mathrm{min}},v_{\\mathrm{max}})$   \n210 denotes constraining $w$ within the range $[v_{\\operatorname*{min}},v_{\\operatorname*{max}}]$ , and $\\epsilon$ is a hyper-parameter. $\\tau_{s}^{\\theta}$ denotes a   \n211 trajectory sampled by $\\pi_{\\theta}$ with $s$ as the initial state, while $\\tau_{s}^{b}$ for the trajectory greedily decoded by $\\pi_{b}$ .   \n212 In greedy decoding, the action with the maximum probability is selected at each step. $R(\\tau_{s}^{\\theta})-R(\\tau_{s}^{b})$   \n213 serves as an advantage measure, quantifying the advantage of the current policy $\\pi_{\\theta}$ compared to $\\pi_{b}$ .   \n214 We maximize Eq. (9) through gradient descent, which forces the model to select actions that yield   \n215 higher advantages. The baseline policy\u2019s parameters are updated if $\\pi_{\\theta}$ outperforms $\\pi_{b}$ . ", "page_idx": 5}, {"type": "text", "text": "216 4.3 Path Optimization via Dynamic Programming ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "217 The complexity of the problem is heightened by the increasing capacity constraint, making it challeng  \n218 ing for the neural network to make accurate decisions regarding the depot return positions. In this sec  \n219 tion, we propose a dynamic programming (DP) based strategy to assist our model in optimizing these   \n220 positions. Assuming that $_{P}$ is assigned with the terminal state $s_{T}=(a r c_{x_{0}},a r c_{x_{1}},...,a r c_{x_{T-1}})$ ,   \n221 representing a generated path. Initially, we remove all the depot arcs in $_{P}$ to obtain a new   \n222 path $P^{'}\\;=\\;(a r c_{x_{0}^{\\prime}},a r c_{x_{1}^{\\prime}},...,a r c_{x_{T^{\\prime}-1}^{\\prime}})$ , where $\\{x_{i}^{\\prime}|i\\in\\,[0,T^{\\prime}-1]\\}$ denotes a subsequence of   \n223 $\\{x_{i}|i\\in[0,T-1]\\}$ . Subsequently, we aim to insert several new depot arcs into the path $P^{'}$ to   \n224 achieve a lower cost while adhering to capacity constraints. To be specific, we recursively find the   \n225 return point that minimizes the overall increasing cost, which is implemented by the state transition   \n226 equation as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{f(P^{'})=\\underset{i}{\\operatorname*{min}}(f(P_{0:i}^{'})+S C(a r c_{x_{i}^{'}},a r c_{0})+S C(a r c_{0},a r c_{x_{i+1}^{'}})-S C(a r c_{x_{i}^{'}},a r c_{x_{i+1}^{'}}))}&{}\\\\ {\\mathrm{s.t.}\\quad0\\leq\\,i<T^{'}-1,}&{\\sum_{j=i+1}^{T^{'}-1}d e m a n d_{x_{j}^{'}}\\leq Q}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "227 where $S C(a r c_{x_{i}^{\\prime}},a r c_{0})=|e_{x_{i}^{\\prime}0}|$ denotes the shortest path cost from $a r c_{x_{i}^{\\prime}}$ to the depot. $Q$ is the   \n228 vehicle capacity. According to Eq. (10), we insert the depot arc $a r c_{0}$ after an appropriate position   \n229 $a r c_{x_{i}^{\\prime}}$ , which meets with the capacity constraint of the subpath $P_{i+1:T}^{'}{'}_{-1}$ . $f(\\cdot)$ denotes a state   \n230 featuring dynamic programming. By enumerating the position $i$ , we compute the minimum increasing   \n231 cost $f(\\boldsymbol{P}^{'})$ utilizing its sub-state $f(\\mathbf{{P}}_{0:i}^{\\prime})$ . The final minimum cost for path $_{P}$ is $f(P^{\\prime})+g(P^{\\prime})$ , here   \n232 $g(P^{\\prime})$ is the unoptimized cost of $P^{\\prime}$ . Since $P^{'}$ includes only the required edges, i.e., $T^{\\prime}=\\left|\\mathbf{E}_{R}\\right|$ ,   \n233 the time complexity of DP is ${\\mathcal O}(\\left|\\mathbf E_{R}\\right|^{2})$ . During Path Optimization, we use beam search to generate   \n234 two paths with the trained policy, one with capacity-constrained and one without. Both paths are   \n235 optimized using DP and the one with the minimum cost is selected as the final result. ", "page_idx": 5}, {"type": "text", "text": "236 5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "237 5.1 Setup", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "238 Problem Instances. We extracted sub-graphs from the roadmap of Beijing, China, obtained from   \n239 OpenStreetMap [8], to create CARP instances for both the training and testing phases. All instances   \n240 are divided into seven datasets, each representing different problem scales, as presented in Table 2.   \n241 Each dataset consists of 30,000 instances, further divided into two disjoint subsets: 20,000 instances   \n242 for training and the remaining for testing. For each instance, the vehicle capacity is set to 100.   \n243 Implementation Details. Our neural network is implemented using the PyTorch framework and   \n244 trained on a single NVIDIA RTX 3090 GPU. The heuristics and metaheuristics algorithms are   \n245 evaluated on an Intel Core i9-7920X with 24 cores and a CPU frequency of $4.4\\mathrm{GHz}$ . We optimize   \n246 the model using Adam optimizer [12]. The dimension of MDS coordinates $d$ is set to 8, and the   \n247 learning rate is set to $1e^{-\\bar{4}}$ . We set $\\epsilon$ in the PPO training at 0.1. Notably, our PPO training does not   \n248 incorporate discounted cumulative rewards, i.e., $\\gamma$ is set to 1.   \n49 Metrics and Settings. For each method and dataset, We compute the mean tour cost across all test   \n50 instances, indicated by \u201cCost\u201d. Employing the state-of-the-art MAENS [25] as a baseline, we measure   \n51 the \u201cCost\u201d gap between alternative algorithms and MAENS, indicated by \u201cGap\u201d. We compare our   \n52 method against the heuristic Path-Scanning algorithms (PS) [6, 22, 1] and two NN-based algorithms.   \n53 In the absence of publicly available code for prior NN-based CARP methods, we modify two NN  \n54 based NRP solvers to suit CARP, i.e, S2V-DQN [11] and VRP-DL [19]. Note that, for S2V-DQN,   \n55 we replace structure2vec with GAT to achieve more effective graph embedding learning. For our   \n56 method, we incrementally add supervised pre-training (SL), reinforcement learning fine-tuning (RL),   \n57 and path optimization (PO) to assess the effectiveness of our training scheme and optimization,   \n58 respectively. Due to the excessively long computation times of MAENS on larger-scale datasets, SL   \n59 is only performed on Task20, Task 30, and Task40. The batch size for SL is set to 128. During the RL   \n60 stage, greedy decoding is used to generate solutions, and except for the Task20 dataset, we utilize the   \n61 training results obtained from the preceding smaller-scale dataset to initialize the model. The beam   \n62 width in the PO stage is set to 2. For each dataset, we compare the mean cost of different methods on   \n63 10,000 problem instances. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "7ug4oSmN7l/tmp/f5177fb1535c69a251a7e3e4118032fec246c52f2727abd98b8b4547bd11715f.jpg", "table_caption": ["Table 2: Datasets information. $|\\mathbf{V}|$ is the number of nodes, $|{\\bf E}_{R}|$ is the number of required edges. demand represents the demand range for each required edge. Each dataset has 20,000 training instances and 10,000 test instances. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "7ug4oSmN7l/tmp/5462ad3d6f32f41c0fecba082f373a1e33d0d9bae5617339fafdffc82bc95e27.jpg", "table_caption": ["Table 3: Solution quality comparison. All methods are evaluated on 10,000 CARP instances in each scale. We measure the gap $(\\%)$ between different methods and MAENS. Methods marked with an asterisk were originally proposed for NRP, but we modified them to solve CARP. The best results are indicated in bold, while the second-best results are underlined. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "264 5.2 Evaluation Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "265 Solution Quality Table 6 shows the result. Our algorithm outperforms all heuristic and NN-based   \n266 methods across all scales, achieving costs comparable to MAENS, trailing by less than $8\\%$ . The   \n267 advantage over PS demonstrates that neural networks can learn more effective policies than hand  \n268 crafted ones, attributed to our well-designed modeling approach. Moreover, as the problem scale   \n269 increases, it becomes time-consuming to obtain CARP annotation by MAENS. Therefore, we leverage   \n270 the model pre-trained on small-scale instances as the initial policy for RL fine-tuning on Task50,   \n271 Task60, Task80, and Task100, yielding commendable performance. This proves the generalization of   \n272 our training scheme across varying problem scales. The performance gap with MAENS highlights   \n273 our algorithm\u2019s superiority in CARP-solving approaches.   \n227745 Generalization Ability. In Table 4, we assess DaAM\u2019s generalization on large-scale CARP in  \n276 stances using the policy trained on Task100. We remove MAENS and PS due to failing to run on   \n277 large-scale graphs, and remove S2V-DQN and VRP-DL due to poor performance. Although DaAM   \n278 is not trained on large-scale instances, it achieves or even exceeds the performance of PS, which   \n279 shows its potential application on larger-scale CARP instances.   \n80 Run Time. We compare the total time required for solving 100 CARP instances across datasets   \n1 Task20 to Task100 datasets using our method, MAENS, and PS algorithms, and show the run time in   \n2 log space. For datasets Task200 to Task600, we compare the same metric using variants of PS and   \n83 out method. For our method, we measured the solving time with and without PO. Fig. 2 demonstrates   \n84 that our method exhibits a significant speed advantage over MAENS, even outperforming variants of   \n35 PS [1] on most datasets. In comparison, the consumption time of MAENS increases exponentially as   \n6 the problem scale increases. Our method efficiently generates paths for large-scale CARP instances   \nby leveraging GPU data-level parallelism and CPU instruction-level parallelism. ", "page_idx": 6}, {"type": "table", "img_path": "7ug4oSmN7l/tmp/6ffa4c437f3a74d75e453d161c3856ed99033d7698591cf8b3a0143c3b4594b2.jpg", "table_caption": ["Table 4: Generalization to larger problem instances. All methods are evaluated on 10,000 CARP instances in each scale. For DaAM, we employ the policy trained on Task100. The best results are indicated in bold, while the second-best results are underlined. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "", "img_caption": ["Figure 2: Run time comparison. For each dataset, the total run time of each method on 100 CARP instances is shown. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Effectiveness of Combining MDS and GAT. To evaluate the combination of MDS and GAT for embedding exhibiting, we individually evaluate the performance of models using only MDS or GAT, as well as their combined performance. The experiment is conducted on Task30, Task40, ", "page_idx": 7}, {"type": "table", "img_path": "7ug4oSmN7l/tmp/0034641fed060c5b94f9558602b831e0db7ef42a45b7698ae536ea43cae1a3c7.jpg", "table_caption": ["Table 5: Costs of DaAM using different encoder. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Task50, and Task60 by comparing the average performance of 1,000 instances on each dataset. In the RL stage, we use the policy pre-trained on Task30 for initialization. Table 5 indicates that using MDS or GAT individually yields worse quality in most cases, highlighting that combining MDS and GAT enhances the model\u2019s capacity to capture arc correlations. Fig. 3 depicts the convergence trends in these scenes, which shows that the synergy of MDS and GAT contributes to the stability of training. ", "page_idx": 7}, {"type": "text", "text": "299 Solution Visualization. For a more intuitive understanding of the paths generated by different   \n300 methods, we visualize and compare the results of our method with PS [6] and MAENS across four   \n301 road scenes in Beijing. Fig. 4 visualizes all results alongside scene information. We observe that our   \n302 model obtains similar paths with MAENS since we leverage the annotation generated by MAENS for   \n303 supervised learning. MAENS paths exhibit superior spatial locality, clearly dividing the scene into   \n304 regions, whereas PS paths appear more random. ", "page_idx": 7}, {"type": "image", "img_path": "7ug4oSmN7l/tmp/b84882f7e52d9057c40fc2ecd7ea6f146fd8a718ce7f99abaf629d4e7197f969.jpg", "img_caption": ["Figure 3: Convergence trends of different methods in reinforcement learning training. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "7ug4oSmN7l/tmp/10941f2fc8da164b5c405e25ca7ca84f43576f7a7d881038a6617848d2f35e56.jpg", "img_caption": ["Figure 4: Qualitative comparison in four real street scenes. The paths are marked in different colors, with gray indicating roads that do not require service and red points indicating depots. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "305 6 Conclusion and Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "306 In this paper, we propose a learning-based CARP solver that competes with state-of-the-art meta  \n307 heuristics. Firstly, we encode the potential serving direction of edges into embeddings, ensuring   \n308 that edge directionality is taken into account in decision-making. Secondly, we present a supervised   \n309 reinforcement learning approach that effectively learns policies to solve CARP. With the aid of   \n310 these contributions, our method surpasses all heuristics and achieves performance comparable to   \n311 metaheuristics for the first time while maintaining excellent efficiency.   \n312 Limitations and future work. Decomposing undirected edges increases the decision elements,   \n313 which complicates the problem and may widens the gap between DaAM and traditional state-of-the  \n314 art approaches as the problem instance scale increases. Our future work focuses on designing an   \n315 efficient graph transformation method that does not significantly increase problem complexity. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "316 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "317 [1] Rafael Kendy Arakaki and Fabio Luiz Usberti. An efficiency-based path-scanning heuristic for   \n318 the capacitated arc routing problem. Computers & Operations Research (COR), 103:288\u2013295,   \n319 2019.   \n320 [2] Irwan Bello\\*, Hieu Pham\\*, Quoc V. Le, Mohammad Norouzi, and Samy Bengio. Neural com  \n321 binatorial optimization with reinforcement learning. In International Conference on Learning   \n322 Representations (ICLR), 2017.   \n323 [3] Hongyun Cai, Vincent W Zheng, and Kevin Chen-Chuan Chang. A comprehensive survey of   \n324 graph embedding: Problems, techniques, and applications. IEEE transactions on knowledge   \n325 and data engineering (TKDE), 30(9):1616\u20131637, 2018.   \n326 [4] Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving   \n327 the over-smoothing problem for graph neural networks from the topological view. In AAAI   \n328 conference on artificial intelligence (AAAI), 2020.   \n329 [5] Hanjun Dai, Bo Dai, and Le Song. Discriminative embeddings of latent variable models for   \n330 structured data. In International conference on machine learning (ICML), 2016.   \n331 [6] Bruce L Golden, James S DeArmon, and Edward K Baker. Computational experiments with   \n332 algorithms for a class of routing problems. Computers & Operations Research (COR), 10(1):47\u2013   \n333 59, 1983.   \n334 [7] Bruce L Golden and Richard T Wong. Capacitated arc routing problems. Networks, 11(3):305\u2013   \n335 315, 1981.   \n336 [8] Mordechai Haklay and Patrick Weber. Openstreetmap: User-generated street maps. IEEE   \n337 Pervasive computing, 7(4):12\u201318, 2008.   \n338 [9] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large   \n339 graphs. In Advances in neural information processing systems (NeurIPS), 2017.   \n340 [10] Wenjing Hong and Tonglin Liu. Faster capacitated arc routing: A sequence-to-sequence   \n341 approach. IEEE Access, 10:4777\u20134785, 2022.   \n342 [11] Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial   \n343 optimization algorithms over graphs. In Advances in neural information processing systems   \n344 (NeurIPS), 2017.   \n345 [12] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International   \n346 Conference on Learning Representations (ICLR), 2015.   \n347 [13] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional   \n348 networks. In International Conference on Learning Representations (ICLR), 2017.   \n349 [14] Wouter Kool, Herke van Hoof, and Max Welling. Attention, learn to solve routing problems! In   \n350 International Conference on Learning Representations (ICLR), 2019.   \n351 [15] Natalio Krasnogor and James Smith. A tutorial for competent memetic algorithms: model, taxon  \n352 omy, and design issues. IEEE transactions on Evolutionary Computation (TEVC), 9(5):474\u2013488,   \n353 2005.   \n354 [16] Han Li and Guiying Li. Learning to solve capacitated arc routing problems by policy gradient.   \n355 In IEEE Congress on Evolutionary Computation (CEC), 2019.   \n356 [17] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap,   \n357 Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforce  \n358 ment learning. In International conference on machine learning (ICML), pages 1928\u20131937,   \n359 2016.   \n360 [18] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G   \n361 Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.   \n362 Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015.   \n363 [19] Mohammadreza Nazari, Afshin Oroojlooy, Lawrence Snyder, and Martin Tak\u00e1c. Reinforcement   \n364 learning for solving the vehicle routing problem. In Advances in neural information processing   \n365 systems (NeurIPS), 2018.   \n366 [20] Muhilan Ramamoorthy and Violet R. Syrotiuk. Learning heuristics for arc routing problems.   \n367 Intelligent Systems with Applications (ISWA), 21:200300, 2024.   \n368 [21] Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self  \n369 critical sequence training for image captioning. In IEEE conference on computer vision and   \n370 pattern recognition (CVPR), 2017.   \n371 [22] Lu\u00eds Santos, Jo\u00e3o Coutinho-Rodrigues, and John R Current. An improved heuristic for the   \n372 capacitated arc routing problem. Computers & Operations Research (COR), 36(9):2632\u20132637,   \n373 2009.   \n374 [23] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal   \n375 policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n376 [24] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural   \n377 networks. In Advances in neural information processing systems (NeurIPS), 2014.   \n378 [25] Ke Tang, Yi Mei, and Xin Yao. Memetic algorithm with extended neighborhood search for   \n379 capacitated arc routing problems. IEEE Transactions on Evolutionary Computation (TEVC),   \n380 13(5):1151\u20131166, 2009.   \n381 [26] Cong Dao Tran and Truong Son Hy. Graph attention-based deep reinforcement learning for solv  \n382 ing the chinese postman problem with load-dependent costs. arXiv preprint arXiv:2310.15516,   \n383 2023.   \n384 [27] Petar Velic\u02c7kovic\u00b4, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua   \n385 Bengio. Graph Attention Networks. In International Conference on Learning Representations   \n386 (ICLR), 2018.   \n387 [28] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances in neural   \n388 information processing systems (NeurIPS), 2015.   \n389 [29] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforce  \n390 ment learning. Machine learning, 8:229\u2013256, 1992.   \n391 [30] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger.   \n392 Simplifying graph convolutional networks. In International conference on machine learning   \n393 (ICML), 2019.   \n394 [31] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A   \n395 comprehensive survey on graph neural networks. IEEE transactions on neural networks and   \n396 learning systems (TNNLS), 32(1):4\u201324, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "397 A Appendix ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "398 A.1 Source Code and Dataset ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "399 The source code of DaAM and the datasets used for testing are available at DaAM. Once the paper is   \n400 accepted, we will promptly release the source code and datasets. ", "page_idx": 11}, {"type": "text", "text": "401 A.2 Pseudocode of PPO with self-critical strategy ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "402 Algorithm 1 presents the pseudocode for the PPO training algorithm we used. In the code implemen  \n403 tation, the trajectory $\\tau_{s}^{\\theta}$ can be replaced by $(s,a)$ \u2019s original trajectory $\\tau_{o}$ for efficiency. Once $\\tau_{o}$ is   \n404 sampled, the cumulative rewards from any state $s\\in\\tau_{o}$ can be quickly computed. ", "page_idx": 11}, {"type": "text", "text": "Algorithm 1 PPO algorithm with self-critical strategy ", "text_level": 1, "page_idx": 11}, {"type": "table", "img_path": "7ug4oSmN7l/tmp/abdce0e681804865740ad1fad13e20b35166d9a40fc45c9c3f679c02a774121e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 11}, {"type": "text", "text": "405 A.3 Experimental Results of Additional Datasets ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "406 For small-scale problem instances, we generated two additional datasets, Task30 and Task50. In 407 Task30 the range of $\\vert V\\vert$ is 25-30, while in Task50, it spans 55-60. Correspondingly, $|\\bf E_{R}|$ is set to 408 30 and 50, respectively The demand for each edge ranges from 5 to 10 in both tasks. Table 6 is the complete experimental data from the solution quality experiments. ", "page_idx": 11}, {"type": "text", "text": "Table 6: Solution quality comparison. All methods are evaluated on 10,000 CARP instances in each scale. We measure the gap $(\\%)$ between different methods and MAENS. Methods marked with an asterisk were originally proposed for NRP, but we modified them to solve CARP. The gray indicates that MAENS is taken as the baseline when calculating \u201cGap\u201d. The best results are indicated in bold, while the second-best results are underlined. ", "page_idx": 11}, {"type": "table", "img_path": "7ug4oSmN7l/tmp/eb7ef33d18a82e73bb19f6937e0a98363792320c10576537317c98e7663d8275.jpg", "table_caption": [], "table_footnote": [], "page_idx": 11}, {"type": "text", "text": "410 A.4 Licences of Assets Used for Experiments ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "411 The code we used does not require special consent from the authors. We follow their licenses as   \n412 specified below:   \n413 \u2022 https://github.com/wouterkool/attention-learn-to-route: MIT Licence.   \n414 \u2022 https://github.com/Hanjun-Dai/graph_comb_opt: MIT Licence. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "415 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "416 1. Claims   \n417 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n418 paper\u2019s contributions and scope?   \n419 Answer: [Yes]   \n420 Justification: See Abstract and Introduction.   \n421 Guidelines:   \n422 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n423 made in the paper.   \n424 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n425 contributions made in the paper and important assumptions and limitations. A No or   \n426 NA answer to this question will not be perceived well by the reviewers.   \n427 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n428 much the results can be expected to generalize to other settings.   \n429 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n430 are not attained by the paper.   \n431 2. Limitations   \n432 Question: Does the paper discuss the limitations of the work performed by the authors?   \n433 Answer: [Yes]   \n434 Justification: See section Conclusion and Limitations.   \n435 Guidelines:   \n436 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n437 the paper has limitations, but those are not discussed in the paper.   \n438 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n439 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n440 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n441 model well-specification, asymptotic approximations only holding locally). The authors   \n442 should reflect on how these assumptions might be violated in practice and what the   \n443 implications would be.   \n444 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n445 only tested on a few datasets or with a few runs. In general, empirical results often   \n446 depend on implicit assumptions, which should be articulated.   \n447 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n448 For example, a facial recognition algorithm may perform poorly when image resolution   \n449 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n450 used reliably to provide closed captions for online lectures because it fails to handle   \n451 technical jargon.   \n452 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n453 and how they scale with dataset size.   \n454 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n455 address problems of privacy and fairness.   \n456 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n457 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n458 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n459 judgment and recognize that individual actions in favor of transparency play an impor  \n460 tant role in developing norms that preserve the integrity of the community. Reviewers   \n461 will be specifically instructed to not penalize honesty concerning limitations.   \n462 3. Theory Assumptions and Proofs   \n463 Question: For each theoretical result, does the paper provide the full set of assumptions and ", "page_idx": 13}, {"type": "text", "text": "464 a complete (and correct) proof? ", "page_idx": 13}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 13}, {"type": "text", "text": "66 Justification: This papar does not include theoretical results.   \n67 Guidelines:   \n68 \u2022 The answer NA means that the paper does not include theoretical results.   \n69 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n70 referenced.   \n71 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n72 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n73 they appear in the supplemental material, the authors are encouraged to provide a short   \n74 proof sketch to provide intuition.   \n75 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n76 by formal proofs provided in appendix or supplemental material.   \n77 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "478 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "79 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n480 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n81 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: This papar discusses the detail to reproduce the main experimental results of the paper. ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 14}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 14}, {"type": "text", "text": "521 Answer: [Yes]   \n522 Justification: The source code and datasets are provided in the Appendix.   \n523 Guidelines:   \n524 \u2022 The answer NA means that paper does not include experiments requiring code.   \n525 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n526 public/guides/CodeSubmissionPolicy) for more details.   \n527 \u2022 While we encourage the release of code and data, we understand that this might not be   \n528 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n529 including code, unless this is central to the contribution (e.g., for a new open-source   \n530 benchmark).   \n531 \u2022 The instructions should contain the exact command and environment needed to run to   \n532 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n533 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n534 \u2022 The authors should provide instructions on data access and preparation, including how   \n535 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n536 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n537 proposed method and baselines. If only a subset of experiments are reproducible, they   \n538 should state which ones are omitted from the script and why.   \n539 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n540 versions (if applicable).   \n541 \u2022 Providing as much information as possible in supplemental material (appended to the   \n542 paper) is recommended, but including URLs to data and code is permitted.   \n543 6. Experimental Setting/Details   \n544 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n545 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n546 results?   \n547 Answer: [Yes]   \n548 Justification: See section Experiments.   \n549 Guidelines:   \n550 \u2022 The answer NA means that the paper does not include experiments.   \n551 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n552 that is necessary to appreciate the results and make sense of them.   \n553 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n554 material.   \n555 7. Experiment Statistical Significance   \n556 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n557 information about the statistical significance of the experiments?   \n558 Answer: [No]   \n559 Justification: Since the experimental results are deterministic, we did not repeat the exper  \n560 iments multiple times. However, to reduce errors, we calculated the average over 10,000   \n561 problem instances for each dataset of any scale.   \n562 Guidelines:   \n563 \u2022 The answer NA means that the paper does not include experiments.   \n564 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n565 dence intervals, or statistical significance tests, at least for the experiments that support   \n566 the main claims of the paper.   \n567 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n568 example, train/test split, initialization, random drawing of some parameter, or overall   \n569 run with given experimental conditions).   \n570 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n571 call to a library function, bootstrap, etc.)   \n572 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n573 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n574 of the mean.   \n575 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n576 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n577 of Normality of errors is not verified.   \n578 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n579 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n580 error rates).   \n581 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n582 they were calculated and reference the corresponding figures or tables in the text.   \n583 8. Experiments Compute Resources   \n584 Question: For each experiment, does the paper provide sufficient information on the com  \n585 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n586 the experiments?   \n587 Answer: [Yes]   \n588 Justification: See section Experiments.   \n589 Guidelines:   \n590 \u2022 The answer NA means that the paper does not include experiments.   \n591 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n592 or cloud provider, including relevant memory and storage.   \n593 \u2022 The paper should provide the amount of compute required for each of the individual   \n594 experimental runs as well as estimate the total compute.   \n595 \u2022 The paper should disclose whether the full research project required more compute   \n596 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n597 didn\u2019t make it into the paper).   \n598 9. Code Of Ethics   \n599 Question: Does the research conducted in the paper conform, in every respect, with the   \n600 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n601 Answer: [Yes]   \n602 Justification:   \n603 Guidelines:   \n604 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n605 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n606 deviation from the Code of Ethics.   \n607 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n608 eration due to laws or regulations in their jurisdiction).   \n609 10. Broader Impacts   \n610 Question: Does the paper discuss both potential positive societal impacts and negative   \n611 societal impacts of the work performed?   \n612 Answer: [No]   \n613 Justification: The aim of our work is to provide a better solution for a class of combinatorial   \n614 optimization problems, though it is difficult to predict its impact on society.   \n615 Guidelines:   \n616 \u2022 The answer NA means that there is no societal impact of the work performed.   \n617 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n618 impact or why the paper does not address societal impact.   \n619 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n620 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n621 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n622 groups), privacy considerations, and security considerations.   \n623 \u2022 The conference expects that many papers will be foundational research and not tied   \n624 to particular applications, let alone deployments. However, if there is a direct path to   \n625 any negative applications, the authors should point it out. For example, it is legitimate   \n626 to point out that an improvement in the quality of generative models could be used to   \n627 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n628 that a generic algorithm for optimizing neural networks could enable people to train   \n629 models that generate Deepfakes faster.   \n630 \u2022 The authors should consider possible harms that could arise when the technology is   \n631 being used as intended and functioning correctly, harms that could arise when the   \n632 technology is being used as intended but gives incorrect results, and harms following   \n633 from (intentional or unintentional) misuse of the technology.   \n634 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n635 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n636 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n637 feedback over time, improving the efficiency and accessibility of ML).   \n638 11. Safeguards   \n639 Question: Does the paper describe safeguards that have been put in place for responsible   \n640 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n641 image generators, or scraped datasets)?   \n642 Answer: [NA]   \n643 Justification:   \n644 Guidelines:   \n645 \u2022 The answer NA means that the paper poses no such risks.   \n646 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n647 necessary safeguards to allow for controlled use of the model, for example by requiring   \n648 that users adhere to usage guidelines or restrictions to access the model or implementing   \n649 safety filters.   \n650 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n651 should describe how they avoided releasing unsafe images.   \n652 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n653 not require this, but we encourage authors to take this into account and make a best   \n654 faith effort.   \n655 12. Licenses for existing assets   \n656 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n657 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n658 properly respected?   \n659 Answer: [Yes]   \n660 Justification: See the Appendix.   \n661 Guidelines:   \n662 \u2022 The answer NA means that the paper does not use existing assets.   \n663 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n664 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n665 URL.   \n666 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n667 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n668 service of that source should be provided.   \n669 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n670 package should be provided. For popular datasets, paperswithcode.com/datasets   \n671 has curated licenses for some datasets. Their licensing guide can help determine the   \n672 license of a dataset.   \n673 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n674 the derived asset (if it has changed) should be provided.   \n675 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n676 the asset\u2019s creators.   \n677 13. New Assets   \n678 Question: Are new assets introduced in the paper well documented and is the documentation   \n679 provided alongside the assets?   \n680 Answer: [Yes]   \n681 Justification: See the Appendix.   \n682 Guidelines:   \n683 \u2022 The answer NA means that the paper does not release new assets.   \n684 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n685 submissions via structured templates. This includes details about training, license,   \n686 limitations, etc.   \n687 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n688 asset is used.   \n689 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n690 create an anonymized URL or include an anonymized zip file.   \n691 14. Crowdsourcing and Research with Human Subjects   \n692 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n693 include the full text of instructions given to participants and screenshots, if applicable, as   \n694 well as details about compensation (if any)?   \n695 Answer: [NA]   \n696 Justification:   \n697 Guidelines:   \n698 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n699 human subjects.   \n700 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n701 tion of the paper involves human subjects, then as much detail as possible should be   \n702 included in the main paper.   \n703 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n704 or other labor should be paid at least the minimum wage in the country of the data   \n705 collector. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 18}]