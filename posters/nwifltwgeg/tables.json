[{"figure_path": "NwiFLtWGEg/tables/tables_13_1.jpg", "caption": "Table 1: Kinematic parameters of the tasks. n: number of limbs. d1: number of DoFs of the torso limb (i = 1). N1-DoF, N2-DoF, N3-DOF: number of 1-, 2-, 3-DoF joints.", "description": "This table lists kinematic parameters for ten continuous control tasks from the DeepMind Control Suite.  For each task, it provides the number of limbs (n), the number of degrees of freedom (DoFs) in the torso (d1), and the counts of joints with 1, 2, and 3 DoFs (n1-DoF, n2-DoF, n3-DoF, respectively). This information is crucial for understanding the complexity of each task and how it relates to the proposed data augmentation method.", "section": "A.1 Task details"}, {"figure_path": "NwiFLtWGEg/tables/tables_13_2.jpg", "caption": "Table 2: Sensory observations in the tasks.", "description": "This table lists the sensory observations for all ten tasks in the paper. It categorizes them into invariant and equivariant features under SO\u011d(3) rotations. Invariant features remain unchanged under these rotations, while equivariant features transform according to the rotation. The table provides a count of each type of feature for each task, offering a detailed view of the sensory input used in the experiments.", "section": "A.1 Task details"}, {"figure_path": "NwiFLtWGEg/tables/tables_14_1.jpg", "caption": "Table 3: DDPG hyperparameters used in our experiments.", "description": "This table lists the hyperparameters used for the Deep Deterministic Policy Gradient (DDPG) algorithm in the experiments.  It details settings for learning rate, optimizer, n-step return, mini-batch size, actor and target network update frequencies, target network soft-update, target policy smoothing, MLP hidden size, replay buffer capacity, discount factor, seed frames, exploration steps, exploration standard deviation schedule, and action repeat.", "section": "A.2 DDPG hyperparameters"}, {"figure_path": "NwiFLtWGEg/tables/tables_15_1.jpg", "caption": "Table 4: Hyperparameters for our SEGNN-based DDPG implementation for Reacher_hard.", "description": "This table lists the hyperparameters used in the SEGNN-based DDPG implementation for the Reacher_hard task.  It includes settings for the learning rate, optimizer, n-step return, mini-batch size, actor and target network update frequencies, target network soft update, target policy smoothing standard deviation clip value, SEGNN hidden size, replay buffer capacity, discount factor, seed frames, exploration steps, exploration standard deviation schedule, and action repeat.", "section": "A.3 SEGNN"}]