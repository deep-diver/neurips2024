[{"figure_path": "E7en5DyO2G/tables/tables_5_1.jpg", "caption": "Table 1: The 4 Hessian approximations.", "description": "This table summarizes four different methods for approximating the Hessian matrix used in the Bayesian Online Natural Gradient (BONG) algorithm.  The methods are categorized by whether they use Monte Carlo (MC) sampling or a linearization approach (LIN), and whether they calculate the full Hessian or use an empirical Fisher approximation (EF).  The equation numbers reference the formulas used for each approximation within the paper.", "section": "4 Methods"}, {"figure_path": "E7en5DyO2G/tables/tables_6_1.jpg", "caption": "Table 2: The 4 update algorithms.", "description": "This table summarizes four different update algorithms: BONG, BOG, BLR, and BBB.  Each algorithm optimizes a different loss function (either expected negative log-likelihood or variational loss) using either natural gradient descent (NGD) or vanilla gradient descent (GD). The number of iterations (I) is also specified. BONG uses NGD with a single iteration, BOG uses GD with a single iteration, BLR uses NGD with multiple iterations, and BBB uses GD with multiple iterations.", "section": "4 Methods"}, {"figure_path": "E7en5DyO2G/tables/tables_7_1.jpg", "caption": "Table 3: Time complexity of the algorithms. P: params, C': observation dim, M: MC samples, I: iterations, R: DLR rank. We assume P \u226b {I, C, R, M} so display only the terms of leading order in P. Time complexities for MC-HESS algorithms (not shown) are always at least as great as for the corresponding MC-EF. Full (full covariance) and Diag (diagonal covariance) columns indicate natural parameters; corresponding algorithms using moment parameters have the same complexities except BOG-FC_MOM which is O(MP\u00b2) for MC-EF, O(CP2) for LIN-HESS, and O(P2) for LIN-EF. [Names] correspond to the following existing methods (or variants thereof) in the literature: RVGA: [Lambert et al., 2021] (explicit update version); VON: [Khan et al., 2018b] (modified for online); SLANG: [Mishkin et al., 2018] (modified for online); BBB: [Blundell et al., 2015] (modified for online and uses moment parameters); CM-EKF: [Ollivier, 2018, Tronarp et al., 2018]; VD-EKF: [Chang et al., 2022]; LO-FI: [Chang et al., 2023].", "description": "This table shows the time complexity of various Bayesian online learning algorithms.  It breaks down the complexity by the update rule (BONG, BLR, BOG, BBB), Hessian approximation method (MC-EF, LIN-HESS, LIN-EF), and variational family (Full, Diag, DLR).  The table also provides references to related work for several of the algorithms.", "section": "4 Methods"}, {"figure_path": "E7en5DyO2G/tables/tables_18_1.jpg", "caption": "Table 3: Time complexity of the algorithms. P: params, C': observation dim, M: MC samples, I: iterations, R: DLR rank. We assume P \u226b {I, C, R, M} so display only the terms of leading order in P. Time complexities for MC-HESS algorithms (not shown) are always at least as great as for the corresponding MC-EF. Full (full covariance) and Diag (diagonal covariance) columns indicate natural parameters; corresponding algorithms using moment parameters have the same complexities except BOG-FC_MOM which is O(MP\u00b2) for MC-EF, O(CP2) for LIN-HESS, and O(P2) for LIN-EF. [Names] correspond to the following existing methods (or variants thereof) in the literature: RVGA: [Lambert et al., 2021] (explicit update version); VON: [Khan et al., 2018b] (modified for online); SLANG: [Mishkin et al., 2018] (modified for online); BBB: [Blundell et al., 2015] (modified for online and uses moment parameters); CM-EKF: [Ollivier, 2018, Tronarp et al., 2018]; VD-EKF: [Chang et al., 2022]; LO-FI: [Chang et al., 2023].", "description": "This table shows the time complexity of various Bayesian online learning algorithms. The complexity is expressed in terms of the number of parameters (P), observation dimension (C), Monte Carlo samples (M), iterations (I), and DLR rank (R).  It also indicates the corresponding existing methods in the literature for each algorithm. The table shows that the algorithms using natural parameters often have lower time complexity.", "section": "4 Methods"}]