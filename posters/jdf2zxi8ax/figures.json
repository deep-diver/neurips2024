[{"figure_path": "jDF2ZXI8AX/figures/figures_0_1.jpg", "caption": "Figure 1: Full Pipeline of MV2Cyl.", "description": "This figure illustrates the overall pipeline of the MV2Cyl method. It starts with multi-view images of an object as input. These images are processed through 2D segmentation networks to extract 2D surface and curve segments. The 2D information is then integrated into a 3D representation using neural fields, resulting in 3D surface and curve reconstructions. Finally, the 3D information is used to extract CAD parameters (primitives, translation, scale, boolean operations, extrusion parameters) to reconstruct the final CAD model.", "section": "3 MV2Cyl: Reconstructing 3D Extrusion Cylinders from Multi-View Images"}, {"figure_path": "jDF2ZXI8AX/figures/figures_2_1.jpg", "caption": "Figure 1: Full Pipeline of MV2Cyl.", "description": "The figure shows the overall pipeline of MV2Cyl. It starts with multi-view images as input, which are then processed by two 2D segmentation networks for surface and curve information extraction.  The surface segmentation network provides instance segmentation and start/end/barrel segmentation. Similarly, the curve segmentation network extracts instance segmentation and start/end segmentation. The extracted 2D information is then integrated into a 3D field using neural fields. Finally, a 3D reconstruction process is performed to obtain the reconstructed CAD parameters and the primitives. This figure provides a high-level overview of the proposed method.", "section": "3 MV2Cyl: Reconstructing 3D Extrusion Cylinders from Multi-View Images"}, {"figure_path": "jDF2ZXI8AX/figures/figures_3_1.jpg", "caption": "Figure 2: Example of segmentation prediction. From left to right: input rendered image, surface instance segmentation, surface start-end-barrel segmentation, curve instance segmentation, and curve start-end segmentation.", "description": "This figure shows example outputs of the 2D segmentation networks.  The input is a rendered image of a 3D object. The network predicts four segmentation maps: surface instance segmentation (identifying individual extrusion cylinders), surface start-end-barrel segmentation (classifying surface regions as start, end, or barrel), curve instance segmentation (identifying individual extrusion cylinder curves), and curve start-end segmentation (classifying curve regions as start or end).  The figure visually demonstrates the different types of segmentations produced by the networks.", "section": "3.2 Learning 2D Priors for 3D Extrusions"}, {"figure_path": "jDF2ZXI8AX/figures/figures_4_1.jpg", "caption": "Figure 3: Overview of the learned surface and curve fields. (Left-to-Right) Density field of surface, instance semantic field of surface, start-end semantic field of surface, density field of curve, instance semantic field of curve, and start-end semantic field of curve.", "description": "This figure illustrates the learned 3D fields for both surface and curve information.  The left side shows the surface fields: a density field showing the likelihood of a point being on a surface, an instance semantic field showing which instance each point belongs to, and a start-end semantic field indicating whether a point is on the start, end, or barrel section of an extrusion. The right side shows similar fields for curve information, representing the 2D curves at the top and bottom of each extrusion cylinder. This figure highlights the key components of MV2Cyl, demonstrating how 2D information from multi-view images is integrated into a coherent 3D representation.", "section": "3 MV2Cyl: Reconstructing 3D Extrusion Cylinders from Multi-View Images"}, {"figure_path": "jDF2ZXI8AX/figures/figures_7_1.jpg", "caption": "Figure 4: Converting 3D reconstructed geometry and semantics into CAD parameters.", "description": "This figure illustrates the process of converting 3D reconstructed geometry and semantics into CAD parameters. The process involves four main steps. First, a plane is fitted to the reconstructed 3D point cloud using RANSAC, and the curve points are projected onto this plane. Then, the projected curve is normalized, and a curve is fitted to it. Third, the start and end centers of the extrusion are found, and the extrusion height is computed. Finally, the instance center is computed.  The output is the CAD parameters for an extrusion cylinder: extrusion axis (n), sketch (S), extrusion height (h), and extrusion center (c).", "section": "3 MV2Cyl: Reconstructing 3D Extrusion Cylinders from Multi-View Images"}, {"figure_path": "jDF2ZXI8AX/figures/figures_8_1.jpg", "caption": "Figure 5: Qualitative comparisons with the baselines. Each instance is identified by a different color. MV2Cyl produces high-quality geometry and even outperforms Point2Cyl [58] that directly consumes 3D point clouds. Furthermore, the comparison against a naive baseline that pipelines NeuS2 [62], a multi-view surface reconstruction technique, to Point2Cyl demonstrates the importance of edge information when inferring 3D structures.", "description": "This figure compares the qualitative results of MV2Cyl against three baselines: Point2Cyl, NeuS2+Point2Cyl (using NeuS2 for 3D reconstruction from multi-view images as a pre-processing step before Point2Cyl), and a naive pipeline combining NeuS2 and Point2Cyl.  Each row shows the ground truth CAD model, followed by the reconstruction from MV2Cyl and the three baselines.  The results illustrate MV2Cyl's superior ability to reconstruct complex shapes, even outperforming methods that directly use clean 3D point cloud data.  The comparison with the NeuS2+Point2Cyl baseline highlights the importance of using edge information (which is preserved better in multi-view images than in point clouds produced by NeuS2) for accurate 3D structure reconstruction.", "section": "4 Results"}, {"figure_path": "jDF2ZXI8AX/figures/figures_9_1.jpg", "caption": "Figure 6: Example of a failure case.", "description": "This figure shows an example where MV2Cyl fails to reconstruct a part of the object. The target CAD model contains an inset hexagonal cylinder on one side, which is hidden by an outer cylinder.  MV2Cyl successfully reconstructs the outer cylinder but fails to reconstruct the inset cylinder because it is fully occluded in the input multi-view images.", "section": "4.2 Real Object Reconstruction"}, {"figure_path": "jDF2ZXI8AX/figures/figures_18_1.jpg", "caption": "Figure A8: Example of the inferred segmentation maps for a real image (top row) and a synthetic image (bottom row). The first image in the top row is the real image, which is then grayscaled (second image) and background-removed (third image). The fine-tuned SAM [24] model segments the processed image into the instance segment (fourth column) and the start-end segment (last column). When comparing the real and synthetic images from similar viewpoints, the segmentation outcomes are similarly effective.", "description": "This figure shows the results of applying the fine-tuned Segment Anything Model (SAM) to both real and synthetic images.  The left side displays the processing steps applied to a real-world image: conversion to grayscale, background removal, and then instance and start/end segmentation using SAM.  The right side shows the corresponding processed synthetic image for comparison. The goal is to demonstrate that the SAM model produces similar, useful segmentations for both real and synthetic images, despite differences in image characteristics.", "section": "B. Real-World Demo"}, {"figure_path": "jDF2ZXI8AX/figures/figures_18_2.jpg", "caption": "Figure A9: Processing steps for aligning the pose of the real demo output with the ground truth CAD point cloud.", "description": "This figure shows the three steps involved in aligning the real-world 3D model output to the ground truth CAD model for quantitative evaluation.  First, the real demo CAD output is exported to a mesh. Second, a point cloud is sampled from this mesh and registered to the ground truth CAD model's point cloud using a shape registration technique (presumably ICP, Iterative Closest Point). Third, Open3D's ICP point cloud registration is used to finally align the point clouds and compute the Chamfer distance. This process ensures accurate comparison of the reconstructed model to the ground truth.", "section": "A Ablation Study"}, {"figure_path": "jDF2ZXI8AX/figures/figures_19_1.jpg", "caption": "Figure A8: Example of the inferred segmentation maps for a real image (top row) and a synthetic image (bottom row). The first image in the top row is the real image, which is then grayscaled (second image) and background-removed (third image). The fine-tuned SAM [24] model segments the processed image into the instance segment (fourth column) and the start-end segment (last column). When comparing the real and synthetic images from similar viewpoints, the segmentation outcomes are similarly effective.", "description": "This figure shows a comparison of segmentation results between real and synthetic images. The real image undergoes preprocessing steps such as converting to grayscale and removing the background before being processed by the fine-tuned SAM model, which segments the image into instance segments (different parts of the objects) and start-end segments (start and end planes of extrusions).  The results demonstrate that the fine-tuned model performs similarly well on both real and synthetic images, highlighting its robustness.", "section": "B Real-World Demo"}, {"figure_path": "jDF2ZXI8AX/figures/figures_21_1.jpg", "caption": "Figure A10: Network architecture of Point2Cyl [58]", "description": "This figure shows the network architecture of the Point2Cyl method, which is used as a baseline in the paper.  The architecture takes as input a point cloud representing the object's geometry and is designed to infer the parameters of the extrusion cylinders that make up the object. It consists of several modules: a PointNet++ backbone to extract features from the point cloud; a segmentation and reordering module to group points belonging to the same cylinder instance; a differentiable module for parameter estimation, a projector operator, and modules to predict 2D sketches.  The network is trained to minimize a loss function that combines losses related to segmentation, boundary box fitting, sketch reconstruction, and parameter estimation.", "section": "D Comparison with Curve+Point2Cyl [58]"}, {"figure_path": "jDF2ZXI8AX/figures/figures_21_2.jpg", "caption": "Figure A11: Network architecture of Curve+Point2Cyl [58]", "description": "This figure shows the network architecture of Curve+Point2Cyl, a modified version of Point2Cyl that incorporates curve information into the input point cloud.  The original Point2Cyl takes as input a tensor of size N x 3, representing the point cloud. Curve+Point2Cyl concatenates M curve points after the N input points, adding an extra channel with labels (1 for curve points, 0 for others). The rest of the architecture remains unchanged. The figure highlights the inputs, network components, variables, external solver, differentiable module, and the training and inference stages.", "section": "D Comparison with Curve+Point2Cyl [58]"}, {"figure_path": "jDF2ZXI8AX/figures/figures_23_1.jpg", "caption": "Figure A7: Ablation study. Each example illustrates the limitations of surface and curve representations. The surface-only model, whose instance label point cloud, start-end label point cloud, and output CAD model are shown in columns 2, 3, and 4, The table shows the final output of the surface-only model and the curve-only model along with their semantic fields. The surface-only model failed to reconstruct the proper start-end field and thus the sketch is invalid. The curve-only model was able to reconstruct the geometry in case (a) but predicted zero height for the thin cylinder in (b). Ours were able to make better estimations in both cases.", "description": "This figure presents an ablation study comparing the performance of using only surface information, only curve information, and both surface and curve information for 3D CAD reconstruction. The results demonstrate that using both surface and curve information leads to more accurate and complete reconstructions, particularly when dealing with occlusion or challenging geometric features.", "section": "A Ablation Study"}, {"figure_path": "jDF2ZXI8AX/figures/figures_24_1.jpg", "caption": "Figure A13: Example of the binary operation prediction of the reconstructed extrusion cylinders.", "description": "This figure illustrates the process of determining the optimal combination of binary operations (union, difference, intersection) among the reconstructed extrusion cylinders.  Given K reconstructed parts, there are 2<sup>K</sup> possible combinations of operations. Each combination is rendered using the camera poses from the input multi-view images, and the combination that yields the rendered image with the minimal L2 distance to the original input image is chosen as the best one. This example shows an instance where K=3.", "section": "E.6 Binary Operation Prediction"}, {"figure_path": "jDF2ZXI8AX/figures/figures_25_1.jpg", "caption": "Figure 3: Overview of the learned surface and curve fields. (Left-to-Right) Density field of surface, instance semantic field of surface, start-end semantic field of surface, density field of curve, instance semantic field of curve, and start-end semantic field of curve.", "description": "This figure illustrates the learned 3D fields used for surface and curve reconstruction in MV2Cyl.  It shows six different fields: surface density field, surface instance semantic field, surface start/end semantic field, curve density field, curve instance semantic field, and curve start/end semantic field. Each field provides different types of information about the object's geometry, which are then combined for 3D reconstruction. The left-to-right arrangement corresponds to the order in which the fields are presented.", "section": "3.3 Integrating 2D Segments into a 3D Field"}, {"figure_path": "jDF2ZXI8AX/figures/figures_28_1.jpg", "caption": "Figure A13: Example of the binary operation prediction of the reconstructed extrusion cylinders.", "description": "This figure shows an example of how the proposed method predicts binary operations for the reconstructed extrusion cylinders. The input image of the corresponding camera pose is given along with 2<sup>k</sup> combinations of k reconstructed parts.  The method renders the images using camera poses from the input images and selects the combination with the lowest L2 distance to the given input image.", "section": "E.6 Binary Operation Prediction"}, {"figure_path": "jDF2ZXI8AX/figures/figures_29_1.jpg", "caption": "Figure A13: Example of the binary operation prediction of the reconstructed extrusion cylinders.", "description": "This figure shows an example of how the method predicts binary operations.  It demonstrates that given a set of K reconstructed parts, it evaluates 2^K possible combinations of those parts with various binary operations (+, -). It then renders each combination to compare against the input multi-view image and chooses the best combination based on the minimal L2 distance from the input image. This effectively reconstructs the final CAD model using the identified binary operations.", "section": "E.6 Binary Operation Prediction"}]