[{"type": "text", "text": "SelectIT: Selective Instruction Tuning for LLMs via Uncertainty-Aware Self-Reflection ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Liangxin Liu1 Xuebo ${\\mathbf{L}}{\\mathbf{i}}{\\mathbf{u}}^{1*}$ Derek F. Wong2 Dongfang Li1 Ziyi Wang1 Baotian $\\mathbf{H}\\mathbf{u}^{1}$ Min Zhang1 ", "page_idx": 0}, {"type": "text", "text": "1Institute of Computing and Intelligence, Harbin Institute of Technology, Shenzhen, China $\\mathrm{^2NLP^{2}C T}$ Lab, Department of Computer and Information Science, University of Macau lliangxin967@gmail.com, {liuxuebo,hubaotian,zhangmin2021}@hit.edu.cn derekfw@um.edu.mo, {crazyofapple,ziyiwang676}@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Instruction tuning (IT) is crucial to tailoring large language models (LLMs) towards human-centric interactions. Recent advancements have shown that the careful selection of a small, high-quality subset of IT data can significantly enhance the performance of LLMs. Despite this, common approaches often rely on additional models or data, which increases costs and limits widespread adoption. In this work, we propose a novel approach, termed SelectIT, that capitalizes on the foundational capabilities of the LLM itself. Specifically, we exploit the intrinsic uncertainty present in LLMs to more effectively select high-quality IT data, without the need for extra resources. Furthermore, we introduce a curated IT dataset, the Selective Alpaca, created by applying SelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT using Selective Alpaca leads to substantial model ability enhancement. The robustness of SelectIT has also been corroborated in various foundation models and domain-specific tasks. Our findings suggest that longer and more computationally intensive IT data may serve as superior sources of IT, offering valuable insights for future research in this area. Data, code, and scripts are freely available at https://github.com/Blue-Raincoat/SelectIT. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) have attracted much attention due to their impressive capabilities in following instructions and solving intricate problems (Touvron et al., 2023b,a; Achiam et al., 2023; Penedo et al., 2023). A crucial aspect of enhancing LLMs\u2019 performance is instruction tuning (IT), which involves the supervised adjustment of LLMs using pairs of instructional data, essential for refining the models\u2019 ability to accurately respond to human instructions. Recent groundbreaking research, such as the LIMA (Zhou et al., 2023), highlights the critical importance of instructional data quality over quantity. Contrary to the approach of merely increasing the dataset size, a carefully selected, smaller dataset of higher quality can significantly improve LLMs\u2019 performance. ", "page_idx": 0}, {"type": "image", "img_path": "QNieOPt4fg/tmp/9192f81ea639385c1e67ad8c54f169a1cd77c69f3c97f9f5fd8c71d428b6d4f1.jpg", "img_caption": ["Figure 1: Existing advanced data selection strategies rely heavily on external models or data; however, SelectIT effectively overcomes this limitation. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Despite the development of various high-quality data selection methods, they often depend on external resources, limiting wider implementation. External Model: Chen et al. (2024); Liu et al. (2023) propose the employment of closed-source LLMs to evaluate or rank IT data. To circumvent the closed-source limitations, Li et al. (2023a,b); Kung et al. (2023) recommend fine-tuning open-source LLMs, which requires more computational resources. External Data: Cao et al. (2023) split all mixed data into several bins and fully trained the models to evaluate different indicators of high-quality IT data. Despite these advancements, the challenge of precise and efficient high-quality data selection without external resources remains unresolved. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we introduce SelectIT, a novel approach designed to enhance IT data selection by fully leveraging the foundation model itself, eliminating the need for external resources. SelectIT employs different grain uncertainty of LLMs: token, sentence, and model, which can effectually improve the accuracy of IT data selection. We first use the foundation model itself to rate the IT data from 1 to $K$ based on the uncertainty of various tokens. Next, we use sentence-level uncertainty to improve the rating process by exploiting the effect of different prompts on LLMs. At a higher model level, we utilize the uncertainty between different LLMs, enabling a collaborative decision-making process for IT data selection. By applying SelectIT to the original Alpaca, we curate a compact and superior IT dataset, termed Selective Alpaca. ", "page_idx": 1}, {"type": "text", "text": "Experimental results show that SelectIT outperforms existing high-quality data selection methods, improving LLM\u2019s performance on the open-instruct benchmark (Wang et al., 2024). Further analysis reveals that SelectIT can effectively discard abnormal data and tends to select longer and more computationally intensive IT data. The primary contributions of SelectIT are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose SelectIT, a novel IT data selection method which exploits the uncertainty of LLMs without using additional resources.   \n\u2022 We introduce a curated IT dataset, Selective Alpaca, by selecting the high-quality IT data from the Alpaca-GPT4 dataset.   \n\u2022 SelectIT can substantially improve the performance of LLMs across a variety of foundation models and domain-specific tasks.   \n\u2022 Our analysis suggests that longer and more computationally intensive IT data may be more effective, offering a new perspective on the characteristics of optimal IT data. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Instruction Tuning Dataset Recent empirical research highlights the substantial benefits of finetuning LLMs on specialized datasets containing instructions and responses, significantly enhancing their generalization capabilities and responsiveness to new questions (Chung et al., 2022; Longpre et al., 2023; Honovich et al., 2022; Sun et al., 2023). FLAN (Wei et al., 2022a) reformulates traditional natural language processing tasks as instructions formats, thereby improving model performance. Alpaca (Taori et al., 2023; Peng et al., 2023a) exemplifies the effectiveness of merging a select set of manual instruction seeds with advanced LLMs, like text-davinci-003 or GPT-4, to compile a comprehensive dataset. Similarly, Vicuna (Chiang et al., 2023) leverages 70,000 conversations from ChatGPT interactions, benefiting from the diverse data types and structures within these dialogues. WizardLM (Xu et al., 2023) introduces a novel approach by using LLMs to automatically generate open-domain instructions of varying complexities, achieving controlled instructional difficulty variation. However, LIMA (Zhou et al., 2023) demonstrates that only $1K$ high-quality IT data can match or exceed the performance of LLMs fine-tuned on larger IT datasets, presenting a promising direction for future research. ", "page_idx": 1}, {"type": "text", "text": "Instruction Data Selection The recognition of IT data quality\u2019s superiority over quantity in the context of IT is well-established, yet the efficient and precise identification of high-quality data continues to be a challenging frontier for research. One straightforward approach is utilizing the closed-source advanced LLMs for IT data evaluation and selection (Chen et al., 2024; Liu et al., 2023). To circumvent the constraints associated with closed-source, existing research opt to fine-tune LLMs directly to select high-quality IT data (Li et al., 2023b; Kung et al., 2023). Li et al. (2023c); Gururangan et al. (2020); Chen et al. (2023a); Cao et al. (2023) use pre-defined notions of useful data or other IT datasets to develop a data quality assessment framework. Li et al. (2023a) propose training a specialized model and utilizing two unique, condition-based losses on this for a comprehensive IT ", "page_idx": 1}, {"type": "image", "img_path": "QNieOPt4fg/tmp/dd49d7e11165b55674fc967d885e27b326d5cf3088cf33469ba803cbef072d22.jpg", "img_caption": ["Token-levelSelf-Reflection "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 2: Overall framework of SelectIT. In Token-level Self-Reflection, we employ the foundation model to rate the IT data from 1 to $K$ . In Sentence-level Self-Reflection, we leverage the uncertainty of varied prompts on LLMs to enhance the rating process. In Model-level Self-Reflection, we harness uncertainty among different LLMs to facilitate a collaborative decision-making process in selecting IT data. Finally, different levels of self-reflection are reasonably combined into SelectIT, which can effectively select high-quality IT data without relying on additional resources. ", "page_idx": 2}, {"type": "text", "text": "data selection. Wu et al. (2023) explore where data selection is informed by the similarity of samples within the embedding space of a fine-tuned model. N-gram features (Xie et al., 2023) or model gradients (Xia et al., 2024; Han et al., 2023) are also important features for selecting high-quality data in fine-tuned LLMs. However, the methods described above depend, to varying degrees, on supplementary datasets, the use of closed-source models, or open-source models that have been specially fine-tuned, which results in increased consumption of resources and potentially limits the broader impact. ", "page_idx": 2}, {"type": "text", "text": "3 Our SelectIT Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Utilizing advanced LLMs for the sample evaluation is a widely adopted approach in the $\\operatorname{IT}$ data selection (Chen et al., 2024; Li et al., 2023b; Liu et al., 2023). Given an IT dataset $D$ containing a sample $S=(\\operatorname*{input}X$ , response $Y$ ), a designated rating prompt $R P$ , and the foundation LLMs $M$ , the goal is to leverage both $R P$ and $S$ to prompt $M$ to assign an evaluation $S$ core to the sample $S$ on a scale from 1 to $K$ . A higher score typically signifies superior IT data Quality. ", "page_idx": 2}, {"type": "equation", "text": "$$\nQ u a l i t y\\propto S c o r e\\in[1,K]=M(R P,S)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "While existing methods (Chen et al., 2024; Cao et al., 2023) are adept at identifying high-quality samples, they often over-rely on external resources. To address these challenges, we introduce SelectIT, a strategy that capitalizes on the internal uncertainty of LLMs to efficiently select highquality IT data. SelectIT incorporates three grains of sample evaluation modules: token, sentence, and model-level self-reflections, which effectively improve the reliability of IT data selection. The comprehensive framework of SelectIT is depicted in Figure 2. ", "page_idx": 2}, {"type": "text", "text": "3.1 Token-level Self-Reflection ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Numerous studies have demonstrated that foundation models exhibit robust capabilities for next-token prediction during their pre-training phase (Touvron et al., 2023b,a). Yet, this predictive strength is frequently underutilized in evaluating IT data quality. In SelectIT, we adopt a similar idea to evaluate IT data. Specifically, we calculate the next-token probability (from 1 to $K$ ) based on the rating prompt $R P$ and sample $S$ . The score token with the highest probability is then considered as the sample\u2019s quality. ", "page_idx": 3}, {"type": "equation", "text": "$$\nS^{b a s e}=\\underset{k\\in\\{1,...,K\\}}{\\arg\\operatorname*{max}}\\;P_{k}^{\\prime},P_{k}^{\\prime}=\\left(\\frac{P_{k}}{\\sum_{j=1}^{K}P_{j}}\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $P_{k}$ and $P_{k}^{\\prime}$ mean the probability and normalized probability of token $k$ . ", "page_idx": 3}, {"type": "text", "text": "The probability distribution among score tokens reflects the internal uncertainty of LLMs on sample evaluation. The higher $P_{S^{b a s e}}^{\\prime}$ , the more confidence of LLMs, which is not well exploited in Equation 2. To capture this subtle difference, we introduce the token-level self-reflection (Token-R), which uses the distribution between tokens that reflect the internal uncertainty of LLMs, to enhance the credibility of quality assessment. Specifically, we assess the average disparity between the predicted $S^{b a{\\bar{s}}e}$ token and the other, where the greater the disparity, the more the confidence of LLMs. This disparity is then utilized to refine the original $S^{b a s e}$ , resulting in a token-level score $S^{t o k e n}$ . ", "page_idx": 3}, {"type": "equation", "text": "$$\nS^{t o k e n}=S^{b a s e}\\times\\underbrace{\\frac{1}{K-1}\\sum_{i=1}^{K}|P_{i}^{\\prime}-P_{S^{b a s e}}^{\\prime}|}_{U n c e r t a i n t y}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.2 Sentence-level Self-Reflection ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Different prompts can significantly affect outputs of LLMs (Kung et al., 2023; Peng et al., 2023b), introducing uncertainty into IT data evaluation at the sentence level. To make better use of this uncertainty to bolster the reliability of our method, we implement sentence-level self-reflection (SentenceR). Building upon Token-R, we devise $K$ semantically similar rating prompts $\\{R P_{0},R P_{1},\\ldots,R P_{K}\\}$ to obtain a series of quality scores $\\{S_{0}^{t o k e n},S_{1}^{t o k e n},\\ldots,S_{K}^{t o k e n}\\}$ based on a given sample $S$ . We calculate the average of these scores to represent the overall quality of sample $S$ , because of the importance of incorporating assessments from diverse prompts. Additionally, we use the standard deviation to quantify the LLMs\u2019 uncertainty to rating prompt; a higher standard deviation suggests greater sensitivity to prompt variation, while a lower standard deviation indicates more consistent and confident quality ratings by LLMs (Zhou et al., 2020). By integrating a holistic sample evaluation with the quantification of model uncertainty, we derive the sentence-level score $S^{s\\bar{e}n t}$ , offering a more nuanced and reliable measure of $\\operatorname{IT}$ data quality. ", "page_idx": 3}, {"type": "equation", "text": "$$\nS^{s e n t}=\\frac{\\mathbf{Avg\\{}S_{i}^{t o k e n}\\}_{i=1}^{K}}{1+\\alpha\\times\\underbrace{\\mathbf{Std\\{}S_{i}^{t o k e n}\\}_{U n c e r t a i n t y}^{K}}_{U n c e r t a i n t y}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{Avg\\{\\cdot\\}}$ and $\\mathbf{Std}\\{\\cdot\\}$ respectively denote the mean and standard deviation of $S_{i}^{t o k e n}$ , $\\mathbf{K}$ means the number of rating prompts $R P$ . Moreover, we use the uncertainty factor $\\alpha$ to control for the impact of the uncertainty of LLMs on overall scores. ", "page_idx": 3}, {"type": "text", "text": "3.3 Model-level Self-Reflection ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A sample affirmed by multiple foundation models can truly be deemed as high-quality. Different foundation models have different quality assessments of the sample, which introduce model-level uncertainty. To maximize the utilization of this uncertainty, we introduce model-level self-reflection (Model-R). This strategy leverages the capabilities of existing open-source models without the need for additional resources or the complexities associated with fine-tuning. However, the challenge lies in the diverse capabilities of various LLMs and determining how to reasonably combine their sample evaluation based on their performance. It is widely acknowledged that the capabilities of LLMs tend to increase with their parameter count (Hendrycks et al., 2021). Thus, we suggest using the parameter count of LLMs as an initial metric for assessing their capabilities to properly weight sample quality scores. Given $N$ foundation models with parameter counts $\\{\\theta_{1},\\theta_{2},\\ldots,\\theta_{N}\\}$ and their respective sentence-level scores for a sample $S$ being $\\left\\{S_{0}^{s e n t},S_{1}^{s e n t},\\ldots,\\mathbf{\\bar{{S}}}_{N}^{s e n t}\\right\\}$ , we formulate the model-level score $S^{m o d e l}$ to reflect a comprehensive evaluation of sample quality. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nQ u a l i t y\\propto S^{m o d e l}=\\sum_{i=1}^{N}\\left(\\frac{\\theta_{i}}{\\sum_{j=1}^{N}\\theta_{j}}\\times S_{i}^{s e n t}\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $N$ means the number of the foundation models. By obtaining LLM parameters without resource expenditure, Model-R effectively allows us to employ more powerful foundation models, which is advantageous for selecting higher-quality data. Finally, we use $S^{m o d e l}$ as the final evaluation of sample $S$ in SelectIT. The higher $S^{m\\dot{o}d e l}$ , the better sample quality. We sort the samples in descending order based on their $\\bar{S}^{m o d e l}$ and then select the top-ranked samples as high-quality data. ", "page_idx": 4}, {"type": "text", "text": "3.4 Selective Alpaca ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We apply SelectIT to the widely-used Alpaca-GPT4 (Peng et al., 2023a). Specifically, we use the most popular LLaMA-2 (7B, 13B, 70B) as our foundation models and set the hyper-parameters $\\alpha=0.2$ and $K=5$ , which decides the range of LLMs rating in Token-R and the number of rating prompts in Sentence-R. We finally select the top $20\\%$ , a total of $10.4\\mathrm{K}$ pairs as the high-quality data and obtain a curated IT dataset called Selective Alpaca. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Setups ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Benchmark To gain a more comprehensive understanding of the capabilities of LLMs, we evaluate our approach in diverse downstream tasks (Wang et al., 2024; Ivison et al., 2023). Factual knowledge: We use the Massive Multitask Language Understanding dataset (MMLU (Hendrycks et al., 2021)) to assess the factual knowledge of LLMs and report 5-shot results. Reasoning: We evaluate the reasoning abilities of LLMs using two widely utilized datasets: the Grade School Math dataset (GSM (Cobbe et al., 2021)) and Big-Bench-Hard (BBH (Suzgun et al., 2022)) with the CoT setting (Wei et al., 2022b). Multilinguality: we assess this ability by TyDiQA, a multilingual question-answering benchmark that encompasses 11 diverse languages, with the gold-passage setup. Coding: We evaluate this ability using the HumanEval dataset (Chen et al., 2021) and report pass $@10$ results with a temperature of 0.8. Open-ended generation: We utilize AlpacaEval (Dubois et al., 2023), which employs GPT-4 to effectively assess model outputs. This can evaluate whether the text produced by LLMs aligns with humans. ", "page_idx": 4}, {"type": "text", "text": "Implementation Details We use LLaMA-2 as our testbed. We fine-tune it for 3 epochs, with a batch size of 128. We use Adam with $\\beta_{1}=0.9$ , $\\beta_{2}=0.999$ , and the cosine learning rate scheduler starts from $2\\mathrm{e}{-5}$ , and decays to 0. we opted for a 4096 input length because it can show the best performance of LLMs. We employ the beam $=4$ for decoding. We set the temperature parameter to 0.8 and the top\u2212p sampling parameter to 0.9 to improve the originality of the output text while ensuring the accuracy and relevance of the content. ", "page_idx": 4}, {"type": "text", "text": "Baselines We compare with the following baselines: ", "page_idx": 4}, {"type": "text", "text": "\u2022 Alpaca-GPT4 (Peng et al., 2023a) is a widely-used IT dataset that implements a self-instruct method to autonomously generate instructions by the advanced GPT4.   \n\u2022 LIMA (Zhou et al., 2023) primarily consists of 1000 manually crafted high-quality instructional data, which can better stimulate the alignment capability of LLMs.   \n\u2022 AlpaGasus (Chen et al., 2024) involves utilizing the robust ChatGPT to score and select data from the original Alpaca-GPT4 dataset.   \n\u2022 Q2Q (Li et al., 2023a) operates by training a precursor model, determining the quality of the IT data based on the two different loss values within this model.   \n\u2022 Instruction Mining (Cao et al., 2023) entails ftiting data features and loss values to derive a formula for assessing data quality. ", "page_idx": 4}, {"type": "table", "img_path": "QNieOPt4fg/tmp/48de0c79f06984bda620f75d9e97f73147d20456c48321744b0d309169a82111.jpg", "table_caption": ["Table 1: Overall results on IT. \u201cCodeX\u201d and \u201cAE\u201d mean HumanEval and AlpacaEval benchmarks. All the scores are averages of three independent runs with different random seeds. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We focus on the discussion of LLaMA-2-13B because both 7B and 13B models exhibit similar trends in Table 1. System (10) shows the vanilla IT on LLMs with the original Alpaca. By using the data selection strategies, the ability of LLMs has a moderate enhancement in Systems (12) to (14). Additionally, we can use $S^{b a s e}$ as the input for Equations 4 and 5 to construct individual methods of Sentence-R and Model-R. Systems (15) to (17) illustrate that applying each submodule of SelectIT incrementally enhances LLMs\u2019 performance, rivaling contemporary advanced methods. ", "page_idx": 5}, {"type": "text", "text": "Most remarkably, SelectIT can better boost LLaMA-2\u2019s performance compared to vanilla IT in the System (18). Compared to other IT data selection strategies, this enhancement is particularly evident in the computational and reasoning tasks on the BBH and GSM benchmarks. This may be attributed to the characteristics of selected data by SelectIT, and we will analyze this phenomenon in a later section. These gains in reasoning ability also positively impact the coding proficiency of LLMs. The improvement of LLMs on the TydiQA dataset is also obvious enough, which shows that SelectIT can effectively eliminate similar samples and retain sufficient diversity in multilingual aspects. ", "page_idx": 5}, {"type": "text", "text": "5 Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "This part aims to answer the research questions through the following experiments: How to select high-quality data in SelectIT? (\u00a75.1) Is SelectIT adaptable to various models and domains? (\u00a75.2) How about the efficiency of SelectIT? (\u00a75.3) What are the advantages of Selective Alpaca?(\u00a75.4) ", "page_idx": 5}, {"type": "text", "text": "5.1 Abalation Study of SelectIT ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Effect of IT Data Quantity While SelectIT already excels at assessing and ranking samples effectively, selecting an appropriate number of samples in a redundant dataset remains a crucial aspect of our method. We divide the Alpaca dataset into multiple subsets ranging from $10\\%$ to $100\\%$ based on SelectIT\u2019s evaluation and evaluate the overall ability of LLMs on the open-instruct benchmark. As illustrated in Figure 3, compared to using the full Alpaca dataset, we observe that LLMs achieve optimal performance using the top $20\\%$ to $40\\%$ data. Hence, considering the tradeoff of training resources, training time, and model performance, we opt for $20\\%$ for implementing the SelectIT on the Alpaca dataset. ", "page_idx": 5}, {"type": "image", "img_path": "QNieOPt4fg/tmp/a2c7793d6f6f153ddb585eb7f58e84677b340150fc232d4e0ed1df8edd982715.jpg", "img_caption": ["Figure 3: Comparison of LLM abilities with varying Alpaca proportions. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Effect of Multiple Rating Prompts $K$ is a critical parameter for our method, impacting not only the range of scores assigned by the LLMs but also the number of rating prompts. We set $K\\,=\\,3,5,7,9$ and apply SelectIT for sample selection within the Alpaca to get different subset datasets. Table 2 indicates that variations in the value of $K$ have a minor impact on the overall performance of the LLMs. This is attributed to our multi-granularity self-reflection mechanism, which effectively enhances the accuracy and stability of sample selection. Although the model achieves competitive performance at $K$ values of 5 and 7, to minimize resource consumption, we set $K=5$ as the default value in SelectIT. ", "page_idx": 6}, {"type": "table", "img_path": "QNieOPt4fg/tmp/6c440a34b1d77f3eec3e935429d8e6afe446015e4e2bb46493ef6234bdb460ed.jpg", "table_caption": ["Table 2: Effect of different $K$ . "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Effect of Uncertainty $\\alpha$ is an uncertainty factor, integral to calibrating the equilibrium between the mean and the standard deviation of scores derived from Token-R. We assign $\\alpha$ four different values, i.e., 0.2, 0.4, 0.6, and 0.8, and incorporate SelectIT for sample selection from within Alpaca to generate disparate ", "page_idx": 6}, {"type": "table", "img_path": "QNieOPt4fg/tmp/9a1507a563aa94946746aa7a1a1ac99ea4d8352400a18b30dc5f4ab00c08268d.jpg", "table_caption": ["Table 3: Effect of different $\\alpha$ . "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "subset datasets, with all other parameters remaining constant. As shown in Table 3, with a rise in the $\\alpha$ value, Sentece-R tends to emphasize the uncertainty innate to LLMs. This results in the neglect of the average score, a fundamental indicator of sample quality, thereby contributing to a decrease in the overall performance of LLMs. Consequently, we ascertain that an $\\alpha$ value of 0.2 is optimally suited to establish an effective balance between the sample\u2019s quality and the model\u2019s uncertainty. ", "page_idx": 6}, {"type": "text", "text": "Effect of Different Reflection Strategy We analyze the relationship between individual selection strategies and SelectIT, from the following two aspects. We first account for the number of high-quality data that can only be selected by a unique selection strategy, referred to as unique selection. Secondly, we calculate which samples in Selective Alpaca can be selected by individual selection strategies in Selective Alpaca, called overall selection. As shown in Table 4, Sentence-R plays the most important role in the final SelectIT strategy. This is because rating prompts play an important role in sample evaluation and exploiting the effect of ", "page_idx": 6}, {"type": "table", "img_path": "QNieOPt4fg/tmp/d0cc3f4d02954d451dfd1ef26a76e007a804bb2a59d842d19756fefa9aad6104.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 4: The relationship between the SelectIT and the individual selection strategy. Sentence-R plays the most significant impact on the final rating of the IT data. IDs 6, 7, and 8 correspond to the system of the same IDs in Table 1. ", "page_idx": 6}, {"type": "text", "text": "different prompts on LLM can effectively better improve the accuracy of sample evaluation than Token-R and Model-R. Additionally, this phenomenon also aligns with the model\u2019s performance reported in Table 1, showing the rationality of our proposed uncertainty-aware self-reflection methods. ", "page_idx": 6}, {"type": "table", "img_path": "QNieOPt4fg/tmp/d6a4f697e00530f12a6bdb643c88774a153124e09780090893eee3fb9af38b42.jpg", "table_caption": ["Table 5: Results on IT for different datasets with the same number of instances. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Effect of Data Imbalance To eliminate unfair comparison caused by IT data quantity imbalance, we adjust the size of the Selective Alpaca dataset to 1,000 and 9,229 respectively, aligning with the LIMA (Zhou et al., 2023) and AlpaGasus (Chen et al., 2024) datasets. The results in Table 5 show that, when facing the same amount of data, SelectIT can still demonstrate better performances, which further illustrates its effectiveness. ", "page_idx": 6}, {"type": "table", "img_path": "QNieOPt4fg/tmp/dd82d97f01bc722efb1504d06d3b1138cabd9c371b10357963598099bb53a6b9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "QNieOPt4fg/tmp/75a2f6f3f999baf7b745d293544a1317425ea779caf76fb1cb9e8aac33edbe52.jpg", "table_caption": ["Table 6: Results of IT with various foundation models. ", "Table 7: Results of IT with various IT datasets. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.2 Robustness across Models, Datasets and Domains ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Various Foundation Models Although Selective Alpaca achieved impressive improvements in LLaMA-2, applying it to other foundation models remains a challenging task. To address this, we apply Selective Alpaca on the Mistral-7B and LLaMA-3-8B LLMs and present our results on the open-instruct benchmark alignment with the above test configuration. As depicted in Table 6, although Selective Alpaca is selected by the LLaMA-2 models, it is also applicable to the Mistral-7B, LLaMA-3-8B and improves their capabilities across various tasks, especially on MMLU, BBH, and GSM benchmarks. This experiment fully demonstrates the flexibility of SelectIT which does not rely on a specific foundation model for data selection and the universality of Selective Alpaca which can effectively improve the capabilities of different series or scale LLMs. ", "page_idx": 7}, {"type": "text", "text": "Various Instruction Tuning Datasets We further validate the robustness of SelectIT by deploying it on two additional, widely-utilized datasets: WizardLM (Xu et al., 2023) and Orca-GPT4 (Subhabrata & Arindam, 2023). WizardLM introduces an innovative method of using LLMs to auto-generate open-domain instructions of varying complexities. This allows for a controlled variation in instructional difficulty and the dataset comprises 143K samples. Orca-GPT4 on the other hand, leverages rich signals from GPT-4 that include explanation traces, step-by-step thought processes, and other multifaceted instructions, all under the guidance of teacher assistance from ChatGPT. Additionally, we maintain consistent hyperparameters, such as $\\alpha$ and $K$ , choosing LLaMA-2-7B as our base model. We limit the fine-tuning of these datasets to one epoch. As shown in Figure 7, SelectIT consistently enhances the performance of the model on both the WizardLM and Orca-GPT4 datasets. Notably, this augmentative effect is especially pronounced in the computational and reasoning tasks within the BBH and GSM benchmarks. In evaluating three separate IT datasets, specifically Alpaca-GPT4, WizardLM, and the more extensive Orca-GPT4, our extensive experimental conclusions validate the broad utility and durability of SelectIT. ", "page_idx": 7}, {"type": "text", "text": "Various Domain-specific Tasks Machine translation (MT) is a representative domain-specific task of LLMs. Previous works have already demonstrated significant improvements with LLMs, but they usually use redundant translation IT datasets. This part tests the robustness of SelectIT on the IT dataset of MT. We select the powerful MT LLM ALMA (Xu et al., 2024) as our backbone model. ", "page_idx": 7}, {"type": "text", "text": "We choose the representative language pairs {German, Chinese} $\\Leftrightarrow$ English from WMT\u201917 to WMT\u201920 human-written test datasets, and development and test sets from Flores-200, totaling 30K training examples. We used WMT\u201922 test data for testing, and finally, 6K high-quality examples were selected using SelectIT. We utilize both BLEU (Post, 2018; Ott et al., 2018) and COMET (Rei et al., 2022) based on the wmt22-comet-da model for evaluation. We report results for the two language pairs in four directions, using ALL to represent their average. Table 8 shows that SelectIT consistently improves ALMA\u2019s translation performance. These results indicate that SelectIT is a versatile and scalable method, effective not only for IT data selection but also for domain-specific tasks like MT. For more detailed analysis and results, please see Appendix A.1. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "QNieOPt4fg/tmp/ddb24523f5a0fa0aaf17981b662eac7eafcde0bca047611bbfe5311c5d50bc38.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.3 Efficiency of SelectIT ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "SelectIT is a faster and more cost-effective method for IT data selection. We compared different selection methods on the Alpaca-GPT4 dataset. For ChatGPT (AlpaGasus) or GPT-4, we randomly select 500 instruction data from Alpaca-GPT4, analyze various metrics, and estimate the resource consumption for selecting the entire dataset. Using SelectIT, we employ 4 A800 80G ", "page_idx": 8}, {"type": "table", "img_path": "QNieOPt4fg/tmp/d768cda75a74673ba4778be6002710799212055d2dc6c5b8e0d10e159962ebd6.jpg", "table_caption": ["Table 8: The overall results on MT LLMs. ", "Table 9: Comparison of selection efficiency. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "GPUs to select high-quality IT data, calculating the total cost based on Google Cloud\u2019s rate of $\\mathbb{S}1.15/\\mathrm{h}$ per single GPU. As shown in Table 9, SelectIT is significantly faster and uses the least resources. This efficiency is due to computing only the probability of the next token for input sentences, bypassing the full sentence generation and decoding process, resulting in lower resource consumption. Additionally, using our own GPU at a low cost enhances transparency, allowing us to preserve all intermediate outputs and results for thorough analysis in data selection. ", "page_idx": 8}, {"type": "text", "text": "5.4 Insights of Selective Data Curation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Different Selection Strategies This part compares three different selection strategies, namely, randomly selecting $20\\%$ in the full Alpaca and unselected dataset of Selective Alpaca, and selecting $20\\%$ data based on sample length (Zhao et al., 2024). As shown in Table 10, the random-based strategies show certain performance degradation and the random selection in the unselected dataset is even worse, which reflects the effectiveness of our method from the side. Selection based on sample length is a simple approach to defining high-quality data, but it does not take into account the content of IT data, resulting in the limited performance of LLMs. SelectIT can significantly improve the abilities of LLMs. ", "page_idx": 8}, {"type": "table", "img_path": "QNieOPt4fg/tmp/df40c98c28547a0b24d705cded0b382450d73a074935d1c885694636173d87be.jpg", "table_caption": ["Table 10: Comparasion with variants. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Data Representation Analysis This part explores the relationship between Selective Alpaca and the original datasets from a representation perspective. Following Gao et al. (2024), we use the outputs of the last layer corresponding to the last token in the input sequence as sample representations. We then apply T-SNE (Hinton & Roweis, 2002) for dimensionality reduction, mapping high-dimensional embeddings onto a 2D space. Figure 4 shows the intermediate representations generated by the full and Selective Alpaca datasets. Randomly selected data struggle to distinguish abnormal data far from the center, making it hard to define high-quality IT data. In contrast, Selective Alpaca data are mostly concentrated around the center, indicating that our dataset predominantly contains high-quality data near the center and effectively discards abnormal data, supporting the conclusion of Table 10. ", "page_idx": 8}, {"type": "text", "text": "Data Characteristic Analysis We analyze the Selective Alpaca from the following two perspectives, to explore why our dataset is better than the original dataset and its variants. Firstly, as shown in Figure 5, the length of instructions from the Selective Alpaca is significantly longer than those in the Alpaca dataset and AlpaGasus which is selected by ChatGPT. This implies that, with the same amount of data, our dataset contains more information, aligned with the results in Table 10. Secondly, by using ChatGPT to examine IT data types, we find a substantial increase in the proportion of computational problems in Selective Alpaca. This indicates that Selective Alpaca tends to select high-quality mathematical data, providing a solid explanation for the observed improvement in the reasoning abilities of LLMs as demonstrated in Table 1. Appendix A.3 shows the case study of comparing the Selective Alpaca with AlpaGasus. ", "page_idx": 8}, {"type": "image", "img_path": "QNieOPt4fg/tmp/96cc874390afa142f9a1d0a6237a18fc82637c2f5d90d3a392399f76f44031be.jpg", "img_caption": ["Figure 4: Instruction embeddings representations of different selection strategies. The red and blue points are representations of full Alpaca datasets and selected data respectively. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "QNieOPt4fg/tmp/025f15701ce23c8b6a10939f3fcf44e47cadf1108c30fd5b3b873f78c3eeb5e0.jpg", "img_caption": ["Figure 5: Left: The average length of samples. Right: The proportion of calculation type. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Insights of High-Quality Data in SelectIT Furthermore, we analyze the proportion of calculation and sample average length in Alpaca-GPT4 with different proportions after sorting by SelectIT to explore its intrinsic characteristics and the definition of high-quality data. As shown in Figure 6, with the proportion of Alpaca-GPT4 data continuing to increase, the proportion of calculation and sample average length gradually decreases. This phenomenon clearly indicates that SelectIT can reasonably rank samples based on their characteristics. When the data size is more than $50\\%$ , the proportion of calculation IT data sharply declines, falling below $6\\%$ , causing a noticeable decrease in the model\u2019s overall capability, as depicted in Figure 3. This analysis shows that more computationally intensive IT data may be a new perspective on the characteristics of optimal IT data, which not only effectively improves the LLMs\u2019 reasoning ability, but also further drives the improvement of other abilities. ", "page_idx": 9}, {"type": "image", "img_path": "QNieOPt4fg/tmp/ad366ca469a8a6d08a33349850b9f00b9b74dbb1d73426687fc033a2b42ad84f.jpg", "img_caption": ["Figure 6: Changing trends of the calculation and sample length with different data sizes. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper introduces a novel data selection strategy, SelectIT, for LLM instruction tuning, which uses LLM uncertainty to efficiently identify high-quality IT data without requiring additional resources. SelectIT includes three types of self-reflection: token, sentence, and model, which can individually and jointly improve the performance of IT data selection. By applying SelectIT to the Alpaca-GPT4 dataset, we introduce a compact and strong IT dataset, called Selective Alpaca. Different models and domain tasks demonstrate the effectiveness of SelectIT. Our analysis reveals that SelectIT effectively excludes abnormal data and tends to select longer and calculational data. ", "page_idx": 9}, {"type": "text", "text": "Limitation ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper could be further strengthened as follows: ", "page_idx": 9}, {"type": "text", "text": "\u2022 Instruction Data Quantity: Our findings suggest that prioritizing the top $20\\%$ of highquality data optimizes results for Alpaca. Future studies might explore adjusting this threshold based on the data quality in different datasets to enhance performance. \u2022 Models at Different Scales: Our analysis is currently limited to models smaller than 30B parameters due to computational constraints. Investigating the efficacy of Selective Alpaca on larger-scale LLMs, could provide valuable insights into the method\u2019s scalability. \u2022 Expansion to Additional Instruction Datasets: Although SelectIT has been applied to the Alpaca dataset due to its widespread adoption, extending this methodology to incorporate other IT datasets could offer substantial advantages to the broader LLM research community. ", "page_idx": 9}, {"type": "text", "text": "Broader Impacts ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Our work follows the NeurIPS Ethics Policy. Our findings are based on publicly available datasets for reproducibility purposes. LLMs can contain potential racial and gender bias. Therefore, if someone finds our work interesting and would like to use it in a specific environment, we strongly suggest the user check the potential bias before usage. In addition, it is hard to control the generation of LLMs. We should be aware of the potential problems caused by hallucinations. ", "page_idx": 10}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported in part by the National Natural Science Foundation of China (Grant No. 62206076), Guangdong Basic and Applied Basic Research Foundation (Grant No. 2024A1515011491), Shenzhen Science and Technology Program (Grant Nos. ZDSYS20230626091203008, KJZD20231023094700001, RCBS20221008093121053), and Shenzhen College Stability Support Plan (Grant Nos. GXWD20220811173340003, GXWD20220817123150002). Derek F. Wong was supported in part by the Science and Technology Development Fund, Macau SAR (Grant Nos. FDCT/060/2022/AFJ, FDCT/0070/2022/AMJ), National Natural Science Foundation of China (Grant No. 62261160648), the Multi-year Research Grant from the University of Macau (Grant No. MYRG-GRG2024-00165-FST), and the Tencent AI Lab Rhino-Bird Gift Fund (Grant No. EF2023-00151-FST). We would like to thank the anonymous reviewers and meta-reviewer for their insightful suggestions. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. ArXiv preprint, abs/2303.08774, 2023. URL https://arxiv.org/abs/2303.08774.   \nYihan Cao, Yanbin Kang, and Lichao Sun. Instruction mining: High-quality instruction data selection for large language models. ArXiv preprint, abs/2307.06290, 2023. URL https://arxiv.org/ abs/2307.06290.   \nLichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. Alpagasus: Training a better alpaca with fewer data. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $\\equiv$ FdVXgSJhvz.   \nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. ArXiv preprint, abs/2107.03374, 2021. URL https://arxiv. org/abs/2107.03374.   \nMayee F Chen, Nicholas Roberts, Kush Bhatia, Jue WANG, Ce Zhang, Frederic Sala, and Christopher Re. Skill-it! a data-driven skills framework for understanding and training language models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a. URL https: //openreview.net/forum?id $\\cdot$ IoizwO1NLf.   \nYijie Chen, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, and Jie Zhou. Improving translation faithfulness of large language models via augmenting instructions. ArXiv preprint, abs/2308.12674, 2023b. URL https://arxiv.org/abs/2308.12674.   \nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90\\%^{*}$ chatgpt quality, 2023. URL https://lmsys. org/blog/2023-03-30-vicuna/.   \nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. ArXiv preprint, abs/2210.11416, 2022. URL https://arxiv.org/abs/2210.11416.   \nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. ArXiv preprint, abs/2110.14168, 2021. URL https://arxiv.org/abs/ 2110.14168.   \nMarta R Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. No language left behind: Scaling human-centered machine translation. ArXiv preprint, abs/2207.04672, 2022. URL https:// arxiv.org/abs/2207.04672.   \nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id $\\cdot$ 4hturzLcKX.   \nPengzhi Gao, Zhongjun He, Hua Wu, and Haifeng Wang. Towards boosting many-to-many multilingual machine translation with large language models. ArXiv preprint, abs/2401.05861, 2024. URL https://arxiv.org/abs/2401.05861.   \nSuchin Gururangan, Ana Marasovi\u00b4c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. Don\u2019t stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 8342\u20138360, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020. acl-main.740. URL https://aclanthology.org/2020.acl-main.740.   \nXiaochuang Han, Daniel Simig, Todor Mihaylov, Yulia Tsvetkov, Asli Celikyilmaz, and Tianlu Wang. Understanding in-context learning via supportive pretraining data. In Anna Rogers, Jordan BoydGraber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 12660\u201312673, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.708. URL https://aclanthology.org/2023.acl-long.708.   \nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=d7KBjmI3GmQ.   \nGeoffrey E. Hinton and Sam T. Roweis. Stochastic neighbor embedding. In Suzanna Becker, Sebastian Thrun, and Klaus Obermayer (eds.), Advances in Neural Information Processing Systems 15 [Neural Information Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, British Columbia, Canada], pp. 833\u2013840. MIT Press, 2002. URL https://proceedings.neurips. cc/paper/2002/hash/6150ccc6069bea6b5716254057a194ef-Abstract.html.   \nOr Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language models with (almost) no human labor. ArXiv preprint, abs/2212.09689, 2022. URL https://arxiv.org/abs/2212.09689.   \nHamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. Camels in a changing climate: Enhancing lm adaptation with tulu 2, 2023. URL https://arxiv.org/abs/ 2311.10702.   \nPo-Nien Kung, Fan Yin, Di Wu, Kai-Wei Chang, and Nanyun Peng. Active instruction tuning: Improving cross-task generalization by training on prompt sensitive tasks. ArXiv preprint, abs/2311.00288, 2023. URL https://arxiv.org/abs/2311.00288.   \nMing Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning. ArXiv preprint, abs/2308.12032, 2023a. URL https: //arxiv.org/abs/2308.12032.   \nXian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis. Self-alignment with instruction backtranslation. ArXiv preprint, abs/2308.06259, 2023b. URL https://arxiv.org/abs/2308.06259.   \nYunshui Li, Binyuan Hui, Xiaobo Xia, Jiaxi Yang, Min Yang, Lei Zhang, Shuzheng Si, Junhao Liu, Tongliang Liu, Fei Huang, et al. One shot learning as instruction data prospector for large language models. ArXiv preprint, abs/2312.10302, 2023c. URL https://arxiv.org/abs/2312.10302.   \nWei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning. ArXiv preprint, abs/2312.15685, 2023. URL https://arxiv.org/abs/2312.15685.   \nShayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective instruction tuning. ArXiv preprint, abs/2301.13688, 2023. URL https://arxiv.org/abs/2301. 13688.   \nMyle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation. In Proceedings of the Third Conference on Machine Translation: Research Papers, pp. 1\u20139, Brussels, Belgium, 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6301. URL https://aclanthology.org/W18-6301.   \nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. ArXiv preprint, abs/2306.01116, 2023. URL https://arxiv.org/abs/2306.01116.   \nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. ArXiv preprint, abs/2304.03277, 2023a. URL https://arxiv.org/abs/2304.03277.   \nKeqin Peng, Liang Ding, Qihuang Zhong, Li Shen, Xuebo Liu, Min Zhang, Yuanxin Ouyang, and Dacheng Tao. Towards making the most of ChatGPT for machine translation. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 5622\u20135633, Singapore, 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.373. URL https://aclanthology.org/ 2023.findings-emnlp.373.   \nMatt Post. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pp. 186\u2013191, Brussels, Belgium, 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6319. URL https://aclanthology.org/ W18-6319.   \nRicardo Rei, Jos\u00e9 G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and Andr\u00e9 F. T. Martins. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pp. 578\u2013585, Abu Dhabi, United Arab Emirates (Hybrid), 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.wmt-1.52.   \nMukherjee Subhabrata and Mitra Arindam. Orca: Progressive learning from complex explanation traces of gpt-4. https://arxiv.org/pdf/2306.02707, 2023. URL https://arxiv.org/pdf/2306. 02707.   \nZhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. ArXiv preprint, abs/2305.03047, 2023. URL https://arxiv.org/ abs/2305.03047.   \nMirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. ArXiv preprint, abs/2210.09261, 2022. URL https://arxiv.org/abs/2210.09261.   \nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.   \nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. ArXiv preprint, abs/2302.13971, 2023a. URL https: //arxiv.org/abs/2302.13971.   \nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. ArXiv preprint, abs/2307.09288, 2023b. URL https://arxiv.org/ abs/2307.09288.   \nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? exploring the state of instruction tuning on open resources. Advances in Neural Information Processing Systems, 36, 2024. URL https://arxiv.org/abs/2306.04751.   \nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022a. URL https://openreview.net/forum?id $\\equiv$ gEZrGCozdqR.   \nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. ArXiv preprint, abs/2201.11903, 2022b. URL https://arxiv.org/abs/2201.11903.   \nShengguang Wu, Keming Lu, Benfeng Xu, Junyang Lin, Qi Su, and Chang Zhou. Self-evolved diverse data sampling for efficient instruction tuning. ArXiv preprint, abs/2311.08182, 2023. URL https://arxiv.org/abs/2311.08182.   \nMengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting influential data for targeted instruction tuning. ArXiv preprint, abs/2402.04333, 2024. URL https://arxiv.org/abs/2402.04333.   \nSang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy S Liang. Data selection for language models via importance resampling. Advances in Neural Information Processing Systems, 36: 34201\u201334227, 2023. URL https://arxiv.org/abs/2302.03169.   \nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. ArXiv preprint, abs/2304.12244, 2023. URL https://arxiv.org/abs/2304.12244.   \nHaoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. A paradigm shift in machine translation: Boosting translation performance of large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/ forum?id $\\equiv$ farT6XXntP.   \nWen Yang, Chong Li, Jiajun Zhang, and Chengqing Zong. Bigtranslate: Augmenting large language models with multilingual translation capability over 100 languages. ArXiv preprint, abs/2305.18098, 2023. URL https://arxiv.org/abs/2305.18098.   \nJiali Zeng, Fandong Meng, Yongjing Yin, and Jie Zhou. Tim: Teaching large language models to translate with comparison. ArXiv preprint, abs/2307.04408, 2023. URL https://arxiv.org/ abs/2307.04408.   \nShaolei Zhang, Qingkai Fang, Zhuocheng Zhang, Zhengrui Ma, Yan Zhou, Langlin Huang, Mengyu Bu, Shangtong Gui, Yunji Chen, Xilin Chen, and Yang Feng. Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models. ArXiv preprint, abs/2306.10968, 2023. URL https://arxiv.org/abs/2306.10968.   \nHao Zhao, Maksym Andriushchenko, Francesco Croce, and Nicolas Flammarion. Long is more for alignment: A simple but tough-to-beat baseline for instruction fine-tuning, 2024. URL https://arxiv.org/abs/2402.04833. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. LIMA: Less is more for alignment. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id $\\cdot$ KBMOKmX2he. Yikai Zhou, Baosong Yang, Derek F. Wong, Yu Wan, and Lidia S. Chao. Uncertainty-aware curriculum learning for neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 6934\u20136944, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.620. URL https://aclanthology. org/2020.acl-main.620. ", "page_idx": 14}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Applying SelectIT on Machine Translation LLMs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Machine Translation (MT) is a important task for LLMs, demonstrating their domain-specific capabilities. Prior research, including TIM (Zeng et al., 2023), SWIE (Chen et al., 2023b), BigTranslate (Yang et al., 2023), and Bayling (Zhang et al., 2023), has shown significant improvements in LLMs, often relying on extensive translation training datasets. In this section, we examine the impact of training data quality on MT performance, employing the robust MT LLM, ALMA, as our foundational model (Xu et al., 2024). ", "page_idx": 15}, {"type": "text", "text": "For training data, we select representative language pairs: German $\\Leftrightarrow$ English and Chinese $\\Leftrightarrow$ English, sourced from WMT\u201917 to WMT\u201920 human-authored test datasets, supplemented with development and test sets from Flores-200, totaling 30K training instances. We use the corresponding language pair\u2019s test data from WMT\u201922 as evaluation datasets. Subsequently, 6K high-quality instances are selected for LORA fine-tuning via SelectIT. ", "page_idx": 15}, {"type": "text", "text": "We report both the widely used BLEU score (Post, 2018; Ott et al., 2018) and the COMET score (Rei et al., 2022) based on the wmt22-comet-da model, which shows higher correlation with human judgments for evaluating the LLMs\u2019 translation abilities. Table 11 consistently demonstrates that SelectIT enhances ALMA\u2019s translation efficacy. Notably, SelectIT primarily focuses on improving translations from English to other languages, likely due to ALMA\u2019s inherent proficiency in English, which presents challenges for further enhancements. These findings highlight SelectIT\u2019s adaptability and scalability, validating its effectiveness not only in IT data selection but also in domain-specific tasks such as MT. ", "page_idx": 15}, {"type": "table", "img_path": "QNieOPt4fg/tmp/52d4ee6de62b112770a3d9d57c1506fdc88505f512e37bde8972fe3bef633dd1.jpg", "table_caption": [], "table_footnote": ["Table 11: Overall results on machine translation LLMs. \u201c\u2020\u201d the improvement is significant by contrast to the ALMA model $(p<0.05)$ . "], "page_idx": 15}, {"type": "text", "text": "A.2 Details of Sentence-level Rating ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Based on the preceding analysis, Sentence-R is integral to the functionality of SelectIT. As illustrated in Equation 4, the Token-level Rating forms the foundation for the Sentence-level Rating. The Model-level Rating is derived through multiple iterations of the Sentence-level Rating across different foundational LLMs. Therefore, a detailed explanation of Sentence-R is sufficient to demonstrate the operational mechanism of SelectIT. As depicted in Figure 7, we utilize five distinct rating prompts along with a single input to formulate the final input for Sentence-R. Initially, each rating prompt produces a score of $S^{t{\\dot{o}}k e n}$ . We then compute the mean and standard deviation of these $S^{t o k\\overline{{e}}n}$ values to obtain the final $S^{s e n t}$ , as outlined in Equation 4. ", "page_idx": 15}, {"type": "text", "text": "A.3 Case Study ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "As demonstrated in Figure 8, we illustrate the selection tendencies of SelectIT in contrast to AlpaGasus, which leverages advanced ChatGPT for data selection. In samples 1 to 4, SelectIT shows a preference for instruction-tuning data containing intricate mathematical problems that contribute ", "page_idx": 15}, {"type": "text", "text": "to improving the reasoning skills of the LLMs. On the contrary, AlpacaGasus frequently chooses IT data in samples 5 to 7 that primarily offer solutions to queries or lack coherent reasoning, which might limit its effectiveness. ", "page_idx": 16}, {"type": "text", "text": "<| Rating Prompt 1 |> ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Assign a score from 1 to 5 to each input based on how accurately they follow the instructions and response provided, ensuring the score is represented clearly on its own. ", "page_idx": 17}, {"type": "text", "text": "<| Input > ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Input: For the given input, you need to predict the result of the operation 5 - 9 Output: The result of the operation 5 - 9 is -4. ", "page_idx": 17}, {"type": "text", "text": "<| Next Token Prediction [> ", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "QNieOPt4fg/tmp/a3722c2ac7f10f8f510040e4cc2b008609082d74e369334a6018615a7c626510.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "<| Rating Prompt 2 |> ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Score each input on a scale from 1 to 5, reflecting the accuracy of their adherence to the instructions and input, and present this score plainly without the need for extra details. ", "page_idx": 17}, {"type": "text", "text": "Input $|>$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Input: For the given input, you need to predict the result of the operation 5 - 9 Output: The result of the operation 5 - 9 is -4. ", "page_idx": 17}, {"type": "text", "text": "< Next Token Prediction [ ", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "QNieOPt4fg/tmp/3fd318e3a3bc7404a09c17607d9e7d75d4a8c7c567887a71b030b6407174e8ed.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "<| Rating Prompt 3 |> ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Rate each inut auracyto th giventask and input ona scale f1 to5, withbeing themst precise; the score shoulde self-exlantory and presented as a single line. ", "page_idx": 17}, {"type": "text", "text": "< Input $\\mid>$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Input: For the given input, you need to predict the result of the operation 5 - 9 Output:The result of the operation 5-9 is-4. ", "page_idx": 17}, {"type": "text", "text": "$\\triangleleft$ Next Token Prediction $\\mid>$ ", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "QNieOPt4fg/tmp/c94afabfe50aafe38104b0bb9b32baeec2234b3bb137ab837451358c66ae05f3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "<| Rating Prompt 4 [> ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Rate each input on a scale of 1 to 5 based on their adherence to the instructions and the accuracy of their responses, with the score clearly displayed. ", "page_idx": 17}, {"type": "text", "text": "<| Input |> ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Input: For the given input, you need to predict the result of the operation 5 - 9 Output: The result of the operation 5 - 9 is -4. ", "page_idx": 17}, {"type": "text", "text": "<| Next Token Prediction [> ", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "QNieOPt4fg/tmp/f28f4af08c5f7b3cad317a48ee96eea45bed2acf9b6a740c2fe9022bde018047.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "<| Rating Prompt 5 |> ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Assign to every input a score ranging from 1 to 5, evaluating their compliance with instructions and the precision of their feedback, with the score being conspicuously presented. ", "page_idx": 17}, {"type": "text", "text": "<Input $\\mid>$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Input: For the given input, you need to predict the result of the operation 5 - 9 Output: The result of the operation 5 - 9 is -4. ", "page_idx": 17}, {"type": "text", "text": "<| Next Token Prediction [> ", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "QNieOPt4fg/tmp/5eaf63ca4da3a6c44088510531c847d1aec0235592d21993e4fb3f661e892def.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "<| Evaluation |> ", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "QNieOPt4fg/tmp/f87447c365362b16701a8858060db7a60bf32cd66e446446a3e358a6b4a272cc.jpg", "table_caption": ["Figure 7: Example on Sentence-R calculation of SelectIT. "], "table_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "QNieOPt4fg/tmp/7fd998bf24335fe035f77552c98eea6e89a906b8c8acfe7ea936cdaff5ed0d3d.jpg", "img_caption": ["Figure 8: Examples of IT data selected by SelectIT or AlpaGasus. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: See the abstract and introduction sections. Our proposed SelectIT can capitalize on the foundational capabilities of the LLM itself to more effectively select high-quality IT data, without the need for extra resources. We run comprehensive experiments to support our assumption. Our contributions are stated clearly in the abstract and introduction. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: In the limitation part, we have discussed the points where SelectIT could be further optimized, including the data quantity, model scales, other foundation models, and datasets. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: Our paper does not include theoretical results. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: In the abstract section, we provide the GitHub link to open source all the code, scripts, and datasets (Selective Alpaca) for other researchers to replicate the results. We also provide the implementation details to better reproduce the experiments. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: In the abstract section, we provide the GitHub link to open access to data and code. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: In Section 4.1, we have discussed datasets, baselines, and experimental setup used in our experiments. More training details are included in the code. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: In the section about applying the SelectIT on MT LLMs, we do the statistical significance tests in Table 9, which is reported in the appendix. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: In Section 5.3, we provide the type of computing workers, memory, and time of execution to help other researchers reproduce the Selective Alpaca. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: we have read the guidelines and ensured that our paper conforms to them. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: In Section 5.1, Selective can use fewer computing resources to select highquality data, which has a positive impact on society. We also have a section to discuss the broader impacts. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The model and datasets we used are all open-sourced, and we strictly follow their terms once the terms are carried out. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have cited the creators in the main part of the paper and the supplement material. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should citep the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: In the abstract section, we provide the GitHub link to open access to our code and data. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: SelectIT does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: SelectIT does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]