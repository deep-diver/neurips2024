[{"type": "text", "text": "OTTER: Effortless Label Distribution Adaptation of Zero-shot Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Changho Shin, Jitian Zhao, Sonia Cromp, Harit Vishwakarma, Frederic Sala ", "page_idx": 0}, {"type": "text", "text": "Department of Computer Sciences University of Wisconsin-Madison {cshin23, jzhao326, cromp, hvishwakarma, fsala}@wisc.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Popular zero-shot models suffer due to artifacts inherited from pretraining. One particularly detrimental issue, caused by unbalanced web-scale pretraining data, is mismatched label distribution. Existing approaches that seek to repair the label distribution are not suitable in zero-shot settings, as they have mismatching requirements, such as needing access to labeled downstream task data or knowledge of the true label balance in the pretraining distribution. We sidestep these challenges and introduce a simple and lightweight approach to adjust pretrained model predictions via optimal transport. Our technique requires only an estimate of the label distribution of a downstream task. Theoretically, we characterize the improvement produced by our procedure under certain mild conditions and provide bounds on the error caused by misspecification. Empirically, we validate our method in a wide array of zero-shot image and text classification tasks, improving accuracy by $4.8\\%$ and $15.9\\%$ on average, and beating baselines like prior matching\u2014often by significant margins\u2014in 17 out of 21 datasets. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zero-shot models are popular but struggle with biases inherited from their large pretraining datasets [21; 59; 2]. In particular, zero-shot classification is strongly biased by the label distribution of the pretraining task. When the label distribution of the downstream task differs from pretraining, the performance of zero-shot classifiers suffers greatly. For example, Figure 1 illustrates the effects of mismatched distributions on a pet image classification task. Two CLIP models (RN50, and ViT-B/16) produce biased predictions on the Abyssinian and Persian classes. Furthermore, datasets with a large number of classes, such as ImageNet, may contain both extremely common and very rare classes, resulting in an outsized probability that a zero-shot model will predict some classes over others. As a result, even large models intended for use in zero-shot settings, such as CLIP [49], naturally have a label distribution mismatch between pretraining data and downstream tasks. ", "page_idx": 0}, {"type": "image", "img_path": "RsawwSBCs7/tmp/51de74db2033a7eef60902a7602419d3723e3fb6c05beec4795b8bfbdd6f05fa.jpg", "img_caption": ["Figure 1: Label distribution mismatch example in zero-shot classification. In the OxfordIIIT-Pet dataset, the ground-truth labels are uniformly distributed, while zero-shot models exhibit biased predictions toward certain classes. This bias is influenced by the distribution of labels in the pretraining task. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Existing methods that seek to address label distri  \nbution make strong assumptions or have expensive   \nrequirements. For example, to fine-tune a model, we must obtain a labeled fine-tuning dataset of adequate size, then obtain the time and compute to further train the model. To perform label shift adap", "page_idx": 0}, {"type": "text", "text": "tation techniques, we must know the true label distribution of the pretraining distribution\u2014difficult to impossible for real-world tasks. ", "page_idx": 1}, {"type": "text", "text": "Can we deal with label distribution mismatch without additional training or access to groundtruth downstream task information? While seemingly challenging, one cause for optimism is the observation that zero-shot models still give relatively high prediction probabilities for correct classes, though classes common in pretraining tend to have relatively inflated scores overall. Intuitively, the model has already learned to identify examples of its downstream classes (and so does not require further training) and is already impacted by the pretraining label distribution (and so does not need access to the ground-truth pretraining label distribution). Instead, the model\u2019s prediction probabilities must be adjusted based on an estimated downstream label distribution specification. ", "page_idx": 1}, {"type": "text", "text": "To perform this label distribution adjustment, we view zero-shot learning through the lens of optimal transport (OT) and develop a technique called OTTER (Optimal TransporT adaptER). This OT-based approach offers a systematic way to rebalance predicted labels: data points are transported to optimal downstream classes, minimizing the overall cost in accordance with the estimated the downstream label distribution specifications. ", "page_idx": 1}, {"type": "text", "text": "Theoretically, we show that optimal transport given the true label distribution of the downstream can recover the Bayes-optimal classifier under mild conditions. Additionally, we provide error bounds on our adaptation method for misspecification. We provide synthetic experiments validating our theoretical claims. In real-world data settings, we validate our method on a wide variety of image and text classification tasks, showing $4.8\\%$ and $15.5\\%$ accuracy improvement on average in image and text zero-shot classification tasks, respectively. Finally, we evaluate our method in few-shot adaptation scenarios, where OTTER provides further improvements when combined with linear probing. Our contributions include: ", "page_idx": 1}, {"type": "text", "text": "\u2022 OTTER, an algorithm to deal with label distribution mismatch at inference time via optimal transport,   \n\u2022 Theoretical results showing the effectiveness of our method, including the ability to recover the Bayes-optimal classifier and a sensitivity analysis with respect to the label distribution specification estimation,   \n\u2022 Extensive empirical results on zero-shot classification for text and image datasets, showing accuracy improvements of up to $25\\%$ ,   \n\u2022 Experimental results demonstrating the applicability of OTTER to few-shot settings, showing accuracy improvements of up to $15\\%$ , even with noisy label distribution specifications,   \n\u2022 Extensions of OTTER to leverage label hierarchy information or relax the batched prediction requirement and   \n\u2022 Application to LLM selection bias mitigation. ", "page_idx": 1}, {"type": "text", "text": "2 Background and Problem Formulation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Before presenting OTTER and providing our results, we introduce some crucial background and describe the problem setting formally. ", "page_idx": 1}, {"type": "text", "text": "2.1 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We briefly describe zero-shot models, the technical tool we use (optimal transport), along with other techniques that seek to address shifts. We have extended related work, including in-depth characterizations and comparisons with related methods, in Appendix B. ", "page_idx": 1}, {"type": "text", "text": "Zero-shot Models. Zero-shot classification, popularized by models such as CLIP [49], is a powerful paradigm that enables prediction on downstream tasks without additional fine-tuning. Image, language, and multimodal models have been increasingly employed for zero-shot prediction [64; 36]. These models undergo extensive pretraining on massive datasets with concept and label spaces that may be very different from those of downstream applications. ", "page_idx": 1}, {"type": "text", "text": "Optimal Transport. Optimal Transport (OT) is a framework for matching two probability distributions [48; 53]. We predominantly consider optimal transport between empirical discrete measures. Suppose that we are given points $x_{1},x_{2},\\ldots,x_{n}\\,\\in\\,{\\mathcal{X}}$ and $y_{1},\\dotsc,y_{K}\\in\\mathcal{Y}$ , a source measure $\\mu$ defined by $\\textstyle\\mu\\,=\\,\\sum_{i=1}^{n}w_{i}\\delta_{x_{i}}$ , and a target measure given by $\\begin{array}{r}{\\nu\\,=\\,\\sum_{j=1}^{K}p_{j}\\delta_{j}}\\end{array}$ , where $w_{i},p_{j}$ are positive values such that $\\begin{array}{r}{\\sum_{i=1}^{n}w_{i}=1,\\sum_{j=1}^{K}p_{j}=1}\\end{array}$ . Suppose also that $\\delta_{x}$ is a Dirac delta function at $x$ , i.e. $\\delta_{x}(x^{\\prime})\\,=\\,1$ if $x^{\\prime}=x$ , and $\\delta_{x}(x^{\\prime})\\,=\\,0$ otherwise. Given a cost matrix $C\\in\\mathbb{R}^{n\\times K}$ , the Monge-Kantorovich formulation of optimal transport is to find a minimal cost transport plan $\\pi$ such that ", "page_idx": 1}, {"type": "table", "img_path": "RsawwSBCs7/tmp/e1fd205932b5f87a548f9171d311d6126c7273a3d61c1433b7bfbf07427fddd1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pi=\\underset{\\gamma\\in\\Pi(\\mu,\\nu)}{\\arg\\operatorname*{min}}\\langle\\gamma,C\\rangle,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\Pi(\\mu,\\nu)=\\{\\gamma\\in\\mathbb{R}_{+}^{n\\times K}|\\gamma\\mathbf{1}=\\mu,\\gamma^{T}\\mathbf{1}=\\nu\\}.$ . ", "page_idx": 2}, {"type": "text", "text": "Distribution and Label Shifts. Distribution shift refers to the discrepancy between a source distribution $P_{s}$ on which the model is trained, and a target distribution $P_{t}$ on which the model is deployed. Distribution shift often degrades trained model performance on target tasks. Label shift is a specific type of distribution shift such that $P_{s}(Y)\\neq P_{t}(Y)$ and the data generation process is fixed \u2014 in other words, the conditional distributions of the inputs are the same: $P_{s}(X|\\bar{Y})=P_{t}(X|Y)$ . Techniques such as importance sampling [35; 6; 24], recalibration [3] and domain adaptation [55] are commonly used to mitigate the effects of label shift. Unfortunately, these methods assume access to source distribution data, whereas zero-shot models\u2019 pretraining data is inaccessible (often proprietary or blocked for privacy reasons). Thus, adapting zero-shot models to new label distributions poses challenges unmet by these pre-existing methods. ", "page_idx": 2}, {"type": "text", "text": "2.2 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let $\\textbf{X}=~\\{x_{1},x_{2},...\\,,x_{n}\\}$ be an inference dataset with $x_{i}~\\in~{\\mathcal{X}}$ . Furthermore, let $\\textbf{Y}=$ $\\{y_{1},y_{2},\\ldots,y_{n}\\}$ be the true labels of the $K$ -class classification dataset, such that $y_{i}\\in\\mathcal{Y}=[K]$ , are sampled according to the downstream label distribution $\\pmb{\\nu}=(p_{1},p_{2},.~.~.~,p_{K})$ . ", "page_idx": 2}, {"type": "text", "text": "Let $s_{\\theta}(x,j):=P_{\\theta}(Y=j|X=x)$ be a pretrained classification model constrained to the downstream label space. During pretraining, $s_{\\theta}$ has been biased to the source label distribution $\\nu^{s}$ . We wish to offset such label distribution bias with a label distribution specification $\\hat{\\nu}$ for the target distribution. $\\hat{\\nu}$ is expected to be closer to the true label distribution of the downstream task. Given a label distribution specification, our goal is to rebalance predictions so that the predicted label distribution follows the label distribution specification. ", "page_idx": 2}, {"type": "text", "text": "3 Proposed Framework ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We propose OTTER (Optimal TransporT adaptER), an optimal transport-based label distribution adaptation approach. Our goal is to have the $n$ input data points allocated to $K$ classes match a given label distribution $\\hat{\\nu}$ , where $\\begin{array}{r}{\\sum_{j=1}^{K}\\hat{\\nu}_{j}=1,\\hat{\\nu}_{j}\\ge0}\\end{array}$ . Specifically, we want to classify $n\\hat{\\nu}_{1}$ points as the first class, $n\\hat{\\nu}_{2}$ points as the second, and so on. However, there are many such allocations, and it is not a priori clear which one should be selected. We propose formulating an optimal transport problem that selects the allocation minimizing a particular cost: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pi=\\underset{\\gamma\\in\\Pi(\\mu,\\hat{\\nu})}{\\arg\\operatorname*{min}}\\langle\\gamma,C\\rangle,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\Pi(\\mu,\\hat{\\nu})=\\{\\gamma\\in\\mathbb{R}_{+}^{n\\times K}|\\gamma\\mathbf{1}=\\mu,\\gamma^{T}\\mathbf{1}=\\hat{\\nu}\\}$ , $\\textstyle\\mu={\\frac{1}{n}}\\mathbf{1}$ and $C$ is the cost (loss) matrix such that $C_{i j}$ represents a loss when we classify $x_{i}$ as class $j$ . This procedure is described in Algorithm 1. Note that this procedure naturally matches the given label distribution specification $\\hat{\\nu}$ . ", "page_idx": 2}, {"type": "text", "text": "We wish to use Algorithm 1 for zero-shot classification given the pretrained model $s_{\\theta}$ . To do so, we must select a cost function and produce $C_{i j}$ . An ideal choice of such a function is $C_{i j}\\;=\\;$ $-\\log P_{t}(Y\\,=\\,j\\vert X\\,=\\,i)$ such that optimal transport minimizes the negative log posterior under constraints. However, the target distribution $P_{t}$ is unknown. Instead, we replace the posterior with the classifier scores $s_{\\theta}(x_{i},j)$ . We highlight that this choice of cost matrix is an natural extension of ", "page_idx": 2}, {"type": "text", "text": "zero-shot classification under the label distribution constraint. We prove this claim in the next section.   \nFirst, we show a toy example that shows how OTTER improves zero-shot accuracy. ", "page_idx": 3}, {"type": "text", "text": "Example. To illustrate the beneftis of OTTER, consider the following example for binary classification. We have two data points, $X=\\{x_{1},x_{2}\\}$ with $Y=\\{1,2\\}$ , and true label distribution $\\begin{array}{r}{\\dot{\\nu}=\\left(\\frac{1}{2},\\frac{1}{2}\\right)}\\end{array}$ . Suppose that the zero-shot model\u2019s prediction scores are $s_{1}\\bar{=}\\left(0.4,0.6\\right)$ and $s_{2}=(0.1,0.9)$ . ", "page_idx": 3}, {"type": "text", "text": "Traditional classification yields $\\hat{y}_{1}=2,\\hat{y}_{2}=2$ , producing a $50\\%$ error rate. However, given the cost matrix $C$ derived from the prediction score matrix ", "page_idx": 3}, {"type": "equation", "text": "$$\nS=\\left[\\!\\!\\begin{array}{c c}{{0.4}}&{{0.6}}\\\\ {{0.1}}&{{0.9}}\\end{array}\\!\\!\\right],C=\\left[\\!\\!\\begin{array}{c c}{{-\\log{0.4}}}&{{-\\log{0.6}}}\\\\ {{-\\log{0.1}}}&{{-\\log{0.9}}}\\end{array}\\!\\!\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "along with $\\mu=(0.5,0.5)$ and $\\nu=(0.5,0.5)$ , the optimal transport procedure discovers the transport map $\\pi={\\left[\\!\\!\\begin{array}{l l}{0.5}&{0.0}\\\\ {0.0}&{0.5}\\end{array}\\!\\!\\right]}$ , yielding $\\hat{y}_{1}=1,\\hat{y}_{2}=2$ . This corrects the original zero-shot prediction error. ", "page_idx": 3}, {"type": "text", "text": "Extension to Online Predictions While OTTER offers efficient label distribution adaptation, it requires a batched set of inference data points, making online predictions challenging. To address this, we introduce R-OTTER (Reweighting OTTER), which learns a reweighting factor\u2014an estimate of $P_{t}(Y)/P_{s}(Y)$ \u2014using OTTER\u2019s predictions $y_{\\mathrm{OTTER}}$ on a validation set. Once learned, these reweighting parameters can be applied to online predictions by adjusting logit probability scores. We use a reweighting formulation equivalent to logit adjustment in [71]. The reweighted probability scores of $P_{\\theta}$ , with a reweighting vector $r\\in\\mathbb{R}^{K}$ , are defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nP_{\\theta,r}(Y=j|X=x)=\\frac{r_{j}P_{\\theta}(Y=j|X=x)}{\\sum_{j^{\\prime}=1}^{K}r_{j^{\\prime}}P_{\\theta}(Y=j^{\\prime}|X=x)}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The parameter $r$ is learned using cross-entropy loss on yOTTER. We expect R-OTTER to perform comparably to OTTER, with the additional advantage of not requiring OTTER to be run over the entire dataset. In the following section, we provide a theoretical result showing that $r^{*}=$ $P_{t}(Y)/P_{s}(Y)$ is an optimal reweighting parameter when learned from yOTTER as pseudolabels, effectively addressing label shift. ", "page_idx": 3}, {"type": "text", "text": "4 Theoretical Results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In practical scenarios, label distribution specifications are frequently subject to noise, and prediction probabilities may not be well-calibrated. To understand the impact of these factors, we examine how errors in label distribution specification and calibration influence the transport plan. Our theoretical analysis yields following findings: (a) OTTER can recover the Bayes-optimal classifier in the label shift setting, (b) for a noisy cost matrix with the noisy label distribution specification setup, the suboptimality can be bounded by the deviation of cost matrix and label distribution, and (c) R-OTTER effectively addresses label shift when learned from yOTTER as pseudolabels. ", "page_idx": 3}, {"type": "text", "text": "Classification as optimal transport. Prior to discussing the main theoretical results, we demonstrate that standard classification\u2014expressed as $\\hat{y}_{i}\\,=\\,\\arg\\operatorname*{max}_{j\\in[K]}P_{\\theta}(Y=j|X=x_{i})$ \u2014can be interpreted as a (trivial) solution derived from optimal transport. ", "page_idx": 3}, {"type": "text", "text": "Theorem 4.1. Let $\\begin{array}{r}{\\nu_{j}^{Z S}=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{1}[\\hat{y}_{i}^{Z S}=j]}\\end{array}$ , where $\\begin{array}{r}{\\hat{y}_{i}^{Z S}=\\arg\\operatorname*{max}_{j^{\\prime}\\in[K]}P_{\\theta}(Y=j^{\\prime}|X=x_{i})}\\end{array}$ . Then, given $C_{i j}=-\\log P_{\\theta}(Y=j|X=x_{i})$ ), ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi=\\arg\\underset{\\gamma\\in\\Pi(\\mu,\\nu^{Z S})}{\\operatorname*{min}}\\left\\langle\\gamma,C\\right\\rangle,}\\\\ &{\\hat{y}_{i}^{O T}=\\arg\\underset{j\\in[K]}{\\operatorname*{max}}\\;\\pi_{i j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Assuming there are no ties in scores, i.e. $P_{\\theta}(Y=j|X=x_{i})\\neq P_{\\theta}(\\underline{{Y}}=j^{\\prime}|\\underline{{X}}=x_{i})$ , for all $j\\neq j^{\\prime}$ , the OTTER predictions are equivalent zero-shot predictions, i.e. $\\hat{y}_{i}^{O T}=\\hat{y}_{i}^{Z S}$ for all $i\\in[n]$ . ", "page_idx": 3}, {"type": "text", "text": "This theorem has the following implications. First, it suggests that the predictions will remain unchanged if we set $\\hat{\\nu}=\\nu^{Z S}$ . Second, Bayes-optimal classifiers can be derived through optimal transport, using a (true) cost matrix defined as $C_{i j}^{*}=-\\log P_{t}(Y=j|X=x_{i})$ , coupled with the true label distribution $\\nu^{*}$ . ", "page_idx": 3}, {"type": "text", "text": "Our analysis begins with the label shift setup, which is a commonly-studied type of distribution shift\u2014as well as a prominent issue when applying zero-shot models. We demonstrate that when the label distribution is correctly specified, optimal transport preserves Bayes-optimal classifier predictions under label shift. Next, we consider general perturbations in label distribution and cost matrix as well as their impact on the resulting solutions. ", "page_idx": 4}, {"type": "text", "text": "4.1 Label Shift Invariance ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this setting, we assume features follow the same conditional distribution across source and target distributions, i.e. $P_{s}(X|Y)=P_{t}(X|Y)$ . Furthermore, we suppose that the prediction scores are accurately calibrated in the training dataset, such that $s_{\\theta}(x,j)\\stackrel{..}{=}P_{s}(Y=j|\\bar{X^{}}=x)$ . For zero-shot models, we often lack access to $P_{s}$ . This is a typical scenario in zero-shot model applications: after training on large-scale corpora, we use the pretrained model without the source dataset. ", "page_idx": 4}, {"type": "text", "text": "For a given downstream task with the target label distribution $\\nu^{*}=P_{t}(Y)$ , one standard approach to achieve the Bayes-optimal classifier for the target distribution is to reweight the score function outputs using the rati o PPst((YY  ==jj)). This adjustment leads to: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{s}_{\\theta}(x,j)=s_{\\theta}(x,j)\\cdot\\frac{P_{t}(Y=j)}{P_{s}(Y=j)}\\propto P_{t}(X=x|Y=j)\\cdot P_{t}(Y=j)\\propto P_{t}(Y=j|X=x).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This reweighted score function aligns with the target distribution, thus correcting label shift. ", "page_idx": 4}, {"type": "text", "text": "Although reweighting the score function is a popular solution, it faces an important obstacle when applied to zero-shot models like CLIP, where the source distribution $P_{s}(Y)$ is typically unknown. We show that OTTER successfully induces a Bayes classifier for the target distribution, represented as $\\begin{array}{r}{f_{t}(x)=\\arg\\operatorname*{max}_{j\\in[K]}P_{t}(Y=j|X=x)}\\end{array}$ , without requiring access to $P_{s}(Y)$ . This capability is particularly significant for zero-shot models, enabling them to adapt to target distributions effectively, even in the absence of explicit knowledge of the source distribution. ", "page_idx": 4}, {"type": "text", "text": "Now, we show that optimal transport can be an effective tool to correct label shift. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.2. Suppose the pretrained model is well-calibrated for the source distribution, ", "page_idx": 4}, {"type": "equation", "text": "$$\nP_{\\theta}(Y=j|X=x_{i})=P_{s}(Y=j|X=x_{i})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and there is no tie probability, for all $j\\neq j^{\\prime},i\\in[n]$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P_{\\theta}(Y=j|X=x_{i})\\neq P_{\\theta}(Y=j^{\\prime}|X=x_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Denote the Bayes optimal predictions in the target distribution as $\\hat{y}_{i}^{*}=\\arg\\operatorname*{max}_{j\\in[K]}\\log P_{t}(Y=$ $j|X\\,=\\,x_{i})$ . Then OTTER predictions $\\hat{y}\\,=\\,\\mathrm{OTTER}({\\bf X},\\nu^{*},C)$ are the same as Bayes optimal predictions $\\hat{y}^{*}$ . ", "page_idx": 4}, {"type": "text", "text": "That is, OTTER recovers a Bayes classifier in the target distribution without access to the source distribution, given the target distribution and a well-calibrated model for the source dataset. ", "page_idx": 4}, {"type": "text", "text": "4.2 General Perturbation Sensitivity ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In practical applications, calibration error could extend beyond noise in the elements of the cost matrix. A key source of error is label distribution estimation error. Hence, we address a more general setting, examining the impact of simultaneous perturbations in the label distribution and cost matrix of the transport plan. Our result applies techniques from perturbation theory for linear programming . ", "page_idx": 4}, {"type": "text", "text": "We rewrite our optimal transport problem $\\operatorname*{min}_{\\pi\\in\\Pi(\\mu,\\nu)}\\left\\langle\\pi,C\\right\\rangle$ as a linear programming problem. Let $\\pi$ and $C$ be the transport plan and cost matrix respectively. Matrix $G$ and vector $g$ are used to denote the row and column constraints on $\\pi$ to form a feasible plan which transports distribution from $\\mu$ to $\\nu$ . ", "page_idx": 4}, {"type": "equation", "text": "$$\nH:=\\left[\\mathbb{1}_{n}^{T}\\otimes\\mathbb{I}_{K}\\right],G=\\left[\\!\\!\\begin{array}{l}{H}\\\\ {-H}\\end{array}\\!\\!\\right],g=\\left[\\!\\!\\begin{array}{l}{\\mu}\\\\ {\\nu}\\\\ {-\\mu}\\\\ {-\\nu}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then, we have the equivalent linear programming problem, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\left\\{\\sum_{i,j}C_{i,j}\\pi_{i,j}|G\\cdot\\mathrm{vec}(\\pi)\\geq g,\\pi\\geq0\\right\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We adapt a theorem from Robinson [52] with our optimal transport problem notation. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.3. Let the primal linear programming problem be defined as in equation $(I)$ , and its dual problem be $\\operatorname*{max}\\{w^{T}g|w^{T}G\\leq\\nu e c(C)^{T},w\\geq0\\}$ . Suppose perturbed cost matrix is $\\hat{C}=C+\\Delta_{C}$ , the perturbed class distribution $\\hat{\\nu}=\\nu+\\Delta_{\\nu}$ , such that ${\\hat{g}}=g+\\Delta_{g}$ where ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Delta_{g}=\\left[\\begin{array}{c}{{0}}\\\\ {{\\hat{\\nu}-\\nu}}\\\\ {{0}}\\\\ {{-\\hat{\\nu}+\\nu}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Assume that primal and dual problems are solvable. Denote the original solutions as $\\pi,w$ and perturbed solutions as $\\hat{\\pi}$ and $\\hat{w}$ . Then, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|\\pi-\\hat{\\pi}\\|_{F}^{2}\\leq\\kappa^{2}\\left(\\|\\Delta_{\\nu}\\|_{2}^{2}+\\|[\\nu e c(\\Delta_{C})]_{+}\\|_{2}^{2}+\\|\\nu e c(C)^{T}\\nu e c(\\hat{\\pi})-g^{T}\\hat{w}\\|_{2}^{2}\\right)-\\|w-\\hat{w}\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": ", where $1\\leq p\\leq\\infty$ and $\\kappa$ is a Hoffman constant that only relates to the original problem [30]. ", "page_idx": 5}, {"type": "text", "text": "Ignoring the constant and the subtraction part, the upper bound can be decomposed into three components, ", "page_idx": 5}, {"type": "text", "text": "\u2022 $\\Delta_{\\nu}$ : noise (or the estimation error) of the target balance, \u2022 $\\left[{\\bf v e c}(\\Delta_{C})\\right]_{+}$ : noise (or the calibration error) of the cost matrix, \u2022 $\\mathrm{vec}(C)^{T}\\mathrm{vec}(\\hat{\\pi})-g^{T}\\hat{w}$ : the suboptimality of perturbed solution $\\hat{w}$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.3 implies that the deviation from perturbed solution to true solution is bounded by the magnitude of perturbations and suboptimality of the perturbed solution. From this result, we can expect prediction accuracy to deteriorate with perturbations in the label distribution and calibration. ", "page_idx": 5}, {"type": "text", "text": "4.3 Effectiveness of R-OTTER ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We provide a theoretical result showing that R-OTTER can learn an optimal parameter by learning reweighting parameters from $\\hat{y}$ OTTER as pseudolabels, and produce identical predictions to OTTER once the optimal parameter is obtained in the label shift setup. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.4. Under the same assumptions as in Theorem 4.2, the parameter $r^{\\ast}=P_{t}(Y)/P_{s}(Y)$ is optimal when learning with yOTTER as pseudolabels. ", "page_idx": 5}, {"type": "text", "text": "The proof is provided in Appendix D.6. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The primary objective of our experiments is to (1) validate that OTTER improves zero-shot model performance when given accurate label distribution estimates and (2) investigate its sensitivity to perturbations. In experiments on real datasets (Section 5.1), we confirm that OTTER can improve zero-shot classification significantly in a variety of settings. In synthetic experiments (Section 5.2), we validate our theoretical claims\u2014label shift invariance and sensitivity to perturbation in a fully controllable setting. Additionally, we show that OTTER can be combined with label distribution estimation methods in the few-shot learning setting (Section 5.3). Next, we show the empirical results for H-OTTER that leverages label hierarchy (Section 5.4) and R-OTTER that mitigates the batched prediction requirement (Section 5.5). Finally, we we apply OTTER to mitigate LLM selection bias (Section 5.6). Our code is available at https://github.com/SprocketLab/OTTER. ", "page_idx": 5}, {"type": "text", "text": "5.1 Real Data Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We hypothesize that the model performance can improve significantly when the label distribution specification is exact. ", "page_idx": 5}, {"type": "text", "text": "Setup and Procedure. We used 17 image classification datasets and 4 text classification datasets. We employed CLIP [49] for image zero-shot classification, and BERT [19]. A comprehensive list and details of experiments can be found in Appendix E. ", "page_idx": 5}, {"type": "text", "text": "Baseline. We adopt Prior Matching (PM) [37] as a baseline. It optimizes score weighting parameters to align with the label distribution specification. A detailed explanation of Prior Matching is given in Appendix C. It is worth noting that the performance of Prior Matching is highly sensitive to hyperparameters such as temperature and learning rate. Optimal hyperparameters may vary across different datasets. We selected hyperparameters through grid search, by evaluating their performance on a validation set, consisting of 10 labeled examples per class. In contrast, we highlight that OTTER is tuning-free. ", "page_idx": 5}, {"type": "table", "img_path": "RsawwSBCs7/tmp/d3a39e3d976ec1e47831d63d53231b7f62a3ca429d44d18aa3eb4f3fa9e75b70.jpg", "table_caption": ["Zero-shot Prior Matching OTTER Zero-shot Prior Matching OTTER "], "table_footnote": ["Table 1: Accuracy $(\\%)$ in zero-shot image classification (ViT-B/16) and text classification (BERT). We use the true label distribution as the label distribution specification. The numbers in parenthesis of Prior Matching represent the standard deviation of 10 different samplings of the validation set. OTTER produces improvements nearly across-the-board, with an average lift $4.9\\%$ in image classification and $15.5\\%$ in text classification, outperforming a powerful baseline, prior matching in almost all cases. "], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Results. Table 1 shows the image classification results with CLIP (ViT-B/16) and the text classification results with BERT. Notably, OTTER demonstrates a $4.8\\%$ and $I5.5\\%$ enhancement on average in image and text zero-shot classification, respectively. While Prior Matching shows competitive performance when accurately tuned, it often struggles. We found that hyperparameter tuning fails in the class-imbalanced datasets such as Caltech256, SUN397, ImageNet-r (Appendix E, Table 7). This suggests that the hyperparameter selection process necessitates a validation set label distribution similar to the target distribution\u2014rendering it unusable in zero-shot scenarios. More details and additional experiment results \u2014 including the sensitivity study on the label distribution specification error, computation time, and combination with other prompting methods \u2014 are provided in Appendix E.3. ", "page_idx": 6}, {"type": "text", "text": "5.2 Synthetic Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We hypothesize OTTER is invariant to label shift under the conditions in Theorem 4.2. We also investigate the sensitivity to perturbations of the cost matrix and the label distribution. ", "page_idx": 6}, {"type": "text", "text": "Setup and Procedure. We simulate label shift in logistic regression. Suppose $X|Y\\;=\\;0\\;\\sim$ $\\mathcal{N}(-1,1)$ and $X|Y\\;=\\;1\\;\\sim\\;{\\mathcal{N}}(1,1)$ . Training data is sampled from a mixture of Gaussians $X_{s}\\,\\sim\\,\\nu_{0}^{s}{\\mathcal N}(-1,1)\\,+\\,\\nu_{1}^{s}{\\mathcal N}(1,1)$ such that $P_{s}(Y\\,=\\,0)\\,=\\,\\nu_{0}^{s},P_{s}(Y\\,=\\,1)\\,=\\,\\nu_{1}^{s}$ , $\\nu_{0}^{s}+\\nu_{1}^{s}\\,=\\,1$ . Similarly, we sample the test data from $X_{t}\\sim\\nu_{0}^{t}/N(-1,1)+\\nu_{1}^{t}/N(1,1)$ . We fix the training set label distribution as $\\nu_{0}^{s}=0.1,\\nu_{1}^{s}=0.9$ and vary test set label distribution $\\nu^{t}$ to simulate label shift. We train a logistic regression model with 10,000 samples from the source distribution, and test the model with 10,000 samples from the target distribution. A Bayes-optimal classifier in the target distribution is given by $\\begin{array}{r}{\\overline{{f_{B a y e s}(x)}}=\\mathbb{1}[\\bar{x}\\geq\\frac{1}{2}(\\log\\frac{\\nu_{0}^{t}}{\\nu_{1}^{t}}+1)]}\\end{array}$ . The naive classifier is defined as the maximizer of the predicted score. The OTTER predictions are produced with Algorithm 1, with the cost matrix $\\bar{C_{i j}}=-\\log P_{\\theta}(Y=j|X=x_{i})$ and the label distribution specification $\\nu^{t}$ , where $P_{\\theta}(Y|X)$ represents the logistic regression model scores. ", "page_idx": 6}, {"type": "text", "text": "We separately investigate perturbed prediction score matrix and perturbed label distribution specification\u2019s impact on the prediction accuracy. For perturbed prediction scores, we fix the label distribution to be the true one, and add noise $\\dot{\\delta}\\sim\\mathcal{N}(\\bar{0},\\sigma^{2})$ of varying levels $\\sigma$ to the predicted score $P_{\\theta}(Y=1|X)$ . For the perturbed label distribution specification, we fix the prediction score to be true scores and add noise $\\epsilon$ : $\\hat{\\nu}=\\nu^{t}+(\\epsilon,-\\epsilon)$ . We use these perturbed variants to obtain perturbed solutions and compare with ground-truth solution. ", "page_idx": 6}, {"type": "text", "text": "Results. Figure 2 illustrates how accuracy changes with label shift when the predicted score is perturbed and when label distribution specification is perturbed. We observe that the naive classifier ", "page_idx": 6}, {"type": "image", "img_path": "RsawwSBCs7/tmp/b6e6d93783722646f490c7a743865e4e26641692bfb374d90da76ba6915d448c.jpg", "img_caption": ["(a) Prediction accuracy changes with perturbed confidence score. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "RsawwSBCs7/tmp/96125cba1c692e567dfd88c77d10e3a02cbe34fe3e84e6d087ec81aefbde3a56.jpg", "img_caption": ["(b) Prediction accuracy changes with perturbed label distribution. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 2: Synthetic experiment results. X-axis represents total variation distance between the source and the target distribution, describing label shift severity. Y-axis represents prediction accuracy. Curves represent different methods and noise levels. Our approaches dramatically outperform the baseline at higher mismatch levels. ", "page_idx": 7}, {"type": "text", "text": "deteriorates as the total variation distance between source and target distributions increases. It indicates that naive classifier is sensitive to label shift. However, without perturbation, OTTER remains unaffected by the label distribution shift, which validates our invariance result in Section 4. ", "page_idx": 7}, {"type": "text", "text": "In the case of confidence prediction perturbation, both the naive classifier and OTTER have accuracy decreasing as perturbation level increases. For simplicity, we omitted the naive classifier\u2019s performances under different levels of noise as adding zero-mean noise does not alter its accuracy significantly. We observe that OTTER has better performance than the naive method when significant label shift exists. Similarly, for label distribution perturbation, we observe as the noise level $\\epsilon$ increases, OTTER\u2019s accuracy downgrades\u2014but still yields better performance when label shift is severe. ", "page_idx": 7}, {"type": "text", "text": "Our experimental results suggest simply using prediction scores for zero-shot classification leads to inaccurate predictions under label shift, while OTTER is robust to label shift when no perturbations are present. Perturbations in both predicted score and label distribution specification downgrades the predicted accuracy, as expected, but OTTER still yields better results than the naive baseline. ", "page_idx": 7}, {"type": "image", "img_path": "RsawwSBCs7/tmp/a7835945ae54b491a1f4923c42f72de87c3d7edb1aefe0e0580de24d98f156dd.jpg", "img_caption": ["5.3 Few-shot adaptation with label distribution estimation "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: Ablation on the number of samples in few-shot learning. In (a), We can observe that BBSE estimation get more precise as the number of samples increases. Following this, OTTER gets better accuracy in (b). Additionally, OTTER consistently improves linear probing when combined. ", "page_idx": 7}, {"type": "text", "text": "We anticipate that OTTER can be used in few-shot learning when combined with label distribution estimation methods. We expect OTTER can improve zero-shot classification if the label distribution estimation error is sufficiently small. Also, we expect OTTER can improve linear probing, which is one of standard approaches for few-shot learning. ", "page_idx": 7}, {"type": "text", "text": "Setup and Procedure. We use the same datasets as the previous experiment. We consider a 10-shot learning setting: 10 labeled samples per class are given. Note that labeled samples have uniform label distribution, while the label distribution in the target distribution may not be uniform. This setting requires the use of label distri4]. We estimate the target label bution estimation methods used in label shift adaptation [35; 6; 2 ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "RsawwSBCs7/tmp/92bd6bf646d0206041ef95e2d8e6b221d6bc5ab11f2ca6d813c6a195cd16c000.jpg", "table_caption": ["Dataset ZS ZS BBSE+PM ZS BBSE+OT LP LP BBSE+PM LP BBSE+OT "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 2: Accuracy $\\overline{{(\\%)}}$ with OTTER combined with class balance estimation. ZS BBSE denotes BBSE label distribution estimation based on zero-shot prediction scores, and LP BBSE denotes BBSE label distribution estimation based on linear probing prediction scores. We report the mean of 10 different random samplings of the validation set. OTTER produces moderate improvements when comined with linear probing in image classification tasks. In text classification tasks, OTTER significantly improves accuracy, up to $15.1\\%$ , even with noisy label distribution estimation. ", "page_idx": 8}, {"type": "text", "text": "distribution with Black Box Shift Estimation (BBSE) [35]. BBSE estimates the target balance using confusion matrix, under the label shift assumption. For detailed explanation, refer to Appendix C. ", "page_idx": 8}, {"type": "text", "text": "Results. Table 2 shows the image and text zero-shot classification results with the label distribution estimation via BBSE and linear probing. The image classification results show that OTTER can yield mild improvement over linear probing, even with the label distribution estimation errors. Figure 4 shows that accuracy improvement is consistent across the number of samples used for linear probing. In text classification, we found OTTER improves zero-shot text classifications where the number of classes is small ( $K=2$ or 3). While it shows a relatively high variance due to the small sample size $(20\\sim30)$ ), the average accuracy improves significantly over zero-shot classification. More detailed analysis regarding label distribution estimation error and the number of samples is provided in Appendix E.4. ", "page_idx": 8}, {"type": "table", "img_path": "RsawwSBCs7/tmp/204eab6f86c3791584c5f58b95037a6dd80cc7526a362a96a159d17b73f6765f.jpg", "table_caption": ["5.4 Zero-shot prediction improvement with class hierarchy "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 3: Accuracy $(\\%)$ with hierarchical OTTER (H-OTTER). (H-OTTER) yields additional improvements over OTTER, up to $5.1\\%$ , using the hierarchy information of labels. ", "page_idx": 8}, {"type": "text", "text": "We hypothesize incorporating class hierarchy information can enhance few-shot label distribution estimation and thus improve zero-shot predictions. ", "page_idx": 8}, {"type": "text", "text": "Setup and Procedure. We use a subset of CIFAR100 data with WordNet hierarchy. Specifically, we take \u2018fish\u2019 and \u2018tree\u2019 as superclasses and have 5 subclasses in each of them. We suppose we can access 10 labeled samples per each subclass. We first apply OTTER with the superlevel label distribution estimation and make pseudo-labels of superlevel class in the test set. Using them, we estimate the sublevel label distribution and use OTTER. ", "page_idx": 8}, {"type": "text", "text": "Results. Table 3 presents the results. As anticipated, we note an enhancement in accuracy when compared to the naive implementation of OTTER. Specifically, we observe a significant improvement in accuracy for RN50, RN101, and ViT-B/16, which we attribute primarily to the reduction in label distribution estimation error. Further details are provided in Appendix E.5. ", "page_idx": 8}, {"type": "table", "img_path": "RsawwSBCs7/tmp/6dc35fb8fa03ad918d6eebec49f20dcad9c897ad9cd4ec00fab5b793445b1d6d.jpg", "table_caption": ["Zero-shot OTTER R-OTTER Zero-shot OTTER R-OTTER "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "RsawwSBCs7/tmp/3a6c7ae0039b78235db56117ba5019effac7fd5b0e16cefd79ade612c7c75b50.jpg", "table_caption": [], "table_footnote": ["Table 4: Accuracy (%) of naive zero-shot, OTTER, and R-OTTER in zero-shot image classification (ViT-B/16) ", "Table 5: Mitigation of selection bias via OTTER. "], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5.5 Effectiveness of R-OTTER ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We show that R-OTTER provides a performance comparable to that of OTTER empirically. ", "page_idx": 9}, {"type": "text", "text": "Setup and Procedure. We use the identical setup for Section 5.1. R-OTTER learns reweighting parameters in validation set using yOTTER. Note that the validation set is not required to be labeled since yOTTER is used as pseudolabels in the validation set. ", "page_idx": 9}, {"type": "text", "text": "Results. Although R-OTTER is suboptimal compared to OTTER due to generalization issues, it still provides label distribution correction, improving accuracy over zero-shot predictions. We also provide synthetic experiments for R-OTTER in Appendix E.6. ", "page_idx": 9}, {"type": "text", "text": "5.6 Mitigating selection bias in LLM multiple-choice questions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Selection bias is the tendency of LLMs to favor certain prefix tokens in multiple-choice questions [69; 17; 12; 65]. We demonstrate that OTTER can effectively mitigate selection bias by randomly shuffling the options and enforcing a uniform class balance in OTTER. ", "page_idx": 9}, {"type": "text", "text": "Setup and Procedure. Our experimental setup follows Zheng et al. [69], utilizing the MMLU [28], ARC-Challenge [15], and CommonsenseQA (CSQA) [57] datasets. We test with llama-2(- chat)-7/13B [60] and vicuna-v1.3-7/13B [11] models, treating the probabilities of each option token (A/B/C/D) as prediction probabilities. OTTER is applied under the assumption of a uniform distribution. We use 0-shot predictions and evaluate performance using accuracy and the standard deviation of recalls (RStd) as metrics. ", "page_idx": 9}, {"type": "text", "text": "Results. Table 5 presents the experimental results. OTTER significantly reduces selection bias, enhancing accuracy by up to $10.6\\%$ and lowering RStd by as much as $25.7\\%$ . ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "While zero-shot models have been successful, pretraining using Internet-scale datasets yields artifacts that may harm downstream tasks. In this paper, we identify the bias in class balance, and provide a simple but powerful solution using optimal transport. Theoretically, we describe how OT can fix label distribution mismatch and its sensitivity to perturbations. Empirically, we validated our approach\u2019s ability to improve zero-shot classification accuracy, mitigating label distribution mismatch in zero-shot models. We believe our method can expedite the deployment of zero-shot classification, reducing the necessity of finetuning for downstream tasks. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We are grateful for the support of the NSF under CCF2106707 (Program Synthesis for Weak Supervision) and the Wisconsin Alumni Research Foundation (WARF). Jitian Zhao gratefully acknowledge support from the IFDS at UW-Madison and NSF through TRIPODS grant 2023239 for their support. We thank Nick Roberts for his valuable discussions and insights. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Dyah Adila, Changho Shin, Linrong Cai, and Frederic Sala. Zero-shot robustification of zero-shot models. In The Twelfth International Conference on Learning Representations, 2023.   \n[2] Sandhini Agarwal, Gretchen Krueger, Jack Clark, Alec Radford, Jong Wook Kim, and Miles Brundage. Evaluating clip: towards characterization of broader capabilities and downstream implications. arXiv preprint arXiv:2108.02818, 2021.   \n[3] Amr Alexandari, Anshul Kundaje, and Avanti Shrikumar. Maximum likelihood with biascorrected calibration is hard-to-beat at label shift adaptation. In International Conference on Machine Learning, pages 222\u2013232. PMLR, 2020.   \n[4] Bang An, Sicheng Zhu, Michael-Andrei Panaitescu-Liess, Chaithanya Kumar Mummadi, and Furong Huang. More context, less distraction: Visual classification by inferring and conditioning on contextual attributes. arXiv preprint arXiv:2308.01313, 2023.   \n[5] Dominique Az\u00e9 and Jean-No\u00ebl Corvellec. On the sensitivity analysis of hoffman constants for systems of linear inequalities. SIAM Journal on Optimization, 12(4):913\u2013927, 2002.   \n[6] Kamyar Azizzadenesheli, Anqi Liu, Fanny Yang, and Animashree Anandkumar. Regularized learning for domain adaptation under label shifts. In International Conference on Learning Representations, 2019.   \n[7] Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Nuanced metrics for measuring unintended bias with real data for text classification. In Companion proceedings of the 2019 world wide web conference, pages 491\u2013500, 2019.   \n[8] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative components with random forests. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13, pages 446\u2013461. Springer, 2014.   \n[9] Wanxing Chang, Ye Shi, Hoang Tuan, and Jingya Wang. Unified optimal transport framework for universal domain adaptation. Advances in Neural Information Processing Systems, 35: 29512\u201329524, 2022.   \n[10] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16: 321\u2013357, 2002.   \n[11] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90\\%^{*}$ chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.   \n[12] Hyeong Kyu Choi, Weijie Xu, Chi Xue, Stephanie Eckman, and Chandan K Reddy. Mitigating selection bias with node pruning and auxiliary options. arXiv preprint arXiv:2409.18857, 2024.   \n[13] Ching-Yao Chuang, Varun Jampani, Yuanzhen Li, Antonio Torralba, and Stefanie Jegelka. Debiasing vision-language models via biased prompts. arXiv preprint arXiv:2302.00070, 2023.   \n[14] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3606\u20133613, 2014.   \n[15] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.   \n[16] Adam Coates, Andrew $\\mathrm{Ng}$ , and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 215\u2013223. JMLR Workshop and Conference Proceedings, 2011.   \n[17] Fahim Dalvi, Abdul Rafae Khan, Firoj Alam, Nadir Durrani, Jia Xu, and Hassan Sajjad. Discovering latent concepts learned in bert. In International Conference on Learning Representations.   \n[18] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \n[20] Emily Dinan, Angela Fan, Ledell Wu, Jason Weston, Douwe Kiela, and Adina Williams. Multidimensional gender bias classification. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 314\u2013331, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.23. URL https://www.aclweb.org/anthology/2020.emnlp-main.23.   \n[21] Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Measuring and mitigating unintended bias in text classification. 2018.   \n[22] Li Fei-Fei, Robert Fergus, and Pietro Perona. One-shot learning of object categories. IEEE transactions on pattern analysis and machine intelligence, 28(4):594\u2013611, 2006.   \n[23] Alberto Fern\u00e1ndez, Salvador Garc\u00eda, Mikel Galar, Ronaldo C Prati, Bartosz Krawczyk, and Francisco Herrera. Learning from imbalanced data sets, volume 10. Springer, 2018.   \n[24] Saurabh Garg, Yifan Wu, Sivaraman Balakrishnan, and Zachary Lipton. A unified view of label shift estimation. Advances in Neural Information Processing Systems, 33:3290\u20133300, 2020.   \n[25] Gregory Griffin, Alex Holub, and Pietro Perona. Caltech-256 object category dataset. 2007.   \n[26] Dandan Guo, Zhuo Li, He Zhao, Mingyuan Zhou, Hongyuan Zha, et al. Learning to re-weight examples with optimal transport for imbalanced classification. Advances in Neural Information Processing Systems, 35:25517\u201325530, 2022.   \n[27] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12(7):2217\u20132226, 2019.   \n[28] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.   \n[29] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8340\u20138349, 2021.   \n[30] Alan J Hoffman. On approximate solutions of systems of linear inequalities. Journal of Research of the National Bureau of Standards, 49(4), 1952.   \n[31] Jonathan Kahana, Niv Cohen, and Yedid Hoshen. Improving zero-shot models with label distribution priors. arXiv preprint arXiv:2212.00784, 2022.   \n[32] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for finegrained categorization. In Proceedings of the IEEE international conference on computer vision workshops, pages 554\u2013561, 2013.   \n[33] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[34] Zachary Lipton, Yu-Xiang Wang, and Alexander Smola. Detecting and correcting for label shift with black box predictors. In International conference on machine learning, pages 3122\u20133130. PMLR, 2018.   \n[35] Zachary Lipton, Yu-Xiang Wang, and Alexander Smola. Detecting and correcting for label shift with black box predictors. In International conference on machine learning, pages 3122\u20133130. PMLR, 2018.   \n[36] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023.   \n[37] Adian Liusie, Potsawee Manakul, and Mark JF Gales. Mitigating word bias in zero-shot prompt-based classifiers. arXiv preprint arXiv:2309.04992, 2023.   \n[38] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2018.   \n[39] Binny Mathew, Punyajoy Saha, Seid Muhie Yimam, Chris Biemann, Pawan Goyal, and Animesh Mukherjee. Hatexplain: A benchmark dataset for explainable hate speech detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 14867\u201314875, 2021.   \n[40] Sachit Menon and Carl Vondrick. Visual classification via description from large language models. arXiv preprint arXiv:2210.07183, 2022.   \n[41] Jianmo Ni, Jiacheng Li, and Julian McAuley. Justifying recommendations using distantlylabeled reviews and fine-grained aspects. In Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP), pages 188\u2013197, 2019.   \n[42] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In 2008 Sixth Indian conference on computer vision, graphics & image processing, pages 722\u2013729. IEEE, 2008.   \n[43] Zachary Novack, Julian McAuley, Zachary Chase Lipton, and Saurabh Garg. Chils: Zero-shot image classification with hierarchical label sets. In International Conference on Machine Learning, pages 26342\u201326362. PMLR, 2023.   \n[44] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012 IEEE conference on computer vision and pattern recognition, pages 3498\u20133505. IEEE, 2012.   \n[45] Javier Pena, Juan C Vera, and Luis F Zuluaga. New characterizations of hoffman constants for systems of linear constraints. Mathematical Programming, 187:79\u2013109, 2021.   \n[46] Javier F Pe\u00f1a. An easily computable upper bound on the hoffman constant for homogeneous inequality systems. Computational Optimization and Applications, 87(1):323\u2013335, 2024.   \n[47] Hanyu Peng, Mingming Sun, and Ping Li. Optimal transport for long-tailed recognition with learnable cost matrix. In International Conference on Learning Representations, 2021.   \n[48] Gabriel Peyr\u00e9, Marco Cuturi, et al. Computational optimal transport: With applications to data science. Foundations and Trends\u00ae in Machine Learning, 11(5-6):355\u2013607, 2019.   \n[49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[50] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. arXiv preprint arXiv:1908.10084, 2019.   \n[51] Nicholas Roberts, Xintong Li, Dyah Adila, Sonia Cromp, Tzu-Heng Huang, Jitian Zhao, and Frederic Sala. Geometry-aware adaptation for pretrained models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[52] Stephen M Robinson. Bounds for error in the solution set of a perturbed linear program. Linear Algebra and its applications, 6:69\u201381, 1973.   \n[53] Filippo Santambrogio. Optimal transport for applied mathematicians. Birk\u00e4user, NY, 55(58-63): 94, 2015.   \n[54] Liangliang Shi, Haoyu Zhen, Gu Zhang, and Junchi Yan. Relative entropic optimal transport: a (prior-aware) matching perspective to (unbalanced) classification. Advances in Neural Information Processing Systems, 36, 2024.   \n[55] Remi Tachet des Combes, Han Zhao, Yu-Xiang Wang, and Geoffrey J Gordon. Domain adaptation with conditional distribution matching and generalized label shift. Advances in Neural Information Processing Systems, 33:19276\u201319289, 2020.   \n[56] Kai Sheng Tai, Peter D Bailis, and Gregory Valiant. Sinkhorn label allocation: Semi-supervised classification via annealed self-training. In International Conference on Machine Learning, pages 10065\u201310075. PMLR, 2021.   \n[57] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149\u20134158, 2019.   \n[58] Nguyen Thai-Nghe, Zeno Gantner, and Lars Schmidt-Thieme. Cost-sensitive learning methods for imbalanced data. In The 2010 International joint conference on neural networks (IJCNN), pages 1\u20138. IEEE, 2010.   \n[59] Antonio Torralba and Alexei A. Efros. Unbiased look at dataset bias. In CVPR 2011, pages 1521\u20131528, 2011. doi: 10.1109/CVPR.2011.5995347.   \n[60] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[61] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011.   \n[62] Haobo Wang, Mingxuan Xia, Yixuan Li, Yuren Mao, Lei Feng, Gang Chen, and Junbo Zhao. Solar: Sinkhorn label refinery for imbalanced partial-label learning. Advances in Neural Information Processing Systems, 35:8104\u20138117, 2022.   \n[63] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. Advances in Neural Information Processing Systems, 32, 2019.   \n[64] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.   \n[65] Sheng-Lun Wei, Cheng-Kuang Wu, Hen-Hsen Huang, and Hsin-Hsi Chen. Unveiling selection biases: Exploring order and token sensitivity in large language models. arXiv preprint arXiv:2406.03009, 2024.   \n[66] Ruicheng Xian, Lang Yin, and Han Zhao. Fair and optimal classification via post-processing. 2023.   \n[67] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on computer vision and pattern recognition, pages 3485\u20133492. IEEE, 2010.   \n[68] Chuyu Zhang, Hui Ren, and Xuming He. $\\mathrm{{P^{2}}}$ ot: Progressive partial optimal transport for deep imbalanced clustering. arXiv preprint arXiv:2401.09266, 2024.   \n[69] Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. Large language models are not robust multiple choice selectors. In The Twelfth International Conference on Learning Representations, 2023.   \n[70] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. International Journal of Computer Vision, 130(9):2337\u20132348, 2022.   \n[71] Beier Zhu, Kaihua Tang, Qianru Sun, and Hanwang Zhang. Generalized logit adjustment: Calibrating fine-tuned models by removing label bias in foundation models. Advances in Neural Information Processing Systems, 36, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The appendix contains glossary, algorithm details, proofs, and detailed experimental results. The glossary contains a convenient reminder of our terminology (Appendix A). Appendix B provides more related works and discussion about the relationship between our work and related papers. Appendix C describes the relevant algorithms used in our experiments, including Prior Matching [37] and BBSE [35]. Appendix D provides the proofs of theorems that appeared in Section 4. Finally, we give more details and analysis of the experiments and provide additional experiment results in Appendix E. ", "page_idx": 15}, {"type": "text", "text": "A Glossary ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The glossary is given in Table 6 below. ", "page_idx": 15}, {"type": "table", "img_path": "RsawwSBCs7/tmp/a55871fc11c7d9cdfee82976a127a94204396ba9622a8412891f98a80e80af87.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "B Extended Related Work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Improving Zero-shot Classification at Inference Time. As zero-shot classification has gained popularity, several works have been developed to improve zero-shot classification at inference time. Chuang et al. [13]; Adila et al. [1] use vector projection methods to remove spurious correlations at inference time. Menon and Vondrick [40]; Novack et al. [43]; An et al. [4] augment prompts with language models and combine their classification output to improve zero-shot performance. Roberts et al. [51] uses additional information of label space geometry to extend model pre-trained on the label subset to broader use-cases. While these works try to improve zero-shot classification at inference time in common, the main difference is that our method tackles the inherent class prior of zero-shot models. ", "page_idx": 15}, {"type": "text", "text": "Label Shift Adaptation. Label shift adaptation methods are designed to address the negative impacts arising from changes in the label distribution. These methods typically follow a two-step process [35; 6; 24]. The first step involves estimating the label distribution within the target dataset using labeled data from the source distribution and unlabeled data from the target distribution. Next, the prediction scores are reweighted using the estimated target label distribution and the source label distribution. However, the standard approach requires access to the labeled source distribution data, which is usually not possible in zero-shot classification scenarios. OTTER provides a solution decoupled from the source data distribution, overcoming this limitation. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Improving Zero-shot Classification using Prior. Several studies have explored leveraging prior information to enhance zero-shot classification, even in the absence of access to source distributions. In the context of prompt-based zero-shot models, prior matching [37] employs word prior distribution to alleviate word bias inherent in pretraining data. We adopted their adaptation method as a baseline. Similarly, Kahana et al. [31] develop adaptation layers trained using priors, aiming to maintain proximity to the original scores. However, both approaches entail training additional layers and necessitate hyperparameter tuning, which may pose challenges in the context of zero-shot predictions. In contrast, OTTER presents a straightforward and efficient adaptation method to new label distributions without the need for any hyperparameter tuning, backed by theoretical guarantees. ", "page_idx": 16}, {"type": "text", "text": "Leveraging Optimal Transport for Enhanced Pseudo-labeling and Classification. A number of studies have explored the enhancement of pseudo-labeling and classification tasks through optimal transport, using label distribution specifications, in a similar spirit to our work, but within different contexts. Tai et al. [56] uses optimal transport to allocate pseudo-labels to unlabeled datasets based on the label distribution specification in the semi-supervised setup. Wang et al. [62] deals with long-tail distribution in the partial-label learning setup based on optimal transport. Zhang et al. [68] uses partial optimal transport as a pseudo-labeling based approach for deep imbalanced clustering, progressively expanding the labeled sample proportion. Guo et al. [26] reweights the training dataset to match the label distribution specification using optimal transport. This work mainly deals with the class imbalance problem in the training step. Shi et al. [54] studies classification from a matching perspective, revealing the connection between softmax loss and inverse optimal transport and suggesting a new loss function to address long-tail distributions. Their analysis provides useful insights for OTTER \u2014 why cost matrix induced by pre-trained models can be useful in the inference step. Xian et al. [66] uses optimal transport as a postprocessing method to guarantee demographic parity. While sharing aspects of the approach, our work addresses class bias in zero-shot models. Peng et al. [47] used optimal transport to handle long-tail recognition with a learned cost matrix. Our study provides a theoretical basis for understanding their empirical results. Chang et al. [9] employs optimal transport to detect common and private classes between the source and the target domain, under the universal domain adaptation setting, where knowledge is transferred from source to target domain without any constraints on the label sets. In the context of zero-shot classification, there is no need to manage label space disparities between the source and target domains. Instead, the main concern of zero-shot classification is dealing with the distribution shift between the pretraining dataset and the downstream task. We tackle the label distribution shifts using optimal transport. ", "page_idx": 16}, {"type": "text", "text": "Class Imbalance. Class imbalance problems occur when the number of instances across different classes in a dataset is disproportionately distributed. This imbalance can severely bias the traininig process of a machine learning model, leading to poor generalization performance, especially for the minority classes. It has been extensively studied in the context of traditional machine learning [23]. Oversampling [10] and cost-sensitive learning [58] are well-known approaches to address class imbalance. Nonetheless, the inherent nature of class imbalance in pretraining datasets introduces a distinct set of challenges, especially when attempting to rectify such biases within the context of zero-shot classification scenarios. ", "page_idx": 16}, {"type": "text", "text": "C Algorithm details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Prior matching [37] proposed prior matching as a reweighting method for prompt-based classifiers to mitigate word bias \u2014 the distribution shift between pretrained language models\u2019 word prior and the class priors in the downstream task. We use it as a zero-shot model adaptation method given a class balance estimation. ", "page_idx": 16}, {"type": "text", "text": "Define reweighted probability scores of $P_{\\theta}$ with $r$ as $\\begin{array}{r l r l r l}{P_{\\theta,r}(Y}&{{}=}&{j|X}&{{}=}&{x_{i})}&{{}=}&{}\\end{array}$ $r_{j}P_{\\theta}(Y=j|X=x_{i})$ . Ideally, we hope to estimate the weight vector $r^{*}\\in\\mathbb{R}^{n}$ such that   \n$\\begin{array}{r}{\\overline{{\\sum_{j^{\\prime}=1}^{K}r_{j^{\\prime}}P_{\\theta}(Y=j^{\\prime}|X=x_{i})}}}\\end{array}$   \nreweighted scores $P_{\\theta,r^{*}}(Y=j|X=x_{i})$ maximizes the accuracy in the target distribution. Since   \nthe labels are not given, this is impossible. Instead, prior matching matches the label distribution of ", "page_idx": 16}, {"type": "text", "text": "Algorithm 2 Black Box Shift Estimator (BBSE) ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Input: Source input data $\\mathbf{X^{s}}\\,=\\,\\{x_{1}^{s},\\ldots,x_{m}^{s}\\}$ , Source labels $\\mathbf{Y^{s}}\\,=\\,\\{y_{1}^{s},\\dotsc,y_{m}^{s}\\}$ , Target input data   \n$\\mathbf{X}^{\\bar{\\mathbf{t}}}=\\{x_{1}^{t},\\dots,x_{n}^{t^{\\star}}\\}$ , model prediction distribution $P_{\\theta}$   \nEstimate the source class balance \u03bds such that \u03bdjs =  i=1 P\u03b8(Y m= j|X = xis)   \nCompute the naive target class balance \u03bd\u02dct such that \u03bd\u02dcjt =  i=1 P\u03b8(Y n= j|X = xi)   \nEstimate confusion matrix $V$ such that $\\cal{A}_{j k}=\\frac{1}{m}{\\sum}_{i=1}^{m}\\,P_{\\theta}(Y=k{|X=x_{i}^{s})}$   \nEstimate the refined target class balance $\\hat{\\nu}^{t}=A\\tilde{\\nu}^{t}$   \nReturn $\\hat{\\nu}$ ", "page_idx": 17}, {"type": "text", "text": "predicted classes with the class balance estimate $\\hat{\\nu}$ , i.e. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{r}_{j}=\\underset{r_{j}\\in\\mathbb{R}}{\\arg\\operatorname*{min}}\\left|\\sum_{i=1}^{n}P_{\\theta,r}(Y=j|X=x_{i})-\\nu_{j}\\right|.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It can be solved using the standard optimization techniques \u2014 we used [38]. While this is equivalently effective with OTTER when properly optimized, we found that the temperature parameter and learning rate crucially affect the final accuracy, making it less ideal for the zeroshot adaptation. We used the grid search with the small validation set (10 samples per class) in each task to select hyperparameters. The hyperparameter ranges are as follows. ", "page_idx": 17}, {"type": "text", "text": "\u2022 Temperature: [1e-3, 1e-4, 1e-5, 1e-6, 1e-7] \u2022 Learning rate: [1e-3, 1e-4, 1e-5, 1e-6, 1e-7] ", "page_idx": 17}, {"type": "text", "text": "Black Box Shift Estimation (BBSE) Label shift adaptation methods [35; 6; 24] aims to estimate the class balance in the target distribution using the labeled source distribution data and the unlabeled target distribution data. We use Black Box Shift Estimation (BBSE) to estimate the class balance in the downstream task. Algorithm 2 describes the procedure. Note that the derivation of this algorithm depends on the label shift assumptions, thus the label distribution estimation with real-world data can be heavily biased. ", "page_idx": 17}, {"type": "text", "text": "D Theory details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. Suppose $\\hat{y}_{i}^{O T}\\neq\\hat{y}_{i}^{Z S}$ for some $i\\,\\in\\,[n]$ . It implies $\\begin{array}{r}{\\sum_{i=1}^{n}-\\log P_{\\theta}(Y=\\hat{y}_{i}^{O T}|X=x_{i})<}\\end{array}$ $\\begin{array}{r}{\\sum_{i=1}^{n}-\\log P_{\\theta}(Y=\\hat{y}_{i}^{Z S}|X=x_{i})}\\end{array}$ . However, this is a contradiction since, for any $i\\in[n],\\hat{y}_{i}^{Z S}=$ ar $;\\operatorname*{max}_{j\\in[K]}P_{\\theta}(Y=j|X=x_{i})$ , thus \u2212log P\u03b8(Y = y\u02c6iZS|X = xi) \u2264\u2212log P\u03b8(Y = j|X = xi) for all $j\\in[K]$ , which results in $\\hat{y}_{i}^{O T}=\\hat{y}_{i}^{Z S}$ . \u53e3 ", "page_idx": 17}, {"type": "text", "text": "D.2 Proof of Theorem 4.2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To prove Theorem 4.2, we show a specific form of invariance property of optimal transport first. ", "page_idx": 17}, {"type": "text", "text": "Theorem D.1. Suppose $\\pi^{*}=\\arg\\operatorname*{min}_{\\gamma\\in\\Pi(\\mu,\\nu)}\\left\\langle\\gamma,C\\right\\rangle$ and $E$ is a columnwise perturbation, i.e., ", "page_idx": 17}, {"type": "equation", "text": "$$\nE=\\left[\\epsilon_{1}{\\bf1}\\quad\\epsilon_{2}{\\bf1}\\quad\\cdot\\cdot\\cdot\\epsilon_{K}{\\bf1}\\right],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where 1 denotes $n$ dimensional vectors and $\\epsilon_{1},\\dots,\\epsilon_{K}$ are constants. Then the perturbed cost matrix $\\tilde{C}=C+E_{i}$ , then $\\pi^{*}$ is also an optimal transport map with respect to the cost matrix $\\tilde{C}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. By the optimality condition, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{i,j}\\pi_{i j}^{*}C_{i j}\\leq\\sum_{i,j}\\pi_{i j}C_{i j}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for any $\\pi\\in\\Pi(\\mu,\\nu)$ . Then, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{i,j}\\pi_{i j}^{*}C_{i j}+\\sum_{j=1}^{K}\\nu_{j}\\epsilon_{j}\\leq\\sum_{i,j}\\pi_{i j}C_{i j}+\\sum_{j=1}^{K}\\nu_{j}\\epsilon_{j},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{i,j}\\pi_{i j}^{*}C_{i j}+\\sum_{j=1}^{K}\\sum_{i=1}^{n}\\pi_{i j}^{*}\\epsilon_{j}\\leq\\sum_{i,j}\\pi_{i j}C_{i j}+\\sum_{j=1}^{K}\\sum_{i=1}^{n}\\pi_{i j}\\epsilon_{j}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{i,j}\\pi_{i j}^{*}\\tilde{C}_{i j}\\leq\\sum_{i,j}\\pi_{i j}\\tilde{C}_{i j}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This theorem is also valid for row-wise perturbations as well with a similar proof. Consequently, a straightforward implication is that ", "page_idx": 18}, {"type": "text", "text": "Corollary D.2. Suppose $\\pi^{*}=\\arg\\operatorname*{min}_{\\gamma\\in\\Pi(\\mu,\\nu)}\\left\\langle\\gamma,C\\right\\rangle$ , $E$ is a columnwise perturbation and $F$ is $a$ row-wise perturbation, such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{E=[\\epsilon_{1}\\mathbf{1}\\,\\,\\,\\,\\,\\epsilon_{2}\\mathbf{1}\\,\\,\\,\\,\\cdot\\,\\cdot\\,\\epsilon_{K}\\mathbf{1}]\\,,\\ }\\\\ {F=\\left[\\begin{array}{l}{\\eta_{1}\\mathbf{1}^{T}}\\\\ {\\eta_{2}\\mathbf{1}^{T}}\\\\ {\\cdot\\,\\cdot\\,}\\\\ {\\eta_{K}\\mathbf{1}^{T}}\\end{array}\\right],\\ \\ \\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where 1 denotes n dimensional vectors with $I s$ , and $\\epsilon_{1},\\ldots,\\epsilon_{K},\\eta_{1},\\ldots,\\eta_{K}$ are constants. Suppose the perturbed cost matrix is defined as $\\tilde{C}=C+E+F$ , then $\\pi^{*}$ is also an optimal transport map with respect to the perturbed cost matrix $\\tilde{C}$ . ", "page_idx": 18}, {"type": "text", "text": "Now we provide the proof of Thoerem 4.2. ", "page_idx": 18}, {"type": "text", "text": "Proof. Given ", "page_idx": 18}, {"type": "text", "text": "$C_{i j}=-\\log P_{\\theta}(Y=j|X=x_{i})=-\\log P_{s}(Y=j|X=x_{i}),$ the posteriors in the target distribution can be defined as $C_{i j}^{*}=-\\log P_{t}(Y=j|X=x_{i})$ . From ", "page_idx": 18}, {"type": "equation", "text": "$$\nP_{t}(Y=j|X=x_{i})=P_{s}(Y=j|X=x_{i})\\frac{P_{s}(X=x_{i})P_{t}(Y=j)}{P_{t}(X=x_{i})P_{s}(Y=j)},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "we can see that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{i j}^{*}=-\\log P_{t}(Y=j|X=x_{i})}\\\\ &{\\qquad=-\\log P_{s}(Y=j|X=x_{i})\\frac{P_{s}(X=x_{i})P_{t}(Y=j)}{P_{t}(X=x_{i})P_{s}(Y=j)}}\\\\ &{\\qquad=-\\log P_{s}(Y=j|X=x_{i})+\\log P_{s}(Y=j)}\\\\ &{\\qquad-\\log P_{t}(Y=j)-\\log P_{s}(X=x_{i})+\\log P_{t}(X=x_{i})}\\\\ &{\\qquad=C_{i j}+E_{\\cdot j}+F_{i\\cdot}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $E_{\\cdot j}=\\log P_{s}(Y=j)-\\log P_{t}(Y=j)$ , $F_{i\\cdot}=-\\log P_{s}(X=x_{i})+\\log P_{t}(X=x_{i})$ . And, assuming $\\begin{array}{r}{\\nu_{j}=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{1}[\\hat{y}_{i}^{*}=j]}\\end{array}$ , where $\\hat{y}^{*}$ is the Bayes classifier prediction such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{y}_{i}^{*}=\\arg\\underset{j\\in[K]}{\\operatorname*{max}}\\,P_{t}(Y=j|X=x_{i})}\\\\ &{\\quad=\\arg\\underset{j\\in[K]}{\\operatorname*{min}}-\\log P_{t}(Y=j|X=x_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, optimal transport solution ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\pi^{*}=\\arg\\operatorname*{min}_{\\gamma\\in\\Pi(\\mu,\\nu)}\\left\\langle\\gamma,C^{*}\\right\\rangle\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "leads to Bayes classifier predictions by Theorem 4.1. ", "page_idx": 18}, {"type": "text", "text": "Finally, by Corollary D.2, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\pi^{*}=\\arg\\operatorname*{min}_{\\gamma\\in\\Pi(\\mu,\\nu)}\\left\\langle\\gamma,C^{*}\\right\\rangle=\\arg\\operatorname*{min}_{\\gamma\\in\\Pi(\\mu,\\nu)}\\left\\langle\\gamma,C\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "D.3 Proof of Theorem 4.3 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The proof of Theorem 4.3 relies on the following result of [52]. ", "page_idx": 19}, {"type": "text", "text": "Lemma D.3 ([52], Corollary 3.1.). Let the primal linear programming problem be ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{z}\\{p^{T}z|G z\\geqslant g,z\\geqslant0\\}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and its dual be ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{w}\\{w^{T}g|w^{T}G\\leqslant p^{T},w\\geqslant0\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let $\\bar{z},\\bar{w}$ be the primal, dual solution. And, let the perturbed primal linear programming problem be ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{z}\\{\\hat{p}^{T}z|\\hat{G}z\\geqslant\\hat{g},z\\geqslant0\\}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and its dual be ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{w}\\{w^{T}\\hat{g}|w^{T}\\hat{G}\\leqslant\\hat{p}^{T},w\\geqslant0\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let $\\hat{z},\\hat{w}$ be the corresponding primal, dual solution. ", "page_idx": 19}, {"type": "text", "text": "Suppose that the primal and dual problems are solvable. Then, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|\\left(\\frac{\\bar{z}}{\\bar{w}}\\right)-\\left(\\hat{\\hat{z}}\\right)\\right\\|_{2}\\leq\\kappa\\left\\|\\left[\\left(G-\\hat{G}\\right)\\hat{z}-(g-\\hat{g})\\right]^{-}\\right\\|_{p},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $1\\leq p\\leq\\infty$ and $\\kappa$ is the Hoffmann constant determined by $p,G,g.\\ [30].$ ", "page_idx": 19}, {"type": "text", "text": "This Lemma provides a bound for error in the solution of the perturbed linear program. Since discrete optimal transport can be translated to standard linear program, we obtain Theorem 4.3 by plugging in the definitions. ", "page_idx": 19}, {"type": "text", "text": "Proof of Theorem 4.3 A discrete optimal transport problem ", "text_level": 1, "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\left\\{\\sum_{i,j}C_{i,j}\\pi_{i,j}|\\pi\\mathbf{1}=\\mu,\\pi^{T}\\mathbf{1}=\\nu,\\pi_{i j}\\ge0\\right\\}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "can be written as a linear program ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\{p^{T}z|G z\\geqslant g,z\\geqslant0\\},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $p=v e c(C),z=v e c(\\pi),H=\\left[\\mathbf{1}_{n}^{T}\\otimes\\mathbf{1}_{K}^{T}\\right],G=\\left[\\!\\!\\begin{array}{l}{H}\\\\ {-H}\\end{array}\\!\\!\\right],g=\\left[\\!\\!\\begin{array}{l}{\\mu}\\\\ {\\nu}\\\\ {-\\mu}\\\\ {-\\nu}\\end{array}\\!\\!\\right].$ Note that the equality constraints are converted to stacked inequalities. We have noisy cost matrix and label distribution in our setting, which leads to the perturbation on cost matrix $C$ and $\\nu$ such that the perturbed cost matrix is $\\hat{C}=C+\\Delta_{C}$ , the perturbed label distribution $\\hat{\\nu}=\\nu+\\Delta_{\\nu}$ , such that ${\\hat{g}}=g+\\Delta_{g}$ where $\\Delta_{g}=\\left[\\begin{array}{c}{{0}}\\\\ {{\\hat{\\nu}-\\nu}}\\\\ {{0}}\\\\ {{-\\hat{\\nu}+\\nu}}\\end{array}\\right].$ Since we don\u2019t have perturbation on the constraint matrix $G,{\\hat{G}}=G$ . By plugging in these terms to Lemma D.3. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\bigg\\|\\bigg(\\frac{\\bar{x}}{\\bar{w}}\\bigg)-\\bigg(\\frac{\\bar{x}}{\\bar{w}}\\bigg)\\bigg\\|_{2}\\le\\kappa\\bigg\\|\\frac{[(G-\\bar{G})^{\\bar{z}}-(g-\\bar{g})]-1}{[(G-\\bar{G})^{\\bar{z}}\\bar{w}-(p-\\bar{g})]_{+}}\\bigg\\|_{2}}&{}\\\\ {=\\kappa\\bigg\\|\\frac{[(g-\\bar{g})^{\\bar{z}}\\bar{z}-(g-\\bar{g})]^{\\bar{z}}}{[(p-\\bar{p})^{\\bar{z}}\\bar{z}-(g-\\bar{g})]^{\\bar{w}}}\\bigg\\|_{2}}&{}\\\\ {=\\kappa\\bigg\\|\\frac{[(g-\\bar{g})]-\\bar{x}}{[(p-\\bar{p})^{\\bar{z}}\\bar{z}-(g-\\bar{g})]^{\\bar{w}}}\\bigg\\|_{2}}&{}\\\\ {=\\kappa\\bigg\\|\\frac{[\\bar{x}-\\bar{v}]-\\bigg\\|}{[(p-\\bar{v})^{\\bar{z}}]-\\bigg\\|}}&{\\qquad\\mathrm{Optimality}\\,,}\\\\ {=\\kappa\\bigg\\|\\frac{[\\bar{x}-\\bar{p}]-\\bigg\\|}{[\\bar{y}^{2}-\\bar{g}]^{\\bar{z}}}\\bigg\\|_{2}}&{}\\\\ {=\\kappa\\bigg\\|\\frac{[\\bar{x}-\\bar{p}]}{[(p-\\bar{x})^{\\bar{z}}]}\\bigg\\|_{2}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|{\\binom{\\bar{z}}{\\bar{w}}}-{\\binom{\\hat{z}}{\\hat{w}}}\\right\\|_{2}^{2}\\leq\\kappa^{2}\\left\\|{\\binom{[\\hat{\\nu}-\\nu]}{[p-\\hat{p}]_{+}}}\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\bar{z}-\\hat{z}\\|_{2}^{2}\\leq\\kappa^{2}\\left\\|\\left[\\stackrel{\\left[\\hat{\\nu}-\\nu\\right]}{p-\\hat{p}}\\right]_{+}\\right\\|^{2}-\\|\\bar{w}-\\hat{w}\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Plugging in $\\bar{z}=v e c(\\pi),\\hat{z}=v e c(\\hat{\\pi}),\\Delta_{\\nu}=\\hat{\\nu}-\\nu,\\Delta_{C}=\\hat{C}-C$ , and rearranging, we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\pi-\\hat{\\pi}\\|_{F}^{2}\\leq\\kappa^{2}\\left(\\|\\Delta_{\\nu}\\|_{2}^{2}+\\|[\\mathrm{vec}(\\Delta_{C})]_{+}\\|_{2}^{2}+\\|\\mathrm{vec}(C)^{T}\\mathrm{vec}(\\hat{\\pi})-g^{T}\\hat{w}\\|_{2}^{2}\\right)-\\|w-\\hat{w}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We use the definition of the Hoffman constant $\\kappa$ from [52]. Computing Hoffman constant or even bounding it has been a long-standing problem [5; 45; 46]. However, it has been shown that the Hoffman constant is a finite real number [52], and specifically under our problem setup, it is independent from perturbations and only related to original optimization problem. This suggests the possibility to regularize the parameters $C,G,g$ in the original problem such that $\\kappa$ does not depend on the dimensionality of cost matrix or target distribution. We leave further exploration of $\\kappa$ in this context as future work. ", "page_idx": 20}, {"type": "text", "text": "D.4 Bounding label distribution estimation errors in few-shot learning ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In few-shot learning, we assume that a few labeled samples per class are given. They can be used for estimating label distribution in the target distribution using label shift estimation methods [35; 6; 24]. They comes with the sample complexity analysis under the label shift assumptions, which can be used to obtain bound the label distribution estimation errors. ", "page_idx": 20}, {"type": "text", "text": "Lemma D.4. Let m and n denote the number of few-shot learning data and test datapoints, $w_{i}=$ $\\nu_{i}/\\nu_{i}^{s}$ and $\\hat{w}_{i}=\\hat{\\nu}_{i}/\\nu_{i}^{s}$ . Also let $\\sigma_{\\mathrm{min}}$ be the smallest eigenvalue of the covariance matrix $V_{\\hat{y},y}$ where $[V_{\\hat{y},y}]_{i,j}=P_{s}(f(x)=i,y=j)$ . For $m>80\\log(m)\\sigma_{\\mathrm{min}}^{-2}$ and constant $c>0$ , the perturbation $\\Delta_{\\nu}$ may be bounded as ", "page_idx": 20}, {"type": "equation", "text": "$$\n||\\Delta_{\\nu}||^{2}\\leq||\\nu^{s}||^{2}\\frac{c}{\\sigma_{\\operatorname*{min}}^{2}}\\left(||w||^{2}\\frac{\\log m}{m}+K\\frac{\\log n}{n}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with probability at least $1-3K m^{-10}-2K n^{-10}$ . ", "page_idx": 20}, {"type": "text", "text": "The proof of Lemma D.4 relies on the following result of [34]. ", "page_idx": 20}, {"type": "text", "text": "Lemma D.5. Assume that ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. $\\forall x\\in\\mathcal{X}$ and $\\forall y\\in\\mathcal{y},\\,P_{s}(x|y)=P_{t}(x|y),$   \n2. if $P_{t}(y)>0$ then $P_{s}(y)>0\\,\\forall y\\in\\mathcal{y}$ , and ", "page_idx": 20}, {"type": "text", "text": "3. the expected confusion matrix $C_{s}(f)=P_{s}(f(x),y)\\in\\mathbb{R}^{K\\times K}$ for classifier $f$ is invertible. ", "page_idx": 21}, {"type": "text", "text": "Then, there exists a constant $c>0$ such that for all $m>80\\log(m)\\sigma_{\\mathrm{min}}^{-2}$ , with probability at least $1-3K m^{-10}-2K n^{-10}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n||\\hat{w}-w||^{2}\\leq\\frac{c}{\\sigma_{\\operatorname*{min}}^{2}}\\left(||w||^{2}\\frac{\\log m}{m}+K\\frac{\\log n}{n}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof of Lemma $D.4$ . Where all norms are Euclidean unless otherwise denoted, we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lvert|\\Delta_{\\nu}\\rvert|^{2}=\\lvert|\\hat{\\nu}-\\nu\\rvert|^{2}}&{}\\\\ {=\\lvert|\\nu^{s}\\otimes(\\hat{w}-w)\\rvert|^{2}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for element-wise multiplication operation $\\otimes$ . Further, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|\\nu^{s}\\otimes(\\hat{w}-w)\\|^{2}\\leq\\|\\nu^{s}(\\hat{w}-w)\\|_{F}^{2}}}\\\\ &{\\leq\\|\\nu^{s}\\|_{2}^{2}||\\hat{w}-w||^{2}}\\\\ &{\\leq\\|\\nu^{s}\\|^{2}\\frac{c}{\\sigma_{\\operatorname*{min}}^{2}}\\left(\\|w\\|^{2}\\frac{\\log m}{m}+K\\frac{\\log n}{n}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last line follows from Lemma D.5 with probability at least $1-3K m^{-10}-2K n^{-10}$ . ", "page_idx": 21}, {"type": "text", "text": "D.5 Bounding the perturbation on cost matrix ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Further, we can bound $[v e c(\\Delta_{C})]_{+}$ by the Total Variation Distance (TVD) between $P_{s}$ and $P_{\\theta}$ as follows. ", "page_idx": 21}, {"type": "text", "text": "Lemma D.6. Let $\\begin{array}{r}{\\tau=\\frac{1}{2}||P_{s}-P_{\\theta}||_{1}}\\end{array}$ denote the Total Variation Distance between $P_{s}$ and $P_{\\theta}$ and define $\\operatorname*{min}(C)=\\operatorname*{min}_{i,j}\\tilde{C}_{i j}$ . Then, ", "page_idx": 21}, {"type": "equation", "text": "$$\n||[v e c(\\Delta_{C})]_{+}||\\leq\\sqrt{m(K-1)}\\log\\left(\\frac{\\tau}{\\operatorname*{min}(C)}+1\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof of Lemma D.6. For each element $\\Delta_{C}^{(i j)}$ of $\\Delta_{C}$ , we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{C}^{(i j)}=\\log P_{\\theta}(Y=j|X=x_{i})-\\log P_{s}(Y=j|X=x_{i})}\\\\ &{\\qquad=\\log\\frac{P_{\\theta}(Y=j|X=x_{i})}{P_{s}(Y=j|X=x_{i})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For $i,j$ such that $P_{\\theta}(Y\\ =\\ j|X\\ =\\ x_{i})\\ \\leq\\ P_{s}(Y\\ =\\ j|X\\ =\\ x_{i})$ , clearly $\\Delta_{C}^{(i j)}\\ \\leq\\ 0$ and so $[v e c(\\Delta_{C}^{(i j)})]_{+}=0$ . ", "page_idx": 21}, {"type": "text", "text": "Otherwise, for $i,j$ such that $P_{\\theta}(Y=j|X=x_{i})>P_{s}(Y=j|X=x_{i})$ , it follows that $\\Delta_{C}^{(i j)}>0$ and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{C}^{(i j)}=\\log\\frac{P_{\\theta}\\left(Y=j\\vert X=x_{i}\\right)-P_{s}\\left(Y=j\\vert X=x_{i}\\right)+P_{s}\\left(Y=j\\vert X=x_{i}\\right)}{P_{s}\\left(Y=j\\vert X=x_{i}\\right)}}\\\\ &{\\qquad\\le\\log\\left(\\frac{\\tau}{P_{s}\\left(Y=j\\vert X=x_{i}\\right)}+1\\right)}\\\\ &{\\qquad\\le\\log\\left(\\frac{\\tau}{\\operatorname*{min}(C)}+1\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For each $i\\in[m]$ , there are at most $K-1$ possible $j$ such that $P_{\\theta}(Y=j|X=x_{i})>P_{s}(Y=$ $j|X=x_{i})$ , because $\\begin{array}{r}{\\sum_{j\\in[K]}P_{\\theta}(Y=j|X=x_{i})=\\sum_{j\\in K}P_{s}(Y=j|X=x_{i})}\\end{array}$ . Therefore, there are at most $m(K-1)$ pairs $(i,j)\\in[m]\\times[K]$ such that $\\begin{array}{r}{0<\\Delta_{C}^{(i j)}\\le\\log\\left(\\frac{\\tau}{\\operatorname*{min}(C)}+1\\right)}\\end{array}$ . Thus, ", "page_idx": 21}, {"type": "equation", "text": "$$\n||[v e c(\\Delta_{C})]_{+}||\\leq\\sqrt{m(K-1)}\\log\\left(\\frac{\\tau}{\\operatorname*{min}(C)}+1\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "D.6 Proof of Theorem 4.4 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We provide the proof of Theorem 4.4. ", "page_idx": 22}, {"type": "text", "text": "Proof. By assumption, we have $P_{\\theta}(Y|X)\\,=\\,P_{s}(Y|X)$ . By the result of Theorem 4.2, yOTTER samples are generated by arg $\\mathrm{max}_{j\\in[K]}\\,P_{t}(Y=j|X=x)$ . Suppose $r^{*}=P_{t}(Y)/P_{s}(Y)$ . Then, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{P_{\\mathrm{s}},\\lefteqn{(Y=j|X=x)}=\\frac{r_{0}!P_{0}\\left(Y=j\\right)X=x}{\\sum_{x=y}^{r}r_{0}!P_{0}(Y=j|X=x)}}\\\\ &{=\\frac{r_{0}!P_{0}\\left(Y=j\\right)\\left(X=x\\right)}{\\sum_{x=y}^{r}r_{0}!P_{0}(X=x)!}}\\\\ &{=\\frac{r_{0}!P_{0}\\left(Y=j\\right)\\left(X=x\\right)!}{\\sum_{x=y}^{r}r_{0}!}\\frac{(r_{0}!-r_{1})!(r_{0}!-r_{2})!}{\\sum_{x=x}^{\\theta}r_{0}!}}\\\\ &{=\\frac{r_{0}!P_{0}\\left(Y=j\\right)!}{\\sum_{x=y}^{r}r_{0}!}\\frac{(r_{0}!-r_{1})!(r_{0}!-r_{2})!}{\\sum_{x=x}^{\\theta}r_{0}!}}\\\\ &{=\\frac{r_{0}!(Y=j)!}{\\sum_{x=y}^{r}P_{0}(Y=j)!}\\mathrm{e}^{-z/\\infty}}\\\\ &{=\\frac{r_{0}!(Y=j)!}{\\sum_{x=y}^{r}P_{0}(Y=j)!(Y=x)!}}\\\\ &{=\\frac{r_{0}!(Y=j)!(X=x)!\\times(r=j)!}{\\sum_{x=y}^{r}P_{0}(Y=j)!(Y=x)!\\times(r=j)!}}\\\\ &{=\\frac{r_{0}!(Y=j)!(Y=x)!(x=x)!}{\\sum_{x=y}^{r}\\frac{(r_{0}!-r_{1})!(r_{0}!-r_{2})!}{\\sum_{x=y}^{r}P_{0}(Y=x)!(Y=x)!(X=x)!}}}\\\\ &{=\\frac{r_{0}!(Y=x)!}{\\sum_{x=y}^{r}P_{0}(Y=j)!(Y=x)!}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y_{\\mathrm{\\tiny{R-OTTER}}}=\\arg\\underset{j}{\\operatorname*{max}}\\,P_{\\theta,r^{*}}(Y=j|X=x)}\\\\ &{\\qquad\\qquad=\\underset{j}{\\operatorname{arg\\,max}}\\,\\frac{P_{t}(Y=j|X=x)}{\\sum_{j^{\\prime}=1}^{K}P_{t}(Y=j^{\\prime}|X=x)}}\\\\ &{\\qquad\\quad=\\underset{j}{\\operatorname{arg\\,max}}\\,P_{t}(Y=j|X=x)}\\\\ &{\\qquad\\quad=y_{\\mathrm{\\tiny{OTTER}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which implies $r^{*}$ is an optimal parameter. ", "page_idx": 22}, {"type": "table", "img_path": "RsawwSBCs7/tmp/37fe9fdc90281a182899b0c447797e89ec1757fc7e9ff9423ac0f1b50b29b14c.jpg", "table_caption": [], "table_footnote": ["Table 7: Statistics of the test dataset in each task. Class imbalance is measured by $\\frac{\\operatorname*{max}_{j\\in[K]}P_{t}(Y=j)}{\\operatorname*{min}_{j\\in[K]}P_{t}(Y=j)}.$ "], "page_idx": 23}, {"type": "text", "text": "E Experiment details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "E.1 Datasets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Zeroshot image classification datasets We use CIFAR10, CIFAR100 [33], Caltech101 [22], Caltech256 [25], Food101 [8], STL10 [16], SUN397 [67], Flower102 [42], EuroSAT [27], OxfordIIIT Pet [44], Stanford Cars [32], DTD [14], CUB [61], ImageNet [18], ImageNet-r [29], and ImageNet-Sketch [63]. ", "page_idx": 23}, {"type": "text", "text": "Zeroshot text classification datasets We use Amazon [41], Gender [20], CivilComments [7], and HateXplain [39]. ", "page_idx": 23}, {"type": "table", "img_path": "RsawwSBCs7/tmp/965bea8939313353eb53713cc5241cc326ae2dd6efb149dd2b56a92119480b55.jpg", "table_caption": ["Table 8: CLIP Zero-shot image classification accuracy $(\\%)$ "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "E.2 Zero-shot classification setup ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Image zero-shot classification For zero-shot image classification, we emply CLIP [49] models. We used \u201ca photo of a [CLASS]\" prompt. Scores are computed by $s_{\\theta}(x_{i},j)=\\stackrel{*}{P_{\\theta}}(Y=j|X=x_{i})=$ $\\exp{(\\cos(f(x_{i}),g(y_{j}))/\\tau)}$   \n$\\begin{array}{r}{\\overline{{\\sum_{j^{\\prime}=1}^{K}\\exp\\left(\\cos(f(x_{i}),g(y_{j^{\\prime}}))/\\tau\\right)}}}\\end{array}$ for image $x_{i}$ regarding the label $j$ , given the image encoder $f$ , the text encoder $g$ . Cost matrix is constructed by $C=[C_{i j}]_{i\\in[n],j\\in[K]}$ , where $c_{i j}=-\\log s_{\\theta}(x_{i},j)$ . We run Algorithm 1 with the true class balance of the test dataset. ", "page_idx": 24}, {"type": "text", "text": "Text zero-shot classification We employ BERT and text-embedding-ada-002 sentence embeddings for text classification [50]. This process parallels the methodology used in image zero-shot classification \u2014 we compute the prediction scores from the cosine similarity and then construct the cost matrix with the negative log probabilities. ", "page_idx": 24}, {"type": "text", "text": "E.3 Detailed experiment results of Section 5.1 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Ablation on zero-shot models For the ablation study on zero-shot models, we provide experimental results with varying architectures, given the exact prior. Table 8, 9 show the image zero-shot classification results, and Table 10 shows the text zero-shot classification results. We also provide another baseline results with CLIPPR [31], which uses the label distribution for adapter training. CLIPPR is similar with Prior Matching in the point that it requires adapter training, but it has more adapter layers and additional loss function to make the predictions stick to the original prediction scores. While the performance gain varies, we can observe that OTTER is effective for the most cases. ", "page_idx": 24}, {"type": "table", "img_path": "RsawwSBCs7/tmp/1847add4ba7dee280797e3f69aaee4922bfc8a7d58345d52391d432c230b1d79.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "RsawwSBCs7/tmp/d98b077773e405f2203432710f063827542eb756f590be3cfb7cec5d2d8294a6.jpg", "table_caption": ["Table 9: CLIP Zero-shot image classification accuracy $(\\%)$ continued. ", "Table 10: Text embedding zero-shot classification mean accuracy and standard deviation $(\\%)$ "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Ablation on the class balance specification We conducted a semi-synthetic experiment to investigate the sensitivity to the label distribution specification error in real datasets. We generate the noisy label distribution specification and see how the accuracy changes. We control the noise in the label distribution specification as follows. Given the true class balance $\\nu^{*}$ , first we make adversarial class balance $\\bar{\\nu}^{\\bar{a}d v}$ such that $\\nu_{j^{*}}^{a d v}=1$ for $j^{*}=\\arg\\operatorname*{min}_{j\\in[K]}\\nu_{j}^{*}$ and $\\nu_{j}^{a d v}=0$ for $j\\neq j^{*}$ To measure distance between class balance specification and true class balance, we use the total variance $\\begin{array}{r}{T V(\\nu,\\hat{\\nu})=\\frac{1}{2}\\left|\\left|\\nu-\\hat{\\nu}\\right|\\right|_{1}}\\end{array}$ . Next, we intepolate $\\nu^{*}$ and $\\nu^{a d v}$ such that $T V(\\nu^{*},\\nu^{\\alpha})=\\alpha$ , by $\\begin{array}{r}{\\nu^{\\alpha}=(1-\\frac{\\alpha}{T V(\\nu^{*},\\nu^{a d v})})\\nu^{*}+\\frac{\\alpha}{T V(\\nu^{*},\\nu^{a d v})}\\nu^{a d v}}\\end{array}$ . We set the interval of alpha as 0.01 and vary it up to 0.2. ", "page_idx": 25}, {"type": "text", "text": "Figure 5 shows the result. We observe the sensitivity to the label distribution specification error varies depending on the datasets, but generally we can observe that the accuracy degrades linearly proportionally to the class balance error. While the result may vary depending on the interaction between class balance error and calibration error in cost matrix, we can expect performance improvement if the class balance specification is good enough. ", "page_idx": 25}, {"type": "image", "img_path": "RsawwSBCs7/tmp/9f0f0660d0bdad6ded60ac4ddb69a2f4768fc8712bdcb1c75c9985c6230f7514.jpg", "img_caption": ["Figure 5: Ablation experiment on the class balance specification. X-axis represents the total variation distance between the class specification true class balance $P_{t}(Y)$ and $\\bar{\\hat{P}}_{t}(Y)$ . Y-axis represents accuracy. ViT-B/16 is used as the image zero-shot classifier, and BERT is used as the text zero-shot classifier. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Inference time comparison To show that the additional computation complexity induced by OTTER is not heavy, we measured the time consumption (in seconds) for the inference step in the experiments in Section 5.1, with the pre-computed embeddings. Table 11 presents the result. Time reduction column represents the time reduction rate of OTTER compared to PM. Measurements were taken using a machine equipped with an Intel $\\textsuperscript{\\textregistered}$ Core\u2122i7-11700K $\\textcircled{a}~3.60\\mathrm{GHz}$ processor, 64GB RAM, and NVIDIA GPU RTX-4090. For most cases $\\langle n<30000\\rangle$ , our method takes less than 1 second, while the prior matching baseline takes more than 3 seconds. It\u2019s worth noting that the time consumption for computing embeddings is more substantial; even 10 seconds is negligible compared to the embedding time consumption (ranging from 5 to 30 minutes for each evaluation set), which is common for all inference conditions. ", "page_idx": 26}, {"type": "table", "img_path": "RsawwSBCs7/tmp/4ae59643e495dc099e4882d586bcb7f1871c2fe8ae129df441ec9320cf118d33.jpg", "table_caption": [], "table_footnote": ["Table 11: Inference time comparison with pre-computed embeddings (in seconds). "], "page_idx": 27}, {"type": "table", "img_path": "RsawwSBCs7/tmp/a17e7fd54692d1b5c31f394288b60e5f2a06fc9d3a7271aee5ff56eb2326a7e5.jpg", "table_caption": ["Table 12: Accuracy in the prompt-enhanced zero-shot classification by Classification by Description (CD) [40]. We can observe OTTER\u2019s capability to provide further enhancements upon refined the improvements achieved through refined prompts. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "Ablation on prompts Recent studies have demonstrated the efficacy of enhancing prompts as a means to improve zero-shot models [70; 40]. In order to further illustrate the potential enhancements offered by OTTER beyond enhanced prompts, we reproduced Menon and Vondrick [40]\u2019s approach (Classification by Description, CD), which employs multiple prompts generated by language models and takes max scores of them for each class. We applied OTTER to CD. The results of this experiment are summarized in Table 12. As anticipated, OTTER exhibits enhancements in zero-shot classification, even when prompt sets are refined using language models. ", "page_idx": 27}, {"type": "table", "img_path": "RsawwSBCs7/tmp/a3b95fa2fd6b7ae26579fec86c9b3e3c8400566537126a4246fe84b604ae3ebb.jpg", "table_caption": [], "table_footnote": ["Table 13: Class balance estimation error with BBSE in Section 5.3. We report the mean of 10 different random samplings of the validation set. Lower is better. "], "page_idx": 28}, {"type": "text", "text": "E.4 Detailed experiment results of Section 5.3 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Label distribution estimation errors We report the label distribution estimation errors in Section 5.3. As a metric, we use the total variance $\\begin{array}{r}{\\hat{T V}(\\nu,\\hat{\\nu})=\\frac{1}{2}\\left|\\left|\\nu-\\hat{\\nu}\\right|\\right|_{1}}\\end{array}$ . We use zeroshot prediction scores and linear probing prediction scores for BBSE. $\\hat{\\nu}^{z s}$ denotes the estimated class balance based on zero-shot prediction scores, and $\\hat{\\nu}^{l p}$ represents the estimated class balance based on linear probing prediction scores. ", "page_idx": 28}, {"type": "text", "text": "Table 13 shows the result. We can see that total variation decreases with linear probing in image classification tasks since they reduces the violation of label shift assumptions. However, total variation increases in text classification tasks due to the small number of labeled sample size, following the size of label space ( $K=2$ or 3). Accordingly, we can expect OTTER will be more useful with linear probing, and just rebalancing zero-shot predictions with OTTER could be enough for text classification tasks. ", "page_idx": 28}, {"type": "text", "text": "Ablation experiments on linear probing We provide full results of Section 5.3. Specifically, we additionally report the results of combination with linear probing in text classification tasks and the results of zero-shot classification results in image classification tasks. ", "page_idx": 28}, {"type": "text", "text": "The results are presented in Table 2. While OTTER often provides additional improvement over LP, zero-shot classification was a strong baseline in image classification tasks. Meanwhile, class balance adaptation in text classification tasks is effective in all cases, giving a significant improvement over zero-shot predictions. ", "page_idx": 28}, {"type": "image", "img_path": "RsawwSBCs7/tmp/1c3302171cf30eb0c462d4ca975425ba8078b3caefbcbb2e3bc6580fc52ede52.jpg", "img_caption": ["Figure 6: Ablation experiment on the number of samples. We report the mean of 10 different samplings in each setting. We use ViT-B/16 for image classification, and BERT for text classification. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "RsawwSBCs7/tmp/b67bf72432343d9f259f046010b0b06cdc43ab14345fbd4415d44e91a75d3e72.jpg", "img_caption": ["Figure 7: Comparison between OTTER, LP, and LP+OTTER with varying the number of samples. We report the mean of 10 different samplings in each setting. We use ViT-B/16 for image classification, and BERT for text classification. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Ablation experiments on the number of examples per class Few-shot adaptation scenario assumes we have access to labeled data to estimate the target distribution. We hypothesize that an increase in the number of labeled samples enhances the accuracy of the class balance estimation, thereby improving the performance of OTTER. To test this hypothesis, we use few-shot adaptation in image and text classification tasks, without linear probing. The experiment varies the number of samples per class from 10 to 100, anticipating a reduction in class balance estimation error and an improvement in OTTER\u2019s accuracy with the increase in labeled samples. ", "page_idx": 29}, {"type": "text", "text": "The results, as depicted in Figure 6, corroborate our hypothesis. It is evident that the error in class balance estimation diminishes with an increasing number of samples, leading to a subsequent enhancement in the accuracy of OTTER. ", "page_idx": 29}, {"type": "text", "text": "Comparison between OTTER and Linear Probing with varying number of classes In the few-shot adaptation scenario, we explored three approaches: OTTER, linear probing (LP), and a combination of LP $^+$ OTTER. We formulated two hypotheses. The first posits that OTTER might outperform LP, particularly in situations with a limited number of samples. The second hypothesis suggests that OTTER could provide further enhancements to LP even when LP already surpasses the naive version of OTTER. This experiment was conducted using the same setup as the previous one. ", "page_idx": 29}, {"type": "text", "text": "The results, displayed in Figure 7, reveal several insights regarding our hypotheses. To begin with, OTTER demonstrates performance on par with LP, especially in scenarios with fewer samples. Interestingly, OTTER achieves superior accuracy compared to LP in datasets like Amazon and CivilComments, characterized by a small number of classes ( $[K=2]$ ), resulting in a relatively low total sample count. Furthermore, it is observed that incorporating OTTER into LP leads to an average increase in accuracy. ", "page_idx": 29}, {"type": "text", "text": "E.5 Detailed experiment setup and results of Section 5.4 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Class hierarchy We used the following superclasses and subclasses classes for the proof of concept. ", "page_idx": 29}, {"type": "table", "img_path": "RsawwSBCs7/tmp/7c201672f6f81f58582fb297d58f11f5d642e14e82020bc648fa1203be9e105b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "RsawwSBCs7/tmp/c8e7f1cfb59913fce9dd23fb9f3513d0a188123f5a2f633aa8db9ff5fe0dfa4f.jpg", "img_caption": ["Table 14: Class balance estimation error in the Section 5.4 experiment. Class balance estimation error is measured by total variation distance. We report the mean of 10 different samplings of the validation set. H-BBSE denotes the class balance estimation using hierarchy upon BBSE. ", "Figure 8: Synthetic experiment result with R-OTTER. As expected, R-OTTER can successfully resolve the effects of label shift. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "\u2022 fish: aquarium fish, flatfish, ray, shark , trout \u2022 tree: maple tree, oak tree, palm tree, pine tree, willow tree ", "page_idx": 30}, {"type": "text", "text": "Class balance estimation error We report the class balance estimation error in Section 5.4. Table 14 shows the total variation between true class balance and estimated class balance. We can expect a significant accuracy improvement for RN50, RN101, and ViT-B/16 based on this table. ", "page_idx": 30}, {"type": "text", "text": "E.6 Synthetic experiments with R-OTTER ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We validate our theoretical result (Theorem 4.4) by testing R-OTTER in a fully controllable setting. ", "page_idx": 30}, {"type": "text", "text": "Setup and Procedure. We use our synthetic experiment setup (Section 5.2) with perturbation noise $\\delta=0$ and label distribution $\\alpha=0$ . Additionally, we generated a validation set that follows the same distribution as the test set. After learning the reweighting parameter $r$ in the validation set using $y_{o t t e r}$ as pseudolabels, we evaluated R-OTTER on the test set, comparing the results to those of zero-shot and OTTER. We expect that, if successful, R-OTTER will similarly gain improvement over zero-shot when the source and target distributions increasingly differ. ", "page_idx": 30}, {"type": "text", "text": "Results Figure 8 presents the experimental result. As expected, R-OTTER presents the experimental results. As expected, R-OTTER performs closely to the Bayes optimal solution, demonstrating its effectiveness, though it exhibits slight suboptimality due to generalization issues. ", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 31}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 31}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] . ", "page_idx": 31}, {"type": "text", "text": "\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available. \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 31}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 31}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 31}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 31}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 31}, {"type": "text", "text": "1. Claims ", "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We substantiate every claim made in the introduction in the experimental result. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper. ", "page_idx": 31}, {"type": "text", "text": "\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. ", "page_idx": 31}, {"type": "text", "text": "\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. ", "page_idx": 31}, {"type": "text", "text": "\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Yes. We discuss the potentials and limitations across several sections. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: Yes, we included the full set of assumptions and a complete (and correct) proof in Section 4.2. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We provided detailed exhaustive experimental hyperparameters and setup. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. ", "page_idx": 33}, {"type": "text", "text": "\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. ", "page_idx": 33}, {"type": "text", "text": "\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. ", "page_idx": 33}, {"type": "text", "text": "\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example ", "page_idx": 33}, {"type": "text", "text": "(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: We submit a zip folder of our code, and will release github link upon publication. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Yes, we provide all hyperparameters in the Appendix. We provide all data split and details in the main experimental result section ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: Yes, we provide error bars as much as possible, unless repeating an experiment is expensive. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Provided in the appendix. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: We adhere to the NeurIPS code of ethics ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: In our knowledge, there is no additional societal impact beyond that of LLM\u2019s. This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. As of now, we are not aware of additional potential societal impacts beyond the typical implications associated with zero-shot models. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 35}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for ", "page_idx": 35}, {"type": "text", "text": "disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 36}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: We do not release a new pre-trained model. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: We cite all dataset and model original owners. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets. ", "page_idx": 37}, {"type": "text", "text": "\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. ", "page_idx": 37}, {"type": "text", "text": "\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used. ", "page_idx": 37}, {"type": "text", "text": "\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 37}, {"type": "text", "text": "\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. ", "page_idx": 37}, {"type": "text", "text": "\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 37}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. ", "page_idx": 37}, {"type": "text", "text": "\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. ", "page_idx": 37}, {"type": "text", "text": "\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]