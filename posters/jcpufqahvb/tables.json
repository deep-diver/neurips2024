[{"figure_path": "jCPufQaHvb/tables/tables_6_1.jpg", "caption": "Table 1: Overview of MVD datasets used in this work.", "description": "This table presents a summary of the four datasets (fNIRS, HHAR, Sleep, SEEG) used in the paper's experiments.  For each dataset, it lists the sampling frequency of the data, the number of features, the number of classes, the number of subjects, the number of groups used in cross-validation, the number of cross-validation intervals, the total number of intervals, the interval length, the window length used for segmentation, the stride length, and the total number of segments.  This information is crucial for understanding the experimental setup and the characteristics of the data used to evaluate the proposed Con4m model and its baselines.", "section": "4.1 Experimental Setup"}, {"figure_path": "jCPufQaHvb/tables/tables_7_1.jpg", "caption": "Table 2: Comparison with baseline methods in the testing F\u2081 score (%) on three datasets. The best results are in bold and we underline the second best results. The worst results are denoted in italics.", "description": "This table compares the performance of the proposed Con4m model against several baseline models across three public datasets (fNIRS, HHAR, Sleep) and one private dataset (SEEG) with varying degrees of label noise (0%, 20%, 40%).  The F1-score, a common metric for evaluating classification performance, is used to assess the models' accuracy.  The best performing model for each dataset and noise level is highlighted in bold, the second-best is underlined, and the worst is italicized.", "section": "4.2 Label Disturbance Experiment"}, {"figure_path": "jCPufQaHvb/tables/tables_8_1.jpg", "caption": "Table 2: Comparison with baseline methods in the testing F\u2081 score (%) on three datasets. The best results are in bold and we underline the second best results. The worst results are denoted in italics.", "description": "This table compares the performance of the proposed Con4m model against several baseline methods across three public datasets (fNIRS, HHAR, Sleep) at different levels of label noise (0%, 20%, 40%). The F1 score, a common metric for evaluating classification performance, is used to assess each model's accuracy.  The best performing model for each dataset and noise level is highlighted in bold, while the second-best is underlined. The worst performing model is indicated in italics. This allows for a clear comparison of Con4m's performance against existing state-of-the-art models in the field of segmented time series classification.", "section": "4.2 Label Disturbance Experiment"}, {"figure_path": "jCPufQaHvb/tables/tables_20_1.jpg", "caption": "Table 2: Comparison with baseline methods in the testing F\u2081 score (%) on three datasets. The best results are in bold and we underline the second best results. The worst results are denoted in italics.", "description": "This table compares the performance of the proposed Con4m model against various baseline methods across three public datasets (fNIRS, HHAR, Sleep) and one private dataset (SEEG). The comparison is made using the F1 score, a metric that balances precision and recall.  The table shows the F1 scores for each method across different levels of label noise (0%, 20%, 40%) to assess the robustness of each method in handling noisy labels. The best performing method for each setting is highlighted in bold, while the second-best performing methods are underlined. The worst-performing method is shown in italics.", "section": "4.2 Label Disturbance Experiment"}, {"figure_path": "jCPufQaHvb/tables/tables_21_1.jpg", "caption": "Table 2: Comparison with baseline methods in the testing F\u2081 score (%) on three datasets. The best results are in bold and we underline the second best results. The worst results are denoted in italics.", "description": "This table compares the performance of the proposed Con4m model against various baseline models (including other TSC and noisy label learning models) on three public datasets (fNIRS, HHAR, Sleep) and one private dataset (SEEG). The performance is measured using the F1 score, a common metric for evaluating classification models.  The table shows the results for different noise levels (0%, 20%, 40%), and a 'raw' column indicates performance on the original, undisturbed data. The best and second-best results for each dataset and noise level are highlighted.", "section": "4.2 Label Disturbance Experiment"}, {"figure_path": "jCPufQaHvb/tables/tables_21_2.jpg", "caption": "Table 2: Comparison with baseline methods in the testing F\u2081 score (%) on three datasets. The best results are in bold and we underline the second best results. The worst results are denoted in italics.", "description": "This table compares the performance of the proposed Con4m model against various baseline methods across three datasets (fNIRS, HHAR, and Sleep).  The F1 score, a common metric for evaluating classification performance, is reported for each method under different conditions (0%, 20%, and 40% label disturbance).  The best performing method for each dataset and condition is highlighted in bold, the second best is underlined, and the worst performing is italicized. This allows for a direct comparison of Con4m's performance relative to existing state-of-the-art methods.", "section": "4.2 Label Disturbance Experiment"}, {"figure_path": "jCPufQaHvb/tables/tables_22_1.jpg", "caption": "Table 2: Comparison with baseline methods in the testing F\u2081 score (%) on three datasets. The best results are in bold and we underline the second best results. The worst results are denoted in italics.", "description": "This table compares the performance of the proposed Con4m model against various baseline methods across three datasets (fNIRS, HHAR, Sleep) at different levels of label noise (0%, 20%, 40%). The F1 score, a common metric for evaluating classification performance, is used to assess each model's accuracy. The best performing model for each dataset and noise level is highlighted in bold, the second-best is underlined, and the worst is italicized. This allows for a direct comparison of Con4m's performance relative to state-of-the-art models in handling inconsistent labels and varying data conditions.", "section": "4.2 Label Disturbance Experiment"}]