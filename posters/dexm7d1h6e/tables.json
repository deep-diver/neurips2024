[{"figure_path": "DexM7d1H6e/tables/tables_2_1.jpg", "caption": "Table 1: Comparison of existing video understanding benchmarks. In contrast to other benchmarks, Animal-Bench mitigates the limitations of prior video question-answering benchmarks that lack animal agents. The dataset is characterized by its richness and diversity, facilitating a comprehensive evaluation of models across multiple dimensions of performance.", "description": "This table compares Animal-Bench with other existing video understanding benchmarks, highlighting the differences in their properties such as the main agent, task types, and dataset size. It emphasizes Animal-Bench's focus on animals, richer diversity, and more comprehensive task evaluation.", "section": "2 Related Work"}, {"figure_path": "DexM7d1H6e/tables/tables_7_1.jpg", "caption": "Table 2: The evaluation results of 8 multimodal video models on our Animal-Bench (the first place for each task is marked in red, and the second place is marked in blue, and those below random accuracy are marked in gray).", "description": "This table presents the performance of eight existing multimodal video models on the Animal-Bench benchmark.  The benchmark consists of 13 tasks designed to evaluate different aspects of animal-centric video understanding. The table shows the accuracy of each model for each task, highlighting the top two performing models for each task.  Models significantly underperforming random accuracy are indicated in gray. ", "section": "4.2 Evaluation results"}, {"figure_path": "DexM7d1H6e/tables/tables_8_1.jpg", "caption": "Table 3: Sensitivity of multimodal video models to different variations(relative accuracy drop(%)).", "description": "This table presents the sensitivity analysis results of four top-performing multimodal video models to four different types of simulated real-world data variations: snowy weather, frosty weather, shooting distance variation, and shooting direction variation.  The results are expressed as the relative decrease in accuracy (%) for each model and each variation type. It provides insight into the robustness of these models against real-world challenges.", "section": "4.2 Evaluation results"}, {"figure_path": "DexM7d1H6e/tables/tables_15_1.jpg", "caption": "Table 1: Comparison of existing video understanding benchmarks. In contrast to other benchmarks, Animal-Bench mitigates the limitations of prior video question-answering benchmarks that lack animal agents. The dataset is characterized by its richness and diversity, facilitating a comprehensive evaluation of models across multiple dimensions of performance.", "description": "This table compares Animal-Bench with other existing video understanding benchmarks, highlighting Animal-Bench's unique focus on animal agents and its rich, diverse dataset.  It emphasizes Animal-Bench's more comprehensive evaluation across multiple performance dimensions compared to human-centric benchmarks which lack animal data and focus primarily on humans and objects.", "section": "2 Related Work"}, {"figure_path": "DexM7d1H6e/tables/tables_16_1.jpg", "caption": "Table 1: Comparison of existing video understanding benchmarks. In contrast to other benchmarks, Animal-Bench mitigates the limitations of prior video question-answering benchmarks that lack animal agents. The dataset is characterized by its richness and diversity, facilitating a comprehensive evaluation of models across multiple dimensions of performance.", "description": "This table compares Animal-Bench with other existing video understanding benchmarks, highlighting the unique characteristics of Animal-Bench.  It shows that unlike other benchmarks which primarily focus on human or object agents, Animal-Bench centers on animal agents, offering a richer and more diverse dataset for evaluating models' performance across various dimensions.", "section": "2 Related Work"}, {"figure_path": "DexM7d1H6e/tables/tables_17_1.jpg", "caption": "Table 1: Comparison of existing video understanding benchmarks. In contrast to other benchmarks, Animal-Bench mitigates the limitations of prior video question-answering benchmarks that lack animal agents. The dataset is characterized by its richness and diversity, facilitating a comprehensive evaluation of models across multiple dimensions of performance.", "description": "This table compares Animal-Bench with other existing video understanding benchmarks, highlighting Animal-Bench's unique focus on animal agents and its richer, more diverse dataset.  It provides a quantitative comparison of the number of question-answer pairs and the types of agents primarily featured (human, animal, object) in each benchmark, showcasing Animal-Bench's comprehensive evaluation across various performance dimensions.", "section": "2 Related Work"}, {"figure_path": "DexM7d1H6e/tables/tables_18_1.jpg", "caption": "Table 2: The evaluation results of 8 multimodal video models on our Animal-Bench (the first place for each task is marked in red, and the second place is marked in blue, and those below random accuracy are marked in gray).", "description": "This table presents the performance of eight existing multimodal video models on the Animal-Bench benchmark across thirteen tasks.  The tasks are categorized into common tasks (shared with human-centric benchmarks) and special tasks (related to animal conservation).  The table shows the accuracy of each model for each task.  Models with higher accuracy are shown in red and blue to highlight top performers.  Gray indicates performance worse than random chance.", "section": "4.2 Evaluation results"}, {"figure_path": "DexM7d1H6e/tables/tables_18_2.jpg", "caption": "Table 1: Comparison of existing video understanding benchmarks. In contrast to other benchmarks, Animal-Bench mitigates the limitations of prior video question-answering benchmarks that lack animal agents. The dataset is characterized by its richness and diversity, facilitating a comprehensive evaluation of models across multiple dimensions of performance.", "description": "This table compares Animal-Bench with other existing video understanding benchmarks.  It highlights that Animal-Bench addresses limitations of previous benchmarks by focusing on animal agents instead of primarily humans and objects, improving the diversity and richness of the data to comprehensively evaluate model performance.", "section": "2 Related Work"}, {"figure_path": "DexM7d1H6e/tables/tables_19_1.jpg", "caption": "Table 1: Comparison of existing video understanding benchmarks. In contrast to other benchmarks, Animal-Bench mitigates the limitations of prior video question-answering benchmarks that lack animal agents. The dataset is characterized by its richness and diversity, facilitating a comprehensive evaluation of models across multiple dimensions of performance.", "description": "This table compares Animal-Bench with other existing video understanding benchmarks, highlighting the key differences and advantages of Animal-Bench. Animal-Bench focuses on animal agents, unlike others that predominantly feature humans or objects, offering a more comprehensive evaluation across various tasks and model capabilities.  The table showcases differences in the main agents included in the data, the number of questions and answers used for evaluation and highlights the inclusion of \"common\" and \"special\" tasks.", "section": "2 Related Work"}]