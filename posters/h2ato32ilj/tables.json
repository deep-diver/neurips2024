[{"figure_path": "H2ATO32ilj/tables/tables_3_1.jpg", "caption": "Table 1: Comparisons between concurrent works and ART.", "description": "This table compares ART with other methods for red-teaming text-to-image models.  It highlights key differences in whether the methods are model-agnostic, adaptable to various categories of harmful content, capable of generating safe prompts, able to continuously generate test cases, the diversity of the test cases generated, and how easily the methods can be expanded to incorporate new models or evaluation benchmarks.  ART is shown to be superior to previous methods.", "section": "2.2 Red-teaming for Text-to-image Models"}, {"figure_path": "H2ATO32ilj/tables/tables_4_1.jpg", "caption": "Table 2: The number of prompts in each category.", "description": "This table shows the number of prompts collected for each of the seven harmful categories in the meta-dataset MD.  The categories are hate, harassment, violence, self-harm, sexual, shocking, and illegal activity. The number of prompts for each category varies significantly, reflecting the different levels of prevalence and representation of each category on the websites used for data collection.", "section": "3.3 Datasets in ART"}, {"figure_path": "H2ATO32ilj/tables/tables_6_1.jpg", "caption": "Table 3: Prompt toxicity for Stable Diffusion 1.5. The abbreviations of the Judge Models can be found in Appendix B. Results for others are in Appendix I.", "description": "This table presents the results of evaluating prompt toxicity for the Stable Diffusion 1.5 model.  It shows the number of times prompts triggered each of the different Judge Models (toxicity detectors), categorized by the type of harmful content (hate, harassment, violence, etc.). The table also shows the number of safe prompts generated and the percentage of safe prompts, providing a measure of the model's ability to generate safe prompts when given various types of toxic prompts.  The abbreviations for the Judge Models are detailed in Appendix B, and results for other Stable Diffusion models are provided in Appendix I.", "section": "4.3 Baselines"}, {"figure_path": "H2ATO32ilj/tables/tables_7_1.jpg", "caption": "Table 3: Prompt toxicity for Stable Diffusion 1.5. The abbreviations of the Judge Models can be found in Appendix B. Results for others are in Appendix I.", "description": "This table presents the results of evaluating prompt toxicity for the Stable Diffusion 1.5 model.  It shows the number of times prompts triggered various toxicity detection models (TD, NSFW-P, TCD, LlamaGuard) and the resulting percentage of safe prompts. The data is broken down by toxic category (hate, harassment, violence, self-harm, sexual, shocking, illegal activity). Appendix B provides the meaning of the abbreviations of the Judge Models, and Appendix I contains additional data for other models.", "section": "4.3 Baselines"}, {"figure_path": "H2ATO32ilj/tables/tables_13_1.jpg", "caption": "Table 2: The number of prompts in each category.", "description": "This table shows the number of prompts collected for each of the seven harmful categories used in the ART dataset.  These prompts, though safe in appearance, are designed to test the robustness of text-to-image models by potentially triggering the generation of unsafe images.", "section": "3.3 Datasets in ART"}, {"figure_path": "H2ATO32ilj/tables/tables_13_2.jpg", "caption": "Table 6: Abbreviations for the Judge Models in ART.", "description": "This table lists the abbreviations used in the paper for various detection models that form part of the Judge Models in the ART framework.  These models assess the safety of prompts and generated images during the red-teaming process.  The abbreviations are used for brevity in other tables and figures, making it easier to understand the results presented.", "section": "3.3 Datasets in ART"}, {"figure_path": "H2ATO32ilj/tables/tables_17_1.jpg", "caption": "Table 7: Hyperparameters used in fine-tuning LLaVA-1.6-Mistral-7B.", "description": "This table lists the hyperparameters used during the fine-tuning process of the LLaVA-1.6-Mistral-7B model.  The hyperparameters cover aspects such as LORA rank and alpha, learning rate (for both the model and the mm projector), floating point precision, number of training epochs, batch size, weight decay, warmup ratio, learning rate scheduler, maximum model length, and image aspect ratio. These settings are crucial for achieving optimal performance during the fine-tuning phase.", "section": "F Fine-tuning Details"}, {"figure_path": "H2ATO32ilj/tables/tables_17_2.jpg", "caption": "Table 7: Hyperparameters used in fine-tuning LLaVA-1.6-Mistral-7B.", "description": "This table lists the hyperparameters used for fine-tuning the LLaVA-1.6-Mistral-7B model.  It includes values for LORA Rank, LORA \u03b1, learning rate, float type, number of epochs, batch size, weight decay, and the learning rate scheduler used.", "section": "F Fine-tuning Details"}, {"figure_path": "H2ATO32ilj/tables/tables_18_1.jpg", "caption": "Table 3: Prompt toxicity for Stable Diffusion 1.5. The abbreviations of the Judge Models can be found in Appendix B. Results for others are in Appendix I.", "description": "This table presents the results of evaluating prompt toxicity for Stable Diffusion 1.5 model using different methods (Naive, Curiosity, Groot, and ART).  It shows the number of times different toxic content detectors (TD, NSFW-P, TCD, LlamaGuard) flagged prompts as unsafe, and the overall success rate of generating unsafe prompts for each method across various categories of toxicity (hate, harassment, violence, self-harm, sexual, shocking, illegal activity).  Appendix B provides a key to the abbreviations used for the Judge Models, while Appendix I provides the complete data for the other models (Stable Diffusion 2.1 and XL).", "section": "4.3 Baselines"}, {"figure_path": "H2ATO32ilj/tables/tables_18_2.jpg", "caption": "Table 3: Prompt toxicity for Stable Diffusion 1.5. The abbreviations of the Judge Models can be found in Appendix B. Results for others are in Appendix I.", "description": "This table presents the results of evaluating prompt toxicity for the Stable Diffusion 1.5 model.  It shows the number of times different toxicity detectors (TD, NSFW-P, TCD, LlamaGuard) were triggered for various prompt categories (hate, harassment, violence, self-harm, sexual, shocking, illegal activity). The table also indicates the number of safe prompts and the percentage of safe prompts out of the total number of prompts evaluated. Appendix B provides the abbreviations for the judge models used, while Appendix I shows results for other Stable Diffusion versions.", "section": "4.3 Baselines"}, {"figure_path": "H2ATO32ilj/tables/tables_18_3.jpg", "caption": "Table 1: Comparisons between concurrent works and ART.", "description": "This table compares the proposed ART framework with four other existing red-teaming methods for text-to-image models.  The comparison focuses on several key aspects including whether the method is model-agnostic, its ability to generate safe prompts, whether it provides continuous generation of prompts, the diversity of the prompts it creates, and its expandability to adapt to new models and datasets. The table highlights the advantages of ART over existing methods by showing that ART is model-agnostic, generates safe prompts, allows continuous generation, produces diverse prompts, and is highly expandable.", "section": "2.2 Red-teaming for Text-to-image Models"}, {"figure_path": "H2ATO32ilj/tables/tables_18_4.jpg", "caption": "Table 3: Prompt toxicity for Stable Diffusion 1.5. The abbreviations of the Judge Models can be found in Appendix B. Results for others are in Appendix I.", "description": "This table shows the results of evaluating prompt toxicity for the Stable Diffusion 1.5 model.  It lists the number of times prompts triggered various safety detectors (toxicity detector, not-safe-for-work detector, toxic comment detector, Meta-LlamaGuard), broken down by categories of harmful content (hate, harassment, violence, self-harm, sexual, shocking, illegal activity).  The table shows the percentage of safe prompts generated for each category.  The results from other Stable Diffusion models are given in Appendix I, and Appendix B provides a key for the detector abbreviations.", "section": "4.3 Baselines"}, {"figure_path": "H2ATO32ilj/tables/tables_19_1.jpg", "caption": "Table 3: Prompt toxicity for Stable Diffusion 1.5. The abbreviations of the Judge Models can be found in Appendix B. Results for others are in Appendix I.", "description": "This table presents the results of evaluating prompt toxicity for the Stable Diffusion 1.5 model.  It shows the number of times prompts triggered each of the judge models (toxicity detector, not-safe-for-work detector, toxic comment detector, and LlamaGuard) across different harmful categories (hate, harassment, violence, self-harm, sexual, shocking, illegal activity). The percentage of safe prompts is calculated based on the total number of prompts (255). Appendix B provides the full names for the abbreviated judge models, and Appendix I contains results for the other Stable Diffusion models.", "section": "4.3 Baselines"}, {"figure_path": "H2ATO32ilj/tables/tables_20_1.jpg", "caption": "Table 3: Prompt toxicity for Stable Diffusion 1.5. The abbreviations of the Judge Models can be found in Appendix B. Results for others are in Appendix I.", "description": "This table presents the results of evaluating prompt toxicity for the Stable Diffusion 1.5 model.  It shows the number of times prompts triggered different toxicity detectors (TD, NSFW-P, TCD, LlamaGuard), the number of prompts deemed safe, and the percentage of safe prompts generated for various toxic categories (hate, harassment, violence, self-harm, sexual, shocking, illegal activity).  Appendix B provides the full names for the abbreviated judge models, and Appendix I contains similar data for other Stable Diffusion models.", "section": "4.3 Baselines"}, {"figure_path": "H2ATO32ilj/tables/tables_22_1.jpg", "caption": "Table 3: Prompt toxicity for Stable Diffusion 1.5. The abbreviations of the Judge Models can be found in Appendix B. Results for others are in Appendix I.", "description": "This table presents the results of evaluating prompt toxicity for the Stable Diffusion 1.5 model.  It shows the number of times prompts triggered various toxicity detectors (TD, NSFW-P, TCD, LlamaGuard) across different harmful categories (hate, harassment, violence, self-harm, sexual, shocking, illegal activity). The percentage of safe prompts is also included.  Appendix B provides the full names of the abbreviated Judge Models, and Appendix I contains the results for other Stable Diffusion models.", "section": "4.3 Baselines"}, {"figure_path": "H2ATO32ilj/tables/tables_23_1.jpg", "caption": "Table 13: Prompt toxicity for FLUX. The abbreviations of the Judge Models can be found in Appendix B.", "description": "This table compares the performance of Adversarial Nibbler and ART on the FLUX model. It shows the number of times each category of unsafe content was triggered by the model, the number of safe prompts generated, and the percentage of safe prompts generated by each method for each category of unsafe content.  The results highlight the ability of ART to generate a higher percentage of safe prompts that still trigger unsafe images, indicating its effectiveness in uncovering model vulnerabilities.", "section": "4.4 Results"}, {"figure_path": "H2ATO32ilj/tables/tables_24_1.jpg", "caption": "Table 14: Image toxicity for FLUX. The abbreviations of the Judge Models can be found in Appendix B.", "description": "This table presents the results of evaluating the safety of the FLUX text-to-image model using the ART framework.  It shows the number of times various toxicity detection models triggered (indicating unsafe image generation) across different categories of harmful content (hate, harassment, violence, self-harm, sexual, shocking, illegal activity) for both Adversarial Nibbler and ART methods. The success ratio under safe prompts is calculated, representing the percentage of safe prompts that generated at least one unsafe image.  Appendix B provides details on the abbreviations used for the Judge Models.", "section": "4.4 Results"}]