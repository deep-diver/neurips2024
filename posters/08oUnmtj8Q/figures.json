[{"figure_path": "08oUnmtj8Q/figures/figures_2_1.jpg", "caption": "Figure 1: Diagram of our FSEO framework.", "description": "This figure illustrates the overall working mechanism of the Few-Shot Evolutionary Optimization (FSEO) framework.  It shows the two main procedures: experience learning from related tasks and the adaptation procedure for the target task. The experience learning procedure uses datasets from related tasks to compute task-independent parameters for the MDKL surrogate. The adaptation procedure then uses these parameters, along with data from the target task, to continually update the surrogate, improving sampling efficiency and optimization performance. The framework combines meta-learning with surrogate-assisted evolutionary algorithms (SAEAs).", "section": "4 Few-Shot Evolutionary Optimization (FSEO) Framework"}, {"figure_path": "08oUnmtj8Q/figures/figures_6_1.jpg", "caption": "Figure 3: IGD+ curves averaged over 30 runs on the DTLZ problems. Solid lines are mean values, while shadows are error regions. Upper: DTLZ1, DTLZ2, DTLZ3, DTLZ4. Lower: DTLZ5, DTLZ6, DTLZ7. MOEA/D-FSs and comparison algorithms initialize their surrogates with 10, 100 samples, respectively. X-axis denotes the extra 50 evaluations allowed in the further optimization. Note that \u2018FS(out)\u2019 indicates the target task is excluded from the range of related tasks during the meta-learning procedure.", "description": "This figure shows the IGD+ (Inverted Generational Distance Plus) values over 30 runs for different DTLZ (Deb, Thiele, Laumanns, and Zitzler) test problems.  The solid lines represent the mean IGD+ values, and the shaded areas represent the standard deviations.  The upper panels display results for DTLZ1-4, and the lower panels for DTLZ5-7.  Different algorithms are compared, with MOEA/D-FS (Few-Shot Evolutionary Optimization using MOEA/D) being the main focus.  Two versions of MOEA/D-FS are shown: one using experience from related tasks (in-range) and another excluding the target task from the related tasks (out-of-range).  The algorithms initialize their surrogates with either 10 or 100 samples.  The x-axis shows the number of additional evaluations (beyond initialization) conducted during the optimization process.", "section": "5.1 Performance on EMOPs"}, {"figure_path": "08oUnmtj8Q/figures/figures_8_1.jpg", "caption": "Figure 4: Results of 30 runs on the real-world engine calibration problem, all BSFC values are normalized. Solid lines are mean values, while shadows are error regions. Left figure shows how BSFC varies with the number of evaluations. The star markers highlight the results achieved when 20 evaluations are used in the optimization process. Right figure illustrates how the number of feasible solutions varies with the number of evaluations.", "description": "This figure shows the results of 30 runs of the real-world gasoline engine calibration problem.  The left plot displays the normalized brake-specific fuel consumption (BSFC) against the number of evaluations.  A star marker indicates the point where 20 evaluations were used for surrogate initialization in the cons_FS algorithm. The right plot shows the number of feasible solutions found as a function of the number of evaluations.", "section": "5.3 Computational Studies"}, {"figure_path": "08oUnmtj8Q/figures/figures_19_1.jpg", "caption": "Figure 3: IGD+ curves averaged over 30 runs on the DTLZ problems. Solid lines are mean values, while shadows are error regions. Upper: DTLZ1, DTLZ2, DTLZ3, DTLZ4. Lower: DTLZ5, DTLZ6, DTLZ7. MOEA/D-FSs and comparison algorithms initialize their surrogates with 10, 100 samples, respectively. X-axis denotes the extra 50 evaluations allowed in the further optimization. Note that \u2018FS(out)\u2019 indicates the target task is excluded from the range of related tasks during the meta-learning procedure.", "description": "The figure shows the IGD+ values (Inverted Generational Distance Plus, a metric for evaluating the performance of multi-objective optimization algorithms) over 30 independent runs for seven different DTLZ (Deb, Thiele, Laumanns, and Zitzler) test problems.  Two versions of MOEA/D-FS (MOEA/D with Few-Shot optimization) are compared, one where the target task is included in the training data and one where it's excluded.  The results are compared against several other multi-objective optimization algorithms.  The shaded area represents the standard deviation.  The x-axis represents the number of additional evaluations after the initial surrogate is trained.", "section": "5.1 Performance on EMOPs"}, {"figure_path": "08oUnmtj8Q/figures/figures_20_1.jpg", "caption": "Figure 3: IGD+ curves averaged over 30 runs on the DTLZ problems. Solid lines are mean values, while shadows are error regions. Upper: DTLZ1, DTLZ2, DTLZ3, DTLZ4. Lower: DTLZ5, DTLZ6, DTLZ7. MOEA/D-FSs and comparison algorithms initialize their surrogates with 10, 100 samples, respectively. Extra 50 evaluations are allowed in the further optimization. Note that \u2018FS(out)\u2019 indicates the target task is excluded from the range of related tasks during the meta-learning procedure). X-axis denotes the number of evaluations used after the surrogate initialization.", "description": "The figure shows the IGD+ values over 30 runs for 7 DTLZ problems, comparing MOEA/D-FS (with 10 and 100 initial samples) against other algorithms.  It visualizes the convergence speed and performance of the algorithms, highlighting the impact of surrogate initialization size and the use of related task experience (with and without the target task included).", "section": "5.1 Performance on EMOPs"}, {"figure_path": "08oUnmtj8Q/figures/figures_23_1.jpg", "caption": "Figure 3: IGD+ curves averaged over 30 runs on the DTLZ problems. Solid lines are mean values, while shadows are error regions. Upper: DTLZ1, DTLZ2, DTLZ3, DTLZ4. Lower: DTLZ5, DTLZ6, DTLZ7. MOEA/D-FSs and comparison algorithms initialize their surrogates with 10, 100 samples, respectively. X-axis denotes the extra 50 evaluations allowed in the further optimization. Note that \u2018FS(out)\u2019 indicates the target task is excluded from the range of related tasks during the meta-learning procedure).", "description": "This figure compares the performance of MOEA/D-FS with other multi-objective optimization algorithms on seven DTLZ problems.  The y-axis shows the inverted generational distance plus (IGD+) metric, a lower value indicating better performance. The x-axis represents the number of extra evaluations performed after surrogate initialization.  Two versions of MOEA/D-FS are shown: one using related tasks ('in-range') and another excluding the target task from the related tasks ('FS(out)'). The results illustrate MOEA/D-FS's effectiveness with fewer evaluations, particularly when leveraging experience from related tasks.", "section": "5.1 Performance on EMOPs"}, {"figure_path": "08oUnmtj8Q/figures/figures_24_1.jpg", "caption": "Figure 3: IGD+ curves averaged over 30 runs on the DTLZ problems. Solid lines are mean values, while shadows are error regions. Upper: DTLZ1, DTLZ2, DTLZ3, DTLZ4. Lower: DTLZ5, DTLZ6, DTLZ7. MOEA/D-FSs and comparison algorithms initialize their surrogates with 10, 100 samples, respectively. X-axis denotes the extra 50 evaluations allowed in the further optimization. Note that 'FS(out)' indicates the target task is excluded from the range of related tasks during the meta-learning procedure).", "description": "The figure shows the IGD+ values (Inverted Generational Distance Plus) over 30 independent runs for seven different DTLZ (Deb, Thiele, Laumanns, and Zitzler) test problems.  The solid lines represent the mean IGD+ values, and the shaded regions indicate the standard deviation.  The top row displays results for DTLZ1-4, and the bottom row shows results for DTLZ5-7. Two versions of MOEA/D-FS are included: one where the target task (the problem being optimized) is included in the training data for the meta-learning model, and one where it is excluded ('FS(out)').  The figure demonstrates how the MOEA/D-FS algorithm, which incorporates the meta-learning model, performs compared to other multi-objective evolutionary algorithms.  The x-axis represents the number of additional evaluations beyond the initial set used for surrogate initialization.  The results indicate how many additional evaluations are needed to reach a certain IGD+ value (lower is better).", "section": "5.1 Performance on EMOPs"}, {"figure_path": "08oUnmtj8Q/figures/figures_24_2.jpg", "caption": "Figure 3: IGD+ curves averaged over 30 runs on the DTLZ problems. Solid lines are mean values, while shadows are error regions. Upper: DTLZ1, DTLZ2, DTLZ3, DTLZ4. Lower: DTLZ5, DTLZ6, DTLZ7. MOEA/D-FSs and comparison algorithms initialize their surrogates with 10, 100 samples, respectively. X-axis denotes the extra 50 evaluations allowed in the further optimization. Note that \u2018FS(out)\u2019 indicates the target task is excluded from the range of related tasks during the meta-learning procedure.", "description": "The figure shows the IGD+ (Inverted Generational Distance Plus) curves for seven different DTLZ (Deb, Thiele, Laumanns, and Zitzler) test problems.  Two versions of the MOEA/D-FS algorithm are presented: one where the target task is included in the training data, and one where it is excluded ('FS(out)').  This allows for comparison of performance with several other multi-objective optimization algorithms.  The plot shows the mean IGD+ over 30 runs, with shaded regions representing error, illustrating the algorithms' convergence behavior across various problems.", "section": "5.1 Performance on EMOPs"}, {"figure_path": "08oUnmtj8Q/figures/figures_25_1.jpg", "caption": "Figure 3: IGD+ curves averaged over 30 runs on the DTLZ problems. Solid lines are mean values, while shadows are error regions. Upper: DTLZ1, DTLZ2, DTLZ3, DTLZ4. Lower: DTLZ5, DTLZ6, DTLZ7. MOEA/D-FSs and comparison algorithms initialize their surrogates with 10, 100 samples, respectively. Extra 50 evaluations are allowed in the further optimization. Note that \u2018FS(out)\u2019 indicates the target task is excluded from the range of related tasks during the meta-learning procedure). X-axis denotes the number of evaluations used after the surrogate initialization.", "description": "The figure shows the inverted generational distance plus (IGD+) values over 30 runs for seven different DTLZ problems (DTLZ1-DTLZ7).  Two versions of MOEA/D-FS are compared: one where the target task is included in the training data ('MOEA/D-FS'), and another where it's excluded ('MOEA/D-FS(out)').  The results are compared against several other multi-objective optimization algorithms.  The plot illustrates the convergence speed and final IGD+ values, showing how using the proposed FSEO framework (MOEA/D-FS) improves performance compared to other algorithms, especially when using only 10 initial samples in surrogate initialization, and demonstrating that the framework's effectiveness is not heavily dependent on task similarity.", "section": "5.1 Performance on EMOPs"}, {"figure_path": "08oUnmtj8Q/figures/figures_25_2.jpg", "caption": "Figure 3: IGD+ curves averaged over 30 runs on the DTLZ problems. Solid lines are mean values, while shadows are error regions. Upper: DTLZ1, DTLZ2, DTLZ3, DTLZ4. Lower: DTLZ5, DTLZ6, DTLZ7. MOEA/D-FSs and comparison algorithms initialize their surrogates with 10, 100 samples, respectively. X-axis denotes the extra 50 evaluations allowed in the further optimization. Note that \u2018FS(out)\u2019 indicates the target task is excluded from the range of related tasks during the meta-learning procedure.", "description": "This figure shows the inverted generational distance plus (IGD+) values over 30 runs for 7 different DTLZ problems.  The solid lines represent the mean IGD+ values, and the shaded regions represent the error.  The top row displays results for DTLZ1-4, and the bottom row shows results for DTLZ5-7.  The x-axis represents the number of additional evaluations performed after the initial surrogate is created.  Two different versions of the MOEA/D-FS algorithm are shown: one where the target task is included in the related tasks used for training (MOEA/D-FS), and one where it's excluded (MOEA/D-FS(out)). The results are compared against several other multi-objective optimization algorithms, all initializing their surrogates with either 10 or 100 samples.", "section": "5.1 Performance on EMOPs"}]