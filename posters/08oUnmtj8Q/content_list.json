[{"type": "text", "text": "FSEO: A Few-Shot Evolutionary Optimization Framework for Expensive Multi-Objective Optimization and Constrained Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Meta-learning has been demonstrated to be useful to improve the sampling effi  \n2 ciency of Bayesian optimization (BO) and surrogate-assisted evolutionary algo  \n3 rithms (SAEAs) when solving expensive optimization problems (EOPs). However,   \n4 existing studies focuses on only single-objective optimization, leaving other ex  \n5 pensive optimization scenarios unconsidered. We propose a generalized few-shot   \n6 evolutionary optimization (FSEO) framework and focus on its performance on two   \n7 common expensive optimization scenarios: multi-objective EOPs (EMOPs) and   \n8 constrained EOPs (ECOPs). We develop a novel meta-learning modeling approach   \n9 to train surrogates for our FSEO framework, an accuracy-based update strategy is   \n10 designed to adapt surrogates during the optimization process. The surrogates in   \n11 FSEO framework combines neural network with Gaussian Processes (GPs), their   \n12 network parameters and some parameters of GPs represent useful experience and   \n13 are meta-learned across related optimization tasks, the remaining GPs parameters   \n14 are task-specific parameters that represent unique features of the target task. We   \n15 demonstrate that our FSEO framework is able to improve sampling efficiency on   \n16 both EMOP and ECOP. Empirical conclusions are made to guide the application of   \n17 our FSEO framework. ", "page_idx": 0}, {"type": "text", "text": "18 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "19 Expensive optimization problems (EOPs) aim to find as good as possible solutions within a budget   \n20 of limited solution evaluations. Conventional Bayesian optimization (BO) and surrogate-assisted   \n21 evolutionary algorithms (SAEAs) have been widely used to solve EOPs, but they train surrogate   \n22 models from the scratch. To further improve the sampling efficiency and optimization performance,   \n23 many efforts have been made to pre-train surrogates with the prior experience gain from related   \n24 optimization tasks, resulting in experience-based optimization algorithms [1, 21, 36, 35].   \n25 This work considers solving EOPs on the context of few-shot problems [5, 41], where plenty of   \n26 expensive related tasks are available and each of them can provide a small dataset for experience   \n27 learning. Therefore, many experience-based optimization approaches such as multi-tasking optimiza  \n28 tion [43, 2, 47], transfer optimization [35, 17, 16] are not considered as they cannot learn experience   \n29 from small related tasks (A discussion is available in Appendix A). In comparison, meta-learning   \n30 [14] has been proved to be powerful in solving few-shot problems, leading to a new subcategory of   \n31 experience-based optimization, namely few-shot optimization (FSO) [46].   \n32 Existing studies on FSO are mainly few-shot Bayesian optimization (FSBO) where meta-learning   \n33 approaches are combined with BO to solve EOPs with only one objective. In this paper, we propose   \n34 a generalized few-shot evolutionary optimization (FSEO) framework to address EOPs from the   \n35 perspective of SAEAs and consider two expensive optimization scenarios which have been limited   \n36 studied: multi-objective EOPs (EMOPs) and constrained EOPs (ECOPs). Major contributions are   \n37 summarized as follows.   \n38 \u2022 A novel meta-learning method, namely Meta Deep Kernel Learning (MDKL), is developed   \n39 to gain prior experience from related expensive tasks. Our model architecture and parameter   \n40 designs make it possible to generate a regression-based surrogate on the prior experience   \n41 and then continually adapt the surrogate to approximate the target task.   \n42 \u2022 We propose a FSEO framework to solve EOPs from the perspective of SAEAs. FSEO   \n43 framework is applicable to regression-based SAEAs since FSEO embed our meta-learning   \n44 models in these SAEAs as their surrogates. In addition, an update strategy is designed to   \n45 adapt surrogates constantly during the optimization. Note that our FSEO framework is a   \n46 general framework but we focus on its performance on EMOPs and ECOPs in this paper.   \n47 \u2022 Experiments are conducted on EMOPs and ECOPs to show our FSEO framework is effec  \n48 tive. Our comprehensive ablation studies discover the influence of some factors on FSEO   \n49 performance and provide empirical guidance to the application of FSEO framework. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "50 2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "51 Experience-based optimization can be divided into several subcategories according to the techniques   \n52 of learning prior experience from related tasks. A detailed classification and discussion on these   \n53 subcategories is available in Appendix A. This subsection focuses on related work on FSO.   \n54 FSO studies in the literature can be classified based on their model architectures. Most studies meta  \n55 learn parameters for Gaussian Processes (GPs) [44], namely FSBO or Meta Bayesian Optimization   \n56 (MBO) [32, 42, 26, 38]. In addition, [23] meta-learns with transformer neural processes and [46, 6]   \n57 meta-learn parameters for the architecture of deep kernel learning (DKL) [45]. The MDKL model in   \n58 our FSEO belongs to the last category as its model architecture is relevant to DKL.   \n59 Our work is different from existing studies in three points: Firstly, many studies [46] use existing   \n60 meta-learning models [27] as their surrogates. No further adaptations are made to these surrogates   \n61 during optimization since they are not originally designed for optimization. In comparison, we try to   \n62 develop a meta-learning model, MDKL, for optimization purpose. MDKL has explicit task-specific   \n63 parameters, which allows continually model adaptations during the optimization. Secondly, existing   \n64 work investigated only global optimization, leaving other optimization scenarios such as EMOP and   \n65 ECOP still awaiting for investigation. As our MDKL is designed for optimization and is capable of   \n66 continually adaptation, we pay attention on EMOPs and ECOPs which require more effective models   \n67 than global optimization. Our work widens the scope of existing FSO research and it focuses on the   \n68 perspective of SAEAs instead of BO. Lastly, in-depth ablation studies are lacking in the literature,   \n69 making it unclear which factors affect the performance of FSO. Our extensive ablation studies fill   \n70 this gap and we conclude some empirical rules to improve the performance of FSO. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "71 3 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "72 This section gives preliminaries about meta-learning and DKL. The former is the method of experience   \n73 learning, the latter is the underlying structure of experience representation. ", "page_idx": 1}, {"type": "text", "text": "74 3.1 Meta-Learning in Few-Shot Problems ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "75 In the context of few-shot problems, we have plenty of related tasks, each task $T$ contributes a couple   \n76 of small datasets $D=\\{(S,Q)\\}$ , namely support dataset $S$ and query dataset $Q$ , respectively. After   \n77 learning from datasets of random related tasks, a support set $S_{*}$ from new unseen task $T_{*}$ is given   \n78 and one is asked to estimate the labels or values of a query set $Q_{*}$ . The problem is called 1-shot or   \n79 5-shot when only 1 data point or 5 data points are provided in $S_{*}$ . A comprehensive definition of   \n80 few-shot problems is available in [5, 41].   \n81 Meta-learning methods have been widely used to solve few-shot problems [41]. They learn domain  \n82 specific features that are shared among related tasks as experience, such experience is used to   \n83 understand and interpret the data collected from new tasks encountered in the future. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "84 3.2 Deep Kernel Learning (DKL) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "85 DKL aims at constructing kernels that encapsulate the expressive power of deep architectures for GPs.   \n86 To create expressive and scalable closed form covariance kernels, DKL combines the non-parametric   \n87 flexibility of kernel methods and the structural properties of deep neural networks. In practice, a deep   \n88 kernel $k\\big(\\mathbf{x}^{i},\\mathbf{x}^{j}|\\gamma\\big)$ transforms the inputs $\\mathbf{X}$ of a base kernel $k(\\mathbf{x}^{i^{*}},\\mathbf{x}^{j}|\\pmb{\\theta})$ through a non-linear mapping   \n89 given by a deep architecture $\\phi(\\mathbf{x}|\\mathbf{w},\\mathbf{\\bar{b}})$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nk(\\mathbf{x}^{i},\\mathbf{x}^{j}|\\gamma)=k(\\phi(\\mathbf{x}^{i}|\\mathbf{w},\\mathbf{b}),\\phi(\\mathbf{x}^{j}|\\mathbf{w},\\mathbf{b})|\\theta),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "90 where $\\pmb{\\theta}$ and $(\\mathbf{w},\\mathbf{b})$ are parameter vectors of the base kernel and the deep architecture, respectively.   \n91 $\\boldsymbol{\\gamma}=\\{\\boldsymbol{\\theta},\\mathbf{w},\\mathbf{b}\\}$ is the set of all parameters in this deep kernel. Note that in DKL, all parameters $\\gamma$ of a   \n92 deep kernel $k(\\mathbf{x}^{i},\\mathbf{x}^{j}|\\gamma)$ are learned jointly by using the log marginal likelihood function of GPs as a   \n93 loss function. Such a jointly learning strategy has been shown to make a DKL algorithm outperform   \n94 a combination of a deep neural network and a GP model, where a trained GP model is applied to the   \n95 output layer of a trained deep neural network [45]. ", "page_idx": 2}, {"type": "text", "text": "96 3.3 Meta-Learning on DKL ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "97 An important distinction between DKL algorithms and the applications of meta-learning to DKL is   \n98 that DKL algorithms learn their deep kernels from single tasks instead of collections of related tasks.   \n99 Such a difference alleviates two drawbacks of single task DKL [39]: First, the scalability of deep   \n100 kernels is no longer an issue as datasets in meta-learning are small. Second, the risk of overftiting is   \n101 decreased since diverse data points are sampled across tasks. ", "page_idx": 2}, {"type": "text", "text": "102 4 Few-Shot Evolutionary Optimization (FSEO) Framework ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "103 In this paper, $T_{*}$ denotes the target optimization task, and plenty of small datasets $D_{i}$ sampled from   \n104 related tasks $T_{i}$ are available for experience learning. A complete list of notations is available at the   \n105 beginning of Appendix. ", "page_idx": 2}, {"type": "text", "text": "106 4.1 Overall Working Mechanism ", "text_level": 1, "page_idx": 2}, {"type": "image", "img_path": "08oUnmtj8Q/tmp/caef1d62a2dbde90806ff7143a4e7b2db97425fec3abd43d72662bf7cebf6f43.jpg", "img_caption": ["Figure 1: Diagram of our FSEO framework. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "107 As illustrated in Fig. 1, all modules covering the optimization of target task $T_{*}$ are included in a   \n108 grey block. The modules beyond the grey block are associated with related tasks $T_{i}$ and experience   \n109 learning, which distinguishes our FSEO framework from conventional SAEAs and BO. The MDKL   \n110 surrogate modeling method consists of two procedures: meta-learning procedure and adaptation   \n111 procedure. The former learns prior experience from $T_{i}$ , the latter uses experience to adapt surrogates   \n112 to fit $T_{*}$ . The framework of FSEO is depicted in Alg. 1, it consists of the following major steps.   \n113 1. Experience learning: Before expensive optimization starts, a meta-learning procedure is   \n114 conducted to train task-independent parameters $\\gamma^{e}$ for MDKL surrogates (line 2). $N_{m}$   \n115 datasets $\\{D_{m1},\\dotsc,D_{m N_{m}}\\}$ collected from $N$ related tasks $\\{T_{1},\\dotsc,T_{N}\\}$ are used to train   \n116 $\\gamma^{e}$ . $\\gamma^{e}$ is the experience that represents the domain-specific features of related tasks.   \n117 2. Initialize surrogates with experience: Optimization starts when a target optimization task   \n118 $T_{*}$ is given. An initial dataset $S_{*}$ is sampled (line 3) to adapt task-specific parameters $\\gamma^{*}$ on   \n119 the basis of experience $\\gamma^{e}$ . After that, MDKL surrogates are updated (line 4).   \n120 3. Reproduction: MDKL surrogates $h(\\gamma^{*})$ are combined with a SAEA optimizer Opt to   \n121 search for optimal solution(s) $\\mathbf{x}^{*}$ on $h(\\gamma^{*})$ (line 7). This is implemented by replacing the   \n122 original (regression-based) surrogates in a SAEA with $h(\\gamma^{*})$ .   \n123 4. Update archive and surrogates: New optimal solution(s) $\\mathbf{x}^{*}$ is evaluated on target task $T_{*}$   \n124 (line 8). The evaluated solutions will be added to dataset $S_{*}$ (line 9) which serves as an   \n125 archive. Then, surrogate adaptation is triggered, surrogates $h(\\gamma^{*})$ are updated (line 10).   \n126 5. Stop criterion: Once the evaluation budget has run out, the evolutionary optimization will   \n127 be terminated and the optimal solutions in dataset $S_{*}$ will be outputted. Otherwise, the   \n128 algorithm goes back to step 3. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 FSEO Framework. ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "1: Input: $D_{i}$ : Datasets collected from related tasks $T_{i}$ , $\\mathrm{i}{=}\\{1,\\ldots,N\\}$ ; $N_{m}$ : Number of subsets $D_{m}$   \nfor meta-learning; $|D_{m}|$ : Size of subsets $D_{m}$ . $|D_{m}|\\,\\leq\\,|D_{i}|$ due to $D_{m}\\subseteq D_{i}$ ; Batch size $B$ ;   \nSurrogate learning rates $\\alpha,\\beta$ ; Target task $T_{*}$ ; A SAEA optimizer $O p t$ ; Fitness evaluation budget   \nFEmax.   \n2: Experience $\\gamma^{e}\\gets\\mathrm{Meta-learning}(D_{i},N_{m},|D_{m}|,B,\\alpha)$ . /\u2217Alg. 2.\u2217/   \n3: $S_{*}\\gets$ Sampling $1d$ solutions from $T_{*}$ .   \n4: $h(\\gamma^{*})\\gets\\mathrm{Adaptation}(\\gamma^{e},S_{*},\\beta).$ . /\u2217Initialize surrogate. $^*/$   \n5: Set evaluation counter $F E=|S_{*}|$ .   \n6: while $F E<F E_{m a x}$ do   \n7: Candidate solution(s) $\\mathbf{x}^{*}\\gets\\mathbf{S}$ urrogate-assisted optimization $(O p t,h(\\gamma^{*}))$ .   \n8: $f(\\mathbf{x}^{*})\\leftarrow$ Evaluate $\\mathbf{x}^{*}$ on $T_{*}$ .   \n9: $S_{*}\\gets S_{*}\\cup\\{(\\mathbf{x}^{*},f(\\mathbf{x}^{*}))\\}$ .   \n10: $h(\\gamma^{*})\\gets\\mathrm{Update}(\\gamma^{*},S_{*},\\beta).\\;/^{*}\\mathrm{Alg}.\\;4.^{*}/$   \n11: Update $F E$ .   \n12: end while   \n13: Output: Optimal solutions in $S_{*}$ . ", "page_idx": 3}, {"type": "text", "text": "129 4.2 Learning and Using Experience by MDKL ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "130 In MDKL, the domain-specific features of related tasks are used as experience, which are represented   \n131 by the task-independent parameters $\\gamma^{e}$ learned across related tasks. To make MDKL more capable of   \n132 expressing complex domain-specific features, the base kernel $k(\\mathbf{x}^{i},\\mathbf{x}^{j}|\\mathbf{\\theta}\\theta)$ in GP is combined with a   \n133 neural network $\\phi(\\mathbf{w},\\mathbf{b})$ to construct a deep kernel (see Eq.(1)). The modeling of a MDKL model   \n134 consists of two procedures: meta-learning procedure and adaptation procedure. To make a clear   \n135 illustration, we introduce frameworks of two procedures and then explain them in detail. ", "page_idx": 3}, {"type": "text", "text": "136 Meta-learning procedure: Learning experience ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "137 Our MDKL model uses the kernel in [18] as its base kernel: ", "page_idx": 3}, {"type": "equation", "text": "$$\nk(\\mathbf{x}^{i},\\mathbf{x}^{j}|\\theta,\\mathbf{p})=e x p(-\\sum_{k=1}^{d}\\theta_{k}|x_{k}^{i}-x_{k}^{j}|^{p_{k}}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "138 Therefore, the deep kernel will be: ", "page_idx": 3}, {"type": "equation", "text": "$$\nk(\\mathbf{x}^{i},\\mathbf{x}^{j}|\\gamma)=e x p(-\\sum_{k=1}^{d}\\theta_{k}|\\phi(x_{k}^{i})-\\phi(x_{k}^{j})^{|p_{k}}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "139 where $\\boldsymbol{\\gamma}=\\{\\mathbf{w},\\mathbf{b},\\boldsymbol{\\theta},\\mathbf{p}\\}$ is a set of deep kernel parameters. $\\phi,\\mathbf{w}$ and $\\mathbf{b}$ are neural network and its   \n140 parameters (see Eq.(1)). Details about alternative base kernels are available in [44].   \n141 The aim of meta-learning procedure is to learn experience $\\gamma^{e}$ from related tasks $\\{T_{1},\\dotsc,T_{N}\\}$ ,   \n142 including neural network parameters $\\mathbf{w},\\mathbf{b}$ , and task-independent base kernel parameters $\\pmb{\\theta}^{e},\\mathbf{p}^{e}$ . The   \n143 pseudo-code of meta-learning procedure is given in Alg. 2. Ideally, the experience $\\gamma^{e}$ is learned from   \n144 plenty of $(N_{m})$ small datasets $D_{m}$ collected from different related tasks. However, in practice, the   \n145 number of available related tasks $N$ may be much smaller than $N_{m}$ . Hence, the meta-learning is   \n146 conducted gradually over $U$ update iterations (line 3). During each update iteration, a small batch of ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "rBelatedstmaaslkls codntartiabsuettesAlgorithm 2 Meta-learning(Di, Nm, |Dm|, B, \u03b1)   \n$\\{D_{m1},...,D_{m B}\\}$ for 1: Input: $D_{i}$ : Datasets collected from related tasks $T_{i}$ , $\\mathrm{i}{=}\\{1,\\ldots,N\\}$ ; $N_{m}$ : meta-learning purpose Number of subsets $D_{m}$ for meta-learning; $|D_{m}|$ : Size of subsets $D_{m}$ . (lines 5 and 7). Note that $|D_{m}|\\leq|D_{i}|$ due to $D_{m}\\subseteq D_{i}$ ; Batch size $B$ ; Learning rate for priors $\\alpha$ . if $N<N_{m}$ , a related task 2: Randomly initialize ${\\bf w},{\\bf b},\\pmb{\\theta}^{e},{\\bf p}^{e}$ .   \n$T_{i}$ can be used multiple 3: Set the number of update iterations $\\mathrm{U}=N_{m}/B$ .   \ntimes in the meta-learning 4: for $j=1$ to $U$ do   \nprocedure. 5: $\\begin{array}{r l}{\\{\\boldsymbol D_{1}^{\\prime},\\ldots,\\boldsymbol D_{B}^{\\prime}\\}}&{{}\\leftarrow}\\end{array}$ Randomly select a batch of datasets from For a given dataset $D_{m i}$ , $\\{D_{1},...,D_{N}\\}$ .   \nwe denote $\\pmb{\\theta}^{i}=\\pmb{\\theta}^{e}+\\Delta\\pmb{\\theta}^{i}$ 6: for all $D_{i}^{\\prime}$ in the batch do   \nand $\\mathbf{p}^{i}\\ =\\ \\mathbf{p}^{e}\\,+\\,\\Delta\\mathbf{p}^{i}$ as 7: $D_{m i}\\leftarrow\\mathrm{A}$ subset of size $|D_{m}|$ from $D_{i}^{\\prime}$ .   \nthe task-specific kernel pa- 8: Initialize task-specific increment $\\Delta\\pmb{\\theta}^{i},\\Delta\\mathbf{p}^{i}$ .   \nrameters, where $\\Delta\\pmb{\\theta}^{i},\\Delta\\mathbf{\\dot{p}}^{i}$ 9: Compute task-specific parameters: $\\pmb{\\theta}^{i}=\\pmb{\\dot{\\theta}}^{e}+\\Delta\\pmb{\\theta}^{i},\\mathbf{p}^{i}=\\mathbf{p}^{e}+\\Delta\\mathbf{p}^{i}$ . are the distance we need10: Obtain deep kernel $\\bar{k}(\\mathbf{x}^{i},\\mathbf{x}^{j}|\\gamma)$ based GP: $h(\\gamma)$ , where $\\gamma\\ =$ to move from the task- $\\{\\mathbf{w},\\mathbf{b},\\pmb{\\theta}^{i},\\mathbf{p}^{i}\\}$ (Eq.(3)).   \nindependent parameters to11: Compute the loss function $L(D_{m i},h(\\gamma))$ (Eq.(4)).   \nthe task-specific parame-12: end for   \nters (line 9). The loss func-13: Update ${\\bf w},{\\bf b},\\pmb{\\theta}^{e},{\\bf p}^{e}$ via gradient descent: $\\alpha\\bigtriangledown L(D_{m i},h(\\gamma))$ (Eq.(5)). tion $L$ of MDKL is the like-14: end for   \nlihood function defined as15: Output: Task-independent parameters: $\\gamma^{e}=\\{\\mathbf{w},\\mathbf{b},\\pmb{\\theta}^{e},\\mathbf{p}^{e}\\}$ .   \nfollows [18]: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{1}{(2\\pi)^{n/2}(\\sigma^{2})^{n/2}|\\mathbf{R}|^{1/2}}e x p[-\\frac{(\\mathbf{y}-\\mathbf{1}\\mu)^{T}\\mathbf{R}^{-1}(\\mathbf{y}-\\mathbf{1}\\mu)}{2\\sigma^{2}}],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "148 where $|\\bf R|$ is the determinant of the correlation matrix $\\mathbf{R}$ , each element in the matrix is computed   \n149 through Eq.(3). y is the fitness vector of $D_{m i}$ . $\\mu$ and $\\sigma^{2}$ are the mean and the variance of the prior   \n150 distribution, respectively. Experience $\\gamma^{e}=\\{\\mathbf{w},\\mathbf{b},\\pmb{\\theta}^{e},\\mathbf{p}^{e}\\}$ is updated by gradient descent (line 13),   \n151 take $\\pmb{\\theta}^{e}$ as an example: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{\\theta}^{e}\\leftarrow\\pmb{\\theta}^{e}-\\frac{\\alpha}{B}\\sum_{i=1}^{B}\\nabla\\pmb{\\theta}^{e}L(D_{m i},h(\\pmb{\\gamma})).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "152 After $U$ iterations, $\\gamma^{e}$ has been trained sufficiently by $N_{m}$ small datasets $D_{m}$ and will be used in   \n153 target task $T_{*}$ later. ", "page_idx": 4}, {"type": "text", "text": "154 Adaptation procedure: Using experience ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "155 The meta-learning of experience $\\gamma^{e}$ enables MDKL to handle a family of related tasks in general. To   \n156 approximate a specific task $T_{*}$ well, surrogate $h(\\gamma^{e})$ needs to adapt task-specific increments $\\Delta\\pmb{\\theta}^{*}$   \n157 and $\\Delta{\\bf p}^{*}$ in the way described in Alg. 3. A diagram of the deep kernel implemented in our MDKL   \n158 model is illustrated in Fig. 2. ", "page_idx": 4}, {"type": "text", "text": "159 ", "page_idx": 4}, {"type": "table", "img_path": "08oUnmtj8Q/tmp/0a8fc19a2228e5ccb3146b7ea7a3410406c4b230c82ba9c7e99b8e488d048bbc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "160 Surrogate prediction. Due to the nature of a GP, when predicting the fitness of a solution $\\mathbf{x}^{*}$ , a   \n161 MDKL surrogate produces a predictive Gaussian distribution $\\mathcal{N}(\\hat{y}(\\mathbf{x}^{*}),\\hat{s}^{2}(\\mathbf{x}^{*}))$ , the predicted mean   \n162 $\\hat{y}(\\mathbf x^{*})$ and covariance $\\hat{s}^{2}(\\mathbf{x}^{\\ast})$ are specified as [18]: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{y}(\\mathbf x^{*})=\\mu+\\mathbf r^{\\prime}\\mathbf{R}^{-1}(\\mathbf y-\\mathbf1\\mu),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "163 ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{s}^{2}(\\mathbf{x}^{*})=\\sigma^{2}(1-\\mathbf{r}^{\\prime}\\mathbf{R}^{-1}\\mathbf{r}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "164 where $\\mathbf{r}$ is a correlation vector consisting of covariances between $\\mathbf{x}^{*}$ and $S_{*}$ , other variables are   \n165 explained in Eq.(4). ", "page_idx": 5}, {"type": "text", "text": "166 4.3 Surrogate Update Strategy ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "167 In this subsection, we describe the update strategy in our FSEO framework. To properly integrate   \n168 experience and data from $T_{*}$ , our update strategy is designed to determine whether a MDKL surrogate   \n169 should be adapted in the current iteration or not, ensuring an optimal update frequency of surrogates. ", "page_idx": 5}, {"type": "text", "text": "70 As illustrated in Alg. 4, the surrogate update starts when a new optimal solution(s) has been evaluated on expensive functions and an updated archive $S_{*}$ is available. For a given surrogate $h(\\gamma^{*})$ , its mean squared error (MSE) on $S_{*}$ is selected as the update criterion: If the MSE after an adaptation $e_{1}$ (line 4) is larger than the MSE without an adaptation $e_{0}$ (line 2), then the surrogate will roll back to the status before the adaptation. This indicates the surrogate update has been refused and $h(\\gamma^{*})$ will not be adapted in the current iteration. Otherwise, the adapted surrogate will be chosen (line 6). Note that no matter whether surrogate adaptations are accepted or refused, the resulting surrogates will be treated as updated surrogates, which are employed to assist the SAEA optimizer in the next iteration. ", "page_idx": 5}, {"type": "table", "img_path": "08oUnmtj8Q/tmp/fb28aced901632ebd450b56cdfcc20f3e5485adc72a279b923c895c7fcf7cfbe.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "171 5 Computational Studies ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "172 Our computational studies can be divided into three parts: (1). Appendix D evaluates the effectiveness   \n173 of learning experience through a synthetic problem and a real-world engine modeling problem. (2).   \n174 Sections 5.1 to 5.2 use EMOPs as examples to investigate the performance of our FSEO framework   \n175 in depth. Empirical evidence is provided to guide the use of our FSEO framework. (3). Section   \n176 5.3 investigates the performance of our FSEO framework on a real-world ECOP. The source code   \n177 is available online1 For all meta-learning methods used in our experiments, their basic setups are   \n178 listed in Table 1. The neural network structure is suggested by [10, 27], and the learning rates are the   \n179 default values that have been widely used in many meta-learning methods [13, 27]. ", "page_idx": 5}, {"type": "text", "text": "180 5.1 Performance on EMOPs ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "181 In the following subsections, we aim to demonstrate the effectiveness of our FSEO framework. The   \n182 experiment in this subsection is designed to answer the question below: With the experience learned   \n183 from related tasks, can our FSEO framework helps a SAEA to save $9d$ solutions without a loss of   \n184 optimization performance?   \n185 The computational study is conducted on the DTLZ test problems [8]. All the DTLZ problems have   \n186 $d=10$ decision variables and 3 objectives, as the setups that have been widely used in [25, 33].   \n187 The details of generating DTLZ variants (related tasks) are provided in Appendix C. We test our   \n188 FSEO framework using an instantiation on MOEA/D-EGO, resulting MOEA/D-FS. Details of the   \n189 comparison algorithms are given in Appendix E.1. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "08oUnmtj8Q/tmp/649a2330b8b2bf62be7cdc017b13b32217f05f147bd59b8026cf3b4173b43969.jpg", "table_caption": ["Table 1: Parameter setups for meta-learning methods. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "08oUnmtj8Q/tmp/745e0f675921c682c1cb65332e798927fee3b70b8e8b019c05c1f3d2f3f0b5c7.jpg", "table_caption": ["Table 2: Parameter setups for DTLZ optimization. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "190 5.1.1 Experimental setups ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "191 The parameter setups for this multi-objective optimization experiment are listed in Table 2. During   \n192 the optimization process, an initial dataset $S_{*}$ is sampled using Latin-Hypercube Sampling (LHS)   \n193 method [24], then extra evaluations are conducted until the evaluation budget has run out. Note that   \n194 we aim to use related tasks to save $9d$ evaluations without a loss of SAEA optimization performance.   \n195 Hence, the total evaluation budgets for MOEA/D-FS and comparison algorithms are different.   \n196 Since the test problems have 3 objectives, we employ inverted generational distance plus $(\\mathrm{IGD}+)$ [15]   \n197 as our performance indicator, where smaller $\\mathrm{IGD+}$ values indicate better optimization results. 5000   \n198 reference points are generated for computing $\\mathrm{IGD+}$ values, as suggested in [25]. More results in IGD   \n199 [4] and HV [55] metrics are reported in Appendix E.3. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "200 5.1.2 Results and analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "201 The statistical test results are reported in Fig. 3 and Appendix E.2 (Table 5). It can be seen from Fig. 3   \n202 that, although 90 fewer evaluations are used in surrogate initialization, MOEA/D-FS can still achieve   \n203 competitive or even smaller $\\mathrm{IGD+}$ values than MOEA/D-EGO on all DTLZ problems except for   \n204 DTLZ7. In addition, the $\\mathrm{IGD+}$ values obtained by MOEA/D-FS drop rapidly, especially during the   \n205 first few evaluations, implying the experience learned from DTLZ variants are effective. Therefore,   \n206 in most situations, our FSEO framework is able to assist MOEA/D-EGO in reaching competitive   \n207 or even better optimization results, with the number of evaluations used for surrogate initialization   \nreduced from $10d$ to only $1d$ . ", "page_idx": 6}, {"type": "image", "img_path": "08oUnmtj8Q/tmp/a723aec48b9e009205a86d4e4892569c8b226324797547afbc9d56e69f7f2bb3.jpg", "img_caption": ["Figure 3: $\\mathrm{IGD+}$ curves averaged over 30 runs on the DTLZ problems. Solid lines are mean values, while shadows are error regions. Upper: DTLZ1, DTLZ2, DTLZ3, DTLZ4. Lower: DTLZ5, DTLZ6, DTLZ7. MOEA/D-FSs and comparison algorithms initialize their surrogates with 10, 100 samples, respectively. X-axis denotes the extra 50 evaluations allowed in the further optimization. Note that \u2018FS(out)\u2019 indicates the target task is excluded from the range of related tasks during the meta-learning procedure) (see Appendix F). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "208 ", "page_idx": 6}, {"type": "text", "text": "209 MOEA/D-FS is less effective on DTLZ7 than on other DTLZ problems, which might be attributed to   \n210 the discontinuity of the Pareto front on DTLZ7. Note that MOEA/D-FS learns experience from small   \n211 datasets such as $D_{m}$ and $S_{*}$ . The solutions in these small datasets are sampled at random, hence, the   \n212 probability of having optimal solutions being sampled is small. However, it is difficult to learn the   \n213 discontinuity of the Pareto front from the sampled non-optimal solutions. As a result, the knowledge   \n214 of \u2018there are four discrete optimal regions\u2019 cannot be learned from such small datasets $(|D_{m}|=2\\bar{0})$ )   \n215 collected from related tasks. The performance analysis between MOEA/D-FS and other comparison   \n216 algorithms are available in Appendix E.2. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "217 5.1.3 More comparison experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "218 We also compared the performance of our FSEO framework when only 10 evaluations are used for   \n219 surrogate initialization for comparison algorithms. The results are reported in Table 8 in Appendix   \n220 E.4. In addition, the performance of our FSEO framework in the context of extremely expensive   \n221 optimization has been investigated in Appendix H (Table 11 and Fig. 7).   \n222 The question raised at the beginning of this subsection can be answered by the results discussed so   \n223 far. Due to the integration of the experience learned from related tasks (DTLZ variants), although the   \n224 evaluation cost of surrogates initialization has been reduced from $10d$ to $1d$ , our FSEO framework is   \n225 still capable of assisting regression-based SAEAs to achieve competitive or even better optimization   \n226 results in most situations. ", "page_idx": 7}, {"type": "text", "text": "227 5.2 Ablation Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "228 We conduct two ablation studies to investigate the influence of task similarity and that of the dataset   \n229 size used in meta-learning, results and analysis are reported in Appendixes F and G, respectively. ", "page_idx": 7}, {"type": "text", "text": "230 5.3 Performance on Real-World ECOPs ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "231 The experiments on EMOPs have investigated the performance of our FSEO framework in depth. In   \n232 this subsection, we evaluate our FSEO framework on a real-world gasoline motor engine calibration   \n233 problem, which is an ECOP.   \n234 The calibration problem has 6 adjustable engine parameters, namely the throttle angle, waste gate   \n235 orifice, ignition timing, valve timings, state of injection, and air-fuel-ratio. The calibration aims at   \n236 minimizing the BSFC while satisfying 4 constraints in terms of temperature, pressure, CA50, and   \n237 load simultaneously [53]. ", "page_idx": 7}, {"type": "text", "text": "238 5.3.1 Comparison algorithms ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "239 Since the comparison algorithms in the DTLZ optimization experiments are not designed for handling   \n240 constrained optimization, our comparison is conducted with 3 state-of-the-art constrained optimization   \n241 algorithms used in industry: A variant of EGO designed to handle constrained optimization problems   \n242 (cons_EGO) [53], a GA customized for this calibration problem (adaptiveGA) [53], and a bilevel   \n243 constrained SAEA (SAB-DE) [50]. The settings of the comparison algorithms are the same as   \n244 suggested in the literature. In this experiment, we apply our FSEO framework to cons_EGO and   \n245 investigate its optimization performance. The GP surrogates in cons_EGO are replaced by our MDKL   \n246 surrogates to conduct the comparison, and the resulting algorithm is denoted as cons_FS. ", "page_idx": 7}, {"type": "text", "text": "247 5.3.2 Experimental setups ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "248 The setup of related tasks $(N,D_{i})$ is the same as described in Appendix D. In the meta-learning   \n249 procedure, both the support set and the query set contain 6 data points, thus $|D_{m}|=12$ . The total   \n250 evaluation budget for all algorithms is set to 60. For adaptiveGA, all evaluations are used in the   \n251 optimization process as it is not a SAEA. For cons_EGO and SAB-DE, 40 samples are used to   \n252 initialize the surrogates and 20 extra evaluations are used in the optimization process. For cons_FS,   \n253 only 6 samples are used to initialize MDKL surrogates, and the remaining evaluations are used for   \n254 further optimization. ", "page_idx": 7}, {"type": "text", "text": "255 5.3.3 Optimization results and analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "256 The left side and right side of Fig. 4 plot the normalized BSFC results and the number of feasible   \n257 solutions found over the number of used evaluations, respectively. Solid lines are mean lines, while   \n258 shadows are error regions. From the left side of Fig. 4, it can be observed that the minimal BSFC   \n259 obtained by cons_FS decreases drastically in the first few evaluations, implying that the experience   \n260 learned from related tasks is effective. In comparison, the minimal BSFC obtained by adaptiveGA   \n261 and cons_EGO drops in a relatively slow rate, even though cons_EGO has used 34 more samples   \n262 to initialize its surrogates. The star marker denotes the point at which cons_FS has evaluated 20   \n263 samples after surrogate initialization. It is worth noting that when 20 samples have been evaluated   \n264 in the optimization, cons_FS achieves a smaller BSFC value than cons_EGO. After the star marker,   \n265 the decrease of BSFC becomes slow as cons_FS has reached the optimal region. Therefore, further   \n266 improvement in the normalized BSFC value is not significant and thus hard to be observed. The   \n267 advantages of our FSEO framework can also be observed in constraint handling. In the right side of   \n268 Fig. 4, cons_FS finds more feasible solutions than the 3 comparison algorithms. These results indicate   \n269 that our FSEO framework improves the performance of cons_EGO on both objective function and   \n270 constraint functions. Meanwhile, only $1d$ evaluations are used to initialize surrogates. ", "page_idx": 7}, {"type": "image", "img_path": "08oUnmtj8Q/tmp/58c19bc77c9b89967dc171205b7de7e362ad50fe09f2e9833c6026336a1b02d4.jpg", "img_caption": ["Figure 4: Results of 30 runs on the real-world engine calibration problem, all BSFC values are normalized. Solid lines are mean values, while shadows are error regions. Left figure shows how BSFC varies with the number of evaluations. The star markers highlight the results achieved when 20 evaluations are used in the optimization process. Right figure illustrates how the number of feasible solutions varies with the number of evaluations. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "271 5.3.4 Discussion on runtime ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "272 It should be noted that real engine performance evaluations on engine facilities are very costly in   \n273 terms of both time and financial budget [49]. Since a single real engine performance evaluation can   \n274 cost several hours [22, 49], the time cost of the meta-learning procedure is negligible as it takes only   \n275 a few minutes. Savings from reduced real engine performance evaluations on engine facilities and the   \n276 reduced development cycle due to our FSEO framework could amount to millions of dollars [49]. our   \n277 FSEO framework is an effective and efficient method to solve this real-world calibration problem. ", "page_idx": 8}, {"type": "text", "text": "278 6 Conclusion and further work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "279 Conclusion. In this paper, we present a FSEO framework to address EMOPs and ECOPs from the   \n280 perspective of SAEAs. A novel meta-learning approach MDKL is proposed to learn prior experience   \n281 from related expensive tasks. Our MDKL model is designed for optimization and has explicit   \n282 task-specific parameters, which allows continually update of task-specific parameters during the   \n283 optimization process. Our empirical experiments show that the FSEO framework is able to improve   \n284 the sampling efficiency and thus save expensive evaluations for existing regression-based SAEAs.   \n285 Ablation studies reveal the influence between optimization performance and solutions similarity as   \n286 well as the size of datasets for meta-learning.   \n287 Limitation and further work. The limitations of this work can be summarized as the following   \n288 two points: First, we do not have a mathematical definition of related tasks. Second, the proposed   \n289 framework is currently for regression-based SAEAs only. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "290 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "291 [1] Tianyi Bai, Yang Li, Yu Shen, Xinyi Zhang, Wentao Zhang, and Bin Cui. Transfer learning for   \n292 bayesian optimization: A survey. arXiv preprint arXiv:2302.05927, 2023.   \n293 [2] Kavitesh Kumar Bali, Yew-Soon Ong, Abhishek Gupta, and Puay Siew Tan. Multifactorial   \n294 evolutionary algorithm with online transfer parameter estimation: MFEA-II. IEEE Transactions   \n295 on Evolutionary Computation, 24(1):69\u201383, 2019.   \n296 [3] Hongli Bian, Jie Tian, Jialiang Yu, and Han Yu. Bayesian co-evolutionary optimization based   \n297 entropy search for high-dimensional many-objective optimization. Knowledge-Based Systems,   \n298 274:110630, 2023.   \n299 [4] Peter AN Bosman and Dirk Thierens. The balance between proximity and diversity in multiob  \n300 jective evolutionary algorithms. IEEE Transactions on Evolutionary Computation, 7(2):174\u2013188,   \n301 2003.   \n302 [5] Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer   \n303 look at few-shot classification. In Proceedings of the 7th International Conference on Learning   \n304 Representations (ICLR\u201919), 2019.   \n305 [6] Wenlin Chen, Austin Tripp, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Meta-learning adaptive deep ker  \n306 nel gaussian processes for molecular property prediction. In Proceedings of the 11th International   \n307 Conference on Learning Representations (ICLR\u201923), 2023.   \n308 [7] Tinkle Chugh, Yaochu Jin, Kaisa Miettinen, Jussi Hakanen, and Karthik Sindhya. A surrogate  \n309 assisted reference vector guided evolutionary algorithm for computationally expensive many  \n310 objective optimization. IEEE Transactions on Evolutionary Computation, 22(1):129\u2013142, 2016.   \n311 [8] Kalyanmoy Deb, Lothar Thiele, Marco Laumanns, and Eckart Zitzler. Scalable test problems   \n312 for evolutionary multiobjective optimization. In Evolutionary Multiobjective Optimization, pages   \n313 105\u2013145. Springer, London, U.K., 2005.   \n314 [9] Jinliang Ding, Cuie Yang, Yaochu Jin, and Tianyou Chai. Generalized multitasking for evolu  \n315 tionary optimization of expensive problems. IEEE Transactions on Evolutionary Computation,   \n316 23(1):44\u201358, 2017.   \n317 [10] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adapta  \n318 tion of deep networks. In Proceedings of the 34th International Conference on Machine Learning   \n319 (ICML\u201917), pages 1126\u20131135, 2017.   \n320 [11] Zhendong Guo, Haitao Liu, Yew-Soon Ong, Xinghua Qu, Yuzhe Zhang, and Jianmin Zheng.   \n321 Generative multiform Bayesian optimization. IEEE Transactions on Cybernetics, 53(7):4347\u2013   \n322 4360, 2022.   \n323 [12] Abhishek Gupta, Yew-Soon Ong, and Liang Feng. Insights on transfer optimization: Be  \n324 cause experience is the best teacher. IEEE Transactions on Emerging Topics in Computational   \n325 Intelligence, 2(1):51\u201364, 2017.   \n326 [13] James Harrison, Apoorva Sharma, and Marco Pavone. Meta-learning priors for efficient online   \n327 Bayesian regression. In Proceedings of the 13th Workshop on the Algorithmic Foundations of   \n328 Robotics (WAFR\u201918), pages 318\u2013337, 2018.   \n329 [14] Timothy M. Hospedales, Antreas Antoniou, Paul Micaelli, and Amos J Storkey. Meta-learning   \n330 in neural networks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence,   \n331 2021.   \n332 [15] Hisao Ishibuchi, Hiroyuki Masuda, Yuki Tanigaki, and Yusuke Nojima. Modified distance   \n333 calculation in generational distance and inverted generational distance. In Proceedings of the   \n334 8th International Conference on Evolutionary Multi-criterion Optimization (EMO\u201915), pages   \n335 110\u2013125, 2015.   \n336 [16] Min Jiang, Zhenzhong Wang, Shihui Guo, Xing Gao, and Kay Chen Tan. Individual-based   \n337 transfer learning for dynamic multiobjective optimization. IEEE Transactions on Cybernetics,   \n338 51(10):4968\u20134981, 2020.   \n339 [17] Min Jiang, Zhenzhong Wang, Liming Qiu, Shihui Guo, Xing Gao, and Kay Chen Tan. A fast   \n340 dynamic evolutionary multiobjective algorithm via manifold transfer learning. IEEE Transactions   \n341 on Cybernetics, 51(7):3417\u20133428, 2020.   \n342 [18] Donald R. Jones, Matthias Schonlau, and William J. Welch. Efficient global optimization of   \n343 expensive black-box functions. Journal of Global Optimization, 13(4):455\u2013492, 1998.   \n344 [19] Joshua Knowles. ParEGO: A hybrid algorithm with on-line landscape approximation for   \n345 expensive multiobjective optimization problems. IEEE Transactions on Evolutionary Computation,   \n346 10(1):50\u201366, 2006.   \n347 [20] Rung-Tzuo Liaw and Chuan-Kang Ting. Evolutionary manytasking optimization based on   \n348 symbiosis in biocoenosis. In Proceedings of the 33rd AAAI Conference on Artificial Intelligence   \n349 (AAAI\u201919), pages 4295\u20134303, 2019.   \n350 [21] Shengcai Liu, Ke Tang, and Xin Yao. Experience-based optimization: A coevolutionary   \n351 approach. arXiv preprint arXiv:1703.09865, 2017.   \n352 [22] He Ma. Control Oriented Engine Modeling and Engine Multi-objective Optimal Feedback   \n353 Control. PhD thesis, University of Birmingham, 2013.   \n354 [23] Alexandre Maraval, Matthieu Zimmer, Antoine Grosnit, and Haitham Bou Ammar. End-to-end   \n355 meta-bayesian optimisation with transformer neural processes. arXiv preprint arXiv:2305.15930,   \n356 2023.   \n357 [24] Michael D. McKay, Richard J. Beckman, and William J. Conover. A comparison of three   \n358 methods for selecting values of input variables in the analysis of output from a computer code.   \n359 Technometrics, 42(1):55\u201361, 2000.   \n360 [25] Linqiang Pan, Cheng He, Ye Tian, Handing Wang, Xingyi Zhang, and Yaochu Jin. A   \n361 classification-based surrogate-assisted evolutionary algorithm for expensive many-objective opti  \n362 mization. IEEE Transactions on Evolutionary Computation, 23(1):74\u201388, 2018.   \n363 [26] Jiarong Pan, Stefan Falkner, Felix Berkenkamp, and Joaquin Vanschoren. MALIBO: Meta  \n364 learning for likelihood-free bayesian optimization. arXiv preprint arXiv:2307.03565, 2023.   \n365 [27] Massimiliano Patacchiola, Jack Turner, Elliot J Crowley, Michael O\u2019Boyle, and Amos Storkey.   \n366 Bayesian meta-learning for the few-shot setting via deep kernels. In Advance in Neural Information   \n367 Processing Systems 33 (NeurIPS\u201920), 2020.   \n368 [28] Shufen Qin, Chaoli Sun, Farooq Akhtar, and Gang Xie. Expensive many-objective evolutionary   \n369 optimization guided by two individual infill criteria. Memetic Computing, pages 1\u201315, 2023.   \n370 [29] Gan Ruan, Leandro L. Minku, Stefan Menzel, Bernhard Sendhoff, and Xin Yao. When and how   \n371 to transfer knowledge in dynamic multi-objective optimization. In Proceedings of the 2019 IEEE   \n372 Symposium Series on Computational Intelligence (SSCI\u201919), pages 2034\u20132041, 2019.   \n373 [30] Gan Ruan, Leandro L. Minku, Stefan Menzel, Bernhard Sendhoff, and Xin Yao. Computa  \n374 tional study on effectiveness of knowledge transfer in dynamic multi-objective optimization. In   \n375 Proceedings of the 22nd IEEE Congress on Evolutionary Computation (CEC\u201920), pages 1\u20138,   \n376 2020.   \n377 [31] Jerome Sacks, William J. Welch, Toby J. Mitchell, and Henry P. Wynn. Design and analysis of   \n378 computer experiments. Statistical Science, 4(4):409\u2013423, 1989.   \n379 [32] Gresa Shala, Thomas Elsken, Frank Hutter, and Josif Grabocka. Transfer NAS with meta  \n380 learned bayesian surrogates. In Proceedings of the 11th International Conference on Learning   \n381 Representations (ICLR\u201923), 2023.   \n382 [33] Zhenshou Song, Handing Wang, Cheng He, and Yaochu Jin. A Kriging-assisted two-archive   \n383 evolutionary algorithm for expensive many-objective optimization. IEEE Transactions on Evolu  \n384 tionary Computation, 25(6):1013\u20131027, 2021.   \n385 [34] Michael L. Stein. Interpolation of Spatial Data: Some Theory for Kriging. Springer Science &   \n386 Business Media, New York, NY, 1999.   \n387 [35] Kay Chen Tan, Liang Feng, and Min Jiang. Evolutionary transfer optimization-a new frontier   \n388 in evolutionary computation research. IEEE Computational Intelligence Magazine, 16(1):22\u201333,   \n389 2021.   \n390 [36] Ke Tang, Shengcai Liu, Peng Yang, and Xin Yao. Few-shots parallel algorithm portfolio   \n391 construction via co-evolution. IEEE Transactions on Evolutionary Computation, 25(3):595\u2013607,   \n392 2021.   \n393 [37] Ye Tian, Ran Cheng, Xingyi Zhang, and Yaochu Jin. PlatEMO: A MATLAB platform for   \n394 evolutionary multi-objective optimization [educational forum]. IEEE Computational Intelligence   \n395 Magazine, 12(4):73\u201387, 2017.   \n396 [38] Petru Tighineanu, Lukas Grossberger, Paul Baireuther, Kathrin Skubch, Stefan Falkner, Julia   \n397 Vinogradska, and Felix Berkenkamp. Scalable meta-learning with gaussian processes. arXiv   \n398 preprint arXiv:2312.00742, 2023.   \n399 [39] Prudencio Tossou, Basile Dura, Francois Laviolette, Mario Marchand, and Alexandre Lacoste.   \n400 Adaptive deep kernel learning. arXiv preprint arXiv:1905.12131, 2019.   \n401 [40] Michael Volpp, Lukas P. Fr\u00f6hlich, Kirsten Fischer, Andreas Doerr, Stefan Falkner, Frank   \n402 Hutter, and Christian Daniel. Meta-learning acquisition functions for transfer learning in Bayesian   \n403 optimization. In Proceedings of the 8th International Conference on Learning Representations   \n404 (ICLR\u201920), 2020.   \n405 [41] Yaqing Wang, Quanming Yao, James T. Kwok, and Lionel M. Ni. Generalizing from a few   \n406 examples: A survey on few-shot learning. ACM Computing Surveys, 53(3):1\u201334, 2020.   \n407 [42] Zi Wang, George E. Dahl, Kevin Swersky, Chansoo Lee, Zachary Nado, Justin Gilmer, Jasper   \n408 Snoek, and Zoubin Ghahramani. Pre-trained gaussian processes for bayesian optimization. arXiv   \n409 preprint arXiv:2109.08215, 2021.   \n410 [43] Tingyang Wei, Shibin Wang, Jinghui Zhong, Dong Liu, and Jun Zhang. A review on evolutionary   \n411 multi-task optimization: Trends and challenges. IEEE Transactions on Evolutionary Computation,   \n412 26(5):941\u2013960, 2021.   \n413 [44] Christopher KI Williams and Carl Edward Rasmussen. Gaussian Processes for Machine   \n414 Learning. MIT press, Cambridge, MA, 2006.   \n415 [45] Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P. Xing. Deep kernel   \n416 learning. In Proceedings of the 19th International Conference on Artificial Intelligence and   \n417 Statistics (AISTATS\u201916), pages 370\u2013378, 2016.   \n418 [46] Martin Wistuba and Josif Grabocka. Few-shot Bayesian optimization with deep kernel surro  \n419 gates. In Proceedings of the 9th International Conference on Learning Representations (ICLR\u201921),   \n420 2021.   \n421 [47] Xiaoming Xue, Kai Zhang, Kay Chen Tan, Liang Feng, Jian Wang, Guodong Chen, Xinggang   \n422 Zhao, Liming Zhang, and Jun Yao. Affine transformation-enhanced multifactorial optimization   \n423 for heterogeneous problems. IEEE Transactions on Cybernetics, pages 1\u201315, 2020.   \n424 [48] Xunzhao Yu, Xin Yao, Yan Wang, Ling Zhu, and Dimitar Filev. Domination-based ordinal   \n425 regression for expensive multi-objective optimization. In Proceedings of the 2019 IEEE Symposium   \n426 Series on Computational Intelligence (SSCI\u201919), pages 2058\u20132065, 2019.   \n427 [49] Xunzhao Yu, Ling Zhu, Yan Wang, Dimitar Filev, and Xin Yao. Internal combustion engine   \n428 calibration using optimization algorithms. Applied Energy, 305:117894, 2022.   \n429 [50] Xunzhao Yu, Yan Wang, Ling Zhu, Dimitar Filev, and Xin Yao. Engine calibration with   \n430 surrogate-assisted bilevel evolutionary algorithm. IEEE Transactions on Cybernetics (Early   \n431 Access), 2023.   \n432 [51] Qingfu Zhang, Wudong Liu, Edward Tsang, and Botond Virginas. Expensive multiobjective   \n433 optimization by MOEA/D with gaussian process model. IEEE Transactions on Evolutionary   \n434 Computation, 14(3):456\u2013474, 2010.   \n435 [52] Liangjie Zhang, Yuling Xie, Jianjun Chen, Liang Feng, Chao Chen, and Kai Liu. A study on   \n436 multiform multi-objective evolutionary optimization. Memetic Computing, 13(3):307\u2013318, 2021.   \n437 [53] Ling Zhu, Yan Wang, Anuj Pal, and Guoming Zhu. Engine calibration using global optimization   \n438 methods with customization. Technical Report 2020-01-0270, SAE Technical Paper, 2020.   \n439 [54] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui   \n440 Xiong, and Qing He. A comprehensive survey on transfer learning. Proceedings of the IEEE,   \n441 109(1):43\u201376, 2020.   \n442 [55] Eckart Zitzler and Lothar Thiele. Multiobjective optimization using evolutionary algorithms - a   \n443 comparative case study. In Proceedings of the 5th International Conference on Parallel Problem   \n444 Solving from Nature (PPSN V), pages 292\u2013301, 1998.   \n446 In the past decade, experience-based optimization has attracted much attention as it uses the experience   \n447 gained from other optimization problems to improve the optimization efficiency of target problems,   \n448 which mimics human capabilities of cognitive and knowledge generalization [12]. The optimization   \n449 problems that provide experience or knowledge are regarded as source tasks, while the target   \n450 optimization problems are regarded as target tasks. To obtain useful experience, the tasks that are   \n451 related to target tasks are chosen as source tasks since they usually share domain-specific features   \n452 with target tasks. Diverse experience-based optimization methods have been proposed to use the   \n453 experience gained from related tasks to tackle target tasks. They can be divided into two categories   \n454 based on the direction of experience transformation.   \n455 In the first category, experience is transformed mutually. Every considered optimization problem is a   \n456 target task and also is one of the source tasks of other optimization problems. In other words, the   \n457 roles of source task and target task are compatible. One representative tributary is EMTO that aims to   \n458 solve multiple optimization tasks concurrently [9, 43, 20, 2, 47]. In EMTO, experience is learned,   \n459 updated, and spontaneously shared among target tasks through multi-task learning techniques. A   \n460 variant of EMTO is multiforms optimization [12, 52, 11]. In multiforms optimization, multi-task   \n461 learning methods are employed to learn experience from distinct formulations of a single target task.   \n462 In the second category, experience is transformed unidirectionally. The roles of source task and   \n463 target task are not compatible, an optimization problem cannot serve as a source task and a target   \n464 task simultaneously. One popular tributary is transfer optimization which employs transfer learning   \n465 techniques to transform experience from source tasks to target tasks [35, 17, 16, 40]. In transfer   \n466 learning, experience can be transformed from a single source task, multiple source tasks, or even   \n467 source tasks from a different domain [54]. However, these transfer learning techniques pay more   \n468 attention to experience transformation instead of experience learning. Despite diverse and complex   \n469 situations of experience transformation have been studied [29, 30], the difficult of learning experience   \n470 from small (expensive) source tasks has not been well studied. Actually, a common scenario in   \n471 transfer learning is that the source task(s) is/are large enough such that useful experience can be   \n472 obtained easily through solving source task(s) [54]. In contrast to transfer optimization, recently, some   \n473 experience-based optimization algorithms attempted to use meta-learning methods to learn experience   \n474 from small source tasks, which are known as few-shot optimization (FSO)[46]. Since meta-learning   \n475 only works for related tasks in the same domain, the situations of experience transformation are less   \n476 complex than that of transfer learning. As a result, meta-learning pays more attention to experience   \n477 learning instead of experience transformation. Domain-specific features are extracted as experience   \n478 and no related task needs to be solved.   \n479 Our work belongs to the FSO in the second category discussed above since our experience is   \n480 transformed unidirectionally. More importantly, our experience is learned across many related   \n481 expensive tasks, rather than gained through solving more or less source tasks. Therefore, our work is   \n482 different from transfer optimization. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "483 B Discussion on Framework Compatibility and Limitation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "484 Our FSEO framework is applicable to regression-based SAEAs as our MDKL surrogates can be   \n485 embedded in these SAEAs directly. Classification-based SAEAs are not compatible with our FSEO   \n486 framework. The classification surrogates in these SAEAs are employed to learn the relation between   \n487 pairs of solutions, or the relation between solutions and a set of reference solutions. The class labels   \n488 used for surrogate training can be fluctuating during the optimization and thus hard to be learned   \n489 over related tasks. Similarly, in ordinal-regression-based SAEAs, the ordinal relation values to be   \n490 learned are not as stable as the ftiness of expensive functions. So ordinal-regression-based SAEAs are   \n491 also not compatible with our FSEO framework. In this paper, we focus on FSO for regression-based   \n492 SAEAs, while other SAEA categories are left to be discussed in future work. ", "page_idx": 13}, {"type": "text", "text": "493 C Generation of DTLZ variants ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "494 Our DTLZ optimization experiments generate $m$ -objective DTLZ variants in the following ways:   \n495 DTLZ1: ", "page_idx": 14}, {"type": "equation", "text": "$$\nf_{1}=(a_{1}+g)0.5\\prod_{i=1}^{m-1}x_{i},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "496 ", "page_idx": 14}, {"type": "equation", "text": "$$\nf_{j=2:m-1}=(a_{j}+g)(0.5\\prod_{i=1}^{m-j}x_{i})(1-x_{m-j+1}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "497 ", "page_idx": 14}, {"type": "equation", "text": "$$\nf_{m}=(a_{m}+g)0.5(1-x_{1}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "498 ", "page_idx": 14}, {"type": "equation", "text": "$$\ng=100\\left(k+\\sum_{i=1}^{k}\\left((z_{i}-0.5)^{2}-c o s\\left(20\\pi(z_{i}-0.5)\\right)\\right)\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "499 where $\\mathbf{z}$ is a vector consisting of the last $\\textit{k}=\\textit{d}-\\textit{m}+1$ variables in $\\mathbf{X}$ . In other words,   \n500 $\\textbf{z}=~\\{z_{1},\\ldots,z_{k}\\}~=~\\{x_{m},\\bar{~}...,x_{d}\\}$ . The variants of DTLZ1 introduce only one variable   \n501 $\\mathbf{a}\\in[0.1,5.0]^{m}$ in Eq.(8), Eq.(9), and Eq.(10), where $\\mathbf{a}=\\mathbf{1}$ in the original DTLZ1. For out-of-range   \n502 test, $\\mathbf{a}\\in[1.5,5.0]^{m}$ . ", "page_idx": 14}, {"type": "text", "text": "503 ", "page_idx": 14}, {"type": "text", "text": "504 DTLZ2: ", "text_level": 1, "page_idx": 14}, {"type": "equation", "text": "$$\nf_{1}=(a_{1}+g)\\prod_{i=1}^{m-1}c o s(\\frac{x_{i}\\pi}{b_{1}}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "505 ", "page_idx": 14}, {"type": "equation", "text": "$$\nf_{j=2:m-1}=(a_{j}+g)\\left(\\prod_{i=1}^{m-j}c o s(\\frac{x_{i}\\pi}{b_{j}})\\right)s i n(\\frac{x_{m-j+1}\\pi}{b_{j}}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "506 ", "page_idx": 14}, {"type": "equation", "text": "$$\nf_{m}=(a_{m}+g)s i n(\\frac{x_{1}\\pi}{b_{m}}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "507 ", "page_idx": 14}, {"type": "equation", "text": "$$\ng=\\sum_{i=1}^{k}(z_{i}-0.5)^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "508 The variants of DTLZ2 introduce two variables $\\mathbf{a}\\,\\in\\,[0.1,5.0]^{m}$ and $\\mathbf{b}\\,\\in\\,[0.5,2.0]^{m}$ in Eq.(12),   \n509 Eq.(13), and Eq.(14), where $\\textbf{a}=1$ and $\\mathbf{b}\\,=\\,2$ in the original DTLZ2. For out-of-range test,   \n510 $\\mathbf{a}\\in[1.5,5.0]^{m}$ , $\\mathbf{b}\\in[0.5,1.5]^{m}$ .   \n512 DTLZ3: The variants of DTLZ3 are generated using the same way as described in DTLZ2, except   \n513 the equation $g$ from Eq.(15) is replaced by the one from Eq.(11).   \n515 DTLZ4: The variants of DTLZ4 are generated using the same way as described in DTLZ2, except   \n516 all $x_{i}$ are replaced by $x_{i}^{100}$ .   \n518 DTLZ5: The variants of DTLZ5 are generated using the same way as described in DTLZ2, except   \n519 all x2, . . . , xm\u22121 are replaced by 12(+12+ggx)i . ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "521 DTLZ6: ", "page_idx": 14}, {"type": "equation", "text": "$$\ng=\\sum_{i=1}^{k}z_{i}^{0.1}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "522 The variants of DTLZ6 are generated using the same way as described in DTLZ5, except the equation   \n523 $g$ from Eq.(15) is replaced by the one from Eq.(16). ", "page_idx": 14}, {"type": "text", "text": "525 DTLZ7: ", "page_idx": 14}, {"type": "equation", "text": "$$\nf_{j=1:m-1}=x_{j}+a_{j},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\nf_{m}=(1+g)\\left(m-\\sum_{i=1}^{m-1}\\left(\\frac{f_{i}}{1+g}\\left(1+s i n(3\\pi f_{i})\\right)\\right)\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "527 ", "page_idx": 15}, {"type": "equation", "text": "$$\ng=a_{m}+9\\sum_{i=1}^{k}\\frac{z_{i}}{k}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "528 The variants of DTLZ7 introduce one variable a $\\in\\ [0.1,5.0]^{m}$ in Eq.(17) and Eq.(19), where   \n529 $a_{j=1:m-1}=0$ and $a_{m}=1$ in the original DTLZ7. For out-of-range test, $\\mathbf{a}\\in[1.5,5.0]^{m}$ . ", "page_idx": 15}, {"type": "text", "text": "530 D Effectiveness of Learning Experience ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "531 Evaluating the effectiveness of learning experiences aims to demonstrate that our MDKL model can   \n532 learn experience from related tasks and outperforms other meta-learning models. For this reason, the   \n533 experiment is designed to answer the following questions:   \n534 \u2022 Given a small dataset $S_{*}$ from target task $T_{*}$ , can MDKL learn experience from related tasks   \n535 and then generate a model that has the smallest MSE?   \n536 \u2022 If yes, which components of MDKL contribute to the effectiveness of learning experience?   \n537 Meta-learning or/and deep kernel learning? If not, why not?   \n538 To answer the two questions above, we consider two experiments to evaluate the effectiveness   \n539 of learning experience: amplitude prediction for unknown periodic sinusoid functions, and fuel   \n540 consumption prediction for a gasoline motor engine. The former is a few-shot regression problem   \n541 that motivates many meta-learning studies [10, 13, 39, 27], while the latter is a real-world regression   \n542 problem [53]. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "543 D.1 Effectiveness of Learning Experience: Sinusoid Function Regression ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "544 In the sinusoid regression experiment, we learn experience from a series of 1-dimensional sinusoid   \n545 functions: ", "page_idx": 15}, {"type": "equation", "text": "$$\ny=A s i n(w x+b)+\\epsilon,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "546 where the amplitude $A$ and phase $w$ of sine waves are varied between functions. The target is to   \n547 approximate an unknown sinusoid function with a small support dataset $S_{*}$ and the learned experience.   \n548 Clearly, by integrating experience with $S_{*}$ , we estimate parameters $(A,w,b)$ for an unknown sinusoid   \n549 function. As a result, the output $y$ of the given sinusoid function can be predicted once a query data $x$   \n550 is inputted. ", "page_idx": 15}, {"type": "text", "text": "551 D.1.1 Generation of Sinusoid Function Variants ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "552 As suggested in [10, 13], we set amplitude $A\\in[0.1,5.0]$ , frequency $w\\in[0.999,1.0]$ , phase $b\\in[0$ ,   \n553 $\\pi]$ , and Gaussian noise $\\epsilon\\sim(0,0.1)$ . Therefore, a sinusoid function can be generated by sampling   \n554 three parameters $(A,w,b)$ from their ranges uniformly. In total, $N_{m}=N=20000$ related sinusoid   \n555 functions are generated at random. ", "page_idx": 15}, {"type": "text", "text": "556 D.1.2 Experimental Setups ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "557 All data points $x$ are sampled from the range $\\in[-5.0$ , 5.0]. In the meta-learning procedure, both   \n558 support set and query set contain 5 data points. Hence, a dataset $D_{i}$ is sampled from each (related)   \n559 sinusoid function $T_{i}$ , and $|D_{i}|\\;=\\;|D_{m}|\\;=\\;10$ . Six experiments are conducted where $|S_{*}|\\;=$   \n560 $\\{2,3,5,10,20,30\\}$ data points are sampled from the target function. Considering Gaussian noise $\\epsilon$   \n561 could be relatively large when amplitude $A$ is close to 0.1, normalized mean squared error (NMSE) is   \n562 chosen as a performance indicator. NMSE is measured using a dataset that contains 100 data points   \n563 sampled uniformly from the $x$ range. ", "page_idx": 15}, {"type": "text", "text": "Table 3: Mean NMSE and standard deviation (in parentheses) of 30 runs on the amplitude regression of sinusoid function. GP [34] is a widely used surrogate in SAEAs, MAML [10], ALPaCA [13], and DKT [27] are meta-learning methods. GP_Adam is a GP model fitted by Adam optimizer. DKL is a deep kernel learning algorithm that adds a neural network to GP_Adam. MDKL_NN applies meta-learning to DKL, but no task-independent base kernel parameters are shared between related tasks. Support data points are used to train non-meta surrogates or adapt meta-learning surrogates. $\\leftrightarrow,\\,\\approx\\,$ , and \u2018\u2212\u2019 denote MDKL is statistically significantly superior to, almost equivalent to, and inferior to the compared modelling methods in the Wilcoxon rank sum test (significance level is 0.05), respectively. The last row counts the total win/tie/loss results. It shows that MDKL and DKT have lower NMSE than other models. The effectiveness of meta-learning on both the neural network and the base kernel has been demonstrated on this example. ", "page_idx": 16}, {"type": "table", "img_path": "08oUnmtj8Q/tmp/64311893b2a7bc423be3e1d022215fd983b65dc218804d4759b4ae786a48e59b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "564 D.1.3 Comparison methods ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "565 In this experiment, three families of modeling methods are compared with our MDKL model: ", "page_idx": 16}, {"type": "text", "text": "566 \u2022 Meta-learning methods that were proposed for regression tasks: MAML [10], ALPaCA   \n567 [13], and DKT [27]. The configurations of MAML, ALPaCA, and DKT are the same as   \n568 suggested in the original literature.   \n569 \u2022 Non-meta-learning method that is widely used for regression tasks: the GP model. We   \n570 choose a GP as a baseline since it is effective and more relevant to MDKL than other   \n571 non-meta-learning modeling methods. We set the range of base kernel parameters in the GP   \n572 model as $\\theta\\in[10^{\\overline{{-5}}},10]$ and $p\\in[1,2]$ .   \n573 \u2022 MDKL related methods that are designed to investigate which components of MDKL   \n574 contribute to the modeling performance: GP_Adam, DKL, and MDKL_NN. GP_Adam is a   \n575 GP model fitted by Adam optimizer. The combination of GP_Adam and a neural network   \n576 results in a kind of DKL algorithm. MDKL_NN is a meta-learning version of DKL, but it   \n577 learns only neural network parameters through meta-learning and has no task-independent   \n578 base kernel parameters. ", "page_idx": 16}, {"type": "text", "text": "579 D.1.4 Results and Analysis ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "580 Table 3 reports the statistical test results of the NMSE values achieved by comparison algorithms   \n581 in sinusoid function regression experiments. Each row lists the results obtained when the same   \n582 number of fitness evaluations $|S_{*}|$ are used to train models. The results of Wilcoxon rank sum test   \n583 between MDKL and other compared algorithms are listed in the last row. It can be observed that both   \n584 MDKL and DKT have achieved the smallest NMSE values on all tests in the comparison with other   \n585 meta-learning and non-meta-learning modeling methods.   \n586 Contributions of MDKL components are analyzed through statistical tests between MDKL related   \n587 methods. The statistical test results between DKL and GP_Adam are 5/1/0, showing that DKL is   \n588 preferable to GP_Adam when only a few data points are available for modeling. Hence, using a   \n589 neural network to build a deep kernel for GP is able to enhance the performance of modeling. When   \n590 meta-learning technique is applied to DKL, the statistical test results between MDKL_NN and DKL   \n591 are 3/3/0. The meta-learning of neural network parameters is necessary since it contributes to the   \n592 performance of MDKL. Further statistical test between MDKL and MDKL_NN gives results of 3/3/0,   \n593 indicating that the meta-learning of base kernel parameters is effective on this regression problem. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "594 D.2 Effectiveness of Learning Experience: Engine Performance Regression ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "595 In this subsection, we focus on a Brake Special Fuel Consumption (BSFC) regression task for a   \n596 gasoline motor engine [53], where BSFC is evaluated on a gasoline engine simulation (denoted by   \n597 $T_{*}$ ). ", "page_idx": 16}, {"type": "text", "text": "Table 4: Mean MSE and standard deviation (in parentheses) of 30 runs on the regression of engine fuel consumption. Support data points are used to train non-meta surrogates or adapt meta-learning surrogates. All results are normalized since the actual engine data is unable to be disclosed. The symbols $\\mathbf{\\omega}^{\\star}+\\mathbf{\\omega}^{\\star}$ \u2019, $\\surd\\approx\\mathrel{\\hat{\\,}}$ , \u2018\u2212\u2019 denote the win/tie/loss result of Wilcoxon rank sum test (significance level is 0.05) between MDKL and comparison modeling methods, respectively. The last row counts the total win/tie/loss results. ", "page_idx": 17}, {"type": "table", "img_path": "08oUnmtj8Q/tmp/d9694a4ed570d01f4596aa7e5aef18c7474450b75c88688f0e1a80a15a98b724.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "598 D.2.1 Experimental setups ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "599 The related tasks $T_{i}$ used in our experiment are $N=100$ gasoline engine models. These engine   \n600 models have different behaviors when compared with $T_{*}$ , but they share the basic features of gasoline   \n601 engines. All related tasks and the target task have the same six decision variables. Each related task   \n602 $T_{i}$ provides only 60 solutions, forming a dataset $D_{i}$ . The size of datasets used for meta-learning   \n603 $|D_{m}|$ is set to 40. Six tests are conducted where $\\vert S_{*}\\vert=\\{2,3,5,10,20,40\\}$ data points are sampled   \n604 from the real engine simulation $T_{*}$ . MSE is chosen as an indicator of modeling accuracy, which is   \n605 measured using a dataset consisting of 12500 data points that are sampled uniformly from the engine   \n606 decision space. The comparison algorithms are the same as described in Appendix D.1. ", "page_idx": 17}, {"type": "text", "text": "607 D.2.2 Results and analysis ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "608 The statistical test results of the MSE values achieved by comparison algorithms in BSFC regression   \n609 experiments are summarized in Table 4. Each row lists the results obtained when the same number   \n610 of fitness evaluations $|S_{*}|$ are used to train models. The results of Wilcoxon rank sum test between   \n611 MDKL and other compared algorithms are listed in the last row. It can be observed that MDKL and   \n612 MDKL_NN outperform other comparison modeling methods since they have achieved the smallest   \n613 MSE on all tests.   \n614 Additional Wilcoxon rank sum tests have been conducted between MDKL related algorithms to   \n615 answer our second question (results are not reported in Table 4). The statistical test results between   \n616 DKL and GP_Adam are 1/5/0, indicating that the neural network in DKL makes some contributions   \n617 to the performance of MDKL. The statistical test results between MDKL_NN and DKL are $6/0/0$ ,   \n618 demonstrating that the meta-learning of neural network parameters constructs a useful deep kernel   \n619 and contributes to the improvement of modeling accuracy. However, there is no significant difference   \n620 between the performance of MDKL and that of MDKL_NN, the meta-learning on base kernel   \n621 parameters does not play a critical role on this engine problem. In comparison, the meta-learning on   \n622 base kernel parameters is effective in sinusoid function regression experiments (see Appendix D.1).   \n623 In addition, the statistical test results between MDKL_NN and MAML are $4/2/0$ . Considering that   \n624 MAML is a neural network regressor learned through meta-learning, we can conclude that GP is an   \n625 essential component of our MDKL. In summary, all components in MDKL are necessary, they all   \n626 contribute to the effectiveness of learning experience.   \n627 The comparison experiments on sinusoid functions and the gasoline motor engine have demonstrated   \n628 the effectiveness of our MDKL modeling method in the learning of experience. Given a small   \n629 dataset of the target task, the model learned through MDKL method has the smallest MSE among   \n630 all comparison models. Additionally, the investigation between MDKL and its variants shows that   \n631 all components in MDKL have made their contributions to the effectiveness of learning experience.   \n632 However, similar to other meta-learning studies [10, 13], we have not defined the similarity between   \n633 tasks. In other words, the boundary between related tasks and unrelated tasks has not been defined.   \n634 This should be a topic of further study on meta-learning. Moreover, the relationship between task   \n635 similarity and modeling performance has not been investigated. Instead, we study the relationship   \n636 between task similarity and SAEA optimization performance in Section F, since our main focus is   \n637 the surrogate-assisted evolutionary optimization. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Table 5: Mean $\\mathrm{{IGD+}}$ values and standard deviation (in parentheses) obtained from 30 runs on the DTLZ problems. MOEA/D-FS and the comparison algorithms initialize their surrogates with 10, 100 samples, respectively. Extra 50 evaluations are allowed in the further optimization. ", "page_idx": 18}, {"type": "table", "img_path": "08oUnmtj8Q/tmp/d7f677743a9dcdd5b5608b0c62d2b09419efc2b53763e931547a785dd9d907ba.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "638 E Additional Details on Expensive Multi-Objective Optimization ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "639 E.1 Comparison algorithms ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "640 As explained in Section B, our FSEO framework is compatible with regression-based SAEAs. Hence,   \n641 we select MOEA/D-EGO [51] as an example and replace its GP surrogates by our MDKL surrogates.   \n642 The resulting algorithm is denoted as MOEA/D-FS. Note that it is not necessary to specially select a   \n643 newly proposed regression-based SAEA as our example, our main objective is to save evaluations   \n644 with experience and observe if there is any damage to the optimization performance caused by the   \n645 saving of evaluations. Therefore, it does not make any difference which regression-based SAEA   \n646 or BO we choose as our example. Additionally, to demonstrate the improvement of optimization   \n647 performance caused by using experience on DTLZ problems is significant, several state-of-the-art   \n648 SAEAs and MOBO are also compared as baselines, including ParEGO [19], K-RVEA [7], CSEA [25],   \n649 OREA [48], KTA2 [33], ESBCEO [3], and KMOEA-TIC [28]. Among these algorithms, ParEGO,   \n650 K-RVEA, KTA2, KMOEA-TIC use regression-based surrogates, CSEA uses a classification-based   \n651 surrogate, OREA employs an ordinal-regression-based surrogate, and ESBCEO is a recently proposed   \n652 MOBO.   \n653 We implemented the FSEO framework, MOEA/D-EGO, ParEGO, and OREA, while the code of   \n654 K-RVEA, CSEA, KTA2, and ESBCEO [3] is available on PlatEMO [37], an open source MATLAB   \n655 platform for evolutionary multi-objective optimization. The code of KMOEA-TIC [28] is obtained   \n656 from its authors. To make a fair comparison, all comparison algorithms share the same initial dataset   \n657 $S_{*}$ in an independent run. We also set $\\pmb{\\theta}\\in[10^{-5},10\\hat{0}]^{d}$ and $\\mathbf{p}=2$ for all GP surrogates as suggested   \n658 in [33], these GP surrogates are implemented through DACE [31]. Other configurations are the same   \n659 as suggested in their original literature. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "660 E.2 Result Table and Analysis of Expensive Multi-Objective Optimization ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "661 The experience learned from related tasks makes MOEA/D-EGO more competitive when compared   \n662 to other SAEAs. The use of MDKL surrogates results in significantly smaller $\\mathrm{IGD+}$ values on DTLZ1,   \n663 DTLZ2, DTLZ3, and DTLZ5 than before. As a result, MOEA/D-FS achieves the smallest $\\mathrm{IGD+}$   \n664 values on DTLZ2 and DTLZ3, and its optimization results on DTLZ1 and DTLZ5 are much closer   \n665 to the best optimization results (e.g. results obtained by ParEGO and OREA) than MOEA/D-EGO.   \n666 Although MOEA/D-FS does not achieve the smallest $\\mathrm{IGD+}$ values on all DTLZ problems, it should   \n667 be noted that MOEA/D-FS is still the best algorithm among comparison SAEAs due to its overall   \n668 performance. Table 5 shows that no comparison SAEA outperforms MOEA/D-FS on three DTLZ   \n669 problems, but MOEA/D-FS outperforms all comparison SAEAs on at least three DTLZ problems.   \n670 Furthermore, the $\\mathrm{IGD+}$ values of MOEA/D-FS are achieved with an evaluation budget of 60, while   \n671 the $\\mathrm{IGD+}$ values of other SAEAs are reached with a cost of 150 evaluations (see Table 2). ", "page_idx": 18}, {"type": "text", "text": "672 E.3 Result Tables and Figures in IGD and HV Metrics ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "673 The performance of our method and the comparison algorithms are also evaluated on inverted   \n674 generational distance (IGD) [4] and Hypervolume (HV) [55] metrics.   \n675 Results in IGD values are reported in Table 6 and Fig. 5. A smaller IGD value indicates a better   \n676 optimization result.   \n677 Results in HV values are reported in Table 7 and Fig. 6. A larger HV value indicates a better   \n678 optimization result. ", "page_idx": 18}, {"type": "text", "text": "Table 6: Mean IGD values and standard deviation (in parentheses) obtained from 30 runs on 7 DTLZ problems. MOEA/D-FS and comparison algorithms initialize their surrogates with 10, 100 samples, respectively. Extra 50 evaluations are allowed in the further optimization. $\"+\"$ , \u2018\u2248\u2019, and \u2018\u2212\u2019 denote MOEA/D-FS is statistically significantly superior to, almost equivalent to, and inferior to the compared algorithms in the Wilcoxon rank sum test (significance level is 0.05), respectively. The last row counts the total win/tie/loss results. ", "page_idx": 19}, {"type": "table", "img_path": "08oUnmtj8Q/tmp/95912dfb010e90ce76a2ad933e00170305bd4052bfca7cd479a09230ff4e0311.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "08oUnmtj8Q/tmp/36c544f93ee3695c04d4474d9d3bd6ac5811a973583d41f01d41c0295717db76.jpg", "img_caption": ["Figure 5: IGD curves averaged over 30 runs on 7 DTLZ problems. Solid lines are mean values, while shadows are error regions. Upper: DTLZ1, DTLZ2, DTLZ3, DTLZ4. Lower: DTLZ5, DTLZ6, DTLZ7. MOEA/D-FSs and comparison algorithms initialize their surrogates with 10, 100 samples, respectively. Extra 50 evaluations are allowed in the further optimization. Note that \u2018FS(out)\u2019 indicates the target task is excluded from the range of related tasks during the meta-learning procedure). X-axis denotes the number of evaluations used after the surrogate initialization. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 7: Mean HV values and standard deviation (in parentheses) obtained from 30 runs on 7 DTLZ problems. MOEA/D-FS and comparison algorithms initialize their surrogates with 10, 100 samples, respectively. Extra 50 evaluations are allowed in the further optimization. $\\cdot+;\\cdot\\approx$ , and \u2018\u2212\u2019 denote MOEA/D-FS is statistically significantly superior to, almost equivalent to, and inferior to the compared algorithms in the Wilcoxon rank sum test (significance level is 0.05), respectively. The last row counts the total win/tie/loss results. ", "page_idx": 19}, {"type": "table", "img_path": "08oUnmtj8Q/tmp/14dad8f4d1986f387b63aea82a22ee9055798a3ce8e7dfbd932b027ba8bdf1db.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "679 E.4 Performance on Expensive Multi-Objective Optimization Under the Same Evaluation 680 Budget ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "681 The statistical test results reported in the last row of Table 5 show that ParEGO [19] and OREA [48]   \n682 are the best two comparison algorithms when compared with our MOEA/D-FS. In this subsection,   \n683 we evaluate the performance of MOEA/D-FS when no extra evaluation is saved. For this purpose, we   \n684 compare the optimization performance of these three SAEAs under the same evaluation budget: 10   \n685 evaluations $(1d)$ for surrogate initialization and 50 evaluations for further optimization. The statistical   \n686 test results are reported in Table 8. It can be seen that our MOEA/D-FS generally outperforms the ", "page_idx": 19}, {"type": "image", "img_path": "08oUnmtj8Q/tmp/763141fd58ec17f2a0af7970851536bce266c9d79c77ac624f95275850a29367.jpg", "img_caption": ["Figure 6: HV curves averaged over 30 runs on 7 DTLZ problems. Solid lines are mean values, while shadows are error regions. Upper: DTLZ1, DTLZ2, DTLZ3, DTLZ4. Lower: DTLZ5, DTLZ6, DTLZ7. MOEA/D-FSs and comparison algorithms initialize their surrogates with 10, 100 samples, respectively. Extra 50 evaluations are allowed in the further optimization. Note that \u2018FS(out)\u2019 indicates the target task is excluded from the range of related tasks during the meta-learning procedure). X-axis denotes the number of evaluations used after the surrogate initialization. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 8: Mean $\\mathrm{{IGD+}}$ values and standard deviation (in parentheses) obtained from 30 runs on DTLZ problems. MOEA/D-FS is compared with ParEGO and OREA under the same evaluation budget: 10 evaluations for surrogate initialization and 50 evaluations for the optimization process. $\\leftrightarrow,\\leftrightarrow$ , and \u2018\u2212\u2019 denote MOEA/D-FS is statistically significantly superior to, almost equivalent to, and inferior to the compared two algorithms in the Wilcoxon rank sum test (significance level is 0.05), respectively. The last row counts the total win/tie/loss results. ", "page_idx": 20}, {"type": "table", "img_path": "08oUnmtj8Q/tmp/6a649902ad60a2340f8257a1321b11c20fb17097f8b4c1e78fdbedfa9d406664.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "687 compared SAEAs when only $1d$ evaluations are used to initialize their surrogates. The effectiveness   \n688 of our FSEO framework has been demonstrated. Note that OREA is an evolutionary algorithm   \n689 assisted by ordinal-regression-based surrogates. Currently, our FSEO framework is applicable to the   \n690 SAEAs working with regression-based surrogates. The meta-learning of ordinal-regression models   \n691 can be a topic of further research. ", "page_idx": 20}, {"type": "text", "text": "692 F Influence of Task Similarity ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "693 In real-world applications, it is optimistic to assume that some related tasks are very similar to the   \n694 target task. A more common situation is that all related tasks have limited similarity to the target task.   \n695 To investigate the relationship between task similarity and FSEO optimization performance, we also   \n696 test the performance in an \u2018out-of-range\u2019 situation, where the original DTLZ is excluded from the   \n697 range of DTLZ variants during the MDKL meta-learning procedure. As a result, only the DTLZ   \n698 variants that are quite different from the original DTLZ problem can be used to learn experience. The   \n699 \u2018out-of-range\u2019 situation eliminates the probability that MDKL surrogates benefit greatly from the   \n700 DTLZ variants that are very similar to the original DTLZ problem. Detailed definitions of the related   \n701 tasks used in the \u2018out-of-range\u2019 situation are given in Appendix C. Apart from the related tasks used,   \n702 the remaining experimental setups are the same as the setups described in Section 5.1. For the sake   \n703 of convenience, we denote the situation we tested in Section 5.1 as \u2018in-range\u2019 below.   \n704 The statistical test results reported in Table 9 show that the \u2018out-of-range\u2019 situation achieves competi  \n705 tive $\\mathrm{IGD+}$ values to the \u2018in-range\u2019 situation on all 7 test instances. This suggests that related tasks   \n706 that are very similar to the target task have a limited impact on the optimization performance of our   \n707 FSEO framework. Useful experience can be learned from related tasks that are not very similar to   \n708 the target task. Crucially, when comparing the performance of the \u2018out-of-range\u2019 situation and that   \n709 of MOEA/D-EGO, we can still observe competitive or improved optimization results on 6 DTLZ   \n710 problems (see Table 9, the row titled by \u2018vs MOEA/D-EGO\u2019, or Fig. 3). Moreover, it can be seen   \n711 from the last row of Table 9 that the \u2018out-of-range\u2019 situation achieves better/competitive/worse $\\mathrm{{IGD+}}$   \n712 values than all compared SAEAs on 27/7/8 test instances. In comparison, the corresponding statistical   \n713 test results for the \u2018in-range\u2019 situation are 26/9/7. The difference between these statistical test results   \n714 is not significant.   \n715 A study on the \u2018out-of-range\u2019 situation in the context of extremely expensive multi-objective opti  \n716 mization is presented in Appendix H.2. Consistent results can be observed from Table 12 and Fig.   \n717 7.   \n718 Consequently, related tasks that are very similar to the target task are not essential to the optimization   \n719 performance of our FSEO framework. In the \u2018out-of-range\u2019 situation, our MOEA/D-FS can still   \n720 achieve competitive or better optimization results than MOEA/D-EGO while using only $1d$ samples   \n721 for surrogate initialization. ", "page_idx": 20}, {"type": "table", "img_path": "08oUnmtj8Q/tmp/181b79e58396f2c272a26ff14395c67a20446f6864cf68690cd93e4337842168.jpg", "table_caption": ["Table 9: Mean $\\mathrm{{IGD+}}$ values and standard deviation (in parentheses) obtained from 30 runs on DTLZ problems. Both MOEA/D-FSs initialize their surrogates with 10 samples, extra 50 evaluations are allowed in the further optimization. The last two rows count the statistical test results between MOEA/D-FSs and other compared algorithms. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "722 G Influence of the Size of Datasets Used in Meta-Learning ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "723 We also investigated the performance of our FSEO framework when different sizes of datasets $|D_{m}|$   \n724 are used in the meta-learning procedure. The experimental setups are the same as the setups of   \n725 MOEA/D-FS in Section 5.1 except for $|D_{m}|$ .   \n726 It is evident from Table 10 that when each DTLZ variant provides $|D_{m}|\\,=\\,60$ samples for the   \n727 meta-learning of MDKL surrogates, the performance of both MOEA/D-FSs are improved on 2 or   \n728 3 DTLZ problems. Particularly, a significant improvement can be observed from the optimization   \n729 results of DTLZ7. As we discussed in Section 5.1, the poor performance of our experience-based   \n730 optimization on DTLZ7 is caused by the small size of $D_{m}$ . Optimal solutions have few chances to   \n731 be included in a small $D_{m}$ , which makes $D_{m}$ fails to provide the experience about the discontinuity   \n732 of optimal regions. In comparison, the experience of \u2018optimal regions\u2019 can be learned from large   \n733 datasets $D_{m}$ and thus the optimization results are improved significantly.   \n734 In conclusion, for our FSEO framework, a large $D_{m}$ for the meta-learning procedure indicates   \n735 more useful experience can be learned from related tasks, which further improves the performance   \n736 of experience-based optimization. Therefore, when applying our FSEO framework to real-world ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Table 10: Mean $\\mathrm{IGD+}$ values and standard deviation (in parentheses) obtained from 30 runs on DTLZ problems. 10 samples are used for initialization and extra 50 evaluations are allowed in the further optimization. $|D_{m}|$ is the size of the dataset collected from each related task. ", "page_idx": 22}, {"type": "table", "img_path": "08oUnmtj8Q/tmp/abcde99a2225097bfc98a48e6766321f4624a6802aaf7135f571d23a47958a68.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 11: Mean $\\mathrm{IGD+}$ values and standard deviation (in parentheses) obtained from 30 runs on the DTLZ problems. MOEA/D-FS and the comparison algorithms initialize their surrogates with 10, 60 samples, respectively. Extra 30 evaluations are allowed in the further optimization. $\\cdot+;\\cdot\\approx$ , and \u2018\u2212\u2019 denote MOEA/D-FS is statistically significantly superior to, equivalent to, and inferior to the compared algorithms in the Wilcoxon rank sum test (significance level is 0.05), respectively. The last row is the total win/tie/loss results. Performance improvement can be observed from the comparisons between MOEA/D-FS and MOEA/D-EGO, while 50 evaluations are saved from surrogate initialization. ", "page_idx": 22}, {"type": "table", "img_path": "08oUnmtj8Q/tmp/88148e7c0a52f45c63703965fdc9ab1b4f0b616afd67f176d2c7ff603262dc55.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "737 optimization problems, it is preferable to collect more data from related tasks for experience learning.   \n738 ", "page_idx": 22}, {"type": "text", "text": "739 H Experiments on Extremely Expensive Multi-Objective Optimization ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "740 In this section, we investigate the performance of our FSEO framework in the context of extremely   \n741 expensive optimization, where the allowed ftiness evaluations on target problems are fewer than that   \n742 in the experiment carried out in Sections 5.1 of the main file and Appendix F. ", "page_idx": 22}, {"type": "text", "text": "743 H.1 Performance between Comparison Algorithms ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "744 We conduct the experiment described in Section 5.1 of the main file, but with a smaller evaluation   \n745 budget than the budget listed in Table 2. The size of the initial dataset $S_{*}$ is set to 10, 60 for our   \n746 MOEA/D-FS and comparison algorithms, respectively. 30 extra evaluations for further optimization   \n747 are allowed. The total evaluation budget is 40, 90 for our MOEA/D-FS and comparison algorithms,   \n748 respectively. ", "page_idx": 22}, {"type": "text", "text": "749 The aim of this subsection is to answer the question below: ", "page_idx": 22}, {"type": "text", "text": "750 \u2022 Is our FSEO framework more suitable for the optimization problems in which evaluations are   \n751 extremely expensive? In other words, will the advantage of our FSEO framework become   \n752 more prominent if the optimization problems allow a smaller evaluation budget?   \n753 The comparison results reported in Fig. 7 and Table 11 show that MOEA/D-FS has achieved   \n754 competitive or smaller $\\mathrm{IGD+}$ values than MOEA/D-EGO on all DTLZ problems except for DTLZ7.   \n755 Meanwhile, $5d$ evaluations have been saved.   \n756 Consistent with the results discussed in Section 5.1 of the main file, MOEA/D-FS fails to achieve a   \n757 competitive result compared to MOEA/D-EGO on DTLZ7 since experience is learned from small   \n758 datasets collected from related tasks. Although we set a different evaluation budget for all SAEAs,   \n759 the size of datasets for meta-learning $|D_{m}|$ has not been modified. However, it can be observed from   \n760 the statistical test results (see the last row of Tables 5 and 11) that our MOEA/D-FS outperforms   \n761 the comparison algorithms on 26, 29 test instances when the total evaluation budget of comparison   \n762 algorithms is set to 150, 90, respectively. This answers the question we raised before: The advantage   \n763 of our FSEO framework is more prominent in the extremely expensive problems where a smaller   \n764 evaluation budget is allowed. The comparison between the results obtained from Tables 5 and 11 has   \n765 demonstrated that our FSEO framework is preferable when solving optimization problems within a   \n766 very limited evaluation budget. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "image", "img_path": "08oUnmtj8Q/tmp/caae1758a6d08b964b45afdbeee7f32053caaea8e2ff35644f82aea8e559fb08.jpg", "img_caption": ["Figure 7: $\\mathrm{IGD+}$ curves averaged over 30 runs on 7 DTLZ problems. Solid lines are mean values, while shadows are error regions. Upper: DTLZ1, DTLZ2, DTLZ3, DTLZ4. Lower: DTLZ5, DTLZ6, DTLZ7. MOEA/D-FSs and comparison algorithms initialize their surrogates with 10, 60 samples, respectively. Extra 30 evaluations are allowed in the further optimization. Note that \u2018FS(out) indicates the target task is excluded from the range of related tasks during the meta-learning procedure. X-axis denotes the number of evaluations used after the surrogate initialization. In comparison to MOEA/D-EGO, both MOEA/D-FSs achieve smaller or competitive $\\mathrm{IGD+}$ values on all DTLZ test problems except for DTLZ7, while 50 evaluations are saved with the assistance from related tasks. Moreover, MOEA/D-FSs achieve the smallest $\\mathrm{IGD+}$ values on DTLZ2, DTLZ3, DTLZ4, DTLZ5 and DTLZ6. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "767 H.2 Out-Of-Range Analysis on Extremely Expensive Optimization ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "768 In Section F of the main file, we carried out an experiment to study the influence of task similarity   \n769 on the performance of experience-based expensive multi-objective optimization. The optimization   \n770 results obtained from the \u2018in-range\u2019 and the \u2018out-of-range\u2019 situations are compared. In this subsection,   \n771 we conduct an experiment to investigate the difference between the \u2018in-range\u2019 and the \u2018out-of-range   \n772 situations for extremely expensive multi-objective optimization. The experimental setups are the   \n773 same as the setups described in Section F of the main flie, except the allowed ftiness evaluation budget   \n774 is the same as described in Appendix H.1.   \n775 Table 12 gives the statistical test results, it can be seen that the \u2018out-of-range\u2019 situation achieves   \n776 competitive $\\mathrm{IGD+}$ values to the \u2018in-range\u2019 situation on all 7 test instances. In comparison to MOEA/D  \n777 EGO, the experience gained in the \u2018out-of-range\u2019 situation leads to competitive or smaller $\\mathrm{IGD+}$   \n778 values on 6 DTLZ problems. Furthermore, similar results can be observed in the last row of Table 12,   \n779 the \u2018out-of-range\u2019 situation achieves better/competitive/worse $\\mathrm{IGD+}$ values than all compared SAEAs   \n780 on 28/9/5 test instances. In comparison, the \u2018in-range\u2019 situation achieves better/competitive/worse   \n781 $\\mathrm{IGD+}$ values than all compared SAEAs on 29/8/5 test instances. There is only a minor difference   \n782 between the optimization results obtained in two situations. These observations are consistent with   \n783 the conclusions we made in Section F of the main file. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Table 12: Mean $\\mathrm{IGD+}$ values and standard deviation (in parentheses) obtained from 30 runs on DTLZ problems. \u2018Out-of-range\u2019 indicates the target task is excluded from the range of related tasks during the meta-learning procedure. Both MOEA/D-FSs initialize their surrogates with 10 samples, extra 30 evaluations are allowed in the further optimization. $\\leftrightarrow,\\;\\approx\\,$ , and \u2018\u2212\u2019 denote the result of the \u2018out-of-range\u2019 situation is statistically significantly superior to, almost equivalent to, and inferior to that of the \u2018in-range\u2019 situation in the Wilcoxon rank sum test (significance level is 0.05), respectively. The last two rows count the statistical test results between MOEA/D-FSs and other compared algorithms. ", "page_idx": 24}, {"type": "table", "img_path": "08oUnmtj8Q/tmp/e6bf80ce6f8102890f09676972ea9bc69b2bbf75d66f3812939e7643c3dc50f7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "784 H.3 Result Tables and Figures in IGD and HV Metrics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "785 Results in IGD values are reported in Table 13 and Fig. 8. A smaller IGD value indicates a better optimization result. ", "page_idx": 24}, {"type": "text", "text": "Table 13: Mean IGD values and standard deviation (in parentheses) obtained from 30 runs on 7 DTLZ problems. MOEA/D-FS and comparison algorithms initialize their surrogates with 10, 60 samples, respectively. Extra 30 evaluations are allowed in the further optimization. $\\cdot+;\\cdot\\approx$ , and \u2018\u2212\u2019 denote MOEA/D-FS is statistically significantly superior to, almost equivalent to, and inferior to the compared algorithms in the Wilcoxon rank sum test (significance level is 0.05), respectively. The last row counts the total win/tie/loss results. ", "page_idx": 24}, {"type": "image", "img_path": "08oUnmtj8Q/tmp/29321a9a5cfc1587b89f7520508910a840d3549f71c056002cbb48dfc2d8266f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "787 Results in HV values are reported in Table 14 and Fig. 9. A larger HV value indicates a better optimization result. ", "page_idx": 24}, {"type": "text", "text": "Table 14: Mean HV values and standard deviation (in parentheses) obtained from 30 runs on 7 DTLZ problems. MOEA/D-FS and comparison algorithms initialize their surrogates with 10, 60 samples, respectively. Extra 30 evaluations are allowed in the further optimization. $\\cdot+;\\cdot\\approx$ , and \u2018\u2212\u2019 denote MOEA/D-FS is statistically significantly superior to, almost equivalent to, and inferior to the compared algorithms in the Wilcoxon rank sum test (significance level is 0.05), respectively. The last row counts the total win/tie/loss results. ", "page_idx": 24}, {"type": "image", "img_path": "08oUnmtj8Q/tmp/ee22a98ff87b87d4e471c447651f717b32cf48a4e40b73e4b1eb50b5a09b0a41.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "788 ", "page_idx": 24}, {"type": "text", "text": "789 I Summary of Experiments ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "790 Our computational studies have demonstrated the following: First, we provide empirical evidence to   \n791 show the effectiveness of learning experience: The meta-learning of neural network parameters and   \n792 base kernel parameters are essential to the modeling accuracy of a MDKL model. As a result, our   \n793 MDKL model outperforms the compared meta-learning modeling and non-meta-learning modeling   \n794 methods on both the engine fuel consumption regression task and the sinusoid function regression   \n795 task.   \n796 Second, we demonstrate the main contribution of this work: In most situations, the proposed FSEO   \n797 framework can assist regression-based SAEAs to reach competitive or even better optimization   \n798 results, while the cost of surrogate initialization is only $1d$ samples. Due to the effectiveness of   \n799 saving evaluations, our FSEO framework is preferable to other SAEAs when solving problems within   \n800 a very limited evaluation budget. Moreover, some empirical guidelines are concluded to help the   \n801 application of our FSEO framework. For the influence of task similarity, we find that related tasks that   \n802 are very similar to the target task are not necessary to the application of our approach. The influence   \n803 of these similar tasks on the optimization performance is limited. Our FSEO framework can achieve   \n804 competitive results without datasets from very similar related tasks. Besides, for the related tasks   \n805 used for meta-learning, we have demonstrated that more useful experience can be learned if more   \n806 data points are sampled from related tasks.   \n807 Third, the effectiveness of our FSEO framework is validated on a real-world engine calibration   \n808 problem. Competitive or better results are achieved on the objective and constraint functions, while   \n809 $1d$ samples are used to initialize surrogates. Therefore, our FSEO framework can also be applied to   \n810 optimization scenarios such as single-objective optimization and constrained optimization. ", "page_idx": 24}, {"type": "image", "img_path": "08oUnmtj8Q/tmp/617c308095abe614a5184f9236a0f9affa6c62b51ce87bab5244409ffbb5c5a1.jpg", "img_caption": ["Figure 8: IGD curves averaged over 30 runs on 7 DTLZ problems. Solid lines are mean values, while shadows are error regions. Upper: DTLZ1, DTLZ2, DTLZ3, DTLZ4. Lower: DTLZ5, DTLZ6, DTLZ7. MOEA/D-FSs and comparison algorithms initialize their surrogates with 10, 60 samples, respectively. Extra 30 evaluations are allowed in the further optimization. Note that \u2018FS(out)\u2019 indicates the target task is excluded from the range of related tasks during the meta-learning procedure. X-axis denotes the number of evaluations used after the surrogate initialization. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "08oUnmtj8Q/tmp/1ffa9f33d747c59878984f35f01906905520e9f1d9932474a1cda7ce46d1e391.jpg", "img_caption": ["Figure 9: HV curves averaged over 30 runs on 7 DTLZ problems. Solid lines are mean values, while shadows are error regions. Upper: DTLZ1, DTLZ2, DTLZ3, DTLZ4. Lower: DTLZ5, DTLZ6, DTLZ7. MOEA/D-FSs and comparison algorithms initialize their surrogates with 10, 60 samples, respectively. Extra 30 evaluations are allowed in the further optimization. Note that \u2018FS(out)\u2019 indicates the target task is excluded from the range of related tasks during the meta-learning procedure. X-axis denotes the number of evaluations used after the surrogate initialization. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "811 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "812 1. Claims   \n813 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n814 paper\u2019s contributions and scope?   \n815 Answer: [Yes]   \n816 Justification: Claims we made accurately reflect the paper\u2019s contributions and scope.   \n817 Guidelines:   \n818 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n819 made in the paper.   \n820 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n821 contributions made in the paper and important assumptions and limitations. A No or   \n822 NA answer to this question will not be perceived well by the reviewers.   \n823 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n824 much the results can be expected to generalize to other settings.   \n825 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n826 are not attained by the paper.   \n827 2. Limitations   \n828 Question: Does the paper discuss the limitations of the work performed by the authors?   \n829 Answer: [Yes]   \n830 Justification: See Appendix B.   \n831 Guidelines:   \n832 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n833 the paper has limitations, but those are not discussed in the paper.   \n834 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n835 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n836 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n837 model well-specification, asymptotic approximations only holding locally). The authors   \n838 should reflect on how these assumptions might be violated in practice and what the   \n839 implications would be.   \n840 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n841 only tested on a few datasets or with a few runs. In general, empirical results often   \n842 depend on implicit assumptions, which should be articulated.   \n843 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n844 For example, a facial recognition algorithm may perform poorly when image resolution   \n845 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n846 used reliably to provide closed captions for online lectures because it fails to handle   \n847 technical jargon.   \n848 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n849 and how they scale with dataset size.   \n850 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n851 address problems of privacy and fairness.   \n852 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n853 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n854 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n855 judgment and recognize that individual actions in favor of transparency play an impor  \n856 tant role in developing norms that preserve the integrity of the community. Reviewers   \n857 will be specifically instructed to not penalize honesty concerning limitations.   \n858 3. Theory Assumptions and Proofs   \nQuestion: For each theoretical result, does the paper provide the full set of assumptions and ", "page_idx": 27}, {"type": "text", "text": "a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "862 Justification: Not applicable.   \n863 Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "874 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "875 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n876 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n77 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Experimental setups are described in detail. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Justification: Will release our code after acceptation, or we can provide the code if any reviewers are interested in it during the review process. Anyway, the details about the code have already described in the paper. ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We have described all the details about of experiments. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We have conducted statistical tests in our experiments, error bars are plotted in figures. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [No] ", "page_idx": 30}, {"type": "text", "text": "Justification: We did not provide information about compute workers and memory since our experiments do not have specific requirements on memory or other computation resource. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "968   \n969   \n970   \n971   \n972   \n973   \n974   \n975   \n976   \n977   \n978   \n979   \n980   \n981   \n982   \n983   \n984   \n985   \n986   \n987   \n988   \n989   \n990   \n991   \n992   \n993   \n994   \n995   \n996   \n997   \n998   \n999   \n1000   \n1001   \n1002   \n1003   \n1004   \n1005   \n1006   \n1007   \n1008   \n1009   \n1010   \n1011   \n1012   \n1013   \n1014   \n1015   \n1016   \n1017   \n1018 ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Not applicable. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [No] ", "page_idx": 30}, {"type": "text", "text": "Justification: We do not think optimization algorithm can cause any negative social impacts. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [No] ", "page_idx": 31}, {"type": "text", "text": "Justification: Code will be released after acceptation, it would be open access, no safeguards are required. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "1019   \n1020   \n1021   \n1022   \n1023   \n1024   \n1025   \n1026   \n1027   \n1028   \n1029   \n1030   \n1031   \n1032   \n1033   \n1034   \n1035   \n1036   \n1037   \n1038   \n1039   \n1040   \n1041   \n1042   \n1043   \n1044   \n1045   \n1046   \n1047   \n1048   \n1049   \n1050   \n1051   \n1052   \n1053   \n1054   \n1055   \n1056   \n1057   \n1058   \n1059   \n1060   \n1061   \n1062   \n1063   \n1064   \n1065   \n1066   \n1067   \n1068   \n1069   \n1070   \n1071 ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We cited the algorithm platform and the data we used in our paper. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 31}, {"type": "text", "text": "1072 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n1073 the asset\u2019s creators.   \n1074 13. New Assets   \n1075 Question: Are new assets introduced in the paper well documented and is the documentation   \n1076 provided alongside the assets?   \n1077 Answer: [NA]   \n1078 Justification: We did not introduce any new assets.   \n1079 Guidelines:   \n1080 \u2022 The answer NA means that the paper does not release new assets.   \n1081 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n1082 submissions via structured templates. This includes details about training, license,   \n1083 limitations, etc.   \n1084 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n1085 asset is used.   \n1086 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n1087 create an anonymized URL or include an anonymized zip file.   \n1088 14. Crowdsourcing and Research with Human Subjects   \n1089 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n1090 include the full text of instructions given to participants and screenshots, if applicable, as   \n1091 well as details about compensation (if any)?   \n1092 Answer: [NA]   \n1093 Justification: We do not have any experiments or research with human subjects.   \n1094 Guidelines:   \n1095 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1096 human subjects.   \n1097 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n1098 tion of the paper involves human subjects, then as much detail as possible should be   \n1099 included in the main paper.   \n1100 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n1101 or other labor should be paid at least the minimum wage in the country of the data   \n1102 collector.   \n1103 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n1104 Subjects   \n1105 Question: Does the paper describe potential risks incurred by study participants, whether   \n1106 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n1107 approvals (or an equivalent approval/review based on the requirements of your country or   \n1108 institution) were obtained?   \n1109 Answer: [NA]   \n1110 Justification: We do not have any experiments or research with human subjects.   \n1111 Guidelines:   \n1112 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1113 human subjects.   \n1114 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n1115 may be required for any human subjects research. If you obtained IRB approval, you   \n1116 should clearly state this in the paper.   \n1117 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n1118 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n1119 guidelines for their institution.   \n120 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n1121 applicable), such as the institution conducting the review. ", "page_idx": 32}]