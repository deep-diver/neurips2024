[{"figure_path": "08oUnmtj8Q/tables/tables_4_1.jpg", "caption": "Table 3: Mean NMSE and standard deviation (in parentheses) of 30 runs on the amplitude regression of sinusoid function. GP [34] is a widely used surrogate in SAEAS, MAML [10], ALPaCA [13], and DKT [27] are meta-learning methods. GP_Adam is a GP model fitted by Adam optimizer. DKL is a deep kernel learning algorithm that adds a neural network to GP_Adam. MDKL_NN applies meta-learning to DKL, but no task-independent base kernel parameters are shared between related tasks. Support data points are used to train non-meta surrogates or adapt meta-learning surrogates. '+', '\u2248', and '-' denote MDKL is statistically significantly superior to, almost equivalent to, and inferior to the compared modeling methods in the Wilcoxon rank sum test (significance level is 0.05), respectively. The last row counts the total win/tie/loss results. It shows that MDKL and DKT have lower NMSE than other models. The effectiveness of meta-learning on both the neural network and the base kernel has been demonstrated on this example.", "description": "This table presents the results of a comparison of several machine learning models for the amplitude regression task.  It compares the performance of the proposed MDKL model with other meta-learning and non-meta-learning approaches including GP, MAML, ALPaCA, DKT, GP_Adam, DKL, and MDKL_NN. The results are measured using NMSE (normalized mean squared error) and statistical significance tests are performed to indicate the superior performance of MDKL and DKT over others.", "section": "D.1 Effectiveness of Learning Experience: Sinusoid Function Regression"}, {"figure_path": "08oUnmtj8Q/tables/tables_5_1.jpg", "caption": "Table 3: Mean NMSE and standard deviation (in parentheses) of 30 runs on the amplitude regression of sinusoid function. GP [34] is a widely used surrogate in SAEAS, MAML [10], ALPaCA [13], and DKT [27] are meta-learning methods. GP_Adam is a GP model fitted by Adam optimizer. DKL is a deep kernel learning algorithm that adds a neural network to GP_Adam. MDKL_NN applies meta-learning to DKL, but no task-independent base kernel parameters are shared between related tasks. Support data points are used to train non-meta surrogates or adapt meta-learning surrogates. '+', '\u2248', and '-' denote MDKL is statistically significantly superior to, almost equivalent to, and inferior to the compared modelling methods in the Wilcoxon rank sum test (significance level is 0.05), respectively. The last row counts the total win/tie/loss results. It shows that MDKL and DKT have lower NMSE than other models. The effectiveness of meta-learning on both the neural network and the base kernel has been demonstrated on this example.", "description": "This table presents the results of the amplitude regression experiment using various methods, including GP, MAML, ALPaCA, DKT, MDKL_NN, and MDKL (the proposed method).  It compares the mean NMSE (normalized mean squared error) and standard deviation across 30 runs for different numbers of support data points.  Statistical significance (Wilcoxon rank sum test) is indicated to show the relative performance of MDKL against other methods. The table highlights the effectiveness of MDKL and demonstrates the impact of meta-learning on both neural networks and base kernels.", "section": "D.1 Effectiveness of Learning Experience: Sinusoid Function Regression"}, {"figure_path": "08oUnmtj8Q/tables/tables_6_1.jpg", "caption": "Table 1: Parameter setups for meta-learning methods.", "description": "This table lists the parameter settings used for the meta-learning methods employed in the paper's experiments.  It includes details about the number of meta-learning datasets, the number of update iterations, the batch size, the neural network architecture (number of hidden layers and units per layer), learning rates (alpha and beta), and the activation function used. These parameters are crucial for configuring the meta-learning process and its performance.", "section": "5.1 Performance on EMOPs"}, {"figure_path": "08oUnmtj8Q/tables/tables_6_2.jpg", "caption": "Table 2: Parameter setups for DTLZ optimization.", "description": "This table shows the parameter settings used in the multi-objective optimization experiment on the DTLZ test problems.  It specifies the number of related tasks used for meta-learning, the size of datasets from these tasks, and the number of evaluations used for both surrogate initialization and further optimization. The total number of evaluations is also provided.  There are separate settings for the MOEA/D-FS algorithm (the proposed method) and the comparison algorithms.", "section": "5.1 Performance on EMOPs"}, {"figure_path": "08oUnmtj8Q/tables/tables_16_1.jpg", "caption": "Table 3: Mean NMSE and standard deviation (in parentheses) of 30 runs on the amplitude regression of sinusoid function. GP [34] is a widely used surrogate in SAEAS, MAML [10], ALPaCA [13], and DKT [27] are meta-learning methods. GP_Adam is a GP model fitted by Adam optimizer. DKL is a deep kernel learning algorithm that adds a neural network to GP_Adam. MDKL_NN applies meta-learning to DKL, but no task-independent base kernel parameters are shared between related tasks. Support data points are used to train non-meta surrogates or adapt meta-learning surrogates. '+', '\u2248', and '-' denote MDKL is statistically significantly superior to, almost equivalent to, and inferior to the compared modelling methods in the Wilcoxon rank sum test (significance level is 0.05), respectively. The last row counts the total win/tie/loss results. It shows that MDKL and DKT have lower NMSE than other models. The effectiveness of meta-learning on both the neural network and the base kernel has been demonstrated on this example.", "description": "This table presents the Mean Normalized Mean Squared Error (NMSE) and standard deviation from 30 runs of an amplitude regression experiment on sinusoid functions.  It compares the performance of several methods: a standard Gaussian Process (GP), a GP optimized with the Adam optimizer (GP_Adam), Deep Kernel Learning (DKL), a meta-learning version of DKL (MDKL_NN), the proposed MDKL method, and other meta-learning methods (MAML, ALPaCA, DKT). The results show that MDKL and DKT generally outperform other methods.", "section": "D.1.2 Results and Analysis"}, {"figure_path": "08oUnmtj8Q/tables/tables_17_1.jpg", "caption": "Table 4: Mean MSE and standard deviation (in parentheses) of 30 runs on the regression of engine fuel consumption. Support data points are used to train non-meta surrogates or adapt meta-learning surrogates. All results are normalized since the actual engine data is unable to be disclosed. The symbols '+', '\u2248', '-' denote the win/tie/loss result of Wilcoxon rank sum test (significance level is 0.05) between MDKL and comparison modeling methods, respectively. The last row counts the total win/tie/loss results.", "description": "This table presents the mean and standard deviation of the Mean Squared Error (MSE) for 30 runs of engine fuel consumption regression.  Different amounts of support data points (2, 3, 5, 10, 20, 40) are used to train various models:  a standard Gaussian Process (GP), a GP optimized with the Adam optimizer, Deep Kernel Learning (DKL), MDKL-NN (a meta-learning variant of DKL without shared parameters), the authors' MDKL model, and two other meta-learning models (DKT and MAML). The results are normalized.  A Wilcoxon rank sum test is used to statistically compare MDKL against the other methods, indicating whether MDKL performed significantly better (+), similarly (\u2248), or worse (-) than each comparison model. The final row summarizes the total number of wins, ties, and losses for MDKL.", "section": "D.2 Results and analysis"}, {"figure_path": "08oUnmtj8Q/tables/tables_18_1.jpg", "caption": "Table 5: Mean IGD+ values and standard deviation (in parentheses) obtained from 30 runs on the DTLZ problems. MOEA/D-FS and the comparison algorithms initialize their surrogates with 10, 100 samples, respectively. Extra 50 evaluations are allowed in the further optimization.", "description": "This table presents the mean inverted generational distance plus (IGD+) values and their standard deviations obtained from 30 independent runs on seven different DTLZ test problems (DTLZ1-DTLZ7). The MOEA/D-FS algorithm and several comparison algorithms (ParEGO, K-RVEA, KTA2, CSEA, OREA, ESBCEO, KMOEA-TIC) are evaluated. For each algorithm, the surrogates are initialized using either 10 or 100 samples, and an additional 50 evaluations are performed during further optimization.  Smaller IGD+ values indicate better optimization performance.", "section": "5.1 Performance on EMOPs"}, {"figure_path": "08oUnmtj8Q/tables/tables_19_1.jpg", "caption": "Table 5: Mean IGD+ values and standard deviation (in parentheses) obtained from 30 runs on the DTLZ problems. MOEA/D-FS and the comparison algorithms initialize their surrogates with 10, 100 samples, respectively. Extra 50 evaluations are allowed in the further optimization.", "description": "This table presents the mean inverted generational distance plus (IGD+) values and their standard deviations obtained from 30 independent runs of the MOEA/D-FS algorithm and several comparison algorithms on seven DTLZ test problems.  The algorithms initialize their surrogates with either 10 or 100 samples, and an additional 50 evaluations are conducted after initialization. Smaller IGD+ values indicate better optimization performance.", "section": "5.1 Performance on EMOPs"}, {"figure_path": "08oUnmtj8Q/tables/tables_19_2.jpg", "caption": "Table 5: Mean IGD+ values and standard deviation (in parentheses) obtained from 30 runs on the DTLZ problems. MOEA/D-FS and the comparison algorithms initialize their surrogates with 10, 100 samples, respectively. Extra 50 evaluations are allowed in the further optimization.", "description": "This table presents the mean inverted generational distance plus (IGD+) values and their standard deviations obtained from 30 independent runs of the MOEA/D-FS algorithm and several comparison algorithms on seven DTLZ problems.  The algorithms differ in their surrogate initialization strategies, with MOEA/D-FS and some comparisons initializing with 10 samples, and others with 100 samples.  An additional 50 evaluations are performed after surrogate initialization. The results show the performance of each algorithm in terms of IGD+ values after the initial surrogate generation and the subsequent optimization.", "section": "E.2 Result Table and Analysis of Expensive Multi-Objective Optimization"}, {"figure_path": "08oUnmtj8Q/tables/tables_20_1.jpg", "caption": "Table 5: Mean IGD+ values and standard deviation (in parentheses) obtained from 30 runs on the DTLZ problems. MOEA/D-FS and the comparison algorithms initialize their surrogates with 10, 100 samples, respectively. Extra 50 evaluations are allowed in the further optimization.", "description": "The table shows the mean IGD+ values and standard deviations obtained from 30 runs of the DTLZ problems using different algorithms.  MOEA/D-FS and the comparison algorithms begin by initializing their surrogates with either 10 or 100 samples. An additional 50 evaluations are then performed for further optimization. The IGD+ metric (Inverted Generational Distance Plus) measures the performance of the algorithms in approximating the Pareto front, with lower values indicating better performance.", "section": "5.1 Performance on EMOPs"}, {"figure_path": "08oUnmtj8Q/tables/tables_21_1.jpg", "caption": "Table 9: Mean IGD+ values and standard deviation (in parentheses) obtained from 30 runs on DTLZ problems. Both MOEA/D-FSs initialize their surrogates with 10 samples, extra 50 evaluations are allowed in the further optimization. The last two rows count the statistical test results between MOEA/D-FSs and other compared algorithms.", "description": "This table presents the mean inverted generational distance (IGD+) values and their standard deviations obtained from 30 independent runs of the MOEA/D-FS algorithm on seven DTLZ problems. The MOEA/D-FS algorithm is tested under two conditions: 'in-range' where the target task DTLZ is included in the set of related tasks for meta-learning, and 'out-of-range' where the target task is excluded.  The table shows that for most problems, the performance difference between the two conditions ('in-range' and 'out-of-range') is not statistically significant. The last two rows summarize the statistical significance tests comparing MOEA/D-FS with MOEA/D-EGO and 6 other comparison algorithms.", "section": "F Additional Details on Expensive Multi-Objective Optimization"}, {"figure_path": "08oUnmtj8Q/tables/tables_22_1.jpg", "caption": "Table 3: Mean NMSE and standard deviation (in parentheses) of 30 runs on the amplitude regression of sinusoid function. GP [34] is a widely used surrogate in SAEAS, MAML [10], ALPaCA [13], and DKT [27] are meta-learning methods. GP_Adam is a GP model fitted by Adam optimizer. DKL is a deep kernel learning algorithm that adds a neural network to GP_Adam. MDKL_NN applies meta-learning to DKL, but no task-independent base kernel parameters are shared between related tasks. Support data points are used to train non-meta surrogates or adapt meta-learning surrogates. '+', '\u2248', and '-' denote MDKL is statistically significantly superior to, almost equivalent to, and inferior to the compared modelling methods in the Wilcoxon rank sum test (significance level is 0.05), respectively. The last row counts the total win/tie/loss results. It shows that MDKL and DKT have lower NMSE than other models. The effectiveness of meta-learning on both the neural network and the base kernel has been demonstrated on this example.", "description": "This table presents the results of an experiment comparing different meta-learning and non-meta-learning methods for regression on sinusoid functions.  It shows the Mean Normalized Mean Squared Error (NMSE) and standard deviation for each method with varying amounts of support data.  The results demonstrate the effectiveness of meta-learning and the benefits of MDKL's approach.", "section": "D.1 Effectiveness of Learning Experience: Sinusoid Function Regression"}, {"figure_path": "08oUnmtj8Q/tables/tables_22_2.jpg", "caption": "Table 5: Mean IGD+ values and standard deviation (in parentheses) obtained from 30 runs on the DTLZ problems. MOEA/D-FS and the comparison algorithms initialize their surrogates with 10, 100 samples, respectively. Extra 50 evaluations are allowed in the further optimization.", "description": "This table presents the mean inverted generational distance plus (IGD+) values and their standard deviations, calculated from 30 independent runs for each algorithm on seven different DTLZ (Deb, Thiele, Laumanns, and Zitzler) benchmark problems.  The MOEA/D-FS algorithm uses 10 initial samples for surrogate initialization, while the comparison algorithms use 100. An additional 50 evaluations are performed after initialization. Smaller IGD+ values indicate better performance.", "section": "5.1 Performance on EMOPs"}, {"figure_path": "08oUnmtj8Q/tables/tables_24_1.jpg", "caption": "Table 5: Mean IGD+ values and standard deviation (in parentheses) obtained from 30 runs on the DTLZ problems. MOEA/D-FS and the comparison algorithms initialize their surrogates with 10, 100 samples, respectively. Extra 50 evaluations are allowed in the further optimization.", "description": "This table presents the mean inverted generational distance plus (IGD+) values and their standard deviations for 30 independent runs on seven DTLZ (Deb, Thiele, Laumanns, and Zitzler) multi-objective optimization problems.  Two different surrogate initialization methods were compared: MOEA/D-FS (the proposed method) using 10 samples and other comparison algorithms using 100 samples.  An additional 50 evaluations were allowed after the initialization for all algorithms.  Lower IGD+ values indicate better optimization performance.", "section": "5.1 Performance on EMOPs"}]