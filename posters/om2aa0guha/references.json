{"references": [{"fullname_first_author": "A. Agarwal", "paper_title": "On the theory of policy gradient methods: Optimality, approximation, and distribution shift", "publication_date": "2021-01-01", "reason": "This paper provides a comprehensive theoretical analysis of policy gradient methods, which is foundational to the current work's investigation of Policy Mirror Descent (PMD)."}, {"fullname_first_author": "Y. Efroni", "paper_title": "Beyond the one-step greedy approach in reinforcement learning", "publication_date": "2018-07-10", "reason": "This paper introduces the concept of multi-step greedy policy improvement, a key idea that underpins the h-PMD algorithm proposed in the current work."}, {"fullname_first_author": "E. Johnson", "paper_title": "Optimal convergence rate for exact policy mirror descent in discounted Markov decision processes", "publication_date": "2023-01-01", "reason": "This work establishes a dimension-free linear convergence rate for PMD, which the current research extends to the h-PMD algorithm with an improved convergence rate."}, {"fullname_first_author": "J. Schulman", "paper_title": "Trust region policy optimization", "publication_date": "2015-01-01", "reason": "Trust Region Policy Optimization (TRPO) is a seminal policy gradient algorithm that is closely related to PMD, and its success motivates the current work's exploration of improving PMD."}, {"fullname_first_author": "J. Schulman", "paper_title": "Proximal policy optimization algorithms", "publication_date": "2017-01-01", "reason": "Proximal Policy Optimization (PPO) builds upon TRPO and has achieved widespread empirical success, making its connection to PMD an important aspect of this research."}]}