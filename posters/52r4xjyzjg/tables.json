[{"figure_path": "52r4XJYzjg/tables/tables_2_1.jpg", "caption": "Table 2: Context-specific datasets. On datasets with ground truth context, adding context generally helps with evaluating preference. Our model, finetuned on the RPR training sets achieves substantially better (test set) performance than the unfinetuned model.", "description": "This table presents the performance of several language models on various preference datasets, both with and without added context.  The models' ability to predict preferences accurately is evaluated.  It demonstrates how the addition of context improves performance, particularly for models fine-tuned on context-aware datasets. The table highlights the superior performance of the authors' model, demonstrating significant gains when trained with context.", "section": "5 Results"}, {"figure_path": "52r4XJYzjg/tables/tables_7_1.jpg", "caption": "Table 2: Context-specific datasets. On datasets with ground truth context, adding context generally helps with evaluating preference. Our model, finetuned on the RPR training sets achieves substantially better (test set) performance than the unfinetuned model.", "description": "This table presents the performance of various models on four different datasets: RPR Criteria, RPR Scenarios, Preference Bench, and Multifaceted Bench.  For each dataset, the model's performance is shown with and without the addition of context. The results demonstrate that adding context generally improves the model's performance. Notably, the authors' model, which was finetuned on the RPR datasets, significantly outperforms the unfinetuned version.  The table highlights the importance of context in accurate preference prediction.", "section": "5.2 Results"}, {"figure_path": "52r4XJYzjg/tables/tables_7_2.jpg", "caption": "Table 3: Sensitivity of current models to adversarial contexts. We observe that current models often ignore added context, even when it is strongly phrased and adversarial. Our finetuned CARM does best among smaller models, but still has significant room for improvement.", "description": "This table presents the results of evaluating several language models' performance on adversarial contexts.  The models were tested on two types of adversarial contexts: nonsense criteria (where the ideal response is nonsensical) and negative criteria (where the ideal response is of low quality). The table shows that most models struggle with these adversarial contexts, often ignoring the context and producing responses similar to those generated without context. The authors' fine-tuned context-aware reward model (Mistral CARM) performs better than other smaller models but still has room for improvement.", "section": "5.2 Results"}, {"figure_path": "52r4XJYzjg/tables/tables_8_1.jpg", "caption": "Table 4: Context-augmented general preference datasets. We augmented three preference datasets (no context; NC) with context (CTX): HHH (by subset), Reward Bench (by subset), and Chatbot Arena (according to a GPT-4 teacher (CTX) and a GPT-4 oracle that also sees the user preference (CTX*)). The stronger context-specific performance of our finetuned CARM allows it to outperform other small models given high quality context.", "description": "This table presents the performance of various language models on three general preference datasets (HHH, Reward Bench, and Chatbot Arena) with and without added context.  The models are evaluated on their ability to predict human preferences.  The table shows that adding context generally improves performance, especially for the authors' model (Mistral CARM).  The performance is broken down by dataset, with and without context and with different levels of context provided (teacher or oracle context).", "section": "5.2 Results"}, {"figure_path": "52r4XJYzjg/tables/tables_8_2.jpg", "caption": "Table 5: RPR Profiles. We used GPT-4 Turbo to label preferences in the RPR test set according to 5 diverse user profiles. The results show that a single context may contain significant signal for a range of prompts. Perhaps surprisingly, our finetuned CARM and Llama 3 Instruct proved to be better interpreters of GPT-4's profile-conditioned preferences than GPT-4 itself.", "description": "This table presents the results of an experiment where five different user profiles were used to label preferences in the RPR test set.  The goal was to determine if a single context (user profile) could effectively capture preference across a range of prompts. The table shows the performance of several language models, including the authors' finetuned CARM model, on this task.  The results suggest that a single context can be quite effective, and that the authors' model performs better than even GPT-4 in interpreting profile-based preferences.", "section": "5.2 Results"}, {"figure_path": "52r4XJYzjg/tables/tables_9_1.jpg", "caption": "Table 6: User profile inference on the RPR dataset. Each of 5 profiles (P1-P5) was used to annotate the RPR test set, as well as three small subsets (32 samples each) of the RPR training set (one subset for each of 3 seeds). Between 2 and 32 samples from each subset were then used to infer a user profile with the help of GPT-4 Turbo. Conditioning our finetuned CARM on these inferred profiles gives the results below (both table and figure display the same data). We see that just 2 samples carry substantial signal, and 32 samples capture most of the benefit of the ground truth context.", "description": "This table presents the results of an experiment where five different user profiles were used to annotate a dataset of preferences.  The goal was to see how well a model could infer a user's preferences from a small number of samples.  The table shows that even with only two samples, the model could make reasonably accurate predictions, and accuracy increased as the number of samples increased. The results suggest that a single, well-defined context can be sufficient to capture a wide range of user preferences.", "section": "5.2 Results"}, {"figure_path": "52r4XJYzjg/tables/tables_22_1.jpg", "caption": "Table 2: Context-specific datasets. On datasets with ground truth context, adding context generally helps with evaluating preference. Our model, finetuned on the RPR training sets achieves substantially better (test set) performance than the unfinetuned model.", "description": "This table presents the performance of several language models on various context-specific datasets.  The \"NC\" column shows performance without using any context, while the \"CTX\" column shows performance with context. The table highlights the improvement in performance when context is added, especially for the authors' fine-tuned model (Mistral CARM) which significantly outperforms the other models on the test set.", "section": "5 Results"}, {"figure_path": "52r4XJYzjg/tables/tables_23_1.jpg", "caption": "Table 2: Context-specific datasets. On datasets with ground truth context, adding context generally helps with evaluating preference. Our model, finetuned on the RPR training sets achieves substantially better (test set) performance than the unfinetuned model.", "description": "This table presents the performance of several models on various datasets that evaluate preference with and without context. The results show that adding context generally improves the model's performance and demonstrates the impact of finetuning on a new dataset (RPR) in enhancing the context-aware preference modeling capabilities.", "section": "5.2 Results"}, {"figure_path": "52r4XJYzjg/tables/tables_24_1.jpg", "caption": "Table 2: Context-specific datasets. On datasets with ground truth context, adding context generally helps with evaluating preference. Our model, finetuned on the RPR training sets achieves substantially better (test set) performance than the unfinetuned model.", "description": "This table presents the performance of various language models on several context-specific datasets. The \"NC\" column represents the performance without using any context, while the \"CTX\" column indicates the performance with context. The results show that adding context generally improves the models' performance in evaluating preference, especially in datasets where context is essential. This table also highlights that the model finetuned using the Reasonable Preference Reversal (RPR) datasets significantly outperforms the unfinetuned model on these datasets.", "section": "5.2 Results"}]