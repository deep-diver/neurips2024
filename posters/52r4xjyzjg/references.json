{"references": [{"fullname_first_author": "R. Akrour", "paper_title": "April: Active preference learning-based reinforcement learning", "publication_date": "2012-09-24", "reason": "This paper introduces active preference learning for reinforcement learning, a technique relevant to the paper's exploration of preference modeling for language models."}, {"fullname_first_author": "P. F. Christiano", "paper_title": "Deep reinforcement learning from human preferences", "publication_date": "2017-00-00", "reason": "This foundational paper details deep reinforcement learning from human preferences, a core method underlying the alignment of language models discussed in the current paper."}, {"fullname_first_author": "N. Stiennon", "paper_title": "Learning to summarize with human feedback", "publication_date": "2020-00-00", "reason": "This work demonstrates learning to summarize using human feedback, a technique directly relevant to the paper's focus on aligning language models to human preferences."}, {"fullname_first_author": "T. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-00-00", "reason": "This highly influential paper establishes the few-shot learning capabilities of language models, which is a key aspect of the current paper's approach to preference modeling."}, {"fullname_first_author": "L. Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-00-00", "reason": "This paper presents a method for training language models to follow instructions using human feedback, a technique directly relevant to the paper's focus on aligning language models with human preferences."}]}