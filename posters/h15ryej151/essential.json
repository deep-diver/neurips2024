{"importance": "This paper is crucial because **it resolves the open problem of theoretically explaining the benefits of complex parameterizations in structured state space models (SSMs)**, a core component of many prominent neural network architectures.  Its findings **provide a strong theoretical foundation for future research and guide the design of more efficient and effective neural network architectures.** The experiments confirm the theoretical findings and suggest potential avenues for extending the theory, especially considering recent advances in SSMs with selectivity.", "summary": "Complex numbers boost neural network performance! This study proves that complex parameterizations in structured state space models (SSMs) enable more efficient and practical learning of complex mappings compared to real parameterizations.", "takeaways": ["Complex parameterizations in SSMs offer significant advantages in expressiveness and practical learnability over real parameterizations.", "Real SSMs struggle to learn oscillatory mappings efficiently, requiring exponentially large dimensions or parameters; complex SSMs handle these mappings effectively.", "The findings suggest a potential extension of the theory to account for selectivity in SSMs, providing valuable insights into real-world performance discrepancies."], "tldr": "Structured state space models (SSMs) are fundamental to many neural networks, but the use of complex (rather than real) numbers in their parameterization remains poorly understood.  This paper addresses this gap by formally demonstrating that while both real and complex SSMs can theoretically express any linear time-invariant mapping, complex SSMs do so far more efficiently.  The current use of complex numbers in SSMs is based on empirical evidence alone, and therefore, the paper's theoretical explanation is significant.\nThis research uses formal proofs to show that real SSMs often require exponentially larger dimensions or parameter magnitudes to approximate the same mappings achieved by complex SSMs.  This issue is further exacerbated when handling common oscillatory mappings.  The paper's controlled experiments validate its theoretical findings, highlighting the substantial performance improvements gained by using complex parameterizations. The results also suggest potential refinements to the theory by incorporating the recently introduced concept of 'selectivity' in SSM architectures.", "affiliation": "Tel Aviv University", "categories": {"main_category": "AI Theory", "sub_category": "Optimization"}, "podcast_path": "h15RyEj151/podcast.wav"}