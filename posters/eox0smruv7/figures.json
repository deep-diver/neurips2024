[{"figure_path": "eOx0SMRUv7/figures/figures_3_1.jpg", "caption": "Figure 1: Learning the threshold 1{x > 0} on R. The nearest neighbor classifier makes a mistake every single round on the sequence Xn = (-1/3)n, where subsequent test points alternate sign.", "description": "The figure shows a simple example where the 1-nearest neighbor rule fails to learn a threshold function. The instance sequence is constructed such that the nearest neighbor always has the opposite label, resulting in a mistake at every round.  The x-axis represents the feature space, and the points illustrate the sequence of instances presented to the learner. The labels are implicitly represented by the position of the points relative to the zero on the x-axis.", "section": "Non-convergence for worst-case sequences"}, {"figure_path": "eOx0SMRUv7/figures/figures_14_1.jpg", "caption": "Figure 2: (Left) A visualization of a mutually-labeling set (orange ball). There are two classes (dark and light gray) separated by a decision boundary (blue line). The length of the dashed line measures the margin of a point in the set. The length of the dotted line is bounded above by the diameter of the set. (Right) An example of a collection of mutually-labeling sets (dark and light blue balls) covering all but a region of small mass. Since the nearest neighbor rule makes at most one mistake per ball, eventually all mistakes must come from the white, uncovered region.", "description": "The figure demonstrates the concept of mutually-labeling sets.  The left panel shows a single mutually-labeling set (orange circle) where all points within the set are closer to each other than they are to points of a different class. This means the nearest neighbor algorithm will make at most one mistake in this region. The right panel illustrates how many such sets can cover most of the space, leaving only a small region (white area) where mistakes might be made. This small region shrinks as more data arrives, illustrating how the nearest neighbor algorithm's mistake rate eventually vanishes.", "section": "Consistency for functions with negligible boundaries"}, {"figure_path": "eOx0SMRUv7/figures/figures_18_1.jpg", "caption": "Figure 2: (Left) A visualization of a mutually-labeling set (orange ball). There are two classes (dark and light gray) separated by a decision boundary (blue line). The length of the dashed line measures the margin of a point in the set. The length of the dotted line is bounded above by the diameter of the set. (Right) An example of a collection of mutually-labeling sets (dark and light blue balls) covering all but a region of small mass. Since the nearest neighbor rule makes at most one mistake per ball, eventually all mistakes must come from the white, uncovered region.", "description": "The figure visualizes the concept of mutually-labeling sets and how they're used to prove the consistency of the nearest neighbor rule.  The left panel shows a single mutually-labeling set (a ball) where all points within have a margin (distance to nearest point of a different class) greater than the set's diameter.  The right panel illustrates how multiple such sets can cover almost all of the space, leaving only a small region where mistakes might occur.", "section": "Consistency for functions with negligible boundaries"}]