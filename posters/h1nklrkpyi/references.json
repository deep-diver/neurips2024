{"references": [{"fullname_first_author": "Mohamed Afham", "paper_title": "CrossPoint: Self-supervised cross-modal contrastive learning for 3d point cloud understanding", "publication_date": "2022-00-00", "reason": "This paper is highly relevant due to its focus on self-supervised contrastive learning for 3D point cloud understanding, a key area of research also explored in the current paper."}, {"fullname_first_author": "Hangbo Bao", "paper_title": "BEiT: BERT pre-training of image transformers", "publication_date": "2021-00-00", "reason": "This paper is cited as a foundational work for masked image modeling, which serves as a crucial inspiration for masked point modeling, the core method of the current paper."}, {"fullname_first_author": "Angel X Chang", "paper_title": "ShapeNet: An information-rich 3D model repository", "publication_date": "2015-00-00", "reason": "ShapeNet is a key dataset used in the current paper's pre-training experiments, making this dataset's description a crucial reference."}, {"fullname_first_author": "Angela Dai", "paper_title": "ScanNet: Richly-annotated 3D reconstructions of indoor scenes", "publication_date": "2017-00-00", "reason": "ScanNet is a key dataset for downstream tasks in the current paper, providing real-world point cloud data for evaluating the model's performance."}, {"fullname_first_author": "Runpei Dong", "paper_title": "Autoencoders as cross-modal teachers: Can pretrained 2D image transformers help 3D representation learning?", "publication_date": "2023-00-00", "reason": "This paper is highly relevant because it explores the use of pretrained 2D image transformers to improve 3D representation learning, a concept relevant to the current paper's approach."}]}