[{"figure_path": "7RwKMRMNrc/figures/figures_2_1.jpg", "caption": "Figure 1: Top: Visual description of pretraining losses. In blue: the local to global DINO loss, in red: the global to global DINO loss and in green the latent masked token prediction (iBOT) loss. Bottom: Our different augmentation strategies. 'Original' uses several augmentations (RandomResizedCrop, ColorJitter, RandomGrayscale, GaussianBlur, RandomHorizontalFlip and RandomSolarize), 'Shared' uses the same augmentations but shares them between each view of the same image obtained with RandomResizedCrop. The 'Crop + Resize' setting only uses RandomResizedCrop. We also introduce a 'Crop' setup which uses RandomCrop without random rescaling and that is visually similar to 'Crop + Resize'.", "description": "The figure illustrates the different data augmentation strategies used in the paper's experiments. The top part shows the three loss functions used during pretraining: local-to-global DINO loss, global-to-global DINO loss, and the iBOT loss. The bottom part visualizes the four augmentation strategies used: 'Original' (with multiple augmentations), 'Shared' (sharing augmentations between views), 'Crop + Resize' (only using RandomResizedCrop), and 'Crop' (using RandomCrop without resizing).  Each strategy is represented with example images, showing the transformations applied.", "section": "3 Approach"}, {"figure_path": "7RwKMRMNrc/figures/figures_6_1.jpg", "caption": "Figure 2: Impact of dataset size when varying data augmentations. Results of ViT-L on linear evaluation benchmarks, including classification (ImageNet1k, Places 205 and INaturalist18), depth estimation (NYU-Depth) and segmentation (ADE20k). Cropping without resizing ('Crop') reaches very high performances on a wide variety of benchmarks, given that the dataset size is large enough.", "description": "This figure shows the performance of a ViT-L model on various downstream tasks (ImageNet1k classification, Places 205 classification, iNaturalist18 classification, NYU-Depth depth estimation, and ADE20k segmentation) when trained with different data augmentation strategies and varying dataset sizes (ImageNet-1k, ImageNet-22k, and LVD-142M). The results demonstrate that using only cropping without resizing ('Crop') achieves high performance, especially with larger datasets. This finding suggests that data augmentation's primary role might be to effectively increase dataset size rather than to enforce invariance learning.", "section": "Experiments and discussion"}, {"figure_path": "7RwKMRMNrc/figures/figures_6_2.jpg", "caption": "Figure 2: Impact of dataset size when varying data augmentations. Results of ViT-L on linear evaluation benchmarks, including classification (ImageNet1k, Places 205 and INaturalist18), depth estimation (NYU-Depth) and segmentation (ADE20k). Cropping without resizing (\"Crop\") reaches very high performances on a wide variety of benchmarks, given that the dataset size is large enough.", "description": "This figure shows the impact of dataset size on the performance of a Vision Transformer (ViT-L) model trained with different data augmentation strategies.  The x-axis represents the number of training epochs (100, 300, 500).  The y-axis represents the Top-1 accuracy achieved on various downstream tasks (ImageNet1k, Places205, INaturalist18, NYU-Depth, and ADE20k).  The different lines represent different augmentation strategies: \"Original\" (full augmentations), \"Shared\" (augmentations shared between views), \"Crop + Resize\" (only RandomResizedCrop), and \"Crop\" (only cropping without resizing). The figure demonstrates that with a sufficiently large dataset, cropping without resizing achieves results comparable to using more complex augmentation strategies.  The results across various benchmarks highlight the importance of sufficient training data over the specific augmentation strategies employed.", "section": "Experiments and discussion"}, {"figure_path": "7RwKMRMNrc/figures/figures_8_1.jpg", "caption": "Figure 2: Impact of dataset size when varying data augmentations. Results of ViT-L on linear evaluation benchmarks, including classification (ImageNet1k, Places 205 and INaturalist18), depth estimation (NYU-Depth) and segmentation (ADE20k). Cropping without resizing (\"Crop\") reaches very high performances on a wide variety of benchmarks, given that the dataset size is large enough.", "description": "This figure displays the results of experiments conducted to assess the effect of dataset size on the performance of a ViT-L model trained with different data augmentation strategies.  It shows the Top-1 accuracy for ImageNet1k, Places205, and iNaturalist18; mIoU for ADE20k; and RMSE for NYU-Depth.  The key finding is that using only cropping without resizing (\"Crop\") achieves high performance when the dataset size is sufficiently large, indicating that data augmentation's primary role is in artificially inflating the dataset's size rather than enforcing invariance.", "section": "4 Experiments and discussion"}]