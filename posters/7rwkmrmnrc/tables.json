[{"figure_path": "7RwKMRMNrc/tables/tables_1_1.jpg", "caption": "Table 1: Comparison of our model trained using RandomCrop, without random resizing nor other photometric augmentations against SSL models that do not leverage hand-crafted augmentations. All other models are reconstruction based, in the pixel space or in the latent space, and use more augmentations than our setup. We only use RandomCrop without resizing, and masking.", "description": "This table compares the performance of the authors' self-supervised learning model (using only RandomCrop data augmentation) against other state-of-the-art models that do not use hand-crafted data augmentations.  It highlights that the authors' model achieves comparable or superior results despite using significantly fewer augmentations. The table presents results on various image classification, segmentation and depth estimation benchmarks, demonstrating the generalizability of the approach.", "section": "2 Related Work"}, {"figure_path": "7RwKMRMNrc/tables/tables_5_1.jpg", "caption": "Table 2: New domains classification results of DINOv2 ViT-L trained on ImageNet-22k when varying data augmentations. None of those domains were used in the pretraining data of the models.", "description": "This table presents the classification accuracy (ACC) results for the DINOv2 ViT-L model trained on ImageNet-22k and tested on various new domains (not used for pretraining).  The performance of four different data augmentation strategies is compared: Original (with all augmentations), Shared (same photometric augmentations for both views), Crop+Resize (RandomResizedCrop), and Crop (RandomCrop without resizing).  The results highlight the impact of data augmentation on model performance across diverse domains, including remote sensing and medical imaging.", "section": "4 Experiments and discussion"}, {"figure_path": "7RwKMRMNrc/tables/tables_5_2.jpg", "caption": "Table 3: (left): New task classification results of DINOv2 ViT-L trained on ImageNet-22k when varying data augmentations. None of those tasks were used to tune DINOv2's hyperparameters. (right): Measure of invariance toward augmentation. Higher cosine similarity means higher invariance as the model embeds multiple augmentations of the same image to closer vectors.", "description": "This table presents a comparison of the performance of DINOv2 on new classification tasks (left) and a measure of invariance towards augmentation (right), using different data augmentation strategies.  The left side shows the accuracy on several datasets not seen during training, highlighting the effects of various augmentation methods. The right side quantifies invariance by measuring the cosine similarity between embeddings of multiple augmented versions of the same image. Higher cosine similarity indicates greater invariance.", "section": "Experiments and discussion"}, {"figure_path": "7RwKMRMNrc/tables/tables_7_1.jpg", "caption": "Table 4: Impact of the iBOT loss on linear evaluation for multiple datasets for a ViT-L trained for 500 epochs on LVD-142M. We compare results with and without using masking and the iBOT loss.", "description": "This table presents the results of linear evaluation on multiple datasets (ImageNet1k, iNaturalist18, ADE20k, and NYU-Depth) for a ViT-L model trained for 500 epochs on the LVD-142M dataset.  The impact of using the iBOT loss (a local masked-image reconstruction loss) and masking is analyzed.  The table compares four different data augmentation strategies: Original (with all augmentations), Shared (sharing augmentations between views), Crop+Resize (with only RandomResizedCrop), and Crop (with RandomCrop without resizing and optionally masking).  The results show the performance of each augmentation strategy with and without the iBOT loss and masking, illustrating their relative effects on downstream tasks.", "section": "4.2 Can we remove hand-crafted augmentations totally?"}]