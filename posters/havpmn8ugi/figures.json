[{"figure_path": "haVPmN8UGi/figures/figures_0_1.jpg", "caption": "Figure 1: Left: Accuracy improvement of GraphVis compared to the base model's performance on commonsense reasoning tasks. Right: Improvement by GraphVis on multiple VQA benchmarks over its base LVLM model LLaVA-v1.6 (Liu et al., 2024).", "description": "This figure presents a comparison of GraphVis's performance against a baseline model across various question-answering benchmarks. The left panel shows the accuracy improvements achieved by GraphVis on commonsense reasoning tasks (CSQA and OBQA), highlighting its effectiveness in enhancing LLMs' reasoning capabilities.  The right panel displays the performance gains of GraphVis on several visual question-answering (VQA) benchmarks (ScienceQA, MMBench, POPE-random, POPE-popular, POPE-adversarial).  This demonstrates GraphVis's ability to improve not only textual QA but also zero-shot VQA performance by leveraging visual knowledge graphs.", "section": "Abstract"}, {"figure_path": "haVPmN8UGi/figures/figures_2_1.jpg", "caption": "Figure 2: Overview of GraphVis. Given an input question and answer pair in the training data, we retrieve and visualize the relevant subgraph. With pre-defined questions on the basic features of the visual graphs such as numbers of nodes and node degree, we first construct data for visual graph comprehension fine-tuning. Subsequently, we incorporate the QA pair with the visual graph for KG-enhanced QA fine-tuning.", "description": "This figure illustrates the GraphVis framework.  It shows how a question-answer pair is used to retrieve a subgraph from a Knowledge Graph (KG). This subgraph is then visualized using Graphviz and used for two-stage fine-tuning of a Large Vision Language Model (LVLM).\n\nThe first stage is visual graph comprehension fine-tuning, where the LVLM is trained to understand basic visual features of the graph (e.g., node count, edge count). The second stage is KG-enhanced QA fine-tuning, where the LVLM is trained to answer questions using both the visual graph and the original question-answer pair. This curriculum learning approach helps the LVLM effectively integrate knowledge from the KG into its reasoning process.", "section": "Method"}, {"figure_path": "haVPmN8UGi/figures/figures_5_1.jpg", "caption": "Figure 1: Left: Accuracy improvement of GraphVis compared to the base model's performance on commonsense reasoning tasks. Right: Improvement by GraphVis on multiple VQA benchmarks over its base LVLM model LLaVA-v1.6 (Liu et al., 2024).", "description": "This figure presents a comparison of the GraphVis model's performance against a baseline model across two sets of tasks. The left panel shows improved accuracy on commonsense reasoning tasks, while the right panel demonstrates performance gains on visual question answering (VQA) benchmarks.  The results highlight GraphVis's effectiveness in both textual and visual reasoning domains.", "section": "Abstract"}, {"figure_path": "haVPmN8UGi/figures/figures_6_1.jpg", "caption": "Figure 1: Left: Accuracy improvement of GraphVis compared to the base model's performance on commonsense reasoning tasks. Right: Improvement by GraphVis on multiple VQA benchmarks over its base LVLM model LLaVA-v1.6 (Liu et al., 2024).", "description": "This figure presents a comparison of GraphVis's performance against a baseline model. The left side shows improvements in accuracy across various commonsense reasoning tasks. The right side displays GraphVis's performance boost on multiple visual question answering (VQA) benchmarks compared to its base large vision language model (LLaVA-v1.6).", "section": "Abstract"}, {"figure_path": "haVPmN8UGi/figures/figures_6_2.jpg", "caption": "Figure 1: Left: Accuracy improvement of GraphVis compared to the base model's performance on commonsense reasoning tasks. Right: Improvement by GraphVis on multiple VQA benchmarks over its base LVLM model LLaVA-v1.6 (Liu et al., 2024).", "description": "This figure presents a comparison of GraphVis's performance against a baseline model on two sets of tasks: commonsense reasoning and visual question answering (VQA). The left panel shows a bar chart illustrating the accuracy gains achieved by GraphVis across several commonsense reasoning benchmarks (CSQA, OBQA, MMBench, POPE). The right panel displays a radar chart comparing the performance improvements of GraphVis against the baseline model across various VQA benchmarks (CSQA, OBQA, ScienceQA, POPE). The radar chart provides a visual representation of the relative improvements across different benchmarks.", "section": "Abstract"}, {"figure_path": "haVPmN8UGi/figures/figures_7_1.jpg", "caption": "Figure 6: Example images from VQA tasks that share resemblance to the visualized KG subgraphs.", "description": "This figure shows two example images from visual question answering (VQA) datasets, ScienceQA and MMBench.  These images are included to highlight the similarity between the types of diagrams found in those datasets and the visualized knowledge graphs produced by the GraphVis method.  The ScienceQA image is a food web, while the MMBench image is a chart.  The paper argues that the visual similarity makes the GraphVis approach applicable and effective in enhancing the performance of Large Vision Language Models (LVLMs) on VQA tasks.", "section": "5.2.1 Leveraging KG and Textual Data to Enhance LVLM"}, {"figure_path": "haVPmN8UGi/figures/figures_7_2.jpg", "caption": "Figure 6: Example images from VQA tasks that share resemblance to the visualized KG subgraphs.", "description": "This figure shows two example images from visual question answering (VQA) benchmarks, ScienceQA and MMBench.  These images are examples of diagrams that visually resemble knowledge graphs, illustrating the relevance of the GraphVis approach which leverages visual representations of knowledge graphs to enhance LLM performance on VQA tasks. The left image (ScienceQA) is a food web, while the right image (MMBench) is a chart.  Both demonstrate how real-world VQA data contains visually similar graph structures to the synthetic graph images generated by the GraphVis methodology.", "section": "5.2.1 Leveraging KG and Textual Data to Enhance LVLM"}, {"figure_path": "haVPmN8UGi/figures/figures_8_1.jpg", "caption": "Figure 7: Comparison between different fine-tuning schemes with GraphVis on CSQA.", "description": "This figure compares the performance of GraphVis on the CSQA benchmark using different fine-tuning strategies.  It shows that a curriculum-based approach, where the model is first trained on basic visual graph comprehension tasks before progressing to more complex reasoning tasks, yields the best results.  Joint fine-tuning, where all tasks are trained simultaneously, performs significantly worse.  Furthermore, the order of the detailed tasks within the curriculum also matters, with training on OCR tasks first followed by graph tasks, producing slightly better results than the reverse.", "section": "Ablation Study"}, {"figure_path": "haVPmN8UGi/figures/figures_9_1.jpg", "caption": "Figure 8: Example of model output on ScienceQA (VQA task). Note that after fine-tuning the base LVLM with GraphVis on CSQA with synthetic KG images, the model can successfully traverse the graph to locate the correct answer.", "description": "This figure shows an example of how GraphVis improves the performance of a Large Vision Language Model (LVLM) on a visual question answering (VQA) task from the ScienceQA benchmark.  The task requires following the connections in a food web diagram to answer a question about the flow of matter.  The original model (LLaVA-v1.6 7B) gives an incorrect answer (C), while the model fine-tuned with GraphVis correctly answers (A). This demonstrates GraphVis's ability to improve LVLM performance on complex reasoning tasks involving graph-like images.", "section": "Qualitative example"}, {"figure_path": "haVPmN8UGi/figures/figures_15_1.jpg", "caption": "Figure 1: Left: Accuracy improvement of GraphVis compared to the base model's performance on commonsense reasoning tasks. Right: Improvement by GraphVis on multiple VQA benchmarks over its base LVLM model LLaVA-v1.6 (Liu et al., 2024).", "description": "This figure shows a comparison of the performance of the GraphVis model against a baseline model across two types of tasks: commonsense reasoning and visual question answering (VQA).  The left-hand side displays bar charts comparing accuracy on four commonsense reasoning benchmarks (CSQA, OBQA, MMBench, POPE). The right-hand side presents a radar chart comparing improvements in accuracy across several VQA benchmarks (CSQA, OBQA, Science QA).  The figure demonstrates GraphVis's improved performance over the baseline across both task types.", "section": "Abstract"}]