{"references": [{"fullname_first_author": "Thomas Blumensath", "paper_title": "Iterative thresholding for sparse approximations", "publication_date": "2008-01-01", "reason": "This paper introduces iterative thresholding, a fundamental algorithm used in the CRATE architecture's sparse coding block."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-01-01", "reason": "This paper's introduction of few-shot learning in language models provides a critical foundation for the CRATE-a model's scalability, influencing the training techniques employed."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-10-26", "reason": "This pioneering paper on Vision Transformers (ViTs) provides a benchmark against which CRATE-a's performance and scalability are compared."}, {"fullname_first_author": "Kaiming He", "paper_title": "Masked autoencoders are scalable vision learners", "publication_date": "2022-01-01", "reason": "MAE's innovative self-supervised learning approach for scaling ViTs significantly impacted the development of CRATE-a's training strategy, particularly the approach to large-scale pre-training."}, {"fullname_first_author": "Yaodong Yu", "paper_title": "White-box transformers via sparse rate reduction: Compression is all there is?", "publication_date": "2023-11-13", "reason": "This foundational paper introduces the original CRATE architecture and the sparse rate reduction objective, providing the theoretical underpinnings for the enhancements in CRATE-a."}]}