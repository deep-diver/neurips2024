[{"type": "text", "text": "PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jinghui Lu 1\u2217, Ziwei Yang 1\u2217, Yanjie Wang 1\u2217, Xuejing Liu 2, Brian Mac Namee 3, Can Huang 1 ", "page_idx": 0}, {"type": "text", "text": "1 ByteDance 2 University of Chinese Academy of Sciences, China 3 School of Computer Science, University College Dublin {lujinghui, yangziwei.1221, wangyanjie.prince, can.huang}@bytedance.com xuejing931210@gmail.com brian.macnamee@ucd.ie ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this study, we aim to reduce generation latency for Named Entity Recognition (NER) with Large Language Models (LLMs). The main cause of high latency in LLMs is the sequential decoding process, which autoregressively generates all labels and mentions for NER, significantly increase the sequence length. To this end, we introduce Parallel Decoding in LLM for NER (PaDeLLM-NER), a approach that integrates seamlessly into existing generative model frameworks without necessitating additional modules or architectural modifications. PaDeLLMNER accelerates decoding by simultaneously generating all mentions at once, i.e., a label-mention pair per sequence. This results in shorter sequences and faster inference. Experiments reveal that PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times faster than the autoregressive approach for both English and Chinese. Concurrently, it maintains the prediction quality as evidenced by the micro F-score that is on par with the state-of-the-art approaches under both zero-shot and supervised setting. All resources are available at https: //github.com/GeorgeLuImmortal/PaDeLLM_NER. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Named Entity Recognition (NER), a fundamental task in Natural Language Processing (NLP), aims to extract structured information from unstructured text data. This includes identifying and categorizing key elements such as Organization, Geopolitical Entity and so on (referred to as \u201clabels\u201d) in inputs, and pairing them with relevant text spans extracted from the text (termed \u201cmentions\u201d). Conventionally, NER tasks are carried out through an extractive paradigm that entails token-level classification and the subsequent extraction of identified tokens [1, 2]. ", "page_idx": 0}, {"type": "text", "text": "Recent advancements in Large Language Models (LLMs) [8\u201313] have revolutionized numerous foundational tasks in NLP, including NER tasks [3\u20137, 14\u201317], through the adoption of a generative paradigm. This paradigm involves instruction-tuning a sequence-to-sequence (seq2seq) model. The model takes a sequence of unstructured text as input and produces a sequence of structured label-mention pairs as output. Generally, the output structured string should be formatted to meet two criteria: (1) it should have a clear and straightforward structure that facilitates post-processing for label and mention extraction, and (2) it needs to be generated fluidly and efficiently from the perspective of language models [18]. ", "page_idx": 0}, {"type": "table", "img_path": "vjw4TIf8Bo/tmp/41689c08308d9d5219273b96fa6337eb7130809964034a7a832379832cd4b1af.jpg", "table_caption": ["Table 1: Structured output string format used in the literature. The examples come from CoNLL2003 dataset. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "In Table 1, we list two typically used autoregressive output formats found in the literature : (1) accommodate original input text to contain label information, which is referred to as \u201caugmented language\u201d [3, 4]; (2) directly using a customized, easily-parsed structured format to output all labels and mentions, which is called \u201cstructured annotation\u201d [5\u20137]. These formats present certain challenges. For example, augmented language necessitates duplicating all original input text, thereby increasing output length and resulting in inference inefficiency. While structure annotation avoids replicating the entire input, it produces all labels and mentions in an autoregressive manner. This implies that each subsequently generated pair depends on its preceding pairs, and when the number of label-mention pairs is large, it will lead to longer sequences. As demonstrated in Chen et al. [19], Ning et al. [20], high latency in LLMs mainly stems from lengthy sequence generation, we believe that by reducing the length of sequence, a more efficient inference scheme can be provided for NER tasks. ", "page_idx": 1}, {"type": "text", "text": "In light of this, we propose Parallel Decoding in LLM for NER (PaDeLLM-NER), a novel approach to accelerate the inference of NER tasks for LLMs. PaDeLLM-NER empowers the model with the capability to predict a single label-mention pair within a single sequence, subsequently aggregating all sequences to generate the final NER outcome. Specifically, in the training phase, we reconstruct the instruction tuning tasks, enabling LLMs to predict the count of mentions for a specific label and to identify the $n^{t h}$ mention within the entire input for that label (Figure 1). In the inference phase, LLMs first predict the number of mentions for all labels, then predict all label-mention pairs in parallel (Figure 2). Finally, results from all sequences are aggregated and duplicate mentions across labels are eliminated based on prediction probability. This approach results in a more efficient inference method, producing shorter sequences and enabling parallel decoding label-mention pairs in batches. ", "page_idx": 1}, {"type": "text", "text": "Comprehensive experiments have been conducted, demonstrating that PaDeLLM-NER effectively reduces the number of tokens produced in each sequence, thereby decreasing inference latency. Additionally, it maintains or even enhances prediction quality in both flat and nested NER for English and Chinese languages, compared to existing methods in the literature under both zero-shot and supervised setting. To conclude, our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We present PaDeLLM-NER, a novel approach tailored for NER using LLMs. This approach can predict all label-mention pairs in parallel, effectively reducing inference latency.   \n\u2022 Extensive experiments have been conducted, revealing that PaDeLLM-NER significantly improves inference efficiency. By completely decoupling the generation of label-mention pairs, the average sequence length is reduced to around $13\\%$ of that produced by conventional autoregressive methods. Correspondingly, the inference speed is 1.76 to 10.22 times faster than these previous approaches.   \n\u2022 Comprehensive experiments demonstrate that, in addition to its enhanced prediction speed, PaDeLLM-NER also maintains or surpasses the prediction quality of conventional autoregressive methods, on par with state-of-the-art (SOTA) performance on many NER datasets, including zero-shot as well as the supervised scenarios. ", "page_idx": 1}, {"type": "text", "text": "To the best of our knowledge, our technique stands as a pioneering approach in accelerating NER inference in LLMs by parallel decoding all label-mention pairs. This unique characteristic makes it complementary to other inference acceleration methods such as LLM.int8() [21] and speculative sampling [22, 23]. Thus, it can be efficiently integrated with these methods. ", "page_idx": 1}, {"type": "image", "img_path": "vjw4TIf8Bo/tmp/a01842ec39b3b4b6aa180d6fb42dd8d5e44c01d410ce10a379d76a1c86d0fa6c.jpg", "img_caption": ["Figure 1: PaDeLLM-NER training paradigm: texts within frames of the same color represents one training example, where texts inside the solid-line frame are the input, and those inside the dashed-line frame are the output. Italic texts are prompt templates. The \u201centity type\u201d signifies the label being predicted. The $^{\\ast}{<}n u m{>}^{\\,,}$ indicates count of mentions for that label, and \u201c<mention $n{>}$ \u201d refers to the $\\dot{n}^{t h}$ mention of a label in the input. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Generative Models for NER ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Before the era of LLMs, most research approached NER as a sequence labeling task, where each token is assigned a pre-defined tag (e.g., BIO scheme). In this line of work, usually pre-trained transformer-based language models [1, 2] is combined with a tailored prediction head to perform a token-level classification, followed by the extraction of identified tokens. ", "page_idx": 2}, {"type": "text", "text": "Encouraged by the success of unifying multiple NLP tasks into a single seq2seq paradigm [24, 25], especially with the evolution of LLMs [10, 13, 26, 27], the trend of applying seq2seq models to NER tasks is gaining momentum [28], with both inputs and outputs being represented as sequences of text [3\u20137]. Recently, the focus of work on NER using LLMs has shifted towards zero-shot [29, 30] or few-shot learning [4, 18, 31, 32], utilizing in-context learning [18, 32], self-consistency [29, 33] or learning programming [30, 34]. ", "page_idx": 2}, {"type": "text", "text": "Unlike previous studies emphasizing few-shot performance with training-free prompt learning, our work focus on a fully supervised setting. More importantly, our primary objective is to speed up NER inference. ", "page_idx": 2}, {"type": "text", "text": "2.2 Inference Speedup in LLMs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Modern LLMs employ a sequential decoding strategy for token generation, which poses a significant challenge in terms of parallelization, especially as model size and sequence length increase [20]. There is plenty of work in the literature to address this challenge [35\u201338]. One line of work falls into training-free category such as introducing extra modules for speculative sampling [22, 23]. Another approaches explore modifying model architecture to accelerate inference, such as exiting at earlier layer [39, 40], or designing entirely different training and inference mechanisms [41\u201343]. Different from previous works, we focus on exploring the inference speedup in LLMs with a focus on the NER task without the change of model architecture or introducing extra modules. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we delve into the details of PaDeLLM-NER. First, we focus on reframing the instruction tuning tasks as outlined in Section 3.1. Second, we explore the two-step inference process, detailed in Section 3.2. Finally, we discuss the aggregation of results and the technique for ", "page_idx": 2}, {"type": "image", "img_path": "vjw4TIf8Bo/tmp/29e69a1adf5d247199371e08172954071c7ffb541d0b372a4c66c953a24a941c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: PaDeLLM-NER inference paradigm: texts enclosed in frames with identical colors indicate sequences of the same label. Specifically, the texts within solid-lined frames represent the added templates, while those within dashed-lined frames denote the prediction. In Step 1, the model predicts the number of mentions for all labels while in Step 2, it predicts the mentions. By aggregating mentions and labels from all sequences, the final NER results are obtained. Duplicate mentions appearing in different labels are resolved using prediction probabilities. ", "page_idx": 3}, {"type": "text", "text": "eliminating duplicate mentions across labels, which is elaborated in Section 3.3. An illustration of PaDeLLM-NER is shown in Figure 1 and Figure 2. ", "page_idx": 3}, {"type": "text", "text": "3.1 Reframing of Instruction Tuning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Illustration of the reframing is presented in Figure 1. As an example, we use a case from the CoNLL2003 dataset including four labels: person (PER), miscellaneous (MISC), location (LOC), and organization (ORG). The specifics of the input text and the corresponding ground truth are provided in the second row of Table 1. ", "page_idx": 3}, {"type": "text", "text": "During reformulation, a single unstructured text containing all label-mention pairs is split into several sequences. Each new sequence\u2019s output includes the count of mentions for a specified label (denoted as \u201centity type\u201d), followed by the $\\dot{n}^{t h}$ mention of that label (denoted as \u201c<mention $n{>}^{,}$ ). Note that the count of mentions and their respective indices are represented using corresponding digit tokens from the LLM\u2019s vocabulary. Specifically, if there are no mentions, the model is trained to immediately predict the $\\begin{array}{r}{\\mathbf{\\Psi}^{\\leftarrow}\\mathbf{<}e o s\\mathbf{>}\\mathbf{\\Psi}^{\\\"}}\\end{array}$ token, bypassing the need to predict mentions. ", "page_idx": 3}, {"type": "text", "text": "Therefore, in this example, one original training data is transformed into five new training data entries. These include two for predicting $\\mathrm{~\\textdegree~}^{\\ast}L O C^{\\,,\\ast}$ (with 2 mentions), one for predicting \u201cMISC\u201d (with 1 mention), one for predicting $^{\\epsilon}P E R^{\\;,}$ (with 1 mention), and one for predicting $^{\\ast}O R G^{\\,,}$ (with 0 mentions, directly predicting $\\cdots<e o s>\"$ ). Moreover, the number of mentions for each label and the text corresponding to each mention index can be easily obtained from the original ground truth, meaning that the number of new examples depends on the ground truth of that particular example. ", "page_idx": 3}, {"type": "text", "text": "With the newly reformulated training examples, we then apply the standard instruction tuning procedure. The model takes a sequence of text $t_{1},t_{2},\\dots,t_{T}$ consisting of input unstructured text and output structured label-mention pair. The optimization objective is cross-entropy loss $\\mathcal{L}$ which can be defined as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}=-\\frac{1}{T}\\sum_{i=1}^{T}\\log P\\left(t_{i}\\mid t_{1},t_{2},\\ldots,t_{i-1}\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $P\\left(t_{i}\\mid t_{1},t_{2},\\ldots,t_{i-1}\\right)$ represents the probability of $i^{t h}$ token $t_{i}$ given the sequence of preceding tokens $t_{1},t_{2},\\ldots,t_{i-1}$ , as predicted by the model. Note that loss calculation begins from the number of mention tokens (i.e., texts enclosed by dashed-line frames). Theoretically, loss from text spans such as \u201c<mention $n{>}$ \u201d could be ignored during this calculation, since they simply prompt the mention\u2019s order, which does not necessarily need to be generated by the model. However, our ablation studies show that ignoring these texts has negligible impact on model performance, a point further discussed in Appendix A. Therefore, we adhere to the standard instruction tuning procedure. This reformulation allows the model to focus one label-mention pair at a time, shortening the generated length per sequence. More details are shown in Appendix C. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.2 Inference of Label-Mention Pairs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Given a trained LLM, we propose a two-step inference approach: firstly, to predict the number of mentions for a specific label based on the prompt; and secondly, given the label and provided index to precisely identify the corresponding mention. ", "page_idx": 4}, {"type": "text", "text": "Figure 2 shows the overview of PaDeLLM-NER inference. In Step 1, the model predicts the total number of mentions for each label in the input, based on the label prompt. A separate token $^{\\star}\\!\\!\\setminus\\!\\!\\!\\{^,\\!\\!\\!\\!\\!\\Psi^{\\,\\prime}\\!\\!\\!\\}^{\\star}$ signals the completion of this count prediction. If no mentions of the given label exist, the model generates an $^{\\ast}{<}e o s{>}^{\\,,}$ token, skipping Step 2 for that label. In Step 2, following adding the predicted mention count to the input, mention indexes templates are appended. Formally, if the predicted number of mention is $m$ , then \u201c<mention $n{>}^{\\,,}$ , indicating the $n^{\\ddag\\hat{h}}$ mention of the specified label, is appended for each $n$ within the set $\\{1,2,3,\\ldots,m\\}$ and $n$ is an integer. Subsequently, the corresponding mention is generated by the model conditioned on preceding tokens. Note that the decoding of all label-mention pairs occurs in parallel, allowing for their simultaneous generation. Additionally, to justify the efficacy of the proposed two-step inference approach, we also implement a one-step parallel decoding method. In this approach, multiple mentions of the same label are predicted in a single sequence and compared to the two-step method in a preliminary experiment. Further details are provided in the Appendix A. ", "page_idx": 4}, {"type": "text", "text": "In practice, if there are sufficient GPU resources, the inference for the number of mentions for each label, as well as the subsequent inference for the mention text spans, can be allocating on separate GPUs. If GPU resources are limited, the inference can also be deployed on a single GPU using batch inference, facilitating parallel decoding. Using Figure 2 as an example, in Step 1, the batch size is four, as there are four labels in the dataset. In Step 2, the batch size is five, reflecting the five label-mention pairs determined in Step 1 (i.e., 1 in \u201cPER\u201d, 2 in \u201cMISC\u201d, 2 in ${}^{\\prime}L O C^{\\prime\\prime}$ ). This parallel decoding strategy is effective in reducing inference latency, especially in scenarios where inputs are received in a streaming manner. ", "page_idx": 4}, {"type": "text", "text": "3.3 Removal of Duplicate Mentions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Unlike autoregressive decoding, where subsequent label-mention pairs can attend preceding ones, PaDeLLM-NER generates each label-mention pair independently. This inference strategy means that the model might generate mentions erroneously repeated in multiple labels. As exemplified in Figure 2, the model correctly predicts the first mention of $^{\\ast}L O C^{\\ast}$ as \u201cItaly\u201d, but it also incorrectly predicts the second mention of \u201cMISC\u201d as \u201cItaly\u201d. ", "page_idx": 4}, {"type": "text", "text": "To address the issue of duplicate mentions, we suggest employing prediction probability to remove repeated mentions. Specifically, we calculate the prediction probability for each instance of the mention. This is done using the formula: $\\begin{array}{r}{P=\\prod_{i=b}^{e^{-}}P(t_{i}|t_{1},\\bar{t}_{2},\\dots,t_{i-1})}\\end{array}$ where $b$ represents the starting token index of the mention text, and $e$ denotes the ending token index. Then, for a mention that appears in multiple labels, the mention instance with the highest probability will be preserved. As illustrated in Figure2, \u201cItaly\u201d is categorized as \u201cMISC\u201d with only a 0.61 probability, which is lower than that for $^{\\ast}L O C^{\\ast}$ , resulting in its removal. In practice, the probability of each token can be calculated concurrently with token generation. Consequently, this method enables an efficient and accurate identification of duplicate mentions without incurring additional costs. The effectiveness of this de-duplication approach is further explored in Appendix A. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we showcase the effectiveness of PaDeLLM-NER in terms of prediction quality and inference acceleration through experiments. ", "page_idx": 4}, {"type": "text", "text": "4.1 Setup", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "\u2022 Zero-shot Datasets: To align with the methodology proposed by [44], we train PaDeLLM using the Pile-NER dataset [45]. This dataset comprises around 240,000 entities categorized into 13,000 distinct types, derived from the Pile Corpus [46]. The passages in Pile-NER are enhanced through processing with ChatGPT, which facilitates the transparent generation of inherent entities. For assessing the model\u2019s zero-shot capabilities on previously unseen entity categories, following [30, 44, 45] we select two established benchmarks: CrossNER [47] and MIT [48].   \n\u2022 Supervised Datasets: we evaluate our method on supervised English and Chinese NER datasets. Following [30, 49, 50], English datasets include the general domain flat NER CoNLL2003 [51], the nested NER ACE2005 [52], and the biomedical nested NER GENIA [53]. Following [6, 54, 55], Chinese datasets include four commonly used general domain flat NER benchmarks Resume [56], Weibo [57], MSRA [58] and Ontonotes 4.0 [59] and two vertical industrial domain flat NER datasets YouKu [60] and Ecommerce [61]. The statistics of all datasets are shown in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "Training setup We employ pre-trained version of Llama2-7b [11] and Baichuan2-7b [13] as base models for English and Chinese study respectively. Additional implementation details are in Appendix D. ", "page_idx": 5}, {"type": "text", "text": "Inference setup For all generative models, we use greedy search with a beam size of 1, a maximum of 512 new tokens, and a temperature of 1.0. As described in Section 3.2, for PaDeLLM-NER, we adopt two inference settings: (1) each example is inferred on multiple GPUs to implement parallel decoding (i.e., each sequence is assigned on one GPU), termed as $\\mathbf{PaDeLLM_{Multi}}$ ; and (2) each example is inferred on a single GPU, employing batch decoding for parallel decoding, termed as $\\mathbf{PaDeLLM_{Batch}}$ . Note that for $\\mathrm{\\DeltaPaDeLLM_{Multi}}$ , we sequentially predict each sequence of one example to simulate parallel decoding on multiple GPUs. ", "page_idx": 5}, {"type": "text", "text": "Baselines The baseline used in our experiments include: ", "page_idx": 5}, {"type": "text", "text": "\u2022 Inference Latency Baselines: As the primary focus of this work is on reducing inference latency in NER tasks using LLMs, we compare our method, PaDeLLM-NER, with traditional autoregressive approaches. As mentioned in Section 1, the main points of comparison are autoregressive structured output formats used in [3, 4] and [5\u20137], referred to respectively as $\\mathbf{AutoReg}_{\\mathrm{Aug}}$ and AutoRegStruct, as these are the approaches very close to our system. We reimplemented all these methods for both English and Chinese datasets, utilizing the same pre-trained LLMs as in PaDeLLM-NER.   \n\u2022 Zero-shot Baselines: LLMs are known for their generalizability, therefore, following Ding et al. [44], we we also evaluate the zero-shot performance of PaDeLLM. Several most recent SOTA LLM-based approaches are selected as strong baselines as their great generalizability in zero-shot NER scenarios including GoLLIE-7B [30], UniNER-7B [45], GLiNER-L [62], GNER-LLaMA-7B [44].   \n\u2022 Supervised Baselines: We compare our approach with other recent SOTA supervised approaches, including BINDER [50], Gollie [30], and DeepStruct [49] for English benchmarks, as well as $\\mathbf{W}^{\\bar{2}}\\mathbf{N}\\mathbf{E}\\mathbf{R}$ [63], NEZHA-BC [54], and SSCNN [55] for Chinese benchmarks, to show PaDeLLM-NER\u2019s efficacy in prediction quality. ", "page_idx": 5}, {"type": "text", "text": "More details on the re-implementation and model size of each method are provided in Appendix D. ", "page_idx": 5}, {"type": "text", "text": "Evaluation Our evaluation encompasses two dimensions: prediction quality and acceleration of NER inference. For assessing prediction quality, in line with Lu et al. [5], Wang et al. [7], we employ the micro F-score. ", "page_idx": 5}, {"type": "table", "img_path": "vjw4TIf8Bo/tmp/9ab40ec97627d0759b5f248e9b115820b41262d355ccd1f104b655cc464f1efc.jpg", "table_caption": [], "table_footnote": ["Table 2: Comparison of inference latency (in milliseconds) between PaDeLLM-NER and baseline methods. Underscored font is the second-best method, while a bold font is the best method, also applied to subsequent tables. "], "page_idx": 6}, {"type": "image", "img_path": "vjw4TIf8Bo/tmp/6737efc15d6cc557beaaf01a5b0c82585a8d9510bf221a570a020fcc3229fef5.jpg", "img_caption": ["Figure 3: Speedup of PaDeLLM-NER compared to Autoregressive methods. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Following Ning et al. [20], we evaluate inference speed using latency (in milliseconds). We record the latency with the code: start $=$ time.time(); model.generate(); latenc $\\nu=t i m e.t i m e()\\cdot s t a r t$ . In PaDeLLM-NER, we add the latency of mention counting and label-mention pair generation as the latency of each sequence. The final latency for the example is determined by the highest latency across sequences, as the user can only obtain the result of an example when the slowest sequence is generated. We conduct experiments three times and use the average result to alleviate the effect of randomness. We also report the average sequence length (tokenized) to clearly demonstrate the extent of sequence length reduction in Appendix E. Evaluations of all models were performed on the same NVIDIA A100 GPU. ", "page_idx": 6}, {"type": "table", "img_path": "vjw4TIf8Bo/tmp/f423dbabb309c1e0d27ba5e8858b7bee400609e8f51d4c19e19020dfb6e02849.jpg", "table_caption": ["4.2 Main Results "], "table_footnote": ["Table 3: Comparison of prediction quality with recent SOTA models in zero-shot setting. "], "page_idx": 6}, {"type": "text", "text": "Evaluation on inference latency We investigate how PaDeLLM-NER reduces the end-to-end latency compared to baseline methods. Table 2 presents the average latency for each method across all datasets. First, it\u2019s clear that both $\\mathrm{\\DeltaPaDeLLM_{Multi}}$ and $\\mathrm{PaDeLLM_{Batch}}$ significantly reduce inference latency when compared to baseline methods, as highlighted by the substantial reduction in mean latency. For example, the mean latency reduction achieved between $\\mathrm{{PaDeLLM}_{M u l t i}}$ and AutoRegStruct stands at an impressive 791.55 ms, underscoring the significant improvement. ", "page_idx": 6}, {"type": "text", "text": "To more intuitively quantify the latency reduction of PaDeLLM-NER, we break down its speedup across different datasets in comparison to baseline methods in Figure 3. The speedup is computed by dividing the latency of baselines by the latency of PaDeLLM-NER. We can observe that PaDeLLM", "page_idx": 6}, {"type": "table", "img_path": "vjw4TIf8Bo/tmp/82482a22a5c5dc9e44ee8d2a59de048d038b7dcd3edc32cb9d12d92660e20c7b.jpg", "table_caption": [], "table_footnote": ["Table 4: Comparison of prediction quality with recent SOTA methods on English supervised datasets. "], "page_idx": 7}, {"type": "text", "text": "NER consistently show a speedup over baseline methods across all datasets. The highest speedup is observed in the Weibo dataset when comparing AutoRegStruct vs. PaDeLLMMulti, with a speedup of $10.22\\mathrm{x}$ . When we narrow our focus to the comparison between PaDeLLMBatch and the baseline methods, considering these methods utilize a single GPU for inference, we can still observe substantial speedup ranging from 1.76x to $4.73\\mathrm{x}$ . The speedup factor varies across different datasets, suggesting that the efficiency gains of PaDeLLM-NER may be influenced by the characteristics of each dataset. Interestingly, we can observe that the $\\mathrm{PaDeLLM_{Batch}}$ is slower than PaDeLLMMulti (378.40 ms vs. $223.57\\,\\mathrm{ms}$ ), more analysis about this is shown in Section 5. ", "page_idx": 7}, {"type": "text", "text": "Overall, the Table 2 and Figure 3 suggest that PaDeLLM-NER significantly reduces latency compared to autoregressive methods, though the extent of this reduction varies by dataset and the specific baseline method it\u2019s compared to. ", "page_idx": 7}, {"type": "text", "text": "Evaluation on zero-shot prediction quality Table 3 compares the prediction quality of different models across various domains like AI, Literature, Music, Politics, Science, Movie, and Restaurant in a zero-shot setting. Among all these methods, GoLLIE-7B scores range from 43.4 in Restaurants to 67.8 in Music, with an average of 60.02. UniNER-7B has lower scores, particularly in Restaurants (31.7), and averages 53.45. GLiNER-L shows a fairly balanced performance with a high of 72.6 in Politics and an average of 60.92. GNER-LLaMA-7B excels in Music with a 75.7 score and has the highest average of all at 66.05. Our model, PaDeLLM-NER, which consistently performs well across all domains. It has the second-best average score of 61.68, following GNER-LLaMA7B. This highlights that while it is not the top performer, it offers robust and balanced prediction capabilities across a diverse set of topics in zero-shot setting. Note that the training of PaDeLLM-NER does not incorporate the additional task scheme prompt for describing unseen entities as used in GNER-LLaMA-7B [44], which may account for the observed differences in performance. ", "page_idx": 7}, {"type": "text", "text": "Evaluation on supervised prediction quality Table 4 and Table 5 present the micro F-scores of PaDeLLM-NER in comparison to other SOTA methods on supervised datasets. Notably, the micro F-scores for both $\\mathrm{\\DeltaPaDeLLM_{Multi}}$ and $\\mathrm{PaDeLLM_{Batch}}$ are identical. Initially, it is evident that encoderbased methods surpass LLM-based approaches, such as AutoReg and PaDeLLM-NER, within the supervised context. Nonetheless, the strength of LLM-based methods lies not in their performance under task-specific supervised settings, but rather in their superior zero-shot capabilities, which compensates for their relative shortcomings in supervised scenarios. Nevertheless, PaDeLLM-NER demonstrates SOTA performance on certain task-specific datasets, exemplified by its exceptional results on the Youku dataset. ", "page_idx": 7}, {"type": "text", "text": "Upon comparing PaDeLLM-NER with AutoReg, both of which are LLM-based methods, it becomes evident that PaDeLLM-NER outperforms AutoReg across both English and Chinese supervised datasets, as evidenced by its superior mean F-score. This outcome indicates that PaDeLLM-NER not only achieves lower inference latency but also maintains a higher level of prediction quality when contrasted with baseline methods. ", "page_idx": 7}, {"type": "text", "text": "In summary, the results presented in Table 2, 3, 4 and 5, demonstrate that our approach not only maintains superior prediction quality in both zero-shot and supervised environments but also significantly reduces inference latency. ", "page_idx": 7}, {"type": "table", "img_path": "vjw4TIf8Bo/tmp/a2ce8aab64ee42c554ff644ba29ad63f87fa47d0f22c5c219a28f05995f53407.jpg", "table_caption": ["Table 5: Comparison of prediction quality with recent SOTA methods on English supervised datasets. \u201c\\*\u201d indicates that results are not directly comparable. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Speedup Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "One concern noted is that batch inference does not speed up as much as inference distributed across multiple GPUs. This observation is consistent with our expectations and supported by Chen et al. [19] who found that batch inference in LLMs tends to be slower than single sequence inference under identical conditions, likely due to limitations in GPU memory bandwidth [64]. ", "page_idx": 8}, {"type": "text", "text": "Transitioning from these performance considerations, it\u2019s noteworthy that PaDeLLM-NER is selfcontained and can be seamlessly integrated with various generative architectures, including wellestablished decoder-only models [8\u201313] and recent innovations like RWKV [65], as well as multimodal LLMs [66, 67] for tasks like Key Information Extraction tasks [68], all without needing architectural changes or additional data/modules. Also, it could be incorporated with off-the-shelf LLMs such as ChatGPT [27] and Claude-2 through prompt engineering without the need for further training, an aspect we plan to explore in future research. ", "page_idx": 8}, {"type": "text", "text": "6 Data Contamination Concerns ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Since we are using LLMs as our foundational models, trained on extensive datasets from various online sources [11, 13], there is a chance that the models may have encountered parts of our evaluation sets during their pre-training phase, albeit unintentionally. This could potentially affect our experimental results. However, the primary focus of our experiments is the comparison of our proposed method with baseline methods. Given that these methods employ the same LLM as the base model, data contamination is unlikely to significantly impact the results. ", "page_idx": 8}, {"type": "text", "text": "7 Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "One clear disadvantage of PaDeLLM-NER is the multiplication of training examples from one to $m*n$ , where $m$ is the label count and $n$ the mention count. Despite this, given that low latency is a major bottleneck in LLMs, trading longer training for lower latency is justifiable. Also, given the impressive generalization ability of LLMs, we believe that this method can be smoothly adapted to few-shot scenarios requiring less computation resources, which will be explored in future work. ", "page_idx": 8}, {"type": "text", "text": "Additionally, accurately counting the number of mentions remains a challenge for LLMs as discussed in Appendix F. This issue could be alleviated by implementing a specialized counting model dedicated to this task [69]. Another drawback is that reformulating label-mention pairs loses location information, which hinders tasks like downstream editing. We will address this in future work. Additionally, the de-duplication mechanism is overly aggressive, potentially removing mentions that can appear under different labels\u2014a common issue in real-world applications (see Appendix A for more details). ", "page_idx": 8}, {"type": "text", "text": "Finally, there are several instances of re-computation within the pipeline that can be optimized. Specifically, input texts are encoded multiple times throughout the process. During batch decoding, certain sequences may encounter the $^{\\ast\\epsilon}{<}e o s{>}^{\\mathrm{~!}}$ \u201d token earlier, but due to the nature of batch inference, these sequences continue to predict. We plan to improve this in the future by implementing enhancements like KV cache reuse and batch inference with an early quit mechanism, among other strategies. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this study, we present PaDeLLM-NER, a parallel decoding framework for NER within LLMs. This approach enables batch parallel decoding of all label-mention pairs, significantly cutting down inference time by 1.76 to 10.22 times without sacrificing prediction accuracy. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Ruotian Ma, Minlong Peng, Qi Zhang, Zhongyu Wei, and Xuanjing Huang. Simplify the usage of lexicon in Chinese NER. In Proceedings of the ACL 2020, pages 5951\u20135960, Online, July 2020. doi: 10.18653/v1/2020.acl-main.528. URL https://aclanthology.org/2020. acl-main.528.   \n[2] Wei Liu, Xiyan Fu, Yue Zhang, and Wenming Xiao. Lexicon enhanced Chinese sequence labeling using BERT adapter. In Proceedings of the ACL 2021 and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 5847\u20135858, Online, August 2021. doi: 10.18653/v1/2021.acl-long.454. URL https://aclanthology. org/2021.acl-long.454.   \n[3] Giovanni Paolini, Ben Athiwaratkun, Jason Krone, Jie Ma, Alessandro Achille, RISHITA ANUBHAI, Cicero Nogueira dos Santos, Bing Xiang, and Stefano Soatto. Structured prediction as translation between augmented natural languages. In International Conference on Learning Representations, 2020.   \n[4] Sarkar Snigdha Sarathi Das, Haoran Zhang, Peng Shi, Wenpeng Yin, and Rui Zhang. Unified low-resource sequence labeling by sample-aware dynamic sparse finetuning. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6998\u20137010, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.433. URL https:// aclanthology.org/2023.emnlp-main.433.   \n[5] Yaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu Lin, Xianpei Han, Le Sun, and Hua Wu. Unified structure generation for universal information extraction. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5755\u20135772, 2022.   \n[6] Jinghui Lu, Rui Zhao, Brian Mac Namee, and Fei Tan. Punifiedner: A prompting-based unified ner system for diverse datasets. Proceedings of the AAAI Conference on Artificial Intelligence, 37(11):13327\u201313335, Jun. 2023. doi: 10.1609/aaai.v37i11.26564. URL https: //ojs.aaai.org/index.php/AAAI/article/view/26564.   \n[7] Xiao Wang, Weikang Zhou, Can Zu, Han Xia, Tianze Chen, Yuansen Zhang, Rui Zheng, Junjie Ye, Qi Zhang, Tao Gui, et al. Instructuie: Multi-task instruction tuning for unified information extraction. arXiv preprint arXiv:2304.08085, 2023.   \n[8] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020. URL http://jmlr.org/papers/v21/20-074.html.   \n[9] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786, 2022.   \n[10] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[11] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[12] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.   \n[13] Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023.   \n[14] Jinghui Lu, Haiyang Yu, Yanjie Wang, Yongjie Ye, Jingqun Tang, Ziwei Yang, Binghong Wu, Qi Liu, Hao Feng, Han Wang, et al. A bounding box is worth one token: Interleaving layout and text in a large language model for document understanding. arXiv preprint arXiv:2407.01976, 2024.   \n[15] Jinghui Lu, Dongsheng Zhu, Weidong Han, Rui Zhao, Brian Mac Namee, and Fei Tan. What makes pre-trained language models better zero-shot learners? In Anna Rogers, Jordan BoydGraber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2288\u20132303, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.128. URL https://aclanthology.org/2023.acl-long.128.   \n[16] Jinghui Lu, Linyi Yang, Brian Namee, and Yue Zhang. A rationale-centric framework for human-in-the-loop machine learning. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6986\u20136996, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.481. URL https://aclanthology.org/2022.acl-long.481.   \n[17] Hao Feng, Zijian Wang, Jingqun Tang, Jinghui Lu, Wengang Zhou, Houqiang Li, and Can Huang. Unidoc: A universal large multimodal model for simultaneous text detection, recognition, spotting and understanding. arXiv preprint arXiv:2308.11592, 2023.   \n[18] Shuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang, Fei Wu, Tianwei Zhang, Jiwei Li, and Guoyin Wang. Gpt-ner: Named entity recognition via large language models. arXiv preprint arXiv:2304.10428, 2023.   \n[19] Lequn Chen, Zihao Ye, Yongji Wu, Danyang Zhuo, Luis Ceze, and Arvind Krishnamurthy. Punica: Multi-tenant lora serving. arXiv preprint arXiv:2310.18547, 2023.   \n[20] Xuefei Ning, Zinan Lin, Zixuan Zhou, Huazhong Yang, and Yu Wang. Skeleton-of-thought: Large language models can do parallel decoding. arXiv preprint arXiv:2307.15337, 2023.   \n[21] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. GPT3.int8(): 8-bit matrix multiplication for transformers at scale. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id $=$ dXiGWqBoxaD.   \n[22] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023.   \n[23] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 19274\u201319286. PMLR, 2023.   \n[24] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[25] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045\u20133059, 2021.   \n[26] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.   \n[27] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[28] Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang, Tong Xu, Xiangyu Zhao, Xian Wu, Yefeng Zheng, and Enhong Chen. Large language models for generative information extraction: A survey. arXiv preprint arXiv:2312.17617, 2023.   \n[29] Tingyu Xie, Qi Li, Jian Zhang, Yan Zhang, Zuozhu Liu, and Hongwei Wang. Empirical study of zero-shot ner with chatgpt. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7935\u20137956, 2023.   \n[30] Oscar Sainz, Iker Garc\u00eda-Ferrero, Rodrigo Agerri, Oier Lopez de Lacalle, German Rigau, and Eneko Agirre. Gollie: Annotation guidelines improve zero-shot information-extraction. arXiv preprint arXiv:2310.03668, 2023.   \n[31] Dhananjay Ashok and Zachary C Lipton. Promptner: Prompting for named entity recognition. arXiv preprint arXiv:2305.15444, 2023.   \n[32] Jiawei Chen, Yaojie Lu, Hongyu Lin, Jie Lou, Wei Jia, Dai Dai, Hua Wu, Boxi Cao, Xianpei Han, and Le Sun. Learning in-context learning for named entity recognition. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13661\u201313675, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. acl-long.764. URL https://aclanthology.org/2023.acl-long.764.   \n[33] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2022.   \n[34] Dan Friedman, Alexander Wettig, and Danqi Chen. Learning transformer programs. arXiv preprint arXiv:2306.01128, 2023.   \n[35] Hanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efficient sparse attention architecture with cascade token and head pruning. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA), pages 97\u2013110. IEEE, 2021.   \n[36] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. OPTQ: Accurate quantization for generative pre-trained transformers. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=tcbBPnfwxS.   \n[37] Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin, and Emanuele Rodola. Accelerating transformer inference for translation via parallel decoding. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12336\u201312355, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.689. URL https://aclanthology.org/2023.acl-long.689.   \n[38] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 38087\u201338099. PMLR, 2023.   \n[39] Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli. Depth-adaptive transformer. In International Conference on Learning Representations, 2019.   \n[40] Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Tran, Yi Tay, and Donald Metzler. Confident adaptive language modeling. Advances in Neural Information Processing Systems, 35:17456\u201317472, 2022.   \n[41] Tian Lan, Deng Cai, Yan Wang, Heyan Huang, and Xian-Ling Mao. Copy is all you need. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id $\\cdot$ CROlOA9Nd8C.   \n[42] Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang, Rangan Majumder, and Furu Wei. Inference with reference: Lossless acceleration of large language models. arXiv preprint arXiv:2304.04487, 2023.   \n[43] Xinpeng Zhang, Ming Tan, Jingfan Zhang, and Wei Zhu. Nag-ner: a unified non-autoregressive generation framework for various ner tasks. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track), pages 676\u2013686, 2023.   \n[44] Yuyang Ding, Juntao Li, Pinzheng Wang, Zecheng Tang, Bowen Yan, and Min Zhang. Rethinking negative instances for generative named entity recognition. arXiv preprint arXiv:2402.16602, 2024.   \n[45] Wenxuan Zhou, Sheng Zhang, Yu Gu, Muhao Chen, and Hoifung Poon. Universalner: Targeted distillation from large language models for open named entity recognition. ICLR 2024, 2023.   \n[46] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.   \n[47] Zihan Liu, Yan Xu, Tiezheng Yu, Wenliang Dai, Ziwei Ji, Samuel Cahyawijaya, Andrea Madotto, and Pascale Fung. Crossner: Evaluating cross-domain named entity recognition. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 13452\u201313460, 2021.   \n[48] Jingjing Liu, Panupong Pasupat, Scott Cyphers, and Jim Glass. Asgard: A portable architecture for multilingual dialogue systems. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 8386\u20138390. IEEE, 2013.   \n[49] Chenguang Wang, Xiao Liu, Zui Chen, Haoyun Hong, Jie Tang, and Dawn Song. DeepStruct: Pretraining of language models for structure prediction. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Findings of the Association for Computational Linguistics: ACL 2022, pages 803\u2013823, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.67. URL https://aclanthology.org/2022. findings-acl.67.   \n[50] Sheng Zhang, Hao Cheng, Jianfeng Gao, and Hoifung Poon. Optimizing bi-encoder for named entity recognition via contrastive learning. In The Eleventh International Conference on Learning Representations, 2022.   \n[51] Erik F. Tjong Kim Sang and Fien De Meulder. Introduction to the conll-2003 shared task. In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003 -, Jan 2003. doi: 10.3115/1119176.1119195. URL http://dx.doi.org/10.3115/1119176. 1119195.   \n[52] Andy Kirkpatrick. Researching english as a lingua franca in asia: The asian corpus of english (ace) project. Asian Englishes, 13(1):4\u201318, 2010.   \n[53] Tomoko Ohta, Yuka Tateisi, Jin-Dong Kim, Hideki Mima, and Junichi Tsujii. The genia corpus: An annotated research abstract corpus in molecular biology domain. In Proceedings of the human language technology conference, pages 73\u201377. Citeseer, 2002.   \n[54] Xin Zhang, Yong Jiang, Xiaobin Wang, Xuming Hu, Yueheng Sun, Pengjun Xie, and Meishan Zhang. Domain-specific ner via retrieving correlated samples. In Proceedings of the 29th International Conference on Computational Linguistics, pages 2398\u20132404, 2022.   \n[55] Miao Zhang and Ling Lu. A local information perception enhancement\u2013based method for chinese ner. Applied Sciences, 13(17):9948, 2023.   \n[56] Yue Zhang and Jie Yang. Chinese NER using lattice LSTM. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1554\u20131564, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1144. URL https://aclanthology.org/P18-1144.   \n[57] Nanyun Peng and Mark Dredze. Named entity recognition for chinese social media with jointly trained embeddings. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Jan 2015. doi: 10.18653/v1/d15-1064. URL http://dx.doi.org/10. 18653/v1/d15-1064.   \n[58] Gina-Anne Levow. The third international Chinese language processing bakeoff: Word segmentation and named entity recognition. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 108\u2013117, Sydney, Australia, July 2006. Association for Computational Linguistics. URL https://aclanthology.org/W06-0115.   \n[59] Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Hwee Tou Ng, Anders Bj\u00f6rkelund, Olga Uryupina, Yuchen Zhang, and Zhi Zhong. Towards robust linguistic analysis using OntoNotes. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 143\u2013152, Sofia, Bulgaria, August 2013. Association for Computational Linguistics. URL https://aclanthology.org/W13-3516.   \n[60] Zhanming Jie, Pengjun Xie, Wei Lu, Ruixue Ding, and Linlin Li. Better modeling of incomplete annotations for named entity recognition. In Proceedings of NAACL, 2019.   \n[61] Ruixue Ding, Pengjun Xie, Xiaoyan Zhang, Wei Lu, Linlin Li, and Luo Si. A neural multidigraph model for chinese ner with gazetteers. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1462\u20131467, 2019.   \n[62] Urchade Zaratiana, Nadi Tomeh, Pierre Holat, and Thierry Charnois. Gliner: Generalist model for named entity recognition using bidirectional transformer. arXiv preprint arXiv:2311.08526, 2023.   \n[63] Jingye Li, Hao Fei, Jiang Liu, Shengqiong Wu, Meishan Zhang, Chong Teng, Donghong Ji, and Fei Li. Unified named entity recognition as word-word relation classification. Proceedings of the AAAI Conference on Artificial Intelligence, 36(10):10965\u201310973, Jun. 2022. doi: 10.1609/aaai. v36i10.21344. URL https://ojs.aaai.org/index.php/AAAI/article/view/21344.   \n[64] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774, 2024.   \n[65] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023.   \n[66] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023.   \n[67] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023.   \n[68] Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and CV Jawahar. Icdar2019 competition on scanned receipt ocr and information extraction. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 1516\u20131520. IEEE, 2019.   \n[69] Tiedong Liu and Bryan Kian Hsiang Low. Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks. arXiv preprint arXiv:2305.14201, 2023.   \n[70] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2018.   \n[71] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations, 2017. URL https://openreview. net/forum?id $\\cdot$ Skq89Scxx.   \n[72] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11048\u201311064, 2022.   \n[73] Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. Towards understanding chain-of-thought prompting: An empirical study of what matters. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2717\u20132739, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.153. URL https://aclanthology.org/2023.acl-long.153.   \n[74] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. LIMA: Less is more for alignment. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=KBMOKmX2he. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "table", "img_path": "vjw4TIf8Bo/tmp/30a482b7a1a20f79cd535a177d5488f450b3b0cfed5a6968763567030cf6b213.jpg", "table_caption": [], "table_footnote": ["Table 6: Ablations on ignoring loss and de-duplication. "], "page_idx": 15}, {"type": "text", "text": "In this section, we set out to investigate the effects of the different aspects of PaDeLLM-NER. ", "page_idx": 15}, {"type": "text", "text": "Ignoring text spans in loss As discussed in Section 3.1, during training, it is permissible to overlook the loss of text span \u201c<mention $n{>}^{,}$ , as the model does not need to generate this specific text, which is appended during inference. However, as shown in Table 6 illustrate, omitting these texts has minimal impact on prediction quality. ", "page_idx": 15}, {"type": "text", "text": "One possible explanation is that during training, the more significant challenge for LLMs lies in predicting the appropriate mention texts, rather than their format. As the model can readily learns to correctly position the format \u201c<mention $n{>}^{,}$ , this aspect contributes minimally to the loss computation in training. In this case, computing the loss for all text is almost equivalent to \u201cneglecting\u201d the computation of loss for \u201c<mention $n{>}^{,}$ . ", "page_idx": 15}, {"type": "text", "text": "De-duplication To demonstrate the effectiveness of the de-duplication technique, we established two configurations as detailed in Table 6. The -De-duplication denotes the pipeline operating without the de-duplication technique; $+D e$ -duplicationReverse indicates the pipeline that removes mentions with the highest probability, opposite to the original de-duplication technique. ", "page_idx": 15}, {"type": "text", "text": "Theoretically, PaDeLLM-NER should be the top-performing method, as its de-duplication eliminates noisy mentions, enhancing precision. Following closely is the -De-duplication, allows duplicate mentions to persist. $+D e$ -duplicationReverse ranks lowest since it removes correct mentions and retains incorrect ones, lowering recall and precision simultaneously. As shown in Table 6, the results consistently align with our expectations, thereby verifying the effectiveness of the de-duplication process. Moreover, the difference among these variants is subtle, which can be attributed to the rare cases where duplicate mentions exist. This further highlights the robustness of proposed method. ", "page_idx": 15}, {"type": "text", "text": "We also report statistics in Table 7 and 8 showing that mentions under multiple labels are rare for both ground truth and PaDeLLM predictions. However, we recognize that the de-duplication mechanism can be overly aggressive, potentially removing mentions that appear under multiple labels\u2014a common scenario in real-world applications. In such cases, opting not to use the de-duplication mechanism may be preferable. ", "page_idx": 15}, {"type": "table", "img_path": "vjw4TIf8Bo/tmp/91d3f9641030b516f98cc7f3284f235a8e2bdd8efa70d9d72616c002f99e9aa3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Preliminary experiments for justifying the importance of two-step prediction We conducted preliminary experiment using one-step prediction, where all mentions of the same label are predicted in a single sequence, which is referred to as OneStep in this paper. An example of OneStep parallel decoding is shown in Table 9. Note that the order of mentions is preserved as in the ground truth, following the data from the corresponding dataset. The overall latency for each example is determined by the latency of the slowest sequence. The preliminary experiment is conducted on three English dataset, i.e., CoNLL03, ACE05 and GENIA. ", "page_idx": 15}, {"type": "table", "img_path": "vjw4TIf8Bo/tmp/f7ec600fefa51bc4ae4f013ab0afa85dc82907232a7de2ad894016ea09837039.jpg", "table_caption": [], "table_footnote": ["Table 8: Mentions appear under multiple labels in PaDeLLM prediction. "], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "table", "img_path": "vjw4TIf8Bo/tmp/3e20882e6320288d781b44ab86f95f609623de878eff00f2834ade91b5d6f012.jpg", "table_caption": [], "table_footnote": ["Table 9: Illustration of one-step parallel decoding NER approach. "], "page_idx": 16}, {"type": "table", "img_path": "vjw4TIf8Bo/tmp/a527598ee5be2dae7d4e62000b2e96e85da077000f57c30c0cf3dbba3f853714.jpg", "table_caption": [], "table_footnote": ["Table 10: Comparison of inference latency. "], "page_idx": 16}, {"type": "table", "img_path": "vjw4TIf8Bo/tmp/34531b492bd589adf53df9e90624953e02727d09fc809787540ee0fae2225f59.jpg", "table_caption": [], "table_footnote": ["Table 11: Comparison of prediction quality. "], "page_idx": 16}, {"type": "text", "text": "The results are reported in Table 10 and Table 11. As expected, the inference speed of one-step approach falls between that of the two-step prediction (i.e., PaDeLLM) and the purely autoregressive model. However, the prediction quality is lower compared to the two-step prediction. In other words, PaDeLLM outperforms the one-step approach in both inference speed and prediction quality, which again verifies the efficacy of PaDeLLM. ", "page_idx": 16}, {"type": "text", "text": "Preliminary experiments for zero-shot autoregressive baseline We did not report zero-shot results for AutoR $R\\mathrm{g_{aug}}$ and AutoRegstruct as they are unsuitable for this setting. Preliminary experiments show higher latency and lower F-scores compared to PaDeLLM (see Table 12 for details). ", "page_idx": 17}, {"type": "table", "img_path": "vjw4TIf8Bo/tmp/0ce97ba483fbe4982e86de7eb2a5fc7c8e05af3f6bd0f7e0ce23b29bb8fb5523.jpg", "table_caption": [], "table_footnote": ["Table 12: Comparison of Latency and F-score between PaDeLLM and AutoRegAug under zero-shot scenarios. "], "page_idx": 17}, {"type": "text", "text": "B Dataset Statistics ", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "vjw4TIf8Bo/tmp/d52dcac6890265b5d9e7de874f495e611569e4a014f6e48a21495cea74736ec3.jpg", "table_caption": [], "table_footnote": ["Table 13: Ablations on model scaling up. "], "page_idx": 17}, {"type": "image", "img_path": "vjw4TIf8Bo/tmp/1f25b18b49a04262e5fcf6e7ea40217da8e316c12433d7fa7a335cacc886c016.jpg", "img_caption": ["Figure 4: Percentage of different error types. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "We evaluate our framework on 3 English and 6 Chinese flat/nested NER datasets. In Table 15, we present the detailed statistics. Note that while the statistics of the development set are reported, our training process does not involve the development set. ", "page_idx": 17}, {"type": "text", "text": "For the MSRA dataset, we excluded four outlier instances from the test set due to their excessively high number of names, significantly deviating from typical examples. These outliers not only posed challenges for model inference but also risked distorting the evaluation metrics, potentially leading to an inaccurate assessment of the model\u2019s performance on representative data. ", "page_idx": 17}, {"type": "table", "img_path": "vjw4TIf8Bo/tmp/692bbd6c560a0e0ce22e54920babda50376833c886ec48aac5ae6f4389ac5e04.jpg", "table_caption": [], "table_footnote": ["Table 14: Comparison of the number of generated tokens per sequence by PaDeLLM-NER with baseline methods. "], "page_idx": 18}, {"type": "text", "text": "Also, we perform label mapping to convert ground truth from special tokens to Chinese words following [6]. Further details are provided in Table 16. ", "page_idx": 18}, {"type": "table", "img_path": "vjw4TIf8Bo/tmp/9082e2609a9df925bd7ff3bf7880bc594cd6c1d5b0a632665f020128dd9c5441.jpg", "table_caption": [], "table_footnote": ["Table 15: Dataset Statistics. \u201c#\u201d denotes the amount. For MSRA, we remove four outlier examples in test set. "], "page_idx": 18}, {"type": "table", "img_path": "vjw4TIf8Bo/tmp/b764780c6b6b42e6d057fdc048216843439376d7b96799c9d438748bdcc4ce4f.jpg", "table_caption": [], "table_footnote": ["Table 16: Entity tag of each dataset and the conversion from tag used in dataset to corresponding Chinese natural language. For some tags that are hard to understand, we provide their meaning in brackets. \u201c#\u201d denotes the amount of entity types. "], "page_idx": 18}, {"type": "text", "text": "C Reformulation Examples ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Two compete reformulated examples are presented in Table 17 for English and Chinese, respectively. ", "page_idx": 18}, {"type": "table", "img_path": "vjw4TIf8Bo/tmp/9b7f3f18f51ad634a3b7ba21cf03dc10b5778d51d89a948400cc18895f9c39b5.jpg", "table_caption": ["Table 17: Reformulated examples for English and Chinese dataset, respectively. We provide translations to facilitate understanding. The examples come from CoNLL2003 and MSRA dataset. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "D Implementation Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We train our model on all datasets for 4 epochs, using a batch size of 128 and a learning rate of $1e-5$ , with the AdamW optimizer [70] and a cosine scheduler [71]. The maximum input and output sequence lengths are set to 2048 and 512, respectively. Training is conducted on 8 NVIDIA A100 GPUs. This configuration is applied across all PaDeLLM-NER models, as well as three baseline models: AutoRegAug, AutoRegStruct as well as Onestep baseline reported in preliminary experiment. We also report the model size of each NER method in Table 18 ", "page_idx": 19}, {"type": "table", "img_path": "vjw4TIf8Bo/tmp/ec0cafee179ba7322f4d62745a92456bbab276549adf73d71af9ece03739a6c1.jpg", "table_caption": ["Table 18: Model size of each NER method. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "E Sequence Length Reduction ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Results of average sequence length produced by different approaches are presented in Table 14. Most notably, PaDeLLM-NER generates much shorter sequences than the other models across all datasets. The lengths range from 6.54 on CoNLL20023 to 10.05 on GENIA for English datasets, and from 2.19 on Weibo to 4.87 on Resume for Chinese datasets. The mean length for PaDeLLM-NER is 4.86, which is significantly lower than the means of the other approaches: 35.54 for AutoReg $\\mathrm{Aug}$ and 36.48 for AutoRegStruct. ", "page_idx": 19}, {"type": "text", "text": "In summary, the result shows that PaDeLLM-NER produces much shorter generated sequences compared to the other methods, which is around $13.19\\%$ to $13.67\\%$ of the original length, respectively, indicating higher efficiency in its inference. ", "page_idx": 19}, {"type": "text", "text": "F Error analysis ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "PaDeLLM-NER error analysis For our error analysis, we utilize the ACE2005 dataset. We sample and manually examine 50 erroneous examples for analysis. We seek to identify the root causes of errors, which we have categorized into three types: (1) incorrect mention count, referred to as Count Mismatch; (2) inaccuracies in the mention corresponding to a specific index, termed Index Inaccuracy; and (3) errors in the ground truth data, known as Ground Truth Errors. ", "page_idx": 20}, {"type": "text", "text": "The distribution of each error type is illustrated in Figure 4. It is important to note that a significant portion of the errors stem from inaccuracies in mention counts (i.e., Count Mismatch, about $56.8\\%$ ), underscoring the necessity for enhancements in the model\u2019s counting capabilities. Accurate mention counts are pivotal for the quality of predictions. Overestimating the mention count often leads the model to either repeat the last entity or, more problematically, fabricate an entity, thereby escalating the rate of false positives. Conversely, underestimating the mention count results in the model\u2019s inability to identify some entities, thus increasing the incidence of false negatives. Following closely is the Index Inaccuracy error, indicating that the model sometimes struggles to accurately pinpoint the correct mention for a given index, further emphasizing areas for improvement. ", "page_idx": 20}, {"type": "text", "text": "Interestingly, our analysis reveal that a significant portion of the model\u2019s predictions, specifically $19.3\\%$ , are actually correct, challenging the accuracy of the ground truth data. This observation suggests the presence of inaccuracies within the ground truth, contributing to an elevated rate of false positives. Prior research, as noted in studies by Min et al. [72], Wang et al. [73], Zhou et al. [74], has demonstrated that LLMs predominantly acquire their knowledge during the pre-training phase. These models develop certain \u201ccore beliefs\u201d that tend to align more closely with human judgment. In this context, it appears that the models possess an inherent capability to rectify errors in the ground truth data, demonstrating their potential to improve data accuracy beyond initial human annotation. ", "page_idx": 20}, {"type": "text", "text": "G Model Scaling Up ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "As we increase the model size to 13B, Table 13 presents a mix of results. In datasets like CoNLL2003 and GENIA, the model shows a significant improvement in predictions. In contrast, the results on $A C E2O O5$ are slightly worse. Note that the improvement in GENIA is substantial, at approximately $1.18\\%$ . Based on these findings, it seems reasonable to suggest that continuously scaling up the model size has the potential to maintain the performance that is at least on par, or even superior, especially in specific industrial domains. However, this hypothesis warrants further investigation, involving more families of models [8\u201313] and a broader range of datasets. We leave this exploration for future work. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We have detailed the contributions accurately in the abstract and introduction. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: As discussed in Section. 7, we have listed some limitations of our work and shown corresponding failure cases in the supplementary material. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This work does not include theoretical results. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: In Section 4.1 and Appendix D, we have described the details of implementing and training the proposed model to ensure the reproducibility of our work. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have released the model and code, available at URL masked for anonymous review. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have detailed the experimental setting and implementation details in Appendix B, D and Section 4.1. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [No] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our deep learning model is designed for a complex task (requiring huge computing resources) where traditional error bars are less informative due to the high variability in model training and initialization. We ensured the robustness of our model by fix the random seed during inference. In addition, comparative analysis with baseline models demonstrated improvements in key performance areas, underscoring the practical effectiveness of our approach. We acknowledge the limitation of not using traditional statistical tests and suggest that future work could explore statistical significance in more controlled settings. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have reported the needed computer resources in Section 4.1 of the supplementary material. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The research in this paper conforms with the NeurIPS Code of Ethics in every respect. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] Justification: There is no societal impact of the work performed. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 24}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: All the datasets used in this paper are publicly available and they contain no unsafe images. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: For the used datasets and pre-trained models, we have cited their corresponding works. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper does not release new assets ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]