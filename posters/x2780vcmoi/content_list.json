[{"type": "text", "text": "A polar coordinate system represents syntax in large language models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Pablo Diego-Sim\u00f3n ENS, PSL University, Paris, France pablo-diego.simon@psl.eu ", "page_idx": 0}, {"type": "text", "text": "St\u00e9phane D\u2019Ascoli Meta AI, Paris, France stephane.dascoli@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Emmanuel Chemla ENS, PSL University, Paris, France emmanuel.chemla@ens.psl.eu ", "page_idx": 0}, {"type": "text", "text": "Yair Lakretz ENS, PSL University, Paris, France yair.lakretz@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Jean-R\u00e9mi King Meta AI, Paris, France jeanremi@meta.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Originally formalized with symbolic representations, syntactic trees may also be effectively represented in the activations of large language models (LLMs). Indeed, a \u201cStructural Probe\u201d can find a subspace of neural activations, where syntacticallyrelated words are relatively close to one-another. However, this syntactic code remains incomplete: the distance between the Structural Probe word embeddings can represent the existence but not the type and direction of syntactic relations. Here, we hypothesize that syntactic relations are, in fact, coded by the relative direction between nearby embeddings. To test this hypothesis, we introduce a \u201cPolar Probe\u201d trained to read syntactic relations from both the distance and the direction between word embeddings. Our approach reveals three main findings. First, our Polar Probe successfully recovers the type and direction of syntactic relations, and substantially outperforms the Structural Probe by nearly two folds. Second, we confirm that this polar coordinate system exists in a low-dimensional subspace of the intermediate layers of many LLMs and becomes increasingly precise in the latest frontier models. Third, we demonstrate with a new benchmark that similar syntactic relations are coded similarly across the nested levels of syntactic trees. Overall, this work shows that LLMs spontaneously learn a geometry of neural activations that explicitly represents the main symbolic structures of linguistic theory. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Human languages have long been proposed to systematically follow tree-like structures (Chomsky, 1957; Tesni\u00e8re, 1953). In a sentence, words that are far apart can be syntactically linked. For example \"cats\" is the subject of \"chase\" in the sentence \u201cThe cats in cities chase the mice\u201d. In dependency grammar, the edges of such trees are directed and labelled to indicate the type of syntactic relation between words (\u201ccats\u201d is the subject of \u201cchase\u201d, Fig. 1B). ", "page_idx": 0}, {"type": "text", "text": "Despite their conceptual soundness and alignment with human behavior (Robins, 2013), syntactic trees have long been the crux of a core challenge in cognitive science (Smolensky, 1987): trees are symbolic representations, which can superficially appear incompatible with the vectorial representations of neural networks. This opposition between symbols and vectors has been a major challenge to the unification of linguistic theories on the one hand, and neuroscience and connectionist AI on the other hand. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Recently, Hewitt and Manning (Hewitt and Manning, 2019) proposed an important concept for this issue, by suggesting that the existence of syntactic link between two words may be represented by the distance between their corresponding embeddings. Specifically, their \u201cStructural Probe\u201d consists in finding a subspace of contextualized word embeddings such that the squared euclidean distance between words represents their distance in the dependency tree. They showed that the Structural Probe is most powerful in the intermediate layers of language models: these layers contain a subspace where syntactically-related words are closer together. ", "page_idx": 1}, {"type": "text", "text": "This Structural Probe, however, can only reveal one aspect of dependency trees: namely, the existence of syntactic relations, between word pairs. However, whether and how the direction and the type of syntactic relations are represented in language models remains unknown. ", "page_idx": 1}, {"type": "text", "text": "Here, we hypothesize that syntactic relations are represented by a polar coordinate system, where the existence and type of syntactic relations are represented by distances and direction, respectively (Fig. 1). To test this hypothesis, we introduce a \u201cPolar Probe\u201d: a linear transformation trained such that pairs of words linked by the same dependency type are collinear, while remaining orthogonal to different dependency types. ", "page_idx": 1}, {"type": "image", "img_path": "x2780VcMOI/tmp/14911fd1ca12f95563ebc1b84bb9031a7662177cefa1af09cd2e6124abd034cf.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Dependency trees hypothesized in linguistics and in neural networks. A. According to the dependency grammar framework, the sentences can be described as linear sequences of words connected by an acyclic graph. B. More precisely, such acyclic graph is both labeled and directed, where each edge has a direction, representing the hierarchy of the syntactic relation, and a label, representing the type of syntactic relation. C. The Structural Probe (Hewitt and Manning, 2019) finds a a linear transform (gray plane) of the language model\u2019s activations (here simplified as a 3D space), such that the distance between word embeddings is predicted by their dependency tree. In the Structural Probe subspace, however, it is not possible to distinguish whether \"The cat chases the mouse\" or \"The mouse chases the cat.\" D. Our Polar Probe finds a linear transformation where the angle between syntactically-related word additionally represents the type and direction of these syntactic relations, and the distance codes its presence. The colored arrows indicate orthogonal directions in the Polar-Probe subspace. ", "page_idx": 1}, {"type": "text", "text": "2 Methods ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The goal of this work is to find a linear readout of the activations of pretrained language models which explicitly represents both the presence and the types of syntactic relations between words. ", "page_idx": 1}, {"type": "text", "text": "2.1 Problem statement ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Dependency grammar represents the syntax of a sentence as a symbolic tree. Accordingly, let: ", "page_idx": 1}, {"type": "text", "text": "\u2022 $w_{i}$ be the $i^{t h}$ word in a sentence, \u2022 $d:w_{i},w_{j}\\mapsto\\mathbb{Z}^{+}$ indicate the syntactic distance between two words, ", "page_idx": 1}, {"type": "text", "text": "\u2022 $C$ be the set of syntactic types, ", "page_idx": 2}, {"type": "text", "text": "\u2022 $t:w_{i},w_{j}\\mapsto C$ indicate the type of syntactic relation between directly-connected words. \u2022 $u:w_{i}$ , $w_{j}\\mapsto\\{w_{i},w_{j}\\}$ indicate the head word (and thus direction) of the syntactic relation between directly-connected words. ", "page_idx": 2}, {"type": "text", "text": "Language models based on neural networks represent sentences as sequences of word vectors (a.k.a word embeddings)1. As these embeddings propagate through the layers of the network, they incorporate information about the sentence they belong within, becoming contextualized word embeddings. ", "page_idx": 2}, {"type": "text", "text": "Let $\\mathbf{h}_{i}^{\\ell}\\in\\mathbb{R}^{k}$ be the $i^{t h}$ contextualized word embedding of $w_{i}$ obtained at the output of layer $\\ell$ of a neural network. For simplicity, we will drop the layer index $\\ell$ in what follows, keeping in mind that different layers yield different representations. ", "page_idx": 2}, {"type": "text", "text": "Retrieving syntactic trees from a neural network requires to obtain two read-out functions $\\hat{d}$ : $\\mathbb{R}^{k},\\mathbb{R}^{k}\\mapsto\\bar{\\mathbb{Z}}^{+},\\hat{u}:\\mathbb{R}^{k},\\mathbb{R}^{k}\\rightarrow\\mathbb{R}^{k}$ and $\\hat{t}:\\mathbb{R}^{k},\\mathbb{R}^{k}\\mapsto C$ such that the following conditions are met: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{d}(\\mathbf{h}_{i},\\mathbf{h}_{j})\\approx d(w_{i},w_{j}),\\qquad\\quad\\hat{t}(\\mathbf{h}_{i},\\mathbf{h}_{j})\\approx t(w_{i},w_{j}),\\qquad\\quad\\hat{u}(\\mathbf{h}_{i},\\mathbf{h}_{j})\\approx u(w_{i},w_{j}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "While ${\\hat{d}},{\\hat{t}}$ and $\\hat{u}$ could be any complex functions, the goal of the present work is to identify a simple, interpretable \u201ccode\u201d of how syntactic trees may be represented in vectorial systems. Following the classic definition of a representation as a linearly readable information, we focus on linear operators (DiCarlo and Cox, 2007; Kriegeskorte and Bandettini, 2007; King and Dehaene, 2014). ", "page_idx": 2}, {"type": "text", "text": "2.2 Structural Probe ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In (Hewitt and Manning, 2019), the authors propose to solve $\\hat{d}$ as a distance in a subspace of the contextualized word embeddings. Their \u201cStructural Probe\u201d is a linear transform, $\\mathbf{B}_{S}:\\mathbf{\\dot{\\mathbb{R}}^{k}\\mapsto\\mathbb{R}^{k\\prime}}$ , which projects word embeddings such that their relative distances correspond to their distances in the syntactic tree. Formally, if $\\mathbf{s}_{i,j}\\in\\mathbb{R}^{k}$ is the (directed) \u201cedge embedding\u201d between words $w_{i}$ and $w_{j}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{s}_{i,j}=\\mathbf{h}_{i}-\\mathbf{h}_{j}\\in\\mathbb{R}^{k},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Then, the predicted distance $\\hat{d}\\in\\mathbb{R}^{+}$ between two words can be computed directly as a function of this and no other information coming from the individual word embeddings2 : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{d}(\\mathbf{h}_{i},\\mathbf{h}_{j}):=\\lVert\\mathbf{B}_{S}\\mathbf{s}_{i j}\\rVert^{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Given a set of sentences, the authors extract the set $\\Omega_{S}$ of pairs of words belonging to the same sentence and optimize $\\mathbf{B}_{S}$ to minimize the absolute difference between the distances within the syntactic tree and the distances between the probed word embeddings: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{S}=\\frac{1}{|\\Omega_{S}|}\\sum_{(w_{i},w_{j})\\in\\Omega_{S}}|d(w_{i},w_{j})-\\hat{d}(w_{i},w_{j})|\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Following the development of the Structural Probe, squared Euclidean distances between probed word embeddings are not designed to represent both the presence of dependency relations and their types and directions simultaneously. (Hewitt and Manning, 2019) thus only propose a representational system to solve $\\hat{e}$ , but not $\\hat{t}$ . Whether and how the directed and labeled syntactic tree is encoded in neural networks, thus remains unknown. ", "page_idx": 2}, {"type": "text", "text": "2.3 Angular Probe ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Here, we hypothesize that neural networks use the orientation of the relations formed by connected word pairs to represent the type and direction of their syntactic dependency. ", "page_idx": 3}, {"type": "text", "text": "To test this hypothesis, we first introduce an \u201cAngular Probe\u201d consisting of a linear transform $\\mathbf{B}_{A}:\\mathbb{R}^{k^{\\prime}}\\mapsto\\mathbb{R}^{k^{\\prime\\prime}}$ . By abuse of notation, we denote as $t(\\mathbf{s}_{i,j})\\equiv t(w_{i},w_{j})$ the syntactic type of the corresponding edge. For the Structural Probe, the function $d$ to be recovered is defined on all pairs of words; here the function $t$ to be recovered is only defined on pairs of syntactically linked words, hence we only consider word pairs $(w_{i},w_{j})$ which are indeed syntactically linked. ", "page_idx": 3}, {"type": "text", "text": "We use contrastive learning to align relations of the same type and ensure that different types are pointing to different directions. This approach is designed such that a linear readout could explicitly categorize dependency types. ", "page_idx": 3}, {"type": "text", "text": "Specifically, the objective for the Angular Probe is to ensure that given two edge embeddings s and ${\\bf{s}}^{\\prime}$ of syntactic types $c=t(\\mathbf{s})$ and $c^{\\prime}=\\bar{t}(\\mathbf{s}^{\\prime})$ , the linear transforms $\\mathbf{B}_{A}\\mathbf{s}$ and $\\mathbf{B}_{A}\\mathbf{s}^{\\prime}$ are colinear if $c=c^{\\prime}$ , and orthogonal if $c\\neq c^{\\prime}$ . ", "page_idx": 3}, {"type": "text", "text": "Formally, the Angular Probe is the linear transform which minimizes the contrastive loss: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{A}=\\frac{1}{\\Omega_{A}}\\sum_{s,s^{\\prime}\\in\\Omega_{A}}\\left(\\mathcal{L}(\\mathbf{B}_{A}\\mathbf{s},\\mathbf{B}_{A}\\mathbf{s}^{\\prime})-\\mathbb{1}\\left[t(\\mathbf{s})=t(\\mathbf{s}^{\\prime})\\right]\\right)^{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "\u2022 $\\Omega_{A}$ is the set of edge embeddings of syntactically connected words, ", "page_idx": 3}, {"type": "text", "text": "\u2022 $\\begin{array}{r}{\\mathcal{L}:\\mathbf{x},\\mathbf{y}\\mapsto\\frac{\\mathbf{x}\\cdot\\mathbf{y}}{\\|\\mathbf{x}\\|\\|\\mathbf{y}\\|}}\\end{array}$ is the cosine similarity, $\\begin{array}{r}{\\mathrm{~\\cdot~\\mathbb{1}:\\mathcal{X}\\mapsto\\left\\{\\begin{array}{l l}{\\mathrm{1~if~\\mathcal{X}~i s~t r u e}\\quad}&{\\mathrm{is~the~indicator~function.}}\\\\ {\\mathrm{0~otherwise}}&{}\\end{array}\\right.}}\\end{array}$ ", "page_idx": 3}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We can then construct a prototypical vector for each dependency type, by averaging all the probed edge embeddings belonging to that type: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{V}_{c}=\\sum_{\\mathbf{s}\\in\\Omega_{A}^{(c)}}\\mathbf{B}_{A}\\mathbf{s},\\quad\\Omega_{A}^{(c)}=\\{\\mathbf{s}\\in\\Omega_{A}|t(\\mathbf{s})=c\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The Angular Probe gives us the following function to retrieve the syntactic type of any given edge: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{t}(\\mathbf{h}_{i},\\mathbf{h}_{j}):=\\underset{c}{\\operatorname{argmax}}\\left|\\angle(\\mathbf{B}_{A}\\mathbf{s}_{i j},\\mathbf{V}_{c})\\right|.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We also get the function $\\hat{u}$ to predict the head word (and direction) of the syntactic relation given a predicted type $\\hat{t}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{u}(\\mathbf{h}_{i},\\mathbf{h}_{j}):=\\left\\{\\mathbf{h}_{i}\\quad\\mathrm{if~}\\mathcal{L}(\\mathbf{B}_{A}\\mathbf{s}_{i j},\\mathbf{V}_{\\hat{t}(\\mathbf{h}_{i},\\mathbf{h}_{j})})\\geq0\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "2.4 Polar Probe ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The Angular Probe and the Structural Probe have independent objectives applied to two different datasets: $\\mathcal{L}_{S}$ relies on $\\Omega_{S}$ , which includes all word pairs from any sentence, whereas $\\mathcal{L}_{A}$ relies on $\\Omega_{A}$ , which contains only pairs of words that are syntactically linked. ", "page_idx": 3}, {"type": "text", "text": "Consequently, we define the Polar Probe as a single linear transformation $\\mathbf{B}_{P}:\\mathbb{R}^{k}\\mapsto\\mathbb{R}^{k^{\\prime\\prime}}$ which results from the joint optimization of both the the Angular and Structural objectives. ", "page_idx": 3}, {"type": "text", "text": "This Polar Probe defines the final functions to identify syntactic distances and relation types: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{d}(\\mathbf{h}_{i},\\mathbf{h}_{j}):=\\|\\mathbf{B}_{P}\\mathbf{s}_{i j}\\|^{2}}\\\\ &{\\hat{t}(\\mathbf{h}_{i},\\mathbf{h}_{j}):=\\operatorname*{argmax}_{c}\\left|\\mathcal{L}(\\mathbf{B}_{P}\\mathbf{s}_{i j},\\mathbf{V}_{c})\\right|}\\\\ &{\\hat{u}(\\mathbf{h}_{i},\\mathbf{h}_{j}):=\\left\\{\\mathbf{h}_{i}\\quad\\mathrm{if}\\ \\mathcal{L}(\\mathbf{B}_{P}\\mathbf{s}_{i j},\\mathbf{V}_{\\hat{t}(\\mathbf{h}_{i},\\mathbf{h}_{j})})\\geq0\\right.}\\\\ &{\\hat{u}(\\mathbf{h}_{i},\\mathbf{h}_{j}):=\\left\\{\\mathbf{h}_{j}\\quad\\mathrm{if}\\ \\mathcal{L}(\\mathbf{B}_{P}\\mathbf{s}_{i j},\\mathbf{V}_{\\hat{t}(\\mathbf{h}_{i},\\mathbf{h}_{j})})<0\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Therefore, the Polar Probe minimizes the following loss function, with $\\lambda$ a hyper-parameter weighing the Angular objective. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\underset{\\mathbf{B}_{P}}{\\mathrm{argmin}}\\,\\mathcal{L}_{S}+\\lambda\\mathcal{L}_{A}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "2.5 Data ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Natural dataset. We consider natural sentences extracted from the English Web Treebank dataset (Silveira et al., 2014)3. This corpus contains 254,820 words from 16,622 sentences, sourced from a diverse array of web media genres, including weblogs, newsgroups, emails or reviews. All the sentences in the dataset are manually annotated according to the Universal Dependencies framework (Nivre et al., 2017), where each word is a node, each syntactic link is a labelled directed edge, and the syntactic tree is acyclic. ", "page_idx": 4}, {"type": "text", "text": "Sentences containing email or web addresses are excluded from the dataset. Such fliter removes noisy sentences not interesting from a syntactic point of view. We follow the default splitting provided by the English Web TreeBank resulting in a total of 11827 sentences for training, 1851 for validation, and 1869 for testing. ", "page_idx": 4}, {"type": "text", "text": "Controlled dataset. To precisely evaluate our approach on well-controlled sentences, we designed a dataset, extending previous work (Lakretz et al., 2021b), comprising 100 sentences built with a long-nested structure (e.g., \u201cThe book that the boy besides the car reads fascinates my teacher\u201d). In these sentences, a subordinate clause branches off the main phrase. There is also a constituent \u2018besides the car\u2019 that forms a branch inside the subordinate clause, adding further complexity to the syntactic structure. ", "page_idx": 4}, {"type": "text", "text": "This controlled dataset is designed to clarify how the Polar Probe reconstructs the syntactic tree in complex conditions, but conditions well theorized in linguistics. In particular, we can create several variations of these long-nested sentences. ", "page_idx": 4}, {"type": "text", "text": "\u2022 Short: \u201cThe book fascinates my teacher\u201d \u2022 Relative clause: \u201cThe book that the boy reads fascinates my teacher\u201d \u2022 Long-nested: \u201cThe book that the boy besides the car reads fascinates my teacher\u201d ", "page_idx": 4}, {"type": "text", "text": "Thanks to this dataset we can study whether the Polar Probe maps embeddings of different syntactically-identical sentences to consistent latent locations and orientations. In addition, with the different \u201csentence levels\u201d,word position and syntactic role can be disentangled to compare the Polar Probe\u2019s activations accross levels, as shown in Fig: 5. ", "page_idx": 4}, {"type": "text", "text": "2.6 Training ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We train the Polar Probe on the neural activations of Mistral-7B-v0.1 and Llama-2-7b-hf (Touvron et al., 2023; Jiang et al., 2023), in response to sentences of the \u201cNatural Dataset\u201d described above. Both models are Auto-Regressive Language Models, they aim to identify future words from input sentences. Furthermore, these models read all the words simultaneously, building representations which depend on the whole sequence of tokens. ", "page_idx": 4}, {"type": "text", "text": "We trained the Polar Probes with gradient descent, using the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 0.005, and a batch size of 200 sentences. The duration of the training is 30 epochs, we perform model selection using the validation set. Hyperparameter $\\lambda$ is set to 10.0, ensuring an optimal balance between the Angular and Structural objective. ", "page_idx": 4}, {"type": "image", "img_path": "x2780VcMOI/tmp/f8eca9a45cdaa5bfa5a9851483f1e07e33142ecb511a909f6d0316b4e2f8a8ae.jpg", "img_caption": ["Figure 2: The Polar Probe reliably identifies dependency types. A. PCA visualization of edges linearly read by the Polar Probe. The color of each edge corresponds to one of three different dependency types (\u2018nsubj\u2019, \u2018obj\u2019, \u2018det\u2019): the linear readouts point in systematic directions. B. AUC and Balanced Accuracy metrics obtained for dependency type classification. C. Pairwise cosine similarity ( $0{=}$ orthogonal, $1{=}$ collinear) matrices obtained without a probe (left) the Structural Probe (middle) and the Polar Probe (right). "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "2.7 Evaluation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We evaluate each probe either on its ability to faithfully represent (i) the unlabelled and undirected dependency tree (\u201cstructure\u201d), (ii) the type and direction of dependencies and (iii) both of these elements. ", "page_idx": 5}, {"type": "text", "text": "Dependency structure. Following (Hewitt and Manning, 2019), we evaluate whether the probes accurately predicts the existence of each syntactic relation by using the Undirected Unlabeled Attachment Score (UUAS). UUAS quantifies the proportion of dependency relations (directly connected words) in the dependency tree that are correctly identified by the probe, irrespective of their dependency types and direction. ", "page_idx": 5}, {"type": "text", "text": "Dependency type. To evaluate the accuracy of the predicted dependency types, we use three distinct metrics: Area Under the Curve (AUC), Dependency Type Accuracy and Dependency Type Balanced Accuracy. ", "page_idx": 5}, {"type": "text", "text": "For AUC, we compute the cosine similarity between each edge $\\mathbf{s}_{i}$ and all other edges $\\mathbf{s}_{j}$ , that are either from the same dependency type, or not. This procedure ends with a distance vector $\\bar{x}\\in\\mathbb{R}^{m}$ of $m$ edge pairs and a binary vector of $y\\in\\mathbb{I}^{m}$ . We can finally input these two vectors into scikit-learn\u2019s roc_auc_score (Pedregosa et al., 2011). ", "page_idx": 5}, {"type": "text", "text": "For Accuracy and Balanced Accuracy, we classify dependency types by comparing relations to prototypical relations. For this, from the training set, we pool 10,000 relations and define a prototype $\\tilde{\\bf s}_{k}$ for each dependency type $k$ , by computing the centroid of all relations belonging to the same type $k$ . Then, we predict dependency types from the cosine similarity between each relation $\\mathbf{s}_{i}$ and each prototype $\\mathbf{V}_{c}$ , using scikit-learn\u2019s KNeighborsClassifier.(Pedregosa et al., 2011). Finally, we use scikit-learn accuracy_score or balanced_accuracy_score to limit the effect of imbalance between dependency types. ", "page_idx": 5}, {"type": "text", "text": "Combined dependency type and structure. Finally, to provide a metric which evaluates both dependency structures and dependency types, we compute the Labeled Attachment Score (LAS). LAS is defined as the proportion of correctly predicted labeled and directed edges in a sentence. ", "page_idx": 6}, {"type": "text", "text": "Baselines. We compare the Polar Probe to a variety of baselines: (i) Structural Probe, (ii) \u201cPolar Probe Random LLM\u201d: a Polar Probe trained on top of a random language model and (iii) \u201cNo Probe\u201d: the raw activations of the Language Model without any transformation. ", "page_idx": 6}, {"type": "text", "text": "3 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Reliable coding of dependency types. We first analyze the Polar Probe on the $16^{\\mathrm{th}}$ layer of Llama2-7b-hf, Mistral-7B-v0.1 and BERT-large (Touvron et al., 2023; Jiang et al., 2023; Devlin et al., 2019) on the English Web Treebank (EWT) sentences (Silveira et al., 2014) annotated with dependency trees. We evaluate, on an independent test set with 10000 relations, whether pairs of words linked by similar syntactic relations point towards similar orientations in the probe\u2019s representational space (Fig. 2). ", "page_idx": 6}, {"type": "text", "text": "Fig. 2.A shows a Principal Component Analysis (PCA) projection of the dependency relation embeddings from the test set, once linearly read by the Polar Probe. For readability, we restrict ourselves to three of the most common types of dependencies in the dataset. As expected, the three types of dependency consistently point in different directions. ", "page_idx": 6}, {"type": "text", "text": "We then compute the cosine similarity between all pairs of edge embeddings probed with the Polar Probe (Fig. 2.C (right)), indeed showing that relations of the same types are collinear, while relations of different types are orthogonal. This is much clearer for the Polar Probe than for baselines (Fig. 2.C). ", "page_idx": 6}, {"type": "text", "text": "Comparison with baselines. In Fig. 2.B, we summarize with AUC and Balanced Accuracy the extent to which the orientations of these edge embeddings reliably represent the syntactic types. ", "page_idx": 6}, {"type": "text", "text": "On average across dependency types, the Polar Probe reaches a AUC score of $95\\%$ , well above the Structural Probe $\\langle\\mathrm{AUC}{=}73\\%$ ), the Angular Probe $\\Delta{\\mathrm{UC}}{=}97\\%$ ), and no-probe $(\\mathrm{AUC}{=}80\\%)$ ). The same relative results accross probes are conserved for the Balanced Accuracy score. Importantly, these results confirm that the Polar Probe outperforms the Structural Probe in predicting dependency types of the edge embeddings. The latter is therefore something not emergent in the Structural Probe. ", "page_idx": 6}, {"type": "text", "text": "Unexpectedly, \u201cNo Probe\u201d predicts syntactic types well above chance, and significantly better than the Structural. This hints to the fact that syntactic types are already represented in the raw activations. A likely explanation for this is that words belonging to the same part-of-speech (such as verbs, nouns) are clustered in the embedding space, thus partially guiding the inference of syntactic dependencies. ", "page_idx": 6}, {"type": "text", "text": "Moreover, training a Polar Probe on a random initialization of a language model does not accomplish the contrastive objective better than chance. Resulting in near chance-level Balanced Accuracy and AUC scores. This confirms that the linear probe needs a rich underlying representation space to work, and cannot learn to cluster the different syntactic types on its own. ", "page_idx": 6}, {"type": "text", "text": "Layer-wise analysis. To evaluate how the Angular and Structural performance interact in the Polar Probe, and whether the mechanism generalizes to both Mistral-7B-v0.1, Llama-2-7b-hf and BERT-large, we evaluate the layers of the three models on Labeled Attachment Score (LAS) (Fig. 3). (See Supplementary for BERT-large and Mistral-7B-v0.1) ", "page_idx": 6}, {"type": "text", "text": "Interestingly, BERT-large, Mistral-7B-v0.1 and Llama-2-7b-hf all peak at layer 16, which is the same layer reported in (Hewitt and Manning, 2019). At layer 16, the models achieve a LAS on the test set of 70.2, 60.6 and 62.9 respectively. As recently reported in (Eisape et al., 2022) for the Structural Probe, these results suggest that the Polar Probe also works best with Masked Language Models. The Polar Probe, despite its conceptual simplicity matches performance with a more intricate and modular labeled probe (M\u00fcller-Eberstein et al., 2022). ", "page_idx": 6}, {"type": "text", "text": "Structural evaluation. The Polar Probe is optimized with both a Structural and an Angular objective. This means that the optimization of such probe might affect the original performance of the Structural Probe. To verify that the gap in performance, we compute the UUAS between the predicted tree and the annotated tree for both the Structural and Polar probe (Fig. 3). The results confirm that the Polar Probe preserves (but does not improve) the syntactic distances between the linear readout of the probed word embeddings. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "x2780VcMOI/tmp/e47b709ce27728bdf6a5b0bf233f30ac1d807c319c4b3869f7b6a6dc1378c305.jpg", "img_caption": ["Figure 3: The Polar Probe outperforms the Structural Probe at identifying labeled and directed dependencies. A. For dependency existence, the Polar Probe matches the UUAS performance of the Structural probe, peaking at layer 16. B. For dependency type, the Polar Probe outperforms in Label Accuracy the Structural (LAS) Probe by around $80\\%$ accross the different layers of Llama-2-7b-hf. C. For both dependency existence and type, the Polar Probe outperforms in LAS the Structural Probe by around $90\\%$ accross the different layers of Llama-2-7b-hf. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Dimensionality analysis. How many dimensions are necessary to successfully represent the full syntactic tree with the proposed polar coordinate system? To address this question, we varied $k^{\\prime\\prime}$ , namely the dimensionality of the space of the Polar Probe (Fig. 4). Analogously to the rank analysis of Structural Probe (Hewitt and Manning, 2019), we observe a peak around $k^{\\prime\\prime}=128$ . Contrary to theoretical predictions (Smolensky, 1987), these results suggest that the space representing the complete syntactic tree needs not be unreasonably large. For dependency types, this phenomenon could be relatively intuitive, as the unit circle (i.e. only 2 dimensions) can easily separate many different dependency types, such that a weakly non-linear readout would isolate these categories. ", "page_idx": 7}, {"type": "image", "img_path": "x2780VcMOI/tmp/8ac232920a5297d734c15bc4240d1710e3e94a91759c6fa0027fbe4827be9da0.jpg", "img_caption": ["Figure 4: The optimal dimensionality for the Polar Probe is an order of magnitude small than model\u2019s layer size. Polar Probe performance as a function of dimensionality, measured by A. UUAS, B. Dependency Type Accuracy and C. LAS for Llama-2-7b-hf as a function of $k^{\\prime\\prime}$ , the dimensionality of the probe\u2019s space. The optimal dimensionality for the Polar Probe is 128, achieving the highest LAS. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Controlled sentences. Natural sentences are highly variable in structure and content. To verify more precisely the behavior of the Polar Probe, we evaluate it on the \u201cControlled Dataset\u201d and its different sentence levels: Short, Relative Clause and Long-Nested. ", "page_idx": 7}, {"type": "text", "text": "First, we observe that in the space of the Polar Probe, the representation of dependency trees appears to be consistent across sentences\u2019 length and substructures. For example, as shown in Fig. 5, the coordinates of the main phrase are virtually identical whether it is attached to a prepositional phrase and/or a long-nested structure. This invariance supports the notion that dependency trees are represented by a systematic coordinate system that can be recovered with the Polar Probe. ", "page_idx": 7}, {"type": "text", "text": "We also observe that dependency types are robustly identified, whether they are part of the main phrase or not. ", "page_idx": 8}, {"type": "text", "text": "Overall, these results visually confirm that the Polar Probe reliably represents the dependency structure and dependency types of complex syntactic structures. The latter agrees with the extensive Structural and Angular evaluations performed. ", "page_idx": 8}, {"type": "image", "img_path": "x2780VcMOI/tmp/2de291901a1d1855e70a09c04c88f07ff81b21b43e1c7be9f32bbed984013342.jpg", "img_caption": ["Figure 5: Visualization of the dependency tree uncovered by the Polar Probe on a set of sentences with increasingly complex hierarchical structures. A. We display a PCA visualization of the distributions of word embeddings (once linearly read out by the Polar Probe), for the different syntactic levels in the \u201cControlled Dataset\u201d. Each individual distribution corresponds to a specific role of the word in the sentence. The centroids are linked with black lines, displaying the syntactic tree of the corresponding sentence. B. Most frequent syntactic tree prediction by the Polar Probe for the different syntactic levels. The relations between words are color coded according to the type of syntactic dependency. The incorrectly predicted relations are represented with dashed arrows. That is, either a dependency relation existence (no arrow), or a dependency type (with arrow) was erroneously identified. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Summary. We show that within the activation space of language models, there exists a subspace, where syntactic trees are fully represented by a polar coordinate system. There, the presence and type of a syntactic relation between two words is represented by their distance and relative direction, respectively. Importantly, the Polar Probe preserves the structural properties of the Structural Probe (Hewitt and Manning, 2019), but better represents the type of syntactic relations. ", "page_idx": 8}, {"type": "text", "text": "Limitations. The present work presents four main limitations. First, we only investigate the English language. Yet, human languages use different grammatical rules, and may, consequently, be structured according to different types of trees. Interestingly, as language models become increasingly able to process a wide spectrum of languages (Costa-juss\u00e0 et al., 2022), the present framework opens the exciting possibility to explore universal (or divergent) grammatical representations in artificial neural networks, following (M\u00fcller-Eberstein et al., 2022; Chi et al., 2020). ", "page_idx": 8}, {"type": "text", "text": "Second, syntactic structures are not necessarily restricted to the description of relations between words. In particular, morphology predicts that words themselves may be represented as trees of morphemes. Consequently, whether and how the present framework generalizes to the different scales of linguistic structures remains to be further investigated. ", "page_idx": 8}, {"type": "text", "text": "Third, like the Structural Probe, the Polar Probe is based on a supervised task: we optimize a linear transformation that maximally retrieves a known syntactic structure from the neural activations. ", "page_idx": 8}, {"type": "text", "text": "Developing an unsupervised probe would be important to help discovers unsuspected syntactic structures. In addition, we here focused on dependency structures. Yet, other formalisms, based on phrase structures (Chomsky, 1957; Joshi and Schabes, 1997; Cinque and Rizzi, 2009; Chomsky, 2014) could offer alternative trees, and could be equally probed through the present framework. This approach could thus offer the possibility of experimentally testing which of these linguistic theories best account for the representations of human languages in neural networks. ", "page_idx": 9}, {"type": "text", "text": "Finally, we assume that syntactic trees can be best read out using Euclidean probes. However, alternative assumptions, such as hyperbolic representations, have been a fruitful tool to interpret deep learning models\u2019 representations in both text and image modalities (Dhingra et al., 2018; Nickel and Kiela, 2017; Desai et al., 2023). We speculate that this direction could provide a valuable avenue for extending the current work. ", "page_idx": 9}, {"type": "text", "text": "5 Related work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Syntax in artificial neural networks. Overall, this study complements previous research on syntax in artificial neural networks. Originally, (Smolensky, 1987) demonstrated that vectorial systems could, in principle, represent symbolic structures with tensor products but did not provide an empirical demonstration that neural networks did, in fact, demonstrate this property. More recently, language models were tested on their capacity to process syntactic structures by evaluating their behavior on grammatical and ungrammatical sentences (Lakretz et al., 2020, 2021a; Hewitt and Manning, 2019; Hale et al., 2022; Evanson et al., 2023; Linzen et al., 2016). Finally, several groups explored how this capacity was instantiated in the neural activations (Huang et al., 2017; Palangi et al., 2017; Soulos et al., 2019; Lakretz et al., 2019), culminating in (Hewitt and Manning, 2019)\u2019s Structural Probe. Since the discovery of the Structural Probe different adaptations have been developed, notably including hyperbolic (Chen et al., 2021), orthoghonal (Limisiewicz and Mare\u02c7cek, 2021), nonlinear (Eisape et al., 2022; White et al., 2021) variants. The present work completes this long effort by showing how an interpretable syntactic code based on both distances and orientations spontaneously emerges in language models. ", "page_idx": 9}, {"type": "text", "text": "Syntax in biological neural networks. This link between linguistics and artificial neural networks holds significant potential for neuroscience. In particular, until the latest rise of large language models, many experimental neuroimaging studies aimed to identify the neural bases of syntax in the human brain (Hale et al., 2022). For example, (Pallier et al., 2011) showed with functional Magnetic Resonance Imaging (fMRI) that several regions of the superior temporal lobe and prefrontal cortex responded proportionally to constituent size. Critically, language models are now becoming standard bases to predict and explain the brain responses to natural language processing: The activations of these artificial neural networks have indeed been shown to linearly map onto fMRI, intracranial and MEG recordings of the brain in responses to the same words and sentences (Jain and Huth, 2018; Caucheteux and King, 2022; Reddy and Wehbe, 2021; Caucheteux et al., 2021; Pasquiou et al., 2022, 2023). However, this mapping remains difficult to interpret, and the neural code for syntax in the brain remains a major unknown. The present work thus provides a testable hypothesis to understand how syntactic trees may be explicitly represented in the brain. ", "page_idx": 9}, {"type": "text", "text": "Broader impact. Combined with the works outlined above, our results open the exciting possibility that the polar coordinate system may, in fact, explain how syntax is encoded in the human brain. Critically, this framework may generalize beyond syntactic tree structures, and apply to any compositional problem, including compositional semantics, object-feature binding in vision, and representation of knowledge graphs. Above all, while many have long opposed symbolic and connectionist formalisms, this work contributes to show how these two systems of representations may be largely compatible with one another, as long predicted (Smolensky, 1990; Smolensky et al., 2022). This reconciliation thus holds great promises to understand the brain mechanisms of language and composition. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This project was provided with computer and storage resources by GENCI at IDRIS thanks to the grant 2023-AD011014766 on the supercomputer Jean Zay\u2019s the V100 and A100 partition (PDS). ", "page_idx": 9}, {"type": "text", "text": "This project has received funding from the European Union\u2019s Horizon 2020 research and innovation program under the Marie Sklodowska-Curie grant agreement No 945304 (PDS). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Caucheteux, C., Gramfort, A., and King, J.-R. (2021). Disentangling syntax and semantics in the brain with deep networks. In International conference on machine learning, pages 1336\u20131348. PMLR. ", "page_idx": 11}, {"type": "text", "text": "Caucheteux, C. and King, J.-R. (2022). Brains and algorithms partially converge in natural language processing. Communications biology, 5(1):134. ", "page_idx": 11}, {"type": "text", "text": "Chen, B., Fu, Y., Xu, G., Xie, P., Tan, C., Chen, M., and Jing, L. (2021). Probing bert in hyperbolic spaces. arXiv preprint arXiv:2104.03869. ", "page_idx": 11}, {"type": "text", "text": "Chi, E. A., Hewitt, J., and Manning, C. D. (2020). Finding universal grammatical relations in multilingual BERT. In Jurafsky, D., Chai, J., Schluter, N., and Tetreault, J., editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5564\u20135577, Online. Association for Computational Linguistics. ", "page_idx": 11}, {"type": "text", "text": "Chomsky, N. (1957). Syntactic Structures. De Gruyter. ", "page_idx": 11}, {"type": "text", "text": "Chomsky, N. (2014). The minimalist program. MIT press. ", "page_idx": 11}, {"type": "text", "text": "Cinque, G. and Rizzi, L. (2009). The cartography of syntactic structures. ", "page_idx": 11}, {"type": "text", "text": "Costa-juss\u00e0, M. R., Cross, J., \u00c7elebi, O., Elbayad, M., Heafield, K., Heffernan, K., Kalbassi, E., Lam, J., Licht, D., Maillard, J., et al. (2022). No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672. ", "page_idx": 11}, {"type": "text", "text": "Desai, K., Nickel, M., Rajpurohit, T., Johnson, J., and Vedantam, S. R. (2023). Hyperbolic image-text representations. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J., editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 7694\u20137731. PMLR. ", "page_idx": 11}, {"type": "text", "text": "Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). Bert: Pre-training of deep bidirectional transformers for language understanding. pages 4171\u20134186. Association for Computational Linguistics. ", "page_idx": 11}, {"type": "text", "text": "Dhingra, B., Shallue, C., Norouzi, M., Dai, A., and Dahl, G. (2018). Embedding text in hyperbolic spaces. ", "page_idx": 11}, {"type": "text", "text": "DiCarlo, J. J. and Cox, D. D. (2007). Untangling invariant object recognition. Trends in cognitive sciences, 11(8):333\u2013341. ", "page_idx": 11}, {"type": "text", "text": "Eisape, T., Gangireddy, V., Levy, R., and Kim, Y. (2022). Probing for incremental parse states in autoregressive language models. In Goldberg, Y., Kozareva, Z., and Zhang, Y., editors, Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2801\u20132813, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. ", "page_idx": 11}, {"type": "text", "text": "Evanson, L., Lakretz, Y., and King, J.-R. (2023). Language acquisition: do children and language models follow similar learning stages? arXiv preprint arXiv:2306.03586. ", "page_idx": 11}, {"type": "text", "text": "Hale, J. T., Campanelli, L., Li, J., Bhattasali, S., Pallier, C., and Brennan, J. R. (2022). Neurocomputational models of language processing. Annual Review of Linguistics, 8:427\u2013446. ", "page_idx": 11}, {"type": "text", "text": "Hewitt, J. and Manning, C. D. (2019). A structural probe for finding syntax in word representations. pages 4129\u20134138. Association for Computational Linguistics. ", "page_idx": 11}, {"type": "text", "text": "Huang, Q., Smolensky, P., He, X., Deng, L., and Wu, D. (2017). Tensor product generation networks for deep nlp modeling. arXiv preprint arXiv:1709.09118. ", "page_idx": 11}, {"type": "text", "text": "Jain, S. and Huth, A. (2018). Incorporating context into language encoding models for fmri. Advances in neural information processing systems, 31. ", "page_idx": 11}, {"type": "text", "text": "Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.-A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. (2023). Mistral 7b. ", "page_idx": 11}, {"type": "text", "text": "Joshi, A. K. and Schabes, Y. (1997). Tree-adjoining grammars. In Handbook of Formal Languages: Volume 3 Beyond Words, pages 69\u2013123. Springer. King, J.-R. and Dehaene, S. (2014). Characterizing the dynamics of mental representations: the temporal generalization method. Trends in cognitive sciences, 18(4):203\u2013210. ", "page_idx": 12}, {"type": "text", "text": "Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. ", "page_idx": 12}, {"type": "text", "text": "Kriegeskorte, N. and Bandettini, P. (2007). Analyzing for information, not activation, to exploit high-resolution fmri. Neuroimage, 38(4):649\u2013662. ", "page_idx": 12}, {"type": "text", "text": "Lakretz, Y., Dehaene, S., and King, J.-R. (2020). What limits our capacity to process nested long-range dependencies in sentence comprehension? Entropy, 22(4):446. ", "page_idx": 12}, {"type": "text", "text": "Lakretz, Y., Desbordes, T., King, J.-R., Crabb\u00e9, B., Oquab, M., and Dehaene, S. (2021a). Can rnns learn recursive nested subject-verb agreements? arXiv preprint arXiv:2101.02258. ", "page_idx": 12}, {"type": "text", "text": "Lakretz, Y., Hupkes, D., Vergallito, A., Marelli, M., Baroni, M., and Dehaene, S. (2021b). Mechanisms for handling nested dependencies in neural-network language models and humans. Cognition, 213:104699. ", "page_idx": 12}, {"type": "text", "text": "Lakretz, Y., Kruszewski, G., Desbordes, T., Hupkes, D., Dehaene, S., and Baroni, M. (2019). The emergence of number and syntax units in. pages 11\u201320. Association for Computational Linguistics. ", "page_idx": 12}, {"type": "text", "text": "Limisiewicz, T. and Marec\u02c7ek, D. (2021). Introducing orthogonal constraint in structural probes. In Zong, C., Xia, F., Li, W., and Navigli, R., editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 428\u2013442, Online. Association for Computational Linguistics. ", "page_idx": 12}, {"type": "text", "text": "Linzen, T., Dupoux, E., and Goldberg, Y. (2016). Assessing the ability of lstms to learn syntaxsensitive dependencies. Transactions of the Association for Computational Linguistics, 4:521\u2013535. ", "page_idx": 12}, {"type": "text", "text": "M\u00fcller-Eberstein, M., van der Goot, R., and Plank, B. (2022). Probing for labeled dependency trees. In Muresan, S., Nakov, P., and Villavicencio, A., editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7711\u20137726, Dublin, Ireland. Association for Computational Linguistics. ", "page_idx": 12}, {"type": "text", "text": "Nickel, M. and Kiela, D. (2017). Poincar\u00e9 embeddings for learning hierarchical representations. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R., editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc. ", "page_idx": 12}, {"type": "text", "text": "Nivre, J., Zeman, D., Ginter, F., and Tyers, F. (2017). Universal Dependencies. ACL Anthology. ", "page_idx": 12}, {"type": "text", "text": "Palangi, H., Huang, Q., Smolensky, P., He, X., and Deng, L. (2017). Grammatically-interpretable learned representations in deep nlp models. In Advances in Neural Information Processing Systems Workshop. ", "page_idx": 12}, {"type": "text", "text": "Pallier, C., Devauchelle, A.-D., and Dehaene, S. (2011). Cortical representation of the constituent structure of sentences. Proceedings of the National Academy of Sciences, 108:2522\u20132527. ", "page_idx": 12}, {"type": "text", "text": "Pasquiou, A., Lakretz, Y., Hale, J., Thirion, B., and Pallier, C. (2022). Neural language models are not born equal to fit brain data, but training helps. In ICML 2022-39th International Conference on Machine Learning, page 18. ", "page_idx": 12}, {"type": "text", "text": "Reddy, A. J. and Wehbe, L. (2021). Can fmri reveal the representation of syntactic structure in the brain? Advances in Neural Information Processing Systems, 34:9843\u20139856. ", "page_idx": 13}, {"type": "text", "text": "Robins, R. H. (2013). A Short History of Linguistics. Routledge. ", "page_idx": 13}, {"type": "text", "text": "Silveira, N., Dozat, T., de Marneffe, M.-C., Bowman, S., Connor, M., Bauer, J., and Manning, C. D. (2014). A gold standard dependency corpus for English. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC-2014).   \nSmolensky, P. (1987). Connectionist ai, symbolic ai, and the brain. Artificial Intelligence Review, 1:95\u2013109.   \nSmolensky, P. (1990). Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial Intelligence, 46:159\u2013216.   \nSmolensky, P., McCoy, R. T., Fernandez, R., Goldrick, M., and Gao, J. (2022). Neurocompositional computing: From the central paradox of cognition to a new generation of ai systems. AI Magazine, 43(3):308\u2013322.   \nSoulos, P., McCoy, T., Linzen, T., and Smolensky, P. (2019). Discovering the compositional structure of vector representations with role learning networks. arXiv preprint arXiv:1910.09113. ", "page_idx": 13}, {"type": "text", "text": "Tesni\u00e8re, L. (1953). Esquisse d\u2019une syntaxe structurale. Klincksieck. ", "page_idx": 13}, {"type": "text", "text": "Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. (2023). Llama 2: Open foundation and fine-tuned chat models. ", "page_idx": 13}, {"type": "text", "text": "White, J. C., Pimentel, T., Saphra, N., and Cotterell, R. (2021). A non-linear structural probe. In Toutanova, K., Rumshisky, A., Zettlemoyer, L., Hakkani-Tur, D., Beltagy, I., Bethard, S., Cotterell, R., Chakraborty, T., and Zhou, Y., editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 132\u2013138, Online. Association for Computational Linguistics. ", "page_idx": 13}, {"type": "image", "img_path": "x2780VcMOI/tmp/dfc3b4065267df65b2e371801f164b86c6c9a041acd4a7d91d3832c47a00db71.jpg", "img_caption": ["Figure 6: Polar Probe performance on the EN-EWT dataset for Language Models with different families and sizes "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "x2780VcMOI/tmp/97edbbc3c612c88aa58c2c6ab9969654fc428f1190a2b52a9093c3eb474a04e6.jpg", "img_caption": ["Figure 7: Comparative analysis of Polar Probe performance on the N-EWT dataset as a function of sentence length (left) and sentence depth (right). The scores are shown across various model sizes (ranked by model size), with darker lines indicating larger models. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "x2780VcMOI/tmp/087d32d77e2b397a9cbad6631402e09d1800a0d18da905a26e978ce6648fe202.jpg", "img_caption": ["Figure 8: Polar Probe performance across different sentence structures and dependency types in a controlled dataset. The three categories (Short, Relative clause, and Long-nested) show the performance breakdown by Unlabeled Attachment Score (UUAS), Labeled Attachment Score (LAS), and specific dependency relations in the main phrase. Error bars represent the standard error across relations. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 15}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 15}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 15}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 15}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 15}, {"type": "text", "text": "IMPORTANT, please: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 15}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Justification: The claim iof the paper is to have found a more complete code for syntax in the activations of LLMs. Such claim is supported with extensive experiments on 2 SOTA LLM models. The results solidly reflect the claim made in the paper and can be replicated easily. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 15}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: Yes, there is a section dedicated to limitations of the work where we explain follow-up work to be done as well as the weak points we perceive in this current approach. Guidelines ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper is not theoretical in nature, it rather is a proposal of a neural code for syntax that is empirically demonstrated with the presented experiments. There are no mathematical theoretical results. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 16}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper provides with detailed information about the training procedure as well as the details of the experimental setting. Furthermore a code repository will be made public to replicate all the results shared with the camera-ready version of the paper. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: As stated in the previous section a code repository will be shared together with the controlled datasets that we created. Furthermore, the \"natural dataset\" (UD-EN-EWT) is public. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: In the methods sections all the training details are written. This details are more than enough to understand and replicate the results. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [No] ", "page_idx": 18}, {"type": "text", "text": "Justification: Due to the nature of the experiments and the scarcity of compute, to get error bars we would need to train the probes several times, this is unfortunately not possible and would take a lot of time. In previous works like (Hewitt and Manning, 2019), error bars were also not shown. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 18}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: All experiments are done with a single A100 GPU, however, these details are not relevant for the scope of the paper. The only important aspect is to be able to fti a LLM in the memory of the GPU to do inference. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: All the ethics are strictly respected. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Unfortunately due to space constraints we could not add this section. However this is written and would be shared in the extra page of the camera-ready version. We do not think this paragraph is essential as others due to the fundamental nature of our work, it is far from the application. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA]   \nJustification: Not applicable   \nGuidelines:   \n\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: Not applicable ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] Justification: Not applicable ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: Not applicable Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: Not applicable ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}]