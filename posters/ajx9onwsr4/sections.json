[{"heading_title": "AGL for Foundation Models", "details": {"summary": "The paper explores the applicability of Agreement-on-the-Line (AGL), a method for predicting out-of-distribution (OOD) performance, to foundation models (FMs).  **A key challenge is that FMs, unlike classical neural networks, undergo minimal finetuning, potentially reducing ensemble diversity needed for AGL**.  The authors investigate various methods to induce diversity, including random linear head initialization, data ordering, and data subsetting.  **Surprisingly, only random head initialization reliably induces AGL across multiple vision and language benchmarks.** The study also shows that ensembles of different FMs, even pretrained on diverse data, can exhibit AGL. These findings **highlight the importance of careful ensemble construction to leverage AGL's benefits in FMs**. The results demonstrate that **AGL can provide high-precision OOD performance predictions for FMs**, surpassing other estimation methods."}}, {"heading_title": "Diversity in Finetuning", "details": {"summary": "The concept of 'Diversity in Finetuning' within the context of foundation models is crucial for achieving robust out-of-distribution (OOD) generalization.  The paper highlights that **lightly finetuning multiple runs from a single foundation model can yield drastically different levels of ensemble diversity**, impacting the reliability of agreement-on-the-line (AGL) for OOD performance prediction.  **Randomly initializing the linear head emerges as the most effective approach for inducing such diversity**, consistently promoting AGL across various vision and language benchmarks.  However, other sources of randomness like data ordering and subsetting are less reliable.  Furthermore, the study explores using multiple foundation models pretrained on different datasets, observing that **carefully constructed ensembles of such models also demonstrate AGL**, showcasing its broader applicability beyond single-model finetuning.  This **diverse ensemble approach significantly enhances the precision of AGL-based OOD performance estimation** compared to alternative methods."}}, {"heading_title": "OOD Performance Prediction", "details": {"summary": "The paper explores **out-of-distribution (OOD) performance prediction** for foundation models (FMs), focusing on the phenomenon of \"agreement-on-the-line\" (AGL).  AGL leverages the correlation between in-distribution (ID) and OOD agreement among multiple models to predict OOD accuracy, even without OOD labels.  The study investigates the effectiveness of AGL on lightly finetuned FMs, highlighting the crucial role of diverse ensembles.  **Random initialization of linear heads** emerges as a key strategy for inducing sufficient model diversity, improving OOD performance estimation.  The research also examines the use of ensembles from multiple base FMs, demonstrating that careful ensemble construction is vital for reliable AGL-based predictions.  **The results show AGL's versatility across various tasks and modalities**, showcasing its potential for robust OOD performance estimation in FMs."}}, {"heading_title": "AGL Across Modalities", "details": {"summary": "The concept of \"AGL Across Modalities\" in the context of foundation models (FMs) suggests that the agreement-on-the-line (AGL) phenomenon, where a strong linear correlation exists between in-distribution (ID) and out-of-distribution (OOD) agreement among models, generalizes across different data types or modalities (image, text). This is significant because it implies that a single method of OOD performance estimation using AGL can be applied to various FMs trained on different tasks and datasets. **The success of this approach, however, crucially depends on carefully ensuring diversity within the model ensembles**, often achieved through random initialization of the linear head, rather than methods like data ordering or subsetting which may lead to less reliable results. This implies **practical advantages** by using AGL in a wide range of applications. **A careful consideration of ensemble diversity is crucial for the reliability of AGL-based methods.** "}}, {"heading_title": "Limitations of AGL", "details": {"summary": "The Agreement-on-the-Line (AGL) method, while promising for out-of-distribution (OOD) performance prediction, has limitations.  **AGL's reliance on a strong linear correlation between in-distribution (ID) and OOD performance may not hold for all distribution shifts.**  This limits its generalizability and predictive power in diverse scenarios.  **The diversity of the ensemble is crucial for AGL to succeed,** but achieving this diversity through simple techniques such as data shuffling or subsetting may be unreliable.  **Careful ensemble construction is needed**, and the choice of randomness (e.g., head initialization) significantly impacts AGL's effectiveness. Finally, while AGL offers a computationally efficient approach, it does not provide theoretical guarantees, especially when applied to lightly finetuned foundation models. Further research is needed to explore the robustness and expand the applicability of AGL."}}]