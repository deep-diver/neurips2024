[{"figure_path": "t3BhmwAzhv/tables/tables_6_1.jpg", "caption": "Table 2: Performance comparison. \"Expert models\" are tailored for specific tasks using task-oriented heads, while \u201cLLM-based models\u201d are designed for general instructions and responses.", "description": "This table compares the performance of different models on five 3D scene understanding benchmarks: ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, and SQA3D.  It categorizes the models into two groups: expert models, which are specifically designed and trained for each task, and LLM-based models, which are general-purpose and can be adapted to different tasks with minimal fine-tuning.  The results show that the proposed Chat-Scene model, an LLM-based model, outperforms existing methods across all benchmarks.", "section": "4 Experiments"}, {"figure_path": "t3BhmwAzhv/tables/tables_7_1.jpg", "caption": "Table 3: Ablation studies on object identifiers. \"Plain Text\" employs plain text for object numbers, \"Gaussian\" uses fixed Gaussian embeddings, and \"Learnable\" learns new identifier tokens. \"Token Cost\" denotes the total tokens for N objects, including object identifiers.", "description": "This table presents the results of ablation studies performed to evaluate the impact of different object identifier token types on the model's performance across various downstream tasks.  Three different identifier types were tested: \"Plain Text\" (using numerical text for object IDs), \"Gaussian\" (using fixed Gaussian embeddings), and \"Learnable\" (learning new identifier tokens). The table shows the performance (accuracy and F1 scores) for each task and identifier type, along with the total number of tokens used for N objects.", "section": "4.3 Ablation Study"}, {"figure_path": "t3BhmwAzhv/tables/tables_7_2.jpg", "caption": "Table 4: Ablation studies on multi-modal object-centric representations. \u201cEarly Fusion\u201d merges object features before language model input, whereas \u201cSeparate Token\u201d keeps them distinct. \u201cSingle\u201d denotes using a single image to extract 2D feature of an object, while \u201cMulti\u201d uses multi-view images.", "description": "This table presents the ablation study results on the impact of different multi-modal object-centric representation methods on the model's performance across various downstream tasks.  It compares using a single modality (3D or 2D features from single or multiple views), early fusion of 3D and 2D features, or keeping them as separate tokens in the LLM. The results show performance improvements using multi-view features and keeping them as separate tokens, indicating that comprehensive object representation is crucial for optimal results.", "section": "4.3 Ablation Study"}, {"figure_path": "t3BhmwAzhv/tables/tables_8_1.jpg", "caption": "Table 5: Evaluation results for video input.", "description": "This table presents the quantitative results of experiments using video input.  The model's performance is evaluated on video grounding using Acc@0.25 and Acc@0.5,  and on visual question answering tasks using CIDEr for ScanQA and EM for SQA3D.  An upper bound is also provided for the video grounding task, showing the potential performance if the video object masks were perfect.", "section": "4.4 Experiments with 2D Video Input"}, {"figure_path": "t3BhmwAzhv/tables/tables_14_1.jpg", "caption": "Table 6: Performance comparison on the validation set of ScanRefer [4].", "description": "This table presents a performance comparison of different methods on the ScanRefer benchmark's validation set.  It breaks down the results into three categories: Unique (referencing a single object), Multiple (referencing multiple objects), and Overall accuracy.  The table shows the accuracy at two different Intersection over Union (IoU) thresholds (0.25 and 0.5) for each category.  This allows for a detailed comparison of the various models' performance across different referencing scenarios.", "section": "4 Experiments"}, {"figure_path": "t3BhmwAzhv/tables/tables_15_1.jpg", "caption": "Table 7: Performance comparison on the validation set of Multi3DRefer [75].", "description": "This table compares the performance of different methods on the Multi3DRefer dataset.  It shows the F1 scores at IoU thresholds of 0.25 and 0.5 for various scenarios: ZT (Zero-shot Transfer), ST (Semi-supervised Training), MT (Multi-task), and ALL (all tasks).  The results show how well each method performs on multi-object visual grounding.", "section": "4 Experiments"}, {"figure_path": "t3BhmwAzhv/tables/tables_15_2.jpg", "caption": "Table 8: Performance comparison on the validation set of Scan2Cap [12].", "description": "This table compares the performance of different methods on the Scan2Cap benchmark dataset. The metrics used for comparison are CIDEr, BLEU-4, METEOR, and ROUGE-L. The table shows the results for both unique and multiple object grounding tasks. The results suggest that the proposed method outperforms the existing state-of-the-art methods on this benchmark.", "section": "4.2 Comparison with State-of-the-art Methods"}, {"figure_path": "t3BhmwAzhv/tables/tables_16_1.jpg", "caption": "Table 9: Performance comparison on the validation set of ScanQA [2].", "description": "This table presents a comparison of the performance of different methods on the ScanQA dataset for visual question answering.  The metrics used for evaluation include Exact Match (EM@1), BLEU scores (B-1, B-2, B-3, B-4), ROUGE-L, METEOR, CIDEr, and SPICE.  The table shows that the proposed 'Ours' method outperforms existing state-of-the-art methods across multiple metrics, demonstrating improved performance in 3D visual question answering.", "section": "4 Experiments"}, {"figure_path": "t3BhmwAzhv/tables/tables_16_2.jpg", "caption": "Table 10: Performance comparison on the test set of SQA3D [38].", "description": "This table compares the performance of different methods on the SQA3D benchmark dataset.  The benchmark focuses on visual question answering in 3D scenes and is broken down into different question types (What, Is, How, Can, Which, Others).  The \"Avg.\" column provides the average performance across all question types. The table shows that the proposed \"Ours\" method outperforms existing state-of-the-art methods on this benchmark.", "section": "4 Experiments"}, {"figure_path": "t3BhmwAzhv/tables/tables_18_1.jpg", "caption": "Table 2: Performance comparison. \"Expert models\" are tailored for specific tasks using task-oriented heads, while \u201cLLM-based models\u201d are designed for general instructions and responses.", "description": "This table compares the performance of different models on five 3D scene understanding benchmarks: ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, and SQA3D.  It contrasts the performance of expert models (designed for specific tasks) and LLM-based models (designed for more general instructions).  The table shows that the proposed Chat-Scene model outperforms prior state-of-the-art models on most benchmarks.", "section": "4 Experiments"}]