[{"figure_path": "5tOVh81aze/tables/tables_5_1.jpg", "caption": "Table 1: Default number of parameters N and token multiplier M to fit our scaling laws. We invest ~100 A100 hours to fit Equation (4) and ~1,000 A100 hours to fit Equation (5).", "description": "This table shows the model parameters (N) and token multipliers (M) used to fit the scaling laws proposed in the paper.  It indicates that smaller models with a token multiplier of 20 were used to fit Equation 4, while a 1.4B parameter model and a 0.011B parameter model with a token multiplier of 320 were also used. The total computational cost (in FLOPS) for fitting each equation is also given.", "section": "3 Constructing a scaling testbed"}, {"figure_path": "5tOVh81aze/tables/tables_7_1.jpg", "caption": "Table 2: Downstream relative prediction error at 6.9B parameters and 138B tokens. While predicting accuracy on individual zero-shot downstream evaluations can be challenging (\"Individual\"), predicting averages across downstream datasets is accurate (\"Avg.\").", "description": "This table presents the relative prediction errors for downstream tasks using a 6.9B parameter model trained with 138B tokens.  It shows that while predicting individual task accuracies can be difficult, predicting average accuracy across multiple datasets is quite accurate. The results are broken down by individual tasks and an average across 17 tasks.", "section": "Results: Reliable extrapolation"}, {"figure_path": "5tOVh81aze/tables/tables_22_1.jpg", "caption": "Table 3: Main models and hyperparameters used in our investigation. Models have number of parameters N, with number of layers nlayers, number of attention heads nheads, model width dmodel, and width per attention head dhead. Batch sizes are global and in units of sequences. Each sequence has 2,048 tokens. A100 GPU hours are at M = 20, which are near compute-optimal runs. For the 1.4B scale, a batch size of 256 performs slightly better than 512.", "description": "This table lists the main models and their hyperparameters used in the experiments.  It shows the number of parameters (N), the number of layers (nlayers), the number of attention heads (nheads), model width (dmodel), width per attention head (dhead), the warmup steps, learning rate, batch size, and the total A100 GPU hours used to train each model at a token multiplier (M) of 20 (near compute-optimal).  The table also notes that for the 1.4B parameter model, a batch size of 256 was slightly better than 512.", "section": "3 Constructing a scaling testbed"}, {"figure_path": "5tOVh81aze/tables/tables_31_1.jpg", "caption": "Table 3: Main models and hyperparameters used in our investigation. Models have number of parameters N, with number of layers nlayers, number of attention heads nheads, model width dmodel, and width per attention head dhead. Batch sizes are global and in units of sequences. Each sequence has 2,048 tokens. A100 GPU hours are at M = 20, which are near compute-optimal runs. For the 1.4B scale, a batch size of 256 performs slightly better than 512.", "description": "This table presents the main models and their hyperparameters used in the research.  It shows the number of parameters (N), layers (nlayers), attention heads (nheads), model width (dmodel), and width per head (dhead) for each model. Batch sizes (in sequences), sequence length (tokens), warmup steps, learning rate, and token multiplier (M) are also included. The table notes that the A100 GPU hours listed are for runs near compute-optimal conditions (M=20). It also mentions a slight performance advantage of using a batch size of 256 over 512 for the 1.4B parameter model.", "section": "3 Constructing a scaling testbed"}, {"figure_path": "5tOVh81aze/tables/tables_32_1.jpg", "caption": "Table 5: 46 downstream tasks. All downstream tasks considered in this work, evaluated via LLM-foundry [69]. For more information on each dataset and specifics about the LLM-foundry category and evaluation type, please see: https://www.mosaicml.com/llm-evaluation.", "description": "This table lists 46 downstream tasks used in the paper's experiments to evaluate the performance of language models.  Each task is categorized by the LLM-foundry taxonomy, indicating the type of evaluation (multiple-choice, language modeling, schema), the number of shots (number of examples used for few-shot learning), the number of samples, and a baseline performance score.", "section": "3.4 Evaluation setup"}, {"figure_path": "5tOVh81aze/tables/tables_32_2.jpg", "caption": "Table 6: Scaling law fit parameters. Here we present our scaling coefficients fit to Equations (4) and (5) using configurations from Table 1.", "description": "This table presents the fitted parameters for the scaling laws used to predict the validation loss and downstream error.  The parameters are for equations (4) and (5) in the paper and are fitted using the model configurations specified in Table 1.  Each row corresponds to a different training dataset (C4, RedPajama, and RefinedWeb). The parameters in each row allow the prediction of validation loss and average top-1 error.", "section": "3.3 Fitting scaling laws"}, {"figure_path": "5tOVh81aze/tables/tables_33_1.jpg", "caption": "Table 7: Downstream relative prediction error at 6.9B, 138B tokens, with and without the 1.4B data point. Recall in Table 1, we introduce a N = 1.4B, M = 20 run to get better downstream error predictions. Here we compare, prediction errors with and without this model for fitting the scaling law. Note that without the model (i.e., rows with \"w/o 1.4B\") average top-1 predictions, over the 17 tasks. are less accurate.", "description": "This table compares the downstream relative prediction error for models with 6.9B parameters and 138B tokens, evaluating the impact of including a 1.4B parameter model in the scaling law fitting process. It shows that removing the 1.4B model increases the relative error, especially for the RedPajama and RefinedWeb datasets, indicating that including this data point in scaling law fitting improves prediction accuracy.", "section": "Results: Reliable extrapolation"}, {"figure_path": "5tOVh81aze/tables/tables_33_2.jpg", "caption": "Table 8: Token multipliers of existing models. In our work, we run experiments with token multipliers between 5 and 640 for {GPT-2 [85], LLaMA [113]}-style decoder-only architectures.", "description": "This table shows the token multipliers (the ratio of training tokens to model parameters) used in several existing large language models.  The authors use this information to provide context for their own experiments, which explore a wider range of token multipliers (from 5 to 640).", "section": "3 Constructing a scaling testbed"}]