[{"type": "text", "text": "DiffusionPDE: Generative PDE-Solving Under Partial Observation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jiahe Huang1 Guandao Yang2 Zichen Wang1 Jeong Joon Park1 1University of Michigan 2Stanford University {chloehjh, zzzichen, jjparkcv}@umich.edu guandao@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We introduce a general framework for solving partial differential equations (PDEs) using generative diffusion models. In particular, we focus on the scenarios where we do not have the full knowledge of the scene necessary to apply classical solvers. Most existing forward or inverse PDE approaches perform poorly when the observations on the data or the underlying coefficients are incomplete, which is a common assumption for real-world measurements. In this work, we propose DiffusionPDE that can simultaneously flil in the missing information and solve a PDE by modeling the joint distribution of the solution and coefficient spaces. We show that the learned generative priors lead to a versatile framework for accurately solving a wide range of PDEs under partial observation, significantly outperforming the state-of-the-art methods for both forward and inverse directions. See our project page for results: jhhuangchloe.github.io/Diffusion-PDE/. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Partial differential equations (PDEs) are a cornerstone of modern science, underpinning many contemporary physical theories that explain natural phenomena. The ability to solve PDEs grants us the power to predict future states of a system (forward process) and estimate underlying physical properties from state measurements (inverse process). ", "page_idx": 0}, {"type": "text", "text": "To date, numerous methods [1, 2] have been proposed to numerically solve PDEs for both the forward and inverse directions. However, the classical methods can be prohibitively slow, prompting the development of data-driven, learning-based solvers that are significantly faster and capable of handling a family of PDEs. These learning-based approaches [3\u20136] typically learn a deterministic mapping between input coefficients and their solutions using deep neural networks. ", "page_idx": 0}, {"type": "text", "text": "Despite the progress, existing learning-based approaches, much like classical solvers, rely on complete observations of the coefficients to map solutions. However, complete information on the underlying physical properties or the state of a system is rarely accessible; in reality, most measurements are sparse in space and time. Both classical solvers and the state-of-the-art data-driven models often overlook these scenarios and consequently fail when confronted with partial observations. This limitation confines their use primarily to synthetic simulations, where full scene configurations are available by design, making their application to real-world cases challenging. ", "page_idx": 0}, {"type": "text", "text": "We present a comprehensive framework, DiffusionPDE, for solving PDEs in both forward and inverse directions under conditions of highly partial observations\u2014typically just $1{\\sim}3\\%$ of the total information. This task is particularly challenging due to the numerous possible ways to complete missing data and find subsequent solutions. Our approach uses a generative model to formulate the joint distribution of the coefficient and solution spaces, effectively managing the uncertainty and simultaneously reconstructing both spaces. During inference, we sample random noise and iteratively denoise it following standard diffusion models [7]. However, we uniquely guide this denoising process with sparse observations and relevant PDE constraints, generating plausible outputs that adhere to the imposed constraints. Notably, DiffusionPDE can handle observations with arbitrary density and patterns with a single pre-trained generative network. ", "page_idx": 0}, {"type": "image", "img_path": "z0I2SbjN0R/tmp/409cfe8fd3f0bbb5add9ba3eb580516dee15dd0a8f690d32964900edce1cccaa.jpg", "img_caption": ["Figure 1: We propose DiffusionPDE, a generative PDE solver under partial observations. Given a family of PDE with coefficient (initial state) $a$ and solution (final state) $u$ , we train the diffusion model on the joint distribution of $a$ and $u$ . During inference, we gradually denoise a Gaussian noise, guided by sparse observation and known PDE function, to recover the full prediction of both $a$ and $u$ that align well with the sparse observations and the given equation. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We conduct extensive experiments to show the versatility of DiffusionPDE as a general PDE-solving framework. We evaluate it on a diverse set of static and temporal PDEs, including Darcy Flow, Poisson, Helmholtz, Burger\u2019s, and Navier-Stokes equations. DiffusionPDE significantly outperforms existing state-of-the-art learning-based methods for solving PDEs [3\u20136, 8] in both forward and inverse directions with sparse measurements, while achieving comparable results with full observations. Highlighting the effectiveness of our model, DiffusionPDE accurately reconstructs the complete state of Burgers\u2019 equation using time-series data from just five sensors (Fig. 4), suggesting the potential of generative models to revolutionize physical modeling in real-world applications. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our work builds on the extensive literature of three areas: forward PDE solvers, inverse PDE solvers, and diffusion models. Please see relevant surveys for more information [9\u201313]. ", "page_idx": 1}, {"type": "text", "text": "Forward PDE Solvers. PDE solvers take the specification of a physics system and predict its state in unseen space and time by solving an equation involving partial derivatives. Since Most PDEs are very challenging to solve analytically, people resolve to numerical techniques, such as Finite Element Method [14, 2] and Boundary Element Method [1, 15]. While these techniques show strong performance and versatility in some problems, they can be computationally expensive or difficult to set up for complex physics systems. Recently, advancements in deep-learning methods have inspired a new set of PDE solvers. Raissi et al. [16, 6] introduce Physics-Informed Neural Networks (PINNs), which optimize a neural network using PDE constraints as self-supervised losses to output the PDE solutions. PINNs have been extended to solving specific fluid [17, 18], Reynolds-averaged Navier\u2013Stokes equations [19], heat equations [20], and dynamic power systems [21]. While PINNs can tackle a wide range of complex PDE problems, they are difficult to scale due to the need for network optimization. An alternative approach, neural operators [3, 5], directly learn the mapping from PDE parameters (e.g.initial and boundary condition) to the solution function. Once trained, this method avoids expensive network optimization and can instantly output the solution result. This idea has been extended to solve PDE in 3D [22, 23] , multiphase flow [24], seismic wave [25, 26], 3D turbulence [27, 28], and spherical dynamics [29]. People have also explored using neural networks as part of the PDE solver, such as compressing the physics state [30\u201333]. These solvers usually assume known PDE parameters, and applying them to solve the inverse problem can be challenging. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "PDE inverse problem. The inverse problem refers to finding the coefficients of a PDE that can induce certain observations, mapping from the solution of a PDE solver to its input parameters. People have tried to extend traditional numerical methods to this inverse problem [34\u201338], but these extensions are non-trivial to implement efficiently. There are similar attempts to inverse deep-learning PDE solvers. For example, one can inverse PINNs by optimizing the network parameters such that their outputs satisfy both the observed data and the governing equations. iFNO [39] and NIO [40] tries to extend FNO [3]. Other methods [41, 42] directly learn the operator functions for the inverse problem. PINO [4] further combines neural operators with physics constraints to improve the performance of both forward and inverse problems. These methods assume full observations are available. To address the inverse problem with partial observations, people have tried to leverage generative priors with Graph neural networks [43, 8]. These works have not demonstrated the ability to solve high-resolution PDEs, possibly limited by the power of generative prior. We want to leverage the state-of-the-art generative model, diffusion models, to develop a better inverse PDE solver. ", "page_idx": 2}, {"type": "text", "text": "Diffusion models. Diffusion models have shown great promise in learning the prior with higher resolutions by progressively estimating and removing noise. Models like DDIM [44], DDPM [7], and EDM [45] offer expressive generative capabilities but face challenges when sampling with specific constraints. Guided diffusion models [46\u201349] enhance generation processes with constraints such as image inpainting, providing more stable and accurate solutions. Prior works on diffusion models for PDEs highlight the potential of diffusion approaches by generating PDE datasets such as 3D turbulence [50, 51] and Navier-Stokes equations [52] with diffusion models. Diffusion models can also be used to model frequency spectrum and denoise the solution space [53], and conditional diffusion models are applied to solve 2D flows with sparse observation [54]. However, the application of diffusion models to solve inverse problems under partial observation remains underexplored. In this work, we aim to take the initial steps towards addressing this gap. ", "page_idx": 2}, {"type": "text", "text": "3 Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Overview ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To solve physics-informed forward and inverse problems under uncertainty, we start by pre-training a diffusion generative model on a family of partial differential equations (PDEs). This model is designed to learn the joint distribution of the PDE coefficients (or the initial state) and its corresponding solutions (or the final state). Our approach involves recovering full data in both spaces using sparse observations from either or both sides. We achieve this through the iterative denoising of random Gaussian noise as in regular diffusion models but with additional guidance from the sparse observations and the PDE function enforced during denoising. The schematic description of our approach is shown in Fig. 1. ", "page_idx": 2}, {"type": "text", "text": "3.2 Prelimary: Diffusion Models and Guided Diffusion ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffusion models involve a predefined forward process that gradually adds Gaussian noise to the data and a learned reverse process that denoises the data to reconstruct the original distribution. Specifically, Song et al. [55] propose a deterministic diffusion model that learns an $N$ -step denoising process that eventually outputs a denoised data $\\pmb{x}_{N}$ and satisfies the following ordinary differential equations (ODE) at each timestep $t_{i}$ where $i\\in\\{0,1,...,N-1\\}$ ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}\\pmb{x}=-\\dot{\\sigma}(t)\\sigma(t)\\nabla_{\\pmb{x}}\\log p\\big(\\pmb{x};\\sigma(t)\\big)\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here $\\nabla_{x}\\log{p(x;\\sigma(t))}$ is the score function [56] that helps to transform samples from a normal distribution $\\mathcal{N}(0,\\sigma(t_{0})^{2}\\mathbf{I})$ to a target probability distribution $p(\\mathbf{\\boldsymbol{x}};\\sigma(t))$ . To estimate the score function, Karras et al. [45] propose to learn a denoiser function $D(x;\\sigma)$ such that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\nabla_{x}\\log p\\big(\\pmb{x};\\sigma(t)\\big)=(D(\\pmb{x};\\sigma(t))-\\pmb{x})/\\sigma(t)^{2}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "To enable control over the generated data, guided diffusion methods [48] add guidance gradients to the score function during the denoising process. Recently, diffusion posterior sampling (DPS) [46] ", "page_idx": 2}, {"type": "text", "text": "Algorithm 1 Sparse Observation and PDE Guided Diffusion Sampling Algorithm. ", "page_idx": 3}, {"type": "text", "text": "1: input DeterministicSampler $D_{\\theta}(x;\\sigma)$ , $\\sigma(t_{i\\in\\{0,\\dots,N\\}})$ , TotalPointCount $m$ , ObservedPointCount   \n$n$ , Observation $\\textit{\\textbf{y}}$ , PDEFunction $f$ , Weights $\\zeta_{o b s},\\zeta_{p d e}$   \n2: sample $\\pmb{x}_{0}\\sim\\mathcal{N}\\big(\\mathbf{0},\\ \\sigma(t_{0})^{2}\\mathbf{I}\\big)$ \u25b7Generate initial sampling noise   \n3: for $i\\in\\{0,\\ldots,N-1\\}$ do   \n4: $\\hat{\\pmb{x}}_{N}^{i}\\gets D_{\\theta}\\left(\\pmb{x}_{i};\\sigma(t_{i})\\right)$ \u25b7Estimate the denoised data at step $t_{i}$   \n5: $d_{i}\\gets\\left(\\pmb{x}_{i}-\\hat{\\pmb{x}}_{N}^{i}\\right)/\\sigma(t_{i})$ $\\triangleright$ Evaluate $\\mathrm{d}\\mathbf{x}/\\mathrm{d}\\sigma(t)$ at step $t_{i}$   \n6: $\\pmb{x}_{i+1}\\gets\\pmb{x}_{i}+(\\sigma(t_{i+1})-\\sigma(t_{i}))\\pmb{d}_{i}$ \u25b7Take an Euler step from $\\sigma(t_{i})$ to $\\sigma(t_{i+1})$   \n7: if $\\sigma(t_{i+1})\\neq0$ then   \n8: $\\hat{\\pmb{x}}_{N}^{i}\\gets D_{\\theta}(\\pmb{x}_{i+1};\\sigma(t_{i+1}))$ $\\triangleright$ Apply $2^{\\mathrm{nd}}$ order correlation unless $\\sigma=0$   \n9: $d_{i}^{\\prime}\\gets\\left(\\pmb{x}_{i+1}-\\hat{\\pmb{x}}_{N}^{i}\\right)/\\sigma(t_{i+1})$ $\\triangleright$ Evaluate $\\mathrm{d}\\mathbf{x}/\\mathrm{d}\\sigma(t)$ at step $t_{i+1}$   \n10: $\\begin{array}{r}{\\pmb{x}_{i+1}\\leftarrow\\pmb{x}_{i}+(\\sigma(t_{i+1})-\\sigma(t_{i}))\\left(\\frac{1}{2}\\pmb{d}_{i}+\\frac{1}{2}\\pmb{d}_{i}^{\\prime}\\right)\\ \\mathrm{~\\triangleright~Apply~th~}}\\end{array}$ e trapezoidal rule at step $t_{i+1}$   \n11: end if   \n12: Lobs \u2190n1\u2225y \u2212\u02c6xiN\u222522 \u25b7Evaluate the observation loss of $\\hat{\\pmb{x}}_{N}^{i}$   \n13: Lpde \u2190m1\u22250 \u2212f(\u02c6xiN)\u222522 \u25b7Evaluate the PDE loss of $\\hat{\\pmb{x}}_{N}^{i}$   \n14: $\\pmb{x}_{i+1}\\leftarrow\\pmb{\\widetilde{x}}_{i+1}-\\zeta_{o b s}\\nabla_{\\pmb{x}_{i}}\\mathcal{L}_{o b s}-\\zeta_{p d e}\\nabla_{\\pmb{x}_{i}}\\mathcal{L}_{p d e}$ \u25b7Guide the sampling with $\\mathcal{L}_{o b s}$ and $\\mathcal{L}_{p d e}$   \n15: end for   \n16: return $\\pmb{x}_{N}$ \u25b7Return the denoised data ", "page_idx": 3}, {"type": "text", "text": "made notable progress in guided diffusion for tackling various inverse problems. DPS uses corrupted measurements $\\textit{\\textbf{y}}$ derived from $\\textbf{\\em x}$ to guide the diffusion model in outputting the posterior distribution $p(\\pmb{x}|\\pmb{y})$ . A prime application of DPS is the inpainting problem, which involves recovering a complete image from sparsely observed pixels, which suits well with our task. This approach modifies Eq. 1 to ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}\\boldsymbol{x}=-\\dot{\\sigma}(t)\\sigma(t)\\big(\\nabla_{\\boldsymbol{x}}\\log{p}\\big(\\boldsymbol{x};\\sigma(t)\\big)+\\nabla_{\\boldsymbol{x}}\\log{p}\\big(\\boldsymbol{y}|\\boldsymbol{x};\\sigma(t)\\big)\\big)\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "DPS [46] showed that under Gaussian noise assumption of the sparse measurement operator $\\mathcal{M}(\\cdot)$ , i.e., ${\\pmb y}|{\\pmb x}\\sim\\mathcal{N}({\\mathcal{M}}({\\pmb x}),\\delta^{2}{\\bf I})$ with some S.D. $\\delta$ , the log-likelihood function can be approximated with: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla_{x}\\log p\\big(\\pmb{y}|\\pmb{x}_{i};\\sigma(t_{i})\\big)\\approx\\nabla_{x_{i}}\\log p\\big(\\pmb{y}|\\hat{\\pmb{x}}_{N}^{i};\\sigma(t_{i})\\big)\\approx-\\frac{1}{\\delta^{2}}\\nabla_{x_{i}}\\|\\pmb{y}-\\mathcal{M}(\\hat{\\pmb{x}}_{N}^{i}(\\pmb{x}_{i};\\sigma(t_{i}))\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\hat{\\pmb{x}}_{N}^{i}:=D(\\pmb{x}_{i};\\pmb{\\sigma}(t_{i}))$ denotes the estimation of the final denoised data at each denoising step $i$ Applying the Baye\u2019s rule, the gradient direction of the guided diffusion is therefore: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla_{\\pmb{x}_{i}}\\log p(\\pmb{x}_{i}|\\pmb{y})\\approx s(\\pmb{x}_{i})-\\zeta\\nabla_{\\pmb{x}_{i}}\\|\\pmb{y}-\\mathcal{M}(\\hat{\\pmb{x}}_{N}^{i})\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $s(\\pmb{x})=\\nabla_{\\pmb{x}}\\log p(\\pmb{x})$ is the original score function, and $\\zeta=1/\\delta^{2}$ . ", "page_idx": 3}, {"type": "text", "text": "3.3 Solving PDEs with Guided Diffusion ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our work focuses on two classes of PDEs: static PDEs and dynamic time-dependent PDEs. Static systems (e.g., Darcy Flow or Poisson equations) are defined by a time-independent function $f$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nf(\\mathbf{c};\\pmb{a},\\mathbf{u})=0\\ \\mathrm{in}\\ \\Omega\\subset\\mathbb{R}^{d},\\qquad\\mathbf{u}(\\mathbf{c})=\\pmb{g}(\\mathbf{c})\\ \\mathrm{in}\\ \\partial\\Omega,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Omega$ is a bounded domain, $c\\in\\Omega$ is a spatial coordinate, $\\mathbf{a}\\in A$ is the PDE coefficient field, and $\\mathbf{u}\\in\\mathcal{U}$ is the solution field. $\\partial\\Omega$ is the boundary of the domain $\\Omega$ and $\\mathbf{u}|_{\\partial\\Omega}=\\pmb{g}$ is the boundary constraint. We aim to recover both $\\textbf{\\em a}$ and $\\mathbf{u}$ from sparse observations on either $\\textbf{\\em a}$ or $\\mathbf{u}$ or both. ", "page_idx": 3}, {"type": "text", "text": "Similarly, we consider the dynamic systems (e.g., Navier-Stokes): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{f(c,\\tau;\\mathbf{a},\\mathbf{u})=0,}&{{}}&{\\mathrm{in}\\;\\Omega\\times(0,\\infty)}\\\\ {\\mathbf{u}(c,\\tau)=g(c,\\tau),}&{{}}&{\\mathrm{in}\\;\\partial\\Omega\\times(0,\\infty)}\\\\ {\\mathbf{u}(c,\\tau)=a(c,\\tau),}&{{}}&{\\mathrm{in}\\;\\bar{\\Omega}\\times\\{0\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\tau$ is a temporal coordinate, $\\pmb{a}=\\mathbf{u}_{0}\\in\\mathcal{A}$ is the initial condition, $\\mathbf{u}$ is the solution field, and ${\\mathbf{u}}|_{\\Omega\\times(0,\\infty)}={\\pmb{g}}$ is the boundary constraint. We aim to simultaneously recover both $\\textbf{\\em a}$ and the solution $\\mathbf{u}_{T}:=\\mathbf{u}(\\cdot,T)$ at a specific time $T$ from sparse observations on either $\\mathbf{\\delta}a,\\mathbf{u}_{T}$ , or both. ", "page_idx": 3}, {"type": "text", "text": "Finally, we explore the recovery of the states across all timesteps $\\mathbf{u}_{0:T}$ in 1D dynamic systems governed by Burger\u2019s equation. Our network $D_{\\theta}$ models the distribution of all 1D states, including the initial condition $\\mathbf{u}_{\\mathrm{0}}$ and solutions $\\mathbf{u}_{1:T}$ stacked in the temporal dimension, forming a 2D dataset. ", "page_idx": 3}, {"type": "image", "img_path": "z0I2SbjN0R/tmp/407ff43fd7744b1d1ddb4f21eeebf666bdf7a9a09771b093e66ca92d01ca4111.jpg", "img_caption": ["Figure 2: Different from forward and inverse PDE solvers, DiffusionPDE can take sparse observations on either the coefficient $\\textbf{\\em a}$ or the solution u to recover both of them, using one trained network. Here, we show the recovered $\\textbf{\\em a}$ and $\\mathbf{u}$ of the Darcy\u2019s eqaution given sparse observations on $\\textbf{\\em a}$ , u, or both. Compared with the ground truth, we see that our method successfully recovers the PDE in all cases. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Guided Diffusion Algorithm In the data-driven PDE literature, the above tasks can be achieved by learning directional mappings between $\\textbf{\\em a}$ and $\\mathbf{u}$ (or $u_{T}$ for dynamic systems). Thus, existing methods typically train separate neural networks for the forward solution operator $\\mathcal{F}:\\mathcal{A}\\rightarrow\\mathcal{U}$ and the inverse solution operator $\\mathcal{T}:\\mathcal{U}\\rightarrow A$ . ", "page_idx": 4}, {"type": "text", "text": "Our method unifies the forward and inverse operators with a single network and an algorithm using the guided diffusion framework. DiffusionPDE can handle arbitrary sparsity patterns with one pre-trained diffusion model $D_{\\theta}$ that learns the joint distribution of $\\boldsymbol{\\mathcal{A}}$ and $\\boldsymbol{\\mathcal{U}}$ , concatenated on the channel dimension, denoted $\\mathcal{X}$ . Thus, our data $\\pmb{x}\\in\\mathcal{X}$ , where $\\mathcal{X}:=\\mathcal{A}\\times\\mathcal{U}$ . We follow the typical diffusion model procedures [45] to train our model on a family of PDEs. ", "page_idx": 4}, {"type": "text", "text": "Once we train the diffusion model $D_{\\theta}$ , we employ our physics-informed DPS [46] formulation during inference to guide the sampling of $\\pmb{x}\\in\\mathcal{X}$ that satisfies the sparse observations and the given PDE, as detailed in Algorithm 1. We follow Eq. 5 to modify the score function using the two guidance terms: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{x_{i}}\\log{p({x_{i}}|{y_{o b s}},f)}\\approx\\nabla_{x_{i}}\\log{p\\big({x_{i}}\\big)}-\\zeta_{o b s}\\nabla_{x_{i}}\\mathcal{L}_{o b s}-\\zeta_{p d e}\\nabla_{x_{i}}\\mathcal{L}_{p d e},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\pmb{x}_{i}$ is the noisy data at denoising step $i$ , $\\pmb{y}_{o b s}$ are the observed values, and $f(\\cdot)\\ =\\ \\mathbf{0}$ is the underlying PDE condition. $\\mathcal{L}_{o b s}$ and $\\mathcal{L}_{p d e}$ respectively represent the MSE loss of the sparse observations and the PDE equation residuals: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\mathcal{L}_{o b s}(\\pmb{x}_{i},\\pmb{y}_{o b s};D_{\\theta})=\\frac{1}{n}\\|\\pmb{y}_{o b s}-\\hat{\\pmb{x}}_{N}^{i}\\|_{2}^{2}=\\frac{1}{n}\\sum_{j=1}^{n}(\\pmb{y}_{o b s}(\\pmb{o}_{j})-\\hat{\\pmb{x}}_{N}^{i}(\\pmb{o}_{j}))^{2},}\\\\ {\\displaystyle\\mathcal{L}_{p d e}(\\pmb{x}_{i};D_{\\theta},f)=\\frac{1}{m}\\|\\pmb{0}-f(\\hat{\\pmb{x}}_{N}^{i})\\|_{2}^{2}=\\frac{1}{m}\\sum_{j}\\sum_{k}f(\\pmb{c}_{j},\\tau_{k};\\hat{\\mathbf{u}}_{j},\\hat{\\pmb{a}}_{j})^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\hat{\\pmb x}_{N}^{i}=D_{\\theta}(\\pmb x_{i})$ is the clean image estimate at denoising timestep $i$ , which can be split into coefficient $\\hat{\\bf u}_{i}$ and solution $\\hat{\\pmb{a}}_{i}$ . Here, $m$ is the total number of grid points (i.e., pixels), $n$ is the number of sparse observation points. $o_{j}$ represents the spatio-temporal coordinate of $j$ th observation. Note that, without loss of generality, $\\mathcal{L}_{p d e}$ can be accumulated for all applicable PDE function $f$ in the system, and the time component $\\tau_{k}$ is ignored for static systems. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 PDE Problem Settings ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We show the usefulness of DiffusionPDE across various PDEs for inverse and forward problems and compare it against recent learning-based techniques. We test on the following families of PDEs. ", "page_idx": 4}, {"type": "image", "img_path": "z0I2SbjN0R/tmp/85382915a0783f61054ff63b7af7c1af4495c9455e63779e1f33250439462719.jpg", "img_caption": ["Figure 3: Usefulness of PDE loss. We visualize the absolute errors of the recovered coefficient and solution of the Helmholtz equation with and w/o PDE loss. We compare having only the observation loss with applying the additional PDE loss. The errors drop significantly when using PDE loss. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Darcy Flow. Darcy flow describes the movement of fluid through a porous medium. In our experiment, we consider the static Darcy Flow with a no-slip boundary $\\partial\\Omega$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{-\\nabla\\cdot(\\mathbf{a}(c)\\nabla\\mathbf{u}(c))=q(c),\\quad}&{{}c\\in\\Omega}\\\\ {\\mathbf{u}(c)=0,\\quad}&{{}c\\in\\partial\\Omega}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here the coefficient $\\textbf{\\em a}$ has binary values. We set $q(c)=1$ for constant force. The PDE guidance function is thus $f=\\nabla\\cdot({\\boldsymbol{a}}(c){\\bar{\\nabla}}\\mathbf{\\dot{u}}(c))+q(c)$ . ", "page_idx": 5}, {"type": "text", "text": "Inhomogeneous Helmholtz Equation. We consider the static inhomogeneous Helmholtz Equation with a no-slip boundary on $\\partial\\Omega$ , which describes wave propagation: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\nabla^{2}\\mathbf{u}(c)+k^{2}\\mathbf{u}(c)=\\mathbf{a}(c),\\quad}&{{}c\\in\\Omega}\\\\ {\\mathbf{u}(c)=0,\\quad}&{{}c\\in\\partial\\Omega}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The coefficient $\\textbf{\\em a}$ is a piecewise constant function and $k$ is a constant. Note 11 is the Poisson equation when $k\\,=\\,0$ . Setting $k\\,=\\,1$ for Helmholtz equations, the PDE guidance function is $\\dot{f}=\\nabla^{2}\\mathbf{u}(c)+k^{2}\\mathbf{u}(c)-\\mathbf{a}(c)$ . ", "page_idx": 5}, {"type": "text", "text": "Non-bounded Navier-Stokes Equation. We study the non-bounded incompressive Navier-Stokes equation regarding the vorticity. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\partial_{t}w(c,\\tau)+v(c,\\tau)\\cdot\\nabla w(c,\\tau)\\!=\\!\\nu\\Delta w(c,\\tau)+q(c),}&{c\\in\\Omega,\\tau\\in(0,T]}\\\\ {\\nabla\\cdot v(c,\\tau)=0,}&{c\\in\\Omega,\\tau\\in[0,T]}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here $w=\\nabla\\times v$ is the vorticity, $v(c,\\tau)$ is the velocity at $^c$ at time $\\tau$ , and $q(c)$ is a force field. We set the viscosity coefficient $\\nu=10^{-3}$ and correspondingly the Reynolds number $\\begin{array}{r}{R e=\\frac{1}{\\nu}=1000}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "DiffusionPDE learns the joint distribution of $w_{0}$ and $w_{T}$ and we take $T\\,=\\,10$ which simulates 1 second. Since $T\\gg0$ , we cannot accurately compute the PDE loss from our model outputs. Therefore, given that $\\nabla\\cdot w(\\mathbf{c},\\tau)=\\nabla\\cdot(\\nabla\\times v)=0$ , we use simplified $\\boldsymbol{f}=\\boldsymbol{\\nabla}\\cdot\\boldsymbol{w}(\\boldsymbol{c},\\tau)$ . ", "page_idx": 5}, {"type": "text", "text": "Bounded Navier-Stokes Equation. We study the bounded 2D imcompressive Navier Stokes regarding the velocity $v$ and pressure $p$ . ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}{\\partial_{t}\\boldsymbol{v}(\\boldsymbol{c},\\tau)+\\boldsymbol{v}(\\boldsymbol{c},\\tau)\\cdot\\nabla\\boldsymbol{v}(\\boldsymbol{c},\\tau)+\\frac{1}{\\rho}\\nabla p=\\nu\\nabla^{2}\\boldsymbol{v}(\\boldsymbol{c},\\tau),}&{}&{\\boldsymbol{c}\\in\\Omega,\\tau\\in(0,T]}\\\\ {\\nabla\\cdot\\boldsymbol{v}(\\boldsymbol{c},\\tau)=0,}&{}&{\\boldsymbol{c}\\in\\Omega,\\tau\\in(0,T].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We set the viscosity coefficient $\\nu=0.001$ and the fluid density $\\rho=1.0$ . We generate 2D cylinders of random radius at random positions inside the grid. Random turbulence flows in from the top of the grid, with the velocity field satisfying no-slip boundary conditions at the left and right edges, as well as around the cylinder $\\partial\\Omega_{l e f t,r i g h t,c y l i n d e r}$ . DiffusionPDE learns the joint distribution of $v_{0}$ and $v_{T}$ at $T=4$ , which simulates 0.4 seconds. Therefore, we similarly use $\\boldsymbol{\\dot{f}}=\\boldsymbol{\\nabla}\\cdot\\boldsymbol{v}(\\boldsymbol{c},\\tau)$ as before. ", "page_idx": 5}, {"type": "text", "text": "Burgers\u2019 Equation. We study the Burgers\u2019 equation with periodic boundary conditions on a 1D spatial domain of unit length $\\Omega=(0,1)$ . We set the viscosity to $\\nu=0.01$ . In our experiment, the initial condition $u_{0}$ has a shape of $128\\times1$ , and we take 127 more time steps after the initial state to form a 2D $u_{0:T}$ of size $128\\times128$ . ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\partial_{t}u(c,\\tau)+\\partial_{c}(u^{2}(c,\\tau)/2)=\\nu\\partial_{c c}u(c,\\tau),}&&{c\\in\\Omega,\\tau\\in(0,T]}\\\\ &{\\qquad\\qquad\\qquad u(c,0)=u_{0}(c),}&&{c\\in\\Omega}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We can reliably compute $f=\\partial_{t}u(c,\\tau)+\\partial_{c}(u^{2}(c,\\tau)/2)-\\nu\\partial_{c c}u(c,\\tau)$ with finite difference since we model densely on the time dimension. ", "page_idx": 5}, {"type": "table", "img_path": "z0I2SbjN0R/tmp/59ce689774fd9dd9c78f0fb8c7954fa368ab2e46503aa713e7202c3ad8fdbbdd.jpg", "table_caption": ["Table 1: Relative errors of solutions (or final states) and coefficients (or initial states) when solving forward and inverse problems respectively with sparse observations. Error rates are used for the inverse problem of Darcy Flow. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.2 Dataset Preparation and Training ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first test DiffusionPDE on jointly learning the forward mapping $\\mathcal{F}:\\mathcal{A}\\rightarrow\\mathcal{U}$ and the inverse mapping $\\mathcal{T}:\\mathcal{U}\\to A$ given sparse observations. In our experiments, we define our PDE over the unit square $\\bar{\\Omega}=(0,1)^{2}$ , which we represent as a $128\\times128$ grid. We utilize Finite Element Methods (FEM) to generate our training data. Specifically, we run FNO\u2019s [3] released scripts to generate Darcy Flows and the vorticities of the Navier-Stokes equation. Similarly, we generate the dataset of Poisson and Helmholtz using second-order finite difference schemes. To add more complex boundary conditions, we use Difftaichi [57] to generate the velocities of the bounded Navier-Stokes equation. We train the joint diffusion model for each PDE on three A40 GPUs for approximately 4 hours, using 50,000 data pairs. For Burgers\u2019 equation, we train the diffusion model on a dataset of 50,000 samples produced as outlined in FNO [3]. We randomly select 5 out of 128 spatial points on $\\Omega$ to simulate sensors that provide measurements across time. ", "page_idx": 6}, {"type": "text", "text": "4.3 Baseline Methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We compare DiffusionPDE with state-of-the-art learning-based methods, including PINO [4], DeepONet [5], PINNs [6], and FNO [3]. However, note that none of these methods show operation on partial observations. These methods can learn mappings between $\\textbf{\\em a}$ and $\\mathbf{u}$ or $\\mathbf{u}_{\\mathrm{0}}$ and $\\mathbf{u}_{1:T}$ with full observations, allowing them to also solve the mapping between $\\mathbf{u}_{\\mathrm{0}}$ and $\\mathbf{u}_{T}$ . PINNs map input $\\textbf{\\em a}$ to output u by optimizing a combined loss function that incorporates both the solution u and the PDE residuals. DeepONet employs a branch network to encode input function values sampled at discrete points and a trunk network to handle the coordinates of the evaluated outputs. FNO maps from the parametric space to the solution space using Fourier transforms. PINO enhances FNO by integrating PDE loss during training and refining the model with PDE loss finetuning. We train all four baseline methods on both forward and inverse mappings using full observation of $\\textbf{\\em a}$ or $\\mathbf{u}$ for both static and dynamic PDEs. We tried training the baseline models on partial observations, but we noticed degenerate training outcomes (see supplementary for details). Overall, they are intended for full observations and may not be suitable for sparse measurements. ", "page_idx": 6}, {"type": "text", "text": "More closely related to our method, GraphPDE [8] demonstrates the ability to recover the initial state using sparse observations on the final state, a task that other baselines struggle with. Therefore, we compare against GraphPDE for the inverse problem of bounded Navier-Stokes (NS) equation, which is the setup used in their report. GraphPDE uses a trained latent space model and a bounded forward GNN model to solve the inverse problem with sparse sensors and thus is incompatible with unbounded Navier-Stokes. We create bounded meshes using our bounded grids to train the GNN model and train the latent prior with $v_{0:T}$ for GraphPDE. ", "page_idx": 6}, {"type": "text", "text": "While we employ guided sampling to reconstruct the solutions, Classifier-Free Guidance (CFG) [58] offers an alternative approach where the diffusion model is conditioned on sparse input data. Shu et al. [54] extend this method by developing an optimized CFG approach that conditions on the PDE loss, using the observation as a low-resolution input. Additionally, OFormer [59] is another model designed to reconstruct the full solution using transformers, offering a shorter inference runtime. Consequently, we compare our approach against these methods for solving the unbounded Navier-Stokes equation. ", "page_idx": 6}, {"type": "image", "img_path": "z0I2SbjN0R/tmp/23a10a9c39b2efb0b3e6d49faa38d8c9a48c5fcedbcf5e1b47e123b11a17675a.jpg", "img_caption": ["Figure 4: We compare DiffusionPDE with state-of-the-art neural PDE solvers [3\u20136]. In the forward Navier-Stokes problem, we give 500 sparse observations of the initial state to solve for the final state. In the inverse set-up, we take observations of the final state and solve for the initial. For the Burgers\u2019 equation, we use 5 sensors throughout all time steps and want to recover the solution at all time steps. Note that we train on neighboring snapshot pairs for the baselines in order to add continuous observations of the Burgers\u2019 equation. Results show that existing methods do not support PDE solving under sparse observations, and we believe they are not easily extendable to do so. We refer readers to the supplementary for a complete set of visual results. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.4 Main Evaluation Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We respectively address the forward problem and the inverse problem with sparse observations of $\\textbf{\\em a}$ or u. For the forward problem, we randomly select coefficients (initial states) as sparse observations and then compare the predicted solutions (final states) with the ground truth. Specifically, we select 500 out of $128\\times128$ points, approximately $3\\%$ , on the coefficients of Darcy Flow, Poisson equation, Helmholtz equation, and the initial state of the non-bounded Navier-Stokes equation. For the bounded Navier-Stokes equation, we use $1\\%$ observed points beside the boundary of the cylinder in 2D. Similarly, for the inverse problem, we randomly sample points on solutions (final states) as sparse observations, using the same number of observed points as in the forward model for each PDE. ", "page_idx": 7}, {"type": "text", "text": "We show the relative errors of all methods regarding both forward and inverse problems in Table 1. Since the coefficients of Darcy Flow are binary, we evaluate the error rates of our prediction. Non-binary data is evaluated using mean pixel-wise relative error. We report error numbers averaged across 1,000 random scenes and observations for each PDE. DiffusionPDE outperforms other methods including PINO [4], DeepONet [5], PINNs [6], and FNO [3] for both directions with sparse observations, demonstrating the novelty and uniqueness of our approach. For the inverse problems of the Poisson and Helmholtz equations, DiffusionPDE exhibits higher error rates due to the insufficient constraints within the coefficient space, produced from random fields. In Fig. 4, we visualize the results for solving both the forward and inverse problem of the non-bounded Navier-Stokes. We refer to the supplementary for additional visual results. While other methods may produce partially correct results, DiffusionPDE outperforms them and can recover results very close to the ground truth. ", "page_idx": 7}, {"type": "text", "text": "For the inverse problem of the bounded Navier-Stokes equation, we further compare DiffusionPDE with GraphPDE, as illustrated in Fig. 5. Our findings reveal that DiffusionPDE surpasses GraphPDE [8] in accuracy, reducing the relative error from $12.0\\%$ to $2.7\\%$ with only $1\\%$ observed points. ", "page_idx": 7}, {"type": "text", "text": "We further show whether DiffusionPDE can jointly recover both $\\textbf{\\em a}$ and u by analyzing the retrieved $\\textbf{\\em a}$ and $\\mathbf{u}$ with sparse observations on different sides as well as on both sides. In Fig. 2, we recover the coefficients and solutions of Darcy Flow by randomly observing 500 points on only coefficient space, only space solution space, and both. Both coefficients and solutions can be recovered with low errors for each situation. We therefore conclude that DiffusionPDE can solve the forward problem and the inverse problem simultaneously with sparse observations at any side without retraining our network. ", "page_idx": 7}, {"type": "image", "img_path": "z0I2SbjN0R/tmp/0563cb50bcbe59a7eb187b1080141bab8fe9663e3838ede449c0434fe63c3d61.jpg", "img_caption": ["Figure 5: We compare GraphPDE [8] and our method for solving the inverse bounded Navier-Stokes equation. Given the boundary conditions and $1\\%$ observations of the final vorticity field, we solve the initial vorticity field. We set the filuds to flow in from the top, with boundary conditions at the edges and a middle cylinder. While GraphPDE can recover the overall pattern of the initial state, it suffers from noise when the fluid passes the cylinder and misses the high vorticities at the bottom. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.5 Advantage of Guided Sampling ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To demonstrate the clear advantage of our guided sampling method, we evaluate both the forward and inverse processes of the unbounded Navier-Stokes equation, comparing our DiffusionPDE approach with Diffusion using CFG when considering only the initial and final states given 500 observation points, as illustrated in Fig. 6. Our DiffusionPDE method consistently achieves lower relative errors across both evaluations. ", "page_idx": 8}, {"type": "text", "text": "Furthermore, in Fig. 7, we compare our results with those of Shu et al. [54], where the full time intervals are solved autoregressively using an optimized CFG method. In their approach, the error in the final state increases to approximately $13\\%$ , which is notably higher than that of our two-state model. Additionally, the relative errors of the transformer-based approach, OFormer [59], are around $17\\%$ and $23\\%$ , which are significantly larger than those observed with DiffusionPDE. ", "page_idx": 8}, {"type": "text", "text": "4.6 Recovering Solutions Throughout a Time Interval ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We demonstrate that DiffusionPDE is capable of retrieving all time steps throughout the time interval $[0,T]$ from continuous observations on sparse sensors. To evaluate its ability to recover $u_{0:T}$ with sparse sensors, we study the 1D dynamic Burgers\u2019 equation, where DiffusionPDE learns the distribution of $u_{0:T}$ using a 2D diffusion model. To apply continuous observation on PINO, DeepONet, FNO, and PINNs, we train them on neighboring snapshot pairs. Our experiment results in a test relative error of $2.68\\%$ , depicted in Fig. 4, which is significantly lower than other methods. ", "page_idx": 8}, {"type": "text", "text": "4.7 Additional Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We examine the effects of different components of our algorithm such as PDE loss and observation samplings. We strongly encourage readers to view the supplementary for more details of these analyses as well as additional experiments. ", "page_idx": 8}, {"type": "text", "text": "PDE Loss. To verify the role of the PDE guidance loss of Eq. 8 during the denoising process, we visualize the errors of recovered $\\textbf{\\em a}$ and $\\mathbf{u}$ of Helmholtz equation with or without PDE loss. Here, we run our DPS algorithm with 500 sparse observed points on both the coefficient $\\textbf{\\em a}$ and solution u and study the effect of the additional PDE loss guidance. The relative error of $\\mathbf{u}$ reduces from $9.3\\%$ to $0.6\\%$ , and the relative error of $\\textbf{\\em a}$ reduces from $13.2\\%$ to $9.4\\%$ . Therefore, we conclude that PDE guidance helps smooth the prediction and improve the accuracy. ", "page_idx": 8}, {"type": "text", "text": "Number of Observations. We examine the results of DiffusionPDE in solving forward and inverse problems when there are 100, 300, 500, and 1000 random observations on $\\mathbf{\\delta}_{a,\\mathbf{\\deltau}}$ , or both $\\textbf{\\em a}$ and u. The error of DiffusionPDE decreases as the number of sparse observations increases. DiffusionPDE is capable of recovering both $\\textbf{\\em a}$ and $\\mathbf{u}$ with errors $1\\%\\sim\\bar{1}0\\%$ with approximately $6\\%$ observation points at any side for most PDE families. DiffusionPDE becomes insensitive to the number of observations and can solve the problems well once more than $3\\%$ of the points are observed. ", "page_idx": 8}, {"type": "image", "img_path": "z0I2SbjN0R/tmp/50b41d4db39fb0869afdc81e01e14efbc880ed27d67271d90b02fdc7b863aa57.jpg", "img_caption": ["Figure 6: We compare the performance of DiffusionPDE and Diffusion with CFG for the unbounded Navier-Stokes equation, and visualize the error. With 500 observation points, DiffusionPDE demonstrates superior accuracy, achieving lower errors in both forward and inverse problem-solving. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "z0I2SbjN0R/tmp/48f25891b0f2d828fedaa0c7b0716c09564d35af9b1179f1620cc4e07daf91c0.jpg", "img_caption": ["Figure 7: We compare our DiffusionPDE method with the approaches of Shu et al. [54] and OFormer [59] for the unbounded Navier-Stokes equation. Using 500 observation points, DiffusionPDE effectively solves both the forward and inverse problems, achieving significantly lower errors. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Observation Sampling Pattern. While CFG struggles with robustness, we show that DiffusionPDE is robust to different sampling patterns of the sparse observations, including grid and non-uniformly concentrated patterns. Note that even when conditioned on the full observations, our approach performs on par with the current best methods, likely due to the inherent resilience of our guided diffusion algorithm. Additionally, DiffusionPDE can leverage continuous coordinates with bilinear interpolation in the prediction space to obtain predicted values for points that do not lie directly on the grid, without compromising accuracy. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we develop DiffusionPDE, a diffusion-based PDE solver that addresses the challenge of solving PDEs from partial observations by filling in missing information using generative priors. We formulate a diffusion model that learns the joint distribution of the coefficient (or initial state) space and the solution (or final state) space. During the sampling process, DiffusionPDE can flexibly generate plausible data by guiding its denoising with sparse measurements and PDE constraints. Our new approach leads to significant improvements over existing state-of-the-art methods, advancing toward a general PDE-solving framework that leverages the power of generative models. ", "page_idx": 9}, {"type": "text", "text": "Several promising directions for future research have emerged from this work. Currently, DiffusionPDE is limited to solving slices of 2D dynamic PDEs; extending its capabilities to cover full time intervals of these equations presents a significant opportunity. Moreover, the model\u2019s struggle with accuracy in spaces that lack constraints is another critical area for exploration. DiffusionPDE also suffers from a slow sampling procedure, and a faster solution might be desired. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Ferri MH Aliabadi. Boundary element methods. In Encyclopedia of continuum mechanics, pages 182\u2013193. Springer, 2020.   \n[2] Pavel \u02c6Sol\u00edn. Partial differential equations and the finite element method. John Wiley & Sons, 2005.   \n[3] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. arXiv preprint arXiv:2010.08895, 2020.   \n[4] Zongyi Li, Hongkai Zheng, Nikola Kovachki, David Jin, Haoxuan Chen, Burigede Liu, Kamyar Azizzadenesheli, and Anima Anandkumar. Physics-informed neural operator for learning partial differential equations. ACM/JMS Journal of Data Science, 2021.   \n[5] Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning nonlinear operators via deeponet based on the universal approximation theorem of operators. Nature machine intelligence, 3(3):218\u2013229, 2021.   \n[6] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational physics, 378:686\u2013707, 2019.   \n[7] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[8] Qingqing Zhao, David B Lindell, and Gordon Wetzstein. Learning to solve pde-constrained inverse problems with graph networks. arXiv preprint arXiv:2206.00711, 2022.   \n[9] Lawrence C Evans. Partial differential equations, volume 19. American Mathematical Society, 2022.   \n[10] Kenji Omori and Jun Kotera. Overview of pdes and their regulation. Circulation research, 100(3):309\u2013327, 2007.   \n[11] Ryan Po, Wang Yifan, Vladislav Golyanik, Kfir Aberman, Jonathan T Barron, Amit H Bermano, Eric Ryan Chan, Tali Dekel, Aleksander Holynski, Angjoo Kanazawa, et al. State of the art on diffusion models for visual computing. arXiv preprint arXiv:2310.07204, 2023.   \n[12] Walter A Strauss. Partial differential equations: An introduction. John Wiley & Sons, 2007.   \n[13] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and applications. ACM Computing Surveys, 56(4):1\u201339, 2023.   \n[14] Alfio Quarteroni and Alberto Valli. Numerical approximation of partial differential equations, volume 23. Springer Science & Business Media, 2008.   \n[15] Sergio R Idelsohn, Eugenio Onate, Nestor Calvo, and Facundo Del Pin. The meshless finite element method. International Journal for Numerical Methods in Engineering, 58(6):893\u2013912, 2003.   \n[16] Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Physics informed deep learning (part i): Data-driven solutions of nonlinear partial differential equations. arXiv preprint arXiv:1711.10561, 2017.   \n[17] Shengze Cai, Zhiping Mao, Zhicheng Wang, Minglang Yin, and George Em Karniadakis. Physics-informed neural networks (pinns) for fluid mechanics: A review. Acta Mechanica Sinica, 37(12):1727\u20131738, 2021.   \n[18] Zhiping Mao, Ameya D Jagtap, and George Em Karniadakis. Physics-informed neural networks for high-speed flows. Computer Methods in Applied Mechanics and Engineering, 360:112789, 2020.   \n[19] Hamidreza Eivazi, Mojtaba Tahani, Philipp Schlatter, and Ricardo Vinuesa. Physics-informed neural networks for solving reynolds-averaged navier\u2013stokes equations. Physics of Fluids, 34(7), 2022.   \n[20] Shengze Cai, Zhicheng Wang, Sifan Wang, Paris Perdikaris, and George Em Karniadakis. Physics-informed neural networks for heat transfer problems. Journal of Heat Transfer, 143(6):060801, 2021.   \n[21] George S Misyris, Andreas Venzke, and Spyros Chatzivasileiadis. Physics-informed neural networks for power systems. In 2020 IEEE power & energy society general meeting (PESGM), pages 1\u20135. IEEE, 2020.   \n[22] Zongyi Li, Nikola Kovachki, Chris Choy, Boyi Li, Jean Kossaif,i Shourya Otta, Mohammad Amin Nabian, Maximilian Stadler, Christian Hundt, Kamyar Azizzadenesheli, et al. Geometry-informed neural operator for large-scale 3d pdes. Advances in Neural Information Processing Systems, 36, 2024.   \n[23] Louis Serrano, Lise Le Boudec, Armand Kassa\u00ef Koupa\u00ef, Thomas X Wang, Yuan Yin, Jean-No\u00ebl Vittaut, and Patrick Gallinari. Operator learning with neural fields: Tackling pdes on general geometries. Advances in Neural Information Processing Systems, 36, 2024.   \n[24] Gege Wen, Zongyi Li, Kamyar Azizzadenesheli, Anima Anandkumar, and Sally M Benson. U-fno\u2014an enhanced fourier neural operator-based deep-learning model for multiphase flow. Advances in Water Resources, 163:104180, 2022.   \n[25] Fanny Lehmann, Filippo Gatti, Micha\u00ebl Bertin, and Didier Clouteau. Fourier neural operator surrogate model to predict 3d seismic waves propagation. arXiv preprint arXiv:2304.10242, 2023.   \n[26] Bian Li, Hanchen Wang, Shihang Feng, Xiu Yang, and Youzuo Lin. Solving seismic wave equations on variable velocity models with fourier neural operator. IEEE Transactions on Geoscience and Remote Sensing, 61:1\u201318, 2023.   \n[27] Zhijie Li, Wenhui Peng, Zelong Yuan, and Jianchun Wang. Fourier neural operator approach to large eddy simulation of three-dimensional turbulence. Theoretical and Applied Mechanics Letters, 12(6):100389, 2022.   \n[28] Wenhui Peng, Zelong Yuan, Zhijie Li, and Jianchun Wang. Linear attention coupled fourier neural operator for simulation of three-dimensional turbulence. Physics of Fluids, 35(1), 2023.   \n[29] Boris Bonev, Thorsten Kurth, Christian Hundt, Jaideep Pathak, Maximilian Baust, Karthik Kashinath, and Anima Anandkumar. Spherical fourier neural operators: Learning stable dynamics on the sphere. In International conference on machine learning, pages 2806\u20132823. PMLR, 2023.   \n[30] Peter Yichen Chen, Jinxu Xiang, Dong Heon Cho, Yue Chang, GA Pershing, Henrique Teles Maia, Maurizio M Chiaramonte, Kevin Carlberg, and Eitan Grinspun. Crom: Continuous reduced-order modeling of pdes using implicit neural representations. arXiv preprint arXiv:2206.02607, 2022.   \n[31] Zilu Li, Guandao Yang, Xi Deng, Christopher De Sa, Bharath Hariharan, and Steve Marschner. Neural caches for monte carlo partial differential equation solvers. In SIGGRAPH Asia 2023 Conference Papers, pages 1\u201310, 2023.   \n[32] Thomas M\u00fcller, Fabrice Rousselle, Jan Nov\u00e1k, and Alexander Keller. Real-time neural radiance caching for path tracing. arXiv preprint arXiv:2106.12372, 2021.   \n[33] Hong Chul Nam, Julius Berner, and Anima Anandkumar. Solving poisson equations using neural walk-on-spheres. In ICLR 2024 Workshop on AI4DifferentialEquations In Science, 2024.   \n[34] M Cho, B Jadamba, R Kahler, AA Khan, and M Sama. First-order and second-order adjoint methods for the inverse problem of identifying non-linear parameters in pdes. Industrial Mathematics and Complex Systems: Emerging Mathematical Models, Methods and Algorithms, pages 147\u2013163, 2017.   \n[35] Colin Fox and Geoff Nicholls. Statistical estimation of the parameters of a pde. Can. appl. Math. Quater, 10:277\u2013810, 2001.   \n[36] Bastian Harrach. An introduction to finite element methods for inverse coefficient problems in elliptic pdes. Jahresbericht der Deutschen Mathematiker-Vereinigung, 123(3):183\u2013210, 2021.   \n[37] Krishna Kumar and Yonjin Choi. Accelerating particle and fluid simulations with differentiable graph networks for solving forward and inverse problems. In Proceedings of the SC\u201923 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis, pages 60\u201365, 2023.   \n[38] Tristan van Leeuwen and Felix J Herrmann. A penalty method for pde-constrained optimization in inverse problems. Inverse Problems, 32(1):015007, 2015.   \n[39] Da Long and Shandian Zhe. Invertible fourier neural operators for tackling both forward and inverse problems. arXiv preprint arXiv:2402.11722, 2024.   \n[40] Roberto Molinaro, Yunan Yang, Bj\u00f6rn Engquist, and Siddhartha Mishra. Neural inverse operators for solving pde inverse problems. arXiv preprint arXiv:2301.11167, 2023.   \n[41] Maarten V de Hoop, Matti Lassas, and Christopher A Wong. Deep learning architectures for nonlinear operator functions and nonlinear inverse problems. Mathematical Statistics and Learning, 4(1):1\u201386, 2022.   \n[42] Samira Pakravan, Pouria A Mistani, Miguel A Aragon-Calvo, and Frederic Gibou. Solving inverse-pde problems with physics-aware neural networks. Journal of Computational Physics, 440:110414, 2021.   \n[43] Valerii Iakovlev, Markus Heinonen, and Harri L\u00e4hdesm\u00e4ki. Learning continuous-time pdes from sparse data with graph neural networks. arXiv preprint arXiv:2006.08956, 2020.   \n[44] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.   \n[45] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Proc. NeurIPS, 2022.   \n[46] Hyungjin Chung, Jeongsol Kim, Michael T Mccann, Marc L Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. arXiv preprint arXiv:2209.14687, 2022.   \n[47] Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. Improving diffusion models for inverse problems using manifold constraints. Advances in Neural Information Processing Systems, 35:25683\u201325696, 2022.   \n[48] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780\u20138794, 2021.   \n[49] Sihan Xu, Yidong Huang, Jiayi Pan, Ziqiao Ma, and Joyce Chai. Inversion-free image editing with natural language. arXiv preprint arXiv:2312.04965, 2023.   \n[50] Christian Jacobsen, Yilin Zhuang, and Karthik Duraisamy. Cocogen: Physically-consistent and conditioned score-based generative models for forward and inverse problems. arXiv preprint arXiv:2312.10527, 2023.   \n[51] Marten Lienen, David L\u00fcdke, Jan Hansen-Palmus, and Stephan G\u00fcnnemann. From zero to turbulence: Generative modeling for 3d flow simulation. In The Twelfth International Conference on Learning Representations, 2023.   \n[52] Gefan Yang and Stefan Sommer. A denoising diffusion model for fluid field prediction. arXiv preprint arXiv:2301.11661, 2023.   \n[53] Phillip Lippe, Bas Veeling, Paris Perdikaris, Richard Turner, and Johannes Brandstetter. Pderefiner: Achieving accurate long rollouts with neural pde solvers. Advances in Neural Information Processing Systems, 36, 2024.   \n[54] Dule Shu, Zijie Li, and Amir Barati Farimani. A physics-informed diffusion model for highfidelity flow field reconstruction. Journal of Computational Physics, 478:111972, 2023.   \n[55] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021.   \n[56] Aapo Hyv\u00e4rinen and Peter Dayan. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(4), 2005.   \n[57] Yuanming Hu, Luke Anderson, Tzu-Mao Li, Qi Sun, Nathan Carr, Jonathan Ragan-Kelley, and Fr\u00e9do Durand. Difftaichi: Differentiable programming for physical simulation. arXiv preprint arXiv:1910.00935, 2019.   \n[58] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.   \n[59] Zijie Li, Kazem Meidani, and Amir Barati Farimani. Transformer for partial differential equations\u2019 operator learning. arXiv preprint arXiv:2205.13671, 2022.   \n[60] Evan F Bollig, Natasha Flyer, and Gordon Erlebacher. Solution to pdes using radial basis function finite-differences (rbf-fd) on multiple gpus. Journal of Computational Physics, 231(21):7133\u20137151, 2012. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Overview ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this supplementary material, we provide additional details to complement the main paper. Section B elaborates on the data generation process. Section C outlines the sampling implementation, and Section D highlights error reductions achieved by integrating PDE loss. Section E presents comprehensive visual results for both forward and inverse computations using sparse observations, which are not included in the main text. In Section F, we discuss results from full observation scenarios across all methods. Section G justifies our decision to train the baselines on complete observation data, while Section H shows results from optimized baseline methods. Section I and J provide standard deviation and runtime analyses, and Section K examines the model\u2019s robustness against random noise and varying observation locations, as well as the stochasticity of the model. Section L and M explores how different observation numbers and resolutions affect result accuracy, offering further insight into the model\u2019s performance under varying conditions. Lastly, Section N compares DiffusionPDE with additional baseline methods, including RBF kernel and U-Net. ", "page_idx": 14}, {"type": "text", "text": "B Data Generation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We generate 50,000 samples for each PDE and all diffusion models are trained on Nvidia A40 GPUs. ", "page_idx": 14}, {"type": "text", "text": "B.1 Static PDEs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We derived the methods of data generation for static PDEs from [3]. We first generate Gaussian random fields on $(0,1)^{2}$ so that $\\mu\\ensuremath{\\widetilde{\\sim}}\\mathcal{N}(0,(-\\Delta+9\\mathbf{I})^{-2})$ . For Darcy Flow, we let $a=f(\\mu)$ so that: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c c}{{\\displaystyle{\\left\\{a(x)=12,\\qquad\\mathrm{if}\\;\\mu(x)\\geq0\\right.}}}\\\\ {{\\displaystyle a(x)=3,\\qquad\\mathrm{if}\\;\\mu(x)<0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For the Poisson equation and Helmholtz equation, we let $a=\\mu$ as the coefficients. We then use second-order finite difference schemes to solve the solution $u$ and enforce the no-slip boundary condition for solutions by multiplying a mollifier $\\sin(\\pi x_{1})\\sin(\\pi x_{2})$ for point $x=(x_{1},x_{2}^{\\cdot})\\in(0,1)^{\\bar{2}}$ Both $a$ and $u$ have resolutions of $128\\times128$ . ", "page_idx": 14}, {"type": "text", "text": "B.2 Non-bounded Navier-Stokes Equation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We derived the method to generate non-bounded Navier-Stokes equation from [3]. The initial condition $w_{0}$ is generated by Gaussian random field $\\mathcal{N}(0,7^{1.5}(-\\Delta\\!+\\!4\\bar{9}\\mathbf{I})^{-2.5})$ . The forcing function follows the fixed pattern for point $(x_{1},x_{2})$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\nq(x)={\\frac{1}{10}}(\\sin(2\\pi(x_{1}+x_{2}))+\\cos(2\\pi(x_{1}+x_{2})))\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We then use the pseudo-spectral method to solve the Navier-Stokes equations in the stream-function formulation. We transform the equations into the spectral domain using Fourier transforms, solving the vorticity equation in the spectral domain, and then using inverse Fourier transforms to compute nonlinear terms in physical space. We simulate for 1 second with 10 timesteps, and $w_{t}$ has a resolution of $128\\times128$ . ", "page_idx": 14}, {"type": "text", "text": "B.3 Bounded Navier-Stokes Equation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We use Difftaichi [57] to generate data for the bounded Navier-Stokes equation. Specifically, we apply the Marker-and-Cell (MAC) method by solving a pressure-Poisson equation to enforce incompressibility and iterating through predictor and corrector steps to update the velocity and pressure fields. The grid is of the resolution $128\\times128$ and the center of the cylinder is at a random location in $[30,60]\\times[30,90]$ with a random radius in [5, 20]. The fluid flows into the grid from the upper boundary with a random initial vertical velocity in [0.5, 3]. We simulate for 1 second with 10 timesteps and study steps 4 to 8 when the turbulence is passing the cylinder. ", "page_idx": 14}, {"type": "text", "text": "B.4 Burgers\u2019 Equation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We derived the method to generate Burgers\u2019 equation from [3]. The initial condition $u_{0}$ is generated by Gaussian random field $\\bar{\\mathcal{N}}(0,625(-\\bar{\\Delta}+25\\mathbf{\\bar{I}})^{-2})$ . We solve the PDE with a spectral method and simulate 1 second with 127 additional timesteps. The final $u_{0:T}$ space has a resolution of $128\\times128$ . ", "page_idx": 14}, {"type": "text", "text": "C Guided Sampling Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For experiments with sparse observations or sensors, we find that DiffusionPDE performs the best when weights $\\zeta$ are selected as shown in Table 2. During the initial $80\\%$ of iterations in the sampling process, guidance is exclusively provided by the observation loss $\\mathcal{L}_{o b s}$ . Subsequently, after $80\\%$ of the iterations have been completed, we introduce the PDE loss $\\mathcal{L}_{p d e}$ , and reduce the weighting factor $\\zeta_{o b s}$ for the observation loss, by a factor of 10. This adjustment shifts the primary guiding influence to the PDE loss, thereby aligning the diffusion model more closely with the dynamics governed by the partial differential equations. ", "page_idx": 15}, {"type": "table", "img_path": "z0I2SbjN0R/tmp/1c9a6a845b5f405442edf1c8bb10c174a305c98d4c648f62668d5f488fbfdbe4.jpg", "table_caption": ["Table 2: The weights assigned to the PDE loss and the observation loss vary depending on whether the observations pertain to the coefficients (or initial states) $a$ or to the solutions (or final states) $u$ . "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "D Improvement in Prediction through PDE Loss Term ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "DiffusionPDE performs better when we apply the PDE loss term $\\mathcal{L}_{p d e}$ in addition to the observation loss term $\\mathcal{L}_{o b s}$ as guidance, as shown in Table 3. The errors in both the coefficients ( initial states) $a$ and the solutions (final states) $u$ significantly decrease. We also visualize the recovered $a$ and $u$ and corresponding absolute errors of Darcy Flow, Poisson equation, and Helmholtz equation in Fig. 8. It is demonstrated that the prediction becomes more accurate with the combined guidance of PDE loss and observation loss than with only observation loss. ", "page_idx": 15}, {"type": "table", "img_path": "z0I2SbjN0R/tmp/40af722b22b185821fdf1ce51815f952761738a37b73c9c737fc9e662374c55f.jpg", "table_caption": ["Table 3: DiffusionPDE\u2019 prediction errors of coefficients (initial states) $a$ and solutions (final states) $u$ with sparse observation on both $a$ and $u$ , guided by different loss functions. "], "table_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "z0I2SbjN0R/tmp/bba467c492073ad0d2764878d3eec301a5d68923898b6ca72d6b4f2ed6aaa395.jpg", "img_caption": ["(a) Recovered coefficients, solutions, and corresponding absolute errors of Darcy Flow. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "z0I2SbjN0R/tmp/a6686bf54c7edf078a98365a2ba67edd401cc07bbd7b759fc8269c2e6154a271.jpg", "img_caption": ["Figure 8: Recovered coefficients, solutions, and their corresponding visualized absolute errors for various PDE families. ", "(c) Recovered coefficients, solutions, and corresponding absolute errors of Helmholtz equation. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "E Additional Results on All PDEs with Sparse Observation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We present the recovered results of another Burgers\u2019 equation in Fig. 9. DiffusionPDE outperforms all other methods with 5 sensors for continuous observation. We also present the recovered results for both the forward and inverse problems of all other PDEs with sparse observations, as shown in Fig. 10. Specifically, we solve the forward and inverse problems for the Darcy Flow, Poisson equation, Helmholtz equation, and non-bounded Navier-Stokes equation using 500 random points observed in either the solution space or the coefficient space. Additionally, for the bounded Navier-Stokes equation, we observe $1\\%$ of the points in the velocity field. Our findings indicate that DiffusionPDE outperforms all other methods, providing the most accurate solutions. ", "page_idx": 16}, {"type": "text", "text": "Additional Data Setting for Darcy Flow To further demonstrate the generalization capability of our model, we conducted additional tests on different data settings for Darcy Flow. In Fig. 11, we solve the forward and inverse problems of Darcy Flow with 500 observation points, adjusting the binary values of $a$ to 20 and 16 instead of the original 12 and 3 in Section B, i.e., ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{a(x)=20,\\qquad\\mathrm{if}\\;\\mu(x)\\geq0\\right.}\\\\ {a(x)=16,\\qquad\\mathrm{if}\\;\\mu(x)<0}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Our results indicate that DiffusionPDE performs equally well under these varied data settings, showcasing its robustness and adaptability. ", "page_idx": 16}, {"type": "image", "img_path": "z0I2SbjN0R/tmp/81187b07c9895d41a576929de18624e785ab939c5819e132de754d3178635916.jpg", "img_caption": ["Figure 9: Results of another Burgers\u2019 equation recovered by 5 sensors throughout the time interval. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "z0I2SbjN0R/tmp/3afbb293a9aad12b3a6c23fbceb85bd4448896bb96b1ca76a5d52967d6afb778.jpg", "img_caption": ["(a) Forward and inverse results of Darcy Flow recovered by 500 observation points. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "z0I2SbjN0R/tmp/3a9125d8e208d1e96fb8e7d1bdd2be40ba75bcca476b9a6784a88676ab2abeb7.jpg", "img_caption": ["(b) Forward and inverse results of Poisson equation recovered by 500 observation points. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "z0I2SbjN0R/tmp/461ea69e0c63f4003327969844827626a0771bd0fe2128dbfa65d0343532d894.jpg", "img_caption": ["(c) Forward and inverse results of Helmholtz equation recovered by 500 observation points. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "z0I2SbjN0R/tmp/7994972ba51dc398b04987c6503ca043ac4070d3da966737303eae5d35bf6894.jpg", "img_caption": ["(d) Forward and inverse results of another non-bounded Navier-Stokes equation recovered by 500 observation points. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "z0I2SbjN0R/tmp/ec4875a5e4dd95afa6eadf2b64ea75c51ecb35c31098daac50c4bd37502c19ba.jpg", "img_caption": ["(e) Forward and inverse results of bounded Navier-Stokes equation recovered by $1\\%$ observation points. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "z0I2SbjN0R/tmp/570cdac1e163689e225c1b8aef46eab0d31cc53b68ca4c78553dd31bcb51c74a.jpg", "img_caption": ["(f) Inverse results of DiffusionPDE and GraphPDE of another bounded Navier-Stokes equation recovered by $1\\%$ observation points and the known boundary of the cylinder. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 10: Results of forward and inverse problems for different PDE families with sparse observation. ", "page_idx": 18}, {"type": "image", "img_path": "z0I2SbjN0R/tmp/84d5c233337b8ad99194782c1a0e661da017098e3c52c176d8e879900bec29a7.jpg", "img_caption": ["Figure 11: Forward and inverse results of Darcy Flow recovered by 500 observation points under a different data setting. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "z0I2SbjN0R/tmp/8d4f1fe45b530a084a7ac77736d2aab24616a9cbe3a6290330a82b862af3adb8.jpg", "img_caption": ["Figure 12: Results of Navier-Stokes equation and Burgers\u2019 equation with 10 times smaller viscosity. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Additional Data Setting for Non-bounded Navier-Stokes Equation and Burgers\u2019 Equation We also test DiffusionPDE on the Burgers\u2019 equation with a viscosity of $1\\times10^{-3}$ and on the non-bounded Navier-Stokes equation with a viscosity of $1\\times10^{-4}$ , which are 10 times smaller than the ones in the main paper, as shown in Fig. 12. For the Burgers\u2019 equation, we are able to recover the full time interval with 5 fixed sensors at a relative error of approximately $6\\%$ , which is close to the error of approximately $2\\sim5\\%$ in the main paper. For the Navier-Stokes equation, we can solve the forward and inverse problems with relative errors of approximately $7\\%$ and $9\\%$ , respectively, using 500 observation points. The errors are also close to the ones in the main paper, where the forward and inverse errors of Navier-Stokes equation are approximately $7\\%$ and $10\\%$ . ", "page_idx": 18}, {"type": "text", "text": "F Solving Forward and Inverse Problems with Full Observation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We have also included the errors of all methods when solving both the forward and inverse problems with full observation, as displayed in Table 4. ", "page_idx": 18}, {"type": "table", "img_path": "z0I2SbjN0R/tmp/53479932f1abb1fd42b21513d8d736514a3571ec983a7981aaf7936d855273fa.jpg", "table_caption": ["Table 4: Relative errors of solutions (or final states) and coefficients (or initial states) when solving forward and inverse problems with full observations. Error rates are used for the inverse problem of Darcy Flow. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "In general, DiffusionPDE and PINO outperform all other methods, and DiffusionPDE performs the best for all static PDEs. DiffusionPDE is capable of solving both forward and inverse problems with errors of less than $10\\%$ for all classes of discussed PDEs and is comparable to the state-of-the-art. Results of all methods regarding Darcy Flow and non-bounded Navier-Stokes equation are included in Fig. 13. ", "page_idx": 19}, {"type": "image", "img_path": "z0I2SbjN0R/tmp/daf917903360e8396527e523346c95ff3c77b082c2a59ae0caff9c7064030f72.jpg", "img_caption": ["(b) Forward and inverse results of non-bounded Navier-Stokes equation recovered by full observation. ", "Figure 13: Results of forward and inverse problems for different PDE families with full observation. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "G Training Baselines Methods on Partial Inputs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For our main experiments, we opt to train the baseline models (PINO, DeepONet, PINNs, FNO) on full observations for several compelling reasons: First, physics-informed models such as PINNs and PINO are unable to effectively compute the PDE loss when only sparse observations are available. Second, other models like DeepONet and FNO perform poorly with sparse observations. For instance, training the DeepONet model on 500 uniformly random points for each training sample in the context of the forward problem of Darcy Flow leads to testing outcomes that are consistently similar, as illustrated in Fig. 14, regardless of the testing input. This pattern suggests that the model tends to generate a generalized solution that minimizes the average error across all potential solutions rather than converging based on specific samples. Furthermore, the partial-input-trained model exhibits poor generalization when faced with a different distribution of observations from training, indicating that it lacks flexibility\u2014a critical attribute of our DiffusionPDE. ", "page_idx": 19}, {"type": "table", "img_path": "z0I2SbjN0R/tmp/47ed44ccfb57e37ba3d7cb7053569cb99ca34e3f1f445dad150beb1e34d98171.jpg", "table_caption": ["Table 5: Relative errors of solutions (or final states) and coefficients (or initial states) when solving forward and inverse problems respectively with sparse observations after optimizing the baselines. Error rates are used for the inverse problem of Darcy Flow. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "image", "img_path": "z0I2SbjN0R/tmp/2606f5f5db057a98c98e84a71b71797918d6ed408f0cbd96e3e1312ff8a1945d.jpg", "img_caption": ["Figure 14: Predicted solutions obtained using the DeepONet model trained with 500 observation points across different numbers of observation points. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "H Baseline Optimization ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We further refine the noisy outputs generated by baseline methods such as DeepONet, PINO, FNO, and PINNs. Specifically, given a partially observed parameter $\\textbf{\\em a}$ for the PDE $f(c;a,\\mathbf{u})=0$ and a pre-trained forward operator ${\\mathcal{F}}^{\\prime}$ , we address the problem by solving the optimization equation: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{a}\\mathcal{L}_{p d e}(\\pmb{a},\\mathcal{F}^{\\prime}(\\pmb{a});f)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and the results are shown in Table 5 and Fig. 15. Optimization reduces errors and smooths the solutions. However, the resulting values are smaller due to the smoothing effect from minimizing PDE loss, and the overall error compared to the ground truth remains much higher than DiffusionPDE. This may be due to the difficulty in optimizing the derivatives of noisy $\\textbf{\\em a}$ and $\\mathbf{u}$ . ", "page_idx": 20}, {"type": "text", "text": "I Standard Deviation of DiffusionPDE Experiment Results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We further assess the statistical significance of our DiffusionPDE by analyzing the standard deviations for forward and inverse problems under conditions of 500 sparse observation points and full observation, respectively, as detailed in Table 6. We evaluate our model using test sets comprising 1,000 samples for each PDE. Our findings confirm that full observation enhances the stability of the results, a predictable outcome as variability diminishes with an increase in observation points. The standard deviations are notably higher for more complex PDEs, such as the inverse problems of the Poisson and Helmholtz equations, reflecting the inherent challenges associated with these computations. Overall, DiffusionPDE demonstrates considerable stability, evidenced by relatively low standard deviations across various tests. ", "page_idx": 20}, {"type": "image", "img_path": "z0I2SbjN0R/tmp/7329da310a9cc46e201ad68a0dac8daf911e71815ba667e702fabb6b96f778af.jpg", "img_caption": ["Figure 15: Results of Poisson equation after optimizing baseline methods. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "table", "img_path": "z0I2SbjN0R/tmp/a89b1a8c1bbf7b34036595d9cd900d3d88e21ccb8908de7c5ccb30afee906f62.jpg", "table_caption": ["Table 6: Standard deviation of DiffusionPDE when solving forward and inverse problems with sparse or full observations. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "J Runtime Analysis ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We evaluate the computing cost during the inference stage by testing a single data point on a single A40 GPU for the Navier-Stokes equation, as shown in Table 7. DiffusionPDE has a lower computing cost compared to Shu et al. [54], which autoregressively solves the full time interval. This advantage becomes more significant when we increase the number of time steps. ", "page_idx": 21}, {"type": "table", "img_path": "z0I2SbjN0R/tmp/873bc09875e9ce76ac1928e48792543e6dfc6e3fbe6e1960df061dcb6a1f7b3a.jpg", "table_caption": ["Table 7: Inference computing cost of sparse-observation-based methods. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Further, we evaluate the inference runtimes on one single A40 GPU of vanilla full-observation-based methods and also the optimization time of them during the inference as introduced in Appendix H. The optimization runtimes are significantly slower, especially when using Fourier transforms. ", "page_idx": 21}, {"type": "table", "img_path": "z0I2SbjN0R/tmp/573c3013769683100a1b2147fea3a25f12defa3d046686355c3185d69fbab8ab.jpg", "table_caption": ["Table 8: Average inference runtimes (in seconds) of full-observation-based methods with and without optimization. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "K Robustness of DiffusionPDE ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We find that DiffusionPDE is robust against sparse noisy observation. In Fig. 16, we add Gaussian noise to the 500 observed points of Darcy Flow coefficients. Our DiffusionPDE can maintain a relative error of around $10\\%$ with a $15\\%$ noise level concerning the forward problem, and the recovered solutions are shown in Fig. 17. Baseline methods such as PINO also exhibit robustness against random noise under sparse observation conditions; this is attributed to their limited applicability to sparse observation problems, leading them to address the problem in a more randomized manner. ", "page_idx": 22}, {"type": "image", "img_path": "z0I2SbjN0R/tmp/4ba1ae1a06524274631832906f9b4355ccc3c14d2175823939e84be277200641.jpg", "img_caption": ["Figure 16: Relative errors of recovered Darcy Flow solutions with sparse noisy observation. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "z0I2SbjN0R/tmp/080a0b83bfc74a9538d8192270dec48ef6a9490182d38ad26325c6a8e915044e.jpg", "img_caption": ["Figure 17: Recovered solutions for Darcy Flow with noisy observations. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Robustness on Sampling Patterns Moreover, as mentioned in the main document, we investigate the robustness of DiffusionPDE on different sampling patterns of the observation points. Here, we address the forward problem of Darcy Flow using 500 observed coefficient points, which are non-uniformly concentrated on the left and right sides or are regularly distributed across the grid, as depicted in Fig. 18. Our results demonstrate that DiffusionPDE flexibly solves problems with arbitrary sparse observation locations within the spatial domain, without re-training the neural network model. However, the CFG method faces challenges when solving with varying sampling patterns, as demonstrated in Fig. 19. In this figure, we compare the reconstruction results of DiffusionPDE and Diffusion with CFG for the unbounded Navier-Stokes equation, where all observation points are located on the left side of the grid. The CFG approach struggles with this asymmetric sampling pattern, while DiffusionPDE maintains more accurate reconstructions. ", "page_idx": 22}, {"type": "image", "img_path": "z0I2SbjN0R/tmp/8b261d29e591c05d35f3e60b785b8382c72a5a9c8f57659a10a0f577357220dd.jpg", "img_caption": ["Figure 18: Recovered solutions for Darcy Flow with observations sampled using non-uniform distributions. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "z0I2SbjN0R/tmp/78ab41b114251a093ae892a4c4c83c764edca1a8d5b4a8e273e42c620f8d0c5a.jpg", "img_caption": ["Figure 19: Comparison between DiffusionPDE and Diffusion CFG under different sampling patterns for non-bounded Navier-Stokes equation. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Stochasticity Evaluation Since we employ a deterministic diffusion model, with partial observations as input, the only source of stochasticity or uncertainty in our approach arises from the initial random noise. To examine this, we conducted experiments to assess the impact of different noise seeds on both the initial and final states of the Navier-Stokes equations, as demonstrated in Fig. 20. Our findings indicate that the diffusion model exhibits some degree of uncertainty in its predictions, despite the deterministic nature of the underlying framework. ", "page_idx": 23}, {"type": "image", "img_path": "z0I2SbjN0R/tmp/d0be9a56be2eda06a5abf4db557bf26a6136395155c283dfe620ac765a076152.jpg", "img_caption": ["Figure 20: Different predictions of DiffusionPDE generated by different initial noise for non-bounded Navier-Stokes equation. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "L Solving Forward and Inverse Problems with Different Numbers of Observations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We also investigate how our DiffusionPDE handles varying degrees of sparse observation. Experiments are conducted on the Darcy Flow, Poisson equation, Helmholtz equation, and non-bounded Navier-Stokes equation. We examine the results of DiffusionPDE in solving forward and inverse problems when there are 100, 300, 500, and 1000 random observations on $a,\\,u,$ , or both $a$ and $u$ , as shown in Fig. 21. We have observed that the error of DiffusionPDE decreases as the number of sparse observations increases. Overall, we recover $u$ better than $a$ . DiffusionPDE can recover $u$ with approximately $2\\%$ observation points at any side pretty well. DiffusionPDE is also capable of recovering both $a$ and $u$ with errors $1\\%\\sim10\\%$ with approximately $6\\%$ observation points at any side for most PDE families. We also conclude that our DiffusionPDE becomes insensitive to the number of observations once more than $3\\%$ of the points are observed. ", "page_idx": 23}, {"type": "image", "img_path": "z0I2SbjN0R/tmp/fc2f0913936d4c019e2f4b55d521c3d795b6f05f5eb56449d6f630d3d1384bc2.jpg", "img_caption": ["(a) Error rates for Darcy Flow and relative errors for other PDEs of recovered coefficients or initial states $\\footnote{T w o t y p i c a l a p p l i c a t i o n s c e n a r i o s f o r t h e p r o p o s e d s y s t e m a r e h e a l t h c a r e,a n d l o g i s t i c s a n d w a r e h o u s i n g,i n w h i c h m u l t i p l e I o T d e v i c e s a r e d e p l o y e d c l o s e t o t h e r e c e i v e r a n d t h e t i m e d e l a y b e t w e e n t h e d i r e c t l i n k a n d b a c k s c a t t e r l i n k i s t h u s n e g l i g i b l e.}$ . "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "z0I2SbjN0R/tmp/71a19d14bb6abb674d939ec33600d27e2b038b45e7871c534bfc7cada69095ed.jpg", "img_caption": ["Figure 21: Error rate or relative error of both coefficients (or initial states) $a$ and solutions (or final states) $u$ with different numbers of observations. ", "(b) Relative errors of recovered solutions or final states $u$ . "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "M Solving Forward and Inverse Problems across Varied Resolutions ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "To evaluate the generalizability of DiffusionPDE, we implemented the model on various resolutions, including $64\\times64$ and $256\\times256$ , while maintaining the same percentage of observed points. For resolutions of $64\\times64$ , $128\\times128$ , and $256\\times256$ , we observe 125, 500, and 2000 points on $a$ or $u$ respectively, which are approximately $3\\%$ for each resolution. Overall, DiffusionPDE is capable of handling different resolutions effectively. For instance, Table 9 presents the forward relative errors of the solution $u$ and inverse error rates of the coefficient $a$ for the Darcy Flow, demonstrating that DiffusionPDE performs consistently well with similar error rates across various resolutions. ", "page_idx": 24}, {"type": "table", "img_path": "z0I2SbjN0R/tmp/2156da1a59a805e27785c9e9ed068f87676d8e221c08fdab78c29b885b08b519.jpg", "table_caption": ["Table 9: Forward relative errors and inverse error rates of Darcy Flow across different resolutions. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "N Comparison with Other Baselines ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We have compared the results using the RBF kernel [60], as shown in Fig. 22. For the forward process of solving the Poisson, Helmholtz, and Darcy Flow equations, the RBF kernel achieved solution errors of approximately $14.3\\%$ , $23.1\\%$ , and $18.4\\%$ , respectively, with 500 random observation points. However, when addressing the inverse problem, the errors increased significantly to $141.2\\%$ , $143.1\\%$ , and $34.0\\%$ , respectively. This increase in error is likely due to the inherent challenges of solving inverse problems with such a straightforward method. ", "page_idx": 25}, {"type": "image", "img_path": "z0I2SbjN0R/tmp/c0b1d4586989b9797d3931d5c889c2aa3cbde3fc4b5a0af010757ef51e204b96.jpg", "img_caption": ["Figure 22: Forward and Inverse Results of Poisson equation recovered by 500 observation points using RBF Kernel. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Additionally, we compare our DiffusionPDE method with a single U-Net model. The U-Net is trained based on our EDM diffusion model, where we initially train it to map between 500 fixed input points and the full output space, as illustrated in Fig. 23. For the Navier-Stokes equation, the prediction of the final state results in an average test error of approximately $39\\%$ , which is significantly higher than the error produced by our diffusion model. Furthermore, when making predictions using 500 different sampling points, the relative error increases to approximately $49\\%$ . We also train another U-Net model to map between 500 random input points and the full output space, but this model results in a test error of $101\\%$ , indicating that the U-Net struggles to adapt to varying sampling patterns and fails to flexibly solve different configurations. ", "page_idx": 25}, {"type": "image", "img_path": "z0I2SbjN0R/tmp/8178d7ef2b7b46e495816b2ed7e66026b2ae1b9f2e1e6a22377a8e55469795ea.jpg", "img_caption": ["Figure 23: Comparison between DiffusionPDE and U-Net regarding non-bounded Navier-Stokes equation. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We clearly state our contribution to solving forward and inverse problems of PDEs simultaneously with partial observation in the abstract and introduction (Section 1). ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We include the limitation of our work and future direction in Section 5. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We include theoretical results with full assumptions and complete proofs in Section 3. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We include all implementation details for reproducibility in the supplemental materials (Sections B and C). ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: In the Abstract, we include the URL to our project page, which provides access to the data and code. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We include implementation details of data generation and sampling in the supplemental materials (Sections B and C). ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We report the standard deviation of our DiffusionPDE in the supplemental materials (Section I). ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We talk about the compute resources in Sections 4.2 and J. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We follow the NeurIPS Code of Ethics in every respect. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our work, which concentrates on solving both forward and inverse problems of partial differential equations (PDEs) with sparse observations, promises to significantly advance scientific and engineering disciplines that rely on these computations. The potential negative impacts are minimal, as this research primarily aims to provide more accurate and efficient computational methods without adverse effects on existing systems or practices. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The paper poses no such risks. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 31}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]