[{"figure_path": "Iq2IAWozNr/tables/tables_7_1.jpg", "caption": "Table 2: FD distance among Ds and Du, representing the average distribution distance (\u00b1 standard deviation) after normalization per encoder varying splitting criteria (e.g., few and many shots regime) and tokens considered. Representations with higher FD distance on position splitting (where the background changes the most) compared to the other splitting rely on more spurious correlations for our task (i.e., not minimal).", "description": "This table shows the Fr\u00e9chet Distance (FD) between the annotated samples (Ds) and the unannotated samples (Du) for different encoders and annotation splitting criteria.  The FD measures the distance between the distributions of feature embeddings from Ds and Du. Higher FD indicates a larger difference in distribution between annotated and unannotated samples, suggesting more spurious correlations between the features and the treatment effect.", "section": "Results"}, {"figure_path": "Iq2IAWozNr/tables/tables_17_1.jpg", "caption": "Table 3: Annotation splitting criteria details for the extensive experiments on ISTAnt described in Section 5 and 6.", "description": "This table details the data splits used in the ISTAnt experiments.  Different annotation strategies are compared, each with 'many' or 'few' annotations. The strategies include random sampling, experiment-based selection (selecting specific batches), and position-based selection (selecting specific positions).  For each strategy, the number of samples in the annotated set (Ds) and the unannotated set (Du) are given.", "section": "5 Experimental setup"}, {"figure_path": "Iq2IAWozNr/tables/tables_17_2.jpg", "caption": "Table 4: Model and training details for the extensive experiments on ISTAnt described in Section 5 and 6.", "description": "This table details the hyperparameters used in the training of the models for the ISTAnt experiments.  It specifies the encoders used (Vision Transformers), the token selection method, the MLP head architecture (number of layers and nodes, activation function), the tasks performed (single or double grooming prediction), whether dropout and regularization were used, the loss function (binary cross-entropy with positive weighting), the learning rates, the optimizer (Adam), batch size, number of epochs, and random seeds used.  This information is crucial for reproducibility of the experiments.", "section": "5 Experimental setup"}, {"figure_path": "Iq2IAWozNr/tables/tables_19_1.jpg", "caption": "Table 5: Annotation splitting criteria details for CausalMNIST experiments.", "description": "This table details the different data splits used in the CausalMNIST experiments.  It shows the annotation criteria (random or biased), the number of samples in the annotated set (Ds, ns), and the number of samples in the unannotated set (Du, nu) for both many-shot and few-shot settings.  The \"biased\" criteria annotates only images with a black pen, introducing a potential bias for downstream causal estimations.", "section": "5.2 CausalMNIST"}, {"figure_path": "Iq2IAWozNr/tables/tables_20_1.jpg", "caption": "Table 6: Training details for the ConvNets training on CausalMNIST.", "description": "This table details the hyperparameters used for training the convolutional neural networks (ConvNets) on the CausalMNIST dataset.  It specifies settings such as pre-processing, dropout, regularization, loss function, positive weight for the loss, learning rates, optimizer, batch size, number of epochs, and the number of random seeds used during training.", "section": "5.2 CausalMNIST"}, {"figure_path": "Iq2IAWozNr/tables/tables_21_1.jpg", "caption": "Table 7: Two-sided t-test for H0: E[TEB(f)] = 0. We found statistical evidence to reject the hypothesis that f is unbiased for (almost) each annotation criterion.", "description": "This table presents the results of two-sided t-tests performed to assess the null hypothesis that the treatment effect bias (TEB) of a predictive model (f) is equal to zero.  The tests were conducted for different annotation criteria (random and biased) and annotation regimes (many and few annotations). The p-values indicate the statistical significance of rejecting the null hypothesis for each scenario.  Small p-values (less than a significance level, e.g., 0.05) suggest strong evidence against the null hypothesis, indicating that the model is likely biased for those conditions.", "section": "Results"}]