[{"figure_path": "JjQl8hXJAS/figures/figures_1_1.jpg", "caption": "Figure 1: Left: Overview of our approach compared to standard sim2real transfer on puck pushing task. Standard sim2real transfer first trains a policy to solve the goal task in sim and then transfers this policy to real. This policy may fail to solve the task in real due to the sim2real gap, and furthermore may not provide sufficient data coverage to successfully learn a policy that does solve the goal task in real. In contrast, our approach trains a set of exploratory policies in sim which achieve high-coverage data when deployed in real, even if they are unable to solve the task 0-shot. This high-coverage data can then be used to successfully learn a policy that solves the goal task in real. Right: Quantitative results running our approach on the puck pushing task illustrated on left, compared to standard sim2real transfer. Over 6 real-world trials, our approach solves the task 6/6 times while standard sim2real transfer solves the task 0/6 times.", "description": "This figure compares the proposed approach with standard sim2real transfer for a puck pushing task. The left panel illustrates the methods. Standard sim2real transfer trains a policy in simulation and directly transfers it to the real world, which may fail due to the sim2real gap and insufficient data coverage.  The proposed approach trains exploratory policies in simulation to collect high-coverage data in the real world, even if they don't solve the task initially. This data is then used to learn a policy that successfully solves the task in the real world. The right panel shows quantitative results for six real-world trials, where the proposed method successfully solves the task in all trials, while standard sim2real transfer fails in all trials.", "section": "5 Practical Algorithm and Experiments"}, {"figure_path": "JjQl8hXJAS/figures/figures_8_1.jpg", "caption": "Figure 2: Left: Illustration of Combination Lock Example. Right: Results on Combination Lock.", "description": "This figure shows a comparison of three different reinforcement learning approaches on a didactic combination lock task.  The left panel illustrates the structure of the combination lock problem, highlighting the state transitions and rewards. The right panel presents the results, comparing the performance of Exploration Policy Transfer (the authors' proposed method), Direct Policy Transfer (a standard baseline), and Q-learning with naive exploration (another standard baseline).  The graph shows the average reward achieved over time, demonstrating that the Exploration Policy Transfer approach significantly outperforms the baselines in learning to solve the combination lock problem.", "section": "Didactic Combination Lock Experiment"}, {"figure_path": "JjQl8hXJAS/figures/figures_8_2.jpg", "caption": "Figure 1: Left: Overview of our approach compared to standard sim2real transfer on puck pushing task. Standard sim2real transfer first trains a policy to solve the goal task in sim and then transfers this policy to real. This policy may fail to solve the task in real due to the sim2real gap, and furthermore may not provide sufficient data coverage to successfully learn a policy that does solve the goal task in real. In contrast, our approach trains a set of exploratory policies in sim which achieve high-coverage data when deployed in real, even if they are unable to solve the task 0-shot. This high-coverage data can then be used to successfully learn a policy that solves the goal task in real. Right: Quantitative results running our approach on the puck pushing task illustrated on left, compared to standard sim2real transfer. Over 6 real-world trials, our approach solves the task 6/6 times while standard sim2real transfer solves the task 0/6 times.", "description": "The figure compares the proposed approach with standard sim2real transfer methods on a puck pushing task. The left panel illustrates the key differences: standard sim2real transfer trains a policy in simulation and directly transfers it to the real world, which may fail due to the sim-to-real gap and insufficient data coverage. In contrast, the proposed method trains exploratory policies in simulation to gather high-coverage data in the real world, enabling efficient learning of a successful policy.  The right panel presents the quantitative results from 6 real-world trials. The proposed method achieves perfect success (6/6), while standard sim2real transfer fails completely (0/6).", "section": "1 Introduction"}, {"figure_path": "JjQl8hXJAS/figures/figures_9_1.jpg", "caption": "Figure 1: Left: Overview of our approach compared to standard sim2real transfer on puck pushing task. Standard sim2real transfer first trains a policy to solve the goal task in sim and then transfers this policy to real. This policy may fail to solve the task in real due to the sim2real gap, and furthermore may not provide sufficient data coverage to successfully learn a policy that does solve the goal task in real. In contrast, our approach trains a set of exploratory policies in sim which achieve high-coverage data when deployed in real, even if they are unable to solve the task 0-shot. This high-coverage data can then be used to successfully learn a policy that solves the goal task in real. Right: Quantitative results running our approach on the puck pushing task illustrated on left, compared to standard sim2real transfer. Over 6 real-world trials, our approach solves the task 6/6 times while standard sim2real transfer solves the task 0/6 times.", "description": "The figure illustrates the difference between standard sim2real transfer and the proposed approach. Standard sim2real transfer trains a policy in simulation and directly transfers it to the real world, which often fails due to the sim-to-real gap. The proposed approach trains exploratory policies in simulation to collect high-coverage data in the real world, enabling efficient real-world learning.  The right panel shows quantitative results demonstrating that the proposed method successfully solved the task in all 6 trials, whereas standard sim2real transfer failed in all trials.", "section": "1 Introduction"}, {"figure_path": "JjQl8hXJAS/figures/figures_40_1.jpg", "caption": "Figure 1: Left: Overview of our approach compared to standard sim2real transfer on puck pushing task. Standard sim2real transfer first trains a policy to solve the goal task in sim and then transfers this policy to real. This policy may fail to solve the task in real due to the sim2real gap, and furthermore may not provide sufficient data coverage to successfully learn a policy that does solve the goal task in real. In contrast, our approach trains a set of exploratory policies in sim which achieve high-coverage data when deployed in real, even if they are unable to solve the task 0-shot. This high-coverage data can then be used to successfully learn a policy that solves the goal task in real. Right: Quantitative results running our approach on the puck pushing task illustrated on left, compared to standard sim2real transfer. Over 6 real-world trials, our approach solves the task 6/6 times while standard sim2real transfer solves the task 0/6 times.", "description": "This figure compares the proposed approach with standard sim2real transfer in a puck-pushing task. The left panel illustrates the methods: standard sim2real transfer trains a policy in simulation and directly transfers it to the real world, which often fails due to the sim-to-real gap.  In contrast, the proposed approach trains exploratory policies in simulation to collect high-coverage data in the real world, even if these policies don't initially solve the task. This data is used to learn a real-world policy that successfully solves the task. The right panel shows quantitative results, demonstrating the proposed approach\u2019s success rate of 6/6 compared to 0/6 for standard sim2real transfer.", "section": "5 Practical Algorithm and Experiments"}, {"figure_path": "JjQl8hXJAS/figures/figures_42_1.jpg", "caption": "Figure 1: Left: Overview of our approach compared to standard sim2real transfer on puck pushing task. Standard sim2real transfer first trains a policy to solve the goal task in sim and then transfers this policy to real. This policy may fail to solve the task in real due to the sim2real gap, and furthermore may not provide sufficient data coverage to successfully learn a policy that does solve the goal task in real. In contrast, our approach trains a set of exploratory policies in sim which achieve high-coverage data when deployed in real, even if they are unable to solve the task 0-shot. This high-coverage data can then be used to successfully learn a policy that solves the goal task in real. Right: Quantitative results running our approach on the puck pushing task illustrated on left, compared to standard sim2real transfer. Over 6 real-world trials, our approach solves the task 6/6 times while standard sim2real transfer solves the task 0/6 times.", "description": "The figure shows a comparison between the standard sim2real transfer approach and the proposed approach for solving a puck pushing task. The standard approach directly transfers a policy trained in simulation to the real world, which often fails due to the sim-to-real gap. In contrast, the proposed approach trains exploratory policies in simulation to gather high-coverage data in the real world, even if the policies do not initially solve the task. This high-coverage real-world data enables efficient learning of a policy that solves the task in the real world. The figure includes a qualitative illustration (left) and quantitative results (right) demonstrating the significant improvement achieved by the proposed approach.", "section": "1 Introduction"}, {"figure_path": "JjQl8hXJAS/figures/figures_42_2.jpg", "caption": "Figure 1: Left: Overview of our approach compared to standard sim2real transfer on puck pushing task. Standard sim2real transfer first trains a policy to solve the goal task in sim and then transfers this policy to real. This policy may fail to solve the task in real due to the sim2real gap, and furthermore may not provide sufficient data coverage to successfully learn a policy that does solve the goal task in real. In contrast, our approach trains a set of exploratory policies in sim which achieve high-coverage data when deployed in real, even if they are unable to solve the task 0-shot. This high-coverage data can then be used to successfully learn a policy that solves the goal task in real. Right: Quantitative results running our approach on the puck pushing task illustrated on left, compared to standard sim2real transfer. Over 6 real-world trials, our approach solves the task 6/6 times while standard sim2real transfer solves the task 0/6 times.", "description": "This figure compares the proposed sim2real transfer approach with a standard sim2real transfer approach on a puck-pushing task. The left panel illustrates the two approaches: Standard sim2real directly transfers a policy trained in simulation to the real world, which often fails due to sim2real gap and insufficient data coverage.  The proposed method trains exploratory policies in simulation that, when deployed in the real world, collect high-coverage data, enabling efficient real-world policy learning. The right panel quantitatively shows the results of six real-world trials, where the proposed method solved the task in all trials, while standard sim2real failed in all.", "section": "1 Introduction"}, {"figure_path": "JjQl8hXJAS/figures/figures_43_1.jpg", "caption": "Figure 1: Left: Overview of our approach compared to standard sim2real transfer on puck pushing task. Standard sim2real transfer first trains a policy to solve the goal task in sim and then transfers this policy to real. This policy may fail to solve the task in real due to the sim2real gap, and furthermore may not provide sufficient data coverage to successfully learn a policy that does solve the goal task in real. In contrast, our approach trains a set of exploratory policies in sim which achieve high-coverage data when deployed in real, even if they are unable to solve the task 0-shot. This high-coverage data can then be used to successfully learn a policy that solves the goal task in real. Right: Quantitative results running our approach on the puck pushing task illustrated on left, compared to standard sim2real transfer. Over 6 real-world trials, our approach solves the task 6/6 times while standard sim2real transfer solves the task 0/6 times.", "description": "This figure compares the authors' approach to standard sim2real transfer methods on a puck-pushing task.  The left panel illustrates the two approaches: standard sim2real (training a policy in simulation and directly transferring it to the real world, which may fail due to the sim-to-real gap), and the authors' proposed method (training exploratory policies in simulation to gather high-coverage data in the real world, then using that data to learn a policy that solves the task). The right panel shows quantitative results from six real-world trials demonstrating that the authors' method successfully solved the task in all trials, while standard sim2real failed in all trials.", "section": "1 Introduction"}, {"figure_path": "JjQl8hXJAS/figures/figures_43_2.jpg", "caption": "Figure 1: Left: Overview of our approach compared to standard sim2real transfer on puck pushing task. Standard sim2real transfer first trains a policy to solve the goal task in sim and then transfers this policy to real. This policy may fail to solve the task in real due to the sim2real gap, and furthermore may not provide sufficient data coverage to successfully learn a policy that does solve the goal task in real. In contrast, our approach trains a set of exploratory policies in sim which achieve high-coverage data when deployed in real, even if they are unable to solve the task 0-shot. This high-coverage data can then be used to successfully learn a policy that solves the goal task in real. Right: Quantitative results running our approach on the puck pushing task illustrated on left, compared to standard sim2real transfer. Over 6 real-world trials, our approach solves the task 6/6 times while standard sim2real transfer solves the task 0/6 times.", "description": "This figure compares the proposed exploration policy transfer approach with the standard sim2real transfer method for a puck pushing task.  The left panel illustrates the two approaches.  The standard sim2real method trains a policy in simulation and then directly transfers it to the real world, which often fails due to the sim-to-real gap and insufficient data coverage. The proposed method trains exploratory policies in simulation that are then used in the real world to gather high-coverage data, which then allows for learning a successful policy in the real world. The right panel shows quantitative results demonstrating the superior performance of the proposed approach.  Over six real-world trials, the proposed approach successfully solved the task in all six cases, while the standard sim2real method failed in all six.", "section": "1 Introduction"}]