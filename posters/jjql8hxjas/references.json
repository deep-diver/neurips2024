{"references": [{"fullname_first_author": "Alekh Agarwal", "paper_title": "Flambe: Structural complexity and representation learning of low rank mdps", "publication_date": "2020-MM-DD", "reason": "This paper provides foundational theoretical results on low-rank MDPs, a key concept underpinning the current paper's theoretical analysis."}, {"fullname_first_author": "Chi Jin", "paper_title": "Provably efficient reinforcement learning with linear function approximation", "publication_date": "2020-MM-DD", "reason": "This paper establishes the provable efficiency of reinforcement learning algorithms with linear function approximation, a setting directly relevant to the current paper's theoretical results."}, {"fullname_first_author": "Yuri Burda", "paper_title": "Exploration by random network distillation", "publication_date": "2018-MM-DD", "reason": "This work introduces a novel exploration strategy (RND) which is used as a baseline for comparison in the experimental analysis of the paper."}, {"fullname_first_author": "Benjamin Eysenbach", "paper_title": "Maximum entropy rl (provably) solves some robust rl problems", "publication_date": "2021-MM-DD", "reason": "This paper explores robust RL, which is important given the sim-to-real gap addressed in the current work.  It also provides relevant algorithms used in the current paper's experimental setup."}, {"fullname_first_author": "Tuomas Haarnoja", "paper_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor", "publication_date": "2018-MM-DD", "reason": "This paper introduces the Soft Actor-Critic (SAC) algorithm, which is a key component of the current paper's experimental methodology."}]}