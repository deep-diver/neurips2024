[{"figure_path": "JjQl8hXJAS/tables/tables_6_1.jpg", "caption": "Table 1: Hyperparameters used in Tycho training and finetuning", "description": "This table lists the hyperparameters used in the TychoEnv sim2sim experiment.  It includes values for reward balance (alpha in OS algorithm), learning rate, Q update magnitude, discount factor (gamma), batch size, steps per episode, replay buffer size, and the number of training steps (N) performed in the real environment. These settings are crucial for reproducibility and understanding the experimental setup.", "section": "5.3 Realistic Robotics sim2sim Experiment"}, {"figure_path": "JjQl8hXJAS/tables/tables_17_1.jpg", "caption": "Table 1: Hyperparameters used in Tycho training and finetuning", "description": "This table lists the hyperparameters used for training and fine-tuning the TychoEnv robotic simulator.  It includes values for reward balancing (alpha in OS algorithm), learning rate, Q update magnitude, discount factor (gamma), batch size, steps per episode, replay buffer size and the total number of training steps in the simulator (Mreal). The table details the specific settings used in the Tycho sim2sim experiments detailed in section 5.3 of the paper.", "section": "5 Practical Algorithm and Experiments"}, {"figure_path": "JjQl8hXJAS/tables/tables_42_1.jpg", "caption": "Table 1: Hyperparameters used in Tycho training and finetuning", "description": "This table lists the hyperparameters used for training and fine-tuning the Tycho robotic simulator.  It specifies the reward balance (alpha) for the OS algorithm, learning rate, Q-update magnitude (tau), discount factor (gamma), batch size, steps per episode, replay buffer size, and the total number of training steps in the real environment (Mreal). The reward balance alpha is varied across a range of values to find the optimal balance between reward and exploration.", "section": "5.3 Realistic Robotics sim2sim Experiment"}, {"figure_path": "JjQl8hXJAS/tables/tables_43_1.jpg", "caption": "Table 2: Hyperparameters used in Franka training and finetuning", "description": "This table lists the hyperparameters used in the Franka Emika Panda robot arm experiments, specifically for both the training and fine-tuning phases.  It includes values for reward balancing (alpha), reward threshold (epsilon), learning rate, Q-update magnitude (tau), discount factor (gamma), batch size, steps per episode, replay buffer size, and total training steps (N). These settings were used to optimize the performance of the reinforcement learning algorithms in the robotic manipulation tasks.", "section": "E.4 Franka sim2sim Transfer on Franka Emika Panda Robot Arm"}]