[{"type": "text", "text": "Pure Message Passing Can Estimate Common Neighbor for Link Prediction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kaiwen Dong1,2 Zhichun Guo1,2 Nitesh V. Chawla1,2 ", "page_idx": 0}, {"type": "text", "text": "1Computer Science and Engineering, University of Notre Dame 2Lucy Family Institute for Data and Society, University of Notre Dame {kdong2, zguo5, nchawla}@nd.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Message Passing Neural Networks (MPNNs) have emerged as the de facto standard in graph representation learning. However, when it comes to link prediction, they are not always superior to simple heuristics such as Common Neighbor (CN). This discrepancy stems from a fundamental limitation: while MPNNs excel in node-level representation, they stumble with encoding the joint structural features essential to link prediction, like CN. To bridge this gap, we posit that, by harnessing the orthogonality of input vectors, pure message-passing can indeed capture joint structural features. Specifically, we study the proficiency of MPNNs in approximating CN heuristics. Based on our findings, we introduce the Message Passing Link Predictor (MPLP), a novel link prediction model. MPLP taps into quasiorthogonal vectors to estimate link-level structural features, all while preserving the node-level complexities. We conduct experiments on benchmark datasets from various domains, where our method consistently outperforms the baseline methods, establishing new state-of-the-arts. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Link prediction is a cornerstone task in the field of graph machine learning, with broad-ranging implications across numerous industrial applications. From identifying potential new acquaintances on social networks [1] to predicting protein interactions [2], from enhancing recommendation systems [3] to completing knowledge graphs [4], the impact of link prediction is felt across diverse domains. Recently, with the advent of Graph Neural Networks (GNNs) [5] and more specifically, Message-Passing Neural Networks (MPNNs) [6], these models have become the primary tools for tackling link prediction tasks. Despite the resounding success of MPNNs in the realm of node and graph classification tasks [5, 7\u20139], it is intriguing to note that their performance in link prediction does not always surpass that of simpler heuristic methods [10]. ", "page_idx": 0}, {"type": "text", "text": "Zhang et al. [11] highlights the limitations of GNNs/MPNNs for link prediction tasks arising from its intrinsic property of permutation invariance. Owing to this property, isomorphic nodes invariably receive identical representations. This poses a challenge when attempting to distinguish links whose endpoints are isomorphic nodes. As illustrated in Figure 1a, nodes $v_{1}$ and $v_{3}$ share a Common Neighbor $v_{2}$ , while nodes $v_{1}$ and $v_{5}$ do not. Ideally, due to their disparate local structures, these two links $(v_{1},v_{3})$ and $(v_{1},v_{5})$ should receive distinct predictions. However, the permutation invariance of MPNNs results in identical representations for nodes $v_{3}$ and $v_{5}$ , leading to identical predictions for the two links. As Zhang et al. [11] asserts, such node-level representation, even with the most expressive MPNNs, cannot capture structural link representation such as Common Neighbors (CN), a critical aspect of link prediction. ", "page_idx": 0}, {"type": "text", "text": "In this work, we posit that the pure Message Passing paradigm [6] can indeed capture structural link representation by exploiting orthogonality within the vector space. We begin by presenting a motivating example, considering a non-attributed graph as depicted in Figure 1a. In order to fulfill the Message Passing\u2019s requirement for node vectors as input, we assign a one-hot vector to each node $v_{i}$ , such that the $i$ -th dimension has a value of one, with the rest set to zero. These vectors, viewed as signatures rather than mere permutation-invariant node representations, can illuminate pairwise relationships. Subsequently, we execute a single iteration of message passing as shown in Figure 1b, updating each node\u2019s vector by summing the vector of its neighbors. This process enables us to compute CN for any node pair by taking the inner product of the vectors of the two target nodes. ", "page_idx": 0}, {"type": "image", "img_path": "Xa3dVaolKo/tmp/2091444fae5cbb46bba04108f34d6940e569e62fd8aeaad6e9e6748a83c626d2.jpg", "img_caption": ["Figure 1: (a) Isomorphic nodes result in identical MPNN node representation, making it impossible to distinguish links such as $(v_{1},v_{3})$ and $(v_{1},v_{5})$ based on these representations. (b) MPNN counts Common Neighbor through the inner product of neighboring nodes\u2019 one-hot representation. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "At its core, this naive method employs an orthonormal basis as the node signatures, thereby ensuring that the inner product of distinct nodes\u2019 signatures is consistently zero. While this approach effectively computes CN, its scalability poses a significant challenge, given that its space complexity is quadratically proportional to the size of the graph. To overcome this, we draw inspiration from DotHash [12] and capitalize on the premise that the family of vectors almost orthogonal to each other swells exponentially, even with just linearly scaled dimensions [13]. Instead of relying on the orthogonal basis, we can propagate these quasi-orthogonal (QO) vectors and utilize the inner product to estimate the joint structural information of any node pair. ", "page_idx": 1}, {"type": "text", "text": "In sum, our paper presents several pioneering advances in the realm of GNNs for link prediction: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We are the first, both empirically and theoretically, to delve into the proficiency of GNNs in approximating heuristic predictors like CN for link prediction. This uncovers a previously uncharted territory in GNN research.   \n\u2022 Drawing upon the insights gleaned from GNNs\u2019 capabilities in counting CN, we introduce MPLP, a novel link prediction model. Uniquely, MPLP discerns joint structures of links and their associated substructures within a graph, setting a new paradigm in the field.   \n\u2022 Our empirical investigations provide compelling evidence of MPLP\u2019s dominance. Benchmark tests reveal that MPLP not only holds its own but outstrips state-of-the-art models in link prediction performance. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries and Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Notations. Consider an undirected graph $G=(V,E,X)$ , where $V$ represents the set of nodes with cardinality $n$ , indexed as $\\{1,\\ldots,n\\}$ , $E\\subseteq V\\times V$ denotes the observed set of edges, and $\\mathbf{X}_{i,:}\\in\\mathbb{R}^{F_{x}}$ encapsulates the attributes associated with node $i$ . Additionally, let $\\mathcal{N}_{v}$ signify the neighborhood of a node $v$ , that is $\\mathcal{N}_{v}=\\{u|\\mathrm{SPD}(u,v)=1\\}$ where the function $\\operatorname{SPD}(\\cdot,\\cdot)$ measures the shortest path distance between two nodes. Furthermore, the node degree of $v$ is given by $d_{v}=|\\mathcal{N}_{v}|$ . To generalize, we introduce the shortest path neighborhood $\\mathcal{N}_{v}^{s}$ , representing the set of nodes that are $s$ hops away from node $v$ , defined as $\\bar{\\mathcal{N}}_{v}^{s}=\\{u\\bar{|}\\mathrm{SPD}(u,v)=\\bar{s}\\}$ . ", "page_idx": 1}, {"type": "text", "text": "Link predictions. Alongside the observed set of edges $E$ , there exists an unobserved set of edges, which we denote as $E_{c}\\,\\subseteq\\,V\\,\\times\\,V\\setminus E$ . This unobserved set encompasses edges that are either absent from the original observation or are anticipated to materialize in the future within the graph $G$ ", "page_idx": 1}, {"type": "image", "img_path": "Xa3dVaolKo/tmp/b49a8d8e539341558870a1abd515974768690316fec852e556a6efb57b9c7f37.jpg", "img_caption": ["Figure 2: GNNs estimate CN, AA and RA via MSE regression, using the mean value as a Baseline. Lower values are better. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Consequently, we can formulate the link prediction task as discerning the unobserved set of edges $E_{c}$ . Heuristics link predictors include Common Neighbor (CN) [1], Adamic-Adar index (AA) [14], and Resource Allocation (RA) [15]. CN is simply counting the cardinality of the common neighbors, while AA and RA count them weighted to reflect their relative importance as a common neighbor. ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{CN}(u,v)=\\sum_{k\\in N_{u}\\cap N_{v}}1;\\;\\;\\mathbf{AAA}(u,v)=\\sum_{k\\in N_{u}\\cap N_{v}}\\frac{1}{\\log d_{k}};\\;\\;\\mathbf{RA}(u,v)=\\sum_{k\\in N_{u}\\cap N_{v}}\\frac{1}{d_{k}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Though heuristic link predictors are effective across various graph domains, their growing computational demands clash with the need for low latency. To mitigate this, approaches like ELPH [16] and DotHash [12] propose using estimations rather than exact calculations for these predictors. Our study, inspired by these works, seeks to further refine techniques for efficient link predictions. A detailed comparison with related works and our method is in Appendix A. ", "page_idx": 2}, {"type": "text", "text": "GNNs for link prediction. The advent of graphs incorporating node attributes has caused a significant shift in research focus toward methods grounded in GNNs. Most practical GNNs follow the paradigm of the Message Passing [6]. It can be formulated as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{m}_{v}^{(l)}=\\mathrm{AGGREGATE}\\left(\\{\\pmb{h}_{v}^{(l)},\\pmb{h}_{u}^{(l)},\\forall u\\in\\mathcal{N}_{v}\\}\\right),\\ \\pmb{h}_{v}^{(l+1)}=\\mathrm{UPDATE}\\left(\\{\\pmb{h}_{v}^{(l)},\\pmb{m}_{v}^{(l)}\\}\\right),\\ \\pmb{m}_{v}^{(l)}=\\frac{\\pmb{h}_{v}^{(l)}}{\\pmb{h}_{v}^{(l)}},\\ \\pmb{m}_{v}^{(l)}=\\frac{\\pmb{h}_{v}^{(l)}}{\\pmb{h}_{v}^{(l)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $h_{v}^{(l)}$ represents the vector of node $v$ at layer $l$ and $h_{v}^{(0)}=X_{v,:}$ . For simplicity, we use $h_{v}$ to represent the node vector at the last layer. The specific choice of the neighborhood aggregation function, AGGREGATE $(\\cdot)$ , and the updating function, UPDATE $(\\cdot)$ , dictates the instantiation of the GNN model, with different choices leading to variations of model architectures. In the context of link prediction tasks, the GAE model [17] derives link representation, $h(i,j)$ , as a Hadamard product of the target node pair representations, $\\pmb{h}_{(i,j)}=\\pmb{h}_{i}\\odot\\pmb{h}_{j}$ . Despite its seminal approach, the SEAL model [18], which labels nodes based on proximity to target links and then performs message-passing for each target link, is hindered by computational expense, limiting its scalability. Efficient alternatives like ELPH [16] estimate node labels, while NCNC [19] directly learns edgewise features by aggregating node representations of common neighbors. ", "page_idx": 2}, {"type": "text", "text": "3 Can Message Passing count Common Neighbor? ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we delve deep into the potential of MPNNs for heuristic link predictor estimation. We commence with an empirical evaluation to recognize the proficiency of MPNNs in approximating link predictors. Following this, we unravel the intrinsic characteristics of 1-layer MPNNs, shedding light on their propensity to act as biased estimators for heuristic link predictors and proposing an unbiased alternative. Ultimately, we cast light on how successive rounds of message passing can estimate the number of walks connecting a target node pair with other nodes in the graph. All proofs are provided in Appendix G. ", "page_idx": 2}, {"type": "text", "text": "3.1 Estimation via Mean Squared Error Regression ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To explore the capacity of MPNNs in capturing the overlap information inherent in heuristic link predictors, such as CN, AA and RA, we conduct an empirical investigation, adopting the GAE framework [17] with GCN [5] and SAGE [7] as representative encoders. SEAL [18], known for its proven proficiency in capturing heuristic link predictors, serves as a benchmark in our comparison. Additionally, we select a non-informative baseline estimation, simply using the mean of the heuristic link predictors on the training sets. The datasets comprise eight non-attributed graphs (more details in Section 5). Given that GNN encoders require node features for initial representation, we have to generate such features for our non-attributed graphs. We achieved this by sampling from a highdimensional Gaussian distribution with a mean of 0 and standard deviation of 1. Although one-hot encoding is frequently employed for feature initialization on non-attributed graphs, we choose to forgo this approach due to the associated time and space complexity. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "To evaluate the ability of GNNs to estimate CN information, we adopt a training procedure analogous to a conventional link prediction task. However, we reframe the task as a regression problem aimed at predicting heuristic link predictors, rather than a binary classification problem predicting link existence. This shift requires changing the objective function from cross-entropy to Mean Squared Error (MSE). Such an approach allows us to directly observe GNNs\u2019 capacity to approximate heuristic link predictors. ", "page_idx": 3}, {"type": "text", "text": "Our experimental findings, depicted in Figure 2, reveal that GCN and SAGE both display an ability to estimate heuristic link predictors, albeit to varying degrees, in contrast to the non-informative baseline estimation. More specifically, GCN demonstrates a pronounced aptitude for estimating RA and nearly matches the performance of SEAL on datasets such as C.ele, Yeast, and PB. Nonetheless, both GCN and SAGE substantially lag behind SEAL in approximating CN and AA. In the subsequent section, we delve deeper into the elements within the GNN models that facilitate this approximation of link predictors while also identifying factors that impede their accuracy. ", "page_idx": 3}, {"type": "text", "text": "3.2 Estimation capabilities of GNNs for link predictors ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "GNNs exhibit the capability of estimating link predictors. In this section, we aim to uncover the mechanisms behind these estimations, hoping to offer insights that could guide the development of more precise and efficient methods for link prediction. We commence with the following theorem: ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1. Let $G=(V,E)$ be a non-attributed graph and consider a $^{\\,l}$ -layer GCN/SAGE. Define the input vectors $\\pmb{X}\\,\\in\\,\\dot{\\mathbb{R}}^{N\\times\\Bar{F}}$ initialized randomly from a zero-mean distribution with standard deviation $\\sigma_{n o d e}$ . Additionally, let the weight matrix $\\dot{W}\\in\\mathbb{R}^{F^{\\prime}\\times F}$ be initialized from a zero-mean distribution with standard deviation \u03c3weight. After performing message passing, for any pair of nodes $\\{(u,v)|(u,v)\\in V\\times V\\setminus E\\}$ , the expected value of their inner product is given by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{I};\\;\\mathbb{E}(h_{u}\\cdot h_{v})=\\frac{C}{\\sqrt{\\hat{d}_{u}\\hat{d}_{v}}}\\sum_{k\\in\\mathcal{N}_{u}\\cap\\mathcal{N}_{v}}\\frac{1}{\\hat{d}_{k}};\\;\\;\\mathrm{\\boldmath~\\mathbb{S}A G E};\\;\\mathbb{E}(h_{u}\\cdot h_{v}^{'})=\\frac{C}{\\sqrt{d_{u}d_{v}}}\\sum_{k\\in\\mathcal{N}_{u}\\cap\\mathcal{N}_{v}}1,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The theorem suggests that given proper initialization of input vectors and weight matrices, MPNNbased models, such as GCN and SAGE, can adeptly approximate heuristic link predictors. This makes them apt for encapsulating joint structural features of any node pair. Interestingly, SAGE predominantly functions as a CN estimator, whereas the aggregation function in GCN grants it the ability to weigh the count of common neighbors in a way similar to RA. This particular trait of GCN is evidenced by its enhanced approximation of RA, as depicted in Figure 2. ", "page_idx": 3}, {"type": "text", "text": "Quasi-orthogonal vectors. The GNN\u2019s capability to approximate heuristic link predictors is primarily grounded in the properties of their input vectors in a linear space. When vectors are sampled from a high-dimensional linear space, they tend to be quasi-orthogonal, implying that their inner product is nearly 0 w.h.p. With message-passing, these QO vectors propagate through the graph, yielding in a linear combination of QO vectors at each node. The inner product between pairs of QO vector sets essentially echoes the norms of shared vectors while nullifying the rest. Such a trait enables GNNs to estimate CN through message-passing. A key advantage of QO vectors, especially when compared with orthonormal basis, is their computational efficiency. For a modest linear increment in space dimensions, the number of QO vectors can grow exponentially, given an acceptable margin of error [13]. An intriguing observation is that the orthogonality of QO vectors remains intact even after GNNs undergo linear transformations post message-passing, attributed to the randomized weight matrix initialization. This mirrors the dimension reduction observed in random projection [20]. ", "page_idx": 3}, {"type": "text", "text": "Limitations. While GNNs manifest a marked ability in estimating heuristic link predictors, they are not unbiased estimators and can be influenced by factors such as node pair degrees, thereby compromising their accuracy. Another challenge when employing such MPNNs is their limited generalization to unseen nodes. The neural networks, exposed to randomly generated vectors, may struggle to transform newly added nodes in the graph with novel random vectors. This practice also violates the permutation-invariance principle of GNNs when utilizing random vectors as node representation. It could strengthen generalizability if we regard these randomly generated vectors as signatures of the nodes, instead of their node features, and circumvent the use of MLPs for them. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Unbiased estimator. Addressing the biased element in Theorem 3.1, we propose the subsequent instantiation for the message-passing functions: ", "page_idx": 4}, {"type": "equation", "text": "$$\nh_{v}^{(l+1)}=\\sum_{u\\in\\mathcal{N}_{v}}h_{u}^{(l)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Such an implementation aligns with the SAGE model that employs sum aggregation devoid of selfnode propagation. This methodology also finds mention in DotHash [12], serving as a cornerstone for our research. With this kind of message-passing design, the inner product of any node pair signatures can estimate CN impartially: ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2. Let $G=(V,E)$ be a graph, and let the vector dimension be given by $F\\in\\mathbb{N}_{+}$ . Define the input vectors $X=(X_{i,j})$ , which are initialized from a random variable x having a mean of 0 and a standard deviation of $\\textstyle{\\frac{1}{\\sqrt{F}}}$ . Using the $^{\\,l}$ -layer message-passing in Equation 3, for any pair of nodes $\\{(u,v)|(u,v)\\in V\\times\\check{V}\\}$ , the expected value and variance of their inner product are: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathrm{~}}&{{}\\mathbb{E}(h_{u}\\cdot h_{v})=\\mathrm{CN}(u,v);}&{\\mathrm{~}}\\\\ {\\mathrm{~Var}(h_{u}\\cdot h_{v})=\\displaystyle\\frac{1}{F}\\left(d_{u}d_{v}+\\mathrm{CN}(u,v)^{2}-2\\mathrm{CN}(u,v)\\right)+F\\mathrm{Var}(\\mathrm{x}^{2})\\mathrm{CN}(u,v).}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Though this estimator provides an unbiased estimate for CN, its accuracy can be affected by its variance. Specifically, DotHash recommends selecting a distribution for input vector sampling from vertices of a hypercube with unit length, which curtails variance given that $\\mathrm{Var}\\left(\\mathbf{x}^{2}\\right)=0$ . However, the variance influenced by the graph structure isn\u2019t adequately addressed, and this issue will be delved into in Section 4. ", "page_idx": 4}, {"type": "text", "text": "Orthogonal node attributes. Both Theorem 3.1 and Theorem 3.2 underscore the significance of quasi orthogonality in input vectors, enabling message-passing to efficiently count CN. Intriguingly, in most attributed graphs, node attributes, often represented as bag-of-words [21], exhibit inherent orthogonality. This brings forth a critical question: In the context of link prediction, do GNNs primarily approximate neighborhood overlap, sidelining the intrinsic value of node attributes? We earmark this pivotal question for in-depth empirical exploration in Appendix E, where we find that random vectors as input to GNNs can catch up with or even outperform node attributes. ", "page_idx": 4}, {"type": "text", "text": "3.3 Multi-layer message passing ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Theorem 3.2 elucidates the estimation of CN based on a single iteration of message passing. This section explores the implications of multiple message-passing iterations and the properties inherent to the iteratively updated node signatures. We begin with a theorem delineating the expected value of the inner product for two nodes\u2019 signatures derived from any iteration of message passing: ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.3. Under the conditions defined in Theorem 3.2, let $h_{u}^{(l)}$ denote the vector for node $u$ after the $l$ -th message-passing iteration. We have: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big(h_{u}^{(p)}\\cdot h_{v}^{(q)}\\Big)=\\sum_{k\\in V}|w a l k s^{(p)}(k,u)||w a l k s^{(q)}(k,v)|,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $|w a l k s^{(l)}(u,v)|$ counts the number of length- $l$ walks between nodes u and $v$ . ", "page_idx": 4}, {"type": "text", "text": "This theorem posits that the message-passing procedure computes the number of walks between the target node pair and all other nodes. In essence, each message-passing trajectory mirrors the path of the corresponding walk. As such, $h_{u}^{(l)}$ aggregates the initial QO vectors originating from nodes reachable by length- $l$ walks from node $u$ . In instances where multiple length- $l$ walks connect node $k$ to $u$ , the associated QO vector $X_{k_{\\mathrm{i}}}$ ,: is incorporated into the sum $\\bar{|\\mathbf{walks}^{(l)}(k,u)|}$ times. ", "page_idx": 4}, {"type": "text", "text": "One might surmise a paradox, given that message-passing calculates the number of walks, not nodes. However, in a simple graph devoid of self-loops, where at most one edge can connect any two nodes, it is guaranteed that $|\\mathbf{walks}^{(1)}(u,v)|=1$ iff $\\mathrm{SPD}(u,v)=1$ . Consequently, the quantity of length-1 walks to a target node pair equates to CN, a first-order heuristic. It\u2019s essential to recognize, however, that $|\\mathrm{walks}^{(l)}(u,v)|\\geq1$ only implies $\\mathrm{SPD}(u,v)\\le l$ . This understanding becomes vital when employing message-passing for estimating the local structure of a target node pair in Section 4. ", "page_idx": 5}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we introduce our novel link prediction model, denoted as MPLP. Distinctively designed, MPLP leverages the pure essence of the message-passing mechanism to adeptly learn joint structural features of the target node pairs. ", "page_idx": 5}, {"type": "text", "text": "Node representation. While MPLP is specifically designed for its exceptional structural capture, it also embraces the inherent attribute associations of graphs that speak volumes about individual node characteristics. To fuse the attributes (if they exist in the graph) and structures, MPLP begins with a GNN, utilized to encode node $u$ \u2019s representation: $\\mathrm{GNN}(u)\\in\\mathbb{R}^{F_{x}}$ . This node representation will be integrated into the structural features when constructing the QO vectors. Importantly, this encoding remains flexible, permitting the choice of any node-level GNN. ", "page_idx": 5}, {"type": "text", "text": "4.1 QO vectors construction ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Probabilistic hypercube sampling. Though deterministic avenues for QO vector construction are documented [22, 23], our preference leans toward probabilistic techniques for their inherent simplicity. We inherit the sampling paradigm from DotHash [12], where each node $k$ is assigned with a node signature $h_{k}^{(0)}$ , acquired via random sampling from the vertices of an $F$ -dimensional hypercube with unit vector norms. Consequently, the sampling space for hk $h_{k}^{(0)}$ becomes $\\{-1/\\sqrt{F},1/\\sqrt{F}\\}^{F}$ . ", "page_idx": 5}, {"type": "image", "img_path": "Xa3dVaolKo/tmp/4d0b6f86df0f398ad26cca083e5f4f951fcd8a8da627c3a27679382a6e7ba0d7.jpg", "img_caption": ["Figure 3: Representation of the target link $\\bar{(u,v)}$ within our model (MPLP), with nodes color-coded based on their distance from the target link. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Harnessing One-hot hubs for variance reduction. The stochastic nature of our estimator brings along an inevitable accompaniment: variance. Theorem 3.2 elucidates that a graph\u2019s topology can augment estimator variance, irrespective of the chosen QO vector distribution. At the heart of this issue is the imperfectness of quasi-orthogonality. While a pair of vectors might approach orthogonality, the same cannot be confidently said for the subspaces spanned by larger sets of QO vectors. ", "page_idx": 5}, {"type": "text", "text": "Capitalizing on the empirical observation that real-world graphs predominantly obey the powerlaw distribution [24], we propose a strategy to control variance. Leveraging the prevalence of high-degree nodes\u2014or hubs\u2014we designate unique one-hot vectors for the foremost hubs. Conside\u221ar the graph\u221a\u2019s top- $b$ hubs; while other nodes draw their QO vectors from a hypercube $\\{-1/\\sqrt{F-b},1/\\sqrt{F-b}\\}^{F-b}\\times\\{0\\}^{b}$ , these hubs are assigned one-hot vectors from $\\{0\\}^{F-b}\\times\\{0,1\\}^{b}$ , reserving a distinct subspace of the linear space to safeguard orthogonality. Note that when new nodes are added, their QO vectors are sampled the same way as the non-hub nodes, which can ensure a tractable computation complexity. ", "page_idx": 5}, {"type": "text", "text": "Norm rescaling to facilitate weighted counts. Theorem 3.1 alludes to an intriguing proposition: the estimator\u2019s potential to encapsulate not just CN, but also RA. Essentially, RA and AA are nuanced heuristics translating to weighted enumerations of shared neighbors, based on their node degrees. In Theorem 3.2, such counts are anchored by vector norms during dot products. MPLP enhances this count methodology by rescaling node vector norms, drawing inspiration from previous works [12, 25]. ", "page_idx": 5}, {"type": "text", "text": "This rescaling is determined by the node\u2019s representation, ${\\mathrm{GNN}}(u)$ , and its degree $d_{u}$ . The rescaled vector is formally expressed as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\tilde{\\pmb h}_{k}^{(0)}=f(\\mathbf G\\mathbf N\\mathbf N(k)||[d_{k}])\\cdot\\pmb h_{k}^{(0)},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $f\\colon\\mathbb{R}^{F_{x}+1}\\rightarrow\\mathbb{R}$ is an MLP mapping the node representation and degree to a scalar, enabling the flexible weighted count paradigm. ", "page_idx": 6}, {"type": "text", "text": "4.2 Structural feature estimations ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Node label estimation. The estimator in Theorem 3.2 can effectively quantify CN. Nonetheless, solely relying on CN fails to encompass diverse topological structures embedded within the local neighborhood. To offer a richer representation, we turn to Distance Encoding (DE) [26]. DE acts as an adept labeling tool [11], demarcating nodes based on their shortest-path distances relative to a target node pair. For a given pair $(u,v)$ , a node $k$ belongs to a node set $\\mathrm{DE}(p,q)$ iff $\\operatorname{SPD}(u,k)=p$ and $\\operatorname{SPD}(v,k)=q$ . Unlike its usage as node labels, we opt to enumerate these labels, producing a link feature defined by $\\#(p,q)={\\bar{|}}\\mathrm{DE}(p,q)|$ . Our model adopts a philosophy akin to ELPH [16], albeit with a distinct node-estimation mechanism. ", "page_idx": 6}, {"type": "text", "text": "Returning to Theorem 3.3, we recall that message-passing as in Equation 3 essentially corresponds to walks. Our ambition to enumerate nodes necessitates a single-layer message-passing alteration, reformulating Equation 3 to: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\eta_{v}^{\\left(s\\right)}=\\sum_{k\\in\\mathcal{N}_{v}^{s}}\\tilde{h}_{k}^{\\left(0\\right)}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Here, $\\mathcal{N}_{v}^{s}$ pinpoints $v$ \u2019s shortest-path neighborhoods distanced by the shortest-path $s$ . This method sidesteps the duplication dilemma highlighted in Theorem 3.3, ensuring that $\\eta_{v}^{(s)}$ aggregates at most one QO vector per node. Similar strategies are explored in [27, 28]. ", "page_idx": 6}, {"type": "text", "text": "For a tractable computation, we limit the largest shortest-path distance as $r\\geq m a x(p,q)$ . Consequently, to capture the varied proximities of nodes to the target pair $(u,v)$ , we can deduce: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\#(p,q)=\\left\\{\\begin{array}{l l}{\\displaystyle\\mathbb{E}\\Big(\\eta_{u}^{(p)}\\cdot\\eta_{v}^{(q)}\\Big),\\qquad r\\geq p,q\\geq1}\\\\ {|\\!|\\mathcal{N}_{v}^{q}|-\\sum_{1\\leq s\\leq r}\\#(s,q),\\qquad p=0}\\\\ {|\\!|\\mathcal{N}_{u}^{p}|-\\sum_{1\\leq s\\leq r}\\#(p,s),\\qquad q=0}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Concatenating the resulting estimates yields the expressive structural features of MPLP. ", "page_idx": 6}, {"type": "text", "text": "Shortcut removal. The intricately designed structural features improve the expressiveness of MPLP. However, this augmented expressiveness introduces susceptibility to distribution shifts during link prediction tasks [29]. Consider a scenario wherein the neighborhood of a target node pair contains a node $k$ . Node $k$ resides a single hop away from one of the target nodes but requires multiple steps to connect with the other. When such a target node pair embodies a positive instance in the training data (indicative of an existing link), node $k$ can exploit both the closer target node and the link between the target nodes as a shortcut to the farther one. This dynamic ensures that for training-set positive instances, the maximum shortest-path distance from any neighboring node to the target pair is constrained to the smaller distance increased by one. This can engender a discrepancy in distributions between training and testing phases, potentially diminishing the model\u2019s generalization capability. ", "page_idx": 6}, {"type": "text", "text": "To circumvent this pitfall, we adopt an approach similar to preceding works [18, 30, 19, 31]. Specifically, we exclude target links from the original graph during each training batch, as shown by the dash line in Figure 3. This maneuver ensures these links are not utilized as shortcuts, thereby preserving the fidelity of link feature construction. ", "page_idx": 6}, {"type": "text", "text": "Feature integration for link prediction. Having procured the structural features, we proceed to formulate the encompassing link representation for a target node pair $(u,v)$ as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\pmb{h}_{(u,v)}=(\\mathbf{GNN}(u)\\odot\\mathbf{GNN}(v))||[\\#(1,1),\\dots,\\#(r,r)],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "which can be fed into a classifier for a link prediction between nodes $(u,v)$ . ", "page_idx": 6}, {"type": "table", "img_path": "Xa3dVaolKo/tmp/7208d0f9a2e74d824d2b5b624c186190aa525d4eb1ecaf41681c47a4b47c08d0.jpg", "table_caption": ["Table 1: Link prediction results on non-attributed benchmarks. The format is average score $\\pm$ standard deviation. The top three models are colored by First, Second, Third. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "Xa3dVaolKo/tmp/fe9a89bb15f1356a976a5b05c0f68b6b0b1c5461997bcb263379ea960031dd57.jpg", "table_caption": ["Table 2: Link prediction results on attributed benchmarks. The format is average score $\\pm$ standard deviation. The top three models are colored by First, Second, Third. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.3 More scalable estimation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "MPLP estimates the cardinality of the distinct node sets with different distances relative to target node pairs in Equation 6. However, this operation requires a preprocessing step to construct the shortest-path neighborhoods $\\mathcal{N}_{v}^{s}$ for $s\\leq r$ , which can cause computational overhead on large-scale graph benchmarks. To overcome this issue, we simplify the structural feature estimations as: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\#(p,q)=\\mathbb{E}\\Big(\\tilde{\\pmb{h}}_{u}^{(p)}\\cdot\\tilde{\\pmb{h}}_{v}^{(q)}\\Big),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "wGhNerNes $\\begin{array}{r}{\\tilde{\\pmb{h}}_{v}^{(l+1)}=\\sum_{u\\in\\mathcal{N}_{v}}\\tilde{\\pmb{h}}_{u}^{(l)}}\\end{array}$ nfgo lloonlwys  rtehqe uimreesss tahgee -opnaes-shinogp  dneeifginhebd oirnh oEoqdu ,n  w3.h iScihm iisl aprr toov icdoemd mino na $\\mathcal{N}_{v}$ format of adjacency matrices/lists by most graph datasets. Therefore, we can substitute the structural features of MPLP with the estimation in Equation 7. We denote such a model with walk-level features as MPLP+. ", "page_idx": 7}, {"type": "text", "text": "4.4 Triangular substructure estimation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our method, primarily designed to encapsulate the local structure of a target node pair, unexpectedly exhibits the capacity for estimating the count of triangles linked to individual nodes. This capability, traditionally considered beyond the reach of GNNs, marks a significant advancement in the field [32]. Although triangle counting is less directly relevant in the context of link prediction, the implications of this capability are noteworthy. To maintain focus, we relegate the detailed discussion on pure message-passing for effective triangle counting to Appendix C. ", "page_idx": 7}, {"type": "table", "img_path": "Xa3dVaolKo/tmp/04bf31e30bdaaaef34d671301a85f0cd77224a8ea3172ef058b0131dcba86aed.jpg", "table_caption": ["Table 3: Link prediction results on OGB datasets under HeaRT [33]. The top three models are colored by First, Second, Third. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Datasets, baselines and experimental setup We conduct evaluations across a diverse spectrum of 15 graph benchmark datasets, which include 8 non-attributed and 7 attributed graphs 2. It also includes three datasets from OGB [10] with predefined train/test splits. In the absence of predefined splits, links are partitioned into train, validation, and test sets using a 70-10-20 percent split. Our comparison spans three categories of link prediction models: (1) heuristic-based methods encompassing CN, AA, and RA; (2) node-level models like GCN and SAGE; and (3) link-level models, including SEAL, Neo-GNN [25], ELPH [16], and NCNC [19]. Each experiment is conducted 10 times, with the average score and standard deviations reported. The evaluation metrics are aligned with the standard metrics for OGB datasets, and we utilize Hits $@50$ for the remaining datasets. We limit the number of hops $r\\,=\\,2$ , which results in a good balance of performance and efficiency. A comprehensive description of the experimental setup is available in Appendix D. ", "page_idx": 8}, {"type": "image", "img_path": "Xa3dVaolKo/tmp/b372cff0843fba61e9dbb71b32f1d479411e747bb12b925b7e74f26ec35a102d.jpg", "img_caption": ["Figure 4: Evaluation of inference time on large-scale OGB datasets. The inference time encompasses the entire cycle within a full-batch inference. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Results Performance metrics are shown in Tables 1 and 2. Our methods, MPLP and $\\mathrm{MPLP+}$ , demonstrate superior performance, surpassing baseline models across all evaluated benchmarks by a significant margin. Notably, MPLP tends to outperform $\\mathrm{MPLP+}$ in various benchmarks, suggesting that node-level structural features (Equation 6) might be more valuable for link prediction tasks than the walk-level features (Equation 7). In large-scale graph benchmarks such as PPA and Citation2, $\\mathrm{MPLP+}$ sets new benchmarks, establishing state-of-the-art results. For other datasets, our methods show a substantial performance uplift, with improvements in Hits $@50$ ranging from $2\\%$ to $10\\%$ compared to the closest competitors. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "We extend our evaluation of $\\mathrm{MPLP+}$ to assess its performance on large-scale datasets under the challenging HeaRT setting proposed by Li et al. [33]. HeaRT introduces a more rigorous and realistic set of negative samples during evaluation, typically resulting in a notable decline in performance across link prediction methods. As detailed in Table 3, $\\mathrm{MPLP+}$ consistently outperform all other methods across three OGB graph benchmarks in this demanding context. This underscores the robustness of $\\mathrm{MPLP+}$ , affirming its ability to maintain superior performance across a variety of graph benchmarks and evaluation settings. ", "page_idx": 9}, {"type": "text", "text": "Time efficiency We conduct an analysis of the time efficiency of our methods, MPLP and MPLP+, against established baselines using three large-scale OGB datasets. The results, illustrated in Figure 4, demonstrate that our approaches not only deliver superior performance across the graph benchmarks but also set a new benchmark for state-of-the-art time efficiency in full-batch inference. In particular, the primary component underlying our methods is the message-passing operation, which allows their inference speeds to rival that of the baseline GCN. Additionally, the structural feature estimations enhance the models\u2019 expressiveness, enabling more accurate representation of graph structures, particularly in the context of link prediction tasks. More details can be found in Appendix D.3. ", "page_idx": 9}, {"type": "image", "img_path": "Xa3dVaolKo/tmp/654e1d90ea73e99a61fdfadcb428796c174def247a75890b669033c53a8bc9b3.jpg", "img_caption": ["Figure 5: MSE of estimation for $\\#(1,1)$ , $\\#(1,2)$ and $\\#(1,0)$ on Collab. Lower values are better. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Estimation accuracy We investigate the precision of MPLP in estimating $\\#(p,q)$ , which denotes the count of node labels, using the Collab dataset. The outcomes of this examination are illustrated in Figure 5. Although ELPH possesses the capability to approximate these counts utilizing techniques like MinHash and Hyperloglog, our method exhibits superior accuracy. Moreover, ELPH runs out of memory when the dimension is larger than 3000. Remarkably, deploying a one-hot encoding strategy for the hubs further bolsters the accuracy of MPLP, concurrently diminishing the variance introduced by inherent graph structures. An exhaustive analysis, including time efficiency considerations, is provided in Appendix F.1. ", "page_idx": 9}, {"type": "text", "text": "Extended ablation studies Further ablation studies have been carried out to understand the individual contributions within MPLP. These include: (1) an exploration of the distinct components of MPLP in Appendix F.2; (2) an analysis of the performance contributions from different structural estimations in Appendix F.3; and (3) an examination of parameter sensitivity in Appendix F.4. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We study the potential of message-passing GNNs to encapsulate link structural features. Based on this, we introduce a novel link prediction paradigm that consistently outperforms state-of-the-art baselines across various graph benchmarks. The inherent capability to adeptly capture structures enhances the expressivity of GNNs, all while maintaining their computational efficiency. Our findings hint at a promising avenue for elevating the expressiveness of GNNs through probabilistic approaches. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We would like to thank the anonymous reviewers for their insightful comments and helpful discussions. This research was supported in part by the University of Notre Dame\u2019s Lucy Family Institute for Data and Society and the NSF Center for Computer-Assisted Synthesis (C-CAS), under grant number CHE-2202693. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] David Liben-Nowell and Jon Kleinberg. The link prediction problem for social networks. In Proceedings of the twelfth international conference on Information and knowledge management, CIKM \u201903, pages 556\u2013559, New York, NY, USA, November 2003. Association for Computing Machinery. ISBN 978-1-58113-723-1. doi: 10.1145/956863.956972. URL http://doi.org/ 10.1145/956863.956972.   \n[2] Damian Szklarczyk, Annika L. Gable, David Lyon, Alexander Junge, Stefan Wyder, Jaime Huerta-Cepas, Milan Simonovic, Nadezhda T. Doncheva, John H. Morris, Peer Bork, Lars J. Jensen, and Christian von Mering. STRING v11: protein-protein association networks with increased coverage, supporting functional discovery in genome-wide experimental datasets. Nucleic Acids Research, 47(D1):D607\u2013D613, January 2019. ISSN 1362-4962. doi: 10.1093/ nar/gky1131.   \n[3] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender systems. Computer, 42(8):30\u201337, 2009. Publisher: IEEE.   \n[4] Zhaocheng Zhu, Zuobai Zhang, Louis-Pascal Xhonneux, and Jian Tang. Neural bellman-ford networks: A general graph neural network framework for link prediction. Advances in Neural Information Processing Systems, 34, 2021.   \n[5] Thomas N. Kipf and Max Welling. Semi-Supervised Classification with Graph Convolutional Networks. arXiv:1609.02907 [cs, stat], February 2017. URL http://arxiv.org/abs/1609. 02907. arXiv: 1609.02907.   \n[6] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural Message Passing for Quantum Chemistry. CoRR, abs/1704.01212, 2017. URL http: //arxiv.org/abs/1704.01212. arXiv: 1704.01212.   \n[7] William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive Representation Learning on Large Graphs. arXiv:1706.02216 [cs, stat], September 2018. URL http://arxiv.org/abs/ 1706.02216. arXiv: 1706.02216.   \n[8] Petar Veli\u02c7ckovi\u00b4c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph Attention Networks. arXiv:1710.10903 [cs, stat], February 2018. URL http://arxiv.org/abs/1710.10903. arXiv: 1710.10903.   \n[9] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How Powerful are Graph Neural Networks? CoRR, abs/1810.00826, 2018. URL http://arxiv.org/abs/1810.00826. arXiv: 1810.00826.   \n[10] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open Graph Benchmark: Datasets for Machine Learning on Graphs. arXiv:2005.00687 [cs, stat], February 2021. URL http://arxiv.org/abs/2005.00687. arXiv: 2005.00687.   \n[11] Muhan Zhang, Pan Li, Yinglong Xia, Kai Wang, and Long Jin. Labeling Trick: A Theory of Using Graph Neural Networks for Multi-Node Representation Learning. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 9061\u20139073. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/ 4be49c79f233b4f4070794825c323733-Paper.pdf.   \n[12] Igor Nunes, Mike Heddes, Pere Verg\u00e9s, Danny Abraham, Alexander Veidenbaum, Alexandru Nicolau, and Tony Givargis. DotHash: Estimating Set Similarity Metrics for Link Prediction and Document Deduplication, May 2023. URL http://arxiv.org/abs/2305.17310. arXiv:2305.17310 [cs].   \n[13] Paul C. Kainen and V\u02d8era K\u02daurkov\u00e1. Quasiorthogonal dimension of euclidean spaces. Applied Mathematics Letters, 6(3):7\u201310, May 1993. ISSN 0893-9659. doi: 10.1016/ 0893-9659(93)90023-G. URL https://www.sciencedirect.com/science/article/ pii/089396599390023G.   \n[14] Lada A. Adamic and Eytan Adar. Friends and neighbors on the Web. Social Networks, 25(3): 211\u2013230, 2003. ISSN 0378-8733. doi: https://doi.org/10.1016/S0378-8733(03)00009-1. URL https://www.sciencedirect.com/science/article/pii/S0378873303000091.   \n[15] Tao Zhou, Linyuan L\u00fc, and Yi-Cheng Zhang. Predicting missing links via local information. The European Physical Journal B, 71(4):623\u2013630, 2009. Publisher: Springer.   \n[16] Benjamin Paul Chamberlain, Sergey Shirobokov, Emanuele Rossi, Fabrizio Frasca, Thomas Markovich, Nils Yannick Hammerla, Michael M. Bronstein, and Max Hansmire. Graph Neural Networks for Link Prediction with Subgraph Sketching. September 2022. URL https://openreview.net/forum?id $\\overrightharpoon{}$ m1oqEOAozQU.   \n[17] Thomas N. Kipf and Max Welling. Variational Graph Auto-Encoders, 2016. _eprint: 1611.07308.   \n[18] Muhan Zhang and Yixin Chen. Link Prediction Based on Graph Neural Networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/ 53f0d7c537d99b3824f0f99d62ea2428-Paper.pdf.   \n[19] Xiyuan Wang, Haotong Yang, and Muhan Zhang. Neural Common Neighbor with Completion for Link Prediction, February 2023. URL http://arxiv.org/abs/2302.00890. arXiv:2302.00890 [cs].   \n[20] William Johnson and Joram Lindenstrauss. Extensions of Lipschitz maps into a Hilbert space. Contemporary Mathematics, 26:189\u2013206, January 1984. ISSN 9780821850305. doi: 10.1090/conm/026/737400.   \n[21] Skye Purchase, Yiren Zhao, and Robert D. Mullins. Revisiting Embeddings for Graph Neural Networks. November 2022. URL https://openreview.net/forum?id $\\equiv$ Ri2dzVt_a1h.   \n[22] Paul C Kainen. Orthogonal dimension and tolerance. Unpublished report, Washington DC: Industrial Math, 1992.   \n[23] Paul C Kainen and V\u02c7era Kurkova. Quasiorthogonal dimension. In Beyond traditional probabilistic data processing techniques: Interval, fuzzy etc. Methods and their applications, pages 615\u2013629. Springer, 2020.   \n[24] Albert-L\u00e1szl\u00f3 Barab\u00e1si and R\u00e9ka Albert. Emergence of Scaling in Random Networks. Science, 286(5439):509\u2013512, 1999. doi: 10.1126/science.286.5439.509. URL https://www.science.org/doi/abs/10.1126/science.286.5439.509. _eprint: https://www.science.org/doi/pdf/10.1126/science.286.5439.509.   \n[25] Seongjun Yun, Seoyoon Kim, Junhyun Lee, Jaewoo Kang, and Hyunwoo J. Kim. Neo-GNNs: Neighborhood Overlap-aware Graph Neural Networks for Link Prediction. November 2021. URL https://openreview.net/forum?id $\\cdot$ Ic9vRN3VpZ.   \n[26] Pan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec. Distance Encoding: Design Provably More Powerful Neural Networks for Graph Representation Learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 4465\u20134478. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 2f73168bf3656f697507752ec592c437-Paper.pdf.   \n[27] Ralph Abboud, Radoslav Dimitrov, and Ismail Ilkan Ceylan. Shortest Path Networks for Graph Property Prediction. November 2022. URL https://openreview.net/forum?id= mWzWvMxuFg1.   \n[28] Jiarui Feng, Yixin Chen, Fuhai Li, Anindya Sarkar, and Muhan Zhang. How Powerful are K-hop Message Passing Graph Neural Networks. May 2022. URL https://openreview. net/forum?id $\\cdot$ nN3aVRQsxGd.   \n[29] Kaiwen Dong, Yijun Tian, Zhichun Guo, Yang Yang, and Nitesh Chawla. FakeEdge: Alleviate Dataset Shift in Link Prediction. December 2022. URL https://openreview.net/forum? id=QDN0jSXuvtX.   \n[30] Haoteng Yin, Muhan Zhang, Yanbang Wang, Jianguo Wang, and Pan Li. Algorithm and System Co-design for Efficient Subgraph-based Graph Representation Learning. Proceedings of the VLDB Endowment, 15(11):2788\u20132796, July 2022. ISSN 2150-8097. doi: 10.14778/3551793. 3551831. URL http://arxiv.org/abs/2202.13538. arXiv:2202.13538 [cs].   \n[31] Jiarui Jin, Yangkun Wang, Weinan Zhang, Quan Gan, Xiang Song, Yong Yu, Zheng Zhang, and David Wipf. Refined Edge Usage of Graph Neural Networks for Edge Prediction. December 2022. doi: 10.48550/arXiv.2212.12970. URL https://arxiv.org/abs/2212.12970v1.   \n[32] Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna. Can Graph Neural Networks Count Substructures? arXiv:2002.04025 [cs, stat], October 2020. URL http://arxiv.org/abs/ 2002.04025. arXiv: 2002.04025.   \n[33] Juanhui Li, Harry Shomer, Haitao Mao, Shenglai Zeng, Yao Ma, Neil Shah, Jiliang Tang, and Dawei Yin. Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking, July 2023. URL http://arxiv.org/abs/2306.10453. arXiv:2306.10453 [cs].   \n[34] Leo Katz. A new status index derived from sociometric analysis. Psychometrika, 18(1):39\u201343, March 1953. ISSN 1860-0980. doi: 10.1007/BF02289026. URL https://doi.org/10. 1007/BF02289026.   \n[35] Gerard Salton and Michael J. McGill. Introduction to Modern Information Retrieval. McGrawHill, Inc., USA, 1986. ISBN 0-07-054484-0.   \n[36] Sergey Brin and Lawrence Page. The Anatomy of a Large-Scale Hypertextual Web Search Engine. Computer Networks, 30:107\u2013117, 1998. URL http://www-db.stanford.edu/ \\~backrub/google.html.   \n[37] Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks, November 2021. URL http://arxiv.org/abs/1810.02244. arXiv:1810.02244 [cs, stat].   \n[38] Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably Powerful Graph Networks. arXiv:1905.11136 [cs, stat], June 2020. URL http://arxiv.org/abs/ 1905.11136. arXiv: 1905.11136.   \n[39] Muhan Zhang and Pan Li. Nested Graph Neural Networks, 2021. URL https://arxiv.org/ abs/2110.13197.   \n[40] Fabrizio Frasca, Beatrice Bevilacqua, Michael M. Bronstein, and Haggai Maron. Understanding and Extending Subgraph GNNs by Rethinking Their Symmetries, June 2022. URL http: //arxiv.org/abs/2206.11140. arXiv:2206.11140 [cs].   \n[41] Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Random Features Strengthen Graph Neural Networks, 2021. _eprint: 2002.03155.   \n[42] Ralph Abboud, Ismail Ilkan Ceylan, Martin Grohe, and Thomas Lukasiewicz. The Surprising Power of Graph Neural Networks with Random Node Initialization, 2021. _eprint: 2010.01179.   \n[43] P\u00e1l Andr\u00e1s Papp, Karolis Martinkus, Lukas Faber, and Roger Wattenhofer. DropGNN: Random Dropouts Increase the Expressiveness of Graph Neural Networks, November 2021. URL http://arxiv.org/abs/2111.06283. arXiv:2111.06283 [cs].   \n[44] Vladimir Batagelj and Andrej Mrvar. Pajek datasets website, 2006. URL http://vlado.fmf. uni-lj.si/pub/networks/data/.   \n[45] Mark EJ Newman. Finding community structure in networks using the eigenvectors of matrices. Physical review E, 74(3):036104, 2006. Publisher: APS.   \n[46] Robert Ackland and others. Mapping the US political blogosphere: Are conservative bloggers more prominent? In BlogTalk Downunder 2005 Conference, Sydney, 2005.   \n[47] Christian Von Mering, Roland Krause, Berend Snel, Michael Cornell, Stephen G Oliver, Stanley Fields, and Peer Bork. Comparative assessment of large-scale data sets of protein\u2013protein interactions. Nature, 417(6887):399\u2013403, 2002. Publisher: Nature Publishing Group.   \n[48] Duncan J. Watts and Steven H. Strogatz. Collective dynamics of \u2018small-world\u2019 networks. Nature, 393:440\u2013442, 1998. URL https://api.semanticscholar.org/CorpusID:3034643.   \n[49] Neil Spring, Ratul Mahajan, and David Wetherall. Measuring ISP topologies with Rocketfuel. ACM SIGCOMM Computer Communication Review, 32(4):133\u2013145, 2002. Publisher: ACM New York, NY, USA.   \n[50] Muhan Zhang, Zhicheng Cui, Shali Jiang, and Yixin Chen. Beyond link prediction: Predicting hyperlinks in adjacency space. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.   \n[51] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. Pitfalls of Graph Neural Network Evaluation, June 2019. URL http://arxiv.org/abs/ 1811.05868. arXiv:1811.05868 [cs, stat].   \n[52] Matthias Fey and Jan E. Lenssen. Fast Graph Representation Learning with PyTorch Geometric. In ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Related work 16 ", "page_idx": 14}, {"type": "text", "text": "B Efficient inference at node-level complexity 16 ", "page_idx": 14}, {"type": "text", "text": "C.1 Method 17   \nC.2 Experiments 17 ", "page_idx": 14}, {"type": "text", "text": "D Experimental details 18 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D.1 Benchmark datasets . . 18   \nD.2 More details in baseline methods 19   \nD.3 Evaluation Details: Inference Time . 19   \nD.4 Software and hardware details . . . . 19   \nD.5 Time Complexity . . . . . . . 19   \nD.6 Hyperparameters . . . 20 ", "page_idx": 14}, {"type": "text", "text": "E Exploring Bag-Of-Words Node Attributes 20 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "E.1 Node Attribute Orthogonality . . . 21   \nE.2 Role of Node Attribute Information . 21   \nE.3 Expanding QO Vector Dimensions 22 ", "page_idx": 14}, {"type": "text", "text": "F Additional experiments 22 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "F.1 Node label estimation accuracy and time 22   \nF.2 Model enhancement ablation . 23   \nF.3 Structural features ablation 24   \nF.4 Parameter sensitivity . . 25 ", "page_idx": 14}, {"type": "text", "text": "G Theoretical analysis 25 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "G.1 Proof for Theorem 3.1 25   \nG.2 Proof for Theorem 3.2 . 27   \nG.3 Proof for Theorem 3.3 . 29 ", "page_idx": 14}, {"type": "text", "text": "H Limitations 29 ", "page_idx": 14}, {"type": "text", "text": "I Broader Impact 30 ", "page_idx": 14}, {"type": "text", "text": "A Related work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Link prediction Link prediction, inherent to graph data analysis, has witnessed a paradigm shift from its conventional heuristic-based methods to the contemporary, more sophisticated GNNs approaches. Initial explorations in this domain primarily revolve around heuristic methods such as CN, AA, RA, alongside seminal heuristics like the Katz Index [34], Jaccard Index [35], Page Rank [36], and Preferential Attachment [24]. However, the emergence of graphs associated with node attributes has shifted the research landscape towards GNN-based methods. Specifically, these GNN-centric techniques bifurcate into node-level and link-level paradigms. Pioneers like Kipf and Welling introduce the Graph Auto-Encoder (GAE) to ascertain node pair similarity through GNNgenerated node representation. On the other hand, link-level models, represented by SEAL [18], opt for subgraph extractions centered on node pairs, even though this can present scalability challenges. ", "page_idx": 15}, {"type": "text", "text": "Amplifying GNN Expressiveness with Randomness The expressiveness of GNNs, particularly those of the MPNNs, has been the subject of rigorous exploration [9]. A known limitation of MPNNs, their equivalence to the 1-Weisfeiler-Lehman test, often results in indistinguishable representation for non-isomorphic graphs. A suite of contributions has surfaced to boost GNN expressiveness, of which [37\u201340] stand out. An elegant, yet effective paradigm involves symmetry-breaking through stochasticity injection [41\u201343]. Although enhancing expressiveness, such random perturbations can occasionally undermine generalizability. Diverging from these approaches, our methodology exploits probabilistic orthogonality within random vectors, culminating in a robust structural feature estimator that introduces minimal estimator variance. ", "page_idx": 15}, {"type": "text", "text": "Link-Level Link Prediction While node-level models like GAE offer enviable efficiency, they occasionally fall short in performance when compared with rudimentary heuristics [16]. Efforts to build scalable link-level alternatives have culminated in innovative methods such as Neo-GNN [25], which distills structural features from adjacency matrices for link prediction. Elsewhere, ELPH [16] harnesses hashing mechanisms for structural feature representation, while NCNC [19] adeptly aggregates common neighbors\u2019 node representation. Notably, DotHash [12], which profoundly influenced our approach, employs quasi-orthogonal random vectors for set similarity computations, applying these in link prediction tasks. ", "page_idx": 15}, {"type": "text", "text": "Distinctively, our proposition builds upon, yet diversifies from, the frameworks of ELPH and DotHash. While resonating with ELPH\u2019s architectural spirit, we utilize a streamlined, efficacious hashing technique over MinHash for set similarity computations. Moreover, we resolve ELPH\u2019s limitations through strategic implementations like shortcut removal and norm rescaling. When paralleled with DotHash, our approach magnifies its potential, integrating it with GNNs for link predictions and extrapolating its applicability to multi-hop scenarios. It also judiciously optimizes variance induced by the structural feature estimator in sync with graph data. We further explore the potential of achieving higher expressiveness with linear computational complexity by estimating the substructure counting [32]. ", "page_idx": 15}, {"type": "text", "text": "B Efficient inference at node-level complexity ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In addition to its superior performance, MPLP stands out for its practical advantages in industrial applications due to its node-level inference complexity. This design is akin to employing an MLP as the predictor. Our method facilitates offline preprocessing, allowing for the caching of node signatures or representations. Consequently, during online inference in a production setting, MPLP merely requires fetching the relevant node signatures or representations and processing them through an MLP. This approach significantly streamlines the online inference process, necessitating only node-level space complexity and ensuring constant time complexity for predictions. This efficiency in both space and time makes MPLP particularly suitable for real-world applications where rapid, on-the-fly predictions are crucial. ", "page_idx": 15}, {"type": "text", "text": "C Estimate triangular substructures ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Not only does MPLP encapsulate the local structure of the target node pair by assessing node counts based on varying shortest-path distances, but it also pioneers in estimating the count of triangles ", "page_idx": 15}, {"type": "text", "text": "Table 4: Performance of different GNNs on learningFigure 6: Representation of the target link the counts of triangles, measured by MSE divided by(u, v) of MPLP after including the triangular variance of the ground truth counts. Shown here are theestimation component. ", "page_idx": 16}, {"type": "table", "img_path": "Xa3dVaolKo/tmp/b10059ae9a2f44ee1d8e762e2b16da10a5d5108f33ab37ca7fb199c0d21d8855.jpg", "table_caption": ["median (i.e., third-best) performances of each mo "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "linked to any of the nodes\u2014 an ability traditionally deemed unattainable for GNNs [32]. In this section, we discuss a straightforward implementation of the triangle estimation. ", "page_idx": 16}, {"type": "text", "text": "C.1 Method ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Constructing the structural feature with DE can provably enhance the expressiveness of the link prediction model [26, 11]. However, there are still prominent cases where labelling trick also fails to capture. Since labelling trick only considers the relationship between the neighbors and the target node pair, it can sometimes miss the subtleties of intra-neighbor relationships. For example, the nodes of $\\mathrm{DE}(1,1)$ in Figure 3 exhibit different local structures. Nevertheless, labelling trick like DE tends to treat them equally, which makes the model overlook the triangle substructure shown in the neighborhood. Chen et al. [32] discusses the challenge of counting such a substructure with a pure message-passing framework. We next give an implementation of message-passing to approximate triangle counts linked to a target node pair\u2014equivalent in complexity to conventional MPNNs. ", "page_idx": 16}, {"type": "text", "text": "For a triangle to form, two nodes must connect with each other and the target node. Key to our methodology is recognizing the obligatory presence of length-1 and length-2 walks to the target node. Thus, according to Theorem 3.3, our estimation can formalize as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\#(\\underset{u}{\\triangle})=\\frac{1}{2}\\mathbb{E}\\Big(\\tilde{h}_{u}^{(1)}\\cdot\\tilde{h}_{u}^{(2)}\\Big).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Augmenting the structural features with triangle estimates gives rise to a more expressive structural feature set of MPLP. ", "page_idx": 16}, {"type": "text", "text": "C.2 Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Following the experiment in Section 6.1 of [32], we conduct an experiment to evaluate MPLP\u2019s ability to count triangular substructures. Similarly, we generate two synthetic graphs as the benchmarks: the Erdos-Renyi graphs and the random regular graphs. We also present the performance of baseline models reported in [32]. Please refer to [32] for details about the experimental settings and baseline models. The results are shown in Table 4. ", "page_idx": 16}, {"type": "text", "text": "As the results show, the triangle estimation component of MPLP can estimate the number of triangles in the graph with almost negligible error, similar to other more expressive models. Moreover, MPLP achieves this with a much lower computational cost, which is comparable to 1-WL GNNs like GCN, GIN, and SAGE. It demonstrates MPLP\u2019s advantage of better efficiency over more complex GNNs like 2-IGN and PPGN. ", "page_idx": 16}, {"type": "table", "img_path": "Xa3dVaolKo/tmp/7ef598c92f5754b91ad97fce9b0ba628d70d5e4ff5655a1ec1af8141a202752e.jpg", "table_caption": ["Table 5: Statistics of benchmark datasets. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "D Experimental details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1 Benchmark datasets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The statistics of each benchmark dataset are shown in Table 5. The benchmarks without attributes are: ", "page_idx": 17}, {"type": "text", "text": "\u2022 USAir [44]: a graph of US airlines;   \n\u2022 NS [45]: a collaboration network of network science researchers;   \n\u2022 PB [46]: a graph of links between web pages on US political topics;   \n\u2022 Yeast [47]: a protein-protein interaction network in yeast;   \n\u2022 C.ele [48]: the neural network of Caenorhabditis elegans;   \n\u2022 Power [48]: the network of the western US\u2019s electric grid;   \n\u2022 Router [49]: the Internet connection at the router-level;   \n\u2022 E.coli [50]: the reaction network of metabolites in Escherichia coli. ", "page_idx": 17}, {"type": "text", "text": "4 out of 7 benchmarks with node attributes come from [51], while Collab, PPA and Citation2 are from Open Graph Benchmark [10]: ", "page_idx": 17}, {"type": "text", "text": "\u2022 CS: co-authorship graphs in the field of computer science, where nodes represent authors, edges represent that two authors collaborated on a paper, and node features indicate the keywords for each author\u2019s papers;   \n\u2022 Physics: co-authorship graphs in the field of physics with the same node/edge/feature definition as of CS;   \n\u2022 Computers: a segment of the Amazon co-purchase graph for computer-related equipment, where nodes represent goods, edges represent that two goods are frequently purchased together together, and node features represent the product reviews;   \n\u2022 Physics: a segment of the Amazon co-purchase graph for photo-related equipment with the same node/edge/feature definition as of Computers;   \n\u2022 Collab: a large-scale collaboration network, showcasing a wide array of interdisciplinary partnerships.   \n\u2022 PPA: a large-scale protein-protein association network, representing the biological interaction between proteins. ", "page_idx": 17}, {"type": "text", "text": "Since OGB datasets have a fixed split, no train test split is needed for it. For the other benchmarks, we randomly split the edges into 70-10-20 as train, validation, and test sets. The validation and test sets are not observed in the graph during the entire cycle of training and testing. They are only used for evaluation purposes. For Collab, it is allowed to use the validation set in the graph when evaluating on the test set. ", "page_idx": 18}, {"type": "text", "text": "We run the experiments 10 times on each dataset with different splits. For each run, we cache the split edges and evaluate every model on the same split to ensure a fair comparison. The average score and standard deviation are reported in Hits $@100$ for PPA, MMR for Citation2 and Hits $@50$ for the remaining datasets. ", "page_idx": 18}, {"type": "text", "text": "D.2 More details in baseline methods ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In our experiments, we explore advanced variants of the baseline models ELPH and NCNC. Specifically, for ELPH, Chamberlain et al. [16] propose BUDDY, a link prediction method that preprocesses node representations to achieve better efficiency but compromises its expressiveness. NCNC [19] builds upon its predecessor, NCN, by first estimating the complete graph structure and then performing inference. In our experiments, we select the most expressiveness variant to make sure it is a fair comparison between different model architectures. Thus, we select ELPH over BUDDY, and NCNC over NCN to establish robust baselines in our study. We conduct a thorough hyperparameter tuning for ELPH and NCNC to select the best-performing models on each benchmark dataset. We follow the hyperparameter guideline of ELPH and NCNC to search for the optimal structures. For ELPH, we run through hyperparameters including dropout rates on different model components, learning rate, batch size, and dimension of node embedding. For NCNC, we experiment on dropout rates on different model components, learning rates on different model components, batch size, usage of jumping knowledge, type of encoders, and other model-specific terms like alpha. For Neo-GNN and SEAL, due to their relatively inferior efficiency, we only tune the common hyperparameters like learning rate, size of hidden dimensions. ", "page_idx": 18}, {"type": "text", "text": "D.3 Evaluation Details: Inference Time ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Figure 4, we assess the inference time across different models on the OGB datasets for a single epoch of test links. Specifically, we clock the wall time taken by models to score the complete test set. This encompasses preprocessing, message-passing, and the actual prediction. For the SEAL model, we employ a dynamic subgraph generator during the preprocessing phase, which dynamically computes the subgraph. We substitute ELPH with BUDDY from [16] in this evaluation, since BUDDY exhibits better time efficiency compared to ELPH. For both BUDDY and our proposed methods, we initially propagate the node features and signatures just once at the onset of inference. These are then cached for subsequent scoring sessions. ", "page_idx": 18}, {"type": "text", "text": "D.4 Software and hardware details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We implement MPLP in Pytorch Geometric framework [52]. We run our experiments on a Linux system equipped with an NVIDIA A100 GPU with 80GB of memory. ", "page_idx": 18}, {"type": "text", "text": "D.5 Time Complexity ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The efficiency of MPLP stands out when it comes to link prediction inference. Let\u2019s denote $t$ as the number of target links, $d$ as the maximum node degree, $r$ as the number of hops to compute, and $F$ as the dimension count of node signatures. ", "page_idx": 18}, {"type": "text", "text": "For preprocessing node signatures, MPLP involves two primary steps: ", "page_idx": 18}, {"type": "text", "text": "1. Initially, the algorithm computes all-pairs unweighted shortest paths across the input graph to acquire the shortest-path neighborhood $\\mathcal{N}_{v}^{s}$ for each node. This can be achieved using a BFS approach for each node, with a time complexity of $O(|V||E|)$ .   \n2. Following this, MPLP propagates the QO vectors through the shortest-path neighborhood, which has a complexity of $O(t d^{r}F)$ , and then caches these vectors in memory. ", "page_idx": 18}, {"type": "text", "text": "During online scoring, MPLP performs the inner product operation with a complexity of $O(t F)$ , enabling the extraction of structural feature estimations. ", "page_idx": 19}, {"type": "text", "text": "However, during training, the graph\u2019s structure might vary depending on the batch of target links due to the shortcut removal operation. As such, MPLP proceeds in three primary steps: ", "page_idx": 19}, {"type": "text", "text": "1. Firstly, the algorithm extracts the $r$ -hop induced subgraph corresponding to these $t$ target links. In essence, we deploy a BFS starting at each node of the target links to determine their receptive fields. This process, conceptually similar to message-passing but in a reversed message flow, has a time complexity of $O(t d r)$ . Note that, different from SEAL, we extract one $r$ -hop subgraph induced from a batch of target links.   \n2. To identify the shortest-path neighborhood $\\mathcal{N}_{v}^{s}$ , we simply apply sparse-sparse matrix multiplications of the adjacency matrix to get the $s$ -power adjacency matrix, where $s=$ $1,2,\\ldots,r$ . Due to the sparsity, this takes $O\\bar{(|\\boldsymbol{V}|d^{r})}$ .   \n3. Finally, the algorithm engages in message-passing to propagate the QO vectors along the shortest-path neighborhoods, with a complexity of $O(t d^{r}F)$ , followed by performing the inner product at $O(t F)$ . ", "page_idx": 19}, {"type": "text", "text": "Summing up, the overall time complexity for MPLP in the training phase stands at $O(t d r+|V|d^{r}+$ $t d^{r}F)$ . ", "page_idx": 19}, {"type": "text", "text": "For $\\mathrm{MPLP+}$ , it does not require the preprocessing step for the shortest-path neighborhood. Thus, the time complexity is the same as any standard message-passing GNNs, $O(t d^{r}F)$ . ", "page_idx": 19}, {"type": "text", "text": "D.6 Hyperparameters ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We determine the optimal hyperparameters for our model through systematic exploration. The setting with the best performance on the validation set is selected. The chosen hyperparameters are as follows: ", "page_idx": 19}, {"type": "text", "text": "\u2022 Number of Hops $(r)$ : We set the maximum number of hops to $r=2$ . Empirical evaluation suggests this provides an optimal trade-off between accuracy and computational efficiency.   \n\u2022 Node Signature Dimension $(F)$ : The dimension of node signatures, $F$ , is fixed at 1024, except for Citation2 with 512. This configuration ensures that MPLP is both efficient and accurate across all benchmark datasets.   \n\u2022 The minimum degree of nodes to be considered as hubs $(b)$ : This parameter indicates the minimum degree of the nodes which are considered as hubs to one-hot encode in the node signatures. We experiment with values in the set [50, 100, 150].   \n\u2022 Batch Size $(B)$ : We vary the batch size depending on the graph type: For the 8 nonattributed graphs, we explore batch sizes within [512, 1024]. For the 4 attributed graphs coming from [51], we search within [2048, 4096]. For OGB datasets, we use 32768 for Collab and PPA, and 261424 for Citation2. ", "page_idx": 19}, {"type": "text", "text": "E Exploring Bag-Of-Words Node Attributes ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Section 3, we delved into the capability of GNNs to discern joint structural features, particularly when presented with Quasi-Orthogonal (QO) vectors. Notably, many graph benchmarks utilize text data to construct node attributes, representing them as Bag-Of-Words (BOW). BOW is a method that counts word occurrences, assigning these counts as dimensional values. With a large dictionary, these BOW node attribute vectors often lean towards QO due to the sparse nature of word representations. Consequently, many node attributes in graph benchmarks inherently possess the QO trait. Acknowledging GNNs\u2019 proficiency with QO vector input, we propose the question: Is it the QO property or the information embedded within these attributes that significantly impacts link prediction in benchmarks? This section is an empirical exploration of this inquiry. ", "page_idx": 19}, {"type": "image", "img_path": "Xa3dVaolKo/tmp/ab37e98400cde3d00916c00181e8a6f8383e50ef54b13ae5a0ca99638f672b90.jpg", "img_caption": ["Figure 7: Heatmap illustrating the inner product of node attributes across CS, Photo, and Collab datasets. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "Xa3dVaolKo/tmp/daf303940c55838e8aaea6f2478909c240f2e1b19d0f203b542c580166992d00.jpg", "img_caption": ["Figure 8: Heatmap illustrating the inner product of node attributes, arranged by node labels, across CS and Photo. The rightmost showcases the inner product of QO vectors. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "E.1 Node Attribute Orthogonality ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Our inquiry begins with the assessment of node attribute orthogonality across three attributed graphs: CS, Photo, and Collab. CS possesses extensive BOW vocabulary, resulting in node attributes spanning over 8000 dimensions. Contrarily, Photo has a comparatively minimal dictionary, encompassing just 745 dimensions. Collab, deriving node attributes from word embeddings, limits to 128 dimensions. ", "page_idx": 20}, {"type": "text", "text": "For our analysis, we sample 10000 nodes (7650 for Photo) and compute the inner product of their attributes. The results are visualized in Figure 7. Our findings confirm that with a larger BOW dimension, CS node attributes closely follow QO. However, this orthogonality isn\u2019t as pronounced in Photo and Collab\u2014especially Collab, where word embeddings replace BOW. Given that increased node signature dimensions can mitigate estimation variance (as elaborated in Theorem 3.2), one could posit GNNs might offer enhanced performance on CS, due to its extensive BOW dimensions. Empirical evidence from Table 2 supports this claim. ", "page_idx": 20}, {"type": "text", "text": "Further, in Figure 8, we showcase the inner product of node attributes in CS and Photo, but this time, nodes are sequenced by class labels. This order reveals that nodes sharing labels tend to have diminished orthogonality compared to random pairs\u2014a potential variance amplifier in structural feature estimation using node attributes. ", "page_idx": 20}, {"type": "text", "text": "E.2 Role of Node Attribute Information ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To discern the role of embedded information within node attributes, we replace the original attributes in CS, Photo, and Collab with random vectors\u2014denoted as random feat. These vectors maintain the original attribute dimensions, though each dimension gets randomly assigned values from $\\{-1,1\\}$ . The subsequent findings are summarized in Table 6. Intriguingly, even with this \u201cnoise\u201d as input, performance remains largely unaltered. CS attributes appear to convey valuable insights for link predictions, but the same isn\u2019t evident for the other datasets. In fact, introducing random vectors to Computers and Photo resulted in enhanced outcomes, perhaps due to their original attribute\u2019s ", "page_idx": 20}, {"type": "table", "img_path": "Xa3dVaolKo/tmp/e4bd65eb518900ecd733fa51a5c1cabbeb8ca55786ca7b59654dc40a3deaee9a.jpg", "table_caption": ["Table 6: Performance comparison of GNNs using node attributes versus random vectors $(\\mathrm{Hits}@50)$ . For simplicity, all GNNs are configured with two layers. "], "table_footnote": ["insufficient orthogonality hampering effective structural feature capture. Collab shows a performance drop with random vectors, implying that the original word embedding can contribute more to the link prediction than structural feature estimation with merely 128 QO vectors. "], "page_idx": 21}, {"type": "text", "text": "E.3 Expanding QO Vector Dimensions ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Lastly, we substitute node attributes with QO vectors of varied dimensions, utilizing GCN as the encoder. The outcomes of this experiment are cataloged in Table 6. What\u2019s striking is that GCNs, when furnished with lengthier random vectors, often amplify link prediction results across datasets, with the exception of CS. On Computers and Photo, a GCN even rivals our proposed model (Table 2), potentially attributed to the enlarged vector dimensions. This suggests that when computational resources permit, expanding our main experiment\u2019s node signature dimensions (currently set at 1024) could elevate our model\u2019s performance. On Collab. the performance increases significantly compared to the experiments which are input with 128-dimensional vectors, indicating that the structural features are more critical for Collab than the word embedding. ", "page_idx": 21}, {"type": "text", "text": "F Additional experiments ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "F.1 Node label estimation accuracy and time ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In Figure 5, we assess the accuracy of node label count estimation. For ELPH, the node signature dimension corresponds to the number of MinHash permutations. We employ a default hyperparameter setting for Hyperloglog, with $p=8$ , a configuration that has demonstrated its adequacy in [16]. For time efficiency evaluation, we initially propagate and cache node signatures, followed by performing the estimation. ", "page_idx": 21}, {"type": "text", "text": "Furthermore, we evaluate the node label count estimation for $\\#(2,2)$ and $\\#(2,0)$ . The outcomes are detailed in Figure 9. While MPLP consistently surpasses ELPH in estimation accuracy, the gains achieved via one-hot hubs diminish for $\\#(2,2)$ and $\\#(2,0)$ relative to node counts at a shortest-path distance of 1. This diminishing performance gain can be attributed to our selection criteria for one-hot encoding, which prioritizes nodes that function as hubs within a one-hop radius. However, one-hop hubs don\u2019t necessarily serve as two-hop hubs. While we haven\u2019t identified a performance drop for these two-hop node label counts, an intriguing avenue for future research would be to refine variance reduction strategies for both one-hop and two-hop estimations simultaneously. ", "page_idx": 21}, {"type": "image", "img_path": "Xa3dVaolKo/tmp/4ddd5b4077929704aff6fbd7bf5af8a8ab83b3c73cb9780fecfd381ebfee1d4a.jpg", "img_caption": ["Figure 9: MSE of estimation for $\\#(2,2)$ , $\\#(2,0)$ and estimation time on Collab. Lower values are better. "], "img_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "Xa3dVaolKo/tmp/3aa9ab701115b7c081a5616a2eafb0537236b711f1f911a2a0d22d9584e8a63e.jpg", "table_caption": ["Table 7: Ablation study on non-attributed benchmarks evaluated by Hits $@50$ . The format is average score $\\pm$ standard deviation. The top three models are colored by First, Second, Third. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Regarding the efficiency of estimation, MPLP consistently demonstrates superior computational efficiency in contrast to ELPH. When we increase the node signature dimension to minimize estimation variance, ELPH\u2019s time complexity grows exponentially and becomes impractical. In contrast, MPLP displays a sublinear surge in estimation duration. ", "page_idx": 22}, {"type": "text", "text": "It\u2019s also worth noting that ELPH exhausts available memory when the node signature dimension surpasses 3000. This constraint arises as ELPH, while estimating structural features, has to cache node signatures for both MinHash and Hyperloglog. Conversely, MPLP maintains efficiency by caching only one type of node signatures. ", "page_idx": 22}, {"type": "text", "text": "F.2 Model enhancement ablation ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We investigate the individual performance contributions of three primary components in MPLP: Shortcut removal, One-hot hubs, and Norm rescaling. To ensure a fair comparison, we maintain consistent hyperparameters across benchmark datasets, modifying only the specific component under evaluation. Moreover, node attributes are excluded from the model\u2019s input for this analysis. The outcomes of this investigation are detailed in Table 7 and Table 8. ", "page_idx": 22}, {"type": "text", "text": "Among the three components, Shortcut removal emerges as the most pivotal for MPLP. This highlights the essential role of ensuring the structural distribution of positive links is aligned between the training and testing datasets [29]. ", "page_idx": 22}, {"type": "text", "text": "Regarding One-hot hubs, while they exhibited strong results in the estimation accuracy evaluations presented in Figure 5 and Figure 9, their impact on the overall performance is relatively subdued. We hypothesize that, in the context of these sparse benchmark graphs, the estimation variance may not be sufficiently influential on the model\u2019s outcomes. ", "page_idx": 22}, {"type": "table", "img_path": "Xa3dVaolKo/tmp/af9088a33e6ac3862cd2531fdfc65d4325fd36a7ed06e1035f741aefef07fdd4.jpg", "table_caption": ["Table 8: Ablation study on attributed benchmarks evaluated by Hits $@50$ . The format is average score $\\pm$ standard deviation. The top three models are colored by First, Second, Third. "], "table_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "Xa3dVaolKo/tmp/bf90d7f9ddafd16c02e36ac71673afbbcd15f1775e531d48a30e23346fedda41.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "Xa3dVaolKo/tmp/f50905d57a8919dc0f38fed64d0ee17c70f20d990e7a5c894d96935505007964.jpg", "table_caption": ["Table 10: Ablation analysis highlighting the impact of various structural features on link prediction. Refer to Table 9 for detailed configurations of the structural features used. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Finally, Norm rescaling stands out as a significant enhancement in MPLP. This is particularly evident in its positive impact on datasets like Yeast, Physics, Photo, and Collab. ", "page_idx": 23}, {"type": "text", "text": "F.3 Structural features ablation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We further examine the contribution of various structural features to the link prediction task. These features include: $\\#(1,1)$ , $\\#(1,2)$ , $\\#(1,0)$ , $\\#(2,2)$ , $\\#(2,0)$ , and $\\#(\\triangle)$ . To ensure fair comparison, we utilize only the structural features for link representation, excluding the node representations derived from $\\mathrm{{GNN}(\\cdot)}$ . Given the combinatorial nature of these features, they are grouped into four categories: ", "page_idx": 23}, {"type": "text", "text": "Table 11: Ablation study of Batch Size $(B)$ on non-attributed benchmarks evaluated by Hits $@50$ . The format is average score $\\pm$ standard deviation. The top three models are colored by First, Second, Third. ", "page_idx": 24}, {"type": "table", "img_path": "Xa3dVaolKo/tmp/df5dbb424ca1f6c84d882b78649f97c5f39f96466b0869d330c0c1da6e50eea7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "\u2022 #(1, 1);   \n\u2022 #(1, 2), #(1, 0);   \n\u2022 #(2, 2), #(2, 0);   \n\u2022 #(\u25b3). ", "page_idx": 24}, {"type": "text", "text": "The configuration of these structural features and their corresponding results are detailed in Table 9 and Table 10. ", "page_idx": 24}, {"type": "text", "text": "Our analysis reveals that distinct benchmark datasets have varied preferences for structural features, reflecting their unique underlying distributions. For example, datasets PB and Power exhibit superior performance with 2-hop structural features, whereas others predominantly favor 1-hop features. Although $\\#(1,1)$ , which counts Common Neighbors, is often considered pivotal for link prediction, the two other 1-hop structural features, $\\#(1,2)$ and $\\#(1,0)$ , demonstrate a more pronounced impact on link prediction outcomes. Meanwhile, while the count of triangles, $\\#(\\triangle)$ , possesses theoretical significance for model expressiveness, it seems less influential for link prediction when assessed in isolation. However, its presence can bolster link prediction performance when combined with other key structural features. ", "page_idx": 24}, {"type": "text", "text": "F.4 Parameter sensitivity ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We perform an ablation study to assess the hyperparameter sensitivity of MPLP, focusing specifically on two parameters: Batch Size $(B)$ and Node Signature Dimension $(F)$ . ", "page_idx": 24}, {"type": "text", "text": "Our heightened attention to $B$ stems from its role during training. Within each batch, MPLP executes the shortcut removal. Ideally, if $B=1$ , only one target link would be removed, thereby preserving the local structures of other links. However, this approach is computationally inefficient. Although shortcut removal can markedly enhance performance and address the distribution shift issue (as elaborated in Appendix F.2), it can also inadvertently modify the graph structure. Thus, striking a balance between computational efficiency and minimal graph structure alteration is essential. ", "page_idx": 24}, {"type": "text", "text": "Our findings are delineated in Table 11, Table 12, Table 13, and Table 14. Concerning the batch size, our results indicate that opting for a smaller batch size typically beneftis performance. However, if this size is increased past a certain benchmark threshold, there can be a noticeable performance drop. This underscores the importance of pinpointing an optimal batch size for MPLP. Regarding the node signature dimension, our data suggests that utilizing longer QO vectors consistently improves accuracy by reducing variance. This implies that, where resources allow, selecting a more substantial node signature dimension is consistently advantageous. ", "page_idx": 24}, {"type": "text", "text": "G Theoretical analysis ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "G.1 Proof for Theorem 3.1 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We begin by restating Theorem 3.1 and then proceed with its proof: ", "page_idx": 24}, {"type": "text", "text": "Let $G\\,=\\,(V,E)$ be a non-attributed graph and consider a 1-layer GCN/SAGE. Define the input vectors $\\dot{\\boldsymbol{X}}\\in\\mathbb{R}^{\\dot{N}\\times\\boldsymbol{F}}$ initialized randomly from a zero-mean distribution with standard deviation $\\sigma_{n o d e}$ Additionally, let the weight matrix $W\\in\\mathbb{R}^{F^{\\prime}\\times F}$ be initialized from a zero-mean distribution with standard deviation $\\sigma_{w e i g h t}$ . After performing message passing, for any pair of nodes $\\{(u,v)|(u,v)\\in$ $V\\times V\\setminus E\\}$ , the expected value of their inner product is given by: ", "page_idx": 24}, {"type": "text", "text": "Table 12: Ablation study of Batch Size $(B)$ on attributed benchmarks evaluated by Hits $@50$ . The format is average score $\\pm$ standard deviation. The top three models are colored by First, Second, Third. ", "page_idx": 25}, {"type": "table", "img_path": "Xa3dVaolKo/tmp/88a491ed56d58e09dc4014abae2edfcda858305effca9261540015003f123ff0.jpg", "table_caption": [], "table_footnote": ["Table 13: Ablation study of Node Signature Dimension $(F)$ on non-attributed benchmarks evaluated by Hits $@50$ . The format is average score $\\pm$ standard deviation. The top three models are colored by First, Second, Third. "], "page_idx": 25}, {"type": "table", "img_path": "Xa3dVaolKo/tmp/7740af5b428dd83ef7035adfb31aedcf8de7d2f9e0f0941997af2f97dd070a12.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "For GCN: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}(h_{u}\\cdot h_{v})=\\frac{C}{\\sqrt{\\hat{d}_{u}\\hat{d}_{v}}}\\sum_{k\\in\\ensuremath{\\mathcal{N}}_{u}\\bigcap\\mathcal{N}_{v}}\\frac{1}{\\hat{d}_{k}},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For SAGE: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}(h_{u}\\cdot h_{v})=\\frac{C}{\\sqrt{d_{u}d_{v}}}\\sum_{k\\in\\mathcal{N}_{u}\\bigcap\\mathcal{N}_{v}}1,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\hat{d}_{v}=d_{v}+1$ and the constant $C$ is defined as $C=\\sigma_{n o d e}^{2}\\sigma_{w e i g h t}^{2}F F^{\\prime}$ ", "page_idx": 25}, {"type": "text", "text": "Using GCN as the MPNN, the node representation is updated by: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\boldsymbol{h}_{\\boldsymbol{u}}=\\boldsymbol{W}\\sum_{\\boldsymbol{k}\\in\\mathcal{N}(\\boldsymbol{u})\\cup\\{\\boldsymbol{u}\\}}\\frac{1}{\\sqrt{\\hat{d}_{\\boldsymbol{k}}\\hat{d}_{\\boldsymbol{u}}}}\\boldsymbol{X}_{\\boldsymbol{k}},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\hat{d}_{v}=d_{v}+1$ . ", "page_idx": 25}, {"type": "text", "text": "Table 14: Ablation study of Node Signature Dimension $(F)$ on attributed benchmarks evaluated by Hits $@50$ . The format is average score $\\pm$ standard deviation. The top three models are colored by First, Second, Third. ", "page_idx": 25}, {"type": "table", "img_path": "Xa3dVaolKo/tmp/c7003df23eea62073a3675b36905356393b4cc4f3f77171bd4bc96af28ea889a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "For any two nodes $(u,v)$ from $\\{(u,v)|(u,v)\\in V\\times V\\setminus E\\}$ , we compute: ", "page_idx": 26}, {"type": "text", "text": "hu \u00b7 hv = hu\u22a4 hv ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{{}}&{{=\\displaystyle\\left(W\\sum_{a\\in\\mathcal{N}(u)\\cup\\{u\\}}\\frac{1}{\\sqrt{\\hat{d}_{a}\\hat{d}_{u}}}X_{a}\\right)^{\\top}\\left(W\\sum_{b\\in\\mathcal{N}(v)\\cup\\{v\\}}\\frac{1}{\\sqrt{\\hat{d}_{b}\\hat{d}_{v}}}X_{b}\\right)}}\\\\ {{}}&{{=\\displaystyle\\sum_{a\\in\\mathcal{N}(u)\\cup\\{u\\}}\\frac{1}{\\sqrt{\\hat{d}_{a}\\hat{d}_{u}}}X_{a}^{\\top}W^{\\top}W\\sum_{b\\in\\mathcal{N}(v)\\cup\\{v\\}}\\frac{1}{\\sqrt{\\hat{d}_{b}\\hat{d}_{v}}}X_{b}}}\\\\ {{}}&{{=\\displaystyle\\sum_{a\\in\\mathcal{N}(u)\\cup\\{u\\}}\\frac{1}{\\sqrt{\\hat{d}_{a}\\hat{d}_{u}}}X_{a}^{\\top}\\left(\\sum_{b\\}^{W_{1}^{\\top}}W_{1}\\quad\\cdots\\quad W_{1}^{\\top}W_{F}\\right)\\sum_{b\\in\\mathcal{N}(v)\\cup\\{v\\}}\\frac{1}{\\sqrt{\\hat{d}_{b}\\hat{d}_{v}}}X_{b}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Given that ", "page_idx": 26}, {"type": "text", "text": "we obtain: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}(h_{u}\\cdot h_{v})=\\sigma_{w e i g h t}^{2}F^{\\prime}\\sum_{a\\in\\mathcal{N}(u)\\cup\\{u\\}}\\frac{1}{\\sqrt{\\hat{d}_{a}\\hat{d}_{u}}}X_{a}^{\\top}\\sum_{b\\in\\mathcal{N}(v)\\cup\\{v\\}}\\frac{1}{\\sqrt{\\hat{d}_{b}\\hat{d}_{v}}}X_{b}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Also the orthogonal of the random vectors guarantee that $\\mathbb{E}\\big(X_{a}^{\\top}X_{b}\\big)=0$ when $a\\neq b$ . Then, we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}(h_{u}\\cdot h_{v})=\\frac{C}{\\sqrt{\\hat{d}_{u}\\hat{d}_{v}}}\\sum_{k\\in\\mathcal{N}_{u}\\bigcap\\mathcal{N}_{v}}\\frac{1}{\\hat{d}_{k}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $C=\\sigma_{n o d e}^{2}\\sigma_{w e i g h t}^{2}F F^{\\prime}$ ", "page_idx": 26}, {"type": "text", "text": "This completes the proof for the GCN variant. A similar approach, utilizing the probabilistic orthogonality of the input vectors and weight matrix, can be employed to derive the expected value for SAGE as the MPNN. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "G.2 Proof for Theorem 3.2 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We begin by restating Theorem 3.2 and then proceed with its proof: ", "page_idx": 26}, {"type": "text", "text": "Let $G=(V,E)$ be a graph, and let the vector dimension be given by $F\\in\\mathbb{N}_{+}$ . Define the input vectors $X\\,=\\,(X_{i,j})$ , which are initialized from a random variable $\\mathbf{X}$ having a mean of 0 and a standard deviation of $\\textstyle{\\frac{1}{\\sqrt{F}}}$ . Using the message-passing as described by Equation 3, for any pair of nodes $\\{(u,v)|(u,v)\\in\\dot{V}\\times V\\}$ , the expected value and variance of their inner product are: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}(h_{u}\\cdot h_{v})=\\mathbf{CN}(u,v),\\quad\\quad\\quad\\quad}\\\\ &{}&{\\mathrm{Var}(h_{u}\\cdot h_{v})=\\displaystyle\\frac{1}{F}\\left(d_{u}d_{v}+\\mathbf{CN}(u,v)^{2}-2\\mathbf{CN}(u,v)\\right)+F\\mathrm{Var}\\big(\\mathbf{x}^{2}\\big)\\mathbf{CN}(u,v).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. We follow the proof of the theorem in [12]. Based on the message-passing defined in Equation 3: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}(h_{u}\\cdot h_{v})=\\mathbb{E}\\bigg(\\bigg(\\displaystyle\\sum_{k_{u}\\in\\mathcal{N}_{u}}X_{k_{u},:}\\bigg)\\cdot\\bigg(\\displaystyle\\sum_{k_{v}\\in\\mathcal{N}_{v}}X_{k_{v},:}\\bigg)\\bigg)}\\\\ &{\\qquad\\qquad=\\mathbb{E}\\bigg(\\displaystyle\\sum_{k_{u}\\in\\mathcal{N}_{u}}\\sum_{k_{v}\\in\\mathcal{N}_{v}}X_{k_{u},:}X_{k_{v},:}\\bigg)}\\\\ &{\\qquad=\\displaystyle\\sum_{k_{u}\\in\\mathcal{N}_{u}}\\sum_{k_{v}\\in\\mathcal{N}_{v}}\\mathbb{E}(X_{k_{u},:}X_{k_{v},:}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since the sampling of each dimension is independent of each other, we get: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}(h_{u}\\cdot h_{v})=\\sum_{k_{u}\\in\\mathcal{N}_{u}}\\sum_{k_{v}\\in\\mathcal{N}_{v}}\\sum_{i=1}^{F}\\mathbb{E}(X_{k_{u},i}X_{k_{v},i}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "When $k_{u}=k_{v}$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}(X_{k_{u},i}X_{k_{v},i})=\\mathbb{E}(\\mathbf{x}^{2})=\\frac{1}{F}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "When $k_{u}\\neq k_{v}$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}(X_{k_{u},i}X_{k_{v},i})=\\mathbb{E}(X_{k_{u},i})\\mathbb{E}(X_{k_{v},i})=0.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}(h_{u}\\cdot h_{v})=\\displaystyle\\sum_{k_{u}\\in\\mathcal{N}_{u}}\\sum_{k_{v}\\in\\mathcal{N}_{v}}\\sum_{i=1}^{F}\\mathbf{1}\\left(k_{u}=k_{v}\\right)\\frac{1}{F}}\\\\ &{=\\displaystyle\\sum_{k\\in\\mathcal{N}_{u}\\cap\\mathcal{N}_{v}}1=\\mathbf{CN}(u,v).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For the variance, we separate the equal from the non-equal pairs of $k_{u}$ and $k_{v}$ . Note that there is no covariance between the equal pairs and the non-equal pairs due to the independence: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Var}(\\boldsymbol{h}_{u}\\cdot\\boldsymbol{h}_{v})=\\mathrm{Var}\\left(\\displaystyle\\sum_{k_{u}\\in\\mathcal{N}_{u}}\\sum_{k_{v}\\in\\mathcal{N}_{v}}\\sum_{i=1}^{F}X_{k_{u},i}X_{k_{v},i}\\right)}\\\\ &{\\phantom{=}=\\displaystyle\\sum_{i=1}^{F}\\mathrm{Var}\\left(\\displaystyle\\sum_{k_{u}\\in\\mathcal{N}_{u}}\\sum_{k_{v}\\in\\mathcal{N}_{v}}X_{k_{u},i}X_{k_{v},i}\\right)}\\\\ &{\\phantom{=}=\\displaystyle\\sum_{i=1}^{F}\\left(\\mathrm{Var}\\left(\\displaystyle\\sum_{k\\in\\mathcal{N}_{u}\\cap\\mathcal{N}_{v}}\\boldsymbol{\\mathrm{x}}^{2}\\right)+\\mathrm{Var}\\left(\\displaystyle\\sum_{k_{u}\\in\\mathcal{N}_{u}}\\sum_{k_{v}\\in\\mathcal{N}_{v}\\setminus\\{k_{u}\\}}X_{k_{u},i}X_{k_{v},i}\\right)\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For the first term, we can obtain: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{Var}\\Bigg(\\sum_{k\\in\\mathcal{N}_{u}\\cap\\mathcal{N}_{v}}\\mathbf{x}^{2}\\Bigg)=\\mathrm{Var}\\big(\\mathbf{x}^{2}\\big)\\mathbf{C}\\mathbf{N}(u,v).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For the second term, we further split the variance of linear combinations to the linear combinations of variances and covariances: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{Var}\\left(\\sum_{k_{u}\\in\\mathcal{N}_{u}}\\sum_{\\boldsymbol{k_{v}}\\in\\mathcal{N}_{v}\\backslash\\{\\boldsymbol{k_{u}}\\}}X_{k_{u},i}X_{k_{v},i}\\right)=\\sum_{\\boldsymbol{k_{u}}\\in\\mathcal{N}_{u}}\\sum_{\\boldsymbol{k_{v}}\\in\\mathcal{N}_{v}\\backslash\\{\\boldsymbol{k_{u}}\\}}\\mathrm{Var}(X_{k_{u},i}X_{k_{v},i})+}\\\\ {\\underset{a\\in\\mathcal{N}_{u}\\backslash\\{\\boldsymbol{k_{u}}\\}}{\\sum}\\underset{b\\in\\mathcal{N}_{v}\\backslash\\{\\boldsymbol{k_{v}},\\boldsymbol{\\lambda}\\}}{\\sum}\\mathrm{Cov}(X_{k_{u},i}X_{k_{v},i},X_{a,i}X_{b,i}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Note that the $\\mathrm{Cov}(X_{k_{u},i}X_{k_{v},i},X_{a,i}X_{b,i})$ is $\\begin{array}{r}{\\mathrm{Var}(X_{k_{u},i}X_{k_{v},i})\\,=\\,\\frac{1}{F^{2}}}\\end{array}$ when $(k_{u},k_{v})\\,=\\,(b,a)$ , and otherwise 0. ", "page_idx": 27}, {"type": "text", "text": "Thus, we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{Var}\\left(\\sum_{k_{u}\\in\\mathcal{N}_{u}}\\sum_{k_{v}\\in\\mathcal{N}_{v}\\backslash\\{k_{u}\\}}X_{k_{u},i}X_{k_{v},i}\\right)=\\frac{1}{F^{2}}\\left(d_{u}d_{v}+\\mathbf{CN}(u,v)^{2}-2\\mathbf{CN}(u,v)\\right),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and the variance is: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{Var}(h_{u}\\cdot h_{v})=\\frac{1}{F}\\left(d_{u}d_{v}+\\mathrm{CN}(u,v)^{2}-2\\mathrm{CN}(u,v)\\right)+F\\mathrm{Var}(\\mathrm{x}^{2})\\mathrm{CN}(u,v).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "G.3 Proof for Theorem 3.3 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We begin by restating Theorem 3.3 and then proceed with its proof: ", "page_idx": 28}, {"type": "text", "text": "Under the conditions defined in Theorem 3.2, let $h_{u}^{(l)}$ denote the vector for node $u$ after the $l$ -th message-passing iteration. We have: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big(h_{u}^{(p)}\\cdot h_{v}^{(q)}\\Big)=\\sum_{k\\in V}|\\mathrm{walks}^{(p)}(k,u)||\\mathrm{walks}^{(q)}(k,v)|,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $|\\mathbf{walks}^{(l)}(u,v)|$ counts the number of length- $l$ walks between nodes $u$ and $v$ . ", "page_idx": 28}, {"type": "text", "text": "Proof. Reinterpreting the message-passing described in Equation 3, we can equivalently express it as: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathsf{m s}_{v}^{(l+1)}=\\bigcup_{u\\in\\mathcal{N}_{v}}\\mathsf{m s}_{u}^{(l)},\\pmb{h}_{v}^{(l+1)}=\\sum_{u\\in\\mathsf{m s}_{v}^{(l+1)}}\\pmb{h}_{u}^{(0)},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\mathrm{m}\\mathrm{s}_{v}^{(l)}$ refers to a multiset, a union of multisets from its neighbors. Initially, $\\mathrm{ms}_{v}^{(0)}=\\{\\{v\\}\\}$ The node vector $h_{v}^{(l)}$ is derived by summing the initial QO vectors of the multiset\u2019s elements. ", "page_idx": 28}, {"type": "text", "text": "We proceed by induction: Base Case $[l=1$ ): ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{ms}_{v}^{(1)}=\\bigcup_{u\\in\\mathcal{N}_{v}}\\operatorname*{ms}_{u}^{(0)}=\\bigcup_{u\\in\\mathcal{N}_{v}}\\left\\{\\{u\\}\\right\\}=\\left\\{\\left\\{k|\\omega\\in\\mathrm{walks}^{(1)}(k,v)\\right\\}\\right\\}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Inductive Step $(l\\ \\geq\\ 1)$ ): Let\u2019s assume that $\\mathrm{ms}_{v}^{(l)}\\,=\\,\\{\\{k|\\omega\\,\\in\\,\\mathrm{walks}^{(l)}(k,v)\\}\\}$ holds true for an arbitrary $l$ . Utilizing Equation 9 and the inductive hypothesis, we deduce: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathrm{ms}_{v}^{(l+1)}=\\bigcup_{u\\in\\mathcal{N}_{v}}\\{\\{k|\\omega\\in\\mathrm{walks}^{(l)}(k,u)\\}\\}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "If $k$ initiates the $l$ -length walks terminating at $v$ and if $v$ is adjacent to $u$ , then $k$ must similarly initiate the $l$ -length walks terminating at $u$ . This consolidates our inductive premise. ", "page_idx": 28}, {"type": "text", "text": "With the induction established: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big(h_{u}^{(p)}\\cdot h_{v}^{(q)}\\Big)=\\mathbb{E}\\left(\\sum_{k_{u}\\in\\mathrm{ms}_{u}^{(p)}}\\pmb{h}_{k_{u}}^{(0)}\\cdot\\sum_{k_{v}\\in\\mathrm{ms}_{v}^{(q)}}\\pmb{h}_{k_{v}}^{(0)}\\right)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The inherent independence among node vectors concludes the proof. ", "page_idx": 28}, {"type": "text", "text": "H Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Despite the promising capabilities of MPLP, there are distinct limitations that warrant attention: ", "page_idx": 28}, {"type": "text", "text": "1. Training cost vs. inference cost: The computational cost during training significantly outweighs that of inference. This arises from the necessity to remove shortcut edges for positive links in the training phase, causing the graph structure to change across different batches. This, in turn, mandates a repeated computation of the shortest-path neighborhood. Even though $\\mathrm{MPLP+}$ can avoid the computation of the shortest-path neighborhood for each batch, it shows suboptimal performance compared to MPLP. A potential remedy is to consider only a subset of links in the graph as positive instances and mask them, enabling a single round of preprocessing. Exploring this approach will be the focus of future work.   \n2. Estimation variance influenced by graph structure: The structure of the graph itself can magnify the variance of our estimations. Specifically, in dense graphs or those with a high concentration of hubs, the variance can become substantial, thereby compromising the accuracy of structural feature estimation.   \n3. Optimality of estimating structural features: Our research demonstrates the feasibility of using message-passing to derive structural features. However, its optimality remains undetermined. Message-passing, by nature, involves sparse matrix multiplication operations, which can pose challenges in terms of computational time and space, particularly for exceedingly large graphs. ", "page_idx": 28}, {"type": "text", "text": "I Broader Impact ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Our study is centered on creating a more efficient and expressive method for link prediction, with the goal of significantly advancing graph machine learning. The potential applications of our method are diverse and impactful, extending to recommendation systems, social network analysis, and biological interaction networks, among others. While we have not identified any inherent biases in our method, we acknowledge the necessity of rigorous bias assessments, particularly when integrating our method into industrial-scale applications. ", "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The claims made in abstract and introduction clearly reflect the contribution. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The limitation is discussed in Appendix H. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The proof is rigorously shown in the Appendix. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: The experimental details are extensively discussed in the main body and appendix. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The code and data are publicly available. The code is open source. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The experimental settings are discussed in the main body and the appendix. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: The error bars are clearly reported in the experimental results. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The compute resource is discussed in Appendix D.4. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Yes, the research conforms with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: Broader impact is discussed in Appendix I. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 33}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 34}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This study is not feasible for safeguards. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: It is all properly cited. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: No new asset is introduced. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: No crowdsourcing experiments or research with human subjects are used in this study. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: No human participants in this study. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]