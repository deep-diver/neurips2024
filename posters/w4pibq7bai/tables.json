[{"figure_path": "W4pIBQ7bAI/tables/tables_5_1.jpg", "caption": "Table 2: Accuracy at varying information availabilities. BASIC gives LLM the option to ask questions: with the same starting information, BASIC performance degrades from non-interactive Initial. Bold entries surpass non-interactive Initial, but there is still a gap between Full (complete information upper bound) and interactive BEST.", "description": "This table presents the accuracy of different LLMs under various information availability conditions (Full, Initial, None) and interactive settings (BASIC, BEST). The results demonstrate that even with the same initial information, interactive settings lead to a decrease in performance compared to the non-interactive setup with initial information.  However, improved interactive systems, particularly the \"BEST\" ones, achieve higher accuracy than the non-interactive model with only initial information, though still falling short of the accuracy achieved when full information is provided.", "section": "4 Results"}, {"figure_path": "W4pIBQ7bAI/tables/tables_5_2.jpg", "caption": "Table 1: Patient system reliability.", "description": "This table presents the results of evaluating the reliability of three different Patient System variants: Direct, Instruct, and Fact-Select.  The evaluation metrics used are Factuality and Relevance.  Factuality measures the percentage of statements in the Patient System's response that are supported by the information in the patient record. Relevance measures the average embedding semantic similarity between the generated response and the ground truth statement. The Fact-Select system significantly outperforms the other two systems in terms of factuality, while all three show relatively similar performance on relevance.", "section": "3.1 Patient System Reliability Evaluation"}, {"figure_path": "W4pIBQ7bAI/tables/tables_21_1.jpg", "caption": "Table 3: Win rates from manual evaluation of Patient variants.", "description": "This table presents the results of a manual evaluation comparing three different variants of the Patient System: Direct, Instruct, and Fact-Select.  The \"Win Rate\" column indicates the percentage of times each variant's response was judged as higher quality than the other variant's response in a pairwise comparison.  Fact-Select demonstrates a significantly higher win rate, suggesting it produces more factually accurate and complete responses compared to the Direct and Instruct variants.", "section": "A Patient System"}, {"figure_path": "W4pIBQ7bAI/tables/tables_23_1.jpg", "caption": "Table 2: Accuracy at varying information availabilities. BASIC gives LLM the option to ask questions: with the same starting information, BASIC performance degrades from non-interactive Initial. Bold entries surpass non-interactive Initial, but there is still a gap between Full (complete information upper bound) and interactive BEST.", "description": "This table presents the accuracy of different LLMs under various information availability levels.  It compares non-interactive settings (Full, Initial, None) with the interactive BASIC setting, where the LLM has the option to ask questions.  The results show that providing less information degrades accuracy.  While the BASIC interactive setting sometimes outperforms the non-interactive Initial setting, it still lags behind the non-interactive Full setting (where complete information is available).", "section": "4 Results"}, {"figure_path": "W4pIBQ7bAI/tables/tables_24_1.jpg", "caption": "Table 2: Accuracy at varying information availabilities. BASIC gives LLM the option to ask questions: with the same starting information, BASIC performance degrades from non-interactive Initial. Bold entries surpass non-interactive Initial, but there is still a gap between Full (complete information upper bound) and interactive BEST.", "description": "This table compares the accuracy of different LLMs (Llama-3-8b, Llama-3-70b, GPT-3.5, GPT-4) under three different information availability levels (Full, Initial, None) and two interaction settings (Non-interactive, Interactive).  The \"Full\" setup represents standard QA tasks where complete information is provided, while the \"Initial\" and \"None\" setups reflect realistic scenarios with incomplete or missing information. The \"BASIC\" interactive setting allows LLMs to ask follow-up questions, highlighting their information-seeking ability. The \"BEST\" interactive setting shows the improved accuracy after using additional techniques to address the challenges.  The table demonstrates that LLM performance degrades significantly when starting with limited information, even with the ability to ask questions. While advanced techniques can close the performance gap partially, there remains a considerable difference between ideal (full information) and realistic (limited information) scenarios.", "section": "4 Results"}, {"figure_path": "W4pIBQ7bAI/tables/tables_25_1.jpg", "caption": "Table 2: Accuracy at varying information availabilities. BASIC gives LLM the option to ask questions: with the same starting information, BASIC performance degrades from non-interactive Initial. Bold entries surpass non-interactive Initial, but there is still a gap between Full (complete information upper bound) and interactive BEST.", "description": "This table presents the accuracy of different LLMs (Llama-3-8b, Llama-3-70b, GPT-3.5, GPT-4) under various information availability levels (Full, Initial, None) for two medical question answering tasks (iMEDQA, iCRAFT-MD).  It compares the performance of non-interactive models with an interactive model (BASIC) that allows the LLM to ask questions. The results demonstrate that while LLMs perform well with complete information, their accuracy significantly decreases in realistic scenarios with limited initial information and the ability to ask follow-up questions does not naturally improve their performance. The best performing interactive model (BEST) still lags behind the performance with complete information, showcasing a gap between idealized and real-world scenarios.", "section": "4 Results"}, {"figure_path": "W4pIBQ7bAI/tables/tables_25_2.jpg", "caption": "Table 2: Accuracy at varying information availabilities. BASIC gives LLM the option to ask questions: with the same starting information, BASIC performance degrades from non-interactive Initial. Bold entries surpass non-interactive Initial, but there is still a gap between Full (complete information upper bound) and interactive BEST.", "description": "This table presents the accuracy of different LLMs across various information availability levels (Full, Initial, None) in both non-interactive and interactive settings.  The non-interactive setting serves as a baseline, while the interactive setting (BASIC) allows the LLM to ask questions. The results show that while LLMs perform well with complete information, their accuracy decreases significantly when information is incomplete, even when they have the ability to ask clarifying questions.  The \"BEST\" column demonstrates improved performance, representing an optimized interactive system.", "section": "4 Results"}, {"figure_path": "W4pIBQ7bAI/tables/tables_26_1.jpg", "caption": "Table 2: Accuracy at varying information availabilities. BASIC gives LLM the option to ask questions: with the same starting information, BASIC performance degrades from non-interactive Initial. Bold entries surpass non-interactive Initial, but there is still a gap between Full (complete information upper bound) and interactive BEST.", "description": "This table presents the accuracy of different LLMs (Llama-3-8b, Llama-3-70b, GPT-3.5, GPT-4) on two tasks (iMEDQA and iCRAFT-MD) under different information availability scenarios.  The \"Full\" condition provides complete information, \"Initial\" provides limited initial information, and \"None\" provides no initial information.  The \"BASIC\" setting allows the LLMs to ask questions interactively, while the \"BEST\" setting represents the best performance achieved with improved question-asking strategies.  The table highlights the performance gap between models given complete information versus those only given partial information, even when allowed to ask clarifying questions.", "section": "4 Results"}, {"figure_path": "W4pIBQ7bAI/tables/tables_27_1.jpg", "caption": "Table 2: Accuracy at varying information availabilities. BASIC gives LLM the option to ask questions: with the same starting information, BASIC performance degrades from non-interactive Initial. Bold entries surpass non-interactive Initial, but there is still a gap between Full (complete information upper bound) and interactive BEST.", "description": "This table presents the accuracy of different LLMs (Llama-3, GPT-3.5, GPT-4) under various information availability settings (Full, Initial, None).  The \"Full\" setup provides complete information, the \"Initial\" setup provides partial information (only age, gender, and chief complaint), and the \"None\" setup provides no information.  The \"BASIC\" setting allows the LLMs to ask questions interactively. The table shows how the accuracy changes across these conditions, highlighting the impact of interactive questioning. Bold values indicate where interactive performance exceeds the performance when only initial information was provided.", "section": "4 Results"}]