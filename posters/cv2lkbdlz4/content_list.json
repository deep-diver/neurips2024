[{"type": "text", "text": "On Statistical Rates and Provably Efficient Criteria of Latent Diffusion Transformers (DiTs) ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jerry Yao-Chieh Hu\\*++ Weimin Wu\\*++ Zhuoru Lib Sophia PitZhao Songs Han Liutth ", "page_idx": 0}, {"type": "text", "text": "+Center for Foundation Models and Generative AI, +Department of Computer Science, Department of Statistics and Data Science, Northwestern University, Evanston, IL 60208, USA School of Mathematical Science, Fudan University, Yangpu, Shanghai 200433, China \\$Simons Institute for the Theory of Computing, UC Berkeley, Berkeley, CA 94720, USA ", "page_idx": 0}, {"type": "text", "text": "{jhu, wwm, sophiapi2026.1}@u.northwestern.edu; 21300180107@m.fudan.edu.cn; magic.linuxkde@gmail.com; hanliu@northwestern.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We investigate the statistical and computational limits of latent Diffusion Transformers (DiTs) under the low-dimensional linear latent space assumption. Statistically, we study the universal approximation and sample complexity of the DiTs score function, as well as the distribution recovery property of the initial data. Specifically, under mild data assumptions, we derive an approximation error bound for the score network of latent DiTs, which is sub-linear in the latent space dimension. Additionally, we derive the corresponding sample complexity bound and show that the data distribution generated from the estimated score function converges toward a proximate area of the original one. Computationally, we characterize the hardness of both forward inference and backward computation of latent DiTs, assuming the Strong Exponential Time Hypothesis (SETH). For forward inference, we identify efficient criteria for all possible latent DiTs inference algorithms and showcase our theory by pushing the efficiency toward almost-linear time inference. For backward computation, we leverage the low-rank structure within the gradient computation of DiTs training for possible algorithmic speedup. Specifically, we show that such speedup achieves almost-linear time latent DiTs training by casting the DiTs gradient as a series of chained low-rank approximations with bounded error. Under the low-dimensional assumption, we show that the statistical rates and the computational efficiency are all dominated by the dimension of the subspace, suggesting that latent DiTs have the potential to bypass the challenges associated with the high dimensionality of initial data. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We investigate the statistical and computational limits of latent diffusion transformers (DiTs), assuming the data is supported on an unknown low-dimensional linear subspace. This analysis is not only practical but also timely. On one hand, DiTs have demonstrated revolutionary success in generative AI and digital creation by using Transformers as score networks [Esser et al., 2024, Ma et al., 2024, Chen et al., 2024a, Mo et al., 2023, Peebles and Xie, 2023]. On the other hand, they require significant computational resources [Liu et al., 2024], making them challenging to train outside of specialized industrial labs. Therefore, it is natural to ask whether it is possible to make them lighter and faster without sacrificing performance. Answering these questions requires a fundamental understanding of the DiT architecture. This work provides a timely theoretical analysis of the fundamental limits of DiT architecture, aided by the analytical feasibility provided by the low-dimensional data assumption. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Empirically, Latent Diffusion is a go-to design for effectiveness and computational efficiency [Rombach et al., 2022, Liu et al., 2021, Pope et al., 2021, Su and Wu, 2018]. Theoretically, it is capable of hosting the assumption of low-dimensional data structure (see Assumption 2.1 for formal definition) for detailed analytical characterization [Chen et al., 2023, Bortoli, 2022]. In essence, diffusion models with low-dimensional data structures manifest a natural lower-dimensional diffusion process through an encoder/decoder within a robust and informative latent representation feature space [Rombach et al., 2022, Pope et al., 2021]. Such lower-dimensional diffusion improves computational efficiency by reducing data complexity without sacrificing essential information [Liu et al., 2021]. With this assumption, Chen et al. [2023] decompose the score function of U-Net based diffusion models into on-support and orthogonal components. This decomposition allows for the characterization of the distinct behaviors of the two components: the on-support component facilitates latent distribution learning, while the orthogonal component facilitates subspace recovery. ", "page_idx": 1}, {"type": "text", "text": "In our work, we utilize low-dimensional data structure assumption to explore statistical and computational limits of latent DiTs. Our analysis includes the characterizations of statistical rates and provably efficient criteria. Statistically, we pose two questions and provide a theory to characterize the statistical rates of latent DiT under the assumption of a low-dimensional data: ", "page_idx": 1}, {"type": "text", "text": "Question 1. What is the approximation limit of using transformers to approximate the DiT score function, particularly in the low-dimensional data subspace? ", "page_idx": 1}, {"type": "text", "text": "Question 2. How accurate is the estimation limit for such a score estimator in practical training scenarios? With the score estimator, how well can diffusion transformers recover the data distribution? ", "page_idx": 1}, {"type": "text", "text": "Computationally, the primary challenge of DiT lies in the transformer blocks\u2019 quadratic complexity. This computational burden applies to both inference and training, even with latent diffusion. Thus, it is essential to design algorithms and methods to circumvent this $\\Omega(L^{2})$ where $L$ is the latent DiT sequence length. However, there are no formal results to support and characterize such algorithms. To address this gap, we pose the follwing questions and provide a fundamental theory to fully characterize the complexity of latent DiT under the low-dimensional linear subspace data assumption: ", "page_idx": 1}, {"type": "text", "text": "Question 3. Is it possible to improve the $\\Omega(L^{2})$ time complexity with a bounded approximation error for both forward and backward passes? What is the computational limit for such an improvement? ", "page_idx": 1}, {"type": "text", "text": "Contributions.  We study the fundamental limits of latent DiT. Our contributions are threefold: ", "page_idx": 1}, {"type": "text", "text": "\u00b7 Score Approximation. We address Question 1 by characterizing the approximation limit of matching the DiT score function with a transformer-based score estimator. Specifically, under mild data assumptions, we derive an approximation error bound for the score network, sub-linear in the latent space dimension (Theorem 3.1). These results not only explain the expressiveness of latent DiT (under mild assumptions) but also provide guidance for the structural configuration of the score network for practical implementations (Theorem 3.1).   \n\u00b7 Score and Distribution Estimation. We address Question 2 by exploring the limitations of score and distribution estimations of latent DiTs in practical training scenarios. Specifically, we provide a sample complexity bound for score estimation (Theorem 3.2), using norm-based covering number bound of transformer architecture. Additionally, we show that the learned score estimator is able to recover the initial data distribution (Corollary 3.2.1).   \n\u00b7 Provably Efficient Criteria and Existence of Almost Linear Time Algorithms. We address Question 3 by providing provably efficient criteria for latent DiTs in both forward inference and backward computation/training. For forward inference, we characterize all possible efficient DiT algorithms using a norm-based efficiency threshold for both conditional and unconditional generation (Proposition 4.1). Efficient algorithms, including almost-linear time algorithms (Proposition 4.2), are possible only below this threshold. For backward computation, we prove the existence of almost-linear time DiT training algorithms (Theorem 4.1) by utilizing the inherent low-rank structure in DiT gradients through a chained low-rank approximation. ", "page_idx": 1}, {"type": "text", "text": "Interestingly, both our statistical and computational results are dominated by the subspace dimension under the low-dimensional assumption, suggesting that latent DiT can potentially bypass the challenges associated with the high dimensionality of initial data. ", "page_idx": 1}, {"type": "text", "text": "Organization. Section 2 includes background on score decomposition and Transformer-based score networks. Section 3 includes DiTs\u2032 statistical rates. Section 4 includes DiTs\u2032 provably efficient criteria. Section 5 includes concluding remarks. We defer discussions of related works to Appendix C. ", "page_idx": 2}, {"type": "text", "text": "Notations._ We use lower case letters to denote vectors, e.g., $z\\ \\ \\in\\ \\mathbb{R}^{D}$ $\\|z\\|_{2}$ and $\\|z\\|_{\\infty}$ denote its Euclidean norm and Infinite norm respectively. We use upper case letters to denote matrix, e.g.. $Z~\\in~\\mathbb{R}^{d\\times L}$ $\\|Z\\|_{2}$ \uff0c $\\|Z\\|_{\\mathrm{op}}$ , and $\\|Z\\|_{F}$ denote the 2-norm, operator norm and Frobenius norm respectively. $\\|Z\\|_{p,q}$ denotes the $p,q$ -norm where the $p$ -norm is over columns and $q$ -norm is over rows. Given a function $f$ , let $\\begin{array}{r}{\\|f(x)\\|_{L^{2}}:=(\\int\\|f(x)\\|_{2}^{2}\\mathrm{d}x)^{1/2}}\\end{array}$ , and $\\|f(\\cdot)\\|_{L i p}=\\operatorname*{sup}_{x\\neq y}(\\|f(x)-f(y)\\|_{2}/\\|x-y\\|_{2})$ With a distribution $P$ ,we denote $\\left\\|f\\right\\|_{L^{2}(P)}=$ $(\\textstyle\\int_{P}\\|f(x)\\|_{2}^{2}\\mathrm{d}x)^{1/2}$ as the $L^{2}(P)$ norm. Let $f_{\\sharp}P$ be a pushforward measure, i.e., for any measurable $\\Omega$ $,(f_{\\sharp}P)(\\Omega)=P(f^{-1}(\\Omega))$ . We use $\\psi$ for (conditional) Gaussian density functions. ", "page_idx": 2}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section reviews the ideas we built on, including an overview of diffusion models (Section 2.1), the score decomposition under the linear latent space assumption (Section 2.2), and the transformer backbone in DiT (Section 2.3). ", "page_idx": 2}, {"type": "text", "text": "2.1  Score-Matching Denoising Diffusion Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We briefly review forward process, backward process and score matching in diffusion models ", "page_idx": 2}, {"type": "text", "text": "Forward and Backward Process.  In the forward process, Diffusion models gradually add noise to the original data $\\boldsymbol{x}_{0}\\,\\in\\,\\mathbb{R}^{D}$ , and $x_{0}\\sim P_{0}$ . Let $x_{t}$ denote the noisy data at time stamp $t$ with marginal distribution and destiny as $P_{t}$ and $p_{t}$ . The conditional distribution $P(x_{t}|x_{0})$ follows $N(\\beta(t)x_{0},\\sigma(t)I_{D})$ ,where $\\beta(t)\\,=\\,\\exp(-\\textstyle\\int_{0}^{t}w(s)\\mathrm{d}s/2)$ \uff0c $\\sigma(t)\\,=\\,1\\,-\\,\\beta^{2}(t)$ , and $w(t)\\,>\\,0$ is a nondecreasing weighting function. In practice, the forward process terminates at a large enough $T$ such that $P_{T}$ is close to $\\bar{N}(0,I_{D})$ . In the backward process, we obtain $y_{t}$ by reversing the forward process. The generation of $y_{t}$ depends on the score function $\\nabla\\log{p_{t}(\\cdot)}$ . However, this is unknown in practice, we use a score estimator $s_{W}(\\cdot,t)$ to replace $\\nabla\\log{p_{t}(\\cdot)}$ , where $s_{W}(\\cdot,t)$ is usually a neural network with parameters $W$ . See Appendix D.1 for the details. ", "page_idx": 2}, {"type": "text", "text": "Score Matching. To estimate the score function, we use the following loss ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{W}\\int_{T_{0}}^{T}\\gamma(t)\\mathbb{E}_{x_{t}\\sim P_{t}}\\left[\\Vert s_{W}(x_{t},t)-\\nabla\\log p_{t}(x_{t})\\Vert_{2}^{2}\\right]\\mathrm{d}t,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\gamma(t)$ is the weight function, and $T_{0}$ is a small value to stabilize training and prevent score function from blowing up [Vahdat et al., 2021]. However, it is hard to compute $\\nabla\\operatorname{\\bar{log}}p_{t}(\\cdot)$ With available data samples. Therefore, we minimize the equivalent denoising score matching objective ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{W}\\int_{T_{0}}^{T}\\gamma(t)\\mathbb{E}_{x_{0}\\sim P_{0}}\\left[\\mathbb{E}_{x_{t}|x_{0}}\\left[\\|s_{W}(x_{t},t)-\\nabla_{x_{t}}\\log\\psi_{t}(x_{t}\\mid x_{0})\\|_{2}^{2}\\right]\\right]\\mathrm{d}t,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\psi_{t}(x_{t}|x_{0})$ is the transition kernel, then $\\nabla_{x_{t}}\\log\\psi_{t}(x_{t}|x_{0})=\\left(\\beta(t)x_{0}-x_{t}\\right)/\\sigma(t).$ ", "page_idx": 2}, {"type": "text", "text": "To train the parameters $W$ in the score estimator $s_{W}(\\cdot,t)$ , we use the empirical version of (2.1). We select $n$ i.i.d. data samples $\\{x_{0,i}\\}_{i=1}^{n}\\sim P_{0}$ , and sample time $t_{i}$ $1\\leq i\\leq n\\rangle$ uniformly from interval $[T_{0},T]$ . Given $x_{0,i}$ , we sample $x_{t_{i}}$ from $N(\\beta(t_{i})x_{0,i},\\sigma(t_{i})I_{D})$ . The empirical loss is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{L}}(W)=\\frac{1}{n}\\sum_{i=1}^{n}\\|s_{W}(x_{t_{i}},t_{i})-x_{0,i}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For convenience of notation, we denote population loss $\\mathcal{L}(W)=\\mathbb{E}_{P_{0}}[\\widehat{\\mathcal{L}}(W)]$ ", "page_idx": 2}, {"type": "text", "text": "2.2  Score Decomposition in Linear Latent Space ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this part, we review the score decomposition in [Chen et al., 2023]. We consider that the $D$ dimensional input data $x$ supported on a $d_{0}$ -dimensional subspace, where $d_{0}\\leq D$ ", "page_idx": 2}, {"type": "text", "text": "Assumption 2.1 (Low-Dimensional Linear Latent Space). Let $x$ denote the initial data at $t=0$ $x$ has a latent representation via $x=B h$ where $B\\in\\dot{\\mathbb{R}^{D}}^{\\times\\dot{d}_{0}}$ is an unknown matrix with orthonormal columns. The latent variable $h\\in\\mathbb{R}^{d_{0}}$ follows the distribution $P_{h}$ with a density function $p_{h}$ ", "page_idx": 2}, {"type": "text", "text": "Remark 2.1. By \u201clinear latent space,\u2019 we mean that each entry of a given latent vector is a linear combination of the corresponding input, i.e., $x=B h$ . This is also known as the \u201clow-dimensional data\u201d assumption in literature [Chen et al., 2023]. ", "page_idx": 3}, {"type": "text", "text": "Let $\\textstyle{\\overline{{x}}}$ and $\\overline{{h}}$ denote the perturbed data and its associated latent variable at $t>0$ , respectively. Based on the low-dimensional data structure assumption, we have the following score decomposition theory: on-support score $s_{+}(B^{\\top}\\bar{x},t)$ and orthogonal score $s_{-}(\\bar{x},t)$ ", "page_idx": 3}, {"type": "text", "text": "Lemma 2.1 (Score Decomposition, Lemma 1 of [Chen et al., 2023]). Let data $x\\,=\\,B h$ follow Assumption 2.1. The decomposition of score function $\\nabla\\log{p_{t}(\\bar{x})}$ is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla\\log p_{t}(\\overline{{x}})=\\underbrace{B\\nabla\\log p_{t}^{h}(\\overline{{h}})}_{s_{+}(\\overline{{h}},t)}\\underbrace{-\\left(I_{D}-B B^{\\top}\\right)\\bar{x}/\\sigma(t)}_{s_{-}(\\overline{{x}},t)},\\quad\\overline{{h}}=B^{\\top}\\overline{{x}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{p_{t}^{h}(\\overline{{h}}):=\\int\\psi_{t}(\\overline{{h}}|h)p_{h}(h)\\mathrm{d}h,\\psi_{t}(\\cdot|h)}\\end{array}$ is the Gaussian density function of $N(\\beta(t)h,\\sigma(t)I_{d_{0}})$ $\\beta(t)=e^{-t/2}$ and $\\sigma(t)=1-e^{-t}$ . We restate the proof in Appendix D.2 for completeness. ", "page_idx": 3}, {"type": "text", "text": "Additionally, our theoretical analysis is based on two following assumptions as in [Chen et al., 2023]. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.2 (Tail Behavior of $P_{h}$ ). The density function ${p_{h}>0}$ is twice continuously differentiable. Moreover, there exist positive constants $A_{0},A_{1},A_{2}$ such that when $\\|h\\|_{2}\\geq A_{0}$ , the density function $p_{h}(h)\\leq(2\\pi)^{-d_{0}/2}A_{1}\\mathrm{exp}(-A_{2}\\|h\\|_{2}^{2}/2)$ ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.3 ( $\\mathbb{L}_{s_{+}}$ -Lipschitz of $s_{+}(\\overline{{h}},t))$ . The on-support score function $s_{+}(\\bar{h},t)$ is $L_{s_{+}}$ Lipschitz in $\\overline{{h}}\\in\\mathbb{R}^{d_{0}}$ for any $t\\in[0,T]$ ", "page_idx": 3}, {"type": "text", "text": "2.3 Score Network and Transformers ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this part, we introduce the score network architecture and Transformers. Transformers are the backbone of the score network in DiT. By Assumption 2.1, $\\bar{h}=B^{\\top}\\bar{x}\\in\\mathbb{R}^{d_{0}}$ With $d_{0}<D$ ", "page_idx": 3}, {"type": "text", "text": "(Latent) Score Network. Following [Chen et al., 2023], we rearrange (2.3) into ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla\\log p_{t}(\\bar{x})=B(\\underbrace{\\sigma(t)\\nabla\\log p_{t}^{h}(B^{\\top}\\bar{x})+B^{\\top}\\bar{x}}_{:=q(B^{\\top}\\bar{x},t):\\;\\mathbb{R}^{d_{0}}\\times[T_{0},T]\\;\\rightarrow\\;\\mathbb{R}^{d_{0}}})/\\sigma(t)-\\bar{x}/\\sigma(t).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Weuse $W_{B}\\in\\mathbb{R}^{D\\times d_{0}}$ to approximate $\\boldsymbol{B}\\in\\mathbb{R}^{D\\times d_{0}}$ , and a neural network $f(W_{B}^{\\top}\\bar{x},t)$ to approximate $q(B^{\\top}\\bar{x},t)$ . We adopt the following score network class for diffusion in latent space (i.e., in $\\overline{{h}}\\in\\mathbb{R}^{d_{0}}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{S}=\\left\\lbrace s_{W}(\\bar{x},t)=W_{B}f(W_{B}^{T}\\bar{x},t)/\\sigma(t)-\\bar{x}/\\sigma(t),\\;W=\\{W_{B},f\\}\\right\\rbrace,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the columns in $W_{B}$ are orthogonal, $f:\\mathbb{R}^{d_{0}}\\times[T_{0},T]\\rightarrow\\mathbb{R}^{d_{0}}$ is a neural network. In this work, we focus on the diffusion transformers (DiTs), i.e., using Transformer for $f$ [Peebles and Xie, 2023]. ", "page_idx": 3}, {"type": "text", "text": "Transformers. A Transformer block consists of a self-attention layer and a feed-forward layer, with both layers having skip connection. We use $\\tau^{r,m,l}:\\mathbb{R}^{d\\times L}\\rightarrow\\mathbb{R}^{d\\times L}$ to denote a Transformer block. Here $r$ and $m$ are the number of heads and head size in self-attention layer, and $l$ is the hidden dimension in feed-forward layer. Let $X\\in\\mathbb{R}^{d\\times L}$ be the model input, then we have the model output ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Attn}(X)=X+\\sum_{i=1}^{r}W_{O}^{i}W_{V}^{i}X\\cdot\\mathrm{Softmax}\\left(\\left(W_{K}^{i}X\\right)^{\\top}W_{Q}^{i}X\\right),}\\\\ &{\\mathrm{FF\\circAttn}(X)=\\mathrm{Attn}(X)+W_{2}\\cdot\\mathrm{ReLU}(W_{1}\\cdot\\mathrm{Attn}(X)+b_{1}\\mathbb{I}_{L}^{\\top})+b_{2}\\mathbb{I}_{L}^{\\top},}\\\\ &{_{\\ell},W_{Q}^{i},W_{V}^{i}\\in\\mathbb{R}^{m\\times d},W_{O}^{i}\\in\\mathbb{R}^{d\\times m},W_{1}\\in\\mathbb{R}^{l\\times d},W_{2}\\in\\mathbb{R}^{d\\times l},b_{1}\\in\\mathbb{R}^{l},b_{2}\\in\\mathbb{R}^{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In our work, we use Transformer networks with positional encoding $E\\,\\in\\,\\mathbb{R}^{d\\times L}$ .We define the Transformer networks as the composition of Transformer blocks ", "page_idx": 3}, {"type": "equation", "text": "$\\mathcal{T}_{P}^{r,m,l}=\\{f_{\\mathcal{T}}:\\mathbb{R}^{d\\times L}\\rightarrow\\mathbb{R}^{d\\times L}\\ |\\ f_{\\mathcal{T}}$ ", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For example, the following is a Transformer network consisting $K$ blocks and positional encoding ", "page_idx": 3}, {"type": "equation", "text": "$$\nf_{\\mathcal T}(X)=\\mathrm{FF}^{(K)}\\circ\\mathrm{Attn}^{(K)}\\circ\\cdots\\mathrm{FF}^{(1)}\\circ\\mathrm{Attn}^{(1)}(X+E).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3  Statistical Rates of Latent DiTs with Subspace Data Assumption ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we analyze the statistical rates of latent DiTs. Section 3.1 introduces the class of latent DiT score networks. In Section 3.2, we prove the approximation limit of matching the DiT ", "page_idx": 3}, {"type": "image", "img_path": "cV2LKBdlz4/tmp/e476d5b89e0f75dd234eb44d537b191a7b40620f18ed1392736c51455ec1be44.jpg", "img_caption": ["Figure 1: Overview of DiT Score Network Architecture $s_{W}(\\cdot,t)$ $W_{B}^{T}$ denotes the linear layer from the input data space to the linear latent space. $f(\\cdot)=R^{-1}\\circ f_{T}\\circ R(\\cdot)$ denotes the transformer network $f_{T}(\\cdot)$ with reshaping layer $R(\\cdot)$ where $f_{T}(\\cdot)\\in T_{p}^{r,m,l}$ $W_{B}$ denotes the linear layer from the linear latent space to the input data space. $\\sigma(t)$ denote the variance of the conditional distribution $P(x_{t}\\mid x_{0})$ "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "score function with the score network class, and characterize the structural configuration of the score network when a specified approximation error is required. Following this, in Section 3.3, utilizing the characterized structural configuration, we prove the score and distribution estimation for latent DiTs. ", "page_idx": 4}, {"type": "text", "text": "3.1DiT Score Network Class ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Here, we provide the details about DiT score network class used in our analysis. In (2.5), $f$ is a network with Transformer as the backbone, and $(\\overline{{h}},t)\\,\\in\\,\\mathbb{R}^{d_{0}}\\,\\times\\,[T_{0},T]$ denotes the input data. Following [Peebles and Xie, 2023], DiT uses time point $t$ to calculate the scale and shift value in the Transformer backbone, and it transforms an input picture into a sequential version. To achieve the transformation, we introduce a reshape layer. ", "page_idx": 4}, {"type": "text", "text": "Definition 3.1 (DiT Reshape Layer $R(\\cdot))$ . Let $R(\\cdot):\\mathbb{R}^{d_{0}}\\rightarrow\\mathbb{R}^{d\\times L}$ be a reshape layer that transforms the $d_{0}$ -dimensional input into a $d\\times L$ matrix. Specifically, for any $d_{0}=i\\times i$ image input, $R(\\cdot)$ converts it into a sequence representation with feature dimension $d:=p^{2}$ (where $p\\ \\geq\\ 2$ ) and sequence length $L:=\\left(i/p\\right)^{2}$ . Besides, we define the corresponding reverse reshape (fatten) layer $R^{-1}(\\cdot):\\mathbb{R}^{d\\times L}\\rightarrow\\mathbb{R}^{d_{0}}$ as the inverse of $R(\\cdot)$ By $d_{0}=d L,R,R^{-1}$ are associative w.r.t. their input. ", "page_idx": 4}, {"type": "text", "text": "To simplify the self-attention block in (2.6), lt $W_{O V}^{i}=W_{O}^{i}W_{V}^{i}$ and $W_{K Q}^{i}=(W_{K}^{i})^{\\mathsf{T}}W_{Q}^{i}$ ", "page_idx": 4}, {"type": "text", "text": "Demmuou J.z (llansiUimeI IvetwUin Ciass / p' )\u3002 We uce ut HlansiUimeI nttwoiN Ciass as ", "page_idx": 4}, {"type": "text", "text": "$\\mathcal{T}_{p}^{r,m,l}(K,C_{T},C_{O V}^{2,\\infty},C_{O V},C_{K Q}^{2,\\infty},C_{K Q},C_{F}^{2,\\infty},C_{F},C_{E},L_{T})$ stisfying the constraints \u00b7 Model architecture with $K$ blocks: $f_{\\mathcal T}(X)=\\operatorname{FF}^{(K)}\\circ\\operatorname{Attn}^{(K)}\\circ\\dots\\operatorname{FF}^{(1)}\\circ\\operatorname{Attn}^{(1)}(X);$ \u00b7 Model output bound: $\\operatorname*{sup}_{X}\\|f_{\\mathcal{T}}(X)\\|_{2}\\leq C_{\\mathcal{T}}$ \u00b7 Parameter bound in $\\mathrm{{Attn}^{(i)}}$ $:\\left\\|(W_{O V}^{i})^{\\top}\\right\\|_{2,\\infty}\\,\\leq\\,C_{O V}^{2,\\infty},\\,\\left\\|(W_{O V}^{i})^{\\top}\\right\\|_{2}\\,\\leq\\,C_{O V},\\,\\left\\|W_{K Q}^{i}\\right\\|_{2,\\infty}\\,\\leq$ $\\begin{array}{r}{C_{K Q}^{2,\\infty},\\left\\|\\boldsymbol{W}_{K Q}^{i}\\right\\|_{2}\\leq C_{K Q},\\left\\|\\boldsymbol{E}^{\\top}\\right\\|_{2,\\infty}\\leq C_{E},\\forall i\\in[K]}\\end{array}$ \u00b7 Parameter bound in FF(i): $\\left\\|W_{j}^{i}\\right\\|_{2,\\infty}\\leq C_{F}^{2,\\infty}$ ,Will \u2264Cr,vj\u2208 [2],i e [K]; \u00b7 Lipschitz of $f_{\\mathcal{T}}\\colon\\|f_{\\mathcal{T}}(X_{1})-f_{\\mathcal{T}}(X_{2})\\|_{F}\\le L_{\\mathcal{T}}\\|X_{1}-X_{2}\\|_{F},\\forall X_{1},X_{2}\\in\\mathbb{R}^{d\\times L}.$ ", "page_idx": 4}, {"type": "text", "text": "Definition 33 (DiT Score Network Class $S_{T_{p}^{r,m,l}}$ (Figure 1). We denote $S_{T_{p}^{r,m,l}}$ as the DiT score network class in (2.5), replacing $f$ with $R^{-1}\\circ f_{\\mathcal{T}}\\circ R$ , and $f_{T}$ is from the Transformer class $T_{p}^{r,m,l}$ ", "page_idx": 4}, {"type": "text", "text": "3.2 Score Approximation of DiT ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Hewxxfar $S_{T_{p}^{r,m,l}}$ under linear latent space assumption. Recall that $P_{t}$ is the distribution of $x_{t}$ $\\sigma(t)$ is the variance of $P(x_{t}|\\boldsymbol{x}_{0})$ \uff0c $d_{0}$ is the dimension of latent space, $L$ is the sequence length of transformer input, $T$ is the stopping time in forward process, $T_{0}$ is the early stopping time in backward process, and $L_{s_{+}}$ is the Lipschitz coefficient of on-support score function. Then we have the following Theorem 3.1. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1 (Score Approximation of DiT). For any approximation error $\\epsilon\\,>\\,0$ and any data distribution $P_{0}$ under Assumptions 2.1 to 2.3, there exists a DiT score network $s_{\\widehat{W}}$ from $S_{T_{p}^{2,1,4}}$ (defined in Definition 3.2), where $\\widehat{\\boldsymbol{W}}=\\{\\widehat{\\boldsymbol{W}}_{B},\\widehat{\\boldsymbol{f}}_{T}\\}$ , such that for any $t\\in[T_{0},T]$ , we have: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\|s_{\\widehat{W}}(\\cdot,t)-\\nabla\\log p_{t}(\\cdot)\\right\\|_{L^{2}(P_{t})}\\leq\\epsilon\\cdot\\sqrt{d_{0}}/\\sigma(t),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\sigma(t)=1-e^{-t}$ , and the upper bound of hyperparameters in $S_{T_{p}^{2,1,4}}$ are ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K=\\mathcal{O}(\\epsilon^{-2L}),\\;C_{T}=\\mathcal{O}\\left(d_{0}L_{s_{+}}\\sqrt{d_{0}\\log(d_{0}/T_{0})+\\log(1/\\epsilon)}\\right),}\\\\ &{C_{O V}^{2,\\infty}=(1/\\epsilon)^{\\mathcal{O}(1)},\\;C_{O V}=(1/\\epsilon)^{\\mathcal{O}(1)},\\;C_{K Q}^{2,\\infty}=(1/\\epsilon)^{\\mathcal{O}(1)},\\;C_{K Q}=(1/\\epsilon)^{\\mathcal{O}(1)},}\\\\ &{C_{E}=\\mathcal{O}(L^{3/2}),\\;C_{F}^{2,\\infty}=(1/\\epsilon)^{\\mathcal{O}(1)},\\;C_{F}=(1/\\epsilon)^{\\mathcal{O}(1)},\\;L_{T}=\\mathcal{O}\\left(d_{0}L_{s_{+}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proof Sketch. Our proof is built on the key observation that there is a tail behavior of the low-dimensional latent variable distribution $P_{h}$ (Assumption 2.2). Recall that $\\nabla\\log p_{t}(\\overline{{x}})\\;=\\;$ $B q(\\overline{{h}},t)/\\sigma(t)-\\overline{{x}}/\\sigma(t)$ ,where $\\overline{{h}}=B^{\\top}\\overline{{x}}$ (defined in (2.4)). By taking $\\widehat{W}_{B}=B$ , our aim reduces to construct a transformer network to approximate $q(\\overline{{h}},t)$ . To achieve this, we firstly approximate $q(\\overline{{h}},t)$ with a compact-supported continuous function, based on the tail behavior of $P_{h}$ . Then we construct a transformer to approximate the compact-supported continuous function using the universal approximation capacity of transformer [Yun et al., 2020]. See Appendix F.1 for a detailed proof. ", "page_idx": 5}, {"type": "text", "text": "Intuitively, Theorem 3.1 indicates the capability of the transformer-based score network to approximate the score function with precise guarantees. Furthermore, Theorem 3.1 provides empirical guidance for the design choices of the score network when a specified approximation error is required. ", "page_idx": 5}, {"type": "text", "text": "Remark 3.1 (Comparing with Existing Works). Theoretical analysis of DiTs is limited. Previous works that do not specify the model architecture assume that the score estimator is well-approximated [Benton et al., 2024, Wibisono et al., 2024]. To the best of our knowledge, this work is the first to present an approximation theory for DiTs, offering the estimation theory in Theorem 3.2 and Corollary 3.2.1 based on the estimated score network, rather than a perfectly trained one. ", "page_idx": 5}, {"type": "text", "text": "Remark 3.2 (Latent Dimension Dependency). Theorem 3.1 suggests that the approximation capacity and Transformer network size primarily depend on the latent variable dimension $d_{0}=d\\times L$ .This indicates that DiTs can potentially bypass the challenges associated with the high dimensionality of initial data by transforming input data into a low-dimensional latent variable. ", "page_idx": 5}, {"type": "text", "text": "3.3 Score Estimation and Distribution Estimation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Besides score approximation capability, Theorem 3.1 also characterizes the structural configuration of the score network for any specific precision, e.g., $K,C_{E},C_{F}$ , etc. This characterization enables further analysis of the performance of score network in practical scenarios. In Theorem 3.2, we provide a sample complexity bound for score estimation. In Corollary 3.2.1, show that the learned score estimator is able to recover the initial data distribution. ", "page_idx": 5}, {"type": "text", "text": "Score Estimation. To derive a sample complexity for score estimation using $S_{T_{p}^{2,1,4}}$ ,werewrite the score matching objective in (2.2) as $\\widehat{W}\\in\\operatorname{argmin}_{s_{W}\\in S_{\\tau_{p}^{2,1,4}}}\\widehat{\\mathcal{L}}(s_{W}),\\;\\widehat{W}=\\{\\widehat{W}_{B},\\widehat{f}_{7}\\}.$ ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2 shows that as sample size $n\\to\\infty$ \uff0c $s_{W}(\\cdot,t)$ convergences to $\\nabla\\log{p_{t}(\\cdot)}$ ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2 (Score Estimation of DiT). Under Asumptions 2.1 to 2.3, we choose $S_{\\mathcal{T}_{p}^{2,1,4}}$ as in Theorem 3.1 using $\\epsilon\\in(0,1)$ and $L>1$ ,With probability $1-1/\\mathrm{poly}(n)$ ,wehave ", "page_idx": 5}, {"type": "text", "text": "$\\frac{1}{T-T_{0}}\\int_{T_{0}}^{T}\\left\\|s_{\\widehat{W}}(\\cdot,t)-\\nabla\\log p_{t}(\\cdot)\\right\\|_{L^{2}(P_{t})}\\mathrm{d}t=\\widetilde{\\mathcal{O}}\\left(\\frac{1}{n^{1/3}T_{0}T}\\cdot2^{(1/\\epsilon)^{2L}}+\\frac{1}{n^{1/3}T_{0}T}+\\frac{1}{T_{0}T}\\epsilon^{2}\\right),$ where $\\widetilde O$ hides the factors related to $D,d_{0},d,L_{s_{+}}$ , and $\\log n$ ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Proof. See Appendix F.2 for a detailed proof. ", "page_idx": 5}, {"type": "text", "text": "Intuitively, Theorem 3.2 shows a sample complexity bound for score estimation in practice. ", "page_idx": 5}, {"type": "text", "text": "Remark 3.3 (Comparing with Existing Works). [Zhu et al., 2023] provides a sample complexity for simple ReLU-based diffusion models under the assumption of an accurate score estimator. To the best of our knowledge, we are the first to provide a sample complexity for DiTs, based on the learned score network in Theorem 3.1 and the quantization (piece-wise approximation) approach for transformer universality [Yun et al., 2020]. Furthermore, our first term shows a convergence rate of $1/T$ , outperforming [Chen et al., 2023], in which the first term is independent of $T$ ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Remark 3.4 (Double Exponential Factor and Inconsistent Convergence). Theorem 3.2 reports an explicit result on sample complexity bounds for score estimation of latent DiTs: a double exponential factor $2^{(1/\\epsilon)^{2L}}$ in the first term. We remark that this arises from the required depth $K$ $\\mathcal{O}(\\epsilon^{-2L})$ \uff0c and the norm of required weight parameters is $(1/\\epsilon)^{\\mathcal{O}(1)}$ as shown in Theorem 3.1, assuming the universality of transformers requires dense layers [Yun et al., 2020]. This double exponential factor causes inconsistent convergence with respect to sample size $n$ , as its large value prevents setting $\\epsilon$ as a function of $n$ to balance the first and second terms in (3.1). This motivates us to rethink transformer universality and explore new proof techniques for DiTs, which we leave for future work. ", "page_idx": 6}, {"type": "text", "text": "DenitonFlanie, wd $\\begin{array}{r}{\\xi(n,\\epsilon,L):=\\frac{1}{n^{1/3}}\\cdot2^{\\left(1/\\epsilon\\right)^{2L}}+\\frac{1}{n^{1/3}}+\\epsilon^{2}.}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "Distribution Estimation. In practice, DiTs generate data using the discretized version with step size $\\mu$ , see Appendix D.1 for details. Let $\\widehat{P}_{T_{0}}$ be the distribution generated by ${\\boldsymbol{s}}_{\\widehat{W}}$ using the discretized backward processin Theorem 3.2. Let $P_{T_{0}}^{h}$ and $p_{T_{0}}^{h}$ bethdistibutoanddensityfutf on-support latent variable $\\bar{h}$ at $T_{0}$ . We have the following results for distribution estimation. ", "page_idx": 6}, {"type": "text", "text": "Corollary 3.2.1 (Distribution Estimation of DiT, Modified From Theorem 3 of [Chen et al., 2023]) Let $T=\\mathcal{O}(\\log n),T_{0}=\\mathcal{O}(\\operatorname*{min}\\{c_{0},1/L_{s_{+}}\\})$ , where $c_{0}$ is the minimum eigenvalue of $\\mathbb{E}_{P_{h}}[h h^{\\top}]$ With the estimated DiT score network ${}s_{\\widehat{W}}$ in Theorem 3.2,we have thefollowing with probability $1-1/\\mathrm{poly}(n)$ ", "page_idx": 6}, {"type": "text", "text": "(i) The accuracy to recover the subspace $B$ .s $\\begin{array}{r}{\\left\\|W_{B}W_{B}^{\\top}-B B^{\\top}\\right\\|_{F}^{2}=\\widetilde{\\mathcal{O}}\\left(\\xi(n,\\epsilon,L)/c_{0}\\right)\\!.}\\end{array}$ (ii) With the conditions $\\mathsf{K L}(P_{h}||N(0,I_{d_{0}}))<\\infty$ , there exists an orthogonal matrix $U\\in\\mathbb{R}^{d\\times d}$ such that we have the following upper bound for the total variation distance ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathsf{T V}(P_{T_{0}}^{h},(W_{B}U)_{\\sharp}^{\\top}\\,\\widehat P_{T_{0}})=\\widetilde{\\mathcal{O}}(\\sqrt{\\xi(n,\\epsilon,L)\\cdot\\log n}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\widetilde O$ hides the factor about $D,d_{0},d,L_{s_{+}},\\log n$ , and $T-T_{0}$ . and $(W_{B}U)_{\\sharp}^{\\top}\\widehat{P}_{T_{0}}$ denotes the pushforward distribution. ", "page_idx": 6}, {"type": "text", "text": "(ii) For the generated data distribution $\\widehat{P}_{T_{0}}$ , the orthogonal pushforward $(I-W_{B}W_{B}^{\\top})_{\\sharp}\\widehat{P}_{T_{0}}$ is $N(0,\\Sigma)$ ,where $\\Sigma\\preceq a T_{0}I$ for a constant $a>0$ ", "page_idx": 6}, {"type": "text", "text": "Proof. See Appendix F.3 for a detailed proof. ", "page_idx": 6}, {"type": "text", "text": "Intuitively, Corollary 3.2.1 shows the estimation results in 3 parts: (i) the accuracy of recovering the $B$ $\\widehat{P}_{T_{0}}$ $P_{T_{0}}^{h}$ $\\tilde{P_{T_{0}}}$ initial data distribution. Notably, Corollary 3.2.1 is agnostic to the specifics of $\\xi(n,\\epsilon,L)$ ", "page_idx": 6}, {"type": "text", "text": "Remark 3.5 (Comparing with Existing Works). Oko et al. [2023] analyze the distribution estimation under the assumption that the initial density is supported on $[-\\dot{1},1]^{\\dot{D}}$ and smooth in the boundary. Our Assumption 2.2 demonstrates greater practical relevance. This suggests that our method of distribution estimation aligns more closely with empirical realities. ", "page_idx": 6}, {"type": "text", "text": "Remark 3.6 (Subspace Recovery Accuracy). (i) of Corollary 3.2.1 confirms that the subspace is learned by DiTs. The error is proportional to the sample complexity for score estimation and depends on the minimum eigenvalue of the covariance of $P_{h}$ ", "page_idx": 6}, {"type": "text", "text": "4 Provably Efficient Criteria ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Here, we analyze the computational limits of latent DiTs under low-dimensional linear subspace data assumption (i.e., Assumption 2.1). The hardness of DiT models ties to both forward and backward passes of the score network in Definition 3.3. We characterize them separately. ", "page_idx": 6}, {"type": "text", "text": "4.1  Computational Limits of Backward Computation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Following Setion 2, supose we have $n$ i.i.d. data samples $\\{x_{0,i}\\}_{i=1}^{n}\\sim P_{d}$ and time $t_{i_{0}}$ $[1\\leq$ $i\\,\\leq\\,n\\,$ uniformly sampled from $[T_{0},T]$ . For each data $x_{0,i}\\,\\in\\,\\mathbb{R}^{D}$ , we sample $\\boldsymbol{x}_{t_{i_{0}}}\\in\\mathbb{R}^{D}$ from ", "page_idx": 6}, {"type": "text", "text": "$N(\\beta(t_{i_{0}})x_{0,i},\\sigma(t_{i_{0}})I_{D})$ Let $(W_{A}R^{-1}(\\cdot))^{\\dagger}$ be the inverse transformation of $W_{A}R^{-1}(\\cdot)$ , and denote $Y_{0,i}:=(W_{A}R^{-1})^{\\dagger}(x_{0,i})\\in\\mathbb{R}^{d\\times L}$ We rewrite the empirical denoising score-matching loss(2.2)as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{1}{\\i}\\sum_{i=1}^{n}\\bigg\\|W_{A}R^{-1}(f_{T}(R(\\underbrace{W_{A}^{\\top}x_{t_{i_{0}}}}_{d_{0}\\times1}))-x_{0,i}\\bigg\\|_{F}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}\\bigg\\|\\underbrace{W_{A}}_{D\\times d_{0}}\\underbrace{R^{-1}\\Big(\\overbrace{f_{T}(R(W_{A}^{\\top}x_{t_{i_{0}}}))}^{d\\times L}-\\underbrace{Y_{0,i}}_{d\\times L}\\Big)\\bigg\\|_{F}^{2}}_{d_{0}\\times1}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "For efficiency, it suffices to focus on just transformer attention heads of the DiT score network due to their dominating quadratic time complexity in both passes. Thus, we consider only a single layer attention for $f_{T}$ , to simplify our analysis. Further, we consider the following simplifications: ", "page_idx": 7}, {"type": "text", "text": "(SO) To prove the hardness of (4.1) for both full gradient descent and stochastic mini-batch gradient descent methods, it sufices to consider training on a single data point. ", "page_idx": 7}, {"type": "text", "text": "(S1) For the convenience of our analysis, we consider the following expression for attention mechanism. Let $X,Y\\ \\in\\ \\mathbb{R}^{d\\times L}$ . Let $W_{K},W_{Q},W_{V}\\ \\in\\ \\mathbb{R}^{s\\times d}$ be attention weights such that $Q\\,=\\,W_{Q}X\\,\\in\\,\\mathbb{R}^{d\\times L}$ $K\\,=\\,W_{K}X\\,\\in\\,\\mathbb{R}^{s\\times\\dot{L}}$ and $V\\,=\\,W_{V}X\\,\\in\\,\\mathbb{R}^{s\\times L}$ . We write attention mechanism of hidden size $s$ and sequence length $L$ as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname{Att}(X)=\\underbrace{(W_{O}W_{V}X)}_{V\\mathrm{\\,multiplication}}\\underbrace{D^{-1}\\exp\\bigl(X^{\\mathsf{T}}W_{K}^{\\mathsf{T}}W_{Q}X\\bigr)}_{K\\cdot Q\\mathrm{\\,multiplication}}\\in\\mathbb{R}^{d\\times L},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "with $D:=\\mathrm{diag}(\\exp\\bigl(X W_{Q}W_{K}^{\\top}X^{\\top}\\bigr)\\mathbb{1}_{L})$ . Here, $\\exp(\\cdot)$ is entry-wise exponential function, i.e., $\\exp(A)_{i,j}=\\exp(A_{i,j})$ for any matrix $A\\,,\\mathrm{diag}(\\cdot)$ converts a vector into a diagonal matrix with the vector's entries on the diagonal, and ${\\mathbb I}_{L}$ is the length- $L$ all ones vector. ", "page_idx": 7}, {"type": "text", "text": "(S2) Since $V$ multiplication is linear in weight while $K{-}Q$ multiplication is exponential in weights, we only need to focus on the gradient update of $K{-}Q$ multiplication. Therefore, for efficiency analysis of gradient, it is equivalent to analyzing a reduced problem with fixed $W_{O}W_{V}X=$ const.. ", "page_idx": 7}, {"type": "text", "text": "(S3) To focus on the DiT, we consider the low-dimensional linear encoder $W_{A}$ to be pretrained and to not participate in gradient computation. This aligns with common practice [Rombach et al., 2022] and is justified by the trivial computation cost due to the linearity of $W_{A}{}^{2}$ ", "page_idx": 7}, {"type": "text", "text": "(S4)  To further simplify, we introduce $A_{1},A_{2},A_{3}\\in\\mathbb{R}^{s\\times L}$ and W E Rdxd via ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\Big\\|W_{A}R^{-1}\\big(f_{T}\\underbrace{(R(W_{A}^{\\top}x_{t_{i_{0}}})}_{:=X\\in\\mathbb R^{d\\times L}})-\\underbrace{Y_{0,i}}_{:=Y\\in\\mathbb R^{d\\times L}}\\big)\\Big\\|_{F}^{2}}}&{\\quad\\mathrm{(By~(S0),(S1)~and~(S2))}}\\\\ &{=\\Big\\|W_{A}R^{-1}\\big(\\underbrace{W_{O}W_{V}}_{:=W_{O V}\\in\\mathbb R^{d\\times L}}\\underbrace{X}_{:=A_{3}\\in\\mathbb R^{d\\times L}}\\big(\\underbrace{X^{\\top}}_{:=A_{1}^{\\top}\\in\\mathbb R^{L\\times d}}\\underbrace{W_{K}^{\\top}W_{Q}}_{:=W\\in\\mathbb R^{d\\times d}}\\underbrace{X}_{:=A_{2}\\in\\mathbb R^{d\\times L}}\\big)-Y\\big)\\Big\\|_{F}^{2}.}&\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Notably, $A_{1},A_{2},A_{3},X,Y$ are constants w.r.t. training above loss with gradient updates. ", "page_idx": 7}, {"type": "text", "text": "Therefore, we simplify the objective of training DiT into ", "page_idx": 7}, {"type": "text", "text": "Definition4.1 Training Generic DiT Loss). Given $A_{1},A_{2},A_{3},Y\\in\\mathbb{R}^{d\\times L}$ and $W_{O V},W\\in\\mathbb{R}^{d\\times d}$ following (S4), Training a DiT with $\\ell_{2}$ loss on a single data point $X,Y\\in\\mathbb{R}^{d\\times L}$ is formulated as ", "page_idx": 7}, {"type": "text", "text": "Here ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{W}{\\operatorname*{min}}~\\mathcal{L}_{0}(W)=\\underset{W}{\\operatorname*{min}}~\\frac{1}{2}\\Big\\|W_{A}R^{-1}\\big(W_{O V}A_{3}D^{-1}\\exp\\bigl(A_{1}^{\\top}W A_{2}\\bigr)-Y\\bigr)\\Big\\|_{F}^{2}.}\\\\ &{\\underset{\\mathrm{diag}\\big(\\exp\\bigl(A_{1}^{\\top}W A_{2}\\bigr)\\mathbb{1}_{n}\\bigr)\\in\\mathbb{R}^{L\\times L}.}{\\mathrm{diag}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Remark 4.1 (Conditional and Unconditional Generation). ${\\mathcal{L}}_{0}$ is generic. If $A_{1}\\neq A_{2}\\in\\mathbb{R}^{d\\times L}$ Definition 4.1 reduces to cross-attention in DiT score net (for conditional generation). If $A_{1}=A_{2}\\in$ $\\mathbb{R}^{d\\times L}$ , Definition 4.1 reduces to self-attention in DiT score net (for unconditional vanilla generation). ", "page_idx": 7}, {"type": "text", "text": "We introduce the next problem to characterize all possible gradient computations of optimizing (4.4). ", "page_idx": 7}, {"type": "text", "text": "Problem1(Approximate DiT Gradient Computation $(\\mathrm{ADITGC}(L,d,\\Gamma,\\epsilon)))$ . Given $A_{1},A_{2},A_{3},Y~\\in~\\mathbb{R}^{d\\times L}$ . Let $\\epsilon\\mathrm{~\\,~>~\\,~0~}$ Assume all numerical values are in ${\\mathcal{O}}(\\log(L))$ -bits encoding. Let loss function $\\scriptstyle{\\mathcal{L}}_{0}$ follow Definition 4.1. The problem of approximating gradient computation of optimizing empirical DiT loss (4.4) is to find an approximated gradient matrix $\\widetilde{G}^{(W)}\\in\\mathbb{R}^{d\\times d}$ such that -Ilmax \u2264 1/poly(L). Here,IIAll max = maxi,5 |Az) for any matrix $A$ ", "page_idx": 8}, {"type": "text", "text": "In this work, we aim to investigate the computational limits of all possible efficient algorithms of ADITGCwith $\\epsilon=1/\\mathrm{poly}(L)$ . Yet, the explicit gradient of DiT denoising score matching loss (4.4) is too complicated to characterize AD1TGC. To combat this, we make the following observations. ", "page_idx": 8}, {"type": "text", "text": "(O2) Vectorization of $f_{T}$ . For the ease of presentation, we use notation flexibly that $f_{T}$ to denote both a matrix in $\\bar{\\mathbb{R}}^{d\\times L}$ and a vector in $\\mathbb{R}^{d L}$ in the following analysis. This practice does not affect correctness. The context in which $f_{T}$ is used should clarify whether it refers to a matrix or a vector. Explicit vectorization follows Definition D.1. ", "page_idx": 8}, {"type": "text", "text": "(O3) Linearity of $g_{1}$ . By linearity of $W_{A}R^{-1}(\\cdot)$ , we treat $g_{1}$ as a matrix in $\\mathbb{R}^{d_{0}\\times d L}$ acting on vector $f_{T}(\\cdot)\\in\\mathbb{R}^{d L}$ ", "page_idx": 8}, {"type": "text", "text": "Therefore, we have $\\mathcal{L}_{0}=\\left|\\left|g_{1}\\cdot\\left[g_{2}(g_{3})-Y\\right]\\right|\\right|_{2}^{2}$ such tatits gadien nvolves $\\begin{array}{r}{\\frac{\\mathrm{d}\\mathcal{L}_{0}}{\\mathrm{d}W}=g_{1}\\frac{\\mathrm{d}g_{2}}{\\mathrm{d}W}}\\end{array}$ .From above,weonlyd tofousnproing the comutationtmeanderrorconrolof $\\frac{\\mathrm{d}g_{2}}{\\mathrm{d}W}$ for gradient w.r.t $W$ . Luckily, with tools from fine-grained complexity theory [Alman and Song, 2023, 2024a,b,c] and tensor trick (see Appendix D.3), we prove the existence of almost-linear time algorithms for Problem 1 in the next theorem. Let $\\mathrm{vec}(W)\\,:=\\,\\underline{{W}}$ for any matrix $W$ following Definition D.1. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.1 (Existence of Almost-Linear Time Algorithms for AD1TGC). Suppose all numerical values are in ${\\mathcal{O}}(\\log L)$ -bits encoding. Let $\\operatorname*{max}(\\|W_{O V}A_{3}\\|_{\\operatorname*{max}},\\|W_{K}A_{1}\\|_{\\operatorname*{max}},\\|W_{Q}A_{2}\\|_{\\operatorname*{max}})\\leq\\Gamma$ There exists a $L^{1+o(1)}$ time algorithm to solve ADITC $\\}\\mathbf{C}(L_{p},L,d=\\mathcal{O}(\\log L),\\Gamma=o(\\sqrt{\\log L}))$ (i.e., Problem 1) with loss $\\scriptstyle{\\mathcal{L}}_{0}$ from Definition 4.1 up to $1/\\mathrm{poly}(L)$ accuracy. In particular, this algorithm outputs gradient matrices $\\widetilde{G}^{(W)}\\in\\mathbb{R}^{d\\times d}$ such that $\\begin{array}{r}{\\|\\widetilde{\\underline{G}}^{(W)}-\\frac{\\partial\\mathcal{L}}{\\partial\\underline{W}}\\|_{\\operatorname*{max}}\\leq1/\\mathrm{poly}(L).}\\end{array}$ ", "page_idx": 8}, {"type": "text", "text": "Proof Sketch. Our proof is built on the key observation that there exist low-rank structures within the DiT training gradients. Using the tensor trick [Diao et al., 2019, 2018] and computational hardness results of attention [Hu et al., 2024b, Alman and Song, 2023], we approximate DiT training gradients with a series of low-rank approximations and carefully match the multiplication dimensions so that the computation of d forms a chained low-rank approximation. We complete the proof by demonstrating that this approximation is bounded by a $1/\\mathrm{poly}(L)$ error and requires only almostlinear time. See Appendix G.2 for a detailed proof. \u53e3 ", "page_idx": 8}, {"type": "text", "text": "Remark 4.2. We remark that Theorem 4.1 is dominated by the relation between $L$ and $d$ henceby the subspace dimension3 $d_{0}=d L$ .Asmaller $d_{0}$ makes Theorem 4.1 more likely to hold. ", "page_idx": 8}, {"type": "text", "text": "4.2  Computational Limits of Forward Inference ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Since the inference of score-matching diffusion models is a forward pass of the trained score estimator $s_{W}$ , the computational hardness of DiT ties to the transformer-based score network, ", "page_idx": 8}, {"type": "equation", "text": "$$\ns_{W}(A_{1},A_{2},A_{3})=W_{A}R^{-1}\\big(\\underbrace{W_{O V}A_{3}}_{d\\times L}\\underbrace{D^{-1}}_{L\\times L}\\exp\\big(\\underbrace{A_{1}^{\\top}W_{K}^{\\top}}_{L\\times s}\\underbrace{W_{Q}A_{2}}_{s\\times L}\\big)\\big),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "following notation in Definition 4.1. For inference, we study the following approximation problem.   \nNotably, by Remark 4.1, (4.5) subsumes both conditional and unconditional DiT inferences. ", "page_idx": 8}, {"type": "text", "text": "Problem 2(ApproximateDiT Inference ADiTI $(d,L,\\Gamma,\\delta_{F});$ . Let $\\delta_{F}\\,>\\,0$ and $B\\,>\\,0$ . Given $A_{1},A_{2},A_{3}\\;\\;\\in\\;\\;\\mathbb{R}^{d\\times L}$ and $W_{O V},W_{K},W_{Q}\\;\\;\\in\\;\\;\\mathbb{R}^{d\\times d}$ with guarantes that $\\begin{array}{r l}{\\|W_{O V}A_{3}\\|_{\\infty}}&{{}\\leq}\\end{array}$ $B$ \uff0c $\\|W_{K}A_{1}\\|_{\\infty}\\;\\leq\\;B$ and $\\|W_{Q}A_{2}\\|_{\\infty}\\;\\leq\\;B$ ,  we aim to study an approximation problem $\\mathrm{ADITI}(d,L,B,\\delta_{F}).$ that approximates $s_{W}(A_{1},A_{2},A_{3})$ with a vector $\\widetilde{z}\\in\\mathbb{R}^{d_{0}}$ (with $d_{0}=d\\cdot L_{\\sun}$ such that $\\left|\\widetilde{z}-W_{\\perp}R_{.}^{-1}\\left(W_{O V}A_{3}D^{-1}\\exp\\!\\left(A_{1}^{\\top}W_{K}^{\\top}W_{Q}A_{2}\\right)\\right)\\right|\\right|_{\\operatorname*{max}}\\leq\\delta_{F}$ Here, $\\left\\|A\\right\\|_{\\operatorname*{max}}:=\\operatorname*{max}_{i,j}|A_{i j}|$ for any matrix $A$ ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "By (O2) and (O3), we make an observation that Problem 2 is just a special case of [Alman and Song, 2023]. Hence, we characterize the all possible efficient algorithms for AD1TI with next proposition. ", "page_idx": 9}, {"type": "text", "text": "Proposition 4.1 (Norm-Based Efficiency Phase Transition). Let $\\|W_{Q}A_{2}\\|_{\\infty}\\leq B$ \uff0c $\\|W_{K}A_{1}\\|_{\\infty}\\leq B$ and $\\|W_{O V}A_{3}\\|_{\\infty}\\leq B$ with $B=\\mathcal{O}(\\sqrt{\\log L})$ . Assuming SETH (Hypothesis 1), for every $q>0$ \uff0c there are constants $C,C_{a},C_{b}>0$ such that: there is no $O(n^{2-q})$ -time sub-quadratic) algorithm for the problem AD1T $\\mathrm{I}(L,d=C\\log L,B=C_{b}\\sqrt{\\log L},\\delta_{F}=L^{-C_{a}}$ ). ", "page_idx": 9}, {"type": "text", "text": "Remark 4.3. Proposition 4.1 suggests an effciency threshold for the upper bound of $\\|W_{K}A_{1}\\|_{\\infty}$ $\\|W_{Q}A_{2}\\|_{\\infty},\\|W_{O V}\\bar{A}_{3}\\|_{\\infty}$ . Only below this threshold are efficient algorithms for Problem 2 possible. ", "page_idx": 9}, {"type": "text", "text": "Moreover, there exist almost-linear DiT inference algorithms following [Alman and Song, 2023]. ", "page_idx": 9}, {"type": "text", "text": "Proposition 4.2 (Almost-Linear Time DiT Inference). Assuming SETH, the DiT inference problem ADiTI $\\langle L,d=\\mathcal{O}(\\log L),B=o(\\sqrt{\\log L}),\\delta_{F}=1/\\mathrm{poly}(L))$ can be solved in $L^{1+o(1)}$ time. ", "page_idx": 9}, {"type": "text", "text": "Remark 4.4. Proposition 4.2 is a special case of Proposition 4.1 under the effciency threshold. ", "page_idx": 9}, {"type": "text", "text": "Remark 4.5. Propositions 4.1 and 4.2 are dominated by the relation between $L$ and $d$ ,hencebythe subspace dimension $d_{0}=d L$ .A smaller $d_{0}$ makes Propositions 4.1 and 4.2 more likely to hold. ", "page_idx": 9}, {"type": "text", "text": "5  Discussion and Concluding Remarks ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We explore the fundamental limits of latent DiTs with 3 key contributions. First, we prove that transformers are universal approximators for the score functions in DiTs (Theorem 3.1), with approximation capacity and model size dependent only on the latent dimension, suggesting DiTs can handle high-dimensional data challenges. Second, we show that Transformer-based score estimators converge to the true score function (Theorem 3.2), ensuring the generated data distribution closely approximates the original (Corollary 3.2.1). Third, we provide provably efficient criteria (Proposition 4.1) and prove the existence of almost-linear time algorithms for forward inference (Proposition 4.2) and backward computation (Theorem 4.1). Our computational results hold for both unconditional and conditional generation of DiTs (Remark 4.1). These results highlight the potential of latent DiTs to achieve both computational efficiency and robust performance in practical scenarios. ", "page_idx": 9}, {"type": "text", "text": "Practical Guidance from Computational Results.  Section 4 analyzes the computational feasibility and identifies all possible efficient DiT algorithms/methods for both forward inference and backward training. These results provide practical guidance for designing efficient methods: ", "page_idx": 9}, {"type": "text", "text": "\u00b7 The latent dimension should be sufficiently small: $d=O(\\log L)$ (Theorem 4.1, Propositions 4.1 and 4.2).   \n\u00b7 Normalization of $K$ \uff0c $Q$ , and $V$ in DiT attention heads enhances performance and efficiency: - For efficient inference: max $\\{\\|W_{K}A_{1}\\|,\\|W_{Q}A_{2}\\|,\\|W_{O V}A_{3}\\|\\}\\,\\le\\,B$ with $B\\,=\\,o(\\sqrt{\\log L})$ (Proposition 4.2) and $A_{1},A_{2},A_{3}$ being the input data associated with $K,Q,V$ - For effcient training: max $\\{||W_{K}A_{1}||,||W_{Q}A_{2}||,||W_{O V}A_{3}||\\}\\leq\\Gamma$ with $\\Gamma=o(\\sqrt{\\log L})$ (Theorem 4.1). ", "page_idx": 9}, {"type": "text", "text": "We remark that these conditions are necessary but not sufficient; sufficient conditions depend on the specific design of the methods used. This is due to the best- or worst-case nature of hardness results. ", "page_idx": 9}, {"type": "text", "text": "Limitations and Future Direction.  As discussed in Remark 3.4, the double exponential factor in our explicit sample complexity bound (Theorem 3.2) suggests a possible gap in our understanding of transformer universality and its interplay with DiT architecture. This motivates us to rethink transformer universality and explore new proof techniques for DiTs, which we leave for future work. Besides, due to its formal nature, this work does not provide immediate practical implementations. However, we expect that our findings provide valuable insights for future diffusion generative models. ", "page_idx": 9}, {"type": "text", "text": "Post-Acceptance Note [October, 29, 2024]. During preparation of the camera-ready version, we learn of a follow-up work [Anonymous, 2024b] that alleviates the double exponential factor and achieves minimax optimal statistical rates for DiTs under Holder smoothness data assumptions. ", "page_idx": 9}, {"type": "text", "text": "Broader Impact ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This theoretical work aims to shed light on the foundations of diffusion generative models and is not anticipated to have negative social impacts. ", "page_idx": 10}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "JH would like to thank to Minshuo Chen, Sophia Pi, Yi-Chen Lee, Yu-Chao Huang, Yibo Wen, Damien Jian, Jialong Li, Zijia Li, Tim Tsz-Kit Lau, Chenwei Xu, Dino Feng and Andrew Chen for enlightening discussions on related topics, and the Red Maple Family for support. The authors would like to thank the anonymous reviewers and program chairs for constructive comments. ", "page_idx": 10}, {"type": "text", "text": "JH is partially supported by the Walter P. Murphy Fellowship. HL is partially supported by NIH R01LM1372201, AbbVie and Dolby. The content is solely the responsibility of the authors and does not necessarily represent the official views of the funding agencies. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Beatrice Achilli, Enrico Ventura, Gianluigi Silvestri, Bao Pham, Gabriel Raya, Dmitry Krotov, Carlo Lucibello, and Luca Ambrogioni. Losing dimensions: Geometric memorization in generative diffusion. arXiv preprint arXiv:2410.08727, 2024.   \nJosh Alman and Zhao Song. Fast attention requires bounded entries. Advances in Neural Information Processing Systems (NeurIPS), 36, 2023.   \nJosh Alman and Zhao Song. How to capture higher-order correlations? generalizing matrix softmax attention to kronecker computation. In The Twelfth International Conference on Learning Representations (ICLR), 2024a.   \nJosh Alman and Zhao Song. The fine-grained complexity of gradient computation for training large language models. In NeurIPS. arXiv preprint arXiv:2402.04497, 2024b.   \nJosh Alman and Zhao Song. Fast rope attention: Combining the polynomial method and fast fourier transform. In manuscript, 2024c.   \nLuca Ambrogioni. In search of dispersed memories: Generative diffusion models are associative memory networks. arXiv preprint arXiv:2309.17290, 2023.   \nAnonymous. Fundamental limits of prompt tuning transformers: Universality, capacity and efficiency. In Submited to The Thirteenth International Conference on Learning Representations, 2024a. URL https://openreview.net/forum?id $=$ jDpdQPMo s W. under review.   \nAnonymous. On statistical rates of conditional diffusion transformer: Approximation and estimation. In Submitted to The Thirteenth International Conference on Learning Representations, 2024b. URL https://openreview.net/forum?id $=$ c5 4apoozCS. under review.   \nFan Bao, Chongxuan Li, Yue Cao, and Jun Zhu. All are worth words: a vit backbone for score-based diffusion models. In NeurIPS 2022 Workshop on Score-Based Methods, 2022.   \nJoe Benton, Valentin De Bortoli, Arnaud Doucet, and George Deligiannidis. Nearly d-linear convergence bounds for diffusion models via stochastic localization. In The Twelth International Conference on Learning Representations (ICLR), 2024.   \nValentin De Bortoli. Convergence of denoising diffusion models under the manifold hypothesis. Transactions on Machine Learning Research,2022. ISSN 2835-8856.   \nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.   \nJunsong Chen, Jincheng YU, Chongjian GE, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart- $\\mathbb{S}$ Aalpha\\$: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In The Twelfth International Conference on Learning Representations (ICLR), 2024a.   \nMinshuo Chen, Xingguo Li, and Tuo Zhao. On generalization bounds of a family of recurrent neural networks. In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics (AISTATS), volume 108, pages 1233-1243, 2020a.   \nMinshuo Chen, Wenjing Liao, Hongyuan Zha, and Tuo Zhao. Distribution approximation and statistical estimation guarantees of generative adversarial networks. arXiv preprint arXiv:2002.03938, 2020b.   \nMinshu Chen,axuanHuang,uZha, and MengdiWang.cor approximation,stimatin and distribution recovery of diffusion models on low-dimensional data. In International Conference on Machine Learning (ICML), pages 4672-4712. PMLR, 2023.   \nMinshuChe, Sng Mei, Jiaqing Fan, andMengdi Wang.An verviewof diffusionmodels - plications, guided generation, statistical rates and optimization. arXiv preprint arXiv:2404.07771, 2024b.   \nMarek Cygan, Holger Dell, Daniel Lokshtanov, Daniel Marx, Jesper Nederlof, Yoshio Okamot, Ramamohan Paturi, Saket Saurabh, and Magnus Wahlstrom. On problems as hard as cnf-sat. ACM Transactions on Algorithms (TALG), 12(3):1-24, 2016.   \nHuaian Diao, Zhao Song, Wen Sun, and David Woodruff Sketching for kronecker product regression and p-splines. In International Conference on Artificial Intelligence and Statistics (AISTATS), pages 1299-1308. PMLR, 2018.   \nHuaian Diao, Rajesh Jayaram, Zhao Song, Wen Sun, and David Woodruff. Optimal sketching for kronecker product regression and low rank approximation. Advances in neural information processing systems (NeurIPS), 32, 2019.   \nBenjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation in self-attention mechanisms. In International Conference on Machine Learning (ICML), pages 5793-5831. PMLR, 2022.   \nPatrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024.   \nLuciano Floridi and Massimo Chiriatti. Gpt-3: Its nature, scope,limits, and consequences. Minds and Machines, 30:681-694, 2020.   \nShanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is a strong image synthesizer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 23164-23173, 2023a.   \nYeqi Gao, Zhao Song, Weixin Wang, and Junze Yin. A fast optimization view: Reformulating single layer attention in llm based on tensor and svm trick, and solving it in matrix multiplication time. arXiv preprint arXiv:2309.07418, 2023b.   \nYeqi Gao, Zhao Song, and Shenghao Xie. In-context learning for attention scheme: from single softmax regression to multiple softmax regression via a tensor trick. arXiv preprint arXiv:2307.02419, 2023c.   \nJiuxiang Gu, Yngyu Liang,Zhenmei Shi, Zhao Song, and Yufa Zhou. Tensor attention training: Provably efficient learning of higher-order transformers. arXiv preprint arXiv:2405.16411, 2024.   \nJiaqi Guan, Xiangxin Zhou, Yuwei Yang, Yu Bao, Jian Peng, Jianzhu Ma, Qiang Liu, Liang Wang, and Quanquan Gu. Decompdiff: diffusion models with decomposed priors for structure-based drug design. arXiv preprint arXiv:2403.07902, 2024.   \nJonathan Ho, Ajay Jain, and Pieter Abbeel Denoising diffusion probabilistic models. Advances n neural information processing systems, 33:6840-6851, 2020.   \nBenjamin Hoover, Hendrik Strobelt, Dmitry Krotov, Judy Hoffman, Zsolt Kira, and Duen Horng Chau. Memory in plain sight: A survey of the uncanny resemblances between difusion models and associative memories. arXiv preprint arXiv:2309.16750, 2023.   \nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang. and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \nJerry Yao-Chieh Hu, Donglin Yang, Dennis Wu, Chenwei Xu, Bo-Yu Chen, and Han Liu. On sparse modern hopfield model. In Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS), 2023.   \nJerry Yao-Chieh Hu, Pei-Hsuan Chang, Haozheng Luo, Hong-Yu Chen, Weijian Li, Wei-Po Wang, and Han Liu. Outlier-efficient hopfield layers for large transformer-based models. In Forty-first International Conference on Machine Learning (ICML), 2024a.   \nJerry Yao-Chieh Hu, Thomas Lin, Zhao Song, and Han Liu. On computational limits of modern hopfield models: A fine-grained complexity analysis. In Forty-first International Conference on Machine Learning (ICML),2024b.   \nJerry Yao-Chieh Hu, Maojiang Su, En-Jui Kuo, Zhao Song, and Han Liu. Computational limits of low-rank adaptation (lora) for transformer-based models. arXiv preprint arXiv:2406.03136, 2024c.   \nJerry Yao-Chieh Hu, Dennis Wu, and Han Liu. Provably optimal memory capacity for modern hopfield models: Transformer-compatible dense associative memories as spherical codes. In Thirty-eighth Conference on Neural Information Processing Systems (NeurIPS), volume 38, 2024d.   \nRussell Impagliazzo and Ramamohan Paturi. On the complexity of k-sat. Journal of Computer and System Sciences, 62(2):367-375, 2001.   \nYanrong Ji, Zhihan Zhou, Han Liu, and Ramana V Davuluri. Dnabert: pre-trained bidirectional encoder representations from transformers model for dna-language in genome. Bioinformatics, 37 (15):2112-2120, 2021.   \nHaotian Jiang and QianxiaoLi. Approximation theory of transformer networks for sequence modeling. arXiv preprint arXiv:2305.18475, 2023.   \nTokio Kajitsuka and Issei Sato. Are transformers with one layer self-attention using low-rank weight matrices universal approximators? arXiv preprint arXiv:2307.14023, 2023.   \nJunghwan Kim, Michelle Kim, and Baran Mozafari. Provable memorization capacity of transformers. In The Eleventh International Conference on Learning Representations (ICLR), 2022.   \nYingyu Liang, Heshan Liu, Zhenmei Shi, Zhao Song, Zhuoyan Xu, and Junze Yin. Conv-basis: A new paradigm for efficient attention inference and gradient computation in transformers. arXiv preprint arXiv:2405.05219, 2024a.   \nYingyu Liang, Jiangxuan Long,Zhenmei Shi,Zhao Song, and Yufa Zhou Beyond linear approximations: A novel pruning approach for attention matrix. arXiv preprint arXiv:2410.11261, 2024b.   \nYingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song, and Yufa Zhou. Multi-layer transformers gradient can be approximated in almost linear time. arXiv preprint arXiv:2408.13233, 2024c.   \nYingyu Liang, Zhenmei Shi, Zhao Song, and Yufa Zhou. Tensor attention training: Provably effcient learning of higher-order transformers. arXiv preprint arXiv:2405.16411, 2024d.   \nYixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, Lifang He, and Lichao Sun. Sora: A review on background,techology, limitations, and opportunities of large vision models, 2024.   \nZhonghua Liu, Yue Lu, Zhihui Lai, Weihua Ou, and Kaibing Zhang. Robust sparse low-rank embedding for image dimension reduction. Applied Soft Computing, 113:107907, 2021.   \nZhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion models for high-quality video generation. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10209-10218. IEEE, 2023.   \nNanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M Boff, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024.   \nSadegh Mahdavi, Renjie Liao, and Christos Thrampoulidis. Memorization capacity of multi-head attention in transformers. arXiv preprint arXiv:2306.02010, 2023.   \nShentong Mo, Enze Xie, Ruihang Chu, Lanqing Hong, Matthias Niessner, and Zhenguo Li. Dit-3d: Exploring plain diffusion transformers for 3d shape generation. Advances in Neural Information Processing Systems (NeurIPS), 36, 2023.   \nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.   \nKazusato Oko, Shunta Akiyama, and Taiji Suzuki. Diffusion models are minimax optimal distribution estimators. In International Conference on Machine Learning (ICML), pages 26517-26582. PMLR, 2023.   \nOpenAI. Sora: A video generative model based on transformer diffusion. OpenAI Research, 2024. Accessed: 08/16/2024.   \nWilliam Peebles and Saining Xie. Scalable difusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 4195-4205, 2023.   \nPhillip Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom Goldstein. The intrinsic dimension of images and its impact on learning. arXiv preprint arXiv:2104.08894, 2021.   \nAlec Radford, Jeffrey Wu, Rwn Child, David Luan, Dario Amdei, Iya Sutskever, et al. Language models are unsupervised multitask learners. OpenAl blog, 1(8):9, 2019.   \nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.   \nHubert Ramsauer, Bernhard Schaf, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlovic, Geir Kjetil Sandve, et al. Hopfield networks is all you need. arXiv preprint arXiv:2008.02217, 2020.   \nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pages 10684-10695, 2022.   \nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems (NeurIPS), 32, 2019.   \nBing Su and Ying Wu. Learning low-dimensional temporal representations. In International Conference on Machine Learning (ICML), pages 4761-4770. PMLR, 2018.   \nWenpin Tang and Hanyang Zhao. Score-based diffusion models via stochastic differential equations-a technical tutorial. arXiv preprint arXiv:2402.07487, 2024.   \nArash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. In Advances in Neural Information Processing Systems (NeurIPS), volume 34, pages 11287-11302, 2021.   \nXinyou Wang,Zaixiang Zheng,Fe Ye,Dongyu Xue Shujian Huang, andQuanguanGuDiffusion language models are versatile protein learners. arXiv preprint arXiv:2402.18567, 2024a.   \nYan Wang, Lihao Wang, Yuning Shen, Yiqun Wang, Huizhuo Yuan, Yue Wu, and Quanquan Gu. Protein conformation generation via force-guided se (3) diffusion models. arXiv preprint arXiv:2403.14088, 2024b.   \nYihan Wang, Jatin Chauhan, Wei Wang, and Cho-Jui Hsieh. Universality and limitations of prompt tuning. Advances in Neural Information Processing Systems (NeurIPS), 36, 2023.   \nAndre Wibisono, Yihong Wu, and Kaylee Yingxi Yang. Optimal score estimation via empirical bayes smoothing. arXiv preprint arXiv:2402.07747, 2024.   \nVirginia Vassilevska Williams. On some fine-grained questions in algorithms and complexity. In Proceedings of the international congress of mathematicians: Rio de janeiro 2018, pages 3447- 3487. World Scientific, 2018.   \nDennis Wu, Jerry Yao-Chieh Hu, Teng- Yun Hsiao, and Han Liu. Uniform memory retrieval with larger capacity for modern hopfield models. In Forty-first International Conference on Machine Learning (ICML), 2024a.   \nDennis Wu, Jerry Yao-Chieh Hu, Weijian Li, Bo-Yu Chen, and Han Liu. STanhop: Sparse tandem hopfield model for memory-enhanced time series prediction. In The Twelfth International Conference on Learning Representations (ICLR), 2024b.   \nChulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions? In International Conference on Learning Representations (ICLR), 2020.   \nHongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. arXiv preprint arXiv:2306.09305, 2023.   \nXiangxin Zhou, Xiwei Cheng, Yuwei Yang, Yu Bao, Liang Wang, and Quanquan Gu. Decompopt: Controllable and decomposed diffusion models for structure-based molecular optimization. arXiv preprint arXiv:2403.13829, 2024a.   \nXiangxin Zhou, Dongyu Xue, Ruizhe Chen, Zaixiang Zheng, Liang Wang, and Quanquan Gu. Antigen-specific antibody design via direct energy-based preference optimization. arXiv preprint arXiv:2403.16576, 2024b.   \nZhihan Zhou, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. Dnabert-2: Efficient foundation model and benchmark for multi-species genome. arXiv preprint arXiv:2306.15006, 2023.   \nZhihan Zhou, Weimin Wu, Harrison Ho, Jiayi Wang, Lizhen Shi, Ramana V Davuluri, Zhong Wang, and Han Liu. Dnabert-s: Learning species-aware dna embedding with genome foundation models. ArXiv, 2024c.   \nZhenyu Zhu, Francesco Locatello, and Volkan Cevher. Sample complexity bounds for score-matching: Causal discovery and generative modeling. Advances in Neural Information Processing Systems (NeurIPS), 36, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A More Discussions on Low-Dimensional Linear Latent Space 17 ", "page_idx": 15}, {"type": "text", "text": "B Notation Table 18 ", "page_idx": 15}, {"type": "text", "text": "C Related Works 19 ", "page_idx": 15}, {"type": "text", "text": "D  Supplementary Theoretical Background 21 ", "page_idx": 15}, {"type": "text", "text": "D.1 Diffusion Models. 21   \nD.2Proof of Lemma 2.1 . 21   \nD.3 Preliminaries: Strong Exponential Time Hypothesis (SETH) and Tensor Trick . 23 ", "page_idx": 15}, {"type": "text", "text": "E More Background and Auxiliary Lemmas: Universal Approximation of Transformers via Piecewise Approximation 25 ", "page_idx": 15}, {"type": "text", "text": "E.1 Piecewise-Constant Function Approximates Compact-Supported Continuous Function 25   \nE.2  Modified Transformer Approximates Piecewise-constant Function . . . 26 E.2.1 Quantization by Modified Feed-forward Layers . . . 27 E.2.2  Contextual Mapping by Modified Self-attention Layers . . . 28 E.2.3 Map to the Desired Output by Modified Feed-forward Layers 29   \nE.3 Standard Transformers Approximate Modified Transformers . . 29   \nE.4 All Together: Standard Transformers Approximate Compact-supported Continuous Functions . . 29   \nE.5 Supplementary Proofs . . . . 30 E.5.1  Preliminaries 30 E.5.2 Proof of Lemma E.2 227 E.5.3 Proof of Lemma E.4 E.5.4Proof of Lemma E.5 E.5.5Proof of Lemma E.7 39 ", "page_idx": 15}, {"type": "text", "text": "F Proofs of Section 3 40 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "F.1Proof of Theorem 3.1 40   \nF.1.1 Auxiliary Lemmas for Theorem 3.1. 40   \nF.1.2 Main Proof of Theorem 3.1 41   \nF.2 Proof of Theorem 3.2 46   \nF.2.1 Auxiliary Lemmas for Theorem 3.2 46   \nF.2.2 Proof of Theorem 3.2 . 47   \nF.3Proof of Corollary 3.2.1 . . 52   \nF.3.1 Auxiliary Lemmas 52   \nF.3.2 Main Proof of Corollary 3.2.1 54 ", "page_idx": 15}, {"type": "text", "text": "G Proofs of Section 4 55 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "G.1  Auxiliary Theoretical Results for Theorem 4.1 . 55   \nG.1.1 Low-Rank Decomposition of DiT Gradients . . 55   \nG.1.2  Low-Rank Approximations of Building Blocks Part I: $f(\\cdot),q(\\cdot)$ , and $c(\\cdot)$ 58   \nG.1.3  Low-Rank Approximations of Building Blocks Part II: $p(\\cdot)$ . 59   \nG.2 Proof of Theorem 4.1 61 ", "page_idx": 15}, {"type": "text", "text": "A More Discussions on Low-Dimensional Linear Latent Space ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Our analysis is based on the low-dimensional linear latent space assumption (Assumption 2.1). Here we further discuss this in light of our theoretical results ", "page_idx": 16}, {"type": "text", "text": "Our results are more general and extend beyond Assumption 2.1. In addition to the case where $d_{0}<D$ , our theoretical results apply to two other settings: $d_{0}=D$ and $d_{0}>D$ . Especially, for $d_{0}=D$ , our results still hold by setting $B$ as the identity matrix $I_{D}$ . Namely, our results hold after removing the linear subspace assumption. ", "page_idx": 16}, {"type": "text", "text": "\u00b7 Statistically, for score approximation, score estimation, and distribution estimation, the upper bounds depend on the dimension of the latent variable $d_{0}$ ,other than $d$ .A smaller $d_{0}$ allows for a reduced model size to achieve a specified approximation error compared to a larger one (Theorem 3.1). Additionally, with a smaller $d_{0}$ , both score and distribution estimation errors are reduced relative to scenarios with larger ones (Theorem 3.2 and Corollary 3.2.1). \u00b7 Computationally, smaller $d_{0}$ benefits the provably efficient criteria (Proposition 4.1, almost-linear time algorithms for forward inference (Proposition 4.2) and backward computation (Theorem 4.1). ", "page_idx": 16}, {"type": "text", "text": "B Notation Table ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We summarize our notations in the following table for easy reference. ", "page_idx": 17}, {"type": "table", "img_path": "cV2LKBdlz4/tmp/e8e54a92a7df66e6dcdaff36b8f6eff6027abfb1cc59509c6c563032451778d4.jpg", "table_caption": ["Table 1: Mathematical Notations and Symbols "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "C Related Works ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Organization. In the following, we first discuss recent developments in DiTs. Then, we discuss the main technique of our statistical results: the universality (universal approximation) of transformer. Next, we discuss recent theoretical developments in diffusion generative models. Lastly, we discuss other aspects of transformer in foundation models beyond diffusion models. ", "page_idx": 18}, {"type": "text", "text": "Diffusion Transformers. Diffusion [Ho et al., 2020] and score-based generative models [Song and Ermon, 2019] have been particularly successful as generative models of images, video and biomedical data [Nichol et al., 2021, Ramesh et al., 2022, Liu et al., 2024, Zhou et al., 2024a,b, Wang et al., 2024a,b]. Recently, transformer-based diffusion models have garnered significant attention in research. The U-ViT model [Bao et al., 2022] incorporates transformer blocks into a U-net architecture, treating all inputs as tokens. In contrast, DiT [Peebles and Xie, 2023] utilizes a straightforward, non-hierarchical transformer structure. Empirically, diffusion transformers (DiTs) [Peebles and Xie, 2023] have emerged as a significant advancement (e.g., SoRA [OpenAI, 2024, Liu et al., 2024] from OpenAI), effectively combining the strengths of transformer architectures and diffusion-based approaches. Models like MDT [Gao et al., 2023a] and MaskDiT [Zheng et al., 2023] improve the training efficiency of DiT by applying a masking strategy. ", "page_idx": 18}, {"type": "text", "text": "Universality and Memory Capacity of Transformers.  The universality of transformers refers to their ability to serve as universal approximators. This means that transformers theoretically models any sequence-to-sequence function to a desired degree of accuracy. Yun et al. [2020] establish that transformers can universally approximate sequence-to-sequence functions by stacking numerous layers of feed-forward functions and self-attention functions. In a different approach, Jiang and Li [2023] affirm the universality of transformers by utilizing the Kolmogorov-Albert representation Theorem. Most recently, Kajitsuka and Sato [2023] show that transformers with one self-attention layer is a universal approximator. ", "page_idx": 18}, {"type": "text", "text": "The memory capacity of a transformer is a practical measure to test the theoretical results of the transformer's universality, by ensuring the model can handle necessary context and dependencies. By memory capacity, we refer to the minimal set of parameters such that the model (i.e., transformer) approximates all input-output pairs in the training dataset with a bounded error. Several works address the memory capacity of transformers. Kim et al. [2022] show that transformers with $\\widetilde{O}(d\\!+\\!L\\!+\\!\\sqrt{N L})$ parameters are sufficient to memorize $N$ length- $L$ and dimension- $d$ sequence-to-sequence data points by constructing a contextual mapping with $\\mathcal{O}(L)$ attention layers. Mahdavi et al. [2023] show that a multi-head-attention with $h$ heads is able to memorize $\\mathcal{O}(h L)$ examples under a linear independence data assumption. Kajitsuka and Sato [2023] show that a single layer transformer with $\\mathcal{O}(N\\dot{L}d+d^{2})$ parameters is able to memorize $N$ length- $L$ and dimension- $d$ sequence-to-sequence data points by utilizing the connection between the softmax function and Boltzmann operator. Anonymous [2024a], Wang et al. [2023] extend the results of [Kajitsuka and Sato, 2023, Yun et al., 2020] to prompt tuning and discuss the memorization of the data sequences. Another line of research establishes a different kind of memory capacity for transformers by connecting transformer attention with dense associative memory models (modern Hopfield models) [Hu et al., 2024a,b,d, 2023, Wu et al., $2024\\mathrm{a},\\mathrm{b}$ Ramsauer et al., 2020]. Notably, they define memory capacity as the smallest number of (length- $L$ and dimension- $\\cdot d$ ) data points the model (transformer attention) is able to store and derive exponential-in- $d$ high-probability capacity lower bounds. In particular, Hu et al. [2024d] report a tight exponential scaling of capacity with feature dimension from the perspective of spherical codes. ", "page_idx": 18}, {"type": "text", "text": "Our work is motivated by and builds on [Yun et al., 2020] to bridge the transformer's function approximation ability with data distribution estimation. While we do not address the memorization of DiTs (or diffusion models in general), recent studies on dense associative models suggest viewing pretrained diffusion generative models as associative memory models [Achilli et al., 2024, Ambrogioni, 2023, Hoover et al., 2023]. We plan to explore this aspect in future work. ", "page_idx": 18}, {"type": "text", "text": "Theories of Diffusion Models. In addition to empirical success, there has been several theoretical analysis about diffusion models [Chen et al., 2024b, Tang and Zhao, 2024]. Chen et al. [2023] studies score approximation, estimation, and distribution recovery of U-Net based diffusion models. Benton et al. [2024] provide convergence bounds linear in data dimensions, assuming accurate score function approximation. Zhu et al. [2023], Wibisono et al. [2024] provide statistical sample complexity bounds for score-matching under the similar assumptions. Oko et al. [2023] analyze the distribution estimation under the assumption that the initial density is supported on $[-1,1]^{D}$ and smooth in the boundary. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Among these works, our work is built on and closest to [Chen et al., 2023], as both assume the data has a low-dimensional structure. However, our work differs in three key aspects. First, beyond the simple ReLU networks considered in [Chen et al., 2023], we provide the first score approximation analysis for DiTs with a transformer-based score estimator. Second, our work is the first to provide the statistical rates of DiTs (score and distribution estimation) based on transformer universality [Yun et al., 2020] and norm-based converging number bound [Edelman et al., 2022], supporting the practical success of DiTs [Esser et al., 2024, Ma et al., 2024]. Lastly, our work provides the first comprehensive analysis of the computational limits and all possible efficient DiT algorithms/methods for both forward inference and backward training. This offers timely insights into the empirical computational inefficiency of DiTs [Liu et al., 2024] and guidance for future DiT architectures. ", "page_idx": 19}, {"type": "text", "text": "Transformers in Foundation Models: Transformer-Based Pretrained Models. Transformerbased pretrained models utilize attention mechanisms to process sequential data, enabling the learning of contextual relationships for tasks like natural language understanding and generation. These models encompass three types: encoder-based, decoder-based, and diffusion transformers. Encoder-based transformers, such as DNABERT [Zhou et al., 2024c, 2023, Ji et al., 2021], employ bidirectional attention to extract feature representations DNABERT shows great potential to capture complex patterns of genome sequences and improve tasks such as gene prediction. Decoder-based transformers generate output sequences from encoded information using unidirectional attention, such as ChatGPT [Radford et al., 2019, Floridi and Chiriatti, 2020, Brown et al., 2020] for natural language. The diffusion transformers generate a sequence toward a target distribution, such as SoRA [Liu et al., 2024] and Videofusion [Luo et al., 2023] for video generation and DecompDiff [Guan et al., 2024] for drug design. In our paper, we present an early exploration of the statistical and computational limits of diffusion transformer models. ", "page_idx": 19}, {"type": "text", "text": "D  Supplementary Theoretical Background ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we provide some further background. We show the details about the forward and backward process in Diffusion Models in Appendix D.1. Besides, we give the details of the proof about the score decomposition in Appendix D.2. ", "page_idx": 20}, {"type": "text", "text": "D.1  Diffusion Models ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Forward Process. Diffusion models gradually add noise to the original data in the forward process. We describe the forward process as the following SDE ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{d}x_{t}=-\\frac{1}{2}w(t)x_{t}\\mathrm{d}t+\\sqrt{w(t)}\\mathrm{d}W_{t},\\;x_{t}\\in\\mathbb{R}^{D},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $x_{0}\\sim P_{0}$ \uff0c $(W_{t})_{t\\geq0}$ is a standard Brownian motion, and $w(t)>0$ is a nondecreasing weighting function. Let $P_{t}$ and $p_{t}$ denote the marginal distribution and destiny of $x_{t}$ . The conditional distribution $P(x_{t}|\\boldsymbol{x}_{0})$ follows $N(\\beta(t)x_{0},\\sigma(t)I_{D})$ where $\\begin{array}{r}{\\beta(t)=\\exp\\Bigl(-\\int_{0}^{t}w(s)\\mathrm{d}s/2\\Bigr)}\\end{array}$ and $\\sigma(t)=1-\\beta^{2}(t)$ In practice, (D.1) terminates at a large enough $T$ such that $P_{T}$ is close to $N(0,I_{D})$ ", "page_idx": 20}, {"type": "text", "text": "Backward Process. We obtain the backward process $y_{t}:=x_{T-t}$ by reversing (D.1). The backward process satisfies ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{d}y_{t}=\\left[\\frac{1}{2}w(T-t)y_{t}+w(T-t)\\nabla\\log p_{T-t}(y_{t})\\right]\\mathrm{d}t+\\sqrt{w(T-t)}\\mathrm{d}\\overline{{W}}_{t},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the score function $\\nabla\\log{p_{t}(\\cdot)}$ is the gradient of log probability density function of $x_{t}$ , and $\\overline{{W}}_{t}$ is a reversed Brownian motion. However, $\\nabla\\log p_{t}(\\cdot)$ and $P_{T}$ are both unknown in (D.2). To resolve this, we use a score estimator $s_{W}(\\cdot,t)$ to replace $\\nabla\\log p_{t}(\\cdot)$ ,where $s_{W}(\\cdot,t)$ is usually a neural network with parameters $W$ . Secondly, we replace $P_{T}$ by the standard Gaussian distribution. Consequently, we obtain the following SDE ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{d}\\tilde{y}_{t}=\\bigg[\\frac{1}{2}w(T-t)\\tilde{y}_{t}+w(T-t)s_{W}(\\tilde{y}_{t},T-t)\\bigg]\\,\\mathrm{d}t+\\sqrt{w(T-t)}\\mathrm{d}\\overline{{W}}_{t},\\;\\tilde{y}_{0}\\sim N(0,I_{D}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In practice, we use discrete schemes of (D.3) to generate data, following [Song and Ermon, 2019]. We use $\\mu>0$ to denote the discretization step size. For $t\\in[k\\mu,(k+1)\\bar{\\mu}]$ ,wehave ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{d}\\widetilde{y}_{t}^{\\mu}=\\left[\\frac{1}{2}w(T-t)\\widetilde{y}_{k\\mu}^{\\mu}+w(T-t)s_{W}(\\widetilde{y}_{k\\mu}^{\\mu},T-k\\mu)\\right]\\mathrm{d}t+\\sqrt{w(T-t)}\\mathrm{d}\\overline{{W}}_{t}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "D.2 Proof of Lemma 2.1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Here we restate the proof of [Chen et al., 2023, Lemma 1] for completeness. ", "page_idx": 20}, {"type": "text", "text": "Proof. Recall $x=B h$ by Assumption 2.1 with $x\\in\\mathbb{R}^{D}$ \uff0c $\\boldsymbol{B}\\in\\mathbb{R}^{D\\times d_{0}}$ and $h\\in\\mathbb{R}^{d_{0}}$ ", "page_idx": 20}, {"type": "text", "text": "By the forward process (D.1), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\np_{t}(\\boldsymbol{\\overline{{x}}})=\\int\\psi_{t}(\\boldsymbol{\\overline{{x}}}\\mid B h)p_{h}(h)\\mathrm{d}h,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\psi_{t}(\\bar{x}\\mid B h)=[2\\pi h(t)]^{-D/2}\\exp\\left(-\\frac{\\|\\beta(t)B h-\\bar{x}\\|_{2}^{2}}{2\\sigma(t)}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "is the Gaussian transition kernel. ", "page_idx": 20}, {"type": "text", "text": "Then we write the score function as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla\\log p_{t}(\\bar{x})=\\frac{\\nabla p_{t}(\\bar{x})}{p_{t}(\\bar{x})}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{\\nabla\\int\\psi_{t}(\\bar{x}\\mid B h)p_{h}(h)\\mathrm{d}h}{\\int\\psi_{t}(\\bar{x}\\mid B h)p_{h}(h)\\mathrm{d}h}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{\\int\\nabla\\psi_{t}(\\bar{x}\\mid B h)p_{h}(h)\\mathrm{d}h}{\\int\\psi_{t}(\\bar{x}\\mid B h)p_{h}(h)\\mathrm{d}h},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last equality holds since $\\psi_{t}(\\bar{x}\\mid B h)$ is continuously differentiable in $\\textstyle{\\overline{{x}}}$ ", "page_idx": 21}, {"type": "text", "text": "Plugging (D.6) into (D.7), we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\nabla\\log p_{t}(\\overline{{x}})}\\\\ &{=\\frac{[2\\pi h(t)]^{-D/2}}{\\int\\psi_{t}(\\overline{{x}}\\mid B h)p_{h}(h)\\mathrm{d}h}\\int\\frac{1}{\\sigma(t)}\\left(\\beta(t)B h-\\overline{{x}}\\right)\\exp\\left(-\\frac{\\|\\beta(t)B h-\\overline{{x}}\\|_{2}^{2}}{2\\sigma(t)}\\right)p_{h}(h)\\mathrm{d}h.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We then decompose above score function by projecting of $\\textstyle{\\overline{{x}}}$ into $\\operatorname{Span}(B)$ , i.e.,replacing $-\\overline{{x}}$ with $-B B^{\\top}\\bar{x}-(I_{D}-B B^{\\top})\\bar{x}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\nabla\\log p_{t}(\\bar{x})}\\\\ &{=\\frac{[2\\pi h(t)]^{-D/2}}{\\int\\psi_{t}(\\bar{x}\\mid B h)p_{h}(h)\\mathrm{d}h}}\\\\ &{\\quad\\cdot\\int\\frac{1}{\\sigma(t)}\\Bigg[\\left(\\beta(t)B h-B B^{\\top}\\bar{x}\\right)-\\left(I_{D}-B B^{\\top}\\right)\\bar{x}\\Bigg]\\exp\\left(-\\frac{\\|\\beta(t)B h-\\bar{x}\\|_{2}^{2}}{2\\sigma(t)}\\right)p_{h}(h)\\mathrm{d}h.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Absorbing the factor of $[2\\pi h(t)]^{-D/2}$ into the Gaussian kernel $\\psi_{t}(\\bar{x}\\mid B h)$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\nabla\\log p_{t}(\\bar{x})}\\ ~}\\\\ {{\\displaystyle=\\frac{[2\\pi h(t)]^{-D/2}}{\\int\\psi_{t}(\\bar{x}\\mid B h)p_{h}(h)\\mathrm{d}h}\\int\\frac{1}{\\sigma(t)}\\left(\\beta(t)B h-B B^{\\top}\\bar{x}\\right)\\exp\\left(-\\frac{\\|\\beta(t)B h-\\bar{x}\\|_{2}^{2}}{2\\sigma(t)}\\right)p_{h}(h)\\mathrm{d}h}\\ ~}\\\\ {{\\displaystyle~~~-\\frac{1}{\\int\\psi_{t}(\\bar{x}|B h)p_{h}(h)\\mathrm{d}h}\\left(\\frac{1}{\\sigma(t)}\\left(I_{D}-B B^{\\top}\\right)\\bar{x}\\right)\\int\\psi_{t}(\\bar{x}\\mid B h)p_{h}(h)\\mathrm{d}h}\\ ~}\\\\ {{\\displaystyle=\\frac{1}{\\int\\psi_{t}(\\bar{x}\\mid B h)p_{h}(h)\\mathrm{d}h}\\int\\frac{1}{\\sigma(t)}\\left(\\beta(t)B h-B B^{\\top}\\bar{x}\\right)\\psi_{t}(\\bar{x}\\mid B h)p_{h}(h)\\mathrm{d}h}\\ {\\displaystyle\\underbrace{-\\frac{1}{\\sigma(t)}\\left(I_{D}-B B^{\\top}\\right)\\bar{x}}_{=s_{-}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To further simplify $s_{+}$ , we decompose $\\psi_{t}(\\bar{x}\\mid B h)$ as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\psi_{t}(\\bar{x}\\mid B h)}\\\\ &{=[2\\pi h(t)]^{-D/2}\\exp\\left(-\\frac{1}{2\\sigma(t)}\\|\\beta(t)B h-\\bar{x}\\|_{2}^{2}\\right)}\\\\ &{=[2\\pi h(t)]^{-D/2}\\exp\\left(-\\frac{1}{2\\sigma(t)}\\|\\beta(t)B h-B B^{\\top}\\bar{x}-\\left(I_{D}-B B^{\\top}\\right)\\bar{x}\\|_{2}^{2}\\right)}\\\\ &{=[2\\pi h(t)]^{-D/2}\\exp\\left(-\\,\\frac{1}{2\\sigma(t)}\\Big(\\|\\beta(t)B h-B B^{\\top}\\bar{x}\\|_{2}^{2}+\\big\\|\\big(I_{D}-B B^{\\top}\\big)\\bar{x}\\|_{2}^{2}\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.-\\,2(B(\\beta(t)h-B^{\\top}\\bar{x}))^{\\top}(I_{D}-B B^{\\top})\\bar{x}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=[2\\pi h(t)]^{-D/2}\\exp\\left(-\\frac{1}{2\\sigma(t)}\\left(\\left\\|\\beta(t)B h-B B^{\\top}\\overline{{x}}\\right\\|_{2}^{2}+\\left\\|\\left(I_{D}-B B^{\\top}\\right)\\overline{{x}}\\right\\|_{2}^{2}\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left(B(\\beta(t)h-B^{\\top}\\overline{{x}})\\operatorname{is\\,in}\\mathrm{Span}(B)\\operatorname{while}\\left(I_{D}-B B^{\\top}\\right)\\overline{{x}}\\operatorname{is\\,orthogonal}\\operatorname{to}\\mathrm{Span}(B)\\right)}\\\\ &{=\\underbrace{\\left[2\\pi h(t)\\right]^{-d_{0}/2}\\exp\\left(-\\frac{\\left\\|\\beta(t)h-B^{\\top}\\overline{{x}}\\right\\|_{2}^{2}}{2\\sigma(t)}\\right)}_{:=\\psi_{t}(B^{\\top}\\overline{{x}}|h)}\\cdot\\underbrace{\\left[2\\pi h(t)\\right]^{-(D-d_{0})/2}\\exp\\left(-\\frac{\\left\\|\\left(I_{D}-B B^{\\top}\\right)\\overline{{x}}\\right\\|_{2}^{2}}{2\\sigma(t)}\\right)}_{:=\\psi_{t}((I_{D}-B B^{\\top})\\overline{{x}})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where both $\\psi_{t}\\left(B^{\\top}\\bar{x}\\mid h\\right)$ and $\\psi_{t}\\left((I_{D}-B B^{\\top})\\bar{x}\\right)$ are Gaussian. ", "page_idx": 22}, {"type": "text", "text": "Plugging $\\psi_{t}(\\bar{\\boldsymbol{x}}\\mid B\\boldsymbol{h})=\\psi_{t}\\left(B^{\\top}\\bar{\\boldsymbol{x}}\\mid\\boldsymbol{h}\\right)\\psi_{t}\\left((I_{D}-B B^{\\top})\\bar{\\boldsymbol{x}}\\right)$ into $s_{+}$ , we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{s_{+}(\\bar{x},t)=C\\int\\frac{1}{\\sigma(t)}\\left(\\beta(t)B h-B B^{\\top}\\bar{x}\\right)\\psi_{t}(B^{\\top}\\bar{x}\\mid h)\\psi_{t}((I_{D}-B B^{\\top})\\bar{x})p_{h}(h)\\mathrm{d}h}}\\\\ &{}&{=C\\psi_{t}((I_{D}-B B^{\\top})\\bar{x})\\int\\frac{1}{\\sigma(t)}\\left(\\beta(t)B h-B B^{\\top}\\bar{x}\\right)\\psi_{t}(B^{\\top}\\bar{x}\\mid h)p_{h}(h)\\mathrm{d}h}\\\\ &{}&{=\\frac{1}{\\int\\psi_{t}(B^{\\top}\\bar{x}\\mid h)p_{h}(h)\\mathrm{d}h}\\int\\frac{1}{\\sigma(t)}\\left(\\beta(t)B h-B B^{\\top}\\bar{x}\\right)\\psi_{t}(B^{\\top}\\bar{x}\\mid h)p_{h}(h)\\mathrm{d}h,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$\\begin{array}{r}{C:=[\\psi_{t}((I_{D}-B B^{\\top})\\bar{x})\\int\\psi_{t}(B^{\\top}\\bar{x}\\mid h)p_{h}(h)\\mathrm{d}h]^{-1}.}\\end{array}$ ", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Notably, $s_{+}$ depends only on the projected data $B^{\\top}\\bar{x}$ . Therefore, we are able to replace $s_{+}(\\bar{x},t)$ with $s_{+}(B^{\\top}\\bar{x},t)$ . The benefit is that the dimension $d_{0}$ of the first input in $s_{+}(B^{\\top}\\bar{x},t)$ is much smaller. ", "page_idx": 22}, {"type": "text", "text": "Lastly, by denoting $\\overline{{h}}=B^{\\top}\\overline{{x}}$ such that $\\nabla_{\\overline{{h}}}\\psi_{t}(\\overline{{h}}\\mid h)=(\\beta(t)h-\\overline{{h}})\\psi_{t}(\\overline{{h}}\\mid h)/\\sigma(t)$ , we arrive at ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{s_{+}(B^{\\top}\\bar{x},t)=B\\displaystyle\\int\\frac{\\nabla_{\\bar{h}}\\psi_{t}(\\bar{h}\\mid h)p_{h}(h)}{\\int\\psi_{t}(\\bar{h}\\mid h)p_{h}(h)\\mathrm{d}h}\\mathrm{d}h}\\\\ &{\\qquad\\qquad=B\\nabla\\log p_{t}^{h}(B^{\\top}x).}&&{\\qquad\\big(p_{t}^{h}(\\bar{h}):=\\int\\psi_{t}(\\bar{h}|h)p_{h}(h)\\mathrm{d}h\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This completes the proof. ", "page_idx": 22}, {"type": "text", "text": "D.3 Preliminaries: Strong Exponential Time Hypothesis (SETH) and Tensor Trick ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Here we present the ideas we built upon for Section 4. ", "page_idx": 22}, {"type": "text", "text": "Strong Exponential Time Hypothesis (SETH). Impagliazzo and Paturi [2001] introduce the Strong Exponential Time Hypothesis (SETH) as a stronger form of the ${\\mathsf{P}}\\neq{\\mathsf{N P}}$ conjecture.It suggests that our current best SAT algorithms are optimal and is a popular conjecture for proving fine-grained lower bounds for a wide variety of algorithmic problems [Cygan et al., 2016, Williams, 2018]. ", "page_idx": 22}, {"type": "text", "text": "Hypothesis 1 (SETH). For every $\\epsilon>0$ there is a positive integer $k\\geq3$ such that $k$ -SAT on formulas With $n$ variables cannot be solved in $O(2^{(1-\\epsilon)n})$ time, even by a randomized algorithm. ", "page_idx": 22}, {"type": "text", "text": "Tensor Trick for Computing Gradients. The tensor trick [Diao et al., 2019, 2018] is an instrument to compute complicated gradients in a clean and tractable fashion. We start with some definitions. ", "page_idx": 22}, {"type": "text", "text": "Definition D.1 (Vectorization). For any matrix $X\\in\\mathbb{R}^{L\\times d}$ , we define $\\underline{{X}}:=\\mathrm{vec}\\left(X\\right)\\in\\mathbb{R}^{L d}$ such that $X_{i,j}=\\underline{{X}}_{(i-1)d+j}$ for all $i\\in[L]$ and $j\\in[d]$ ", "page_idx": 22}, {"type": "text", "text": "Definition D.2 (Matrixization). For any vector $\\underline{{X}}\\:\\in\\:\\mathbb{R}^{L d}$ , we define $\\mathrm{mat}(\\underline{{X}})\\;=\\;X$ such that $X_{i,j}=\\mathrm{mat}(\\underline{{X}}):=\\underline{{X}}_{(i-1)d+j}$ for all $i\\in[L]$ and $j\\in[d]$ , namely $\\mathrm{mat}(\\cdot)=\\mathrm{vec}^{-1}(\\cdot)$ ", "page_idx": 22}, {"type": "text", "text": "Definition D.3 (Kronecker Product). Let $\\boldsymbol{A}\\in\\mathbb{R}^{L_{a}\\times d_{a}}$ and $\\boldsymbol{B}\\in\\mathbb{R}^{L_{b}\\times d_{b}}$ . We define the Kronecker productofAadBasABdd suchthat(B(i+i(1\uff09isqual t $A_{i_{a},j_{a}}B_{i_{b},j_{b}}$ with $i_{a}\\in[L_{a}],j_{a}\\in[d_{a}],i_{b}\\in[L_{b}],j_{b}\\in[d_{b}]$ ", "page_idx": 22}, {"type": "text", "text": "Definition D.4 (Sub-Block of a Tensor). For any $A\\in\\mathbb{R}^{L_{a}\\times d_{a}}$ and $\\boldsymbol{B}\\in\\mathbb{R}^{L_{b}\\times d_{b}}$ , let $\\mathsf{A}:=A\\otimes B\\in$ $\\mathbb{R}^{L_{a}L_{b}\\times d_{a}d_{b}}$ For any $\\underline{{j}}\\in[L_{a}]$ , we define $\\mathsf{A}_{\\underline{{j}}}\\in\\dot{\\mathbb{R}}^{L_{b}\\times d_{a}d_{b}}$ be the $\\underline{{j}}$ $L_{b}\\times d_{a}d_{b}$ sub-block of A. ", "page_idx": 23}, {"type": "text", "text": "Lemma D.1 (Tensor Trick [Diao et al., 2019, 2018]). For any $A\\,\\in\\,\\mathbb{R}^{L_{a}\\times d_{a}}$ $\\boldsymbol{B}\\,\\in\\,\\mathbb{R}^{L_{b}\\times d_{b}}$ and $X\\in\\mathbb{R}^{d_{a}\\times d_{b}}$ , it holds vec $\\left(A^{\\top}X B\\right)=(A^{\\top}\\otimes B^{\\top})\\underline{{X}}\\in\\mathbb{R}^{L_{a}L_{b}}$ ", "page_idx": 23}, {"type": "text", "text": "To showcase the tensor trick, let's consider a (single data point) attention following [Gao et al., 2023b,c]. Setting $D:=\\operatorname{diag}\\left(\\exp\\bigl(X^{\\mathsf{T}}W_{K}^{\\mathsf{T}}W_{Q}\\dot{X}\\bigr)\\overset{\\sim}{\\mathbb{1}_{L}}\\right)$ and $\\mathring{W}:=\\mathbf{\\mathring{W}}_{K}W_{Q}^{\\mathsf{T}}\\in\\mathbb{R}^{d\\times d}$ we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{L}_{0}:=\\big\\|\\underbrace{W_{V}}_{d\\times d}\\underbrace{X}_{\\in\\mathbb{R}^{d\\times L}}\\underbrace{D^{-1}}_{\\in\\mathbb{R}^{L\\times L}}\\underbrace{\\exp\\bigl\\{X^{\\mathsf{T}}W X\\bigr\\}}_{\\in\\mathbb{R}^{L\\times L}}-\\underbrace{Y}_{\\in\\mathbb{R}^{d\\times L}}\\big\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proposition D.1 (Definition 4.7 of [Gao et al., 2023b]). By Definition D.3 and Definition D.4, we identify $D_{\\underline{{j}},\\underline{{j}}}\\;\\;:=\\;\\;\\Big\\langle\\mathrm{exp}\\Big(\\mathsf{A}_{\\underline{{j}}}\\,\\underline{{W}}\\Big),\\mathbb{1}_{L}\\Big\\rangle\\;\\;\\in\\;\\;\\mathbb{R}$ for all $\\underline{{j}}\\ \\in\\ [L]$ , with $\\textsf{A}:=\\ X\\ \\otimes\\ X\\ \\in$ $\\mathbb{R}^{L^{2}\\times d^{2}}$ and $\\underline{{W}}\\quad\\in\\quad\\mathbb{R}^{d^{2}}$ .Therefore, for each $\\begin{array}{r l r}{\\underline{{j}}}&{{}\\in}&{[L]}\\end{array}$ and $\\begin{array}{r l r}{\\underline{{i}}}&{{}\\in}&{[d]}\\end{array}$ , it holds $\\begin{array}{r l}{\\mathcal{L}_{0}}&{{}=}\\end{array}$ $\\begin{array}{r}{\\sum_{\\underline{{j}}=1}^{L}\\sum_{\\underline{{i}}=1}^{d}\\frac{1}{2}\\left(\\left\\langle D_{\\underline{{j}},\\underline{{j}}}^{-1}\\exp\\!\\left(\\mathsf{A}_{\\underline{{j}}}\\frac{W}{\\underline{{K}}}\\right),X W_{V}[\\cdot,\\underline{{i}}]\\right\\rangle-Y_{\\underline{{j}},\\underline{{i}}}\\right)^{2};}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "The elegance of Proposition D.1 emerges when we vectorize the weights into vectors ${\\underline{{W}}},{\\underline{{W}}}_{V}$ , making the gradient computations (e.g., ${\\mathrm{d}}\\mathcal{L}_{0}/\\underline{{W}}$ and ${\\mathrm{d}}{\\mathcal{L}}_{0}/\\underline{{W}}_{V}.$ ) more tractable by avoiding complex matrix or tensor derivatives. This approach systematically simplifies the handling of chain-rule terms in the gradient computation of losses like ${\\mathcal{L}}_{0}$ ", "page_idx": 23}, {"type": "text", "text": "Fine-Grained Complexity for Transformer. Many recent works also utilize similar techniques from fine-grained complexity to analyze transformer architectures. Alman and Song [2023, 2024b], Liang et al. [2024d], Alman and Song [2024a] explore the computational feasibility of inference and training for standard softmax and tensor attention. Liang et al. [2024c] extend the singlelayer training results from [Alman and Song, 2024b] to deep transformer models. [Liang et al., 2024a] extend [Alman and Song, 2024b] to provide a fast attention gradient approximation based on Fourier transform. [Liang et al., 2024b] extend [Alman and Song, 2024b] to sparse attention matrix. Anonymous [2024a] study the computational limits of inference and training in prompt-tuning for pretrained transformers. Hu et al. [2024c] study the computational limits of LoRA [Hu et al., 2021] in transformers, identifying norm-bound conditions for efficient LoRA training and proving the existence of nearly linear-time LoRA algorithms. ", "page_idx": 23}, {"type": "text", "text": "Our work is closest to [Alman and Song, 2024b, 2023]. Our forward inference computational results build on [Alman and Song, 2023]. Our backward training computational results are related to [Alman and Song, 2024b] but include additional analysis on reshaping and latent embedding. ", "page_idx": 23}, {"type": "text", "text": "E  More Background and Auxiliary Lemmas: Universal Approximation of Transformers via Piecewise Approximation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Here, we review the universal approximation of transformers following [Yun et al., 2020]. ", "page_idx": 24}, {"type": "text", "text": "Our goal is to reproduce the results of [Yun et al., 2020] and use or modify them as auxiliary lemmas for proofs of Section 3 (i.e., Appendix F.) ", "page_idx": 24}, {"type": "text", "text": "We start with their central result and prove it in the rest of the section. ", "page_idx": 24}, {"type": "text", "text": "Lemma E.1 (Universal Approximation of Transformers, Theorem 3 of [Yun et al., 2020]). Let $\\epsilon>0$ For any given compact-supported continuous function $f:\\mathbb{R}^{d\\times L}\\rightarrow\\mathbb{R}^{d\\times L}$ , there exists a transformer networkfTET21.4,suchthat ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left(\\int\\|f_{\\mathcal{T}}(X)-f(X)\\|_{F}^{2}\\mathrm{d}X\\right)^{1/2}\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof Overview.   We use the following proof strategy: ", "page_idx": 24}, {"type": "text", "text": "\u00b7 Step 1. We show that the piecewise-constant function is able to approximate compact-supported continuous function in Appendix E.1.   \n\u00b7 Step 2. We define modified self-attention and feed-forward layers to construct the modified transformer. We show that the modified transformer is able to approximate piecewise-constant function in Appendix E.2.   \n\u00b7 Step 3. We show that the modified transformer is able to approximate the normal transformer in AppendixE.3. We provide details of Step 1. in Appendix E.1, Step 2. in Appendix E.2, and Step 3. in Appendix E.3.   \nThen we summarize our results in Appendix E.4. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "E.1 Piecewise-Constant Function Approximates Compact-Supported Continuous Function ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this subsection, we show that the piecewise-constant function is able to approximate compactsupported continuous function. ", "page_idx": 24}, {"type": "text", "text": "We start with the definition of the compact-supported continuous functions of interest. ", "page_idx": 24}, {"type": "text", "text": "Assumption E.1. Without loss of generality, we assume that the target function in discussion is supportedon $[0,1]^{d\\times L}$ Wedenotetheset of $[0,1]^{d\\times L}$ supported continuous functions as $\\mathcal{F}$ ", "page_idx": 24}, {"type": "text", "text": "We introduce the notion of grid and cube for the compact support $[0,1]^{d\\times L}$ ", "page_idx": 24}, {"type": "text", "text": "Definition E.1 (Grid and Cube with Width $\\delta$ ). Given a grid width $\\delta$ , let $\\mathcal{G}_{\\delta}:=\\{0,\\delta,\\dots,1-\\delta\\}^{d\\times L}$ denote the set of grids within $[0,1]^{d\\times L}$ . For a grid point $G=(G_{j\\in[d],k\\in[L]})\\in\\mathcal{G}_{\\delta}$ , we denote its associated cube as ", "page_idx": 24}, {"type": "equation", "text": "$$\nS_{G}:=\\otimes_{j=1}^{d}\\otimes_{k=1}^{L}\\left[G_{j,k},G_{j,k}+\\delta\\right)\\subset[0,1]^{d\\times L}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Eachcube $S_{G}$ represents a hyper rectangular in the multi-dimensional space $[0,1]^{d\\times L}$ , constructed to discretize the space into smaller subspaces. ", "page_idx": 24}, {"type": "text", "text": "We introduce the notion of piecewise-constant fucntion class w.r.t. the $[0,1]^{d\\times L}$ -supported continuous function class $\\mathcal{F}$ ", "page_idx": 24}, {"type": "text", "text": "Definition E.2 (Piecewise-Constant Function Class). Let $f_{\\delta}$ denote the piesewise constant function of grid width $\\delta$ and $\\mathbb{1}\\{\\cdot\\}$ denote the indicator function. For each $G\\in\\mathcal G_{\\delta}$ , and any matrix $A_{G}\\in\\mathbb{R}^{d\\times L}$ \uff0c we define the piecewise-constant function class as ", "page_idx": 24}, {"type": "equation", "text": "$$\n{\\mathcal{F}}(\\delta):=\\left\\{f_{\\delta}:X\\to\\sum_{G\\in{\\mathcal{G}}_{\\delta}}A_{G}\\cdot\\mathbb{1}\\{X\\in{\\mathcal{S}}_{G}\\},A_{G}\\in\\mathbb{R}^{d\\times L}\\right\\}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We recall that for a given sequence-to-sequence function $f$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|f\\|_{L^{2}}:=\\bigg(\\int\\|f(X)\\|_{F}^{2}\\mathrm{d}X\\bigg)^{1/2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We approximate the compact-supported function with a piecewise-constant function in the next lemma. ", "page_idx": 25}, {"type": "text", "text": "Lemma E.2. (Lemma 8 of [Yun et al., 2020]) For any given $f\\in\\mathcal F$ and $\\epsilon/3>0$ , we can find a $\\delta^{\\star}>0$ , such that there exists a $f_{\\delta^{\\star}}\\in\\mathcal{F}(\\delta^{\\star})$ satisfying $\\|f-f_{\\delta^{\\star}}\\|_{L^{2}}\\leq\\epsilon/3$ ", "page_idx": 25}, {"type": "text", "text": "Proof. See Appendix E.5.2 for a detailed proof. ", "page_idx": 25}, {"type": "text", "text": "E.2  Modified Transformer Approximates Piecewise-constant Function ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this subsection, we define modified self-attention and feed-forward layers to construct the modified transformers. We use the modified transformers to approximate the piecewise-constant function. ", "page_idx": 25}, {"type": "text", "text": "Defnition E.3 (Modifed Transformer Networks). The modifed transformer network $\\overline{{T}}_{p}^{r,m,l}$ includes two modifications to the standard transformer network $T_{p}^{r,m,l}$ ", "page_idx": 25}, {"type": "text", "text": "\u00b7 Modified attention layer: Replace Softmax operator with Hardmax operator $\\sigma_{H}(\\cdot)$ ", "page_idx": 25}, {"type": "text", "text": "\u00b7 Modified feed-forward layer: Replace ReLU() with an activation function $\\zeta\\,\\in\\,\\Psi$ .Here, $\\Psi$ denotes the set of all piecewise linear functions with at most three pieces and at least one constant. ", "page_idx": 25}, {"type": "text", "text": "We approximate ${\\mathcal{F}}(\\delta)$ with this modified transformer networks $\\overline{{T}}_{p}^{r,m,l}$ ", "page_idx": 25}, {"type": "text", "text": "Lemma E.3 (Modified from Proposition 4 of [Yun et al., 2020]). For each $f_{\\delta}\\in\\mathcal{F}(\\delta)$ , there exists a $f_{T,c}\\in\\overline{{\\mathcal{T}_{p}^{2,1,1}}}$ such that $\\|f_{\\delta}-f_{\\mathcal{T},c}\\|_{L^{2}}=\\mathcal{O}(\\delta^{d/2})$ ", "page_idx": 25}, {"type": "text", "text": "Proof Sketch. Given $\\delta$ , and for any grid $G\\in\\mathcal G_{\\delta}$ , we have a grid set $\\mathcal{G}_{\\delta}$ and the cube $S_{G}$ ", "page_idx": 25}, {"type": "text", "text": "Our proof follows two steps: ", "page_idx": 25}, {"type": "text", "text": "\u00b7 Quantization. For all $X\\in\\mathbb{R}^{d\\times L}$ , we quantize it to a fnite set: -If $X\\in S_{G}\\subset[0,1]^{d\\times L}$ , we quantize it to the element $G\\in\\mathcal G_{\\delta}$ -If $X\\not\\in[0,1]^{d\\times L}$ , we quantize it to an element out of $\\mathcal{G}_{\\delta}$ ", "page_idx": 25}, {"type": "text", "text": "\u00b7 Mapping. For any $G\\in\\mathcal G_{\\delta}$ , we map it to the desired output $A_{G}$ ", "page_idx": 25}, {"type": "text", "text": "For Quantization, we achieve this by a series of modified feed-forward layers. We show this in Appendix E.2.1. ", "page_idx": 25}, {"type": "text", "text": "For Mapping, we follow two steps: ", "page_idx": 25}, {"type": "text", "text": "\u00b7For any $G\\neq G^{\\prime}\\in\\mathcal{G}_{\\delta}$ , we use a \u201ccontextual mapping\" $q_{c}(\\cdot)$ (defined as Definition E.4). The mapping maps all the elements in $q_{c}(G)$ and $q_{c}(G^{\\prime})$ to different values. Then, we use a series of modified self-attention layers to achieve \u201ccontextual mapping\". We show this in Appendix E.2.2. ", "page_idx": 25}, {"type": "text", "text": "Definition E.4 (Contextual Mapping). Consider a finite set $\\mathcal{G}_{\\delta}\\in\\mathbb{R}^{d\\times L}$ . A map $q_{c}:\\mathcal{G}_{\\delta}\\to\\mathbb{R}^{1\\times L}$ defines a contextual mapping if the map satisfies the following:   \n- For any $G\\in\\mathcal G_{\\delta}$ , the entries in $q_{c}(G)$ are all distinct.   \n- For any $G\\neq G^{\\prime}\\in\\mathcal{G}_{\\delta}$ , all entries of $q_{c}(G)$ and $q_{c}(G^{\\prime})$ are distinct. ", "page_idx": 25}, {"type": "text", "text": "\u00b7For any $G\\in\\mathcal G_{\\delta}$ , we use a series of modified feed-forward layers to map $q_{c}(G)$ to $A_{G}$ Weshow this in Appendix E.2.3. ", "page_idx": 25}, {"type": "text", "text": "Remark E.1. Our proof differs from [Yun et al., 2020] in one aspect: Although [Yun et al., 2020, Proposition 4] outlines a proof for transformer networks without positional encoding and sketches the proof for networks with it, we provide a detailed proof for the latter to support our proof. ", "page_idx": 26}, {"type": "text", "text": "E.2.1 Quantization by Modified Feed-forward Layers ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We use a series of modied fe-forward layers n $\\overline{{T}}_{p}^{r,m,l}$ to quantize an input $X\\,\\in\\,\\mathbb{R}^{d\\times L}$ to an element $G$ of the following grid: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\{-J,0,\\delta,\\dots,1-\\delta\\}^{d\\times L},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $J>L>0$ is a large number to be determined later. We achieve this via two steps. ", "page_idx": 26}, {"type": "text", "text": "\u00b7 Step 1: Map the element out of $[0,1)$ to $-J$ ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Weuse $e_{i}$ to represent the standard unit vector where the $i$ -th element is 1. For the $i$ -throwof $X$ we define the following feed-forward layer to achieve our aim. ", "page_idx": 26}, {"type": "text", "text": "Definition E.5 (Feed-forward Layer 1). The vector $e_{i}$ acts as the weight parameters, and $\\zeta_{1}(\\cdot)$ acts as the activation function in the feed-forward layer ", "page_idx": 26}, {"type": "equation", "text": "$$\nX\\to X+e_{i}\\zeta_{1}(e_{i}^{\\top}X),\\;\\;\\zeta_{1}(t)={\\left\\{\\begin{array}{l l}{-t-J,}&{{\\mathrm{for~}}t<0{\\mathrm{~or~}}t\\geq1,}\\\\ {0,}&{{\\mathrm{otherwise.}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Wetake $i=1$ as an example to give the specific calculation. Let $X=(x_{i,j})_{d\\times L}$ , then we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname{FF}(X)=X+\\left(\\begin{array}{c}{1}\\\\ {0}\\\\ {\\vdots}\\\\ {0}\\end{array}\\right)(\\zeta_{1}(x_{1,1})}&{\\zeta_{1}(x_{1,2})}&{\\cdots}&{\\zeta_{1}(x_{1,L}))}\\\\ &{=X+\\left(\\begin{array}{c c c c}{\\zeta_{1}(x_{1,1})}&{\\zeta_{1}(x_{1,2})}&{\\cdots}&{\\zeta_{1}(x_{1,L})}\\\\ {0}&{0}&{\\cdots}&{0}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}\\\\ {0}&{0}&{\\cdots}&{0}\\end{array}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "In the first row of $X$ , the above layer transforms the element that is out of $[0,1)$ to $-J$ ", "page_idx": 26}, {"type": "text", "text": "We stack the above layers together for $i=1,2,\\ldots,d$ . If the element of $X$ is out of $[0,1)$ , the series of layers maps it to $J$ ", "page_idx": 26}, {"type": "text", "text": "\u00b7 Step 2: Map the element in $[0,1)$ to $\\{0,\\delta,2\\delta,.\\,.\\,.\\,,1-\\delta\\}$ ", "page_idx": 26}, {"type": "text", "text": "For the $i$ -th row of $X$ , we take $k=0,1,\\ldots,1/\\delta-1$ respectively. We defne the following layer ", "page_idx": 26}, {"type": "text", "text": "Definition E.6 (Feed-forward Layer 2). The vector $e_{i}$ acts as the weight parameters and $\\zeta_{2}(\\cdot)$ acts as the activation function in the feed-forward layer ", "page_idx": 26}, {"type": "equation", "text": "$$\nX\\rightarrow X+e_{i}\\zeta_{2}(e_{i}^{\\top}X-k\\delta\\mathbb{1}_{n}^{\\top}),\\;\\;\\zeta_{2}(t)=\\left\\{0,\\begin{array}{l l}{t<0\\;\\mathrm{or}\\;t\\geq\\delta,}\\\\ {-t,}&{0\\leq t<\\delta.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Wetake $i=1$ and $k=1$ as an example. We give the following specific calculation ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{FF}(X)=X+\\left(\\begin{array}{c c c c}{1}\\\\ {0}\\\\ {\\vdots}\\\\ {0}\\end{array}\\right)\\left(\\zeta_{2}(x_{1,1}-\\delta)\\quad\\zeta_{2}(x_{1,2}-\\delta)\\quad\\cdots\\quad\\zeta_{2}(x_{1,L}-\\delta)\\right)}\\\\ &{=X+\\left(\\begin{array}{c c c c}{\\zeta_{2}(x_{1,1}-\\delta)}&{\\zeta_{2}(x_{1,2}-\\delta)}&{\\cdots}&{\\zeta_{2}(x_{1,L}-\\delta)}\\\\ {0}&{0}&{\\cdots}&{0}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}\\\\ {0}&{0}&{\\cdots}&{0}\\end{array}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "In the first row of $X$ , the above layer transforms the element in $[\\delta,2\\delta]$ to $\\delta$ ", "page_idx": 27}, {"type": "text", "text": "We stack the above layers together for $i=1,2,\\ldots,d$ and $k=0,1,\\ldots,1/\\delta-1$ . If the element of $X$ is in $[k\\delta,(k+1)\\delta]$ , the series layers maps it to $k\\delta$ ", "page_idx": 27}, {"type": "text", "text": "Combining the above two parts, we achieve our goal with $d/\\delta+d$ feed-forward layers. We denote the $d/\\delta+\\bar{d}$ series layers as $f_{T,c1}$ ", "page_idx": 27}, {"type": "text", "text": "E.2.2 Contextual Mapping by Modified Self-attention Layers ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In our attention layers, we use the following positional encoding $E\\in\\mathbb{R}^{d\\times L}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\nE=\\left(\\begin{array}{c c c c c}{0}&{1}&{2}&{\\cdot\\cdot\\cdot}&{L-1}\\\\ {0}&{1}&{2}&{\\cdot\\cdot\\cdot}&{L-1}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{}&{\\vdots}\\\\ {0}&{1}&{2}&{\\cdot\\cdot}&{L-1}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "According to Appendix E.2.1, the output of $f_{T,c1}$ is in the grid $\\{-J,0,\\delta,\\dots,1-\\delta\\}^{d\\times L}$ . For any $X$ in this grid, the first column of $X+E$ is in ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\{-J,0,\\delta,\\ldots,1-\\delta\\}^{d},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and the second column is in ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\{-J+1,1,1+\\delta,\\ldots,2-\\delta\\}^{d}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The results are similar in the other columns. ", "page_idx": 27}, {"type": "text", "text": "For $i=0,1,\\ldots,L-1$ , we use the following notation: ", "page_idx": 27}, {"type": "equation", "text": "$$\n[i:\\delta:i+1-\\delta]_{J}:=\\{i-J,i,i+\\delta,\\ldots,i+1-\\delta\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then, we define the grid $\\mathcal{G}_{\\delta}^{+}$ as the following. ", "page_idx": 27}, {"type": "text", "text": "Definition E.7 (Grid $\\mathcal{G}_{\\delta}^{+}$ ). We add $E$ to all the grid points in $\\boldsymbol{\\mathcal{G}}_{\\delta}$ to generate the modified grid $\\mathcal{G}_{\\delta}^{+}$ defined as follows: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{G}_{\\delta}^{+}:=[0:\\delta:1-\\delta]_{J}^{d}\\times[1:\\delta:2-\\delta]_{J}^{d}\\times\\cdot\\cdot\\cdot\\times[L-1:\\delta:L-\\delta]_{J}^{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Next, we show that the modified attention layer computes contextual mapping Definition E.4) for $\\mathcal{G}_{\\delta}^{+}$ .For $i=1,2,\\dots,L-1$ , we use the following notation: ", "page_idx": 27}, {"type": "equation", "text": "$$\n[i:\\delta:i+1-\\delta]:=\\{i,i+\\delta,i+2\\delta,\\ldots,i+1-\\delta\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Lemma E.4 (Modified from Lemma 6 of [Yun et al., 2020]). We consider the following subset of $\\mathcal{G}_{\\delta}^{+}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{G}}_{\\delta}:=\\underbrace{[0:\\delta:1-\\delta]^{d}\\times[1:\\delta:2-\\delta]^{d}\\times\\cdots\\times[L-1:\\delta:L-\\delta]^{d}}_{L}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Assume that $L\\ge2$ and $\\delta^{-1}\\geq2$ Then, there exist a function $f_{T,c2}:\\mathbb{R}^{d\\times L}\\rightarrow\\mathbb{R}^{d\\times L}$ composed of $\\delta^{-d}+1$ modified attention layers (Definition E.3), a vector $u\\in\\mathbb{R}^{d}$ , and two constants $t_{l},t_{r}\\in\\mathbb{R}$ $(0<t_{l}<t_{r})$ , such that $q_{c}(G)\\stackrel{*}{:}=u^{\\top}f_{T,c2}(G),G\\in\\mathcal{G}_{\\delta}^{+}$ satisfies the following properties: ", "page_idx": 27}, {"type": "text", "text": "1. For any $G\\in\\widetilde{\\mathcal{G}}_{\\delta}$ , all the entries of $q_{c}(G)$ are distinct.   \n2. For any different $G,G^{\\prime}\\!\\in\\!\\widetilde{\\mathcal{G}}_{\\delta}$ , all the entries of $q_{c}(G)$ \uff0c $q_{c}(G^{\\prime})$ are distinct.   \n3. For any $G\\in\\widetilde{\\mathcal{G}}_{\\delta}$ , all the entries of $q_{c}(G)$ are in $[t_{l},t_{r}]$   \n4. For any $G\\in\\mathcal{G}_{\\delta}^{+}\\setminus\\widetilde{\\mathcal{G}}_{\\delta}$ , all the entries of $q_{c}(G)$ are outside $[t_{l},t_{r}]$ ", "page_idx": 27}, {"type": "text", "text": "Remark E.2. Our proof differs from [Yun et al., 2020] in one aspect: The original [Yun et al., 2020, Lemma 6] does not include positional encoding (E.4). Although Yun et al. [2020] sketches the proof for networks with (E.4) in the attention layer input, we detail the proof. ", "page_idx": 28}, {"type": "text", "text": "E.2.3 Map to the Desired Output by Modified Feed-forward Layers ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Next, we show that a series of feed-forward layers map the output of modified attention layers $f_{T,c2}$ to the desired output of function $f_{\\delta^{\\star}}$ ", "page_idx": 28}, {"type": "text", "text": "Lemma E.5 (Lemma 7 of [Yun et al., 2020]). There exists a function $f_{T,c3}:\\mathbb{R}^{d\\times L}\\,\\to\\,\\mathbb{R}^{d\\times L}$ composed of ${\\mathcal{O}}(L(1/\\delta)^{d L}/L!)$ modified feed-forward layers, such that ", "page_idx": 28}, {"type": "equation", "text": "$$\nf_{\\mathcal{T},c3}\\circ f_{\\mathcal{T},c2}(G)=\\left\\{\\!\\!\\begin{array}{l l}{A_{G}}&{\\mathrm{~if~}G\\in\\widetilde{\\mathcal{G}}_{\\delta},}\\\\ {\\mathbf{0}_{d\\times L}}&{\\mathrm{~if~}G\\in\\mathcal{G}_{\\delta}^{+}\\setminus\\widetilde{\\mathcal{G}}_{\\delta}.}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. See Appendix E.5.4 for a detailed proof. ", "page_idx": 28}, {"type": "text", "text": "In conclusion, we have the following lemma for the required number of layers in the modified transformer. ", "page_idx": 28}, {"type": "text", "text": "Lemma E.6 (Total Number of Layers). From the proof of Lemma E.3, if we want to achieve a approximation error ${\\mathcal{O}}(\\delta^{d/2})$ by the modified transformer, we need $\\mathcal{O}(\\delta^{-1})$ modified feed-forward layers in $f_{T,c1}$ \uff0c $O(\\delta^{-d})$ modified self-attention layers in $f_{T,c2}$ , and $O(\\delta^{-d L})$ modified feed-forward layers in fT,c3. ", "page_idx": 28}, {"type": "text", "text": "Proof. By the proof of Lemma E.3, we complete the proof. ", "page_idx": 28}, {"type": "text", "text": "E.3 Standard Transformers Approximate Modified Transformers ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this subsection, we show that standard neural network layers are able to approximate the modified self-attention layers and the modified feed-forward layers (Definition E.3). We have the following Lemma E.7. ", "page_idx": 28}, {"type": "text", "text": "Lemma E.7 Lemma 9 of [Yune l., 2020]) For each $f_{\\mathcal{T},c}\\in\\overline{{\\mathcal{T}_{p}^{2,1,1}}}$ andany $\\epsilon>0$ there exists $f_{T}\\in\\mathcal{T}_{p}^{2,1,4}$ such that $\\|f_{\\mathcal{T}}-f_{\\mathcal{T},c}\\|_{L^{2}}\\leq\\epsilon/3$ ", "page_idx": 28}, {"type": "text", "text": "Proof. See Appendix E.5.5 for a detailed proof. ", "page_idx": 28}, {"type": "text", "text": "E.4  All Together: Standard Transformers Approximate Compact-supported Continuous Functions ", "page_idx": 28}, {"type": "text", "text": "We summarize the results of Lemmas E.2, E.3 and E.7. Then we prove Lemma E.1. ", "page_idx": 28}, {"type": "text", "text": "Furthermore, to achieve the $\\epsilon$ approximation error in Lemma E.1, we take $\\delta=\\mathcal{O}(\\epsilon^{2/d})$ in Lemma E.3. ", "page_idx": 28}, {"type": "text", "text": "E.5  Supplementary Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We first present two preliminary concepts: selective shift operation and bijective column ID mapping in Appendix E.5.1. ", "page_idx": 29}, {"type": "text", "text": "Then we show ", "page_idx": 29}, {"type": "text", "text": "\u00b7 Proof of Lemma E.2 in Appendix E.5.2   \n\u00b7 Proof of Lemma E.4 in Appendix E.5.3   \n\u00b7 Proof of Lemma E.5 in Appendix E.5.4   \n\u00b7 Proof of Lemma E.7 in Appendix E.5.5 ", "page_idx": 29}, {"type": "text", "text": "E.5.1 Preliminaries ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Here, we give the definition of two preliminary concepts: selective shift operation and bijective column ID mapping. ", "page_idx": 29}, {"type": "text", "text": "Selective Shift Operation.  This operation refers to shifting certain entries of the input selectively. To achieve this, we consider the folowing function $\\xi(\\cdot;\\cdot):\\mathbb{R}^{d\\times L}\\rightarrow\\mathbb{R}^{d\\times L}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\xi(X;b_{Q})=e_{1}u^{\\top}X\\sigma_{H}\\left[(u^{\\top}X)^{\\top}(u^{\\top}X-b_{Q}\\mathbb{1}_{n}^{\\top})\\right],\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $X\\in\\mathbb{R}^{d\\times L}$ \uff0c $e_{1}=(1,0,0,\\cdots\\,,0)^{\\top}\\in\\mathbb{R}^{d}$ , and $b_{Q}\\in\\mathbb{R}.\\ u\\in\\mathbb{R}^{d}$ is a vector to be determined. To see the output, we consider the $j$ -th column of $u^{\\top}X\\sigma_{H}\\left[(u^{\\top}X)^{\\top}(u^{\\top}X-b_{Q}\\mathbb{1}_{n}^{\\top})\\right]$ ", "page_idx": 29}, {"type": "text", "text": "\u00b7If $u^{\\top}X_{:,j}>b_{Q}$ , it calculates argmax of $u^{\\top}X$ \u00b7If $u^{\\top}X_{:,j}<b_{Q}$ , it calculates argmin of $u^{\\top}X$ ", "page_idx": 29}, {"type": "text", "text": "All rows of $\\xi(X;b_{Q})$ except the first row are zero. We consider the $j$ -th entry of the first row in $\\xi(X;b_{Q})$ , which is denoted as $\\xi(X;b_{Q})_{1,j}$ . Then for all $j\\in[L]$ ,wehave ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\xi(X;b_{Q})_{1,j}=u^{\\top}X\\sigma_{H}\\left[(u^{\\top}X)^{\\top}(u^{\\top}X_{:,j}-b_{Q})\\right]=\\left\\{\\operatorname*{max}_{k}u^{\\top}X_{:,k}\\quad{\\mathrm{if~}}u^{\\top}X_{:,j}>b_{Q},\\quad{\\mathrm{if~}}u^{\\top}X_{:,k}\\right\\}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "From this observation, we define a function parametrized by $b_{Q}$ and $b_{Q}^{\\prime}$ (with $b_{Q}<b_{Q.}^{\\prime}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\xi(X;b_{Q},b_{Q}^{\\prime}):=\\xi(X;b_{Q})-\\xi(X;b_{Q}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\xi(X;b_{Q},b_{Q}^{\\prime})_{1,j}=\\binom{\\operatorname*{max}_{k}u^{\\top}X;_{k}-\\operatorname*{min}_{k}u^{\\top}X_{:,k},}{0,}\\quad\\mathrm{if}\\;b_{Q}<u^{\\top}X_{:,j}<b_{Q}^{\\prime},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We define an attention layer of the form $X\\rightarrow X+\\xi(X;b_{Q},b_{Q}^{\\prime})$ . For any column $X_{:,j}$ ,if $b_{Q}<$ $u^{\\top}X_{:,j}<b_{Q}^{\\prime}$ , its frst coordinate $X_{1,j}$ is shifted up by $\\begin{array}{r}{\\operatorname*{max}_{k}u^{\\top}\\dot{X}_{:,k}-\\operatorname*{min}_{k}u^{\\top}X_{:,k}}\\end{array}$ , while all the other coordinates stay untouched. We call this the selective shift operation because we can choose $b_{Q}$ and $b_{Q}^{\\prime}$ to shift certain entries of the input selectively. ", "page_idx": 29}, {"type": "text", "text": "Bijective Column ID Mapping. We consider the input $G\\in\\mathcal{G}_{\\delta}^{+}$ (Definition E.7). We use ", "page_idx": 29}, {"type": "equation", "text": "$$\nJ=L+3L\\delta^{-d L},\\;\\mathrm{and}\\;u=(1,\\delta^{-1},\\delta^{-2},\\ldots,\\delta^{-d+1}).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For any $j\\in[L]$ , we have the following two conclusions: ", "page_idx": 29}, {"type": "text", "text": "\u00b7If $G_{i,j}\\geq0$ for all $i\\in[d]$ , i.e., $G_{:,j}\\in[j-1:\\delta:j-\\delta]^{d}$ , then we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\boldsymbol{u}^{\\top}\\boldsymbol{G}_{:,j}\\in\\left[\\delta_{j}:\\delta:\\delta_{j}+\\delta^{-d+1}-\\delta\\right],\\mathrm{~where~}\\delta_{j}=(j-1)\\cdot\\left(\\frac{\\delta-\\delta^{-d+1}}{\\delta-1}\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "The mapping $G_{:,j}\\to u^{\\top}G_{:,j}$ maps the elements in $[j-1:\\delta:j-\\delta]^{d}$ to $\\left[\\delta_{j}:\\delta:\\delta_{j}+\\delta^{-d+1}-\\delta\\right]$ This is a bijection. ", "page_idx": 30}, {"type": "text", "text": "\u00b7 If there exists $i\\in[d]$ such that $G_{i,j}=-J+j$ , then ", "page_idx": 30}, {"type": "equation", "text": "$$\nu^{\\top}G_{:,j}\\leq-3L\\delta^{-d L}+(j-1)\\cdot\\left(\\frac{\\delta^{-d+1}-\\delta}{1-\\delta}\\right)+\\delta^{-d+1}<0.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We say that $u^{\\top}G_{:,j}$ gives the \u201ccolumn $\\mathbf{ID}^{\\bullet}$ for each possible value of $G_{:,j}\\in[j-1:\\delta:j-\\delta]^{d}$ ", "page_idx": 30}, {"type": "text", "text": "Remark E.3 (Illustration of Bijection Properity). For the bijection property, we give the following illustration. Let $G_{:j}\\,=\\,(g_{1j},g_{2j},\\cdot\\cdot\\cdot\\,,g_{d j})^{\\top}$ and $\\overline{{G}}_{:j}\\,=\\,\\bigl(\\overline{{g}}_{1j},\\overline{{g}}_{2j},\\cdot\\cdot\\cdot\\,,\\overline{{g}}_{d j}\\bigr)^{\\intercal}$ .If $u^{\\top}G_{:j}\\,=\\,u^{\\top}\\overline{{G}}_{:j}$ and $G_{:j}\\neq\\overline{{G}}_{:j}$ ,wededuce ", "page_idx": 30}, {"type": "equation", "text": "$$\n(g_{1j}-\\bar{g}_{1j})+\\delta^{-1}(g_{2j}-\\bar{g}_{2j})+\\cdot\\cdot\\cdot+\\delta^{-d+1}(g_{d j}-\\bar{g}_{d j})=0.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Because $G_{:j}\\neq\\overline{{G}}_{:j}$ , then there exists a $k$ $(k<d)$ , such that $g_{k j}\\neq\\overline{{g}}_{k j}$ and $g_{i j}=\\overline{{g}}_{i j}(i>k)$ .We have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|{\\delta^{-k+1}(g_{k j}-\\overline{{g}}_{k j})}\\right|\\geq{\\delta^{-k+2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "However, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|(g_{1j}-\\overline{{g}}_{1j})+\\cdot\\cdot\\cdot+\\delta^{-k+2}(g_{k-1,j}-\\overline{{g}}_{k-1,j})\\right|}\\\\ &{\\leq|g_{1j}-\\overline{{g}}_{1j}|+\\cdot\\cdot\\cdot+|\\delta^{-k+2}(g_{k-1,j}-\\overline{{g}}_{k-1,j})|}\\\\ &{\\leq(1-\\delta)+\\cdot\\cdot\\cdot+\\delta^{-k+2}(1-\\delta)}\\\\ &{<\\delta^{-k+2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "This contradicts with (E.10). Thus we prove the property of bijection. ", "page_idx": 30}, {"type": "text", "text": "E.5.2 Proof of Lemma E.2 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Proof of Lemma E.2. We restate the proof from [Yun et al., 2020] for completeness. ", "page_idx": 31}, {"type": "text", "text": "By the nature of the compact-supported continuous function, $f$ is uniformly continuous. ", "page_idx": 31}, {"type": "text", "text": "Because $\\|\\cdot\\|_{\\infty}$ is equivalent to $\\|\\cdot\\|_{F}$ when the number of entries are finite, we have the following by the definition of uniform continuity. ", "page_idx": 31}, {"type": "text", "text": "For any $\\epsilon/3>0$ there exists a $\\delta^{\\star}>0$ , such that for any $X,Y\\in\\mathbb{R}^{d\\times L}$ , and $\\|X-Y\\|_{\\infty}<\\delta^{\\star}$ we have $\\|f(X)-f(Y)\\|_{F}<\\epsilon/3$ ", "page_idx": 31}, {"type": "text", "text": "Then we perform the following steps following Definitions E.1 and E.2: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 We create a grid $\\mathcal{G}_{\\delta}.$ by choosing grid width $\\delta^{\\star}$ . We also create cube ${\\cal S}_{G}$ with respect to $G\\in{\\mathcal{G}}_{\\delta^{\\star}}$ ", "page_idx": 31}, {"type": "text", "text": "\u00b7 For any grid point $G\\in{\\mathcal{G}}_{\\delta^{\\star}}$ , we define $C_{G}\\in{\\cal S}_{G}$ as the center point of the cube $S_{G}$ ", "page_idx": 31}, {"type": "text", "text": "\u00b7 We define a piecewise-constant function $\\begin{array}{r}{f_{\\delta^{\\star}}(X)=\\sum_{L\\in\\mathcal G_{\\delta^{\\star}}}f(C_{G})\\mathbb1\\{X\\in\\mathcal S_{G}\\}.}\\end{array}$ ", "page_idx": 31}, {"type": "text", "text": "For any $X\\in S_{G}$ ,we have $\\|X-C_{G}\\|_{\\infty}<\\delta^{\\star}$ . According to the uniform continuity, we drive ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\|f(X)-f_{\\delta^{\\star}}(X)\\|_{F}=\\|f(X)-f(C_{G})\\|_{F}<\\epsilon/3.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "This implies that $\\|f-f_{\\delta^{\\star}}\\|_{L^{2}}<\\epsilon/3$ and completes the proof. ", "page_idx": 31}, {"type": "text", "text": "E.5.3 Proof of Lemma E.4 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We give the proof of Lemma E.4 by constructing the network to satisfy the requirements. ", "page_idx": 31}, {"type": "text", "text": "Proof of Lemma E.4. Recall the selective shift operation in Appendix E.5.1. The overall idea of the construction includes two steps: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 Step 1: For each $j\\in[L]$ ,westack $\\delta^{-d}$ attention layers. For $g\\in[\\delta_{j}:\\delta:\\delta_{j}+\\delta^{-d+1}-\\delta]$ (E.8) in the increasing order, we use the attention layer as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\delta^{-d}\\xi(\\cdot;g-\\delta/2,g+\\delta/2).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The total number of layers is $L\\delta^{-d}$ . These layers cast $G\\in\\widetilde{\\mathcal{G}}_{\\delta}$ to $L$ different entries required by Property 1 of Lemma E.4. ", "page_idx": 31}, {"type": "text", "text": "\u00b7 Step 2: We add an extra single-head attention layer with the following attention part ", "page_idx": 31}, {"type": "equation", "text": "$$\nL\\delta^{-(L+1)d-1}\\xi(\\cdot;0).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "This layer achieves a global shifting and casts different $G\\in\\widetilde{\\mathcal{G}}_{\\delta}$ to unique elements required by the Property 2 of Lemma E.4. ", "page_idx": 31}, {"type": "text", "text": "The two operations map $\\widetilde{\\mathcal{G}}_{\\delta}$ and $\\mathcal{G}_{\\delta}^{+}\\setminus\\widetilde{\\mathcal{G}}_{\\delta}$ to different sets, as required by Property 3 and Property 4 of Lemma E.4. The bounds $t_{l}$ and $t_{r}$ are calculated then. ", "page_idx": 31}, {"type": "text", "text": "Then, we give a detailed proof by showing the impact of the two steps and verifying the four properties of Lemma E.4. We achieve this by making a category division of ${{\\bar{\\mathcal G}}_{\\delta}^{+}}$ ", "page_idx": 31}, {"type": "text", "text": "\u00b7 Category 1: $G\\in\\widetilde{\\mathcal{G}}_{\\delta}$ , all entries in the point $G$ are between O and $L-\\delta$ \u00b7 Category 2: $G\\in\\mathcal{G}_{\\delta}^{+}\\setminus\\widetilde{\\mathcal{G}}_{\\delta}$ , the point $G$ has at least one entry that equals to $-J$ ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "Let $u=(1,\\delta^{-1},\\delta^{-2},\\ldots,\\delta^{-d+1})$ . Recall that $\\delta_{j}=(j-1)(\\delta-\\delta^{-d+1})/(\\delta-1)$ for any $j\\in[L]$ .n (E.8). ", "page_idx": 31}, {"type": "text", "text": "Category 1.  We denote $g_{j}:=u^{\\top}G_{:,j}$ , then we have $g_{1}\\,<\\,g_{2}\\,<\\,\\cdot\\,\\cdot\\,<\\,g_{L}$ . The first $\\delta^{-d}$ layers sweep the set $[\\delta_{j}:\\delta:\\delta_{j}+\\delta^{-d+1}-\\delta],j\\in[L]$ and apply selective shift operation on each element in the set. This means that selective shift operation wili be applied to $g_{1}$ first, then $g_{2}$ , followed by $g_{3}$ and so on. ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The First Shift Operation. In the first selective shift operation with $g$ going through $[\\delta_{1}:\\delta:$ $\\delta_{1}+\\delta^{-d+1}-\\delta]$ , the $(1,1)$ -th entry of $G$ (i.e, $G_{1,1})$ is shifted by the peration, while thethr entries are left untouched. The updated value $\\widetilde{G}_{1,1}$ is ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\widetilde{G}_{1,1}=G_{1,1}+\\delta^{-d}\\left[\\operatorname*{max}_{k}\\left(\\boldsymbol{u}^{\\top}G_{:,k}\\right)-\\operatorname*{min}_{k}\\left(\\boldsymbol{u}^{\\top}G_{:,k}\\right)\\right]=G_{1,1}+\\delta^{-d}(g_{L}-g_{1}).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Therefore, the output of the layer after the operation is ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}{\\left(\\widetilde{G}_{:,1}\\right.}&{{}G_{:,2}}&{\\cdot\\cdot\\cdot}&{{}G_{:,L}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Let $\\widetilde{g}_{1}:=u^{T}\\widetilde{G}_{:,1}$ .We have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\widetilde{g}_{1}=\\widetilde{G}_{1,1}+\\sum_{i=2}^{d}\\delta^{-i+1}G_{i,1}}}\\\\ &{=G_{1,1}+\\delta^{-d}(g_{L}-g_{1})+\\displaystyle\\sum_{i=2}^{d}\\delta^{-i+1}G_{i,1}}\\\\ &{=g_{1}+\\delta^{-d}(g_{L}-g_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then we deduce $g_{L}<\\widetilde{g}_{1}$ , because ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{g}_{1}=g_{1}+\\delta^{-d}\\big(g_{L}-g_{1}\\big)}\\\\ &{\\quad\\ge0+\\delta^{-d}\\left[(L-1)\\cdot\\frac{\\delta-\\delta^{-d+1}}{\\delta-1}-\\delta^{-d+1}+\\delta\\right]}\\\\ &{\\quad=\\delta^{-d}\\left[(L-1)\\frac{\\delta}{1-\\delta}+\\delta+(L-1)\\frac{\\delta^{-d+1}}{1-\\delta}-\\delta^{-d+1}\\right]}\\\\ &{\\quad\\ge\\delta^{-d}\\cdot\\left((L-1)\\frac{\\delta}{1-\\delta}+\\delta\\right)}\\\\ &{\\quad=(L-1)\\frac{\\delta^{-d+1}}{1-\\delta}+\\delta^{-d+1}}\\\\ &{\\quad>g_{L}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Thus, after updating, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{max}u^{\\top}\\left(\\widetilde{G}_{:,1}\\quad G_{:,2}\\quad\\cdot\\cdot\\quad G_{:,L}\\right)=\\operatorname*{max}\\{\\widetilde{g}_{1},g_{2},\\ldots,g_{L}\\}=\\widetilde{g}_{1},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and the new minimum is $g_{2}$ ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The Second Shift Operation. In the second selective shift operation with $g$ going through $[\\delta_{2}:\\delta:\\delta_{2}+\\delta^{-d+1}-\\Bar{\\delta}]$ the $(1,2)$ -th entry of $G$ (i.e, $G_{1,2})$ is shiftedby the operation,while the other entries are left untouched. The updated value $\\widetilde{G}_{1,2}$ is ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{G}_{1,2}=G_{1,2}+\\delta^{-d}(\\widetilde{g}_{1}-g_{2})}\\\\ &{\\qquad=G_{1,2}+\\delta^{-d}(g_{1}-g_{2})+\\delta^{-2d}(g_{L}-g_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Therefore, the output of the layer after the operation is ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left(\\widetilde{G}_{:,1}\\quad\\widetilde{G}_{:,2}\\quad\\cdot\\cdot\\quad G_{:,L}\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{g}_{2}:=u^{\\top}\\widetilde{G}_{:,2}}\\\\ &{\\quad=g_{2}+\\delta^{-d}(g_{1}-g_{2})+\\delta^{-2d}(g_{L}-g_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Then we deduce $\\widetilde{g}_{1}<\\widetilde{g}_{2}$ , because ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{1}+\\delta^{-d}(g_{L}-g_{1})<g_{2}+\\delta^{-d}(g_{1}-g_{2})+\\delta^{-2d}(g_{L}-g_{1})}\\\\ &{\\Longleftrightarrow\\ (\\delta^{-d}-1)(g_{2}-g_{1})<\\delta^{-d}(\\delta^{-d}-1)(g_{L}-g_{1}).\\quad\\quad\\quad\\quad\\left(\\mathrm{By~}\\delta^{-d}>1\\mathrm{~and~}g_{L}>g_{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus, after updating, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{max}u^{\\top}\\left(\\widetilde{G}_{:,1}\\quad\\widetilde{G}_{:,2}\\quad\\cdot\\cdot\\quad G_{:,L}\\right)=\\operatorname*{max}\\{\\widetilde{g}_{1},\\widetilde{g}_{2},\\dots,g_{L}\\}=\\widetilde{g}_{2},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and the new minimum is $g_{3}$ ", "page_idx": 33}, {"type": "text", "text": "\u00b7 Repeating the Process. By repeating this process, we show that the $j$ -th shift operation shifts $G_{1,j}$ by $\\delta^{\\overline{{-}}d}(\\widetilde{g}_{j-1}-g_{j})$ .Thenwe have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{g}_{j}:=\\boldsymbol{u}^{\\top}\\widetilde{G}_{:,j}}\\\\ &{\\quad=g_{j}+\\displaystyle\\sum_{k=1}^{j-1}\\delta^{-k d}(g_{j-k}-g_{j-k+1})+\\delta^{-j d}(g_{L}-g_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We deduce $\\widetilde{g}_{j-1}<\\widetilde{g}_{j}$ holds for all $2\\le j\\le L$ , because ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{g}_{j-1}<\\widetilde{g}_{j}}\\\\ {\\iff}&{g_{j-1}+\\displaystyle\\sum_{k=2}^{j-1}\\delta^{-k d+d}(g_{j-k}-g_{j-k+1})+\\delta^{-(j-1)d}(g_{L}-g_{1})}\\\\ &{\\quad<g_{j}+\\displaystyle\\sum_{k=1}^{j-1}\\delta^{-k d}(g_{j-k}-g_{j-k+1})+\\delta^{-j d}(g_{L}-g_{1})}\\\\ {\\iff}&{\\displaystyle\\sum_{k=1}^{j-1}\\delta^{-k d+d}(\\delta^{-d}-1)(g_{j-k+1}-g_{j-k})<\\delta^{-(j-1)d}(\\delta^{-d}-1)(g_{L}-g_{1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the last inequality holds because ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{k=1}^{j-1}\\delta^{-k d+d}\\big(g_{j-k+1}-g_{j-k}\\big)}\\\\ &{<\\delta^{-(j-1)d}\\displaystyle\\sum_{k=1}^{j-1}\\big(g_{j-k+1}-g_{j-k}\\big)}\\\\ &{<\\delta^{-(j-1)d}\\big(g_{L}-g_{1}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Therefore, after the $j$ -th selective shift operation, $\\widetilde{g}_{j}$ is the new maximum among $\\{\\widetilde{g}_{1},\\dots,\\widetilde{g}_{j},g_{j+1},\\dots,g_{L}\\}$ and $g_{j+1}$ is the new minimum. ", "page_idx": 33}, {"type": "text", "text": "\u00b7 After $L$ Shift Operations. After the whole $L$ shift operations, the input $G$ is mapped to a new point $\\widetilde{G}$ where $\\boldsymbol{u}^{\\top}\\widetilde{\\boldsymbol{G}}=(\\widetilde{g}_{1}\\quad\\widetilde{g}_{2}\\quad\\ldots\\quad\\widetilde{g}_{L})$ and $\\widetilde{g}_{1}<\\widetilde{g}_{2}<\\dots<\\widetilde{g}_{L}$ . For the lower and upper bound of $\\widetilde{g}_{L}$ , we have the following lemma. ", "page_idx": 33}, {"type": "text", "text": "Lemma E.8 (Lemma 10 of [Yun et al., 2020]). $\\widetilde{g}_{L}=u^{\\top}\\widetilde{G}_{:,L}$ satisfies the following bounds: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\delta^{-(L-1)d+1}(\\delta^{-d}-1)\\leq\\widetilde{g}_{L}\\leq L\\delta^{-(L+1)d}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "\u00b7 Global Shifting by the Last Layer. We note that after the above $L$ shift operations, there is another attention layer with attention part $L\\delta^{-(L+1)d-1}\\xi(\\cdot;0)$ .Since $0<\\widetilde{g}_{1}<\\dots<\\widetilde{g}_{L}$ it ads the following to each entry in the first row of G: ", "page_idx": 34}, {"type": "equation", "text": "$$\nL\\delta^{-(L+1)d-1}\\operatorname*{max}_{k}u^{\\top}\\widetilde{G}_{:,k}=L\\delta^{-(L+1)d-1}\\widetilde{g}_{L}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The output of this layer is defined to be the function $f_{T,c2}(G)$ ", "page_idx": 34}, {"type": "text", "text": "In summary, for any $G\\in\\widetilde{\\mathcal{G}}_{\\delta},i\\in[d]$ , and $j\\in[L]$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\nf_{\\mathcal{T},c2}(G)_{i,j}=\\left\\{\\!\\!\\begin{array}{l l}{G_{1,j}+\\delta_{j}^{+}}&{\\mathrm{~if~}i=1,}\\\\ {G_{i,j}}&{\\mathrm{~if~}2\\leq i\\leq d,}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\begin{array}{r}{\\delta_{j}^{+}=\\sum_{k=1}^{j-1}\\delta^{-k d}(g_{j-k}-g_{j-k+1})+\\delta^{-j d}(g_{L}-g_{1})+L\\delta^{-(L+1)d-1}\\tilde{g}_{L}.}\\end{array}$ For any $G\\in\\widetilde{\\mathcal{G}}_{\\delta}$ and $j\\in[L]$ ", "page_idx": 34}, {"type": "equation", "text": "$$\nu^{\\top}f_{\\mathcal{T},c2}(G)_{:,j}=\\widetilde{g}_{j}+L\\delta^{-(L+1)d-1}\\widetilde{g}_{L}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Next, we check the Property 1, Property 2 and Property 3 of Lemma E.4. ", "page_idx": 34}, {"type": "text", "text": "\u00b7 Checking Property 1 of Lemma E.4. Given any $G\\in\\widetilde{\\mathcal{G}}_{\\delta}$ , we already prove that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\widetilde{g}_{1}<\\widetilde{g}_{2}<\\cdot\\cdot<\\widetilde{g}_{L},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "All of them are distinct. ", "page_idx": 34}, {"type": "text", "text": "\u00b7 Checking Property 2 of Lemma E.4. Note that the upper bound on $\\widetilde{g}_{L}$ from Lemma E.8 also holds for other $\\widetilde{g}_{j}$ $\\bar{(j}\\in[L-1])$ . For all $j\\in[L]$ wehave ", "page_idx": 34}, {"type": "equation", "text": "$$\nL\\delta^{-(L+1)d-1}\\widetilde{g}_{L}\\leq u^{\\top}f_{\\mathcal{T},c2}(G)_{:,j}<L\\delta^{-(L+1)d-1}\\widetilde{g}_{L}+L\\delta^{-(L+1)d}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "By Lemma E.8, two different $G,G^{\\prime}\\in{\\widetilde{\\mathcal{G}}}_{\\delta}$ are mapped to different $\\widetilde{g}_{L}$ and $\\widetilde{g}_{L}^{\\prime}$ , and they differ at leastby $\\delta$ . This means that the following two intervals are guaranteed to be disjoint: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{[L\\delta^{-(L+1)d-1}\\widetilde{g}_{L},L\\delta^{-(L+1)d-1}\\widetilde{g}_{L}+L\\delta^{-(L+1)d}),}\\\\ &{[L\\delta^{-(L+1)d-1}\\widetilde{g}_{L}^{\\prime},L\\delta^{-(L+1)d-1}\\widetilde{g}_{L}^{\\prime}+L\\delta^{-(L+1)d}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Thus, the entries of $u^{\\top}f_{\\mathcal{T},c2}(G)$ and $u^{\\top}f_{\\top,c2}(G^{\\prime})$ are all distinct. ", "page_idx": 34}, {"type": "text", "text": "Now, we finish showing that the mapping $f_{T,c2}(\\cdot)$ uses $(1/\\delta)^{d}+1$ attention layers to implement a contextual mapping on $\\widetilde{\\mathcal{G}}_{\\delta}$ ", "page_idx": 34}, {"type": "text", "text": "\u00b7 Checking  Property 3 of Lemma E.4.  Given Lemma E.8  and $u^{\\top}f_{\\mathcal{T},c2}(G)_{:,j}\\quad\\in$ $[L\\delta^{-(L+1)d-1}\\widetilde{g}_{L},L\\delta^{-(L+1)d-1}\\widetilde{g}_{L}+L\\delta^{-(L+1)d})$ for any $G\\in\\widetilde{\\mathcal{G}}_{\\delta}$ we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u^{\\top}f_{\\mathcal{T},c2}(G)_{:,j}\\geq L\\delta^{-2(L+1)d}(\\delta^{-d}-1),}\\\\ &{u^{\\top}f_{\\mathcal{T},c2}(G)_{:,j}<L^{2}\\delta^{-2(L+1)d-1}+L\\delta^{-(L+1)d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This proves that all $u^{\\top}f_{\\mathcal{T},c2}(L)_{:,j}$ are between $t_{l}$ and $t_{r}$ , where ", "page_idx": 34}, {"type": "equation", "text": "$$\nt_{l}=L\\delta^{-2(L+1)d}(\\delta^{-d}-1),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\nt_{r}=L^{2}\\delta^{-2\\left(L+1\\right)d-1}+L\\delta^{-\\left(L+1\\right)d}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Category 2. Now we check the Property 4 of Lemma E.4. For the input points $G\\in\\mathcal{G}_{\\delta}^{+}\\setminus\\widetilde{\\mathcal{G}}_{\\delta}$ note that the point $G$ has at least one entry that equals to $-J+k,k\\in[L-1]$ . Let $g_{j}:=u^{\\top}G_{:,j}$ Recall that whenever a column $G_{:,j}$ has an entry that equals to $-J+k,k\\in[L-1]$ , we have $g_{j}<0$ Without loss of generality, assume that $g_{1}<0$ ", "page_idx": 35}, {"type": "text", "text": "Because the selective shift operation is applied to each element of $[0:\\delta:\\delta_{L}+\\delta^{-d+1}-\\delta]$ and is not applied to negative values, thus we have $\\operatorname*{min}_{k}u^{\\top}G_{:,k}=g_{1}<0$ $g_{1}$ never gets shifted upwards and remains the minimum for the whole time. ", "page_idx": 35}, {"type": "text", "text": "\u00b7 All $g_{j}$ 's are Negative. When all $g_{j}$ 's are negative, selective shift operation never shifts the input $G$ Thus $\\tilde{G}\\,=\\,G$ . Recall that $u^{\\top}\\widetilde G_{:,j}\\,<\\,0$ for all $j\\in[L]$ . The last layer with attention part $L\\delta^{-(L+1)d-1}\\xi(\\cdot;0)$ adds $L\\delta^{-(L+1)d-1}\\operatorname*{min}_{k}u^{\\top}\\widetilde{G}_{:,k}<0$ to each entry in the frst row of $\\widetilde{G}$ This makes $\\widetilde{G}$ remain negative. Therefore, $f_{T,c2}(G)$ satisfies $u^{\\top}f_{T,c2}(G)_{:,j}<0<t_{l}$ for all $j\\in[L]$ \u00b7 Not All $g_{j}$ 's are Negative. Now consider the case where at least one $g_{j}$ is positive. Suppose that there are $\\dot{k}$ positive elements and they satisfy $g_{i_{1}}<g_{i_{2}}<\\cdot\\cdot<g_{i_{k}}$ . Thus selective shift operation does not affect $g_{i}$ , where $i\\in[L]\\setminus\\{i_{1},\\ldots,i_{k}\\}$ It shifts $g_{i_{1}}$ by ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\delta^{-d}(\\operatorname*{max}^{u^{\\top}}\\!G_{:,k}-\\operatorname*{min}_{k}u^{\\top}G_{:,k})}\\\\ &{\\geq\\delta^{-d}(2L\\delta^{-d L}-(L-1)\\frac{\\delta^{-d+1}-\\delta}{1-\\delta}-\\delta^{-d+1}+(i_{k}-1)\\frac{\\delta^{-d+1}-\\delta}{1-\\delta})}\\\\ &{=\\delta^{-d}(3L\\delta^{-d L}-\\delta^{-d+1}-(L-i_{k})\\frac{\\delta^{-d+1}-\\delta}{1-\\delta})}\\\\ &{\\geq\\delta^{-d}\\cdot2L\\delta^{-d L}}\\\\ &{=2L\\delta^{-(L+1)d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The next shift operations shift $g_{i_{2}},\\ldots,g_{i_{k}}$ by an even larger amount. Therefore, at the end of the frst $L(1/\\delta)^{d}$ layers, we have $L\\delta^{-(\\widetilde{L}+1)d}\\,\\leq\\,\\widetilde{g}_{i_{1}}\\,\\leq\\,\\cdots\\,\\leq\\,\\widetilde{g}_{i_{k}}$ and $\\widetilde{g}_{j}\\;<\\;0$ for all $j~\\in$ $[L]\\setminus\\{i_{1},\\ldots,i_{k}\\}$ ", "page_idx": 35}, {"type": "text", "text": "Then, we shift $G$ by the last layer. The last layer with attention part $L\\delta^{-(L+1)d-1}\\xi(\\cdot;0)$ acts differently for negative and positive $\\widetilde{g}_{j}$ 's. (i). For negative $\\widetilde{g}_{j}$ 's, it adds the following to $\\widetilde{g}_{j},j\\in$ $[L]\\setminus\\{i_{1},\\ldots,i_{k}\\}$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\ensuremath{L}\\delta^{-(L+1)d-1}\\operatorname*{min}_{k}u^{\\top}\\widetilde{G}_{:,k}=\\ensuremath{L}\\delta^{-(L+1)d-1}g_{1}<0.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "This term pushes them further to the negative side. (i). For positive $\\widetilde{g}_{i}$ 's, it adds ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L\\delta^{-(L+1)d-1}\\operatorname*{max}_{k}u^{\\top}\\widetilde{G}_{k}=L\\delta^{-(L+1)d-1}\\widetilde{g}_{i_{k}}\\geq2L^{2}\\delta^{-2(L+1)d-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Thus they are all greater than or equal to $2L^{2}\\delta^{-2(L+1)d+1}$ . Note that ", "page_idx": 35}, {"type": "equation", "text": "$$\n2L^{2}\\delta^{-2(L+1)d-1}>t_{r}\\mathrm{,~where~}t_{r}=L^{2}\\delta^{-2(L+1)d-1}+L\\delta^{-(L+1)d}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then the final output $f_{T,c2}(G)$ satisfies $u^{\\top}f_{T,c2}(G)_{:,j}\\not\\in[t_{l},t_{r}]$ , for all $j\\in[L]$ . This completes the verification of Property 4 of Lemma E.4. ", "page_idx": 35}, {"type": "text", "text": "In conclusion, we need $O(L\\delta^{-d})$ layers of modified self-attention layer to obtain our approximation. This completes the proof. \u53e3 ", "page_idx": 35}, {"type": "text", "text": "E.5.4 Proof of Lemma E.5 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Proofof Lemma $E.5$ We restate the proof from [Yun et al., 2020] for completeness. ", "page_idx": 36}, {"type": "text", "text": "Note that $|\\mathcal{G}_{\\delta}^{+}|=(1/\\delta+1)^{d L}<\\infty$ , so the output of $f_{T,c2}(\\mathcal{G}_{\\delta}^{+})$ has finite number of distinct real values. Let $M$ be the upper bound of all these possible values. By the construction of $f_{T,c2}$ \uff0c $M>0$ ", "page_idx": 36}, {"type": "text", "text": "Construct the Layers: $f_{T,c3}(f_{T,c2}(G))=\\mathbf{0}_{d\\times L}$ if $G\\in\\mathcal{G}_{\\delta}^{+}\\setminus\\widetilde{\\mathcal{G}}_{\\delta}$ .According to Lemma E.4, for all $j\\in[L]$ , we have $u^{\\top}f_{T,c2}(G)_{:,j}\\in[t_{l},t_{r}]$ $G\\in\\widetilde{\\mathcal{G}}_{\\delta}$ , and $u^{\\top}f_{T,c2}(G)_{:,j}\\not\\in[t_{l},t_{r}]$ $G\\in\\mathcal{G}_{\\delta}^{+}\\setminus\\widetilde{\\mathcal{G}}_{\\delta}$ Due to this property, we add the following feed-forward layer. ", "page_idx": 36}, {"type": "text", "text": "Definition E.8 (Feed-forward Layer 3). The vectors $u$ and ${\\mathbb I}_{L}$ act as the weight parameters, and $\\zeta_{3}(\\cdot)$ acts as the activation function in the feed-forward layer. ", "page_idx": 36}, {"type": "equation", "text": "$$\nX\\rightarrow X-(M+1)\\mathbb{1}_{L}\\zeta_{3}(u^{\\top}X),\\,\\,\\,\\zeta_{3}(t)={\\left\\{\\begin{array}{l l}{0}&{{\\mathrm{if~}}t\\in[t_{l},t_{r}]}\\\\ {1}&{{\\mathrm{if~}}t\\not\\in[t_{l},t_{r}].}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "\u00b7 Case for $G\\in\\mathcal{G}_{\\delta}^{+}\\setminus\\widetilde{\\mathcal{G}}_{\\delta}$ . We have $\\zeta_{3}(u^{\\top}f_{T,c2}(G))=\\mathbb{1}_{L}^{\\top}$ . Thus, all the entries of the input are shifted by $-M-1$ and become strictly negative. ", "page_idx": 36}, {"type": "text", "text": ". Case for $G\\in\\widetilde{\\mathcal{G}}_{\\delta}$ . We have $\\zeta_{3}(u^{\\top}f_{T,c2}(G))=\\mathbf{0}_{L}^{\\top}$ So the output stays the same as the $f_{T,c2}(G)$ ", "page_idx": 36}, {"type": "text", "text": "With the input $f_{T,c2}(G)$ if $G\\in\\widetilde{\\mathcal{G}}_{\\delta}$ , then $\\zeta_{3}(u^{\\top}f_{T,c2}(G))=\\mathbf{0}_{L}^{\\top}$ . Thus, the output stays the same as the input. If $G\\in\\mathcal{G}_{\\delta}^{+}\\setminus\\widetilde{\\mathcal{G}}_{\\delta}$ , then $\\zeta_{3}(u^{\\top}f_{T,c2}(G))=\\mathbb{1}_{L}^{\\top}$ .Thus, all the entries of the input are shiftd by $-M-1$ and become strictly negative. ", "page_idx": 36}, {"type": "text", "text": "Next, we map those negative entries to zero. For $i=1,2,\\cdots,d$ , we add the following layer: ", "page_idx": 36}, {"type": "text", "text": "Definition E.9 (Feed-forward Layer 4). The vectors $u$ and $e_{i}$ act as the weight parameters and $\\zeta_{4}(\\cdot)$ acts as the activation function in the feed-forward layer. ", "page_idx": 36}, {"type": "equation", "text": "$$\nX\\rightarrow X+e_{i}\\zeta_{4}((e_{i})^{\\top}X),\\;\\;\\zeta_{4}(t)={\\left\\{\\begin{array}{l l}{-t}&{{\\mathrm{if~}}t<0}\\\\ {0}&{{\\mathrm{if~}}t\\geq0.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "After these $d$ layers, the output for $G\\in\\mathcal{G}_{\\delta}^{+}\\setminus\\widetilde{\\mathcal{G}}_{\\delta}$ is a zero matrix, while the output for $G\\in\\widetilde{\\mathcal{G}}_{\\delta}$ remains $f_{T,c2}(G)$ ", "page_idx": 36}, {"type": "text", "text": "Construct the Layers: $f_{T,c3}(f_{T,c2}(G))=A_{G}$ if $G\\in\\widetilde{\\mathcal{G}}_{\\delta}$ . Each different $G$ is mapped to $L$ unique numbers $u^{\\top}f_{\\mathcal{T},c2}(G)$ , which are at least $\\delta$ apart from each other. We map each unique number to the corresponding output column as follows. We choose one $\\overline{{G}}\\in\\widetilde{\\mathcal{G}}_{\\delta}$ . For each $\\boldsymbol{u}^{\\top}f_{T,c2}(\\overline{{G}})_{:,j},\\boldsymbol{j}\\in[L],$ we add the following feed-forward layer. ", "page_idx": 36}, {"type": "text", "text": "Definition E.10 (Feed-forward Layer 5). The vectors $u$ and $e_{i}$ act as the weight parameters, and $\\zeta_{4}(\\cdot)$ acts as the activation function in the feed-forward layer. ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{X\\rightarrow X+\\left((A_{\\overline{{G}}})_{:,j}-f_{\\mathcal{T},c2}(\\overline{{G}})_{:,j}\\right)\\zeta_{5}(u^{\\top}X-u^{\\top}f_{\\mathcal{T},c2}(\\overline{{G}})_{:,j}\\mathbb{1}_{L}^{\\top}),}\\\\ &{\\qquad\\zeta_{5}(t)=\\left\\{\\begin{array}{l l}{1}&{-\\delta/2\\leq t<\\delta/2,}\\\\ {0}&{\\mathrm{others}.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "\u00b7 Case for $G\\in\\mathcal{G}_{\\delta}^{+}\\setminus\\widetilde{\\mathcal{G}}_{\\delta}$ .  Recall that the input $X$ of this layer is $f_{T,c2}(G)$ If $X$ is a zero matrix, which is the case for $G\\in\\mathcal{G}_{\\delta}^{+}\\backslash\\widetilde{\\mathcal{G}}_{\\delta}$ , we have $u^{\\top}X=\\mathbf{0}_{L}^{\\top}$ . Then $u^{\\top}X\\,{-}\\,u^{\\top}\\,f_{\\mathcal{T},c2}(\\overline{{G}})_{:,j}\\mathbb{1}_{L}^{\\top}<-t_{l}\\mathbb{1}_{L}$ Since $t_{l}>\\delta/2$ , the output remains the same as $X$ ", "page_idx": 36}, {"type": "text", "text": "\u00b7 Case for $G\\in\\widetilde{\\mathcal{G}}_{\\delta}$ . Let the input $X$ be $f_{T,c2}(G)$ , where $G\\in\\widetilde{\\mathcal{G}}_{\\delta}$ is not equal to $\\overline{G}$ .According to the Property 2 of Lemma E.4 and given a $j\\;\\in\\;[L],\\;u^{\\top}f_{\\mathcal{T},c2}(G)_{:,k},(k\\;\\in\\;[L])$ differs from $u^{\\top}f_{\\tau,c2}(\\overline{{G}})_{:,j}$ by at least $\\delta$ .Then we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\zeta_{5}(u^{\\top}f_{\\mathcal{T},c2}(G)-u^{\\top}f_{\\mathcal{T},c2}(\\overline{{G}})_{:,j}\\mathbb{1}_{L}^{\\top})=\\mathbf{0}_{L}^{\\top}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Thus the input is left untouched. ", "page_idx": 37}, {"type": "text", "text": "If $G=\\overline{{G}}$ , then ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\zeta_{5}(\\boldsymbol{u}^{\\top}f_{\\mathcal{T},c2}(\\boldsymbol{G})-\\boldsymbol{u}^{\\top}f_{\\mathcal{T},c2}(\\overline{{\\boldsymbol{G}}})_{:,j}\\mathbb{1}_{L}^{\\top})=(\\boldsymbol{e}_{j})^{\\top}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Thus we shift the $j$ -th column of $f_{T,c2}(G)$ ", "page_idx": 37}, {"type": "equation", "text": "$$\nf_{T,c2}(G)_{:,j}+((A_{\\bar{G}})_{:,j}-f_{T,c2}(\\bar{G})_{:,j})=f_{T,c2}(G)_{:,j}+((A_{G})_{:,j}-f_{T,c2}(G)_{:,j})=(A_{G})_{:,j}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "In other word, this layer maps the column $f_{T,c2}(G)_{:,j}$ to $\\left(A_{G}\\right)_{:,j}$ , without affecting any other columns. ", "page_idx": 37}, {"type": "text", "text": "For each $G\\in\\widetilde{\\mathcal{G}}_{\\delta}$ we defer that we need one layer for each unique value of $u^{\\top}f_{\\tau,c2}(G)_{:,j}$ . Note that there are $O(\\delta^{-d L})$ such numbers, so we use $O(\\delta^{-d L})$ layers to finish our construction. ", "page_idx": 37}, {"type": "text", "text": "This completes the proof. ", "page_idx": 37}, {"type": "text", "text": "E.5.5 Proof of Lemma E.7 ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Proof of Lemma E.7. We restate the proof from [Yun et al., 2020] for completeness. ", "page_idx": 38}, {"type": "text", "text": "The proof follows two steps: (i) Approximate the modified self-attention layers. (i) Approximate the modified feed-forward layers. ", "page_idx": 38}, {"type": "text", "text": "\u00b7 Step 1: Approximate the Modified Self-Attention Layers. ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "We achieve this by approximating the Softmax operator $\\sigma_{S}$ with the Hardmax operator $\\sigma_{H}$ Given a matrix $\\boldsymbol{X}\\in\\dot{\\mathbb{R}}^{\\dot{d}\\times L}$ wehave ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\sigma_{S}(\\lambda X)\\rightarrow\\sigma_{H}(X),\\quad\\mathrm{as}\\quad\\lambda\\rightarrow\\infty.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "The operator is the only difference between the normal and the modified self-attention layers. We approximate the modified self-attention layer in $\\overline{{T}}_{p}^{r,m,l}$ by the normal self-aention layer with the same number of heads $r$ and head size $m$ ", "page_idx": 38}, {"type": "text", "text": "\u00b7 Step2: Approximate the Modified Feed-Forward Layers. ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "We achieve this by approximating the activation function in $\\Psi$ with four ReLU functions. From Definition E.3, we recall that $\\Psi$ denotes three-piecewise functions with at least a constant piece. We consider the following $\\zeta\\in\\Psi$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\zeta(x)={\\binom{b_{1}}{a_{2}x+b_{2}}}\\quad{\\mathrm{~if~}}x<c_{1},}\\\\ {\\zeta(x)={\\binom{b_{1}}{a_{3}x+b_{3}}}\\quad{\\mathrm{~if~}}c_{2}\\leq x,}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $a_{2},a_{3},b_{1},b_{2},b_{3},c_{1},c_{2}\\in\\mathbb{R}$ and $c_{1}<c_{2}$ ", "page_idx": 38}, {"type": "text", "text": "We approximate $\\zeta(x)$ by $\\widetilde{\\zeta}(x)$ composed of four ReLU functions: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\zeta}(x)=b_{1}+\\frac{a_{2}c_{1}+b_{2}-b_{1}}{\\epsilon}\\mathrm{ReLU}(\\mathbf{x}-\\mathbf{c}_{1}+\\epsilon)+\\left(a_{2}-\\frac{a_{2}c_{1}+b_{2}-b_{1}}{\\epsilon}\\right)\\mathrm{ReLU}(\\mathbf{x}-\\mathbf{c}_{1})}\\\\ &{\\qquad\\qquad+\\left(\\frac{a_{3}c_{2}+b_{3}-a_{2}(c_{2}-\\epsilon)-b_{2}}{\\epsilon}-a_{2}\\right)\\mathrm{ReLU}(\\mathbf{x}-\\mathbf{c}_{2}+\\epsilon)}\\\\ &{\\qquad\\qquad+\\left(a_{3}-\\frac{a_{3}c_{2}+b_{3}-a_{2}(c_{2}-\\epsilon)-b_{2}}{\\epsilon}\\right)\\mathrm{ReLU}(\\mathbf{x}-\\mathbf{c}_{2})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\epsilon}\\\\ &{\\qquad=\\left\\{\\begin{array}{l l}{b_{1}}&{\\mathrm{if~}x<c_{1}-\\epsilon,}\\\\ {(a_{2}c_{1}+b_{2}-b_{1})(x-c_{1})/\\epsilon+a_{2}c_{1}+b_{2}}&{\\mathrm{if~}c_{1}-\\epsilon\\leq x<c_{1},}\\\\ {a_{2}x+b_{2}}&{\\mathrm{if~}c_{1}\\leq x<c_{2}-\\epsilon,}\\\\ {(a_{3}c_{2}+b_{3}-a_{2}(c_{2}-\\epsilon)-b_{2})(x-c_{2})/\\epsilon+a_{3}c_{2}+b_{3}}&{\\mathrm{if~}c_{2}-\\epsilon\\leq x<c_{2},}\\\\ {a_{3}x+b_{3}}&{\\mathrm{if~}c_{2}\\leq x.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "As $\\epsilon\\rightarrow0$ , we approximate $\\zeta(x)$ by $\\widetilde{\\zeta}(x)$ . The activation function is the only difference between the normal and modified feed-forward layers. We approximate the modified feed-forward layer in $\\overline{{T}}_{p}^{r,m,l}$ by the normal one. ", "page_idx": 38}, {"type": "text", "text": "Thus, for any $f_{T,c}\\in\\overline{{\\mathcal{T}_{p}^{2,1,1}}}$ , there exists afunction $f_{T}\\in\\mathcal{T}_{p}^{2,1,4}$ to approximate $f_{T,c}$ ", "page_idx": 38}, {"type": "text", "text": "This completes the proof. ", "page_idx": 38}, {"type": "text", "text": "F Proofs of Section 3 ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Our proofs are motivated by the approximation and estimation theory of U-Net-based diffusion models in [Chen et al., 2023]. We use transformer networks\u2019 universal approximation theory in Appendix E and the covering number to proceed with our proof. Specifically, we derive the approximation error bound in Appendix F.1 and the corresponding sample complexity bound in Appendix F.2. Then we show that the data distribution generated from the estimated score function converges toward a proximate area of the original one in Appendix F.3. ", "page_idx": 39}, {"type": "text", "text": "F.1Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Here we present some auxiliary theoretical results in Appendix F.1.1 to prepare for our main proof of Theorem 3.1. Then we derive the approximation error bound of DiTs (i.e., the proof of Theorem 3.1) in Appendix F.1.2. ", "page_idx": 39}, {"type": "text", "text": "F.1.1 Auxiliary Lemmas for Theorem 3.1. ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "We restate some auxiliary lemmas and their proofs from [Chen et al., 2023] for later convenience. ", "page_idx": 39}, {"type": "text", "text": "Lemma F.1 (Lemma 16 of [Chen et al., 2023]). Consider a probability density function $p_{h}(h)=$ $\\exp\\!\\left(-C\\|h\\|_{2}^{2}/2\\right)$ for $h\\in\\mathbb{R}^{d_{0}}$ and constant $C>0$ . Let $r_{h}>0$ be a fixed radius. Then it holds ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int_{\\|h\\|_{2}>r_{h}}p_{h}(h)\\mathrm{d}h\\leq\\frac{2d_{0}\\pi^{d_{0}/2}}{C\\Gamma(d_{0}/2+1)}r_{h}^{d_{0}-2}\\exp\\bigl(-C r_{h}^{2}/2\\bigr),}\\\\ &{\\displaystyle\\int_{\\|h\\|_{2}>r_{h}}\\|h\\|_{2}^{2}p_{h}(h)\\mathrm{d}h\\leq\\frac{2d_{0}\\pi^{d_{0}/2}}{C\\Gamma(d_{0}/2+1)}r_{h}^{d_{0}}\\exp\\bigl(-C r_{h}^{2}/2\\bigr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Lemma F.2 (Lemma 2 of [Chen et al., 2023]). Suppose Assumption 2.2 holds and $g$ is defined as: ", "page_idx": 39}, {"type": "equation", "text": "$$\nq(\\overline{{h}},t)=\\int\\frac{h\\psi_{t}(\\overline{{h}}|h)p_{h}(h)}{\\int\\psi_{t}(\\overline{{h}}|h)p_{h}(h)\\mathrm{d}h}\\mathrm{d}h,\\quad\\overline{{h}}=B^{\\top}\\overline{{x}}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Given $\\epsilon>0$ , with $r_{h}=c\\left(\\sqrt{d_{0}\\log(d_{0}/T_{0})+\\log(1/\\epsilon)}\\right)$ for an absolute constant $c$ , it holds ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left\\|q({\\overline{{h}}},t)\\mathbf{1}\\{\\left\\|{\\overline{{h}}}\\right\\|_{2}\\geq r_{h}\\}\\right\\|_{L^{2}(P_{t})}\\leq\\epsilon,{\\mathrm{~for~}}t\\in[T_{0},T].\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Lemma F.3 (Theorem 1 of [Chen et al., 2023]). We denote ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\tau(r_{h})=\\operatorname*{sup}_{t\\in[T_{0},T]}\\operatorname*{sup}_{\\bar{h}\\in[0,r_{h}]^{d}}\\bigg\\|\\frac{\\partial}{\\partial t}q(\\bar{h},t)\\bigg\\|_{2}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "With $\\begin{array}{r}{q(\\overline{{h}},t)=\\int h\\psi_{t}(\\overline{{h}}|h)p_{h}(h)/(\\int\\psi_{t}(\\overline{{h}}|h)p_{h}(h)\\mathrm{d}h)\\mathrm{d}h}\\end{array}$ and $p_{h}$ satisfies Assumption 2.2, we have a coarse upper bound for $\\tau(r_{h})$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\tau(r_{h})=\\mathcal{O}\\left(\\frac{1+\\beta^{2}(t)}{\\beta(t)}\\left(L_{s_{+}}+\\frac{1}{\\sigma(t)}\\right)\\sqrt{d_{0}}r_{h}\\right)=\\mathcal{O}\\left(e^{T/2}L_{s_{+}}r_{h}\\sqrt{d_{0}}\\right).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Lemma F.4 (Lemma 10 of [Chen et al., 2020b]). For any given $\\epsilon>0$ ,and $L$ -Lipschitz function $g$ defined on $[0,1]^{d_{0}}$ , there exists a continuous function $\\bar{f}$ constructed by trapezoid function, such that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left\\|g-{\\bar{f}}\\right\\|_{\\infty}\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Moreover, the Lipschitz continuity of $\\bar{f}$ is bounded: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\overline{{f}}(x)-\\overline{{f}}(y)\\right|\\leq10d_{0}L\\|x-y\\|_{2}\\quad\\mathrm{for~any}\\quad x,y\\in[0,1]^{d_{0}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "F.1.2 Main Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Proof of Theorem 3.1. With $\\nabla\\log p_{t}^{h}\\left(\\bar{h}\\right)=B^{\\top}s_{+}(\\bar{h},t)$ , we have the following in (2.4) ", "page_idx": 40}, {"type": "equation", "text": "$$\nq(\\bar{h},t)=\\sigma(t)\\nabla\\log p_{t}^{h}\\left(\\bar{h}\\right)+B^{\\top}\\bar{x}=\\sigma(t)B^{\\top}(s_{+}(\\bar{h},t)+\\bar{x}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "We proceed as follows: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 Step 1. Approximate $q(\\overline{{h}},t)$ with a compact-supported continuous function $\\bar{f}(\\overline{{h}},t)$ ", "page_idx": 40}, {"type": "text", "text": "\u00b7 Step 2. Approximate $\\bar{f}(\\overline{{h}},t)$ with a transformer network. ", "page_idx": 40}, {"type": "text", "text": "Step 1. Approximate $q(\\overline{{h}},t)$ with a Compact-supported Continuous Function $\\bar{f}(\\overline{{h}},t)$ .We partition $\\mathbb{R}^{d_{0}}$ into a compact subset $\\begin{array}{r}{H_{1}:=\\left.\\{\\bar{h}|\\|\\bar{h}\\|_{2}\\leq r_{h}\\right\\}}\\end{array}$ and its complement $H_{2}$ , where $r_{h}$ is to be determined later. We approximate $q(\\overline{{h}},t)$ on the two subsets respectively and then prove $\\bar{f}$ continuity. Such a step achieves an estimation error of $\\sqrt{d_{0}}\\epsilon$ between $q(\\overline{{h}},t)$ and $\\bar{f}(\\bar{h},t)$ .We show the main proof here. ", "page_idx": 40}, {"type": "text", "text": "\u00b7 Approximation on $H_{2}\\times[T_{0},T]$ . For any $\\epsilon>0$ , we take $r_{h}=c(\\sqrt{d_{0}\\log(d_{0}/T_{0})-\\log\\epsilon})$ . From Lemma F.2, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left\\|q(\\overline{{h}},t)\\mathbb{1}\\{\\left\\|\\overline{{h}}\\right\\|_{2}\\geq r_{h}\\}\\right\\|_{L^{2}(P_{t})}\\leq\\epsilon\\quad\\mathrm{for}\\quad t\\in[T_{0},T].\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "So we set $\\overline{{f}}(\\overline{{h}},t)=0$ on $H_{2}\\times[T_{0},T]$ ", "page_idx": 40}, {"type": "text", "text": "\u00b7Approximation on $H_{1}\\times[T_{0},T]$ . On $H_{1}\\times[T_{0},T]$ , we approximate $q(\\overline{{h}},t)$ by approximating each coordinate $q_{k}(\\overline{{h}},t)$ respectively, where $q(\\bar{h},t)=[q_{1}(\\bar{h},t),q_{2}(\\bar{h},t),\\cdot\\cdot\\cdot\\,,q_{d_{0}}(\\bar{h},t)]$ . We rescale the input by $y^{\\prime}=(\\bar{h}\\!+\\!r_{h}\\mathbb{1})/2r_{h}$ and $t^{\\prime}=t/T$ . Then the transformed input space is $[0,1]^{d_{0}}\\times[T_{0}/T,1]$ We implement such a transformation by a single feed-forward layer. ", "page_idx": 40}, {"type": "text", "text": "By Assumption 2.3, on-support score $s_{+}(\\bar{h},t)$ is $L_{s_{+}}$ -Lipschitz in $\\bar{h}$ .This implies $q(\\overline{{h}},t)$ is $(1+L_{s_{+}})$ -Lipschitz in $\\overline{{h}}$ . When taking the transformed inputs, $g(y^{\\prime},t^{\\prime})=q(2r_{h}y^{\\prime}-r_{h}\\mathbb{1},T t^{\\prime})$ becomes $2r_{h}(1+L_{s_{+}})$ -Lipschitz in $y^{\\prime}$ . Similarly, each coordinate $g_{k}(y^{\\prime},t)$ is also $2r_{h}(1+L_{s_{+}})$ Lipschitz in $y^{\\prime}$ . Here we take ${{L}_{h}}=1+{{L}_{{{s}_{+}}}}$ ", "page_idx": 40}, {"type": "text", "text": "Besides, $g(y^{\\prime},t^{\\prime})$ is $T\\tau(r_{h})$ -Lipsichitz with respect to $t$ ,where ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\tau(r_{h})=\\operatorname*{sup}_{t\\in[T_{0},T]}\\operatorname*{sup}_{\\bar{h}\\in[0,r_{h}]^{d}}\\bigg\\|\\frac{\\partial}{\\partial t}q(\\bar{h},t)\\bigg\\|_{2}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "We have a coarse upper bound for $\\tau(r_{h})$ in Lemma F.3. We restate it here for convenience ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\tau(r_{h})=\\mathcal{O}\\left(\\frac{1+\\beta^{2}(t)}{\\beta(t)}\\left(L_{s_{+}}+\\frac{1}{\\sigma(t)}\\right)\\sqrt{d_{0}}r_{h}\\right)=\\mathcal{O}\\left(e^{T/2}L_{s_{+}}r_{h}\\sqrt{d_{0}}\\right).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "In conclusion, each $g_{k}(y^{\\prime},t)$ is Lipsichitz continuous. So we can apply Lemma F.4 to determine $\\overline{{f}}_{k}(y^{\\prime},t)$ for approximating each coordinate. We concatenate $\\bar{f_{i}}$ 's together and construct ${\\overline{{f}}}\\,=$ $[\\overline{{f}}_{1},\\ldots,\\overline{{f}}_{d_{0}}]^{\\top}$ . According to the construction in Lemma F.4 and for any given $\\epsilon$ weachieve ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{y^{\\prime},t^{\\prime}\\in[0,1]^{d}\\times[T_{0}/T,1]}\\left\\|\\bar{f}(y^{\\prime},t^{\\prime})-g(y^{\\prime},t^{\\prime})\\right\\|_{\\infty}\\leq\\epsilon,\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Considering the input rescaling (i.e., $\\overline{{h}}\\rightarrow y^{\\prime}$ and $t\\rightarrow t^{\\prime}$ ), we obtain: ", "page_idx": 41}, {"type": "text", "text": "- The constructed function is Lipschitz continuous in $\\overline{{h}}$ . For any $\\overline{{h}}_{1},\\overline{{h}}_{2}\\in H_{1}$ and $t\\in[T_{0},T]$ ,it holds ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left\\|\\bar{f}(\\bar{h}_{1},t)-\\bar{f}(\\bar{h}_{2},t)\\right\\|_{\\infty}\\leq10d_{0}L_{h}\\left\\|\\bar{h}_{1}-\\bar{h}_{2}\\right\\|_{2}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "- The function is also Lipschitz in $t$ . For any $t_{1},t_{2}\\in[T_{0},T]$ and $\\left\\|\\overline{{h}}\\right\\|_{2}\\leq r_{h}$ , it holds ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\bar{f}(\\bar{h},t_{1})-\\bar{f}(\\bar{h},t_{2})\\right\\|_{\\infty}\\leq10\\tau(r_{h})\\|t_{1}-t_{2}\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Due to the fact that the construction of $\\bar{f}(\\bar{h},t)$ is based on trapezoid function, we have $\\overline{{f}}(\\overline{{h}},t)=0$ for $\\left\\|\\overline{{h}}\\right\\|_{2}=r_{h}$ and any $t\\in[T_{0},T]$ . Thus, the two parts of $\\bar{f}(\\overline{{h}},t)$ can be joined together. To be more specific, the above Lipschitz continuity in $\\bar{h}$ extends to the whole $\\mathbb{R}^{d_{0}}$ ", "page_idx": 41}, {"type": "text", "text": "\u00b7 Approximation Error Analysis under $L^{2}$ Norm. The $L^{2}$ approximation error of $\\bar{f}$ can be decomposed into two terms: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\lvert\\left\\lvert q(\\bar{h},t)-\\bar{f}(\\bar{h},t)\\right\\rvert\\right\\rvert_{L^{2}(P_{t}^{h})}}\\\\ &{=\\left\\lVert(q(\\bar{h},t)-\\bar{f}(\\bar{h},t))\\Im\\{\\left\\lVert\\bar{h}\\right\\rVert_{2}<r_{h}\\}\\right\\rVert_{L^{2}(P_{t}^{h})}+\\left\\lVert q(\\bar{h},t)\\Im\\{\\left\\lVert\\bar{h}\\right\\rVert_{2}>r_{h}\\}\\right\\rVert_{L^{2}(P_{t}^{h})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "The second term in the RHS above has already been bounded with the selection of $r_{h}$ ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left\\|g(\\bar{h},t)\\Im\\left\\{\\left\\|\\bar{h}\\right\\|_{2}>r_{h}\\right\\}\\right\\|_{L^{2}(P_{t}^{h})}\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "The first term is bounded by: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\Vert(q(\\bar{h},t)-\\bar{f}(\\bar{h},t))\\mathbb{1}\\{\\left\\Vert\\bar{h}\\right\\Vert_{2}<r_{h}\\}\\right\\Vert_{L^{2}(P_{t}^{h})}}\\\\ &{\\leq\\sqrt{d_{0}}\\operatorname*{sup}_{y^{\\prime},t^{\\prime}\\in[0,1]^{d}\\times[T_{0}/T,1]}\\left\\Vert\\bar{f}(y^{\\prime},t^{\\prime})-g(y^{\\prime},t^{\\prime})\\right\\Vert_{\\infty}}\\\\ &{\\leq\\sqrt{d_{0}}\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Then we obtain ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\|q(\\overline{{h}},t)-\\overline{{f}}(\\overline{{h}},t)\\|_{L^{2}(P_{t}^{h})}\\leq(\\sqrt{d_{0}}+1)\\epsilon.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "If we substitute $\\epsilon$ with $\\epsilon/2$ , we obtain that the approximation error of $\\bar{f}(\\bar{h},t)$ is $\\sqrt{d_{0}}\\epsilon$ ", "page_idx": 41}, {"type": "text", "text": "Step 2. Approximate $\\bar{f}(\\bar{h},t)$ by a Transformer. This step is based on the universal approximation of transformers for the compact-supported continuous function in Lemma E.1. DiT uses time point $t$ to calculate the scale and shift value in the transformer backbone [Peebles and Xie, 2023]. It also transforms an input picture into a sequential version. We ignore time point $t$ in the notation of the transformer network in DiT. Recall the reshape layer $R(\\cdot)$ in Definition 3.1, we consider using $f(\\cdot):=R^{-1}\\circ f_{T}\\circ R(\\cdot)$ to approximate $\\overline{{f}}_{t}(\\cdot):=\\overline{{f}}(\\cdot,t)$ , where $f_{T}\\in\\mathcal{T}_{p}^{2,1,4}$ ", "page_idx": 41}, {"type": "text", "text": "\u00b7 Overall Approximation Error.  With Lemma E.1, we approximate $\\overline{{f}}_{t}(\\cdot)$ With ${\\widehat{f}}(\\cdot)\\ :=$ $R^{-1}\\circ\\widehat{f}_{\\mathcal{T}}\\circ R(\\cdot)$ Wedenote ", "page_idx": 41}, {"type": "equation", "text": "$$\nH=R(\\overline{{h}}).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left\\|{\\overline{{f}}}_{t}({\\overline{{h}}})-{\\widehat{f}}({\\overline{{h}}})\\right\\|_{L^{2}(P_{t}^{h})}=\\left(\\int_{P_{t}^{h}}\\left\\|{\\overline{{f}}}_{t}({\\overline{{h}}})-{\\widehat{f}}({\\overline{{h}}})\\right\\|_{2}^{2}\\!\\mathrm{d}h\\right)^{1/2}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\left(\\displaystyle\\int_{P_{t}^{h}}\\Big\\|R\\circ\\bar{f}_{t}\\circ R^{-1}(H)-R\\circ\\widehat{f}\\circ R^{-1}(H)\\Big\\|_{F}^{2}\\mathrm{d}h\\right)^{1/2}}\\\\ &{=\\left(\\displaystyle\\int_{P_{t}^{h}}\\Big\\|R\\circ\\bar{f}_{t}\\circ R^{-1}(H)-\\widehat{f}_{T}(H)\\Big\\|_{F}^{2}\\mathrm{d}h\\right)^{1/2}}\\\\ &{<\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Along with Step 1, we obtain ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left\\|q(\\overline{{h}},t)-\\widehat{f}(\\overline{{h}})\\right\\|_{L^{2}(P_{t}^{h})}\\leq\\left\\|q(\\overline{{h}},t)-\\overline{{f}}(\\overline{{h}},t)\\right\\|_{L^{2}(P_{t}^{h})}+\\left\\|\\overline{{f}}(\\overline{{h}},t)-\\widehat{f}(\\overline{{h}})\\right\\|_{L^{2}(P_{t}^{h})}\\leq(1+\\sqrt{d_{0}})\\epsilon.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "The constructed approximator to $\\nabla\\log{p_{t}(x)}$ .s $\\boldsymbol{s}_{\\widehat{W}}=(\\boldsymbol{B}\\widehat{f}(\\boldsymbol{B}^{\\top}\\boldsymbol{x},t)-\\boldsymbol{x})/\\sigma(t)$ , and the approximation error is ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\Big\\|\\nabla\\log p_{t}(\\cdot)-s_{\\widehat{W}}(\\cdot,t)\\Big\\|_{L^{2}(P_{t})}\\leq\\frac{1+\\sqrt{d_{0}}}{\\sigma(t)}\\epsilon\\quad\\mathrm{for~any}\\quad t\\in[T_{0},T].\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "\u00b7 Settling-down of Hyperparameters. We settle down the hyperparameters to configure our network here. We refer to Appendix E.2 for some of the following calculations. ", "page_idx": 42}, {"type": "text", "text": "1. Model Architecture Depth $K$ ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "From Lemma E.6,we have $K\\,=\\,\\mathcal{O}((1/\\delta)^{d L})$ .To achieve $\\epsilon\\cdot$ error approximation, we set $\\delta=\\mathcal{O}\\left(\\epsilon^{2/d}\\right)$ according to Lemma E.3. Thus we obtain ", "page_idx": 42}, {"type": "equation", "text": "$$\nK=\\mathcal{O}\\left(\\epsilon^{-2L}\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "2. Lipchitz Upperbound for Transformer: $L\\tau$ ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "We denote $\\overline{{f}}_{t,R}(\\cdot)=R\\circ\\overline{{f}}_{t}\\circ R^{-1}(\\cdot)$ .We get the Lipshitz upper bound for $\\widehat{f}_{T}\\in T_{p}^{2.1.4}$ in the following way ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left\\|\\widehat{f}_{T}\\left(H_{1}\\right)-\\widehat{f}_{T}\\left(H_{2}\\right)\\right\\|_{F}\\leq\\left\\|\\widehat{f}_{T}\\left(H_{1}\\right)-\\overline{{f}}_{t,R}\\left(H_{1}\\right)\\right\\|_{F}+\\left\\|\\overline{{f}}_{t,R}\\left(H_{1}\\right)-\\overline{{f}}_{t,R}\\left(H_{2}\\right)\\right\\|_{F}}}\\\\ &{}&{+\\left\\|\\overline{{f}}_{t,R}\\left(H_{2}\\right)-\\widehat{f}_{T}\\left(H_{2}\\right)\\right\\|_{F}}\\\\ &{}&{\\leq2\\epsilon+\\left\\|\\overline{{f}}_{t,R}\\left(H_{1}\\right)-\\overline{{f}}_{t,R}\\left(H_{2}\\right)\\right\\|_{F}}\\\\ &{}&{\\leq2\\epsilon+10d_{0}L_{s+}\\|H_{1}-H_{2}\\|_{F}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Then we get ", "page_idx": 42}, {"type": "equation", "text": "$$\nL_{T}=\\mathcal{O}\\left(d_{0}L_{s_{+}}\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "3. Model Output Bound for $S_{\\mathcal{T}_{p}^{2,1,4}}$ ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "For the output of the constructed transformer $\\widehat{f}_{T}(\\cdot)$ , according to Lemma E.5, we have $\\widehat{f}_{\\mathcal{T}}(O)=$ $O$ , where $O=\\mathbf{0}_{d\\times L}$ . Thus, with the Lipschitz upperbound $\\mathcal{O}(d_{0}L_{s_{+}})$ , we have $\\|\\widehat{f}_{\\mathcal{T}}(H)\\|_{F}=$ $O(d_{0}L_{s_{+}}r_{h})$ ,where $\\|H\\|_{F}\\leq r_{h}$ .With $r_{h}=c(\\sqrt{d_{0}\\log(d_{0}/T_{0})+\\log(1/\\epsilon)})$ , we obtain ", "page_idx": 42}, {"type": "equation", "text": "$$\nC_{T}=\\mathcal{O}\\left(d_{0}L_{s_{+}}\\cdot\\sqrt{d_{0}\\log(d_{0}/T_{0})+\\log(1/\\epsilon)}\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "4. Model Parameters Bound: $C_{O V}^{2,\\infty},C_{O V},C_{K Q}^{2,\\infty},C_{K Q},C_{E}.$ ", "page_idx": 42}, {"type": "text", "text": "By definition, we have: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|(W_{O V}^{i})^{\\top}\\right\\|_{2,\\infty}\\leq C_{O V}^{2,\\infty},\\ \\left\\|(W_{O V}^{i})^{\\top}\\right\\|_{2}\\leq C_{O V},\\ \\left\\|W_{K Q}^{i}\\right\\|_{2,\\infty}\\leq C_{K Q}^{2,\\infty},\\ \\left\\|W_{K Q}^{i}\\right\\|_{2}\\leq C_{K Q},}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $i=1,2$ . For simplicity, we omit $i$ hereafter, which does not affect our discussion. ", "page_idx": 43}, {"type": "text", "text": "Recall that $\\|Z\\|_{2,\\infty}$ denotes the $2,\\infty$ -norm, where the 2-norm is over columns and $\\infty$ -norm is over rows. By the construction of modified attention layers (E.11) and (E.12) in Appendix E.5.3, we consider $W_{O V}$ to have the largest norm, i.e., ", "page_idx": 43}, {"type": "equation", "text": "$$\nW_{O V}=L\\delta^{-(L+1)d-1}\\cdot\\left(\\begin{array}{c c c c}{{1}}&{{\\delta^{-1}}}&{{\\cdots}}&{{\\delta^{-d+1}}}\\\\ {{0}}&{{0}}&{{\\cdots}}&{{0}}\\\\ {{\\vdots}}&{{\\vdots}}&{{\\ddots}}&{{\\vdots}}\\\\ {{0}}&{{0}}&{{\\cdots}}&{{0}}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "We give the following upper bounds ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|{W_{O V}^{\\top}}\\right\\|_{2,\\infty}=L d\\delta^{-(L+2)d}=\\mathcal{O}\\left(\\delta^{-L d}\\right),}\\\\ &{\\quad\\left\\|{W_{O V}^{\\top}}\\right\\|_{2}=\\underset{\\|x\\|_{2}=1}{\\operatorname*{sup}}\\left\\|{W_{O V}^{\\top}}x\\right\\|_{2}=L\\delta^{-(L+1)d-1}\\cdot\\sqrt{\\underset{i=0}{\\sum}\\delta^{-2i}}=\\mathcal{O}\\left(\\delta^{-L d}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "By (E.11) and (E.12) in Appendix E.5.3, and the self-attention layers in Appendix E.5.5, we consider $W_{K Q}$ to have the largest norm, i.e., ", "page_idx": 43}, {"type": "equation", "text": "$$\nW_{K Q}:=\\left(\\begin{array}{c}{{1}}\\\\ {{\\delta^{-1}}}\\\\ {{\\vdots}}\\\\ {{\\delta^{-d+1}}}\\end{array}\\right)\\left(1,\\delta^{-1},\\cdots,\\delta^{-d+1}\\right)=\\left(\\begin{array}{c c c c}{{1}}&{{\\delta^{-1}}}&{{\\cdots}}&{{\\delta^{-d+1}}}\\\\ {{\\delta^{-1}}}&{{\\delta^{-2}}}&{{\\cdots}}&{{\\delta^{-d}}}\\\\ {{\\vdots}}&{{\\vdots}}&{{\\ddots}}&{{\\vdots}}\\\\ {{\\delta^{-d+1}}}&{{\\delta^{-d}}}&{{\\cdots}}&{{\\delta^{-2d+2}}}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Then we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|W_{K Q}\\|_{2,\\infty}=\\sqrt{\\displaystyle\\sum_{i=0}^{d-1}\\delta^{-2i-2d+2}}=\\mathcal{O}(\\delta^{-2d}),}\\\\ &{\\|W_{K Q}\\|_{2}=\\operatorname*{sup}_{\\|x\\|_{2}=1}\\|W_{K Q}x\\|_{2}=\\delta^{-2d+2}=\\mathcal{O}(\\delta^{-2d}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "We substitute $\\delta$ with $\\mathcal{O}\\left(\\epsilon^{2/d}\\right)$ (according to Appendix E.4) and get: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C_{O V}^{2,\\infty}=(1/\\epsilon)^{\\mathcal{O}(1)},}\\\\ {C_{O V}=(1/\\epsilon)^{\\mathcal{O}(1)},}\\\\ {C_{K Q}^{2,\\infty}=(1/\\epsilon)^{\\mathcal{O}(1)},}\\\\ {C_{K Q}=(1/\\epsilon)^{\\mathcal{O}(1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "From the construction of positional encoder (E.4) in Appendix E.2, we have ", "page_idx": 43}, {"type": "equation", "text": "$$\nE=\\left(\\begin{array}{c c c c}{{0}}&{{1}}&{{\\cdot\\cdot\\cdot}}&{{L-1}}\\\\ {{}}&{{1}}&{{\\cdot\\cdot\\cdot}}&{{L-1}}\\\\ {{\\vdots}}&{{\\vdots}}&{{\\vdots}}&{{\\vdots}}\\\\ {{\\vdots}}&{{\\vdots}}&{{\\vdots}}&{{\\vdots}}\\\\ {{0}}&{{1}}&{{\\cdot\\cdot}}&{{L-1}}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "We deduce ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\|E^{\\top}\\|_{2,\\infty}=\\sqrt{L}(L-1)=\\mathcal{O}(L^{3/2}).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Thus we have ", "page_idx": 44}, {"type": "equation", "text": "$$\nC_{E}={\\cal O}(L^{3/2}).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "5. Parameters Bound in Feed Forward Layers: $C_{F}^{2,\\infty},C_{F}$ ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Recall the construction of modified feed-forward layers in the proof of Lemma E.4, which includes Definitions E.5, E.6 and E.8 to E.10. With the approximation by normal feed-forward layers in Appendix E.5.5, we consider the weight parameters with the largest norm in the feed-forward layers, i.e., ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{W_{1}:=\\left(\\begin{array}{l}{1}\\\\ {1}\\\\ {1}\\\\ {1}\\end{array}\\right)\\left(1,\\delta^{-1},\\cdots,\\delta^{-d+1}\\right)=\\left(\\begin{array}{l l l l}{1}&{\\delta^{-1}}&{\\cdots}&{\\delta^{-d+1}}\\\\ {1}&{\\delta^{-1}}&{\\cdots}&{\\delta^{-d+1}}\\\\ {1}&{\\delta^{-1}}&{\\cdots}&{\\delta^{-d+1}}\\\\ {1}&{\\delta^{-1}}&{\\cdots}&{\\delta^{-d+1}}\\end{array}\\right)\\in\\mathbb{R}^{4\\times d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Then we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{C_{F}^{2,\\infty}=\\mathcal{O}\\left(\\sqrt{\\sum_{i=0}^{d-1}\\delta^{-2i}}\\right)=\\mathcal{O}\\left(\\delta^{-d}\\right)}}\\\\ &{}&{=(1/\\epsilon)^{\\mathcal{O}(1)}.\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\left(\\mathrm{By~sett}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{C_{F}=\\operatorname*{sup}_{\\|\\boldsymbol{x}\\|_{2}=1}\\|W_{1}\\boldsymbol{x}\\|_{2}=\\mathcal{O}\\left(\\delta^{-d}\\right)}}\\\\ &{}&\\\\ &{=(1/\\epsilon)^{\\mathcal{O}(1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "This completes the proof. ", "page_idx": 44}, {"type": "text", "text": "F.2Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Here we present the auxiliary theoretical results about the covering number of transformer networks in Appendix F.2.1. The results are based on [Edelman et al., 2022, Theorem A.17]. Then we derive the sample complexity bound of DiTs (i.e., the proof of Theorem 3.2) in Appendix F.2. ", "page_idx": 45}, {"type": "text", "text": "F.2.1 Auxiliary Lemmas for Theorem 3.2 ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Lemma F.5 (Lemma 15 of [Chen et al., 2023]). Let $\\mathcal{G}$ be a bounded function class. Then there exists a constant $b$ such that the output of any $g\\in\\mathcal{G}:\\mathbb{R}^{d_{0}}\\mapsto[0,b]$ is bounded by $b$ . Let $z_{1},z_{2},\\cdots\\,,z_{n}\\in\\mathbb{R}^{d_{0}}$ be i.i.d. random variables. For any $\\delta\\in(0,1),a\\le1$ , and $c>0$ , we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{P\\displaystyle\\left(\\underset{g\\in\\mathcal{G}}{\\operatorname*{sup}}\\frac{1}{n}\\sum_{i=1}^{n}g(z_{i})-(1+a)\\mathbb{E}\\left[g(z)\\right]>\\frac{(1+3/a)B}{3n}\\log\\frac{\\mathcal{N}(c,\\mathcal{G},\\|\\cdot\\|_{\\infty})}{\\delta}+(2+a)c\\right)\\leq\\delta,}\\\\ {P\\displaystyle\\left(\\underset{g\\in\\mathcal{G}}{\\operatorname*{sup}}\\mathbb{E}\\left[g(z)\\right]-\\frac{1+a}{n}\\sum_{i=1}^{n}g(z_{i})>\\frac{(1+6/a)B}{3n}\\log\\frac{\\mathcal{N}(c,\\mathcal{G},\\|\\cdot\\|_{\\infty})}{\\delta}+(2+a)c\\right)\\leq\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Now, we give the definition of the covering number as follows. ", "page_idx": 45}, {"type": "text", "text": "Definition F.1 (Covering Number). Given a function class $\\mathcal{F}$ and a data distribution $P$ .  Sample n data points $\\{X_{i}\\}_{i=1}^{n}$ from $P$ . For any $\\epsilon>0$ , the covering number ${\\mathcal{N}}(\\epsilon,{\\mathcal{F}},\\{X_{i}\\}_{i=1}^{n},\\|\\cdot\\|)$ is the smallest size of a collection (a cover) ${\\mathcal{C}}\\in{\\mathcal{F}}$ , such that for any $f\\in\\mathcal F$ , there exists a $\\widehat{f}\\in\\mathcal{C}$ satisfying ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i}\\left\\|f(X_{i})-{\\widehat{f}}(X_{i})\\right\\|\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Furthermore, we define the covering number with respect to the data distribution as ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathcal{N}(\\epsilon,\\mathcal{F},\\|\\cdot\\|)=\\operatorname*{sup}_{\\{X_{i}\\}_{i=1}^{n}\\sim P}\\mathcal{N}(\\epsilon,\\mathcal{F},\\{X_{i}\\}_{i=1}^{n},\\|\\cdot\\|).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Then we give the covering number of the transformer networks. ", "page_idx": 45}, {"type": "text", "text": "Lemma F.6 (Modified from Theorem A.17 of [Edelman et al., 2022]). Let $\\mathcal{T}_{p}^{r,m,l}(K,C_{T},C_{O V}^{2,\\infty},C_{O V},C_{K Q}^{2,\\infty},C_{K Q},C_{F}^{2,\\infty},C_{F},C_{E},L_{T})$ repesent the las of funtions of $K$ -layer transformer blocks satisfying the norm bound for matrix and Lipsichitz property for feed-forward layers. Then for all data point $\\|X\\|_{2,\\infty}\\leq C_{X}$ wehave ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ \\log\\mathcal{N}(\\epsilon_{c},\\mathcal{T}_{p}^{r,m,l}(K,C_{T},C_{O V}^{2,\\infty},C_{O V},C_{K Q}^{2,\\infty},C_{K Q},C_{F}^{2,\\infty},C_{F},C_{E},L_{T}),\\|\\cdot\\|_{2})}\\\\ &{\\le\\!\\frac{\\log(n L)}{\\epsilon_{c}^{2}}\\cdot\\left(\\displaystyle\\sum_{i=1}^{K}\\alpha^{\\frac{2}{3}}\\left(d^{\\frac{2}{3}}\\left(C_{F}^{2,\\infty}\\right)^{\\frac{4}{3}}+d^{\\frac{2}{3}}\\left(2(C_{F})^{2}C_{O V}C_{K Q}^{2,\\infty}\\right)^{\\frac{2}{3}}+\\tau m^{\\frac{2}{3}}\\left((C_{F})^{2}C_{O V}^{2,\\infty}\\right)^{\\frac{2}{3}}\\right)\\right)^{3}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where $\\begin{array}{r}{\\alpha:=\\prod_{j<i}(C_{F})^{2}C_{O V}(1+4C_{K Q})(C_{X}+C_{E}).}\\end{array}$ ", "page_idx": 45}, {"type": "text", "text": "Remark F.1. We modify [Edelman et al., 2022, Theorem A.17] in seven aspects: ", "page_idx": 45}, {"type": "text", "text": "1. We do not consider the last linear layer in the model, which converts each column vector of the transformer output to a scalar. Therefore, we ignore the item related to the last linear layer in [Edelman et al., 2022, Theorem A.17].   \n2. We do not consider the normalization layer in our model. Because the normalization layer $\\prod_{\\mathrm{norm}}(\\cdot)$ in the original proof only ensures that $\\begin{array}{r}{\\left\\|\\prod_{\\mathrm{norm}}(X_{1})-\\prod_{\\mathrm{norm}}(X_{2})\\right\\|_{2,\\infty}\\ \\leq}\\end{array}$ $\\|X_{1}-X_{2}\\|_{2,\\infty}$ , ignoring this layer does not change the result.   \n3. Our activation function is ReLU. Thus, we replace the Lipschitz upperbound of activate function by 1. ", "page_idx": 45}, {"type": "text", "text": "4. We consider the positional encoding (E.4). Then we need to replace the upperbound $C_{X}$ for the inputs with the upperbound $C_{X}+C_{E}$ . Besides, for multi-layer transformer, the original conclusion in [Edelman et al., 2022, Theorem A.17] uses 1 as the upperbound for the 2, $\\infty$ -norm of inputs. We incorporate the upperbound for the inputs into the result stated in Lemma F.6. ", "page_idx": 46}, {"type": "text", "text": "5. We use (2.7) as the feed-forward layer, including two linear layers and a residual layer. Thus, we replace the original upperbound for the norm of weight matrix with the upperbound for the norm Oof $I_{d}+W_{2}W_{1}$ in Lemma F.6. In the following, we use $\\scriptscriptstyle\\mathcal{O}$ to estimate the log-covering number, thus we ignore the item for $I_{d}$ here for converience. This is the same for the self-attention layer. ", "page_idx": 46}, {"type": "text", "text": "6. We use multi-head attention, and incorporate the number of heads $\\tau$ into our result, which is similar to [Edelman et al., 2022, Theorem A.12]. ", "page_idx": 46}, {"type": "text", "text": "7. In our work, we use the transformer $\\mathcal{T}_{p}^{2,1,4}$ , i.e., $\\tau=2,m=1$ ", "page_idx": 46}, {"type": "text", "text": "F.2.2 Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Proof of Theorem 3.2. Our proof is built on [Chen et al., 2023, Appendix B.2]. For one data sample, we define the empirical score matching loss objective (2.1) as follows ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\ell(x;s_{\\widehat W})=\\frac{1}{T-T_{0}}\\int_{T_{0}}^{T}\\mathbb{E}_{x_{t}|x_{0}=x}[\\Big\\|\\nabla_{x_{t}}\\log\\psi_{t}(x_{t}|x_{0})-s_{\\widehat W}(x_{t},t)\\Big\\|_{2}^{2}]\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Then we define $\\mathcal{L}(s_{\\widehat{W}})=\\mathbb{E}_{x\\sim P_{0}}\\left[\\ell(x;s_{\\widehat{W}})\\right]$ ", "page_idx": 46}, {"type": "text", "text": "Following [Chen et al., 2023, Appendix B.2], for any $a\\in(0,1)$ ,wehave ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{L}(s_{\\widehat{W}})}\\\\ &{\\leq\\underbrace{\\mathcal{L}^{\\mathrm{trunc}}(s_{\\widehat{W}})-(1+a)\\widehat{\\mathcal{L}}^{\\mathrm{trunc}}(s_{\\widehat{W}})}_{(I)}+\\underbrace{\\mathcal{L}(s_{\\widehat{W}})-\\mathcal{L}^{\\mathrm{trunc}}(s_{\\widehat{W}})}_{(I I)}+(1+a)\\underbrace{\\operatorname*{inf}_{s_{W}\\in\\mathcal{S}_{\\mathrm{NN}}}\\widehat{\\mathcal{L}}(s_{W})}_{(I I I)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal L^{\\mathrm{trunc}}\\big({s}_{\\widehat W}\\big):=\\mathbb E_{{x}\\sim{P}_{0}}\\left[\\ell^{\\mathrm{trunc}}\\big({x};{s}_{\\widehat W}\\big)\\right]=\\mathbb E_{{x}\\sim{P}_{0}}\\left[\\ell({x};{s}_{\\widehat W})\\mathbb{1}\\{\\|{x}\\|_{2}\\leq{r}_{x}\\}\\right],\\;{r}_{x}>B.}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "We denote ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta:=4C_{\\mathcal{T}}(C_{\\mathcal{T}}+r_{x})(r_{x}/D)^{D-2}\\exp\\bigl(-r_{x}^{2}/\\sigma(t)\\bigr)/(T_{0}(T-T_{0})),}\\\\ &{r_{x}:=\\mathcal{O}\\left(\\sqrt{d_{0}\\log d_{0}+\\log C_{\\mathcal{T}}+\\log\\bigl(n/\\bar{\\delta}\\bigr)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Then we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\eta\\leq\\frac{1}{n T_{0}(T-T_{0})}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "For any $\\overline{{\\delta}}>0$ , according to Lemma F.5, the following holds for term $(I)$ with probability $1-\\bar{\\delta}$ \uff0c ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(I)=\\mathcal{O}\\left(\\frac{\\left(1+6/a\\right)\\left(C_{T}^{2}+r_{x}^{2}\\right)}{n T_{0}\\left(T-T_{0}\\right)}\\log\\frac{\\mathcal{N}\\left(\\frac{\\left(T-T_{0}\\right)\\left(\\iota-\\eta\\right)}{\\left(C_{T}+r_{x}\\right)\\log\\left(T/T_{0}\\right)},S_{T_{p}^{2,1,4}},\\|\\cdot\\|_{2}\\right)}{\\bar{\\delta}}+(2+a)\\iota\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where $c\\leq0$ is a constant, and $\\iota>0$ will be determined later. ", "page_idx": 46}, {"type": "text", "text": "We set ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\iota:=\\frac{2}{n^{b}T_{0}(T-T_{0})},\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where $0<b\\le1$ is a constant to be determined later. ", "page_idx": 47}, {"type": "text", "text": "Remark F.2 (Selection Criteria of $\\tau$ ). We have two criteria: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 Recall that the covering number used in our setting is ((CT+a)log(/o)  ST2,1,Il2)  Thus, we must ensure $\\iota\\geq\\eta$ . According to (F.14), we consider $\\iota$ satisfying the condition $\\iota\\geq(n{\\stackrel{\\cdot}{n}}T_{0}(T-$ $T_{0}))^{-1}$ . Therefore, we consider $0<b\\le1$ ", "page_idx": 47}, {"type": "text", "text": "\u00b7 For the exponent of $(T-T_{0})$ , although selecting a value smaller than 1 is possible, we find that the convergence rate with respect to $T$ is dominated by the $1/T$ term appearing later in the second term of (F.18). Therefore, we continue to consider the exponent to be 1. ", "page_idx": 47}, {"type": "text", "text": "Then we have ", "page_idx": 47}, {"type": "equation", "text": "$$\nI)=\\mathcal{O}\\left(\\frac{(1+6/a)\\left(C_{T}^{2}+r_{x}^{2}\\right)}{n T_{0}(T-T_{0})}\\log\\frac{N\\left((n^{b}(C_{T}+r_{x})T_{0}\\log(T/T_{0}))^{-1},S_{T_{p}^{2,1,4},\\|\\cdot\\|_{2}}\\right)}{\\bar{\\delta}}+\\frac{4+2a}{n^{b}T_{0}(T-T_{0})}\\right)\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "with probability $1-\\bar{\\delta}$ ", "page_idx": 47}, {"type": "text", "text": "Following the proof structure of term $(I I)$ in [Chen et al., 2023, Appendix B.2], we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n(I I)=\\mathcal{O}\\left(\\frac{1}{T_{0}}C_{T}^{2}r_{x}^{2}\\exp\\bigl\\{-A_{2}r_{x}^{2}/2\\bigr\\}\\right).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "For any $\\epsilon>0$ let $s_{\\overline{{W}}}$ be the transformer network approximator to the score function in Theorem 3.1. For the term $(I I I)$ ,wehave ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(I I I)\\le\\underbrace{\\widehat{\\mathcal{L}}(s_{\\overline{{W}}})-(1+a)\\mathcal{L}^{\\mathrm{trunc}}(s_{\\overline{{W}}})}_{(I I I)_{1}}+(1+a)\\underbrace{\\mathcal{L}^{\\mathrm{trunc}}(s_{\\overline{{W}}})}_{(I I I)_{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "For any $\\overline{{\\delta}}>0$ , according to Lemma F.5 and given that $s_{\\overline{{W}}}$ is a fixed function, the following holds for term $(I I I)_{1}$ with probability $1-\\bar{\\delta}$ \uff0c ", "page_idx": 47}, {"type": "equation", "text": "$$\n(I I I)_{1}=\\mathcal{O}\\left(\\frac{(1+3/a)\\left(C_{7}^{2}+r_{x}^{2}\\right)}{n T_{0}(T-T_{0})}\\log\\frac{1}{\\bar{\\delta}}\\right).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Following the proof structure of term $(I I I)_{2}$ in [Chen et al., 2023, Appendix B.2], we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n(I I I)_{2}=\\mathcal{O}\\left(\\frac{d\\epsilon^{2}}{T_{0}(T-T_{0})}\\right)+C_{3},\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where $C_{3}$ is a constant. ", "page_idx": 47}, {"type": "text", "text": "Putting $(I),(I I)$ , and $(I I I)$ together and setting $a=\\epsilon^{2}$ , then we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{1}{T-T_{0}}\\int_{T_{0}}^{T}\\left\\lVert s_{\\widehat{W}}(\\cdot,t)-\\nabla\\log p_{t}(\\cdot)\\right\\rVert_{L^{2}(P_{t})}^{2}\\mathrm{d}t}\\\\ &{=\\mathcal{O}\\left(\\frac{\\big(C_{T}^{2}+r_{x}^{2}\\big)}{\\epsilon^{2}n T_{0}(T-T_{0})}\\log\\frac{N\\big((n^{b}(C_{T}+r_{x})T_{0}\\log(T/T_{0}))^{-1},S_{T_{p}^{2,1,4}},\\lVert\\cdot\\rVert_{2}\\big)}{\\bar{\\delta}}+\\frac{n^{-b}+d_{0}\\epsilon^{2}}{T_{0}(T-T_{0})}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "with probability $\\mathrm{1-3\\bar{\\delta}}$ ", "page_idx": 47}, {"type": "text", "text": "Covering Number of $S_{T_{p}^{2,1,4}}$ : The next step is to calculate the covering number of $S_{T_{p}^{2,1,4}}.\\,S_{T_{p}^{2,1,4}}$ consists of two components: (i) Matrix $W_{B}$ with orthonormal columns; (i) Network function $\\dot{f}_{T}$ ", "page_idx": 48}, {"type": "text", "text": "Suppose  we  have $W_{B1},W_{B2}$ and $f_{1},f_{2}$ ,  such  that $\\begin{array}{r l r l}{\\|W_{B1}-W_{B2}\\|_{F}}&{{}\\le}&{\\delta_{1}}\\end{array}$ and supellr+\u221aDogD[,)f(x,t)f(,t\uff09l2,where f=Rf,f= $R^{-1}\\circ f_{T2}\\circ R$ .Then we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{|\\mathbb{T}|_{2}\\leq3\\tau_{x}+\\sqrt{D\\log D},t\\in[\\Gamma_{0},T]}{\\operatorname*{sup}}\\left\\||\\omega_{W B_{1},f_{1}}(\\bar{x},t)-s_{W B_{2},f_{2}(\\overline{{x}},t)}|\\right\\|_{2}}\\\\ &{=\\frac{1}{\\sigma(t)}\\underset{|\\mathbb{T}|_{2}\\leq3\\tau_{x}+\\sqrt{D\\log D},t\\in[\\Gamma_{0},T]}{\\operatorname*{sup}}\\left\\|W_{B1}f_{1}(W_{B1}^{\\top}\\bar{x},t)-W_{B2}f_{2}(W_{B2}^{\\top}\\bar{x},t)\\right\\|_{2}}\\\\ &{\\leq\\frac{1}{\\sigma(t)}\\underset{|\\mathbb{T}|_{2}\\leq3\\tau_{x}+\\sqrt{D\\log D},t\\in[\\Gamma_{0},T]}{\\operatorname*{sup}}\\left(\\left\\|W_{B1}f_{1}(W_{B1}^{\\top}\\bar{x},t)-W_{B1}f_{1}(W_{B2}^{\\top}\\bar{x},t)\\right\\|_{2}\\right.}\\\\ &{\\quad+\\left\\|W_{B1}f_{1}(W_{B2}^{\\top}\\bar{x},t)-W_{B1}f_{2}(W_{B2}^{\\top}\\bar{x},t)\\right\\|_{2}+\\left\\|W_{B1}f_{2}(W_{B2}^{\\top}\\bar{x},t)-W_{B2}f_{2}(W_{B2}^{\\top}\\bar{x},t)\\right\\|_{2}\\right)}\\\\ &{\\leq\\frac{1}{\\sigma(t)}\\left(L\\tau\\delta_{1}\\sqrt{d_{0}}(3r_{x}+\\sqrt{D\\log D})+\\delta_{2}+\\delta_{1}K\\right),\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{(F1)}\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where $L_{T}$ upper bounds the Lipschitz constant of $f_{T}$ ", "page_idx": 48}, {"type": "text", "text": "For the set $\\{W_{B}\\in\\mathbb{R}^{D\\times d_{0}}:\\|W_{B}\\|_{2}\\leq1\\}$ , its $\\delta_{1}$ -covering number is $\\left(1+2\\sqrt{d_{0}}/\\delta_{1}\\right)^{D d_{0}}$ [Chen et al., 2020a, Lemma 8]. The $\\delta_{2}$ -covering number of $f$ needs further discussion as there is a reshaping process in our network. The input is reshaped from $\\overline{{h}}\\in\\mathbb{R}^{d_{0}}$ to $H\\in\\mathbb{R}^{d\\times L}$ , and ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\left\\|{\\bar{h}}\\right\\|_{2}\\leq r_{x}\\Longleftrightarrow\\left\\|H\\right\\|_{F}\\leq r_{x}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Thus we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad\\operatorname*{sup}_{\\substack{\\left\\|\\bar{h}\\right\\|_{2}\\le3r_{x}+\\sqrt{D\\log D},t\\in[T_{0},T]}}\\left\\|f_{1}(\\bar{h},t)-f_{2}(\\bar{h},t)\\right\\|_{2}\\le\\delta_{2}}\\\\ &{\\Longleftrightarrow\\qquad\\qquad\\operatorname*{sup}_{\\substack{\\left\\|H\\right\\|_{F}\\le3r_{x}+\\sqrt{D\\log D},t\\in[T_{0},T]}}\\left\\|f_{71}(H)-f_{72}(H)\\right\\|_{2}\\le\\delta_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Then we follow the covering number of sequence-to-sequence transformer $\\mathcal{T}_{p}^{2,1,4}$ in Lemma F.6. We get the following $\\delta_{2}$ -covering number ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\frac{\\log(n L)}{\\delta_{2}^{2}}\\cdot\\left(\\sum_{i=1}^{K}\\alpha_{i}^{\\frac{2}{3}}\\left(d^{\\frac{2}{3}}\\left(C_{F}^{2,\\infty}\\right)^{\\frac{4}{3}}+d^{\\frac{2}{3}}\\left(2(C_{F})^{2}C_{O V}C_{K Q}^{2,\\infty}\\right)^{\\frac{2}{3}}+\\tau m^{\\frac{2}{3}}\\left((C_{F})^{2}C_{O V}^{2,\\infty}\\right)^{\\frac{2}{3}}\\right)\\right)^{3},\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\alpha_{i}:=\\prod_{j<i}(C_{F})^{2}C_{O V}(1+4C_{K Q})(C_{X}+C_{E}).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "According to the (F.4), (F.5), (F.7), (F.8), (F.9), (F.10), (F.12), (F.13), (F.11) and (F.6) in Appendix F.1.2, we derive the following with $\\delta=\\mathcal{O}(\\epsilon^{2/d})$ (Appendix E.4) and $d=4$ (Theorem 3.1): ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K=\\mathcal{O}\\left(\\epsilon^{-2L}\\right),L_{T}=\\mathcal{O}\\left(d_{0}L_{s_{+}}\\right),\\ C_{O V}^{2,\\infty}=\\mathcal{O}(d\\epsilon^{-4L}),\\ C_{O V}=\\mathcal{O}(\\epsilon^{-4L}),}\\\\ &{C_{K Q}^{2,\\infty}=\\mathcal{O}(\\epsilon^{-4}),\\ C_{K Q}=\\mathcal{O}(\\epsilon^{-4}),\\ C_{F}^{2,\\infty}=\\mathcal{O}(\\epsilon^{-4}),\\ C_{F}=\\mathcal{O}(\\epsilon^{-2}),\\ C_{E}=\\mathcal{O}(L^{3/2}),\\qquad\\mathrm{(F.17)}}\\\\ &{C_{T}=\\mathcal{O}\\left(d_{0}L_{s_{+}}\\cdot\\sqrt{d_{0}\\log(d_{0}/T_{0})+\\log(1/\\epsilon)}\\right),\\ r_{x}=\\mathcal{O}\\left(\\sqrt{d_{0}\\log d_{0}+\\log C_{T}+\\log(n/\\bar{\\delta})}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Each element of the input data is within $[0,1]$ , as shown in Appendix E. ", "page_idx": 48}, {"type": "text", "text": "For any $\\delta_{3}>0$ , we get the log-covering number of $\\mathcal{T}_{p}^{2,1,4}$ ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log\\mathcal{N}\\left(\\delta_{3},\\mathcal{T}_{p}^{2,1,4},\\lVert\\cdot\\rVert_{2}\\right)=\\mathcal{O}\\left(\\frac{\\epsilon^{-8K}\\cdot L^{K}d^{2}\\log(n L)}{\\delta_{3}}\\right)}\\\\ &{\\phantom{\\log\\mathcal{N}\\left(\\delta_{3},\\mathcal{T}_{p}^{2,1,4},\\lVert\\cdot\\rVert_{2}\\right)}=\\mathcal{O}(1)\\cdot\\left(\\frac{2^{8K\\log(L/\\epsilon)}d^{2}\\log(n L)}{\\delta_{3}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "According to (F.15), we adopt the following value for $\\delta_{3}$ in our setting ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\delta_{3}={\\frac{1}{n^{b}(C_{T}+r_{x})T_{0}\\log(T/T_{0})}}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "According to [Chen et al., 2023, Appendix B.2], the log-covering number of $S_{T_{p}^{2,1,4}}$ is ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ \\ \\mathrm{log}.N\\left(\\delta_{3},S_{T_{\\rho}^{2,1,4}}^{\\prime\\,2,1,4},||\\cdot|_{2}\\right)}\\\\ &{=\\mathcal{O}\\left(2D d_{0}\\cdot\\log\\left(1+\\frac{6C_{T}L_{T}\\sqrt{d_{0}}(3r_{x}+\\sqrt{D\\log D})}{T_{0}\\delta_{3}}\\right)+\\frac{2^{8K\\log(L/\\ell)}d^{2}\\log(n L)}{T_{0}^{2}\\delta_{3}^{2}}\\right)}\\\\ &{=\\mathcal{O}\\left(n^{2b}2^{8(1/\\epsilon)^{L}\\log(L/\\epsilon)}D d^{2}d_{0}^{6}L_{s_{+}}^{2}\\cdot\\log(n L)\\right)}\\\\ &{=\\mathcal{O}\\left(n^{2b}2^{(1/\\epsilon)^{2L}D}d^{2}d_{0}^{6}L_{s_{+}}^{2}\\cdot\\log(n L)\\right)}\\\\ &{=\\mathcal{O}\\left(n^{2b}2^{(1/\\epsilon)^{2L}D}d^{2}d_{0}^{6}L_{s_{+}}^{2}\\cdot\\log(n L)\\right)}\\\\ &{=\\tilde{\\mathcal{O}}\\left(n^{2b}2^{(1/\\epsilon)^{2L}D}d^{2}d_{0}^{6}L_{s_{+}}^{2}\\right)}\\\\ &{=\\tilde{\\mathcal{O}}\\left(n^{2b}2^{(1/\\epsilon)^{2L}D}d^{2}d_{0}^{6}L_{s_{+}}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Substituting the log-covering number into (F.15), we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{1}{T-T_{0}}\\int_{T_{0}}^{T}\\Big\\lVert s_{\\widehat W}(\\cdot,t)-\\nabla\\log p_{t}(\\cdot)\\Big\\rVert_{L^{2}(P_{t})}^{2}\\mathrm{d}t}\\\\ &{=\\mathcal{O}\\Big(\\frac{C_{T}^{2}+r_{x}^{2}}{\\epsilon^{2}n T_{0}(T-T_{0})}(\\log N(\\delta_{3},S_{T_{p}^{2,1,4}},\\|\\cdot\\|_{2})+\\log\\big(1/\\bar{\\delta}\\big))+\\frac{1}{n^{b}T_{0}(T-T_{0})}+\\frac{d_{0}}{T_{0}(T-T_{0})}\\epsilon^{2}\\Big)}\\\\ &{=\\mathcal{O}\\Big(\\underbrace{\\frac{C_{T}^{2}+r_{x}^{2}}{\\epsilon^{2}n T_{0}T}(\\log N(\\delta_{3},S_{T_{p}^{2,1,4}},\\|\\cdot\\|_{2})+\\log\\big(1/\\bar{\\delta}\\big))}_{\\mathrm{1st~term~}}+\\frac{1}{n^{b}T_{0}T}+\\underbrace{\\frac{d_{0}}{T_{0}T}\\epsilon^{2}}_{\\mathrm{2nd~term}}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Recall the following parameters: ", "page_idx": 49}, {"type": "text", "text": "$C_{T}^{2}=\\mathcal{O}(d_{0}^{2}L_{s_{+}}^{2}d_{0}\\log(d_{0}/T_{0})+\\log(1/\\epsilon)),$   \n$r_{x}^{2}=\\mathcal{O}(d_{0}\\log d_{0}+\\log C_{7}+\\log\\bigl(n/\\bar{\\delta}\\bigr)),$   \n\u00b7 $\\bar{\\delta}$ probability error, $\\epsilon$ : approximation error, $n$ : sample size,   \n\u00b7 $T_{0}<T/2$   \n\u00b7 $D,d,d_{0}>1$ : feature dimension,   \n\u00b7 $L>1$ : sequence length, ", "page_idx": 49}, {"type": "text", "text": "\u00b7 $d_{0}=L\\cdot d$ ", "page_idx": 50}, {"type": "text", "text": "$L_{s_{+}}$ : Lipschitz coefficient. ", "page_idx": 50}, {"type": "text", "text": "Ignoring the log factors and p $\\mathrm{{\\oly}}(D,d,d_{0},L_{S_{+}})$ , the first term in (F.18) becomes ", "page_idx": 50}, {"type": "equation", "text": "$$\n{\\frac{1}{n^{1-2b}}}\\cdot{\\frac{1}{T_{0}T}}\\cdot2^{(1/\\epsilon)^{2L}}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "The second term is simplified to ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\frac{1}{T_{0}T}\\epsilon^{2}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Thus, the final bound is ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\widetilde{O}\\left(\\frac{1}{n^{1-2b}}\\cdot\\frac{1}{T_{0}T}\\cdot2^{(1/\\epsilon)^{2L}}+\\frac{1}{n^{b}T_{0}T}+\\frac{1}{T_{0}T}\\epsilon^{2}\\right).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "To balance the first and second terms with respect to $n$ ,weselect $b=1/3$ . Therefore, we give the final bound as ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\widetilde{\\cal O}\\left(\\frac{1}{n^{1/3}}\\cdot\\frac{1}{T_{0}T}\\cdot2^{(1/\\epsilon)^{2L}}+\\frac{1}{n^{1/3}T_{0}T}+\\frac{1}{T_{0}T}\\epsilon^{2}\\right).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "This completes the proof. ", "page_idx": 50}, {"type": "text", "text": "F.3Proof of Corollary 3.2.1 ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Our proof is built on [Chen et al., 2023, Appendix C]. The main difference between our work and [Chen et al., 2023] is our score estimation error in Theorem 3.2. Consequently, only the subspace error and the total variation distance differ from [Chen et al., 2023, Theorem 3]. ", "page_idx": 51}, {"type": "text", "text": "First, we introduce the ground truth backward SDE and the learned backward SDE of the latent variable. Recall from (D.2), $y_{t}$ denotes the backward process. We denote the backward latent variable by $h_{t}^{\\leftarrow}=B^{\\top}y_{t}$ . Since we write the time index explicitly, we drop the $\\bar{y},\\bar{h}$ notation for $t>0$ ", "page_idx": 51}, {"type": "text", "text": "Following [Chen et al.,2023, Appendix C.2], we have the following ground truth backward process ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathrm{d}h_{t}^{\\leftarrow}=\\left[\\frac{1}{2}h_{t}^{\\leftarrow}+\\nabla\\log p_{T-t}^{h}(h_{t}^{\\leftarrow})\\right]\\mathrm{d}t+\\mathrm{d}\\big(B^{\\top}\\overline{{W}}_{t}\\big)\\,,\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where $\\overline{{W}}_{t}$ denotes the reversed Wiener process (standard Brownian motion) at time $t$ (see Section 2 for more details). ", "page_idx": 51}, {"type": "text", "text": "We defne $P_{T_{0}}^{h}$ as the grodtuhmarginaldistribtnf $h_{T_{0}}^{\\leftarrow}$ ", "page_idx": 51}, {"type": "text", "text": "For the learned process $\\widetilde{y}_{t}$ , we consider $\\widetilde{h}_{t}^{\\leftarrow}=W_{B}^{\\top}\\widetilde{y}_{t}$ . For any orthogonal matrix $U\\in\\mathbb{R}^{d_{0}\\times d_{0}}$ ,we define the $U$ transformed version of $\\widetilde{h}_{t}^{\\leftarrow}$ $\\widetilde{h}_{t}^{\\leftarrow,U}=U^{\\top}\\widetilde{h}_{t}^{\\leftarrow}$ Then the backward SDE for $\\widetilde{h}_{t}^{\\leftarrow,U}$ is ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\widetilde{h}_{t}^{\\leftarrow,U}=\\left[\\widetilde{h}_{t}^{\\leftarrow,U}+\\widetilde{s}_{U,f}^{h}(\\widetilde{h}_{t}^{\\leftarrow,U},T-t)\\right]\\mathrm{d}t+\\mathrm{d}\\big(U^{\\top}W_{B}^{\\top}\\overline{{W}}_{t}\\big)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\widetilde{s}_{U,f}^{h}\\left(\\widetilde{h}_{t}^{\\leftarrow,U},t\\right):=\\frac{1}{\\sigma(t)}[-\\widetilde{h}_{t}^{\\leftarrow,U}+U^{\\top}f(U\\widetilde{h}_{t}^{\\leftarrow,U},t)].\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "We define $\\widehat{P}_{T_{0}}^{h}$ a theettalis $\\widetilde{h}_{T_{0}}^{\\leftarrow,U}$ from above contiuous SDE. ", "page_idx": 51}, {"type": "text", "text": "The discretized backward SDE f $\\widetilde{h}_{T_{0}}^{\\leftarrow,U}$ .s ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\widetilde{h}_{t}^{\\leftarrow,U}=\\left[\\widetilde{h}_{k\\mu}^{\\leftarrow,U}+\\widetilde{s}_{U,f}^{h}(\\widetilde{h}_{k\\mu}^{\\leftarrow,U},T-k\\mu)\\right]\\mathrm{d}t+\\mathrm{d}\\big({U}^{\\top}{W}_{B}^{\\top}\\overline{{W}}_{t}\\big)\\,,t\\in[k\\mu,(k+1)\\mu).}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "We define $\\widehat{P}_{T_{0}}^{h,\\mathrm{dis}}$ as the estimated marginal distribution of $\\widetilde{h}_{T_{0}}^{\\leftarrow,U}$ from above discrete SDE. ", "page_idx": 51}, {"type": "text", "text": "Next, we present the auxiliary theoretical results in Appendix F.3.1 to prepare our main proof of Corollary 3.2.1. Then we give a detailed proof of Corollary 3.2.1 in Appendix F.3.2. ", "page_idx": 51}, {"type": "text", "text": "F.3.1 Auxiliary Lemmas ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Here we include a few auxiliary lemmas from [Chen et al., 2023] without proofs. Recall the definition of Lipschitz norm: for a given function $\\begin{array}{r}{f,\\|f(\\cdot)\\|_{L i p}=\\operatorname*{sup}_{x\\neq y}(\\|f(x)\\stackrel{-}-f(y)\\|_{2}/\\|x-y\\|_{2})}\\end{array}$ ", "page_idx": 51}, {"type": "text", "text": "Lemma F.7 (Lemma 3 of [Chen et al., 2023]). Assume that the following holds ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{h\\sim P_{h}}\\|\\nabla\\log p_{h}(h)\\|_{2}^{2}\\leq C_{s h},\\quad\\lambda_{\\operatorname*{min}}\\mathbb{E}_{h\\sim P_{h}}[h h^{\\top}]\\geq c_{0},\\quad\\mathbb{E}_{h\\sim P_{h}}\\|h\\|_{2}^{2}\\leq C_{h},}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where $\\lambda_{\\operatorname*{min}}$ denotes the smallest eigenvalue. We denote ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\overline{{\\mathbb{E}}}[\\phi(\\overline{{h}},t)]=\\int_{T_{0}}^{T}\\frac{1}{\\sigma^{2}(t)}\\mathbb{E}_{\\overline{{x}}\\sim P_{t}}[\\phi(B^{\\top}\\overline{{x}},t)]d t.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Let $T_{0}\\le\\operatorname*{min}\\{2\\log(d_{0}/C_{s h}),1,2\\log(c_{0}),c_{0}\\}$ and $T\\geq\\operatorname*{max}\\{2\\log(C_{h}/d_{0}),1\\}$ . Suppose we have ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\big\\|W_{B}f(W_{B}^{\\top}\\bar{x},t)-B q(B^{\\top}\\bar{x},t)\\big\\|_{2}^{2}\\leq\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Then we have ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\left\\|\\boldsymbol{W_{B}}\\boldsymbol{W}_{\\boldsymbol{B}}^{\\top}-\\boldsymbol{B}\\boldsymbol{B}^{\\top}\\right\\|_{\\mathrm{F}}^{2}=\\mathcal{O}(\\epsilon T_{0}/c_{0}),\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "and there exists an orthogonal matrix $U\\in\\mathbb{R}^{d_{0}\\times d_{0}}$ , such that: ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~\\mathbb{\\bar{E}}\\big\\|{\\cal U}^{\\top}f(U\\bar{h},t)-q(\\bar{h},t)\\big\\|_{2}^{2}}\\\\ &{=\\epsilon\\cdot\\mathcal{O}\\left(1+\\displaystyle\\frac{T_{0}}{c_{0}}\\left[(T-\\log T_{0})d_{0}\\cdot\\operatorname*{max}_{t}\\|f(\\cdot,t)\\|_{\\mathrm{Lip}}^{2}+C_{s h}\\right]+\\displaystyle\\frac{\\operatorname*{max}_{t}\\|f(\\cdot,t)\\|_{\\mathrm{Lip}}^{2}\\cdot C_{h}}{c_{0}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Lemma F.8 (Lemma 4 of [Chen et al., 2023]). Assume that $P_{h}$ is sub-Gaussian and that $f(\\overline{{h}},t)$ and $\\nabla\\log p_{t}^{h}(\\overline{{h}})$ are Lipschitz continuous with respect to $\\bar{h}$ and $t$ . For any orthogonal matrix $U\\in\\mathbb{R}^{d_{0}\\times d_{0}}$ \uff0c we define ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\widetilde{s}_{U,f}^{h}\\left(\\overline{{h}},t\\right):=\\frac{1}{\\sigma(t)}[-\\overline{{h}}+U^{\\top}f(U\\overline{{h}},t)].\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Assume that we have the latent score matching error-bound ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\int_{T_{0}}^{T}\\mathbb{E}_{\\overline{{h}}\\sim P_{t}^{h}}\\left\\lVert\\tilde{s}_{U,f}^{h}\\left(\\overline{{h}},t\\right)-\\nabla\\log p_{t}^{h}\\left(\\overline{{h}}\\right)\\right\\rVert_{2}^{2}\\;\\mathrm{d}t\\leq\\epsilon_{\\mathrm{latent}}\\left(T-T_{0}\\right)\\!,\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where $\\epsilon_{\\mathrm{latent}}>0$ . Then we have the following latent distribution estimation error for the continuous backward SDE: ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{TV}\\left(P_{T_{0}}^{h},\\widehat{P}_{T_{0}}^{h}\\right)\\lesssim\\sqrt{\\epsilon_{\\mathrm{latent}}\\left(T-T_{0}\\right)}+\\sqrt{\\mathrm{KL}\\left(P_{h}\\|N\\left(0,I_{d_{0}}\\right)\\right)}\\cdot\\exp(-T),}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where $\\widehat{P}_{T_{0}}^{h}$ is the marginal distribution of the generated $h_{T_{0}}$ using the continuous backward SDE. Furthermore, let $\\widehat{P}_{T_{0}}^{h,\\mathrm{dis}}$ denote the marginal distribution of the generated $h_{T_{0}}$ using the discretized backward SDE. Then we have the following latent distribution estimation error for the discretized backward SDE ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{TV}\\left(P_{T_{0}}^{h},\\widehat{P}_{T_{0}}^{h,\\mathrm{dis}}\\right)\\lesssim\\sqrt{\\epsilon_{\\mathrm{latent}}(T-T_{0})}+\\sqrt{\\mathrm{KL}\\left(P_{h}\\|N\\left(0,I_{d_{0}}\\right)\\right)}\\cdot\\exp(-T)+\\sqrt{\\epsilon_{\\mathrm{dis}}(T-T_{0})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\epsilon_{\\mathrm{dis}}=\\left(\\frac{\\operatorname*{max}_{\\bar{h}}{\\left\\|{f(\\bar{h},\\cdot)}\\right\\|}_{\\mathrm{Lip}}}{{\\sigma\\left(T_{0}\\right)}}+\\frac{\\operatorname*{max}_{\\bar{h},t}{\\left\\|{f(\\bar{h},t)}\\right\\|}_{2}}{{T_{0}^{2}}}\\right)^{2}\\eta^{2}}\\\\ &{\\;\\;\\;\\;\\;\\;+\\left(\\frac{\\operatorname*{max}_{t}{\\left\\|{f(\\cdot,t)}\\right\\|}_{\\mathrm{Lip}}}{{\\sigma\\left(T_{0}\\right)}}\\right)^{2}\\eta^{2}\\operatorname*{max}\\left\\{\\mathbb{E}{\\left\\|{h_{0}}\\right\\|}^{2},d_{0}\\right\\}+\\eta d_{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "and $\\eta$ is the step size in the backward process. ", "page_idx": 52}, {"type": "text", "text": "Lemma F.9 (Lemma 6 of [Chen et al., 2023]). Consider the following discretized SDE with step size $\\mu$ satisfying $T-T_{0}=K_{T}\\mu$ for some $K_{T}\\in\\ensuremath{\\mathbb{N}}_{+}$ ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\mathrm{d}y_{t}=\\left[\\frac{1}{2}-\\frac{1}{\\sigma(T-k\\mu)}\\right]y_{k\\mu}\\mathrm{d}t+\\mathrm{d}B_{t},\\quad\\mathrm{for}\\quad t\\in[k\\mu,(k+1)\\mu),\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where $y_{0}\\,\\sim\\,\\mathrm{N}(0,I)$ .Then, for $T\\ >\\ 1$ and $T_{0}+\\mu\\,\\leq\\,1\\,$ , we have $y_{T-T_{0}}\\;\\sim\\;\\mathrm{N}\\left(0,\\sigma^{2}I\\right)$ with $\\sigma^{2}\\leq e\\left(T_{0}+\\mu\\right)$ ", "page_idx": 52}, {"type": "text", "text": "Lemma F.10 (Lemma 10 in [Chen et al., 2023]). Assume that $\\nabla\\log p_{h}(h)$ $L_{h}$ -Lipschitz. Then we have $\\mathbb{E}_{h\\sim P_{h}}\\left\\|\\nabla\\log p_{h}(h)\\right\\|_{2}^{2}\\leq d_{0}L_{h}$ ", "page_idx": 53}, {"type": "text", "text": "F.3.2Main Proof of Corollary 3.2.1 ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Proof. Recall the estimation error in Theorem 3.2 is $\\xi(n,\\epsilon,L)/(T T_{0})$ ,where ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\xi(n,\\epsilon,L):=\\frac{1}{n^{1/3}}\\cdot2^{(1/\\epsilon)^{2L}}+\\frac{1}{n^{1/3}}+\\epsilon^{2}.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "\u00b7 Proof of (i). By the definition of (F.19) and the estimation error in Theorem 3.2, the error bound in (F.20) equals to $\\xi(n,\\epsilon,L)(T-T_{0})/(T T_{0})$ in Lemma F.7. By Lemma F.10, we set $C_{s h}=d_{0}L_{h}$ Then,wehave ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\big\\|W_{B}W_{B}^{\\top}-B B^{\\top}\\big\\|_{F}^{2}=\\mathcal{O}\\Bigg(\\frac{\\xi(n,\\epsilon,L)}{c_{0}}\\Bigg).\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "By substituting the value of $\\xi(n,\\epsilon,L)$ and $T={\\mathcal{O}}(\\log n)$ into the bound above, we deduce ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\left\\|W_{B}W_{B}^{\\top}-B B^{\\top}\\right\\|_{F}^{2}=\\mathcal{O}\\left(\\frac{1}{c_{0}n^{1/3}}2^{\\left(1/\\epsilon\\right)^{2L}}+\\frac{1}{c_{0}n^{1/3}}+\\frac{\\epsilon^{2}}{c_{0}}\\right).\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "\u00b7 Proof of (i). Recall that $\\operatorname*{max}_{t}\\|f(\\cdot,t)\\|_{\\mathrm{Lip}}\\leq L_{T}$ . Furthermore, according to Lemma F.7 and Lemma F.10, we have ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{\\left|}{\\left|U^{\\top}f(U\\bar{h},t)-q(\\bar{h},t)\\right|}\\right|_{2}^{2}=\\mathcal{O}(\\epsilon_{\\mathrm{latent}}(T-T_{0})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\epsilon_{\\mathrm{latent}}=\\frac{\\xi(n,\\epsilon,L)}{T T_{0}}\\cdot\\mathcal{O}\\left(\\frac{T_{0}}{c_{0}}\\left[(T-\\log T_{0})d_{0}\\cdot L_{T}^{2}+d_{0}L_{h}\\right]+\\frac{L_{T}^{2}\\cdot C_{h}}{c_{0}}\\right).\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Following the proof structure in [Chen et al., 2023, Appendix C.4], we get ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\big\\|U^{\\top}f(U\\bar{h},t)-q(\\bar{h},t)\\big\\|_{2}^{2}=\\displaystyle\\int_{T_{0}}^{T}\\mathbb{E}_{\\bar{h}\\sim P_{t}^{h}}\\bigg\\|\\frac{U^{\\top}f(U\\bar{h},t)-\\bar{h}}{\\sigma(t)}-\\nabla\\log p_{t}^{h}(\\bar{h})\\bigg\\|_{2}^{2}\\mathrm{d}t}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\epsilon_{\\mathrm{latent}}(T-T_{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Following the proof structure in [Chen et al., 2023, Appendix C.4] and seting $T={\\mathcal{O}}(\\log n)$ we obtain ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{T V}(P_{T_{0}}^{h},\\widehat{P}_{T_{0}}^{h,\\mathrm{dis}})=\\widetilde{\\mathcal{O}}\\left(\\sqrt{\\epsilon_{\\mathrm{latent}}\\left(T-T_{0}\\right)}\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\widetilde{\\mathcal{O}}\\left(\\sqrt{\\left(\\frac{1}{n^{1/3}}2^{(1/\\epsilon)^{2L}}+\\frac{1}{n^{1/3}}+\\epsilon^{2}\\right)\\cdot\\log n}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where $\\widetilde O$ hides the factor about $D,d_{0},d,L_{s_{+}},\\log n$ , and $T-T_{0}$ ", "page_idx": 53}, {"type": "text", "text": "By dein, $\\widehat{P}_{T_{0}}^{h,\\mathrm{dis}}=(U W_{B})_{\\sharp}^{\\top}\\widehat{P}_{T_{0}}$ where $\\widehat{P}_{T_{0}}$ isthedisributiongeneratedby ${\\boldsymbol{s}}_{\\widehat{W}}$ usingthe discretized backward process. This completes the proof of the total variation distance. ", "page_idx": 53}, {"type": "text", "text": "\u00b7 Proof of (i). We apply Lemma F.9 due to our score decomposition. With the marginal distribution at time $T-T_{0}$ and observing $\\mu\\ll T_{0}$ , we obtain the last property. ", "page_idx": 53}, {"type": "text", "text": "G Proofs of Section 4 ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Our proofs are motivated by the observation of low-rank gradient decomposition in transformer-like models [Alman and Song, 2024b, Gu et al., 2024]. With our simplifications and observations made in Section 4, we utilize the fine-grained complexity results of transformer and attention [Hu et al., 2024b, Alman and Song, 2024a,b] and tensor trick (Lemma D.1 and [Dia0 et al., 2019, 2018]) to proceed our proofs. Specifically, we approximate DiT training gradients with a series of low-rank approximations in Appendices G.1.1 to G.1.3, and carefully match the multiplication dimensions so thathe computation of $\\frac{\\mathrm{d}g_{2}}{\\mathrm{d}\\underline{{W}}}$ forms a chained low-rankapproximation in Appendix G.2. ", "page_idx": 54}, {"type": "text", "text": "G.1 Auxiliary Theoretical Results for Theorem 4.1 ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Here we present some auxiliary theoretical results to prepare our main proof of the Existence of almost-linear Time Algorithms for ADITGC Theorem 4.1. ", "page_idx": 54}, {"type": "text", "text": "G.1.1 Low-Rank Decomposition of DiT Gradients ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "We start by some definitions. Recall that $W\\,\\in\\,\\mathbb{R}^{d\\times d}$ and $\\underline{{W}}\\in\\mathbb{R}^{d^{2}}$ denotes the vectorization of $W\\in\\mathbb{R}^{d\\times\\check{d}}$ following Definition D.1. ", "page_idx": 54}, {"type": "text", "text": "Definition G.1. Let $A_{1},A_{2}\\in\\mathbb{R}^{d\\times L}$ be two matrices. Suppose $\\mathsf{A}=A_{1}^{\\top}\\otimes A_{2}^{\\top}\\in\\mathbb{R}^{L^{2}\\times d^{2}}$ .Define $\\mathsf{A}_{j_{0}}\\in\\mathbb{R}^{L\\times d^{2}}$ as an $L\\times d^{2}$ sub-block of A. There are $L$ such sub-blocks in total. For each $j_{0}\\in[L]$ define the function $u(\\underline{{W}})_{j0}:\\mathbb{R}^{d^{2}}\\to\\mathbb{R}^{L}$ by $u(\\underline{{W}})_{j_{0}}:=\\exp(\\mathsf{A}_{j_{0}}\\,\\underline{{W}})\\in\\mathbb{R}^{L}$ ", "page_idx": 54}, {"type": "text", "text": "Definition G.2. Let $A_{1},A_{2}\\in\\mathbb{R}^{d\\times L}$ be two matrices. Suppose $\\mathsf{A}=A_{1}^{\\top}\\otimes A_{2}^{\\top}\\in\\mathbb{R}^{L^{2}\\times d^{2}}$ . Define $\\mathsf{A}_{j_{0}}\\in\\mathbb{R}^{L\\times d^{2}}$ as an $L\\times d^{2}$ sub-block of A. There are $L$ such sub-blocks in total. For every index $j_{0}\\in[L]$ conside the funtion $\\alpha(\\mathbb{W})_{j_{0}}:\\mathbb{R}^{d^{2}}\\rightarrow\\mathbb{R}$ defined by $\\alpha(\\underline{{W}})_{j_{0}}:=\\langle\\underbrace{\\mathrm{exp}(\\mathsf{A}_{j_{0}}\\,\\underline{{W}})}_{L\\times1},\\underbrace{\\mathbb{1}_{L}}_{L\\times1}\\rangle$ \uff0c ", "page_idx": 54}, {"type": "image", "img_path": "", "img_caption": [], "img_footnote": [], "page_idx": 54}, {"type": "text", "text": "Definition G.3. Suppose that $\\alpha(\\underline{{W}})_{j_{0}}\\in\\mathbb{R}$ and $u(\\underline{{W}})_{j_{0}}\\,\\in\\,\\mathbb{R}^{L}$ are defined as in Definitions G.1 and G.2, respectively. For a fixed $j_{0}\\in[L]$ , consider the function $f(\\underline{{W}})_{j_{0}}:\\mathbb{R}^{d^{2}}\\to\\mathbb{R}^{L}$ defined by ", "page_idx": 54}, {"type": "equation", "text": "$$\nf(\\underline{{W}})_{j_{0}}:=\\underbrace{\\alpha(\\underline{{W}})_{j_{0}}^{-1}}_{\\mathrm{scalar}}\\underbrace{u(\\underline{{W}})_{j_{0}}}_{L\\times1}.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Define $f(\\underline{{W}})\\in\\mathbb{R}^{L\\times L}$ as the matrix where the $j_{0}$ -th row is $(f(\\underline{{W}})_{j_{0}})^{\\top}$ ", "page_idx": 54}, {"type": "text", "text": "Definition G.4. For every $i_{0}\\in[d]$ , define the function $h(\\underline{{W}}_{O V})_{i_{0}}:\\mathbb{R}^{d^{2}}\\to\\mathbb{R}^{L}$ by ", "page_idx": 54}, {"type": "equation", "text": "$$\nh(\\underline{{W}}_{O V})_{i_{0}}:=\\underbrace{A_{3}^{\\top}}_{L\\times d}\\underbrace{(W_{O V}^{\\top})_{*,i_{0}}}_{d\\times1}.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Here, $W_{O V}\\in\\mathbb{R}^{d\\times d}$ denotes the matrix representation of $\\underline{{W}}_{O V}\\in\\mathbb{R}^{d^{2}}$ , and $(W_{O V})_{*,i_{0}}^{\\top}$ represents the $i_{0}$ -th column of $W_{O V}^{\\top}$ . Define $h(\\underline{{W}}_{O V})\\in\\mathbb{R}^{L\\times d}$ as the matrx where the $i_{0}$ -th column is $h(\\underline{{W}}_{O V})_{i_{0}}$ ", "page_idx": 54}, {"type": "text", "text": "Definition G.5. For each $j_{0}\\,\\in\\,[L]$ : we denote $f(\\underline{{W}})_{j_{0}}\\,\\in\\,\\mathbb{R}^{L}$ as the normalized vector defined by Definition G.3. For each $i_{0}\\,\\in\\,[d],\\,h(\\underline{{W}}_{O V})_{i_{0}}$ is defined as per Definition G.4. For every pair $(j_{0},i_{0})\\in[L]\\times[d]$ , define the function $c(\\underline{{W}})_{j_{0},i_{0}}:\\mathbb{R}^{d^{2}}\\times\\mathbb{R}^{d^{2}}\\to\\mathbb{R}$ by ", "page_idx": 54}, {"type": "equation", "text": "$$\nc(\\underline{{W}})_{j_{0},i_{0}}:=\\langle f(\\underline{{W}})_{j_{0}},h(\\underline{{W}}_{O V})_{i_{0}}\\rangle-Y_{j_{0},i_{0}}^{\\top},\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where $(W_{O V})_{j_{0},i_{0}}$ is the element at the $(j_{0},i_{0})$ position of the matrix $W_{O V}\\in\\mathbb{R}^{L\\times d}.$ $c(\\cdot)$ has matrix form ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\underbrace{c(W)}_{L\\times d}=\\underbrace{f(W)}_{L\\times L}\\underbrace{h(W_{O V})}_{L\\times d}-\\underbrace{Y^{\\top}}_{L\\times d}.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "With the tensor trick (Appendix D.3), we compute the gradient $\\frac{\\mathrm{d}g_{2}}{\\mathrm{d}\\underline{{W}}}$ of the DiT loss as follows: ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}g_{2}}{\\mathrm{d}\\underline{{W}}}=\\frac{\\mathrm{d}}{\\mathrm{d}\\underline{{W}}}\\left[\\frac{1}{2}\\sum_{j_{0}=1}^{L}\\sum_{i_{0}=1}^{d}c_{j_{0},i_{0}}^{2}(\\underline{{W}})\\right].\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "(G.1) presents a neat decomposition of $\\frac{\\mathrm{d}g_{2}}{\\mathrm{d}W}$ .Eachterm is easyenough tohandl. hus, we arrive at the following lemma. Let $Z[i,\\cdot]$ and $Z[\\cdot,j]$ be the $i$ -th row and $j$ -th column of matrix $Z$ ", "page_idx": 55}, {"type": "text", "text": "Lemma G.1 (Low-Rank Decomposition of DiT Gradient). Let matrix $A_{1},A_{2},A_{3},W,W_{O V},Y$ and loss function $\\mathcal{L}$ follow Definition 4.1, and $\\mathsf{A}:=A_{1}^{\\top}\\otimes A_{2}^{\\top}$ It holds ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}g_{2}}{\\mathrm{d}W}=\\sum_{j_{0}=1}^{L}\\sum_{i_{0}=1}^{d}c(\\underline{{W}})_{j_{0},i_{0}}\\,\\mathsf{A}_{j_{0}}^{\\top}\\underbrace{\\left(\\overbrace{\\mathrm{diag}\\left(f(\\underline{{W}})_{j}\\right)}^{(I I)}-\\overbrace{f(\\underline{{W}})_{j_{0}}f(\\underline{{W}})_{j_{0}}^{\\top}}^{(I I I)}\\right)}_{(I)}h(\\underline{{W}}_{O V})_{i_{0}}.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Proof. Let $Z[i,\\cdot]$ and $Z[\\cdot,j]$ be the $i$ -th row and $j$ -th column of matrix $Z$ With DiT loss Definition 4.1, we have ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{d_{22}}{d t|\\mathbf{Z}}=\\frac{1}{2}\\sum_{k=1}^{\\infty}\\frac{\\mathrm{d}}{d t|\\mathbf{Z}^{k}-\\mathbf{z}^{k}|}\\frac{\\mathrm{d}}{d t|\\mathbf{Z}^{k}}{\\mathrm{sind}\\langle\\mathbf{R}\\rangle}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }\\ \\ \\ \\ \\ \\ }\\\\ &{=\\sum_{k=11}^{}\\frac{\\frac{\\sum{\\sum_{i}}}{4}}\\frac{}{\\mathrm{d}\\mathbb{I}^{\\top}}{\\mathrm{d}\\mathbb{Z}^{k}}s_{k}^{-k}e_{k}^{-k}(\\mathbb{R})|\\mathbf{Z}_{k}|e_{k}\\cdot\\frac{\\mathrm{d}(\\mathbb{R})}{d t|\\mathbf{Z}_{k}|}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\frac{\\mathrm{d}(\\mathbb{R})}{d t|\\mathbf{Z}_{k}|}=\\frac{1}{2}\\sum_{k=1}^{\\infty}\\frac{\\mathrm{d}(\\mathbb{R})}{d t|\\mathbf{Z}_{k}|}e_{k}^{-k}{\\frac{\\sum_{i}^{\\infty}(\\mathbb{R})}{d t|\\mathbf{Z}_{k}|}},\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\frac{d_{22}}{d t|\\mathbf{Z}_{k}|}}\\\\ &{=\\sum_{k=1}^{\\sum{\\sum_{i}}}\\frac{\\mathrm{d}}{d t|\\mathbf{Z}_{k}|}e_{k}^{-k}e_{k}^{k}(\\mathbb{R})|\\mathbf{h}_{k}\\cdot\\left\\langle\\frac{d_{22}^{(\\mathbb{R})}}{d t|\\mathbf{Z}_{k}|},b_{k}(\\mathbb{R})e_{k}\\right\\rangle_{k}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "For each $j_{0}\\in[L]$ , we have ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\left(\\mathsf{A}_{j_{0}}\\,\\underline{{W}}\\right)}{\\mathrm{d}\\underline{{W}}_{i_{0}}}=\\mathsf{A}_{j_{0}}\\cdot\\frac{\\mathrm{d}\\underline{{W}}}{\\mathrm{d}\\underline{{W}}_{i_{0}}}=\\left(\\mathsf{A}_{j_{0}}\\right)\\big[\\cdot,i\\big].\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Therefore, for each $j_{0}\\in[L]$ , we have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\mathrm{d}u(\\underline{{W}})_{j_{0}}}{\\mathrm{d}\\underline{{W}}_{i_{0}}}=\\frac{\\mathrm{d}\\exp\\big(\\mathsf{A}_{j_{0}}\\,\\underline{{W}}\\big)}{\\mathrm{d}\\underline{{W}}_{i_{0}}}}}\\\\ &{}&{\\qquad=\\exp\\big(\\mathsf{A}_{j_{0}}\\,\\underline{{W}}\\big)\\odot\\frac{\\mathrm{d}\\,\\mathsf{A}_{j_{0}}\\,\\underline{{W}}}{\\mathrm{d}\\underline{{W}}_{i_{0}}}}\\\\ &{}&{\\qquad=\\mathsf{A}_{j_{0}}[\\cdot,i]\\odot u(\\underline{{W}})_{j_{0}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Similarly, ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\mathrm{d}\\alpha(\\underline{{W}})_{j_{0}}}{\\mathrm{d}\\underline{{W}}_{i_{0}}}=\\frac{\\mathrm{d}\\,\\langle u(\\underline{{W}})_{j_{0}},\\mathbb{1}_{L}\\rangle}{\\mathrm{d}\\underline{{W}}_{i_{0}}}}}\\\\ &{}&{=\\,\\langle\\mathsf{A}_{j_{0}}[\\cdot,i]\\odot u(\\underline{{W}})_{j_{0}},\\mathbb{1}_{L}\\rangle}\\\\ &{}&{=\\,\\langle\\mathsf{A}_{j_{0}}[\\cdot,i],u(\\underline{{W}})_{j_{0}}\\rangle\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Putting all together, we have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\frac{\\mathrm{d}g_{2}(W)_{j_{0},i_{0}}}{\\mathrm{d}W_{i_{0}}}}\\\\ &{=\\big[\\langle h(\\underline{{W}}_{O V})_{i_{0}},\\mathsf{A}_{j_{0}}[\\cdot,i]\\odot f(\\underline{{W}})_{j_{0}}\\rangle-\\langle h(\\underline{{W}}_{O V})_{i_{0}},f(\\underline{{W}})_{j_{0}}\\rangle\\cdot\\langle\\mathsf{A}_{j_{0}}[\\cdot,i],f(\\underline{{W}})_{j_{0}}\\rangle\\big]\\cdot c(\\underline{{W}})_{j_{0},i_{0}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\langle h(W_{O V})_{i_{0}},\\mathsf{A}_{j_{0}}[\\cdot,i]\\odot f(W)_{j_{0}}\\rangle-\\langle h(\\underline{{W_{O V}}})_{i_{0}},f(\\underline{{W}})_{j_{0}}\\rangle\\cdot\\langle\\mathsf{A}_{j_{0}}[\\cdot,i],f(\\underline{{W}})_{j_{0}}\\rangle}\\\\ &{=\\mathsf{A}_{j_{0}}^{\\top}\\left(\\mathrm{diag}\\,(f(\\underline{{W}})_{j_{0}})-f(\\underline{{W}})_{j_{0}}f(\\underline{{W}})_{j_{0}}^{\\top}\\right)h(\\underline{{W}}_{O V})_{i_{0}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "This completes the proof. ", "page_idx": 56}, {"type": "text", "text": "Observe (G.2) carefully. We see that (I) is diagonal and (Il) is low-rank. This provides a hint for algorithmic speedup through low-rank approximation: If we approximate the other parts with low-rank approximation and carefully match the multiplication dimensions, we might formulate the computationof a as a chained low-rank approximation. ", "page_idx": 56}, {"type": "text", "text": "Surprisingly, such an approach makes computing (G.2) as fast as in almost-linear time. To proceed, we further decompose (G.2) according to the chain-rule in the next lemma, and then conduct the approximation term-by-term. ", "page_idx": 56}, {"type": "text", "text": "To facilitate our proof, it's convenient to introduce the following notations. ", "page_idx": 56}, {"type": "text", "text": "Definition G.6 $(q(\\cdot))$ . Define $c(\\underline{W})\\in\\mathbb{R}^{L\\times d}$ as specified in Definition G.5 and $h(\\underline{{W}}_{O V})\\in\\mathbb{R}^{L\\times d}$ as described in Definition G.4. Defne $q(\\underline{{W}})\\in\\mathbb{R}^{L\\dot{\\times}L}$ by ", "page_idx": 56}, {"type": "equation", "text": "$$\nq(\\underline{{W}}):=\\underbrace{c(\\underline{{W}})}_{L\\times d}\\underbrace{h(\\underline{{W}}_{O V})^{\\top}}_{d\\times L}.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "In addition, $q(\\underline{{W}})_{j_{0}}^{\\top}$ denotes the $j_{0}$ -th row of $q(\\underline{{W}})$ , transposed, making it an $L\\times1$ vector. ", "page_idx": 56}, {"type": "text", "text": "Definition G.7 $(p(\\cdot),p_{1}(\\cdot),p_{2}(\\cdot))$ . For each index $j_{0}\\in[L]$ , we define $p(\\underline{W})_{j_{0}}\\in\\mathbb{R}^{n}$ as follows: ", "page_idx": 56}, {"type": "equation", "text": "$$\np(\\underline{{W}})_{j_{0}}:=\\left(\\mathrm{diag}(f(\\underline{{W}})_{j_{0}})-f(\\underline{{W}})_{j_{0}}f(\\underline{{W}})_{j_{0}}^{\\top}\\right)q(\\underline{{W}})_{j_{0}}.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "We define $p(\\underline{W})\\,\\in\\,\\mathbb{R}^{L\\times L}$ such that $p(\\underline{W})_{j_{0}}^{\\top}$ forms the $j_{0}$ th row of $p(\\underline{W})$ . In addition for every index $j_{0}\\in[L]$ , we define $p_{1}(\\underline{{W}})_{j0},p_{2}(\\underline{{W}})_{j0}^{\\'}\\in\\mathbb{R}^{L}$ as ", "page_idx": 57}, {"type": "equation", "text": "$$\np_{1}(\\underline{{W}})_{j_{0}}:=\\mathrm{diag}\\left(f\\left(\\underline{{W}}\\right)_{j_{0}}\\right)q(\\underline{{W}})_{j_{0}},\\quad p_{2}(\\underline{{W}})_{j_{0}}:=f\\left(\\underline{{W}}\\right)_{j_{0}}f\\left(\\underline{{W}}\\right)_{j_{0}}^{\\top}q(\\underline{{W}})_{j_{0}},\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "such that $p(\\underline{{W}})=p_{1}(\\underline{{W}})-p_{2}(\\underline{{W}})$ ", "page_idx": 57}, {"type": "text", "text": "$p(\\cdot)$ allows us to express $\\frac{\\mathrm{d}g_{2}}{\\mathrm{d}\\underline{{W}}}$ in a neat form: ", "page_idx": 57}, {"type": "text", "text": "Lemma G.2. Define the functions $f(\\underline{{W}})\\in\\mathbb{R}^{L\\times L}$ \uff0c $c(\\underline{W})\\in\\mathbb{R}^{d\\times L}$ \uff0c $h(\\underline{W}_{O V})\\,\\in\\,\\mathbb{R}^{d\\times L}$ \uff0c $q(\\underline{{W}})\\in$ $\\mathbb{R}^{L\\times L}$ , and $p(\\underline{W})\\in\\mathbb{R}^{L\\times L}$ as specifd inDenitionsG3 toG7, respectively $A_{1},A_{2}\\in\\mathbb{R}^{d\\times L}$ be two given matrices, and define $\\mathsf{A}=A_{1}^{\\top}\\otimes A_{2}^{\\top}$ . Define $g_{2}$ according to (O1), and let $g_{2}(\\underline{{W}})_{j_{0},i_{0}}$ be as described in (G.1). It holds ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}g_{2}}{\\mathrm{d}\\underline{{W}}}=\\mathrm{vec}\\left(A_{1}p(\\underline{{W}})A_{2}^{\\top}\\right).\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Proof. By definitions, (G.1) gives ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{\\mathrm{d}(g_{2})_{j_{0},i_{0}}}{\\mathrm{d}\\underline{{W}}_{i_{0}}}}\\\\ &{=c_{j_{0},i_{0}}\\cdot\\Big(\\underbrace{\\langle f(\\underline{{W}})_{j_{0}}\\odot\\mathsf{A}_{j_{0},i_{0}},h(\\underline{{W}}_{O V})_{i_{0}}\\rangle}_{=\\mathsf{A}_{j_{0},i}^{\\top}\\mathrm{~diag}(f(\\underline{{W}})_{j_{0}})h(\\underline{{W}}_{O V})_{i_{0}}}-\\underbrace{\\langle f(\\underline{{W}})_{j_{0}},h(\\underline{{W}}_{O V})_{i_{0}}\\rangle}_{=\\mathsf{A}_{j_{0},i}^{\\top}\\mathrm{~}f(\\underline{{W}})_{j_{0}}f(\\underline{{W}})_{j_{0}}h(\\underline{{W}}_{O V})_{i_{0}}}\\Big)\\cdot}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\big(\\mathrm{By}\\ \\langle a\\odot b,c\\rangle=a^{\\top}\\mathrm{~diag}(b)c\\,\\mathrm{for~}a,b,c\\in\\mathcal{C}^{*}\\mathrm{~,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Therefore, (G.4) becomes ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\mathrm{d}(g_{2})_{j_{0},i_{0}}}{\\mathrm{d}\\underline{{W}}_{i_{0}}}=c_{j_{0},i_{0}}\\cdot(\\mathsf{A}_{j_{0},i}^{\\top}\\,\\mathrm{diag}(f(\\underline{{W}})_{j_{0}})h(\\underline{{W_{O V}}})_{i_{0}}-\\mathsf{A}_{j_{0},i}^{\\top}\\,f(\\underline{{W}})_{j_{0}}f(\\underline{{W}})_{j_{0}}^{\\top}h(\\underline{{W_{O V}}})_{i_{0}})}}\\\\ &{}&{=c_{j_{0},i_{0}}\\cdot\\mathsf{A}_{j_{0},i}^{\\top}(\\mathrm{diag}(f(\\underline{{W}})_{j_{0}})-f(\\underline{{W}})_{j_{0}}f(\\underline{{W}})_{j_{0}}^{\\top})h(\\underline{{W}}_{O V})_{i_{0}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Then, by definitions of $q(\\cdot),p(\\cdot)$ , we complete the proof. ", "page_idx": 57}, {"type": "text", "text": "G.1.2 Low-Rank Approximations of Building Blocks Part I: $f(\\cdot),q(\\cdot)$ and $c(\\cdot)$ ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "The defintions of $p,\\,p_{1},\\,p_{2}$ , and Lemma G.2 show that te DiT rining gadient $\\frac{\\mathrm{d}g_{2}}{\\mathrm{d}\\underline{{W}}}$ involves entry-wise products of $f,q$ , and $c$ . Therefore, if we approximate these with inner-dimension-matched low-rank approximations, computing d itsef becomes a low-rank approximation. In the following sections, we present low-rank approximations for $f,\\,q.$ and $c$ ", "page_idx": 57}, {"type": "text", "text": "Lemma G.3 (Approximate $f(\\cdot)$ , Modified from [Alman and Song, 2023]). Let $\\Gamma\\,=\\,o(\\sqrt{\\log L})$ and $k_{1}~=~L^{o(1)}$ . Let $A_{1},A_{2},\\in\\ \\mathbb{R}^{d\\times L}$ \uff0c $W~\\in~\\mathbb{R}^{d\\times d}$ and $f(\\underline{{W}})~=~D^{-1}\\exp\\bigl(A_{1}^{\\top}{\\mathbf{X}}A_{2}\\bigr)$ with D=diag $\\left(\\exp\\left({A_{1}^{\\top}W A_{2}}\\right)\\mathbb{1}_{L}\\right)$ follows DefnionsG. oGI andG 5.If max $\\left(\\left\\Vert A_{1}^{\\top}W\\right\\Vert_{\\operatorname*{max}}\\leq$ $\\Gamma,\\Vert A_{2}\\Vert_{\\operatorname*{max}})\\leq\\Gamma$ thn thexistwomaties $U_{1},V_{1}\\in\\mathbb{R}^{L\\times k_{1}}$ such hat $\\left\\|U_{1}V_{1}^{\\top}-f(\\underline{{W}})\\right\\|_{\\operatorname*{max}}\\leq$ $\\epsilon/\\mathrm{poly}(L)$ . In addition, it takes $L^{1+o(1)}$ time to construct $U_{1}$ and $V_{1}$ ", "page_idx": 57}, {"type": "text", "text": "Proof. By [Alman and Song, 2023, Theorem 3], we complete the proof. ", "page_idx": 57}, {"type": "text", "text": "Lemma G.4 (Approximate $c(\\cdot).$ ). Assume all numerical values are in $O(\\log L)$ bits. Let $d=O(\\log L)$ and $c(\\underline{W})\\,\\in\\,\\mathbb{R}^{L\\times d}$ follows Definition G.5. There exist two matrices $U_{1},V_{1}\\,\\in\\,\\mathbb{R}^{L\\times k_{1}}$ such that $\\big\\|U_{1}V_{1}^{\\top}h(W_{O V})-Y^{\\top}-c(\\underline{{W}})\\big\\|_{\\operatorname*{max}}\\leq\\epsilon/\\mathrm{poly}(L).$ ", "page_idx": 57}, {"type": "text", "text": "Proof of Lemma G.4. ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\big\\|U_{1}V_{1}^{\\top}h(W_{O V})-Y^{\\top}-c(\\underline{{W}})\\big\\|_{\\operatorname*{max}}=\\big\\|U_{1}V_{1}^{\\top}h(W_{O V})-Y^{\\top}-(f(\\underline{{W}})h(W_{O V})-Y^{\\top})\\big\\|_{\\operatorname*{max}}}}\\\\ &{}&{\\quad\\mathrm{~(By~Definition~G.S)~}}\\\\ &{}&{=\\big\\|\\left[U_{1}V_{1}^{\\top}-f(\\underline{{W}})\\right]h(W_{O V})\\big\\|_{\\operatorname*{max}}}\\\\ &{}&{\\leq\\epsilon/\\mathrm{poly}(L).\\qquad\\quad\\mathrm{~(By~[Alman~and~Song,~}2023,\\mathrm{Theorem~}3])}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Lemma G.5 (Approximate $q(\\cdot))$ . Let $k_{2}\\;=\\;L^{o(1)}$ \uff0c $c(\\cdot)\\ \\in\\ \\mathbb{R}^{L\\times d}$ follow Definition G.5 and let $q(\\underline{{W}})\\,:=\\,c(\\underline{{W}})h(\\underline{{W}}_{O V})^{\\mathsf{T}}\\,\\in\\,\\mathbb{R}^{L\\times L}$ (follow Definition G.6). There exist two matrices $U_{2},V_{2}\\in$ $\\mathbb{R}^{L\\times k_{2}}$ such that $\\left\\|U_{2}V_{2}^{\\top}-q(\\underline{{W}})\\right\\|_{\\operatorname*{max}}\\leq\\epsilon/\\mathrm{poly}(L)$ In aditio, it takes $L^{1+o(1)}$ time to construct $U_{2},V_{2}$ ", "page_idx": 58}, {"type": "text", "text": "Proof of Lemma G.5. Our proof is built on [Alman and Song, 2023, Lemma D.3]. ", "page_idx": 58}, {"type": "text", "text": "Let $\\widetilde q(\\cdot)$ denote an approximation to $q(\\cdot)$ ", "page_idx": 58}, {"type": "text", "text": "By Lemma G.4, $U_{1}V_{1}^{\\top}h(W_{O V})-Y$ approximates $c(\\underline{{W}})$ up to accuracy $\\epsilon=1/\\mathrm{poly}(L)$ ", "page_idx": 58}, {"type": "text", "text": "Thus, by setting $\\widetilde{q}(\\underline{{W}})=h(W_{O V})\\left(U_{1}V_{1}^{\\top}h(W_{O V})-Y\\right)^{\\top}$ , we find a low-rank form for $\\widetilde q(\\cdot)$ ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\widetilde{q}(\\underline{{W}})=h(W_{O V})\\left(h(W_{O V})\\right)^{\\top}V_{1}U_{1}^{\\top}-h(W_{O V})Y^{\\top},\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "such that ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Vert\\widetilde{q}(\\underline{{W}})-q(\\underline{{W}})\\Vert_{\\operatorname*{max}}=\\left\\Vert h(W_{O V})\\left(U_{1}V_{1}^{\\top}h(W_{O V})-Y\\right)^{\\top}-h(W_{O V})Y^{\\top}\\right\\Vert_{\\operatorname*{max}}}\\\\ &{\\qquad\\qquad\\qquad\\leq d\\left\\Vert h(W_{O V})\\right\\Vert_{\\operatorname*{max}}\\left\\Vert U_{1}V_{1}^{\\top}h(W_{O V})-Y-c(\\underline{{W}})\\right\\Vert_{\\operatorname*{max}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\epsilon/\\mathrm{poly}(L).}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "By $k_{1},d=L^{o(1)}$ , compute $\\underbrace{\\left(h(W_{O V})\\right)^{\\top}}_{d\\times L}\\underbrace{V_{1}}_{L\\times k_{1}}\\underbrace{U_{1}^{\\top}}_{k_{1}\\times L}$ takes only $L^{1+o(1)}$ time. This completes the ", "page_idx": 58}, {"type": "text", "text": "proof. ", "page_idx": 58}, {"type": "text", "text": "G.1.3  Low-Rank Approximations of Building Blocks Part II: $p(\\cdot)$ ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Now, we use the low-rank approximations of $f,q,c$ to construct low-rank approximations for $p_{1}(\\cdot),p_{2}(\\cdot),p(\\cdot)$ ", "page_idx": 58}, {"type": "text", "text": "Lemma G.6 (Approximate $p_{1}(\\cdot))$ . Let $k_{1},k_{2}\\,=\\,L^{o(1)}$ . Suppose $U_{1},V_{1}\\,\\in\\,\\mathbb{R}^{L\\times k_{1}}$ approximates $f(\\underline{{W}})\\in\\mathbb{R}^{L\\times L}$ such that $\\left\\|U_{1}V_{1}^{\\top}-f(\\underline{{W}})\\right\\|_{\\operatorname*{max}}\\leq\\epsilon/\\mathrm{poly}(L)$ and $U_{2},V_{2}\\in\\mathbb{R}^{L\\times k_{2}}$ approximates the $q(\\underline{{W}})\\,\\in\\,\\mathbb{R}^{L\\times L}$ such that $\\left\\|U_{2}V_{2}^{\\top}-q(\\underline{{W}})\\right\\|_{\\mathrm{max}}\\leq\\epsilon/\\mathrm{poly}(L)$ Then the existwo marices $U_{3},V_{3}\\in\\mathbb{R}^{L\\times k_{3}}$ such that $\\left\\|{U_{3}V_{3}^{\\top}-p_{1}(\\underline{{W}})}\\right\\|_{\\mathrm{max}}\\leq\\epsilon/\\mathrm{poly}(L)$ In aditio, itakes time to construct $U_{3},V_{3}$ ", "page_idx": 58}, {"type": "text", "text": "Proof of Lemma G.6. By tensor trick, we construct $U_{3}$ $V_{3}$ as tensor products of $U_{1},V_{1}$ and $U_{2},V_{2}$ respectively, while preserving their low-rank structures. Then, we show the low-rank approximation of $p_{1}(\\cdot)$ with bounded error by Lemma G.3 and Lemma G.5. ", "page_idx": 58}, {"type": "text", "text": "Let $\\oslash$ be column-wise Kronecker product such that $A\\oslash B:=[A[\\cdot,1]\\otimes B[\\cdot,1]\\mid.\\dots\\mid A[\\cdot,k_{1}]\\otimes$ $B[\\cdot,k_{1}]]\\in\\mathbb{R}^{L\\times k_{1}k_{2}}$ for $A\\in\\mathbb{R}^{L\\times\\bar{k_{1}}},B\\in\\mathbb{R}^{L\\times k_{2}}$ ", "page_idx": 58}, {"type": "text", "text": "Let ${\\widetilde{f}}(\\underline{{W}}):=U_{1}V_{1}^{\\top}$ and $\\widetilde{q}(\\underline{{W}}):=U_{2}V_{2}^{\\mathsf{T}}$ denote matrix-multiplication approximations to $f(\\underline{{W}})$ and $q(\\underline{W})$ , respectively. ", "page_idx": 58}, {"type": "text", "text": "Lxk1 L\u00d7k2 $L\\times k_{1}$ L\u00d7k2 For the case of presentation, let $U_{3}=\\widehat{\\phantom{U_{1}}}\\osl_{\\bigcirc}\\widehat{\\phantom{U_{2}}}$ and $V_{3}=\\widehat{V_{1}\\quad}\\mathcal{O}\\widehat{V_{2}}$ . It holds ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|{U_{3}V_{3}^{\\top}-p_{1}(\\underline{{W}})}\\right\\|_{\\operatorname*{max}}}\\\\ &{=\\left\\|{U_{3}V_{3}^{\\top}-f(\\underline{{W}})\\odot q(\\underline{{W}})}\\right\\|_{\\operatorname*{max}}}\\\\ &{=\\left\\|{(U_{1}\\odot U_{2})\\left(V_{1}\\odot V_{2}\\right)^{\\top}-f(\\underline{{W}})\\odot q(\\underline{{W}})}\\right\\|_{\\operatorname*{max}}}\\\\ &{=\\left\\|{(U_{1}V_{1}^{\\top})\\odot\\left(U_{2}V_{2}^{\\top}\\right)-f(\\underline{{W}})\\odot q(\\underline{{W}})}\\right\\|_{\\operatorname*{max}}}\\\\ &{=\\left\\|{\\tilde{f}(\\underline{{W}})\\odot\\tilde{q}(\\underline{{W}})-f(\\underline{{W}})\\odot q(\\underline{{W}})}\\right\\|_{\\operatorname*{max}}}\\\\ &{\\leq\\underbrace{\\left\\|{\\tilde{f}(\\underline{{W}})\\odot\\tilde{q}(\\underline{{W}})-\\tilde{f}(\\underline{{W}})\\odot q(\\underline{{W}})}\\right\\|_{\\operatorname*{max}}}_{\\leq\\epsilon/\\mathrm{poly}(L)}+\\underbrace{\\left\\|{\\tilde{f}(\\underline{{W}})\\odot q(\\underline{{W}})-f(\\underline{{W}})\\odot q(\\underline{{W}})}\\right\\|_{\\operatorname*{max}}}_{\\leq\\epsilon/\\mathrm{poly}(L)}}\\\\ &{\\leq\\epsilon/\\mathrm{poly}(L).}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Computationally, by $k_{1},k_{2}=L^{o(1)}$ , computing $U_{3}$ and $V_{3}$ takes $L^{1+o(1)}$ time. This completes the proof. \u53e3 ", "page_idx": 59}, {"type": "text", "text": "Lemma G.7 (Approximate $p_{2}(\\cdot)^{\\prime}$ 0. Let $k_{1},k_{2},k_{4}=L^{o(1)}$ . Let $p_{2}(\\underline{{W}})\\in\\mathbb{R}^{L\\times L}$ follow Definition G.7 such that its $j_{0}$ -th column is $p_{2}(\\underline{{W}})_{j_{0}}~=~f(\\underline{{W}})_{j_{0}}f(\\underline{{W}})_{j_{0}}^{\\top}q(\\underline{{W}})_{j_{0}}$ for each $j_{0}~\\in~[L]$ . Suppose $U_{1},V_{1}\\in\\mathbb{R}^{L\\times k_{1}}$ approximates the $\\operatorname{f}(\\mathbf{X})$ such that $\\left\\|U_{1}V_{1}^{\\top}-f(\\underline{{W}})\\right\\|_{\\operatorname*{max}}\\leq\\epsilon/\\mathrm{poly}(L),$ and $U_{2},V_{2}\\in$ $\\mathbb{R}^{L\\times k_{2}}$ approximates the $q(\\underline{{W}})\\in\\mathbb{R}^{L\\times L}$ such that $\\left\\|U_{2}V_{2}^{\\top}-q(\\underline{{W}})\\right\\|_{\\operatorname*{max}}\\leq\\epsilon/\\mathrm{poly}(L).$ Then there exist matrices $U_{4},V_{4}\\in\\mathbb{R}^{L\\times k_{4}}$ such that $\\left\\|U_{4}V_{4}^{\\top}-p_{2}(\\right)\\right\\|_{\\mathrm{max}}\\leq\\epsilon/\\mathrm{poly}(L)$ . In adition, it takes $L^{1+o(1)}$ time to construct $U_{4},V_{4}$ ", "page_idx": 59}, {"type": "text", "text": "Proof of Lemma G.7. From Definition G.7, ", "page_idx": 59}, {"type": "equation", "text": "$$\np_{2}(\\underline{{W}})_{j_{0}}:=\\overbrace{f\\left(\\underline{{W}}\\right)_{j_{0}}\\underbrace{f\\left(\\underline{{W}}\\right)_{j_{0}}^{\\top}q(\\underline{{W}})_{j_{0}}}_{(I)}}^{(I I)}.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "For (I), we show its low-rank approximation by observing the low-rank-preserving property of the multiplication between $f(\\cdot)$ and $q(\\cdot)$ (from Lemma G.3 and Lemma G.5). For (Il), we show its low-rank approximation by the low-rank structure of $f(\\cdot)$ and (I). ", "page_idx": 59}, {"type": "text", "text": "Part I). We defne a function $\\boldsymbol{r}(\\underline{{W}}):\\mathbb{R}^{d^{2}}\\,\\rightarrow\\,\\mathbb{R}^{L}$ such that the $j_{0}$ -th component $r(\\underline{{W}})_{j_{0}}\\,:=$ $\\left(f(\\underline{{W}})_{j_{0}}\\right)^{\\top}q(\\underline{{W}})_{j_{0}}$ for all $j_{0}\\in[L]$ . Let $\\widetilde{r}(\\underline{{W}})$ denote the approximation of $r(\\underline{{W}})$ via decomposing into $f(\\cdot)$ and $q(\\cdot)$ ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{r}(\\underline{{W}})_{j_{0}}:=\\left\\langle\\widetilde{f}(\\underline{{W}})_{j_{0}},\\widetilde{q}(\\underline{{W}})_{j_{0}}\\right\\rangle=\\left(U_{1}V_{1}^{\\top}\\right)\\left[j_{0},\\cdot\\right]\\cdot\\left[\\left(U_{2}V_{2}^{\\top}\\right)\\left[j_{0},\\cdot\\right]\\right]^{\\top}}\\\\ &{\\qquad\\qquad=U_{1}[j_{0},\\cdot\\Big]\\underbrace{V_{1}^{\\top}}_{k_{1}\\times L}\\underbrace{V_{2}}_{L\\times k_{2}}\\left(U_{2}[j_{0},\\cdot]\\right)^{\\top},}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "for all $j_{0}\\in[L]$ . This allows us to write $p_{2}(\\underline{{W}})=f(\\underline{{W}})\\,\\mathrm{diag}(r(\\underline{{W}}))$ with $\\mathrm{diag}(\\widetilde{r}(\\underline{{W}}))$ denoting a diagonal matrix with diagonal entries being components of $\\widetilde{r}(\\underline{{W}})$ ", "page_idx": 59}, {"type": "text", "text": "Part $(\\mathbf{II})$ .With $r(\\cdot)$ , we approximate $p_{2}(\\cdot)$ with $\\widetilde{p}_{2}(\\underline{{W}})=\\widetilde{f}(\\underline{{W}})\\,\\mathrm{diag}(\\widetilde{r}(\\underline{{W}}))$ as follows. ", "page_idx": 59}, {"type": "text", "text": "Since $\\widetilde f(\\underline{{W}})$ has low rank representation, and $\\mathrm{diag}(\\widetilde{r}(\\underline{{W}}))$ is a diagonal matrix, $\\widetilde{p}_{2}(\\cdot)$ has low-rank representation by definition. Thus, we set $\\widetilde{p}_{2}(\\underline{{W}})=U_{4}V_{4}^{\\mathsf{T}}$ with $U_{4}=U_{1}$ and $V_{4}=\\mathrm{diag}(\\widetilde{r}(\\underline{{W}}))V_{1}$ Then, we bound the approximation error ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\left\\|U_{4}V_{4}^{\\top}-p_{2}(\\underline{{W}})\\right\\|_{\\mathrm{max}}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\left.\\lVert\\widetilde{p}_{2}(W)-p_{2}(W)\\rVert_{\\operatorname*{max}}\\right.}\\\\ &{=\\left.\\operatorname*{max}_{j_{0}\\in[L]}\\left\\lVert\\widetilde{f}(\\underline{{W}})_{j_{0}}\\widetilde{r}(\\underline{{W}})_{j_{0}}-f(\\underline{{W}})_{j_{0}}r(\\underline{{W}})_{j_{0}}\\right\\rVert_{\\operatorname*{max}}}\\\\ &{\\leq\\left.\\operatorname*{max}_{j_{0}\\in[L]}\\left[\\left\\lVert\\widetilde{f}(\\underline{{W}})_{j_{0}}\\widetilde{r}(\\underline{{W}})_{j_{0}}-f(\\underline{{W}})_{j_{0}}r(\\underline{{W}})_{j_{0}}\\right\\rVert_{\\operatorname*{max}}+\\left\\lVert\\widetilde{f}(\\underline{{W}})_{j_{0}}\\widetilde{r}(\\underline{{W}})_{j_{0}}-f(\\underline{{W}})_{j_{0}}r(\\underline{{W}})_{j_{0}}\\right\\rVert_{\\operatorname*{max}}\\right]}\\\\ &{\\leq\\epsilon/\\mathrm{poly}(L).}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Computationally, computing $V_{1}^{\\top}V_{2}$ takes $L^{1+o(1)}$ time by. $k_{1},k_{2}\\,=\\,L^{o(1)}$ . Once we have $V_{1}^{\\top}V_{2}$ precomputed, (G.6) only takes $O(k_{1}k_{2})$ time for each $j_{0}\\in[L]$ . Thus, the total time is $O\\left(L k_{1}k_{2}\\right)=$ $L^{1+o(1)}$ .Since $U_{1}$ and $V_{1}$ takes $L^{1+o(1)}$ time to construct and $V_{4}=\\underbrace{\\mathrm{diag}(\\widetilde{r}(W))}_{L\\times L}\\underbrace{V_{1}}_{L\\times k_{1}}$ also takes ", "page_idx": 60}, {"type": "text", "text": "$L^{1+o(1)}$ time, $U_{4}$ and $V_{4}$ takes $L^{1+o(1)}$ time to construct. This completes the proof. ", "page_idx": 60}, {"type": "text", "text": "G.2Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Proof of Theorem 4.1. By the definitions of matrices $p(\\cdot),p_{1}(\\cdot)$ and $p_{2}(\\cdot)$ (Definition G.7), we have ", "page_idx": 60}, {"type": "equation", "text": "$$\np(\\underline{{W}})=p_{1}(\\underline{{W}})-p_{2}(\\underline{{W}}).\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "By Lemma G.2, we have ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}g_{2}}{\\mathrm{d}\\underline{{W}}}=\\mathrm{vec}\\left(A_{1}p(\\underline{{W}})A_{2}^{\\top}\\right).\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "To show the existence of $L^{1+o(1)}$ algorithms for DiT backward computation Problem 1, we prove fast low-rank approximations for $\\bar{A_{1}p_{1}}(\\underline{{W}})A_{2}^{\\top}$ and $A_{1}p_{2}(\\underline{{W}})A_{2}^{\\top}$ as follows. ", "page_idx": 60}, {"type": "text", "text": "Let $\\widetilde{p}_{1}(\\underline{{W}}),\\widetilde{p_{2}}(\\underline{{W}})$ denote the approximations to $p_{1}(\\underline{{W}}),p_{2}(\\underline{{W}})$ , respectively. ", "page_idx": 60}, {"type": "text", "text": "By Lemma G.6, it takes $L^{1+o(1)}$ time to construct $U_{3},V_{3}\\in\\mathbb{R}^{L\\times k_{3}}$ such that ", "page_idx": 60}, {"type": "equation", "text": "$$\nA_{1}\\widetilde{p}_{1}(\\underline{{W}})A_{2}^{\\top}=A_{1}U_{3}V_{3}^{\\top}A_{2}^{\\top}.\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Then, computing $\\underbrace{A_{1}}_{d\\times L}\\underbrace{U_{3}}_{L\\times k_{3}}\\underbrace{V_{3}^{\\top}}_{k_{3}\\times L}\\underbrace{A_{2}^{\\top}}_{L\\times d}$ takes $L^{1+o(1)}$ due to the fact that $d,k_{1}k_{3}=L^{o(1)}$ ", "page_idx": 60}, {"type": "text", "text": "Therefore, total running time for $A_{1}p_{1}(\\underline{{W}})A_{2}^{\\top}$ is $L\\cdot L^{o(1)}=L^{1+o(1)}$ ", "page_idx": 60}, {"type": "text", "text": "For the same reason (by Lemma G.7), total running time for $A_{1}p_{2}(\\underline{{W}})A_{2}^{\\top}$ $L\\cdot L^{o(1)}=L^{1+o(1)}$ ", "page_idx": 60}, {"type": "text", "text": "Lastly, we have ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left\\|\\frac{\\partial g_{2}}{\\partial\\overline{{\\mathbb{B}}}}-\\widetilde{G}^{(W)}\\right\\|_{\\operatorname*{max}}}}\\\\ &{=\\left\\|\\mathrm{vec}\\left(A_{1}\\widetilde{p}(\\underline{{W}})A_{2}^{\\top}\\right)-\\mathrm{vec}\\left(A_{1}\\widetilde{p}(\\underline{{W}})A_{2}^{\\top}\\right)\\right\\|_{\\operatorname*{max}}}&{\\mathrm{(By~Lemma~G.2~}}\\\\ &{=\\left\\|\\left(A_{1}\\widetilde{p}(\\underline{{W}})A_{2}^{\\top}\\right)-\\left(A_{1}\\widetilde{p}(\\underline{{W}})A_{2}^{\\top}\\right)\\right\\|_{\\operatorname*{max}}\\mathrm{~\\Phi(By~definition,\\,\\|\\,\\boldsymbol{A}\\|_{\\operatorname*{max}})=\\operatorname*{max}_{i,j}\\left|A_{i j}\\right|\\mathrm{~for~any~matric~}}A}\\\\ &{\\leq\\left\\|\\left(A_{1}\\left[p_{1}(\\underline{{W}})-\\widetilde{p}_{1}(\\underline{{W}})\\right]A_{2}^{\\top}\\right)\\right\\|_{\\operatorname*{max}}+\\left\\|\\left(A_{1}\\left[p_{2}(\\underline{{W}})-\\widetilde{p}_{2}(\\underline{{W}})\\right]A_{2}^{\\top}\\right)\\right\\|_{\\operatorname*{max}}}\\\\ &{}&{\\mathrm{(By~Definition~G.7~and~tiriangle~inequality}}\\\\ &{\\leq\\|A_{1}\\|_{\\infty}\\|A_{2}\\|_{\\infty}\\left(\\|\\left(p_{1}(\\underline{{W}})-\\widetilde{p}_{1}(\\underline{{W}})\\right)\\|_{\\operatorname*{max}}+\\|\\left(p_{2}(\\underline{{W}})-\\widetilde{p}_{2}(\\underline{{W}})\\right)\\|_{\\operatorname*{max}}\\right)}\\\\ &{}&{\\mathrm{(By~the~sub-multive~property~of~\\mathbb{I}~\\mathbb{J}_{\\infty}~}}\\\\ &{\\leq\\epsilon/\\mathrm{poly}(L).}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Set $\\epsilon=1/\\mathrm{poly}(L)$ . We complete the proof. ", "page_idx": 60}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 61}, {"type": "text", "text": "Justification: Our contributions and scope in Section 3 and Section 4 are reflected by the claims in abstract and introduction. ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 61}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Justification: We discuss the limitations in Section 5. ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational effciency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 61}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 61}, {"type": "text", "text": "Justification: Yes. We include our proofs in the appendix and have made every effort to ensure the correctness of our theoreticalresults. ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 62}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 62}, {"type": "text", "text": "Answer: [NA] Justification: This is a formal theory work without experiments. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 62}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 62}, {"type": "text", "text": "Answer: [NA] Justification: This is a formal theory work without experiments. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: \u00b7 The answer NA means that paper does not include experiments requiring code. ", "page_idx": 62}, {"type": "text", "text": "\u00b7 Please see the NeurIPS code and data submission guidelines (https://nips.CC/ public/guides /CodeSubmi ssionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips . CC/ public/guides /CodeSubmi ssionPolicy)for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 63}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 63}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 63}, {"type": "text", "text": "Justification: This is a formal theory work without experiments. ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 63}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 63}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 63}, {"type": "text", "text": "Justification: This is a formal theory work without experiments. ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 63}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 64}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 64}, {"type": "text", "text": "Justification: This is a formal theory work without experiments. ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 64}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code ofEthics https: / /neurips.Cc/public/EthicsGuidelines? ", "page_idx": 64}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 64}, {"type": "text", "text": "Justification: Yes. We follow the code of ethics in this work. ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 64}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 64}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Justification: This theoretical work aims to shed light on the foundations of diffusion generative models and is not anticipated to have negative social impacts. ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ", "page_idx": 64}, {"type": "text", "text": "\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 65}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 65}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 65}, {"type": "text", "text": "Justification: This is a formal theory work without experiments. ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 65}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 65}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 65}, {"type": "text", "text": "Justification: This is a formal theory work without experiments. ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, papers wit hcode . com/ dataset s has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 65}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 65}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 65}, {"type": "text", "text": "Justification: This is a formal theory work without experiments. ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset is used. ", "page_idx": 65}, {"type": "text", "text": "\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 66}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 66}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 66}, {"type": "text", "text": "Justification: This is a formal theory work without experiments. ", "page_idx": 66}, {"type": "text", "text": "Guidelines: ", "page_idx": 66}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collction, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 66}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 66}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 66}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 66}, {"type": "text", "text": "Justification: This is a formal theory work without experiments. ", "page_idx": 66}, {"type": "text", "text": "Guidelines: ", "page_idx": 66}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 66}]