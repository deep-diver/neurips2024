[{"figure_path": "vo5LONGAdo/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Re-training a larger DiT incurs significant training and inference costs. (b) Multi-expert denoising trains multiple expert models to improve generation quality while maintaining low inference overhead. However, training multiple experts still results in significant training costs. (c) This work introduces RemixDiT, a learnable method to craft any number of experts by mixing basis models.", "description": "This figure illustrates three different approaches to improving the quality of diffusion models. (a) shows the traditional approach of simply increasing the size of the model. This is expensive in terms of training and inference costs. (b) shows a multi-expert approach where multiple smaller models are trained to handle different parts of the denoising process. This reduces the overhead of training and inference but still incurs significant training costs due to training multiple models. (c) shows the proposed RemixDiT approach. This uses learnable mixing coefficients to combine a smaller number of basis models to create a larger number of experts. This reduces training costs while maintaining efficiency.", "section": "1 Introduction"}, {"figure_path": "vo5LONGAdo/figures/figures_3_1.jpg", "caption": "Figure 2: An example of mixing 4 linear layers basis into 6 expert layers. Each expert linear layer is a weighted averaging of the basis layers. At each denoising interval, only one expert is activated for inference or training. To increase the number of experts, we increase the number of coefficients \u03b1, which is more efficient than independently training new experts.", "description": "This figure illustrates the core concept of Remix-DiT, which uses a learnable mixing mechanism to create multiple expert models from a smaller set of basis models.  The figure shows how 4 linear layer basis models are mixed using learnable coefficients \u03b1 to produce 6 expert layers. Each expert layer is a weighted combination of the basis layers. This allows for efficient creation of many experts without the computational cost of training each independently.  The experts are activated one at a time during inference and training, leading to cost efficiency.", "section": "4 Method"}, {"figure_path": "vo5LONGAdo/figures/figures_7_1.jpg", "caption": "Figure 4: (a) Learned mixing coefficients for RemixDiT-S-4-20, which crafts 20 experts by mixing 4 basis models. (b) Learned mixing coefficients for Remix-DiT-S-4-8. (c) Training losses of 8 experts over timesteps", "description": "Figure 4 visualizes the learned mixing coefficients and training losses for Remix-DiT models with different numbers of experts and basis models.  Panel (a) shows the coefficients for a model creating 20 experts from 4 basis models, illustrating how the coefficients change across different timesteps (denoising stages). Panel (b) shows the same visualization for a model with 8 experts and 4 basis models. Finally, panel (c) displays the training loss curves for each of the 8 experts, highlighting their specialized performance at different timesteps within their assigned intervals.", "section": "5.3 Visualization of Learned Experts"}, {"figure_path": "vo5LONGAdo/figures/figures_7_2.jpg", "caption": "Figure 4: (a) Learned mixing coefficients for Remix-DiT-S-4-20, which crafts 20 experts by mixing 4 basis models. (b) Learned mixing coefficients for Remix-DiT-S-4-8. (c) Training losses of 8 experts over timesteps", "description": "Figure 4 visualizes the learned mixing coefficients and training losses for Remix-DiT models with different numbers of experts and basis models.  Subfigure (a) shows the coefficients used to mix 4 basis models into 20 experts for Remix-DiT-S-4-20.  It highlights how the model weights the basis models differently across various timesteps, showing a preference for specific basis models at certain noise levels.  Subfigure (b) shows similar results but with 8 experts instead of 20, using the same 4 basis models. Subfigure (c) presents the training losses for each of the 8 experts across the timesteps, illustrating the specialization achieved by the model; each expert tends to have lower loss within its assigned timestep interval.", "section": "5.3 Visualization of Learned Experts"}, {"figure_path": "vo5LONGAdo/figures/figures_7_3.jpg", "caption": "Figure 4: (a) Learned mixing coefficients for RemixDiT-S-4-20, which crafts 20 experts by mixing 4 basis models. (b) Learned mixing coefficients for RemixDiT-S-4-8 (c) Training losses of 8 experts over timesteps", "description": "Figure 4 visualizes the learned mixing coefficients and training losses for Remix-DiT models with different numbers of experts and basis models.  (a) shows the coefficients used to mix 4 basis models to create 20 experts, revealing how the model allocates capacity across different timesteps. (b) displays similar information but for 8 experts and 4 basis models.  Finally, (c) plots the training losses for each of the 8 experts over 1000 timesteps, showing the specialization of experts across different time intervals.", "section": "5.3 Visualization of Learned Experts"}, {"figure_path": "vo5LONGAdo/figures/figures_9_1.jpg", "caption": "Figure 5: Visualization of generated samples from DiT-B and Remix-DiT-B.", "description": "This figure displays a comparison of image generation results between the standard DiT-B model and the proposed Remix-DiT-B model.  Five example images are shown for each model, illustrating the differences in image quality and detail produced by the two approaches.  Remix-DiT-B aims to improve image quality by using a novel multi-expert denoising method. The visual comparison allows for a qualitative assessment of the effectiveness of the Remix-DiT approach in enhancing image generation.", "section": "5.3 Visualization of Learned Experts"}]