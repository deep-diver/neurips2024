[{"figure_path": "prXfM5X2Db/figures/figures_2_1.jpg", "caption": "Figure 1: Illustration of the sampling process of our rectified-flow based V2A architecture.", "description": "This figure illustrates the sampling process within the rectified flow-based video-to-audio (V2A) architecture.  It shows how the model generates audio by solving an ordinary differential equation (ODE). Starting with noise sampled from a normal distribution (x0 ~ N(0, I)), the model uses a vector field estimator (represented by the ODE solver block) to progressively transform this noise into the latent representation of the mel-spectrogram (x1).  The vector field, v(x, t|c; \u03b8), is conditioned on visual features from the video (c). The resulting mel-spectrogram is then converted to audio using a vocoder.", "section": "3 Method"}, {"figure_path": "prXfM5X2Db/figures/figures_3_1.jpg", "caption": "Figure 2: Illustration of model architecture of FRIEREN at different levels.", "description": "This figure illustrates the architecture of the FRIEREN model at three levels: (a) shows the overall architecture, which includes visual encoder, length regulator, vector field estimator, and decoder; (b) details the vector field estimator network, which consists of visual and audio feature fusion via concatenation, feed-forward transformer blocks, and output vector field; and (c) shows the internal structure of a feed-forward transformer block. This figure helps to visualize the different components of the FRIEREN model and how they interact with each other.", "section": "3.2 Model architecture"}, {"figure_path": "prXfM5X2Db/figures/figures_7_1.jpg", "caption": "Figure 4: IS and FAD of the models with different steps.", "description": "This figure compares the Inception Score (IS) and Frechet Audio Distance (FAD) for different models (Diff-Foley with and without classifier guidance, FRIEREN, FRIEREN with reflow, and FRIEREN with reflow and distillation) across varying numbers of inference steps (1, 5, 10, 15, 20, 25).  It demonstrates the impact of reflow and distillation on the audio quality and diversity as the number of sampling steps decreases.", "section": "4.2 Results and analysis"}, {"figure_path": "prXfM5X2Db/figures/figures_8_1.jpg", "caption": "Figure 5: Model performance of FRIEREN under different CFG scales.", "description": "The figure shows the performance of FRIEREN model under different classifier-free guidance (CFG) scales.  It displays the impact of CFG scaling on multiple metrics: Inception Score (IS), Frechet Distance (FD), Frechet Audio Distance (FAD), Kullback-Leibler Divergence (KL), Kernel Inception Distance (KID), and Alignment Accuracy (Acc).  The plots illustrate how these metrics change as the CFG scale increases, indicating an optimal range for achieving a balance between audio quality, diversity, and alignment.", "section": "4.3.3 Classifier-free guidance scale"}, {"figure_path": "prXfM5X2Db/figures/figures_13_1.jpg", "caption": "Figure 1: Illustration of the sampling process of our rectified-flow based V2A architecture.", "description": "This figure illustrates the sampling process of the FRIEREN model.  It starts with noise sampled from a standard normal distribution (x0 ~ N(0, 1)). This noise is then conditioned on visual features (c) from the video and passed through an ODE solver using the estimated vector field (v(x, t|c; \u03b8)) to generate the mel-spectrogram latent (x1). Finally, this latent representation is decoded using an autoencoder to produce the spectrogram, which is then converted to an audio waveform using a vocoder.", "section": "3 Method"}, {"figure_path": "prXfM5X2Db/figures/figures_14_1.jpg", "caption": "Figure 1: Illustration of the sampling process of our rectified-flow based V2A architecture.", "description": "This figure illustrates the sampling process of the FRIEREN model.  It shows how the model uses an ODE solver to generate audio from a latent representation derived from a noise distribution and conditioned on video features. The process starts with noise sampled from a normal distribution, which is then transformed through a series of steps guided by a vector field, ultimately leading to a latent representation of the audio spectrogram. This representation is then decoded into an audio waveform via an autoencoder and vocoder.", "section": "3 Method"}]