[{"figure_path": "sxZlp9ZoHD/tables/tables_3_1.jpg", "caption": "Table 1: Perplexity results on language modeling and MMLU [24] answers. We use the augmented Transformer architecture proposed in LLaMA [48] for reference. For language modeling, we report perplexity on both the overall validation set and fine-grained diagnosis sets [2], i.e., \u201cAR-Hit\u201d evaluates the associative recall capability, and \"First-Occur\" indicates the regular language modeling performance. Besides, we evaluate the answer perplexity of MMLU subsets.", "description": "This table compares the performance of RetNet with various Transformer variants on language modeling and MMLU tasks.  It shows perplexity scores for the overall validation set and also provides a breakdown into \"AR-Hit\" (associative recall) and \"First-Occur\" (regular language modeling) components.  Additionally, it includes the average perplexity of answers on MMLU subsets.", "section": "3.1 Comparison with Transformer Variants"}, {"figure_path": "sxZlp9ZoHD/tables/tables_4_1.jpg", "caption": "Table 1: Perplexity results on language modeling and MMLU [24] answers. We use the augmented Transformer architecture proposed in LLaMA [48] for reference. For language modeling, we report perplexity on both the overall validation set and fine-grained diagnosis sets [2], i.e., \u201cAR-Hit\u201d evaluates the associative recall capability, and \"First-Occur\" indicates the regular language modeling performance. Besides, we evaluate the answer perplexity of MMLU subsets.", "description": "This table presents the results of a comparison of RetNet with various efficient Transformer variants across two tasks: language modeling and MMLU.  The language modeling results are broken down into overall perplexity and finer-grained metrics (AR-Hit and First-Occur) which measure associative recall ability and regular language modeling performance.  MMLU results provide a measure of knowledge-intensive task performance.  The table uses the augmented Transformer architecture from LLaMA as a baseline for comparison.", "section": "3.1 Comparison with Transformer Variants"}, {"figure_path": "sxZlp9ZoHD/tables/tables_7_1.jpg", "caption": "Table 2: Zero-shot and few-shot learning performance. The language model size is 6.7B.", "description": "This table presents the results of zero-shot and few-shot (4-shot) learning experiments on several downstream tasks using a 6.7B parameter language model.  The tasks include HellaSwag (HS), BoolQ, COPA, PIQA, Winograd, Winogrande, and StoryCloze (SC).  The table shows the accuracy achieved by both a Transformer model and the proposed Retentive Network (RetNet) model on each task, demonstrating the comparative performance of RetNet.", "section": "3.1 Comparison with Transformer Variants"}, {"figure_path": "sxZlp9ZoHD/tables/tables_7_2.jpg", "caption": "Table 3: Perplexity results on language modeling and MMLU [24] answers. For language modeling, we report perplexity on both the overall validation set and fine-grained diagnosis sets [2], i.e., \u201cAR-Hit\u201d evaluates the associative recall capability, and \u201cFirst-Occur\u201d indicates the regular language modeling performance. Besides, we evaluate the answer perplexity of the MMLU subsets.", "description": "This table presents the ablation study results of Retentive Network (RetNet). It shows the impact of removing or modifying certain components of RetNet on language modeling performance, as measured by perplexity on various datasets, including the overall validation set and its fine-grained components (AR-Hit and First-Occur), and the Massive Multitask Language Understanding (MMLU) benchmark.", "section": "3.1 Comparison with Transformer Variants"}, {"figure_path": "sxZlp9ZoHD/tables/tables_8_1.jpg", "caption": "Table 4: Results on vision tasks, i.e., image classification (ImageNet), object detection (COCO), and semantic segmentation (ADE20K). RetNet achieves competitive performance with DeiT, which is a well-tuned vision Transformer.", "description": "This table presents a comparison of the performance of RetNet and DeiT (a well-tuned vision Transformer) on three vision tasks: ImageNet image classification, COCO object detection, and ADE20K semantic segmentation.  The results show that RetNet achieves comparable or slightly better performance than DeiT across these tasks.", "section": "3.8 Results on Vision Tasks"}, {"figure_path": "sxZlp9ZoHD/tables/tables_13_1.jpg", "caption": "Table 1: Perplexity results on language modeling and MMLU [24] answers. We use the augmented Transformer architecture proposed in LLaMA [48] for reference. For language modeling, we report perplexity on both the overall validation set and fine-grained diagnosis sets [2], i.e., \u201cAR-Hit\u201d evaluates the associative recall capability, and \"First-Occur\" indicates the regular language modeling performance. Besides, we evaluate the answer perplexity of MMLU subsets.", "description": "This table presents the perplexity scores achieved by various Transformer models, including RetNet, on language modeling and MMLU tasks. The language modeling perplexity is broken down into two metrics: AR-Hit (associative recall) and First-Occur (predicting unseen tokens).  The MMLU scores are the average perplexity of correct answers.  The table compares RetNet's performance against other efficient Transformer variants, highlighting RetNet's competitive performance and superior scaling properties.", "section": "3.1 Comparison with Transformer Variants"}, {"figure_path": "sxZlp9ZoHD/tables/tables_14_1.jpg", "caption": "Table 6: Language modeling perplexity of RetNet and Transformer with different context length. The results show that RetNet has a consistent advantage across sequence length.", "description": "This table compares the language modeling perplexity achieved by RetNet and Transformer models across various context lengths (512, 1024, and 2048).  The perplexity metric measures how well the model predicts the next word in a sequence; a lower perplexity indicates better performance. The table demonstrates that RetNet consistently outperforms Transformer at all tested context lengths.", "section": "3.3 Long-Context Evaluation"}, {"figure_path": "sxZlp9ZoHD/tables/tables_14_2.jpg", "caption": "Table 1: Perplexity results on language modeling and MMLU [24] answers. We use the augmented Transformer architecture proposed in LLaMA [48] for reference. For language modeling, we report perplexity on both the overall validation set and fine-grained diagnosis sets [2], i.e., \u201cAR-Hit\u201d evaluates the associative recall capability, and \u201cFirst-Occur\u201d indicates the regular language modeling performance. Besides, we evaluate the answer perplexity of MMLU subsets.", "description": "This table compares the performance of Retentive Network (RetNet) with other Transformer variants on language modeling and MMLU (Massive Multitask Language Understanding) tasks.  The language modeling results are broken down into overall perplexity, AR-Hit (associative recall), and First-Occur (novel token prediction) to provide a more nuanced evaluation. MMLU results represent the model's ability to answer knowledge-intensive questions.", "section": "3.1 Comparison with Transformer Variants"}, {"figure_path": "sxZlp9ZoHD/tables/tables_15_1.jpg", "caption": "Table 1: Perplexity results on language modeling and MMLU [24] answers. We use the augmented Transformer architecture proposed in LLaMA [48] for reference. For language modeling, we report perplexity on both the overall validation set and fine-grained diagnosis sets [2], i.e., \u201cAR-Hit\u201d evaluates the associative recall capability, and \"First-Occur\" indicates the regular language modeling performance. Besides, we evaluate the answer perplexity of MMLU subsets.", "description": "This table compares the performance of RetNet with other Transformer variants on language modeling and MMLU tasks.  The language modeling results are broken down into overall perplexity, AR-Hit (associative recall), and First-Occur (novel word prediction), providing a more granular view of model performance.  The MMLU results show the model's ability to handle knowledge-intensive tasks.", "section": "3.1 Comparison with Transformer Variants"}, {"figure_path": "sxZlp9ZoHD/tables/tables_15_2.jpg", "caption": "Table 9: Answer recall of RetNet and Transformer on open-ended question answering.", "description": "This table presents the results of a one-shot evaluation on two open-ended question-answering tasks: SQUAD and WebQS.  The models used are a 6.7B parameter Transformer and a 6.7B parameter RetNet.  The metric reported is recall, indicating whether the answers generated by the models are contained within the correct response. RetNet shows improvements over the Transformer on both tasks.", "section": "3.1 Comparison with Transformer Variants"}, {"figure_path": "sxZlp9ZoHD/tables/tables_16_1.jpg", "caption": "Table 10: Inference cost of RetNet and LLaMA2-70B with difference batch size and length. LLaMA2-70B is equipped with grouped-query attention, reducing key/value heads by 8\u00d7. \u201c-GQ2\u201d means grouped-query retention, which reduces half of key/value heads. \u201c-2k\u201d and \u201c-8k\u201d indicate sequence length for LLaMA2, while RetNet is length-invariant. RetNet is capable of large-batch inference and is favourable in terms of latency, throughput, and GPU memory.", "description": "This table compares the inference cost (latency, throughput, and GPU memory usage) of RetNet and LLaMA2-70B models under different batch sizes (8 and 256) and sequence lengths (2k and 8k).  The experiment highlights RetNet's length-invariant performance and superior efficiency compared to LLaMA2, especially regarding memory usage.  The use of grouped-query retention (GQ2) in RetNet further improves these metrics.", "section": "3.4 Inference Cost"}, {"figure_path": "sxZlp9ZoHD/tables/tables_16_2.jpg", "caption": "Table 1: Perplexity results on language modeling and MMLU [24] answers. We use the augmented Transformer architecture proposed in LLaMA [48] for reference. For language modeling, we report perplexity on both the overall validation set and fine-grained diagnosis sets [2], i.e., \u201cAR-Hit\u201d evaluates the associative recall capability, and \"First-Occur\" indicates the regular language modeling performance. Besides, we evaluate the answer perplexity of MMLU subsets.", "description": "This table presents the results of language modeling and MMLU experiments, comparing RetNet's performance against various Transformer variants.  It breaks down language modeling perplexity into \"AR-Hit\" (associative recall) and \"First-Occur\" (regular language modeling) for a more nuanced understanding of model performance.  The MMLU results assess the models' knowledge-intensive capabilities.", "section": "3.1 Comparison with Transformer Variants"}]