[{"type": "text", "text": "The Poisson Midpoint Method for Langevin Dynamics: Provably Efficient Discretization for Diffusion Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Saravanan Kandasamy\\* Department of Computer Science Cornell University sk3277@cornell.edu ", "page_idx": 0}, {"type": "text", "text": "Dheeraj Nagaraj Google DeepMind dheerajnagaraj@google.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Langevin Dynamics is a Stochastic Differential Equation (SDE) central to sampling and generative modeling and is implemented via time discretization. Langevin Monte Carlo (LMC), based on the Euler-Maruyama discretization, is the simplest and most studied algorithm. LMC can suffer from slow convergence - requiring a large number of steps of small step-size to obtain good quality samples. This becomes stark in the case of diffusion models where a large number of steps gives the best samples, but the quality degrades rapidly with smaller number of steps. Randomized Midpoint Method has been recently proposed as a better discretization of Langevin dynamics for sampling from strongly log-concave distributions. However, important applications such as diffusion models involve non-log concave densities and contain time varying drift. We propose its variant, the Poisson Midpoint Method, which approximates a small step-size LMC with large step-sizes. We prove that this can obtain a quadratic speed up of LMC under very weak assumptions. We apply our method to diffusion models for image generation and show that it maintains the quality of DDPM with 10o0 neural network calls with just 50-80 neural network calls and outperforms ODE based methods with similar compute. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The task of sampling from a target distribution is central to Bayesian inference, generative modeling, differential privacy and theoretical computer science [48, 20, 17, 25]. Sampling algorithms, based on the discretization of a stochastic differential equation (SDE) called the Langevin Dynamics, are widely used. The straightforward time discretization (i.e., Euler Maruyama discretization) of Langevin dynamics, called Langevin Monte Carlo (LMC), is popular due to its simplicity. The convergence properties of LMC have been studied extensively in the literature under various conditions on the target distribution [8, 12, 11, 45, 15, 33, 7, 1, 6, 9, 16, 30, 53, 5]. LMC can suffer from slow convergence to the target distribution, and often requires a large number of steps with a very fine time discretization (i.e., small step-size), making it prohibitively expensive. ", "page_idx": 0}, {"type": "text", "text": "The Poisson Midpoint Method introduced in this paper approximates multiple steps of small step-size Euler-Maruyama discretization with one step of larger step-size via stochastic approximation. In the case of LMC, we show that our method (called PLMC) converges to the target as fast as LMC with a much smaller step-size without any additional assumptions such as isoperimetry or strong log concavity (up to a small additional error term). This is a variant of the Randomized Midpoint Method (RLMC) studied in the literature [40, 18, 50] (see Section 1.1 for comparison). ", "page_idx": 0}, {"type": "text", "text": "Diffusion models are state-of-the-art in generating new samples of images and videos given samples [20, 43, 34]. These start with a Gaussian noise vector and evolve it through the time-reversal of the SDE called the Ornstein-Uhlenbeck process. The time reversed process can be written as an SDE (Langevin Dynamics with a time dependent drift) or as an ODE (see Section 2). The DDPM scheduler [20], which discretizes the SDE, obtains the best quality images with a small step-size and a large number of steps (usually 1000 steps). However, its quality degrades with larger step-sizes and a small number of steps (say 100 steps). Schedulers such as DDIM ([42]), DPM-Solver ([28, 29]), PNDM ([27]) which solve the ODE via numerical methods perform much better than DDPM with a small number of steps. However, it is noted that they do not match the performance of DDPM with 1000 steps over many datasets ([42, 28, 41]). Poisson Midpoint Method gives a scheduler for the time-reversed SDE which maintains the quality of DDPM with 1000 steps, with just 50-80 steps. ", "page_idx": 1}, {"type": "text", "text": "1.1 Prior Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Euler Maruyama discretization of SDEs is known to be inefficient and many powerful numerical integration techniques have been studied extensively ( [23, 32, 3, 26]). However, higher order methods such as the Runge-Kutta method require the existence and boundedness of higher order derivatives of the drift. The Randomized Midpoint Method for LMC (RLMC) was introduced for strongly logconcave sampling [40] and was further explored in [50, 18]. It was shown that RLMC, under certain conditions, can sample with a larger step size for fewer steps compared to Euler Maruyama and yet obtain the same accuracy. RLMC is popular due to its simplicity, and ease of implementation and does not require higher order bounded derivatives of the drift function. However, the current theoretical results are restricted to the case of strongly log-concave sampling, whereas non-log-concave sampling is of immense practical interest. ", "page_idx": 1}, {"type": "text", "text": "1.2 Our Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "(1) We design the Poisson Midpoint Method which discretizes SDEs by approximating $K$ -stepsof Euler-Maruyama discretization with step-size $\\frac{\\alpha}{K}$ by just one step with step-size $\\alpha$ We show a strong error bound between these two processes under general conditions in Theorem 1 (no assumption on mixing, smoothness etc). This is based on a Central Limit Theorem (CLT) based method in [10] to analyze stochastic approximations of LMC. ", "page_idx": 1}, {"type": "text", "text": "(2) We apply our method to LMC to obtain PLMC. We show that it achieves a speed-up in sampling for both Overdamped LMC (OLMC) and Underdamped LMC (ULMC) whenever LMC mixes, without additional assumptions such as isoperimetry or strong-log concavity. ", "page_idx": 1}, {"type": "text", "text": "(3) When the target obeys the Logarithmic Sobolev Inequalities (LSI), we show that PLMC achieves a quadratic speed up for both OLMC and ULMC. Prior works on midpoint methods [40, 18, 50] only considered strongly log-concave distributions. We also show an improvement in computational complexityfor ULMCfrom $\\frac{1}{\\epsilon^{2/3}}$ $\\textstyle{\\frac{1}{\\sqrt{\\epsilon}}}$ to achieve $\\epsilon\\;\\mathrm{error}^{2}$ ", "page_idx": 1}, {"type": "text", "text": "(4) Empirically, we show that our technique can match the quality of DDPM Scheduler with 1000 steps with fewer steps, achieving up to ${4\\bf{x}}$ gains in compute. Over multiple datasets, our method outperforms ODE based schedulers such as DPM-Solver and DDIM in terms of quality. ", "page_idx": 1}, {"type": "text", "text": "1.3 Notation: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "$X_{0:T}$ denotes $(X_{t})_{0\\leq t\\leq T}$ and $X_{K(0:T)}$ denotes $(X_{t K})_{0\\leq t\\leq T}$ . I denotes identity matrix over $\\mathbb{R}^{k\\times k}$ whenever $k$ is clear from context. For any vector $\\mathbf{x}\\in\\mathbb{R}^{k}$ \uff0c $\\|\\mathbf{x}\\|$ denotes its Euclidean norm. For any $a,b\\in\\mathbb{Z}$ and $a>b$ we take the $\\textstyle\\sum_{t=a}^{b}$ to be 0, and the product $\\textstyle\\prod_{t=a}^{b}$ to be 1. Underdamped Langevin Dynamics happens in $\\mathbb{R}^{2d}$ . Here, we take vectors named $X$ (along with subscripts and superscripts) as $\\boldsymbol{X}=[U^{^{\\ast}}\\ V]^{\\intercal}$ where $U,V\\in\\mathbb{R}^{d}$ also carry the same subscripts and superscripts (e.g: ${\\tilde{X}}_{4}$ corresponds to $\\tilde{U}_{4},\\tilde{V}_{4})$ . In this case I represents identity matrix in $\\mathbb{R}^{2d\\times2d}$ and $\\mathbf{I}_{d}$ denotes the identity matrx in $\\mathbb{R}^{d\\times d}$ . For any random variable $X$ , we let $\\mathsf{L a w}(X)$ denote its probability measure. By $\\mathsf{T V}(\\mu,\\nu)$ and KL $\\left(\\mu||\\nu\\right)$ we denote the total variation distance and $\\mathrm{KL}$ divergence (respectively) between two probability measure $\\mu,\\nu.~~{\\cal O},\\Omega,\\Theta$ are standard Kolmogorov complexity notations whereas $\\Tilde{O},\\Tilde{\\Omega},\\Tilde{\\Theta}$ are same as $O,\\Omega,\\Theta$ up to poly-logarithmic factors in the problem parameters such $\\textstyle{\\frac{1}{\\epsilon}},{\\frac{1}{\\alpha}},K,T,d$ ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Problem Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given a random vector $X_{0}\\in\\mathbb{R}^{d}$ , consider an iterative, disrete time process $(X_{t})_{t\\in\\mathbb{N}\\cup\\{0\\}}$ over $\\mathbb{R}^{d}$ with step-size $\\alpha>0$ given by: ", "page_idx": 2}, {"type": "equation", "text": "$$\nX_{t+1}=A_{\\alpha}X_{t}+G_{\\alpha}b(X_{t},t\\alpha)+\\Gamma_{\\alpha}Z_{t}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Where $A_{\\alpha},G_{\\alpha},\\Gamma_{\\alpha}$ are $d\\times d$ matrix valued functionsof the step-size $\\alpha$ and $(Z_{t})_{t\\geq0}\\overset{\\mathrm{i.i.d.}}{\\sim}\\mathcal{N}(0,\\mathbf{I}_{d})$ $b:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ is the drift. Call this process ${\\mathsf{S}}(A,G,\\Gamma,b,\\alpha)$ We consider Overdamped Langevin Monte Carlo (OLMC), Underdamped Langevin Monte Carlo (ULMC) and DDPMs as key examples. ", "page_idx": 2}, {"type": "text", "text": "Overdamped Langevin Monte Carlo: Consider Overdamped Langevin Dynamics for some $F:\\mathbb{R}^{d}\\rightarrow\\mathbf{\\bar{R}}$ ", "text_level": 1, "page_idx": 2}, {"type": "equation", "text": "$$\nd\\bar{X}_{\\tau}=-\\nabla F(\\bar{X}_{\\tau})d\\tau+\\sqrt{2}d B_{\\tau}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here $B_{\\tau}$ is the standard Brownian motion in $\\mathbb{R}^{d}$ . Under mild conditions on $F$ and $\\bar{X}_{0}$ \uff0c $\\mathsf{L a w}(\\bar{X}_{\\tau})\\stackrel{\\tau\\rightarrow\\infty}{\\rightarrow}$ $\\pi^{*}$ where $\\pi^{\\star}(X)\\propto\\exp(-F(X))$ is the stationary distribution. ", "page_idx": 2}, {"type": "text", "text": "Picking $A_{\\alpha}=\\mathbf{I}$ $G_{\\alpha}=\\alpha\\mathbf{I}$ $\\Gamma_{\\alpha}=\\sqrt{2\\alpha}\\mathbf{I}$ and $b(\\mathbf{x},t\\alpha)=-\\nabla F(\\mathbf{x})$ in Equation (1) gives us EulerMaruyama discretization of Overdamped Langevin Dynamics: $X_{t}$ in (1) approximates $\\bar{X}_{\\alpha t}$ [38, 35]. OLMC is the canonical algorithm for sampling and has been studied under assumptions such as log-concavity of $\\pi^{\\star}$ [8, 12, 11] or that $\\pi^{\\star}$ satisfies isoperimetric inequalities [45, 15, 33, 7, 1]. ", "page_idx": 2}, {"type": "text", "text": "Underdamped Langevin Monte Carlo occurs in $2d$ dimensions. We write $X_{t}=[U_{t}\\quad V_{t}]^{\\mathsf{T}}\\in\\mathbb{R}^{2d}$ where $U_{t}\\in\\mathbb{R}^{d}$ is the position and $V_{t}\\in\\mathbb{R}^{d}$ is the velocity. Fix a damping factor $\\gamma>0$ We take: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{{4}_{h}:=\\left[\\!\\!\\begin{array}{c c}{{\\bf I}_{d}}&{\\frac{1}{\\gamma}(1-e^{-\\gamma h}){\\bf I}_{d}}\\\\ {0}&{e^{-\\gamma h}{\\bf I}_{d}}\\end{array}\\!\\!\\right],}&{G_{h}:=\\left[\\!\\!\\begin{array}{c c}{\\frac{1}{\\gamma}(h-\\frac{1}{\\gamma}(1-e^{-\\gamma h})){\\bf I}_{d}}&{0}\\\\ {\\frac{1}{\\gamma}(1-e^{-\\gamma h}){\\bf I}_{d}}&{0}\\end{array}\\!\\!\\right]}&{b(X_{t},t\\alpha):=\\left[\\!\\!\\begin{array}{c c}{-\\nabla F(U_{t})}\\\\ {0}\\end{array}\\!\\!\\right]}\\\\ {{\\bf I}_{h}^{2}:=\\left[\\!\\!\\begin{array}{c c}{\\frac{2}{\\gamma}\\left(h-\\frac{2}{\\gamma}(1-e^{-\\gamma h})+\\frac{1}{2\\gamma}(1-e^{-2\\gamma h})\\right){\\bf I}_{d}}&{\\frac{1}{\\gamma}(1-2e^{-\\gamma h}+e^{-2\\gamma h}){\\bf I}_{d}}\\\\ {\\frac{1}{\\gamma}(1-2e^{-\\gamma h}+e^{-2\\gamma h}){\\bf I}_{d}}&{(1-e^{-2\\gamma h}){\\bf I}_{d}}\\end{array}\\!\\!\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Thischoiceof $A_{h},\\Gamma_{h},G_{h},b(,)$ in Equation (1) gives the Euler-Maruyama discretization of Underdamped Langevin Dynamics (a.k.a. the Kinetic Langevin Dynamics) studied extensively in Physics[14]): ", "page_idx": 2}, {"type": "equation", "text": "$$\nd\\bar{U}_{\\tau}=\\bar{V}_{\\tau}d\\tau;\\qquad d\\bar{V}_{\\tau}=-\\gamma\\bar{V}_{\\tau}-\\nabla F(\\bar{U}_{\\tau})+\\sqrt{2\\gamma}d B_{\\tau}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The stationary distribution of the SDE is given by $\\begin{array}{r}{\\pi^{\\star}(U,V)\\propto\\exp(-F(U)\\!-\\!\\frac{\\|V\\|^{2}}{2})}\\end{array}$ [13, 9].ULMC is popular in the literature and has been analyzed in the strongly log-concave setting [6, 9, 16] and under isoperimetry conditions [30, 53]. We refer to [53] for a complete literature review. ", "page_idx": 2}, {"type": "text", "text": "Denoising Diffusion Models:  In this case the stochastic differential equation is given by: ", "page_idx": 2}, {"type": "equation", "text": "$$\nd\\bar{X}_{\\tau}=(\\bar{X}_{\\tau}+2\\nabla\\log p_{\\tau}(\\bar{X}_{\\tau}))d\\tau+\\sqrt{2}d B_{\\tau}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This also admits an equivalent characteristic ODE given below: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{d\\bar{X}_{\\tau}}{d\\tau}=\\bar{X}_{\\tau}+\\nabla\\log p_{\\tau}(\\bar{X}_{\\tau})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "See [43, 5, 4] for further details. Here $p_{\\tau}$ is the probability density of $e^{-t}X^{*}+{\\sqrt{1-e^{-2t}}}Z$ where $X^{*}$ is drawn from the target and $Z$ is drawn from $\\mathcal{N}(0,\\mathbf{I})$ independently. The drift $\\nabla\\log{p_{\\tau}}$ is learned via neural networks for discrete time instants $\\tau_{0},\\ldots,\\tau_{n-1}$ (usually $n=1000)$ . In practice, the iterations are written in the form 3: $X_{t+1}=a_{t}X_{t}+b_{t}\\nabla\\log p_{\\tau_{t}}(X_{t})+\\sigma_{t}Z_{t}$ where $a_{t},b_{t},\\sigma_{t}$ are chosen for best performance. Aside from the original choice in [20], many others have been proposed ([2, 42]). Since $A_{\\alpha},G_{\\alpha},\\Gamma_{\\alpha}$ in Equation (1) are time independent, we provide a variant of the Poisson Midpoint Method to suit DDPMs in Section A.1, along with a few other optimizations. ", "page_idx": 2}, {"type": "text", "text": "2.1 Technical Notes ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Scaling Relations: We impose the following scaling relations on the matrices $A_{h},G_{h},\\Gamma_{h}$ for every $h\\in\\mathbb{R}^{+}$ \uff0c $n\\in\\mathbb N$ , which are satisfied by both OLMC and ULMC. Whenever $b(\\cdot)$ is a constant function, these ensure that $K$ steps of Equation (1) with step-size $\\alpha/K$ is the same as 1 step with step-size $\\alpha$ in distribution. ", "page_idx": 3}, {"type": "equation", "text": "$$\n(A_{h})^{n}=A_{h n};\\quad(\\sum_{i=0}^{n-1}(A_{h})^{i})G_{h}=G_{h n};\\quad\\sum_{i=0}^{n-1}(A_{h})^{i}\\Gamma_{h}^{2}(A_{h}^{\\top})^{i}=\\Gamma_{h n}^{2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Randomized Midpoint Method and Stochastic Approximation We first illustrate the Randomized Midpoint Method [40, 18, 50] by applying it to OLMC (to obtain RLMC) to motivate our method (the Poisson Midpoint Method) and explain why we expect a quadratic speed up shown in Section 3. Overdamped Langevin Dynamics (2) satisfies: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\bar{X}_{(t+1)\\alpha}=\\bar{X}_{t\\alpha}-\\int_{t\\alpha}^{(t+1)\\alpha}\\nabla F(\\bar{X}_{s})d s+\\sqrt{2}(B_{(t+1)\\alpha}-B_{t\\alpha})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Taking $X_{t}$ as the approximation to $\\bar{X}_{t\\alpha}$ , LMC approximates the integral $\\begin{array}{r}{\\int_{t\\alpha}^{(t+1)\\alpha}\\nabla F(\\bar{X}_{s})d s}\\end{array}$ with $\\alpha\\nabla F(X_{t})$ , giving a \u201cbiased\u2019 estimator to the integral (conditioned on $X_{t}=\\bar{X}_{t\\alpha},$ . This gives the LMC updates $X_{t+1}=X_{t}-\\alpha\\nabla F(X_{t})+\\sqrt{2\\alpha}Z_{t}$ $Z_{t}\\sim\\mathcal{N}(0,\\mathbf{I})$ . RLMC chooses a uniformly random point in the interval $[t\\alpha,(t+1)\\alpha]$ instead of initial point $t\\alpha$ as described below: ", "page_idx": 3}, {"type": "text", "text": "Let $u\\sim\\mathsf{U n i f}([0,1]),Z_{t,1},Z_{t,2}\\sim\\mathcal{N}(0,\\mathbf{I})$ be independent and define the midpoint $X_{t+u}:=X_{t}-$ $\\boldsymbol{u}\\alpha\\nabla F(\\boldsymbol{X}_{t})+\\sqrt{2\\boldsymbol{u}\\alpha}Z_{t,1}$ (notice $X_{t+u}$ is an aproximation for $\\bar{X}_{(t+u)\\alpha})$ .The RLMC update is: ", "page_idx": 3}, {"type": "equation", "text": "$$\nX_{t+1}=X_{t}-\\alpha\\nabla F(X_{t+u})+\\sqrt{2u\\alpha}Z_{t,1}+\\sqrt{2(1-u)\\alpha}Z_{t,2}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Notice that $\\sqrt{2u\\alpha}Z_{t,1}+\\sqrt{2(1-u)\\alpha}Z_{t,2}|u,X_{t}\\sim{\\mathcal N}(0,\\mathbf I)$ and $\\mathbb{E}[\\alpha\\nabla F(X_{t+u})|X_{t},Z_{t,1},Z_{t,2}]=$ $\\begin{array}{r}{\\alpha\\int_{0}^{1}F(X_{t+s})d s}\\end{array}$ which is a better approximation of the integral than $\\alpha\\nabla F(X_{t})$ .Therefore RLMC provides a nearly unbiased approximation to the updates in Equation (6). ", "page_idx": 3}, {"type": "text", "text": "Intuitively, we expect that reducing the bias leads to a quadratic speed-up. Let $Z,Z^{\\prime}\\sim\\mathcal{N}(0,1)$ and independent. For $\\epsilon$ small enough it is easy to show that, $\\mathsf{K L}\\left(\\mathsf{L a w}(Z+\\epsilon)\\big|\\big|\\mathsf{L a w}(Z)\\right)=\\dot{\\Theta}(\\epsilon^{2})$ whereas $\\mathsf{K L}\\left(\\mathsf{L a w}(Z+\\epsilon Z^{\\prime})\\big|\\big|\\mathsf{L a w}(Z)\\right)=\\Theta(\\epsilon^{4})$ . We hypothesize that $Z_{t}+$ error in integral is closer to $\\mathcal{N}(0,\\mathbf{I})$ when the error term has a small mean and a large variance (as in RLMC) than when it has a large mean but O variance (as in LMC). However, rigorous analysis of RLMC has only been done under assumptions like strong log-concavity of the target distribution. This is due to the fact that $Z_{t}$ is dependent on the error in the integral, disallowing the argument above. ", "page_idx": 3}, {"type": "text", "text": "$\\{0,\\frac{1}{K},\\,.\\,.\\,,\\frac{\\check{K}-1}{K}\\}$ $[0,1]$ $\\frac{1}{K}$ Thus, our method is a variant of RLMC which is amenable to more general mathematical analysis. The OPTION 2 of our method (see below) makes this connection clearer. PLMC is naturally suited to DDPMs since the drift function is trained only for a discrete number of timesteps (see Section 2). ", "page_idx": 3}, {"type": "text", "text": "2.2  The Poisson Midpoint Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We introduce the Poisson Midpoint Method (PLMC) which approximates $K$ steps of $\\textstyle\\mathsf{S}(A,G,\\Gamma,b,\\frac{\\alpha}{\\kappa})$ with step-size $\\frac{\\alpha}{K}$ with one step of which has a step-size $\\alpha$ . We denote this by $\\mathsf{P S}(A,G,\\Gamma,b,\\alpha,\\ddot{K})$ and let its iterates be denoted by $(\\tilde{X}_{t K})_{t\\geq0}$ or $(X_{t}^{\\mathsf{P}})_{t\\ge0}$ . Suppose $H_{t,i}\\;\\in\\;\\{0,1\\}$ be any binary sequence and $Z_{t K+i}$ be a sequence of i.i.d. $\\mathcal{N}(0,\\mathbf{I})$ for $t,i\\in\\mathbb{N}\\cup\\{0\\},0\\le i\\le K-1$ Given $\\tilde{X}_{t K}$ \uff0c we define the interpolation: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\tilde{X}}_{t K+i}:=A_{\\frac{\\alpha i}{K}}\\tilde{X}_{t K}+G_{\\frac{\\alpha i}{K}}b(\\tilde{X}_{t K},t\\alpha)+\\sum_{j=0}^{i-1}A_{\\frac{\\alpha(i-j-1)}{K}}\\Gamma_{\\frac{\\alpha}{K}}Z_{t K+j}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that this interpolation is cheap since every one of $\\hat{\\tilde{X}}_{t K+i}$ can be computed with just one evaluation of the function $b()$ . We then define the refined iterates for $0\\leq i\\leq K^{\\underline{{\\star}}}\\!-\\!1$ for a given $t$ as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{X}_{t K+i+1}=A_{\\frac{\\alpha}{K}}\\tilde{X}_{t K+i}+G_{\\frac{\\alpha}{K}}\\left[b(\\tilde{X}_{t K},t\\alpha)+K H_{t,i}(b(\\tilde{X}_{t K+i},\\frac{(t K+i)\\alpha}{K})-b(\\tilde{X}_{t K},t\\alpha))\\right]+\\Gamma_{\\frac{\\alpha}{K}}\\,Z_{t K+i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Wepick $H_{t,i}$ based on the following two options, independent of $Z_{t K+i},\\tilde{X}_{0}$ ", "page_idx": 4}, {"type": "text", "text": "OPTION 1: $H_{t,i}$ are i.i.d. $\\mathsf{B e r}\\big(\\frac{1}{K}\\big)$ ", "page_idx": 4}, {"type": "text", "text": "OPTION 2: Let $u_{t}\\sim\\mathsf{U n i f}(\\{0,\\dotsc,K-1\\})$ i.i.d. and $H_{t,i}:=\\mathbb{1}(u_{t}=i)$ ", "page_idx": 4}, {"type": "text", "text": "Remark 1. We call our method Poisson Midpoint method since in OPTIoN1 the set of midpoints $\\begin{array}{r}{\\{\\alpha t+\\frac{\\alpha i}{K}:H_{t,i}=1\\}}\\end{array}$ convergesto aPoissonprocess over $[\\alpha t,\\alpha(t+1)]$ as $K\\rightarrow\\infty$ ", "page_idx": 4}, {"type": "text", "text": "The Algorithm and Computational ComplexityThe algorithm $\\mathsf{P S}(A,G,\\Gamma,b,\\alpha,K)$ computes $\\tilde{X}_{K(t+1)}$ given $\\tilde{X}_{K t}$ in one step by unrolling the recursion given in Equation (8). For the sake of clarity, we will relabel $\\tilde{X}_{t K}$ to be $X_{t}^{\\mathsf{P}}$ to stress the fact that it is the $t$ -th iteration of $\\mathsf{P S}(A,G,\\Gamma,b,\\alpha,K)$ ", "page_idx": 4}, {"type": "text", "text": "Step 1: Generate $I_{t}\\ =\\ \\{i_{1},\\ldots,i_{N}\\}\\ \\subseteq\\ \\{0,\\ldots,K\\,-\\,1\\}$ such that $H_{t,i}\\;=\\;1$ iff $i\\in\\ I_{t}$ . Let $i_{1}<i_{2}\\cdot\\cdot\\cdot<i_{N}$ hold. When $N=0$ , we take this to be the empty set. ", "page_idx": 4}, {"type": "text", "text": "Step 2: Let $M_{0}~:=~0$ and let $W_{t,k}$ be a sequence of i.i.d. $\\dot{\\mathcal{N}}(0,\\mathbf{I})$ random vectors. For $k\\,=$ $1,\\bar{\\dots},N,N+1$ , we take: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{M_{k}=A_{\\frac{\\alpha(i_{k}-i_{k-1})}{K}}M_{k-1}+\\Gamma_{\\frac{\\alpha(i_{k}-i_{k-1})}{K}}W_{t,k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We use the convention that $i_{0}=0,i_{N+1}=K-1,A_{0}=\\mathbf{I}$ and $\\Gamma_{0}=0$ ", "page_idx": 4}, {"type": "text", "text": "Step 3: For $k=1,\\ldots,N$ compute $\\begin{array}{r}{\\mathring{X}_{t K+i_{k}}:=A_{\\frac{\\alpha i_{k}}{K}}X_{t}^{\\mathsf{P}}+G_{\\frac{\\alpha i_{k}}{K}}b(X_{t}^{\\mathsf{P}},\\alpha t)+M_{k}}\\end{array}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{Step}\\,4\\!\\colon\\!\\mathrm{Corr}:=K\\sum_{k=1}^{N}G_{\\frac{(K-1-i_{k})\\alpha}{K}}\\big(b\\big(\\hat{X}_{t K+i},\\frac{(t K+i)\\alpha}{K}\\big)-b\\big(X_{t}^{\\mathrm{P}},\\alpha t\\big)\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Step5: $X_{t+1}^{\\mathsf{P}}=A_{\\alpha}X_{t}^{\\mathsf{P}}\\!+\\!G_{\\alpha}b(X_{t}^{\\mathsf{P}},\\alpha t)\\!+\\!M_{N+1}\\!+\\!\\mathsf{C o r r}$ ", "page_idx": 4}, {"type": "text", "text": "That s, the algorithm frst generates the random mid-points $H_{t,i}$ , computes the interpolation $\\hat{\\tilde{X}}_{t K+i}$ only when $H_{t,i}=1$ and then computes $b(\\hat{\\tilde{X}}_{t K+i},\\frac{(t K+i)\\alpha}{K})$ (tK+i)a) for these points. These computations are then combined to compute $\\tilde{X}_{(t+1)K}$ . In most applications, it is computationally easy to generate Gaussian random vectors and perform vector operations such as summation. However, the evaluation of the drift function $b()$ is expensive. Therefore, in this work, we consider the number of evaluations of the drift function as the measure of computational complexity. The following proposition establishes that each iteration of PLMC requires 2 evaluations of $b()$ in expectation. ", "page_idx": 4}, {"type": "text", "text": "Proposition 1. When the scaling relations hold (Section 2.1), the trajectory $(X_{t}^{\\mathsf{P}})_{t\\ge0}$ in Equation (9) has the same joint distribution as the trajectory $(\\tilde{X}_{t K})_{t\\geq0}$ given in Equation (8). In expectation, one stepof $\\mathsf{P S}(A,G,\\Gamma,b,\\alpha,K)$ requires two evaluations of the function $b(\\cdot)$ ", "page_idx": 4}, {"type": "text", "text": "We call P $:\\mathsf{S}(A,G,\\Gamma,b,\\alpha,K)$ as PLMC whenever $\\textstyle\\mathsf{S}(A,G,\\Gamma,b,\\frac{\\alpha}{K})$ is either OLMC or ULMC. ", "page_idx": 4}, {"type": "text", "text": "3 Main Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Theorem 1 gives an upper bound for the KL divergence of the trajectory generated by $\\mathsf{P S}(A,G,\\Gamma,\\bar{b_{,}}\\alpha,K)$ to the one generated by $\\mathsf{S}(A,G,\\Gamma,b,\\bar{\\kappa})$ . We refer to Section C for its proof. We note that Theorem 1 does not make any mixing or smoothness assumptions on $b(\\cdot)$ and that it can handle time dependent drifts. We refer to Section 4 for a proof sketch and discussion. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Let $X_{t}$ be the iterates of $\\textstyle\\mathsf{S}(A,G,\\Gamma,b,\\frac{\\alpha}{K})$ and $X_{t}^{\\mathsf{P}}$ be the iterates of $\\mathsf{P S}(A,G,\\Gamma,b,\\alpha,K)$ withOPTION1.Supposethat $X_{0}^{\\mathsf{P}}=X_{0}$ . Let $\\tilde{X}_{t K+i}$ be the iterates in Equation (8).Define random variables: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{B_{t K+i}:=\\Gamma_{\\frac{\\alpha}{K}}^{-1}G_{\\frac{\\alpha}{K}}[b(\\hat{\\tilde{X}}_{t K+i},t\\alpha+\\frac{i\\alpha}{K})-b(\\tilde{X}_{t K+i},t\\alpha+\\frac{i\\alpha}{K})]}\\\\ {\\beta_{t K+i}:=\\|K\\Gamma_{\\frac{\\alpha}{K}}^{-1}G_{\\frac{\\alpha}{K}}[b(\\hat{\\tilde{X}}_{t K+i},t\\alpha+\\frac{\\alpha i}{K})-b(\\tilde{X}_{t K},t\\alpha)]\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then, for some universal constant $C$ and any $r>1$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{K L}\\left(\\mathsf{L a w}(X_{0:T}^{\\mathsf{P}})\\big|\\middle|\\mathsf{L a w}((X_{K t})_{0\\le t\\le T})\\right)}\\\\ &{\\le\\displaystyle\\sum_{s=0}^{T-1}\\sum_{i=0}^{K-1}\\mathbb{E}[\\|B_{s K+i}\\|^{2}]+C\\mathbb{E}\\left[\\frac{\\beta_{s K+i}^{4}}{K^{2}}+\\frac{\\beta_{s K+i}^{10}}{K^{3}}+\\frac{\\beta_{s K+i}^{6}}{K^{2}}+\\frac{\\beta_{s K+i}^{2r}}{K}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We now apply Theorem 1 to the case of OLMC and ULMC under additional assumptions, with the proofs inSections $\\boldsymbol{\\mathrm E}$ and $\\boldsymbol{\\mathrm F}$ respectively. ", "page_idx": 5}, {"type": "text", "text": "Assumption 1. $F:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ . $L$ smooth (i.e., $\\nabla F$ .s $L$ -Lipschitz). $\\mathbf{x}^{*}$ is its global minimizer ", "page_idx": 5}, {"type": "text", "text": "Assumption2. The initializtion $X_{0}$ is such that $\\mathbb{E}\\|X_{0}-\\mathbf{x}^{*}\\|^{14}<C_{\\mathsf{i n i t}}^{14}d^{7}$ ", "page_idx": 5}, {"type": "text", "text": "The assumptions above are very mild and standard in the literature. Specifically, Assumption 2 shows that the initialization is close to global optimum by $O({\\sqrt{d}})$ up to 14th moments. For instance, this is satisfied when the initialization is a standard Gaussian variable with mean $\\mu$ satisfying $\\|\\mu-\\mathbf{x}^{*}\\|=O({\\sqrt{d}})$ .Specifically this is true when $\\mu\\,=\\,0$ and $\\|\\mathbf{x}^{*}\\|=O(\\sqrt{d})$ . This is a weak assumption which is implied from common initialization assumptions in the literature as listed below. It can be replaced with the assumptions in [53, Appendix D and Lemma 27] which considers Gaussian initializations with the right variance and mean. The original randomized midpoint method work [40] considers initializing at $\\mathbf{x}^{*}$ whereas [45] considers a Gaussian initialization with the right variance at a local minimum of $F$ ", "page_idx": 5}, {"type": "text", "text": "We do not make any assumptions regarding isoperimetry of the target distribution $\\pi^{\\star}(\\mathbf{x})~\\propto~$ $\\exp(-F(\\mathbf{x}))$ ", "page_idx": 5}, {"type": "text", "text": "Theorem 2 (OLMC). Consider the setting of Theorem $^{\\,l}$ with OLMC under Assumptions $^{\\,l}$ and 2. There exists constants $c_{1},c_{2}>0$ such that whenever $\\alpha L<c_{1}$ and $\\alpha^{3}L^{3}T<c_{2}$ then: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{K L}\\left(\\mathsf{L a w}(X_{0:T}^{\\mathrm{P}})\\big|\\big|\\mathsf{L a w}(X_{K(0:T)})\\right)\\le C L^{4}\\alpha^{4}(\\mathbb{E}[F(X_{0})-F(\\mathbf{x}^{*})]+1)}\\\\ &{\\phantom{=\\;\\;}+O(C L^{4}\\alpha^{4}K d^{2}T)}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Remark 2. There are lower order terms hidden in the $O()$ notation.These are explicated in Equation (40) in the appendix. The next theorem gives a similar guarantee for ULMC and the lower ordertermsareexplicatedinEquation $60$ intheappendix. ", "page_idx": 5}, {"type": "text", "text": "Theorem3 (ULMC).Consider thesetting of Theorem $^{\\,l}$ withULMCunderAssumptions $^{\\,l}$ and 2. Suppose that $\\mathbf{x}^{*}$ istheglobalminimizerof $F$ .There exist constants $C_{1},c_{1},c_{2}$ such that whenever $\\gamma>C_{1}\\sqrt{L},\\,\\alpha\\gamma<c_{1}$ $\\begin{array}{r}{T<\\frac{c_{2}\\gamma}{L^{2}\\alpha^{3}}}\\end{array}$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{K L}\\left(\\mathsf{L a w}(X_{(0:T)}^{\\mathsf{P}})\\big|\\Big|\\mathsf{L a w}(X_{K(0:T)})\\right)\\le\\frac{C\\alpha^{6}L^{4}}{\\gamma^{2}}\\left[\\mathbb{E}F(U_{0}+\\frac{V_{0}}{\\gamma})-F(\\mathbf{x}^{*})+\\mathbb{E}\\|V_{0}\\|^{2}+1\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\;O(\\frac{K\\alpha^{7}L^{4}T^{2}}{\\gamma}(d+\\log K)^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "OLMC and ULMC are sampling algorithms which output approximate samples ( $X_{T}$ and $U_{T}$ respectively) from the distribution with density $\\pi^{\\star}\\propto\\,e^{{\\textstyle-F}}$ . Given $\\epsilon\\,>\\,0$ , prior works give upper bounds on $T$ and the corresponding step-size $\\alpha$ as a function of $\\epsilon$ to achieve guarantees such as $\\mathsf{K L}\\left(\\mathsf{L a w}(X_{T})\\middle|\\middle|\\pi^{\\star}\\right)\\leq\\epsilon^{2}$ or $\\mathsf{T V}(\\mathsf{L a w}(X_{T}),\\pi^{\\star})\\leq\\epsilon$ . By Pinsker's inequality ${\\mathsf{T}}{\\bar{\\mathsf{V}}}^{2}\\leq2{\\mathsf{K}}{\\mathsf{L}}$ therefore we guarantees for $\\mathsf{K L}\\leq\\epsilon^{2}$ to those for $\\mathsf{T V}\\leq\\epsilon$ as is common in the literature. ", "page_idx": 5}, {"type": "text", "text": "Quadratic Speedup _ Let $T=\\tilde{\\Theta}(1/\\alpha)$ as is standard. Choosing $K=\\Theta(1/\\alpha)$ , our method applied to OLMC achieves a KL divergence of $O(\\alpha^{2})$ to OLMC with step-size $\\alpha^{2}$ . Similarly, our method applied to ULMC achieves a KL divergence of $O(\\alpha^{4})$ to ULMC with step-size $\\alpha^{2}$ . Whenever the KL divergence of OLMC (resp. ULMC) output to $\\pi^{\\star}$ , with step-size $\\eta$ is $\\tilde{\\Omega}(\\eta)$ (resp $\\tilde{\\Omega}(\\eta^{2}))$ Theorem 2 (resp. Theorem 3) demonstrates a quadratic speed up. ", "page_idx": 5}, {"type": "text", "text": "To show the generality of our results, we combine Theorems 2 and 3 with convergence results for OLMC /ULMC in the literature ([45, 53]) when $\\pi^{\\star}$ satisfies the Logarithmic Sobolev Inequality with constant $\\lambda$ $\\lambda$ -LSI). We obtain convergence bounds for the last iterate of PLMC to $\\pi^{\\star}$ under the same conditions. $\\lambda$ -LSI is more general than strong log-concavity ( $\\lambda$ -strongly log-concave $\\pi^{\\star}$ satisfies $\\lambda$ -LSI). It is stable under bounded multiplicative perturbations of the density [21] and Lipschitz mappings. $\\lambda$ -LSI condition has been widely used to study sampling algorithms beyond log-concavity. We present our results in Table 1 and refer to Section $\\mathrm{G}$ for the exact conditions and results. ", "page_idx": 5}, {"type": "text", "text": "Table 1: Comparison of LMC and PLMC guarantees. LMC complexity is the upper bound on the number of drift $(b())$ evaluations to achieve the error guarantee in the referenced work. PLMC complexity is the corresponding upper bound for PLMC. PLMC obtains a quadratic improvement in $\\epsilon,$ and improved dependence on $\\textstyle{\\frac{L}{\\lambda}},d$ The bounds hold up to poly-log factors. ", "page_idx": 6}, {"type": "table", "img_path": "Ylvviju6MD/tmp/eb2515ba93aaa6a8557810cf5067992f2e917f493516bf11cf50d1e1dae15966.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4 Proof Sketch ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Sketch for Theorem 1  For the proof of Theorem 1, we follow the recipe given in [10] in order to analyze the stochastic approximations of LMC - where only an unbiased estimator for the drift function is known. The bias variance decomposition in Lemma 1, shows that the iterations of $\\mathsf{P S}(A,G,\\Gamma,\\alpha,K)$ can be written in the same form of as the iterations of $S(A,G,\\Gamma,{\\frac{\\alpha}{K}})$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{X}_{t K+i+1}=A_{\\frac{\\alpha}{K}}\\tilde{X}_{t K+i}+G_{\\frac{\\alpha}{K}}\\left[b(\\tilde{X}_{t K+i})\\right]+\\Gamma_{\\frac{\\alpha}{K}}\\tilde{Z}_{t K+i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Where $\\tilde{Z}_{t K+i}:=Z_{t K+i}+B_{t K+i}+S_{t K+i},$ $B_{t K+i}$ is the bias\u2019 with a non-zero conditional mean, and $S_{t K+i}$ is the variance with O conditional mean (conditioned on $\\tilde{X}_{t K+i})$ . They are independent of $Z_{t K+i}$ conditioned on $\\tilde{X}_{t K+i}$ . Note that the sequence $(\\tilde{Z}_{t K+i})_{t,i}$ is neither i.i.d. nor Gaussian. If it was a sequence of i.i.d. $\\mathcal{N}(0,\\mathbf{I})$ , then this is exactly same as $\\textstyle\\mathsf{S}(A,G,\\Gamma,\\frac{\\alpha}{K})$ ", "page_idx": 6}, {"type": "text", "text": "The main idea behind the proof of Theorem 1 is that due to data-processing inequality, it is sufficient to show that $(\\tilde{Z}_{t K+i})_{t,i}$ is close to a sequence of i.i.d. Gaussian random vectors in KL-divergence. The bias term can be shown to lead to an error bounded by $\\scriptstyle\\sum_{t,i}\\mathbb{E}\\|B_{t K+i}\\|^{2}$ , which roughly corresponds to the KL divergence between $\\mathcal{N}(B_{t K+i},\\mathbf{I})$ and $\\mathcal{N}(0,\\mathbf{I})$ . We then show that $Z_{t K+i}\\!+\\!S_{t K+i}\\vert\\tilde{Z}_{0:t K+i-1}$ is close in distribution to $\\mathcal{N}(0,\\mathbf{I})$ . In order to achieve this, we first modify the Wasserstein CLT established in [51] to show that $Z_{t K+i}+S_{t K+i}$ is close in distribution to $\\mathcal{N}(0,\\mathbf{I}+\\Sigma_{t,i})$ when conditioned on $\\tilde{Z}_{0:t K+i-1}$ where $\\Sigma_{t,i}$ is the conditional covariance of $S_{t K+i}$ . This CLT step gives us th eror o the form $\\begin{array}{r}{\\sum_{s=0}^{T-1}\\sum_{i=0}^{K-1}C\\mathbb{E}\\left[\\frac{\\beta_{s K+i}^{10}}{K^{3}}+\\frac{\\beta_{s K+i}^{6}}{K^{2}}+\\frac{\\beta_{s K+i}^{2r}}{K}\\right]}\\end{array}$ in Theorem 1. ", "page_idx": 6}, {"type": "text", "text": "We then use the standard formula for KL divergence between Gaussians to bound the distance between $\\mathcal{N}(0,\\mathbf{I}+\\Sigma_{t,i})$ to $\\mathcal{N}(0,\\mathbf{I})$ . This accounts for the fact that the Gaussian noise considered has a slightly higher variance than I. This leads to the leading term $C\\mathbb{E}\\left[\\frac{\\beta_{s K+i}^{4}}{K^{2}}\\right]$ ", "page_idx": 6}, {"type": "text", "text": "Sketch for Theorem 2   Applying Theorem 1 to OLMC , note that the term $\\beta_{t K_{i}}$ depends on how far the coarse estimate $\\hat{\\tilde{X}}_{t K+i}$ is from the true value $\\tilde{X}_{t K+i}$ . Indeed, under the smoothness assumption m $F$ we show that: $\\begin{array}{r}{\\beta_{t K+i}^{2p}\\le(\\frac{L^{2}\\alpha K}{2})^{p}\\operatorname*{sup}_{0\\le j\\le K-1}\\|\\hat{\\tilde{X}}_{t K+j}-\\tilde{X}_{t K}\\|^{2p}}\\end{array}$ Thus: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Vert\\beta_{t K+i}\\Vert^{2p}\\lesssim L^{2p}\\alpha^{3p}K^{p}\\mathbb{E}\\Vert\\nabla F(\\tilde{X}_{t K})\\Vert^{2p}+L^{2p}\\alpha^{2p}K^{p}d^{p}\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Therefore, the proof educes to bounding $\\begin{array}{r}{\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\nabla F(\\tilde{X}_{t K})\\|^{2p}}\\end{array}$ We observe that $\\tilde{X}_{(t+1)K}\\,=$ $\\tilde{X}_{t K}-\\alpha(\\nabla F(\\tilde{X}_{t K})+\\Delta_{t})+\\sqrt{2\\alpha}Z_{t}$ where $\\Delta_{t}$ is a small error term appearing due to Poisson Midpoint Method. Notice that this is approximately stochastic gradient descent on $F$ with a large noise $\\sqrt{2\\alpha}$ . Therefore, using the taylor approximation of $F$ , we can show that: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}F(\\tilde{X}_{(t+1)K})-\\mathbb{E}F(\\tilde{X}_{t K})\\lesssim-\\alpha\\|\\nabla F(\\tilde{X}_{t K})\\|^{2}+\\alpha d+o(\\alpha d)}\\\\ &{\\Longrightarrow\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\nabla F(\\tilde{X}_{t K})\\|^{2}\\lesssim\\frac{F(X_{0})-\\operatorname*{inf}_{\\mathbf{x}}F(\\mathbf{x})}{\\alpha}+L T d+o(L T d)}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The following sophisticated bound derived in this work is novel to the best of our knowledge: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\nabla F(\\tilde{X}_{t K})\\|^{2p}\\lesssim L^{p-1}\\mathbb{E}\\frac{(F(X_{0})-\\operatorname*{inf}_{\\mathbf{x}}F(\\mathbf{x}))^{p}}{\\alpha}+T L^{p}d^{p}(1+(\\alpha L T)^{p-1})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Sketch for Theorem 3  This is similar Theorem 2, but requires us to bound $\\mathbb{E}\\sum_{t}\\|\\tilde{V}_{t K}\\|^{2p}$ and $\\begin{array}{r}{\\mathbb{E}\\sum_{t}\\|\\nabla F(\\tilde{U}_{t K})\\|^{2p}}\\end{array}$ . We track the decay of two different entities across time: (1) $\\|\\tilde{V}_{t K}\\|^{2}$ and (2) $\\begin{array}{r}{F(\\tilde{U}_{t K}+\\frac{\\tilde{V}_{t K}}{\\gamma})}\\end{array}$ . Our proof shows via a similar taylor series based argument that PLMC does not allow either $\\|\\tilde{V}_{t K}\\|^{2}$ $\\begin{array}{r}{F(\\tilde{U}_{t K}+\\frac{\\tilde{V}_{t K}}{\\gamma})}\\end{array}$ to grow toolarge Leting $\\begin{array}{r}{\\Psi_{t}:=\\tilde{U}_{t K}+\\frac{\\tilde{V}_{t K}}{\\gamma}}\\end{array}$ we show (roughly: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\nabla F(\\tilde{U}_{t K})\\|^{2p}\\lesssim\\frac{\\gamma^{2p}}{\\gamma\\alpha}\\left[\\mathbb{E}\\|\\tilde{V}_{0}\\|^{2p}+\\mathbb{E}|F(\\Psi_{0})-F({\\mathbf x}^{*})|^{p}\\right]+T\\left[\\frac{\\gamma^{4p}}{L^{p}}+(\\gamma\\alpha T)^{p-1}\\gamma^{2p}\\right]d^{p}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{T-1}\\mathbb{E}\\Vert\\tilde{V}_{t K}\\Vert^{2p}\\lesssim\\frac{1}{\\gamma\\alpha}\\left[\\mathbb{E}\\Vert\\tilde{V}_{0}\\Vert^{2p}+\\mathbb{E}|(F(\\Psi_{0})-F(\\mathbf{x}^{*})|^{p}\\right]+T\\left[\\frac{\\gamma^{2p}}{L^{p}}+(\\gamma\\alpha T)^{p-1}\\right]d^{p}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now present experiments to evaluate Poisson Midpoint Method as a training-free scheduler for diffusion models. We consider the Latent Diffusion Model (LDM) [39] for CelebAHQ 256, LSUN Churches, LSUN Bedrooms and FFHQ datasets using the official (PyTorch) codebase and checkpoints. We compare the sample quality of the Poisson Midpoint Method against established methods such as DDPM, DDIM and DPM-Solver, varying the number of neural network calls (corresponding to the drift $b(\\mathbf{x},t))$ used to generate a single image. ", "page_idx": 7}, {"type": "text", "text": "To evaluate the quality, we generate $50\\mathbf{k}$ images for each method and number of neural network calls and compare it with the training dataset. We use Fr\u00e9chet Inception Distance (FID) [19] metric for LSUN Churches and LSUN Bedrooms. For CelebAHQ 256 and FFHQ, we use Clip-FID, a more suitable metric as it is known that FID may exhibit inconsistencies with human evaluations datasets outside of Imagenet [24]. We refer to Section A.3 in the appendix for further details. ", "page_idx": 7}, {"type": "text", "text": "We refer to the ODE based sampler with $\\eta=0$ (see [42]) setting as DDIM and use the implementation in [39]. We generate images for number of neural network calls ranging from 20 to 500. For DPMSolver, we port the official codebase of [28] to generate images for different numbers of neural network calls ranging from 10 to 100 using MultistepDPMSolver and tune the hyperparameter ^order' over $\\{2,3\\}$ and \u2018skip_type\u2019 over $\\{^{\\,\\cdot}\\log\\!{\\bf{S}}\\!\\!\\mathrm{N}\\!\\mathrm{R}^{\\,\\cdot}$ ,\u2032\u2018time_uniform\u2019, \u2018time_quadratic' $\\}$ for each instance to obtain the best possible FID score. This ensures that the baseline is competitive. ", "page_idx": 7}, {"type": "text", "text": "For the sake of clarity, we will call all SDE based methods, including DDIM with $\\eta\\,>\\,0$ (see [42]) as DDPM. The DDPM scheduler has many different proposals for coefficients $a_{t},b_{t},c_{t}$ (see Section 2,[42, 2]), apart from the original proposal in the work of [20] . Based on these proposals, we consider three different variants of DDPM in our experiments (see Section A.5 for exact details). This choice can have a significant impact on the performance (See Figure 1 in the Appendix) for a given number of denoising diffusion steps. For the Poisson Midpoint Method, we implement the algorithm shown in Section A.1 for number of diffusion steps ranging from 20 to 500, corresponding to 40 to 750 neural network calls (see Section A.2). This approximates $K$ steps of the 1000 step DDPM with a single step. For both Poisson Midpoint Method and DDPM, we plot the results from the best variant in Table 2 for a given number of neural network calls and refer to Section A.5 for the numbers of all variants. Poisson Midpoint Method incurs additional noise in each iteration due to the randomness introduced by $H_{i}$ . This can lead to a large error when $K$ is large. When $K$ is large, we reduce the variance of the Gaussian noise to compensate as suggested in the literature (see Covariance correction in [10] and [31, Equation 9]). We refer to Section A.5 for full details. ", "page_idx": 7}, {"type": "text", "text": "5.1 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We refer to the outcome of our empirical evaluations in Table 2. The first column compares the performance against DDPM. We see that for all the datasets considered, Poisson Midpoint Method can match the quality of the DDPM sampler with 1000 neural network calls with just 40-80 neural network calls. Observe that for CelebA, LSUN-Church and FFHQ datasets, the performance of DDPM degrades rapidly with lower number of steps, showing the advantage our method in this regime. However, a limitation of our work is that the quality of our method degrades rapidly at around 40-50 neural network calls. We believe this is because our stochastic approximation breaks down with larger step-sizes and further research is needed to mitigate this. ", "page_idx": 7}, {"type": "text", "text": "The second column compares the performance of our method against ODE based methods. It is known in the literature that DDPM with 1000 steps outperforms DDIM and DPM-Solver in terms of the quality for a large number of models and datasets [41, 42]. Thus, in terms of quality, Poisson midpoint method with just 50-80 neural network calls outperforms ODE based methods with a similar amount of compute. Note that we optimize the performance of DPM-Solver over 6 different variants as mentioned above to maintain a fair comparison. However, in the very low compute regime ( $_{\\sim10}$ steps), DPM-Solver remains the best choice. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We introduce the Poisson Midpoint Method, which efficiently discretizes Langevin Dynamics and theoretically demonstrates quadratic speed up over Euler-Maruyama discretization under general conditions. We apply our method to diffusion models for image generation, and show that our method maintains the quality of 1000 step DDPM with just 50-80 neural network calls. This outperforms ODE based methods such as DPM-Solver in terms of quality, with a similar amount of compute. Future work can explore variants of Poisson midpoint method with better performance when fewer than 50 neural network calls are used. An interesting theoretical direction would be to derive convergence bounds for algorithms such as DDPM which have a time dependent drift function. Future work can also consider convergence rates of PLMC under conditions such as the Poincare Inequality and whenever $\\nabla F$ is Holder continuous instead of Lipschitz continuous. ", "page_idx": 8}, {"type": "text", "text": "7 Societal Impact ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our work considers an efficient numerical discretization schemes for making diffusion model inference more efficient. Publicly available, pre-trained diffusion models are very impactful and have significant risk of abuse. In addition to theoretical guarantees, our work considers empirical experiments to evaluate the inference efficiency on publicly available, widely used diffusion models over curated datasets. We do not foresee any significant positive or negative social impact of our work. ", "page_idx": 8}, {"type": "text", "text": "Table 2: Empirical Results for the Latent Diffusion Model [39], comparing the Poisson midpoint method with various SDE and ODE based methods. ", "page_idx": 9}, {"type": "image", "img_path": "Ylvviju6MD/tmp/5b62634ea08963b8fc8ff8b21ac6e102011739a267ee5e7345f6cd3bbc033d50.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Prateek Jain for helpful comments during the course of this research work. This work was done when SK was a student researcher at Google Research. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Krishna Balasubramanian, Sinho Chewi, Murat A Erdogdu, Adil Salim, and Shunshi Zhang. Towards a theory of non-log-concave sampling:first-order stationarity guarantees for langevin monte carlo. In Po-Ling Loh and Maxim Raginsky, editors, Proceedings of Thirty Fifth Conference on Learning Theory, volume 178 of Proceedings of Machine Learning Research, pages 2896-2923. PMLR, 02-05 Jul 2022.   \n[2]  Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-dpm: an analytic estimate of the optimal reverse variance in diffusion probabilistic models. In International Conference on Learning Representations, 2022.   \n[3]  Kevin Burrage, PM Burrage, and Tianhai Tian. Numerical methods for strong solutions of stochastic differential equations: an overview. Proceedings of the Royal Society of London. Series A: Mathematical, Physical and Engineering Sciences, 460(2041):373-402, 2004.   \n[4]  Sitan Chen, Sinho Chewi, Holden Lee, Yuanzhi Li, Jianfeng Lu, and Adil Salim. The probability flow ode is provably fast. Advances in Neural Information Processing Systems, 36, 2024.   \n[5] Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. In International Conference on Learning Representations, 2023.   \n[6]  Xiang Cheng, Niladri S Chatterji, Peter L Bartlett, and Michael I Jordan. Underdamped langevin mcmc: A non-asymptotic analysis. In Conference on learning theory, pages 300-323. PMLR, 2018.   \n[7] Sinho Chewi, Murat A Erdogdu, Mufan Li, Ruoqi Shen, and Shunshi Zhang. Analysis of langevin monte carlo from poincare to log-sobolev. In Po-Ling Loh and Maxim Raginsky, editors, Proceedings of Thirty Fifth Conference on Learning Theory, volume 178 of Proceedings of Machine Learning Research, pages 1-2. PMLR, 02-05 Jul 2022.   \n[8]  Arnak S Dalalyan. Theoretical guarantees for approximate sampling from smooth and logconcave densities. Journal of the Royal Statistical Society Series B: Statistical Methodology, 79(3):651-676, 2017.   \n[9]  ARNAK S DALALYAN and LIONEL RIOU-DURAND. On sampling from a log-concave density using kinetic langevin diffusions. Bernoulli, 26(3):1956-1988, 2020.   \n[10]  Aniket Das, Dheeraj Nagaraj, and Anant Raj. Utilising the clt structure in stochastic gradient based sampling: Improved analysis and faster algorithms. arXiv e-prints, pages arXiv-2206, 2022.   \n[11] Alain Durmus, Szymon Majewski, and Blazej Miasojedow. Analysis of langevin monte carlo via convex optimization. The Journal of Machine Learning Research, 20(1):2666-2711, 2019.   \n[12]  Alain Durmus and Eric Moulines. Nonasymptotic convergence analysis for the unadjusted Langevin algorithm. The Annals of Applied Probability, 27(3):1551 - 1587, 2017.   \n[13]  Andreas Eberle, Arnaud Guillin, and Raphael Zimmer. Couplings and quantitative contraction rates for langevin dynamics. The Annals of Probability, 47(4): 1982-2010, 2019.   \n[14] Albert Einstein. Uber die von der molekularkinetischen theorie der warme geforderte bewegung von in ruhenden fuissigkeiten suspendierten teilchen. Annalen der physik, 4, 1905.   \n[15]  Murat A Erdogdu and Rasa Hosseinzadeh. On the convergence of langevin monte carlo: The interplay between tail growth and smoothness. In Conference on Learning Theory, pages 1776-1822.PMLR, 2021.   \n[16]  Arun Ganesh and Kunal Talwar. Faster differentially private samplers via renyi divergence analysis of discretized langevin mcmc. Advances in Neural Information Processing Systems, 33:7222-7233, 2020.   \n[17]  Sivakanth Gopi, Yin Tat Lee, and Daogao Liu. Private convex optimization via exponential mechanism. In Conference on Learning Theory, pages 1948-1989. PMLR, 2022.   \n[18]  Ye He, Krishnakumar Balasubramanian, and Murat A Erdogdu. On the ergodicity, bias and asymptotic normality of randomized midpoint sampling method. Advances in Neural Information Processing Systems, 33:7366-7376, 2020.   \n[19]  Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Proceedings ofthestIationalofrencnalIfatresinytm 6629-6640, Red Hook, NY, USA, 2017. Curran Associates Inc.   \n[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840-6851, 2020.   \n[21] Richard Holley and Daniel Stroock. Logarithmic sobolev inequalities and stochastic ising models. Journal of Statistical Physics, 46:1159-1194, 1987.   \n[22]  Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. CoRR, abs/1710.10196, 2017.   \n[23] Peter E Kloeden, Eckhard Platen, Peter E Kloeden, and Eckhard Platen. Stochastic differential equations. Springer, 1992.   \n[24] Tuomas Kynkaanniemi, Tero Karras, Mika Aittala, Timo Aila, and Jakko Lehtinen. The role of imagenet classes in frechet inception distance. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023.   \n[25]  Yin Tat Lee and Santosh S Vempala. Convergence rate of riemannian hamiltonian monte carlo and faster polytope volume computation. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1115-1121, 2018.   \n[26]  Xuechen Li, Yi Wu, Lester Mackey, and Murat A Erdogdu. Stochastic runge-kutta accelerates langevin monte carlo and beyond. Advances in neural information processing systems, 32, 2019.   \n[27] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. arXiv preprint arXiv:2202.09778, 2022.   \n[28] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:5775-5787, 2022.   \n[29] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpmsolver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022.   \n[30] Yi-An Ma, Niladri S Chatterji, Xiang Cheng, Nicolas Flammarion, Peter L Bartlett, and Michael I Jordan. Is there an analog of nesterov acceleration for gradient-based mcmc? Bernoulli, 27(3), 2021.   \n[31] Yi-An Ma, Tianqi Chen, and Emily Fox. A complete recipe for stochastic gradient mcmc. Advances in neural information processing systems, 28, 2015.   \n[32]  Grigori N Milstein and Michael V Tretyakov. Stochastic numerics for mathematical physics, volume 39. Springer, 2004.   \n[33] Wenlong Mou, Nicolas Flammarion, Martin J Wainwright, and Peter L Bartlett. Improved bounds for discretization of langevin diffusions: Near-optimal rates without convexity. Bernoulli, 28(3):1577-1601, 2022.   \n[34]  Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 8162-8171. PMLR, 2021.   \n[35]Bermt Oksendal. Stochastic differential equations: an introduction with applications. Springer Science & Business Media, 2013.   \n[36] Ingram Olkin and Friedrich Pukelsheim. The distance between two random vectors with given dispersion matrices. Linear Algebra and its Applications, 48:257-263, 1982.   \n[37] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On aliased resizing and surprising subtleties in gan evaluation. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11400-11410, 2022.   \n[38]  Gareth O Roberts and Richard L Tweedie. Exponential convergence of langevin distributions and their discrete approximations. Bernoulli, pages 341-363, 1996.   \n[39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10684-10695, June 2022.   \n[40] Ruoqi Shen and Yin Tat Lee. The randomized midpoint method for log-concave sampling. Advances in Neural Information Processing Systems, 32, 2019.   \n[41] Andy Shih, Suneel Belkhale, Stefano Ermon, Dorsa Sadigh, and Nima Anari Parall sampling of diffusion models. Advances in Neural Information Processing Systems, 36, 2024.   \n[42]  Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2020.   \n[43]  ang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   \n[44] Ramon Van Handel. Probability in high dimension. Technical report, PRINCETON UNIV NJ, 2014.   \n[45]  Santosh Vempala and Andre Wibisono. Rapid convergence of the unadjusted langevin algorithm: Isoperimetry suffices. Advances in neural information processing systems, 32, 2019.   \n[46] Santosh S. Vempala and Andre Wibisono. Rapid convergence of the unadjusted langevin algorithm: Isoperimetry suffices. In Advances in Neural Information Processing Systems 32: NeurIPS 2019, pages 8092-8104, 2019.   \n[47] Cedric Villani et al. Optimal transport: old and new, volume 338. Springer, 2009.   \n[48]  Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th international conference on machine learning (ICML-li), pages 681-688, 2011.   \n[49] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. LSUN: construction of a large-scale image dataset using deep learning with humans in the loop. CoRR, abs/1506.03365, 2015.   \n[50] Lu Yu, Avetik Karagulyan, and Arnak Dalalyan. Langevin monte carlo for strongly log-concave distributions: Randomized midpoint revisited. arXiv preprint arXiv:2306.08494, 2023.   \n[51]  Alex Zhai. A high-dimensional clt in w _2 w 2 distance with near optimal convergence rate. Probability Theory and Related Fields, 170:821-845, 2018.   \n[52]  Alex Zhai. A high-dimensional clt in w _2 w 2 distance with near optimal convergence rate. Probability Theory and Related Fields, 170:821-845, 2018.   \n[53] Shunshi Zhang, Sinho Chewi, Mufan Li, Krishna Balasubramanian, and Murat A Erdogdu. Improved discretization analysis for underdamped langevin monte carlo. In The Thirty Sixth Annual Conference on Learning Theory, pages 36-71. PMLR, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Details of Empirical Evaluations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Pseudocode ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The Algorithm  Let $a_{t}$ \uff0c $b_{t}$ and $\\sigma_{t}$ be the coefficients of the DDPM denoising step at time $t$ as per Section 2. Let $N$ denote the number of train steps. The main paper used notations and conventions based on the LMC literature. Here we follow different conventions and notations to connect with the diffusion model literature. ", "page_idx": 13}, {"type": "text", "text": "1. We take the dynamics to go backward in time (i.e., compute $X_{t-1}$ from $X_{t}$ 2. We use $\\boldsymbol{b}(\\boldsymbol{X}_{t},t)$ instead of $b(X_{t},\\alpha t)$ to denote the neural network estimate of $\\nabla\\log p_{\\tau_{N-t-1}}(X_{t})$ ", "page_idx": 13}, {"type": "text", "text": "DDPM scheduler samples $X_{N-1}\\sim\\mathcal{N}(0,\\mathbf{I})$ and iteratively computes $X_{0}$ as: ", "page_idx": 13}, {"type": "equation", "text": "$$\nX_{t-1}=a_{t}X_{t}+b_{t}b(X_{t},t)+\\sigma_{t}Z_{t};\\quad Z_{t}\\sim\\mathcal{N}(0,\\mathbf{I})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We take $N=1000$ as is standard in the literature. The Poisson Midpoint Method approximates $K$ steps of the iteration above, with a single step. That is, it obtains an approximation for $X_{t-K}$ directly from $X_{t}$ . The number of iterations deployed by Poisson Midpoint Method to sample $X_{0}$ is $N/K$ We compute the number of neural network calls required in Section A.2. The exact description of Poisson Midpoint Method is given below. ", "page_idx": 13}, {"type": "text", "text": "PoissonMidpointMethod $(\\tilde{X}_{t},t,K,b(,)$ , OPTION): ", "page_idx": 13}, {"type": "text", "text": "1. Let $(A_{k},B_{k},C_{k})\\leftarrow$ InterpolationConstants $(t,k)$ , for every $k\\in[K]$   \n2.If $\\mathrm{OPTION}=1$ Define $H_{i}\\sim\\mathsf{B e r}(1/K)$ i.i.d for every $i\\in[K-1]$ Define $p=K$ If $\\mathrm{{OPTION=2}}$ Define $H_{i}\\gets\\mathbb{1}(u=i)$ where $u\\sim\\mathsf{U n i f}(\\{1,\\ldots,K-1\\})$ i.i.d. Define $p=K-1$   \n3. For every $i\\in[K],Z_{i}\\sim\\mathcal{N}(0,\\mathbf{I})$   \n4. Perform the update: $\\begin{array}{r l r}{\\lefteqn{\\tilde{X}_{t-K}=A_{K}\\tilde{X}_{t}+\\left(\\displaystyle\\sum_{i=1}^{K}B_{K,i}\\right)b(\\tilde{X}_{t},t)+\\left(\\displaystyle\\sum_{i=1}^{K}C_{K,i}Z_{i}\\right)}}\\\\ &{}&{+\\displaystyle\\sum_{i=1}^{K}p H_{i}B_{K,i}\\big(b(\\widehat{X}_{t-i+1},t-i+1)-b(\\tilde{X}_{t},t)\\big)}\\end{array}$ where $\\widehat{X}_{t-\\tau}=A_{\\tau}\\tilde{X}_{t}+\\left(\\sum_{i=1}^{\\tau}B_{\\tau,i}\\right)b(\\tilde{X}_{t},t)+\\sum_{j=1}^{\\tau}C_{\\tau,K-j+1}Z[j],\\quad\\forall\\tau\\in[K-1].$ ", "page_idx": 13}, {"type": "text", "text": "5. Return $\\tilde{X}_{t-K}$ ", "page_idx": 13}, {"type": "text", "text": "InterpolationConstants $(t,k)$ ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "4. Return $(A_{k},B_{k},C_{k})$ ", "page_idx": 13}, {"type": "text", "text": "$\\{0,\\frac{1}{K},\\,.\\,.\\,,\\frac{K-1}{K}\\}$ $\\big\\{\\frac{1}{K},\\cdot\\cdot\\cdot,\\frac{K-1}{K}\\big\\}$ $\\hat{X}_{t}=\\tilde{X}_{t}$ the correction term $p H_{i}B_{K,i}\\big(b(\\widehat{X}_{t-i+1},t-i+1)-b(\\widehat{X}_{t}^{-},t)\\big)=0$ when $i=0$ ", "page_idx": 13}, {"type": "text", "text": "A.2Number of Neural Network Calls ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "While each step of Poisson Midpoint Method approximates $K$ steps of DDPM, it can use more than one neural network calls. Since neural networks calls are the most computationally expensive parts of diffusion models (4 orders of magnitude larger FLOPs compared to other computations), we compute the number of neural network calls for the Poisson Midpoint Method given $K,N$ forbothOPTION 1 and OPTION 2. ", "page_idx": 14}, {"type": "text", "text": "For OPTION 1, $b(\\tilde{X}_{t},t)$ is always computed. $b(\\hat{X}_{t-i+1},t-i+1)$ needs to be computed iff $H_{i}=1$ Itis easyto show that $\\begin{array}{r}{\\mathbb{E}|\\{i\\in[K-1]:H_{i}=1\\}|=\\frac{K-1}{K}}\\end{array}$ Thus, the expectd number of eural network calls per step is $2-{\\textstyle{\\frac{1}{K}}}$ .The total number number of neural network calls is $\\textstyle{\\frac{N}{K}}(2-{\\frac{1}{K}})$ ", "page_idx": 14}, {"type": "text", "text": "OPTION 2 always computes evaluates $b()$ twice per step. Therefore, the number of neural network callsis $\\frac{2N}{K}$ ", "page_idx": 14}, {"type": "text", "text": "A.3FID Evaluation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "FID scores between two image datasets can vary greatly depending on the image processing details which do not have much bearing on visual characteristics (such as png vs jpeg) [37]. Thus, we utilize the standardized Clean-FID codebase [37] to compute the scores for all the datasets. As is standard, we generate $50\\mathrm{k}$ images for all our evaluations. For CelebAHQ 256, we evaluate the CLIP-FID scores against the combined $30\\mathrm{k}$ training and validation samples of the CelebA $1024\\!\\!\\times\\!1024$ dataset [22], where the CLIP-FID is invoked by using the fags model. $n a m e=c l i p\\_{\\nu i t\\_b\\_32}$ and mode $=$ clean [37]. For FFHQ 256, we utilize the precomputed statistics of [37] on the 1024 resolution trainval70k split with the flags dataset_split $=$ trainfull and mode $=$ clean. As for LSUN Churches [49], we utilize the precomputed statistics of [37] with the flags dataset_split $=$ trainfull andmode $=.$ clean to calculate the FID. And for LSUN Bedrooms [49], we downloaded the training split of [39] which consists of 3,033,042 images and used the codebase of [37] to compute the FID between two folders. ", "page_idx": 14}, {"type": "text", "text": "A.4 Hardware Description and Execution Time ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For our experiments, we utilized a high-performance computational setup consisting of 8 NVIDIA A100-SXM4 GPUs, each with $40\\:\\mathrm{GB}$ of VRAM, an Intel Xeon CPU with 96 cores operating at 2.2 GHz, and $1.3\\;\\mathrm{TiB}$ of RAM. The experiments take about 16 hours to generate $50\\mathrm{k}$ images with 1000 neural network calls per image and batch size of 50. The time is proportionally lower when fewer neural network calls are used. ", "page_idx": 14}, {"type": "text", "text": "A.5 DDPM Variant Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We describe 3 different variants for DDPM, based on 3 different choices of coefficients $a_{t},b_{t},\\sigma_{t}$ used in Section A.1. Our observations indicate that each variant performs optimally for different datasets and for different ranges of the diffusion steps. For example, consider the DDPM variants of LSUN Churches in Figure 1b. Note that, Variant 3 outperforms Variant 2 for all steps less than or equal to 125 and vice versa for larger steps. Similar phenomenon can be observed for CelebAHQ 256 as shown in Figure 1a, Variant 2 outperforms the rest of the variants for steps smaller than 125, while Variant 1 achieves the best performance for larger steps. For FFHQ 256, similar phase transitions can be observed in Figure 1c at around 50-100 steps. For LSUN Bedrooms, we observed that Variant 2 outperforms 1 and 3 on all the regimes. Our experiments were carried out on all the variants and on all the steps, and the best scores are considered for comparison with Poisson Midpoint Method in Table 2. Let $\\alpha_{t},\\beta_{t},\\bar{\\alpha}_{t}$ be as defined in [20]. ", "page_idx": 14}, {"type": "text", "text": "Variant 1: The first variant uses the following closed form expression to perform the update. ", "page_idx": 14}, {"type": "equation", "text": "$$\nX_{t+1}={\\frac{1}{\\sqrt{\\alpha_{t}}}}X_{t}-2\\cdot{\\frac{(1-\\sqrt{\\alpha_{t}})}{\\sqrt{\\alpha_{t}}(1-{\\overline{{\\alpha_{t}}}})}}\\cdot b(X_{t},t)+\\sigma_{t}\\cdot Z_{t}\\quad{\\mathrm{~where~}}\\sigma_{t}^{2}={\\frac{\\beta_{t}}{1-\\beta_{t}}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Variant 2: The second variant of DDPM uses the default coefficients of DDIM as provided in the codebase of [39], with $\\eta=1$ and $\\begin{array}{r}{\\sigma_{t}^{2}=\\frac{(1-\\overline{{\\alpha}}_{t})\\cdot\\beta_{t+1}}{(1-\\overline{{\\alpha}}_{t+1})}}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "Variant 3: The third variant of DDPM also uses the default coefficients of DDIM with $\\eta=1$ ,but uses a modified lower variance $\\sigma_{t}^{2}=\\left(1-\\overline{{\\alpha}}_{t}\\right)\\cdot\\beta_{t+1}$ ", "page_idx": 14}, {"type": "image", "img_path": "Ylvviju6MD/tmp/9e9be63aba4aa8c98c9818f6d4a01c1c453768b20236eef562ae525d1a0b8294.jpg", "img_caption": ["(a) CelebAHQ 256 DDPM Variants "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "Ylvviju6MD/tmp/9198c56f5ce60c8866169665c5f61ed6cd1c6b2f15f8c03c95a7d880a655a277.jpg", "img_caption": ["(b) LSUN Churches DDPM Variants "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "Ylvviju6MD/tmp/3b40ecec5961efb2a7993103e6a749596d0a6f8ce87cb33755c126dbed24c181.jpg", "img_caption": ["Figure 1: Comparison of different variants of DDPM "], "img_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "Ylvviju6MD/tmp/9cefd2a710659b52e60179bb6f196adce62d4edbafff46bdc8632c34f237389d.jpg", "table_caption": ["Table 3: CelebAHQ 256 - Comparison of Poisson Midpoint Variants "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.6 Poisson Midpoint Variant Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Similar to DDPM, we perform our experiments for different choice of coefficients $a_{t},b_{t},\\sigma_{t}$ .Variants 2 and 3 of Poisson Midpoint are defined with the same coefficients of the corresponding DDPM variants 2 and 3. Poisson Midpoint Method introduces additional noise due to the randomness present in $H_{i}$ . This becomes very large whenever $K$ is large. Hence we reduce this randomness by reducing the variance of the Gaussian as suggested by [31, 10]. Specifically, for Variant 1, we consider four sub-variants la, 1b, 1c, 1d that respectively correspond to variances $\\begin{array}{r}{\\sigma_{t}^{2}=\\frac{\\beta_{t}}{1+i\\cdot\\beta_{t}}}\\end{array}$ \u03b2 for $i=\\{-1,0,1,2\\}$ , with the rest of the coefficients $a_{t},b_{t}$ defined as in (13). Note that the variance is inversely proportional to $i$ ", "page_idx": 15}, {"type": "image", "img_path": "", "img_caption": ["Poisson Number of Diffusion Steps / Number of Neural Network Calls Midpoint Variant "], "img_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "Ylvviju6MD/tmp/939e249f76d861cf8d72534bcecc32425ae7d468c31dfd03c54d253b236ab264.jpg", "table_caption": [], "table_footnote": ["Table 4: LSUN Churches - Comparison of Poisson Midpoint Variants "], "page_idx": 16}, {"type": "table", "img_path": "Ylvviju6MD/tmp/86d4f867eddec1026027bfe69542c0334fdec3277be17e904b1b6dc709f070a6.jpg", "table_caption": [], "table_footnote": ["Table 5: LSUN Bedrooms - Comparison of Poisson Midpoint Variants "], "page_idx": 16}, {"type": "text", "text": "We observe that lower variance leads to faster convergence when the number of diffusion steps is small. However, higher variances yield slightly better scores at higher steps, better quality at the expense of increased neural network evaluations. This phenomenon can be observed in Tables 3,4, 5 and 6 where the optimal score for each step-size/neural network calls is highlighted in bold. Therefore, we evaluate samples on all of the above mentioned variants and choose the best variant for every number of steps/neural network calls. We see that whenever $K$ is small (2-4), then OPTION 1 uses fewer neural network calls than OPTION 2. However, the without replacement sampling technique in OPTION 2 incurs lower variance error in the updates, allowing better convergence with lower number of steps (See Section A.2). Thus, we use OPTION 2 for smaller number of steps and OPTION 1 for larger number of steps. ", "page_idx": 16}, {"type": "text", "text": "B DPM-Solver Variant Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For unconditional image generation of high-resolution images, third order (order ${=}3$ )MultistepDPMSolvers with uniform time steps are recommended. To ensure a competitive benchmark, we evaluate the DPM-Solver on six different settings by tuning the parameters \u2018order' and \\*skip_type'. Our evaluations are shown in Table 7, where the best score for every setting, highlighted in bold, are chosen in our plots for comparison. We use the official code of [37] with parameters ^algorithm_type $=$ dpmsolver\u2019\u2018method $=$ multistep\u2019 and vary the parameters ^order\u2019 and \u2018skip_type\u2019 accordingly. ", "page_idx": 16}, {"type": "table", "img_path": "Ylvviju6MD/tmp/9da616963d1bf60785e31c35d05049b6bb3c81151ee5fa65b7aeaae85c5ffe5b.jpg", "table_caption": [], "table_footnote": ["Table 6: FFHQ 256 - Comparison of Poisson Midpoint Variants "], "page_idx": 17}, {"type": "text", "text": "C Proof of Theorem 1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The proof relies on the Wasserstein CLT based approach introduced in [10] to compare two discrete time stochastic processes. For the sake of clarity we consider the drift function $b(x,t)$ tobe $b(x)$ (i.e., time invariant). However, the proof goes through for time varying drifts as well. ", "page_idx": 17}, {"type": "text", "text": "C.1  The Bias-Variance Decomposition: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In the Lemma below, we will rewrite the update equations for $\\tilde{X}_{t K+i}$ given in Equation (8) in the same form as the update equations for $\\textstyle\\mathsf{S}(A,G,\\Gamma,\\frac{\\alpha}{K})$ given in Equation (1). The lemma follows by re-arranging the terms in Equation (8). ", "page_idx": 17}, {"type": "text", "text": "Lemma 1. We can write ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{X}_{t K+i+1}=A_{\\frac{\\alpha}{K}}\\tilde{X}_{t K+i}+G_{\\frac{\\alpha}{K}}\\left[b(\\tilde{X}_{t K+i})\\right]+\\Gamma_{\\frac{\\alpha}{K}}\\tilde{Z}_{t K+i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Where $\\tilde{Z}_{t K+i}:=Z_{t K+i}+B_{t K+i}+S_{t K+i}$ such that bias' $B_{t K+i}$ is defined as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{B_{t K+i}:=\\Gamma_{\\frac{\\alpha}{K}}^{-1}G_{\\frac{\\alpha}{K}}[b(\\hat{\\tilde{X}}_{t K+i})-b(\\tilde{X}_{t K+i})]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and the variance $S_{t K+i}$ is defined as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S_{t K+i}:=K(H_{t,i}-\\frac{1}{K})\\Gamma_{\\frac{\\alpha}{K}}^{-1}G_{\\frac{\\alpha}{K}}[b(\\hat{\\tilde{X}}_{t K+i})-b(\\tilde{X}_{t K})]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "C.2  Random Function Representation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "By Equation (1), the iterates of $\\textstyle\\mathsf{S}(A,G,\\Gamma,\\frac{\\alpha}{K})$ satisfy: ", "page_idx": 17}, {"type": "equation", "text": "$$\nX_{t K+i+1}=A\\mathop{\\_\\alpha}X_{t K+i}+G\\mathop{\\_\\alpha}_{K}{\\left[b(X_{t K+i})\\right]}+\\Gamma\\mathop{\\_\\alpha}Z_{t K+i}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By Lemma 1, we can write down the interpolating process as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{X}_{t K+i+1}=A_{\\frac{\\alpha}{K}}\\tilde{X}_{t K+i}+G_{\\frac{\\alpha}{K}}\\left[b(\\tilde{X}_{t K+i})\\right]+\\Gamma_{\\frac{\\alpha}{K}}\\tilde{Z}_{t K+i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that while $Z_{t K+i}$ are i.i.d. $\\mathcal{N}(0,\\mathbf{I})$ vectors, $\\tilde{Z}_{t K+i}$ can be non-Gaussian and non-i.i.d. Below we will show that the KL divergence between the joint laws of $(X_{t K+i})_{t,i}$ and $(\\tilde{X}_{t K+i})_{t,i}$ can be bounded by bounding the KL divergence between $(Z_{t K+i})_{t,i}$ and $(\\tilde{Z}_{t K+i})_{t,i}$ .We now state the following standard results from information theory. ", "page_idx": 17}, {"type": "text", "text": "Lemma 2 (Chain Rule for KL divergence). Let $p,q$ be two probability distributions over $\\mathcal X\\times\\mathcal X$ where $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ are polish spaces. Let $p_{x},q_{x}$ denote their respective marginals over $\\mathcal{X}$ and let $p_{y|x},q_{y|x}$ denote the conditional distribution over $\\boldsymbol{\\wp}$ conditioned on $x\\in\\mathscr{X}$ .Then, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathsf{K L}\\left(p\\Big|\\Big|q\\right)=\\mathsf{K L}\\left(p_{x}\\Big|\\Big|q_{x}\\right)+\\mathbb{E}_{x\\sim p_{x}}\\mathsf{K L}\\left(p_{y|x}\\Big|\\Big|q_{y|x}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Table 7: Comparison of DPM Solver Variants ", "text_level": 1, "page_idx": 18}, {"type": "table", "img_path": "Ylvviju6MD/tmp/ada7926a85152c898ec1b5c645755b4f944ca2b556404da7087713f7b3251603.jpg", "table_caption": ["CelebAHQ 256 "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "Ylvviju6MD/tmp/2848a954766e708dae462c6c16092231611838a72a3b864f990b48bcca086246.jpg", "table_caption": ["LSUN Churches 256 "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "Ylvviju6MD/tmp/de61378129c16609b18bb4230c1b93d5401ac87a018c63e7df87efdf156bdd6b.jpg", "table_caption": ["LSUN Bedrooms 256 "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "Ylvviju6MD/tmp/086cbe5898e1489b4d2363d32fa960feaf1e909513dd4172a65ead95bd02c732.jpg", "table_caption": ["FFHQ 256 "], "table_footnote": ["We refer to [44, Lemma 4.18] for a proof. Lemma 3 (Data Processing Inequality). Let $F:\\mathbb{R}^{k_{1}}\\rightarrow\\mathbb{R}^{k_{2}}$ be any measurable function. Let $P,Q$ be any probability distributions over $\\dot{\\mathbb{R}}^{k_{1}}$ . Then, the following inequality holds: "], "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathsf{K L}\\left(F_{\\#}P\\big|\\big|F_{\\#}Q\\right)\\le\\mathsf{K L}\\left(P\\big|\\big|Q\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The lemma below follows from the fact that Equation (14) and Equation (15) have the same functional form. ", "page_idx": 18}, {"type": "text", "text": "Lemma 4. Suppose $\\textstyle\\mathsf{S}(A,G,\\Gamma,\\frac{\\alpha}{K})$ and $\\mathsf{P S}(A,G,\\Gamma,\\alpha,K)$ are initialized at the same point $X_{0}$ .That is, $X_{0}=\\tilde{X}_{0}$ .Then, there exists a measurable function $F_{T}$ such that the following hold almost surely. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{(X_{\\tau})_{0\\leq\\tau\\leq T}=F_{T}(X_{0},Z_{0},...\\,,Z_{T-1})}}\\\\ {{\\ }}\\\\ {{(\\tilde{X}_{\\tau})_{0\\leq\\tau\\leq T}=F_{T}(X_{0},\\tilde{Z}_{0},...\\,,\\tilde{Z}_{T-1})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "text", "text": "By Lemma 3 and Lemma 4, we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{K L}\\left(\\mathsf{L a w}((\\tilde{X}_{\\tau})_{0\\le\\tau\\le T})\\big|\\big|\\mathsf{L a w}((X_{\\tau})_{0\\le\\tau\\le T})\\right)}\\\\ &{\\le\\mathsf{K L}\\left(\\mathsf{L a w}(X_{0},\\tilde{Z}_{0:T-1})\\big|\\big|\\mathsf{L a w}(X_{0},Z_{0:T-1})\\right)}\\\\ &{=\\mathbb{E}_{X_{0}}\\mathsf{K L}\\left(\\mathsf{L a w}(\\tilde{Z}_{0}|X_{0})\\big|\\big|\\mathsf{L a w}(Z_{0})\\right)+}\\\\ &{\\phantom{=}\\sum_{\\tau=1}^{T-1}\\mathbb{E}_{(X_{0},\\tilde{Z}_{0:T-1})}\\mathsf{K L}\\left(\\mathsf{L a w}(\\tilde{Z}_{\\tau}|\\tilde{Z}_{0:\\tau-1},X_{0})\\big|\\big|\\mathsf{L a w}(Z_{\\tau})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In the last step, we have used the chain rule (Lemma 2). ", "page_idx": 19}, {"type": "text", "text": "C.3 Controlling KL Divergence via Wasserstein Distances ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We now seek to control the individual KL-divergences in the RHS of Equation (16) via Wasserstein distances. We re-state [10, Lemma 26]: ", "page_idx": 19}, {"type": "text", "text": "Lemma 5. Suppose $\\mathbf{Z}\\sim\\mathcal{N}(0,\\sigma^{2}\\mathbf{I})$ . Let A and $\\mathbf{B}$ be random variables independent of $\\mathbf{Z}$ Then, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathsf{K L}\\left(\\mathsf{L a w}(\\mathbf{Z}+\\mathbf{A})\\big|\\big|\\mathsf{L a w}(\\mathbf{Z}+\\mathbf{B})\\right)\\leq\\frac{1}{2\\sigma^{2}}\\mathcal{W}_{2}^{2}(\\mathbf{A},\\mathbf{B})\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The Lemma below follows from the triangle inequality for Wasserstein distance. ", "page_idx": 19}, {"type": "text", "text": "Lemma 6. Let A, B, C be random vectors over $\\mathbb{R}^{k}$ with finite second moments. Then, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{W}_{2}(\\mathsf{L a w}(\\mathbf{A}),\\mathsf{L a w}(\\mathbf{B}+\\mathbf{C}))\\leq\\mathcal{W}_{2}(\\mathsf{L a w}(\\mathbf{A}),\\mathsf{L a w}(\\mathbf{B}))+\\sqrt{\\mathbb{E}\\|\\mathbf{C}\\|^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "From Lemma 1, we show that $\\tilde{Z}_{\\tau}=Z_{\\tau}+B_{\\tau}+S_{\\tau}$ . Conditioned on $\\tilde{Z}_{0:\\tau-1},X_{0},Z_{\\tau}\\sim\\mathcal{N}(0,\\mathbf{I})$ and is independent of $B_{\\tau},S_{\\tau}$ .We now use Gaussian spliting: if $A_{1},A_{2}\\sim\\mathcal{N}(0,{\\bf I})$ i.id, then $\\begin{array}{r}{\\frac{A_{1}+A_{2}}{\\sqrt{2}}\\sim}\\end{array}$ $\\mathcal{N}(0,\\mathbf{I})$ . Therefore, letting $Z_{\\tau,1},Z_{\\tau,2}$ be i.i.d from $\\mathcal{N}(0,\\mathbf{I})$ independent of $\\tilde{Z}_{0:\\tau-1},X_{0},S_{\\tau},B_{\\tau}$ ,we can write: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{K L}\\left(\\mathsf{L a w}(\\tilde{Z}_{\\tau}|\\tilde{Z}_{0:\\tau-1},X_{0})\\big|\\big|\\mathsf{L a w}(Z_{\\tau})\\right)}\\\\ &{\\le\\mathcal{W}_{2}^{2}\\left(\\mathsf{L a w}(\\frac{Z_{\\tau,2}}{\\sqrt{2}}),\\mathsf{L a w}(\\frac{Z_{\\tau,2}}{\\sqrt{2}}+S_{\\tau}+B_{\\tau}|\\tilde{Z}_{0:\\tau-1},X_{0})\\right)}\\\\ &{=\\frac{1}{2}\\mathcal{W}_{2}^{2}\\left(\\mathsf{L a w}(Z_{\\tau,2}),\\mathsf{L a w}(Z_{\\tau,2}+\\sqrt{2}(S_{\\tau}+B_{\\tau})|\\tilde{Z}_{0:\\tau-1},X_{0})\\right)}\\\\ &{\\le\\mathcal{W}_{2}^{2}\\left(\\mathsf{L a w}(Z_{\\tau,2}),\\mathsf{L a w}(Z_{\\tau,2}+\\sqrt{2}S_{\\tau}|\\tilde{Z}_{0:\\tau-1},X_{0})\\right)+2\\mathbb{E}[\\|B_{\\tau}\\|^{2}|\\tilde{Z}_{0:\\tau-1},X_{0}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In the second step, we have used the inequality in Lemma 5. In the third step we have used Lemma 6. ", "page_idx": 19}, {"type": "text", "text": "C.4 Bounding the Error Along the Trajectory: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Equation (17), we note that $Z_{\\tau,2}$ is Gaussian but $S_{\\tau}$ need not be Gaussian (but has zero mean). In the lemma stated below, we show that $Z_{\\tau,2}+\\sqrt{2}S_{\\tau}|\\tilde{Z}_{0:\\tau-1},X_{0}$ is close to a Gaussian of the same variance by adapting the arguments given in the proof of [51, Lemma 1.6]. The proof is defered to Section D ", "page_idx": 19}, {"type": "text", "text": "Lemma 7. Suppose N is random vector such that the following conditions are satisfied: ", "page_idx": 19}, {"type": "text", "text": "1. $\\|\\mathbf{N}\\|\\leq\\beta$ almost surely, $\\mathbb{E}\\mathbf{N}=0$ and $\\mathbb{E}\\mathbf{N}\\mathbf{N}^{\\intercal}=\\boldsymbol{\\Sigma}$ ", "page_idx": 20}, {"type": "text", "text": "2. N takes its value in a one dimensional sub-space almost surely. ", "page_idx": 20}, {"type": "text", "text": "Suppose $\\mathbf{Z}\\,\\sim\\mathcal{N}(0,\\mathbf{I})$ is independent of $\\mathbf{N}$ Let $P\\,=\\,\\mathsf{L a w}(\\sqrt{\\mathbf{I}+\\Sigma}\\mathbf{Z})$ and $Q\\,=\\,\\mathsf{L a w}(\\mathbf{Z}+\\mathbf{N})$ Denoting $\\nu:=\\mathsf{T r}(\\Sigma)$ . Then, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\mathcal{W}_{2}\\left(P,Q\\right)\\right)^{2}\\leq3(1+\\nu)\\left[\\beta^{4}\\nu^{3}+\\beta^{2}\\nu^{2}\\right]\\exp(\\frac{3\\beta^{2}}{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We also have the crude bound: ", "page_idx": 20}, {"type": "equation", "text": "$$\n(\\mathcal{W}_{2}\\left(P,Q\\right))^{2}\\leq2\\nu\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The next lemma demonstrates the convexity of the Wasserstein distance, which is a straightforward consequence of the Kantorovich duality [47]. ", "page_idx": 20}, {"type": "text", "text": "Lemma 8 (Convexity of Wasserstein Distance). Suppose $\\mu$ is a measure over $\\mathbb{R}^{d}$ and $Q(\\cdot,\\cdot)$ be a kernel over $\\mathbb{R}^{d}$ with respect to some arbitrary measurable space $\\Omega$ and let $M$ be a probability measureover $\\Omega$ That is $Q(\\cdot,\\omega)$ is a probability distribution over $\\mathbb{R}^{d}$ for every $\\omega\\in{\\Omega}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{W}_{2}\\left(\\mu,\\int Q(\\cdot,\\omega)d M(\\omega)\\right)\\leq\\int\\mathcal{W}_{2}\\left(\\mu,Q(\\cdot,\\omega)\\right)d M(\\omega)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lemma 9 (Wasserstein Distance Between Gaussians, [36]). ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathscr{W}_{2}^{2}(N(0,\\Sigma_{1}),\\mathscr{N}(0,\\Sigma_{2}))=\\mathsf{T r}(\\Sigma_{1}+\\Sigma_{2}-2(\\Sigma_{2}^{\\frac{1}{2}}\\Sigma_{1}\\Sigma_{2}^{\\frac{1}{2}})^{\\frac{1}{2}})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lemma 10. Let ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{P}:=\\mathsf{L a w}(Z_{K t+i,2})={\\mathcal N}(0,\\mathbf I)\\,,\\qquad\\qquad}\\\\ {\\bar{Q}:=\\mathsf{L a w}(Z_{K t+i,2}+\\sqrt2(S_{K t+i})|X_{0},\\tilde{Z}_{0:K t+i-1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Define the random variable ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\beta=\\|K\\Gamma_{\\frac{\\alpha}{K}}^{-1}G_{\\frac{\\alpha}{K}}[b(\\hat{\\tilde{X}}_{t K+i})-b(\\tilde{X}_{t K})]\\|\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{W}_{2}^{2}(\\bar{P},\\bar{Q})\\leq C\\mathbb{E}\\left[\\frac{\\beta^{4}}{K^{2}}+\\frac{\\beta^{10}}{K^{3}}+\\frac{\\beta^{6}}{K^{2}}+\\frac{\\beta^{2r}}{K}\\big|\\tilde{Z}_{0:K t+i-1},X_{0}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Note that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\sqrt{2}S_{t K+i}|\\tilde{Z}_{0:K t+i-1},Y_{0:K t+i-1},X_{0}]=0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now, define the random variable $\\beta$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\beta:=\\|K\\Gamma_{\\frac{\\alpha}{K}}^{-1}G_{\\frac{\\alpha}{K}}[b(\\hat{\\tilde{X}}_{t K+i})-b(\\tilde{X}_{t K})]\\|\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Clearly, $\\beta$ is measurable with respect to the sigma algebra of $\\tilde{Z}_{0:K t+i-1},Z_{0:K t+i-1},X_{0}$ .By the definition of $S_{t K+i}$ , it is clear that $\\lVert S_{t K+i}\\rVert\\leq\\bar{\\beta}$ almost surely. Define the conditional covariance (only in this proof) to be: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Sigma:=2\\mathbb{E}[S_{t K+i}S_{t K+i}^{\\intercal}|\\tilde{Z}_{0:K t+i-1},Z_{0:K t+i-1}X_{0}]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, almost surely: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathsf{T r}(\\Sigma)\\leq\\frac{2\\beta^{2}}{K}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "$\\sqrt{2}S_{t K+i}$ takes its values in a one-dimensional sub-space almost surely when conditioned on $\\tilde{Z}_{0:K t+i-1},Z_{0:K t+i-1},X_{0}$ . It is independent of $Z_{t K+i}$ conditioned on $\\tilde{Z}_{0:K t+i-1},Z_{0:K t+i-1},X_{0}$ Let us now define the following random probability distributions measurable with respect to the sigma algebra of $\\tilde{Z}_{0:K t+i-1},X_{0}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{\\lambda}.\\ Q:=\\mathsf{L a w}(Z_{K t+i,2}+\\sqrt{2}S_{K t+i}|\\tilde{Z}_{0:K t+i-1},Z_{0,K t+i-1},X_{0})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\nP:=\\mathsf{L a w}(\\sqrt{I+\\Sigma}Z_{K t+i,2}|\\tilde{Z}_{0:K t+i-1},Z_{0,K t+i-1},X_{0})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "First, note that by Lemma 8 and Jensen's inequality, we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{W}_{2}^{2}\\left(\\Bar{P},\\Bar{Q}\\right)\\leq\\mathbb{E}[\\mathcal{W}_{2}^{2}\\left(\\Bar{P},Q\\right)|\\tilde{Z}_{0:K t+i-1},X_{0}]}\\\\ &{\\leq2\\mathbb{E}[\\mathcal{W}_{2}^{2}\\left(\\Bar{P},P\\right)+\\mathcal{W}_{2}^{2}\\left(P,Q\\right)|\\tilde{Z}_{0:K t+i-1},X_{0}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "First consider $\\mathcal{W}_{2}^{2}\\left(\\bar{P},P\\right)$ . Conditioned on $\\tilde{Z}_{0:K t+i-1},Z_{0,K t+i-1},X_{0},\\Sigma$ has at-most one non-zero eigenvalue. Without loss of generality, we can take $\\Sigma=\\nu e_{1}e_{1}^{\\intercal}$ for the calculations below. From Lemma 9, we conclude that: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{W}_{2}^{2}\\left(\\Bar{P},P\\right)\\leq\\mathsf{T r}(2\\mathbf{I}+\\Sigma-2\\sqrt{\\mathbf{I}+\\Sigma})}\\\\ {=2+\\nu-2\\sqrt{1+\\nu}\\leq\\frac{\\nu^{2}}{4}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In the last step, we have used the following inequality which follows from the mean-value theorem: $\\begin{array}{r}{\\sqrt{1+\\nu}\\,\\geq1+\\frac{\\nu}{2}\\,-\\,\\frac{\\nu^{2}}{8}}\\end{array}$ .We check that the conditions for Lemma 7 hold for $P,Q$ (almost surely conditioned on $\\tilde{Z}_{0:K t+i-1},X_{0})$ and $\\begin{array}{r}{\\nu\\le\\frac{\\beta^{2}}{K}}\\end{array}$ to conclude: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\mathcal{W}_{2}^{2}\\left(P,Q\\right)\\leq C\\left[\\frac{\\beta^{10}}{K^{3}}+\\frac{\\beta^{6}}{K^{2}}\\right]\\exp(\\frac{3\\beta^{2}}{2})\\mathbb{1}(\\beta\\leq1)+\\frac{2\\beta^{2}}{K}\\mathbb{1}(\\beta>1)}}\\\\ {{\\leq C\\left[\\left[\\frac{\\beta^{10}}{K^{3}}+\\frac{\\beta^{6}}{K^{2}}\\right]\\mathbb{1}(\\beta\\leq1)+\\frac{\\beta^{2}}{K}\\mathbb{1}(\\beta>1)\\right]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Combining this with Equation (24), we conclude that for any $r\\geq1$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{W}_{2}^{2}(\\bar{P},\\bar{Q})\\leq C\\left[\\frac{\\beta^{4}}{K^{2}}+\\left[\\frac{\\beta^{10}}{K^{3}}+\\frac{\\beta^{6}}{K^{2}}\\right]\\mathbb{1}(\\beta\\leq1)+\\frac{\\beta^{2}}{K}\\mathbb{1}(\\beta>1)\\right]}\\\\ &{\\qquad\\qquad\\leq C\\left[\\frac{\\beta^{4}}{K^{2}}+\\frac{\\beta^{10}}{K^{3}}+\\frac{\\beta^{6}}{K^{2}}+\\frac{\\beta^{2}}{K}\\mathbb{1}(\\beta>1)\\right]}\\\\ &{\\qquad\\qquad\\leq C\\left[\\frac{\\beta^{4}}{K^{2}}+\\frac{\\beta^{10}}{K^{3}}+\\frac{\\beta^{6}}{K^{2}}+\\frac{\\beta^{2r}}{K}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "C.5 Finishing The Proof ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We combine Equations (16) and (17) with Lemma 10 to conclude the result of Theorem 1. ", "page_idx": 21}, {"type": "text", "text": "D Proof of Lemma 7 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The crude bound in Equation (19) follows via a naive coupling argument. We will now sketch a proof Equation (18) in Lemma 7 by showcasing how to modify the proof of [52, Lemma 1.6]. Our proof deals with a specialized case compared to [51, Lemma 1.6]. This allows us to derive a stronger result. In this section, by ^original proof', we refer to the proof in [52]. ", "page_idx": 21}, {"type": "text", "text": "Since $\\mathbf{N}$ is supported on a single dimensional sub-space almost surely, we take this direction to be $e_{1}$ almost surely without loss of generality. We thus take the covariance matrix of $\\mathbf{N}$ tobe $\\Sigma=\\nu e_{1}e_{1}^{\\intercal}$ ", "page_idx": 21}, {"type": "text", "text": "We generate jointly distributed random vectors Z, $\\mathbf{Z}^{\\prime}\\in\\mathbb{R}^{d}$ as follows: Let $\\langle\\mathbf{Z},e_{j}\\rangle$ are i.i.d. standard normal random variables for $j=2,\\ldots,d$ and $\\langle\\mathbf{Z}^{\\prime},e_{j}\\rangle=\\langle\\mathbf{Z},e_{j}\\rangle$ almost surely. We generate $\\langle\\mathbf{Z}^{\\prime},e_{1}\\rangle$ to be standard normal independent of $(\\langle\\mathbf{Z},e_{j}\\rangle)_{j\\geq2}$ . Let $\\langle\\mathbf{N},e_{1}\\rangle$ be independent of $\\mathbf{Z}^{\\prime}$ .We draw $\\langle\\mathbf{Z},e_{1}\\rangle$ to be standard normally distributed and Wasserstein- optimally coupled to $\\frac{\\langle\\mathbf{Z}^{\\prime},e_{1}\\rangle\\!+\\!\\langle\\mathbf{N},e_{1}\\rangle}{\\sqrt{1\\!+\\!\\nu}}$ and independent of all other random variables mentioned above. We can easily check that $(\\sqrt{\\mathbf{I}+\\Sigma}\\mathbf{Z},\\mathbf{Z}^{\\prime}+$ $\\mathbf{N}$ ) as defined above is a coupling between $P,Q$ ", "page_idx": 21}, {"type": "text", "text": "Via this coupling, we conclude that: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{W}_{2}\\left(P,Q\\right)\\leq\\sqrt{\\mathbb{E}(\\sqrt{1+\\nu}\\langle\\mathbf{Z},e_{1}\\rangle-\\langle\\mathbf{Z}^{\\prime},e_{1}\\rangle-\\langle\\mathbf{N},e_{1}\\rangle)^{2}}}\\\\ &{\\quad\\quad\\quad\\quad=\\sqrt{1+\\nu}\\mathcal{W}_{2}\\left(\\langle\\mathbf{Z},e_{1}\\rangle,\\frac{\\langle\\mathbf{Z}^{\\prime},e_{1}\\rangle+\\langle\\mathbf{N},e_{1}\\rangle}{\\sqrt{1+\\nu}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let $Z_{1}:=\\langle\\mathbf{Z},e_{1}\\rangle$ and $Z_{1}^{\\prime}:=\\langle\\mathbf{Z}^{\\prime},e_{1}\\rangle,N_{1}:=\\langle\\mathbf{N},e_{1}\\rangle$ . We define $\\begin{array}{r}{m=1\\!+\\!\\frac{1}{\\nu}}\\end{array}$ (and note throughout the proof that a value of infinity can be easily handled). We see that $\\begin{array}{r}{\\frac{Z_{1}^{\\prime}+N_{1}}{\\sqrt{1+\\nu}}=\\sqrt{1-\\frac{1}{m}}Z_{1}^{\\prime}+\\sqrt{1-\\frac{1}{m}}N_{1}}\\end{array}$ Note that $\\textstyle{\\sqrt{1-{\\frac{1}{m}}}}\\mathbf{N}$ is deoed y $Y$ in the oria rofan the bound is $\\begin{array}{r}{\\|Y\\|\\leq\\frac{\\beta}{\\sqrt{n}}}\\end{array}$ insteadof $\\|\\mathbf{N}\\|\\leq\\beta$ in this proof. ", "page_idx": 22}, {"type": "text", "text": "Consider the function $f(x)$ , which denotes the ratio of density function of $\\frac{Z_{1}^{\\prime}\\!+\\!N_{1}}{\\sqrt{1\\!+\\!\\nu}}$ to that of $Z_{1}$ at the point $x$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}f(Z_{1})^{2}=\\mathbb{E}\\left[\\exp\\left(\\frac{-(N_{1}^{2}+(N_{1}^{\\prime})^{2})+2m N_{1}N_{1}^{\\prime}}{2(m+1)}+\\frac{1}{2(m^{2}-1)}-r(m)\\right)\\right]\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Where $N_{1}^{\\prime}$ is an i.i.d copy of $N_{1}$ and $\\begin{array}{r}{r(n):=\\frac{1}{2(n^{2}-1)}-\\frac{1}{2}\\log(1+\\frac{1}{n^{2}-1})}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "Define $\\begin{array}{r}{Q_{1}=\\frac{-(N_{1}^{2}+(N_{1}^{\\prime})^{2})+2m N_{1}N_{1}^{\\prime}}{2(m+1)}+\\frac{1}{2(m^{2}-1)}-r(m)}\\end{array}$ We modif thestmae in Lema 4 and Lemma 4.5 in the original proof in the following: ", "page_idx": 22}, {"type": "text", "text": ".1. 1. $\\begin{array}{r}{|Q_{1}|\\leq\\frac{m|N_{1}N_{1}^{\\prime}|}{m+1}+\\frac{\\beta^{2}}{m+1}+\\frac{1}{2(m^{2}-1)}}\\end{array}$ almost surely ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2.\\,\\,\\,\\mathbb{E}[Q_{1}]=-\\frac{1}{2(m^{2}-1)}-r(m)}\\\\ &{}\\\\ &{3.\\,\\,\\,\\mathbb{E}[Q_{1}^{2}]\\leq\\frac{m\\beta^{2}+2m^{2}+1}{2(m^{2}-1)^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. ", "page_idx": 22}, {"type": "text", "text": "1. Note that $\\begin{array}{r}{r(m)\\leq\\frac{1}{2(m^{2}-1)}}\\end{array}$ By triangle nequality, we hav: ", "page_idx": 22}, {"type": "equation", "text": "$$\n|Q_{1}|\\leq\\frac{m|N_{1}N_{1}^{\\prime}|}{m+1}+\\frac{\\beta^{2}}{m+1}+\\frac{1}{2(m^{2}-1)}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "2. A direct calculation shows the identity for $\\mathbb{E}Q_{1}$ ", "page_idx": 22}, {"type": "text", "text": "3. Now, consider $\\mathbb{E}Q_{1}^{2}$ : We follow the proof of [52, Lemma 4.5], to conclude the following inequalities. Since $\\bar{\\mathbb{E}}Q_{1}\\leq0$ and 1 2(m\u00b2-1) - r(m) \u2265 0, we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}Q_{1}^{2}\\leq\\mathbb{E}(Q_{1}-\\frac{1}{2(m^{2}-1)}+r(m))^{2}}\\\\ &{\\qquad=\\frac{m^{2}}{(m+1)^{2}}\\mathbb{E}N_{1}^{2}(N_{1}^{\\prime})^{2}+\\frac{1}{4(m+1)^{2}}\\mathbb{E}(N_{1}^{2}+(N_{1}^{\\prime})^{2})^{2}}\\\\ &{\\qquad=\\frac{m^{2}}{(m^{2}-1)^{2}}+\\frac{1}{4(m+1)^{2}}\\mathbb{E}(N_{1}^{2}+(N_{1}^{\\prime})^{2})^{2}}\\\\ &{\\qquad\\leq\\frac{m\\beta^{2}+2m^{2}+1}{2(m^{2}-1)^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that the parameter $\\sigma_{i}$ found in the original proof satisfies $\\sigma_{i}=1$ in our proof. We now proceed with the proof of Lemma 7. Define $\\begin{array}{r}{R(Q)=\\exp(Q)-1-Q-\\frac{Q^{2}}{2}}\\end{array}$ . From the original proof, we conclude via the Talagrand transport inequality that: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left[\\mathcal{W}_{2}\\left(\\langle\\mathbf{Z},e_{1}\\rangle,\\frac{\\langle\\mathbf{Z}^{\\prime},e_{1}\\rangle+\\langle\\mathbf{N},e_{1}\\rangle}{\\sqrt{1+\\nu}}\\right)\\right]^{2}\\leq2[\\mathbb{E}e^{Q_{1}}-1]=2\\left[\\mathbb{E}[Q_{1}]+\\frac{1}{2}\\mathbb{E}[Q_{1}^{2}]+\\mathbb{E}[R(Q_{1})]\\right]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "From Lemma 11, and using the fact that $r(m)\\geq0$ , we note that: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}[Q_{1}]+\\frac{1}{2}\\mathbb{E}[Q_{1}^{2}]\\leq\\frac{3+m\\beta^{2}}{4(m^{2}-1)^{2}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now, let us bound $\\mathbb{E}R(Q_{1})$ . Via a straightforward application of the taylor series, we have almost surely: ", "page_idx": 23}, {"type": "equation", "text": "$$\nR(Q_{1})\\leq{\\frac{|Q_{1}|^{3}\\exp(|Q_{1}|)}{6}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "From Lemma 11, we conclude that $\\begin{array}{r}{|Q_{1}|\\leq\\beta^{2}+\\frac{1}{2(m^{2}-1)}}\\end{array}$ Since $\\nu\\leq\\beta^{2}$ wemust have $\\begin{array}{r}{m=1+\\frac{1}{\\nu}\\geq}\\end{array}$ $\\textstyle1+{\\frac{1}{\\beta^{2}}}$ .Thus, we have $\\begin{array}{r}{|Q_{1}|\\leq\\frac{3\\beta^{2}}{2}}\\end{array}$ almost surely Usin ths in Equatio (2) we ave: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}R(Q_{1})\\leq\\mathbb{E}\\frac{|Q_{1}|^{3}}{6}\\exp\\left(\\frac{3\\beta^{2}}{2}\\right)}\\\\ &{\\qquad\\qquad\\leq\\frac{\\beta^{2}}{4}\\exp(\\frac{3\\beta^{2}}{2})\\mathbb{E}|Q_{1}|^{2}}\\\\ &{\\qquad\\qquad\\leq\\left[\\frac{m\\beta^{4}+m^{2}\\beta^{2}}{4(m^{2}-1)^{2}}\\right]\\exp(\\frac{3\\beta^{2}}{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In the last step, we have used item 3 of Lemma 11 along with the fact that $m\\beta^{2}\\geq1$ Combining these estimates with Equation (31), we conclude: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left[\\mathcal{W}_{2}\\left(\\langle\\mathbf{Z},e_{1}\\rangle,\\frac{\\langle\\mathbf{Z}^{\\prime},e_{1}\\rangle+\\langle\\mathbf{N},e_{1}\\rangle}{\\sqrt{1+\\nu}}\\right)\\right]^{2}\\leq\\left[\\frac{m\\beta^{4}+m^{2}\\beta^{2}}{2(m^{2}-1)^{2}}\\right]\\exp(\\frac{3\\beta^{2}}{2})+\\frac{3+m\\beta^{2}}{2(m^{2}-1)^{2}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now, we use the fact that $\\begin{array}{r}{m=1+\\frac{1}{\\nu}}\\end{array}$ , which implies $\\begin{array}{r}{\\frac{1}{m^{2}-1}\\leq\\frac{1}{(m-1)^{2}}\\leq\\nu^{2}}\\end{array}$ . Thus, we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left[\\mathcal{W}_{2}\\left(\\langle\\mathbf{Z},e_{1}\\rangle,\\frac{\\langle\\mathbf{Z}^{\\prime},e_{1}\\rangle+\\langle\\mathbf{N},e_{1}\\rangle}{\\sqrt{1+\\nu}}\\right)\\right]^{2}\\leq\\left[\\frac{\\beta^{4}\\nu^{3}}{2}+\\frac{\\beta^{2}\\nu^{2}}{2}\\right]\\exp(\\frac{3\\beta^{2}}{2})+\\frac{3\\nu^{4}+\\beta^{2}\\nu^{3}}{2}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Using the faet that 32 > V, we have: 3+3\u00b22-3 $\\begin{array}{r}{\\frac{3\\nu^{4}+\\beta^{2}\\nu^{3}}{2}\\leq2\\beta^{2}\\nu^{3}}\\end{array}$ .Thus, $2(1+\\nu)\\beta^{2}\\nu^{3}\\leq2\\beta^{2}\\nu^{3}+2\\beta^{4}\\nu^{3}\\leq$ $2(1+\\nu)(\\beta^{2}\\nu^{2}+\\beta^{4}\\nu^{3})$ . Plugging this into Equation (27), we conclude the result. ", "page_idx": 23}, {"type": "text", "text": "E  Overdamped Langevin Dynamics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we will prove Theorem 2 after developing some key results regarding OLMC. Recall $b(\\mathbf{x},\\tau)\\,=\\,-\\nabla F(\\mathbf{x})$ $F:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ \uff0c+\u4e00 $G_{\\frac{\\alpha}{K}}={\\frac{\\alpha}{K}}\\mathbf{I}$ $\\begin{array}{r}{\\Gamma_{\\frac{\\alpha}{K}}=\\sqrt{\\frac{2\\alpha}{K}}\\mathbf{I}}\\end{array}$   \n$\\begin{array}{r}{N_{t}:=\\sum_{j=0}^{K-1}H_{t,j}}\\end{array}$ $\\nabla F$ $L$ $T$   \nany $T$ by considering the closest power of 2 above $T$ instead. ", "page_idx": 23}, {"type": "text", "text": "Recall $B_{t K+i}$ from Theorem 1. Instantiating this for Overdamped Langevin Dynamics, we conclude that for any $t,i\\in\\mathbb{N}\\cup\\{0\\}$ and $0\\leq i\\leq K-1$ , we must have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|B_{t K+i}\\|^{2}\\leq\\frac{L^{2}\\alpha}{2K}\\operatorname*{sup}_{0\\leq j\\leq K-1}\\|\\hat{\\tilde{X}}_{t K+j}-\\tilde{X}_{t K+j}\\|^{2}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Here, we have used the fact that $\\nabla F$ is $L$ -Lipschitz. ", "page_idx": 24}, {"type": "text", "text": "Now, consider $\\beta_{t K+i}$ in Theorem 1. For any $p\\geq1,t,i\\in\\mathbb{N}\\cup\\{0\\}$ and $0\\leq i\\leq K-1$ , we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\beta_{t K+i}^{2p}\\le(\\frac{L^{2}\\alpha K}{2})^{p}\\underset{0\\leq j\\leq K-1}{\\operatorname*{sup}}\\|\\hat{\\tilde{X}}_{t K+j}-\\tilde{X}_{t K}\\|^{2p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In order to apply Theorem 1, we will now proceed to bound the quantities in the RHS of Equations (35) and (36) ", "page_idx": 24}, {"type": "text", "text": "E.1  Bounding the Moments ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The following lemma gives us almost sure control over quantities of interest. We refer to Section I.1 for the proof. ", "page_idx": 24}, {"type": "text", "text": "Lemma 12. Suppose the stepsize $\\alpha L<1$ Let $\\begin{array}{r}{M_{t,k}:=\\sqrt{\\frac{2\\alpha}{K}}\\operatorname*{sup}_{0\\leq i\\leq k-1}\\Big\\|\\sum_{j=0}^{i}Z_{t K+j}\\Big\\|}\\end{array}$ Then the following hold almost surely for every $0\\le k\\le K$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\leq i\\leq k-1}\\|\\hat{\\tilde{X}}_{t K+i}-\\tilde{X}_{t K}\\|\\leq\\alpha\\|\\nabla F(\\tilde{X}_{t K})\\|+M_{t,k}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "2. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\leq i\\leq k}\\|\\hat{\\tilde{X}}_{t K+i}-\\tilde{X}_{t K+i}\\|\\leq\\alpha L N_{t}\\operatorname*{sup}_{i\\leq k-1}\\|\\hat{\\tilde{X}}_{t K+i}-\\tilde{X}_{t K}\\|\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "3. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}[M_{t,K}^{p}]\\leq C(p)(\\alpha d)^{\\frac{p}{2}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We will now prove the following growth estimate for the trajectory $\\tilde{X}_{t K}$ . We refer to Section I.2 for its proof. ", "page_idx": 24}, {"type": "text", "text": "Lemma 13. Let $p\\geq1$ be fixed. There exists a large enough constant $\\bar{C}_{p}$ which depends only on $p$ such that whenever s -t < aLCp ,we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\leq h\\leq s}\\left[\\mathbb{E}\\|\\tilde{X}_{h K}-\\tilde{X}_{t K}\\|^{p}|\\tilde{X}_{t K}\\right]^{\\frac{1}{p}}\\leq3\\alpha(s-t)\\|\\nabla F(\\tilde{X}_{t K})\\|+C_{p}\\sqrt{\\alpha d(s-t)}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We apply Lemma 26 along with Lemma 13 to conclude the following result which is proved in Section I.4. ", "page_idx": 24}, {"type": "text", "text": "Lemma 14. There exists a constant $c_{p}$ such that whenever $\\alpha L\\leq c_{p}$ we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}\\|\\nabla F(\\tilde{X}_{t K})\\|^{2p}\\leq C_{p}L^{p}d^{p}T+C_{p}(\\alpha L)^{p-1}\\mathbb{E}(\\sum_{t=1}^{T}\\|\\nabla F(\\tilde{X}_{t K})\\|^{2})^{p}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "While Lemma 14 controlled $\\begin{array}{r}{\\sum_{t=1}^{T}\\mathbb{E}\\|\\nabla F(\\tilde{X}_{t K})\\|^{2p}}\\end{array}$ in terms of $\\begin{array}{r}{\\sum_{t=1}^{T}\\|\\nabla F(\\tilde{X}_{t K})\\|^{2}}\\end{array}$ , in the Lemma below we control $\\begin{array}{r}{\\sum_{t=1}^{T}\\|\\nabla F(\\tilde{X}_{t K})\\|^{2}}\\end{array}$ .We refer to Section I.5 for its proof. ", "page_idx": 24}, {"type": "text", "text": "Lemma 15. Suppose $p\\geq1$ be arbitrary. There exists $c_{p}>0$ small enough such that whenever $\\alpha L<c_{p}$ we have for some constant $C_{p}>0$ depending only on $p$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lefteqn{\\mathbb{E}\\big(\\displaystyle\\sum_{t=0}^{T-1}\\|\\nabla F(\\tilde{X}_{t K})\\|^{2}\\big)^{p}\\leq\\displaystyle\\frac{C_{p}}{\\alpha^{p}}\\mathbb{E}\\|(F(X_{0})-F(\\tilde{X}_{K T}))^{+}\\|^{p}+C_{p}T^{p-1}L^{2p}\\alpha^{2p}\\mathbb{E}\\displaystyle\\sum_{t=0}^{T-1}\\|\\nabla F(\\tilde{X}_{t K})\\|^{2p}}}\\\\ {+\\,C_{p}L^{p}d^{p}T^{p}+\\displaystyle\\frac{C_{p}}{\\alpha^{p}}\\qquad\\qquad\\qquad\\qquad\\qquad(38)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We combine the results of Lemma 14 and Lemma 15 to conclude the following result: ", "page_idx": 24}, {"type": "text", "text": "Lemma 16. Given $p\\geq1$ arbitrary, there exists a constant $c_{p}\\;>\\;0$ depending only $p$ such that whenever $\\alpha L<c_{p}$ and $\\alpha^{3p-1}L^{3p-1}T^{p-1}<c_{p}$ we must have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}\\|\\nabla F(\\tilde{X}_{t K})\\|^{2p}\\leq C_{p}L^{p}d^{p}T(1+(\\alpha L T)^{p-1})+\\frac{C_{p}L^{p-1}}{\\alpha}\\left[\\mathbb{E}|(F(X_{0})-F(\\tilde{X}_{K T}))^{+}|^{p}+1\\right]\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma 17. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\|B_{t K+i}\\|^{2}\\leq\\frac{C L^{4}\\alpha^{5}}{K}\\mathbb{E}\\|\\nabla F(\\tilde{X}_{t K})\\|^{2}+\\frac{C L^{4}\\alpha^{4}}{K}d\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "2. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\beta_{t K+i}^{2p}\\leq C_{p}\\left[L^{2p}\\alpha^{3p}K^{p}\\mathbb{E}\\|\\nabla F(\\tilde{X}_{t K})\\|^{2p}+L^{2p}\\alpha^{2p}K^{p}d^{p}\\right]\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. 1. Using Equation (35), we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|B_{t K+i}\\|^{2}\\leq\\displaystyle\\frac{L^{2}\\alpha}{2K}\\mathbb{E}\\operatorname*{sup}_{0\\leq j\\leq K-1}\\|\\hat{\\tilde{X}}_{t K+j}-\\tilde{X}_{t K+j}\\|^{2}}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{L^{4}\\alpha^{3}}{2K}\\mathbb{E}N_{t}^{2}\\operatorname*{sup}_{0\\leq j\\leq K-1}\\|\\hat{\\tilde{X}}_{t K+j}-\\tilde{X}_{t K}\\|^{2}}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{L^{4}\\alpha^{3}}{K}\\mathbb{E}\\operatorname*{sup}_{0\\leq j\\leq K-1}\\|\\hat{\\tilde{X}}_{t K+j}-\\tilde{X}_{t K}\\|^{2}}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{C L^{4}\\alpha^{3}}{K}\\left[\\alpha^{2}\\mathbb{E}\\|\\nabla F(\\tilde{X}_{t K})\\|^{2}+\\alpha d\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "2. We use Equation (36) and proceed as in item 1. ", "page_idx": 25}, {"type": "text", "text": "E.2 Finishing The Proof ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "For $p\\geq1$ , we define $\\Delta^{(p)}:=\\mathbb{E}[(F(X_{0})-F(\\mathbf{x}^{*}))^{p}]$ . Let $\\lesssim$ denote $\\leq$ up to a universal positive multiplicative constant on the RHS. We now combine Lemma 17, Lemma 15 with Theorem 1 (taking $r=7$ ) to conclude the following bound: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{K L}\\left(\\mathsf{L a w}((X_{t}^{\\mathsf{P}})_{0\\le t\\le T})\\right)\\Bigl\\|\\mathsf{L a w}((X_{K t})_{0\\le t\\le T})\\Bigr)}\\\\ &{\\lesssim L^{4}\\alpha^{4}(\\Delta^{(1)}+1)+L^{5}\\alpha^{5}K(\\Delta^{(2)}+1)+L^{8}\\alpha^{8}K^{2}(\\Delta^{(3)}+1)+L^{14}\\alpha^{14}K^{3}(\\Delta^{(5)}+1)}\\\\ &{\\quad+\\,L^{20}\\alpha^{20}K^{7}(\\Delta^{(7)}+1)+L^{4}\\alpha^{4}K d^{2}T+L^{7}\\alpha^{7}K d^{2}T^{2}+L^{6}\\alpha^{6}K^{2}d^{3}T+L^{11}\\alpha^{11}K^{2}d^{3}T^{3}}\\\\ &{\\quad+\\,L^{10}\\alpha^{10}K^{3}d^{5}T+L^{19}\\alpha^{19}K^{3}d^{5}T^{5}+L^{14}\\alpha^{14}K^{7}d^{7}T+L^{27}\\alpha^{27}K^{7}d^{7}T^{7}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "F  Underdamped Langevin Dynamics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we will prove Theorem 3 after developing some key results regarding ULMC. We will assume that $T$ is a power of 2 in this entire section, which useful to apply Lemma 26. The results hold for any $T$ by considering the closest power of 2 above $T$ instead. Recall that in the case of ULMC,wehave: ", "page_idx": 25}, {"type": "equation", "text": "$$\nA_{h}:=\\left[\\!\\!\\begin{array}{c c}{\\mathbf{I}_{d}}&{\\frac{1}{\\gamma}(1-e^{-\\gamma h})\\mathbf{I}_{d}}\\\\ {0}&{e^{-\\gamma h}\\mathbf{I}_{d}}\\end{array}\\!\\!\\right],\\quad G_{h}:=\\left[\\!\\!\\begin{array}{c c}{\\frac{1}{\\gamma}(h-\\frac{1}{\\gamma}(1-e^{-\\gamma h}))\\mathbf{I}_{d}}&{0}\\\\ {\\frac{1}{\\gamma}(1-e^{-\\gamma h})\\mathbf{I}_{d}}&{0}\\end{array}\\!\\!\\right]\\quad b(X_{t}):=\\left[\\!\\!\\begin{array}{c}{-\\nabla F(U_{t})}\\\\ {0}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Gamma_{h}^{2}:=\\left[\\frac{2}{\\gamma}\\left(h-\\frac{2}{\\gamma}(1-e^{-\\gamma h})+\\frac{1}{2\\gamma}(1-e^{-2\\gamma h})\\right)\\mathbf{I}_{d}\\right.\\quad\\frac{1}{\\gamma}(1-2e^{-\\gamma h}+e^{-2\\gamma h})\\mathbf{I}_{d}\\right]\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "$\\nabla F(\\cdot)$ .s $L$ Lishiz welet $\\begin{array}{r}{N_{t}:=\\sum_{i=0}^{K-1}H_{t,i}}\\end{array}$ Welet $\\Pi$ be the projector to the first $d\\!.$ -dimensions in the standard basis. That is, $\\Pi:={\\left[\\!\\!\\begin{array}{l l}{\\mathbf{I}_{d}}&{0}\\\\ {0}&{0}\\end{array}\\!\\!\\right]}$ ", "page_idx": 26}, {"type": "text", "text": "The $2d$ dimensional space can be decomposed into position (sub-space spanned by the first $d$ standard basis vectors) and velocity (sub-space spanned by the last $d$ standard basis vectors) sub-spaces. We use the convention that $X\\in\\mathbb{R}^{2d}$ is such that $\\overset{\\cdot}{\\b{X}}=\\left[\\!\\!\\begin{array}{l}{\\!\\!U}\\\\ {\\!\\!V}\\end{array}\\!\\!\\right]$ where $U\\in\\mathbb{R}^{d}$ is the position and $V\\in\\mathbb{R}^{d}$ is the velocity. Throughout this section, we implicity let $\\bar{X}_{\\tau}=\\left[\\tilde{\\dot{U}}_{\\tau}\\right]$ [  With some abuse o ntaion we let $U=\\Pi X$ and $V=({\\bf I}-\\Pi)X$ ", "page_idx": 26}, {"type": "text", "text": "The following lemma collects some useful bounds on $A_{h},G_{h}$ and $\\Gamma_{h}$ , and is proved in Section I.6 ", "page_idx": 26}, {"type": "text", "text": "Lemma 18. ", "page_idx": 26}, {"type": "equation", "text": "$$\nG_{h}^{\\mathsf{T}}\\Gamma_{h}^{-2}G_{h}\\preceq\\left[\\begin{array}{c c}{C\\frac{h\\exp(2\\gamma h)}{\\gamma}\\mathbf{I}_{d}}&{0}\\\\ {0}&{0}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For all $0\\leq j\\leq K-1$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|\\Pi A_{\\frac{j\\alpha}{K}}G_{\\frac{\\alpha}{K}}\\|\\leq\\frac{3\\alpha^{2}}{2K}\\exp(\\frac{\\gamma\\alpha}{K})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Analogous to the analysis of Overdamped Langevin Dynamics, we consider the following dynamics. ", "page_idx": 26}, {"type": "text", "text": "F.1  Bounding the Moments: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Using the scaling relations in Section 2.1, we write down the iteration of $\\mathsf{P S}(A,G,\\Gamma,b,\\alpha,K)$ for Underdamped Langevin Dynamics as: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\tilde{X}_{(t+1)K}=A_{\\alpha}\\tilde{X}_{t K}+G_{\\alpha}b(\\tilde{X}_{t K})+\\alpha\\Delta_{t}+\\Gamma_{\\alpha}\\bar{Y}_{t}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\mathrm{Where},\\quad\\alpha\\Delta_{t}}&{:=\\quad\\sum_{i=0}^{K-1}K H_{i}A_{\\frac{\\alpha(K-i-1)}{K}}G_{\\frac{\\alpha}{K}}\\left[b(\\hat{\\tilde{X}}_{t K+i})-b(\\tilde{X}_{t K})\\right]}&{\\mathrm{and}\\quad\\Gamma_{\\alpha}\\bar{Y}_{t}}&{:=}\\\\ &{\\sum_{j=0}^{K-1}A_{\\frac{\\alpha(K-1-j)}{K}}\\Gamma_{\\frac{\\alpha}{K}}Z_{t K+j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Applying the triangle inequality, we conclude the following lemma. ", "page_idx": 26}, {"type": "text", "text": "Lemma 19. Suppose that $\\|\\cdot\\|_{\\mathsf{a n y}}$ is any semi-norm over $\\mathbb{R}^{d}$ Then the following hold almost surely for every $0\\le k\\le K$ ", "page_idx": 26}, {"type": "text", "text": "1. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\hat{\\tilde{X}}_{t K+i}-\\tilde{X}_{t K}\\|_{\\mathsf{a n y}}\\leq\\|\\big(A_{\\frac{i\\alpha}{K}}-\\mathbf{I}\\big)\\tilde{X}_{t K}\\|_{\\mathsf{a n y}}+\\|G_{\\frac{i\\alpha}{K}}b(\\tilde{X}_{t K})\\|_{\\mathsf{a n y}}}\\\\ {+\\|\\displaystyle\\sum_{j=0}^{i-1}A_{\\frac{\\alpha(i-1-j)}{K}}\\Gamma_{\\frac{\\alpha}{K}}Z_{t K+j}\\|_{\\mathsf{a n y}}\\,~~~~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "2. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|\\hat{\\tilde{X}}_{t K+i}-\\tilde{X}_{t K+i}\\|_{\\mathsf{a n y}}\\leq\\sum_{j=0}^{i-1}K H_{j}\\|A_{\\frac{(i-1-j)\\alpha}{K}}G_{\\frac{\\alpha}{K}}\\big(b\\big(\\hat{\\tilde{X}}_{t K+i}\\big)-b\\big(\\tilde{X}_{t K}\\big)\\big)\\|_{\\mathsf{a n y}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Define $\\begin{array}{r}{\\psi_{t}:=\\tilde{U}_{K t}+\\frac{\\tilde{V}_{K t}}{\\gamma}}\\end{array}$ Thefollowing Lma provedinSetion7 givs the timvoltf $\\psi_{t}$ ", "page_idx": 26}, {"type": "text", "text": "Lemma 20. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\psi_{t+1}-\\psi_{t}=-\\frac{\\alpha}{\\gamma}\\nabla F(\\tilde{U}_{t K})-\\sum_{i=0}^{K-1}\\frac{H_{i}\\alpha}{\\gamma}\\left[\\nabla F(\\hat{U}_{t K+i})-\\nabla F(\\tilde{U}_{t K})\\right]+\\tilde{\\Psi}_{t}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Where $\\begin{array}{r}{\\tilde{\\Psi}_{t}\\sim\\mathcal{N}(0,\\frac{2\\alpha}{\\gamma}\\mathbf{I}_{d})}\\end{array}$ is independent of $\\tilde{X}_{t K}$ but not necessarily indepependent of $\\hat{\\tilde{U}}_{t K+i}$ for $i>0$ ", "page_idx": 26}, {"type": "text", "text": "Lemma21.Considertheseminormgivenby $\\begin{array}{r l r}{\\|x\\|_{\\Pi}^{2}}&{{}=}&{x^{\\top}\\Pi x}\\end{array}$ LetMt,K:= $\\begin{array}{r}{\\operatorname*{sup}_{j\\leq K-1}\\|\\sum_{j=0}^{i-1}A_{\\frac{\\alpha(i-1-j)}{K}}\\Gamma_{\\frac{\\alpha}{K}}Z_{t K+j}\\|_{\\Pi}.}\\end{array}$ Forainy $0\\leq i\\leq K-1$ ", "page_idx": 27}, {"type": "text", "text": "1. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\hat{\\tilde{U}}_{t K+i}-\\tilde{U}_{t K}\\|^{2}\\leq C\\alpha^{2}\\|\\tilde{V}_{t K}\\|^{2}+C\\alpha^{4}\\|\\nabla F(\\tilde{U}_{t K})\\|^{2}+C M_{t,K}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "2. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(\\psi_{t+1})-F(\\psi_{t})\\leq-\\displaystyle\\frac{\\alpha}{4\\gamma}(1-\\frac{6\\alpha L}{\\gamma})\\|\\nabla F(\\tilde{U}_{t K})\\|^{2}+\\langle\\nabla F(\\psi_{t})-\\nabla F(\\tilde{U}_{t K}),\\tilde{\\Psi}_{t}\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\langle\\nabla F(U_{t}),\\tilde{\\Psi}_{t}\\rangle+\\displaystyle\\frac{3L}{2}\\|\\tilde{\\Psi}_{t}\\|^{2}+\\frac{3\\alpha L^{2}\\|\\tilde{V}_{t K}\\|^{2}}{2\\gamma^{3}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad+\\displaystyle\\frac{N_{t}^{2}\\alpha L^{2}}{\\gamma}\\left(1+\\frac{3\\alpha L}{2\\gamma}\\right)\\operatorname*{sup}_{0\\leq i\\leq K-1}\\|\\hat{\\tilde{U}}_{t K+i}-\\tilde{U}_{t K}\\|^{2}\\qquad\\qquad\\mathrm{()}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "3. For any $p\\geq1$ , we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}M_{t,K}^{p}\\leq C\\exp(\\frac{p\\gamma\\alpha}{2})\\gamma^{\\frac{p}{2}}\\alpha^{\\frac{3p}{2}}(d+\\log K)^{\\frac{p}{2}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We refer to Section I.8 for the proof. Analogous to Section E, for any $p\\geq1$ wedefine $S_{2p}(V):=$ $\\begin{array}{r}{\\sum_{t=0}^{T-1}\\|\\tilde{V}_{t K}\\|^{2p},S_{2p}(\\nabla F):=\\sum_{t=0}^{T-1}\\|\\nabla F(\\tilde{U}_{t K})\\|^{2p}}\\end{array}$ The folowing lemma,proved in Setion I y, bounds the moments of these quantities. ", "page_idx": 27}, {"type": "text", "text": "Lemma 22. There exists a constant $c_{0}$ such that whenever $\\gamma\\alpha<1$ \uff0c $\\begin{array}{r}{\\frac{\\alpha L}{\\gamma}<c_{0}}\\end{array}$ . Then the following relationships hold: ", "page_idx": 27}, {"type": "text", "text": "1. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\big[\\mathbb{E}(S_{2}(V))^{p}\\big]^{\\frac{1}{p}}\\leq\\displaystyle\\frac{C}{\\gamma\\alpha}[\\mathbb{E}|\\tilde{V}_{0}|]^{2p}\\big]^{\\frac{1}{p}}+\\frac{6}{\\gamma^{2}}[\\mathbb{E}(S_{2}(\\nabla F))^{p}]^{\\frac{1}{p}}+C_{p}(T(d+\\log K)+\\frac{1}{\\gamma\\alpha})}\\\\ &{}&{\\qquad+\\,\\frac{L^{2}T^{1-\\frac{1}{p}}\\alpha^{2}}{\\gamma^{2}}[\\mathbb{E}S_{2p}(V)]^{\\frac{1}{p}}+\\frac{L^{2}T^{1-\\frac{1}{p}}\\alpha^{4}}{\\gamma^{2}}[\\mathbb{E}S_{2p}(\\nabla F)]^{\\frac{1}{p}}\\qquad\\qquad(2)^{\\frac{1}{p}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "2. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\big[\\mathbb{E}(S_{2}(\\nabla F))^{p}\\big]^{\\frac{1}{p}}\\leq\\frac{\\gamma}{\\alpha}[\\mathbb{E}|(F(\\Psi_{0})-F(\\Psi_{T}))^{+}|^{p}]^{\\frac{1}{p}}}\\\\ &{\\qquad\\qquad\\qquad+\\,\\frac{C L^{2}}{\\gamma^{2}}[\\mathbb{E}(S_{2}(V))^{p}]^{\\frac{1}{p}}+C_{p}\\left[L(d+\\log K)T+\\frac{\\gamma}{\\alpha}\\right]}\\\\ &{\\qquad\\qquad\\qquad+\\,C_{p}T^{1-\\frac{1}{p}}L^{2}\\alpha^{2}[\\mathbb{E}S_{2p}(V)]^{\\frac{1}{p}}+C_{p}T^{1-\\frac{1}{p}}L^{2}\\alpha^{4}[\\mathbb{E}S_{2p}(\\nabla F)]^{\\frac{1}{p}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Suppose additionally that $\\gamma\\geq C_{0}\\sqrt{L}$ for some large enough universal constant $C_{0}$ . Then, the bounds above imply the following inequalities: ", "page_idx": 27}, {"type": "text", "text": "1. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\big[\\mathbb{E}(S_{2}(V))^{p}\\big]^{\\frac{1}{p}}\\leq\\displaystyle\\frac{C}{\\gamma\\alpha}[\\mathbb{E}\\|\\tilde{V}_{0}\\|^{2p}]^{\\frac{1}{p}}+\\displaystyle\\frac{C}{\\gamma\\alpha}[\\mathbb{E}|(F(\\Psi_{0})-F(\\Psi_{T}))^{+}|^{p}]^{\\frac{1}{p}}}\\\\ &{\\qquad\\qquad\\qquad+\\,C_{p}(T(d+\\log K)+\\frac{1}{\\gamma\\alpha})+\\displaystyle\\frac{L^{2}T^{1-\\frac{1}{p}}\\alpha^{2}}{\\gamma^{2}}[\\mathbb{E}S_{2p}(V)]^{\\frac{1}{p}}}\\\\ &{\\qquad\\qquad\\qquad+\\,\\displaystyle\\frac{L^{2}T^{1-\\frac{1}{p}}\\alpha^{4}}{\\gamma^{2}}[\\mathbb{E}S_{2p}(\\nabla F)]^{\\frac{1}{p}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "2. ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\big[\\mathbb{E}(S_{2}(\\nabla F))^{p}\\big]^{\\frac{1}{p}}\\leq\\displaystyle\\frac{C\\gamma}{\\alpha}[\\mathbb{E}\\|\\tilde{V}_{0}\\|^{2p}]^{\\frac{1}{p}}+\\displaystyle\\frac{C\\gamma}{\\alpha}[\\mathbb{E}|(F(\\Psi_{0})-F(\\Psi_{T}))^{+}|^{p}]^{\\frac{1}{p}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,C_{p}\\left[L(d+\\log K)T+\\frac{\\gamma}{\\alpha}\\right]}\\\\ &{\\qquad\\qquad\\qquad+\\,C_{p}T^{1-\\frac{1}{p}}L^{2}\\alpha^{2}[\\mathbb{E}S_{2p}(V)]^{\\frac{1}{p}}+C_{p}T^{1-\\frac{1}{p}}L^{2}\\alpha^{4}[\\mathbb{E}S_{2p}(\\nabla F)]^{\\frac{1}{p}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The following lemma gives a growth bound for Overdamped Langevin dynamics. We give the proof in Section I.10. ", "page_idx": 28}, {"type": "text", "text": "Lemma 23. Let $s,t\\in\\mathbb{N}\\cup\\{0\\}$ suchthat $s>t$ There exists a constant $c_{p}>0$ such that whenever L(st)\u2264cp, \u03b1(s -t)\u2264Cp and aL(s-t)\u2264cp,tefolowing statements hold: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{sup}_{t\\leq h\\leq s}\\left[\\mathbb{E}\\|\\tilde{U}_{h k}-\\tilde{U}_{t K}\\|^{p}\\right]^{\\frac{1}{p}}\\leq8\\alpha(s-t)\\left[\\mathbb{E}\\|\\tilde{V}_{t K}\\|^{p}\\right]^{\\frac{1}{p}}+\\frac{8\\alpha(s-t)}{\\gamma}\\mathbb{E}\\left[\\|\\nabla F(\\tilde{U}_{t K})\\|^{p}\\right]^{\\frac{1}{p}}}\\\\ &{\\quad+\\displaystyle C_{p}\\sqrt{\\frac{d+\\log K}{L}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{t\\leq h\\leq s}{\\operatorname*{sup}}\\left[\\mathbb{E}\\|\\tilde{V}_{h k}-\\tilde{V}_{t K}\\|^{p}\\right]^{\\frac{1}{p}}\\leq8\\gamma\\alpha(s-t)\\left[\\mathbb{E}\\|\\tilde{V}_{t K}\\|^{p}\\right]^{\\frac{1}{p}}+8\\alpha(s-t)\\mathbb{E}\\left[\\|\\nabla F(\\tilde{U}_{t K})\\|^{p}\\right]^{\\frac{1}{p}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\left.C_{p}\\gamma\\sqrt{\\frac{d+\\log K}{L}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We combine Lemma 23 with Lemma 26 to conclude: ", "page_idx": 28}, {"type": "text", "text": "Lemma 24. Let $p\\geq1$ be given. There exists $c_{p}>0$ such that for any $N$ which satisfies the following conditions: ", "page_idx": 28}, {"type": "text", "text": "1. $N$ is an integer power of 2 and $N\\leq T$   \n2. $\\begin{array}{r}{\\frac{\\alpha L N}{\\gamma}\\leq c_{p}}\\end{array}$ \uff0c $\\alpha\\gamma N\\leq c_{p}$ and $\\alpha^{2}L N\\leq c_{p}$ ", "page_idx": 28}, {"type": "text", "text": "The following satements hold: ", "page_idx": 28}, {"type": "text", "text": "1. ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}S_{2p}(\\nabla F)\\le C_{p}(L\\alpha N)^{2p}\\mathbb{E}S_{2p}(V)+C_{p}T L^{p}\\left(d+\\log K\\right)^{p}+\\frac{2^{p}}{N^{p-1}}\\mathbb{E}(S_{2}(\\nabla F))^{p}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "2. ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}S_{2p}(V)\\leq C_{p}(\\alpha N)^{2p}\\mathbb{E}S_{2p}(\\nabla F)+T\\left[\\frac{\\gamma^{2}}{L}(d+\\log K)\\right]^{p}+\\frac{2^{p}}{N^{p-1}}\\mathbb{E}(S_{2}(V))^{p}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof.We consider $a_{t}:=\\nabla F(\\tilde{U}_{(t-1)K})$ $b_{t}:=\\tilde{V}_{(t-1)K}$ . Let $N$ satisfy the condition in the Lemma. Since $T$ is a power of 2, clearly, $N$ divides $T$ . Let $\\mathcal{T}_{k}\\overset{\\cdot}{=}\\{(k-1)N+1,\\ldots,k N\\}$ be as defined in Lemma 26. Consider the following upper bound derived using the results of Lemma 23 ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{k=1}^{T/N}\\displaystyle\\sum_{j,j^{\\prime}\\in\\mathcal{T}_{k}}\\mathbb{E}\\|a_{j}-a_{j}^{\\prime}\\|^{2p}\\leq C_{p}L^{2p}\\alpha^{2p}N^{2p+1}\\mathbb{E}S_{2p}(V)+C_{p}\\frac{L^{2p}\\alpha^{2p}N^{2p+1}}{\\gamma^{2p}}\\mathbb{E}S_{2p}(\\nabla F)}&{}\\\\ {\\displaystyle\\qquad\\overset{j^{\\prime}>j}{j^{\\prime}>j}}&{\\quad+\\,C_{p}N T L^{p}\\left(d+\\log K\\right)^{p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Applying Lemma 26, we conclude: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}S_{2p}(\\nabla F)\\le C_{p}(L\\alpha N)^{2p}\\mathbb{E}S_{2p}(V)+C_{p}T L^{p}\\left(d+\\log K\\right)^{p}+\\frac{2^{p}}{N^{p-1}}\\mathbb{E}(S_{2}(\\nabla F))^{p}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Similarly, we have: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{k=1}^{T/N}\\sum_{j,j^{\\prime}\\in T_{k}}\\mathbb{E}\\|b_{j}-b_{j}^{\\prime}\\|^{2}\\leq C_{p}\\gamma^{2p}\\alpha^{2p}N^{2p+1}\\mathbb{E}S_{2p}(V)+C_{p}\\alpha^{2p}N^{2p+1}\\mathbb{E}S_{2p}(\\nabla F)}}\\\\ &{}&{j^{\\prime}>j}\\\\ &{}&{+\\,C_{p}N T\\frac{\\gamma^{2p}}{L^{p}}\\left(d+\\log K\\right)^{p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Applying Lemma 26, we conclude: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}S_{2p}(V)\\leq C_{p}(\\alpha N)^{2p}\\mathbb{E}S_{2p}(\\nabla F)+T\\left[\\frac{\\gamma^{2}}{L}(d+\\log K)\\right]^{p}+\\frac{2^{p}}{N^{p-1}}\\mathbb{E}(S_{2}(V))^{p}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Combining Lemmas 24 and 22 gives the following theorem. ", "page_idx": 29}, {"type": "text", "text": "Theorem 4. Fix $p\\geq1$ .There exist constants $C_{p},c_{p},\\bar{c}_{p}\\;>\\;0$ such that whenever: $\\gamma\\,\\geq\\,C_{p}{\\sqrt{L}}$ $\\alpha\\gamma<c_{p}$ p, the fllowing results hold: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle S_{2p}(\\nabla F)\\leq C_{p}\\frac{\\gamma^{2p-1}}{\\alpha}\\left[\\mathbb{E}\\|\\tilde{V}_{0}\\|^{2p}+\\mathbb{E}|(F(\\Psi_{0})-F(\\Psi_{T}))^{+}|^{p}+1\\right]+}\\\\ {\\displaystyle C_{p}T\\left[\\frac{\\gamma^{4p}}{L^{p}}+(\\gamma\\alpha T)^{p-1}\\gamma^{2p}\\right](d+\\log K)^{p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{2p}(V)\\leq C_{p}\\frac{1}{\\gamma\\alpha}\\left[\\mathbb{E}\\|\\tilde{V}_{0}\\|^{2p}+\\mathbb{E}|(F(\\Psi_{0})-F(\\Psi_{T}))^{+}|^{p}+1\\right]}\\\\ &{\\qquad\\qquad+\\,C_{p}T\\left[\\frac{\\gamma^{2p}}{L^{p}}+(\\gamma\\alpha T)^{p-1}\\right](d+\\log K)^{p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "F.2Bounding the Bias: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Now, consider the following term in the statement of Theorem 1: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{B_{t K+i}:=\\Gamma_{\\frac{\\alpha}{K}}^{-1}G_{\\frac{\\alpha}{K}}[b(\\hat{\\tilde{X}}_{t K+i})-b(\\tilde{X}_{t K+i})]}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Using Lemma 18, and under the conditions of Theorem 4 holding with $p=1$ wehave: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|B_{t K+i}\\|^{2}\\leq C\\frac{\\alpha L^{2}}{K\\gamma}\\underset{0\\leq i\\leq K-1}{\\operatorname*{sup}}\\|\\hat{\\tilde{U}}_{t K+i}-\\tilde{U}_{t K+i}\\|^{2}}\\\\ &{\\qquad\\qquad\\leq C\\frac{\\alpha^{5}L^{4}N_{t}^{2}}{K\\gamma}\\underset{0\\leq i\\leq K-1}{\\operatorname*{sup}}\\|\\hat{\\tilde{U}}_{t K+i}-\\tilde{U}_{t K}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "In the second step, we have used item 2 of Lemma 19 along with the bounds in Lemma 18. ", "page_idx": 29}, {"type": "text", "text": "Applying item 1 from Lemma 21 and Theorem 4 we conclude: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=0}^{T-1}\\displaystyle\\sum_{i=0}^{K-1}\\mathbb{E}\\big\\lVert B_{t K+i}\\big\\rVert^{2}\\le\\frac{C\\alpha^{5}L^{\\frac{4}{2}}}{\\gamma}\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}N_{t}^{2}\\mathbb{E}\\displaystyle\\operatorname*{sup}_{0\\le i\\le K-1}^{0}\\lVert\\hat{R}_{t K+i}-\\bar{U}_{t K}\\rVert^{2}}\\\\ &{\\le\\displaystyle\\frac{C\\alpha^{5}L^{4}}{\\gamma}\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}\\displaystyle\\operatorname*{sup}_{0\\le i\\le K-1}^{0}\\lVert\\hat{U}_{t K+i}-\\bar{U}_{t K}\\rVert^{2}}\\\\ &{\\le\\displaystyle\\frac{C\\alpha^{5}L^{4}}{\\gamma}\\displaystyle\\sum_{t=0}^{T-1}\\Big[\\alpha^{2}\\mathbb{E}\\|\\tilde{V}_{t K}\\|^{2}+\\alpha^{4}\\mathbb{E}\\|\\nabla F(\\tilde{U}_{t K})\\|^{2}+\\mathbb{E}M_{t,K}^{2}\\Big]}\\\\ &{\\le\\displaystyle\\frac{C\\alpha^{5}L^{4}}{\\gamma}\\left[\\alpha^{2}\\mathbb{E}S_{2}(V)+\\alpha^{4}\\mathbb{E}S_{2}(\\nabla F)+T\\gamma\\alpha^{3}(d+\\log K)\\right]}\\\\ &{\\le C\\alpha^{7}L^{3}\\gamma T(d+\\log K)+\\displaystyle\\frac{C\\alpha^{6}L^{4}}{\\gamma^{2}}\\left[\\mathbb{E}\\|\\tilde{V}_{0}\\|^{2}+\\mathbb{E}(F(\\Psi_{0})-F(\\Psi_{T}))^{+}+1\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "In the first step we have used the fact that $N_{t}$ is independent of $\\mathrm{sup}_{0\\leq i\\leq K-1}\\|\\hat{\\tilde{U}}_{t K+i}-\\tilde{U}_{t K}\\|^{2}$ and the second step follows from the fact that $\\mathbb{E}N_{t}^{2}\\,\\leq\\,2$ . In the third step we have used item 1 from Lemma 21. In the fourth step, we have used item 3 of Lemma 21. In the final step we have used Theorem 4. ", "page_idx": 30}, {"type": "text", "text": "F.3  Bounding the Variance: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Now consider $\\beta_{t K+i}\\;=\\;\\|K\\Gamma_{\\frac{\\alpha}{K}}^{-1}G_{\\frac{\\alpha}{K}}[b(\\hat{\\tilde{X}}_{t K+i})\\:-\\:b(\\tilde{X}_{t K})]\\|$ .Using Lemma 18 and item 1 of Lemma 21, we have: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta_{t K+i}^{2}\\le\\displaystyle\\frac{C K\\alpha L^{2}}{\\gamma}\\operatorname*{sup}_{0\\le i\\le K-1}\\|\\hat{\\tilde{U}}_{t K+i}-\\tilde{U}_{t K}\\|^{2}}\\\\ &{\\le\\displaystyle\\frac{C K\\alpha L^{2}}{\\gamma}\\left[\\alpha^{2}\\|\\tilde{V}_{t K}\\|^{2}+\\alpha^{4}\\|\\nabla F(\\tilde{U}_{t K})\\|^{2}+M_{t,K}^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, we note that for any $p\\geq1$ , under the assumptions of Theorem 4, we must have: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=0}^{T-1}\\displaystyle\\sum_{i=0}^{K-1}\\mathbb{E}\\beta_{t K+i}^{2p}\\le C_{p}\\displaystyle\\frac{K^{p+1}\\alpha^{p}L^{2p}}{\\gamma^{p}}\\sum_{t=0}^{T-1}\\Big[\\alpha^{2p}\\mathbb{E}\\|\\tilde{V}_{t K}\\|^{2p}+\\alpha^{4p}\\mathbb{E}\\|\\nabla F(\\tilde{U}_{t K})\\|^{2p}+\\mathbb{E}M_{t,K}^{2p}\\Big]}\\\\ &{\\le C_{p}\\displaystyle\\frac{K^{p+1}\\alpha^{p}L^{2p}}{\\gamma^{p}}\\left[\\alpha^{2p}\\mathbb{E}\\mathcal{S}_{2p}(V)+\\alpha^{4p}\\mathbb{E}\\mathcal{S}_{2p}(\\nabla F)+T\\gamma^{p}\\alpha^{3p}(d+\\log K)^{p}\\right]}\\\\ &{\\le C_{p}\\displaystyle\\frac{K^{p+1}\\alpha^{3p-1}L^{2p}}{\\gamma^{p+1}}\\left[\\mathbb{E}\\|\\tilde{V}_{0}\\|^{2p}+\\mathbb{E}|(F(\\Psi_{0})-F(\\Psi_{T}))^{+}|^{p}+1\\right]}\\\\ &{\\phantom{x x x x x}+C_{p}\\displaystyle\\frac{K^{p+1}\\alpha^{3p}L^{2p}T}{\\gamma^{p}}\\left(\\frac{\\gamma^{2p}}{L^{p}}+(\\gamma\\alpha T)^{p-1}\\right)(d+\\log K)^{p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "F.4  Finishing the Proof ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "For $p\\geq1$ , we define $\\begin{array}{r}{\\Delta^{(p)}=\\mathbb{E}(F(U_{0}+\\frac{V_{0}}{\\gamma})-F(\\mathbf{x^{*}}))^{p}+\\mathbb{E}\\|V_{0}\\|^{2p}+1}\\end{array}$ By $\\mathsf{L H S}\\lesssim\\mathsf{R H S}$ we denote $L\\mathsf{H S}\\leq C.\\mathsf{R H S}$ for some universal positive constant $C$ . We now apply Theorem 1 (with $r=7$ )along with the bounds in Equations (57) and (59): ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{K}\\{\\left(\\lambda\\mathrm{an}(X_{t}^{\\uparrow})\\mathrm{as}_{t}\\varepsilon x\\right)\\}\\mathbf{l}[\\lambda\\mathrm{an}(X_{t}\\mathrm{b})\\mathrm{a}_{t}\\varepsilon z\\varepsilon]}\\\\ &{\\le\\mathbb{E}\\|\\|B_{\\alpha\\kappa+1}\\|^{2}+C\\mathbb{E}\\left[\\frac{\\hat{\\beta}_{t}^{3}K+i}{K^{2}}+\\frac{\\beta_{t}^{3}\\alpha+i}{K^{3}}+\\frac{\\beta_{t}^{6}K+i}{K^{2}}+\\frac{\\beta_{t}^{2}K+i}{K}\\right]}\\\\ &{\\lesssim\\frac{C\\alpha^{6}L^{4}}{\\gamma^{2}}\\Delta^{(1)}+\\frac{K\\alpha^{5}L^{4}}{\\gamma^{3}}\\Delta^{(2)}+\\frac{K^{2}\\alpha^{5}L^{6}}{\\gamma^{4}}\\Delta^{(3)}+\\frac{K^{3}\\alpha^{14}L^{10}}{\\gamma^{6}}\\Delta^{(5)}+\\frac{K^{7}\\alpha^{20}L^{14}}{\\gamma^{8}}\\Delta^{(7)}}\\\\ &{\\quad+C\\alpha^{7}L^{3}\\gamma T(d+\\log K)+K\\alpha^{6}\\gamma^{7}L^{2}T(d+\\log K)^{2}+\\frac{K\\alpha^{7}L^{4}T^{2}}{\\gamma}(d+\\log K)^{2}}\\\\ &{\\quad+K^{2}\\alpha^{5}L^{3}\\gamma^{3}T(d+\\log K)^{3}+\\frac{K^{2}\\alpha^{11}L^{7}T^{3}}{\\gamma}(d+\\log K)^{3}+K^{3}\\alpha^{15}L^{5}\\gamma^{5}T(d+\\log K)^{5}}\\\\ &{\\quad+\\frac{K^{3}\\alpha^{10}L^{10}T^{5}}{\\gamma}(d+\\log K)^{5}+K^{7\\alpha^{21}L^{7}}\\gamma^{7}T(d+\\log K)^{7}}\\\\ &{\\quad+\\frac{K^{7}\\alpha^{27}L^{14}T^{7}}{\\gamma}(d+\\log K)^{7}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "G  Convergence Under Isoperimetry Assumptions ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We now prove the results concerning convergence under the assumption that $\\pi^{\\star}$ satisfies the Logarithmic Sobolev Inequalities (LSI). We first define the LSI by following the discussion in [46] and refer to this work for further details. ", "page_idx": 31}, {"type": "text", "text": "Definition 1. We say that a measure $\\pi^{\\star}$ over $\\mathbb{R}^{d}$ satsifies $\\lambda$ -LSI for some $\\lambda>0$ if for every smooth $g:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ such that $\\mathbb{E}_{X\\sim\\pi^{\\star}}g^{2}(X)<\\infty$ the following inequality is satisfed: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi^{\\star}}g^{2}\\log g^{2}-\\mathbb{E}_{\\pi^{\\star}}g^{2}\\log\\mathbb{E}_{\\pi^{\\star}}g^{2}\\leq\\frac{2}{\\lambda}\\mathbb{E}_{\\pi^{\\star}}\\|\\nabla g\\|^{2}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We note that whenever the density $\\pi^{\\star}({\\bf x})=e^{-{F({\\bf x})}}$ and $F$ .is $\\lambda$ strongly convex, then $\\pi^{\\star}$ satisfies the $\\lambda$ -LSI. It is well known that $\\lambda$ -LSI implies the Poincare inequality (PI) with parameter $\\lambda$ defined below: ", "page_idx": 31}, {"type": "text", "text": "Definition 2. We say that a measure $\\pi^{\\star}$ over $\\mathbb{R}^{d}$ satsifies $\\lambda$ -PIfor some $\\lambda>0,$ if for every smooth $g:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ such that $\\mathbb{E}_{X\\sim\\pi^{\\star}}g^{2}(X)<\\infty$ the following inequality is satisfed: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi^{\\star}}g^{2}-(\\mathbb{E}_{\\pi^{\\star}}g)^{2}\\le\\frac1\\lambda\\mathbb{E}_{\\pi^{\\star}}\\|\\nabla g\\|^{2}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We note the following useful lemma: ", "page_idx": 31}, {"type": "text", "text": "Lemma 25. Let $\\pi^{\\star}({\\bf x})\\propto\\exp(-F({\\bf x}))$ satisfy $\\lambda$ PI or $\\lambda$ -LSI and let $F$ be $L$ -smooth. Then, $L\\geq\\lambda.$ ", "page_idx": 31}, {"type": "text", "text": "Proof. Let $X,X^{\\prime}\\;\\sim\\;\\pi^{\\star}$ be i.i.d. Applying integration by parts, we have: $\\mathbb{E}\\nabla F(X)\\,=\\,0$ and $\\mathbb{E}\\langle X,\\nabla F(X)\\rangle=d$ . Note that LSI implies $\\mathrm{PI}$ and under $\\lambda$ -PI, we must have for any unit norm vector $v\\in\\mathbb{R}^{d}$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathsf{v a r}(\\langle X,v\\rangle)\\leq\\frac{1}{\\lambda}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Let $\\Sigma$ be the covariance of $X$ (it exists due to the assumption of PI). Therefore, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{L}=\\frac{1}{2d L}\\mathbb{E}\\langle X-X^{\\prime},\\nabla F(X)-\\nabla F(X^{\\prime})\\rangle}\\\\ {\\displaystyle\\leq\\frac{1}{2d}\\mathbb{E}\\|X-X^{\\prime}\\|^{2}=\\frac{\\mathsf{T r}(\\Sigma)}{d}\\leq\\frac{1}{\\lambda}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "In the second step we have used the fact that whenever $F$ is $L$ smooth, then $F(x)\\,-\\,F(y)\\,\\leq$ $\\begin{array}{r}{\\langle\\nabla F(y),x-y\\rangle+\\frac{L}{2}\\|x-y\\|^{2}}\\end{array}$ for all $x,y\\in\\mathbb{R}^{d}$ \u53e3 ", "page_idx": 31}, {"type": "text", "text": "We now state [45, Theorem 1] (adapting to our notation) which gives convergence guarantees for OLMC under the assumption of $\\lambda$ -LSI. ", "page_idx": 32}, {"type": "text", "text": "Theorem 5. Suppose $\\pi^{*}(\\mathbf{x})\\propto\\exp(-F(\\mathbf{x}))$ satisfies the $\\lambda$ -LSI and that $F$ . $L$ -smooth. Consider OLMC with set size $\\eta$ which satsifies $\\begin{array}{r}{0<\\eta\\le\\frac{\\lambda}{2L^{2}}}\\end{array}$ . Then, LMC satisfies: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathsf{K L}\\left(\\mathsf{L a w}(X_{N})\\big|\\big|\\pi^{\\star}\\right)\\le e^{-\\lambda\\eta N}\\mathsf{K L}\\left(\\mathsf{L a w}(X_{0})\\big|\\big|\\pi^{\\star}\\right)+\\frac{8d L^{2}\\eta}{\\lambda}\\,.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Where $X_{0},\\ldots,X_{N}$ are the iterates of OLMC. ", "page_idx": 32}, {"type": "text", "text": "Whenever $\\epsilon\\,\\leq\\,1$ , we take $\\begin{array}{r}{\\frac{\\alpha}{K}\\,=\\,\\frac{\\lambda\\epsilon^{2}}{16d L^{2}}}\\end{array}$ $\\begin{array}{r}{T\\,=\\,\\frac{1}{\\alpha\\lambda}\\log\\left(\\frac{2\\mathsf{K L}\\left(\\mathsf{L a w}(X_{0})\\Big|\\Big|\\pi^{\\star}\\right)}{\\epsilon^{2}}\\right)}\\end{array}$ With the suggested initialization in [45] i.e $\\begin{array}{r}{X_{0}\\sim\\mathcal{N}(\\mathbf{x},\\frac{\\mathbf{I}}{L})}\\end{array}$ for any stationary point $\\mathbf{x}$ f $F$ we have $\\mathsf{K L}\\left(\\mathsf{L a w}(X_{0})\\middle|\\middle|\\pi^{\\star}\\right)\\leq$ $\\tilde{O}(d)$ . Now, take $\\begin{array}{r}{\\alpha=c_{0}\\operatorname*{min}\\left(\\frac{\\epsilon\\sqrt{\\lambda}}{L^{\\frac{3}{2}}d^{\\frac{3}{4}}},\\frac{\\lambda^{\\frac{7}{27}}\\epsilon^{\\frac{2}{27}}}{L^{\\frac{20}{27}}d^{\\frac{7}{54}}}\\right)\\!,T=\\tilde{O}(\\frac{C_{1}}{\\lambda\\alpha})\\;\\mathrm{and}\\;K=\\frac{C_{2}}{\\alpha L\\sqrt{d}}.}\\end{array}$ ", "page_idx": 32}, {"type": "text", "text": "Let $X_{0},\\ldots,X_{T K}$ be the iterate of OLMC with step-size $\\frac{\\alpha}{K}$ with the parameters as chosen above. Applying Theorem 5, we conclude: $\\begin{array}{r}{\\mathsf{K L}\\left(\\mathsf{L a w}(X_{T K})\\Big|\\Big|\\pi^{\\star}\\right)\\le\\frac{\\epsilon^{2}}{8}}\\end{array}$   \nLet $X_{0}^{P},\\ldots,X_{T}^{P}$ be the iterates of $\\mathsf{P S}(A,G,\\Gamma,\\alpha,K)$ With $A,G,\\Gamma$ corresponding to OLMC. Applying Theorem 3 (with the explicit lower order terms given in Equation (40)), we conclude that: $\\begin{array}{r}{\\mathsf{K L}\\left(\\mathsf{L a w}(X_{T}^{P})\\big|\\big|\\mathsf{L a w}(X_{T K})\\right)\\leq\\frac{\\epsilon^{2}}{8}.}\\end{array}$ ", "page_idx": 32}, {"type": "text", "text": "Now, by Pinsker's inequality and the triangle inequality for TV distance, we have: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathsf{T V}(\\mathsf{L a w}(X_{T}^{P}),\\pi^{\\star})\\le\\sqrt{2\\mathsf{K L}\\left(\\mathsf{L a w}(X_{T}^{P})\\big|\\big|\\mathsf{L a w}(X_{T K})\\right)}+\\sqrt{2\\mathsf{K L}\\left(\\mathsf{L a w}(X_{T K})\\big|\\big|\\pi^{\\star}\\right)}\\le\\epsilon\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "This proves the result in Table 1 for OLMC. ", "page_idx": 32}, {"type": "text", "text": "We now consider the analogous result for ULMC below. The following result is a re-statement of [53, Theorem 7]. We note that $C_{{\\mathsf{L}}{\\mathsf{S}}{\\mathsf{I}}}$ in this work is $\\frac{1}{\\lambda}$ in our work. ", "page_idx": 32}, {"type": "text", "text": "Theorem 6. Let $X_{0},\\ldots,X_{N}$ be the iterates of ULMC with step-size $\\eta$ .We make the following assumptions: ", "page_idx": 32}, {"type": "text", "text": "1. $F$ is $L$ -smooth and that $\\pi^{\\star}$ satisfies $\\lambda$ -LSI.   \n2. Suppose x be any stationary point of $F$ Initialize $X_{0}$ such that $U_{0},V_{0}$ are independent with $U_{0}^{\\'}\\sim\\mathcal{N}(\\mathbf{x},\\zeta\\mathbf{I})$ $\\zeta$ as given in $[53J]$ and $V_{0}\\sim\\mathcal{N}(0,\\mathbf{I})$ ", "page_idx": 32}, {"type": "equation", "text": "$$\nF(\\mathbf{x})-F(\\mathbf{x}^{*})=\\tilde{O}(d)\\;a n d\\;\\mathbb{E}_{U,V\\sim\\pi^{\\star}}\\|U\\|\\leq\\tilde{O}(d).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Let $\\epsilon\\leq1$ Then, we let $\\begin{array}{r}{\\eta=\\tilde{\\Theta}(\\frac{\\epsilon\\sqrt{\\lambda}}{L\\sqrt{d}}),\\,c\\sqrt{L}\\le\\gamma\\le C\\sqrt{L},\\,T=\\tilde{\\Theta}(\\frac{L^{\\frac{3}{2}}\\sqrt{d}}{\\lambda^{\\frac{3}{2}}\\epsilon})\\,t h e n,}\\end{array}$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathsf{T V}(\\mathsf{L a w}(U_{N}),\\pi^{\\star})\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Let $X_{0},\\ldots,X_{T K}$ be the iterates of ULMC with step-size $\\frac{\\alpha}{K}$ . We take: $\\alpha\\;=\\;\\tilde{\\Theta}(\\sqrt{\\frac{\\epsilon}{L}}(\\frac{\\lambda}{L d})^{\\frac{5}{12}})$ $\\begin{array}{r}{K\\ =\\ \\tilde{\\Theta}(\\frac{1}{\\alpha}\\frac{\\lambda^{\\frac{1}{3}}}{d^{\\frac{1}{3}}L^{\\frac{5}{6}}})}\\end{array}$ and $\\begin{array}{r}{N\\ =\\ \\tilde{\\Theta}(\\frac{\\sqrt{L}}{\\lambda\\alpha})}\\end{array}$ .We now consider $X_{0}^{P},\\ldots,X_{T}^{P}$ to be the iterates of $\\mathsf{P S}(A,G,\\Gamma,\\alpha,K)$ with $A,G,\\Gamma$ corresponding to ULMC, such that $X_{0}^{P}$ and $F$ satisfy the same assumptions in Theorem 6. Applying Theorem 3 along with Pinsker's inequality, we conclude that: ", "page_idx": 32}, {"type": "equation", "text": "$$\n{\\sf T V}(\\mathsf{L a w}(X_{T}^{(P)}),\\mathsf{L a w}(X_{T K}))\\leq\\frac{\\epsilon}{2}\\,.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Now, using Theorem 6, along with the triangle inequality for TV, we conclude: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathsf{T V}(\\mathsf{L a w}(X_{T}^{(P)}),\\pi^{\\star})\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "This proves the bounds for ULMC in Table 1. ", "page_idx": 32}, {"type": "text", "text": "H Some Technical Results ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "We state the following technical lemma which will be proved in Section I.3. ", "page_idx": 33}, {"type": "text", "text": "Lemma 26. Let $a_{1},\\dots,a_{T}$ be random vectors in $\\mathbb{R}^{d}$ \uff1aConsider the partitioning $\\{1,\\dots,T\\}\\,=$ $\\cup_{k=1}^{\\lceil T/N\\rceil}\\mathcal{T}_{k}$ Wwhere $\\mathcal{T}_{k}:=\\{(k-1)N+1,\\ldots,\\operatorname*{min}(k N,T)\\}$ Assmethat $N$ divides $T$ and $T\\geq N$ Then, we have: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{T}\\mathbb{E}\\|a_{j}\\|^{2p}\\leq{\\frac{2^{p}}{N}}\\sum_{k=1}^{\\frac{T}{N}}\\sum_{\\substack{j\\in{\\mathcal{T}}_{k}}}\\sum_{{j^{\\prime}}\\in{\\mathcal{T}}_{k}}\\mathbb{E}\\|a_{j}-a_{j^{\\prime}}\\|^{2p}+\\left({\\frac{2}{N}}\\right)^{p-1}\\mathbb{E}(\\sum_{j=1}^{T}\\|a_{j}\\|^{2})^{p}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Lemma 27. Let $Z_{1},\\L...,Z_{T}$ be i.i.d. standard Gaussian random variables, and a sequence of random vectors $g_{1},\\ldots,g_{T}$ suchthat $g_{t}$ is independent of $Z_{t},\\dots,Z_{T}$ Then, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}|\\sum_{s=1}^{T}\\langle g_{s},Z_{s}\\rangle|^{p}\\leq C_{p}\\sqrt{\\mathbb{E}(\\sum_{s=1}^{T}\\|g_{s}\\|^{2})^{p}}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. Let $\\lambda\\in\\mathbb R$ . Consider the random variable $\\begin{array}{r}{M_{t}=\\exp(\\sum_{s=1}^{t}\\lambda\\langle g_{s},Z_{s}\\rangle-\\frac{\\lambda^{2}\\|g_{s}\\|^{2}}{2})}\\end{array}$ $M_{t}$ is a super martingale and hence, we have: $\\mathbb{E}[M_{T}]\\leq1$ . Applying the Chernoff bound, we conclude that for anly $\\lambda,\\lambda>0$ we must have: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\mathbb{P}(|\\sum_{s=1}^{T}\\langle g_{s},Z_{s}\\rangle|>\\frac{\\lambda}{2}\\sum_{s=1}^{T}\\|g_{s}\\|^{2}+\\frac{h}{\\lambda})\\le2\\exp(-h)\\,.}\\\\ {\\displaystyle\\implies\\mathbb{P}(|\\sum_{s=1}^{T}\\langle g_{s},Z_{s}\\rangle|^{p}>C_{p}\\lambda^{p}(\\sum_{s=1}^{T}\\|g_{s}\\|^{2})^{p}+h)\\le2\\exp(-C_{p}\\lambda h^{\\frac{1}{p}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Now, for any variable $X$ \uff0c $\\mathbb{P}(X^{+}>h)\\,=\\,\\mathbb{P}(X>h)$ whenever $h\\geq0$ . Now using the fact that: $\\begin{array}{r}{\\mathbb{E}X\\leq\\mathbb{E}X^{+}=\\int_{0}^{\\infty}\\mathbb{P}(X^{+}>h)d h}\\end{array}$ and taking $\\begin{array}{r}{X=|\\sum_{s=1}^{T}\\langle g_{s},Z_{s}\\rangle|^{p}-C_{p}\\lambda^{p}(\\sum_{s=1}^{T}||g_{s}||^{2})^{p}}\\end{array}$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}|\\sum_{s=1}^{T}\\langle g_{s},Z_{s}\\rangle|^{p}\\leq C_{p}\\lambda^{p}\\mathbb{E}(\\sum_{s=1}^{T}\\|g_{s}\\|^{2})^{p}+\\int_{0}^{\\infty}2\\exp(-c_{p}\\lambda h^{\\frac{1}{p}})d h\\leq C_{p}\\lambda^{p}\\mathbb{E}(\\sum_{s=1}^{T}\\|g_{s}\\|^{2})^{p}+\\frac{C_{p}}{\\lambda^{p}}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We conclude the result by choosing $\\lambda^{2p}=\\frac{1}{\\mathbb{E}(\\sum_{s=1}^{T}\\|g_{s}\\|^{2})^{p}}$ ", "page_idx": 33}, {"type": "text", "text": "Lemma 28. Let $K$ be any positive integer Consider $\\begin{array}{r}{G\\sim\\mathsf{B i n}(K,\\frac{1}{K})}\\end{array}$ . Then for any $p\\geq1$ wemust have: $\\mathbb{E}G^{p}\\leq C_{p}$ for some constant depending only on $p$ ", "page_idx": 33}, {"type": "text", "text": "Proof. The proof is simple for $K=1$ .Assume $K\\geq2$ . For any $0\\leq n\\leq K$ , let $p_{n}:=\\mathbb{P}(G=n)$ Then, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle n!p_{n}=(1-\\frac{1}{K})^{K-n}\\frac{K!}{K^{n}(K-n)!}}}\\\\ {{\\displaystyle\\leq(1-\\frac{1}{K})^{K-1}\\leq\\frac{2}{e}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "In the second step we have used the inequality $(1-{\\frac{1}{n}})^{n}\\leq{\\frac{1}{e}}$ and the fact that $K\\ge2$ ", "page_idx": 33}, {"type": "text", "text": "Note that $\\textstyle{\\frac{1}{e n!}}$ is the probability that the standard poisson random variableis $n$ (denote this by $p_{n}^{*}$ Therefore, we must have: $p_{n}\\leq2p_{n}^{*}$ . Thus, we must have: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}G^{p}\\leq\\sum_{n\\in\\mathbb{N}}2n^{p}p_{n}^{*}\\leq C_{p}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "1 Proofs of Technical Lemmas ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1.1  Proof of Lemma 12 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Proof. 1. This follows by applying the triangle inequality to the definition of $\\hat{\\tilde{X}}_{t K+i}$ 2. Consider: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|\\hat{\\tilde{X}}_{t K+i}-\\tilde{X}_{t K+i}\\|\\leq\\alpha\\displaystyle\\sum_{j=0}^{i-1}H_{t,j}\\|\\nabla F(\\hat{\\tilde{X}}_{t K+j})-\\nabla F(\\tilde{X}_{t K})\\|}}\\\\ &{\\leq\\displaystyle\\sum_{j=0}^{i-1}H_{t,j}L\\|\\hat{\\tilde{X}}_{t K+j}-\\tilde{X}_{t K}\\|}\\\\ &{\\leq\\alpha N_{t}L\\displaystyle\\sum_{0\\leq j<K-1}^{\\mathrm{up}}\\|\\hat{\\tilde{X}}_{t K+j}-\\tilde{X}_{t K}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "3. This follows by a direct application of Doob's inequality. ", "page_idx": 34}, {"type": "text", "text": "1.2 Proof of Lemma 13 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Proof. We will use $b()$ and $\\nabla F()$ interchangeably in the proof. Consider the update ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\tilde{X}_{(t+1)K}=\\tilde{X}_{t K}+\\alpha b(\\tilde{X}_{t K})+\\alpha\\Delta_{t}+\\sqrt{2\\alpha}\\bar{Y}_{t}\\,.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Where $\\begin{array}{r}{\\bar{Y}_{t}=\\frac{1}{\\sqrt{K}}\\sum_{i=0}^{K-1}Z_{t K+i}}\\end{array}$ and $\\begin{array}{r}{\\Delta_{t}=\\sum_{i=0}^{K-1}H_{i}(b(\\hat{\\tilde{X}}_{t K+i})-b(\\tilde{X}_{t K}))}\\end{array}$ Therefore, we have for any $s>t$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\tilde{X}_{s K}-\\tilde{X}_{t K}=\\sum_{h=t}^{s-1}\\alpha b(\\tilde{X}_{h K})+\\alpha\\Delta_{h}+\\sqrt{2\\alpha}\\tilde{Y}_{h}}}\\\\ &{=\\alpha(s-t)b(\\tilde{X}_{t K})+\\displaystyle\\sum_{h=t}^{s-1}\\alpha(b(\\tilde{X}_{h K})-b(\\tilde{X}_{t K}))+\\alpha\\Delta_{h}+\\sqrt{2\\alpha}\\tilde{Y}_{h}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "In this proof only, for any random variable $U$ ,welet $\\mathcal{M}_{p}(U)\\,:=\\,(\\mathbb{E}[\\|U\\|^{p}\\tilde{X}_{t K}])^{\\frac{1}{p}}$ . Using the triangle inequality for $\\mathcal{M}_{p}$ , we conclude: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle M_{p}(\\tilde{X}_{s K}-\\tilde{X}_{t K})\\leq\\alpha(s-t){\\mathcal{M}}_{p}(b(\\tilde{X}_{t K}))+\\sum_{h=t}^{s-1}[\\alpha{\\mathcal{M}}_{p}(b(\\tilde{X}_{h K})-b(\\tilde{X}_{t K}))+\\alpha{\\mathcal{M}}_{p}(\\Delta_{h})]}\\\\ {\\displaystyle}&{\\displaystyle\\qquad+\\,\\sqrt{2\\alpha}{\\mathcal{M}}_{p}(\\sum_{h=t}^{s-1}\\tilde{Y}_{h})}\\\\ {\\displaystyle}&{\\leq\\alpha(s-t)\\|b(\\tilde{X}_{t K})\\|+\\alpha\\sum_{h=t}^{s-1}\\Big[L M_{p}(\\tilde{X}_{h K}-\\tilde{X}_{t K})+{\\mathcal{M}}_{p}(\\Delta_{h})\\Big]}\\\\ {\\displaystyle}&{\\displaystyle\\quad+\\,C_{p}\\sqrt{2\\alpha d(s-t)}}\\end{array}\\quad\\alpha(\\ell-t)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Now, consider $\\begin{array}{r}{\\Delta_{h}=\\sum_{i=0}^{K-1}H_{i}(b(\\hat{\\tilde{X}}_{h K+i})-b(\\tilde{X}_{h K}))}\\end{array}$ . We must have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{M}_{p}(\\Delta_{h})\\leq\\mathcal{M}_{p}(\\displaystyle\\sum_{i=0}^{K-1}L H_{i}||\\hat{\\tilde{X}}_{h K+i}-\\tilde{X}_{h K}||)\\leq\\mathcal{M}_{p}(L N_{h}\\displaystyle\\operatorname*{sup}_{0\\leq i\\leq K-1}||\\hat{\\tilde{X}}_{h K+i}-\\tilde{X}_{h K}||)}\\\\ &{\\qquad\\qquad=L\\mathcal{M}_{p}(N_{h})\\mathcal{M}_{p}(\\displaystyle\\operatorname*{sup}_{0\\leq i\\leq K-1}||\\hat{\\tilde{X}}_{h K+i}-\\tilde{X}_{h K}||)}\\\\ &{\\qquad\\qquad\\leq L C_{p}\\alpha\\mathcal{M}_{p}(\\|b(\\tilde{X}_{h K})\\|)+L C_{p}\\mathcal{M}_{p}(M_{h,K})}\\\\ &{\\qquad\\qquad\\leq L C_{p}\\alpha\\|b(\\tilde{X}_{t K})\\|+L^{2}C_{p}\\alpha\\mathcal{M}_{p}(\\tilde{X}_{h K}-\\tilde{X}_{t K})+L C_{p}\\sqrt{\\alpha d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "In the second line, we have used the fact that $N_{h}$ is independent of $\\mathrm{sup}_{0\\leq i\\leq K-1}\\|\\hat{\\tilde{X}}_{h K+i}-\\tilde{X}_{h K}\\|$ and the fact that $\\mathcal{M}_{p}(N_{t})\\le C_{p}$ for some constant $C_{p}$ (Lemma 28 ). In the third step we have applied item 1 from Lemma 12. ", "page_idx": 35}, {"type": "text", "text": "Putting this back in Equation 65, we conclude: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{M_{p}(\\tilde{X}_{s K}-\\tilde{X}_{t K})\\leq\\alpha(s-t)(1+\\alpha L C_{p})\\|b(\\tilde{X}_{t K})\\|+\\alpha L(1+\\alpha L C_{p})\\displaystyle\\sum_{h=t}^{s-1}\\left[M_{p}(\\tilde{X}_{h K}-\\tilde{X}_{t K})\\right]}}\\\\ {{+\\left.C_{p}(L\\alpha^{\\frac{3}{2}}\\sqrt{d}(s-t)+\\sqrt{2\\alpha d(s-t)})\\right.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Therefore, we must have: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\underset{t\\leq h\\leq s}{\\operatorname*{sup}}\\mathcal{M}_{p}(\\tilde{X}_{h K}-\\tilde{X}_{t K})\\leq\\alpha L(s-t)(1+\\alpha L C_{p})\\left[\\frac{\\|b(\\tilde{X}_{t K})\\|}{L}+\\underset{t\\leq h\\leq s}{\\operatorname*{sup}}\\mathcal{M}_{p}(\\tilde{X}_{h K}-\\tilde{X}_{t K})\\right]}&\\\\ &{\\quad}&{\\quad\\quad t\\leq C_{p}(L\\sqrt{d}\\alpha^{\\frac{3}{2}}(s-t)+\\sqrt{2\\alpha d(s-t)})~~~~~~~~~~~~~~~~~~~~~(}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Now, supposing s - t \u2264 CpaL for some large enough constant $\\bar{C}_{p}$ which depends only on $p$ . This ensures that $\\begin{array}{r}{\\alpha L(s-t)(1+\\alpha L C_{p})\\leq\\frac{1}{2}}\\end{array}$ in the equation above. Thus, we have: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\leq h\\leq s}\\mathcal{M}_{p}(\\tilde{X}_{h K}-\\tilde{X}_{t K})\\leq3\\alpha L(s-t)\\frac{\\|b(\\tilde{X}_{t K})\\|}{L}+C_{p}\\sqrt{d\\alpha(s-t)}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "1.3Proof of Lemma 26 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Proof. $\\begin{array}{r}{\\bar{a}_{k}:=\\frac{1}{|\\mathcal{T}_{k}|}\\sum_{j\\in\\mathcal{T}_{k}}a_{j}}\\end{array}$ Consider the followingquanty: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{t=1}^{T}\\|a_{t}\\|^{2p}=\\sum_{k}\\sum_{j\\in{\\mathcal{T}_{k}}}\\|a_{j}\\|^{2p}}}\\\\ &{\\leq\\sum_{k=1}^{\\frac{T}{N}}\\sum_{j\\in{\\mathcal{T}_{k}}}(2^{p-1}\\|a_{j}-\\bar{a}_{k}\\|^{2p}+2^{p-1}\\|\\bar{a}_{k}\\|^{2p})}\\\\ &{=2^{p-1}\\displaystyle\\sum_{k}\\sum_{j\\in{\\mathcal{T}_{k}}}(\\|a_{j}-\\bar{a}_{k}\\|^{2p})+2^{p-1}N\\displaystyle\\sum_{k}\\|\\bar{a}_{k}\\|^{2p}}\\\\ &{\\leq2^{p-1}\\displaystyle\\sum_{k}\\sum_{j\\in{\\mathcal{T}_{k}}}(\\|a_{j}-\\bar{a}_{k}\\|^{2p})+2^{p-1}N(\\displaystyle\\sum_{k}\\|\\bar{a}_{k}\\|^{2})^{p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Now, by Jensen's inequality, we must have: $\\begin{array}{r}{\\|\\bar{a}_{k}\\|^{2}\\leq\\frac{1}{|T_{k}|}\\sum_{j\\in\\mathcal{T}_{k}}\\|a_{j}\\|^{2}}\\end{array}$ . Therefore, we conclude: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\|a_{t}\\|^{2p}\\leq2^{p-1}\\sum_{k}\\sum_{j\\in\\mathcal{T}_{k}}(\\|a_{j}-\\bar{a}_{k}\\|^{2p})+2^{p-1}N^{1-p}(\\sum_{j=1}^{T}\\|a_{j}\\|^{2})^{p}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Now, consider ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{j\\in\\mathcal{T}_{k}}\\mathbb{E}\\|a_{j}-\\bar{a}_{k}\\|^{2p}\\leq\\frac{1}{N}\\sum_{j\\in\\mathcal{T}_{k}}\\sum_{j^{\\prime}\\in\\mathcal{T}_{k}}\\mathbb{E}\\|a_{j}-a_{j^{\\prime}}\\|^{2p}}}\\\\ &{}&{\\qquad=\\frac{2}{N}\\sum_{j\\in\\mathcal{T}_{k}}\\sum_{j^{\\prime}>j}\\mathbb{E}\\|a_{j}-a_{j^{\\prime}}\\|^{2p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Using this with Equation (70), we conclude the statement of the lemma. ", "page_idx": 36}, {"type": "text", "text": "1.4Proof of Lemma 14 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Proof. We now apply Lemma 26 with $a_{t}:=b(\\tilde{X}_{(t-1)K})$ . Whenever $\\alpha L\\leq c_{p}$ for some small enough onstanatwefwe \u03b1LC-, 1 for a large enough constant $\\bar{C}_{2p}$ such that the result of Lemma 13 with $s-t\\leq N$ . Applying Lemma 13, we conclude: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{2}{N}\\sum_{j\\in\\mathcal{T}_{k}}\\sum_{j^{\\prime}>j}\\mathbb{E}\\|a_{j}-a_{j}^{\\prime}\\|^{2p}\\leq2\\sum_{j\\in\\mathcal{T}_{k}}\\mathbb{E}(3\\alpha L N\\|a_{j}\\|+C_{p}L\\sqrt{\\alpha d N})^{2p}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq C_{p}L^{2p}(\\alpha d)^{p}N^{p+1}+C_{p}(\\alpha L N)^{2p}\\sum_{j\\in\\mathcal{T}_{k}}\\mathbb{E}\\|a_{j}\\|^{2p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Applying Lemma 26 ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}\\|a_{t}\\|^{2p}\\leq2^{p-1}C_{p}L^{2p}(\\alpha d N)^{p}T+C_{p}(\\alpha L N)^{2p}\\sum_{k}\\sum_{j\\in{\\mathcal{T}}_{k}}\\mathbb{E}\\|a_{j}\\|^{2p}+2^{p-1}N^{1-p}(\\sum_{j=1}^{T}\\|a_{j}\\|^{2})^{p}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Note that whenever the constant $\\bar{C}_{2p}$ defining $N$ above is large enough, this implies ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}\\|a_{t}\\|^{2p}\\leq C_{p}L^{p}d^{p}T+C_{p}(\\alpha L)^{p-1}(\\sum_{j=1}^{T}\\|a_{j}\\|^{2})^{p}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "1.5 Proof of Lemma 15 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Proof. Since $\\nabla F$ is $L$ Lipschitz, we have: ", "page_idx": 36}, {"type": "equation", "text": "$$\nF(\\tilde{X}_{(t+1)K})-F(\\tilde{X}_{t K})\\leq\\langle\\nabla F(\\tilde{X}_{t K}),\\tilde{X}_{(t+1)K}-\\tilde{X}_{t K}\\rangle+\\frac{L}{2}\\|\\tilde{X}_{(t+1)K}-\\tilde{X}_{t K}\\|^{2}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Also note that: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\tilde{X}_{(t+1)K}=\\tilde{X}_{t K}-\\alpha\\nabla F(\\tilde{X}_{t K})+\\alpha\\Delta_{t}+\\sqrt{2\\alpha}\\bar{Y}_{t}\\,.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Where $\\begin{array}{r}{\\bar{Y}_{t}=\\frac{1}{\\sqrt{K}}\\sum_{i=0}^{K-1}Z_{t K+i}\\;\\mathrm{and}\\;\\Delta_{t}=-\\sum_{i=0}^{K-1}H_{t,i}(\\nabla F(\\hat{\\tilde{X}}_{t K+i})-\\nabla F(\\tilde{X}_{t K}))}\\end{array}$ ", "page_idx": 36}, {"type": "text", "text": "Thus, summing Equation (75) from $t=0$ to $t=T-1$ , we conclude: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(\\Tilde{X}_{T K})-F(\\Tilde{X}_{0})\\leq\\displaystyle\\sum_{t=0}^{T-1}\\langle\\nabla F(\\Tilde{X}_{t K}),\\Tilde{X}_{(t+1)K}-\\Tilde{X}_{t K}\\rangle+\\displaystyle\\sum_{t=0}^{T-1}\\frac{L}{2}\\|\\Tilde{X}_{(t+1)K}-\\Tilde{X}_{t K}\\|^{2}}\\\\ &{\\leq\\displaystyle\\sum_{t=0}^{T-1}\\langle\\nabla F(\\Tilde{X}_{t K}),\\Tilde{X}_{(t+1)K}-\\Tilde{X}_{t K}\\rangle+\\displaystyle\\sum_{t=0}^{T-1}\\frac{3L\\alpha^{2}}{2}[\\|\\nabla F(\\Tilde{X}_{t K})\\|^{2}+\\|\\Delta_{t}\\|^{2}]+3L\\alpha\\|\\Tilde{Y}_{t}\\|^{2}}\\\\ &{\\overset{T-1}{\\leq}\\displaystyle\\sum_{t=0}^{T-1}-(\\alpha-\\frac{3\\alpha^{2}L}{2})\\|\\nabla F(\\Tilde{X}_{t K})\\|^{2}+\\alpha\\langle\\nabla F(\\Tilde{X}_{t K}),\\Delta_{t}\\rangle+\\frac{3L\\alpha^{2}}{2}\\|\\Delta_{t}\\|^{2}+3L\\alpha\\|\\Tilde{Y}_{t}\\|^{2}}\\\\ &{\\quad+\\sqrt{2}\\alpha\\langle\\nabla F(\\Tilde{X}_{t K}),\\Tilde{Y}_{t}\\rangle}\\\\ &{\\leq\\displaystyle\\sum_{t=0}^{T-1}\\cdots\\frac{\\alpha}{4}\\|\\nabla F(\\Tilde{X}_{t K})\\|^{2}+3\\alpha\\|\\Delta_{t}\\|^{2}+3L\\alpha\\|\\Tilde{Y}_{t}\\|^{2}+\\sqrt{2\\alpha}\\langle\\nabla F(\\Tilde{X}_{t K}),\\Tilde{Y}_{t}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "In the last step, we have used the fact that $2|\\langle\\nabla F(\\tilde{X}_{t K}),\\Delta_{t}\\rangle|\\leq\\|\\nabla F(\\tilde{X}_{t K})\\|^{2}+\\|\\Delta_{t}\\|^{2}$ and the assumption that $\\alpha L\\leq c_{p}$ for some small enough constant $c_{p}$ ", "page_idx": 37}, {"type": "text", "text": "Therefore, we conclude: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\displaystyle\\sum_{t=0}^{T-1}\\|\\nabla F(\\tilde{X}_{t K})\\|^{2})^{p}\\leq\\displaystyle\\frac{C_{p}}{\\alpha^{p}}\\|(F(X_{0})-F(\\tilde{X}_{K T}))^{+}\\|^{p}+C_{p}(\\displaystyle\\sum_{t=0}^{T-1}\\|\\Delta_{t}\\|^{2})^{p}+C_{p}L^{p}(\\displaystyle\\sum_{t=0}^{T-1}\\|\\bar{Y}_{t}\\|^{2})^{p}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\frac{C_{p}}{\\alpha^{\\frac{p}{2}}}\\displaystyle\\sum_{t=0}^{T-1}\\langle\\nabla F(\\tilde{X}_{t K}),\\bar{Y}_{t}\\rangle\\|^{p}\\qquad\\qquad\\qquad\\qquad\\qquad\\quad(77)}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "The properties of Gaussians show that: $\\begin{array}{r}{\\mathbb{E}(\\sum_{t=0}^{T-1}\\|\\bar{Y}_{t}\\|^{2})^{p}\\leq C_{p}T^{p}d^{p}}\\end{array}$ ", "page_idx": 37}, {"type": "text", "text": "By Lemma 27 and the AM-GM inequality, we conclude that for any $\\kappa>0$ arbitrary, we have: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{\\alpha^{p/2}}\\mathbb{E}\\Big|\\sum_{t=0}^{T-1}\\langle\\nabla F(\\tilde{X}_{t K}),\\bar{Y}_{t}\\rangle\\Big|^{p}\\leq\\displaystyle\\frac{C_{p}}{\\alpha^{p/2}}\\sqrt{\\mathbb{E}(\\sum_{t=0}^{T-1}\\|\\nabla F(\\tilde{X}_{t K})\\|^{2})^{p}}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{\\kappa C_{p}}{2\\alpha^{p}}+\\frac{C_{p}}{2\\kappa}\\mathbb{E}(\\sum_{t=0}^{T-1}\\|\\nabla F(\\tilde{X}_{t K})\\|^{2})^{p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\{\\displaystyle\\sum_{t=0}^{T-1}\\|\\Delta_{t}\\|^{2}\\}^{p}\\leq T^{p-1}\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\Delta_{t}\\|^{2p}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq T^{p-1}\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}(N_{t})^{2p}L^{2p}\\mathbb{E}\\big(\\operatorname*{sup}_{0\\leq i\\leq K-1}\\|\\hat{\\tilde{X}}_{t K+i}-\\tilde{X}_{t K}\\|\\big)^{2p}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq C_{p}T^{p-1}\\displaystyle\\sum_{t=0}^{T-1}L^{2p}\\mathbb{E}\\big(\\operatorname*{sup}_{0\\leq i\\leq K-1}\\|\\hat{\\tilde{X}}_{t K+i}-\\tilde{X}_{t K}\\|\\big)^{2p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "In the first step, we have used Jensen's inequality. In the second step we have used the fact that $\\nabla F$ is $L$ -Lipshcitz and that $N_{t}$ is independent of $\\hat{\\tilde{X}}_{t K+i}$ for $0\\leq i\\leq K-1$ . In the last step, we have used the fact that $\\mathbb{E}(N_{t})^{2p}\\leq C_{p}$ for some constant $C_{p}$ (Lemma 28). ", "page_idx": 37}, {"type": "text", "text": "By Lemma 12, we have: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{E}\\operatorname*{sup}_{0\\leq i\\leq k}\\|\\hat{\\tilde{X}}_{t K+i}-\\tilde{X}_{t K}\\|^{2p}\\leq C_{p}\\alpha^{2p}\\mathbb{E}\\|\\nabla F(\\tilde{X}_{t K})\\|^{2p}+C_{p}\\alpha^{p}d^{p}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Plugging all of these bounds, and taking $\\kappa$ in Equation (78) to be large enough, we conclude: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lefteqn{\\mathbb{E}\\bigl(\\displaystyle\\sum_{t=0}^{T-1}\\|\\nabla F(\\tilde{X}_{t K})\\|^{2}\\bigr)^{p}\\leq\\frac{C_{p}}{\\alpha^{p}}\\mathbb{E}\\|(F(X_{0})-F(\\tilde{X}_{K T}))^{+}\\|^{p}+C_{p}T^{p-1}L^{2p}\\alpha^{2p}\\mathbb{E}\\displaystyle\\sum_{t=0}^{T-1}\\|\\nabla F(\\tilde{X}_{t K})\\|^{2p}}}\\\\ {+\\,C_{p}L^{p}d^{p}T^{p}+\\displaystyle\\frac{C_{p}}{\\alpha^{p}}\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(80)}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "1.6Proof of Lemma 18 ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Proof. For the sake of clarity, define $x=1-\\exp(-\\gamma h)$ $\\begin{array}{r}{\\rho=\\frac{(1-x)^{2}}{\\gamma}\\frac{1}{\\sqrt{\\frac{2}{\\gamma}(h-\\frac{2(1-x)}{\\gamma}+\\frac{(1-x^{2})}{2\\gamma})(1-x^{2})}}}\\end{array}$ ", "page_idx": 38}, {"type": "text", "text": "Algebraic manipulations show that: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\rho=\\sqrt{\\frac{(1-x)^{3}}{2(1+x)\\left(\\gamma h-(1-x)-\\frac{(1-x)^{2}}{2}\\right)}}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Now notethat $\\begin{array}{r}{\\gamma h=-\\log(1-(1-x))=\\sum_{i=1}^{\\infty}\\frac{(1-x)^{i}}{i}}\\end{array}$ Therefore we coneluder $\\gamma h-(1-x)-$ $\\begin{array}{r}{\\frac{(1-x)^{2}}{2}\\geq\\frac{(1-x)^{3}}{3}+\\frac{(1-x)^{4}}{4}}\\end{array}$ .Therefore, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\rho\\leq\\sqrt{\\frac{1}{2\\left(1+x\\right)\\left(\\frac{1}{3}+\\frac{1-x}{4}\\right)}}}\\\\ {\\leq\\sqrt{\\frac{1}{2\\operatorname*{inf}_{t\\in[0,1]}\\left(1+t\\right)\\left(\\frac{1}{3}+\\frac{1-t}{4}\\right)}}\\leq\\sqrt{\\frac{6}{7}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Now, we note that for scalars $A,B>0$ and $\\rho\\in[0,1]$ , we have: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\left[\\!\\!\\begin{array}{c c}{A^{2}\\mathbf{I}_{d}}&{\\rho A B\\mathbf{I}_{d}}\\\\ {\\rho A B\\mathbf{I}_{d}}&{B^{2}\\mathbf{I}_{d}}\\end{array}\\!\\!\\right]\\succeq\\left[\\!\\!\\begin{array}{c c}{A^{2}(1-\\rho)\\mathbf{I}_{d}}&{0}\\\\ {0}&{B^{2}(1-\\rho)\\mathbf{I}_{d}}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Thus, we conclude from the above computations that: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Gamma_{h}^{2}\\succeq c\\left[\\frac{2}{\\gamma}\\left(h-\\frac{2}{\\gamma}(1-\\exp(-\\gamma h))+\\frac{1}{2\\gamma}(1-\\exp(-2\\gamma h))\\right)\\mathbf{I}_{d}\\right.}\\\\ &{\\left.\\quad\\quad\\quad\\quad\\quad\\quad\\quad0\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\left(1-\\exp(-2\\gamma h)\\right)\\mathbf{I}_{d}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\left(1-\\exp(-2\\gamma h)\\right)\\mathbf{I}_{d}\\right]=:\\Gamma_{u b}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We have used the fact that: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\frac{2}{\\gamma}\\left(h-\\frac{2}{\\gamma}(1-\\exp(-\\gamma h))+\\frac{1}{2\\gamma}(1-\\exp(-2\\gamma h))\\right)\\geq\\frac{2}{3\\gamma^{2}}(1-\\exp(-\\gamma h))^{3}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Using the fact that for PSD matrices $A,B\\;A\\preceq B$ implies $B^{-1}\\preceq A^{-1}$ , we have: ", "page_idx": 38}, {"type": "equation", "text": "$$\nG_{h}^{\\mathsf{T}}\\Gamma_{h}^{-2}G_{h}\\preceq G_{h}^{\\mathsf{T}}\\Gamma_{\\mathsf{u b}}^{-2}G_{h}\\preceq\\left[\\begin{array}{c c}{C\\frac{h\\exp(2\\gamma h)}{\\gamma}\\mathbf{I}_{d}}&{0}\\\\ {0}&{0}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "The second inequality follows easily by elementary manipulations. ", "page_idx": 38}, {"type": "text", "text": "1.7 Proof of Lemma 20 ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Proof. Explicit computations show that: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\psi_{t+1}=\\psi_{t}-\\frac{\\alpha}{\\gamma}\\nabla F(\\tilde{U}_{t K})+\\alpha\\left[\\mathbf{I}_{d}\\quad\\frac{\\mathbf{I}_{d}}{\\gamma}\\right]\\Delta_{t}+\\left[\\mathbf{I}_{d}\\quad\\frac{\\mathbf{I}_{d}}{\\gamma}\\right]\\Gamma_{\\alpha}\\tilde{Y}_{t}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Now, note that $\\begin{array}{r}{\\left[{\\bf{I}}_{d}\\quad\\frac{{\\bf{I}}_{d}}{\\gamma}\\right]A_{h}\\;=\\;\\left[{\\bf{I}}\\quad\\frac{{\\bf{I}}_{d}}{\\gamma}\\right]}\\end{array}$ and $\\begin{array}{r l}{\\left[\\mathbf{I}_{d}\\quad\\frac{\\mathbf{I}_{d}}{\\gamma}\\right]G_{h}\\;=\\;\\left[\\frac{h}{\\gamma}\\mathbf{I}_{d}\\quad0\\right]}\\end{array}$ .Therefore, from the definition of $\\Delta_{t}$ , we conclude that: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\alpha\\left[\\mathbf{I}_{d}\\quad\\frac{\\mathbf{I}_{d}}{\\gamma}\\right]\\Delta_{t}=-\\sum_{i=0}^{K-1}\\frac{H_{i}\\alpha}{\\gamma}\\left[\\nabla F(\\hat{\\tilde{U}}_{t K+i})-\\nabla F(\\tilde{U}_{t K})\\right]\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "A straight forward calculation shows that: $\\begin{array}{r}{\\tilde{\\Psi}_{t}:=\\left[\\mathbf{I}_{d}\\quad\\frac{\\mathbf{I}_{d}}{\\gamma}\\right]\\Gamma_{\\alpha}\\tilde{Y}_{t}\\sim\\mathcal{M}(0,\\frac{2h}{\\gamma}\\mathbf{I}_{d}).}\\end{array}$ ", "page_idx": 39}, {"type": "text", "text": "1.8 Proof of Lemma 21 ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Proof. 1. Note that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\hat{\\tilde{U}}_{t K+i}-\\tilde{U}_{t K}=\\Pi\\bigl(A_{\\frac{i\\alpha}{K}}-\\mathbf{I}\\bigr)\\tilde{X}_{t K}+\\Pi G_{\\frac{i\\alpha}{K}}b\\bigl(\\tilde{X}_{t K}\\bigr)+\\sum_{j=0}^{i-1}\\Pi A_{\\frac{\\alpha(i-1-j)}{K}}\\Gamma_{\\frac{\\alpha}{K}}\\,Z_{t K+j}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We bound each of the terms separately. ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\|(A_{\\frac{i\\alpha}{K}}-\\mathbf{I})\\tilde{X}_{t K}\\|_{\\Pi}\\leq\\frac{i\\alpha}{K}\\|\\tilde{V}_{t K}\\|\\leq\\alpha\\|\\tilde{V}_{t K}\\|\n$$", "text_format": "latex", "page_idx": 39}, {"type": "equation", "text": "$$\n\\|G_{\\frac{i\\alpha}{K}}b(\\tilde{X}_{t K})\\|_{\\Pi}\\leq\\frac{\\alpha^{2}i^{2}}{2K^{2}}\\|\\nabla F(\\tilde{U}_{t K})\\|\\leq\\frac{\\alpha^{2}}{2}\\|\\nabla F(\\tilde{U}_{t K})\\|\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Plugging these inequalities gives the result. ", "page_idx": 39}, {"type": "text", "text": "2. Since $\\nabla F$ is $L$ -Lipschitz, we have: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle F(\\psi_{t+1})-F(\\psi_{t})\\leq\\langle\\nabla F(\\psi_{t}),\\psi_{t+1}-\\psi_{t}\\rangle+\\displaystyle\\frac{L}{2}\\|\\psi_{t+1}-\\psi_{t}\\|^{2}}\\\\ {\\displaystyle=\\langle\\nabla F(\\psi_{t})-\\nabla F(\\tilde{U}_{t K}),\\psi_{t+1}-\\psi_{t}\\rangle+\\langle\\nabla F(\\tilde{U}_{t K}),\\psi_{t+1}-\\psi_{t}\\rangle+\\displaystyle\\frac{L}{2}\\|\\psi_{t+1}-\\psi_{t}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Using Lemma 20 to evaluate $\\psi_{t+1}-\\psi_{t}$ , we conclude: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\langle\\nabla F(\\hat{U}_{K}),\\hat{\\psi}_{I+1}-\\hat{\\psi}_{I}\\rangle=-\\frac{1}{\\gamma}\\vert\\nabla F(\\hat{U}_{K})\\vert^{2}+\\langle\\nabla F(U_{I}),\\hat{\\psi}_{I}\\rangle}&{}\\\\ &{\\quad-\\underbrace{F_{I-1}}\\underbrace{H_{I+1}}\\langle\\nabla F(\\hat{U}_{K}),\\nabla F(\\hat{U}_{K+1})-\\nabla F(\\hat{U}_{K})\\rangle}\\\\ &{\\quad-\\underbrace{\\int_{0}}\\underbrace{\\frac{1}{\\gamma}\\langle\\hat{U}_{K}\\rangle\\vert^{2}}_{\\vert\\nabla F(\\hat{U}_{K})\\vert\\vert^{2}}+\\langle\\nabla F(U_{I}),\\hat{\\Psi}_{I}\\rangle}\\\\ &{\\quad+\\frac{N\\alpha}{2}\\vert\\nabla F(\\hat{U}_{K})\\vert\\vert^{2}+\\vert\\nabla F(U_{I})\\vert\\underbrace{\\hat{U}_{K+1}}_{\\vert\\le\\hat{U}_{K}\\vert\\vert\\hat{U}_{K}\\vert\\vert}}\\\\ &{\\quad+\\frac{N\\alpha}{2}\\vert\\nabla F(\\hat{U}_{I+1})\\vert^{2}+\\langle\\nabla F(U_{I}),\\hat{\\Psi}_{I}\\rangle}\\\\ &{\\quad+\\underbrace{\\frac{1}{2}\\vert\\nabla F(\\hat{U}_{K})\\vert\\vert^{2}}_{\\vert\\nabla F(\\hat{U}_{K})\\vert\\vert}+\\frac{N\\alpha^{2}L^{2}}{2\\gamma}\\underbrace{\\operatorname*{sup}_{\\alpha\\le K-1}\\vert\\hat{U}_{K+1}-\\hat{U}_{K}\\vert\\vert^{2}}_{\\vert\\nabla F(\\hat{U}_{K})\\vert\\vert}}\\\\ &{\\quad-\\underbrace{2}_{\\gamma}\\vert\\nabla F(\\hat{U}_{K})\\vert\\vert^{2}+\\langle\\nabla F(U_{I}),\\hat{\\Psi}_{I}\\rangle}\\\\ &{\\quad+\\frac{N\\alpha^{2}L^{2}}{2\\gamma}\\underbrace{\\operatorname*{sup}_{\\alpha\\le K-1}\\vert\\hat{U}_{K+1}-\\hat{U}_{K}\\vert\\vert^{2}}_{\\vert\\nabla F(\\hat{U}_{K})\\vert\\vert}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "In the second step, we have use the fact that $\\nabla F$ is $L$ -Lipschitz along with the CauchySchwarz inequality. In the third step, we have used the AM-GM inequality which states that for any $a,b\\ge0$ ,wehave $2a b\\leq a^{\\bar{2}}+b^{2}$ ", "page_idx": 40}, {"type": "text", "text": "Similarly, we note that: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\nabla F(\\psi_{t})-\\nabla F(\\tilde{U}_{t K}),\\psi_{t+1}-\\psi_{t}\\rangle\\leq\\frac{\\alpha L}{\\gamma^{2}}\\|\\nabla F(\\tilde{U}_{t K})\\|\\|\\tilde{V}_{t K}\\|+\\langle\\nabla F(\\psi_{t})-\\nabla F(\\tilde{U}_{t K}),\\tilde{\\Psi}_{t}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\frac{N_{t}\\alpha L^{2}}{\\gamma^{2}}\\|\\tilde{V}_{t K}\\|\\displaystyle\\operatorname*{sup}_{0\\leq i\\leq K-1}\\|\\hat{\\tilde{U}}_{t K+i}-\\tilde{U}_{t K}\\|}\\\\ &{\\leq\\frac{\\alpha}{4\\gamma}\\|\\nabla F(\\tilde{U}_{t K})\\|^{2}+\\frac{3\\alpha L^{2}\\|\\tilde{V}_{t K}\\|^{2}}{2\\gamma^{3}}+\\langle\\nabla F(\\psi_{t})-\\nabla F(\\tilde{U}_{t K}),\\tilde{\\Psi}_{t}\\rangle}\\\\ &{\\quad+\\frac{N_{t}^{2}\\alpha L^{2}}{2\\gamma}\\displaystyle\\operatorname*{sup}_{0\\leq i<K-1}\\|\\hat{\\tilde{U}}_{t K+i}-\\tilde{U}_{t K}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "We have again used the AM-GM inequality in the second step above. Convexity of $x\\to x^{2}$ implies: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\frac{L}{2}\\Vert\\psi_{t+1}-\\psi_{t}\\Vert^{2}=\\frac{3L}{2}\\left[\\frac{\\alpha^{2}}{\\gamma^{2}}\\Vert\\nabla F(\\tilde{U}_{t K})\\Vert^{2}+\\frac{N_{t}^{2}\\alpha^{2}L^{2}}{\\gamma^{2}}\\operatorname*{sup}_{0\\leq i\\leq K-1}\\Vert\\hat{U}_{t K+i}-\\tilde{U}_{t K}\\Vert^{2}+\\Vert\\tilde{\\Psi}_{t}\\Vert^{2}\\right]\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Plugging these upper bounds into Equation (85), we have: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(\\psi_{t+1})-F(\\psi_{t})\\leq-\\displaystyle\\frac{\\alpha}{4\\gamma}(1-\\frac{6\\alpha L}{\\gamma})\\|\\nabla F(\\tilde{U}_{t K})\\|^{2}+\\langle\\nabla F(\\psi_{t})-\\nabla F(\\tilde{U}_{t K}),\\tilde{\\Psi}_{t}\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\langle\\nabla F(U_{t}),\\tilde{\\Psi}_{t}\\rangle+\\displaystyle\\frac{3L}{2}\\|\\tilde{\\Psi}_{t}\\|^{2}+\\frac{3\\alpha L^{2}\\|\\tilde{V}_{t K}\\|^{2}}{2\\gamma^{3}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad+\\displaystyle\\frac{N_{t}^{2}\\alpha L^{2}}{\\gamma}\\left(1+\\frac{3\\alpha L}{2\\gamma}\\right)\\operatorname*{sup}_{0\\leq i\\leq K-1}\\|\\hat{\\tilde{U}}_{t K+i}-\\tilde{U}_{t K}\\|^{2}\\qquad\\quad\\mathrm{~otherwise}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "3. Using the scaling relations (Section 2.1), it is easy to show that for any $i$ such that $0\\leq i\\leq$ $\\begin{array}{r}{K-1,\\sum_{j=0}^{i-1}\\bar{\\Pi^{A}}_{\\frac{\\alpha(i-1-j)}{K}}\\Gamma_{\\frac{\\alpha}{K}}Z_{t K+j}}\\end{array}$ is a Gaussian with covariance matrix $\\Sigma_{i}$ such that $\\begin{array}{r}{\\Sigma_{i}\\preceq\\frac{2\\exp(\\gamma\\alpha)\\gamma\\alpha^{3}}{3}\\mathbf{I}_{d}}\\end{array}$ ", "page_idx": 40}, {"type": "text", "text": "Applying Gaussian concentration for $\\begin{array}{r}{\\sum_{j=0}^{i-1}\\Pi A_{\\frac{\\alpha(i-1-j)}{K}}\\Gamma_{\\frac{\\alpha}{K}}Z_{t K+j}}\\end{array}$ along with the union bound over all $0\\leq i\\leq K-1$ , we conclude: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(M_{t,K}>C\\exp(\\frac{\\gamma\\alpha}{2})\\sqrt{\\gamma\\alpha^{3}}(\\sqrt{d}+\\beta+\\sqrt{\\log K}))\\le\\exp(-\\beta^{2}/2)}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We conclude the moment bounds by integrating tail probabilities. ", "page_idx": 41}, {"type": "text", "text": "1.9Proof of Lemma 22 ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Proof. 1. Let $g:=e^{-\\gamma\\alpha}$ . Then, we have: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\tilde{V}_{(t+1)K}=g\\tilde{V}_{t K}-\\frac{1-g}{\\gamma}\\nabla F(\\tilde{U}_{t K})+({\\bf I}-\\Pi)(\\alpha\\Delta_{t}+\\Gamma_{\\alpha}\\tilde{Y}_{t})\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Now, consider: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\widehat{V}_{(t+1)K}\\|^{2}=g^{2}\\big\\|\\bar{V}_{K}\\|^{2}+\\frac{(1-g)^{2}}{\\gamma^{2}}\\|\\nabla F(\\bar{U}_{K})\\|^{2}+\\|({\\mathbf I}-\\Pi)(\\alpha\\Delta_{t}+\\Gamma_{\\alpha}\\bar{\\hat{V}}_{t})\\|^{2}}\\\\ &{\\qquad-2\\frac{g(1-g)}{\\gamma}\\langle\\nabla F(\\bar{U}_{K}),\\bar{V}_{K}\\rangle+2g(\\bar{V}_{K},({\\mathbf I}-\\Pi)(\\alpha\\Delta_{t}+\\Gamma_{\\alpha}\\bar{\\hat{V}}_{t}))}\\\\ &{\\qquad-2\\frac{(1-g)}{\\gamma}\\langle\\nabla F(\\bar{U}_{K}),({\\mathbf I}-\\Pi)(\\alpha\\Delta_{t}+\\Gamma_{\\alpha}\\bar{\\hat{V}}_{t})\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\frac{(1-g)}{\\gamma}\\|\\nabla F(\\bar{U}_{K})\\|^{2}+\\|({\\mathbf I}-\\Pi)(\\alpha\\Delta_{t}+\\Gamma_{\\alpha}\\bar{\\hat{V}}_{t})\\|^{2}}\\\\ &{\\quad+2g(\\bar{V}_{K},({\\mathbf I}-\\bar{U}_{K})(\\alpha\\Delta_{t}+\\Gamma_{\\alpha}\\bar{\\hat{V}}_{t}))}\\\\ &{\\qquad-2\\frac{(1-g)}{\\gamma}\\big(\\nabla F(\\bar{U}_{K}),({\\mathbf I}-\\Pi)(\\alpha\\Delta_{t}+\\Gamma_{\\alpha}\\bar{\\hat{V}}_{t})\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\nabla\\big\\|\\bar{V}_{K}\\|^{2}+\\frac{2(1-g)}{\\gamma^{2}}\\|\\nabla F(\\bar{U}_{K})\\|^{2}+\\frac{4}{1-\\sqrt{\\beta}}\\|({\\mathbf I}-\\Pi)(\\alpha\\Delta_{t})\\|^{2}}\\\\ &{\\qquad+2\\|({\\mathbf I}-\\Pi)\\mathrm{L}_{\\alpha}\\bar{\\hat{V}}_{t})\\|^{2}+2g(\\bar{V}_{K},({\\mathbf I}-\\Pi)(\\Gamma_{\\alpha}\\bar{\\hat{V}}_{t}))}\\\\ &{\\qquad-2\\frac{(1-g\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "In the second step, we have use the fact that $\\begin{array}{r l r}{|\\frac{2g(1-g)}{\\gamma}\\langle\\nabla F(\\tilde{U}_{t K}),\\tilde{V}_{t K}\\rangle|}&{{}\\leq}&{g(1~-}\\end{array}$ $g)\\frac{\\|\\nabla F(\\tilde{U}_{t K})\\|^{2}}{\\gamma^{2}}\\;+\\;g(1\\;-\\;g)\\|\\tilde{V}_{t K}\\|^{2}$ In the third step, we have used the fact that: $\\begin{array}{r}{2g\\langle\\tilde{V}_{t K},(\\mathbf{I}\\,-\\,\\Pi)\\alpha\\Delta_{t}\\rangle\\;\\leq\\;(\\sqrt{g}\\,-\\,g)\\|\\tilde{V}_{t K}\\|^{2}\\,+\\,\\frac{\\|(\\mathbf{I}-\\boldsymbol\\Pi)\\alpha\\Delta_{t}\\|^{2}}{1-\\sqrt{g}},\\;-\\frac{2\\left(1-\\alpha\\right)\\alpha\\Delta_{t}}{1-\\sqrt{g}}}\\end{array}$ $-\\frac{2(1-g)}{\\gamma}\\langle\\nabla F(\\tilde{U}_{t K}),(\\mathbf{I}\\mathrm{~-~}$ $\\begin{array}{r}{\\Pi)\\alpha\\Delta_{t}\\rangle\\leq\\frac{1-g}{\\gamma^{2}}\\|\\nabla F(\\tilde{U}_{t K})\\|^{2}+(1-g)\\|(\\mathbf{I}-\\boldsymbol{\\Pi})\\alpha\\Delta\\|^{2}}\\end{array}$ and $\\lVert(\\mathbf{I}-\\boldsymbol{\\Pi})(\\alpha\\Delta_{t}+\\Gamma_{\\alpha}\\tilde{Y}_{t})\\rVert^{2}\\leq$ $2\\|(\\mathbf{I}-\\boldsymbol{\\Pi})(\\alpha\\Delta_{t})\\|^{2}+2\\|(\\mathbf{I}-\\boldsymbol{\\Pi})\\Gamma_{\\alpha}\\tilde{Y}_{t}\\|^{2}$ ", "page_idx": 41}, {"type": "text", "text": "With extention of notation, let $\\begin{array}{r}{S_{2}((\\mathbf{I}-\\Pi)\\alpha\\Delta):=\\sum_{t=0}^{T-1}\\|(\\mathbf{I}-\\Pi)\\Delta_{t}\\|^{2},S_{2}((\\mathbf{I}-\\Pi)\\Gamma_{\\alpha}\\tilde{Y}):=}\\end{array}$ $\\begin{array}{r}{\\sum_{t=0}^{T-1}\\|({\\bf I}-{\\boldsymbol\\Pi})\\boldsymbol\\Gamma_{\\alpha}\\tilde{\\boldsymbol Y}_{t}\\|^{2}}\\end{array}$ . Using Equation (91), we conclude: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{S}_{2}(V)\\leq\\displaystyle\\frac{\\|\\tilde{V}_{0}\\|^{2}-\\|\\tilde{V}_{t K}\\|^{2}}{1-\\sqrt{g}}+\\frac{4}{\\gamma^{2}}\\mathcal{S}_{2}(\\nabla F)+\\displaystyle\\frac{4}{(1-\\sqrt{g})^{2}}\\mathcal{S}_{2}((\\mathbf{I}-\\Pi)\\alpha\\Delta)}\\\\ &{\\quad\\quad\\quad+\\,\\frac{2\\mathcal{S}_{2}((\\mathbf{I}-\\Pi)\\Gamma_{\\alpha}\\tilde{Y})}{1-\\sqrt{g}}+\\displaystyle\\sum_{t=0}^{T-1}2\\frac{g}{1-\\sqrt{g}}\\langle\\tilde{V}_{t K},(\\mathbf{I}-\\Pi)(\\Gamma_{\\alpha}\\tilde{Y}_{t})\\rangle}\\\\ &{\\quad\\quad\\quad-\\,2\\displaystyle\\frac{(1-g)}{\\gamma(1-\\sqrt{g})}\\langle\\nabla F(\\tilde{U}_{t K}),(\\mathbf{I}-\\Pi)(\\Gamma_{\\alpha}\\tilde{Y}_{t})\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Note that $(\\mathbf{I}-\\Pi)\\Gamma_{\\alpha}\\tilde{Y}_{t}\\sim\\mathcal{N}(0,(1-e^{-2\\gamma\\alpha})\\mathbf{I}_{d})$ . By properties of Gaussians, it is clear that for any $p\\geq1$ , we must have $\\begin{array}{r}{\\mathbb{E}\\left(S_{2}(({\\mathbf I}-\\Pi)\\Gamma_{\\alpha}\\tilde{Y})\\right)^{\\bar{p}}\\leq C_{p}(\\gamma\\alpha T d)^{p}}\\end{array}$ . By Lemma 27, we conclude: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}|\\sum_{t=0}^{T-1}\\langle\\tilde{V}_{t K},(\\mathbf{I}-\\Pi)\\Gamma_{\\alpha}\\tilde{Y}_{t}\\rangle|^{p}\\leq C_{p}(\\gamma\\alpha)^{\\frac{p}{2}}\\sqrt{\\mathbb{E}(S_{2}(V))^{p}}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big|\\sum_{t=0}^{T-1}\\langle\\nabla F(\\tilde{U}_{t K}),(\\mathbf{I}-\\Pi)\\Gamma_{\\alpha}\\tilde{Y}_{t}\\rangle|^{p}\\leq C_{p}(\\gamma\\alpha)^{\\frac{p}{2}}\\sqrt{\\mathbb{E}(S_{2}(\\nabla F))^{p}}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "By $L$ smoothnessof $F$ wehave: $\\begin{array}{r}{\\|(\\mathbf{I}-\\Pi)(\\alpha\\Delta_{t})\\|\\le L\\alpha N_{t}\\operatorname*{sup}_{0\\le i\\le K}\\|\\hat{\\tilde{U}}_{t K+i}-\\tilde{U}_{t K}\\|.}\\end{array}$ Using the result in Lemma 21, we conclude: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}(S_{2}((\\mathbf{I}-\\Pi)\\alpha\\Delta))^{p}\\leq C_{p}L^{2p}\\mathbb{E}(\\displaystyle\\sum_{t=0}^{T-1}\\alpha^{4}N_{t}^{2}\\|\\tilde{V}_{t K}\\|^{2}+\\alpha^{6}N_{t}^{2}\\mathbb{E}\\|\\nabla F(\\tilde{U}_{t K})\\|^{2}+\\alpha^{2}N_{t}^{2}M_{t,K}^{2})^{p}}\\\\ &{\\ \\leq C_{p}L^{2p}T^{p-1}\\mathbb{E}(\\displaystyle\\sum_{t=0}^{T-1}\\alpha^{4p}N_{t}^{2p}\\|\\tilde{V}_{t K}\\|^{2p}+\\alpha^{6p}N_{t}^{2p}\\mathbb{E}\\|\\nabla F(\\tilde{U}_{t K})\\|^{2p}+\\alpha^{2p}N_{t}^{2p}M_{t,K}^{2p})}\\\\ &{\\ \\leq C_{p}L^{2p}T^{p-1}\\alpha^{4p}\\mathbb{E}S_{2p}(V)+C_{p}T^{p-1}L^{2p}\\alpha^{6p}\\mathbb{E}S_{2p}(\\nabla F)+C_{p}L^{2p}T^{p}\\alpha^{5p}\\gamma^{p}(d+\\log_{\\alpha\\times}^{N})^{p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "In the second step, we have used jensen's inequality to show that $\\begin{array}{r}{(\\frac{1}{T}\\sum_{t}a_{t}^{2})^{p}\\leq\\frac{1}{T}\\sum_{t}a_{t}^{2p}}\\end{array}$ In the third step, we have used the fact that $N_{t}$ is independent of $\\tilde{X}_{t K}$ and that $\\mathbb{E}N_{t}^{2p}\\le C_{p}$ for some constant $C_{p}$ (Lemma 28). ", "page_idx": 42}, {"type": "text", "text": "We now use the fact that for any two random variables $X,Y$ such that $\\mathbb{E}\\|X\\|^{p},\\mathbb{E}\\|Y\\|^{p}<\\infty$ $\\displaystyle\\big[\\mathbb{E}\\|X\\!+\\!Y\\|^{p}\\big]^{\\frac{1}{p}}\\leq\\big[\\mathbb{E}\\|X\\|^{p}\\big]^{\\frac{1}{p}}\\!+\\!\\big[\\mathbb{E}\\|Y\\|^{p}\\big]^{\\frac{1}{p}}$ . Using the bounds established above and applying them to Equation (91), we conclude: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\{S_{2}(V)\\}^{n}\\geq}&{\\frac{\\|\\bar{g}\\|_{\\bar{\\mathcal{H}}}\\|_{\\mathcal{H}}^{3}\\|_{\\mathcal{H}}^{3}}{1-\\sqrt{9}}+\\frac{4}{\\gamma}\\mathbb{E}\\{S_{2}(\\mathbb{S}_{2}(\\mathbb{V}))^{n}\\}^{\\frac{1}{p}}+\\frac{C|\\mathbb{E}(\\mathcal{S}_{2}(\\mathbb{Z}(1-\\Pi)\\ln\\Delta))|^{p}\\}{(1-\\sqrt{9})^{2}}\\frac{\\|\\mathcal{H}\\|_{\\mathcal{H}}}{p}}\\\\ &{\\quad+\\,C_{p}\\frac{9\\alpha T}{1-\\sqrt{9}}+C_{p}\\sqrt{\\frac{9\\alpha}{1-1}}\\mathbb{E}\\{S_{2}(\\mathbb{V})\\}^{n}|^{\\frac{3}{p}}\\bigg+C_{p}\\sqrt{\\frac{1}{\\gamma}}\\mathbb{E}\\{S_{2}(\\mathbb{V})\\}^{n}|^{\\frac{1}{p}}}\\\\ &{\\leq\\frac{C}{\\gamma_{q}(15|\\mathbb{V})^{2}}+\\frac{4}{\\gamma^{2}}\\mathbb{E}\\{S_{2}(\\mathbb{V})\\}^{n}|^{\\frac{1}{p}}+\\frac{C|\\mathbb{V}|(\\mathcal{S}_{2}(\\mathbb{V}))|^{\\frac{1}{p}}}{\\gamma^{2}\\alpha^{2}}}\\\\ &{\\quad+\\,C_{p}H+C_{p}\\frac{\\|\\mathbb{E}(\\mathcal{S}_{2}(\\mathbb{V}))\\|^{\\frac{1}{p}}}{\\sqrt{9}}+C_{p}\\sqrt{\\frac{9}{\\gamma}}\\mathbb{E}\\{S_{2}(\\mathbb{V})\\}^{n}}\\\\ &{\\leq\\frac{C}{\\gamma_{q}(15|\\mathbb{V})^{2}}\\frac{\\|\\mathcal{H}\\|_{\\mathcal{H}}}{p}+\\frac{C_{p}\\sqrt{1}}{2}\\mathbb{E}\\{S_{2}(\\mathbb{V})\\}^{n}+C_{p}\\frac{\\|\\mathbb{E}(\\mathcal{S}_{2}(\\mathbb{V}))\\|^{\\frac{1}{p}}}{\\sqrt{9}}\\frac{1}{\\sqrt{9}}}\\\\ &{\\quad+\\,C_{p}\\frac{L^{2}\\gamma^{-1}\\frac{8}{9}}{\\gamma^{2}}(\\mathbb{E}\\mathcal{S}_{2}(\\mathbb{V}))^\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Here, we have used that fact that $1-\\sqrt{g}\\,\\geq\\,C\\gamma\\alpha$ whenever $\\gamma\\alpha\\,<\\,1$ .We now use the AM-GM inequality to show that ", "page_idx": 43}, {"type": "equation", "text": "$$\nC_{p}\\frac{[\\mathbb{E}(S_{2}(V))^{p}]^{\\frac{1}{2p}}}{\\sqrt{\\gamma\\alpha}}\\leq\\frac{[\\mathbb{E}(S_{2}(V))^{p}]^{\\frac{1}{p}}}{10}+\\frac{C_{p}^{\\prime}}{\\gamma\\alpha}\\,.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Similarly, we have: ", "page_idx": 43}, {"type": "equation", "text": "$$\nC_{p}\\sqrt{\\frac{\\alpha}{\\gamma}}\\left[\\mathbb{E}(S_{2}(\\nabla F))^{p}\\right]^{\\frac{1}{2p}}\\leq\\frac{\\left[\\mathbb{E}(S_{2}(\\nabla F))^{p}\\right]^{\\frac{1}{p}}}{2\\gamma^{2}}+C_{p}^{\\prime}\\gamma\\alpha\\,.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Applying this to the RHS of Equation (94) and re-arranging, we conclude the statement of the lemma. ", "page_idx": 43}, {"type": "text", "text": "2. Using similar methods as in item 1 above, we apply Lemma 27 and collect the following moment bounds: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{E}\\vert\\sum_{t=0}^{T-1}\\langle\\nabla F(\\psi_{t})-\\nabla F(\\tilde{U}_{t K}),\\tilde{\\Psi}_{t}\\rangle\\vert^{p}\\leq C_{p}\\frac{L^{p}\\alpha^{\\frac{p}{2}}}{\\gamma^{\\frac{3p}{2}}}\\sqrt{\\mathbb{E}(S_{2}(V))^{p}}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{E}|\\sum_{t=0}^{T-1}\\langle\\nabla F(\\tilde{U}_{t K}),\\tilde{\\Psi}_{t}\\rangle|^{p}\\leq C_{p}\\frac{\\alpha^{\\frac{p}{2}}}{\\gamma^{\\frac{p}{2}}}\\sqrt{\\mathbb{E}(S_{2}(\\nabla F))^{p}}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "By a calculation similar to that in item 1, we have: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left(\\displaystyle\\sum_{t}\\frac{N_{t}^{2}\\alpha L^{2}}{\\gamma}\\left(1+\\frac{3\\alpha L}{2\\gamma}\\right)\\operatorname*{sup}_{0\\leq i\\leq K-1}\\|\\hat{\\tilde{U}}_{t K+i}-\\tilde{U}_{t K}\\|^{2}\\right)^{p}}\\\\ &{\\leq C_{p}T^{p-1}\\displaystyle\\frac{L^{2p}}{\\gamma^{p}}\\left[\\alpha^{3p}\\mathbb{E}S_{2p}(V)+\\alpha^{5p}S_{2p}(\\nabla F)+T\\alpha^{4p}\\gamma^{p}(d+\\log K)^{p}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Summing item 2 of Lemma 21 over $t=0$ to $t=T-1$ and applying the triangle inequality for $p$ -th moments, we have: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|\\mathbb{E}(S_{2}(\\nabla F))^{p}\\right|^{\\frac{1}{p}}\\leq\\frac{\\gamma}{\\alpha}[\\mathbb{E}|(F(\\Psi_{0})-F(\\Psi_{T}))^{+}|^{p}]^{\\frac{1}{p}}+C_{p}\\sqrt{\\frac{\\gamma}{\\alpha}}[\\mathbb{E}(S_{2}(\\nabla F))^{p}]^{\\frac{1}{2p}}}\\\\ &{\\quad+C_{p}L\\sqrt{\\frac{1}{\\gamma\\alpha}}[\\mathbb{E}(S_{2}(V))^{p}]^{\\frac{1}{2p}}+\\frac{C L^{2}}{\\gamma^{2}}[\\mathbb{E}(S_{2}(V))^{p}]^{\\frac{1}{p}}+C_{p}L T(d+\\log K)}\\\\ &{\\quad+C_{p}T^{1-\\frac{1}{p}}L^{2}\\alpha^{2}[\\mathbb{E}S_{2p}(V)]^{\\frac{1}{p}}+C_{p}T^{1-\\frac{1}{p}}L^{2}\\alpha^{4}[\\mathbb{E}S_{2p}(\\nabla F)]^{\\frac{1}{p}}}\\\\ &{\\quad+C_{p}T\\gamma L^{2}\\alpha^{3}(d+\\log K)}\\\\ &{\\leq\\frac{\\gamma}{\\alpha}[\\mathbb{E}|(F(\\Psi_{0})-F(\\Psi_{T}))^{+}|^{p}]^{\\frac{1}{p}}+C_{p}\\sqrt{\\frac{\\gamma}{\\alpha}}[\\mathbb{E}(S_{2}(\\nabla F))^{p}]^{\\frac{1}{2p}}}\\\\ &{\\quad+C_{p}L\\sqrt{\\frac{1}{\\gamma\\alpha}}[\\mathbb{E}(S_{2}(V))^{p}]^{\\frac{1}{p}}+\\frac{C L^{2}}{\\gamma^{2}}[\\mathbb{E}(S_{2}(V))^{p}]^{\\frac{1}{p}}+C_{p}L T(d+\\log K)}\\\\ &{\\quad+C_{p}T^{1-\\frac{1}{p}}L^{2}\\alpha^{2}[\\mathbb{E}S_{2p}(V)]^{\\frac{1}{p}}+C_{p}T^{1-\\frac{1}{p}}L^{2}\\alpha^{4}[\\mathbb{E}S_{2p}(\\nabla F)]^{\\frac{1}{p}}\\quad(\\vartheta\\wp)}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "In the last step, we have use the fact that $\\gamma\\alpha<1$ and $\\begin{array}{r}{\\frac{L\\alpha}{\\gamma}<c_{0}}\\end{array}$ . Applying AM-GM inequality 10 $\\begin{array}{r}{C_{p}\\sqrt{\\frac{\\gamma}{\\alpha}}[\\mathbb{E}(S_{2}(\\nabla F))^{p}]^{\\frac{1}{2p}}}\\end{array}$ and $\\begin{array}{r}{C_{p}L\\sqrt{\\frac{1}{\\gamma\\alpha}}[\\mathbb{E}(S_{2}(V))^{p}]^{\\frac{1}{2p}}}\\end{array}$ similar to item 1, we conclude the result. ", "page_idx": 43}, {"type": "text", "text": "1.10Proof of Lemma 23 ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Proof. ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\tilde{X}_{s K}-\\tilde{X}_{t K}=(A_{\\alpha(s-t)}-\\mathbf{I})\\tilde{X}_{t K}+\\sum_{h=t}^{s-1}A_{\\alpha(s-h-1)}\\left[G_{\\alpha}b(\\tilde{X}_{h K})+\\alpha\\Delta_{h}+\\Gamma_{\\alpha}\\tilde{Y}_{h}\\right]\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "In this proof only, for any random variable $W$ and any projection operator $\\mathcal{P}$ over $\\mathbb{R}^{2d}$ we let $\\overset{\\cdot}{\\mathcal{M}_{p}(W;\\mathcal{P})}:=(\\mathbb{E}[\\|\\mathcal{P}W\\|^{p}|\\tilde{X}_{t K}])^{\\frac{1}{p}}$ . We use the convention that $\\mathcal{M}_{p}(W)=(\\mathbb{E}\\|W\\|^{p})^{\\frac{1}{p}}$ . Using the triangle inequality for $\\mathcal{M}_{p}$ , we conclude: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{M}_{p}(\\tilde{X}_{s K}-\\tilde{X}_{t K},\\Pi)\\leq\\mathcal{M}_{p}\\big((A_{\\alpha(s-t)}-\\mathbf{I})\\tilde{X}_{t K},\\Pi\\big)+\\displaystyle\\sum_{h=t}^{s-1}\\mathcal{M}_{p}(A_{\\alpha(s-h-1)}G_{\\alpha}b(\\tilde{X}_{h K}),\\Pi)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\sum_{h=t}^{s-1}\\mathcal{M}_{p}\\big(A_{\\alpha(s-h-1)}\\alpha\\Delta_{h},\\Pi\\big)+\\mathcal{M}_{p}\\big(\\displaystyle\\sum_{h=t}^{s-1}A_{\\alpha(s-h-1)}\\Gamma_{\\alpha}\\tilde{Y}_{h},\\Pi\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Similarly, we have: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{M}_{p}(\\tilde{X}_{s K}-\\tilde{X}_{t K},\\mathbf{I}-\\Pi)\\leq\\mathcal{M}_{p}((A_{\\alpha(s-t)}-\\mathbf{I})\\tilde{X}_{t K},\\mathbf{I}-\\Pi)+\\displaystyle\\sum_{h=t}^{s-1}\\mathcal{M}_{p}(A_{\\alpha(s-h-1)}\\alpha\\Delta_{h},\\mathbf{I}-\\Pi)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\displaystyle\\sum_{h=t}^{s-1}\\mathcal{M}_{p}(A_{\\alpha(s-h-1)}G_{\\alpha}b(\\tilde{X}_{h K}),\\mathbf{I}-\\Pi)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad+\\displaystyle\\mathcal{M}_{p}(\\displaystyle\\sum_{h=t}^{s-1}A_{\\alpha(s-h-1)}\\Gamma_{\\alpha}\\tilde{Y}_{h},\\mathbf{I}-\\Pi)}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "By scaling relations given in Section 2.1, we have: $\\begin{array}{r}{\\sum_{h=t}^{s-1}\\Pi A_{\\alpha(s-h-1)}\\Gamma_{\\alpha}\\tilde{Y}_{h}\\sim\\mathcal{N}(0,\\Pi(\\Gamma_{(s-t)\\alpha}^{2})\\Pi)}\\end{array}$ $\\begin{array}{r l}&{\\mathrm{1d}\\sum_{h=t}^{s-1}(\\mathbf{I}-\\Pi)A_{\\alpha(s-h-1)}\\Gamma_{\\alpha}\\tilde{Y}_{h}\\sim\\mathcal{N}(0,(\\mathbf{I}-\\Pi)(\\Gamma_{(s-t)\\alpha}^{2})(\\mathbf{I}-\\Pi)).\\mathrm{~N}}\\\\ &{\\frac{4\\alpha(s-t)}{\\gamma}\\quad0\\Bigg]\\mathrm{~and~}(\\mathbf{I}-\\Pi)(\\Gamma_{\\alpha(s-t)}^{2})(\\mathbf{I}-\\Pi)\\lesssim\\left[0\\begin{array}{c c}{0}&{0}\\\\ {0}&{2\\gamma\\alpha(s-t)\\biggr]}\\end{array}\\right]}\\end{array}$ oicethat $\\Pi(\\Gamma_{\\alpha(s-t)}^{2})\\Pi\\lesssim$ ", "page_idx": 44}, {"type": "text", "text": "Therefore, we conclude: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathcal{M}_{p}(\\sum_{h=t}^{s-1}A_{\\alpha(s-h-1)}\\Gamma_{\\alpha}\\tilde{Y}_{h},\\Pi)\\le C_{p}\\sqrt{\\frac{d\\alpha(s-t)}{\\gamma}}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathcal{M}_{p}(\\sum_{h=t}^{s-1}A_{\\alpha(s-h-1)}\\Gamma_{\\alpha}\\tilde{Y}_{h},\\mathbf{I}-\\Pi)\\leq C_{p}\\sqrt{d\\alpha\\gamma(s-t)}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Notice that $\\|\\Pi(A_{\\alpha(s-t)}-\\mathbf{I})\\tilde{X}_{t K}\\|\\leq\\alpha(s-t)\\|\\tilde{V}_{t K}\\|$ and $\\|(\\mathbf{I}-\\Pi)(A_{\\alpha(s-t)}-\\mathbf{I})\\tilde{X}_{t K}\\|\\leq\\alpha\\gamma(s-$ $t)\\|\\tilde{V}_{t K}\\|$ . This implies: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathcal{M}_{p}((A_{\\alpha(s-t)}-\\mathbf{I})\\tilde{X}_{t K},\\Pi)\\leq\\alpha(s-t)\\mathcal{M}_{p}(\\tilde{V}_{t K})\n$$", "text_format": "latex", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathcal{M}_{p}((A_{\\alpha(s-t)}-\\mathbf{I})\\tilde{X}_{t K},\\mathbf{I}-\\Pi)\\leq\\alpha\\gamma(s-t)\\mathcal{M}_{p}(\\tilde{V}_{t K})\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Notice that: $\\begin{array}{r}{\\|\\Pi A_{\\alpha(s-h-1)}G_{\\alpha}b(\\tilde{X}_{h K})\\|\\leq\\frac{\\alpha}{\\gamma}\\|\\nabla F(\\tilde{U}_{h K})\\|}\\end{array}$ and $\\begin{array}{r}{\\|(\\mathbf{I}-\\Pi)A_{\\alpha(s-h-1)}G_{\\alpha}\\ensuremath{b(\\tilde{X}_{h K})}\\|\\leq}\\end{array}$ $\\alpha\\|\\nabla F(\\tilde{U}_{h K})\\|$ . This implies: ", "page_idx": 45}, {"type": "text", "text": "Mp $(A_{\\alpha(s-h-1)}G_{\\alpha}b(\\tilde{X}_{h K}),\\Pi)\\le\\frac{\\alpha}{\\gamma}\\mathcal{M}_{p}(\\nabla F(\\tilde{U}_{h K}))\\le\\frac{\\alpha L}{\\gamma}\\mathcal{M}_{p}(\\tilde{X}_{h K}-\\tilde{X}_{t K},\\Pi)+\\frac{\\alpha}{\\gamma}\\mathcal{M}_{p}(\\nabla F(\\tilde{U}_{t K}))$ $\\begin{array}{r}{M_{p}(A_{\\alpha(s-h-1)}G_{\\alpha}b(\\tilde{X}_{h K}),\\mathbf{I}-\\Pi)\\le\\alpha M_{p}(\\nabla F(\\tilde{U}_{h K}))\\le\\alpha L M_{p}(\\tilde{X}_{h K}-\\tilde{X}_{t K},\\Pi)+\\alpha M_{p}(\\nabla F(\\tilde{U}_{t K}),\\mathbf{I}-\\nabla F(\\tilde{U}_{t K})).}\\end{array}$ \uff09 Notice that: $\\begin{array}{r l r}{\\|\\Pi A_{\\alpha(s-h-1)}\\alpha\\Delta_{h}\\|}&{{}\\leq}&{\\frac{\\alpha L N_{h}}{\\gamma}\\operatorname*{sup}_{0\\leq i\\leq K-1}\\|\\hat{\\tilde{U}}_{h K+i}\\ -\\ \\tilde{U}_{h K}\\|\\mathrm{~\\\\and~\\\\|(I~-~\\alpha~)~}\\|\\tilde{U}_{h K+i}\\|_{\\mathrm{~\\\\\\alpha~-~\\}}}\\end{array}$ $\\begin{array}{r}{\\Pi)A_{\\alpha(s-h-1)}\\alpha\\Delta_{h}\\|\\leq\\alpha L N_{h}\\operatorname*{sup}_{0\\leq i\\leq K-1}\\|\\hat{\\tilde{U}}_{h K+i}-\\tilde{U}_{h K}\\|.}\\end{array}$ Applying item 1 in Lemma 21, we conclude: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\|\\Pi A_{\\alpha(s-h-1)}\\alpha\\Delta_{h}\\|\\leq\\frac{C\\alpha^{2}L N_{h}}{\\gamma}\\|\\tilde{V}_{h K}\\|+\\frac{C\\alpha^{3}L N_{h}}{\\gamma}\\|\\nabla F(\\tilde{U}_{h K})\\|+\\frac{C\\alpha L N_{h}}{\\gamma}M_{h K}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Therefore, we conclude: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M_{p}(A_{\\alpha(\\alpha-\\lambda-1)\\alpha\\lambda_{h}},\\Pi)\\le\\frac{C_{p}\\alpha^{2}L}{\\gamma}M_{p}(\\tilde{V}_{h k})+\\frac{C_{p}\\alpha^{3}L}{\\gamma}M_{p}(\\nabla F(\\tilde{U}_{h k}))+\\frac{C_{p}\\alpha^{2}L}{\\gamma}M_{p}(\\nabla F(\\tilde{U}_{h k}))+}\\\\ &{\\le\\frac{C_{p}\\alpha^{2}L}{\\gamma}M_{p}(\\tilde{V}_{h k})+\\frac{C_{p}\\alpha^{3}L}{\\gamma}M_{p}(\\nabla F(\\tilde{U}_{h k}))+\\frac{C_{p}\\alpha^{3}L^{2}}{\\gamma}M_{p}(\\tilde{V}_{h k}-\\tilde{X}_{h k},\\Pi)}\\\\ &{\\quad+\\frac{C_{p}\\alpha^{2}L}{\\gamma}M_{p}(\\tilde{M}_{h k})}\\\\ &{\\le\\frac{C_{p}\\alpha^{2}L}{\\gamma}M_{p}(\\tilde{V}_{h k})+\\frac{C_{p}\\alpha^{3}L}{\\gamma}M_{p}(\\nabla F(\\tilde{U}_{h k}))+\\frac{C_{p}\\alpha^{3}L^{2}}{\\gamma}M_{p}(\\tilde{X}_{t K}-\\tilde{X}_{h k},\\Pi)}\\\\ &{\\quad+\\frac{C_{p}\\alpha^{3}L}{\\gamma}\\frac{\\gamma(d+\\log K)}{\\gamma}}\\\\ &{\\le\\frac{C_{p}\\alpha^{2}L}{\\gamma}M_{p}(\\tilde{\\hat{V}}_{h k},\\Pi-\\Pi)+\\frac{C_{p}\\alpha^{2}L}{\\gamma}M_{p}(\\tilde{X}_{t K}-\\tilde{X}_{h k},\\Pi-\\Pi)+\\frac{C_{p}\\alpha^{3}L}{\\gamma}M_{p}(\\nabla F(\\tilde{U}_{t K}))}\\\\ &{\\quad+\\frac{C_{p}\\alpha^{3}L^{2}}{\\gamma}M_{p}(\\tilde{\\hat{V}}_{h k}-\\tilde{X}_{h k},\\Pi)+\\frac{C_{p}\\alpha^{3}L}{\\gamma}\\frac{\\gamma(d+\\log K)}{\\sqrt{\\gamma}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "In the first step, we have used the triangle inequality for $\\mathcal{M}_{p}$ along with the fact that $N_{h}$ is independent of $M_{h,K}$ and $\\tilde{X}_{h K}$ and the fact that its $p$ -th moment is bounded by a constant $C_{p}$ . In the second step, we have controlled the norm of $\\nabla F(\\tilde{U}_{h K})$ with that of $\\nabla F(\\tilde{U}_{t K})$ using Lipschitzness. In the third step, we have invoked item 3 of Lemma 21 to bound the moments of $M_{h,K}$ . In the fourth step, we have controlled the norm $\\tilde{V}_{h K}$ in terms of $\\tilde{X}_{t K}$ and $\\tilde{X}_{t K}-\\tilde{X}_{h K}$ ", "page_idx": 45}, {"type": "text", "text": "Similarly, we have: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{M}_{p}(A_{\\alpha(s-h-1)}\\alpha\\Delta_{h},\\mathbf{I}-\\Pi)\\le C_{p}\\alpha^{2}L\\mathcal{M}_{p}(\\tilde{X}_{t K},\\mathbf{I}-\\Pi)+C_{p}\\alpha^{2}L\\mathcal{M}_{p}(\\tilde{X}_{t K}-\\tilde{X}_{h K},\\mathbf{I}-\\Pi)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,C_{p}\\alpha^{3}L\\mathcal{M}_{p}(\\nabla F(\\tilde{U}_{t K}))+C_{p}\\alpha^{3}L^{2}\\mathcal{M}_{p}(\\tilde{X}_{t K}-\\tilde{X}_{h K},\\Pi)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,C_{p}\\alpha^{\\frac{5}{2}}\\sqrt{\\gamma}L\\sqrt{(d+\\log K)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "$\\begin{array}{r}{\\frac{\\alpha L(s-t)}{\\gamma}\\leq c_{p}}\\end{array}$ $\\alpha\\gamma(s-t)\\leq c_{p}$ and $\\alpha^{2}L(s-t)\\leq c_{p}$ for some small enough constant $c_{p}$ which depends only on $p$ we conclude: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\underset{t\\leq h\\leq s}{\\operatorname*{sup}}\\mathcal{M}_{p}(\\tilde{X}_{h K}-\\tilde{X}_{t K},\\Pi)\\leq2\\alpha(s-t)\\mathcal{M}_{p}(\\tilde{X}_{t K},{\\mathbf I}-\\Pi)+\\alpha\\underset{t\\leq h\\leq s}{\\operatorname*{sup}}\\mathcal{M}_{p}(\\tilde{X}_{t K}-\\tilde{X}_{h K},{\\mathbf I}-\\Pi)}&\\\\ &{\\quad}&{\\qquad+\\,\\frac{2\\alpha(s-t)}{\\gamma}\\mathcal{M}_{p}(\\nabla F(\\tilde{U}_{t K}))+C_{p}\\sqrt{\\frac{(d+\\log K)}{L}}\\qquad\\qquad(102\\pi)}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Similarly considering Equation (98), we have: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\leq h\\leq s}{\\operatorname*{sup}}\\mathcal{M}_{p}(\\tilde{X}_{h K}-\\tilde{X}_{t K},{\\mathbf I}-\\Pi)\\leq2\\alpha\\gamma(s-t)\\mathcal{M}_{p}(\\tilde{X}_{t K},{\\mathbf I}-\\Pi)+2\\alpha(s-t)\\mathcal{M}_{p}(\\nabla F(\\tilde{X}_{t K}))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\ 2\\alpha L(s-t)\\underset{t\\leq h\\leq s}{\\operatorname*{sup}}\\mathcal{M}_{p}(\\tilde{X}_{h K}-\\tilde{X}_{t K},{\\Pi})+C_{p}\\gamma\\sqrt{\\frac{d+\\log K}{L}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "From Equations (102) and (103), we conclude: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\underset{t\\leq h\\leq s}{\\operatorname*{sup}}\\mathcal{M}_{p}(\\tilde{X}_{h K}-\\tilde{X}_{t K},\\Pi)\\leq8\\alpha(s-t)\\mathcal{M}_{p}(\\tilde{X}_{t K},\\mathbf{I}-\\Pi)+\\frac{8\\alpha(s-t)}{\\gamma}\\mathcal{M}_{p}(\\nabla F(\\tilde{U}_{t K}))}&\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\left.C_{p}\\sqrt{\\frac{d+\\log K}{L}}\\right.}&{\\mathrm{otherwise}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\underset{t\\leq h\\leq s}{\\operatorname*{sup}}\\mathcal{M}_{p}(\\tilde{X}_{h K}-\\tilde{X}_{t K},\\mathbf{I}-\\Pi)\\leq8\\gamma\\alpha(s-t)\\mathcal{M}_{p}(\\tilde{X}_{t K},\\mathbf{I}-\\Pi)+8\\alpha(s-t)\\mathcal{M}_{p}(\\nabla F(\\tilde{U}_{t K}))}&\\\\ &{\\qquad}&{\\qquad+\\left.C_{p}\\gamma\\sqrt{\\frac{d+\\log K}{L}}\\right.}&{\\quad\\mathrm{(10~)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: The contents of the paper match the claims made in the abstract and introduction. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 47}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: We have discussed the limitations in a separate section. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should refect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 47}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 48}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: We explicitly reveal all the hyperparameters and open source models necessary to reproduce our results. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 48}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 48}, {"type": "text", "text": "Answer: [No] ", "page_idx": 49}, {"type": "text", "text": "Justification: We do not have institutional approval for code release. We provide detailed descriptions of the algorithms and hyper-parameters. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 49}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 49}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 49}, {"type": "text", "text": "Answer: [No] ", "page_idx": 49}, {"type": "text", "text": "Justification: It is common in image generation literature to compute FID over $50\\mathrm{k}$ generated images, without reporting error bars since the fuctuations are usually negligible. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 49}, {"type": "text", "text": "", "page_idx": 50}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: Details regarding the GPU, CPU and the RAM are provided in the paper. Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 50}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 50}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: The paper is purely technical and we do not foresee any societal impact due to this. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 50}, {"type": "text", "text": "", "page_idx": 51}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: We use already open sourced models and datasets, widely used in the research community. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 51}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 Ihe answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 51}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 52}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u00b7 'I'he answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 52}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 52}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 52}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 52}]