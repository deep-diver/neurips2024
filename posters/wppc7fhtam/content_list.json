[{"type": "text", "text": "IPO: Interpretable Prompt Optimization for Vision-Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yingjun ${\\bf D}{\\bf u}^{1^{*}}$ , Wenfang $\\operatorname{Sun}^{2}.$ ,\u2217 Cees G. M. Snoek1 1AIM Lab, University of Amsterdam 2University of Science and Technology of China ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Pre-trained vision-language models like CLIP have remarkably adapted to various downstream tasks. Nonetheless, their performance heavily depends on the specificity of the input text prompts, which requires skillful prompt template engineering. Instead, current approaches to prompt optimization learn the prompts through gradient descent, where the prompts are treated as adjustable parameters. However, these methods tend to lead to overfitting of the base classes seen during training and produce prompts that are no longer understandable by humans. This paper introduces a simple but interpretable prompt optimizer (IPO), that utilizes large language models (LLMs) to generate textual prompts dynamically. We introduce a Prompt Optimization Prompt that not only guides LLMs in creating effective prompts but also stores past prompts with their performance metrics, providing rich in-context information. Additionally, we incorporate a large multimodal model (LMM) to condition on visual content by generating image descriptions, which enhance the interaction between textual and visual modalities. This allows for the creation of dataset-specific prompts that improve generalization performance, while maintaining human comprehension. Extensive testing across 11 datasets reveals that IPO not only improves the accuracy of existing gradient-descent-based prompt learning methods but also considerably enhances the interpretability of the generated prompts. By leveraging the strengths of LLMs, our approach ensures that the prompts remain human-understandable, thereby facilitating better transparency and oversight for vision-language models. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vision-language models, trained on a diverse array of image-text pairs encapsulating a broad vocabulary of real-world concepts [1, 2, 3], have demonstrated notable adaptability across various downstream tasks [4, 5, 6, 7]. These models perform zero-shot image classification by filling in a predefined prompt template (e.g., \u201ca photo of a [CLASS]\u201d) with specific class names for the text encoder. Despite their effective generalization to new tasks, the performance can be influenced by minor changes in the wording of prompt templates [8]. Instead of manually creating handcrafted prompts, recent developments in natural language processing [9, 10] and computer vision [8, 11, 12, 13] have proposed methods to learn a set of soft prompts with minimal labeled data. Despite the strides made in learning prompts, the current state of the art remains limited by its lack of interpretability and the overfitting problems on the base classes, which can be prohibitive in diverse and dynamic application environments. These limitations underscore the need for a more adaptable and user-friendly approach to prompt optimization in vision-language models. ", "page_idx": 0}, {"type": "text", "text": "Drawing from recent advancements in using large language models (LLMs) as optimization tools [14], our paper, for the first time, incorporates these capabilities into vision-language modeling. Unlike gradient descent-based methods [8, 11, 13], which often fail to provide explanations for the generated ", "page_idx": 0}, {"type": "image", "img_path": "WPPC7FHtaM/tmp/4b987bb9327bb28a223e9b4ce9c466c1e3e87236f76a26678cd56fc5db521fec.jpg", "img_caption": ["(a) Gradient-based prompt optimization. ", "(b) Interpretable prompt optimization. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Comparison between traditional gradient-based prompt optimization (a) and our interpretable prompt optimization (b) for vision-language models. Traditional gradient descent-based prompt learning methods [8, 11] treat the text prompt as learnable parameters $V$ . By minimizing the loss through gradient descent on the training set, an optimized prompt $\\hat{V}$ is obtained after $I$ iterations, which is not interpretable by humans. In contrast, our interpretable prompt optimization leverages an LLM as optimizer to optimize the loss and accuracy. After $I$ iterations, the resulting optimized top prompt is effective and human-readable. ", "page_idx": 1}, {"type": "text", "text": "prompts and tend to overfti on base classes, natural language-based methods enable LLMs to develop and refine solutions through continuous feedback iteratively. This approach improves interpretability in complex tasks like prompt optimization for vision-language models, making it easier for humans to understand the generated prompts. However, existing research on these methods [14, 15, 16] primarily addresses language tasks and has not yet explored their potential for integrating prompt optimization with an LLM in vision-language models. ", "page_idx": 1}, {"type": "text", "text": "To address these challenges, this paper proposes an interpretable prompt optimizer (IPO) for visionlanguage models that leverages the capabilities of LLMs to generate and refine text prompts dynamically. First, we design a Prompt Optimization Prompt to prompt LLMs to generate more effective prompts that improve the accuracy of CLIP and reduce the loss in base classes. Our Prompt Optimization Prompt also stores past prompts along with their corresponding accuracy and loss as episodic memory, thereby providing richer in-context information to enable LLMs to generate more effective prompts. Second, to incorporate image information within the Prompt Optimization Prompt, we propose using a large multimodal model (LMM) to generate descriptions of images in base classes that can be added to the Prompt Optimization Prompt. This integration facilitates a more intuitive interaction between the textual and visual modalities, which allows the Prompt Optimization Prompt to utilize image information, thereby generating datasetspecific prompts to enhance the generalization performance of CLIP. The framework of our IPO, illustrated in Figure 1, showcases the comparison between traditional gradient-based prompt optimization and the proposed interpretable prompt optimization. Third, the prompts generated by our optimizer are human-interpretable. For example, on the Food101 dataset [17], the initial prompt evolves from [\u201ca photo of a [CLASS]\u201d] to [\u201cCategorize the image depicting a delicious and appetizing <CLASS> with remarkable visual qualities.\u201d]. Our generated prompts perform $10.29\\%$ better in novel classes than the gradient-based method CoOP [8], reducing overfitting while maintaining interpretability. ", "page_idx": 1}, {"type": "text", "text": "We validated our IPO across 11 different datasets, demonstrating that it surpasses traditional gradientbased state-of-the-art methods in accuracy and excels in interpretability. Our approach generates human-comprehensible prompts that can be seamlessly integrated into existing vision-language models to enhance performance. We conducted rigorous comparative experiments to quantify the interpretability between gradient-based prompt learning and our method. We demonstrated the importance of specific keywords in our generated prompts and revealed that not all tokens learned through traditional prompt learning methods are essential. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Prompt learning in vision-language models. Prompt learning, originally introduced in the natural language processing community [18, 19, 20], involves applying a fixed function to input tokens to provide task instructions to the model. In the computer vision community, prompt learning has been explored in various forms, including textual prompt tuning [8, 11, 21, 22, 23, 24], and prefix tuning [13, 12, 25, 26, 27, 28, 29]. 1) Prompt tuning mainly involves treating text prompts as learnable parameters, using a small amount of data to fine-tune these parameters. As pioneered by CoOp [8] and CoCoOp [11], which both fine-tune a CLIP vision-language model [30] for few-shot transfer by optimizing a continuous set of prompt vectors within its language branch. Bayesian prompt learning [21] formulated prompt learning as a variational inference problem and demonstrated its ability for unseen class generalization. 2) Prefix tuning primarily involves adding learnable tokens to the text encoder [31], vision encoder [12, 25], or both encoders [13, 27, 28, 32]. These tokens are fine-tuned using a small amount of data. Note that these methods do not optimize the initial text prompts. Instead, they focus on enhancing the model\u2019s understanding capabilities by integrating these additional, trainable tokens. Our method belongs to prompt tuning, but unlike previous approaches that use gradient descent to optimize prompts, we propose using LLMs to optimize prompts. Our method leverages the natural language capabilities of LLMs to iteratively refine feedback-based prompts, aiming to enhance both the effectiveness and the explainability of the prompts. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "LLMs as prompt optimizers. Several recent works explore the role of LLMs as prompt optimizers for NLP tasks. Some use LLMs to directly optimize the task instruction for in-context learning [33, 34, 14]. Other studies use LLMs to mutate prompts for evolutionary algorithms [35, 36]. However, to the best of our knowledge, no existing studies have investigated how LLMs could be used to optimize text prompts within vision-language models. This approach could potentially open up new avenues for integrating and enhancing the capabilities of vision-language models through more effective and contextually appropriate text prompts. ", "page_idx": 2}, {"type": "text", "text": "Meta-prompting. Suzgun and Kalai [37] introduce meta-prompting to transform a single LLM into a versatile \u201cconductor\u201d capable of managing and integrating multiple independent LLM queries. By using high-level instructions, meta-prompting guides the LLM in decomposing complex tasks into smaller subtasks. The core of OPRO [14] involves designing a meta-prompt for LLMs to optimize prompts for each task. This meta-prompt includes two key pieces of information: previously generated prompts with their corresponding training accuracies and a description of the optimization problem. Self-select [38] leverages meta-prompting to optimize instruction selection. It considers a set of provided templates and chooses the most suitable template. Meta-prompting is related to instruction tuning [39] as both techniques provide high-level guidance to improve the performance and adaptability of LLMs. However, while instruction tuning focuses on fine-tuning models with a variety of tasks to improve generalization, meta-prompting offers the advantage of dynamically guiding the model to decompose and manage complex tasks in real-time. Liu et al. [40] proposes a method that utilizes LLMs as black-box optimizers for vision-language models, iteratively refining prompts based on in-context examples. Their approach focuses on leveraging ChatGPT to improve prompt templates for visual classification tasks. Mirza et al. [41] explores a different aspect of prompt optimization by focusing on zero-shot vision-language models. Our Prompt Optimization Prompt is akin to meta-prompting, it stores past prompts along with their corresponding accuracy and loss, thereby providing richer in-context information to enable LLMs to generate more effective prompts. Different from prior meta-prompting, our Prompt Optimization Prompt generates prompts beyond LLMs for vision-language models. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Contrastive Language-Image Pre-Training (CLIP). The goal of CLIP [30] is to develop an image encoder $f_{I}$ and a text encoder $g_{T}$ via contrastive pre-training with a large collection of paired images and captions. This process aims to map image-text pairs to a common semantic space. After the pre-training phase, CLIP is able to perform zero-shot visual recognition by treating classification as a task of matching images to text. Specifically, the placeholder term \u201c[CLASS]\u201d is used within a prompt template (e.g., \u201ca photo of a [CLASS]\u201d) for the text encoder $g_{T}$ . Here, $g_{T}(\\mathbf{T}_{i})$ denotes the text features adapted for class $i$ , and the probability of classifying class $i$ from an image $\\mathbf{I}$ is: ", "page_idx": 2}, {"type": "equation", "text": "$$\np({\\boldsymbol y}\\!=\\!i|{\\bf I})\\!=\\!\\frac{\\exp(\\langle{\\boldsymbol g}_{T}({\\bf T}_{i}),f_{I}({\\bf I})\\rangle/\\tau)}{\\sum_{j=1}^{K}\\exp(\\langle{\\boldsymbol g}_{T}({\\bf T}_{j}),f_{I}({\\bf I})\\rangle/\\tau)},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\langle g_{T}(\\mathbf{T}_{i}),f_{I}(\\mathbf{I})\\rangle$ represents the cosine similarity between the image feature $f_{I}(\\mathbf{I})$ and the text feature $g_{T}(\\mathbf{T}_{i})$ specific to the $i$ -th class, $K$ is the total number of classes, and $\\tau$ is the temperature parameter that is tuned during training. ", "page_idx": 2}, {"type": "text", "text": "Prompt learning improves the adaptability of the CLIP model by eliminating the need for manual prompt engineering. It facilitates the automatic generation of prompts using a limited number of examples from a downstream task. CoOp [8] presents a method where a set of $M$ continuous context vectors $V{=}\\{v_{1},v_{2},\\ldots,v_{M}\\}$ serve as the learnable prompt. The constructed prompt ${\\cal T}_{i}{=}\\{v_{1},v_{2},\\ldots,v_{M},c_{i}\\}$ merges these learnable context vectors $V$ with the class-specific token embedding $c_{i}$ , which is then processed by the text encoder $g_{T}(\\cdot)$ . In $\\mathrm{CoOp}$ , the optimization of these static context vectors $V$ aims to minimize the negative log-likelihood for the correct class token: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{CE}}(V){=}-\\sum_{i}y_{i}\\log p(\\mathbf{T}_{i}|I),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "here, $\\pmb{y}_{i}$ represents the one-hot encoded ground-truth label for class $i$ . In downstream applications, the pre-trained model parameters are kept unchanged, which allows the learnable prompt vectors $V$ to be optimized efficiently using only a small number of samples through the minimization of the cross-entropy loss. ", "page_idx": 3}, {"type": "text", "text": "4 Methods ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Figure 1 depicts the comprehensive structure of our interpretable prompt optimizer. At each step of the optimization, the LLM generates candidate prompts for the vision-language task by considering both the description of the optimization problem and the feedback from previously evaluated prompts stored in the Prompt Optimization Prompt. These new prompts are then assessed and incorporated into the Prompt Optimization Prompt for future optimization cycles. The optimization process concludes either when the LLM can no longer generate prompts that improve the optimization scores, or when a predefined maximum number of optimization steps is reached. Next, we will detail the design of the Prompt Optimization Prompt and explain how image information is integrated into the Prompt Optimization Prompt. ", "page_idx": 3}, {"type": "text", "text": "Prompt Optimization Prompt design At the core of our optimizer is the design of the Prompt Optimization Prompt, which enhances the performance of the vision-language model by optimizing the prompts through the prompt LLM. Figure 2 shows an example of our Prompt Optimization Prompt. Our Prompt Optimization Prompt consists of the following components: (1) Instructions: These guide the LLM by clearly defining its task to optimize the prompt for achieving better performance in classification tasks. (2) Textual descriptions of training images: These descriptions provide the LLM with detailed information about the images, enabling it to generate dataset-specific prompts. (3) Previously generated prompts and corresponding scores: This component supplies in-context information, including past prompts and their performance metrics, allowing the LLM to refine its prompt generation more accurately. By incorporating these elements, our approach leverages the iterative refinement capabilities of LLMs to dynamically generate and optimize text prompts. The instructions ensure that the LLM understands the optimization goal, the textual descriptions offer rich image-related context, and the historical data aids in producing more effective and precise prompts. ", "page_idx": 3}, {"type": "text", "text": "Textual descriptions of training images For the textual descriptions of training images, we utilize a large multimodal model (LMM) to generate text descriptions for each training image. Specifically, we employ MiniCPM-V-2.0 [43] to generate descriptions of the content of images from base classes. In the appendix, we provide content descriptions for some images from each dataset generated using MiniCPM-V-2.0. We denote the extracted image textual features as $f_{M}(\\cdot)$ . ", "page_idx": 3}, {"type": "text", "text": "Additionally, we have attempted to directly optimize prompts using the LMM with the Prompt Optimization Prompt. Specifically, we input images from the base classes and the Prompt Optimization Prompt into the LMM, aiming for the LMM to generate better prompts. We experimented with six different LMMs: BLIP-2 [44], Qwen-VL-Chat-9.6B [45], FUYU-8B [46], MiniCPM-V-2.0 [43], and llava-llama3-8B [47]. Unfortunately, all six models failed to understand our Prompt Optimization Prompt and generated new prompts that were merely descriptions of the images, not the universal prompts we desired. This failure might be due to the fact that the training of these LMMs did not consider such a task. Note that image descriptions are not mandatory. In our 16-shot experiments, we omitted the image descriptions in the Prompt Optimization Prompt due to the limited text input length that the LLM can handle. ", "page_idx": 3}, {"type": "text", "text": "Episodic memory retrieval We utilize an episodic memory mechanism to retrieve past prompts and their corresponding scores, which include metrics such as loss and accuracy. Here, we denote the memory as $\\mathcal{M}$ . During each iteration, we retrieve the top-20 prompts $\\mathcal{R}(\\mathcal{M})$ , based on their accuracy from $\\mathcal{M}$ and use them as the current memory, denoted as $\\mathbf{m}$ . Moreover, we consistently include the prompt $\\mathbf{\\sigma}^{\\leftarrow}\\mathbf{a}$ photo of <CLASS>\u201d in our history at every step, as this is a frequently used and effective prompt within the CLIP framework [30]. Therefore, our optimization loss is defined as: ", "page_idx": 3}, {"type": "image", "img_path": "WPPC7FHtaM/tmp/4a8f2f7ed473ffa63ee9a54c545ab8f8514b5ccb8328e9e0a3883dbb6c142560.jpg", "img_caption": ["Figure 2: An example of our Prompt Optimization Prompt with input and output on the DTD [42] dataset. The red text represents instructions given to the large language model, the blue text denotes the image descriptions generated by the large multimodal model, and the green text indicates the top-20 previously generated prompts retrieved from episodic memory along with their corresponding scores. yellow indicates the output prompt. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{CE}}=-\\sum_{i}y_{i}\\log p(\\hat{\\mathbf{T}}_{i}|f_{M}(I),\\mathbf{m},\\mathcal{T}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{T}$ indicates the our designed instruction for LLM, $\\hat{\\pmb T}$ represents the new human-interpretable text prompt optimized by the LLM. Note that our optimizer is parameter-free, which differentiates it from traditional gradient-based prompt learning methods. Instead, we leverage the LLM to optimize the prompt, reducing $\\mathcal{L}_{\\mathrm{CE}}$ iteratively until convergence. ", "page_idx": 4}, {"type": "text", "text": "The input and output example in Figure 2 shows the structured information fed into the LLM, while output demonstrates the optimized prompts generated by the LLM. For more detailed examples of Prompt Optimization Prompt input and output, please refer to the appendix. ", "page_idx": 4}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1 Experimental setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We validate the effectiveness of our approach on the base-to-new generalization benchmark for evaluating prompt learning in vision-language models [8, 11]. Across all experiments, we benchmark the models\u2019 performance in a 1-shot and commonly used 16-shot setting. To ensure consistency, all results from learning-based methods are averaged over three random seeds. We use the harmonic mean (H) as the average metric, which is a common approach in prompt learning for vision-language models. ", "page_idx": 5}, {"type": "text", "text": "Eleven Datasets. We follow CLIP [30] and CoOp [8] to use 11 image classification datasets, i.e., ImageNet [48] and Caltech101 [49] for generic object classification, OxfordPets [50], StanfordCars [51], Flowers102 [52], Food101 [17] and FGVCAircraft [53] for fine-grained image recognition, EuroSAT [54] for satellite image classification, UCF101 [55] for action classification, DTD [42] for texture classification, and SUN397 [56] for scene recognition. ", "page_idx": 5}, {"type": "text", "text": "Six Baselines. To conduct a comparative evaluation, we utilize a number of established baselines including CLIP [30], Coop [8], CoCoOp [11], MaPLe [13], PromptSRC [28], and CoPrompt [32]. Note that all methods do not present 1-shot results in their publications, so we perform 1-shot experiments using their available code. ", "page_idx": 5}, {"type": "text", "text": "Training details. We use GPT-3.5 Turbo as our default optimizer, iterating 100 steps for each dataset to derive the final prompt. At each step, we generate five prompts and compare their accuracy with past prompts, storing the top-20 prompts in our history. Ultimately, we select the prompt with the highest accuracy as the final prompt. For generating image descriptions, we employ MiniCPM-V-2.0 [43] as the default LMM, using the prompt: \u201c[Please generate the description in detail, do not provide the class name in the description.]\u201d. We added image descriptions to the 1-shot Prompt Optimization Prompt but not to the 16-shot version due to the character input limitations of GPT-3.5 Turbo, which prevent adding detailed information for each class\u2019s images. All experiments were conducted on a GeForce RTX 3090. Our code is available at https: //github.com/lmsdss/IPO. ", "page_idx": 5}, {"type": "text", "text": "5.2 Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Interpretable prompt analysis. To analyze the importance of specific words or phrases in text prompts, we display a comparative experiment on the Flower102 dataset. Specifically, in Table 1a, we use \u201ca photo of a <CLASS>\u201d as the prompt, which is the most commonly utilized format. By employing occlusion sensitivity analysis, we individually remove each word to test the importance of the remaining words. We find that using just \u201c<CLASS>\u201d as the prompt performs only $0.89\\%$ lower on the novel classes than \u201ca photo of a <CLASS>\u201d, indicating that the CLIP model can generate more discriminative features using just the category name. Additionally, the prompt \u201ca photo <CLASS>.\u201d achieves the best performance. By comparing \u201c<CLASS>\u201d with \u201cphoto <CLASS>\u201d, we determine that the word photo is particularly significant on the novel classes. ", "page_idx": 5}, {"type": "text", "text": "In Table 1b, we present the results of the CoOP and CoCoOP models when some learned prompt tokens are removed to analyze which token is most crucial. Surprisingly, for both models, performance improves in novel classes when some or all tokens are removed. This indicates severe overfitting in base classes by these models, where the learned tokens are only applicable to base classes. Removing all tokens allows the models to retain some of the original performance of CLIP on novel classes. Additionally, these methods, which only learn tokens, make it challenging for humans to understand the specific meanings of each token, complicating interpretation. ", "page_idx": 5}, {"type": "text", "text": "In Table 1c, we also demonstrate the final prompt produced by our model: \u201cIdentify the unique visual features of the ${\\tt{<}}\\mathrm{{CLASS}}{>}$ flower accurately,\u201d which achieves a performance of $79.6\\%$ , surpassing the original CLIP by $2.9\\%$ on novel classes. When comparing this optimal prompt with others, removing any words from it results in worse performance than the original prompt. Specifically, comparing \u201cIdentify $\\mathsf{<C L A S S>^{\\circ}}$ with $^{66}{<}\\mathrm{CLASS}>^{\\circ}$ reveals that including \"Identify\" boosts performance by $1.7\\%$ on novel classes. This highlights the importance of the word \u201cIdentify\u201d in datasets like Flower102. We have included additional analyses on various datasets in the appendix. ", "page_idx": 5}, {"type": "text", "text": "Overall, the comparative experiments demonstrate that our prompt can be more easily interpreted and understood by humans, while also providing insights into the significance of certain key words ", "page_idx": 5}, {"type": "table", "img_path": "WPPC7FHtaM/tmp/4dbbe04485c87b8bf0384a2af3de0ba95979e62ce9ee2fc838ec2f17c619bfe3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "WPPC7FHtaM/tmp/e1f6830924c5976f3cd40fcf4a531aa1eac19978fdf6b17725e234cc6df1dcf9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "WPPC7FHtaM/tmp/272ae208af3745db464d0bd0db071572f7332ba26d3a0180dac7c160826d97f4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "(c) This paper: IPO ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Table 1: Comparison of various prompts with occlusion sensitivity analysis across different models on the Flower102 dataset [52]. The shaded areas in the table indicate the original performance of each method. Bold blue refers to the result of the best prompt for each model. The original CLIP model shows particular sensitivity to the word photo. In contrast, the tokens learned by CoOP and CoCoOP affect especially base class performance, while removing these learned tokens improves novel class performance. By contrast, with our interpretable prompt optimization, every word makes a meaningful contribution to both base and new classes. We provide results for more datasets in the appendix. ", "page_idx": 6}, {"type": "table", "img_path": "WPPC7FHtaM/tmp/0f7bc1d52ce50f5b21816e660e57249f82a47785b21062e2e532877cbbb1d5ad.jpg", "table_caption": [], "table_footnote": ["(a) Impact of large language model. "], "page_idx": 6}, {"type": "table", "img_path": "WPPC7FHtaM/tmp/3969970673bf75cfb47f61fcadbb2fcb24d12386512c1bbfc8be16c1cc71e333.jpg", "table_caption": [], "table_footnote": ["(b) Impact of large multimodal model. "], "page_idx": 6}, {"type": "text", "text": "in the prompt. This understanding can guide us in identifying crucial words that enhance prompt effectiveness. ", "page_idx": 6}, {"type": "text", "text": "Effect of LLM and LMM choice. In Table 2, we analyze the effect of the choice of the LLM optimizer and LMM for generating image descriptions as Prompt Optimization Prompt inputs. We evaluate various models and report their average performance across 11 datasets. Using GPT-3.5- turbo as the optimizer results in better prompt generation, especially improving performance on novel classes. The training speed of our IPO is heavily influenced by the computational efficiency of LLMs used. Since PaLM and GPT-3.5 are not open-source, we rely on their respective APIs to generate prompts. Consequently, our training speed depends on the API call latency and the computational complexity of these models. Alternatively, Phi2 and Phi3 are open-source, allowing us to generate prompts directly using their weights. Therefore, for researchers and practitioners seeking faster and more cost-effective training, we recommend utilizing open-source large models for prompt generation. When comparing different LMMs, we found that LLaVA-Llama-3-8B and MiniCPM-V-2.0 perform almost identically on average. However, MiniCPM-V-2.0 shows better performance on novel classes. Before using LLMs, we first use LMMs to generate image descriptions, which are then used as inputs for the LLMs. In terms of computation cost, MiniCPM-V-2.0, with its smaller parameter size, generates descriptions more quickly. Therefore, we recommend this lightweight LMM as the image description generator for more efficient processing. In summary, the text comprehension capabilities of LLMs are crucial for determining the quality of the optimized prompts. ", "page_idx": 6}, {"type": "table", "img_path": "WPPC7FHtaM/tmp/d0cfef6583ffe7f12bb5b5c901080371ecea2d3c35eb1ce3987f0c736ca7209f.jpg", "table_caption": ["Table 3: Interpretable prompts generated by our method for each dataset in 1-shot scenarios. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Interpretable prompts generated per dataset. Table 3 showcases the diverse prompts generated by our optimizer for each dataset. Our method enhances model accuracy by concentrating on the most relevant attributes, such as those in Flowers102 [52] and Food101 [17]. Consequently, it delivers high-quality text prompts that improve vision-language models. We encourage future researchers to leverage these interpretable prompts in their own downstream tasks. ", "page_idx": 7}, {"type": "text", "text": "Benefit of image description. In Table 4, we assess the impact of incorporating image descriptions into the Prompt Optimization Prompt on the performance of the optimizer. The inclusion of image descriptions enhances the model\u2019s performance. This improve", "page_idx": 7}, {"type": "text", "text": "ment suggests that IPO, when generating new prompts, can effectively integrate information from the images themselves. As a result, the optimizer is able to produce prompts that are more specific to the data, thereby increasing the relevance and accuracy of the generated content. This highlights the importance of multimodal inputs in optimizing the prompting abilities of el ", "page_idx": 7}, {"type": "table", "img_path": "WPPC7FHtaM/tmp/468f868ce314804e7a6d52ad295bafdf24eff7ca27a4c0fda967f2e809d19e61.jpg", "table_caption": ["Table 4: Benefti of image description. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Comparison with knowledge bank-based prompt learning methods. Our IPO leverages LLMs to optimize prompts, which conceptually aligns with previous bank-based prompt learning approaches. To assess its relative effectiveness, we compared IPO to traditional bank-based methods, specifically using L2P [57], which learns to dynamically prompt (L2P) a pre-trained model to learn tasks sequentially under different task transitions. We apply L2P within the visual prompt tuning (VPT) [12] framework, which also learns prompts in the visual space and applies them to few-shot visionlanguage model tasks. Similar to our IPO, VPT $^+$ L2P learns prompts within the visual space and applies them to few-shot VLM tasks. In this setup, $\\mathrm{VPT}+\\mathrm{L2P}$ trains a prompt bank, allowing test samples to query the bank for suitable prompts during testing. The table 5 presents a comparison of $\\mathrm{VPT}+\\mathrm{L2P}$ and our method across 11 datasets in the 16-shot setting. While L2P contributes to enhanced VPT performance, confirming ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "WPPC7FHtaM/tmp/c6248df26767b6fae807a0687c4eede9f5706d76cb483f391dcf200c62d1e766.jpg", "table_caption": ["Table 5: Comparison with knowledge bankbased prompt learning methods. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "WPPC7FHtaM/tmp/2d76f35a7a17da0bc1bf93789ac6374b20dd20a87f9b8ff49cacd501dbb6adf9.jpg", "img_caption": ["Table 6: Comparison with existing state-of-the-art methods for base-to-novel generalization using 1-shot learning. Except for CLIP, the results for other methods are based on our reimplementation of their official code. Our proposed IPO exhibits robust generalization capability and achieves significant improvements on novel classes across 11 datasets. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "L2P\u2019s efficacy in the VLM domain, our IPO method still demonstrates superior results compared to $\\mathrm{VPT}+\\mathrm{L2P}\\!\\!$ . This comparison, which we will include in the revised manuscript, reinforces that IPO \u2019s effectiveness is not merely due to a memory retrieval mechanism but also benefits from prompt optimization through LLMs. ", "page_idx": 8}, {"type": "text", "text": "Comparison with state-of-the-art. Table 6 shows the comparative experiments of IPO against the state-of-the-art across 11 datasets in a 1-shot setting. Our method excels in the novel classes, ", "page_idx": 8}, {"type": "text", "text": "surpassing the second-best performer, the original CLIP [30], by $2.78\\%$ in average performance. Other methods do not perform as well as CLIP in novel classes, indicating overftiting to base classes. For instance, CoCoOP [11] performs better in base classes but falls behind the original CLIP by $2.05\\%$ in novel classes. In particular, on the most challenging FGVCAircraft [53], and EuroSAT [54] dataset, the current state-of-the-art model, CoPrompt [32], performs poorly. This is because methods based on prefixtuning, like CoPrompt [32], PromptSRC [28], require substantial amounts of data for training to achieve adequate generalization. Consequently, on more challenging datasets, when data is scarce, it becomes difficult to finetune these prefixes effectively. In contrast, our model outperforms other methods, exceeding the second-best by $1.78\\%$ in harmonic mean. Additionally, in Table 7, we compared our method\u2019s performance with other traditional gradient-based prompt learning methods on a 16-shot setting across all datasets, where our approach consistently performs well in novel classes. Demonstrating that our approach can mitigate overfitting and generalize better to novel classes. ", "page_idx": 8}, {"type": "table", "img_path": "WPPC7FHtaM/tmp/dd0ac547af329151cbea8251d6b9a4e800285f8f3161bda800b85bae294a0b21.jpg", "table_caption": [], "table_footnote": ["Table 7: Comparison with gradient-based prompt learning methods for 16-shots across 11 datasets. "], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we presented a novel approach to prompt optimization for vision-language models, addressing the limitations of existing gradient-descent-based methods. By integrating large language models for dynamic text prompt generation and optimization, we introduced the IPO system. This system guides LLMs in crafting effective prompts while maintaining a record of past prompts and their performance metrics, offering valuable in-context information. Additionally, we incorporated large multimodal models to generate image descriptions, enhancing the synergy between textual and visual modalities. Our comprehensive evaluation across 11 datasets demonstrated that our method improves the initial accuracy of vision-language models compared to traditional gradient-descentbased prompt learning methods. Most notably, our approach significantly enhances the interpretability of the generated prompts. By leveraging the strengths of LLMs, IPO ensures that the prompts remain human-understandable, thereby facilitating better transparency and oversight for vision-language models. This improvement in interpretability is crucial, as it allows for more effective and trustworthy human-AI collaboration, making vision-language systems more reliable and accessible. ", "page_idx": 9}, {"type": "text", "text": "Limitation. Our IPO method is primarily designed for few-shot scenarios. However, when dealing with large domain-specific datasets, the need to generate extensive image descriptions, which can lead to substantial computational costs due to the large text inputs required for LLMs. Currently, our model uses an input length of approximately 5,000 tokens. When scaled to larger datasets, the input length may increase to around 50,000 tokens. Using GPT-4 with an $8\\mathbf{k}$ context length, the cost for our current input size (5,000 tokens) is approximately 0.15 dollars per input (0.03 dollars per 1,000 tokens). For the expanded input size of 50,000 tokens, the cost would rise to approximately 3.00 dollars per input. If we were to use GPT-4 with a $32\\mathbf{k}$ context length, the cost for the 50,000-token input would be approximately 3.00 dollars for the first 32,000 tokens and an additional 1.08 dollars for the remaining 18,000 tokens, totaling approximately 4.08 dollars per input. Since our IPO method requires 100 iterations during training, the costs would multiply accordingly when scaled to large inputs. In future work, we aim to investigate methods for LMM fine-tuning to enable the direct input of both images and text, thereby generating even more sample-specific prompts. ", "page_idx": 9}, {"type": "text", "text": "Broader Impact. This paper explores the use of an LLM as an optimizer for refining text prompts in vision-language models. We introduce a straightforward yet interpretable approach to prompt optimization, which holds potential for societal impact, particularly in vision-language tasks. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is financially supported by the Inception Institute of Artificial Intelligence, the University of Amsterdam and the allowance Top consortia for Knowledge and Innovation (TKIs) from the Netherlands Ministry of Economic Affairs and Climate Policy. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 8748\u20138763. PMLR, 2021.   \n[2] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, pages 4904\u20134916. PMLR, 2021.   \n[3] Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, and Siliang Tang. Fine-grained semantically aligned vision-language pre-training. NeurIPS, 35:7290\u20137303, 2022.   \n[4] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, pages 740\u2013755. Springer, 2014.   \n[5] Mengze Li, Tianbao Wang, Haoyu Zhang, Shengyu Zhang, Zhou Zhao, Jiaxu Miao, Wenqiao Zhang, Wenming Tan, Jin Wang, and Peng Wang. End-to-end modeling via information tree for one-shot natural language spatial video grounding. In ACL, pages 8707\u20138717, 2022.   \n[6] Mengze Li, Han Wang, Wenqiao Zhang, Jiaxu Miao, Zhou Zhao, Shengyu Zhang, Wei Ji, and Fei Wu. Winner: Weakly-supervised hierarchical decomposition and alignment for spatiotemporal video grounding. In CVPR, pages 23090\u201323099, 2023.   \n[7] Wenqiao Zhang, Haochen Shi, Jiannan Guo, Shengyu Zhang, Qingpeng Cai, Juncheng Li, Sihui Luo, and Yueting Zhuang. Magic: Multimodal relational graph adversarial inference for diverse and unpaired text-based image captioning. In AAAI, pages 3335\u20133343, 2022.   \n[8] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. arXiv preprint arXiv:2109.01134, 2021.   \n[9] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In EMNLP, 2021.   \n[10] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt understands, too. arXiv preprint arXiv:2103.10385, 2021.   \n[11] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In CVPR, 2022.   \n[12] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. arXiv preprint arXiv:2203.12119, 2022.   \n[13] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. Maple: Multi-modal prompt learning. In CVPR, pages 19113\u201319122, 2023.   \n[14] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023.   \n[15] Jiale Cheng, Xiao Liu, Kehan Zheng, Pei Ke, Hongning Wang, Yuxiao Dong, Jie Tang, and Minlie Huang. Black-box prompt optimization: Aligning large language models without model training. arXiv preprint arXiv:2311.04155, 2023.   \n[16] Hao Sun. Offline prompt evaluation and optimization with inverse reinforcement learning. arXiv preprint arXiv:2309.06553, 2023.   \n[17] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative components with random forests. In ECCV, 2014.   \n[18] Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980, 2020.   \n[19] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language models know? TACL, 8:423\u2013438, 2020.   \n[20] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1\u201335, 2023.   \n[21] Mohammad Mahdi Derakhshani, Enrique Sanchez, Adrian Bulat, Victor G Turrisi da Costa, Cees GM Snoek, Georgios Tzimiropoulos, and Brais Martinez. Bayesian prompt learning for image-language model generalization. In ICCV, pages 15237\u201315246, 2023.   \n[22] Yuning Lu, Jianzhuang Liu, Yonggang Zhang, Yajing Liu, and Xinmei Tian. Prompt distribution learning. In CVPR, pages 5206\u20135215, 2022.   \n[23] Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, and Hanwang Zhang. Prompt-aligned gradient for prompt tuning. In ICCV, pages 15659\u201315669, 2023.   \n[24] Hantao Yao, Rui Zhang, and Changsheng Xu. Visual-language prompt tuning with knowledgeguided context optimization. In CVPR, pages 6757\u20136767, 2023.   \n[25] Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and Phillip Isola. Exploring visual prompts for adapting large-scale models. arXiv preprint arXiv:2203.17274, 2022.   \n[26] Qihang Zhou, Guansong Pang, Yu Tian, Shibo He, and Jiming Chen. Anomalyclip: Objectagnostic prompt learning for zero-shot anomaly detection. arXiv preprint arXiv:2310.18961, 2023.   \n[27] Zheng Li, Xiang Li, Xinyi Fu, Xing Zhang, Weiqiang Wang, and Jian Yang. Promptkd: Unsupervised prompt distillation for vision-language models. arXiv preprint arXiv:2403.02781, 2024.   \n[28] Muhammad Uzair Khattak, Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, and Fahad Shahbaz Khan. Self-regulating prompts: Foundational model adaptation without forgetting. In CVPR, pages 15190\u201315200, 2023.   \n[29] Zehao Xiao, Jiayi Shen, Mohammad Mahdi Derakhshani, Shengcai Liao, and Cees GM Snoek. Any-shift prompting for generalization over distributions. arXiv preprint arXiv:2402.10099, 2024.   \n[30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, and Jack Clark. Learning transferable visual models from natural language supervision. In ICML, 2021.   \n[31] Muhammad Uzair Khattak, Muhammad Ferjad Naeem, Muzammal Naseer, Luc Van Gool, and Federico Tombari. Learning to prompt with text only supervision for vision-language models. arXiv preprint arXiv:2401.02418, 2024.   \n[32] Shuvendu Roy and Ali Etemad. Consistency-guided prompt learning for vision-language models. arXiv preprint arXiv:2306.01195, 2023.   \n[33] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. In ICLR, 2022.   \n[34] Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization with\" gradient descent\" and beam search. arXiv preprint arXiv:2305.03495, 2023.   \n[35] Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu Yang. Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. arXiv preprint arXiv:2309.08532, 2023.   \n[36] Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rockt\u00e4schel. Promptbreeder: Self-referential self-improvement via prompt evolution. arXiv preprint arXiv:2309.16797, 2023.   \n[37] Mirac Suzgun and Adam Tauman Kalai. Meta-prompting: Enhancing language models with task-agnostic scaffolding. arXiv preprint arXiv:2401.12954, 2024.   \n[38] Keshav Ramji and Alexander Kyimpopkin. Self-select: Optimizing instruction selection for large language models. In NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023.   \n[39] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. Instruction tuning for large language models: A survey. arXiv preprint arXiv:2308.10792, 2023.   \n[40] Shihong Liu, Samuel Yu, Zhiqiu Lin, Deepak Pathak, and Deva Ramanan. Language models as black-box optimizers for vision-language models. In CVPR, pages 12687\u201312697, 2024.   \n[41] M Jehanzeb Mirza, Leonid Karlinsky, Wei Lin, Sivan Doveh, Jakub Micorek, Mateusz Kozinski, Hilde Kuhene, and Horst Possegger. Meta-prompting for automating zero-shot visual recognition with llms. arXiv preprint arXiv:2403.11755, 2024.   \n[42] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In CVPR, 2014.   \n[43] Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang, Xu Han, Yankai Lin, Jiao Xue, Dahai Li, Zhiyuan Liu, and Maosong Sun. Large multilingual models pivot zero-shot multimodal learning across languages. arXiv preprint arXiv:2308.12038, 2023.   \n[44] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. In ICML, pages 19730\u201319742. PMLR, 2023.   \n[45] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023.   \n[46] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sagnak Tas\u0131rlar. Introducing our multimodal models, 2023. URL https://www. adept. ai/blog/fuyu-8b, 2, 2023.   \n[47] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 36, 2023.   \n[48] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, pages 248\u2013255, 2009.   \n[49] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In CVPR, 2004.   \n[50] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In CVPR, 2012.   \n[51] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In ICCV, 2013.   \n[52] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In ICVGIP, 2008.   \n[53] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Finegrained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.   \n[54] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2019.   \n[55] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.   \n[56] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In CVPR, 2010.   \n[57] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning. In CVPR, pages 139\u2013149, 2022.   \n[58] Mengde Xu, Zheng Zhang, Fangyun Wei, Yutong Lin, Yue Cao, Han Hu, and Xiang Bai. A simple baseline for open-vocabulary semantic segmentation with pre-trained vision-language model. In European Conference on Computer Vision, pages 736\u2013753. Springer, 2022.   \n[59] Ziqin Zhou, Yinjie Lei, Bowen Zhang, Lingqiao Liu, and Yifan Liu. Zegclip: Towards adapting clip for zero-shot semantic segmentation. In CVPR, pages 11175\u201311185, 2023.   \n[60] Yongqin Xian, Subhabrata Choudhury, Yang He, Bernt Schiele, and Zeynep Akata. Semantic projection network for zero-and few-label semantic segmentation. In CVPR, pages 8256\u20138265, 2019.   \n[61] Maxime Bucher, Tuan-Hung Vu, Matthieu Cord, and Patrick P\u00e9rez. Zero-shot semantic segmentation. NeurIPS, 32, 2019.   \n[62] Zhangxuan Gu, Siyuan Zhou, Li Niu, Zihan Zhao, and Liqing Zhang. Context-aware feature generation for zero-shot semantic segmentation. In ICMM, pages 1921\u20131929, 2020.   \n[63] Jiaxin Cheng, Soumyaroop Nandi, Prem Natarajan, and Wael Abd-Almageed. Sign: Spatialinformation incorporated generative network for generalized zero-shot semantic segmentation. In ICCV, pages 9556\u20139566, 2021.   \n[64] Donghyeon Baek, Youngmin Oh, and Bumsub Ham. Exploiting a joint embedding space for generalized zero-shot semantic segmentation. In ICCV, pages 9536\u20139545, 2021.   \n[65] Jian Ding, Nan Xue, Gui-Song Xia, and Dengxin Dai. Decoupling zero-shot semantic segmentation. In CVPR, pages 11583\u201311592, 2022.   \n[66] Yassine Ouali, Adrian Bulat, Brais Matinez, and Georgios Tzimiropoulos. Black box few-shot adaptation for vision-language models. In ICCV, pages 15534\u201315546, 2023.   \n[67] Guangyi Chen, Weiran Yao, Xiangchen Song, Xinyue Li, Yongming Rao, and Kun Zhang. Plot: Prompt learning with optimal transport for vision-language models. arXiv preprint arXiv:2210.01253, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "image", "img_path": "WPPC7FHtaM/tmp/927c233c2f884b3fb147891892ec2b81fb7cd829d76f8cfa6be11595074f0ea5.jpg", "img_caption": ["Table 8: Comparison with existing state-of-the-art methods for base-to-novel generalization using 16-shots learning. Our proposed IPO exhibits robust generalization capability and achieves significant improvements on novel classes across 11 datasets. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "WPPC7FHtaM/tmp/649b48c4863591f9d2bcb1ed048591d89ff5e8b98900ee3853649f06d2f79291.jpg", "img_caption": ["Table 9: Cross-dataset generalization. Accuracy $(\\%)$ evaluation for prompts learned from the source dataset. Our IPO consistently outperforms existing prompt learning methods. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "A Experiments on 16-Shots ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We report the 16-shot performance of our IPO method across 11 datasets, providing detailed results for the Base, Novel, and H metrics in Table 8. Our IPO method consistently outperforms all other approaches on the novel classes and the H metric, highlighting its effectiveness in reducing overftiting. ", "page_idx": 14}, {"type": "text", "text": "B Experiments on cross-dataset ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We conducted a comprehensive cross-dataset experimental evaluation using the standard 16-shot setting to assess the performance of our IPO method. The results, presented in Table 9, demonstrate that IPO consistently outperforms previous gradient-based prompt learning approaches. By applying our task-agnostic, LLM-driven prompt optimization technique, IPO achieved superior results across various datasets, showcasing its robustness and generalizability. These findings highlight the effectiveness of IPO in adapting to diverse tasks and domains, further reinforcing its advantage over traditional gradient-based methods in few-shot learning scenarios. ", "page_idx": 14}, {"type": "table", "img_path": "WPPC7FHtaM/tmp/65411b3befb0a9c0579d4f901a658976fc075bca1cb2bd3e3225203c2ae21b37.jpg", "table_caption": [], "table_footnote": ["Table 10: Impact of large language model. "], "page_idx": 15}, {"type": "table", "img_path": "WPPC7FHtaM/tmp/c3d3b85b7bf0a1b8581d035d9d069ef67b92d88e7e62be1acf8567a99f8a35d6.jpg", "table_caption": [], "table_footnote": ["Table 11: Impact of large language model. "], "page_idx": 15}, {"type": "text", "text": "C Impact of LLM ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "As demonstrated in Table 10, upgrading the LLM capacity yielded a similarly positive impact on performance. To explore how a more advanced LLM, like GPT-4, could generate more effective prompts for our model, we conducted additional experiments with both GPT-4 and GPT-4o. Specifically, when we enhanced the LLM to GPT-4o and paired it with the GPT-4o LMM, we observed a significant overall increase in the H-score by $1.77\\%$ compared to the initial setup using GPT-3.5-turbo alongside MiniCPM-V-2. This improvement underscores the advantages of employing larger, more capable models, as they facilitate greater task generalization and more robust performance. The findings suggest that scaling up model capacity in both the LMM and LLM components can lead to substantial gains in prompt quality and adaptability across various tasks, indicating a promising direction for further enhancing our model\u2019s versatility and effectiveness. ", "page_idx": 15}, {"type": "text", "text": "D Experiments on Segmentation Task with IPO ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In prompt-based vision tasks like segmentation and detection, the design of the text prompt plays a pivotal role. Our task-agnostic method, IPO, can be seamlessly integrated into various vision tasks to optimize text prompts. For example, as shown in Table 13, we applied IPO to pre-trained semantic segmentation models [58, 59], where the original prompt used was \u2019a photo of a [CLASS].\u2019 By utilizing GPT-4o as both the LLM and LMM, we generated more effective, context-specific text prompts tailored to the open-vocabulary semantic segmentation task, leading to significant performance improvements. These results highlight the potential of IPO to enhance text prompt design in segmentation tasks, showcasing its adaptability and value. We plan to extend our exploration of IPO to other vision tasks in future studies, aiming to further validate its effectiveness in optimizing prompt construction across a broader range of applications. ", "page_idx": 15}, {"type": "text", "text": "IPO with GPT-3.5 Turbo, indeed, does not show an improvement on the large-scale ImageNet. This is because ImageNet has a large number of classes and samples, which results in longer LLM input when generating descriptions for each sample. GPT-3.5 Turbo has limited performance in handling long-text inputs. The table 12 shows the results on ImageNet when IPO uses GPT-4o, which has superior long-text understanding compared to GPT-3.5 Turbo. We found that IPO using GPT-4o leads to better performance improvements over other methods as well as a considerable improvement over IPO with GPT-3.5 Turbo. ", "page_idx": 15}, {"type": "text", "text": "E Experiments on segmentation task with IPO ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In other prompt-based vision tasks, such as segmentation and detection, the design of the text prompt is crucial. Our method, being task-agnostic, can be easily embedded into any vision task to optimize the text prompt. For instance, as shown in Table 13, we incorporated IPO into pre-trained semantic segmentation models [58, 59], where the original text prompt was \"a photo of a [CLASS].\" Using GPT-4o as the LLM and LMM, we crafted more effective text prompts specifically suited to the open-vocabulary semantic segmentation task, leading to enhanced performance and demonstrating the value of IPO in optimizing text prompts for this application. We intend to further investigate the use of IPO in other vision tasks in future work. ", "page_idx": 15}, {"type": "table", "img_path": "WPPC7FHtaM/tmp/d889d11b89c79a97ca23ca0801b1c610fc6020202a19d3ddf9f11d9ac27df9a8.jpg", "table_caption": [], "table_footnote": ["Table 12: Performance on large-scale generic datasets. "], "page_idx": 16}, {"type": "table", "img_path": "WPPC7FHtaM/tmp/bd0bac94ecc398a58d544815f165efb68fb886af361b7e1590d40ef3cfad48bc.jpg", "table_caption": [], "table_footnote": ["Table 13: Experiments on segmentation tasks. "], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "F Effect of batch size ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The Table 14 compares the performance of GPT-3.5 turbo and GPT-4o across different batch sizes. We observed that as the batch size increases to 128, GPT-3.5 turbo\u2019s performance begins to decline due to its limited capacity for handling longer input texts effectively. In contrast, GPT-4o maintains strong performance even at larger batch sizes. However, using extremely large batch sizes with GPT-4o becomes cost-prohibitive. Therefore, we selected a batch size of 128 for our experiments. Although even larger batch sizes could potentially improve performance further, the cost considerations become a critical factor. ", "page_idx": 16}, {"type": "table", "img_path": "WPPC7FHtaM/tmp/9ac72513cda0249ed6e409ae09ac291a3f302615842fb98eb493f67aca02a409.jpg", "table_caption": [], "table_footnote": ["Table 14: Effect of batch size. "], "page_idx": 16}, {"type": "table", "img_path": "WPPC7FHtaM/tmp/fcb34bd5fece05ceec3f2fe38d851ef8397a3d269034e22fafb1b2ee13a300cd.jpg", "table_caption": ["Table 15: Impact of prompt history length. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "G Impact of prompt history length ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We evaluated the effect of varying prompt history lengths on model performance, as shown in Table 15. Our findings indicate that without prompt history, performance declines due to the absence of contextual information, making it challenging for the LLM to converge. As the history length increases, performance progressively improves, with convergence observed at $\\scriptstyle{\\mathtt{n}=20}$ . Although using $\\scriptstyle{\\ n=100}$ yields the highest average performance, the extended input length significantly raises API costs. As a result, we selected $\\scriptstyle{\\mathtt{n}=20}$ for our IPO, balancing performance gains with cost efficiency. ", "page_idx": 17}, {"type": "table", "img_path": "WPPC7FHtaM/tmp/ccc065ac604502823fa4f16bb00b08ddeee3777135bc57f903e1bdc48937e243.jpg", "table_caption": ["Table 16: Comparison with recent prompt learning methods "], "table_footnote": ["(a) Comparison with LFA across 11 datasets in 16-shot(b) Comparison with PLOT on average accuracy across scenarios. 11 datasets in 1-shot and 16-shot scenarios. "], "page_idx": 17}, {"type": "text", "text": "H Comparison with recent prompt learning methods ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We conducted a comparative evaluation with LFA [66] and PLOT [67] under the same experimental conditions, as shown in Tables 16. Our IPO method consistently outperforms both LFA and PLOT across the benchmarks. ", "page_idx": 17}, {"type": "text", "text": "I Detailed Prompt Optimization Prompt ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Figure 3, 4, 5 and 6, 7, 8, we show detailed inputs and outputs of different training steps in our Prompt Optimization Prompt. We observed that each optimized prompt is unique, and both loss and accuracy exhibit a downward trend. ", "page_idx": 17}, {"type": "text", "text": "J Loss and accuracy curve. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To demonstrate that IPO can indeed serve as optimizer for prompt learning in vision-language models, we present the optimization process\u2019s loss and accuracy on ImageNet in Figure 9. As the training steps increase, the loss consistently decreases, and the accuracy gradually improves, proving the optimization capability of IPO. Additionally, IPO not only allows for interpretable prompt generation but also reduces the risk of overfitting during training. ", "page_idx": 17}, {"type": "image", "img_path": "WPPC7FHtaM/tmp/bc9826353f188e5e0d9f0ced618cab5daf90747cea285be8baa135cac73810fa.jpg", "img_caption": ["Figure 3: An example of our Prompt Optimization Prompt with input with initial instruction on the Food101 [17] dataset. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "K More generated prompts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Table 17, we provide detailed prompts for each dataset in 16-shot format. We encourage future researchers to utilize these interpretable prompts in their own downstream tasks. ", "page_idx": 18}, {"type": "text", "text": "L Image description with LMM ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Table 18, we provide descriptions of some training samples generated using Mini-CPM-V-2.0 on each dataset, which serve as input for image information in our Prompt Optimization Prompt. Note that we did not use this image information in the 16-shot setup due to the context length limitations of the language model. ", "page_idx": 18}, {"type": "text", "text": "M Prompt Optimization Prompt design ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Our Prompt Optimization Prompt is a crucial component of our optimizer, serving to enhance the performance of the vision-language model by optimizing the prompts through the prompt LLM. Figure 2 displays an example of our Prompt Optimization Prompt. Initially, the instruction in the first segment of the Prompt Optimization Prompt defines the role of the LLM. It introduces two essential tokens, ${\\mathrm{<}}\\mathrm{INS}{\\mathrm{>}}$ and <CLASS>, which represent the prompt and category, respectively. The primary function of this instruction is to inform the LLM of its role and the contents of the Prompt Optimization Prompt, enabling a more effective understanding of the Prompt Optimization Prompt. Note that in this section, our model does not involve image information, hence the Prompt Optimization Prompt here lacks statements like \u201cHere is a description of some features of the flowers ", "page_idx": 18}, {"type": "image", "img_path": "WPPC7FHtaM/tmp/cb8238bd6e5e382b8a6b6b9408dd4cacdc90d30b2e0a237f69c9b2bbbde2a42a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 4: An example of our Prompt Optimization Prompt with input at step 1 on the Food101 [17] dataset. ", "page_idx": 19}, {"type": "image", "img_path": "WPPC7FHtaM/tmp/784adacf90720ac48305e6f67d9deb5bd2a1e4c51660d08b5344e64f79fbd138.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 5: An example of our Prompt Optimization Prompt with input step 100 on the Food101 [17] dataset. ", "page_idx": 20}, {"type": "text", "text": "# OUTPUT STEP 1 Below are the prompts created according to your guidelines:   \n(1) <INS> Classify the image based on visual features of a <CLASS>.\u00a0</INS>   \n(2) <INS>\u00a0 Identify the <CLASS> in the image and evaluate its visual features for classification.\u00a0</INS>   \n(3) <INS> Classify the image as a <CLASS> food item. </INS>   \n(4) <INS>\u00a0\u00a0Classify the image featuring a <CLASS> with precision and accuracy.\u00a0</INS>   \n: (5) <INS> Identify the <CLASS> in the image based on its unique visual features. </INS>   \n(6) <INS> A photo of a <CLASS>, displaying vibrant colors and textures. </INS>   \n(7) <INS>\u00a0\u00a0Identify the primary ingredient in the <CLASS> and describe its texture, color, and presentation.\u00a0</INS>   \n(8) <INS>\u00a0\u00a0Identify the type of food shown in the image and classify it accurately.\u00a0</INS> ", "page_idx": 21}, {"type": "text", "text": "Figure 6: An example of our Prompt Optimization Prompt with output at step 1 on the Food101 [17] dataset. ", "page_idx": 21}, {"type": "text", "text": "# OUTPUT ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "STEP 2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Below are the prompts created according to your guidelines:   \n(1) <INS> Accurately classify the <CLASS> in the image using advanced visual recognition techniques. </INS>   \n(2) <INS> Accurately classify the image for <CLASS> identification based on detailed visual analysis with precision and minimal loss.</INS>   \n: (3) <INS> Accurately classify the image to determine the <CLASS> based on distinctive visual cues. </INS>   \n(4) <INS>\u00a0\u00a0Analyze the visual composition of the image to accurately classify the <CLASS> featured with precision.\u00a0</INS>   \n(5) <INS> Analyze the visual composition of the image to classify the <CLASS> accurately. </INS>   \n; (6) <INS> Analyze the distinct visual elements of the <CLASS> to achieve precise classification. </INS>   \n(7) <INS> Analyze the unique visual features of the <CLASS $>$ for precise classification with superior accuracy. </INS> (8) <INS> Utilize cutting-edge visual recognition algorithms to categorize the image accurately and efficiently. </INS> ", "page_idx": 21}, {"type": "text", "text": "Figure 7: An example of our Prompt Optimization Prompt with output at step 2 on the Food101 [17] dataset. ", "page_idx": 21}, {"type": "image", "img_path": "WPPC7FHtaM/tmp/f3593d4220170fafd10b3e856118e483035f91ff673fbdada341ddf81084dad3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 8: An example of our Prompt Optimization Prompt with output step 99 on the Food101 [17] dataset. ", "page_idx": 22}, {"type": "image", "img_path": "WPPC7FHtaM/tmp/ca4da5491582dfdfb0b50a95249c096b2c894335cf62d8921ec3a055bda5ab39.jpg", "img_caption": ["Figure 9: Training loss and accuracy on ImageNet. Each dot represents the average loss and accuracy across up to 8 generated prompts in the single strip, with the shaded region indicating the standard deviation. Our findings demonstrate that IPO can effectively optimize prompt learning in vision-language models. Notably, the best performance was achieved at step 85. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "in the image\u201d and the subsequent description of the image. In the next section, we will discuss how to integrate image information into the Prompt Optimization Prompt. ", "page_idx": 22}, {"type": "text", "text": "The subsequent instruction concerns the history of the prompts and their associated scores, which include metrics such as loss and accuracy. Our task follows CoOP [8], focusing on base classes with few samples. We calculate the loss and accuracy for these base classes using the generated prompts. Initially, we inform the LLM that the following details are past prompts along with their scores, and we clarify the range of values for loss and accuracy. What follows are the historical prompts and scores. This history functions similarly to episodic memory, to prevent the generation of suboptimal prompts while providing in-context information that enhances the creation of better prompts. Additionally, as the history of past prompts grows, we only retain the top 20 prompts based on accuracy to avoid information overload. The selection of these top 20 prompts is determined by their accuracy scores. Moreover, we consistently include the prompt \u201ca photo of <CLASS>\u201d in our history at every step, as this is a frequently used and effective prompt within the CLIP framework [30]. ", "page_idx": 22}, {"type": "table", "img_path": "WPPC7FHtaM/tmp/8c29a7aafd0ac719ab9d0246702c9959355656e0da69c8d0e3e22e64382a3df3.jpg", "table_caption": ["Table 17: Interpretable prompts generated by our method for each dataset in 16-shot scenarios. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "The final paragraph of the instruction specifies the ultimate goal of the task: to generate improved prompts based on the aforementioned instructions and the history of past prompts. The aim is to achieve lower loss and higher accuracy in this vision-language task, and it also outlines the format for the final generated prompt. These three components constitute the entire content of the Prompt Optimization Prompt, each playing a critical role for the LLM. It is important to note that in this Prompt Optimization Prompt, we do not consider the specific content of the images; we merely use images to calculate scores, and the details of the images are overlooked. In the following section, we will utilize a LMM to generate content from images and then incorporate it into the Prompt Optimization Prompt. ", "page_idx": 23}, {"type": "text", "text": "N More occlusion sensitivity analysis ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In Tables 19-29, we present various prompts analyzed using occlusion sensitivity analysis across different models and datasets. We found that CLIP is particularly sensitive to photos. However, CoOP and CoCoOP exhibit severe overfitting on base classes, leading to poor performance on base classes when some tokens are removed, but improved performance on novel classes. In contrast, our optimized prompts show performance degradation to varying degrees when certain words or phrases are removed, indicating that the key words or phrases in our generated prompts are essential. ", "page_idx": 23}, {"type": "table", "img_path": "WPPC7FHtaM/tmp/f15444b9db5af31aff8e541cb6a6739102a41cfc774740f990f824ec884679c9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "WPPC7FHtaM/tmp/0fa71d2c855611e5c39d510cdbf36f8eea0fa92130aa10fd8255c4b289bf8330.jpg", "table_caption": [], "table_footnote": ["Table 19: Comparison of various prompts with occlusion sensitivity analysis across different models on the ImageNet dataset. "], "page_idx": 25}, {"type": "table", "img_path": "WPPC7FHtaM/tmp/8b6f8db282c161b65463d6a2e34fa895965e95245b9dd0a925c6e10459fe8479.jpg", "table_caption": [], "table_footnote": ["Table 20: Comparison of various prompts with occlusion sensitivity analysis across different models on the Caltech101 dataset. "], "page_idx": 26}, {"type": "table", "img_path": "WPPC7FHtaM/tmp/c332934ceccf11c6ffee7c49aaa5c413e961d4e73c690eeaaa6510cc4a7493eb.jpg", "table_caption": ["Table 21: Comparison of various prompts with occlusion sensitivity analysis across different models on the OxfordPets dataset. "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "WPPC7FHtaM/tmp/4285fc2014223c3fc4d82f378f1b61c86aaef71ca81f4a55df6bdb51de464c03.jpg", "table_caption": ["Table 22: Comparison of various prompts with occlusion sensitivity analysis across different models on the StanfordCars dataset. "], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "WPPC7FHtaM/tmp/3b563b236d5acdbc3e794a6e96a82a3ec3ed94447ccdc5db5ca78b23460a9a33.jpg", "table_caption": ["Table 23: Comparison of various prompts with occlusion sensitivity analysis across different models on the Flowers102 dataset. "], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "WPPC7FHtaM/tmp/a72378cbbdc5b621e45960cf6b23cc944ef67f837aba4abb29a55db52af6b4b6.jpg", "table_caption": [], "table_footnote": ["Table 24: Comparison of various prompts with occlusion sensitivity analysis across different models on the Food101 dataset. "], "page_idx": 30}, {"type": "table", "img_path": "WPPC7FHtaM/tmp/9cf2d10c6fb7dea5eb3a683527edf34d3009f55cc30a2153bce1e97b20b74593.jpg", "table_caption": ["Table 25: Comparison of various prompts with occlusion sensitivity analysis across different models on the FGVCAircraft dataset. "], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "WPPC7FHtaM/tmp/7979c56a4549fc770ac45f53e7ab34fad955ccaceb13445b654d5af5757d7cb6.jpg", "table_caption": [], "table_footnote": ["Table 26: Comparison of various prompts with occlusion sensitivity analysis across different models on the SUN397 dataset. "], "page_idx": 32}, {"type": "table", "img_path": "WPPC7FHtaM/tmp/d957e0f90140a2138b878b232a48fececf99854431380d8ccf560a036199c8ee.jpg", "table_caption": [], "table_footnote": ["Table 27: Comparison of various prompts with occlusion sensitivity analysis across different models on the DTD dataset. "], "page_idx": 33}, {"type": "table", "img_path": "WPPC7FHtaM/tmp/7a29b57f7fa5cd075f47092ddce6e7c1265f43448a4a2921445b3655b0c16a7f.jpg", "table_caption": ["Table 28: Comparison of various prompts with occlusion sensitivity analysis across different models on the EuroSAT dataset. "], "table_footnote": [], "page_idx": 34}, {"type": "table", "img_path": "WPPC7FHtaM/tmp/8550cc6f945e97053fceebc24827f94a42310f7757951dd7ab23444bbd60c6bd.jpg", "table_caption": [], "table_footnote": ["Table 29: Comparison of various prompts with occlusion sensitivity analysis across different models on the UCF101 dataset. "], "page_idx": 35}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The contributions and scope of this paper are claimed in the abstract. Detailed information can be found in the fourth paragraph of the introduction section 1. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 36}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: We provide a \"limitation\" subsection in the conclusion section 6 ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This paper does not include theoretical results. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 37}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We provide all experimental details in the experiment section. The detailed input and output of Prompt Optimization Prompt of our method can be found in Appendix and supplemental materials. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 37}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We provide all experimental details in the experiment section. The detailed Prompt Optimization Prompt and Python codes of our method can be found in Appendix and supplemental materials. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 38}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We follow the standard experimental setup in IPO, where the data splits, is set as the same as previous works CoOP [8] and CoCoOP [11]. Detailed information can be found in Sec. 5 ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 38}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Following the standard experimental setup, we repeat each experiment over 3 random seeds and report the mean of the results. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 38}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 39}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We provide the computing resources in experiments 5. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 39}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We reviewed and followed the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 39}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We provide the potential broader impacts in the conclusion section 6. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 39}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 40}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: The data and models pose no such risks. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: We cite the original papers that produced the code package and datasets. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: Details of the datasets/code/model are provided in the supplemental materials. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 41}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 41}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 41}]