{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-00-00", "reason": "This paper introduces CLIP, a foundational vision-language model that is extensively used and benchmarked in the target paper."}, {"fullname_first_author": "Kaiyang Zhou", "paper_title": "Learning to prompt for vision-language models", "publication_date": "2021-09-01", "reason": "This paper proposes a method to learn prompts for vision-language models, which is directly compared with in the target paper."}, {"fullname_first_author": "Kaiyang Zhou", "paper_title": "Conditional prompt learning for vision-language models", "publication_date": "2022-00-00", "reason": "This paper is another important work by the same authors that improves on their previous prompt learning method and is directly compared with in the target paper."}, {"fullname_first_author": "Chengrun Yang", "paper_title": "Large language models as optimizers", "publication_date": "2023-09-00", "reason": "This paper explores the use of LLMs as optimization tools, providing the foundation for the target paper's approach of using LLMs to optimize prompts for vision-language models."}, {"fullname_first_author": "Mohammad Mahdi Derakhshani", "paper_title": "Bayesian prompt learning for image-language model generalization", "publication_date": "2023-00-00", "reason": "This paper introduces a Bayesian approach to prompt learning, addressing some of the limitations of previous methods and serving as a related work for the target paper."}]}