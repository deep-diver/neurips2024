{"references": [{"fullname_first_author": "A. Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, a foundational model for many modern large language models and a key point of comparison for the paper's analysis of SSMs."}, {"fullname_first_author": "R. E. Kalman", "paper_title": "On the general theory of control systems", "publication_date": "1960-01-01", "reason": "This early work on linear dynamical systems provided the theoretical basis for state-space models (SSMs), which are the focus of the paper's analysis."}, {"fullname_first_author": "S. Bhattamishra", "paper_title": "On the ability and limitations of transformers to recognize formal languages", "publication_date": "2020-11-01", "reason": "This paper provided a benchmark study comparing the abilities of transformers and RNNs to recognize formal languages, a direct point of comparison and a method for the current paper's empirical analysis."}, {"fullname_first_author": "A. Gu", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "publication_date": "2023-12-01", "reason": "This paper introduced the Mamba model, a state-of-the-art SSM used for empirical evaluation in the current paper."}, {"fullname_first_author": "G. Weiss", "paper_title": "On the practical computational power of finite precision rnns for language recognition", "publication_date": "2018-10-01", "reason": "This paper analyzed the computational capabilities of RNNs with finite precision, a crucial factor in evaluating the practical applications of SSMs which are also recurrent models."}]}