[{"figure_path": "eV5YIrJPdy/tables/tables_28_1.jpg", "caption": "Table 1: Tomita Grammars", "description": "This table lists seven Tomita grammars, each defined by a regular expression or a description in words.  The table also indicates whether each grammar is star-free, a property relevant to the paper's investigation of the expressive capacity of state-space models.", "section": "C.1 Regular Languages"}, {"figure_path": "eV5YIrJPdy/tables/tables_29_1.jpg", "caption": "Table 2: Accuracies on the counter Languages from the Bhattamishra et al. [2020] test suite. Transformer results reported based on Bhattamishra et al. [2020]. For Mamba, we report best settings (chosen based on inputs of length [1,50]) at 1 (Mamba1), 2 (Mamba2), 3 (Mamba3) layers. Results for the best-performing layer count, from the first two bins, are shown in Figure 5. On these languages, there is also a third bin.", "description": "This table compares the performance of Mamba and Transformer models on various counter languages, specifically focusing on the accuracy of these models. It demonstrates Mamba's performance at different layer counts (1,2,3) and shows results on three input length ranges: [1,50], [51,100], and [101,150].  It also includes a reference to Figure 5 for additional context.", "section": "4 Experiments"}, {"figure_path": "eV5YIrJPdy/tables/tables_30_1.jpg", "caption": "Table 3: Accuracies on the regular Languages from the Bhattamishra et al. [2020] test suite - 1st half. Transformer results reported based on Bhattamishra et al. [2020]. For Mamba, we report best settings (chosen based on inputs of length [1,50]) at 1 (Mamba1), 2 (Mamba2), 3 (Mamba3) layers. Results for the best-performing layer count are also shown in Figure 5.", "description": "This table compares the performance of Mamba and Transformer models on various regular languages.  It shows accuracy on two input length bins ([1,50] and [51,100]).  Mamba's results are reported for different numbers of layers (1,2,3), indicating the best performing layer configuration for each language.  Results are also visually represented in Figure 5.", "section": "4 Experiments"}, {"figure_path": "eV5YIrJPdy/tables/tables_31_1.jpg", "caption": "Table 4: Accuracies on the regular Languages from the Bhattamishra et al. [2020] test suite - continued. Transformer results reported based on Bhattamishra et al. [2020]. For Mamba, we report best settings (chosen based on inputs of length [1,50]) at 1 (Mamba1), 2 (Mamba2), 3 (Mamba3) layers. Results for the best-performing layer count are also shown in Figure 5.", "description": "This table presents the accuracy results of Mamba and Transformer models on a subset of regular languages from the Bhattamishra et al. (2020) test suite.  It shows the accuracy for three different length bins ([1,50], [51,100], [101,150]).  The best-performing layer count (1,2, or 3 layers) for Mamba is reported, and the results are also visually represented in Figure 5 of the paper.", "section": "Experiments"}]