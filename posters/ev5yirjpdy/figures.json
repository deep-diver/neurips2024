[{"figure_path": "eV5YIrJPdy/figures/figures_5_1.jpg", "caption": "Figure 3: (a) Construction for Flip-Flop (Theorem 1): The first layer stores instruction bits to the hidden state, while data bits are forwarded to the output. Hence, the output always contains both the latest instruction and the associated data bit. In the second layer, if the instruction bit is w, the corresponding data bit is written to the hidden state, else the old value persists. This allows the model to consistently output the correct data bit. (b) Construction for Dyck(K, h) (Theorem 6): The first layer tracks the depth by counting up for each opening bracket, and down for each closing bracket. The second layer builds on the Flip-Flop construction to find the last opening bracket at the current depth; the next symbol can be either the matching closing bracket or if the maximum depth has not been reached an arbitrary opening bracket.", "description": "This figure illustrates the SSM constructions for Flip-Flop and Dyck(K, h) languages.  Panel (a) shows how a two-layer SSM can model the Flip-Flop language using the first layer to store the instruction and the second to update the state based on whether the instruction is write. Panel (b) shows how a two-layer SSM can model bounded hierarchical structure. The first layer uses a counter to track depth and the second layer uses a flip-flop type of construction to remember the last opening bracket.  The overall constructions verify the theorems of the paper.", "section": "3 Theoretical Results"}, {"figure_path": "eV5YIrJPdy/figures/figures_7_1.jpg", "caption": "Figure 4: As predicted by Theorem 6, Mamba with 2 layers can model Dyck(K, h). Results for test set with strings of length 700 \u2264 n \u2264 1400.", "description": "This figure shows the results of an experiment evaluating the performance of the Mamba model on the Dyck(K, h) language.  The x-axis represents the number of memory dimensions used in the model, while the y-axis represents the close accuracy achieved. Two lines are plotted, one for a 2-layer Mamba model and another for a 1-layer model. The results demonstrate that the 2-layer model significantly outperforms the 1-layer model across all memory dimensions, thus confirming that a 2-layer architecture is necessary for effectively modeling the bounded hierarchical structure present in the Dyck language.", "section": "Experiments"}, {"figure_path": "eV5YIrJPdy/figures/figures_8_1.jpg", "caption": "Figure 5: Results on 27 formal languages, comparing our Mamba results (blue) with transformer results reported by Bhattamishra et al. [2020] (orange), on in-distribution lengths (solid) and out-of-distribution lengths (dotted). As predicted by Theorem 4, Mamba performs strongly on star-free languages, and even shows perfect length generalization. Again as predicted by Theorem 4, it performs poorly on non-star-free languages. Results for transformers from Bhattamishra et al. [2020] are mixed. Mamba also succeeds on learning the counter languages from Theorem 5, showing perfect accuracy at in-distribution lengths at in-distribution lengths, but length generalization lags behind transformers.", "description": "The figure presents a comparison of Mamba and Transformer models' performance across various formal languages.  The x-axis categorizes languages by type (star-free, non-star-free, counter). The y-axis shows accuracy.  The different colored bars within each language category represent results for Mamba and Transformers, broken down by in-distribution and out-of-distribution input lengths.  The results align with the paper's theoretical findings, demonstrating Mamba's strength on star-free languages and its relative weakness on non-star-free ones, while also showing its ability to model counter languages, albeit with less success in length generalization compared to Transformers.", "section": "4 Experiments"}, {"figure_path": "eV5YIrJPdy/figures/figures_8_2.jpg", "caption": "Figure 6: Test error on the validation set for LFF, following Liu et al. [2023a]. Mamba shows near-zero test error in both In- (green) and Out-of-distribution (orange) settings, consistent with Theorem 1, and avoids the failure seen in transformers [Liu et al., 2023a]", "description": "This figure shows the test error on the validation set for the Flip Flop language (LFF) obtained from the training of a one-layer Mamba SSM.  The test error is plotted against the number of training iterations.  Two different data distributions are shown: FFL(0.8) and FFL(0.98), representing different levels of sparsity in the data. The results demonstrate that Mamba achieves near-zero test error in both distributions, showcasing its ability to generalize well and avoid the failures observed in transformers for this task, as reported by Liu et al. [2023a].", "section": "Experiments"}, {"figure_path": "eV5YIrJPdy/figures/figures_19_1.jpg", "caption": "Figure 2: (a) Visualizing the SSM equations 1, 2: The hidden state H is updated by a combination of its previous values, transformed by matrix A, and the input X, modulated by matrix B. The updated hidden state and input are then processed through a Mix(.) layer, which can incorporate components like (Swi)GLU or Linear layers, with an optional RMSNorm for normalization. (b) An intuitive construction for recognizing PARITY with SSMs is achieved by setting B = 0 and A = -1 when the input is 1, and A = 1 otherwise. However, this construction violates both NONNEGATIVE and TIME-INVARIANT properties. We show that one of these properties is provably required to recognize PARITY at arbitrary lengths using an SSM (Theorem 2). (c) Modeling a\"b\": the matrix A adds the previous hidden state to the update, and depending on whether the input symbol requires counting up or down, matrix B is set to 1 or -1, thus making the SSM simulate a counter (Theorem 5)", "description": "This figure illustrates the state space model (SSM) equations and how SSMs handle the PARITY problem and unbounded counting.  Panel (a) shows the core SSM update equations, highlighting the roles of the hidden state (H), input (X), transformation function (\u03c6), and matrices A and B. Panel (b) demonstrates an intuitive approach to recognizing PARITY, which, however, violates constraints, illustrating a key limitation of SSMs in handling parity at arbitrary lengths.  Panel (c) shows how SSMs simulate a counter using matrix A and B, successfully modeling the 'a^nb^n' language.", "section": "Theoretical Results"}, {"figure_path": "eV5YIrJPdy/figures/figures_32_1.jpg", "caption": "Figure 7: Mamba Accuracy on Dyck8,10, on the development set (length < 700, same length range as training set) and test set (length 700 \u2264 n \u2264 1400). The latter is also plotted in Figure 4.", "description": "This figure shows the accuracy of Mamba models with varying memory dimensions on the Dyck(8,10) language.  The results are presented separately for the development set (lengths < 700) and test set (lengths 700-1400). The test set results are also shown in Figure 4. The plot compares the performance of 1-layer and 2-layer Mamba models, illustrating the impact of model depth and memory capacity on accuracy in predicting this hierarchical language.", "section": "4 Experiments"}]