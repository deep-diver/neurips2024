[{"type": "text", "text": "The Expressive Capacity of State Space Models: A Formal Language Perspective ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yash Sarrof, Yana Veitsman, Michael Hahn Saarland Informatics Campus Saarland University, Germany {ysarrof, yanav, mhahn}@lst.uni-saarland.de ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recently, recurrent models based on linear state space models (SSMs) have shown promising performance in language modeling (LM), competititve with transformers. However, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement length-generalizing solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Transformers [Vaswani et al., 2017] power most large language models (LLMs) today, as they offer the advantage of parallelized training by avoiding recurrence, compared to the previously dominant recurrent achitectures [RNNs Elman, 1990, Hochreiter and Schmidhuber, 1997]. However, building on a long history of continuous dynamical models [e.g. Kalman, 1960, 1963] and work on faster RNNs [Bradbury et al., 2016, Lei et al., 2018], a recent line of work has developed state space models (SSMs) rivaling the performance of transformers [e.g. Gu et al., 2021, Gu and Dao, 2023, Sun et al., 2023, De et al., 2024, Yang et al., 2024, Qin et al., 2024a]. These SSMs are recurrent models, formulated in terms of iterative state updates, while still allowing efficient parallelization. ", "page_idx": 0}, {"type": "text", "text": "The impressive empirical performance of such SSMs raises the question of whether they might have capabilities that the transformer architecture might lack in principle. Simultaneously, to understand whether SSMs may plausibly overtake the dominant role of transformers, it is an important question whether SSMs may lack abilities present in transformers. A better understanding of these questions may also point the way to future architectures that unite the strengths of both architectures. ", "page_idx": 0}, {"type": "text", "text": "One common approach to understanding the capabilities of computational architectures is through their expressive capacity in simulating automata and modeling language classes; indeed, a sizeable literature has studied transformers [e.g. P\u00e9rez et al., 2019, Hahn, 2020, Bhattamishra et al., 2020, Yao et al., 2021a, Liu et al., 2023b,a, Deletang et al., 2022, Strobl et al., 2024, Chiang et al., 2023, Sanford et al., 2024, Peng et al., 2024] and RNNs [e.g. Siegelman and Sontag, 1995, Horne and Hush, 1993, Indyk, 1995, Weiss et al., 2018, Hewitt et al., 2020] through this lens. As the difficulty of many computational problems is well-understood in terms of such language classes, results about expressive capacity directly yield results about the ability to model specific computational problems. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "While a substantial number of results have been obtained for transformers and traditional RNNs, understanding remains largely open for SSMs. In an initial step, Merrill et al. [2024] showed that all problems computable by SSMs are contained in $\\mathrm{T}\\mathrm{C}^{0}$ , a circuit complexity class that is known to also cover transformers [Merrill and Sabharwal, 2023, Strobl, 2023]. Under standard conjectures, this suggests that certain types of state tracking are hard for both models. Jelassi et al. [2024] and Bhattamishra et al. [2024] provided evidence of differences between these architectures, showing that transformers outperform SSMs on copying or retrieving from long strings\u2013tasks well within $\\mathrm{T}\\mathrm{C}^{0}$ . Zubic\u00b4 et al. [2024] showed that multi-layer SSMs are constrained by their logarithmic space computational capacity, limiting their ability at algorithmic tasks such as multi-digit multiplication. ", "page_idx": 1}, {"type": "text", "text": "However, a more fine-grained understanding of the power of SSMs, and how they compare to RNNs and transformers, remains an open question. Our contribution in this paper is to provide rigorous understanding of SSMs\u2019 abilities in different classes of languages. We show that transformers and SSMs cover overlapping but distinct fragments of $\\mathrm{TC}^{0}$ . For instance, SSMs can model bounded hierarchical structure in ways similar to transformers and traditional RNNs, even without embedding a stack-like structure (Theorem 6). For regular languages involving modular counting, such as the PARITY function (Theorem 2), we identify a design choice that makes extant SSMs struggle in ways similar to transformers. In other cases, we show that SSMs resolve a failure case of transformers: they effortlessly model Flip Flop state tracking (Theorem 1). We discuss take-aways for SSM and LLM research in Section 5; among others, our results suggest future LM architectures might need to combine both attention and state spaces. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 State Space Models ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "SSM Layers We define a single layer of a state space model as a map, at input length $T$ , ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbb{R}^{T\\times d}\\to\\mathbb{R}^{T\\times d}\\qquad\\qquad\\qquad\\qquad(x_{t})_{t=1,\\dots,T}\\mapsto(z_{t})_{t=1,\\dots,T}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "given by the recurrence ", "page_idx": 1}, {"type": "equation", "text": "$$\nh_{t}=A(x_{t})\\circ h_{t-1}+B(x_{t})\\qquad\\qquad\\qquad\\qquad z_{t}=\\Phi(h_{t},x_{t})\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\circ$ denotes elementwise product, and, for each $x_{t}\\in\\mathbb{R}^{d}$ , ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{h_{0}\\in\\mathbb{R}^{d}}&{\\qquad\\qquad}&&{B(x_{t})\\in\\mathbb{R}^{d}\\mathrm{~(increment)~}}\\\\ &{A(x_{t})\\in\\mathbb{R}^{d}\\mathrm{~(gate)~}}&&{\\qquad\\qquad}&&{\\Phi:\\mathbb{R}^{2d}\\to\\mathbb{R}^{d}\\mathrm{~(transform)~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "We allow $A,B$ to be arbitrary smooth maps. The map $\\Phi(h_{t},x_{t})$ includes a cascade of channel-mixing transformations and normalization, which we abstract as follows: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\Phi\\big(h_{t},x_{t}\\big)=\\mathbf{M}\\mathrm{i}\\mathrm{x}_{1}\\big(\\mathrm{Norm}(\\mathbf{M}\\mathrm{i}\\mathrm{x}_{2}\\big(h_{t},x_{t}\\big)\\big),x_{t}\\big)\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathrm{Mix}_{j}(\\cdot)$ can contain linear or (Swi)GLU components [e.g. Qin et al., 2024a, Gu and Dao, 2023]. We will take Norm to implement RMSNorm Zhang and Sennrich [2019]; LayerNorm Ba et al. [2016] can be covered by absorbing centering into $\\mathbf{M}\\mathrm{i}\\mathbf{x}_{2}$ . ", "page_idx": 1}, {"type": "text", "text": "A Full SSM Real-world SSMs typically stack several layers of the form (1\u20132). Where needed, we use superscripts to indicate the layers in an SSM: $h_{t}^{(1)},\\ldots,h_{t}^{(L)}$ , where $L$ is the number of layers. We consider input words $\\mathbf{w}=w_{1\\ldots{\\left|w\\right|}}$ over a discrete alphabet $\\Sigma$ , and assume an encoding in terms of token embeddings $e(\\upsigma)\\in\\mathbb{R}^{d}$ , for $\\upsigma\\in\\Sigma$ . We will also write $e_{\\upsigma}$ for $e(\\upsigma)$ . These feed into the lowest layer as $x_{t}^{(1)}:=e(w_{t})$ . The outputs of each layer feed into the next layer, as $x_{t}^{(l+1)}=z_{t}^{(l)}$ . The transformations in (1) are specific to each layer: $A^{(1)},\\ldots,A^{(L)}$ and similarly for $B,\\Phi$ . To keep notation simple, we will only show the superscripts where necessary for disambiguation. The activations $z_{t}^{(L)}$ at the highest layer are read out by some neural network $\\uprho$ into vectors $q_{t}\\in\\mathbb{R}^{d_{p r e d}}$ describing classification or next-token predictions. We again take $\\uprho$ to be an arbitrary function; importantly, all our constructions will allow $\\uprho$ to operate correctly even at finite precision. ", "page_idx": 1}, {"type": "text", "text": "Implementation Choices In Mamba, (1) directly maps onto Eqs. (2a) and (2b) in Gu and Dao [2023]. The notation of Gu and Dao [2023] use a matrix multiplication $\\overline{{A}}h_{t-1}$ instead of elementwise multiplication $A(x_{t})\\circ h_{t-1}$ in (1), but importantly, Mamba\u2019s $\\overline{{A}}$ is diagonal, so we can take $A(x_{t})_{i}\\!=\\!\\overline{{A}}_{i i}$ . Some SSMs assume nondiagonal $A\\left(x_{t}\\right)$ , but typically this matrix is diagonalizable [e.g. Gu et al., 2021, Sun et al., 2023], so that the SSM is still equivalent to one of the form (1). We discuss how other SSMs instantiate (1) in Appendix A. Some models assume complex-valued activations (Appendix A); our results largely do not depend on this distinction, but take it into account where needed (Theorem 13). Some SSMs [e.g. Gu and Dao, 2023] use different numbers of channels in $x_{t}$ and $h_{t}$ using state expansion; as this does not affect expressive capacity, we will simply assume a constant dimensionality $d$ . Local convolutions [e.g. Fu et al., 2023] can be simulated with an SSM layer and do not increase expressive capacity (Remark 19). ", "page_idx": 2}, {"type": "text", "text": "We will find that two design choices have nontrivial impact on expressive capacity: The first one is time invariance: we call an SSM TIME-INVARIANT if $A\\left(x_{t}\\right)$ does not depend on $x_{t}$ . Some SSMs, such as S4 [Gu et al., 2021] and Retnet [Sun et al., 2023] are time-invariant; Mamba [Gu and Dao, 2023], Griffin [De et al., 2024], GLA [Yang et al., 2024], HGRN [Qin et al., 2024b,a], QRNN/SRU Bradbury et al. [2016], Lei et al. [2018] are not (Appendix A). The second one is the sign of the entries of $A\\left(x_{t}\\right)$ : Across all non-time-invariant SSMs surveyed, we find that the gate is always nonnegative (Appendix A): $A\\!\\left(x_{t}\\right)\\geq0$ (NONNEGATIVE) due to exponential or sigmoid parameterizations of the gate \u2013 this choice turns out to limit expressive capacity (Theorem 2). ", "page_idx": 2}, {"type": "text", "text": "Role of Parameterization While the abstract form (1\u20132) is common across the SSM literature, differences in parameterization may have substantial effect on efficiency and training stability. In particular, the parameterization of $A\\left(x_{t}\\right)$ has been the subject of substantial research [e.g. Gu et al., 2020, 2021, Yu et al., 2023, Wang and Li, 2023]. However, studying expressiveness allows us to abstract away from these differences to a remarkable degree: We will allow $A,B,\\uprho$ to be arbitrary functions with the given input-output properties. Our negative results are based on abstract properties of the setup (1\u20132), which fundamentally bottlenecks SSMs through elementwise linear state updates. For our positive results, will use empirical learnability experiments to verify that learnable solutions instantiating them (though not necessarily implementing the same constructions as used in the proofs) do exist in a recent SSM [Mamba, Gu and Dao, 2023]. ", "page_idx": 2}, {"type": "text", "text": "We contrast SSMs with traditional RNNs such as simple RNNs or LSTMs: for these, the recurrence in Eq. (1) is replaced by $h_{t}=\\Psi(h_{t-1},x_{t})$ where $\\Psi$ could be linear, an MLP [Elman, 1990], or a more complex gated function [Hochreiter and Schmidhuber, 1997]. ", "page_idx": 2}, {"type": "text", "text": "Finite Precision Assumption While Eq.(1) assumes arbitrary real-valued activations, real-world implementations can only represent numbers with bounded precision. Formally, we adopt the finite precision notion used by Weiss et al. [2018] in a study of the expressive power of traditional RNNs: We allow an unbounded number of integer bits, but only $p$ fractional bits, independent of the length of the input. See Appendix E for discussion. ", "page_idx": 2}, {"type": "text", "text": "2.2 Modeling Formal languages ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We study three foundational types of data structures needed for modeling formal languages [Hopcroft et al., 2001]: finite state automata (Theorem 1, 2, 4), counters (Theorem 5), and stacks (Theorem 6). These data structures can be understood in two equivalent forms: One is to track a state sequence over an input, where each input symbol engenders a specific transformation on the state. The other one, more commonly considered in research on expressive capacity, considers formal languages\u2014sets of finite strings that are defined by the property that an automaton reaches one of a pre-specified set of \u201caccepting\u201d states after traversing the word. We focus on the latter, enabling easy comparison with existing results on transformers and RNNs. ", "page_idx": 2}, {"type": "text", "text": "A finite-state-automaton (see Definition 7) represents a general state tracking problem over a finite state space, without imposing further structure on the state space: The automaton keeps track of a single state from a finite state space; when reading a string from left to right, each symbol engenders a specific transformation of the state. At each position, the current state determines which symbols can come next; membership in a formal language is determined by the state reached after reading the full string. Finite-state-automata are equivalent in expressivity to regular expressions, and define the regular languages [Kleene, 1951]. ", "page_idx": 2}, {"type": "text", "text": "Figure 1: Three key formal languages: prefixes with the sets of possible next characters: Flip Flop (Theorem 1), PARITY (Theorem 2), bounded-depth Dyck (Theorem 6). In Flip Flop, after a r (read) instruction, the bit must match what came after the last w (write) instruction (here, 0). For PARITY, EOS can only follow when the number of ones in the prefix is even. For bounded-depth Dyck, a closing bracket can only appear if it matches the last unclosed opening bracket (here, \u201c)\u201d matches \u201c(\u201d)). Opening brackets can appear as long as the maximum depth (here, 5) hasn\u2019t been reached. ", "page_idx": 3}, {"type": "text", "text": "Allowing an automaton to keep track of one or more counters [Fischer et al., 1968b]\u2014integers that are incremented or decremented at each symbol read\u2014turns the state space infinite, but in a highly structured manner. SSMs can model this datastructure (Theorem 5), as can RNNs and transformers [Weiss et al., 2018, Bhattamishra et al., 2020]. Stacks, a first-in-first-out datastructure, enable automata to keep track of hierarchical structure, foundational to natural language [Chomsky, 1957]. We show that SSMs can implement shortcut solutions to bounded hierarchical structure even without implementing a stack (Theorem 6) \u2013 these are likely to be most useful to natural language given the boundedness of human memory [Miller, 1963, Karlsson, 2007]. ", "page_idx": 3}, {"type": "text", "text": "2.3 Formal Language Prediction and Recognition ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We fix a finite alphabet $\\Sigma$ . Its elements are called characters or symbols. The set of all finite strings w over $\\Sigma$ is denoted $\\Sigma^{*}$ ; such strings are often referred to as words. The length of w is denoted $|\\mathbf{w}|$ . A formal language $L$ is a subset of $\\Sigma^{*}$ . Techically, we assume that the alphabet includes BOS and EOS symbols, which occurs at the beginning and end of each element of $L$ and nowhere else. ", "page_idx": 3}, {"type": "text", "text": "We next need to define what it means for an SSM to model a formal language. The notion of recognition, where the task is to classify a full string as belonging to the language or not. Formally, for an SSM with $d_{p r e d}=1$ , we say that it recognizes a language $L$ if the output $\\bar{\\mathsf p}\\bar{(\\boldsymbol z_{\\vert\\mathbf w\\vert}^{(L)})}$ equals\u2014when the SSM is run on $\\mathbf{w}\\in\\Sigma^{*}$ \u20141 if $\\mathbf{w}\\in L$ and 0 else. ", "page_idx": 3}, {"type": "text", "text": "However, such a classification task is arguably not always matched to dominant use cases in predictive sequence modeling, where the task is to predict the next token at each step. Thus, we also cast formal languages into a language modeling and sequence prediction framework. We adopt the task of Bhattamishra et al. [2020], where the model is asked to output at each step in a sequence the set of possible next symbols. Let ${\\mathsf{P r e f i x}}(L):=\\left\\{w:w\\in\\Sigma^{*},w\\Sigma^{*}\\cap L\\neq\\emptyset\\right\\}$ the set of valid prefixes of $L$ . We then say that a model predictively models a language $L$ if (Figure 1), given a valid prefix $w\\in{\\mathrm{Prefix}}(L)$ , it outputs the finite set ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left\\{\\mathbf{\\sigma}\\in\\Sigma:\\boldsymbol{w}\\mathbf{\\sigma}\\Sigma^{*}\\cap L\\neq\\varnothing\\right\\}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We think of each such set as an atomic label; the set of possible labels is the power set of the finite alphabet $\\Sigma$ (here, $d_{p r e d}=2^{|\\Sigma|}.$ ). Importantly, in both recognition and predictive modeling, we test the SSMs\u2019 ability across arbitrary input lengths, i.e. the choice of input length does not affect the inherent capability to recognize or predictively model the language. Predictive modeling can be easily converted into recognition by checking whether any symbol in the sequence is not in the predictive set at the preceding position; this can be done by adding 1 SSM layer. Conversely, if we can show that SSMs cannot recognize a language, this proves they also cannot perform predictive modeling for it, as they then cannot correctly predict where EOS can appear. To get the strongest results, we thus prove positive results for predictive modeling, and negative results for recognition. ", "page_idx": 3}, {"type": "text", "text": "Figure 2: (a) Visualizing the SSM equations 1, 2: The hidden state $H$ is updated by a combination of its previous values, transformed by matrix $A$ , and the input $X$ , modulated by matrix $B$ . The updated hidden state and input are then processed through a $M i x(.)$ layer, which can incorporate components like (Swi)GLU or Linear layers, with an optional RMSNorm for normalization. (b) An intuitive construction for recognizing PARITY with SSMs is achieved by setting $B=0$ and $A=-1$ when the input is 1, and $A=1$ otherwise. However, this construction violates both NONNEGATIVE and TIME-INVARIANT properties. We show that one of these properties is provably required to recognize PARITY at arbitrary lengths using an SSM (Theorem 2). (c) Modeling $a^{n}b^{n}$ : the matrix $A$ adds the previous hidden state to the update, and depending on whether the input symbol requires counting up or down, matrix $B$ is set to 1 or $^{-1}$ , thus making the SSM simulate a counter (Theorem 5) ", "page_idx": 4}, {"type": "text", "text": "3 Theoretical Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.1 Length-Generalizing Representations for Flip-Flop State Tracking ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Flip Flop languages [Liu et al., 2023a] are a simple instance of state tracking defined in terms of write, read, and ignore instructions. Each write instruction comes with a piece of information; whenever a read instruction is encountered, the information written by the last write instruction is recalled. Formally, $\\mathcal{L}_{F F}$ is the set of finite strings $\\mathbf{X}$ over $\\Sigma=\\{\\mathbf{r},\\mathbf{w},\\dot{\\sf i},0,1\\}$ , where $x_{1},x_{3},\\cdots\\in$ $\\{\\mathbf{r},\\mathbf{\\boldsymbol{w}},\\mathbf{i}\\}$ , $x_{2},x_{4},\\cdot\\cdot\\cdot\\in\\{0,1\\}$ , and where the bit following any $\\mathbf{r}$ matches the bit following the last preceding occurrence of $\\mathtt{w}$ . Liu et al. [2023b] show that the Flip Flop language, as an abstraction, is a fundamental ingredient of many long-range reasoning settings. It can be represented with a small finite-state-automaton, and LSTMs learn $\\angle_{F F}$ well [Liu et al., 2023a]. Transformers can in principle represent it [Liu et al., 2023b,a], though known constructions are not inherently lengthgeneralizing, a fact confirmed empirically; intuitively, this may happen because attention heads aggregate information in a commutative manner, and reliably attending to the last write instruction requires strong position dependence in the attention weights. SSMs, similar to traditional RNNs can easily represent Flip Flop at arbitrary input lengths and thus avoid a failure mode of self attention: ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. There is a two-layer SSM that predictively models LFF at all lengths, at finite precision. ", "page_idx": 4}, {"type": "text", "text": "In the construction (Figure 3), the first layer records the last instruction token, achieved in (1) by setting $A{\\left(e(\\mathbf{r})\\right)}\\!=\\!A{\\left(e(\\mathbf{w})\\right)}\\!=\\!A{\\left(e(\\mathbf{i})\\right)}\\!=\\!0$ , and $A(e(0)\\!=\\!A(e(1))=1$ , and setting $B(e(0))\\!=\\!B(e(1))\\!=\\!0$ . Additional dimensions forward the current token to $h_{t}^{(1)}$ . In the output of the first layer $z_{t}^{(1)}$ , whenever the input is 0 or 1, the model now has access both to the current token $w_{t}$ and the preceding token $w_{t-1}$ , which must have been an instruction. Based on this information, the model can set the gate to overwrite the state $h_{t-1}^{(2)}$ with the current input token when the preceding token was $\\mathtt{w}$ , and pass along the state $h_{t-1}^{(2)}$ unaltered otherwise. This, together with $z_{t}^{(1)}$ , is sufficient for always identifying the legal next symbols in $\\mathcal{L}_{F F}$ . The formal proof is in Appendix B.1. ", "page_idx": 4}, {"type": "text", "text": "3.2 Difficulty of PARITY ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "PARITY, the language of bitstrings with an even number of ones, is recognized by a finite-state automaton with 2 states, and is straightforwardly encoded into a traditional RNN, even a linear one, with finite precision. It is in principle expressible for transformers [Chiang and Cholak, 2022], but is empirically hard for transformers to learn [Bhattamishra et al., 2020, Deletang et al., 2022], as it can provably only be represented in sharp minima [Hahn and Rofin, 2024]. A sufficiently general SSM could easily recognize it at $d=1$ by setting $h_{0}=1,A(e_{1})=-1,A(e_{0})=0.$ , $B\\equiv0$ , so that the sign of the single entry of $h_{t}$ indicates the parity (Figure 2). Such an SSM would need to be nontime-invariant and require negative or complex gate values; i.e., satisfy neither TIME-INVARIANT ", "page_idx": 4}, {"type": "image", "img_path": "eV5YIrJPdy/tmp/4043397b4e8ff0bbfb85fb3e8b426d706afa7eb059ef32ac0178c4e487e10fcb.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 3: (a) Construction for Flip-Flop (Theorem 1): The first layer stores instruction bits to the hidden state, while data bits are forwarded to the output. Hence, the output always contains both the latest instruction and the associated data bit. In the second layer, if the instruction bit is w, the corresponding data bit is written to the hidden state, else the old value persists. This allows the model to consistently output the correct data bit. (b) Construction for Dyck(K, h) (Theorem 6): The first layer tracks the depth by counting up for each opening bracket, and down for each closing bracket. The second layer builds on the Flip-Flop construction to find the last opening bracket at the current depth; the next symbol can be either the matching closing bracket or \u2013 if the maximum depth has not been reached \u2013 an arbitrary opening bracket. ", "page_idx": 5}, {"type": "text", "text": "nor NONNEGATIVE. Thus, these design choices necessitated by optimization, limit the power of an SSM in emulating finite-state-automata, establishing an even stronger separation between existing SSM variants and traditional RNNs than the circuit complexity arguments in Merrill et al. [2024] ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. No SSM satisfying NONNEGATIVE can recognize PARITY at arbitrary input lengths with finite precision. In particular, this applies to Mamba. ", "page_idx": 5}, {"type": "text", "text": "The proof is in Appendix B.2; it examines inputs of the form $1^{N}$ and shows that the activations $z N$ converge as $N\\rightarrow\\infty$ , and thus cannot reliably encode the parity of $N$ . It should be noted that we require the layer-wise operations used in the SSM to be either linear or based on the GLU or SwiGLU activation functions, as seen for instance in Mamba (Remark 15). As we show in Theorem 13, the same result holds even for SSMs evading NONNEGATIVE when they are TIME-INVARIANT, at least when the coefficients have rational angles in the complex planes. All extant SSMs we surveyed (Appendix, Section A) satisfy either NONNEGATIVE or TIME-INVARIANT. Hypothetical SSMs evading both NONNEGATIVE and TIME-INVARIANT would be strictly stronger and can represent not only PARITY, but all regular languages known to be in $\\mathbf{T}\\mathbf{C}^{0}$ (Theorem 22). ", "page_idx": 5}, {"type": "text", "text": "3.3 Exact characterization of Regular Languages modeled by SSMs ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We combine Theorems 1 and 2 to derive an exact characterizations of the regular languages that modern non-time-invariant SSMs such as Mamba can recognize or predictively model \u2013 the two notions coincide here \u2013 in the finite-precision setting. The key insight is that $\\mathcal{L}_{F F}$ and PARITY are fundamental building blocks of two classes of regular languages: star-free languages and their complement, non-star-free languages [Sch\u00fctzenberger, 1965, McNaughton and Papert, 1971]: ", "page_idx": 5}, {"type": "text", "text": "Definition 3. A regular language is star-free if it can be defined using regular expressions involving only the empty set, the empty string, individual symbols, concatenation, and Boolean combinations \u2013 avoiding the Kleene star operation. ", "page_idx": 5}, {"type": "text", "text": "$\\mathcal{L}_{F F}$ is star-free: there is a way to define it without Kleene star. PARITY is not star-free; any regular expression for it must involve the Kleene star. Some languages that are intuitively defined with Kleene stars may still be star-free.2 A language is star-free if and only if it can be defined logically using only first-order quantifiers and the order relation [Sch\u00fctzenberger, 1965]. Also, $\\mathcal{L}$ is non-starfree if and only if recognizing it involves counting modulo some finite integer $K$ [McNaughton and Papert, 1971]; Modern non-time-invariant SSMs such as Mamba cannot perform modulo counting, but they can model all star-free languages: ", "page_idx": 5}, {"type": "text", "text": "Theorem 4. Let L be a regular language. The following are equivalent: ", "page_idx": 5}, {"type": "text", "text": "2. L is star-free. ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The proof in Appendix B.3 uses the Krohn-Rhodes theorem [Krohn and Rhodes, 1965] to reduce all star-free languages to flip flop-like state tracking. Importantly, there are well-known constructive criteria for deciding whether a given automaton defines a star-free language [Sch\u00fctzenberger, 1965]; hence, we have a decidable criterion for the finite-state tracking problems that such SSMs satisfying NONNEGATIVE can solve. ", "page_idx": 6}, {"type": "text", "text": "This is much simpler than the situation for transformers, where an exact characterization of their power within the regular languages is complicated: Angluin et al. [2023] show that a certain formal abstraction of transformers (masked unique hard attention) also recognizes exactly the star-free languages, but constructions of realistic transformers via Krohn-Rhodes in Liu et al. [2023b] do not inherently length generalize. Both theoretical [Huang et al., 2024] and empirical research indicate difficulties in generalizing even for some simple star-free languages [Bhattamishra et al., 2020, Liu et al., 2023a]. Known length-generalizing constructions are limited to very simple subclasses such as the piecewise testable languages [Yang and Chiang, 2024]. In contrast, for SSMs we have a single model per language, at finite precision and for arbitrarily long inputs. Thus, we expect that the SSM architecture confers an advantage in star-free state tracking problems when compared to transformers \u2013 a prediction we will find supported experimentally (Figure 5). ", "page_idx": 6}, {"type": "text", "text": "3.4 SSMs can perform unbounded counting ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Having characterized the regular languages modeled by SSMs, we now consider languages requiring unbounded counting [Fischer et al., 1968b], specifically, languages recognized by keeping track of one or more counters, where each character causes a specific increment or decrement to each counter [Krebs et al., 2015, Hahn et al., 2015, Weiss et al., 2018, Kutrib et al., 2021]. A prime example is the Dyck-1 language of well-formed strings over \u201c(\u201d and \u201c)\u201d; here a counter is incremented (decremented) whenever an opening (closing) bracket is encountered; a string is well-formed if and only if the counter is 0 at the end of the string. Some other relevant formal languages are ShuffleDyck- $.k$ (the shuffles of multiple Dyck-1 languages), $a^{n}b^{n}$ \u2013 here, $a$ increments the counter and $^b$ decrements it, and $a^{n}b^{n}c^{n}$ \u2013 here, there are two counters, one keeping track of $a^{n}b^{n}$ and one of $b^{n}c^{n}$ (See Appendix C.2). Such counter languages are fundamental as basic context-free (Dyck-1, $a^{n}b^{n}$ ) or context-sensitive (e.g., $a^{n}b^{n}c^{n},$ languages [Hopcroft et al., 2001], and have been the subject of studies of both transformers [Bhattamishra et al., 2020] and RNNs [Weiss et al., 2018]. ", "page_idx": 6}, {"type": "text", "text": "Theorem 5. The languages Dyck-1, Shuffle-Dyck- $k$ , $n$ -ary Boolean Expressions, $a^{n}b^{n}$ , $a^{n}b^{n}c^{n}$ , and $a^{n}b^{n}c^{n}d^{n}$ , (defined in Appendix C.2) can each be predictively modeled by an SSM. ", "page_idx": 6}, {"type": "text", "text": "The proof is in Appendix B.4. Intuitively (Figure 2), an SSM can directly implement the required counters by setting $A\\equiv1$ and by defining $B(e_{\\upsigma})$ to be the increment or decrement cased by $\\upsigma$ . In modeling such languages, SSMs pattern with both transformers [Bhattamishra et al., 2020] and LSTMs [Weiss et al., 2018]. ", "page_idx": 6}, {"type": "text", "text": "It may seem counterintuitive that NONNEGATIVE SSMs can perform unbounded counting but (by Theorem 2) not modular counting\u2014the latter would seem to just require reading out the value of an unbounded counter. What is key is that, even though $h_{t}$ can encode unbounded counts, reading out the modular value of an unbounded integer is a formidable problem for typical neural network nonlinearities, in particular when the information has been pushed through normalization (2). ", "page_idx": 6}, {"type": "text", "text": "We should note that there is a qualitative difference between this result and the preceding positive results about finite-state languages (Theorems 1 and 4), in that the construction in Theorem 5 uses unboundedly large entries in the state $h_{t}$ , whereas Theorems 1 and 4 use bounded values at finite precision. Indeed, we will find better length generalization in the finite-state case (Figure 5). ", "page_idx": 6}, {"type": "text", "text": "A consequence of Theorem 5 is that SSMs can recognize some languages transcending the contextfree languages, as $a^{n}b^{n}c^{n}$ is not context-free. A second application of the theorem, of great linguistic interest, is to bounded hierarchical structure, as we discuss next. ", "page_idx": 6}, {"type": "text", "text": "3.5 Bounded Hierarchical Structure without Stacks ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "It is generally agreed that hierarchical structure is a key aspect of language, and comprehending language at a human-like level requires the computational ability to process such structures [Chomsky and Sch\u00fctzenberger, 1963, Linzen et al., 2016, Everaert et al., 2015]. The fundamental data structure for the same is a stack, where information is stored and removed as one traverses to higher and lower levels of hierarchical embedding [Hopcroft et al., 2001]. We now show that SSMs\u2019 counting ability can offer shortcuts on languages modeling hierarchical structure, eschewing the need for a stack. ", "page_idx": 7}, {"type": "text", "text": "A useful abstraction of hierarchical structure as relevant to natural language is the family of Dyck languages. The bounded-depth Dyck language $D y c k_{K,h}$ with $K$ types of parentheses and depth $h$ is the language of well-bracketed strings over $(\\,_{1},\\,)_{1}$ , ..., $\\big(\\boldsymbol{\\kappa},\\,\\big)_{K}$ , such that the number of yet unclosed brackets never exceeds $h$ in any prefix [Hewitt et al., 2020, Yao et al., 2021b]. The ChomskySch\u00fctzenberger theorem [Chomsky and Sch\u00fctzenberger, 1963] asserts that any context-free language can be expressed as a homomorphic image of the intersection between a Dyck language and a regular language. Specifically, the Dyck language in question refers to the classical unboundeddepth Dyck language, where $h\\rightarrow\\infty$ , underscoring its fundamental role as the structural backbone of context-free languages. Bounding the depth reflects the fact that deep embedding is rare in natural language [Karlsson, 2007, Blasi et al., 2019]. Prior work has found that two-layer transformers [Yao et al., 2021a] and traditional RNNs [Hewitt et al., 2020, Bhattamishra et al., 2020] both model all $D y c k_{K,h}$ languages. The same turns out to hold for SSMs: ", "page_idx": 7}, {"type": "text", "text": "Theorem 6. There is a two-layer SSM with $d=O(h\\log K)$ that predictively models $D y c k_{K,h}$ at all input lengths, at finite precision. ", "page_idx": 7}, {"type": "text", "text": "The proof is in Appendix B.5. Intuitively (Figure 3), the first layer records the depth of each parenthesis using the ideas from Theorem 5, and the second layer keeps track of the last open bracket at each depth using Theorem 1. We note that, since $D y c k_{K,h}$ is star-free, Theorem 4 already guarantees the existence of representing SSMs, but the depth and width guaranteed by Theorem 6 is likely to be much better than what would be obtained by a black-box application of Theorem 4: As Hewitt et al. [2020] show, $h\\log K$ units is optimal up to constants and is attained by traditional RNNs and LSTMs. The SSM construction is very different from that of Hewitt et al. [2020] for traditional RNNs (both simple RNNs and LSTMs), which directly simulates a stack. Our construction is similar to the transformer construction in Theorem 4.2 in Yao et al. [2021a], which however has to rely on specific positional encodings, unlike the SSM construction. This highlights that stacks are not the only way of simulating bounded hierarchical structure in recurrent architectures, and nonstack-based strategies can even attain the same optimal scaling of hidden units. Probing whether such stack-free shortcuts are learned by SSM-based LLMs is an exciting problem for future research. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "eV5YIrJPdy/tmp/b576a31598c50d3a21265016548b1fada6a8ffde17aeb72aaf2443014c120107.jpg", "img_caption": ["Figure 4: As predicted by Theorem 6, Mamba with 2 layers can model Dyck(K, h). Results for test set with strings of length $700\\leq n\\leq1400$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We have derived a fine-grained theoretical characterization of expressiveness strengths and limitations of SSMs. We now show that our positive results can be instantiated and learned in a realistic SSM implementation, by evaluating a recent highly successful SSM, Mamba [Gu and Dao, 2023]. ", "page_idx": 7}, {"type": "text", "text": "FlipFlop We empirically instantiate Theorem 1 using the dataset of Liu et al. [2023a], reflecting the language $\\angle_{F F}$ as defined in Section 3.1. Matching Figure 2 in Liu et al. [2023a], we evaluated both on in-distribution data, and on out-of-distribution data where the distance between read and write instructions tended to be larger. We evaluate for predicting the bits following $r$ instructions3, ", "page_idx": 7}, {"type": "image", "img_path": "eV5YIrJPdy/tmp/d55161676b41fa3574255641428fd8a10b9107143fb1aa1c532cf1421f7f46e1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 5: Results on 27 formal languages, comparing our Mamba results (blue) with transformer results reported by Bhattamishra et al. [2020] (orange), on in-distribution lengths (solid) and outof-distribution lengths (dotted). As predicted by Theorem 4, Mamba performs strongly on star-free languages, and even shows perfect length generalization. Again as predicted by Theorem 4, it performs poorly on non-star-free languages. Results for transformers from Bhattamishra et al. [2020] are mixed. Mamba also succeeds on learning the counter languages from Theorem 5, showing perfect accuracy at in-distribution lengths at in-distribution lengths, but length generalization lags behind transformers. ", "page_idx": 8}, {"type": "text", "text": "matching the \u201cdeterministic/clean\u201d mode of Liu et al. [2023a], and considered predictions to be correct only if all predictions within a sequence were correct. (Further details in Appendix D.2). A small one-layer4 Mamba model converged to 0 error in both validation sets after $\\sim1400$ steps (Figure 6), compared to 500 steps for an LSTM reported by Liu et al. [2023a]. In contrast, Liu et al. [2023a] found that transformers kept making occasional mistakes despite training for 10K steps. ", "page_idx": 8}, {"type": "text", "text": "Test Suite from Bhattamishra et al. [2020] To test our theoretical results on regular and counter languages (Theorems 2, 4, 5), we test Mamba on 27 formal languages, including 18 regular languages and 9 counter languages, based on a prior study comparing transformers and RNNs [Bhattamishra et al., 2020]. The regular languages include a popular benchmark [Tomita, 1982] and various regular expressions; 11 are star-free. The counter languages include the languages covered by Theorem 5. (Definitions in Appendix C). We chose this test suite as it precisely covers Theorems 4 and 5, and we have proven (in)expressibility results for each language in the set. ", "page_idx": 8}, {"type": "text", "text": "Following Bhattamishra et al. [2020], we trained the model for predictive modeling, i.e., at each step, the model outputs a label indicating the set of possible next characters (3), including EOS when required. Following Bhattamishra et al. [2020], we count the model\u2019s response on an input string as correct if and only if predictive modelling was successful at all positions in the input. Such a evaluation setup makes random baselines low, where a random predictor would have an accuracy exponentially small in $N$ in each of the $N$ positions. Training inputs have length in [1,50]; the model is evaluated on held-out bins with length [1,50] and [51,100]. Further experimental details are in Appendix D.1. ", "page_idx": 8}, {"type": "image", "img_path": "eV5YIrJPdy/tmp/1a8b349bb84dc0e0193b7dcd63075c49b7776fceafad6c0db027be4de1c278e5.jpg", "img_caption": ["Figure 6: Test error on the validation set for $\\angle_{F F}$ , following Liu et al. [2023a]. Mamba shows near-zero test error in both In- (green) and Out-of-distribution (orange) settings, consistent with Theorem 1, and avoids the failure seen in transformers [Liu et al., 2023a] "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We show our Mamba results, together with Transformer results reported by Bhattamishra et al. [2020], in Figure 5. LSTMs perform perfectly on all languages, and are thus not shown. In a striking confirmation of Theorem 4, Mamba learns all star-free languages with strong length generalization but performs poorly on non-star-free languages. Transformers show more mixed results, often failing to length-generalize even on star-free languages. Consistent with Theorem 5 , Mamba, like Transformers, learns counter languages but struggles more with length generalization. The differences in Mamba\u2019s performance between star-free and counter languages may stem from the fact that the construction for the former class (Theorem 4) is able to use finite precision and bounded state values at arbitrary input lengths, while the latter (Theorem 5) uses unbounded state values. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Bounded Hierarchical Structure To test Theorem 6, we recreate the experimental setup from Yao et al. [2021b]. Matching their Figure 4, we trained Mamba to predictively model $D y c k_{K,h}$ at $K=8$ and $h=10$ . The training and the validation set contained samples of length $\\leq700$ , while the test set contained samples of length $700\\leq n\\leq1400$ . Yao et al. [2021b] found both transformers and LSTMs achieved strong performance on this setup. We provide further details in Appendix D.3. Recall that Theorem 6 shows that two-layer SSMs can predictively model $D y c k_{K,h}$ . We trained Mamba with 1 or 2 layers and varying dimensionality, finding that two layers can achieve essentially perfect performance across model sizes, even on the test set (Figure 4 and 7). ", "page_idx": 9}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Related Work Our work belongs to an incipient line of research into the expressiveness of SSMs [Jelassi et al., 2024, Merrill et al., 2024]. It is closely related to a long string of work studying the expressive capacity of neural sequence models, which has so far focused on recurrent networks [e.g. Siegelman and Sontag, 1995, Bhattamishra et al., 2020, Hewitt et al., 2020] and, more recently, self attention [e.g. Chiang et al., 2023, Merrill and Sabharwal, 2023, Strobl et al., 2024]. A second link is to the classical and long-standing study of linear dynamical systems and control theory [Kalman, 1960]. For instance, Theorem 2 relies the asymptotic convergence of an SSM on certain inputs, establishing a link to the asymptotics of linear systems [e.g. Phillips and Solo, 1992]. ", "page_idx": 9}, {"type": "text", "text": "Take-Aways While theoretical in nature, our results have several actionable implications for SSM and LLM research, informing the rapidly growing research on SSM-based LLMs. First, encouragingly, SSMs can keep track of bounded hierarchical structure with optimal memory even without explicitly implementing a stack (Theorem 6), suggesting that simple diagonal linear state updates may be sufficiently powerful for modeling the hierarchical structure of language. Second, SSMs resolve a basic failure mode of self-attention in flip-flop state tracking while being parallellizable (Theorem 1). Overall, SSMs and attention have overlapping but distinct strengths. This lends support to the development of hybrid architectures interleaving SSM and attention layers, as instantiated very recently by Jamba [Lieber et al., 2024]. Third, nonnegative gates as obtained by exponential or sigmoid parameterizations provably restrict expressive capacity, even in non-time-invariant SSMs (Theorem 2). While Gu and Dao [2023] found no evidence that complex-valued paramerizations improved over real-valued ones in the language modality, our results suggest revisiting this question, at least for tasks where periodic state-tracking abilities may be important. Fourth, while exactly characterizing the capacity of transformers has proven difficult even in the finite-state case, Theorem 4 provides a decidable characterization of the regular languages \u2013 equivalently, finite-state tracking problems \u2013 that SSMs such as Mamba can model. Such decidable characterizations may make it easier to theoretically predict abilities and anticipate failures of LLMs; exploring the implications of this characterization in more realistic setups is an exciting direction for future research. ", "page_idx": 9}, {"type": "text", "text": "Limitations The main limitation of our theoretical results is that they focus on in-principle expressiveness, and do not directly make statements about learning and generalization. Future work could address this, for example, by examining whether our constructions result in reasonably flat minima, or by studying gradient flow dynamics. While we empirically verified that our positive results can indeed be instantiated, in a learnable manner, in one realistic SSM implementation, implementational differences might still result in practical differences between implementations. Studying the role of such implementational differences is an interesting problem for future work; we have made a first step by theoretically elucidating the implications of nonnegative gate values. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have studied the expressive capacity of modern state space models (SSMs), through the lens of automata and formal languages. We have shown theoretically that SSMs can express star-free languages, a range of counter languages, and bounded hierarchical structure. By providing rigorous results about the expressiveness of the SSM architecture, our results can provide guidance to work on SSM-based language models. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Mark Rofin for useful discussion about Theorem 2 and we thank anonymous reviewers for their helpful feedback. We gratefully acknowledge the insightful discussions with members of the FLaNN community which contributed to the ideas of this project. Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) \u2013 Project-ID 232722074 \u2013 SFB 1102. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "E. Aky\u00fcrek, B. Wang, Y. Kim, and J. Andreas. In-context language learning: Architectures and algorithms. In Forty-first International Conference on Machine Learning, 2024. URL https: //openreview.net/forum?id=3Z9CRr5srL.   \nJ. Almeida. Finite semigroups and universal algebra, volume 3. World Scientific, 1995.   \nD. Angluin, D. Chiang, and A. Yang. Masked hard-attention transformers and boolean rasp recognize exactly the star-free languages. arXiv preprint arXiv:2310.13897, 2023.   \nR. Arora, A. Basu, P. Mianjy, and A. Mukherjee. Understanding deep neural networks with rectified linear units. arXiv preprint arXiv:1611.01491, 2016.   \nJ. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. stat, 1050:21, 2016.   \nD. A. M. Barrington, K. Compton, H. Straubing, and D. Th\u00e9rien. Regular languages in NC1. Journal of Computer and System Sciences, 44(3):478\u2013499, 1992.   \nS. Bhattamishra, K. Ahuja, and N. Goyal. On the ability and limitations of transformers to recognize formal languages. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7096\u20137116, 2020.   \nS. Bhattamishra, M. Hahn, P. Blunsom, and V. Kanade. Separations in the representational capabilities of transformers and recurrent architectures. arXiv preprint arXiv:2406.09347, 2024.   \nD. Blasi, R. Cotterell, L. Wolf-Sonkin, S. Stoll, B. Bickel, and M. Baroni. On the distribution of deep clausal embeddings: A large cross-linguistic study. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3938\u20133943, 2019.   \nJ. Bradbury, S. Merity, C. Xiong, and R. Socher. Quasi-recurrent neural networks. In International Conference on Learning Representations, 2016.   \nD. Chiang and P. Cholak. Overcoming a theoretical limitation of self-attention. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7654\u20137664, 2022.   \nD. Chiang, P. Cholak, and A. Pillay. Tighter bounds on the expressivity of transformer encoders. In International Conference on Machine Learning, pages 5544\u20135562. PMLR, 2023.   \nN. Chomsky. Syntactic structures, 1957.   \nN. Chomsky and M. P. Sch\u00fctzenberger. The algebraic theory of context-free languages. In Studies in Logic and the Foundations of Mathematics, volume 35, pages 118\u2013161. Elsevier, 1963.   \nG. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems, 2(4):303\u2013314, 1989.   \nY. N. Dauphin, A. Fan, M. Auli, and D. Grangier. Language modeling with gated convolutional networks. In International conference on machine learning, pages 933\u2013941. PMLR, 2017.   \nS. De, S. L. Smith, A. Fernando, A. Botev, G. Muraru, A. Gu, R. Haroun, L. Berrada, Y. Chen, S. Srinivasan, G. Desjardins, A. Doucet, D. Budden, Y. W. Teh, R. Pascanu, N. de Freitas, and \u00c7. G\u00fcl\u00e7ehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models. CoRR, abs/2402.19427, 2024. doi: 10.48550/ARXIV.2402.19427. URL https://doi. org/10.48550/arXiv.2402.19427. ", "page_idx": 10}, {"type": "text", "text": "G. Deletang, A. Ruoss, J. Grau-Moya, T. Genewein, L. K. Wenliang, E. Catt, C. Cundy, M. Hutter, S. Legg, J. Veness, et al. Neural networks and the chomsky hierarchy. In The Eleventh International Conference on Learning Representations, 2022. ", "page_idx": 11}, {"type": "text", "text": "S. Eilenberg. Automata, languages, and machines. Academic press, 1974. ", "page_idx": 11}, {"type": "text", "text": "J. L. Elman. Finding structure in time. Cognitive science, 14(2):179\u2013211, 1990.   \nM. B. Everaert, M. A. Huybregts, N. Chomsky, R. C. Berwick, and J. J. Bolhuis. Structures, not strings: linguistics as part of the cognitive sciences. Trends in cognitive sciences, 19(12):729\u2013743, 2015.   \nP. C. Fischer, A. R. Meyer, and A. L. Rosenberg. Counter machines and counter languages. Mathematical systems theory, 2(3):265\u2013283, Sep 1968a. ISSN 1433-0490. doi: 10.1007/BF01694011. URL https://doi.org/10.1007/BF01694011.   \nP. C. Fischer, A. R. Meyer, and A. L. Rosenberg. Counter machines and counter languages. Math. Syst. Theory, 2(3):265\u2013283, 1968b. doi: 10.1007/BF01694011. URL https://doi.org/10. 1007/BF01694011.   \nD. Y. Fu, T. Dao, K. K. Saab, A. W. Thomas, A. Rudra, and C. Re. Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $\\equiv$ COZDy0WYGg.   \nA. Ginzburg. Algebraic theory of automata. Academic Press, 1968.   \nA. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. CoRR, abs/2312.00752, 2023. doi: 10.48550/ARXIV.2312.00752. URL https://doi.org/10.48550/ arXiv.2312.00752.   \nA. Gu, T. Dao, S. Ermon, A. Rudra, and C. R\u00e9. Hippo: Recurrent memory with optimal polynomial projections. Advances in neural information processing systems, 33:1474\u20131487, 2020.   \nA. Gu, K. Goel, and C. Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2021.   \nM. Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156\u2013171, 2020.   \nM. Hahn and M. Rofin. Why are sensitive functions hard for transformers? In L.-W. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14973\u201315008, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.800. URL https://aclanthology.org/2024.acl-long.800.   \nM. Hahn, A. Krebs, K. Lange, and M. Ludwig. Visibly counter languages and the structure of nc1. In G. F. Italiano, G. Pighizzini, and D. Sannella, editors, Mathematical Foundations of Computer Science 2015 - 40th International Symposium, MFCS 2015, Milan, Italy, August 24- 28, 2015, Proceedings, Part II, volume 9235 of Lecture Notes in Computer Science, pages 384\u2013 394. Springer, 2015. doi: 10.1007/978-3-662-48054-0\\_32. URL https://doi.org/10.1007/ 978-3-662-48054-0_32.   \nJ. Hewitt, M. Hahn, S. Ganguli, P. Liang, and C. D. Manning. Rnns can generate bounded hierarchical languages with optimal memory. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1978\u20132010, 2020.   \nS. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735\u20131780, 1997.   \nJ. E. Hopcroft, R. Motwani, and J. D. Ullman. Introduction to automata theory, languages, and computation. ACM New York, NY, USA, 2001.   \nB. Horne and D. Hush. Bounds on the complexity of recurrent neural network implementations of finite state machines. Advances in neural information processing systems, 6, 1993.   \nX. Huang, A. Yang, S. Bhattamishra, Y. Sarrof, A. Krebs, H. Zhou, P. Nakkiran, and M. Hahn. A formal framework for understanding length generalization in transformers. arXiv preprint arXiv:2410.02140, 2024.   \nP. Indyk. Optimal simulation of automata by neural nets. In Annual Symposium on Theoretical Aspects of Computer Science, pages 337\u2013348. Springer, 1995.   \nY. Ito. Approximation of continuous functions on rd by linear combinations of shifted rotations of a sigmoid function with and without scaling. Neural Networks, 5(1):105\u2013115, 1992.   \nS. Jelassi, D. Brandfonbrener, S. M. Kakade, and E. Malach. Repeat after me: Transformers are better than state space models at copying, 2024.   \nR. E. Kalman. On the general theory of control systems. In Proceedings First International Conference on Automatic Control, Moscow, USSR, pages 481\u2013492, 1960.   \nR. E. Kalman. Mathematical description of linear dynamical systems. Journal of the Society for Industrial and Applied Mathematics, Series A: Control, 1(2):152\u2013192, 1963.   \nF. Karlsson. Constraints on multiple center-embedding of clauses. Journal of Linguistics, 43(2): 365\u2013392, 2007.   \nS. Kleene. Representationof events in nerve nets and finite automata. CE Shannon and J. McCarthy, 1951.   \nA. Krebs, K. Lange, and M. Ludwig. Visibly counter languages and constant depth circuits. In E. W. Mayr and N. Ollinger, editors, 32nd International Symposium on Theoretical Aspects of Computer Science, STACS 2015, March 4-7, 2015, Garching, Germany, volume 30 of LIPIcs, pages 594\u2013607. Schloss Dagstuhl - Leibniz-Zentrum f\u00fcr Informatik, 2015. doi: 10.4230/LIPICS. STACS.2015.594. URL https://doi.org/10.4230/LIPIcs.STACS.2015.594.   \nK. Krohn and J. Rhodes. Algebraic theory of machines. i. prime decomposition theorem for finite semigroups and machines. Transactions of the American Mathematical Society, 116:450\u2013464, 1965.   \nM. Kutrib, A. Malcher, and M. Wendlandt. Input-driven multi-counter automata. Theoretical Computer Science, 870:121\u2013136, 2021.   \nT. Lei, Y. Zhang, S. I. Wang, H. Dai, and Y. Artzi. Simple recurrent units for highly parallelizable recurrence. In E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4470\u2013 4481, Brussels, Belgium, Oct.-Nov. 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1477. URL https://aclanthology.org/D18-1477.   \nO. Lieber, B. Lenz, H. Bata, G. Cohen, J. Osin, I. Dalmedigos, E. Safahi, S. Meirom, Y. Belinkov, S. Shalev-Shwartz, O. Abend, R. Alon, T. Asida, A. Bergman, R. Glozman, M. Gokhman, A. Manevich, N. Ratner, N. Rozen, E. Shwartz, M. Zusman, and Y. Shoham. Jamba: A hybrid transformer-mamba language model, 2024.   \nT. Linzen, E. Dupoux, and Y. Goldberg. Assessing the ability of LSTMs to learn syntax-sensitive dependencies. Transactions of the Association for Computational Linguistics, 4:521\u2013535, 2016.   \nB. Liu, J. T. Ash, S. Goel, A. Krishnamurthy, and C. Zhang. Exposing attention glitches with flip-flop language modeling. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023a. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 510ad3018bbdc5b6e3b10646e2e35771-Abstract-Conference.html.   \nB. Liu, J. T. Ash, S. Goel, A. Krishnamurthy, and C. Zhang. Transformers learn shortcuts to automata. In The Eleventh International Conference on Learning Representations, 2023b. URL https://openreview.net/forum?id=De4FYqjFueZ.   \nR. McNaughton and S. A. Papert. Counter-Free Automata (MIT research monograph no. 65). The MIT Press, 1971.   \nH. Mehta, A. Gupta, A. Cutkosky, and B. Neyshabur. Long range language modeling via gated state spaces. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $\\smash{\\overline{{\\,\\cdot\\,}}}=\\sum_{\\,\\,\\,\\,\\,\\,\\,}$ 5MkYIYCbva.   \nW. Merrill and A. Sabharwal. A logic for expressing log-precision transformers. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \nW. Merrill, J. Petty, and A. Sabharwal. The illusion of state in state-space models. In ICML, 2024.   \nA. Miller. Finitary models of language users. Handbook of mathematical psychology, 2, 1963.   \nA. Orvieto, S. L. Smith, A. Gu, A. Fernando, C. Gulcehre, R. Pascanu, and S. De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pages 26670\u201326698. PMLR, 2023.   \nA. Orvieto, S. De, C. Gulcehre, R. Pascanu, and S. L. Smith. Universality of linear recurrences followed by non-linear projections: Finite-width guarantees and benefits of complex eigenvalues. In Forty-first International Conference on Machine Learning, 2024.   \nB. Peng, S. Narayanan, and C. Papadimitriou. On limitations of the transformer architecture. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id= KidynPuLNW.   \nJ. P\u00e9rez, J. Marinkovic\u00b4, and P. Barcel\u00f3. On the turing completeness of modern neural network architectures. In International Conference on Learning Representations, 2019.   \nP. C. Phillips and V. Solo. Asymptotics for linear processes. The Annals of Statistics, pages 971\u2013 1001, 1992.   \nZ. Qin, S. Yang, W. Sun, X. Shen, D. Li, W. Sun, and Y. Zhong. HGRN2: Gated linear RNNs with state expansion. In First Conference on Language Modeling, 2024a. URL https: //openreview.net/forum?id=y6SqbJfCSk.   \nZ. Qin, S. Yang, and Y. Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024b.   \nJ. Sakarovitch. Elements of automata theory. Cambridge university press, 2009.   \nC. Sanford, D. J. Hsu, and M. Telgarsky. Representational strengths and limitations of transformers. Advances in Neural Information Processing Systems, 36, 2024.   \nM. P. Sch\u00fctzenberger. On finite monoids having only trivial subgroups. Inf. Control., 8(2):190\u2013194, 1965.   \nN. Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.   \nH. Siegelman and E. D. Sontag. On the computational power of neural nets. Journal of Computer and System Sciences, 50:132\u2013150, 1995.   \nH. T. Siegelmann. Neural networks and analog computation: beyond the Turing limit. Springer Science & Business Media, 1999.   \nH. Straubing. Finite automata, formal logic, and circuit complexity. Birkhaeuser, 1994.   \nL. Strobl. Average-hard attention transformers are constant-depth uniform threshold circuits, 2023.   \nL. Strobl, W. Merrill, G. Weiss, D. Chiang, and D. Angluin. What formal languages can transformers express? a survey. Transactions of the Association for Computational Linguistics, 12:543\u2013561, 2024. doi: 10.1162/tacl_a_00663. URL https://aclanthology.org/2024.tacl-1.30.   \nY. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue, J. Wang, and F. Wei. Retentive network: A successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023.   \nM. Suzgun, Y. Belinkov, and S. M. Shieber. On evaluating the generalization of lstm models in formal languages. Proceedings of the Society for Computation in Linguistics (SCiL), pages 277\u2013 286, 2019.   \nM. Tomita. Dynamic construction of finite-state automata from examples using hill-climbing. In Proceedings of the Fourth Annual Conference of the Cognitive Science Society, pages 105\u2013108, 1982.   \nT. D. van Nuland. Noncompact uniform universal approximation. Neural Networks, 173:106181, 2024.   \nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998\u20136008, 2017.   \nS. Wang and Q. Li. Stablessm: Alleviating the curse of memory in state-space models through stable reparameterization. arXiv preprint arXiv:2311.14495, 2023.   \nS. Wang and B. Xue. State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory. Advances in Neural Information Processing Systems, 36, 2024.   \nG. Weiss, Y. Goldberg, and E. Yahav. On the practical computational power of finite precision rnns for language recognition. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 740\u2013745, 2018.   \nA. Yang and D. Chiang. Counting like transformers: Compiling temporal counting logic into softmax transformers. In First Conference on Language Modeling, 2024. URL https:// openreview.net/forum?id=FmhPg4UJ9K.   \nS. Yang, B. Wang, Y. Shen, R. Panda, and Y. Kim. Gated linear attention transformers with hardware-efficient training. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=ia5XvxFUJT.   \nS. Yao, B. Peng, C. Papadimitriou, and K. Narasimhan. Self-attention networks can process bounded hierarchical languages. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Association for Computational Linguistics, 2021a. doi: 10.18653/v1/ 2021.acl-long.292. URL http://dx.doi.org/10.18653/v1/2021.acl-long.292.   \nS. Yao, B. Peng, C. Papadimitriou, and K. Narasimhan. Self-attention networks can process bounded hierarchical languages. In C. Zong, F. Xia, W. Li, and R. Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3770\u20133785, Online, Aug. 2021b. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.292. URL https://aclanthology.org/2021.acl-long.292.   \nA. Yu, A. Nigmetov, D. Morozov, M. W. Mahoney, and N. B. Erichson. Robustifying state-space models for long sequences via approximate diagonalization. arXiv preprint arXiv:2310.01698, 2023.   \nB. Zhang and R. Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019.   \nN. Zubic\u00b4, F. Sold\u00e1, A. Sulser, and D. Scaramuzza. Limits of deep learning: Sequence modeling through the lens of complexity theory. arXiv preprint arXiv:2405.16674, 2024. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Instantiations of General Framework in SSM Models ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here, we survey how (1) is instantiated in a range of SSMs. As stated in Section 2.1, we refer to SSMs where the gate $A$ does not depend on $x_{t}$ as time-invariant. An equivalent terminology is the distinction between \u201cWeak Linear Time Invariant Convolutional Models\u201d (i.e., time-invariant) and \u201cLinear Time Variant Models\u201d (i.e., non-time-invariant) in Aky\u00fcrek et al. [2024]. ", "page_idx": 15}, {"type": "text", "text": "A.1 Non-Time-Invariant Models ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Approximately simultaneously with or more recently than Gu and Dao [2023], a range of non-timeinvariant SSMs have been introduced [De et al., 2024, Yang et al., 2024, Qin et al., 2024b,a]. This category also covers highly similar earlier RNN variants [Bradbury et al., 2016, Lei et al., 2018]. ", "page_idx": 15}, {"type": "text", "text": "Mamba In Mamba, (2) and (3) directly map onto Eqs. (2a) and (2b) in Gu and Dao [2023]. The notation of Gu and Dao [2023] use a matrix multiplication $\\overline{{A}}h_{t-1}$ instead of elementwise multiplication $A(x_{t})\\circ h_{t-1}$ in (REF), but importantly, Mamba\u2019s $\\overline{{A}}$ is diagonal, so we can take $A(x_{t})_{i}=\\overline{{A}}_{i i}$ . Due to exponential parameterization, its entries are nonnegative. ", "page_idx": 15}, {"type": "text", "text": "Griffin The RG-LRU layer of Griffin [De et al., 2024] uses the equation ", "page_idx": 15}, {"type": "equation", "text": "$$\nh_{t}=\\underbrace{a_{t}}_{A(x_{t})}\\circ h_{t-1}+\\underbrace{\\sqrt{1-a_{t}^{2}}\\circ(i_{t}\\circ x_{t})}_{B(x_{t})}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $a_{t},i_{t}$ are neurally parameterized in terms of $x_{t}$ but not $h_{<t}$ ; by design, $a_{t}\\in(0,1)$ . $\\boldsymbol{\\Phi}$ is instantiated in terms of linear transformations, GeLU, and RMSNorm (Figure 2 in De et al. [2024]). The local attention used by Griffin can be subsumed into an SSM layer (Remark 19). ", "page_idx": 15}, {"type": "text", "text": "Gated Linear Attention [GLA Yang et al., 2024] This model (Section 4.4 in Yang et al. [2024]) instantiates our framework using a recurrence of the form (1); while the state is two-dimensional in this model, the update is performed by elementwise products as in (1). The gate is obtained by applying sigmoid to a linear transformation of $x_{t}$ ; thus, its entries are in $(0,1)$ . $\\boldsymbol{\\Phi}$ is instantiated in terms of SwiGLU and LayerNorm. ", "page_idx": 15}, {"type": "text", "text": "HGRN HGRN [Qin et al., 2024b] and HGRN2 [Qin et al., 2024a] are defined by a recurrence of the form (1); the gate entries are $\\in(0,1)$ by design. $\\boldsymbol{\\Phi}$ is instantiated in terms of GLU, linear transformations, and normalization. In HGRN, the state is complex, but crucially the gate remains real-valued. ", "page_idx": 15}, {"type": "text", "text": "A.2 Time-Invariant Models ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Time-invariant SSMs introduced before late 2023 are surveyed by Gu and Dao [2023, Appendix B], such as [Mehta et al., 2023, Sun et al., 2023, Orvieto et al., 2023]. Time-invariant SSMs have often used complex-valued states and gates; this does not have a major impact on our results: First, as complex-valued SSMs subsume real-valued ones, our positive results carry over. Second, our negative result about PARITY is affected by this distinction and requires a separate argument, see Theorem 13. ", "page_idx": 15}, {"type": "text", "text": "Note also that $A h_{t-1}$ is often described as a general matrix multiplication, but $A$ is diagonalizable (e.g. Lemma 3.2 in Gu et al. [2021]; Sun et al. [2023] for RetNet), which \u2014even though implementation may be based on non-diagonalized representations [Gu et al., 2021]\u2014renders the model equivalent to one where $A$ is diagonal from the start. This equivalence is shown as Lemma 3.1 in $\\mathrm{Gu}$ et al. [2021]. ", "page_idx": 15}, {"type": "text", "text": "B Formal Definitions and Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Flip Flop ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We begin by introducing key notions of automata theory. References for automata theory include Eilenberg [1974], Hopcroft et al. [2001], Sakarovitch [2009]. We will provide those key notions that are necessary to prove our results. We will focus on deterministic finite-state-automata (DFA), and simply refer to them as finite-state-automata.5 First, ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Definition 7. A (deterministic) finite-state-automaton A consists of: ", "page_idx": 16}, {"type": "text", "text": "\u2022 a finite alphabet $\\Sigma$   \n\u2022 a finite state set $Q$   \n\u2022 a starting state $q_{0}\\in\\cal{Q}$   \n\u2022 a transition function $u:Q\\times\\Sigma\\to Q$ ", "page_idx": 16}, {"type": "text", "text": "We extend u to a map $u:Q\\times\\Sigma^{*}\\to Q$ by setting: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{u(q,\\mathfrak{E})=q}}\\\\ {{u(q,w_{1...i+1})=u(u(q,w_{1...i}),w_{i+1})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where \u03b5 is the empty word. ", "page_idx": 16}, {"type": "text", "text": "Intuitively, $u(q_{0},\\mathbf{w})$ is the state that $\\mathcal{A}$ is in after reading w. ", "page_idx": 16}, {"type": "text", "text": "The automaton recognizes $a$ language $L\\subseteq\\Sigma^{*}$ if there is $a$ recognizing set $R\\subseteq Q$ such that ", "page_idx": 16}, {"type": "equation", "text": "$$\nL:=\\left\\{w:u(q_{0},\\mathbf{w})\\in R\\right\\}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Kleene\u2019s Theorem [Kleene, 1951] asserts that a language $L\\subseteq\\Sigma^{*}$ is regular (i.e., defined by a regular expression) if and only if it is recognized by some finite-state automaton. ", "page_idx": 16}, {"type": "text", "text": "A very fundamental automaton underlying Flip Flop is: ", "page_idx": 16}, {"type": "text", "text": "Definition 8. A set-reset automaton is a finite-state-automaton where $(Q\\setminus\\{q_{0}\\})\\subseteq\\Sigma$ and ", "page_idx": 16}, {"type": "equation", "text": "$$\nu(q,\\upsigma)=\\left\\{q\\atop\\upsigma\\right.\\quad i f\\upsigma\\not\\in Q.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Intuitively, such an automaton keeps recording the last seen symbol from a designated set $Q\\subseteq\\Sigma$ . Such an automaton is easily simulated with a single non-time-invariant SSM layer: ", "page_idx": 16}, {"type": "text", "text": "Lemma 9. Let $\\mathcal{A}=\\langle\\Sigma,Q,q_{0},u\\rangle$ by a set-reset automaton. Then there is a single-layer SSM with finite precision and width $d=1+\\log Q$ that maps each $w_{1,...T}\\in\\Sigma^{*}$ to the state sequence $u(q_{0},w_{1}),u(q_{0},w_{12}),\\ldots,u(q_{0},w_{1\\ldots T})\\in Q^{T}$ . ", "page_idx": 16}, {"type": "text", "text": "Formally, there is an injective map $V:Q\\rightarrow\\mathbb{R}^{d}$ such that $\\uprho(z_{t})=V\\big(u(q_{0},w_{1\\ldots t})\\big)$ for $t=1,\\dots,T.$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. Let $B(\\pmb{\\upsigma})\\in\\mathbb{R}^{\\log|Q|}$ be a binary encoding if ${\\upsigma}\\in{\\cal Q}$ , and $\\mathbf{0}\\in\\mathbb{R}^{\\log|Q|}$ else. Take $h_{0}=B(q_{0})$ . Let $A(\\upsigma)=\\mathbf{0}$ if ${\\upsigma}\\in{\\cal Q}$ and $A(\\upsigma)=1$ else. After processing a string, the state $h_{t}$ is $B(\\upsigma)$ where $\\upsigma$ is the last symbol in $Q$ that has occurred if any has, and $B(q_{0})$ otherwise. Coming to (2, in order to avoid division by zero when normalizing if no element of $Q$ has been read, we add a dummy dimension to $h_{t}$ whose value is always 1. We take $\\mathrm{Mix}_{1},\\mathrm{Mix}_{2}$ to be the identity. Note that, even though normalization will affect the numerical values, the binary encoding of $\\upsigma\\in{\\cal Q}$ can still be read out with finite precision, as $1\\leq\\|h_{t}\\|_{2}\\leq\\sqrt{1+\\log|Q|}$ , and thus nonzero entries will remain bounded away from zero. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Theorem 10 (Restated from Theorem 1). There is a two-layer SSM that predictively models $\\mathcal{L}_{F F}$ at all lengths, at finite precision. ", "page_idx": 16}, {"type": "text", "text": "Proof. In the first layer, we use Lemma 9 to simulate a set-reset automaton over the input alphabet $\\bar{\\Sigma}_{1}=\\{w,r,i,0,1\\}$ where $Q_{1}=\\Sigma_{1}\\cup\\{q_{0}\\}$ . This layer outputs at each position whether the last instruction was write, read, or ignore. The layer additionally, at each position, forwards the input symbol using additional dimensions. Formally, at the first layer, $\\uprho(h_{t})$ allows us to read out the input symbols $x_{t-1},x_{t}\\in\\Sigma$ . ", "page_idx": 16}, {"type": "text", "text": "In the second layer, we again use Lemma 9 to simulate a set-reset automaton over an extended alphabet $\\Sigma_{2}:=\\Sigma_{1}\\times\\Sigma_{1}$ , where the first component indicates the input symbol $x_{t}$ and where the second component indicates $x_{t-1}$ . In this set-reset automaton, $Q_{2}$ contains, besides a start state $q_{0}$ , those elements of $\\Sigma_{2}$ whose second entry is $w$ . The second layer thus keeps track of the input bit $b\\in\\{0,1\\}$ following the last write instruction. It additionally forwards the input symbol $x_{t}$ using additional dimensions. ", "page_idx": 17}, {"type": "text", "text": "The second layer, via $\\uprho$ , then predicts the possible next symbols on the basis of this information: If $x_{t}\\in\\{0,1\\}$ , any instruction in $\\bar{\\{\\boldsymbol{w},\\boldsymbol{r},i\\}}$ is possible. If $x_{t}\\in\\dot{\\{w,i\\}}$ , any bit in $\\{0,1\\}$ is possible. If $x_{t}=r,$ , the bit stored after the last write instruction is possible; if no write instruction has appeared (hence, the second automaton is still in its start state), any bit in $\\{0,1\\}$ is possible. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "B.2 Difficulty of Representing PARITY ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Definition 11. PARITY is the regular language over $\\Sigma=\\{0,1\\}$ of strings where the number of ones is even. As a regular expression, PARITY is $(0^{*}10^{*}10^{*})^{*}$ . ", "page_idx": 17}, {"type": "text", "text": "Theorem 12 (Restated from Theorem 2). No SSM satisfying NONNEGATIVE can recognize PARITY at arbitrary input lengths with finite precision. ", "page_idx": 17}, {"type": "text", "text": "Proof. We consider an SSM with multiple layers, and indicate the layer in superscript: $h_{t}^{(1)},\\ldots,h_{t}^{(L)}$ . We write zt $z_{t}^{(0)}$ for the input token embedding $e(w_{t})$ . Consider a SSM processing the word $1^{t}$ , for $t\\rightarrow\\infty$ . We show, by induction over the number of layers, the following claim: ", "page_idx": 17}, {"type": "text", "text": "(\u2020) Each entry of $z_{t}^{(k)}$ converges to a value bounded, in absolute value, by a constant. ", "page_idx": 17}, {"type": "text", "text": "By the assumption of finite precision, convergence automatically leads to the entries becoming ultimately constant. Once we have shown this, we know that $z_{t}^{(L)}$ is constant when $t$ is sufficiently large; thus, the parity of the string $1^{t}$ cannot be read out from $z_{t}^{(L)}$ . As a consequence, the SSM cannot recognize PARITY. Indeed, we have shwon the stronger claim that the language $(11)^{*}$ \u2013 the language of even-length strings over one symbol \u2013 is not recognized by an SSM; we will use this stronger statement in Corollary 14. ", "page_idx": 17}, {"type": "text", "text": "We proceed to proving $(\\dagger)$ . The claim $(\\dagger)$ is trivially true at $k=0$ , as the input token is always the same and we defined $z_{t}^{(0)}:=e(w_{t})$ . Now consider $k>0$ . By hypothesis, the activations are given as ", "page_idx": 17}, {"type": "equation", "text": "$$\nh_{t}^{(k)}=A(x_{t})\\circ h_{t-1}^{(k)}+B(x_{t})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $A(x_{t}),B(x_{t})$ are constant $\\alpha:=A(x_{t})$ , $\\upbeta:=B(x_{t})$ when $t>T_{0}$ , for some $T_{0}>0$ . The solution of the recurrence for $t>T_{0}$ is ", "page_idx": 17}, {"type": "equation", "text": "$$\nh_{t}=\\upalpha^{t-T_{0}}\\left(h_{T_{0}}+\\frac{\\upbeta}{\\upalpha-1}\\right)+\\frac{\\upbeta}{1-\\upalpha}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Each dimension j = 1,...,d of this vector can be constant (if (hT0)j +\u03b1j\u03b2\u2212j1 ), diverge exponentially $(\\alpha_{j}>1)$ , converge exponentially $(\\alpha_{j}<1)$ or diverge linearly $(\\alpha_{j}=1$ ). ", "page_idx": 17}, {"type": "text", "text": "We next need to show that $\\boldsymbol z_{t}=\\mathbf{M}\\mathrm{i}\\mathbf{x}_{2}\\big(\\mathbf{Norm}(\\mathbf{M}\\mathrm{i}\\mathbf{x}_{1}(h_{t},\\boldsymbol{x}_{t}))\\big)$ converges. ", "page_idx": 17}, {"type": "text", "text": "First, consider the effect of applying a linear transformation to the state $h_{t}$ . Each entry of the result will be some linear combination ", "page_idx": 17}, {"type": "equation", "text": "$$\nu_{t}=\\lambda_{1}(h_{t})_{1}+\\cdot\\cdot\\cdot+\\lambda_{d}(h_{t})_{d}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "If each $\\alpha_{j}<1$ , then $u_{t}$ converges. If some $\\vert\\alpha_{j}\\vert\\geq1$ , there may be some cancellation if $\\alpha_{i}=\\alpha_{j}$ for some $i\\neq j$ ; cancellation can only lead to full erasure of the relevant terms or to a remaining term with the same exponent. In conclusion, each entry $u_{t}$ will again either converge to a finite value or diverge towards $\\pm\\infty$ . ", "page_idx": 17}, {"type": "text", "text": "We now need to understand the behavior of $\\mathrm{Mix}_{1}(h_{t},x_{t})$ . Recall that, based on our survey (Appendix A), we allowed it to contain linear, GLU [Dauphin et al., 2017], and SwiGLU [Shazeer, 2020] components. If $\\mathrm{Mix}_{1}(h_{t},x_{t})$ implements a linear transformation only, each entry likewise may converge, diverge linearly, or diverge exponentially. We note that\u2014if $\\upsigma$ is the sigmoid function\u2014 $\\sigma(u_{t})$ always converges, as $\\upsigma$ simply saturates to 0 or 1 if $u_{t}$ diverges. Hence, if $\\bar{\\bf M}\\!{\\bf i x}_{1}(h_{t},x_{t})$ implements GLU, each entry likewise may converge, diverge linearly, or diverge exponentially. Finally, if $\\mathrm{Mix}_{1}(h_{t},x_{t})$ implements SwiGLU, each entry of the result will be a product of a linear combination of the form $u_{t}$ , and $S w i s h_{\\upbeta}$ applied to another such linear combination. Depending on the behavior of these two $u_{t}$ -like terms, the outcome will behave as a product of sequences that may converge exponentially, diverge exponentially, or diverge linearly \u2013 e.g., the outcome may also diverge quadratically, or converge as $n\\mathbf{\\alpha}^{-n}$ , etc. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "If all dimensions of ${\\mathrm{Mix}}_{1}(h_{t},x_{t})$ converge, then $\\operatorname{Norm}(\\operatorname{Mix}_{1}(h_{t},x_{t}))$ will also converge to a scaled version o f1\u2212\u03b2i\u03b1i , scaled by a bounded factor as \u03b2i \u0338= 0. Now assume some dimensions of Mix1(ht,xt) do not converge; in this case, for any two dimensions $i,j$ , either their ratio will converge to a constant, or converge to 0 or $\\pm\\infty$ . After applying $\\operatorname{Norm}(\\cdot)$ , the entries asymptotically dominating the others will converge to a finite value bounded, in absolute value, by 1; the others will converge to zero. ", "page_idx": 18}, {"type": "text", "text": "In conclusion, we have found that each entry of ${\\mathrm{Norm}}(\\mathrm{Mix}_{1}(h_{t},x_{t}))$ converges to some number bounded, in absolute value, by 1. As $\\mathbf{M}\\mathrm{i}\\mathbf{x}_{2}$ is continuous, each entry of $z_{t}$ likewise converges, with a bound depending on the Lipschitz constant of $\\mathbf{M}\\mathrm{i}\\mathbf{x}_{2}$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "We next show the result, referenced in the main paper text after Theorem 2, about time-invariant SSMs with complex-valued gates: ", "page_idx": 18}, {"type": "text", "text": "Theorem 13. TIME-INVARIANT SSMs cannot recognize PARITY with finite precision at arbitrary input lengths, even with complex-valued gates, as long as each entry in each A has a rational angle in the complex plane. ", "page_idx": 18}, {"type": "text", "text": "Here, by a rational angle, we refer to an angle that is a rational number when expressed in degrees; such angles are rational multiples of $2\\pi$ when expressed in radians. As the rational angles are dense in the reals, one expects that even if some irrational angles permitted modeling PARITY, such solutions would be very hard to find \u2013 in particular given that irrational numbers are not exactly represented in finite precision. ", "page_idx": 18}, {"type": "text", "text": "Proof. By assumption, any $A_{j}\\in\\mathbb{C}$ in any layer can be written as ", "page_idx": 18}, {"type": "equation", "text": "$$\nA_{j}=r_{j}\\exp(2\\pi i q_{j})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $q_{j}\\in[0,1]$ is rational and $r_{j}\\geq0$ is real \u2013 here, $2\\pi q_{j}$ is known as the argument of $A_{j}$ ; it describes the angle of $A_{j}$ in the complex plane in radians. Correspondingly, the angle in degrees is described by $q_{j}\\cdot360^{\\circ}$ ; this is rational if and only if $q_{j}$ is. ", "page_idx": 18}, {"type": "text", "text": "As a time-invariant SSM has a finite number of such values $A_{j}$ , across all its layers, we can select a positive integer $W$ such that $W q_{j}\\in\\mathbb{N}$ for each $j$ , in each layer. Importantly, $(A_{j})^{W}=(r_{j})^{W}\\in\\mathbb{R}$ . Now consider the action of any layer of the SSM on an input sequence of the form $A_{T}=(10^{W-1})^{T}$ . The claim is that, for each $i=1,\\ldots,W$ , the sequence ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "equation", "text": "$$\nz_{t W+i}^{(k)}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "converges as $t\\rightarrow\\infty$ . As in the proof of Theorem 2, in the finite-precision setting, converge entails that the sequence becomes ultimately stationary. Note that the parity of $A_{T}$ equals the parity of $T$ ; hence, it is impossible to read out the parity from $z_{T W}^{(k)}$ when $T$ is large. ", "page_idx": 18}, {"type": "text", "text": "Now consider, suppressing the index for the dimension in $1,\\ldots,d$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{H_{n}^{(1)}}&{=\\displaystyle\\sum_{j=1}^{n}A^{n-1}B^{(n-j)}(z_{i}^{(k-1)})}\\\\ &{=\\displaystyle\\sum_{j=1}^{n}A^{n-1}B^{(n-j)}(z_{i}^{(j-1)})}\\\\ &{=\\displaystyle\\sum_{j=1}^{n}\\int_{z_{i}^{(j-1)}}^{z_{i}^{(j+1)}-1}A^{n-j}B^{(j+\\frac{1}{j})})}\\\\ &{=\\displaystyle\\sum_{j=1}^{n}\\int_{z_{i}^{(j-1)}}^{z_{i}^{(j+1)}-1}A^{n-j}B^{(j+\\frac{1}{j})})}\\\\ &{=\\displaystyle\\sum_{j=1}^{n}\\int_{z_{i}^{(j-1)}}^{z_{i}^{(j+1)}-1}(z_{i}^{(j-1)})^{(j)-1}\\beta(z_{i}^{(j-1)})}\\\\ &{=\\displaystyle\\sum_{j=1}^{n}\\int_{z_{i}^{(j+1)}}^{z_{i}^{(j+1)}-1}(z_{i}^{(j+2)})^{(j)-1-(j)}B^{(\\frac{1}{j}-1)})}\\\\ &{=\\displaystyle\\sum_{j=1}^{n}\\sum_{j=1}^{n}r_{j}^{(j+1)}\\beta(z_{i}^{(j)})^{(j+1)-1}\\beta(z_{i}^{(j+1)})}\\\\ &{=\\displaystyle\\sum_{j=1}^{n}r_{j}^{(j)}\\beta(z_{i}^{(j)})\\sum_{j=1}^{n}r_{j}^{(j-1)}(\\beta(z_{i}^{(j-1)}))\\beta(z_{i}^{(j+1)})}\\\\ &{=\\displaystyle\\sum_{j=1}^{n}\\sum_{j=0}^{n}(-2\\pi j)\\beta\\sum_{j=1}^{n}(r_{j}^{(j-1)})^{(j)-1}\\beta(z_{i}^{(j-1)}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Separately considering summation beyond $T_{0}$ at which $z_{t W+j}^{(k-1)}$ has become stationary, we get ", "page_idx": 19}, {"type": "image", "img_path": "eV5YIrJPdy/tmp/3f31e6365f285d9bbaa2994f8f0546ad8d2086e3eb3200de5a4848fa1ad8323f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "$U_{1}$ and $U_{2}$ do not depend on $t$ . Intuitively, $U_{2}\\in\\mathbb{C}$ determines a direction in the complex plane, whereas $U_{3}\\in\\mathbb{R}$ determines a magnitude. It remains to understand $U_{3}$ , which can be rewritten as: ", "page_idx": 19}, {"type": "equation", "text": "$$\nU_{3}=\\sum_{s=1}^{t}r^{(s-1)W}=r^{-W}\\sum_{s=1}^{t}(r^{W})^{s}=r^{-W}\\sum_{s=0}^{t}(r^{W})^{s}-r^{-W}=r^{-W}\\left\\{\\frac{1-(r^{W})^{t}}{1-(r^{W})}-1\\right.\\right.\\left.r\\neq1~,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We have now achieved a situation like in the proof of Theorem 2: $U_{3}$ can converge exponentially, diverge linearly, or diverge exponentially. The remainder of the proof is analogous to that proof. ", "page_idx": 19}, {"type": "text", "text": "The following Corollary of Theorem 2 will be used in the proof of Theorem 4: ", "page_idx": 19}, {"type": "text", "text": "Corollary 14. Assume NONNEGATIVE, SSMs with finite precision cannot recognize any non-starfree regular language. ", "page_idx": 19}, {"type": "text", "text": "Proof. For any non-star-free regular language $\\mathcal{L}$ , there are words $u,\\nu,w$ such that the membership $u\\nu^{n}w\\in{\\mathcal{L}}$ is determined by the value of $n$ modulo some finite integer $k$ (depending on $\\mathcal{L}$ ) [McNaughton and Papert, 1971]. Fix any such $u,\\nu,w\\in\\Sigma^{*}$ . ", "page_idx": 19}, {"type": "text", "text": "Now assume an SSM satisfying NONNEGATIVE can recognize $\\mathcal{L}$ with finite precision. We can subsume the action of $u$ into the state $h_{0}$ by taking $h_{0}$ , in each layer, to be the state of the SSM after reading $u$ . We now have an SSM that can determine the parity of $t$ when fed a word of the form $\\nu^{t}w$ . ", "page_idx": 19}, {"type": "text", "text": "For this SSM, we want to show ", "page_idx": 19}, {"type": "text", "text": "(\u2020) When fed words of the form $\\nu,\\nu^{2},\\nu^{3},\\ldots,$ , for each $i=0,\\ldots,|\\nu|-1$ , and each layer $k=1,\\ldots,L,$ the sequence $z_{t\\mid\\nu\\mid+i}^{(k)}$ converges as $t\\rightarrow\\infty,$ . ", "page_idx": 19}, {"type": "text", "text": "As in the preceding two proofs in this section, convergence entails becoming ultimately constant in the finite-precision setting. ", "page_idx": 20}, {"type": "text", "text": "The claim $(\\dagger)$ is immediate at $k=0$ . ", "page_idx": 20}, {"type": "text", "text": "Now at $k>0$ , we write: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{t|\\nu|+i}^{(k)}\\!=\\!A(z_{t|\\nu|+i}^{(k-1)})\\dots A(z_{(t-1)|\\nu|+i+1}^{(k-1)})h_{(t-1)|\\nu|+i}^{(k)}}\\\\ &{\\qquad\\qquad+A(z_{t|\\nu|+i}^{(k-1)})\\dots A(z_{(t-1)|\\nu|+i+2}^{(k-1)})B(z_{(t-1)|\\nu|+i+1}^{(k-1)})}\\\\ &{\\qquad\\qquad+A(z_{t|\\nu|+i}^{(k-1)})\\dots A(z_{(t-1)|\\nu|+i+3}^{(k-1)})B(z_{(t-1)|\\nu|+i+2}^{(k-1)})}\\\\ &{\\qquad\\qquad+\\dots}\\\\ &{\\qquad\\qquad+B(z_{(t-1)|\\nu|+i}^{(k-1)})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "On the RHS, as $t\\rightarrow0$ , all terms except for $h_{(t-1)|\\nu|+i}^{(k)}$ become constant by the inductive hypothesis. Hence, there are some $\\upalpha,\\upbeta$ such that, for sufficiently large $t$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\nh_{t|\\nu|+i}^{(k)}=\\mathsf{a}\\circ h_{(t-1)|\\nu|+i}^{(k)}+\\mathsf{\\beta}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We are now, for each $i$ , in the same situation as in the proof of Theorem 2]: each dimension of this recurrence can converge exponentially, diverge exponentially, or diverge linearly; as in that proof, it follows that zt|v|+i converges as $t\\rightarrow\\infty$ . ", "page_idx": 20}, {"type": "text", "text": "We have shown $(\\dagger)$ . ", "page_idx": 20}, {"type": "text", "text": "We now follow up by showing that ", "page_idx": 20}, {"type": "text", "text": "$(\\ast)$ When fed words of the form $\\nu^{t}w$ , for each $i=1,\\ldots,|w|$ , and each layer $k=1,\\ldots,L,$ the sequence zt|v|+i converges as t \u2192\u221e. ", "page_idx": 20}, {"type": "text", "text": "Again, at finite precision, convergence entails that the sequences are ultimately constant. Again, $(*)$ is true at $k=0$ trivially. When feeding the SSM words of the form $\\nu^{t}w$ , in each layer, the final state is in each layer $k$ , at each $i=1,\\dotsc,|w|$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{t|\\nu|+i}^{(k)}\\!=\\!A(z_{t|\\nu|+|w|}^{(k-1)})\\dots A(z_{t|\\nu|+1}^{(k-1)})h_{t|\\nu|}^{(k)}}\\\\ &{\\qquad\\qquad+A(z_{t|\\nu|+i}^{(k-1)})\\dots A(z_{t|\\nu|+2}^{(k-1)})B(z_{t|\\nu|+1}^{(k-1)})}\\\\ &{\\qquad\\qquad+\\dots}\\\\ &{\\qquad\\qquad+B(z_{t|\\nu|+i}^{(k-1)})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By inductive hypothesis, for large $t$ , there are $\\psi_{i},\\gamma_{i}$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\nh_{t|\\nu|+i}^{(k)}=\\Psi_{i}\\circ h_{t|\\nu|}^{(k)}+\\gamma_{i}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and, as shown before, each entry of ht(|kv)| converges exponentially, diverges exponentially, or diverges linearly. Now, by assumption, one can read out, at finite precisiion, the parity of $t$ from ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\boldsymbol{z}_{t|\\nu|+|w|}^{(L)}=\\mathrm{Mix}_{1}(\\mathrm{Norm}(\\mathrm{Mix}_{2}(\\Psi_{|w|}\\circ h_{t|\\nu|+i}^{(k)}+\\gamma_{|w|})))\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We now simply absorb the operation $X\\mapsto\\Psi_{|w|}\\circ X\\!+\\!\\gamma_{|w|}$ into $\\mathbf{M}\\mathbf{i}\\mathbf{x}_{2}$ , and obtain by the same arguments as in the proof of Theorem 2 that zt(|Lv)|+|w| converges as r \u2192\u221e. This is a contradiction to the claim that the value of $t$ can be read out, modulo $k$ , from $z_{t\\vert\\nu\\vert+\\vert w\\vert}^{(L)}$ at finite precision. ", "page_idx": 20}, {"type": "text", "text": "Remark 15. As outlined in our analysis, the assumptions in Theorem 2 are based on layer-wise operations that are either linear or based on the GLU or SwiGLU activation functions. This assumption is critical to the proof: one could design activation functions that make PARITY expressible. ", "page_idx": 20}, {"type": "text", "text": "iGs idveesni ga nseedq tuoe nscaeti $\\mathbf{x}=x_{1},\\dots,x_{T}$ i,o cno tnhsaitd,e fro trh eb ift-usntcrtiinogns $\\begin{array}{r}{f(\\mathbf{x})=\\frac{e^{i\\pi\\sum_{i=1}^{n}x_{i}}+1}{2}}\\end{array}$ ,( ei\u03c0\u2211i=1 xi+1. This continuous function 2if is even, and $\\mathbf{X}$ $f(\\mathbf{x})=1$ $\\scriptstyle\\sum_{i=1}^{n}x_{i}$ $f(\\mathbf{x})=0$ otherwise. At first glance, it seems like this function can be approximated by a cumulative sum layer in combination with a two-layer SSM to compute $f(x)={\\frac{e^{i\\pi x}+1}{2}}$ ", "page_idx": 21}, {"type": "text", "text": "However, this construction cannot be implemented under the condition for which we prove Theorem 2. This is because computing this function $f(x)$ inherently requires a layer-wise nonlinear operation (such as a MLP) capable of representing sine and cosine functions over arbitrarily large input values. Importantly, achieving a construction that works for any input length requires the ability to handle arbitrarily large inputs within a single operation. ", "page_idx": 21}, {"type": "text", "text": "A single GLU or SwiGLU activation function, or even a more classical MLP with ReLU or sigmoid activations, is not expected to represent sine and cosine functions over unbounded inputs. The reason for this limitation lies in the universal approximation results for feedforward networks. These results generally guarantee approximation within compact convergence on bounded sets, such as in the compactification of $\\mathbb{R}$ or in $L^{p}$ spaces, as described in Cybenko [1989], Ito [1992], and Arora et al. [2016]. None of these results extend to uniform approximation of sine or cosine over the entire real line. ", "page_idx": 21}, {"type": "text", "text": "Recent work by van Nuland [2024] addresses the universal approximation capabilities in the space $C_{b}(\\mathbb{R})$ , which is the class of bounded continuous functions over $\\mathbb{R}$ . This result is particularly relevant since approximating sine and cosine functions uniformly over $\\mathbb{R}$ would fall under this category. According to their Proposition 5.5, sine and cosine functions cannot be uniformly approximated using certain activation functions, limiting the feasibility of approximating $f(x)={\\frac{e^{i\\pi x}+1}{2}}$ ei\u03c0x2+1in a typical MLP architecture. ", "page_idx": 21}, {"type": "text", "text": "Thus, it is unrealistic to expect a typical MLP, with ReLU or sigmoid activations, to implement the function $f(x)={\\frac{e^{i\\pi x}+1}{2}}$ uniformly for arbitrarily large inputs. Consequently, a construction based on such a function would either necessitate custom activation functions, such as periodic activations specifically designed to handle sine and cosine, or require the size of the model to scale with the input length. Either solution removes apparent contradiction with Theorem 2, as these adjustments fall outside the scope of the assumptions made in our proof. ", "page_idx": 21}, {"type": "text", "text": "Wang and Xue [2024] and Orvieto et al. [2024] provide universal approximation guarantees for SSMs, but these guarantees depend on the size of the approximating network growing with input length. This dependency is clearly stated in Proposition 3.6 and Proposition 3.9 of Wang and Xue [2024], and further emphasized by Orvieto et al. [2024]. in their Remark 2. Our results, in contrast, pertain to the existence of a single SSM capable of recognizing a formal language for any input length, independent of network size. Thus, such universal approximation results do not undermine Theorem 2. ", "page_idx": 21}, {"type": "text", "text": "B.3 Proof of Theorem 4 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Our proof of Theorem 4 will rely on the algebraic theory of finite automata, specifically the cascade product and the Krohn-Rhodes Theorem [Krohn and Rhodes, 1965]. These techniques, originally developed in the 1960s, have recently been introduced to the theoretical study of transformers by Liu et al. [2023b]; we provide self-contained definitions and somewhat different notation, tailored to our proofs about state-space models. In general, we will find that the properties of state-space models allow more natural and directly length-generalizing implementations of these algebraic notions than what is possible for transfomers. ", "page_idx": 21}, {"type": "text", "text": "Recall the definition of a finite-state-automaton (Definition 7). Our construction will build on an important operation on automata, the cascade product [Krohn and Rhodes, 1965, Eilenberg, 1974, Ginzburg, 1968]: ", "page_idx": 21}, {"type": "text", "text": "Definition 16. Given two automata ${\\mathcal{A}}_{1},{\\mathcal{A}}_{2}$ with associated alphabets $\\Sigma_{1},\\Sigma_{2}$ and state sets $Q_{1},Q_{2}$ such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Sigma_{2}=Q_{1}\\times\\Sigma_{1},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "the cascade product $A_{2}\\,\\wr A_{1}$ is the automaton given by ", "page_idx": 21}, {"type": "text", "text": "\u2022 $\\Sigma=\\Sigma_{1}$ ", "page_idx": 21}, {"type": "text", "text": "\u2022 $Q=Q_{2}\\times Q_{1}$ \u2022 q0 is the tuple of the starting states of ${\\mathcal{A}}_{2},{\\mathcal{A}}_{1}$ $\\bullet\\,\\,u(\\langle q,p\\rangle,\\mathbf{\\sigma})=\\langle u_{2}\\,(q,\\langle p,\\mathbf{\\sigma})\\rangle\\,,u_{1}(p,\\mathbf{\\sigma})\\rangle$ ", "page_idx": 22}, {"type": "text", "text": "We note that the literature usually uses \u201c $\\ddots$ for the cascade product [e.g. Eilenberg, 1974]. To avoid collision with the elementwise product \u201c\u25e6\u201d (e.g., (1)), we here instead use $^{\\bullet\\bullet}\\mathcal{l}^{\\bullet}$ , usually used for the wreath product \u2013 a product on monoids with an effect analogous to the cascade product [Almeida, 1995]. ", "page_idx": 22}, {"type": "text", "text": "While the formal definition is cumbersome, the intuition behind it is simple: The cascade product corresponds to first reading a word w with $\\mathcal{A}_{1}$ , recording the state sequence $q_{0},q_{1},\\dotsc,q_{|\\mathbf{w}|}\\in Q_{1}$ and \u2013 at each $t=1,\\dots,|\\mathbf{w}|$ \u2013 pasting the state $q_{t-1}$ together with the input symbol $w_{t}\\in\\Sigma_{1}$ \u2013 resulting in a word over a new alphabet $Q_{1}\\times\\Sigma_{1}$ , and then running $A_{2}$ on the resulting word. The overall state of $A_{2}\\wr A_{1}$ after reading a word is the tuple of the states reached by $A_{2}$ and $A_{1}$ . Note that we write $A_{2}\\wr A_{1}$ , rather than, $A_{1}\\,\\wr A_{2}$ , because the second argument of the cascade product $\\left(A_{1}\\right)$ intuitively reads the input first, preprocessing it for the other automaton, $A_{2}$ \u2013 the cascade product can thus be viewed as a kind of function composition. ", "page_idx": 22}, {"type": "text", "text": "The somewhat inscrutable update rule for $u(\\cdot,\\cdot)$ encodes the action of $\\mathcal{A}_{1}$ in the second component, and the action of $\\mathcal{A}_{2}$ on the extended alphabet in the first component. There is a close analogy to the stacking of sequence models, and we will leverage this analogy to translate cascade products into multilayer SSMs. The fundamental background here is the following classical fact: ", "page_idx": 22}, {"type": "text", "text": "Fact 17 (Consequence of Krohn-Rhodes Theorem [Krohn and Rhodes, 1965] and Sch\u00fctzenberger\u2019s Theorem [Sch\u00fctzenberger, 1965]). Each star-free regular language is recognized by an iterated cascade product of set-reset automata, $\\left(\\ldots\\left({\\mathcal{A}}_{1}\\,!\\ldots\\right)\\wr{\\mathcal{A}}_{n-1}\\right)\\wr{\\mathcal{A}}_{n},$ , where each Ai is a set-reset automaton. ", "page_idx": 22}, {"type": "text", "text": "This result follows from the Krohn-Rhodes decomposition theorem [Krohn and Rhodes, 1965], which states that any finite-state automaton can be expressed as an iterated cascade product of simple automata, specifically finite simple groups and reset automata. Moreover, Sch\u00fctzenberger\u2019s Theorem [Sch\u00fctzenberger, 1965] characterizes star-free regular languages as those whose syntactic monoids are aperiodic, meaning they contain no nontrivial groups. Therefore, the decomposition for star-free languages involves only set-reset automata, leading to the stated cascade product structure. We now formally show that cascade products can be translated to SSM stacking. We need an auxiliary lemma, which provides a single-layer SSM that encodes the input $w_{t-1}$ in state $h_{t}$ \u2013 we will use it to forward information about the state of $\\mathcal{A}_{\\mathrm{l}}$ at $t-1$ to $\\mathcal{A}_{2}$ at $t$ : ", "page_idx": 22}, {"type": "text", "text": "Lemma 18. Let $\\Sigma$ be an alphabet, and consider words $w\\in\\Sigma^{*}$ . There is a one-layer SSM with $d=4|\\Sigma|$ such that, for $t=2,\\dots,|w|$ , the character $w_{t-1}$ can be read out from $z_{t}$ at finite precision. ", "page_idx": 22}, {"type": "text", "text": "To prove Lemma 18, a first idea is to use an exponential moving average with $A=1/2$ to encode the recent input characters in $h_{t}$ ; this effectively encodes the full history into the binary expansion of $h_{t}$ , and in particular allows reading out the second-last input in principle. However, such a construction does not work at finite precision, because rounding may make it impossible to extract even the second-most-significant bit.6 We avoid this problem simply by taking $\\bar{A}=1/4$ , effectively utilizing only every two digits in the binary expansion of $h_{t}$ , ensuring that the second-last input can be read out at a constant margin. We now provide the formal proof: ", "page_idx": 22}, {"type": "text", "text": "Proof. We begin by showing the claim in the special case $\\Sigma=\\{1,0\\}$ . Here, we take $d=4$ , and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{h_{0}=[0,0,0,0]^{T}}}\\\\ {{A\\big(e_{0}\\big)=[1/4,1/4,0,0]^{T}}}\\\\ {{A\\big(e_{1}\\big)=[1/4,1/4,0,0\\big]^{T}}}\\\\ {{B\\big(e_{0}\\big)=[1,0,1,0]^{T}}}\\\\ {{B\\big(e_{1}\\big)=[0,1,0,1\\big]^{T}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now we separately consider the state $h_{t}$ depending on the form of the prefix $\\nu_{1,..,t}$ (here $\\nu_{1,..,t}$ refers to first $t$ characters in the word). If $w_{1,\\dots t}=\\dots00$ (the last 2 characters of the prefix are 00), then ", "page_idx": 23}, {"type": "equation", "text": "$$\nh_{t}=\\left(\\in[0,1/8]\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "because ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{t}=\\!A(e_{0})\\circ h_{t-1}+\\!B(e_{0})}\\\\ &{\\begin{array}{c}{=A(e_{0})\\circ A(e_{0})\\circ h_{t-2}+A(e_{0})\\circ B(e_{0})+B(e_{0})}\\\\ {=\\![1/16,1/16,0,0]^{T}\\circ h_{t-2}+[1/16,1/16,0,0]^{T}\\circ[1,0,1,0]^{T}\\!+\\![1,0,1,0]^{T}}\\end{array}}\\\\ &{\\begin{array}{c}{=\\left(\\!\\!\\begin{array}{c c}{\\frac{1}{16}(h_{t-2})_{1}+\\frac{1}{16}+1}\\\\ {\\frac{1}{16}(h_{t-2})_{2}}\\\\ {0}\\end{array}\\!\\!\\right)}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By definition of $A$ and $B$ , each entry in $h_{t-2}$ is in [0,2]; the claim (14) then follows. If $w_{1,..,t}=...10$ , then, by a similar calculation ", "page_idx": 23}, {"type": "equation", "text": "$$\nh_{t}=\\left(\\stackrel{\\in[1,1.25]}{\\in[1/4,1/2]}\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In particular, assuming $w_{t}=0$ , one can read off $w_{t-1}$ from $(h_{t})_{2}$ with a margin of size $1/8$ . As $w_{t}$ is encoded in $h_{t}$ and due to symmetry, analogous statements hold when $w_{t}=1$ . ", "page_idx": 23}, {"type": "text", "text": "Now, for each $\\upsigma\\in\\Sigma$ , we run such a one-layer SSM where 0 represents $\\upsigma$ and 1 represents all other characters.7 By running these in parallel (i.e. executing these operations with the same SSM layer simultaneously, utilising the width of the SSM layer) we obtain an SSM with $d=4|\\Sigma|$ from whose states one \u221acan read out $w_{t-1}$ at finite precision. As the entries in $h_{t}$ are all bounded by 2, we find $\\lVert h_{t}\\rVert_{2}\\leq2\\sqrt{d}$ independent of $t$ , and the margin is still bounded away from zero after normalization, and thus in $z_{t}$ , where we can assume $\\mathbf{M}\\mathrm{i}\\mathbf{x}_{1}$ , $\\mathbf{M}\\mathrm{i}\\mathbf{x}_{2}$ to be the identity. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Remark 19. Some SSMs include local convolutions [e.g. Fu et al., 2023, Gu and Dao, 2023] or local attention [De et al., 2024], which aggregate information from a local window of some width $\\Delta>0$ . These do not increase the expressive capacity beyond SSMs as we have defined in (1-2), as aggregation of local information can be simulated with a single SSM layer: Using the layer constructed in the proof of Lemma 18, given the state $h_{t}$ , once one has read out $w_{t-1}$ as described in the proof, one can recover $h_{t-1}$ from $h_{t}$ and $x_{t}$ ; then inductively read out $w_{t-2}$ using $h_{t-1}$ and $x_{t-1}$ , etc. Thus, up to any given width $\\Delta>0$ , one can read out $w_{t-\\Delta},\\dots,w_{t-1}$ from the state $h_{t}$ of this layer at finite precision. ", "page_idx": 23}, {"type": "text", "text": "We are now ready to translate cascade products into SSM stacking: ", "page_idx": 23}, {"type": "text", "text": "Lemma 20. Let $\\mathcal{A}_{\\mathrm{l}}$ , $\\mathcal{A}_{2}$ be two finite-state-automata, and assume that there are two SSMs with top-level states $z^{(L_{1},1)}$ and $\\ z^{(L_{2},2)}$ that map each w to the state sequences under $\\mathcal{A}_{1},\\;\\mathcal{A}_{2}$ , at finite precision. ", "page_idx": 23}, {"type": "text", "text": "Formally, on a word w, $\\ p_{1}\\big(z_{t}^{(L_{1},1)}\\big)$ and $\\ p_{2}(z_{t}^{(L_{2},2)})$ provide the state sequences of $\\mathcal{A}_{1},\\,\\mathcal{A}_{2}$ . ", "page_idx": 23}, {"type": "text", "text": "Then there is an SSM with $L_{1}+L_{2}+1$ layers that maps each w to the state sequence under $\\mathcal{A}_{2}\\,\\wr\\mathcal{A}_{2}$ , again at finite precision. ", "page_idx": 23}, {"type": "text", "text": "We note that a conceptually related result holds for transformers [Lemma 12 in Liu et al., 2023b]. However, SSMs allow a simpler and length-independent construction, as they do not require positional encodings to implement such a construction. ", "page_idx": 23}, {"type": "text", "text": "Proof. The lower layers are based on the SSM modeling $\\mathcal{A}_{1}$ . We duplicate each channel, so we now have $2d$ dimensions. We further add $d$ further dimensions that directly pass on the input embeddings, i.e., $A\\equiv0$ , $B\\equiv1$ , $\\mathrm{Mix}_{j}\\equiv I d$ on these dimensions. ", "page_idx": 24}, {"type": "text", "text": "In the resulting SSM, $z_{t}^{L_{1}}$ indicates both $w_{t}$ itself, and the state reached by $\\mathcal{A}_{\\mathrm{l}}$ after reading $w_{1,..,t}$ . The state is redundantly indicated by two separate sets of $d$ dimensions; the character $w_{t}$ is indicated by $d$ further state. ", "page_idx": 24}, {"type": "text", "text": "Note, however, that the second automaton in the cascade product requires access to the state $q_{t-1}$ rather than qt. ", "page_idx": 24}, {"type": "text", "text": "For this, we add a layer provided by Lemma 18, of width $4|Q|$ . Additional $2d$ dimensions pass on (1) $w_{t}$ , and (2) the state that $\\mathcal{A}_{\\mathrm{l}}$ reaches after reading the prefix $w_{1,..,t}$ . ", "page_idx": 24}, {"type": "text", "text": "We now have $L_{1}+1$ layers where $z_{t}^{L_{1}+1}$ has $2d+4|Q|$ dimensions and indicates $(1)\\,w_{t}$ , (2) the state that $\\mathcal{A}_{1}$ reaches after reading the prefix $\\nu_{1\\ldots t}$ , (3) the state that $\\mathcal{A}_{1}$ reaches after reading the prefix $w_{1}...t\\mathrm{-}1$ . ", "page_idx": 24}, {"type": "text", "text": "The first and third piece of information are now fed into the second SSM; the second piece is passed on in $d$ additional dimensions. As we allowed $A$ and $B$ to be arbitrary functions, we redefine these in the lowest layer of that second SSM to read out from the $4|Q|$ -dimensional component indicating (3), providing the desired second-to-last state. ", "page_idx": 24}, {"type": "text", "text": "We have constructed an SSM with $L_{1}+L_{2}+1$ layers, where ztL1+L2+1indicates (1) wt, (2) the state that $\\mathcal{A}_{1}$ reaches after reading the prefix $\\nu_{1,..,t}$ , (3) the state that $\\mathcal{A}_{2}$ reaches after reading the prefix $\\nu_{1\\ldots t}$ pasted with the state sequence of $\\mathcal{A}_{1}$ . This information is sufficient for reading out the state sequence of $\\mathcal{A}_{2}\\,\\wr\\mathcal{A}_{1}$ . ", "page_idx": 24}, {"type": "text", "text": "Note that the number of channels may not be consistent, as it is $3d$ in the top and bottom parts, but $2d+4|Q|$ in the middle; we simply pad to the larger dimensionality. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "We are now ready to show the existence of length-generalizing SSMs for any star-free state tracking problem, and conclude with the theorem: ", "page_idx": 24}, {"type": "text", "text": "Theorem 21 (Restated from Theorem 4). Let L be a regular language. The following are equivalent: ", "page_idx": 24}, {"type": "text", "text": "1. There is an SSM satisfying NONNEGATIVE that predictively models L at all input lengths, at finite precision   \n2. L is star-free. ", "page_idx": 24}, {"type": "text", "text": "Proof. We need to show: ", "page_idx": 24}, {"type": "text", "text": "1. SSMs at finite precision can predictively model all star-free languages. For each language, a single SSMs is applicable at arbitrary lengths.   \n2. Assuming NONNEGATIVE, finite-precision SSMs cannot recognize any non-star-free regular language. ", "page_idx": 24}, {"type": "text", "text": "The second statement is Corollary 14; it suffices to prove the first statement. ", "page_idx": 24}, {"type": "text", "text": "Assume $\\mathcal{L}$ is star-free. By the Krohn-Rhodes theorem, there is an automaton $\\mathcal{A}$ that is a cascade product of some set-reset automata that recognizes $\\mathcal{L}$ . By Lemmas 9 and 20, there is an SSM that computes the state sequence of that automaton. ", "page_idx": 24}, {"type": "text", "text": "Now we note that, since $\\mathcal{A}$ recognizes $\\mathcal{L}$ , the state $q$ after reading w is sufficient for determining the set of characters that can follow this prefix in any element of $\\mathcal{L}$ . For, assume otherwise, then there are words w, $\\ensuremath{\\mathbf{w}}^{\\prime}$ such that $u\\mathopen{}\\mathclose\\bgroup\\left(q_{0},\\mathbf{w}\\aftergroup\\egroup\\right)=u\\mathopen{}\\mathclose\\bgroup\\left(q_{0},\\mathbf{w}^{\\prime}\\aftergroup\\egroup\\right)$ and $\\upsigma\\in\\Sigma$ such that $\\mathbf{w}\\pmb{\\Sigma}^{\\ast}\\cap\\mathcal{L}\\neq\\emptyset$ but $\\mathbf{w}^{\\prime}\\mathbf{\\sigma}\\overline{{\\sigma}}\\Sigma^{*}\\cap\\mathcal{L}=$ $\\varnothing$ ; then $u(q_{0},\\mathbf{w}\\mathbf{\\sigma})=u(q_{0},\\grave{\\mathbf{w}^{\\prime}}\\mathbf{\\sigma})$ but the set $R$ (4) is reachable from $u(q_{0},\\mathbf{w}\\mathbf{\\sigma})$ but not $u(q_{0},\\mathbf{w}^{\\prime}\\upsigma)$ , contradiction. ", "page_idx": 24}, {"type": "text", "text": "Hence, the SSM\u2019s outputs can be transformed, by composing $\\uprho$ with a map from states to nextcharacter sets, to predictively model $\\mathcal{L}$ . \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Theorem 22. SSMs with complex-valued coefficients evading both NONNEGATIVE and TIMEINVARIANT can represent all regular languages known to be in TC0. ", "page_idx": 25}, {"type": "text", "text": "We we do not use this theorem in the main paper, due to the nonexistence (as far as we know) of implemented SSMs with this property. ", "page_idx": 25}, {"type": "text", "text": "Proof. SSMs evading both NONNEGATIVE and TIME-INVARIANT can count modulo any integer $k$ , using $d=1$ and $A(e_{1})=e^{2\\pi i/k},A(e_{0})=1,I$ $B\\equiv0$ , $h_{0}=1$ . This is a generalization of the construction for PARITY described in Section B.2, since $e^{2\\pi i/2}=-1$ . ", "page_idx": 25}, {"type": "text", "text": "The set of regular languages known to be in $\\mathbf{T}\\mathbf{C}^{0}$ is the set of regular languages whose syntactic monoid contains no non-solvable groups [Barrington et al., 1992]. These languages are recognized by cascade products of set-reset automata and automata perfoming modular counting [Straubing, 1994]. By the remark above, together with Lemma 9 and Lemma 20, such cascade products can be simulated by SSMs. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "B.4 Maintaining Counters ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "As the first step in showing Theorem 5, we show that SSMs can maintain unbounded counters, and that one can read out the values of such counters, up to finite bounds, even at finite precision: ", "page_idx": 25}, {"type": "text", "text": "Lemma 23. Let $C>0$ be an integer. Let any function $u:\\Sigma\\to\\mathbb{Z}^{C}$ be given. Let $L\\in\\mathbb N$ . Then $a$ one-layer SSM with finite precision can compute, at each position $i=1,\\ldots,T$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left(\\operatorname*{min}\\left(\\sum_{j=1}^{i}u(w_{i}),L\\right),-L\\right)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "in the sense that $\\uprho$ can read this out from $z_{i}^{(1)}$ with finite precision. ", "page_idx": 25}, {"type": "text", "text": "Proof. Define $d=2L+1$ . Define $h_{0}=\\mathbf{0}\\in\\mathbb{R}^{d}$ . For each $x\\in\\Sigma$ , define $A(e_{x})=\\mathbf{1}\\in\\mathbb{R}^{d}$ and $B(e_{x})_{i}\\in$ $\\mathbb{R}^{d}$ by $B(e_{x})_{i}=u(x)$ . In order to read out the state $h_{t}$ up to a limit $L$ , we define ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Phi(h_{t},x_{t})=\\mathrm{Norm}(h_{t}+[0,1,-1,2,-2,...\\,.\\,.\\,-L,L])\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By testing which entries of the result are negative or positive, one can read out the state up to $L$ even after rounding $\\Phi(h_{t},x_{t})$ to finite precision. The proof straightforwardly extends to multiple counters. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "We are ready to prove the theorem: ", "page_idx": 25}, {"type": "text", "text": "Theorem 24 (Restated from Theorem 5). The languages Dyck-1, Shuffle-Dyck, n-ary Boolean Expressions, $a^{n}b^{n}$ , $a^{n}b^{n}c^{n}$ , and $a^{n}b^{n}c^{n}d^{n}$ , (defined in Appendix C) can each be predictively modeled by an SSM. ", "page_idx": 25}, {"type": "text", "text": "For $a^{n}b^{n}$ : (here, $C{=}1$ ) $\\begin{array}{l}{u(a)=1}\\\\ {u(b)=-1}\\end{array}$ For Dyck-1: (here, ${\\cal C}{=}1$ ) $\\begin{array}{l}{{u({^{\\ast}}({^{\\ast}}))=1}}\\\\ {{u({^{\\ast}})^{,\\ast})=-1}}\\end{array}$ For Shuffle-Dyck- $.k$ (here, $C=k$ ) $\\begin{array}{l}{{u({}^{\\ast}(i{}^{,\\ast})=(0,\\dots,0,1,0\\dots0)}}\\\\ {{u({}^{\\ast})_{i}{}^{,\\ast})=(0,\\dots,0,-1,0\\dots0)}}\\end{array}$ where 1 is in the $i^{\\th}$ -th slot where $^{-1}$ is in the $i^{\\th}$ -th slot For $a^{n}b^{n}c^{n}$ : (here, $C{=}2)$ ) $\\begin{array}{l}{{u(a)=(1,0)}}\\\\ {{u(b)=(-1,1)}}\\\\ {{u(c)=(0,-1)}}\\end{array}$ For $a^{n}b^{n}c^{n}d^{n}$ : (here, $C{=}3)$ $\\begin{array}{l}{{u(a)=(1,0,0)}}\\\\ {{u(b)=(-1,1,0)}}\\\\ {{u(c)=(0,-1,1)}}\\\\ {{u(d)=(0,-1,-1)}}\\end{array}$ For Boolean Expressions: (here, $C{=}1$ ) $\\begin{array}{r l}{u(\\langle V A L U E\\rangle)=-1}&{{}}\\\\ {u(\\langle n-A R Y\\rangle)=+n}&{{}}\\end{array}$ ", "page_idx": 26}, {"type": "text", "text": "For each of these mappings, we use Lemma 23 at $L=1$ to construct a one-layer SSMs that can, for each of the $C$ counters, distinguish the values $\\leq-1,0,\\geq1$ . ", "page_idx": 26}, {"type": "text", "text": "In parallel, we pass on the input symbol itself in $\\log\\left|\\Sigma\\right|$ further dimensions. ", "page_idx": 26}, {"type": "text", "text": "Overall, the output $z_{t}$ of single SSM layer provides, at every position, both the original symbol in $\\Sigma$ and an element of $\\{\\le-1,0\\ge1\\}^{c}$ . ", "page_idx": 26}, {"type": "text", "text": "We can thus view the output of this layer as a string over an enriched string of symbols ${\\upsigma}_{1}\\times{\\upsigma}_{2}\\in$ $\\Sigma\\times\\{\\leq-1,0\\geq1\\}^{C}$ . Based on this, one can predictively model these languages as follows. ", "page_idx": 26}, {"type": "text", "text": "For Dyck-1, the next token is EOS or \u201c(\u201d if $\\upsigma_{2}=0$ , and \u201c(\u201d or \u201c)\u201d after any other prefix (note that predictive modeling assumes valid prefixes). ", "page_idx": 26}, {"type": "text", "text": "Shuffle- $.k$ -Dyck is similar: EOS is allowed if and only if all counters are zero. An opening bracket is always allowed. A closing bracket is only allowed if the respective counter is $>0$ . ", "page_idx": 26}, {"type": "text", "text": "For $a^{n}b^{n}$ , the next token is $a$ or $^b$ if $\\upsigma_{1}=a$ ; $^b$ if $\\mathbf{\\sigma}\\sigma=(a,\\geq1)$ or $\\left(b,\\geq1\\right)$ ; EOS if $\\mathbf{\\boldsymbol{\\sigma}}=(b,0)$ . ", "page_idx": 26}, {"type": "text", "text": "Constructions for $a^{n}b^{n}c^{n}$ , $a^{n}b^{n}c^{n}d^{n}$ are similar. ", "page_idx": 26}, {"type": "text", "text": "For Boolean expressions, the next token is $\\langle n-A R Y\\rangle$ or EOS if $\\upsigma_{2}=0$ , and any other token otherwise. ", "page_idx": 26}, {"type": "text", "text": "All of these constructions can be encoded using an appropriate function $\\uprho$ applying to $z_{t}$ . ", "page_idx": 26}, {"type": "text", "text": "B.5 Bounded-Depth Dyck ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Definition 25. The language $D y c k_{K,h}$ [Hewitt et al., 2020, Yao et al., 2021b] is given by the CFG with the nonterminals $\\{S_{0},S_{1},\\ldots,S_{h-1},S_{h}\\}$ and the following production rules: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S_{h}\\rightarrow\\left(1S_{h-1}\\right)\\phantom{|}1\\ldots|\\left(\\phantom{|}K S_{h-1}\\right)\\phantom{|}\\phantom{|}\\varepsilon}\\\\ {S_{h-1}\\rightarrow\\left(\\phantom{|}S_{h-2}\\right)\\phantom{|}1\\ldots|\\left(\\phantom{|}K S_{h-2}\\right)\\phantom{|}\\varepsilon|\\phantom{|}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{S_{2}\\rightarrow(_{1}S_{1})_{1}|\\dots|(_{K}S_{1})_{K}|\\mathfrak{E}}\\\\ {S_{1}\\rightarrow(_{1}S_{0})_{1}|\\dots|(_{K}S_{0})_{K}|\\mathfrak{E}}\\\\ {S_{0}\\rightarrow\\mathfrak{E}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and the start symbol $S_{h}$ . ", "page_idx": 27}, {"type": "text", "text": "Theorem 26 (Restated from Theorem 6). There is a two-layer SSM with $d=O(h\\log K)$ that predictively models $D y c k_{K,h}$ at all input lengths, at finite precision. ", "page_idx": 27}, {"type": "text", "text": "Proof. In the first layer, we calculate each token depth up to $h$ using Lemma 23. After the first layer, at each position, the activations will indicate both the depth up to $h$ , and the identity of the symbol. The space of activations is thus $\\{0,\\ldots,h\\}\\times\\left\\{(1,)1,\\ldots,\\bar{(\\kappa,)}\\kappa\\right\\}$ . We then, for each depth $l=1,\\ldots,h$ , define a set-reset automaton (Definition 8) given by the set $\\bar{Q_{l}}:=\\{l\\}\\times\\{(\\boldsymbol{\\mathbf{\\mathit{\\sigma}}}_{1})\\boldsymbol{\\mathbf{\\mathit{1}}},\\boldsymbol{\\mathbf{\\check{\\ldots}}},\\bar{(\\boldsymbol{\\mathbf{\\mathit{K}}},)}\\boldsymbol{\\mathbf{\\mathit{K}}}\\}$ . Running all of these set-reset automata will tell us, for each depth, the identity of the last bracket at that depth. We can deduce the maximum depth $h^{\\prime}$ at which the last bracket is an opening one, and thus infer the set of valid next symbols. The activity of these set-reset automata can, in parallel, be simulated by a second SSM layer using Lemma 9. We need $h$ such automata, and each SSM has width $\\log K$ \uff0e\u53e3 ", "page_idx": 27}, {"type": "text", "text": "C Definitions of Languages ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Here, we provide formal definitions of languages from the test suite based on Bhattamishra et al.   \n[2020]. Descriptions follow Bhattamishra et al. [2020], and are included here for self-containment.   \nIn all cases, our data generation setup is directly taken from [Bhattamishra et al., 2020]. ", "page_idx": 27}, {"type": "text", "text": "C.1 Regular Languages ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Tomita Grammars. Used primarily as a benchmark language family for assessing sequence to sequence models [Tomita, 1982], some of the languages in this family are star-free (with dot-depth of 1) and some non-star-free. All the regular languages of the family are defined on the alphabet $\\Sigma=\\{0,1\\}$ . Individual language definitions are available in Table 1. ", "page_idx": 27}, {"type": "text", "text": "$D_{n}$ . We follow the definition of Bhattamishra et al. [2020] to define the $D_{n}$ family of star-free languages. In our experiments, we only generate $D_{2},D_{3},D_{4}$ , and $D_{12}$ languages; $D_{1}$ is equivalent to Tomita-2. All the languages of the family are defined on the alphabet of $\\Sigma=\\{a,b\\}$ . $D_{n}=\\bar{(}a D_{n-1}b)^{*}$ has level $n$ in the dot-depth hierarchy. ", "page_idx": 27}, {"type": "text", "text": "PARITY. PARITY is the set of all strings on the alphabet $\\Sigma=\\{0,1\\}$ such that the number of 1\u2019s is even. This language can be easily recognized by a DFA with just two states. ", "page_idx": 27}, {"type": "text", "text": "Others. We further have the non-star-free languages $(a a)^{*}$ , $(a a a a)^{*}$ and $(a b a b)^{*}$ , and the star-free languages $a a^{*}b b^{*}c c^{*}d d^{*}e e^{*}$ , $\\{a b\\}^{*}d\\{b,c\\}^{*}$ , and $\\{0,1,2\\}^{*}02^{*}$ . ", "page_idx": 27}, {"type": "text", "text": "C.2 Counter Languages ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Dyck and Shuffle-Dyck. Dyck-1 is defined on the alphabet $\\Sigma=\\{[,]\\}$ and derived using the following CFG production rule: $S\\dot{\\rightarrow}(S)|S S|\\varepsilon$ . ", "page_idx": 27}, {"type": "text", "text": "We further use the family of Shuffle- $\\cdot\\mathbf{k}$ languages [Suzgun et al., 2019]. Shuffle-Dyck- $\\cdot\\mathbf{k}$ is defined in terms of $\\Sigma=\\{(\\boldsymbol{1},)\\boldsymbol{1},\\cdots,(\\boldsymbol{k},)\\boldsymbol{k}\\}$ . It is defined as the shuffle of $k$ Dyck-1 languages, each defined in terms of the alphabet $\\Sigma_{i}=\\{(i,)_{i}\\}$ where $i=1,\\ldots,k$ . ", "page_idx": 27}, {"type": "text", "text": "$\\pmb{n}$ -ary Boolean Expressions. This is the set of valid expressions over various operators. We focus on up-to-3-ary expressions, defined using the following grammar: ", "page_idx": 27}, {"type": "text", "text": "$S\\rightarrow\\langle\\mathrm{VALUE}\\rangle$$S\\rightarrow$ \u27e8UNARY OPERATOR\u27e9S$S\\rightarrow$ \u27e8BINARY OPERATOR\u27e9S S$S\\rightarrow$ \u27e8TERNARY OPERATOR\u27e9S S S", "page_idx": 28}, {"type": "text", "text": "This language is recognized by a counter automaton [Fischer et al., 1968a]. ", "page_idx": 28}, {"type": "table", "img_path": "eV5YIrJPdy/tmp/4d0438c17988b048e9d316bb5f92fe04c774542b4fbdec83f7ede98f69b6282a.jpg", "table_caption": ["Others We further include the languages of the forms $a^{n}b^{n}$ , $a^{n}b^{n}c^{n}$ , and $a^{n}b^{n}c^{n}d^{n}$ . "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "D Experimental Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "All experiments used the Mamba reference implementation8. xUnless stated otherwise, we followed the defaults given there ( $d_{s t a t e}=16$ , $d_{c o n\\nu}=4$ , $e x p a n d=2\\,\\$ ), as we found the default combination to work better than other options. We tuned $d_{m o d e l}$ for each language. ", "page_idx": 28}, {"type": "text", "text": "D.1 Test Suite from Bhattamishra et al. [2020] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Data Preparation For all the languages, we use either the data prepared by Bhattamishra et al. [2020] or\u2014where not available\u2014their data-generation scripts, allowing full comparability with results they reported for transformers. We used their official code and data release at https://github.com/satwik77/Transformer-Formal-Languages (last commit 48eea2e; MIT license). Training sets typically consist of 10K samples, with lengths varying between 1 to 50. There are two heldout bins: one with in-distribution lengths ([1,50]), and one testing length generalization (lengths [51,100]). The first one was used for hyperparameter optimization. Each bin typically contains around 2K samples. However for languages such as $a^{n}b^{n}$ , where the number of positive examples in each bin was limited, all possible examples for that bin are included. ", "page_idx": 28}, {"type": "text", "text": "Hyperparameters For each language, we conducted extensive hyperparameter search. We varied the $d_{m o d e l}$ parameter in Mamba across the set {16, 32, 64, 128, 256}. Additionally, we experimented with the number of layers in our model, ranging from 1 to 3, training each configuration for 100 epochs. For languages where Mamba performed well, this number of layers was sufficient. However, for languages where Mamba struggled, we increased the number of layers up to 12, with little to no success. ", "page_idx": 28}, {"type": "text", "text": "We used the AdamW optimizer. To identify optimal learning rates, we started with a coarse hyperparameter search using values from the set $\\{0.001,0.0001,0.00001\\}$ . If one of these learning rates showed high performance, we conducted a more fine-grained search to find the optimal learning rate. Finally, we varied the batch size from {16, 32, 64} for datasets with 10K training examples. For languages like $a^{n}b^{n}$ with limited training size, we searched for an optimal batch size within the set {5, 10}. ", "page_idx": 28}, {"type": "text", "text": "D.2 FlipFlop ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We obtained the dataset of Liu et al. [2023a] from their release, https://huggingface.co/ datasets/synthseq/flipflop (MIT license). Our setup corresponds to the deterministic (\u201cclean\u201d) mode in Liu et al. [2023a]. Matching Figure 2 in Liu et al. [2023a], we evaluated both with ", "page_idx": 28}, {"type": "table", "img_path": "eV5YIrJPdy/tmp/43369d6062bf478e5760ab1d3e22f7bcaa533e5e26be2599e767b18bc8c3e81f.jpg", "table_caption": [], "table_footnote": ["Table 2: Accuracies on the counter Languages from the Bhattamishra et al. [2020] test suite. Transformer results reported based on Bhattamishra et al. [2020]. For Mamba, we report best settings (chosen based on inputs of length [1,50]) at 1 (Mamba1), 2 (Mamba2), 3 (Mamba3) layers. Results for the best-performing layer count, from the first two bins, are shown in Figure 5. On these languages, there is also a third bin. "], "page_idx": 29}, {"type": "text", "text": "in-distribution data (matching the distribution of the training dataset) with $p_{i}=0.8,p_{w}=0.1,p_{r}=$ 0.1, and using an out of distribution sparse tail with $p_{i}=0.98,p_{w}=0.01,p_{r}=0.01$ , where $p_{i},p_{w},p_{r}$ refer to the probabilities of that instruction appearing in input sequences. ", "page_idx": 29}, {"type": "text", "text": "We trained a one-layer Mamba with the default parameters9, setting $d_{m o d e l}$ to 16 with the AdamW optimizer using a learning rate of $3x10^{-4}$ and a batch size of 16. ", "page_idx": 29}, {"type": "text", "text": "Following the evaluation criteria for LSTMs in Liu et al. [2023a], we compute the test every 100 training steps on our validation sets of choice, by randomly sampling around $\\mathrm{\\dot{10}^{3}}$ samples from each set in every evaluation cycle. ", "page_idx": 29}, {"type": "table", "img_path": "eV5YIrJPdy/tmp/48002f531438a543e46d4a83742e6b4f1148c573be0970e79280baef8b5b7f6f.jpg", "table_caption": [], "table_footnote": ["Table 3: Accuracies on the regular Languages from the Bhattamishra et al. [2020] test suite - 1st half. Transformer results reported based on Bhattamishra et al. [2020]. For Mamba, we report best settings (chosen based on inputs of length [1,50]) at 1 (Mamba1), 2 (Mamba2), 3 (Mamba3) layers. Results for the best-performing layer count are also shown in Figure 5. "], "page_idx": 30}, {"type": "text", "text": "D.3 Bounded Hierarchical Structure ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We built on the official code and data release of Yao et al. [2021b] at https://github.com/ princeton-nlp/dyck-transformer (last commit: 5d21fcf). We train a 2-layer Mamba and a 1-layer Mamba on $D y c k_{K,h}$ with $K=8$ and $h=10$ . The training set and the validation set contains samples of lengths $\\le700$ , while the test set contains samples of lengths $700\\leq n\\leq1400$ . We train Mamba with a varying number of layers $l\\in\\{1,2\\}$ and $d_{m o d e l}\\in\\{20,30,40,50,60,70,80,90,100\\}$ . ", "page_idx": 30}, {"type": "table", "img_path": "eV5YIrJPdy/tmp/0a70d9190ac595852ca9a8c020496124b57443a4f6c690b30210df4441dd64b8.jpg", "table_caption": [], "table_footnote": ["Table 4: Accuracies on the regular Languages from the Bhattamishra et al. [2020] test suite - continued. Transformer results reported based on Bhattamishra et al. [2020]. For Mamba, we report best settings (chosen based on inputs of length [1,50]) at 1 (Mamba1), 2 (Mamba2), 3 (Mamba3) layers. Results for the best-performing layer count are also shown in Figure 5. "], "page_idx": 31}, {"type": "text", "text": "We use the Adam optimizer with an initial learning rate of 0.01 or 0.001, using cross-entropy loss. After training for 100 epochs (with early stopping allowed in case of convergence), we select the learning rate with the better training performance. ", "page_idx": 31}, {"type": "text", "text": "E Finite Precision Assumption ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "As described in Section 2.1, we adopt the finite precision notion used by Weiss et al. [2018]: We allow an unbounded number of integer bits, but only $p$ fractional bits, where $p$ is a sufficiently large constant (e.g., $p=8$ ), independent of the length of the input. ", "page_idx": 31}, {"type": "text", "text": "There are a variety of related precision notions in the theoretical literature on neural sequence models \u2013 here, we discuss the effect of other notions on our results: ", "page_idx": 31}, {"type": "text", "text": "Infinite precision Infinite precision allows any parameter and intermediate value to be an arbitrary number. Such a setting is unrealistic, as it would allow encoding arbitrary detail about the input into infinite precision [e.g. Siegelmann, 1999] and read these out with sufficiently powerful functions $(A,\\,B,\\,\\,\\Phi)$ in (4) \u2013 this would lead to the unrealistic conclusion that any function and language could be represented. For this reason, theoretical work has often adopted restricted precision notions. ", "page_idx": 31}, {"type": "text", "text": "2. Finite inventory of values, where integer and fractional bits are both restricted. Such a setup may be justified based on the fact that any real computer has bounded memory, ", "page_idx": 31}, {"type": "image", "img_path": "eV5YIrJPdy/tmp/0a0119fa52182b5e60e2ec7e2a8f721ddbfe690bb2bd5b721071413abb9d6d63.jpg", "img_caption": [], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "Figure 7: Mamba Accuracy on $D y c k_{8,10}$ , on the development set (length $\\le700$ , same length range as training set) and test set (length $700\\leq n\\leq1400)$ . The latter is also plotted in Figure 4. ", "page_idx": 32}, {"type": "text", "text": "though such a setup precludes any positive results on non-finite-state problems for any computational architecture.10 ", "page_idx": 32}, {"type": "text", "text": "Such a restrictive setup would not affect our positive results on Flip-Flop, Star-Free, and bounded-depth Dyck languages (Theorems 1, 4, 6), as these all use bounded finite-precision activation values. As this is a more restricted setup than the one we are assuming, this also would not affect our negative results about PARITY and non-star-free languages (Theorems 2, 4). These results are thus highly robust to variations of the finite precision assumption. ", "page_idx": 32}, {"type": "text", "text": "Such a more restrictive definition would, however, mean that, for unbounded counting (Theorem 5), modeling is only possible up to a bound determined by the number of possible values\u2014this is the one place where our results would be impacted. Indeed, we do observe that Mamba learns these counter languages on training lengths but struggles with length generalization. Transformers, on the other hand, can represent these languages with bounded activations (due to the constructions in Bhattamishra et al. [2020]), and show strong length generalization. ", "page_idx": 32}, {"type": "text", "text": "An intermediary between infinite and finite precision is notions of precision where the number of allowed bits slowly increases with the input length, e.g., logarithmically. Such a setup has particularly been adopted for transformers [Merrill and Sabharwal, 2023], because a finite-precision assumption leads to very low expressivity in transformers. For SSMs, on the other hand, we find that finite precision assumptions are sufficient for showing a broad range of positive results. ", "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The abstract and introduction summarise the theoretical and empirical results.   \nWe took to only include well-supported claims. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: The Discussion section includes a paragraph on Limitations ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 33}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Every theorem is proven formally in the appendix. Assumptions about the SSM architecture are stated formally. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Besides describing experimental details, we have publicly released our codebase on GitHub and added a link to it in the main paper with instructions on how to run our experiments, ensuring that our findings can be reproduced. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Yes, we have publicly released our codebase on GitHub and added a link to it in the main paper with instructions on how to run our experiments. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We have specified the training and test details in a dedicated appendix section. Our experiments use freely available dataset or dataset generating scripts from previous studies, which we cite. Our uploaded code base ensures that all settings are reproducible. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [No] ", "page_idx": 35}, {"type": "text", "text": "Justification: In the empirical results relevant to supporting our positive theoretical results, the accuracy attained by Mamba is either very close to $100\\%$ or far from $100\\%$ . As the test sets include hundreds or thousands of data points, error bars would not add substantial further information. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 35}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We have provided information in the appendix section on experimental setup. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We have verified compliance with the NeurIPS Code of Ethics, ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper studies foundational properties of neural network architectures, and we foresee no positive or negative societal impact of the results. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: We are not releasing data or models, and foresee no risk of misues of our theoretical results. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper cites the original papers, and provides details in the Appendix section on experimental details. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 37}, {"type": "text", "text": "\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper releases no new assets. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper involves no crowdsourcing nor human subjects research. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 38}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper involves no crowdsourcing nor human subjects research. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 38}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}]