[{"figure_path": "LqdcdqIeVD/figures/figures_1_1.jpg", "caption": "Figure 1: Difference between our spherical frustum and conventional spherical projection. In conventional spherical projection, the points projected onto the same 2D grid are dropped, which leads to quantized information loss, e.g., dropping the boundary between the person, a small object, and the road, and results in incorrect prediction of the 2D projection-based method RangeViT [21] for the person. In contrast, our spherical frustum preserves all points in the frustum, which eliminates quantized information loss and makes SFCNet correctly segment the person.", "description": "This figure compares conventional spherical projection with the proposed spherical frustum approach.  The conventional method projects 3D LiDAR points onto a 2D plane, discarding points that fall on the same grid cell, leading to information loss and inaccurate segmentation, particularly for small objects. In contrast, the spherical frustum preserves all points projected to the same 2D location, preventing information loss and improving segmentation accuracy, as demonstrated by the example of correctly segmenting a person.", "section": "1 Introduction"}, {"figure_path": "LqdcdqIeVD/figures/figures_4_1.jpg", "caption": "Figure 2: Pipeline of Spherical Frustum sparse Convolution. The spherical frustums in the convolution kernel and the points in these spherical frustums are first selected through the hash table. Then, the nearest point in each spherical frustum is determined by the 3D geometric information. Finally, the sparse convolution is performed on the selected point features.", "description": "This figure illustrates the process of Spherical Frustum Sparse Convolution (SFC).  It starts by selecting spherical frustums using a hash table based on a kernel shift from a central 2D coordinate (u,v).  Then, the closest point within each selected spherical frustum is identified using 3D geometric information.  Finally, a sparse convolution is performed on the features of these selected nearest points.", "section": "3 SFCNet"}, {"figure_path": "LqdcdqIeVD/figures/figures_5_1.jpg", "caption": "Figure 3: Pipeline of Frustum Farthest Point Sampling. According to the downsampling strides, the spherical frustums in each stride window are downsampled. Then, through the hash table, the points in each downsampled spherical frustum are queried. The queried points are sampled by Farthest Point Sampling (FPS) based on the 3D geometric information. Finally, the uniformly sampled spherical frustums and point cloud are obtained.", "description": "This figure illustrates the Frustum Farthest Point Sampling (F2PS) method.  It starts by dividing the 2D spherical plane into windows using strides. Spherical frustums within each window are merged to create downsampled frustums.  The hash table is then used to query the points within these downsampled frustums.  Farthest Point Sampling is then applied to uniformly sample the points within each downsampled frustum. The result is a uniformly sampled point cloud and a set of uniformly sampled spherical frustums, suitable for use in the network.", "section": "3.3 Frustum Farthest Point Sampling"}, {"figure_path": "LqdcdqIeVD/figures/figures_7_1.jpg", "caption": "Figure 4: Qualitive results on SemanticKITTI validation set. The first column presents the ground truths, while the following three columns show the error maps of the predictions from the three methods. Specifically, the reference from point color to the semantic class in the ground truths is shown at the bottom. In addition, the false-segmented points are marked as red in the error maps. Moreover, we use circles with the same color to point out the same objects in the ground truth and the three error maps. Furthermore, the corresponding RGB images of each scene with the colored point cloud projected are demonstrated. We also show the corresponding zoomed RGB image view of circled objects if they are visible in the RGB images.", "description": "This figure shows a qualitative comparison of semantic segmentation results on the SemanticKITTI validation set between three methods: CENet, RangeViT, and SFCNet (the authors' method).  Each row displays a scene with ground truth, then error maps (falsely classified points in red) for each method, highlighting the improvements of SFCNet in object classification accuracy, especially for small objects such as people and poles.  Corresponding RGB images provide visual context. ", "section": "4.4 Qualitative Results"}, {"figure_path": "LqdcdqIeVD/figures/figures_13_1.jpg", "caption": "Figure 5: The Detailed Architecture of SFCNet. (a) presents the detailed pipeline of SFCNet. In addition, (b), (c), and (d) show the detailed module structures of the SFC layer, SFC block, and downsampling SFC block respectively, where SFC means spherical frustum sparse convolution, and F2PS means the frustum farthest point sampling.", "description": "This figure shows the detailed architecture of SFCNet, a deep learning model for LiDAR point cloud semantic segmentation.  It breaks down the model into its core components: a context block, extraction layers, downsampling and upsampling SFC blocks, and a head layer.  Each component is described with its constituent sub-modules (SFC Layer, SFC Block, and Downsampling SFC Block), illustrating the flow of data and feature processing throughout the network.", "section": "3 SFCNet"}, {"figure_path": "LqdcdqIeVD/figures/figures_19_1.jpg", "caption": "Figure 3: Pipeline of Frustum Farthest Point Sampling. According to the downsampling strides, the spherical frustums in each stride window are downsampled. Then, through the hash table, the points in each downsampled spherical frustum are queried. The queried points are sampled by Farthest Point Sampling (FPS) based on the 3D geometric information. Finally, the uniformly sampled spherical frustums and point cloud are obtained.", "description": "This figure illustrates the Frustum Farthest Point Sampling (F2PS) process.  First, the 2D spherical plane is divided into stride windows. Spherical frustums within each window are merged into downsampled spherical frustums.  Points within these are then queried using a hash table. Finally, Farthest Point Sampling (FPS) selects a uniform subset of these points, resulting in a uniformly sampled point cloud.", "section": "3.3 Frustum Farthest Point Sampling"}, {"figure_path": "LqdcdqIeVD/figures/figures_21_1.jpg", "caption": "Figure 1: Difference between our spherical frustum and conventional spherical projection. In conventional spherical projection, the points projected onto the same 2D grid are dropped, which leads to quantized information loss, e.g., dropping the boundary between the person, a small object, and the road, and results in incorrect prediction of the 2D projection-based method RangeViT [21] for the person. In contrast, our spherical frustum preserves all points in the frustum, which eliminates quantized information loss and makes SFCNet correctly segment the person.", "description": "This figure compares the proposed spherical frustum approach with the conventional spherical projection method.  The conventional method drops points that project to the same 2D grid, leading to information loss, especially for small objects like the person shown in the example. The spherical frustum method, however, preserves all points, preventing information loss and improving segmentation accuracy.", "section": "1 Introduction"}, {"figure_path": "LqdcdqIeVD/figures/figures_22_1.jpg", "caption": "Figure 8: More Qualitative Comparison on Semantic Segmentation on SemanticKITTI Test Set. We show the qualitative comparison between our SFCNet and the state-of-the-art 2D image-based method CENet [20] on the SemanticKITTI test set. The visualized challenging autonomous driving scenes include urban, rural, and complex intersection scenes. The predictions projected on the corresponding RGB images are also illustrated. In addition, we use the same color boxes to point out the same objects in the point clouds and images for each scene. Meanwhile, we provide the zoomed-in view of some boxed objects for clear visualization. Moreover, the reference from point color to the semantic class in the predictions is shown at the bottom of the figure.", "description": "This figure compares the qualitative results of SFCNet and CENet on the SemanticKITTI test set.  It shows three example scenes (urban, rural, complex intersection) and visualizes the differences between the ground truth, CENet's predictions, and SFCNet's predictions.  The color coding for semantic classes is provided, and zoomed-in views highlight the improved accuracy of SFCNet, particularly for small objects.", "section": "4.4 Qualitative Results"}, {"figure_path": "LqdcdqIeVD/figures/figures_23_1.jpg", "caption": "Figure 8: More Qualitative Comparison on Semantic Segmentation on SemanticKITTI Test Set. We show the qualitative comparison between our SFCNet and the state-of-the-art 2D image-based method CENet [20] on the SemanticKITTI test set. The visualized challenging autonomous driving scenes include urban, rural, and complex intersection scenes. The predictions projected on the corresponding RGB images are also illustrated. In addition, we use the same color boxes to point out the same objects in the point clouds and images for each scene. Meanwhile, we provide the zoomed-in view of some boxed objects for clear visualization. Moreover, the reference from point color to the semantic class in the predictions is shown at the bottom of the figure.", "description": "This figure shows a qualitative comparison of semantic segmentation results between SFCNet and CENet on the SemanticKITTI test set for three different scene types: urban, rural, and complex intersections. The results demonstrate SFCNet's superior performance in accurately segmenting objects, especially small objects, by preserving complete geometric information and avoiding information loss.", "section": "4.4 Qualitative Results"}, {"figure_path": "LqdcdqIeVD/figures/figures_24_1.jpg", "caption": "Figure 10: More Qualitative Comparison of Semantic Segmentation on NuScenes Validation Set. We show more comparisons between our SFCNet and the state-of-the-art 2D image-based method RangeViT [21] on the nuScenes dataset. The predictions projected on the corresponding RGB images are also illustrated. In addition, we use the same color boxes to point out the same objects in the point clouds and images for each scene. Meanwhile, we provide the zoomed-in view of some boxed objects for clear visualization. Moreover, the reference from point color to the semantic class in the predictions and ground truths is shown at the bottom of the figure.", "description": "This figure compares the qualitative results of semantic segmentation on the nuScenes validation set between SFCNet and RangeViT.  It shows three example scenes, displaying ground truth, RangeViT's output, and SFCNet's output for each.  The color-coded legend indicates semantic classes.  Red highlights incorrect segmentations.  The figure demonstrates SFCNet's improved accuracy and detail, particularly in segmenting smaller objects.", "section": "4.4 Qualitative Results"}]