[{"figure_path": "PAWQvrForJ/figures/figures_1_1.jpg", "caption": "Figure 1: Attack performances of the reference-based MIA (LiRA [8, 46, 47, 72]) and reference-free MIA (LOSS Attack [73]) unsatisfy against LLMs in practical scenarios, where LLMs are in the memorization stage and only domain-specific dataset is available. (a) Reference-based MIA shows a catastrophic plummet in performance when the similarity between the reference and training datasets declines. (b) Existing MIAs are unable to pose privacy leakages on LLMs that only exhibit memorization, an inevitable phenomenon occurs much earlier than overfitting and persists throughout almost the entire training phase [47, 64, 75].", "description": "This figure compares the performance of reference-based and reference-free membership inference attacks (MIAs) against large language models (LLMs) under different conditions.  It shows that existing MIAs perform poorly when the reference dataset is dissimilar to the training data or when the LLM is in the memorization phase rather than the overfitting phase.  The figure highlights the limitations of existing MIAs in practical scenarios.", "section": "1 Introduction"}, {"figure_path": "PAWQvrForJ/figures/figures_1_2.jpg", "caption": "Figure 1: Attack performances of the reference-based MIA (LiRA [8, 46, 47, 72]) and reference-free MIA (LOSS Attack [73]) unsatisfy against LLMs in practical scenarios, where LLMs are in the memorization stage and only domain-specific dataset is available. (a) Reference-based MIA shows a catastrophic plummet in performance when the similarity between the reference and training datasets declines. (b) Existing MIAs are unable to pose privacy leakages on LLMs that only exhibit memorization, an inevitable phenomenon occurs much earlier than overfitting and persists throughout almost the entire training phase [47, 64, 75].", "description": "This figure shows the poor performance of existing membership inference attacks (MIAs) against large language models (LLMs), especially when the LLM is in the memorization phase and only a domain-specific reference dataset is available.  The left subplot (a) demonstrates how reference-based MIAs significantly underperform when the reference dataset differs from the training data, while the right subplot (b) shows that both reference-based and reference-free MIAs fail to detect privacy leaks during the memorization phase, which typically precedes overfitting.", "section": "1 Introduction"}, {"figure_path": "PAWQvrForJ/figures/figures_4_1.jpg", "caption": "Figure 2: The overall workflow of SPV-MIA, where includes the probabilistic calibration via self-prompt reference model and the probabilistic variation assessment via paraphrasing model.", "description": "This figure illustrates the workflow of the Self-calibrated Probabilistic Variation Membership Inference Attack (SPV-MIA).  It shows how a target LLM is prompted to generate a reference dataset, which is then used to train a reference LLM. The target LLM and reference LLM are used to calculate the probabilistic variation of a target text record.  The probabilistic variation, along with a difficulty calibration step, is used to determine if the text record was part of the target LLM's training data.  The figure visually represents the two main components of the attack: self-prompt calibration and probabilistic variation assessment, highlighting their roles in determining membership.", "section": "4 Membership Inference Attack via Self-calibrated Probabilistic Variation"}, {"figure_path": "PAWQvrForJ/figures/figures_8_1.jpg", "caption": "Figure 3: The performances of reference-based MIA on LLaMA while utilizing different reference datasets.", "description": "This figure shows the Area Under the Curve (AUC) of the Likelihood Ratio Attack (LiRA), a membership inference attack, on the LLaMA large language model. The AUC is shown for four different types of reference datasets: identical-distribution, domain-specific, irrelevant, and self-prompt.  The x-axis represents three different datasets (Wiki, AG News, XSum) used for fine-tuning the LLaMA model.  The y-axis represents the AUC score, which ranges from 0.6 to 1.0 and indicates the attack's performance. The figure illustrates that LiRA's performance drops drastically as the similarity between the reference and training datasets decreases.  However, the self-prompt method shows relatively high AUC scores, indicating that the self-generated reference data successfully resembles the training data distribution.", "section": "5 Experiments"}, {"figure_path": "PAWQvrForJ/figures/figures_8_2.jpg", "caption": "Figure 3: The performances of reference-based MIA on LLaMA while utilizing different reference datasets.", "description": "This figure shows the performance of reference-based Membership Inference Attacks (MIAs) on the LLaMA large language model when using reference datasets of varying similarity to the training data.  It illustrates how the attack performance degrades as the similarity between the reference and training datasets decreases.  The datasets used are categorized as Identical-distribution, Domain-specific, Irrelevant, and Self-prompt. The Self-prompt dataset demonstrates that the proposed self-prompt approach can generate a high-quality reference dataset even when the exact training data is not available, reducing the dependence of the MIA on reference dataset quality.", "section": "5.4 The Robustness of SPV-MIA in Practical Scenarios"}, {"figure_path": "PAWQvrForJ/figures/figures_8_3.jpg", "caption": "Figure 3: The performances of reference-based MIA on LLaMA while utilizing different reference datasets.", "description": "The figure shows the AUC scores of the reference-based MIA (LiRA) on the LLaMA model using four different types of reference datasets: identical-distribution, domain-specific, irrelevant, and self-prompt. The results demonstrate that the performance of LiRA decreases as the similarity between the reference dataset and the training dataset decreases. However, the self-prompt approach achieves comparable performance to the identical-distribution dataset, indicating its ability to generate high-quality reference datasets.", "section": "5.4 The Robustness of SPV-MIA in Practical Scenarios"}, {"figure_path": "PAWQvrForJ/figures/figures_20_1.jpg", "caption": "Figure 6: Linear-scale ROC curves of SPV-MIA and the top-three best baselines on LLMs fine-tuned over three datasets.", "description": "This figure shows the Receiver Operating Characteristic (ROC) curves for several membership inference attack (MIA) methods on three different datasets.  The linear scale on the y-axis emphasizes the performance differences, particularly at lower false positive rates. SPV-MIA consistently outperforms the other methods across all three datasets.  The results visually support the claim made in the paper that SPV-MIA significantly improves the AUC of MIAs.", "section": "5.2 Overall Performance"}, {"figure_path": "PAWQvrForJ/figures/figures_20_2.jpg", "caption": "Figure 6: Linear-scale ROC curves of SPV-MIA and the top-three best baselines on LLAMAs fine-tuned over three datasets.", "description": "This figure shows the receiver operating characteristic (ROC) curves for SPV-MIA and three other membership inference attack methods (Neighbour Attack, LiRA (Base), and LiRA (Candidate)).  The ROC curves illustrate the performance of each method across three different datasets (Wiki, AG News, and Xsum).  The x-axis represents the false positive rate, and the y-axis represents the true positive rate.  The curves visually compare the trade-off between true positives and false positives for each method across different datasets.", "section": "5.2 Overall Performance"}]