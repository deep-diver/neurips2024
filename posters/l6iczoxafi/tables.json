[{"figure_path": "L6ICzOxAfi/tables/tables_6_1.jpg", "caption": "Table 1: Results on the pixel correspondence task on the Paired split [36] of ScanNet [10], as introduced by El Banani et al. [12]. We report the recall of accurate pixel correspondences at a reprojection error threshold of 10 pixels, for image pairs with the respective viewpoint changes. We also report the GPU Memory required for training LoCUS-based and LoCo models (for LOCUS we use the authors' values). \u2020 uses 64-dimensional feature vectors.", "description": "This table presents the results of a pixel correspondence task on the ScanNet dataset, comparing various feature extraction methods.  The task assesses the accuracy of matching pixels representing the same 3D location across different viewpoints. The table shows the recall (percentage of correctly matched pixels) for various angular differences between viewpoints (0\u00b0-15\u00b0, 15\u00b0-30\u00b0, 30\u00b0-60\u00b0, 60\u00b0-180\u00b0).  The GPU memory used during training for each method is also indicated. The results highlight the superior performance of the proposed LoCo method compared to existing state-of-the-art techniques.", "section": "4.3 Multi-View Consistency"}, {"figure_path": "L6ICzOxAfi/tables/tables_7_1.jpg", "caption": "Table 2: Scene-stable panoptic segmentation results on unseen Matterport3D [6] and ScanNet [10] environments. Except for MaskDINO, each method extracts d-dimensional feature vectors for 30 \u00d7 40 patches that are then classified into a scene-wide object index using a linear probe. The feature dimension is d = 768, except for LoCUS (d = 64) due to its high memory consumption. *Per-image instance indices are matched to the ground-truth per-scene indices based on mask IoU.", "description": "This table presents the results of scene-stable panoptic segmentation on unseen Matterport3D and ScanNet datasets.  The performance of different models (LOCUS, DINO, DINOv2, CroCo-v2, MaskDINO, and the proposed LoCo) is evaluated using three metrics: Jaccard index (Jac), Intersection over Union (IoU), and Average Precision (AP).  The feature dimension (d) is specified for each model, noting the smaller dimension for LOCUS due to memory constraints.  A crucial point is that per-image object instance indices were matched to ground-truth per-scene indices based on IoU.", "section": "4.4 Scene-Stable Panoptic Segmentation"}, {"figure_path": "L6ICzOxAfi/tables/tables_13_1.jpg", "caption": "Table 3: Hyperparameters of the convolutional layers of the residual network used for the pixel-correspondence task in Section 4.3.", "description": "This table details the architecture of the fully convolutional residual network used in the pixel-correspondence task. It lists the layer number, kernel size, output dimension, dilation, and downsampling factor for each of the six convolutional layers.  This network learns additive residuals to the frozen DINO features, enhancing the location consistency of the features.", "section": "4.3 Multi-View Consistency"}, {"figure_path": "L6ICzOxAfi/tables/tables_19_1.jpg", "caption": "Table 1: Results on the pixel correspondence task on the Paired split [36] of ScanNet [10], as introduced by El Banani et al. [12]. We report the recall of accurate pixel correspondences at a reprojection error threshold of 10 pixels, for image pairs with the respective viewpoint changes. We also report the GPU Memory required for training LoCUS-based and LoCo models (for LOCUS we use the authors' values). \u2020 uses 64-dimensional feature vectors.", "description": "This table presents the results of a pixel correspondence task on the ScanNet dataset.  The task measures the accuracy of matching pixels between different views of the same scene, given varying degrees of viewpoint change (0-15\u00b0, 15-30\u00b0, 30-60\u00b0, 60-180\u00b0).  The table compares the performance of several models: LoCo (the proposed method), LOCUS, DINO, DINOv2, and CroCo-v2, reporting the recall of accurate correspondences at a 10-pixel reprojection error threshold. GPU memory usage during training is also provided for LoCo and LoCUS.  The results demonstrate the superior performance of LoCo in achieving location consistency across varying viewpoints.", "section": "4.3 Multi-View Consistency"}, {"figure_path": "L6ICzOxAfi/tables/tables_20_1.jpg", "caption": "Table 5: Visual Place Recognition Results on VPR datasets constructed from unseen images in Matterport3D [6] and ScanNet [10]. MixVPR produces global image descriptors directly, all other methods extract d-dimensional feature vectors for 30 \u00d7 40 patches that are then aggregated into global descriptors using VLAD. LoCUS\u2020 [26] uses d = 64, rather than d = 768.", "description": "This table presents the results of visual place recognition experiments using various feature extractors on the Matterport3D and ScanNet datasets.  The results are reported using the Recall@1 and Recall@5 metrics, showing the top 1 and top 5 most similar reference images retrieved for each query image.  The table compares the performance of LoCo against several baselines including LOCUS, DINO, DINOv2, CroCo-v2 and MixVPR.  Most methods utilize VLAD for aggregating local feature vectors, except for MixVPR which generates global image descriptors directly.  The LoCo method uses 768-dimensional features, while LOCUS uses 64-dimensional features due to memory limitations.", "section": "F Visual Place Recognition"}]