[{"figure_path": "L6ICzOxAfi/figures/figures_0_1.jpg", "caption": "Figure 1: Our approach\u2014LoCo\u2014offers memory-efficient learning of location-consistent (LoCo) features. That is, features that backproject to nearby 3D locations are encouraged to have similar image patch features (illustrated here by the pair of blue stacked cubes and the pair of red stacked cubes), while those that backproject to well-separated points are trained to have more dissimilar features (here, the blue vs. red cube stacks). This is achieved via a novel ranking loss that reformulates and corrects the smooth average precision loss proposed in previous work [4, 26]. This facilitates the derivation of a close approximation to the loss that is significantly more efficient to compute, allowing the method to scale to much larger models with the same computational resources.", "description": "This figure illustrates the core concept of the LoCo method.  It shows two sets of image patches (blue and red cubes) that project to nearby 3D locations in a scene.  The LoCo method aims to learn features such that these nearby patches have similar feature vectors, while patches that project to far-apart 3D locations have dissimilar feature vectors.  This is achieved by a novel, memory-efficient ranking loss function that improves on previous smooth average precision loss functions.", "section": "Abstract"}, {"figure_path": "L6ICzOxAfi/figures/figures_3_1.jpg", "caption": "Figure 2: (a) Defining the positive and negative sets. (b) Memory-efficient strategy for smooth AP.", "description": "This figure illustrates the concepts of positive and negative sets for pairs of image patches in the context of learning location-consistent image features.  (a) shows that positive pairs (green arrows) have 3D points closer than a threshold \u03c1, while negative pairs (red arrows) have 3D points further apart, but still within another threshold \u03ba.  (b) visualizes the memory-efficient strategy based on the Smooth Average Precision loss, where the loss function saturates when the absolute similarity difference between image pairs is large, enabling optimization improvements. This allows for only calculating similarity differences for a subset of pairs, reducing memory usage.", "section": "3 Efficiently Learning LoCo Image Features"}, {"figure_path": "L6ICzOxAfi/figures/figures_3_2.jpg", "caption": "Figure 2: (a) Defining the positive and negative sets. (b) Memory-efficient strategy for smooth AP.", "description": "This figure illustrates the methodology for creating positive and negative sets for efficient learning. Panel (a) depicts how positive pairs (green arrows) have a 3D distance less than \\(p\\), negative pairs (red arrows) have a 3D distance between \\(p\\) and \\(\u03ba\\), and others have a distance larger than \\(\u03ba\\).  Panel (b) shows a memory-efficient strategy by using the sigmoid function with saturation thresholds. Pairs with large similarity differences saturate the sigmoid and don't impact training; this avoids memory costs by creating 3 subsets of positive and negative pairs (saturated below, unsaturated, and saturated above).", "section": "3 Efficiently Learning LoCo Image Features"}, {"figure_path": "L6ICzOxAfi/figures/figures_8_1.jpg", "caption": "Figure 3: Scene-stable object segmentations for three images drawn from the Matterport3D [6] dataset. Ground-truth segmentations in the top row, predicted segmentations in the bottom row. The object identities and segmentation masks remain stable across significant viewpoint changes.", "description": "This figure shows a qualitative comparison of scene-stable panoptic segmentation results using the proposed LoCo method.  Three images from the Matterport3D dataset are shown, each with its corresponding ground truth segmentation (top row) and the segmentation obtained using the LoCo method (bottom row).  The key observation is that the object identities (colors) and segmentation masks remain consistent across significant viewpoint changes, demonstrating the scene-stable nature of the model's output. This highlights the model's ability to recognize and segment the same objects even under varying viewing angles and perspectives.", "section": "4.4 Scene-Stable Panoptic Segmentation"}, {"figure_path": "L6ICzOxAfi/figures/figures_21_1.jpg", "caption": "Figure 1: Our approach\u2014LoCo\u2014offers memory-efficient learning of location-consistent (LoCo) features. That is, features that backproject to nearby 3D locations are encouraged to have similar image patch features (illustrated here by the pair of blue stacked cubes and the pair of red stacked cubes), while those that backproject to well-separated points are trained to have more dissimilar features (here, the blue vs. red cube stacks). This is achieved via a novel ranking loss that reformulates and corrects the smooth average precision loss proposed in previous work [4, 26]. This facilitates the derivation of a close approximation to the loss that is significantly more efficient to compute, allowing the method to scale to much larger models with the same computational resources.", "description": "This figure illustrates the core concept of LoCo, a memory-efficient method for learning location-consistent image features.  It shows how the method encourages similar features for image patches that project to nearby 3D locations and dissimilar features for patches projecting to distant 3D points.  The use of stacked cubes helps visualize the backprojection of image patches to 3D space and how similar/dissimilar features are learned based on 3D proximity.", "section": "Introduction"}, {"figure_path": "L6ICzOxAfi/figures/figures_21_2.jpg", "caption": "Figure 1: Our approach\u2014LoCo\u2014offers memory-efficient learning of location-consistent (LoCo) features. That is, features that backproject to nearby 3D locations are encouraged to have similar image patch features (illustrated here by the pair of blue stacked cubes and the pair of red stacked cubes), while those that backproject to well-separated points are trained to have more dissimilar features (here, the blue vs. red cube stacks). This is achieved via a novel ranking loss that reformulates and corrects the smooth average precision loss proposed in previous work [4, 26]. This facilitates the derivation of a close approximation to the loss that is significantly more efficient to compute, allowing the method to scale to much larger models with the same computational resources.", "description": "This figure illustrates the core concept of LoCo, a memory-efficient method for learning location-consistent image features.  It shows how the approach encourages similar features for image patches that backproject to nearby 3D points and dissimilar features for patches that backproject to distant 3D points.  This is achieved through a novel ranking loss function which is a more memory-efficient reformulation of the Smooth Average Precision loss.", "section": "Introduction"}]