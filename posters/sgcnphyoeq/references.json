{"references": [{"fullname_first_author": "Nesterov, Y.", "paper_title": "Lectures on Convex Optimization", "publication_date": "2018", "reason": "This book provides foundational knowledge on convex optimization, which is crucial to understanding gradient descent methods."}, {"fullname_first_author": "Koloskova, A.", "paper_title": "Revisiting gradient clipping: Stochastic bias and tight convergence guarantees", "publication_date": "2023", "reason": "This paper provides a theoretical analysis of gradient clipping, which is essential to understanding the convergence rate of the proposed method."}, {"fullname_first_author": "Zhang, J.", "paper_title": "Why gradient clipping accelerates training: A theoretical justification for adaptivity", "publication_date": "2020", "reason": "This paper explains the theoretical benefits of gradient clipping, which is a key hyperparameter for training deep neural networks."}, {"fullname_first_author": "Polyak, B.", "paper_title": "Introduction to Optimization", "publication_date": "1987", "reason": "This book introduces the Polyak stepsize, which is a core component of the proposed parameter-free method."}, {"fullname_first_author": "Hazan, E.", "paper_title": "Revisiting the Polyak step size", "publication_date": "2019", "reason": "This paper analyzes the Polyak stepsize and its convergence properties under various conditions, providing insights into its effectiveness."}]}