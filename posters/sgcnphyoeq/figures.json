[{"figure_path": "SGcnphYOeq/figures/figures_7_1.jpg", "caption": "Figure 1: Convergence behaviors of various methods with the synthetic function.", "description": "This figure compares the convergence behaviors of different optimization methods using a synthetic function. The methods include Gradient Descent, Clipped Gradient Descent, Polyak Stepsize, DecSPS, AdaSPS, and Inexact Polyak Stepsize.  The x-axis represents the number of iterations, and the y-axis represents the value of f(x) - f*, which is the difference between the loss function value at the current iteration and the optimal loss value. Each line represents a different optimization method, and different colors represent different values of L1 in the (L0,L1)-smoothness condition. The figure demonstrates that Inexact Polyak Stepsize achieves a convergence rate that is asymptotically independent of L, which is the key property investigated in the paper.", "section": "6.1 Synthetic function"}, {"figure_path": "SGcnphYOeq/figures/figures_7_2.jpg", "caption": "Figure 2: The final test loss with various hyperparameter settings. For T5, the results of DecSPS and AdaSPS were omitted because their final test loss was much larger than the others, as shown in Fig. 4. Furthermore, the results of SGD were also omitted when the final test loss became nan or infinity.", "description": "This figure displays the final test loss achieved by several different optimization methods across three neural network architectures: LSTM, Nano-GPT, and T5.  Each method was tested with a range of step sizes. For T5, DecSPS and AdaSPS results were excluded due to significantly higher test loss than other methods; results for SGD were also excluded in cases where the loss reached NaN or infinity.  The figure helps to compare the performance of various algorithms under different hyperparameter choices and highlights the impact of hyperparameters on the final model performance.", "section": "6.2 Neural networks"}, {"figure_path": "SGcnphYOeq/figures/figures_8_1.jpg", "caption": "Figure 1: Convergence behaviors of various methods with the synthetic function.", "description": "This figure compares the convergence behaviors of different optimization methods using a synthetic function.  The x-axis represents the number of iterations, and the y-axis represents the loss function value. The methods being compared include Gradient Descent, Clipped Gradient Descent, Polyak Stepsize, DecSPS, AdaSPS, and Inexact Polyak Stepsize.  The figure demonstrates that Inexact Polyak Stepsize is less sensitive to the hyperparameter L1 compared to other methods.", "section": "6.1 Synthetic function"}, {"figure_path": "SGcnphYOeq/figures/figures_17_1.jpg", "caption": "Figure 1: Convergence behaviors of various methods with the synthetic function.", "description": "The figure shows the convergence behaviors of various methods on a synthetic function. The methods compared include gradient descent, clipped gradient descent, Polyak stepsize, DecSPS, AdaSPS, and Inexact Polyak Stepsize. The x-axis represents the number of iterations, and the y-axis represents the difference between the loss function value and the optimal value. The figure demonstrates that the convergence rates of Polyak stepsize, DecSPS, and AdaSPS degrade as L1 increases. In contrast, the convergence behavior of Inexact Polyak Stepsize does not depend on L1, which is consistent with Theorem 5.  Inexact Polyak Stepsize successfully inherits the favorable properties of Polyak Stepsize under (L0,L1)-smoothness.", "section": "6.1 Synthetic function"}, {"figure_path": "SGcnphYOeq/figures/figures_18_1.jpg", "caption": "Figure 5: Loss curves for Nano-GPT with different T.", "description": "This figure shows the training and test loss curves for the Nano-GPT model trained with different numbers of iterations (T=2500 and T=7500).  It compares the performance of various optimization methods, including Clipped SGD, SGD, DoG, AdaSPS, DecSPS, and the proposed Inexact Polyak Stepsize. The plot helps to visualize how the different algorithms converge (or fail to converge) to a solution over time, especially highlighting the effects of the number of iterations on the convergence behavior and the relative performance of Inexact Polyak Stepsize.", "section": "6.2 Neural networks"}]