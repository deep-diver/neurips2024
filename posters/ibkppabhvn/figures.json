[{"figure_path": "ibKpPabHVn/figures/figures_2_1.jpg", "caption": "Figure 1: The illustration of the DeepDRK pipeline, which consists of two components: 1. the training stage that optimizes the knockoff Transformer and swappers by LSL and LDRL; 2. the post-training stage that generates the knockoff XDRP\u2295 via dependency regularized perturbation.", "description": "This figure illustrates the DeepDRK pipeline, a two-stage process for knockoff generation.  The first stage uses a Knockoff Transformer (KT) and multiple swappers to learn a knockoff generation model while minimizing reconstructability and enforcing the swap property. This is done by optimizing a loss function that includes a swap loss (LSL) and a dependency regularization loss (LDRL). The second stage applies a Dependency Regularized Perturbation (DRP) to further refine the knockoffs, yielding XDRP. This post-processing step aims to improve the selection power of the knockoffs by reducing their dependency with the original data.", "section": "3 Method"}, {"figure_path": "ibKpPabHVn/figures/figures_5_1.jpg", "caption": "Figure 2: Power and FDR for different knockoff models on the synthetic datasets with \u03b2 ~ 15\u221anRademacher(0.5). The red horizontal line indicates the 0.1 FDR threshold.", "description": "This figure compares the performance of DeepDRK and four other knockoff methods (Deep Knockoff, KnockoffGAN, SRMMD, and DDLK) on synthetic datasets where the feature coefficients are drawn from a Rademacher distribution.  The figure shows the power (the proportion of true features correctly identified) and the False Discovery Rate (FDR, the proportion of incorrectly identified features among the selected features) for each method.  The red horizontal line indicates a target FDR of 0.1, showing how well each method controls the FDR.  The x-axis represents different synthetic datasets (Gaussian Mixture, and copulas with different marginal distributions).", "section": "4.1 The Synthetic Experiments"}, {"figure_path": "ibKpPabHVn/figures/figures_5_2.jpg", "caption": "Figure 2: Power and FDR for different knockoff models on the synthetic datasets with \u03b2 ~ 15.\u221an Rademacher(0.5). The red horizontal line indicates the 0.1 FDR threshold.", "description": "This figure compares the performance of five different knockoff methods (DeepDRK, Deep Knockoff, KnockoffGAN, sRMMD, and DDLK) on synthetic datasets.  The x-axis shows different data distributions (Gaussian Mixture, and four copula-based distributions) and the y-axis shows the power and FDR (False Discovery Rate). Each bar represents the average result from multiple experiments, with error bars indicating variability.  The red horizontal line signifies the target FDR of 0.1. The figure demonstrates the performance of each method under various data distributions for two different sample sizes (n = 200 and n = 2000). DeepDRK generally achieves higher power while maintaining FDR below the threshold, particularly in smaller sample sizes.", "section": "4.1 The Synthetic Experiments"}, {"figure_path": "ibKpPabHVn/figures/figures_7_1.jpg", "caption": "Figure 4: The knockoff statistics (wj) for different knockoff models on the synthetic datasets with \u03b2~p15.\u221anRademacher(0.5). Each bar in the plot represents the mean of the null/nonnull knockoff statistics averaging on 600 experiments. The error bar indicates the standard deviation. The sample size is 200.", "description": "The figure shows the mean and standard deviation of knockoff statistics (wj) for null and non-null features, for different knockoff methods.  The results are averaged across 600 experiments with a sample size of 200.  It helps to illustrate how well each method distinguishes between truly relevant (non-null) and irrelevant (null) features based on the distribution of the statistics.", "section": "4.1 The Synthetic Experiments"}, {"figure_path": "ibKpPabHVn/figures/figures_7_2.jpg", "caption": "Figure 2: Power and FDR for different knockoff models on the synthetic datasets with \u03b2 ~ 15.\u221anRademacher(0.5). The red horizontal line indicates the 0.1 FDR threshold.", "description": "This figure compares the performance of DeepDRK and other knockoff methods in terms of power and false discovery rate (FDR) on synthetic datasets.  The different colored points represent different datasets, each generated with a different copula function and marginal distribution. The x-axis represents FDR, and the y-axis represents power. The red line indicates the target FDR of 0.1.  The figure demonstrates DeepDRK's ability to maintain a low FDR while achieving higher power compared to other methods.", "section": "4.1 The Synthetic Experiments"}, {"figure_path": "ibKpPabHVn/figures/figures_8_1.jpg", "caption": "Figure 6: Power and FDR for different knockoff models on the semi-synthetic RNA dataset. The red horizontal line indicates the 0.1 FDR threshold.", "description": "This figure compares the performance of DeepDRK and four other knockoff methods (Deep Knockoff, KnockoffGAN, sRMMD, and DDLK) on a semi-synthetic RNA-Seq dataset.  Two different synthetic rules for generating the response variable Y are used: a linear rule and a tanh rule.  The figure shows the power and false discovery rate (FDR) for each method at an FDR threshold of 0.1.  DeepDRK demonstrates higher power with controlled FDR compared to the other methods, especially under the tanh rule.", "section": "4.2 The Semi-synthetic Experiments"}, {"figure_path": "ibKpPabHVn/figures/figures_17_1.jpg", "caption": "Figure 8: The competing relationship between the swap property (LSL in solid curves) and dependency regularization (LDRL in dashed curves).", "description": "This figure shows the training curves of the swap loss (LSL) and dependency regularization loss (LDRL) for different models.  It illustrates the trade-off between satisfying the swap property and minimizing reconstructability.  The swap loss aims to ensure the swap property holds, while the dependency regularization loss attempts to reduce the correlation between original and knockoff data. DeepDRK demonstrates the ability to balance these losses effectively, resulting in competitive performance. ", "section": "3.2 Dependency Regularization Perturbation"}, {"figure_path": "ibKpPabHVn/figures/figures_18_1.jpg", "caption": "Figure 9: The effect of \u03b1<sub>n</sub> for X<sup>DRP</sup> in Eq. (9) on FDR and power. When \u03b1<sub>n</sub> is 0, we consider the knockoff generated from the knockoff transformer without any dependency regularization perturbation. When \u03b1<sub>n</sub> is 1, there is only perturbation X<sup>rp</sup> without the knockoff. The sample size n = 2000.", "description": "This figure shows the effect of the hyperparameter \u03b1<sub>n</sub> (perturbation weight) on the False Discovery Rate (FDR) and power of DeepDRK.  The x-axis represents \u03b1<sub>n</sub>, ranging from 0 (no perturbation) to 1 (only the row-permuted version of X).  The y-axis shows the FDR and power for different datasets.  The figure demonstrates that an optimal value of \u03b1<sub>n</sub> balances FDR control and power, achieving a better tradeoff than using only the knockoff or only the perturbation.", "section": "4.1 The Synthetic Experiments"}, {"figure_path": "ibKpPabHVn/figures/figures_22_1.jpg", "caption": "Figure 10: Power and FDR in the ablation studies. The red horizontal line indicates the 0.1 FDR threshold. DeepDRK\u2020 is the model with dependency regularized perturbation removed. K indicates the number of swappers.", "description": "This figure presents the results of ablation studies conducted to evaluate the impact of different components of the DeepDRK model on its performance in terms of power and false discovery rate (FDR).  The ablation studies remove different components of the model to isolate their effect.  The results show that all components are necessary to achieve both good FDR control and high power.  The red horizontal line shows the target FDR threshold of 0.1.", "section": "4.1 The Synthetic Experiments"}, {"figure_path": "ibKpPabHVn/figures/figures_22_2.jpg", "caption": "Figure 11: Knockoff statistics (wj) for different knockoff models on the synthetic datasets. Each bar in the plot represents the mean of the null/nonnull knockoff statistics averaging on 600 experiments. The error bar indicates the standard deviation. The sample size is 2000.", "description": "This figure compares the distribution of knockoff statistics (wj) for null and non-null features across five different knockoff models. The mean and standard deviation of wj are shown for each model and dataset (Gaussian Mixture, Copula: Clayton & Exponential, Copula: Clayton & Gamma, Copula: Joe & Exponential, Copula: Joe & Gamma).  The sample size is 2000 and the results are averaged over 600 experiments.  Ideally, a good knockoff method will have null features concentrate near zero, while non-null features have larger positive values, indicating strong separation and selection power.", "section": "4.1 The Synthetic Experiments"}, {"figure_path": "ibKpPabHVn/figures/figures_23_1.jpg", "caption": "Figure 13: Knockoff statistics (wj) for different models with the increased correlation of the mixture of Gaussians data. Each bar in the plot represents the mean of the null/nonnull knockoff statistics averaging on 600 experiments. The error bar indicates the standard deviation. The sample size considered is 2000.", "description": "This figure shows the mean and standard deviation of knockoff statistics (wj) for null and non-null features for different models on Gaussian mixture data with increased correlation (pbase = 0.7 and 0.8) and a sample size of 2000.  The results show how different models handle the null and non-null features under increased correlation. DeepDRK maintains good separation between null and non-null features while other models show more overlap.", "section": "4.1 The Synthetic Experiments"}, {"figure_path": "ibKpPabHVn/figures/figures_23_2.jpg", "caption": "Figure 12: Knockoff statistics (wj) for different models with the increased correlation of the Gaussian mixture data. Each bar in the plot represents the mean of the null/nonnull knockoff statistics averaging on 600 experiments. The error bar indicates the standard deviation. The sample size is 200.", "description": "This figure displays the mean and standard deviation of knockoff statistics (wj) for null and non-null features using different feature selection models.  The increased correlation in the Gaussian mixture data is a key condition for this experiment.  The results show the performance of different methods under conditions of high feature correlation. DeepDRK is one of the models presented in the plot.", "section": "4.1 The Synthetic Experiments"}, {"figure_path": "ibKpPabHVn/figures/figures_23_3.jpg", "caption": "Figure 10: Power and FDR in the ablation studies. The red horizontal line indicates the 0.1 FDR threshold. DeepDRK\u2020 is the model with dependency regularized perturbation removed. K indicates the number of swappers.", "description": "This figure presents the results of ablation studies on DeepDRK, investigating the impact of different components on its performance.  It compares the power and false discovery rate (FDR) of DeepDRK with variations: removing the dependency regularized perturbation (DeepDRK\u2020), removing the Lswapper term, removing the REx term, and using only a single swapper (K=1).  The results are shown across five different synthetic datasets, demonstrating the importance of each component in balancing power and FDR control.", "section": "4.1 The Synthetic Experiments"}, {"figure_path": "ibKpPabHVn/figures/figures_24_1.jpg", "caption": "Figure 10: Power and FDR in the ablation studies. The red horizontal line indicates the 0.1 FDR threshold. DeepDRK\u2020 is the model with dependency regularized perturbation removed. K indicates the number of swappers.", "description": "This figure presents the results of ablation studies conducted to evaluate the impact of different components of the DeepDRK model on its performance in terms of power and false discovery rate (FDR). The studies assess the effect of removing the dependency regularized perturbation, using a single swapper instead of multiple, and removing the regularization term, REx. The results are shown for two different sample sizes (n=200 and n=2000) across five different synthetic datasets, allowing for comparison under varying conditions.", "section": "4.1 The Synthetic Experiments"}]