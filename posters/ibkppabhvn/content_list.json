[{"type": "text", "text": "DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hongyu Shen1 Yici Yan2 Zhizhen Zhao1   \nDepartment of Electrical and Computer Engineering1, University of Illinois at Urbana Champaign. Department of Statistics2, University of Illinois at Urbana Champaign.   \n{hongyu2, yiciyan2, zhizhenz}@illinois.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Model-X knockoff has garnered significant attention among various feature selection methods due to its guarantees for controlling the false discovery rate (FDR). Since its introduction in parametric design, knockoff techniques have evolved to handle arbitrary data distributions using deep learning-based generative models. However, we have observed limitations in the current implementations of the deep Model-X knockoff framework. Notably, the \u201cswap property\u201d that knockoffs require often faces challenges at the sample level, resulting in diminished selection power. To address these issues, we develop \u201cDeep Dependency Regularized Knockoff (DeepDRK),\u201d a distribution-free deep learning method that effectively balances FDR and power. In DeepDRK, we introduce a novel formulation of the knockoff model as a learning problem under multi-source adversarial attacks. By employing an innovative perturbation technique, we achieve lower FDR and higher power. Our model outperforms existing benchmarks across synthetic, semi-synthetic, and real-world datasets, particularly when sample sizes are small and data distributions are non-Gaussian. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Feature selection (FS) has garnered significant attention over the past few decades due to the rapidly increasing dimensionality of data, as well as the associated computational, storage, and noise challenges [22, 15]. Successfully identifying the true informative features among inputs can significantly enhance the performance of analysis frameworks and drive advancements in fields such as biology, neuroscience, medicine, economics, and social sciences [15, 55]. However, the task of accurately pinpointing informative features is often considered nearly impossible, particularly due to the limited availability of data relative to the increasing dimensionality [55, 11]. A practical approach to address this challenge is the development of algorithms designed to select features while maintaining controlled error rates. Targeting this goal, Model-X knockoffs, a novel framework, is proposed in [4, 11] to select relevant features while controlling the false discovery rate (FDR). In contrast to the classical setup, where assumptions on the correlations between input features and the response are imposed [6, 19], the Model-X knockoff framework only requires a linear relationship between the response and the features. With a strong finite-sample FDR guarantee, Model-X knockoff saw broad applications in domains such as biology, neuroscience, and medicine, where the sample size is limited [4, 11, 27, 47, 55, 38, 62]. ", "page_idx": 0}, {"type": "text", "text": "There have been considerable developments of knockoffs since its debut. In scenarios where feature distributions are complex, various deep learning methods [27, 47, 55, 38, 62] have been proposed. However, we observe major limitations despite improved performance. First, the performance of existing methods varies across different data distributions. Second, the quality of selection declines when the sample size is relatively small. Third, training the deep knockoff generation models can be challenging due to competing loss terms in the training objective. We elaborate on the drawbacks in Sections 2.2 and 3.2. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we address these issues by proposing the Deep Dependency Regularized Knockoff (DeepDRK), a deep learning-based pipeline. We formulate the knockoff generation as an adversarial attack problem involving multiple sources. By optimizing a model against the adversarial environments, we achieve better \u201cswap property\u201d [4] compared to the baseline models. DeepDRK is also equipped with a novel perturbation technique to reduce \u201creconstructability\u201d [54], which in turn controls FDR and boosts selection power. The experiments conducted on synthetic, semi-synthetic, and real datasets demonstrate that our pipeline outperforms existing methods across various scenarios. ", "page_idx": 1}, {"type": "text", "text": "2 Background and Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Model-X Knockoffs for FDR control ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The Model-X knockoffs framework consists of two main components. Given the explanatory variables $X=(X_{1},X_{2},\\ldots,X_{p})^{\\top}\\in\\mathbb{R}^{p}$ and the response variable $Y$ ( $Y$ continuous for regression and categorical for classification), the framework requires: 1. a knockoff $\\tilde{X}=(\\tilde{X}_{1},\\tilde{X}_{2},\\ldots,\\tilde{X}_{p})^{\\top}$ that \u201cfakes\u201d $X;2$ . the knockoff statistics $w_{j}$ for $j\\in[p]$ that assess the importance of each feature $X_{j}$ . The knockoff $\\tilde{X}$ is required to be independent of $Y$ conditioning on $X$ , and must satisfy the swap property: ", "page_idx": 1}, {"type": "equation", "text": "$$\n(X,\\tilde{X})_{\\mathrm{swap}(B)}{\\overset{d}{=}}(X,\\tilde{X}),\\quad\\forall B\\subset[p].\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Here swap $(B)$ exchanges the positions of any variable $X_{j},j\\in B$ , with its knockoff $\\tilde{X}_{j}$ . The knockoff statistic $w_{j}((X,{\\tilde{X}}),Y)$ (for $j\\in[p];$ ) depends on the concatenated variable $(X,{\\tilde{X}})$ and $Y$ and must satisfy the flip-sign property: ", "page_idx": 1}, {"type": "equation", "text": "$$\nw_{j}\\left((X,\\tilde{X})_{\\mathrm{swap}(B)},Y\\right)=\\left\\{\\begin{array}{l l}{w_{j}((X,\\tilde{X}),Y)\\operatorname{if}j\\notin B}\\\\ {-w_{j}((X,\\tilde{X}),Y)\\operatorname{if}j\\in B}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The functions $w_{j}(\\cdot)$ with $j\\in[p]$ have many candidates, for example $w_{j}\\,=\\,|\\beta_{j}|-|\\tilde{\\beta}_{j}|$ , where $\\beta_{j}$ and $\\tilde{\\beta}_{j}$ are the corresponding regression coefficients of $X_{j}$ and $\\tilde{X}_{j}$ with the regression function $\\begin{array}{r}{Y=\\dot{\\sum}_{j=1}^{p}(X_{j}\\beta_{j}+\\tilde{X}_{j}\\tilde{\\beta}_{j})+\\epsilon}\\end{array}$ , where $\\epsilon$ is independently drawn from the normal distribution. ", "page_idx": 1}, {"type": "text", "text": "Whe n the two knockoff conditions (i.e., Eq. (1) and (2)) are met, one can select features by ${\\boldsymbol{S}}=$ $\\{w_{j}\\geqslant\\tau_{q}\\}$ , where ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\tau_{q}=\\operatorname*{min}_{t>0}\\Big\\{t:\\frac{1+|\\{j:w_{j}\\leqslant-t\\}|}{\\operatorname*{max}(1,|\\{j:w_{j}\\geqslant t\\}|)}\\leqslant q\\Big\\}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "To assess the feature selection quality, FDR is commonly used as an average Type I error of selected features [4], which is defined as follows. Let ${\\mathcal{S}}\\subset[p]$ be an arbitrary set of selected indices, and let $\\beta^{*}$ denote the underlying true regression coefficients. The FDR for the selection $\\boldsymbol{S}$ is ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname{FDR}=\\mathbb{E}\\left[{\\frac{\\#\\left\\{j:\\beta_{j}^{*}=0{\\mathrm{~and~}}j\\in S\\right\\}}{\\#\\{j:j\\in S\\}\\vee1}}\\right]\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The control of FDR is guaranteed by the following theorem from [11]: ", "page_idx": 1}, {"type": "text", "text": "Theorem 2.1. Given the knockoff that satisfies the swap property in Eq. (1), the knockoff statistic that satisfies Eq. (2), and $S=\\{w_{j}\\geqslant\\tau_{q}\\}$ , we have $\\mathrm{FDR}\\leqslant q$ . ", "page_idx": 1}, {"type": "text", "text": "2.2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Model-X knockoff is first studied under Gaussian design. Namely, the original variable $\\ensuremath{\\boldsymbol{X}}\\sim\\mathcal{N}(\\ensuremath{\\boldsymbol{\\mu}},\\ensuremath{\\boldsymbol{\\Sigma}})$ with $\\mu$ and $\\Sigma$ known. Since Gaussian design does not naturally generalize to complex data distributions, several methods are proposed to weaken the assumption. Among them, model-specific ones such as AEknockoff [34], Hidden Markov Model (HMM) knockoff [51], and MASS [20] all propose parametric alternatives to Gaussian design. These methods can better learn the data distribution, while keeping the sampling process relatively simple. Nevertheless, they pose assumptions to the design distribution, which can be problematic if actual data does not coincide. To gain further flexibility, various deep-learning-based models are developed to generate knockoffs from distributions beyond parametric setup. DDLK [55] and sRMMD [38] utilize different metrics to measure the distances between the original and the knockoff covariates. They apply different regularization terms to impose the \u201cswap property\u201d. He et al. [24] introduced a KnockoffScreen procedure to generate multiple knockoffs to improve the stability by minimizing the variance during knockoff construction. KnockoffGAN [27] and Deep Knockoff [47] take advantage of the deep learning structures to create likelihood-free generative models for the knockoff generation. ", "page_idx": 1}, {"type": "image", "img_path": "ibKpPabHVn/tmp/08571cf55346f3c3a2dd79fc9f9a4e925c7a4fdeedb733f7c0601635a6ff9c39.jpg", "img_caption": ["Figure 1: The illustration of the DeepDRK pipeline, which consists of two components: 1. the training stage that optimizes the knockoff Transformer and swappers by $\\mathcal{L}_{\\mathrm{SL}}$ and $\\mathcal{L}_{\\mathrm{DRL}}$ ; 2. the post-training stage that generates the knockoff $\\tilde{X}^{\\mathrm{DRP}_{\\theta}}$ via dependency regularized perturbation. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Despite the flexibility to learn the data distribution, deep-learning-based models suffer from a major drawback. Knockoff generations based on distribution-free sampling methods such as generative adversarial networks (GAN) [21, 3] tend to overfit, namely to learn the data $X$ exactly. The reason is that the notion of swap property for continuous distributions is not well defined at the sample level. To satisfy the swap property, one needs to independently sample $\\tilde{X}_{j}$ from the conditional law $P_{X_{j}}(\\cdot|X_{-j})$ , where $X_{-j}$ denotes the vector $(X_{1},\\dotsc,X_{j-1},X_{j+1},\\dotsc,\\bar{X}_{p})$ . At the sample level, each realization of $X_{-j}=x_{-j}^{i}$ is almost surely different and only associates to one corresponding sample $X_{j}=x_{j}^{i}$ , causing the conditional law to degenerate to sum of Diracs. As a result, minimizing the distance between $(X,{\\tilde{X}})$ and $(X,{\\tilde{X}})_{\\operatorname{swap}(B)}$ will push $\\tilde{X}$ towards $X$ and introduce high collinearity that makes the feature selection powerless, i.e., with high type II error. To tackle this issue, DDLK [55] suggests an entropic regularization. Yet it still lacks power and is computationally expensive. ", "page_idx": 2}, {"type": "text", "text": "2.3 Boost Power by reducing reconstructability ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The issue of lacking power in the knockoff selection is solved in the Gaussian case [54]. Assuming the knowledge of both mean and covariance of $\\boldsymbol{X}\\sim\\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$ , the swap property is easily satisfied by setting $\\tilde{X}_{j}\\;\\sim\\;\\mathcal{N}(\\mu_{j},\\Sigma_{j j})$ and $\\Sigma_{i j}\\;=\\;\\mathrm{Var}(X_{i},\\tilde{X}_{j})$ , for $i\\;\\neq\\;j,\\;i,j\\;\\in\\;[p]$ . Barber & Cand\\`es [4] originally propose to minimize $\\operatorname{Var}(X_{j},{\\tilde{X}}_{j})$ for all $j\\,\\in\\,[p]$ using semi-definite programming (SDP), to prevent $\\tilde{X}_{j}$ to be highly correlated with $X_{j}$ . However, Spector & Janson [54] observed that the SDP knockoff still lacks feature selection power, as merely decorrelating $X_{j}$ and $\\tilde{X}_{j}$ is not enough, and $(X,{\\tilde{X}})$ can still be (almost) linearly dependent in various cases. This is referred to as high reconstructability 1 in their paper, which can be considered as a population counterpart of collinearity (see Appendix A for more details). To tackle the problem, [54] proposed to maximize the expected conditional variance $\\mathbb{E}\\mathrm{Var}(X_{j}\\mid X_{-j},\\tilde{X})$ , which admits close-form solution whenever $X$ is Gaussian. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "DeepDRK in Figure 1 provides a novel way to generate knockoff $\\tilde{X}$ while reducing the reconstructability (see Section 2.3) between the generated knockoff $\\tilde{X}$ and the input $X$ for data with complex distributions. The generated knockoff can then be used to perform FDR-controlled feature selection following the Model-X knockoff framework (see Section 2.1). Overall, DeepDRK contains two main components. It first trains a transformer-based deep learning model, referred to as Knockoff Transformer (KT), to obtain the swap property and reduce the reconstructability of the generated knockoff. This is achieved by incorporating adversarial attacks with multi-swappers. Secondly, a dependency regularized perturbation technique (DRP) is developed to further reduce the reconstructability for $\\tilde{X}$ post training. We will elaborate on these two components in the following two subsections. In this section, we slightly abuse the notation such that $X$ and X\u02dc also denote the corresponding data matrices. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3.1 Training with Knockoff Transformer and Swappers ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The KT aims to generate knockoffs, denoted by $\\tilde{X}_{\\theta}$ , which are parameterized by a transformer network with parameters $\\theta$ . The loss for training the KT contains a swap loss (SL) $\\mathcal{L}_{\\mathrm{SL}}$ , which enforces the swap property, and a dependency regularization loss (DRL) $\\mathcal{L}_{\\mathrm{DRL}}$ , which controls the ,c tairbei luitsye do ft ot htees tk nwohcektohfefr. $K$ dgiefnfeerreatnet dn kenuroaclk nofeftsw soartiks fpya rtahem setwearipz perdo spweratpy.p eTrhs,u sd, etnhoe teKdT  biys $\\{S_{\\omega_{i}}\\}_{i=1}^{K}$   \ntrained adversarially according to the following objective, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\operatorname*{max}_{\\omega_{1},\\dots,\\omega_{K}}\\big\\{\\mathcal{L}_{\\mathrm{SL}}(X,\\tilde{X}_{\\theta},\\{S_{\\omega_{i}}\\}_{i=1}^{K})+\\mathcal{L}_{\\mathrm{DRL}}(X,\\tilde{X}_{\\theta})\\big\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that we use $\\tilde{X}_{\\theta}$ and $\\tilde{X}$ interchangeably; however, the former emphasizes that the knockoff depends on the model weights $\\theta$ . We discuss each loss function below. Details on the network architectures for KT and swappers are deferred to Appendix B.1. The training algorithm is in Appendix B.2. ", "page_idx": 3}, {"type": "text", "text": "3.1.1 Swap Loss ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The swap loss is designed to enforce the swap property and is defined as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{SL}}(X,\\tilde{X}_{\\theta},\\{S_{\\omega_{i}}\\}_{i=1}^{K})=\\displaystyle\\frac{1}{K}\\sum_{i=1}^{K}\\mathrm{SWD}((X,\\tilde{X}_{\\theta}),(X,\\tilde{X}_{\\theta})_{S_{\\omega_{i}}})}\\\\ &{~+\\lambda_{1}\\cdot\\mathrm{REx}(X,\\tilde{X}_{\\theta},\\{S_{\\omega_{i}}\\}_{i=1}^{K})+\\lambda_{2}\\cdot\\mathcal{L}_{\\mathrm{swapper}}(\\{S_{\\omega_{i}}\\}_{i=1}^{K}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\lambda_{1}$ and $\\lambda_{2}$ are hyperparameters. ", "page_idx": 3}, {"type": "text", "text": "The first term in Eq. (6) uses the sliced Wasserstein distance (SWD, see Appendix $\\mathbf{C}$ for definition) to measure the distance between a pair of joint distributions for $(X,{\\tilde{X}}_{\\theta})$ and $(X,\\tilde{X}_{\\theta})_{S_{\\omega_{i}}}$ , where the swapper is parameterized by $\\omega_{i}$ . We sum over $K$ SWDs computed for the distributions modified under different swappers to capture the effects of multiple swap attacks. We utilize SWD to compare distributions because it excels in handling complex data distributions and is computationally efficient [13, 28, 14]. ", "page_idx": 3}, {"type": "text", "text": "Sudarshan et al. [55] introduced a single swap attack parameterized by a neural network. However, we observe that minimizing the worst case swap for all $\\bar{B}\\subset[p]$ , as suggested by [55], cannot guarantee the swap property of $\\tilde{X}_{\\theta}$ . To address this limitation, we introduce a \u201cmulti-swapper\u201d setup that uses multiple swappers to enforce the swap property. And this leads to the introduction of the second and the third terms in the objective in Eq. (6). ", "page_idx": 3}, {"type": "text", "text": "The second term in Eq. (6), $\\mathrm{REx}(X,\\tilde{X}_{\\theta},\\{S_{\\omega_{i}}\\}_{i=1}^{K})$ evaluates the variance of the sliced Wasserstein distances $\\mathrm{SWD}((X,\\tilde{X}_{\\theta}),(X,\\tilde{X}_{\\theta})_{S_{\\omega}})$ with $K$ realizations of $\\omega$ [30]. When $\\mathrm{REx}(X,\\tilde{X}_{\\theta},\\{S_{\\omega_{i}}\\}_{i=1}^{K})=$ 0, the sliced Wasserstein distances are identical across all swappers. Therefore, complementary to the first term, minimizing this term improves the adherence of swap property for the generated $\\tilde{X}_{\\theta}$ . ", "page_idx": 3}, {"type": "text", "text": "The third term in Eq. (6) is introduced to avoid mode collapse on the parameters $\\omega_{i}$ of different swappers and ensure each swapper characterizes a different adversarial environment: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{swapper}}\\big(\\{S_{\\omega_{i}}\\}_{i=1}^{K}\\big)=\\frac{1}{|C|}\\sum_{(i,j)\\in C}\\mathsf{s i m}(S_{\\omega_{i}},S_{\\omega_{j}}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $C=\\{(i,j)|i,j\\in[K],i\\neq j\\}$ , and $\\mathrm{sim}(\\cdot,\\cdot)$ is the cosine similarity between the weights $\\omega$ of a pair of different swappers. Without this regularization, all swappers could collapse to a single mode such that the multi-swapper scheme reduces to a single-swapper setup. ", "page_idx": 4}, {"type": "text", "text": "Overall, the swap loss $\\mathcal{L}_{\\mathrm{SL}}$ enforces the swap property via the novel multi-swapper design. Such design provides a more robust assurance of the swap property through multiple adversarial swap attacks, which is shown in the ablation studies in Appendices I.2 and I.3. ", "page_idx": 4}, {"type": "text", "text": "3.1.2 Dependency Regularization Loss ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As discussed in Section 2.2, pursuing the swap property at the sample level often leads to severe overftiting of $\\tilde{X}_{\\theta}$ , i.e., pushing $\\tilde{X}_{\\theta}$ towards $X$ , which results in high collinearity in feature selection. To address this, the DRL is introduced to reduce the reconstructability between $X$ and X\u02dc: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{DRL}}(X,\\tilde{X}_{\\theta})=\\lambda_{3}\\cdot\\mathrm{SWC}(X,\\tilde{X}_{\\theta}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda_{3}$ is a hyperparameter. The SWC term in Eq. (8) refers to the sliced Wasserstein correlation [33], which quantitatively measures the dependency between two random vectors in the same space. More specifically, let $Z_{1}$ and $Z_{2}$ be two $p$ -dimensional random vectors. $\\operatorname{SWC}(Z_{1},Z_{2})=0$ indicates that $Z_{1}$ and $Z_{2}$ are independent, while $\\operatorname{SWC}(Z_{1},Z_{2})\\,=\\,1$ suggests a linear relationship between each other (see Appendix $\\mathrm{D}$ for more details on SWC). In DeepDRK, we minimize SWC to reduce the reconstructability, a procedure similar to [54]. The intuition is as follows. If the joint distribution of $X$ is known, then for each $j\\in[p]$ , the knockoff $\\tilde{X}_{j}$ should be sampled from $P_{j}(\\cdot|X_{-j})$ , making $X_{j}$ and $\\tilde{X}_{j}$ less dependent. In such case the swap property is ensured, and collinearity/reconstructability is reduced due to independence. As we do not have access to the joint law, we want the variables to be less dependent. Since collinearity exists with in $X$ , merely decorrelate $X_{j}$ and $\\tilde{X}_{j}$ is not enough. Thus, we minimize SWC to reduce the dependence between $X$ and $\\tilde{X}$ . We refer readers to Appendix A for more discussions. ", "page_idx": 4}, {"type": "text", "text": "3.2 Dependency Regularization Perturbation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Empirically we observe a competition between $\\mathcal{L}_{\\mathrm{SL}}$ and $\\mathcal{L}_{\\mathrm{DRL}}$ in Eq. (5), which adds difficulty to the training procedure. Specifically, the $\\mathcal{L}_{\\mathrm{SL}}$ is dominating and the $\\mathcal{L}_{\\mathrm{DRL}}$ increases quickly after a short decreasing period. We are the first to observe this phenomenon in all deep-learning based knockoff generation models when one tries to gain power [47, 55, 38, 27]. We include the experimental evidence in Appendix E. We suggest the following explanation: minimizing the swap loss, which corresponds to FDR control, is the same as controlling Type I error. Similarly, minimizing the dependency loss is to control Type II error. With a fixed number of observations, it is well known that Type I error and Type II error can not decrease at the same time after reaching a certain threshold. In the framework of model-X knockoff, we aim to boost as much power as possible given the FDR is controlled at a certain level, a similar idea as the uniformly most powerful (UMP) test [12]. For this reason, we propose DRP as a post-training technique to further boost power. ", "page_idx": 4}, {"type": "text", "text": "DRP is a sample-level perturbation that further eliminates dependency between $X$ and and the knockoff. More specifically, DRP perturbs the generated $\\tilde{X}_{\\theta}$ with the row-permuted version of $X$ , denoted as $X_{\\tt r p}$ . After applying DRP, the final knockoff $\\tilde{X}_{\\theta,n}^{\\mathrm{DRP}}$ becomes: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{X}_{\\theta,n}^{\\mathrm{DRP}}=(1-\\alpha_{n})\\cdot\\tilde{X}_{\\theta}+\\alpha_{n}\\cdot X_{\\mathrm{rp}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\alpha_{n}$ is a preset perturbation weight, $n$ is the sample size, and $\\alpha_{n}\\,\\rightarrow\\,0$ when $n\\,\\rightarrow\\,\\infty$ . In the following, we remove $n$ or $\\theta$ whenever the context is clear. $\\tilde{X}^{\\mathrm{DRP}}$ has a smaller SWC with $X$ , since $X_{\\mathfrak{r}\\mathfrak{p}}$ is independent of $X$ . Despite the perturbation increases the swap loss, such impact is negligible when the sample size is large. More specifically, we present the following Lemma 3.1 and Proposition 3.2. Let $\\hat{P}_{n}$ and $\\hat{P}_{n}^{B}$ denote the empirical joint distribution of the sample $X$ and $\\tilde{X}$ before and after the swap: $(X,{\\tilde{X}})\\sim{\\hat{P}}_{n}$ , $(X,\\tilde{X})_{\\mathrm{swap}(B)}\\,\\sim\\,\\hat{P}_{n}^{B}$ . And $P$ and $P^{B}$ denote their corresponding population distributions. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.1. Under mild conditions[41], the slice Wasserstein distance between the empirical distributions of $(X,{\\tilde{X}})$ and $(X,{\\tilde{X}})_{\\operatorname{swap}(B)}$ and their corresponding population distributions is of scale $O(n^{-1/2})$ , i.e., $S W D(\\hat{P}_{n},\\hat{P}_{n}^{B})=S W D(P,P^{B})+\\mathcal{O}(n^{-1/2})$ . ", "page_idx": 4}, {"type": "image", "img_path": "ibKpPabHVn/tmp/710110b6611aa942aa34905108c8afb36cb04fcdb346d5f06418e387f6b5cb68.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 2: Power and FDR for different knockoff models on the synthetic datasets with $\\begin{array}{r}{\\beta\\sim\\frac{p}{15\\cdot\\sqrt{N}}}\\end{array}$ Rademacher(0.5). The red horizontal line indicates the 0.1 FDR threshold. ", "page_idx": 5}, {"type": "text", "text": "Proposition 3.2. Let $\\alpha_{n}$ \u00c0 $O(n^{-1/2})$ in Eq. (9), and denote $(X,\\tilde{X}^{D R P})\\quad\\sim\\quad\\hat{P}_{n,D R P},$ , $(X,\\tilde{X}^{D R P})_{\\mathrm{swap}(B)}\\sim\\hat{P}_{n,D R P}^{B}$ . Then $S W D(\\hat{P}_{n,D R P},\\hat{P}_{n,D R P}^{B})=S W D(P,P^{B})$ when $n\\to\\infty$ . ", "page_idx": 5}, {"type": "text", "text": "The proofs can be found in Appendix F. Lemma 3.1 suggests a general case where the swap loss enforced at the sample level differs from that of the population level from a term of scale $n^{-1/2}$ , which vanishes when $n\\to\\infty$ . Proposition 3.2 provides the rationale on the proposal of the perturbation technique in Eq. (9), such that the DRP term is negligible asymptotically. Besides the theoretical justification, we also empirically show in Appendix G that the perturbation is beneficial. ", "page_idx": 5}, {"type": "text", "text": "4 Experiment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We assess the performance of DeepDRK against several benchmark models under three experimental setups: 1. a fully synthetic setup where both the input variables and the response variable follow predefined distributions; 2. a semi-synthetic setup, in which the input variables are derived from realworld datasets and the response variable is generated based on known relationships with the inputs; and 3. feature selection (FS) using a real-world dataset. The experiments are designed to encompass a range of datasets with varying $p/n$ ratios and distributions of $X$ , aiming to provide a comprehensive evaluation of model performance. The benchmark models are Deep Knockoff [47] 2, DDLK [55] 3, KnockoffGAN [27] 4 and sRMMD [38] 5, with links of the code implementation listed in the footnote. The implementation details for the training, feature selection, and data preparation are available in Appendix H. In the following, we describe the datasets and the associated experimental results. We also consider the ablation study to illustrate the benefits obtained by having the following proposed terms: REx, $\\mathcal{L}_{\\mathrm{{swapper}}}$ , the multi-swapper setup, and the dependency regularization perturbation. Empirically, these terms help to improve power and control the FDR. Due to space limitation, we defer details for the ablation studies to Appendix I.2 and I.3. DeepDRK is implemented in PyTorch [45] and is accessible at: https://github.com/nowonder2000/DeepDRK. ", "page_idx": 5}, {"type": "text", "text": "4.1 The Synthetic Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To properly evaluate the performance, we follow a well-designed experimental setup by [55, 38] to generate different datasets specified by $(X,Y)$ . Here $X\\,\\in\\,\\mathbb{R}^{p}$ is the collection dependent variables that follows pre-defined distributions. $Y\\ \\in\\ \\mathbb{R}$ is the response variable that is modeled as $Y\\sim$ $\\mathcal{N}(X^{T}\\beta,1)$ . The underlying true $\\beta$ is a $p$ -dimensional vector, where each entry is drawn independently from the distribution15\u00a8p?n \u00a8 Rademacher(0.5). ", "page_idx": 5}, {"type": "image", "img_path": "ibKpPabHVn/tmp/60e9bae6839f9471241bd358b16876d583c26ce6bbd3402ba96f4b79e559fb59.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 3: Power and FDR for different knockoff models on the mixture of Gaussian data on different $\\rho_{\\mathrm{base}}$ setups. The red horizontal line indicates the 0.1 FDR threshold. This figure is complementary to Figure 2 for including two additional Gaussian mixture data with higher $\\rho_{\\mathrm{base}}$ values. ", "page_idx": 5}, {"type": "table", "img_path": "ibKpPabHVn/tmp/7ad366322981e76b93fcc01448eb8d342aa10c9953f9fd760a35e2342fee59cf.jpg", "table_caption": [], "table_footnote": ["Table 1: Comparison of different methods on FDR and power across different $\\overline{{\\left(\\pi_{1},\\pi_{2},\\pi_{3}\\right)}}$ for the components in the Gaussian mixture setup. "], "page_idx": 6}, {"type": "text", "text": "Compared to the previous works [55, 38], which consider $\\textstyle{\\frac{p}{\\sqrt{n}}}$ as the scaling factor for the Rademacher distribution, we reduce the magnitude of $\\beta$ by a factor of 15. This is because we find that in the original setup, the $\\beta$ scale is too large such that the feature selection enjoys high powers and low FDRs for all models. To compare the performance of the knockoff generation methods on various data, we consider the following distributions for $X$ : ", "page_idx": 6}, {"type": "text", "text": "Gaussian mixture: We consider a Gaussian mixture model X \u201e 3k $\\begin{array}{r}{X\\sim\\sum_{k=1}^{3}\\pi_{k}{\\mathcal N}(\\mu_{k},\\Sigma_{k})}\\end{array}$ , where $\\pi$ is the proportion of the $k$ -th Gaussian with $(\\pi_{1},\\pi_{2},\\pi_{3})\\:=\\:(0.4,0.{\\overline{{{2,}}}}{\\overset{..}{0}}.4{\\overline{{{)}}}}$ . $\\mu_{k}\\,\\in\\,\\mathbb{R}^{p}$ denotes the mean of the $k$ -th Gaussian with $\\begin{array}{r}{\\mu_{k}=\\mathbf{1}_{p}\\cdot20\\cdot(k-1).}\\end{array}$ , where ${\\bf1}_{p}$ is the $p$ -dimensional vector that has universal value 1 for all entries. $\\bar{\\Sigma_{k}}\\in\\mathbb{R}^{p\\times p}$ is the covariance matrices whose $(i,j)$ -th entry taking the value $\\rho_{k}^{\\vert i-j\\vert}$ , where $\\rho_{k}\\,=\\,\\rho_{\\mathrm{base}}^{k-0.1}$ and $\\rho_{\\mathrm{{base}}}\\,=\\,0.6$ . Besides this experiment, we further perform two additional tests. The first one focuses on a mixture of Gaussians data of various to study the feature selection performance with highly correlated features. Namely, we consider additional $\\rho_{\\mathrm{base}}\\in\\{0.7,0.8\\}$ . The second one explores the effect of $(\\pi_{1},\\pi_{2},\\pi_{3})$ to the feature selection performance. In this case, we uniformly draw 10 sets of $(\\pi_{1},\\pi_{2},\\pi_{3})$ , and evaluate the FS performance of all the models considered in this paper. The values of the mixture weights are presented in Table 4 in Appendix H.3.1. ", "page_idx": 6}, {"type": "text", "text": "Copulas [49]: We further use copula to model complex correlations within $X$ . To the best of our knowledge, this is a first attempt to consider complex distributions other than the Gaussian mixture model in the knockoff framework. Specifically, we consider two copula families: Clayton, Joe with the consistent copula parameter of 2 in both cases. For each family, we consider two candidates for the marginal distributions: a uniform distribution (using the identity conversion function) and an exponential distribution with a rate of 1. We implement the copulas according to PyCop 6. ", "page_idx": 6}, {"type": "text", "text": "We consider the following $(n,p)$ setups: p200, 100q and p2000, 100q. This is in contrast to existing works, which consider only the p2000, 100q as the smallest sample size setup [55, 38, 47]. Our goal is to demonstrate the consistent performance of DeepDRK across various $p/n$ ratios, particularly when the sample size is small. ", "page_idx": 6}, {"type": "text", "text": "Results: Figure 2 compare FDRs and powers for all models across the datasets with two different setups for $\\beta$ . Figure 2 shows that DeepDRK consistently controls the false discovery rate (FDR) compared to other benchmark models across various data distributions and $p/n$ ratios, with the exception of a few cases where the FDR exceeds the 0.1 threshold in the small sample size $n=200)$ ) scenarios. Other models, though being able to reach higher power, comes at a cost of sacrificing FDR, which contradicts to the UMP philosophy (see Section 3.2). We also evaluate the performance on a mixture of Gaussians with increasing $\\rho_{\\mathrm{base}}$ , indicating a higher correlation among the input variables. Note that the mixture of Gaussians example in Figure 2 has $\\rho_{\\mathrm{{base}}}\\,=\\,0.6$ . The results for $\\rho_{\\mathrm{base}}\\,\\in\\,\\{0.7,0.8\\}$ are presented in Figure 3. Compared to other baseline models, DeepDRK maintains the lowest FDRs while achieving competitive powers across all $\\rho_{\\mathrm{base}}$ values, highlighting its robustness to correlations in $X$ . Overall, the results demonstrate the ability of DeepDRK in consistently performing FS with controlled FDR compared to other models across a range of different datasets, $p/n$ ratios, and feature correlations in $X$ . In addition, we present common statistics for the FDR and power on the mixture of Gaussians experiment with 10 different sets of $(\\pi_{1},\\pi_{2},\\pi_{3})$ in Table 1. It is clear that DeepDRK improves the robustness in maintaining FDR across various configurations of the Gaussian mixture models compared to existing approaches. ", "page_idx": 6}, {"type": "image", "img_path": "ibKpPabHVn/tmp/f380f26d7c0979e868409481b81c632d3a33b3f09c33b3f312beed8c72f15aad.jpg", "img_caption": ["Figure 4: The knockoff statistics $(w_{j})$ for different knockoff models on the synthetic datasets with $\\begin{array}{r}{\\beta\\stackrel{!}{\\sim}\\frac{p}{15\\cdot\\sqrt{N}}}\\end{array}$ \u00a8 Rademacher(0.5). Each bar in the plot represents the mean of the null/nonnull knockoff statistics averaging on 600 experiments. The error bar indicates the standard deviation. The sample size is 200. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "ibKpPabHVn/tmp/4b6c41d383ddf0d5e8d63bf6aeda1ff3e2c2290d1eae350a859c38f623186d92.jpg", "img_caption": ["Figure 5: Scatter plots of Power against FDR for different datasets and models. The red vertical line indicates the 0.1 FDR threshold. Different scales for \u03b2 (e.g.,5\u00a8p?n,10\u00a8p?n,15\u00a8p?n and $\\textstyle{\\frac{p}{20\\cdot{\\sqrt{n}}}}\\rangle$ ) are indicated by different marker styles. Different models are indicated by different colors. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "To understand why DeepDRK outperforms other baseline models, we consider measuring the distribution of the knockoff statistics, i.e., $w_{j}$ , for both nonnull and null features of $X$ . Fan et al. [17] and Cande\\`s et al. [11] pointed out that a good knockoff requires the corresponding knockoff statistics to concentrate symmetrically around zero for the null features and to maintain high positive values for the nonnulls. However, theoretical analysis on the goodness of FDR or power requires access to the true knockoff $\\tilde{X}$ [17] to compare the distribution of $w_{j}$ \u2019s with the ground truth, which is infeasible for non-Gaussian data. Nevertheless, we can still examine the distribution of the knockoff statistics as a surrogate to analyze model performance in terms of false discovery rate (FDR) or power, given the necessary properties of the knockoff statistics mentioned earlier. ", "page_idx": 7}, {"type": "text", "text": "Figure 4 shows the means and standard deviations of the empirical distributions of the knockoff statistics $w_{j}$ for both null and nonnull variables across different datasets and models. Clearly, compared to other benchmarks, DeepDRK maintains high values of $w_{j}$ for the nonnulls and relatively symmetric values around zero for the nulls. All other models experience positive shifts to the null statistics to some extent. Positive shifts in the null statistics lead to a degeneracy in performance because the threshold selection rule for false discovery rate (FDR) is based on the negative values of $w_{j}$ \u2019s (see Eq. (3)). This has two negative impacts, one on the FS threshold and the other on its subsequent FS process. First, according to Eq. (3), the shift causes the chosen threshold to approach zero, as there are fewer null statistics remaining on the negative side, and those that do remain have smaller amplitudes. Subsequently, a lowered threshold leads to an increase in false positives, a phenomenon that becomes more pronounced with positive shifts in the null statistics. As shown in Figure 4, the $w_{j}$ values calculated using DeepDRK are centered around zero for the nulls, while exhibiting large positive values for the nonnulls. This aligns with the results in Figure 2, which demonstrate that DeepDRK effectively achieves good FDR control and high power. Due to space limit, we defer the results for $n=2000$ and the comparison of $w_{j}$ statistics for the Gaussian correlation setup to Appendix I.3. ", "page_idx": 7}, {"type": "text", "text": "To further verify the performance consistency of the proposed method, we include a comparison on different $\\beta$ scales. Specifically, we consider 4 different sets of $\\beta$ scales, i.e., $\\scriptstyle{\\frac{p}{5\\cdot{\\sqrt{n}}}},\\;{\\frac{p}{10\\cdot{\\sqrt{n}}}},\\;{\\frac{p}{15\\cdot{\\sqrt{n}}}}$ and $\\textstyle{\\frac{p}{20\\cdot{\\sqrt{n}}}}$ , for the previously considered data distributions. The results are summarized in Figure 5. It is observed that in all cases considered with various $\\beta$ scale, the proposed DeepDRK successfully maintains relatively low FDRs with a higher power, compared to baseline methods. This phenomenon is especially pronounced with a low sample size $n=200)$ ). ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "In addition to the above results, we provide the measurement of the swap property in Appendix I.1.   \nThe evaluation of model runtime is also included in Appendix I.4. ", "page_idx": 8}, {"type": "text", "text": "4.2 The Semi-synthetic Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We consider a semi-synthetic study with design $X$ drawn from two real-world datasets and use $X$ to simulate response $Y$ . The first dataset contains single-cell RNA sequencing (scRNA-seq) data from $\\mathrm{10\\timesGe-}$ nomics 7. Each entry in $X\\,\\in\\,\\mathbb{R}^{n\\times p}$ represents the observed gene expression of $p$ genes in $n$ cells. We refer readers to [23] and [1] for more background. Following the same preprocessing in [23], we obtain the final dataset $X$ with $n\\,=\\,10000$ and $p\\,=\\,100^{\\mathrm{~8~}}$ . The preprocessing of $X$ and the synthesis of $Y$ (denoted as \u201cLinear\u201d and \u201cTanh\u201d cases) are deferred to Appendix H.3 ", "page_idx": 8}, {"type": "image", "img_path": "ibKpPabHVn/tmp/58d3608726953255fee9706b5e015bb29a36e419e943cf74de83a57752967264.jpg", "img_caption": ["Figure 6: Power and FDR for different knockoff models on the semi-synthetic RNA dataset. The red horizontal line indicates the 0.1 FDR threshold. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "The second publicly available dataset9 is from a real case study entitled \u201cLongitudinal Metabolomics of the Human Microbiome in Inflammatory Bowel Disease (IBD)\u201d [35]. The study seeks to identify important metabolites of two representative diseases of the inflammatory bowel disease (IBD): ulcerative colitis (UC) and Crohn\u2019s disease (CD). Specifically, we use the C18 Reverse-Phase Negative Mode dataset that has 546 samples and 91 metabolites. To mitigate the effects of missing values, we preprocess the dataset following a common procedure to remove metabolites that have over $20\\%$ missing values, resulting in 80 metabolites. We normalize the data matrix entry-wise to have zero mean and unit variance after a log transform and an imputation via the $k$ -nearest neighbor algorithm following the same procedure in [38]. Finally, we synthesize the response $Y$ with the real dataset of $X$ via $\\boldsymbol{Y}^{\\top}\\!\\sim\\mathcal{N}(\\boldsymbol{X}^{T}{\\bar{\\boldsymbol{\\beta}}},\\boldsymbol{1})$ , where the entries of $\\beta$ drawn independently from one of the following three distributions: Unif(0, 1), ${\\mathcal{N}}(0,1)$ , and Rademacherp0.5q, in three separate experiments. ", "page_idx": 8}, {"type": "text", "text": "Results: Figure 6 and 7 compare the feature selection performance on the RNA data and the IBD data respectively. In Figure 6, we observe that all but DDLK are bounded by the nominal 0.1 FDR threshold in the \u201cTanh\u201d case. However, KnockoffGAN and sRMMD have almost zero power. The power for Deep Knockoff is also very low compared to that of DeepDRK. Although DDLK provides high power, the associated FDR is not controlled by the threshold. In the \u201cLinear\u201d case, almost all models have well controlled FDR, among which DeepDRK provides the highest power. Similar observations can be found in Figure 7. For the IBD data generated under the aforementioned synthesis rules, it is clear that all models except DDLK achieve well-controlled FDR. Apart from DDLK, DeepDRK consistently demonstrates the highest power. These results further underscore the potential of DeepDRK for real-world data applications. ", "page_idx": 8}, {"type": "text", "text": "4.3 A Case Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Besides (semi-)synthetic setups, we carry out a case study with real data for both design $X$ and response $Y$ , in order to qualitatively evaluate the selection performance of DeepDRK. In this subsection, we use the IBD dataset [35] with the empirical response. The response variable $Y$ is categorical: $Y$ equals 1 if a given sample is associated with UC/CD and 0 otherwise. The covariates $X$ is identical to the second semi-synthetic setup considered in Section 4.2. To properly evaluate results with no ground truth available, we search for the evidence of the IBD-associated metabolites using the existing literature. Specifically, we use three sources: 1. metabolites that are explicitly documented to have associations with IBD, UC, or CD in the PubChem database 10; 2. metabolites that are reported in the existing peer-reviewed publications; 3. metabolites that are reported in pre-prints. We identify 47 metabolites that are reported to have association with IBD. All referenced metabolites are included in Table 5 in Appendix H.3.3. ", "page_idx": 8}, {"type": "table", "img_path": "ibKpPabHVn/tmp/08d966299e8ad50633c8488bb30e8d069c809736d932ceacfa21f27b17765b7c.jpg", "table_caption": ["Figure 7: Power and FDR for different knockoff models on the semi-synthetic IBD dataset. The red horizontal line indicates the 0.1 FDR threshold. "], "table_footnote": ["Table 2: The number of literature-supported metabolites among the identified metabolites vs. the number of identified metabolites. "], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Our DeepDRK model training and knockoff generation are the same as before (see Table 3 in Appendix H.1). Likewise, to generate knockoff for the benchmark models, we follow their default setups. During the FS step, however, we use 0.2 as the FDR threshold instead of 0.1, and apply a different algorithm\u2014DeepPINK [37] that is included in the knockpy 11 library\u2014to generate $w_{j}$ The values are subsequently used to identify metabolites. We choose DeepPINK over the previously considered ridge regression due to the nonlinear relationships between metabolites $X$ and responses $Y$ in this case study. ", "page_idx": 9}, {"type": "text", "text": "We compare the FS results with the 47 literature-supported metabolites and report the number of selections in Table 2. A detailed list of selected features for each model can be found in Table 9 in Appendix I.5. From Table 2, it is clear that, compared to the benchmark models, DeepDRK identifies the largest number of referenced metabolites while effectively limiting the number of metabolites not reported in existing literature (see Table 5 in Appendix H.3.3). The sRMMD model achieves the lowest false discovery rate, but this comes at the cost of missing a significant number of documented metabolites. Since there is no ground truth available, the results here should be viewed and analyzed qualitatively. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduce DeepDRK, a deep learning-based knockoff generation pipeline consisting of two steps. First, it trains a Knockoff Transformer with multiple swappers to achieve the swap property while reducing reconstructability. In the post-training stage, a dependency-regularized perturbation is applied to further enhance power with controlled FDR. DeepDRK effectively balances FDR and power, which compete with each other at the sample level. To the best of our knowledge, this relationship has not been previously reported in the literature. Empirically, DeepDRK demonstrates the ability to maintain both controlled false discovery rates (FDR) and high power across various data distributions and different $p/n$ ratios. Additionally, we provide insights into the distribution of knockoff statistics, which elucidate the reasons behind DeepDRK\u2019s consistently strong performance. The numerical results indicate that DeepDRK outperforms existing deep learning-based benchmark models. Experiments with real and semi-synthetic data further highlight the potential of DeepDRK for feature selection tasks involving non-Gaussian data. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research was partially supported by Alfred P. Sloan foundation and NSF #1934757. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Agarwal, D., Wang, J., and Zhang, N. R. Data denoising and post-denoising corrections in single cell RNA sequencing. Statistical Science, 35(1):112 \u2013 128, 2020.   \n[2] Ananthakrishnan, A. N., Luo, C., Yajnik, V., Khalili, H., Garber, J. J., Stevens, B. W., Cleland, T., and Xavier, R. J. Gut microbiome function predicts response to anti-integrin biologic therapy in inflammatory bowel diseases. Cell host & microbe, 21(5):603\u2013610, 2017.   \n[3] Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein generative adversarial networks. In ICML, pp. 214\u2013223, 2017.   \n[4] Barber, R. F. and Cand\\`es, E. J. Controlling the false discovery rate via knockoffs. The Annals of Statistics, 43(5):2055\u20132085, 2015.   \n[5] Bauset, C., Gisbert-Ferr\u00b4andiz, L., and Cos\u00b4\u0131n-Roger, J. Metabolomics as a promising resource identifying potential biomarkers for inflammatory bowel disease. Journal of Clinical Medicine, 10(4):622, 2021.   \n[6] Benjamini, Y. and Hochberg, Y. Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal statistical society: series B (Methodological), 57(1):289\u2013300, 1995.   \n[7] Bin Masud, S., Jenkins, C., Hussey, E., Elkin-Frankston, S., Mach, P., Dhummakupt, E., and Aeron, S. Utilizing machine learning with knockoff flitering to extract significant metabolites in Crohn\u2019s disease with a publicly available untargeted metabolomics dataset. Plos one, 16(7): e0255240, 2021.   \n[8] Blaker, P. A., Arenas-Hernandez, M., Smith, M. A., Shobowale-Bakre, E. A., Fairbanks, L., Irving, P. M., Sanderson, J. D., and Marinaki, A. M. Mechanism of allopurinol induced TPMT inhibition. Biochemical pharmacology, 86(4):539\u2013547, 2013.   \n[9] Bonneel, N., Rabin, J., Peyre\u00b4, G., and Pfister, H. Sliced and Radon Wasserstein barycenters of measures. Journal of Mathematical Imaging and Vision, 51:22\u201345, 2015.   \n[10] Bonnotte, N. Unidimensional and evolution methods for optimal transportation. PhD thesis, Universite\u00b4 Paris Sud-Paris XI; Scuola normale superiore (Pise, Italie), 2013.   \n[11] Cand\\`es, E., Fan, Y., Janson, L., and Lv, J. Panning for gold: \u2018Model-X\u2019 knockoffs for high dimensional controlled variable selection. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 80(3):551\u2013577, 2018.   \n[12] Casella, G. and Berger, R. L. Statistical inference. Cengage Learning, 2021.   \n[13] Deshpande, I., Zhang, Z., and Schwing, A. G. Generative modeling using the sliced Wasserstein distance. In CVPR, pp. 3483\u20133491, 2018.   \n[14] Deshpande, I., Hu, Y.-T., Sun, R., Pyrros, A., Siddiqui, N., Koyejo, S., Zhao, Z., Forsyth, D., and Schwing, A. G. Max-sliced Wasserstein distance and its use for GANs. In CVPR, pp. 10648\u201310656, 2019.   \n[15] Dhal, P. and Azad, C. A comprehensive survey on feature selection in the various fields of machine learning. Applied Intelligence, 52(4):4543\u20134581, 2022.   \n[16] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth $16\\!\\times\\!16$ words: Transformers for image recognition at scale. In ICLR, 2021.   \n[17] Fan, Y., Gao, L., and Lv, J. ARK: Robust Knockoffs inference with coupling. arXiv preprint arXiv:2307.04400, 2023.   \n[18] Fretland, D., Widomski, D., Levin, S., and Gaginella, T. Colonic inflammation in the rabbit induced by phorbol-12-myristate-13-acetate. Inflammation, 14(2):143\u2013150, 1990.   \n[19] Gavrilov, Y., Benjamini, Y., and Sarkar, S. K. An adaptive step-down procedure with proven FDR control under independence. The Annals of Statistics, 37(2):619\u2013629, 2009.   \n[20] Gimenez, J. R., Ghorbani, A., and Zou, J. Knockoffs for the mass: new feature importance statistics with false discovery guarantees. In AISTATS, pp. 2125\u20132133, 2019.   \n[21] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial networks. Communications of the ACM, 63(11):139\u2013144, 2020.   \n[22] Guyon, I. and Elisseeff, A. An introduction to variable and feature selection. Journal of machine learning research, 3(Mar):1157\u20131182, 2003.   \n[23] Hansen, D., Manzo, B., and Regier, J. Normalizing flows for knockoff-free controlled feature selection. In Advances in Neural Information Processing Systems, volume 35, pp. 16125\u201316137, 2022.   \n[24] He, Z., Liu, L., Wang, C., Le Guen, Y., Lee, J., Gogarten, S., Lu, F., Montgomery, S., Tang, H., Silverman, E. K., et al. Identification of putative causal loci in whole-genome sequencing data via knockoff statistics. Nature communications, 12(1):1\u201318, 2021.   \n[25] Hristache, M., Juditsky, A., and Spokoiny, V. Direct estimation of the index coefficient in a single-index model. Annals of Statistics, 29(3):595\u2013623, 2001.   \n[26] Jang, E., Gu, S., and Poole, B. Categorical reparameterization with Gumbel-softmax. In ICLR, 2017.   \n[27] Jordon, J., Yoon, J., and van der Schaar, M. KnockoffGAN: Generating knockoffs for feature selection using generative adversarial networks. In ICLR, 2018.   \n[28] Kolouri, S., Nadjahi, K., Simsekli, U., Badeau, R., and Rohde, G. Generalized sliced Wasserstein distances. In NeurIPS, volume 32, 2019.   \n[29] Koon, H. W. A novel orally active metabolite reverses Crohn\u2019s disease-associated intestinal fibrosis. Inflammatory Bowel Diseases, 28(Supplement 1):S61\u2013S62, 2022.   \n[30] Krueger, D., Caballero, E., Jacobsen, J.-H., Zhang, A., Binas, J., Zhang, D., Le Priol, R., and Courville, A. Out-of-distribution generalization via risk extrapolation (REx). In ICML, pp. 5815\u20135826, 2021.   \n[31] Lavelle, A. and Sokol, H. Gut microbiota-derived metabolites as key actors in inflammatory bowel disease. Nature reviews Gastroenterology & hepatology, 17(4):223\u2013237, 2020.   \n[32] Lee, T., Clavel, T., Smirnov, K., Schmidt, A., Lagkouvardos, I., Walker, A., Lucio, M., Michalke, B., Schmitt-Kopplin, P., Fedorak, R., et al. Oral versus intravenous iron replacement therapy distinctly alters the gut microbiota and metabolome in patients with IBD. Gut, 66(5):863\u2013871, 2017.   \n[33] Li, T., Yu, J., and Meng, C. Scalable model-free feature screening via sliced-Wasserstein dependency. Journal of Computational and Graphical Statistics, 32(4):1501\u20131511, 2023.   \n[34] Liu, Y. and Zheng, C. Auto-encoding knockoff generator for FDR controlled variable selection. arXiv preprint arXiv:1809.10765, 2018.   \n[35] Lloyd-Price, J., Arze, C., Ananthakrishnan, A. N., Schirmer, M., Avila-Pacheco, J., Poon, T. W., Andrews, E., Ajami, N. J., Bonham, K. S., Brislawn, C. J., et al. Multi-omics of the gut microbial ecosystem in inflammatory bowel diseases. Nature, 569(7758):655\u2013662, 2019.   \n[36] Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. In ICLR, 2019.   \n[37] Lu, Y., Fan, Y., Lv, J., and Stafford Noble, W. DeepPINK: reproducible feature selection in deep neural networks. In NeurIPS, volume 31, 2018.   \n[38] Masud, S. B., Werenski, M., Murphy, J. M., and Aeron, S. Multivariate soft rank via entropyregularized optimal transport: Sample efficiency and generative modeling. Journal of Machine Learning Research, 24(160):1\u201365, 2023.   \n[39] Mehta, R. S., Taylor, Z. L., Martin, L. J., Rosen, M. J., and Ramsey, L. B. SLCO1B1 $^{\\ast15}$ allele is associated with methotrexate-induced nausea in pediatric patients with inflammatory bowel disease. Clinical and translational science, 15(1):63\u201369, 2022.   \n[40] Minderhoud, I. M., Oldenburg, B., Schipper, M. E. I., Ter Linde, J. J. M., and Samsom, M. Serotonin synthesis and uptake in symptomatic patients with Crohn\u2019s disease in remission. Clinical Gastroenterology and Hepatology, 5(6):714\u2013720, 2007.   \n[41] Nadjahi, K., Durmus, A., Simsekli, U., and Badeau, R. Asymptotic guarantees for learning generative models with the sliced-Wasserstein distance. In Advances in Neural Information Processing Systems, volume 32, 2019.   \n[42] Narasimhan, R. L., Throm, A. A., Koshy, J. J., Saldanha, K. M. R., Chandranpillai, H., Lal, R. D., Kumravat, M., K. M., A. K., Batra, A., Zhong, F., et al. Inferring intestinal mucosal immune cell associated microbiome species and microbiota-derived metabolites in inflammatory bowel disease. bioRxiv, 2020.   \n[43] Nies, T. G., Staudt, T., and Munk, A. Transport dependency: Optimal transport based dependency measures. arXiv preprint arXiv:2105.02073, 2021.   \n[44] Nuzzo, A., Saha, S., Berg, E., Jayawickreme, C., Tocker, J., and Brown, J. R. Expanding the drug discovery space with predicted metabolite\u2013target interactions. Communications biology, 4 (1):1\u201311, 2021.   \n[45] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, volume 32, 2019.   \n[46] Qin, X. Etiology of inflammatory bowel disease: a unified hypothesis. World journal of gastroenterology: WJG, 18(15):1708, 2012.   \n[47] Romano, Y., Sesia, M., and Cand\\`es, E. Deep knockoffs. Journal of the American Statistical Association, 115(532):1861\u20131872, 2020.   \n[48] Saber, S., Khalil, R. M., Abdo, W. S., Nassif, D., and El-Ahwany, E. Olmesartan ameliorates chemically-induced ulcerative colitis in rats via modulating NF\u03baB and Nrf-2/HO-1 signaling crosstalk. Toxicology and applied pharmacology, 364:120\u2013132, 2019.   \n[49] Schmidt, T. Coping with copulas. Copulas-From theory to application in finance, 3:1\u201334, 2007.   \n[50] Scoville, E. A., Allaman, M. M., Brown, C. T., Motley, A. K., Horst, S. N., Williams, C. S., Koyama, T., Zhao, Z., Adams, D. W., Beaulieu, D. B., et al. Alterations in lipid, amino acid, and energy metabolism distinguish Crohn\u2019s disease from ulcerative colitis and control subjects by serum metabolomic profiling. Metabolomics, 14(1):1\u201312, 2018.   \n[51] Sesia, M., Sabatti, C., and Cand\\`es, E. J. Gene hunting with hidden Markov model knockoffs. Biometrika, 106(1):1\u201318, 08 2018.   \n[52] Soderholm, J. D., Oman, H., Blomquist, L., Veen, J., Lindmark, T., and Olaison, G. Reversible increase in tight junction permeability to macromolecules in rat ileal mucosa in vitro by sodium caprate, a constituent of milk fat. Digestive diseases and sciences, 43(7):1547\u20131552, 1998.   \n[53] S\u00a8oderholm, J. D., Olaison, G., Peterson, K. H., Franzen, L. E., Lindmark, T., Wir\u00b4en, M., Tagesson, C., and Sj\u00a8odahl, R. Augmented increase in tight junction permeability by luminal stimuli in the non-inflamed ileum of Crohn\u2019s disease. Gut, 50(3):307\u2013313, 2002.   \n[54] Spector, A. and Janson, L. Powerful knockoffs via minimizing reconstructability. The Annals of Statistics, 50(1):252\u2013276, 2022.   \n[55] Sudarshan, M., Tansey, W., and Ranganath, R. Deep direct likelihood knockoffs. In NeurIPS, volume 33, 2020.   \n[56] Suhre, K., Shin, S.-Y., Petersen, A.-K., Mohney, R. P., Meredith, D., W\u00a8agele, B., Altmaier, E., Deloukas, P., Erdmann, J., Grundberg, E., et al. Human metabolic individuality in biomedical and pharmaceutical research. Nature, 477(7362):54\u201360, 2011.   \n[57] Uchiyama, K., Odahara, S., Nakamura, M., Koido, S., Katahira, K., Shiraishi, H., Ohkusa, T., Fujise, K., and Tajiri, H. The fatty acid profile of the erythrocyte membrane in initial-onset inflammatory bowel disease patients. Digestive diseases and sciences, 58(5):1235\u20131243, 2013.   \n[58] Uko, V., Thangada, S., and Radhakrishnan, K. Liver disorders in inflammatory bowel disease. Gastroenterology research and practice, 2012(1):642923, 2012.   \n[59] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. In NeurIPS, volume 30, 2017.   \n[60] Villani, C. Optimal transport: old and new, volume 338. Springer, 2009.   \n[61] Wiesel, J. C. Measuring association with Wasserstein distances. Bernoulli, 28(4):2816\u20132832, 2022.   \n[62] Zhu, Z., Fan, Y., Kong, Y., Lv, J., and Sun, F. DeepLINK: Deep learning inference using knockoffs with applications to genomics. Proceedings of the National Academy of Sciences, 118(36):e2104683118, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Reconstructability and Selection Power ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The concept of reconstructability was introduced by [54] as a population-level counterpart to what is commonly referred to as collinearity in linear regression. Under a Gaussian design where $X\\sim$ $\\mathcal{N}(0,\\Sigma)$ , reconstructability is high if $\\Sigma$ is not of full rank, implying that there exists some $j$ such that $X_{j}$ is almost surely a linear combination of $X_{-j}$ . More generally, if there exists more than one representation of the response $Y$ using the explanatory variable $X$ , we qualitatively consider the reconstructability to be high. High collinearity often impairs statistical power, and similarly, high reconstructability diminishes feature selection power. To illustrate this, we state a linear version of Theorem 2.3 from [54], originally formulated for a more general single-index model [25]. ", "page_idx": 14}, {"type": "text", "text": "Theorem A.1. Let $Y\\;=\\;X_{J}\\beta_{J}\\,+\\,X_{-J}\\beta_{-J}\\,+\\,\\varepsilon_{\\mathrm{~\\rightmoon~}}$ , where $J\\ \\subset\\ [p]$ and $\\varepsilon$ is centered Gaussian noise. Equivalently, this implies $Y$ KK $X_{J}$ | $X_{J}\\beta_{J},X_{-J}$ . Suppose there exists a $\\beta_{J}^{*}$ such that $X_{J}\\beta_{J}=X_{J}\\beta_{J}^{*}$ almost surely. Then, denoting $Y^{*}=X_{J}\\beta_{J}^{*}+X_{-J}\\beta_{-J}+\\varepsilon,$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\bigl((X,\\tilde{X}),Y)\\stackrel{d}{=}\\Bigl((X,\\tilde{X})_{\\mathrm{swap}(J)},Y^{*}\\Bigr)}&{{}\\,a n d}\\\\ {\\Bigl((X,\\tilde{X}),Y^{*}\\Bigr)\\stackrel{d}{=}\\Bigl((X,\\tilde{X})_{\\mathrm{swap}(J)},Y\\Bigr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Furthermore, in the knockoff framework $[4J,$ , let $w=w((X,\\tilde{X}),y)$ and $w^{\\ast}=w((X,\\tilde{X}),y^{\\ast})$ . Then, for all $j\\in J$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(w_{j}>0\\right)+\\mathbb{P}\\left(w_{j}^{*}>0\\right)\\leqslant1.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Eq. (11) implies a \u201cno free lunch\u201d situation for selection power when there is exact reconstructability. To address the reconstructability issue, [54] proposed two methods in Gaussian design. The first is the minimal variance-based reconstructability (MVR) knockoff, in which the knockoff $\\tilde{X}$ is sampled to minimize the loss ", "page_idx": 14}, {"type": "equation", "text": "$$\nL_{\\mathrm{MVR}}=\\sum_{j=1}^{p}\\frac{1}{\\mathbb{E}\\left[\\mathrm{Var}\\left(X_{j}\\mid X_{-j},\\tilde{X}\\right)\\right]}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that this is equivalent to maximizing $\\mathbb{E}\\left[{\\mathrm{Var}}\\left(X_{j}\\mid X_{-j},{\\tilde{X}}\\right)\\right]$ for all $j\\in[p]$ . Another approach is the maximum entropy (ME) knockoff, where $\\tilde{X}$ is sampled to maximize ", "page_idx": 14}, {"type": "equation", "text": "$$\nL_{\\mathrm{ME}}=\\int\\int p(x,{\\tilde{x}})\\log(p(x,{\\tilde{x}}))\\,d{\\tilde{x}}\\,d x.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Under Gaussian design, both optimizations have closed-form solutions. Since $X$ is Gaussian, $(X,{\\tilde{X}})$ must also be jointly Gaussian to satisfy the swap property. To optimize, one first calculates the covariance matrix using the SDP method in [4], yielding a diagonal matrix $S$ . Then, both MVR and ME reduce to an optimization on $S$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{\\mathrm{MVR}}(S){\\boldsymbol{\\propto}}\\operatorname{Tr}\\left(G_{S}^{-1}\\right)=\\displaystyle\\sum_{j=1}^{2p}\\frac{1}{\\lambda_{j}\\left(G_{S}\\right)}}\\\\ &{\\quad\\mathrm{and}\\quad L_{\\mathrm{ME}}(S)=\\log\\operatorname*{det}\\left(G_{S}^{-1}\\right)=\\displaystyle\\sum_{j=1}^{2p}\\log\\left(\\frac{1}{\\lambda_{j}\\left(G_{S}\\right)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Although both methods show high power for feature selection, neither MVR nor ME can be directly extended to arbitrary distributions due to the intractability of conditional variance and likelihood. ", "page_idx": 14}, {"type": "text", "text": "In DeepDRK, we consider regularizing with a sliced-Wasserstein-based dependency correlation, which can be considered a stronger dependency regularization than entropy. A post-training perturbation is also applied to further reduce collinearity. However, the theoretical understanding of how these affect the swap property and power remains an open question. ", "page_idx": 14}, {"type": "text", "text": "B DeepDRK Model Architecture and Training Algorithm ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Model Architecture ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "DeepDRK\u2019s knockoff Transformer (KT) model is based on the popular Vision Transformer (ViT) [16]. The primary difference is that the input dimension is 1D, not 2D, for $X$ . We do not use patches as input; instead, we treat all entries of $X$ to account for correlations between each pair of entries in the knockoff $\\tilde{X}$ . This structure is similar to the original Transformer [59]. Nonetheless, we retain other components from ViT, such as patch embedding, PreNorm, and 1D-positional encoding [16]. Since knockoff generation requires a distribution, we feed $X$ and a uniformly distributed random variable $Z$ with the same dimension as $X$ to inject randomness. Specifically for the DeepDRK model, we use 8 attention heads, 8 layers, and a hidden dimension of 512 in the ViT model. ", "page_idx": 15}, {"type": "text", "text": "The swapper module, first introduced in DDLK [55], produces the index subset $B$ for the adversarial swap attack. Optimizing knockoff against these adversarial swaps enforces the swap property. Specifically, the swapper consists of a matrix with shape $2\\times p$ (i.e., trainable model weights), where $p$ is the dimension of $X$ . This matrix controls the Gumbel-softmax distribution [26] for all $p$ entries. Each entry is represented by a binary Gumbel-softmax random variable (i.e., it can only take values of 0 or 1). To generate the subset $B$ , we sample $b_{j}$ from the corresponding $j$ -th Gumbel-softmax random variable, and $B$ is defined as $\\{j\\in[p]\\,;\\,\\,b_{j}=1\\}$ . During optimization, we maximize Eq. (5) with respect to the weights $\\omega_{i}$ of the swapper $S_{\\omega_{i}}$ , so that the sampled indices, with which the swap is applied, lead to a higher SWD in the objective in Eq. (5). Minimizing this objective with respect to $\\tilde{X}_{\\theta}$ requires the knockoff to counteract the adversarial swaps, thereby enforcing the swap property. Compared to DDLK, the proposed DeepDRK utilizes multiple independent swappers (i.e., $K=2$ ). And we set the temperature for the Gumbel-softmax to be 0.2. ", "page_idx": 15}, {"type": "text", "text": "B.2 Training Algorithm ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Algorithm 1, we provide pseudo code for training the Knockoff Transformer and the swappers (i.e., the first stage shown in Figure 1). ", "page_idx": 15}, {"type": "text", "text": "Algorithm 1 DeepDRK Training ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1: Input: Knockoff transformer $\\tilde{X}_{\\theta}$ , denoted as $g_{\\theta}(\\cdot)$ ; swappers $S_{\\omega}$ ; number of swappers $K$ ;   \nlearning rate $\\alpha_{s}$ for the swappers; learning rate $\\alpha_{\\theta}$ for the knockoff transformer; early stop   \ntolerance $\\eta$ ; number of epochs $T$ ; batch size $B_{s}$ ; dataset $\\mathcal{D}$ ; swapper update frequency $\\gamma=3$   \n2: Output: $\\theta$ for $\\tilde{X}_{\\theta}$   \n3: Split dataset $\\mathcal{D}$ into training set $\\mathcal{D}_{\\mathrm{train}}$ and validation set $\\mathcal{D}_{\\mathrm{val}}$   \n4: Initialize the knockoff transformer $g_{\\theta}(\\cdot)$ with random weights   \n5: Initialize swappers $S_{\\omega_{i}}$ , $i=1,\\ldots,K$ with random weights   \n6: Initialize the AdamW optimizer $\\mathrm{opt}_{\\theta}$ with learning rate $\\alpha_{\\theta}$ for $g_{\\theta}(\\cdot)$   \n7: Initialize the AdamW optimizer $\\mathrm{opt}_{\\omega_{i}}$ with learning rate $\\alpha_{s}$ for $S_{\\omega_{i}}$ , $i=1,\\ldots,K$   \n8: for $t=1$ to $T$ do   \n9: for $l=1$ to $\\frac{|\\mathcal{D}_{\\mathrm{train}}|}{B_{s}}$ do   \n10: Sample $B_{s}$ samples of $X$ from $\\mathcal{D}_{\\mathrm{train}}\\colon X_{l}$   \n11: Generate knockoff $\\tilde{X}_{l}=g_{\\theta}(X_{l})$   \n12: Calculate $\\mathcal L_{\\mathrm{SL}}(X_{l},\\tilde{X}_{l},\\{S_{\\omega_{i}}\\}_{i=1}^{K})$ and $\\mathcal{L}_{\\mathrm{DRL}}(X_{l},\\tilde{X}_{l})$   \n13: $\\theta\\gets\\theta+\\mathrm{opt}_{\\theta}(\\mathcal{L}_{\\mathrm{SL}}(X_{l},\\tilde{X}_{l},\\{S_{\\omega_{i}}\\}_{i=1}^{K})+\\mathcal{L}_{\\mathrm{DRL}}(X_{l},\\tilde{X}_{l}))$   \n14: if $l$ mod $\\gamma=0$ then   \n15: $\\omega_{i}\\gets\\dot{\\omega}_{i}+\\mathrm{opt}_{\\omega_{i}}(-\\mathcal{L}_{\\mathrm{SL}}(X_{l},\\tilde{X}_{l},\\{S_{\\omega_{i}}\\}_{i=1}^{K})),i=1,\\dots,K$   \n16: end if   \n1178:: eCnaldc fuolarte the validation loss on $\\mathcal{D}_{\\mathrm{val}}{\\mathrm{:~}\\mathcal{L}_{\\mathrm{SL}}^{\\mathrm{val}}}+\\mathcal{L}_{\\mathrm{DRL}}^{\\mathrm{val}}$   \n19: if $\\mathcal{L}_{\\mathrm{SL}}^{\\mathrm{val}}+\\mathcal{L}_{\\mathrm{DRL}}^{\\mathrm{val}}$ meets early stop condition at tolerance $\\eta$ then   \n20: break   \n21: end if   \n22: end for ", "page_idx": 15}, {"type": "text", "text": "C From Wasserstein to Sliced Wasserstein Distance ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The Wasserstein distance has gained popularity in both mathematics and machine learning due to its ability to compare different types of distributions [60] and its differentiability [3]. Here, we provide its definition. Let $X,Y$ be two $\\mathbb{R}^{d}$ random vectors following distributions $P_{X},P_{Y}$ with finite $p$ -th moments. The Wasserstein- $\\boldsymbol{p}$ distance between $P_{X}$ and $P_{Y}$ is: ", "page_idx": 16}, {"type": "equation", "text": "$$\nW_{p}(P_{X},P_{Y})=\\operatorname*{inf}_{\\gamma\\in\\Gamma(P_{X},P_{Y})}\\left(\\mathbb{E}_{(x,y)\\sim\\gamma}\\|x-y\\|^{p}\\right)^{\\frac{1}{p}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\Gamma(P_{X},P_{Y})$ denotes the set of all joint distributions such that their marginals are $P_{X}$ and $P_{Y}$ . When $d=1$ , the Wasserstein distance between two one-dimensional distributions can be written as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{p}(P_{X},P_{Y})=\\displaystyle\\left(\\displaystyle\\int_{0}^{1}|F_{X}^{-1}(v)-F_{Y}^{-1}(v)|^{p}d v\\right)^{\\frac{1}{p}}}\\\\ &{\\qquad\\qquad=\\|F_{X}^{-1}-F_{Y}^{-1}\\|_{L^{p}([0,1])},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $F_{X}$ and $F_{Y}$ are the cumulative distribution functions (CDFs) of $P_{X}$ and $P_{Y}$ , respectively. Moreover, if $p=1$ , the Wasserstein distance further simplifies to ", "page_idx": 16}, {"type": "equation", "text": "$$\nW_{1}(P_{X},P_{Y})=\\int|F_{X}(v)-F_{Y}(v)|d v=\\|F_{X}-F_{Y}\\|_{L^{1}(\\mathbb{R})}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "From the above, it is evident that the 1D Wasserstein distance is straightforward to compute, leading to the development of the sliced Wasserstein distance (SWD) [9]. To leverage this computational advantage in 1D, one first projects both distributions uniformly onto a 1D direction and computes the Wasserstein- $\\cdot p$ distance between the two projected distributions. SWD is then calculated by taking the expectation over the random direction. Specifically, let $\\mu\\in\\mathbb{S}^{d-1}$ denote a projection direction, and the push-forward distribution [60] $\\mu_{\\sharp}P_{X}$ denotes the law of $\\mu^{T}X$ . When $\\mu$ is uniformly distributed on the $d$ -dimensional sphere, the $p$ -sliced Wasserstein distance between $P_{X}$ and $P_{Y}$ is given by: ", "page_idx": 16}, {"type": "equation", "text": "$$\nS W_{p}(P_{X},P_{Y})=\\int_{\\mu\\in\\mathbb{S}^{d-1}}W_{p}(\\mu_{\\sharp}P_{X},\\mu_{\\sharp}P_{Y})\\,d\\mu.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combining Eq. (18) and Eq. (16) yields: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle S W_{p}(P_{X},P_{Y})=}\\\\ {\\displaystyle}&{\\qquad\\displaystyle\\int_{\\mu\\in\\mathbb{S}^{d-1}}\\left(\\int_{0}^{1}|(F_{X}^{\\mu})^{-1}(v)-(F_{Y}^{\\mu})^{-1}(v)|^{p}d v\\right)^{\\frac{1}{p}}d\\mu}\\\\ {\\displaystyle}&{=\\displaystyle\\int_{\\mu\\in\\mathbb{S}^{d-1}}\\int|F_{X}^{\\mu}(v)-F_{Y}^{\\mu}(v)|d v\\,d\\mu\\quad\\mathrm{when}\\,p=1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Despite faster computation, the convergence of SWD has been shown to be equivalent to the convergence of the Wasserstein distance under mild conditions [10]. In practice, the expectation over $\\mu$ is approximated by a finite summation over a number of projection directions chosen uniformly from d\u00b41. ", "page_idx": 16}, {"type": "text", "text": "D Sliced Wasserstein Correlation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The idea of metricizing independence is recently advanced using the Wasserstein distance [61, 43]. Given a joint distribution $(\\bar{X},Y)\\,\\sim\\,\\Gamma_{X Y}$ and its marginal distributions $X\\,\\sim\\,P_{X},Y\\,\\sim\\,P_{Y}$ , the Wasserstein Dependency (WD) between $X$ and $Y$ is defined by $\\mathrm{WD}(X,Y)=W_{p}(\\Gamma_{X Y},P_{X}\\otimes P_{Y})$ . A trivial observation is that $\\operatorname{WD}(X,Y)\\,=\\,0$ implies that $X$ and $Y$ are independent. Due to the high computational cost of Wasserstein distance, sliced Wasserstein dependency (SWDep) [33] is developed using sliced Wasserstein distance (see Appendix C for SWD details). The SW dependency (SWDep) between $X$ and $Y$ is defined as $\\operatorname{SW}_{p}(\\Gamma_{X Y},P_{X}\\otimes P_{Y})$ , and a zero SWDep indicates independence. Since the dependency metric is not bounded from above, sliced Wasserstein correlation ", "page_idx": 16}, {"type": "image", "img_path": "ibKpPabHVn/tmp/7c7b4cf9c74a13028762175c98b59a8bde81cb4e9787c440fbd6d4c34474c05c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 8: The competing relationship between the swap property ( $\\mathcal{L}_{\\mathrm{SL}}$ in solid curves) and dependency regularization $\\mathcal{L}_{\\mathrm{DRL}}$ in dashed curves). ", "page_idx": 17}, {"type": "text", "text": "(SWC) is introduced to normalize SWDep. More specifically, the SWC between $X$ and $Y$ is defined as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{SWC}_{p}(X,Y):=\\cfrac{\\mathrm{SWDep}_{p}(X,Y)}{\\sqrt{\\mathrm{SWDep}_{p}(X,X)\\,\\mathrm{SWDep}_{p}(Y,Y)}}}\\\\ &{=\\cfrac{\\mathrm{SWD}_{p}(\\Gamma_{X}Y,\\,P_{X}\\otimes P_{Y})}{\\sqrt{\\mathrm{SWD}_{p}\\,(\\Gamma_{X X},\\,P_{X}\\otimes P_{X})\\,\\mathrm{SWD}_{p}\\,(\\Gamma_{Y Y},\\,P_{Y}\\otimes P_{Y})}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\Gamma_{X}X$ and $\\Gamma_{Y}Y$ are the joint distributions of $(X,X)$ and $(Y,Y)$ respectively. It is shown that $0\\leqslant\\mathrm{SWC}_{p}(X,Y)\\leqslant1$ and $\\mathrm{S}\\bar{\\mathrm{WC}}_{p}(X,Y)=1$ when $X$ has a linear relationship with $Y$ [33]. ", "page_idx": 17}, {"type": "text", "text": "In terms of computing SWC, we follow [33] and consider both laws of $X$ and $Y$ to be sums of $2n$ Diracs, i.e., both variables are empirical distributions with data $\\mathcal{Z}_{\\mathrm{full}}\\;=\\;\\left\\{\\left(\\mathbf{x}_{i},\\mathbf{y}_{i}\\right)\\right\\}_{i=1}^{2n}$ . Define $\\mathcal{T}=\\left\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\right\\}_{i=1}^{n}$ and $\\tilde{\\mathcal{Z}}=\\left\\{(\\tilde{\\mathbf{x}}_{i},\\tilde{\\mathbf{y}}_{i})\\right\\}_{i=1}^{n}$ , where $\\left({\\tilde{\\bf x}}_{i},{\\tilde{\\bf y}}_{i}\\right)=\\left({\\bf x}_{n+i},{\\bf y}_{n+i}\\right)$ . Because data is i.i.d., $\\mathcal{T}$ and $\\tilde{\\mathcal{I}}$ are independent. We further introduce the following notation: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{Z}_{\\mathbf{x}\\mathbf{y}}=\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\}_{i=1}^{n}\\,\\tilde{\\mathcal{Z}}_{\\mathbf{x}\\mathbf{y}}=\\{(\\tilde{\\mathbf{x}}_{i},\\mathbf{y}_{i})\\}_{i=1}^{n}}\\\\ &{\\mathcal{Z}_{\\mathbf{x}\\mathbf{x}}=\\{(\\mathbf{x}_{i},\\mathbf{x}_{i})\\}_{i=1}^{n}\\,\\tilde{\\mathcal{Z}}_{\\mathbf{x}\\mathbf{x}}=\\{(\\tilde{\\mathbf{x}}_{i},\\mathbf{x}_{i})\\}_{i=1}^{n}\\,,}\\\\ &{\\mathcal{Z}_{\\mathbf{y}\\mathbf{y}}=\\{(\\mathbf{y}_{i},\\mathbf{y}_{i})\\}_{i=1}^{n}\\,\\tilde{\\mathcal{Z}}_{\\mathbf{y}\\mathbf{y}}=\\{(\\tilde{\\mathbf{y}}_{i},\\mathbf{y}_{i})\\}_{i=1}^{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then the empirical SWC can be computed by: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\widehat{\\mathrm{SWC}_{p}}(X,Y):=\\frac{\\mathrm{SWD}_{p}\\left(I_{\\mathrm{Z_{xy}}},I_{\\tilde{Z}_{\\mathrm{xy}}}\\right)}{\\sqrt{\\mathrm{SWD}_{p}\\left(I_{\\mathrm{Z_{xx}}},I_{\\tilde{Z}_{\\mathrm{xx}}}\\right)\\mathrm{SWD}_{p}\\left(I_{\\mathrm{Z_{yy}}},I_{\\tilde{Z}_{\\mathrm{yy}}}\\right)}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "E Competing Losses ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Figure 8, we present scaled $\\mathcal{L}_{\\mathrm{SL}}$ and $\\mathcal{L}_{\\mathrm{DRL}}$ curves for each model considered. For better visualization, these curves are normalized to values between 0 and 1. We observe the first 20 epochs, as $\\mathcal{L}_{\\mathrm{DRL}}$ flattens out in later epochs without further decrease. The competition between losses is evident: as $\\mathcal{L}_{\\mathrm{SL}}$ decreases, $\\mathcal{L}_{\\mathrm{DRL}}$ increases, indicating difficulty in maintaining low reconstructability. ", "page_idx": 17}, {"type": "text", "text": "F Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "F.1 Proof of Lemma 3.1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Assuming all conditions in [41] hold, we prove this by applying inequalities on $\\mathrm{SWD}(\\hat{P}_{n},\\hat{P}_{n}^{B})$ and $\\operatorname{SWD}(P,{\\bar{P}}^{B})$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{SWD}(\\hat{P}_{n},\\hat{P}_{n}^{B})\\leqslant\\mathrm{SWD}(\\hat{P}_{n},P)+\\mathrm{SWD}(P,P^{B})+\\mathrm{SWD}(P^{B},\\hat{P}_{n}^{B})}\\\\ &{\\qquad\\qquad\\qquad=\\mathrm{SWD}(P,P^{B})+\\mathcal{O}(n^{-1/2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "image", "img_path": "ibKpPabHVn/tmp/93921d7c5534274aa1b7194114deec1a1effd073b9e306867c48ac4dea5d4625.jpg", "img_caption": ["Figure 9: The effect of $\\alpha_{n}$ for $\\tilde{X}_{\\theta}^{\\mathrm{DRP}}$ in Eq. (8) on FDR and power. When $\\alpha_{n}$ is 0, we consider the knockoff generated from the knockoff transformer without any dependency regularization perturbation. When $\\alpha_{n}$ is 1, there is only perturbation $X_{\\tt r p}$ without the knockoff. The sample size $n=2000$ . "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "where the first inequality is a direct application of the triangle inequality, and the second equality follows from Theorem 5 in [41]. Similarly, we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{SWD}(P,P^{B})\\leqslant\\mathrm{SWD}(P,\\hat{P}_{n})+\\mathrm{SWD}(\\hat{P}_{n},\\hat{P}_{n}^{B})+\\mathrm{SWD}(\\hat{P}_{n}^{B},P^{B})}\\\\ &{\\qquad\\qquad\\qquad=\\mathrm{SWD}(\\hat{P}_{n},\\hat{P}_{n}^{B})+{\\mathcal O}(n^{-1/2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combining Eq. (23) and Eq. (24), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{SWD}(P,P^{B})=\\mathrm{SWD}(\\hat{P}_{n},\\hat{P}_{n}^{B})+\\mathcal{O}(n^{-1/2}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "F.2 Proof of Proposition 3.2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "From Eq. (9), we know X\u02dc\u03b8D,RnP\u00dda.\u00dds.\u00d1 X\u02dc\u03b8 as n \u00d1 8. Combining this with Eq. (25), we obtain SWDpP, P Bq \u201c SWDp P\u02c6n, P\u02c6 nB q \u201c SWDp P\u02c6n,DRP, P\u02c6 nB,DRPq as n \u00d1 8. ", "page_idx": 18}, {"type": "text", "text": "G Effect of $\\alpha_{n}$ in $\\tilde{X}_{\\theta}^{\\mathbf{DRP}}$ ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "As discussed in Section 3.2, obtaining the swap property while maintaining low reconstructability at the sample level is empirically challenging. To address this, dependency regularization perturbation (DRP) is introduced. In this section, we evaluate the effect of $\\alpha_{n}$ in $\\tilde{X}_{\\theta}^{\\mathrm{DRP}}$ in Eq. (9) on feature selection performance. Results are summarized in Figure 9 for five synthetic datasets with the sample size $n=2000$ . ", "page_idx": 18}, {"type": "text", "text": "When $\\alpha_{n}$ is decreased, we observe an increase in power. However, the FDR exhibits a bowl-shaped pattern, consistent with the findings of [54]: introducing the permuted $X_{\\tt r p}$ reduces reconstructability, thereby increasing power. However, an overly dominant $X_{\\mathfrak{r}\\mathfrak{p}}$ compromises the swap property, resulting in higher FDRs. Based on our hyperparameter search and the results in Figure 9, we recommend choosing $\\alpha_{n}$ within the range of 0.4 to 0.5. ", "page_idx": 18}, {"type": "text", "text": "H Implementation Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "H.1 Training ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To fit models, we first split datasets of $X$ into training and validation sets with an 8:2 ratio. The training sets are used for model optimization, and the validation sets are used for early stopping based on the validation loss, with a patience period of 6. Since knockoffs are not unique [11], testing sets are not required. To evaluate DeepDRK\u2019s performance, we compare it with four state-of-the-art (SOTA) deep-learning-based knockoff generation models, focusing on non-parametric data. Specifically, we include Deep Knockoff $[47]^{12}$ , DDLK [55]13, KnockoffGAN $[27]^{14}$ , and sRMMD [38]15. We use the recommended hyperparameter settings for each model, with the only difference being the number of training epochs, set to 200 for consistency across models. ", "page_idx": 18}, {"type": "table", "img_path": "ibKpPabHVn/tmp/309c498de9f070110819ad423459f568ed6eb57bfa5001a4da8befafc33ec381.jpg", "table_caption": [], "table_footnote": ["Table 3: Training configuration. "], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "We follow the model configuration in Table 3 to optimize DeepDRK. The architecture of the swappers $S_{\\omega}$ is based on [55]. Both the swappers and $\\tilde{X}_{\\theta}$ are trained using the AdamW optimizer [36]. During training, we alternately optimize $\\tilde{X}_{\\theta}$ and the swappers $S_{\\omega}$ , updating weights $\\theta$ three times for each update of weights $\\omega$ . This training scheme is similar to GAN training [21], though without discriminators. We apply early stopping to prevent overfitting. A pseudocode of the optimization is provided in Appendix B.2. In experiments, we set $\\alpha_{n}~=~0.5$ universally as the dependency regularization coefficient due to its consistent performance 16. A discussion on the effect of $\\alpha_{n}$ is provided in Appendix G. ", "page_idx": 19}, {"type": "text", "text": "Once trained, models generate the knockoff $\\tilde{X}$ given $X$ data. The generated $\\tilde{X}$ is then combined with $X$ for feature selection. Experiments are conducted on a single NVIDIA V100 16GB GPU. ", "page_idx": 19}, {"type": "text", "text": "H.2 Feature Selection ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Once $\\tilde{X}$ is obtained, feature selection is performed in three steps. We first concatenate $X$ and $\\tilde{X}$ to form an $n\\,\\times\\,2p$ design matrix $(X,{\\tilde{X}})$ . In the second step, we use Ridge regression to get the estimated regression coefficients $\\{\\hat{\\beta}_{j}\\}_{j=1}^{2p}$ from $Y$ and $(X,{\\tilde{X}})$ . Finally, we compute the knockoff statistics $w_{j}=|\\hat{\\beta}_{j}|-|\\hat{\\beta}_{j+p}|$ , for $j=1,2,\\ldots,p$ , and then select features using the threshold defined in Eq. (3). We set $q=0.1$ as the FDR threshold, following its common use in other knockoff-based feature selection studies [47, 38]. Each experiment is repeated 600 times, with results reported as the average power and FDR over these 600 repetitions. ", "page_idx": 19}, {"type": "text", "text": "H.3 Data Preparation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "H.3.1 Weights for the Gaussian mixture models ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Table 4 includes 10 sets of $(\\pi_{1},\\pi_{2},\\pi_{3})$ for the Gaussian mixture models. ", "page_idx": 19}, {"type": "text", "text": "H.3.2 Preparation of the RNA Data ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We first normalize the raw data $X$ to value range $[0,1]$ and then standardize it to have zero mean and unit variance. $Y$ is synthesized according to $X$ . We consider two different ways of synthesizing $Y$ . The first, denoted as \u201cLinear\u201d, is similar to the previous setup in the full synthetic case with $\\boldsymbol{Y}\\,\\sim\\,\\mathcal{N}(\\boldsymbol{X}^{T}{\\dot{\\boldsymbol{\\beta}}},\\boldsymbol{1})$ and $\\begin{array}{r}{\\beta\\stackrel{{}^{}}{\\sim}\\frac{p}{12.5\\cdot\\sqrt{N}}}\\end{array}$ 12.5p\u00a8?N \u00a8 Rademacher(0.5). For the second, denoted as \u201cTanh\u201d, the response $Y$ is generated following the expression: ", "page_idx": 19}, {"type": "table", "img_path": "ibKpPabHVn/tmp/2f04221f386978ba6b7276f1943c31132d1bec74b39be5b7beca1a6a157ab768.jpg", "table_caption": [], "table_footnote": ["Table 4: 10 sets of $(\\pi_{1},\\pi_{2},\\pi_{3})$ for the Gaussian mixture models. "], "page_idx": 19}, {"type": "table", "img_path": "ibKpPabHVn/tmp/7ea8eea9f4305beb6953051ad665dcb8ac559f4a8da5033cfff18a9c0c161421.jpg", "table_caption": [], "table_footnote": ["Table 5: IBD-associated metabolites that are supported by the literature. This table includes all 47 referenced metabolites for the IBD case study. Each metabolite is supported by one of the three sources: PubChem, peer-reviewed publications or preprints. For PubChem case, we report the PubChem reference ID (CID), and for the other two cases we report the publication references. "], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{k\\in[m/4]}}\\\\ {{\\displaystyle{\\varphi_{k}^{(1)},\\varphi_{k}^{(2)}\\sim\\mathcal{N}(1,1)}}}\\\\ {{\\displaystyle{\\varphi_{k}^{(3)},\\varphi_{k}^{(4)},\\varphi_{k}^{(5)}\\sim\\mathcal{N}(2,1)}}}\\\\ {{\\displaystyle Y\\mid X=\\epsilon+\\sum_{k=1}^{m/4}\\varphi_{k}^{(1)}X_{4k-3}+\\varphi_{k}^{(3)}X_{4k-2}}}\\\\ {{\\displaystyle+\\varphi_{k}^{(4)}\\operatorname{tanh}\\left(\\varphi_{k}^{(2)}X_{4k-1}+\\varphi_{k}^{(5)}X_{4k}\\right),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\epsilon$ follows the standard normal distribution and the 20 covariates are sampled uniformly. ", "page_idx": 20}, {"type": "text", "text": "H.3.3 Metabolite Information for the IBD Study ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Here we provide the implementation details for the experiments described in Section 4.3. In Table 5, we provide all 47 referenced metabolites based on our comprehensive literature review. ", "page_idx": 20}, {"type": "text", "text": "I Additional Results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we include all results that are deferred from the main paper. ", "page_idx": 20}, {"type": "text", "text": "I.1 Swap Property ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We use different metrics to empirically evaluate the swap property on the generated knockoff $\\tilde{X}$ and the original data $X$ according to Eq. (1). In this paper, three metrics are considered: mean discrepancy distance with linear kernel, or \u201cMMD(Linear)\u201d for ", "page_idx": 20}, {"type": "table", "img_path": "ibKpPabHVn/tmp/4d01b72e0e5a2d1f6f599796af9dfbad4de7ad71c48b506bf4942d1bc10a829b.jpg", "table_caption": ["Table 6: The abbreviations of the names for the datasets. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "ibKpPabHVn/tmp/a94dd05ecc6df1002deca23980dd51e52719fdc6126f3f44d31b75a599cb45af.jpg", "table_caption": [], "table_footnote": ["Table 7: Evaluation of the swap property. This table empirically measures the swap property by three different metrics: MMD(Linear), $\\mathrm{{SWD}_{1}}$ , and $\\mathrm{{SWD_{2}}}$ . The evaluation considers all baseline models and all datasets in the synthetic dataset setup. For space consideration, we use abbreviations to indicate the name of the datasets. The full name can be found in Table 6. "], "page_idx": 21}, {"type": "text", "text": "short; sliced Wasserstein-1 distance $(\\mathrm{SWD_{1}})$ ; and sliced Wasserstein-2 distance $\\mathrm{(SWD_{2})}$ ). We measure the sample level distances between the vector that concatenates $X$ and $\\tilde{X}$ (i.e., $(X,{\\tilde{X}}))$ and the vector after randomly swapping the entries (i.e., $(X,\\tilde{X})_{\\operatorname{swap}(B)})$ . To avoid repetition, please refer to section 2.1 and Eq. (1) for the definition of notation. Empirically, it is time consuming to evaluate all subsets $B$ of the index set $[p]$ . As a result, we alternatively define a swap ratio $r_{s}\\in\\{0.1,0.3,0.5,0.7,0.9\\}$ . The swap ratio controls the amount of uniformly sampled indices (i.e., the cardinality $|B|\\,=\\,r_{s}\\cdot p)$ in a subset of $[p]$ . For any $X$ and $\\tilde{X}$ from the same experiment, 5 different subsets $B$ are formed according to 5 different swap ratios. We report the average values over the swap ratios to represent the empirical quantification of the swap property. Results can be found in Table 7. ", "page_idx": 21}, {"type": "text", "text": "Clearly, compared to other models, the proposed DeepDRK achieves the smallest values in almost every entry across the three metrics and the first 4 datasets (i.e., $\\mathrm{J}+\\mathrm{G}$ , $\\mathrm{C}+\\mathrm{G}$ , $\\mathrm{C+E}$ and $\\mathrm{J}+\\mathrm{E}$ in Table 7). This explains why DeepDRK has lower FDRs as DeepDRK maintains the swap property relatively better than the baseline models (see results in Figure 2). Similarly, we observe that KnockoffGAN also achieves relatively small values, which leads to well-controlled FDRs compared to other baseline models. Overall, this verifies the argument in Cand\\`es et al. [11] that the swap property is important in guaranteeing FDR during feature selection. ", "page_idx": 21}, {"type": "text", "text": "However, we observe a difference for Gaussian mixture data. The proposed DeepDRK achieves the best performance in FDR control and power (see results in Figure 2), yet its swap property measured under the metrics in Table 7 is not the lowest. Despite this counter-intuitive observation, we want to highlight that it does not conflict with the argument in Cand\\`es et al. [11]. Rather, it supports our statement that the low reconstructability and the swap property cannot be achieved at the sample level (e.g., the free lunch dilemma in practice). After all, the swap property is not the only factor that determines FDR and power during feature selection. ", "page_idx": 21}, {"type": "image", "img_path": "ibKpPabHVn/tmp/994e008f552a5a810d66736a78ddbe9d7cbd2e5e118657339ee099c3f97277db.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 10: Power and FDR in the ablation studies. The red horizontal line indicates the 0.1 FDR threshold. DeepDRK: is the model with dependency regularized perturbation removed. $K$ indicates the number of swappers. ", "page_idx": 22}, {"type": "image", "img_path": "ibKpPabHVn/tmp/754c26d0d0bdaf3db64cf70e7b41831e3e40fc7e0e0426de07405bb52bc89d03.jpg", "img_caption": ["Figure 11: Knockoff statistics $(w_{j})$ for different knockoff models on the synthetic datasets. Each bar in the plot represents the mean of the null/nonnull knockoff statistics averaging on 600 experiments. The error bar indicates the standard deviation. The sample size is 2000. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "I.2 Ablation Studies ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this subsection, we perform ablation studies on different terms introduced in Section 3.1.1, to show the necessity of designing these terms during the optimization for knockoffs. We consider the fully synthetic setup described in Section 4.1, and consider $n=200$ and $n=2000$ . The distribution of $\\beta$ is $\\frac{p}{15\\!\\cdot\\!{\\sqrt{N}}}$ \u00a8 Rademacher(0.5). We test the following terms: 1. REx; 2. the number of swappers $K$ ; 3. $\\mathcal{L}_{\\mathrm{swapper};}4$ . the dependency regularized perturbation (denoted as DeepDRK:). Five synthetic datasets are considered as before and we report the values for FDR and Power under each setup. The results are presented in Figure 10. ", "page_idx": 22}, {"type": "text", "text": "Figure 10 illustrates the clear drawbacks of using only a single swapper $\\chi=1$ ) and the case without the loss term REx. In those two cases, we fail to control the FDR on all datasets. The REx term is essential, as it ensures that the adversarial attacks generated by different swappers are addressed simultaneously. $\\mathcal{L}_{\\mathrm{{swapper}}}$ is also an important term, as it promotes a diverse adversarial environments. Without this term we observe an increase in FDR compared to DeepDRK or $\\mathrm{\\DeltaDeepDRK^{\\dagger}}$ . The difference between DeepDRK and DeepDRK: is more subtle as DeepDRK: has already obtained high quality knockoffs. However, we empirically observe that adding the perturbation further increases the power for some datasets without sacrificing the FDR controllability. To interpret these observations, we follow a similar procedure in Section 4.1 to study the distribution of the knockoff statistics. Results are included in Section I.3. ", "page_idx": 22}, {"type": "text", "text": "Overall, we verify that all terms are necessary components to achieve higher powers and controlled FDRs through the ablation studies. ", "page_idx": 22}, {"type": "text", "text": "I.3 Analysis on the distribution of knockoff statistics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We present the additional results on the means and standard deviations of both null and nonnull knockoff statistics under various experimental setups. ", "page_idx": 22}, {"type": "text", "text": "Figure 11 shows the results on $w_{j}$ for $n=2000$ and it complements the results on Power and FDR in Figure 2. We notice that all models have concentrated null features around zero and relatively high values for the nonnull knockoff statistics for the case with 2000 samples. This corresponds to consistently accurate FS results across all models and datasets, as shown in Figure 2. ", "page_idx": 22}, {"type": "image", "img_path": "ibKpPabHVn/tmp/438f5ea5d356f4ca127a72cb151021fc4716fa3d6c588430f7c20ce64b9383d3.jpg", "img_caption": ["Figure 12: Knockoff statistics $(w_{j})$ for different models with the increased correlation of the Gaussian mixture data. Each bar in the plot represents the mean of the null/nonnull knockoff statistics averaging on 600 experiments. The error bar indicates the standard deviation. The sample size is 200. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "ibKpPabHVn/tmp/3923051bc327bf2cc63f6e8bcb69688f82bc014a7503f45427f36b0136a147f3.jpg", "img_caption": ["Figure 13: Knockoff statistics $(w_{j})$ for different models with the increased correlation of the mixture of Gaussians data. Each bar in the plot represents the mean of the null/nonnull knockoff statistics averaging on 600 experiments. The error bar indicates the standard deviation. The sample size considered is 2000. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 12 and Figure 13 show the results for the Gaussian mixture data with increased correlations $(\\rho_{\\mathrm{base}}\\,=\\,0.7$ and 0.8) for $n\\,=\\,200$ and $n\\,=\\,2000$ respectively. The figures complement the FDRpower results in Figure 3. It is clear for both $n=200$ and $n=2000$ cases, all models experience an increase in FDR and a decrease in power. This phenomenon can be reflected by the positive shifts of the knockoff statistics for the null features in Figure 12 and Figure 13. However, DeepDRK significantly controls the shifts, achieving the best results with the lowest FDRs and comparable power as shown in Figure 3. ", "page_idx": 23}, {"type": "text", "text": "In addition, we also compare the knockoff statistics for the models considered in the ablation studies in I.2. The results for both $n=200$ and $n=2000$ are in Figure 14 and Figure 15. Although the null knockoff statistics for DeepDRK, DeepDRK: and \u201cNo Lswapper\u201d models concentrate symmetrically around zero, the heights of the nonnull knockoff statistics for DeepDRK are the highest, resulting higher power. And because some nonnull knockoff statistics have small values for the DeepDRK: and \u201cNo $\\mathcal{L}_{\\mathrm{s}}$ wapper\u201d cases, we also expect them to have higher FDRs (see Figure 10). On the other hand, compared to DeepDRK, the models of \u201cNo REx\u201d and \u201c $K=1^{:}$ experience clear positive shifts for the null knockoff statistics, leading to higher FDRs during FS (see Figure 10). ", "page_idx": 23}, {"type": "text", "text": "I.4 Training time ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We consider evaluating and comparing model training runtime in Table 8 with the (2000, 100) setup, as it is common in the existing literature. Although DeepDRK is not the fastest among the compared models, the time cost\u20147.35 minutes\u2014is still short, especially when the performance is taken into account. ", "page_idx": 23}, {"type": "image", "img_path": "ibKpPabHVn/tmp/eade6b8ab7613f4c19ee452ec6b7726210c89bfc4240fe8058578a5adf43a8cf.jpg", "img_caption": ["Figure 14: Knockoff statistics $(w_{j})$ for different models in ablation studies. Each bar in the plot represents the mean of the null/nonnull knockoff statistics averaging on 600 experiments. The error bar indicates the standard deviation. $K$ refers to the number of swappers. The sample size is 200. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 15: Knockoff statistics $(w_{j})$ for different models in ablation studies. Each bar in the plot represents the mean of the null/nonnull knockoff statistics averaging on 600 experiments. $K$ refers to the number of swappers. The sample size considered is 2000. ", "page_idx": 24}, {"type": "image", "img_path": "ibKpPabHVn/tmp/7d6f390e2c586b761086f51a8f401b21371b89dd87708306e8ecaf67402cc5be.jpg", "img_caption": [], "img_footnote": ["Table 8: The training time for the models with $n=2000$ and $\\overline{{p=100}}$ . The batch size is 64 and the models are all trained with 100 epochs. "], "page_idx": 24}, {"type": "table", "img_path": "ibKpPabHVn/tmp/cf50d1f435abba234b3c95d8bce5cd7082fc7ca223bf3b28e0edb713924cbed3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "I.5 Additional Results for the IBD Study ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Here we provide the supplementary information for the experimental results described in Section 4.3. In Table 9, we provide the list of identified metabolites by each of the considered models. This table provides additional information for Table 2 in the main paper which only includes metabolite counts due to limited space. ", "page_idx": 24}, {"type": "table", "img_path": "ibKpPabHVn/tmp/0e845a1fad1f0f3cdf65f8efbd7889930fb78137a6376f4b25141f8e031c5072.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 26}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 26}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 26}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 26}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \u201d[Yes] \u201d is generally preferable to \u201d[No] \u201d, it is perfectly acceptable to answer \u201d[No] provided a proper justification is given (e.g., \u201derror bars are not reported because it would be too computationally expensive\u201d or \u201dwe were unable to find the license for the dataset we used\u201d). In general, answering \u201d[No] \u201d or \u201d[NA] \u201d is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. IMPORTANT, please: ", "page_idx": 26}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\u201d, \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We clearly describe the scope and the objective of the paper in the abstract and the introduction sections (also in the related work section). ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The limitation is described in the footnote 15 on page 20. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Please refer to the method section and the appendix for the complete proof.   \nWe provide assumptions in the method section. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We run the experiments multiple times for reproducibility check. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We provide open code access. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 28}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: They are included in the experiment section. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Results shown in the experiment section are repeated for 600 times. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g., negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All experiments are performed with a single NVIDIA V100 GPU. It is described in the experiment section. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 29}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: N/A ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The paper focuses purely on methodology. How to use this method, and to serve what purpose is completely dependent on people who use it (see the conclusion section). ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: It is an unreleated question to this paper. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The code is open-sourced on GitHub, with necessary licenses. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Code is the only asset. Since it is associated with the method (see the method section), the answer is yes. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: It is unrelated. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: It is unrelated. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]