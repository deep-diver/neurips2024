[{"figure_path": "LdZ0u1FuXb/figures/figures_7_1.jpg", "caption": "Figure 1: Performance of central Kernel Ridge Regression (centralKRR), central Kernel Regression with Gradient Descent (centralKRGD), DC-NY, DKRR-NY-CM, IED, and DCL-KR on Toy-1D and Toy-3D", "description": "The figure compares the performance of six different algorithms on two toy datasets: Toy-1D and Toy-3D. The algorithms include two centralized kernel regression methods (centralKRR and centralKRGD), three decentralized kernel regression methods (DC-NY, DKRR-NY-CM, and IED), and the proposed distillation-based collaborative learning algorithm (DCL-KR). The y-axis represents the average RMSE, while the x-axis represents the number of parties. The figure shows that DCL-KR outperforms other algorithms in most cases. For both Toy-1D and Toy-3D, DCL-KR maintains the near-optimal convergence rate in terms of the number of parties.", "section": "5.1 Results on Kernel Machine-based Algorithms"}, {"figure_path": "LdZ0u1FuXb/figures/figures_7_2.jpg", "caption": "Figure 2: Performance of IED and DCL-KR with no \u2248 a \u00b7 n2r+s (log10 n)3 on Toy-3D", "description": "The figure shows the performance of the IED and DCL-KR algorithms on the Toy-3D dataset for varying numbers of parties (m) and different values of \u03b1.  The number of public inputs (n0) is proportional to \u03b1 * n^(2r+s) * (log10 n)^3.  It illustrates that DCL-KR requires fewer public inputs (smaller \u03b1) to achieve high performance compared to IED, as predicted by the theoretical results. ", "section": "5.1 Results on Kernel Machine-based Algorithms"}, {"figure_path": "LdZ0u1FuXb/figures/figures_8_1.jpg", "caption": "Figure 5: Performance of IED and DCL-KR with px \u2260 px on Toy-3D", "description": "This figure compares the performance of IED and DCL-KR algorithms on the Toy-3D dataset when the public data distribution (px) is different from the local data distribution (px).  The x-axis represents the logarithm of the number of parties (log m), and the y-axis represents the logarithm of the average of root mean squared errors (log RMSE).  Different lines represent different settings of the parameters \u03b1 and \u03b2, which control the relationship between px and px. The figure shows that DCL-KR maintains a consistent convergence rate even when px \u2260 px, whereas IED's performance is significantly affected by the change of px.", "section": "5.1 Results on Kernel Machine-based Algorithms"}, {"figure_path": "LdZ0u1FuXb/figures/figures_9_1.jpg", "caption": "Figure 4: Kernel performance and CKA (with standard deviations) during the kernel distillation procedure. The performance of the target kernel obtained by (4) is also provided. For additional experimental results, please refer to Appendix C.4.", "description": "This figure shows the results of the kernel distillation procedure on the UTKFace dataset.  The left panel displays the RMSE of a kernel linear regression model trained on all local data, showing the kernel performance improving after initial degradation. The right panel shows the average CKA between the local feature kernels and the target kernel, increasing over time towards higher alignment.  The shaded region represents standard deviations, and the dotted line indicates the performance of the ensemble kernel.", "section": "5 Experiments"}, {"figure_path": "LdZ0u1FuXb/figures/figures_42_1.jpg", "caption": "Figure 5: Performance of IED and DCL-KR with px \u2260 px on Toy-3D", "description": "This figure compares the performance of IED and DCL-KR algorithms on the Toy-3D dataset when the distribution of public data (px) is different from the distribution of local data (px).  The plots show the average root mean squared error (RMSE) against the number of parties (m) and the logarithm of the average RMSE against the logarithm of m. Different lines represent scenarios with varying parameters (\u03b1, \u03b2) controlling the relationship between px and px.  The results demonstrate that DCL-KR maintains near-optimal convergence rates regardless of px, unlike IED.", "section": "5 Experiments"}]