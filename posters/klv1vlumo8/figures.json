[{"figure_path": "KLv1VLuMo8/figures/figures_1_1.jpg", "caption": "Figure 1: Example generalization gap depicted for Cartpole CMDP. The solid lines show the true zero-shot transfer generalization performance across contexts. Source tasks are indicated by dotted lines.", "description": "This figure illustrates the concept of generalization gap in contextual Markov Decision Processes (CMDPs).  It shows how the performance of a policy trained on one task (source task) degrades as the target task becomes more different from the source task. The solid lines represent the true performance of zero-shot transfer (applying a policy trained on a source task directly to a target task without adaptation), and the dotted lines show the performance of policies trained on different source tasks.  The generalization gap increases as the target task becomes less similar to the source task.", "section": "Abstract"}, {"figure_path": "KLv1VLuMo8/figures/figures_2_1.jpg", "caption": "Figure 2: Overview illustration for Model-based Transfer Learning. (a) Gaussian process regression is used to estimate the training performance across tasks using existing policies; (b) marginal generalization performance (red area) is calculated using upper confidence bound of estimated training performance, generalization gap, and generalization performance; (c) selects the next training task that maximizes the acquisition function (marginal generalization performance); (d) once the selected task is trained, calculate generalization performance using zero-shot transfer.", "description": "This figure illustrates the Model-Based Transfer Learning (MBTL) algorithm. Panel (a) shows how Gaussian Processes are used to model the training performance of tasks. Panel (b) shows how marginal generalization performance is calculated using upper confidence bounds, generalization gap, and generalization performance. Panel (c) shows how the next training task is selected by maximizing the acquisition function. Panel (d) shows how the generalization performance is calculated after training a new policy using zero-shot transfer.", "section": "4 Model-Based Transfer Learning (MBTL)"}, {"figure_path": "KLv1VLuMo8/figures/figures_5_1.jpg", "caption": "Figure 3: Empirical results of the restriction of search space by MBTL compared to two examples from Corollaries 2.1 and 2.2.", "description": "This figure empirically validates the theoretical analysis of regret bounds by comparing the actual reduction in search space achieved by MBTL with the theoretical bounds given by Corollaries 2.1 and 2.2.  The x-axis represents the number of transfer steps, and the y-axis represents the size of the maximum search space. The shaded area around the MBTL-GP (Average) line indicates the variability in search space reduction across different runs. The graph shows that MBTL effectively reduces the search space over transfer steps, outperforming the theoretical bounds, which indicates higher sample efficiency.", "section": "5 Experiments and analysis"}, {"figure_path": "KLv1VLuMo8/figures/figures_7_1.jpg", "caption": "Figure 4: Traffic CMDP results. Method comparison of normalized performance over N tasks. MBTL efficiently selects source training tasks. The black dotted line indicates the first training step within MBTL that exceeds both independent and multi-task baselines, indicating up to 25x fewer samples needed.", "description": "This figure compares the performance of different methods for solving contextual reinforcement learning problems on traffic control tasks.  It shows how the normalized performance of each method increases with the number of training samples used. MBTL is shown to significantly outperform independent and multitask learning baselines, achieving comparable performance with drastically fewer training samples (up to 25 times fewer samples).  The black dotted line highlights the point at which MBTL surpasses the baselines.", "section": "5.2 Traffic benchmark experiments"}, {"figure_path": "KLv1VLuMo8/figures/figures_7_2.jpg", "caption": "Figure 4: Traffic CMDP results. Method comparison of normalized performance over N tasks. MBTL efficiently selects source training tasks. The black dotted line indicates the first training step within MBTL that exceeds both independent and multi-task baselines, indicating up to 25x fewer samples needed.", "description": "This figure compares the performance of different methods for solving contextual reinforcement learning problems in traffic signal control, eco-driving, and advisory autonomy.  The x-axis represents the number of samples trained (log scale), and the y-axis shows the normalized generalized performance.  The figure demonstrates that the Model-Based Transfer Learning (MBTL) method significantly outperforms the baselines (Independent, Multi-task, Random, Equidistant, Greedy) by requiring substantially fewer training samples to achieve comparable performance. The black dotted line highlights the point at which MBTL surpasses the baselines, illustrating the significant sample efficiency gains achieved.", "section": "5.2 Traffic benchmark experiments"}, {"figure_path": "KLv1VLuMo8/figures/figures_8_1.jpg", "caption": "Figure 6: The GP sequentially updates estimates of the performance function (blue) based on previously trained models. Then, MBTL selects the next source task that maximizes the acquisition function (red). (CMDP: Pendulum (Time step)).", "description": "This figure shows how the Model-Based Transfer Learning (MBTL) algorithm works step by step. In step 1, the Gaussian Process (GP) makes initial prediction of the performance function. Then, the acquisition function (red line) based on GP prediction and generalization gap is computed to find the next training task. In step 2 and 3, the GP updates the prediction using the new observed performance, then the next training task is selected using the acquisition function. The process repeats until the algorithm reaches the termination condition.", "section": "4.1 Modeling assumptions"}, {"figure_path": "KLv1VLuMo8/figures/figures_8_2.jpg", "caption": "Figure 7: Sensitivity analysis on the DRL algorithm underlying MBTL (DQN, PPO, and A2C), tested on Cartpole with varying length of pole. MBTL remains effective.", "description": "This figure displays the results of a sensitivity analysis performed to evaluate the impact of different deep reinforcement learning (DRL) algorithms on the performance of the Model-Based Transfer Learning (MBTL) method.  Three different DRL algorithms were tested: Deep Q-Network (DQN), Proximal Policy Optimization (PPO), and Advantage Actor-Critic (A2C). The results demonstrate that MBTL maintains its effectiveness across these varying algorithms. The x-axis represents the number of samples trained (log scale), while the y-axis represents the normalized performance. The shaded areas indicate the variability in performance across multiple runs.", "section": "5.3.1 Sensitivity analysis"}, {"figure_path": "KLv1VLuMo8/figures/figures_8_3.jpg", "caption": "Figure 8: Sensitivity analysis on acquisition functions.", "description": "This figure shows the sensitivity analysis performed on different acquisition functions used in the Bayesian optimization within the Model-based Transfer Learning (MBTL) framework.  The acquisition functions compared are Expected Improvement (EI), Upper Confidence Bound (UCB) with three different beta parameter settings (\u03b2 = 1, \u03b2 = c\u2081/k, \u03b2 = c\u2082log(k + 1)). The generalized performance is evaluated across four different tasks: Cartpole (Mass of Pole), Traffic signal control (Road Length), AA-Ramp-Vel, and Eco-driving control (Green time). Error bars are included to represent the uncertainty in the results. This analysis helps understand the impact of different acquisition functions on the overall performance of MBTL and how sensitive MBTL's performance is to the specific choice of acquisition function.", "section": "5.3.1 Sensitivity analysis"}, {"figure_path": "KLv1VLuMo8/figures/figures_14_1.jpg", "caption": "Figure 9: Illustration of the discrepancy between observed (J) and predicted (\u0134) generalized performance after training on source task x\u2081 and attempting zero-shot transfer to x'.", "description": "The figure illustrates the difference between the actual and predicted performance after a model is trained on a source task (x\u2081) and used to predict the performance on a different target task (x'). The solid curve represents the actual performance J(\u03c0x\u2081, x') when the model trained on the source task (x\u2081) is applied to various target tasks (x'). The dashed curve represents the predicted performance \u0134(\u03c0x\u2081, x') based on a model, like Gaussian Process (GP), that estimates the performance.  The difference between the curves highlights the generalization gap, which is a key concept in the paper.", "section": "3 Problem formulation"}, {"figure_path": "KLv1VLuMo8/figures/figures_14_2.jpg", "caption": "Figure 10: Step for choosing x2 that maximizes the estimated marginal improvement (V(x; \u03c0\u2081) - V(x1)). V(x; \u03c01) corresponds to the red area under the red line and V(x1) as the area under J(\u03c01, x').", "description": "This figure illustrates the process of selecting the next source task (x2) to maximize the estimated marginal improvement in generalization performance.  The red curve represents the estimated generalization performance after training on the first selected task (x1), shown as a shaded area under the curve.  The difference between the red area (V(x;\u03c01)) and the area under J(\u03c01,x') (representing the generalization performance achieved so far), represents the marginal improvement. The algorithm aims to select x2 that maximizes this difference. ", "section": "3 Problem formulation"}, {"figure_path": "KLv1VLuMo8/figures/figures_15_1.jpg", "caption": "Figure 2: Overview illustration for Model-based Transfer Learning. (a) Gaussian process regression is used to estimate the training performance across tasks using existing policies; (b) marginal generalization performance (red area) is calculated using upper confidence bound of estimated training performance, generalization gap, and generalization performance; (c) selects the next training task that maximizes the acquisition function (marginal generalization performance); (d) once the selected task is trained, calculate generalization performance using zero-shot transfer.", "description": "This figure illustrates the Model-Based Transfer Learning (MBTL) algorithm. It shows how MBTL uses Gaussian processes to estimate training performance, models the generalization gap, and uses Bayesian optimization to select the next training task to maximize generalization performance.  The figure is broken down into four parts: (a) shows the Gaussian process regression for estimating training performance; (b) shows how marginal generalization performance is calculated; (c) shows the acquisition function used to select the next task; and (d) shows the zero-shot transfer used to evaluate generalization performance after a task is trained. ", "section": "4 Model-Based Transfer Learning (MBTL)"}, {"figure_path": "KLv1VLuMo8/figures/figures_19_1.jpg", "caption": "Figure 11: Illustration of the traffic networks in traffic signal control task.", "description": "The figure shows a schematic of a four-way intersection controlled by a traffic signal.  Vehicles (represented as brown rectangles) approach the intersection from four directions.  The inflow of vehicles is indicated by an arrow, and a speed limit sign is shown.  The figure also includes a legend distinguishing between guided vehicles (darker brown) and default vehicles (lighter brown). This setup is used to model and study the traffic signal control task in the paper.", "section": "5.2 Traffic benchmark experiments"}, {"figure_path": "KLv1VLuMo8/figures/figures_19_2.jpg", "caption": "Figure 12: Examples of transferability heatmap for traffic signal control.", "description": "This figure shows three heatmaps visualizing the transferability of strategies for traffic signal control tasks under different variations: inflow, speed limit, and road length. Each heatmap represents the zero-shot transfer performance from each source task (vertical axis) to each target task (horizontal axis), providing insights into how variations in these parameters affect the effectiveness of learned strategies. The heatmaps illustrate how effectively strategies trained on one task generalize to other tasks with varying conditions.", "section": "5.2 Traffic benchmark experiments"}, {"figure_path": "KLv1VLuMo8/figures/figures_20_1.jpg", "caption": "Figure 4: Traffic CMDP results. Method comparison of normalized performance over N tasks. MBTL efficiently selects source training tasks. The black dotted line indicates the first training step within MBTL that exceeds both independent and multi-task baselines, indicating up to 25x fewer samples needed.", "description": "This figure shows the comparison of normalized performance across different methods for traffic CMDP tasks. It highlights the superior sample efficiency of Model-Based Transfer Learning (MBTL) compared to other baselines (Independent, Multi-task, Random, Equidistant, Greedy, Sequential Oracle). The black dotted line indicates when MBTL surpasses the Independent and Multi-task methods, showcasing an improvement of sample efficiency by up to 25 times.", "section": "5.2 Traffic benchmark experiments"}, {"figure_path": "KLv1VLuMo8/figures/figures_21_1.jpg", "caption": "Figure 14: Illustration of the traffic networks in eco-driving control task.", "description": "This figure shows a simplified representation of a four-way intersection with vehicles approaching from four directions.  The vehicles are represented by brown rectangles, with some labeled as \"Guided vehicle\" and others as \"Default vehicle.\" This distinction likely represents a scenario where some vehicles are part of an eco-driving system and others are not.  A traffic signal is present, with a green and yellow light indicating the current phase.  The diagram illustrates the state variables such as the positions and velocities of all vehicles, and the context variables like the current traffic light status, inflow of vehicles, and the penetration rate of guided vehicles (the proportion of eco-driving vehicles within the system).", "section": "A.4.5 Details about eco-driving control"}, {"figure_path": "KLv1VLuMo8/figures/figures_22_1.jpg", "caption": "Figure 15: Examples of transferability heatmap for eco-driving control.", "description": "The figure displays three heatmaps illustrating the transferability of strategies learned for eco-driving control tasks under different conditions.  Each heatmap shows transferability from various source tasks (vertical axis) to different target tasks (horizontal axis) for a specific contextual variation.  (a) shows heatmap for green phase variation, (b) for inflow variation, and (c) for penetration rate variation. The color intensity represents the level of transferability, with brighter colors indicating higher transferability.", "section": "A.4.5 Details about eco-driving control"}, {"figure_path": "KLv1VLuMo8/figures/figures_22_2.jpg", "caption": "Figure 4: Traffic CMDP results. Method comparison of normalized performance over N tasks. MBTL efficiently selects source training tasks. The black dotted line indicates the first training step within MBTL that exceeds both independent and multi-task baselines, indicating up to 25x fewer samples needed.", "description": "This figure compares the performance of different methods (Random, Independent, Multitask, Greedy, Equidistant, MBTL, and Sequential Oracle) on traffic CMDP tasks.  The x-axis represents the number of samples trained (log scale), while the y-axis shows the normalized generalized performance across all tasks.  The figure showcases that MBTL quickly surpasses the performance of Independent and Multitask training methods, demonstrating a significant improvement in sample efficiency (up to 25x fewer samples). The black dotted line visually emphasizes this significant performance jump achieved by MBTL.", "section": "5.2 Traffic benchmark experiments"}, {"figure_path": "KLv1VLuMo8/figures/figures_23_1.jpg", "caption": "Figure 17: Illustration of the traffic networks in advisory autonomy task.", "description": "The figure shows two different traffic network configurations used in the advisory autonomy task: a single-lane ring and a highway ramp.  The single-lane ring is a simplified scenario where vehicles circulate in a single lane, suitable for testing basic guidance strategies.  The highway ramp introduces a more complex and realistic scenario involving merging traffic from an on-ramp onto the main highway, which presents significant challenges for vehicle guidance systems, particularly concerning stop-and-go traffic and the coordination of multiple vehicles.  The \"guidance hold duration (e.g., 10 sec)\" indicates that the guidance provided to the driver is not continuous but rather happens periodically at a given time interval, making the task more challenging in the context of human driver behavior.", "section": "5.2 Traffic benchmark experiments"}, {"figure_path": "KLv1VLuMo8/figures/figures_23_2.jpg", "caption": "Figure 12: Examples of transferability heatmap for traffic signal control.", "description": "This figure visualizes the transferability of strategies learned from source tasks to target tasks in traffic signal control under three types of context variations: inflow, speed limit, and road length. Each heatmap shows the effectiveness of transferring strategies from each source task (vertical axis) to each target task (horizontal axis).  The color intensity represents the transferability score; warmer colors indicate higher transferability scores and vice versa.  The heatmaps reveal patterns and insights into which source tasks are most effective for generalization to specific target tasks based on the type of context variation. This helps understand the sensitivity of different traffic signal control strategies to changes in the environment and informs the design of more robust and adaptable traffic control systems.", "section": "5.2 Traffic benchmark experiments"}, {"figure_path": "KLv1VLuMo8/figures/figures_24_1.jpg", "caption": "Figure 19: Comparison of normalized generalized performance of all target tasks: Advisory autonomy.", "description": "This figure presents a comparison of the normalized generalized performance across different strategies for the advisory autonomy task.  Three sub-figures show the results for three different runs, indicating the consistency of the results. Each sub-figure shows how the performance of different methods (Random, Independent, Multitask, Greedy, Equidistant, MBTL, Sequential Oracle) changes with the number of samples trained (log scale).  The performance metric is advisory autonomy, and the x-axis represents the number of samples used for training, shown on a logarithmic scale. The lines and shaded areas represent the average performance and confidence intervals across multiple trials for each method. The star symbol (*) indicates the best average performance achieved. The results show that MBTL consistently outperforms the other methods, demonstrating its robust adaptability to changing task parameters.", "section": "5.3 Continuous control benchmark experiments"}, {"figure_path": "KLv1VLuMo8/figures/figures_25_1.jpg", "caption": "Figure 20: Examples of transferability heatmap for Cartpole.", "description": "This figure shows three heatmaps visualizing the transferability of strategies in the Cartpole task. Each heatmap represents a different contextual variation: (a) mass of the cart, (b) length of the pole, and (c) mass of the pole.  The heatmaps' x and y axes represent the source and target tasks respectively, with color intensity representing the transferability.  Brighter colors denote higher transferability, meaning that a policy trained on a source task performs well when applied to a similar target task.  The heatmaps illustrate how contextual similarity influences transferability, which is a key concept explored in the Model-Based Transfer Learning (MBTL) framework discussed in the paper.", "section": "4.1 Modeling assumptions"}, {"figure_path": "KLv1VLuMo8/figures/figures_25_2.jpg", "caption": "Figure 21: Comparison of normalized generalized performance of all target tasks: Cartpole.", "description": "This figure compares the performance of different methods (Random, Independent, Multitask, Greedy, Equidistant, MBTL, and Sequential Oracle) across three variations of the Cartpole task: varying mass of the cart, length of the pole, and mass of the pole. The x-axis represents the number of training samples (log scale), and the y-axis represents the normalized generalized performance. The shaded area around each line indicates the standard deviation across multiple runs. The figure demonstrates the sample efficiency of MBTL, especially when compared to Independent and Multitask training. MBTL shows close-to-oracle performance, highlighting its effectiveness in this control domain.", "section": "5 Experiments and analysis"}, {"figure_path": "KLv1VLuMo8/figures/figures_26_1.jpg", "caption": "Figure 22: Examples of transferability heatmap for Pendulum.", "description": "This figure shows three heatmaps visualizing the transferability of strategies for the Pendulum task across variations in three physical properties: timestep, length of the pendulum, and mass of the pendulum. Each heatmap displays the effectiveness of strategy transfer from source tasks (vertical axis) to target tasks (horizontal axis).  The heatmaps illustrate how effectively strategies trained on one set of parameters generalize to other sets.  High transferability (yellow/green) indicates good generalization across different parameter settings. Low transferability (blue/purple) suggests that strategies trained for one setting may not perform well under different conditions.  The patterns in each heatmap reveal how the pendulum's dynamic behavior changes as a function of timestep, length, and mass.", "section": "A.4.9 Details about Pendulum"}, {"figure_path": "KLv1VLuMo8/figures/figures_26_2.jpg", "caption": "Figure 23: Comparison of normalized generalized performance of all target tasks: Pendulum.", "description": "This figure displays the comparison of normalized generalized performance across various strategies for the Pendulum task. The three subplots show results for variations in timestep, length of the pendulum, and mass of the pendulum. The results show that MBTL strategies consistently achieve high scores, demonstrating adaptability to changes in pendulum dynamics.", "section": "5.3 Continuous benchmark experiments"}, {"figure_path": "KLv1VLuMo8/figures/figures_27_1.jpg", "caption": "Figure 24: Examples of transferability heatmap for BipedalWalker.", "description": "The figure shows three heatmaps visualizing the transferability of strategies in the BipedalWalker task, each with variations in friction, gravity, and scale.  The color intensity in each heatmap represents the success of transferring a policy trained on a source task (vertical axis) to a target task (horizontal axis) with specific parameter variations. Darker colors indicate lower transferability.  The heatmaps offer insights into the impact of these physical properties on the generalizability of learned control strategies.", "section": "A.4.10 Details about BipedalWalker"}, {"figure_path": "KLv1VLuMo8/figures/figures_27_2.jpg", "caption": "Figure 25: Comparison of normalized generalized performance of all target tasks: BipedalWalker.", "description": "This figure shows the results of the BipedalWalker experiment, comparing the performance of different algorithms across variations in three physical properties: friction, gravity, and scale.  Each plot shows the learning curves for various algorithms, including random selection, independent training, multitask learning, a greedy strategy, an equidistant strategy, MBTL, and a sequential oracle.  The shaded regions represent confidence intervals. The green dashed line indicates the performance of the best baseline algorithm. The results illustrate that MBTL consistently achieves performance comparable to or exceeding the best baseline methods, often with significantly fewer samples.", "section": "5.3 Continuous control benchmark experiments"}, {"figure_path": "KLv1VLuMo8/figures/figures_28_1.jpg", "caption": "Figure 26: Examples of transferability heatmap for HalfCheetah.", "description": "The figure shows three heatmaps visualizing the transferability of strategies for the HalfCheetah task across variations in friction, gravity, and stiffness. Each heatmap displays the effectiveness of transferring strategies from source tasks (vertical axis) to target tasks (horizontal axis).  The heatmaps illustrate how different physical properties influence strategy adaptability.  For instance, the friction variation heatmap reveals uniform high transferability, while gravity and stiffness variations show less consistent transferability, indicating a higher sensitivity to changes in those parameters.", "section": "A.4.11 Details about HalfCheetah"}, {"figure_path": "KLv1VLuMo8/figures/figures_28_2.jpg", "caption": "Figure 27: Comparison of normalized generalized performance of all target tasks: HalfCheetah.", "description": "The figure shows a comparison of the normalized generalized performance across various strategies for the HalfCheetah task with respect to varied physical properties (friction, gravity, and stiffness).  The results indicate that MBTL generally outperforms other methods, especially when managing variations in gravity and stiffness, showing superior adaptability to physical changes in the task environment. The trends across different parameters confirm the impact of task-specific dynamics on the effectiveness of the tested strategies.", "section": "5.3 Continuous control benchmark experiments"}]