[{"figure_path": "IG6kd5V4kd/tables/tables_3_1.jpg", "caption": "Table 1: Summary of expert estimation rates (up to a logarithmic factor) under the MoE models equipped with the sigmoid gating (ours) and the softmax gating [26]. In this work, we consider three types of expert functions including experts network with ReLU, GELU activations; polynomial experts; and input-independent experts.", "description": "This table summarizes the expert estimation rates achieved using sigmoid and softmax gating functions under two different regimes (Regime 1 and Regime 2) for three types of expert functions: ReLU/GELU experts, polynomial experts, and input-independent experts.  It shows that sigmoid gating is more sample efficient than softmax gating, particularly under Regime 2. The rates are given up to logarithmic factors.", "section": "Convergence Rates for Expert Estimation"}, {"figure_path": "IG6kd5V4kd/tables/tables_8_1.jpg", "caption": "Table 1: Summary of expert estimation rates (up to a logarithmic factor) under the MoE models equipped with the sigmoid gating (ours) and the softmax gating [26]. In this work, we consider three types of expert functions including experts network with ReLU, GELU activations; polynomial experts; and input-independent experts.", "description": "This table compares the expert estimation rates achieved by using sigmoid gating versus softmax gating in Mixture of Experts (MoE) models.  Three types of expert functions are evaluated: ReLU/GELU networks, polynomial experts, and input-independent experts. The rates are shown for two different regimes of gating parameters, indicating the impact of the gating function on the convergence speed of the expert estimation.", "section": "2 Preliminaries"}, {"figure_path": "IG6kd5V4kd/tables/tables_25_1.jpg", "caption": "Table 1: Summary of expert estimation rates (up to a logarithmic factor) under the MoE models equipped with the sigmoid gating (ours) and the softmax gating [26]. In this work, we consider three types of expert functions including experts network with ReLU, GELU activations; polynomial experts; and input-independent experts.", "description": "This table summarizes the expert estimation rates achieved using sigmoid and softmax gating functions in Mixture of Experts (MoE) models. Three types of expert functions are considered: ReLU/GELU networks, polynomial experts, and input-independent experts.  The table shows the convergence rates under two different regimes (Regime 1 and Regime 2) for both gating functions.  The results highlight the superior sample efficiency of sigmoid gating compared to softmax gating, especially for ReLU/GELU and polynomial experts under Regime 2.", "section": "2 Preliminaries"}]