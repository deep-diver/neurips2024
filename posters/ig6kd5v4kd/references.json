{"references": [{"fullname_first_author": "R. A. Jacobs", "paper_title": "Adaptive mixtures of local experts", "publication_date": "1991-01-01", "reason": "This paper introduces the Mixture of Experts (MoE) model, a fundamental concept that the current paper builds upon and analyzes."}, {"fullname_first_author": "M. I. Jordan", "paper_title": "Hierarchical mixtures of experts and the EM algorithm", "publication_date": "1994-01-01", "reason": "This paper extends the MoE model to a hierarchical structure, providing a more sophisticated framework for modeling complex relationships, relevant to the current paper's analysis."}, {"fullname_first_author": "N. Shazeer", "paper_title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "publication_date": "2017-01-01", "reason": "This work introduces sparse gating mechanisms in MoE, directly relevant to the current paper's comparison of softmax and sigmoid gating approaches."}, {"fullname_first_author": "N. Ho", "paper_title": "Convergence rates for Gaussian mixtures of experts", "publication_date": "2022-01-01", "reason": "This paper provides theoretical convergence rates for Gaussian MoE, establishing a benchmark for the current paper's theoretical analysis of regression convergence."}, {"fullname_first_author": "H. Nguyen", "paper_title": "On least square estimation in softmax gating mixture of experts", "publication_date": "2024-01-01", "reason": "This paper, closely related to the current work, establishes a baseline for comparing the sample efficiency of softmax gating, which the current work contrasts with sigmoid gating."}]}