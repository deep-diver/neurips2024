[{"figure_path": "m9WZrEXWl5/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of actual (solid lines) and theoretical (dashed lines) convergence rates for GD with (i) step-sizes strongly adapted to the directional smoothness (\u03b7\u03ba = 1/M(xk+1, xk)) and (ii) the Polyak step-size. Both problems are logistic regressions on UCI repository datasets (Asuncion and Newman, 2007). Our bounds using directional smoothness are tighter than those based on global L-smoothness of f and adapt to the optimization path. For example, on mammographic our theoretical rate for the Polyak step-size concentrates rapidly exactly when the optimizer shows fast convergence.", "description": "The figure compares the actual and theoretical convergence rates of gradient descent (GD) on two logistic regression datasets.  It contrasts GD with step sizes adapted to directional smoothness (a measure of gradient variation along the optimization path) against GD using the Polyak step size. The results demonstrate that bounds derived using directional smoothness are tighter than those based on global L-smoothness and adapt better to the optimization path.", "section": "1 Introduction"}, {"figure_path": "m9WZrEXWl5/figures/figures_3_1.jpg", "caption": "Figure 2: Illustration of GD with \u03b7\u03ba = 1/L. Even though this step-size exactly minimizes the upper-bound from L-smoothness, Mk directional smoothness better predicts the progress of the gradient step because Mk \u226a L. Our rates improve on L-smoothness because of this tighter bound.", "description": "The figure illustrates how the directional smoothness (Mk) provides a tighter upper bound on the function's behavior compared to the global L-smoothness.  It shows that even though a step-size of 1/L minimizes the upper bound based on global smoothness, the actual progress of gradient descent is better approximated by the directional smoothness (Mk) which is often much smaller than L. This tighter bound leads to improved convergence rate guarantees.", "section": "2 Directional Smoothness"}, {"figure_path": "m9WZrEXWl5/figures/figures_6_1.jpg", "caption": "Figure 3: Performance of GD with different step-size rules for a synthetic quadratic problem. We run GD for 20,000 steps on 20 random quadratic problems with L = 1000 and Hessian skew. Left-to-right, the first plot shows the optimality gap f(xk) \u2212 f(x*), the second shows the point-wise directional smoothness D(xk, Xk+1), and the third shows step-sizes used by the different methods.", "description": "This figure compares the performance of Gradient Descent (GD) with different step size rules on a synthetic quadratic problem.  Three different step size strategies are compared: 1/L (constant step size based on global L-smoothness), 1/Dk (step size adapted to point-wise directional smoothness), and 1/Ak (step size adapted to path-wise directional smoothness). The figure shows three plots: optimality gap, point-wise smoothness, and adapted step sizes over 20,000 iterations. It highlights how adaptive step sizes based on directional smoothness lead to faster convergence than constant step size.", "section": "Adaptive Step-Sizes in the Quadratic Case"}, {"figure_path": "m9WZrEXWl5/figures/figures_9_1.jpg", "caption": "Figure 4: Comparison of GD with \u03b7\u03ba = 1/L, step-sizes strongly adapted to the point-wise smoothness (\u03b7\u03ba = 1/D(xk, Xk+1)), and the Polyak step-size against normalized GD (Norm. GD) and the AdGD method on three logistic regression problems. AdGD uses a smoothed version of the point-wise directional smoothness from the previous iteration to set \u03b7\u03ba. We find that GD methods with adaptive step-sizes consistently outperform GD with \u03b7\u03ba = 1/L and even obtain a linear rate on horse-colic.", "description": "This figure compares the performance of several gradient descent methods on three logistic regression datasets: ionosphere, horse-colic, and ozone.  The methods compared are Gradient Descent (GD) with a constant step size (1/L), GD with step sizes adapted to the pointwise directional smoothness (1/Dk), Polyak step size, Normalized Gradient Descent, and an adaptive gradient descent method (AdGD).  The results show that the adaptive step size methods generally outperform the constant step size method, particularly on the horse-colic dataset, which exhibits linear convergence for the adaptive methods.", "section": "5 Experiments"}]