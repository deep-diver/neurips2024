{"references": [{"fullname_first_author": "Paul Christiano", "paper_title": "Deep reinforcement learning from human preferences", "publication_date": "2017-06-01", "reason": "This paper introduces the foundational concept of RLHF, which is the core subject of the current paper's analysis and critique."}, {"fullname_first_author": "Jan Leike", "paper_title": "Deep reinforcement learning from human preferences", "publication_date": "2017-06-01", "reason": "This paper introduces the foundational concept of RLHF, which is the core subject of the current paper's analysis and critique."}, {"fullname_first_author": "J. M. V. Skalse", "paper_title": "Invariance in policy optimisation and partial identifiability in reward learning", "publication_date": "2023-07-23", "reason": "This paper provides the theoretical groundwork on reward identifiability under RLHF with full observability, which the current paper extends to the partial observability setting."}, {"fullname_first_author": "Chris Park", "paper_title": "RLHF from heterogeneous feedback via personalization and preference aggregation", "publication_date": "2024-05-01", "reason": "This paper offers a formal definition of deception in AI, a key concept used in the analysis of RLHF failure modes in the current paper."}, {"fullname_first_author": "Hyunjik Jeon", "paper_title": "Reward-rational (implicit) choice: A unifying formalism for reward learning", "publication_date": "2020-01-01", "reason": "This paper provides a unifying framework for reward learning, which helps to contextualize and analyze the specific challenges and limitations of RLHF."}]}