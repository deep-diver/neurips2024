[{"type": "text", "text": "On the Scalability of Certified Adversarial Robustness with Generated Data ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Thomas Altstidl1 David Dobre3 Arthur Kosmala5 Bj\u00f6rn Eskofier1,2 Gauthier Gidel3,4 Leo Schwinn5 ", "page_idx": 0}, {"type": "text", "text": "1 Machine Learning and Data Analytics Lab, FAU Erlangen N\u00fcrnberg, Germany 2 Institute of AI for Health, Helmholtz Zentrum M\u00fcnchen, Germany 3 Mila, Universit\u00e9 de Montr\u00e9al, Canada 4 Canada CIFAR AI Chair 5 Data Analytics and Machine Learning, Technische Universit\u00e4t M\u00fcnchen, Germany ", "page_idx": 0}, {"type": "text", "text": "{thomas.r.altstidl,bjoern.eskofier}@fau.de {david-a.dobre,gidelgau}@mila.quebec {a.kosmala,l.schwinn}@tum.de ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Certified defenses against adversarial attacks offer formal guarantees on the robustness of a model, making them more reliable than empirical methods such as adversarial training, whose effectiveness is often later reduced by unseen attacks. Still, the limited certified robustness that is currently achievable has been a bottleneck for their practical adoption. Gowal et al. and Wang et al. have shown that generating additional training data using state-of-the-art diffusion models can considerably improve the robustness of adversarial training. In this work, we demonstrate that a similar approach can substantially improve deterministic certified defenses but also reveal notable differences in the scaling behavior between certified and empirical methods. In addition, we provide a list of recommendations to scale the robustness of certified training approaches. Our approach achieves state-of-the-art deterministic robustness certificates on CIFAR-10 for the $\\ell_{2}$ $\\epsilon=36/255)$ and $\\ell_{\\infty}$ $(\\epsilon=8/255)$ threat models, outperforming the previous results by $+3.95$ and $+1.39$ percentage points, respectively. Furthermore, we report similar improvements for CIFAR-100. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep learning models have been successfully applied for a variety of different applications. However, it is widely recognized that the vulnerability of neural networks to adversarial examples [1] remains an open problem and hinders their adoption in safety-critical domains. Prior research on improving the robustness of neural networks against adversarial examples can be broadly classified into empirical [2, 3] and certified approaches [4]. ", "page_idx": 0}, {"type": "text", "text": "Adversarial training is currently the most prominent empirical robustification method [3]. Here, the training data of neural networks is augmented with adversarial examples, improving the robustness against attacks at inference time. Recent work has demonstrated that adversarial training can be considerably improved using synthetically generated data, even without training the generative model with external data [5, 6]. Nevertheless, empirical robustness has repeatedly been shown to be ineffective against more sophisticated attacks developed in subsequent work [7]. ", "page_idx": 0}, {"type": "text", "text": "In contrast to empirical methods, certified approaches yield robustness guarantees given a predefined threat model, most often based on the $\\ell_{1}$ , $\\ell_{2}$ , or $\\ell_{\\infty}$ norm. As a result, these methods provide reliable protection against future attacks. Nevertheless, the robustness guarantees achieved by certification methods are generally substantially lower than the robustness obtained by empirical defenses for the same threat model [3, 4, 8]. ", "page_idx": 0}, {"type": "image", "img_path": "TFAG9UznPv/tmp/d8e72c02f261f79deea8353d92a5bb5c1540bc950b8e40b5434ea79f9e29c11d.jpg", "img_caption": ["Figure 1: Certified and clean accuracy of top-ranked models on CIFAR-10 taken from the SoK Certified Robustness for Deep Neural Networks [10] leaderboard. By using data generated by an elucidating diffusion model (EDM), accuracy significantly improves for four different models and two different norms $\\mathcal{l}_{\\infty}$ and $\\ell_{2}$ ). Grey arrows indicate improvements stemming from this data augmentation. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, we aim to analyze how well certified robustness scales when utilizing additional data generated by diffusion models during the model training, a recipe that has previously proven successful for empirical robustness. Only Hu et al. [9] have already used generated data, and thus a broader in-depth review of the factors influencing scalability is to date missing. In our empirical study, we analyze models trained to be robust against $\\ell_{\\infty}$ and models trained to be robust against $\\ell_{2}$ norm attacks. The proposed approach improves robustness for both the $(\\ell_{\\infty},\\epsilon\\,=8/25\\bar{5})$ and $(\\ell_{2},\\epsilon=36/255)$ threat models on CIFAR-10, improving upon the previous results in the literature by $3.95\\%p$ and $1.39\\%p$ (percentage points), respectively. In most experiments, the increase in certified accuracy is accompanied by an increase in accuracy on clean data, where we observe improvements by up to $4.83\\%p$ . Figure 1 summarizes the improvements compared to the previous state-of-the-art with respect to clean and certified accuracy on CIFAR-10. Further experiments show that the same approach considerably improves certified accuracy on CIFAR-100 as well. ", "page_idx": 1}, {"type": "text", "text": "Moreover, we conduct ablations to evaluate the impact of different design choices, including regularization, the number of training epochs, the optimization schedule, and the optimal balance between real and generated data. We summarize the most important insights of this empirical study in a list of recommendations that can be followed to more accurately compare and improve the robustness of deterministic certified defenses. Lastly, we discern crucial differences in the scaling behavior between empirical and certified methods. All code used to produce the results and figures in this paper will be released on GitHub after publication. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Empirical Robustness. Adversarial training was first introduced by Goodfellow et al. [2]. The authors employed the single-step Fast Gradient Sign Method (FGSM) to craft adversarial examples during training and thereby robustify the model against these attacks. Later research by Madry et al. [3] demonstrated that single-step adversarial training does not yield considerable robustness against multi-step attacks. They showed that using the multi-step Projected Gradient Descent (PGD) attack during training successfully improves the robustness of neural networks at test time, even against strong attacks. Subsequent work proposed improvements to the loss function, the adversarial attack used during training, and better trade-offs between clean and certified accuracy [11, 12]. ", "page_idx": 1}, {"type": "text", "text": "Certified Robustness. Unlike empirical methods, certified methods yield robustness guarantees, thereby eliminating possible vulnerabilities to future attacks. Certification methods can be broadly classified into two methodologically distinct groups, namely probabilistic and deterministic methods. Probabilistic methods aim to approximate smooth classifiers using Monte Carlo sampling and noise injection [4]. A given sample is verified as robust with a certain probability depending on the noise magnitude and number of Monte Carlo samples. To obtain a tight verification bound, probabilistic methods need to perform a substantial amount of sampling procedures (forward passes) ", "page_idx": 1}, {"type": "text", "text": "for each sample, considerably increasing the computational overhead in practice. Contrary to probabilistic methods, deterministic approaches do not entail considerable computational overhead during inference. ", "page_idx": 2}, {"type": "text", "text": "Convex bound propagation [13, 14, 15, 16] is a group of deterministic methods that leverages interval arithmetic and linear programming to track how perturbations in the input space transform through each layer, effectively constructing an outer envelope that contains all possible network outputs for inputs within the specified perturbation region. Our initial results scaling the work by Palma et al. [16], provided in Appendix H, show that this yields severely deteriorated performance when training with additional generated data and is thus a poor candidate for scaling certified robustness. ", "page_idx": 2}, {"type": "text", "text": "Our main focus is hence devoted to deterministic approaches that bound the Lipschitz constant of each neural network layer to be small (generally smaller or equal to 1) for a predefined $\\ell_{p}$ norm [17, 18]. The Lipschitz constant of the whole network is bounded by the multiplication of the Lipschitz constants of the individual layers [19]. Given a network\u2019s upper bound of the Lipschitz constant, a robustness guarantee can be trivially obtained by computing the distance between the highest two logits in the output space. ", "page_idx": 2}, {"type": "text", "text": "Diffusion Models. More recently, diffusion models have superseded generative adversarial networks (GANs) as the preferred method for image generation [20]. Denoising diffusion probabilistic models (DDPM) [21] can generate high-quality samples on CIFAR-10 [22] with an FID score of 3.17, a common measure of image quality. Since then, other variants have been proposed [23, 24]. By further analyzing the design space of these models [23], elucidating diffusion models (EDMs) achieve a current state-of-the-art FID score of 1.79 on CIFAR-10. With additional discriminator guidance [24], the quality of these EDM-generated images are reported to reach an FID score of 1.64, the best score reported in literature for CIFAR-10 at the time of writing. ", "page_idx": 2}, {"type": "text", "text": "Improving Empirical Robustness with Auxiliary Data. Hendrycks et al. [25] showed that utilizing additional data from external datasets during adversarial training can improve empirical adversarial robustness. Gowal et al. [5] extended this approach to synthetically generated data from generative models only trained on the source dataset. Recently, Wang et al. [6] showed that leveraging the latest advances in diffusion models further improves empirical adversarial robustness. ", "page_idx": 2}, {"type": "text", "text": "In this work, we investigate if leveraging data generated with state-of-the-art diffusion models can also improve certified robustness against adversarial attacks and analyze how certified training approaches can be scaled optimally. ", "page_idx": 2}, {"type": "text", "text": "3 Experiment Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given the recent improvements in adversarial training using additional data generated by diffusion models, we devise a set of experiments to investigate whether this also transfers to certified robustness. We focus on deterministic methods as probabilistic methods entail a tremendous computational overhead during inference time and do not achieve considerable robustness for the $\\ell_{\\infty}$ norm yet [10]. All our experiments are done on a single Nvidia A100 graphics card (40GB of VRAM) without distributed training. ", "page_idx": 2}, {"type": "text", "text": "3.1 Dataset and Threat Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We perform experiments on CIFAR-10 and CIFAR-100 [22], for which EDM-generated data is readily available and a wealth of previous robustness research exists [6]. Our experiments and ablation studies focus on CIFAR-10. We refrain from experiments on larger datasets like ImageNet [26] as robustness guarantees achieved by deterministic methods for these datasets are still comparatively low, and only [9] support it at the time of writing. We perform experiments on two common threat models, specifically $\\bar{\\langle}\\ell_{\\infty},\\epsilon=8/255)$ and $\\zeta_{2},\\epsilon=36/255)$ adversaries. We do not consider the $\\ell_{1}$ threat model, as only smoothing-based approaches achieve considerable robustness for this threat model at the time of writing. For our experiments, we select the two best architectures from the popular certified robustness leaderboard introduced by Li et al. [10] for both the $\\ell_{2}$ (GloroNet [9] and LOT [17]) and $\\ell_{\\infty}$ threat models (SortNet [18] and $\\ell_{\\infty}$ -dist Net [27]). In total, we perform experiments on architectures from four different papers. ", "page_idx": 2}, {"type": "table", "img_path": "TFAG9UznPv/tmp/af4fa5c39b950f80ef311d3b274300b0b596335391a89abab93b6092d359e40f.jpg", "table_caption": ["Table 1: Clean and certified test accuracy $(\\%)$ on CIFAR-10 $(\\ell_{\\infty},\\epsilon=8/255)$ for $\\ell_{\\infty}$ -dist Net and SortNet with dropout rate $\\rho$ . Bold highlights the best model with and without auxiliary data. $|\\mathcal{D}_{g e n}|$ denotes the number of EDM-generated images at which highest accuracy was achieved. "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.2 Generated Auxiliary Data ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To explore the effectiveness of augmenting the original CIFAR-10 and CIFAR-100 [22] datasets with generated data, we adjust the data loader of each model to use a fraction of generated data and original data in every epoch. We use the same generated data used by Wang et al. [6], which was produced by an EDM trained only on the train set of CIFAR-10. In a preliminary experiment, we found the generated-to-real ratio to be optimal when $30\\%$ of training images are real and $70\\%$ are generated in every epoch during training, matching the ratio used by Wang et al. [6]. We performed experiments with 50, 000 (50k), 100, 000 (100k), 200, 000 (200k), 500, 000 (500k), 1 million (1m), 5 million $\\langle5\\mathrm{m})$ and 10 million $^{(10\\mathrm{m})}$ generated images. Wang et al. [6] sub-sampled the 1m images from $5\\mathrm{m}$ images choosing only the $20\\%$ most confidently classified images according to a pretrained WRN-28-10 model. In contrast, we naively sub-sample the $1\\mathrm{m}$ images from the $5\\mathrm{m}$ image dataset to avoid potential selection bias by the classifier used to select the data. Moreover, using the same selection process for all datasets should allow us to assess better the effect of the amount of generated data on the final robustness. ", "page_idx": 3}, {"type": "text", "text": "3.3 Hyperparameters ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "With additional data, it is also expected that both model size and the number of training epochs can be further scaled to improve clean accuracy and robustness. We thus perform experiments on the influence of model depth and the number of epochs on clean and certified accuracy. For some models, we investigate further techniques that add learning capacity. Concretely, for SortNet [18] we also experiment with models that do not employ dropout, and for LOT [17] we adjust the learning rate scheduler to cosine annealing [28]. ", "page_idx": 3}, {"type": "text", "text": "4 Results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the following, we first summarize the effect of using additional generated data on the achievable certified and clean accuracy. Furthermore, we ablate the effect of other design choices on the certified robustness, such as the number of training epochs, model size, the amount of additional synthetic data and other hyperparameters. Lastly, we summarize our findings and provide a list of recommendations to scale certified robustness effectively. ", "page_idx": 3}, {"type": "text", "text": "4.1 Improving Certification Approaches with Generated Data ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Across all four reference models and all two threat models, we find that the inclusion of generated data can improve certified accuracy. In most cases, clean accuracy is considerably improved as well. An overview of our new state-of-the-art results in comparison with existing related work is given in Figure 1. For the $(\\ell_{\\infty},\\epsilon=8/255)$ threat model on CIFAR-10 we can increase the robustness of the existing SortNet [18] to $41.78\\%$ , an improvement of 1.39 percentage points. For the $\\langle\\ell_{2},\\epsilon=36/255\\rangle$ model we achieve a certified accuracy of $69.05\\%$ using LOT [17], a substantial increase of 3.95 points compared to the best result previously reported in the literature [10]. In almost all cases this improvement coincides with an increase in clean accuracy. One exception is SortNet, where the clean accuracy slightly decreases from $54.84\\%$ to $54.75\\%$ . ", "page_idx": 3}, {"type": "image", "img_path": "TFAG9UznPv/tmp/8ff940e8685de64646e4fa7067266d09d87dc93cacf79969098e21adef44fea7.jpg", "img_caption": ["Figure 2: Influence of total CIFAR-10 dataset size $|D_{o r i g}|+|D_{g e n}|$ (number of original and generated images) on certified accuracy. $\\rho$ is the dropout rate of SortNet. All models were trained with a large number of epochs, i.e., 1600 for $\\ell_{\\infty}$ -dist Net, 6000 for SortNet, 600 for LOT, and 2400 for GloroNet. Accuracy generally improves little beyond $1\\mathrm{m}$ generated images. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Full results are given in Tab. 1 for both SortNet and $\\ell_{\\infty}$ -dist Net, as well as Tab. 2 for LOT and Tab. 3 for GloroNet. We find that by removing the dropout from SortNet we can improve certified accuracy from $41.32\\%$ to $41.78\\%$ when using auxiliary data. However, the same leads to a drop from $39.72\\%$ to $37.44\\%$ with only the original data, reinforcing the notion that the additional data acts as a good regularizer. Similarly, for LOT, cosine annealing is superior by a margin of up to 1.12 points compared to a multi-step scheduler, indicating that the model can make better use of its capacity when trained with auxiliary data. For full results using the multi-step scheduler, we refer to App. B, Tab. 2 \u2013 all results discussed in subsequent sections refer to those obtained with cosine annealing. ", "page_idx": 4}, {"type": "text", "text": "Adding auxiliary data improves certified robustness on CIFAR-100 as well, as demonstrated in Tab. 4. We restrict our evaluation to the most effective models from the analysis on CIFAR-10 and do not further scale the number of training epochs. The most substantial improvements on CIFAR-100 are obtained for the SortNet model, where certified accuracy increases by 8.08 percentage points from $9.2\\%$ to $17.28\\%$ , and for GloroNet, where certified accuracy increases by 2.49 percentage points from $36.41\\%$ to $38.9\\%$ . ", "page_idx": 4}, {"type": "text", "text": "4.2 Sensitivity Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Scaling the Amount of Auxiliary Data. The main goal of this work was to evaluate the influence of additional synthetic data during training on the achievable certified accuracy of deterministic certification methods. To this end, we analyze how different amounts of additional data affect the final certified robustness. We evaluate the saturation by training with with 50k, 100k, 200k, 500k, 1m, $5\\mathrm{m}$ , and $10\\mathrm{m}$ auxiliary generated images. All models are trained for at least twice the amount of epochs that were used in the original papers to ensure saturation in terms of training time. As seen in Fig. 2, improvements in certified robustness beyond 1m are mostly negligible. This behavior is also largely independent of model size, which stands in contrast to prior results reported for adversarial training [5]. ", "page_idx": 4}, {"type": "text", "text": "Scaling the Model Size. We performed experiments on several different model sizes to investigate possible correlations between the benefit of additional training data and the model capacity. The $\\ell_{\\infty}$ -based models are largely constructed out of fully connected layers. As a result, the computational effort when scaling these models increases quadratically. As experiments on the $\\ell_{\\infty}$ -based models proved to be computationally too expensive we refrain from scaling these models and focus instead on $\\ell_{2}$ -based models. ", "page_idx": 4}, {"type": "text", "text": "Tables 1 to 3 demonstrate that scaling the model size can increase the certified robustness for both LOT and GloroNet. Shown are the best improvements when adding 1m, 5m, or $10\\mathrm{m}$ auxiliary data when compared to the same model trained without any auxiliary data \u2013 referred to as the base model from here on. The highest gains are for medium models with LOT and for large models with ", "page_idx": 4}, {"type": "table", "img_path": "TFAG9UznPv/tmp/3492c98033bcfea8358305ac8cf2ce520467b0b859825eb94c9985e19da3cda3.jpg", "table_caption": ["Table 2: Clean and certified test accuracy $(\\%)$ on CIFAR-10 $(\\ell_{2},\\epsilon\\,=\\,36/255)$ for LOT. Bold highlights the best model with and without auxiliary data. $|\\mathcal{D}_{g e n}|$ denotes the number of EDMgenerated images at which highest accuracy was achieved. "], "table_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "TFAG9UznPv/tmp/0f16eccef1326dfcbf613824524c630b57d9da9894342233edd6a21b03932002.jpg", "table_caption": ["Table 3: Clean and certified test accuracy $(\\%)$ on CIFAR-10 (\u2113 $_{2},\\epsilon=36/255)$ for GloroNet. Bold highlights the best model with and without auxiliary data. $|\\mathcal{D}_{g e n}|$ denotes the number of EDMgenerated images at which highest accuracy was achieved. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "GloroNet. For GloroNet in particular we observe that model size becomes more important the longer the model is trained, with all models trained for the default 800 epochs showing similar gains. ", "page_idx": 5}, {"type": "text", "text": "Scaling the Number Training Epochs. As larger models and additional training data may require longer model training to achieve optimal results we increased the number of training epochs compared to the original configurations for all tested models1. For the $\\ell_{\\infty}$ -based models in Tab. 1 we see that by doubling the number of epochs we can further improve certified robustness when regularization is removed. A similar picture arises for the $\\ell_{2}$ -based models, presented in Tabs. 2 and 3. Here, we find that an increase in the number of epochs yields a significant improvement regardless of model size. Overall, this parameter had the strongest impact in combination with auxiliary data and all best certified accuracies are achieved at their respective maximum number of epochs, with the only exception being SortNet with dropout. ", "page_idx": 5}, {"type": "text", "text": "4.3 Relationship between Generalization Gap and Certified Robustness ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The individual improvements in certified robustness vary considerably between different model and data configurations in our experiments. One possible explanation for these differences may be that the generalization gap of the respective models trained without auxiliary data \u2013 i.e., the base models \u2013 are different, leading to different gains when closing this generalization gap. To investigate this, we correlate the difference in generalization gap with the improvement in certified accuracy obtained when adding auxiliary data. Here, generalization gap refers to the difference between the train and test accuracy on clean data for the best epoch. Figure 3a demonstrates a considerable correlation between the decrease in generalization gap compared to the base model trained with no auxiliary data and the improvement in certified robustness for models with auxiliary data. Moreover, we perform a line fit between the generalization gap and robustness improvement for all the analyzed models. Surprisingly the slope of the different lines is similar for most models, except $\\ell_{\\infty}$ -dist Net, indicating that robustness gains can be predicted once the offset of the line is known for unseen models. However, the offset of the different lines depends on the base model considered. Generalization gaps for each auxiliary dataset size $|\\mathcal{D}_{g e n}|$ in Fig. 3b also correlate well with scaling curves in Fig. 2. ", "page_idx": 5}, {"type": "image", "img_path": "TFAG9UznPv/tmp/0f386491aa5f3d81f1a4ad7e87fc43bf9a6ef79ca346fa04a8cec75d167e1b0b.jpg", "img_caption": ["Figure 3: a Correlation between generalization gap and certified accuracy improvement. Generalization gap measures difference between training and testing accuracy. $\\Delta$ refers to difference between base model and model trained with auxiliary data. b Generalization gaps by amount of auxiliary data $|\\mathcal{D}_{g e n}|$ for all models trained with maximal epoch count. c Clean and certified accuracy $(\\%)$ for different ratios of generated and real data for $\\ell_{\\infty}$ -dist Net and LOT-S. Here, a generated-to-original ratio of 70 means $70\\%$ of each batch is generated data and the remaining $30\\%$ is real data. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "These results are in line with certified robustness gains achieved for SortNet with and without dropout shown in Tab. 1. Here, the certified robustness of the SortNet model trained without auxiliary data and for 3000 epochs decreases when using less regularization by removing dropout from $39.72\\%$ to $37.44\\%$ . At the same time, the generalization gap of the two models increases from $4.16\\%$ to $12.89\\%$ , respectively, as an effect of removing dropout. However, once additional synthetic data is used, removing dropout actually improves the certified robustness to up to $41.71\\%$ by nearly two points. Here, increasing the generalization gap by removing dropout had a positive effect on the final certified robustness, which fits observations in Fig. 3a. ", "page_idx": 6}, {"type": "text", "text": "4.4 Ratio of Generated and Real Data ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In every training epoch, we use a proportion of synthetic and real images and keep the total amount of images the same as the size of the original training set. The default configuration throughout our experiments, and the one also used by Wang et al. [6], is to use $30\\%$ real images and $70\\%$ generated images in each batch. Figure 3c illustrates how using different proportions for generated and real data affects the certified robustness of the $\\ell_{\\infty}$ -dist Net and LOT-S architectures. We see that at ratios of $60\\%$ generated data the clean accuracy saturates, and with $70\\%$ the certified accuracy saturates. Notably, in all cases the accuracy when only training with generated images was higher than when only training with real images, indicating that it may be possible to fully train these models on only generated data in the future. ", "page_idx": 6}, {"type": "text", "text": "4.5 Certification Radius Distribution ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To examine the underlying factors contributing to the observed increase in robustness when using auxiliary data, we conduct an analysis of the certification radius distribution for the SortNet (without dropout) and LOT-L architectures. Figure 4 displays the number of images on the $y$ -axis with a certification radius equal to or above a specific value, shown on the $x$ -axis. Curves are plotted for the best models trained with and without auxiliary data. Additionally, curves for correct and incorrect classifications are displayed separately. ", "page_idx": 6}, {"type": "image", "img_path": "TFAG9UznPv/tmp/71393f7f96c2e92876fbec5b38852269d465d326c860baa695684012faa9c23a.jpg", "img_caption": ["Figure 4: Cumulative distribution of certification radii for the best $\\ell_{\\infty}$ -based model, Sortnet w/o dropout, and the best $\\ell_{2}$ -based model, LOT-L. Note how for SortNet images exhibit overall smaller certification radii for both correct and incorrect classes, yet clean accuracy slightly decreases. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We observe no considerable differences between the distribution of certification radii for the LOT model obtained with or without auxiliary data. Nevertheless, models trained with auxiliary data show slightly higher robustness for correctly classified samples and lower robustness for misclassified samples on average. The SortNet architecture exhibits considerably higher robustness radii for both correct and incorrect classifications when using auxiliary data. Here, differences in certified robustness do not seem to come from a better generalization ability on clean data but from larger certification radii on unseen data. On the other hand, the LOT architecture shows similar certification radii but considerably better generalization on clean data. A more detailed analysis is given in App. G, Fig. 3. Both models show considerable over-robustness for a considerable fraction of the test set, where the certification radius is well beyond the certification goal \u03f5. ", "page_idx": 7}, {"type": "text", "text": "4.6 Takeaways for Scaling Certified Robustness ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Differences in certified robustness between distinct defense approaches are often marginal and even small improvements over prior work may be relevant. Here, we summarize the most important takeaways from the empirical study presented in this work on how to scale the robustness of deterministic certified models. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Scale the number of training epochs. Among all investigated hyperparameters, we found the number of training epochs had the most consistent effect on certified accuracy when training with auxiliary data. Based on our experiments, we expect the amount of auxiliary data to not matter as long as it is sufficient for closing the generalization gap.   \n\u2022 Increase your model capacity. When using auxiliary data, a large generalization gap between training and testing accuracies is less of an issue and, based on our results, indicative of untapped performance improvements that can be leveraged (see Sec. 4.3). This means model capacity can be scaled with little fear of overfitting. Our experiments show that reducing the amount of regularization, using better optimizers, and increasing the model size all improve certified accuracy when using auxiliary data without leading to large generalization gaps.   \n\u2022 Compare with caution. Even small adjustments, such as a change to dropout, learning rate schedulers, or even different random seeds can improve certified accuracy by about 0.5 to 1 percentage points. This makes it difficult to assess the individual contribution of different architectures towards robustness \u2013 for example, contrary to results reported in the original papers, our results indicate that LOT may actually be superior to GloroNet.   \n\u2022 Benchmark with generated auxiliary data. As the proposed approach does not entail a computational overhead for the same amount of training epochs, we recommend future work to compare their approaches using auxiliary data and ensure that models are trained till convergence. Differentiating between approaches that use auxiliary data and those that only utilize the original dataset may be helpful for future benchmarks. Similar approaches have been adopted in the empirical robustness domain [29]. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Decrease over-robustness. While not related to the usage of auxiliary data, our evaluation in Fig. 4 indicates that a considerable number of samples are noticeably more resistant to adversarial attacks than what was intended during training. Future research may consider using smaller certification objectives for samples that already demonstrate considerable robustness, an approach that has already been shown to be successful in adversarial training [12]. ", "page_idx": 8}, {"type": "text", "text": "5 Comparison to Empirical Robustness ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "While we observe many of the same trends as Gowal et al. [5] and Wang et al. [6], i.e., that larger models and more epochs generally help, we also note a few crucial differences. ", "page_idx": 8}, {"type": "text", "text": "\u2022 Amount of training data. Scaling beyond one million CIFAR-10 generated images did not further improve certified accuracy, regardless of model size. This is different to scaling behavior reported previously for adversarial training, with, e.g., Gowal et al. [5] reporting an improvement of $+2.65$ and $+2.52\\%$ when scaling their WRN-23-10 and WRN-70-16 models, respectively, from one million to 100 million generated images. One hypothesis would be that this is due to the consistency property of the Glivenko-Cantelli class, mentioned by B\u00e9thune et al. in Section 5.1 [30], due to which Lipschitz-1 neural networks\u2019 training loss converges to the testing loss for increasingly large datasets. In contrast, adversarial training can be interpreted as a data-dependent operator norm regularization [31] and seems to require more data samples to close the generalization gap. More recent work suggests that the Bayes error may limit the achievable accuracy of both deterministic [32] and probabilistic Zhang and Sun [33] certified robustness, in line with our results. ", "page_idx": 8}, {"type": "text", "text": "\u2022 Overfitting between best and last epoch. In adversarial training, prior work finds that the difference between the best and the last epoch becomes increasingly smaller with larger amounts of auxiliary data [6]. In contrast, we observe no such effect for certifiably robust models (see App. E, Fig. 1), highlighting another difference in scaling behavior between empirical and certified robustness. ", "page_idx": 8}, {"type": "text", "text": "\u2022 Optimal generated-to-original ratio. Previous research regarding the optimal generatedto-original ratio for adversarial training observes a drop-off when using only generated data. Both Gowal et al. [5] (Fig. 5) and Wang et al. [6] (App. B, Fig. 3) suggest accuracy decreases again significantly beyond $70\\%$ generated data when using diffusion models. As evident in Fig. 3c this is not as pronounced for certifiably robust models. ", "page_idx": 8}, {"type": "text", "text": "Together, these results suggest that for certifiably robust models it may be possible to determine a sufficient amount of generated data beforehand for any given dataset - contrary to what is the case for adversarial training, where more is always better. Moreover, it indicates that certified robustness is considerably harder to scale than empirical robustness, as once saturation with respect to data has been achieved, further gains can only be attained by better algorithms and increased model size. Future experiments should explore concrete scaling laws for certified and empirical adversarial robustness, which was out-of-scope of this paper. Deriving and verifying theoretical properties of certifiably robust models, such as those already known on graphs [34], also remains an exciting topic for further research. ", "page_idx": 8}, {"type": "text", "text": "Table 4: Clean and certified test accuracy $(\\%)$ on CIFAR-100 for both $(\\ell_{\\infty},\\epsilon\\;=\\;8/255)$ and $(\\ell_{2},\\epsilon=36/255)$ threat models. Bold highlights the best overall model for each architecture. ", "page_idx": 8}, {"type": "table", "img_path": "TFAG9UznPv/tmp/1a462509a3e4f1c0284b7d29dd16e260564aeb31bb2954720dd2b8c03d566452.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "6 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Despite our best efforts and extensive experiments, some limitations remain. Our focus is largely on deterministic Lipschitz-bound methods, which incidentally were used in the two best-performing models for the $\\bar{(\\ell}_{\\infty},\\epsilon=8/255)$ and $\\langle\\ell_{2},\\epsilon=36/255\\rangle$ ) attacks on CIFAR-10 at the time the experiments were performed [10]. Other approaches for certified robustness, including probabilistic ones and other deterministic methods, stay largely unexplored except for our experiments on convex bound propagation in Appendix H. The inherently high computational complexity of the evaluated models [9, 17, 18, 27] also inhibited us from running each configuration multiple times, as even a single run already results in an overall compute time of around five thousand GPU hours. Finally, we made the conscious choice to not perform experiments on ImageNet as only one of the evaluated models, that by Hu et al. [9], supported this dataset and thus no meaningful comparison could be made. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We show that deterministic certified robustness can be improved by up to $5.28\\%p$ when additional generated data from a diffusion model is used during training. This is true across four different architectures and two different threat models, $(\\ell_{\\infty},\\epsilon=8/255)$ and $\\zeta_{2},\\epsilon=36/255)$ , on CIFAR-10, where we report improved certified accuracies of $41.78\\%$ and $69.05\\%$ , respectively. For $\\ell_{\\infty}$ we are thus able to achieve a new state-of-the-art, while Hu et al. [9] report a slightly better accuracy of $70.1\\%$ for the $\\ell_{2}$ threat model using data generated by DDPM. In addition, we show the data scaling also improves certified accuracy on CIFAR-100 substantially. ", "page_idx": 9}, {"type": "text", "text": "We find that the highest gains can be achieved for models where the generalization gap, i.e., the difference between training and testing accuracy, is high for the original model. When augmenting with generated data, the generalization gap is mostly eliminated across all models when a sufficient amount of additional images are used. As the generalization gap gets smaller, removing regularization techniques, such as dropout, and switching to learning rate schedulers aimed at better convergence yields additional improvements. We also note that increasing the number of epochs had the greatest impact when paired with generated data. Lastly, we observe that a considerable number of samples are noticeably more resistant to adversarial attacks than required by the $\\epsilon$ -bound. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Thomas Altstidl acknowledges the support by the Bavarian State Ministry of Health and Care, project grant number PBN-MGP-2010-0004-DigiOnko. Leo Schwinn gratefully acknowledges funding by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - Projectnumber 544579844. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Goodfellow, and R. Fergus, \u201cIntriguing properties of neural networks,\u201d in Proc. Intl. Conf. Learn. Representations (ICLR), Y. Bengio and Y. LeCun, Eds., Banff, AB, Canada, Apr. 2014.   \n[2] I. J. Goodfellow, J. Shlens, and C. Szegedy, \u201cExplaining and harnessing adversarial examples,\u201d in Proc. Intl. Conf. Learn. Representations (ICLR), San Diego, CA, USA, May 2015.   \n[3] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, \u201cTowards Deep Learning Models Resistant to Adversarial Attacks,\u201d in Proc. Intl. Conf. Learn. Representations (ICLR), Vancouver, BC, Canada, May 2018.   \n[4] J. M. Cohen, E. Rosenfeld, and J. Z. Kolter, \u201cCertified Adversarial Robustness via Randomized Smoothing,\u201d in Proc. Intl. Conf. Mach. Learn. (ICML), K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97. Long Beach, CA, USA: PMLR, Jun. 2019, pp. 1310\u20131320.   \n[5] S. Gowal, S. Rebuff,i O. Wiles, F. Stimberg, D. A. Calian, and T. A. Mann, \u201cImproving Robustness using Generated Data,\u201d in Adv. Neural Inf. Process. Syst. (NeurIPS), vol. 34, Virtual Event, Dec. 2021, pp. 4218\u20134233.   \n[6] Z. Wang, T. Pang, C. Du, M. Lin, W. Liu, and S. Yan, \u201cBetter Diffusion Models Further Improve Adversarial Training,\u201d in Proc. Intl. Conf. Mach. Learn. (ICML), Honolulu, HI, USA, Jul. 2023.   \n[7] L. Schwinn, R. Raab, A. Nguyen, D. Zanca, and B. M. Eskofier, \u201cExploring Misclassifications of Robust Neural Networks to Enhance Adversarial Attacks,\u201d Appl. Intell. (APIN), 2021.   \n[8] \u2014\u2014, \u201cImproving Robustness against Real-World and Worst-Case Distribution Shifts through Decision Region Quantification,\u201d in Proc. Intl. Conf. Mach. Learn. (ICML), K. Chaudhuri, S. Jegelka, L. Song, C. Szepesv\u00e1ri, G. Niu, and S. Sabato, Eds., vol. 162. Baltimore, MD, USA: PMLR, Jul. 2022, pp. 19 434\u201319 449.   \n[9] K. Hu, A. Zou, Z. Wang, K. Leino, and M. Fredrikson, \u201cUnlocking Deterministic Robustness Certification on ImageNet,\u201d in Adv. Neural Inf. Process. Syst. (NeurIPS), vol. 36, New Orleans, LA, USA, Dec. 2023.   \n[10] L. Li, T. Xie, and B. Li, \u201cSoK: Certified Robustness for Deep Neural Networks,\u201d in Proc. IEEE Symp. Secur. Privacy (SP), San Francisco, CA, USA, May 2023.   \n[11] H. Zhang, Y. Yu, J. Jiao, E. P. Xing, L. El Ghaoui, and M. I. Jordan, \u201cTheoretically principled tradeoff between robustness and accuracy,\u201d in Proc. Intl. Conf. Mach. Learn. (ICML), K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97. Long Beach, CA, USA: PMLR, Jun. 2019, pp. 7472\u20137482.   \n[12] M. Cheng, Q. Lei, P. Chen, I. S. Dhillon, and C. Hsieh, \u201cCAT: Customized Adversarial Training for Improved Robustness,\u201d in Proc. Intl. Joint Conf. Artif. Intell. (IJCAI), L. D. Raedt, Ed., Vienna, Austria, Jul. 2022, pp. 673\u2013679.   \n[13] S. Gowal, K. Dvijotham, R. Stanforth, R. Bunel, C. Qin, J. Uesato, R. Arandjelovic, T. A. Mann, and P. Kohli, \u201cScalable Verified Training for Provably Robust Image Classification,\u201d in Proc. Intl. Conf. Comput. Vision (ICCV), Seoul, Korea, Oct. 2019.   \n[14] M. N. M\u00fcller, F. Eckert, M. Fischer, and M. Vechev, \u201cCertified Training: Small Boxes are All You Need,\u201d in Proc. Intl. Conf. Learn. Representations (ICLR), Kigali, Rwanda, May 2023.   \n[15] Y. Mao, M. N. M\u00fcller, M. Fischer, and M. Vechev, \u201cConnecting Certified and Adversarial Training,\u201d in Adv. Neural Inf. Process. Syst. (NeurIPS), vol. 36, New Orleans, LA, USA, Dec. 2023, pp. 73 422\u201373 440.   \n[16] A. D. Palma, R. Bunel, K. Dvijotham, M. Kumar, R. Stanforth, and A. Lomuscio, \u201cExpressive Losses for Verified Robustness via Convex Combinations,\u201d in Proc. Intl. Conf. Learn. Representations (ICLR), Vienna, Austria, May 2024.   \n[17] X. Xu, L. Li, and B. Li, \u201cLOT: Layer-wise Orthogonal Training on Improving l2 Certified Robustness,\u201d in Adv. Neural Inf. Process. Syst. (NeurIPS), vol. 35, New Orleans, LA, USA, Dec. 2022, pp. 18 904\u201318 915.   \n[18] B. Zhang, D. Jiang, D. He, and L. Wang, \u201cRethinking Lipschitz Neural Networks and Certified Robustness: A Boolean Function Perspective,\u201d in Adv. Neural Inf. Process. Syst. (NeurIPS), vol. 35, New Orleans, LA, USA, Dec. 2022, pp. 19 398\u201319 413.   \n[19] L. Bungert, R. Raab, T. Roith, L. Schwinn, and D. Tenbrinck, \u201cCLIP: Cheap lipschitz training of neural networks,\u201d in Proc. Int. Conf. Scale Space Variational Methods Comput. Vision (SSVM), ser. Lecture Notes Comput. Sci., A. Elmoataz, J. Fadili, Y. Qu\u00e9au, J. Rabin, and L. Simon, Eds., vol. 12679. Virtual Event: Springer, May 2021, pp. 307\u2013319.   \n[20] P. Dhariwal and A. Nichol, \u201cDiffusion Models Beat GANs on Image Synthesis,\u201d in Adv. Neural Inf. Process. Syst. (NeurIPS), vol. 34. Virtual Event: Curran Associates, Inc., Dec. 2021, pp. 8780\u20138794.   \n[21] J. Ho, A. Jain, and P. Abbeel, \u201cDenoising Diffusion Probabilistic Models,\u201d in Adv. Neural Inf. Process. Syst. (NeurIPS), vol. 33, Vancouver, BC, Canada, Dec. 2020, pp. 6840\u20136851.   \n[22] A. Krizhevsky and G. Hinton, \u201cLearning multiple layers of features from tiny images,\u201d Master\u2019s thesis, Department of Computer Science, University of Toronto, 2009, publisher: Citeseer.   \n[23] T. Karras, M. Aittala, T. Aila, and S. Laine, \u201cElucidating the Design Space of Diffusion-Based Generative Models,\u201d in Adv. Neural Inf. Process. Syst. (NeurIPS), vol. 35, New Orleans, LA, USA, Dec. 2022, pp. 26 565\u201326 577.   \n[24] D. Kim, Y. Kim, S. J. Kwon, W. Kang, and I.-C. Moon, \u201cRefining Generative Process with Discriminator Guidance in Score-based Diffusion Models,\u201d in Proc. Intl. Conf. Mach. Learn. (ICML), Honolulu, HI, USA, Jul. 2023.   \n[25] D. Hendrycks, M. Mazeika, S. Kadavath, and D. Song, \u201cUsing Self-Supervised Learning Can Improve Model Robustness and Uncertainty,\u201d in Adv. Neural Inf. Process. Syst. (NeurIPS), vol. 32, Vancouver, BC, Canada, Dec. 2019, pp. 15 637\u201315 648.   \n[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImageNet Classification with Deep Convolutional Neural Networks,\u201d in Adv. Neural Inf. Process. Syst. (NeurIPS), vol. 25, Lake Tahoe, NV, USA, Dec. 2012, pp. 1097\u20131105.   \n[27] B. Zhang, D. Jiang, D. He, and L. Wang, \u201cBoosting the Certified Robustness of L-infinity Distance Nets,\u201d in Proc. Intl. Conf. Learn. Representations (ICLR), Virtual Event, Apr. 2022.   \n[28] I. Loshchilov and F. Hutter, \u201cSGDR: Stochastic Gradient Descent with Warm Restarts,\u201d in Proc. Intl. Conf. Learn. Representations (ICLR), Toulon, France, Apr. 2017.   \n[29] F. Croce, M. Andriushchenko, V. Sehwag, E. Debenedetti, N. Flammarion, M. Chiang, P. Mittal, and M. Hein, \u201cRobustBench: a standardized adversarial robustness benchmark,\u201d Tech. Rep., Oct. 2020, arXiv:2010.09670 [cs] type: article. [Online]. Available: http://arxiv.org/abs/2010.09670   \n[30] L. B\u00e9thune, T. Boissin, M. Serrurier, F. Mamalet, C. Friedrich, and A. Gonzalez Sanz, \u201cPay attention to your loss : understanding misconceptions about Lipschitz neural networks,\u201d in Adv. Neural Inf. Process. Syst. (NeurIPS), vol. 35, New Orleans, LA, USA, Dec. 2022, pp. 20 077\u201320 091.   \n[31] K. Roth, Y. Kilcher, and T. Hofmann, \u201cAdversarial Training is a Form of Data-dependent Operator Norm Regularization,\u201d in Adv. Neural Inf. Process. Syst. (NeurIPS), vol. 33, Virtual Event, Dec. 2020, pp. 14 973\u201314 985.   \n[32] R. Zhang and J. Sun, \u201cCertified Robust Accuracy of Neural Networks Are Bounded Due to Bayes Errors,\u201d in Proc. Intl. Conf. Comput. Aided Verification, Montr\u00e9al, QC, Canada, Jul. 2024.   \n[33] \u2014\u2014, \u201cHow Does Bayes Error Limit Probabilistic Robust Accuracy,\u201d Tech. Rep., May 2024, arXiv:2405.14923 [cs.LG] type: article. [Online]. Available: http://arxiv.org/abs/2405.14923   \n[34] T. Roith and L. Bungert, \u201cContinuum Limit of Lipschitz Learning on Graphs,\u201d Found. Comput. Math., vol. 23, pp. 393\u2013431, Jan. 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Model Details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Table 5 lists the different model configurations used. We limit our summary to the most important parameters \u2013 for the full parameters used during training we refer to the respective papers [9, 11, 17, 18] and repositories, as well as our own scripts provided in our code release. ", "page_idx": 12}, {"type": "table", "img_path": "TFAG9UznPv/tmp/fed3bfed4800d63cd4c4b498956fb0cf62a219dbf348d1b83ed31f3512536b10.jpg", "table_caption": ["Table 5: Model Configurations "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "B Results LOT Multi-Step Scheduler ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Table 6 summarizes the results of LOT trained with the multi-step learning rate scheduler used in the original paper. Overall the results are approximately $0.5\\%$ worse than using a cyclic learning rate. ", "page_idx": 12}, {"type": "table", "img_path": "TFAG9UznPv/tmp/185a467b82be5e5c08f8f04237e2ae7a31103e15223aef53f99a353c8662710b.jpg", "table_caption": ["Table 6: LOT w/ Multi-Step Scheduler, $\\ell_{2}$ $_{2,\\,\\epsilon}=36/255$ "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "C Full Results 1m/5m/10m ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table 7: Clean and certified test accuracy $(\\%)$ on CIFAR-10 $(\\ell_{\\infty},\\epsilon=8/255)$ for $\\ell_{\\infty}$ -dist Net and SortNet. Bold and italics highlight the best model with and without auxiliary data and underlining highlights the overall best model. $\\Delta$ Cert. denotes the highest absolute increase in certified robustness when using auxiliary data, and $e$ the number of epochs. ", "page_idx": 13}, {"type": "table", "img_path": "TFAG9UznPv/tmp/369e6306989fddd3ebe377c7ea99773bf7d6450d8505a76decb5838d28d2564c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Table 8: Clean and certified test accuracy $(\\%)$ on CIFAR-10 $\\zeta_{2},\\epsilon=36/255)$ for LOT and GloroNet. Bold and italics highlight the best model with and without auxiliary data and underlining highlights the overall best model. \u2206Cert. denotes the highest absolute increase in certified robustness when using auxiliary data, $e$ the number of epochs, and XS, S, M, and L the model size. ", "page_idx": 13}, {"type": "table", "img_path": "TFAG9UznPv/tmp/e82825f93072671156f5d829231d3bf6c5bcf4e4d9375354d65ef2c8c695b363.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "D Full Results CIFAR-100 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table 9: Clean and certified test accuracy $(\\%)$ on CIFAR-100 for both $(\\ell_{\\infty},\\epsilon\\;=\\;8/255)$ and $(\\ell_{2},\\epsilon=36/255)$ threat models. Bold highlights the best overall model for each architecture. ", "page_idx": 14}, {"type": "table", "img_path": "TFAG9UznPv/tmp/78b006b135079f7c816315f1fd01cd72addb9c42b06ff6fcf9fc3b4736e2edfe.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "E Robust Overfitting ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In Figure 5 we visualize the number of epochs between the epoch where the best-certified accuracy was achieved and the total amount of epochs trained on the $x$ -axis. On the $y$ -axis we plot the certified accuracy difference between the best and last epoch. Here, the auxiliary data used for the different configurations are visualized with unique colors and symbols. No clear connection between the amount of auxiliary data and the distance between the best and last epoch can be observed. However, the highest distance is observed for models using no auxiliary data. On average, the observed difference between the last and best epoch is small for all models and always below $0.5\\%$ . We conclude that robust overfitting is not an issue for the certified training approaches tested in our experiments. ", "page_idx": 14}, {"type": "image", "img_path": "TFAG9UznPv/tmp/75788bfb91e69d8091a4192818b5df9d95f8260e0d4e8d536494360266cb123a.jpg", "img_caption": ["Figure 5: Relationship between 1) the amount of auxiliary data used, 2) the number of epochs between the epoch where the best certified accuracy was achieved, and 3) the difference in certified accuracy between the best and last epoch. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "F Influence Model Size/Epoch Count ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Figure 6 visualizes the effects we also mention in our sensitivity analysis in Sec. 4.2. Looking at each row, increasing epoch counts yields to a linear increase in improvements, with the sole exception of GloroNet-XS. Model size similarly has a positive effect on certified accuracy, especially in combination with increases of epoch counts. ", "page_idx": 15}, {"type": "image", "img_path": "TFAG9UznPv/tmp/19689ca789e7ada1786e5eb8f15fe949564632943f98e383f40a10409eff2fdb.jpg", "img_caption": ["Figure 6: Influence of model size and increase in the number of epochs for CIFAR-10 $\\zeta_{2},\\epsilon=36/255)$ models. Color shading indicates average absolute improvement across 1m, 5m, and $10\\mathrm{m}$ auxiliary data over the same model trained without auxiliary data. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "G Certification ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Figure 7 illustrates the difference between the best base model (w/o auxiliary) and the best model trained with auxiliary data (w/ auxiliary). We investigate different combinations of correctness and certification for each image. An image may either be correctly or incorrectly classified, and either certified or not certified. If it is certified, this means that its certification radius is larger than $\\epsilon$ . ", "page_idx": 15}, {"type": "image", "img_path": "TFAG9UznPv/tmp/13a55748399ed9d7d97b493d70bacc33913b07b96124bd3af5fb65c1d837d723.jpg", "img_caption": ["Figure 7: Confusion matrices for different correctness and certification constellations between models trained with and without auxiliary data. Here, $\\neg$ means not, i.e., \u00acCert. means that images were not certified to be within the $\\epsilon$ -bound. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "H Convex Bound Propagation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Our focus on Lipschitz-bound methods may raise questions regarding the applicability of data scaling to other certified robustness domains, such as convex bound propagation. Our additional experiments on MTL-IBP [16], a summary of which is reproduced below, indicate vastly different scaling dynamics with no observed improvements even when further adjusting (being originally used by Palma et al. [16] for the larger ImageNet). A few remarks: ", "page_idx": 16}, {"type": "text", "text": "\u2022 Verification runs were aborted after 24 hours to accommodate the discussion period. If aborted, reported numbers indicate percentage of certified/correctly classified samples on the subset of CIFAR-10 test images for which verification completed within 24 hours.   \n\u2022 Non-completed runs included more hard-to-certify samples that required computationally expensive branch-and-bound verification. Extrapolated full runtimes for some configurations are up to about 2 weeks.   \n\u2022 Runs that yielded $0\\%$ certified accuracy oftentimes completed quickly as the comparatively cheap PDG attack already found counterexamples, and thus a full verification was not needed. ", "page_idx": 16}, {"type": "text", "text": "We conclude that MTL-IBP, a convex bound propagation approach, cannot benefit from additional generated data. This indicates that results with regards to scalability do not trivially transfer from one robustness method to another, and a detailed investigation of a single method, such as Lipschitz-bound approaches, seems justified. ", "page_idx": 16}, {"type": "text", "text": "Table 10: Clean (top) and certified (bottom) test accuracy $(\\%)$ on CIFAR-10 $(\\ell_{\\infty},\\epsilon=8/255)$ for MTL-IBP. Bold highlights the best overall model for each configuration. $e$ denotes the number of epochs, $\\alpha$ the expressive loss coefficient. ", "page_idx": 16}, {"type": "table", "img_path": "TFAG9UznPv/tmp/c3bf510ba2cc4b6bffb260d88eb030bd0d950a526d5f161c19120471b5de9c36.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "\\* Verification run did not finish within 24 hours, preliminary result on test data subset shown \u2020 Implementation by Palma et al. [16] triggered an assertion error ", "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Analysis of data scaling behavior for certified robustness described in both abstract and introduction ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: Limitations of the paper transfer from the models by Hu et al. [9], Xu et al. [17], Zhang et al. [18, 27] and the general approach of using generated data by Gowal et al. [5], Wang et al. [6], and hence are not separately discussed ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: No theoretical results provided ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Sec. 3 Experiment Setup describes in detail the configurations used for all experiments, and code is provided ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Datasets CIFAR-10 and CIFAR-100 are readily available, and link to code will be added upon acceptance ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Sec. 3 Experiment Setup describes in detail the configurations used for all experiments ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: With a single model run taking one to two days on a A100 graphics card it proved infeasible to perform multiple runs ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Sec. 3 Experiment Setup describes the compute resources used for experiments Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: No specific concerns to address in terms of potential harmfulness ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: Societal impact related to general impact of robustness research, i.e., robustness may improve safety of systems, but also cause emissions due to increased training times ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 20}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: No specific risks to address Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Both datasets (CIFAR-10 and CIFAR-100) and models ( $\\mathcal{l}_{\\infty}$ -dist Net, SortNet, LOT, and GloroNet) are referenced ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: No new assets provided ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: No human subjects involved ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: No human subjects involved ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}]