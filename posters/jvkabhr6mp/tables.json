[{"figure_path": "JVKABhr6mP/tables/tables_5_1.jpg", "caption": "Table 1: Comparison with the current existing open-source LLVMs, evaluating vision language performances of Meteor on numerous evaluation benchmarks requiring diverse capabilities: Q-Bench [95], SQA\u00b9 [96], AI2D [38], ChartQA [80], SEED\u00b9 [97], POPE [98], HallB [99], MME [35], MathVista [37], MMB [36], MMBCN [36], MM-Vet [100], and LLaVAW [3]. Note that, AI2D and ChartQA performances for LLaVA family models are evaluated under zero-shot conditions, while Meteor uses training dataset for them.", "description": "This table compares the performance of the proposed Meteor model with various open-source Large Language and Vision Models (LLVMs) across multiple evaluation benchmarks.  The benchmarks test diverse vision-language capabilities, including image understanding, common-sense knowledge, diagram interpretation, and mathematical reasoning.  The table highlights Meteor's improved performance, especially when compared to similar-sized models and those without additional vision encoders or computer vision models. Note that some benchmarks use zero-shot settings for certain models, but Meteor uses the training dataset for a fair comparison.", "section": "Experiment"}, {"figure_path": "JVKABhr6mP/tables/tables_6_1.jpg", "caption": "Table 1: Comparison with the current existing open-source LLVMs, evaluating vision language performances of Meteor on numerous evaluation benchmarks requiring diverse capabilities: Q-Bench [95], SQA\u00b9 [96], AI2D [38], ChartQA [80], SEED\u00b9 [97], POPE [98], HallB [99], MME [35], MathVista [37], MMB [36], MMBCN [36], MM-Vet [100], and LLaVAW [3]. Note that, AI2D and ChartQA performances for LLaVA family models are evaluated under zero-shot conditions, while Meteor uses training dataset for them.", "description": "This table compares the performance of Meteor with other open-source Large Language and Vision Models (LLVMs) across various vision-language benchmarks.  The benchmarks assess diverse capabilities including image understanding, common-sense knowledge, and non-object concept understanding.  Note that some comparisons are made under different testing conditions (zero-shot vs. using a training dataset).", "section": "4 Experiment"}, {"figure_path": "JVKABhr6mP/tables/tables_6_2.jpg", "caption": "Table 2: Detailed comparison of Meteor across more challenging evaluation benchmarks.", "description": "This table compares the performance of Meteor against other LLMs on more challenging benchmarks that require diverse capabilities such as image understanding, real-world knowledge, and non-object concept understanding.  It shows Meteor's performance on several tasks (MMStar, MathVerse, etc.) across various model sizes.  The results highlight Meteor's efficiency and its improvements over various open- and closed-source LLMs, even those using additional vision encoders or multiple computer vision models.", "section": "4 Experiment"}, {"figure_path": "JVKABhr6mP/tables/tables_6_3.jpg", "caption": "Table 2: Detailed comparison of Meteor across more challenging evaluation benchmarks.", "description": "This table presents a detailed comparison of the Meteor model's performance against other state-of-the-art Large Language and Vision models (LLVMs) across various challenging evaluation benchmarks.  It highlights Meteor's capabilities in different aspects of vision and language understanding and how it compares to other models that employ additional vision encoders or computer vision models. The benchmarks used represent diverse and complex tasks requiring multifaceted information and capabilities.", "section": "Experiment"}, {"figure_path": "JVKABhr6mP/tables/tables_7_1.jpg", "caption": "Table 3: Ablation studies to identify the effectiveness of Meteor-Mamba and rationale through traversal of rationale by controlling the six main factors.", "description": "This table presents ablation study results to analyze the impact of different factors on the performance of Meteor, a multimodal language and vision model.  The factors include the use of Meteor-Mamba (the embedding module), Meteor-MLM (the main model), the number and distribution of special tokens (<tor>), the inclusion of rationales, and the ratio of question-rationale pairs in the training data.  Each row represents a different experimental configuration, demonstrating the relative contribution of each component to the overall performance. The results highlight the importance of the multifaceted rationale, the Mamba architecture, and the appropriate placement of <tor> tokens for optimal performance.", "section": "Ablation Studies"}, {"figure_path": "JVKABhr6mP/tables/tables_28_1.jpg", "caption": "Table 2: Detailed comparison of Meteor across more challenging evaluation benchmarks.", "description": "This table presents a detailed comparison of the Meteor model's performance against other LLMs on more challenging evaluation benchmarks.  It highlights Meteor's performance across various tasks requiring diverse capabilities such as image understanding, common-sense reasoning, and non-object concept understanding. The benchmarks used include MMStar, MathVerse, and a comparison against LLMs using additional vision encoders and computer vision models.", "section": "4 Experiment"}, {"figure_path": "JVKABhr6mP/tables/tables_31_1.jpg", "caption": "Table 4: Performance comparison between Meteor and Meteor-LLaVA-HR across multiple tasks.", "description": "This table presents a performance comparison between the proposed model, Meteor, and a variant incorporating the LLaVA-HR model, across six different vision-language benchmarks (AI2D, ChartQA, MathVista, MM-Vet, LLaVAW, and MMStar).  Each benchmark assesses various aspects of vision-language capabilities, allowing for a comprehensive evaluation of the models' performance on diverse tasks.", "section": "D Further Ablation Studies"}, {"figure_path": "JVKABhr6mP/tables/tables_31_2.jpg", "caption": "Table 5: Performance of Meteor-Mamba models with different sizes across various tasks.", "description": "This table presents the performance results of Meteor-Mamba models with varying sizes (130M, 790M, and 1.4B parameters) across six different vision-language tasks: AI2D, ChartQA, MathVista, MM-Vet, LLaVAW, and MMStar.  It shows how the performance on these tasks changes as the model size increases.  The results demonstrate the impact of model size on the various vision-language capabilities assessed by these benchmark datasets.", "section": "D Further Ablation Studies"}, {"figure_path": "JVKABhr6mP/tables/tables_31_3.jpg", "caption": "Table 6: Token processing speed comparison between models.", "description": "This table compares the token processing speed of five different large language and vision models: Qwen-VL, LLaVA1.5, CoLLaVO, MoAI, and Meteor.  The speed is measured in tokens per second.  The table shows that Meteor and LLaVA1.5 have similar processing speeds, which are faster than CoLLaVO and MOAI. Qwen-VL has the slowest processing speed.", "section": "Experiment"}, {"figure_path": "JVKABhr6mP/tables/tables_31_4.jpg", "caption": "Table 7: Performance comparison of LLVMs across various tasks.", "description": "This table compares the performance of various Large Language and Vision Models (LLVMs) across multiple vision-language tasks. The tasks are: VQAv2, GQA, SQA-IMG, TextVQA, POPE, MMB, and MM-Vet.  Each task assesses different aspects of visual and language understanding.  Meteor is shown to have superior performance in most cases.", "section": "4 Experiment"}, {"figure_path": "JVKABhr6mP/tables/tables_31_5.jpg", "caption": "Table 8: Performance comparison of LLVMs across Conversation, Detail Description, Complex Reasoning, and Average in LLaVAW.", "description": "This table compares the performance of different Large Language and Vision Models (LLVMs) on three specific tasks within the LLaVAW benchmark: Conversation, Detail Description, and Complex Reasoning.  The models compared are CoLLaVO, MoAI, Meteor without the Meteor-Mamba component, and Meteor with Meteor-Mamba.  The average performance across all three tasks is also provided.  The results show a significant performance improvement when using Meteor with Meteor-Mamba compared to the other models, highlighting the benefit of incorporating multifaceted rationale.", "section": "4 Experiment"}]