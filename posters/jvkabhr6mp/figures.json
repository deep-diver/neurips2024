[{"figure_path": "JVKABhr6mP/figures/figures_1_1.jpg", "caption": "Figure 1: Across 7B to over 110B parameters, comparing lots of open- and closed-source LLVMs with Meteor on MME [35], MMB [36], MathVista [37], and AI2D [38] requiring diverse capabilities for image understanding, common-sense knowledge, non-object concept understanding, etc.", "description": "This figure compares the performance of Meteor against various open- and closed-source Large Language and Vision Models (LLVMs) across four different evaluation benchmarks: MME, MMB, MathVista, and AI2D.  Each benchmark assesses different aspects of LLM capabilities, including image understanding, common sense reasoning, and non-object concept recognition. The figure shows that Meteor, despite its relatively smaller size, achieves comparable or superior performance to much larger models on these diverse tasks.", "section": "1 Introduction"}, {"figure_path": "JVKABhr6mP/figures/figures_2_1.jpg", "caption": "Figure 1: Across 7B to over 110B parameters, comparing lots of open- and closed-source LLVMs with Meteor on MME [35], MMB [36], MathVista [37], and AI2D [38] requiring diverse capabilities for image understanding, common-sense knowledge, non-object concept understanding, etc.", "description": "This figure compares the performance of Meteor against various open- and closed-source Large Language and Vision Models (LLVMs) across multiple evaluation benchmarks.  These benchmarks (MME, MMB, MathVista, and AI2D) test diverse capabilities, including image understanding, common sense reasoning, and non-object concept comprehension.  The figure visually demonstrates Meteor's performance improvements across a wide range of model sizes.", "section": "1 Introduction"}, {"figure_path": "JVKABhr6mP/figures/figures_4_1.jpg", "caption": "Figure 3: Overview of Meteor architecture and its training steps. Note that, 'Meteor-Multimodal Language Model (MLM)' indicates that as training progresses, the pretrained language model evolves into a multimodal language model.", "description": "This figure illustrates the architecture of Meteor and its two-step training process.  The first step involves training Meteor-Mamba (a Mamba-based architecture) to embed long sequential rationales using question-rationale pairs.  The second step trains the entire Meteor model (including Meteor-Mamba and Meteor-MLM, a multimodal language model) on question-rationale-answer triples, leveraging the embedded rationales from the first step. The figure visually represents how the input (image, question, rationale) is processed through the vision encoder, vision projector, Meteor-Mamba, tor projector, and finally the Meteor-MLM to generate the answer. The use of '<tor>' tokens highlights the concept of \"traversal of rationale\" for efficient embedding and passing of information.", "section": "3 Meteor: Mamba-based traversal of rationale"}, {"figure_path": "JVKABhr6mP/figures/figures_8_1.jpg", "caption": "Figure 4: Illuminating how the feature correspondences of cosine similarity are computed under the trained Meteor-Mamba, and showing the feature disparity for <tor> with/without rationale.", "description": "This figure shows the cosine similarity matrix between the feature vectors obtained from Meteor-Mamba with and without rationales. The diagonal values show high similarity, indicating successful embedding of rationales, while off-diagonal values indicate low similarity.  This visualization supports the claim that Meteor-Mamba effectively embeds the rationales even without explicit rationales.", "section": "Meteor-Mamba's Ability to Embed Rationales"}, {"figure_path": "JVKABhr6mP/figures/figures_17_1.jpg", "caption": "Figure 1: Across 7B to over 110B parameters, comparing lots of open- and closed-source LLVMs with Meteor on MME [35], MMB [36], MathVista [37], and AI2D [38] requiring diverse capabilities for image understanding, common-sense knowledge, non-object concept understanding, etc.", "description": "This figure compares the performance of Meteor against various open- and closed-source Large Language and Vision Models (LLVMs) across several benchmarks.  The benchmarks (MME, MMB, MathVista, AI2D) evaluate diverse capabilities, including image understanding, common sense reasoning, and non-object concept understanding. The figure demonstrates Meteor's competitive performance across a range of model sizes, highlighting its efficiency in achieving strong results without requiring large model sizes or additional computer vision modules.", "section": "1 Introduction"}, {"figure_path": "JVKABhr6mP/figures/figures_18_1.jpg", "caption": "Figure 1: Across 7B to over 110B parameters, comparing lots of open- and closed-source LLVMs with Meteor on MME [35], MMB [36], MathVista [37], and AI2D [38] requiring diverse capabilities for image understanding, common-sense knowledge, non-object concept understanding, etc.", "description": "This figure compares the performance of Meteor against various open- and closed-source LLMs across different benchmarks (MME, MMB, MathVista, and AI2D).  These benchmarks test a range of capabilities, including image understanding, common sense reasoning, and understanding non-object concepts. The figure shows that Meteor achieves significant performance improvements across all benchmarks, even with a smaller model size compared to other LLMs.", "section": "1 Introduction"}, {"figure_path": "JVKABhr6mP/figures/figures_19_1.jpg", "caption": "Figure 1: Across 7B to over 110B parameters, comparing lots of open- and closed-source LLVMs with Meteor on MME [35], MMB [36], MathVista [37], and AI2D [38] requiring diverse capabilities for image understanding, common-sense knowledge, non-object concept understanding, etc.", "description": "This figure compares the performance of Meteor against various open- and closed-source Large Language and Vision Models (LLVMs) across multiple evaluation benchmarks.  These benchmarks assess diverse capabilities, including image understanding, common sense reasoning, and non-object concept understanding.  The results show Meteor's performance relative to other LLVMs of varying sizes.", "section": "Abstract"}, {"figure_path": "JVKABhr6mP/figures/figures_19_2.jpg", "caption": "Figure 1: Across 7B to over 110B parameters, comparing lots of open- and closed-source LLVMs with Meteor on MME [35], MMB [36], MathVista [37], and AI2D [38] requiring diverse capabilities for image understanding, common-sense knowledge, non-object concept understanding, etc.", "description": "This figure compares the performance of Meteor against various open- and closed-source Large Language and Vision Models (LLVMs) across multiple evaluation benchmarks.  These benchmarks test diverse capabilities, such as image understanding, common sense reasoning, and non-object concept understanding.  The results show that Meteor achieves significant improvements over the other LLVMs, even those with substantially larger parameter counts, on a range of tasks.", "section": "1 Introduction"}, {"figure_path": "JVKABhr6mP/figures/figures_20_1.jpg", "caption": "Figure 1: Across 7B to over 110B parameters, comparing lots of open- and closed-source LLVMs with Meteor on MME [35], MMB [36], MathVista [37], and AI2D [38] requiring diverse capabilities for image understanding, common-sense knowledge, non-object concept understanding, etc.", "description": "This figure compares the performance of Meteor against various other open- and closed-source Large Language and Vision Models (LLVMs) across multiple evaluation benchmarks.  The benchmarks (MME, MMB, MathVista, AI2D) test diverse capabilities including image understanding, common sense reasoning, and non-object concept understanding.  The figure shows that Meteor achieves significant improvements, even with smaller model sizes, compared to other models.", "section": "1 Introduction"}, {"figure_path": "JVKABhr6mP/figures/figures_20_2.jpg", "caption": "Figure 1: Across 7B to over 110B parameters, comparing lots of open- and closed-source LLVMs with Meteor on MME [35], MMB [36], MathVista [37], and AI2D [38] requiring diverse capabilities for image understanding, common-sense knowledge, non-object concept understanding, etc.", "description": "This figure compares the performance of Meteor against various open and closed-source Large Language and Vision Models (LLVMs) across four different benchmark datasets: MME, MMB, MathVista, and AI2D.  These benchmarks test a wide range of capabilities, including image understanding, common sense reasoning, and non-object concept understanding. The figure showcases Meteor's performance relative to other models, demonstrating its ability to achieve competitive results without needing larger model sizes or additional vision encoders.", "section": "1 Introduction"}, {"figure_path": "JVKABhr6mP/figures/figures_21_1.jpg", "caption": "Figure 1: Across 7B to over 110B parameters, comparing lots of open- and closed-source LLVMs with Meteor on MME [35], MMB [36], MathVista [37], and AI2D [38] requiring diverse capabilities for image understanding, common-sense knowledge, non-object concept understanding, etc.", "description": "This figure compares the performance of Meteor against various open-source and closed-source large language and vision models (LLVMs) across four different benchmark datasets: MME, MMB, MathVista, and AI2D.  Each dataset tests different aspects of LLM capabilities, such as image understanding, common sense reasoning, and non-object concept understanding.  The figure demonstrates that Meteor achieves significant improvements across these benchmarks without increasing the model size or using additional vision encoders.", "section": "Abstract"}, {"figure_path": "JVKABhr6mP/figures/figures_21_2.jpg", "caption": "Figure 1: Across 7B to over 110B parameters, comparing lots of open- and closed-source LLVMs with\n\u2022 Meteor on MME [35], MMB [36], MathVista [37], and AI2D [38] requiring diverse capabilities\nfor image understanding, common-sense knowledge, non-object concept understanding, etc.", "description": "This figure compares the performance of Meteor against various open-source and closed-source Large Language and Vision Models (LLVMs) across multiple evaluation benchmarks (MME, MMB, MathVista, and AI2D).  These benchmarks assess diverse capabilities, including image understanding, common-sense reasoning, and non-object concept understanding.  The figure demonstrates that Meteor achieves significant improvements compared to other models without increasing model size or using additional vision encoders.", "section": "1 Introduction"}, {"figure_path": "JVKABhr6mP/figures/figures_22_1.jpg", "caption": "Figure 1: Across 7B to over 110B parameters, comparing lots of open- and closed-source LLVMs with Meteor on MME [35], MMB [36], MathVista [37], and AI2D [38] requiring diverse capabilities for image understanding, common-sense knowledge, non-object concept understanding, etc.", "description": "This figure compares the performance of Meteor against various open and closed-source Large Language and Vision Models (LLVMs) across different benchmarks.  The benchmarks (MME, MMB, MathVista, and AI2D) test diverse capabilities, including image understanding, common sense reasoning, and non-object concept understanding.  The figure shows that Meteor achieves significant improvements compared to other models, even those with substantially larger numbers of parameters.", "section": "1 Introduction"}, {"figure_path": "JVKABhr6mP/figures/figures_22_2.jpg", "caption": "Figure 1: Across 7B to over 110B parameters, comparing lots of open- and closed-source LLVMs with Meteor on MME [35], MMB [36], MathVista [37], and AI2D [38] requiring diverse capabilities for image understanding, common-sense knowledge, non-object concept understanding, etc.", "description": "This figure compares the performance of Meteor against various open-source and closed-source Large Language and Vision Models (LLVMs) across four different benchmark datasets (MME, MMB, MathVista, AI2D).  The datasets evaluate diverse capabilities, including image understanding, common sense reasoning, and non-object concept understanding.  The x-axis likely represents the model size (parameters) while the y-axis shows the performance scores on each benchmark.  The figure demonstrates Meteor's superior performance across all benchmarks and various model sizes.", "section": "1 Introduction"}, {"figure_path": "JVKABhr6mP/figures/figures_25_1.jpg", "caption": "Figure 1: Across 7B to over 110B parameters, comparing lots of open- and closed-source LLVMs with Meteor on MME [35], MMB [36], MathVista [37], and AI2D [38] requiring diverse capabilities for image understanding, common-sense knowledge, non-object concept understanding, etc.", "description": "This figure compares the performance of Meteor against other open- and closed-source Large Language and Vision Models (LLVMs) across various benchmarks (MME, MMB, MathVista, and AI2D).  These benchmarks assess diverse capabilities, including image understanding, common-sense reasoning, and non-object concept comprehension. The figure visually demonstrates Meteor's performance improvements across different model sizes (7B to over 110B parameters).", "section": "Abstract"}, {"figure_path": "JVKABhr6mP/figures/figures_25_2.jpg", "caption": "Figure 1: Across 7B to over 110B parameters, comparing lots of open- and closed-source LLVMs with Meteor on MME [35], MMB [36], MathVista [37], and AI2D [38] requiring diverse capabilities for image understanding, common-sense knowledge, non-object concept understanding, etc.", "description": "This figure compares the performance of Meteor against various open- and closed-source Large Language and Vision Models (LLVMs) across multiple evaluation benchmarks.  The benchmarks (MME, MMB, MathVista, and AI2D) test diverse capabilities including image understanding, common-sense reasoning, and non-object concept understanding.  The figure demonstrates Meteor's improved performance across a wide range of model sizes.", "section": "1 Introduction"}, {"figure_path": "JVKABhr6mP/figures/figures_26_1.jpg", "caption": "Figure 1: Across 7B to over 110B parameters, comparing lots of open- and closed-source LLVMs with Meteor on MME [35], MMB [36], MathVista [37], and AI2D [38] requiring diverse capabilities for image understanding, common-sense knowledge, non-object concept understanding, etc.", "description": "This figure compares the performance of Meteor against various open and closed-source Large Language and Vision Models (LLVMs) across multiple evaluation benchmarks. The benchmarks assess diverse capabilities, including image understanding, common-sense reasoning, and non-object concept understanding. The figure shows Meteor's performance across different model sizes (from 7B to over 110B parameters), highlighting its improvements over other LLVMs without requiring additional vision encoders or computer vision models.", "section": "1 Introduction"}, {"figure_path": "JVKABhr6mP/figures/figures_26_2.jpg", "caption": "Figure 3: Overview of Meteor architecture and its training steps. Note that, \u2018Meteor-Multimodal Language Model (MLM)\u2019 indicates that as training progresses, the pretrained language model evolves into a multimodal language model.", "description": "This figure provides a visual representation of the Meteor architecture and training process.  It shows two main steps: the first training step focuses on training Meteor-Mamba (the Mamba architecture component) to efficiently embed long sequential rationales. The second training step involves jointly training all components of Meteor (Meteor-Mamba, vision projector, tor projector, and Meteor-MLM) using curated question-rationale-answer triples. The figure highlights the flow of information during training and the role of each component in generating answers.", "section": "3 Meteor: Mamba-based traversal of rationale"}, {"figure_path": "JVKABhr6mP/figures/figures_27_1.jpg", "caption": "Figure 1: Across 7B to over 110B parameters, comparing lots of open- and closed-source LLVMs with Meteor on MME [35], MMB [36], MathVista [37], and AI2D [38] requiring diverse capabilities for image understanding, common-sense knowledge, non-object concept understanding, etc.", "description": "This figure compares the performance of Meteor against various open- and closed-source Large Language and Vision Models (LLVMs) across different benchmark datasets.  The benchmarks (MME, MMB, MathVista, and AI2D) test a range of capabilities including image understanding, common sense reasoning, and non-object concept recognition. The figure demonstrates Meteor's performance improvement across various model sizes, highlighting its efficiency in achieving strong results without needing to scale up the model size or utilize additional computer vision models.", "section": "1 Introduction"}, {"figure_path": "JVKABhr6mP/figures/figures_27_2.jpg", "caption": "Figure 1: Across 7B to over 110B parameters, comparing lots of open- and closed-source LLVMs with Meteor on MME [35], MMB [36], MathVista [37], and AI2D [38] requiring diverse capabilities for image understanding, common-sense knowledge, non-object concept understanding, etc.", "description": "This figure compares the performance of Meteor against various open- and closed-source Large Language and Vision Models (LLVMs) across multiple evaluation benchmarks.  The benchmarks (MME, MMB, MathVista, and AI2D) test diverse capabilities, including image understanding, common sense reasoning, and non-object concept understanding.  The figure highlights Meteor's performance improvements without increasing model size or adding vision encoders.", "section": "1 Introduction"}, {"figure_path": "JVKABhr6mP/figures/figures_28_1.jpg", "caption": "Figure 1: Across 7B to over 110B parameters, comparing lots of open- and closed-source LLVMs with Meteor on MME [35], MMB [36], MathVista [37], and AI2D [38] requiring diverse capabilities for image understanding, common-sense knowledge, non-object concept understanding, etc.", "description": "This figure compares the performance of various open-source and closed-source Large Language and Vision Models (LLVMs) against the proposed model, Meteor, across multiple evaluation benchmarks (MME, MMB, MathVista, AI2D).  The benchmarks test diverse capabilities, including image understanding, common-sense knowledge, and non-object concept understanding.  The figure shows Meteor outperforming most other models, even those with significantly larger parameter counts.", "section": "1 Introduction"}, {"figure_path": "JVKABhr6mP/figures/figures_29_1.jpg", "caption": "Figure 1: Across 7B to over 110B parameters, comparing lots of open- and closed-source LLVMs with Meteor on MME [35], MMB [36], MathVista [37], and AI2D [38] requiring diverse capabilities for image understanding, common-sense knowledge, non-object concept understanding, etc.", "description": "This figure compares the performance of Meteor against various open- and closed-source Large Language and Vision Models (LLVMs) across multiple evaluation benchmarks.  The benchmarks (MME, MMB, MathVista, and AI2D) require diverse capabilities, testing image understanding, common-sense reasoning, and understanding of non-object concepts.  The figure visually represents the performance differences, highlighting Meteor's improved capabilities across different model sizes (7B to 110B+ parameters).", "section": "1 Introduction"}, {"figure_path": "JVKABhr6mP/figures/figures_29_2.jpg", "caption": "Figure 1: Across 7B to over 110B parameters, comparing lots of open- and closed-source LLVMs with Meteor on MME [35], MMB [36], MathVista [37], and AI2D [38] requiring diverse capabilities for image understanding, common-sense knowledge, non-object concept understanding, etc.", "description": "This figure compares the performance of Meteor against various open-source and closed-source Large Language and Vision Models (LLVMs) across multiple evaluation benchmarks.  These benchmarks test diverse capabilities, including image understanding, common-sense reasoning, and understanding of non-object concepts. The results show that Meteor achieves significant improvements compared to other models without increasing model size or adding additional vision encoders.", "section": "1 Introduction"}, {"figure_path": "JVKABhr6mP/figures/figures_30_1.jpg", "caption": "Figure 1: Across 7B to over 110B parameters, comparing lots of open- and closed-source LLVMs with Meteor on MME [35], MMB [36], MathVista [37], and AI2D [38] requiring diverse capabilities for image understanding, common-sense knowledge, non-object concept understanding, etc.", "description": "This figure compares the performance of Meteor against various open and closed source Large Language and Vision Models (LLVMs) across a range of evaluation benchmarks that test diverse capabilities like image understanding, commonsense reasoning, and non-object concept recognition.  The benchmarks used are MME, MMB, MathVista, and AI2D. The figure shows that Meteor performs competitively even with significantly larger models, highlighting its efficiency in leveraging multifaceted rationales.", "section": "Abstract"}, {"figure_path": "JVKABhr6mP/figures/figures_30_2.jpg", "caption": "Figure 3: Overview of Meteor architecture and its training steps. Note that, 'Meteor-Multimodal Language Model (MLM)' indicates that as training progresses, the pretrained language model evolves into a multimodal language model.", "description": "This figure illustrates the architecture of the Meteor model and its two-step training process.  The first step trains the Meteor-Mamba component (a Mamba architecture module) to embed long sequential rationales. The second step jointly trains all components of Meteor (including Meteor-Mamba, a vision projector, a tor projector, and Meteor-MLM, which is a multimodal language model based on a pretrained LLM) using question-answer pairs. The figure visually explains the flow of information and the roles of each component in the model. The concept of 'traversal of rationale' is also depicted, showing how lengthy rationales are effectively handled using special tokens.", "section": "3 Meteor: Mamba-based traversal of rationale"}]