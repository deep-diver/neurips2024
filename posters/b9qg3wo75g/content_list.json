[{"type": "text", "text": "Generative Fractional Diffusion Models ", "text_level": 1, "page_idx": 0}, {"type": "image", "img_path": "B9qg3wo75g/tmp/60fd5c3ee2503e89d0fd8f7e950e001326d1ba78a00e4a51b36586daee8af11b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We introduce the first continuous-time score-based generative model that leverages fractional diffusion processes for its underlying dynamics. Although diffusion models have excelled at capturing data distributions, they still suffer from various limitations such as slow convergence, mode-collapse on imbalanced data, and lack of diversity. These issues are partially linked to the use of light-tailed Brownian motion (BM) with independent increments. In this paper, we replace BM with an approximation of its non-Markovian counterpart, fractional Brownian motion (fBM), characterized by correlated increments and Hurst index $H\\in(0,1)$ , where $H=0.5$ recovers the classical BM. To ensure tractable inference and learning, we employ a recently popularized Markov approximation of fBM (MA-fBM) and derive its reverse-time model, resulting in generative fractional diffusion models (GFDM). We characterize the forward dynamics using a continuous reparameterization trick and propose augmented score matching to efficiently learn the score function, which is partly known in closed form, at minimal added cost. The ability to drive our diffusion model via MA-fBM offers flexibility and control. $H\\leq0.5$ enters the regime of rough paths whereas $H\\,>\\,0.5$ regularizes diffusion paths and invokes long-term memory. The Markov approximation allows added control by varying the number of Markov processes linearly combined to approximate fBM. Our evaluations on real image datasets demonstrate that GFDM achieves greater pixel-wise diversity and enhanced image quality, as indicated by a lower FID, offering a promising alternative to traditional diffusion models1. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent years have witnessed a remarkable leap in generative diffusion models [1, 2, 3], celebrated for their ability to accurately learn data distributions and generate high-fidelity samples. These models have made significant impact across a wide spectrum of application domains, including the generation of complex molecular structures [4] for material [5] or drug discovery [6], realistic audio samples [7, 8], 3D objects [9, 10] or textures [11], medical images [12], aerospace applications [13], and DNA sequence design [14, 15]. Despite these successes, modern score-based generative models formulated in continuous-time [16] face limitations due to their reliance on a simplistic driving noise, the Brownian motion (BM) [17, 18, 19]. As a light-tailed process, using BM can results in slow convergence rates and susceptibility to mode-collapse, especially with imbalanced data [20]. Additionally, its purely Markovian nature may also make it hard to capture the full complexity and richness of real-world data. All these attracted a number of attempts for involving different noise types [20, 21]. In this paper, we propose leveraging fractional noises, particularly the renowned non-Markovian fractional BM (fBM) [22, 23] to drive diffusion models. fBM extends BM to stationary increments with a more complex dependence structure, i.e., long-range dependence vs. roughness/regularity controlled by a Hurst index, a measure of \"mild\" or \"wild\" randomness [24]. This all comes at the expense of computational challenges and intractability of inference, mostly stemming from its non-Markovian nature. Moreover, deriving a reverse-time model poses theoretical challenges, as fBM is not only non-Markovian but also not a semimartingale [25]. To overcome these limitations, we leverage recent works in Markov approximations of fBM (MA-fBM) [26, 27] and establish a framework for training continuous-time score-based generative models using an approximate fractional diffusion process, as well as generating samples from the corresponding tractable reverse process. Notably, our method maintains the same number of score model evaluations during both training and data generation, with only a minimal increase in computational load. Our contributions are: ", "page_idx": 0}, {"type": "image", "img_path": "B9qg3wo75g/tmp/9a233b3e1e78f949dd0605e41150bfc0e09b985d5838ff3e114994aaa39755ef.jpg", "img_caption": ["Figure 1: Each data dimension transitions to a known prior distribution through a forward process that approximates a fractional diffusion process. The Hurst index $H$ on the LHS interpolates between the roughness of a Brownian driven SDE and the underlying integration in PF ODEs. The driving noise process is a linear combination of the correlated processes on the RHS, all driven by the same Brownian motion. The score function of these augmenting processes is available in closed form and serves as guidance for the unknown score function. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "\u2022 We derive the time-reversal of forward dynamics driven by a Markovian approximation of fractional Brownian motion in a way that the dimensionality of the unknown part of the score function matches that of the data.   \n\u2022 We derive an explicit formulae for the marginals of the conditional forward process via a continuous reparameterization trick.   \n\u2022 We introduce a novel augmented score matching loss for learning the score function in our generative fractional diffusion model, which can be minimized by a score model of data-dimension.   \nOur experimental evaluation validates our contributions, demonstrating the gains of correlated-noise ", "page_idx": 1}, {"type": "text", "text": "with long-term memory, approximated by a combination of a number of Markov processes, where the amount of processes further control the diverstiy. ", "page_idx": 1}, {"type": "text", "text": "Differentiation from existing work. Yoon et al. [20] generalizes score-based generative models from an underlying BM to a driving L\u00e9vy process, a stochastic process with independent and stationary increments. A driving noise with correlated increments is not included in the framework of Yoon et al. [20]. Conceptually, every L\u00e9vy process is a semimartingale [28]. Since fBM is not a L\u00e9vy process, it is not included in the framework of Yoon et al. [20]. The closest work to ours is Tong et al. [29] constructing a neural-SDE based on correlated noise and using the neural SDE as a forward process of a score-based generative model. Our framework with exact reverse-time model is based on the integral representation of fBM derived in Harms and Stefanovits [26] and the optimal approximation coefficients of Daems et al. [27], while the fractional noise in [29] is sparsely approximated by a linear combination of independent standard normal random variables without exact reverse-time model. Moreover, the framework of Tong et al. [29] is limited to $H>\\textstyle{\\frac{1}{3}}$ and only compatible with the Euler-Maruyama sample schema [30] while our framework is up to numerical stability applicable for any $H\\in(0,1)$ and compatible with any suitable SDE or ODE solver. To the best of our knowledge, we are the first to build a framework for continuous-time score-based generative models that includes driving noise processes converging to non-Markovian processes with infinite quadratic variation. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Modeling the distribution transforming process of a score-based generative model through stochastic differential equations (SDEs) [16] offers a unifying framework to generate data from an unknown probability distribution. Instead of injecting a finite number of fixed noise scales via a Markov chain, infinitely many noise scales tailored to the continuous dynamics of the Markov process $\\mathbf{X}=(\\mathbf{X}_{t})_{t\\in[0,T]}$ are utilized during the distribution transformation, offering considerable practical advantages over discrete time diffusion models [16]. The forward dynamics, transitioning from a data sample $\\mathbf{X}_{0}\\sim p_{0}$ to a tractable noise sample $\\mathbf{X}_{T}\\sim p_{T}$ are specified by a continuous drift function f and a continuous diffusion coefficient $g$ . These dynamics define a diffusion process that solves the SDE ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm d\\mathbf X_{t}=\\mathbf f(\\mathbf X_{t},t)\\mathrm d t+g(t)\\mathrm d\\mathbf B_{t},\\quad\\mathbf X_{0}\\sim p_{0}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "driven by a multivariate BM B. To sample data from noise, a reverse-time model is needed that defines the backward transformation from the tractable noise distribution to the data distribution. Whenever $\\mathbf{X}=(\\mathbf{X}_{t})_{t\\in[0,T]}$ is a stochastic process and $g$ is a function on $[0,T]$ , we write $\\overline{{\\mathbf{X}}}_{t}=\\mathbf{X}_{T-t}$ for the reverse-time model and $\\bar{g}(t)=g(T-t)$ for the reverse-time function. The marginal density of the stochastic process $\\mathbf{X}$ at time $t$ is denoted by $p_{t}$ throughout this work2. Remarkably, an exact reverse-time model to the forward model in eq. (1) is given by the backward dynamics [31, 32, 33] ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\overline{{\\mathbf{X}}}_{t}=\\left[\\overline{{\\mathbf{f}}}(\\overline{{\\mathbf{X}}}_{t},t)-\\bar{g}^{2}(t)\\nabla_{\\mathbf{x}}\\log\\bar{p}_{t}(\\overline{{\\mathbf{X}}}_{t})\\right]\\mathrm{d}t+\\bar{g}(t)\\mathrm{d}\\overline{{\\mathbf{B}}}_{t},\\quad\\overline{{\\mathbf{X}}}_{0}=\\mathbf{X}_{T}\\sim p_{T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the only unknown is the score function $\\nabla_{\\mathbf{x}}\\log{p_{t}}$ , inheriting the intractability from the unknown initial distribution $p_{0}$ . In addition to the stochastic dynamics, the reverse-time model provides deterministic backward dynamics via an ordinary differential equation (ODE) by the so called probability flow ODE (PF ODE) [16] ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}\\bar{\\mathbf{x}}_{t}=\\left[\\bar{\\mathbf{f}}(\\bar{\\mathbf{x}}_{t},t)-\\frac{1}{2}\\bar{g}^{2}(t)\\nabla_{\\mathbf{x}}\\log\\bar{p}_{t}(\\bar{\\mathbf{x}}_{t},t)\\right]\\mathrm{d}t,\\quad\\mathbf{x}_{T}\\sim p_{T}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Stochasticity is only injected into the system through the random initialization $\\mathbf{x}_{T}\\sim p_{T}$ , implying a deterministic and bijective map from noise to data [16]. Conditioning the forward process on a data sample $\\mathbf{x}_{0}\\sim p_{0}$ results for linear $\\mathbf{f}(\\cdot,t)$ in a tractable Gaussian forward process with conditional score function $\\nabla_{\\mathbf{x}}\\log p_{0t}(\\mathbf{x}|\\mathbf{x}_{0})$ in closed form. To approximate the exact reverse-time model, this tractable score function is used to train a time-dependent score model $S_{\\theta}$ via score matching [34, 35]. Upon training, any solver for SDEs or ODEs can be utilized to generate data from noise by simulating the stochastic or deterministic backward dynamics of the reverse-time model with $S_{\\theta}\\approx\\nabla_{\\mathbf{x}}\\log p$ . ", "page_idx": 2}, {"type": "text", "text": "Simulation error of the reverse-time model. The two main sources of error when simulating the reverse-time model are the approximation error due to $S_{\\theta}$ only approximating $\\nabla_{\\mathbf{x}}\\log{p}$ , and the discretization error, which arises from transitioning from continuous-time to discrete steps. Simulating the PF ODE with the Euler method over $N\\in\\mathbb{N}$ equidistant time steps results in a global error of order $N^{-1}$ [36]. In contrast, the expected global error for simulating the SDE using the Euler-Maruyama method is of a lower order $\\bar{N}^{-\\frac{1}{2}}$ , indicating a larger error for the same number of steps [30, 36]. From this perspective it is reasonable that sampling from the PF ODE requires fewer steps. Yet, the source of qualitative differences between sampling from the ODE and the SDE [16] remains unclear. ", "page_idx": 2}, {"type": "text", "text": "A pathwise perspective on sampling. The roughness of a path can be measured by its H\u00f6lder exponent $0\\,<\\,\\delta\\,\\leq\\,1$ [37]. For example, BM as the integrator in the backward dynamics eq. (2) has $\\delta$ -H\u00f6lder continuous paths for any $0\\,<\\,\\delta\\,<\\,{\\textstyle\\frac{1}{2}}$ , whereas the integrator $t\\mapsto t$ of the PF ODE eq. (3) can be regarded as a H\u00f6lder continuous path with exponent $\\delta=1$ . Therefore, from a pathwise perspective, we move away from a rough path when we sample using the PF ODE. An unexplored topic in score-based generative models is the interpolation between the SDE and the PF ODE in terms of the H\u00f6lder exponent. It remains to be examined whether there is, to some extent, an optimal degree of H\u00f6lder continuity in between, or if an even rougher path with $\\delta\\ll{\\textstyle\\frac{1}{2}}$ could yield an advantageous data generator. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "The process that naturally arises from this line of thought is fBM [22, 23] with Hurst index $H\\in(0,1)$ , where almost all paths are H\u00f6lder continuous for any exponent $\\delta<H$ , controlled by $H$ . In terms of roughness, the Hurst index interpolates between the paths of Brownian driven SDEs and those of the underlying integration in PF ODEs, while also offering the potential for even rougher paths. Motivated by these observations, we define a novel score-based generative model with underlying dynamics that approximate a fractional diffusion process. ", "page_idx": 3}, {"type": "text", "text": "3 Fractional driving noise ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Before describing the challenges in defining a score-based generative model with control over the roughness of the distribution transforming path, we introduce fBM. The literature distinguishes between \u201cType I\u201d fBM and \u201cType II\u201d fBM [38] having stationary and non-stationary increments, respectively. The type II fBM, also called Riemann-Liouville fBM, possesses smaller deviations from its mean, potentially an advantageous property for a driving noise of a score-based generative model, since large deviations of the sampling process to the data mean can lead to sample artifacts [39]. Here and in the experiments we focus on type II fBM. However, our theoretical framework generalizes to both types as detailed in Appendix A. The empirical study of a score-based generative model approximating a fractional diffusion process driven by type I fBM is dedicated to future work. We begin with the definition of Riemann-Liouville fBM [22], a generalization of BM permitting correlated increments. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1 (Type II Fractional Brownian Motion [22]). Let $B=(B_{t})_{t\\geq0}$ be a standard Brownian Motion (BM) and $\\Gamma$ the Gamma function. The centered Gaussian process ", "page_idx": 3}, {"type": "equation", "text": "$$\nB_{t}^{H}=\\frac{1}{\\Gamma(H+\\frac{1}{2})}\\int_{0}^{t}(t-s)^{H-\\frac{1}{2}}\\mathrm{d}B_{s},\\quad t\\ge0,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "uniquely characterized in law by its covariances ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[B_{t}^{H}B_{s}^{H}\\right]=\\frac{1}{\\Gamma^{2}(H+\\frac{1}{2})}\\int_{0}^{\\operatorname*{min}\\{t,s\\}}((t-u)(s-u))^{H-\\frac{1}{2}}\\mathrm{d}u,\\quad t,s\\in[0,\\infty)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "is called type II fractional Brownian motion $(f\\!B\\!M)$ with Hurst index $H\\in(0,1)$ . ", "page_idx": 3}, {"type": "text", "text": "BM being the unique continuous and centered Gaussian process with covariance $\\operatorname*{min}\\{t,s\\}$ is recovered for $H=0.5$ , since $\\Gamma(1)=1$ . In comparison to the purely Brownian setting with independent increments (diffusion), the path of $B^{H}$ becomes more smooth for $H>0.5$ due to positively correlated increments (super-diffusion) and more rough for $H<0.5$ due to negatively correlated increments (sub-diffusion). These three regimes are reflected in the H\u00f6lder exponent of $\\delta<H$ for almost all paths. ", "page_idx": 3}, {"type": "text", "text": "Generalization challenges. The most challenging part in defining a score-based generative model driven by fBM is the derivation of a reverse-time model. Due to its covariance structure, fBM is not a Markov process [40] and the shift in the roughness of the sample path leads to changes in its quadratic variation: from $t$ in the purely Brownian (diffusion) regime to zero in the smooth regime, and to infinite in the rough regime [30]. For that reason fBM is neither a Markov process nor a semimartingale [25] for all $H\\neq0.5$ . Hence, we cannot make use of the Markov property or the Kolmogorov equations (Fokker-Planck) that are used to derive the reverse-time model of Brownian driven SDEs [31, 32, 33]. See Appendix H for a more illustrative view of the problem. The existence of a reverse-time model can be proven in the smooth regime of fBM [41]. However, due to the absence of an explicit score function in Darses and Saussereau [41] it does not provide a sufficient structure to train a score-based generative model. ", "page_idx": 3}, {"type": "text", "text": "To overcome this difficulty we follow [26, 27] and define the driving noise of our generative model by a linear combination of Markovian semimartingales. The approximation is based on the exact infinite-dimensional Markovian representation of fBM given in Theorem A.2. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.2 (Markov approximation of fBM [26, 27]). Choose $K\\in\\mathbb N$ Ornstein\u2013Uhlenbeck (OU) processes ", "page_idx": 3}, {"type": "equation", "text": "$$\nY_{t}^{k}=\\int_{0}^{t}e^{-\\gamma_{k}(t-s)}\\mathrm{d}B_{s},\\quad k\\in\\mathbb{N},\\quad t\\geq0,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with speeds of mean reversion $\\gamma_{1},...,\\gamma_{K}$ and dynamics $\\mathrm{d}Y_{t}^{k}=-\\gamma_{k}Y_{t}^{k}\\mathrm{d}t+\\mathrm{d}B_{t}$ . Given a Hurst index $H\\in(0,1)$ and a geometrically spaced grid $\\gamma_{k}=r^{k-n}$ with $r>1$ and $\\textstyle n={\\frac{K+1}{2}}$ we call the process ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{B}_{t}^{H}:=\\sum_{k=1}^{K}\\omega_{k}Y_{t}^{k},\\quad H\\in(0,1),\\quad t\\geq0,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Markov-approximate fractional Brownian motion $(M A{-}f B M)$ with approximation coefficients $\\omega_{1},...,\\omega_{K}\\,\\in\\,\\mathbb{R}$ and denote by $\\hat{\\mathbf{B}}^{H}\\,=\\,(\\hat{B}_{1}^{H},...,\\hat{B}_{D}^{H})$ the corresponding $D$ -dimensional process where $\\hat{B}_{i}^{H}$ and $\\hat{B}_{j}^{H}$ are independent for $i\\neq j$ inheriting independence from the underlying standard BMs $B_{i}$ and $B_{j}$ . ", "page_idx": 4}, {"type": "text", "text": "Our framework is conceptually independent of the specific choice of spatial grid and approximation coefficients. To achieve strong convergence rates with a high polynomial order in $K$ for $H<0.5$ in the driving noise to fBM, one may follow the approach outlined in Harms [42]. Consequently, our framework includes driving noise processes that converge to non-Markovian processes with infinite quadratic variation. For computational efficiency, we instead follow the approach of Daems et al. [27] to choose the $L^{2}(\\mathbb{P})$ optimal approximation coefficients for a given $K$ , achieving empirically good results in approximating fBM, even with a small number of OU processes. ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.3 (Optimal Approximation Coefficients [27]). The optimal approximation coefficients $\\pmb{\\omega}\\doteq(\\omega_{1},...,\\omega_{K})\\dot{\\in}\\mathbb{R}^{K}$ for a given Hurst index $H\\,\\in\\,(0,1)$ , a terminal time $T>0$ and a fixed geometrically spaced grid to minimize the $L^{2}(\\mathbb{P})$ -error ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{E}(\\omega):=\\int_{0}^{T}\\mathbb{E}\\left[\\left(B_{t}^{H}-\\hat{B}_{t}^{H}\\right)^{2}\\right]\\mathrm{d}t\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "are given by the closed-form expression $\\boldsymbol{A}\\boldsymbol{\\omega}=\\boldsymbol{b}$ with ", "page_idx": 4}, {"type": "equation", "text": "$$\nA_{i,j}:=\\frac{2T+\\frac{e^{-(\\gamma_{i}+\\gamma_{j})T}-1}{\\gamma_{i}+\\gamma_{j}}}{\\gamma_{i}+\\gamma_{j}},\\quad b_{k}:=\\frac{T}{\\gamma_{k}^{H+\\frac{1}{2}}}P\\left(H+\\frac{1}{2},\\gamma_{k}T\\right)-\\frac{H+\\frac{1}{2}}{\\gamma_{k}^{H+\\frac{3}{2}},\\gamma_{k}T},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and where $\\begin{array}{r}{P(z,x)=\\frac{1}{\\Gamma(z)}\\int_{0}^{x}t^{z-1}e^{-t}\\mathrm{d}t}\\end{array}$ is the regularized lower incomplete gamma function. ", "page_idx": 4}, {"type": "text", "text": "MA-fBM serves as the driving noise of our generative model, replacing BM in the distribution transforming process solving eq. (1), approximating a fractional diffusion process. See Figure 1 for an illustration of the underlying processes. ", "page_idx": 4}, {"type": "text", "text": "4 A score-based generative model based on fractional noise ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we define a continuous-time score-based generative model driven by MA-fBM. A detailed treatment of the theory can be found in Appendix A. We begin with the forward dynamics, transitioning data to noise. ", "page_idx": 4}, {"type": "text", "text": "Definition 4.1 (Forward process). Let $\\hat{\\mathbf{B}}^{H}$ be a $D$ -dimensional MA-fBM with Hurst index $H\\in(0,1)$ . For continuous functions $\\mu\\,:\\,[0,T]\\,\\,\\to\\,\\mathbb{R}$ and $g\\;:\\;[0,T]\\;\\rightarrow\\;\\mathbb{R}$ we define the forward process $\\mathbf{X}=(\\mathbf{X}_{t})_{t\\in[0,T]}$ of a generative fractional diffusion model (GFDM) by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{X}_{t}=\\mu(t)\\mathbf{X}_{t}\\mathrm{d}t+g(t)\\mathrm{d}\\hat{\\mathbf{B}}_{t}^{H},\\quad\\mathbf{X}_{0}=\\mathbf{x}_{0}\\sim p_{0},\\quad t\\in[0,T],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $p_{0}$ is the unknown data distribution from which we aim to sample from. ", "page_idx": 4}, {"type": "text", "text": "Considering both the forward process as well as the OU processes defining the driving noise $\\hat{\\mathbf{B}}^{H}$ , we have for every data dimension an augmented vector of correlated processes $(X,{\\check{Y^{1}}},\\ldots,Y^{K})$ , driven by the same BM, approximating the time-correlated behavior of a one-dimensional fractional diffusion process [27]. We denote the stacked process of the $D$ augmented vectors as ${\\bf Z}=({\\bf Z}_{t})_{t\\in[0,T]}$ and refer to the resulting $D(K{+}1)$ -dimensional process as the augmented forward process. Rewriting the dynamics of the forward process we observe that the augmented forward process $\\mathbf{Z}$ solves a linear SDE. Hence, $\\mathbf{Z}|\\mathbf{x}_{\\mathrm{0}}$ , the augmented forward process conditioned on a data sample $\\mathbf{x}_{0}\\sim p_{0}$ , is a linear transformation of BM. Thus $\\mathbf{Z}|\\mathbf{x}_{\\mathrm{0}}$ is a Gaussian process and so is $\\mathbf{X}|\\mathbf{x}_{0}$ [43]. For each dimension $1\\leq d\\leq D$ , we have a system of $K+1$ trajectories that transform $\\mathbf{x}_{0,d}$ according to the augmented forward process with $D=1$ , following the dynamics ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{Z}_{t}=\\mathbf{F}(t)\\mathbf{Z}_{t}\\mathrm{d}t+\\mathbf{G}(t)\\mathrm{d}B_{t},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where all $K+1$ processes are driven by the same one-dimensional BM $B$ with matrix valued functions $\\mathbf{F}$ and $\\mathbf{G}$ defined in Appendix A.2. To efficiently sample for every $t\\in(0,T]$ from the conditional augmented forward distribution during training, we characterize its marginal statistics. ", "page_idx": 5}, {"type": "text", "text": "Derivation of marginal statistics. The marginal mean $\\begin{array}{r}{\\mathbb{E}[{\\bf X}_{t}|{\\bf x}_{0}]\\;=\\;{\\bf x}_{0}\\exp(\\int_{0}^{t}\\mu(s)\\mathrm{d}s)}\\end{array}$ of the conditional forward process is unaffected by changing the driving noise to MA-fBM, and the mean of the augmenting OU processes is zero. See Appendix A.2 for a detailed derivation of the marginal statistics of the augmenting processes. The missing components in the marginal covariance matrix $\\Sigma_{t}$ of the conditional augmented forward process $\\mathbf{Z}|\\mathbf{x}_{\\mathrm{0}}$ are the marginal variance of the forward process and the marginal correlation between the conditional forward process and the augmenting processes. We derive by reparameteriziation an explicit formula for the marginal variance of the conditional forward process. This generalizes the formula for the perturbation kernel $\\begin{array}{r}{p_{0t}(\\mathbf{x}|\\mathbf{x}_{0})=\\mathcal{N}(\\mathbf{x};c(t)\\mathbf{x}_{0},c^{2}(t)\\sigma^{2}\\tilde{(t)}\\mathbf{I}_{D})}\\end{array}$ given in Karras et al. [44] to a driving MA-fBM and is reminiscent of the reparameterization trick used in discrete time. ", "page_idx": 5}, {"type": "text", "text": "Proposition 4.2 (Continuous Reparameterization Trick). The forward process $\\mathbf{X}$ of GFDM conditioned on $\\mathbf{x}_{0}\\in\\mathbb{R}^{D}$ admits the continuous reparameterization ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{X}_{t}=c(t)\\left(\\mathbf{x}_{0}+\\int_{0}^{t}\\alpha(t,s)\\mathrm{d}\\mathbf{B}_{s}\\right)\\sim\\mathcal{N}(c(t)\\mathbf{x}_{0},c^{2}(t)\\sigma^{2}(t)\\mathbf{I}_{d})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with $\\begin{array}{r}{c(t)=\\exp\\left(\\int_{0}^{t}\\mu(s)\\mathrm{d}s\\right)}\\end{array}$ and $\\begin{array}{r}{\\sigma^{2}(t)=\\int_{0}^{t}\\alpha^{2}(t,s)\\mathrm{d}s}\\end{array}$ where $\\alpha$ is given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\alpha(t,s)=\\sum_{k=1}^{K}\\omega_{k}\\left[\\frac{g(s)}{c(s)}-\\gamma_{k}\\int_{s}^{t}f_{k}(u,s)\\mathrm{d}u\\right],\\quad f_{k}(u,s)=\\frac{g(u)}{c(u)}e^{-\\gamma_{k}(u-s)}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Sketch of Proof. Reparameterization of the forward dynamics in eq. (10) and the Stochastic Fubini Theorem yields the Gaussian process $\\begin{array}{r}{{\\bf X}_{t}\\,=\\,c(t)(\\dot{{\\bf x}_{0}}+\\int_{0}^{t}\\alpha(t,s\\dot{\\bf)}\\mathrm{d}{\\bf B}_{s})}\\end{array}$ with variance $\\boldsymbol{J}\\left[\\mathbf{X}_{t}\\right]\\;=\\;\\boldsymbol{\\mathbf{\\rho}}$ $\\begin{array}{r}{c^{2}(t)\\int_{0}^{t}\\alpha^{2}(t,s)\\mathrm{d}s}\\end{array}$ by It\u00f4 isometry. See Theorem A.3 for the full proof. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "By the above definition of $\\alpha$ , we retrieve the perturbation kernel of the purely Brownian setting given in Karras et al. [44, Equation 12] for $K=1$ , $\\gamma_{1}=0$ and $\\omega_{1}=1$ . When, depending on the choice of forward dynamics, $\\textstyle\\int_{0}^{t}\\alpha(t,s)\\mathrm{d}s$ is not accessible in closed form, $\\Sigma_{t}$ can be described by an ODE and solved numerically as described in Appendix B. Thus our method admits any choice of forward dynamics in terms of $\\mu$ and $g$ . ", "page_idx": 5}, {"type": "text", "text": "Explicit fractional forward dynamics. Although our framework is not bound to any specific dynamics, this work\u2019s empirical evaluation focuses on Fractional Variance Exploding (FVE) dynamics given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\mathbf{X}_{t}=\\sigma_{m i n}\\left(\\frac{\\sigma_{m a x}}{\\sigma_{m i n}}\\right)^{t}\\sqrt{2\\log\\frac{\\sigma_{m a x}}{\\sigma_{m i n}}}\\mathrm{d}\\hat{\\mathbf{B}}_{t}^{H},\\quad t\\in[0,T]}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with $(\\sigma_{m i n},\\sigma_{m a x})=(0.01,50)$ and Fractional Variance Preserving (FVP) dynamics given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{X}_{t}=-\\frac{1}{2}{\\beta}(t)\\mathbf{X}_{t}\\mathrm{d}t+\\sqrt{\\beta(t)}\\mathrm{d}\\hat{\\mathbf{B}}_{t}^{H},\\quad t\\in[0,T]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with $\\beta(t)=\\beta(t)=\\bar{\\beta}_{\\mathrm{min}}+t\\left(\\bar{\\beta}_{m a x}-\\bar{\\beta}_{m i n}\\right)$ and $(\\bar{\\beta}_{m i n},\\bar{\\beta}_{m a x})=(0.1,20)$ [16]. Leveraging the continuous reparameterization trick we derive in Appendix B the conditional marginal covariance matrix of FVE in closed form. To the best of our knowledge, the integral in eq. (13), needed to compute $\\alpha$ in the setting of FVP dynamics, is not accessible in closed form. Therefore, we use a numerical ODE solver to estimate this quantity for FVP dynamics. See Appendix B for details on the computation of the marginal variances and an illustration of the resulting variance schedules. ", "page_idx": 5}, {"type": "text", "text": "The reverse-time model. We observe that the augmented forward dynamics of GFDM are already encompassed in the general framework presented in Song et al. [16, Appendix A], although they differ from the Variance Exploding (VE), Variance Preserving (VP), and sub-VP dynamics discussed therein. To simplify notation, we use $p_{t}$ here to denote the marginal density of both $\\mathbf{Z}_{t}$ and $\\mathbf{X}_{t}$ . The specific density referred to will be clear from the context. By the significant results of [31, 32, 33], the reverse-time model of GFDM is given by the backward dynamics ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\overline{{\\mathbf{Z}}}_{t}=\\big[\\overline{{\\mathbf{F}}}(t)\\overline{{\\mathbf{Z}}}_{t}-\\overline{{\\mathbf{G}}}(t)\\overline{{\\mathbf{G}}}(t)^{T}\\nabla_{\\mathbf{z}}\\log\\overline{{p}}_{t}(\\overline{{\\mathbf{Z}}}_{t})\\big]\\,\\mathrm{d}t+\\overline{{\\mathbf{G}}}(t)\\mathrm{d}\\overline{{\\mathbf{B}}}_{t},\\quad t\\in[0,T].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "However, a direct application of [16] would require to train a score model with input and output dimension of $D(K+1)$ . By proposing augmented score matching below, we show that learning a score model with input and output dimension $D$ is sufficient, enabling the use of the same highly curated model architecture as in traditional diffusion models to approximate the score function. ", "page_idx": 6}, {"type": "text", "text": "Augmented score matching. We condition the score function $\\nabla_{\\mathbf{z}}\\log{p_{t}}$ on a data sample $\\mathbf{x}_{0}\\sim p_{0}$ and additionally on the states of the stacked vector Y[tK]:= (Yt1 , ..., YtK ) of augmenting processes. To train our time-dependent score model $s_{\\theta}$ we propose the augmented score matching loss ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\pmb{\\theta}):=\\mathbb{E}_{t}\\left\\{\\mathbb{E}_{(\\mathbf{X}_{0},\\mathbf{Y}_{t}^{[K]})}\\mathbb{E}_{(\\mathbf{X}_{t}\\mid\\mathbf{Y}_{t}^{[K]},\\mathbf{X}_{0})}\\left[\\|s_{\\theta}(\\mathbf{X}_{t}-\\sum_{k}\\eta_{t}^{k}\\mathbf{Y}_{t}^{k},t)-\\nabla_{\\mathbf{x}}\\log p_{0t}(\\mathbf{X}_{t}|\\mathbf{Y}_{t}^{[K]},\\mathbf{X}_{0})\\|_{2}^{2}\\right]\\right\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The weights $\\eta_{t}^{1},...,\\eta_{t}^{K}$ arise from conditioning $\\mathbf{Z}_{t}\\vert\\mathbf{x}_{0}$ on $\\mathbf{Y}_{t}^{[K]}$ and the time points $t$ are uniformly sampled from $\\mathcal{U}[0,T]$ . We show in the following that the optimal $s_{\\theta}$ w.r.t. the augmented score matching loss is the $L^{2}$ -optimal approximation of the score function of our reverse-time model. ", "page_idx": 6}, {"type": "text", "text": "Proposition 4.3 (Optimal Score Model). Assume that $s_{\\theta}$ is optimal w.r.t. the augmented score matching loss $\\mathcal{L}$ . The score model ", "page_idx": 6}, {"type": "equation", "text": "$$\nS_{\\theta}(\\mathbf{Z}_{t},t):=\\left(s_{\\theta}(\\mathbf{X}_{t}-\\sum_{k}\\eta_{t}^{k}\\mathbf{Y}_{t}^{k},t),-\\eta_{t}^{1}s_{\\theta}(\\mathbf{X}_{t}-\\sum_{k}\\eta_{t}^{k}\\mathbf{Y}_{t}^{k},t),...,-\\eta_{t}^{K}s_{\\theta}(\\mathbf{X}_{t}-\\sum_{k}\\eta\\mathbf{Y}_{t}^{k},t)\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "yields the optimal $L^{2}(\\mathbb{P})$ approximation of $\\nabla_{\\mathbf{z}}\\log p_{t}(\\mathbf{Z}_{t})$ via ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S_{\\theta}(\\mathbf{Z}_{t},t)+\\nabla_{\\mathbf{z}}\\log q_{t}(\\mathbf{Y}_{t}^{[K]})\\approx\\nabla_{\\mathbf{z}}\\log p_{t}(\\mathbf{Z}_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Sketch of Proof. Using the relation $\\nabla_{\\mathbf{x}}\\log p_{0t}=-\\eta_{t}^{k}\\nabla_{\\mathbf{y}^{k}}\\log p_{0t}$ and the independence of $\\mathbf{X}_{0}$ and $\\mathbf{Y}_{t}^{[K]}$ yields the claim. See Appendix A.3 for the full proof. \u53e3 ", "page_idx": 6}, {"type": "text", "text": "In addition to the result that a score model of data dimension $D$ minimizes the proposed augmented score matching loss, Proposition 4.3 also implies that GFDM requires the same number of score model evaluations during sampling from the reverse-time model as traditional diffusion models. This is because, for a given time point $t$ , we only need to evaluate $s_{\\theta}(\\cdot,t)$ once at $\\begin{array}{r}{\\mathbf{X}_{t}-\\sum_{k}\\eta_{t}^{k}\\mathbf{Y}_{t}^{k}}\\end{array}$ to compute $S_{\\theta}(\\mathbf{Z}_{t},t)$ according to eq. (18), and $S_{\\theta}$ is all that is required to approximate the reverse-time dynamics described below. We provide a thorough quantitative evaluation of compute time in seconds for GFDM in Appendix F, validating the theoretical reasoning in this section that GFDM incur only minimal additional computational cost. ", "page_idx": 6}, {"type": "text", "text": "Sampling from reverse-time model. Once we trained our score model $S_{\\theta}$ via augmented score matching, we simulate the reverse-time model backward in time and sample from the reverse-time model via the SDE ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{d}\\overline{{\\mathbf{Z}}}_{t}=\\left\\{\\overline{{\\mathbf{F}}}(t)\\overline{{\\mathbf{Z}}}_{t}-\\overline{{\\mathbf{G}}}(t)\\overline{{\\mathbf{G}}}(t)^{T}\\left[\\overline{{S}}_{\\theta}(\\overline{{\\mathbf{Z}}}_{t},t)+\\nabla_{\\mathbf{z}}\\log\\overline{{q}}_{t}(\\overline{{\\mathbf{Y}}}_{t}^{[K]})\\right]\\right\\}\\mathrm{d}t+\\overline{{\\mathbf{G}}}(t)\\mathrm{d}\\overline{{\\mathbf{B}}}_{t},\\quad t\\in[0,T]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "or the corresponding augmented PF ODE [16] ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{d}\\overline{{\\mathbf{z}}}_{t}=\\left\\{\\overline{{\\mathbf{F}}}(t)\\overline{{\\mathbf{z}}}_{t}-\\frac{1}{2}\\overline{{\\mathbf{G}}}(t)\\overline{{\\mathbf{G}}}(t)^{T}\\left[\\overline{{S}}_{\\theta}(\\overline{{\\mathbf{z}}}_{t},t)+\\nabla_{\\mathbf{z}}\\log\\overline{{q}}_{t}(\\overline{{\\mathbf{y}}}_{t}^{[K]})\\right]\\right\\}\\mathrm{d}t,\\quad t\\in[0,T],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where we initialize in both cases the reverse dynamics with the centered (non-isotropic) Gaussian ${\\overline{{\\mathbf{Z}}}}_{0}$ with covariance matrix $\\Sigma_{T}$ . To traverse backward from noise to data, we may deploy any suitable SDE or ODE solver. In both cases, for each data dimension, we have $K+1$ trajectories that transform the Gaussian initialization into an approximate sample of the data distribution. The PF ODE enables in addition negative log-likelihoods (NLLs) estimation of test data under the learned density [16]. See Appendix G for the computation details of NLLs. ", "page_idx": 6}, {"type": "text", "text": "Remark 4.4. We showed in this section that it suffices to approximate a $D$ -dimensional score to reverse the $D(K+1)$ -dimensional MA-fBM driven SDE with unknown starting distribution. Since this holds for any fixed $K\\in\\mathbb N$ an interesting task is to examine the behaviour of the reverse-time model as $K\\rightarrow\\infty$ and potentially link it to the dynamics of a reverse-time model of true fBM. To the best of our knowledge, existence of such a reverse-time model is not known for $H<0.5$ and the drift of the reverse-time model for $H>0.5$ lacks sufficient structure to train a score-based generative model [41]. ", "page_idx": 6}, {"type": "table", "img_path": "B9qg3wo75g/tmp/728211904bfddd582f5c19d4ad6b0916008f8232961cf33a1ed8955e93ade62b.jpg", "table_caption": [], "table_footnote": ["(a) Conditional image generation on MNIST. (b) Conditional image generation on CIFAR10. "], "page_idx": 7}, {"type": "text", "text": "Table 2: FID and pixel-wise diversity $\\mathrm{V}\\ensuremath{\\mathbf{S}}_{p}$ of GFDM compared to the original setting of purely Brownian driven VE and VP. In bold the scores that are better than both purely Brownian driven dynamics. The overall best scores within the experiment are boxed in, indicating that the highest scores on both datasets are achieved in the super-diffusive regime for $H=0.9$ . ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct experiments on MNIST and CIFAR10 to evaluate the ability of GFDM to generate real images. First, we measure the quality and the pixel-wise diversity of the generated images across different numbers of augmenting processes and various Hurst indices, showing that the super-diffusive regime with $H>0.5$ yields better performance compared to the purely Brownian driven dynamics. Second, we further evaluate the best performing models in terms of class-wise image quality and class-wise distribution coverage. We measure image quality by the Frech\u00e9t Inception Distance (FID) [45] and the Inception score (IS) [46], pixel-wise diversity by the pixel Vendi Score $(\\mathrm{V}\\mathrm{S}_{p})$ [47] and class-wise distribution coverage by improved recall (Recall) [48]. See Appendix D for the implementation details and additional experimental results. We begin with the empirical evaluation of how the augmenting processes affect performance on MNIST. ", "page_idx": 7}, {"type": "text", "text": "Effect of augmentation on MNIST. To isolate the effect of the augmenting processes on MNIST while minimally adapting the driving noise distribution, we fix $H=0.5$ so that the weighted sum of the augmenting processes approximates BM, rather than fBM. We observe an increase of the pixel-wise diversity ${\\mathrm{V}}{\\mathrm{S}}_{p}$ for both FVE and FVP dynamics, with increasing $K$ . In Table 1 we can observe that ${\\mathrm{V}}{\\mathrm{S}}_{p}$ increases from 24.20 to 24.54 for FVE dynamics and from 23.64 to 24.56 for FVP dynamics. The enhanced pixel-wise diversity on MNIST comes at the cost of a reduced likelihood of test data under the learned density, indicated by a higher NLLs for more augmenting processes. ", "page_idx": 7}, {"type": "text", "text": "Quality results across different Hurst indices. On both, MNIST and CIFAR10, we obtain the best performance in terms of FID and $\\mathrm{V}\\ensuremath{\\mathbf{S}}_{p}$ in the super-diffusive regime with $H\\,=\\,0.9$ and FVP dynamics. On MNIST we achieve state of the art FID of 0.72, compared to an FID of 1.44 with the purely Brownian VP dynamics (Table 2a). Comparing FVP to the best-performing purely BM driven VP dynamics, we observe not only an improvement in quality but also an increase in pixel-wise diversity from 23.64 to 24.18, as measured by $\\mathrm{V}\\ensuremath{\\mathbf{S}}_{p}$ . In Table 2b we observe the same behaviour on CIFAR10. The best performing configuration in terms of FID and pixelwise diversity is achieved for $\\mathrm{FVP}(H=0.9,K=2)$ ) with an FID of 3.77 instead of 4.85 and an $\\mathrm{V}\\ensuremath{\\mathbf{S}}_{p}$ of 3.60 instead of 3.28. Additionally, in Figure 7, we show the FID evolution of the super-diffusive regime ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "B9qg3wo75g/tmp/9c34097484f083ee13013c6a075da514be4292d82a4906a940d51aafc941ee4d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 2: Comparison of the super-diffusive regime and purely Brownian dynamics in terms of average FID over three rounds of sampling plotted across different NFEs. ", "page_idx": 7}, {"type": "text", "text": "for various numbers of augmenting processes, showing a similar pattern that either that $K=2$ or ", "page_idx": 7}, {"type": "table", "img_path": "B9qg3wo75g/tmp/c5a057935b7ad808939246fc1bc43592d6660af8aa3f83737df0e19937c8faca.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 3: The class-wise image quality and class-wise distribution coverage of the super-diffusive regime FVP( ${\\cal H}=0.9,K=2)$ ) compared to the purely Brownian VP dynamics. ", "page_idx": 8}, {"type": "image", "img_path": "B9qg3wo75g/tmp/5efd9b54a1b09b002404c901e52cd44daec4f0e0d9f17cb646f38a3bed0ce2a6.jpg", "img_caption": ["$K=3$ yields the best performance across different datasets and dynamics. Evaluating the performance with different number of sampling steps in Figure 2 shows that the super-diffusive regime with $K=2$ saturates already at 500 number of function evaluations (NFEs) on a lower level than both purely Brownian driven dynamics VP and VE. See Figure 2 in Appendix D for the exact FID values. ", "Figure 4: Visual comparison of PF ODE samples. (LHS) Purely Brownian VP dynamics. (RHS) Superdiffusive regime $\\mathrm{FVP}({\\bar{H}}=$ $0.9,K=2,$ ). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Class-wise distribution coverage. We evaluate the capability to generate samples from different classes in terms of FID and class-wise distribution coverage, measured by Recall, comparing the best-performing purely Brownian driven dynamics to the super-diffusive regime with $K=2$ . In Table 3 we observe that the super-diffusive regime with $K=2$ outperforms in both FID and Recall, where $H=0.7$ and $H=0.9$ achieve better class-wise FID for all but two and one class, respectively (deer and dog for $H\\,=\\,0.7$ , bird for $H\\,=\\,0.9.$ ). Additionally, the super-diffusive regime shows improved class-wise distribution coverage, as indicated by a higher Recall across all classes. Overall, both $H=0.7$ and $H=0.9$ perform significantly better in terms of distribution coverage than VP dynamics, $H=0.9$ being the best performing model. ", "page_idx": 8}, {"type": "text", "text": "Sampling with the augmented probability flow ODE. We compare the performance of sampling via the PF ODE for the best performing models from above. For MA-fBM driven dynamics, we have $K+1$ deterministic trajectories for each pixel, traversing from noise to data. As shown in Figure 3, the PF ODE associated with purely Brownian dynamics outperforms the super-diffusive regime in terms of FID, while the superdiffusive regime achieves the overall highest pixel-wise diversity of $\\mathrm{VS}_{p}=4.89$ confirmed mildly perceptually in Figure 4. See Appendix E for additional visualization of the generated data. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "B9qg3wo75g/tmp/7a69f41da6387157a10c7dbc8908ab4b9753c42f9b841c04dfd0da3062a9fb8f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Our experiments show that, compared to purely Brownian dynamics, the super-diffusive regime of MA-fBM yields higher image quality with fewer NFEs, improved pixelwise diversity and better distribution coverage. ", "page_idx": 8}, {"type": "text", "text": "Figure 3: Quantitative performance comparison of SDE and PF ODE sampling. ", "page_idx": 8}, {"type": "text", "text": "6 Related work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Diffusion models in continuous-time. The seminal work of Song et al. [16] offers a unifying framework modeling the distribution transforming process by a stochastic processes in continuoustime with exact reverse-time model. Extensive research has been carried out to examine [44, 49, 50] and extend [39, 51, 52, 53, 54, 55] the continuous-time view on generative models through the lens of ", "page_idx": 8}, {"type": "text", "text": "SDEs, including deterministic corruptions [56] and blurring diffusion [57]. While critic on this view question the usefulness of the theoretical superstructure [58], others extend in line with our work the theoretical framework to new types of underlying diffusion processes [59]. Conceptually similar to our work,Yoon et al. [20] generalizes the score-based generative model from an underlying Brownian motion to a driving L\u00e9vy process, thereby dropping the Gaussian assumptions on the increments. In contrast to our work, the framework of Yoon et al. [20] does not include correlated increments. Importantly, every L\u00e9vy process is a semimartingale, which means that fBM is not a L\u00e9vy process. ", "page_idx": 9}, {"type": "text", "text": "Fractional noises in machine learning. Recently, Hayashi and Nakagawa [60] considered neuralSDEs driven by fractional noise. Yet they do not study diffusion models. The closest work to our work, Tong et al. [29] approximated the type-II fBM with sparse Gaussian processes constructing a neural SDE as a forward process of a score-based generative model, without exact reverse-time dynamics. Unfortunately, they are also limited to Euler-Maruyama solvers and to the case of $H>1/3$ , while our framework is up to numerical stability applicable for any $H\\in(0,1)$ and compatible with any suitable SDE or ODE solver. Daems et al. [27], who inspired our Markov-approximate noise, includes a more elaborate discussion as well as a variational inference framework for MA-fBM. ", "page_idx": 9}, {"type": "text", "text": "Rough path theory. The pathwise analysis of SDEs driven by processes with a H\u00f6lder exponent less than 0.5, including fBM for $H<0.5$ and BM, is encompassed by rough path theory [37]. Rough path theory is applied in machine learning in several ways including (i) deriving stability bounds for the trained weights of a residual neural network [61], (ii) enabling rough control of neural ODEs [62], and (iii) modeling long time series behavior via neural rough differential equations [63, 64]. In finance the famous Black-Scholes model [65] is driven by BM, while more recent continuous-time models employ fractional noise to model price processes [66, 67] or rough volatility [68, 69] to more closely mimic real-world behavior. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we propose a generalized framework of continuous-time score-based generative models, introducing a novel generative model driven by MA-fBM with control over the roughness of distribution transformation paths via augmenting processes. Despite the increased dimensionality of the forward process, learning a score model with the dimensionality of the data distribution, guided by the marginal known score of the augmenting processes, is sufficient. Consequently, both training and sampling is efficient. Our experimental results show that the super-diffusive regime of our MA-fBM driven dynamics achieves superior performance in terms of FID and pixel-wise diversity. Additionally, the FID saturates at a lower level with fewer function evaluations compared to purely Brownian driven dynamics. The super-diffusive regime also improves class-wise distribution coverage, as measured by Recall. Based on these results, GFDM offers a promising alternative to traditional diffusion models for generating data from an unknown distribution. ", "page_idx": 9}, {"type": "text", "text": "Limitations and future work. Several practical and theoretical questions remain open. While we draw our conclusions from experiments conducted on MNIST and CIFAR10, generalizing the observed behavior to other datasets and data modalities may not be valid. In future work, we aim to empirically and theoretically determine the optimal degree of correlated noise, and thus the optimal Hurst index, for training and sampling across different data modalities. Beyond image data, a particularly interesting modality could be the generation of rough time series data using dynamics of the sub-diffusive regime. A theoretical open question is the limiting behavior of GFDM\u2019s reverse dynamics with infinitely many augmenting processes and whether this limit is connected to the reverse time model for true fBM. An intriguing extension would be to adapt the dynamics of our framework to switch between two unknown distributions. This adaptation would enable the use of MA-fBM driven dynamics in the sciences to model real-world evolution between two states of unknown distributions. This is a promising direction, as the assumption of independent increments in real-world noise processes is often too strong. ", "page_idx": 9}, {"type": "text", "text": "Broader impact. Our contribution advances generative modeling by introducing a specific driving noise process to improve the learning of an unknown distribution. This conceptual work aims to support impactful applications of generative modeling, such as molecular structure generation, medical imaging, drug discovery, and DNA sequence design. However, we acknowledge that generative models can reflect biases in the datasets they are trained on and may pose risks, including misuse for human impersonation and the spread of fake content. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We would like to give a special thanks to Thorsten Selinger for his support in utilizing Fraunhofer HHI\u2019s GPU cluster. We also thank the anonymous reviewers for their constructive feedback, which helped improve our work. This work was supported by the Federal Ministry of Education and Research (BMBF) as grants [SyReal (01IS21069B)]. R.M-S. & M.A. acknowledge funding from the QuantIC Project funded by EPSRC Quantum Technology Programme (grant EP/MO1326X/1, EP/T00097X/1), and dotPhoton AG. R.M-S acknowledges funding from EP/R018634/1, EP/T021020/1, EP/Y029178/1, and Google. SN acknowledges funding from the German Federal Ministry of Education and Research under the grant BIFOLD24B. RD acknowledges funding from the Flemish Government under the \"Onderzoeksprogramma Artifici\u00eble Intelligentie (AI) Vlaanderen\" programme and from Flanders Make under the SBO project CADAIVISION. TB was supported by a UKRI Future Leaders Fellowship [grant number MR/Y018818/1]. MO has been partially funded by Deutsche Forschungsgemeinschaft (DFG) - Project - ID 318763901 - SFB1294. WS acknowledges financial support by the German Research Foundation (DFG) - Research Unit KI-FOR 5363 (project ID: 459422098). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 2256\u20132265. PMLR, 2015.   \n[2] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, volume 33, pages 6840\u20136851. Curran Associates, Inc., 2020.   \n[3] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \n[4] Emiel Hoogeboom, V\u00edctor Garcia Satorras, Cl\u00e9ment Vignac, and Max Welling. Equivariant diffusion for molecule generation in 3D. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 8867\u20138887. PMLR, 2022.   \n[5] Matteo Manica, Jannis Born, Joris Cadow, Dimitrios Christofidellis, Ashish Dave, Dean Clarke, Yves Gaetan Nana Teukam, Giorgio Giannone, Samuel C Hoffman, Matthew Buchan, et al. Accelerating material design with the generative toolkit for scientific discovery. npj Computational Materials, 9(1):69, 2023.   \n[6] Gabriele Corso, Hannes St\u00e4rk, Bowen Jing, Regina Barzilay, and Tommi S. Jaakkola. Diffdock: Diffusion steps, twists, and turns for molecular docking. In The Eleventh International Conference on Learning Representations, 2023.   \n[7] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Defossez. Simple and controllable music generation. In Advances in Neural Information Processing Systems, volume 36, pages 47704\u201347720. Curran Associates, Inc., 2023.   \n[8] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre D\u00e9fossez, Jade Copet, Devi Parikh, Yaniv Taigman, and Yossi Adi. Audiogen: Textually guided audio generation. In The Eleventh International Conference on Learning Representations, 2023.   \n[9] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten Kreis. Lion: Latent point diffusion models for 3D shape generation. In Advances in Neural Information Processing Systems, volume 35, pages 10021\u201310039. Curran Associates, Inc., 2022.   \n[10] Zhiying Leng, Tolga Birdal, Xiaohui Liang, and Federico Tombari. HyperSDFusion: Bridging Hierarchical Structures in Language and Geometry for Enhanced 3D Text2Shape Generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19691\u201319700, 2024.   \n[11] Simone Foti, Stefanos Zafeiriou, and Tolga Birdal. UV-free texture generation with denoising and geodesic heat diffusions. In Advances in Neural Information Processing Systems, 2024.   \n[12] Marco Aversa, Gabriel Nobis, Miriam H\u00e4gele, Kai Standvoss, Mihaela Chirica, Roderick Murray-Smith, Ahmed Alaa, Lukas Ruff, Daniela Ivanova, Wojciech Samek, Frederick Klauschen, Bruno Sanguinetti, and Luis Oala. DiffInfinite: Large mask-image synthesis via parallel random patch diffusion in histopathology. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.   \n[13] Miguel Espinosa and Elliot J. Crowley. Generate your own scotland: Satellite image generation conditioned on maps. NeurIPS 2023 Workshop on Diffusion Models, Aug 2023.   \n[14] Pavel Avdeyev, Chenlai Shi, Yuhao Tan, Kseniia Dudnyk, and Jian Zhou. Dirichlet diffusion score model for biological sequence generation. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 1276\u20131301. PMLR, 2023.   \n[15] Chenyu Wang, Masatoshi Uehara, Yichun He, Amy Wang, Tommaso Biancalani, Avantika Lal, Tommi Jaakkola, Sergey Levine, Hanchen Wang, and Aviv Regev. Fine-tuning discrete diffusion models via reward optimization with applications to dna and protein design. arXiv preprint arXiv:2410.13643, 2024.   \n[16] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021.   \n[17] Robert Brown. XXVII. A brief account of microscopical observations made in the months of June, July and August 1827, on the particles contained in the pollen of plants; and on the general existence of active molecules in organic and inorganic bodies. The Philosophical Magazine, 4 (21):161\u2013173, 1828.   \n[18] Albert Einstein. \u00dcber die von der molekularkinetischen Theorie der W\u00e4rme geforderte Bewegung von in ruhenden Fl\u00fcssigkeiten suspendierten Teilchen. Annalen der Physik, pages 549\u2013560, 1905.   \n[19] Norbert Wiener. Differential-space. Journal of Mathematics and Physics, 2:131\u2013174, 1923.   \n[20] Eunbi BI Yoon, Keehun Park, Sungwoong Kim, and Sungbin Lim. Score-based generative models with L\u00e9vy processes. In Advances in Neural Information Processing Systems, volume 36, pages 40694\u201340707. Curran Associates, Inc., 2023.   \n[21] Hengyuan Ma, Li Zhang, Xiatian Zhu, and Jianfeng Feng. Approximated anomalous diffusion: Gaussian mixture score-based generative models, 2023. URL https://openreview.net/ forum?id=yc9xen7EAzd.   \n[22] Paul L\u00e9vy. Random functions: general theory with special reference to Laplacian random functions. University of California Publications in Statistics, 1:331\u2013390, 1953.   \n[23] Benoit B. Mandelbrot and John W. Van Ness. Fractional Brownian Motions, fractional noises and applications. SIAM Review, 10(4):422\u2013437, 1968.   \n[24] Jerry Stinson. The (mis) behavior of markets. Journal of Personal Finance, 4(4):99, 2005.   \n[25] Francesca Biagini, Yaozhong Hu, Bernt \u00d8ksendal, and Tusheng Zhang. Stochastic Calculus for Fractional Brownian Motion and Applications. Springer-Verlag London Limited 2008, 2008. doi: 10.1007/978-1-84628-797-8.   \n[26] Philipp Harms and David Stefanovits. Affine representations of fractional processes with applications in mathematical finance. Stochastic Processes and their Applications, 129(4): 1185\u20131228, 2019. ISSN 0304-4149.   \n[27] Rembert Daems, Manfred Opper, Guillaume Crevecoeur, and Tolga Birdal. Variational inference for SDEs driven by fractional noise. In The Twelfth International Conference on Learning Representations, 2024.   \n[28] Philip E. Protter. Stochastic Integration and Differential Equations. Stochastic Modelling and Applied Probability. Springer Berlin, Heidelberg, 2nd edition, 2013. ISBN 978-3-662-10061-5.   \n[29] Anh Tong, Thanh Nguyen-Tang, Toan Tran, and Jaesik Choi. Learning fractional white noises in neural stochastic differential equations. In Advances in Neural Information Processing Systems, volume 35, pages 37660\u201337675. Curran Associates, Inc., 2022.   \n[30] Samuel N. Cohen and Robert J. Elliott. Stochastic Calculus and Applications. Probability and Its Applications. Birkh\u00e4user, New York, NY, 2st edition, 2015. ISBN 978-1-4939-2866-8.   \n[31] R. L. Stratonovich. Conditional Markov Processes. Theory of Probability & Its Applications, 5 (2):156\u2013178, 1960.   \n[32] Brian D.O. Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313\u2013326, 1982. ISSN 0304-4149.   \n[33] Hans F\u00f6llmer. Time reversal on Wiener space. Stochastic Processes - Mathematic and Physics, volume 1158 of Lecture Notes in Math, page 119\u2013129, 1986.   \n[34] Aapo Hyv\u00e4rinen. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(24):695\u2013709, 2005.   \n[35] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach to density and score estimation. In Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, page 204, 2019.   \n[36] Christian Bayer, Antonis Papapantoleon, and Raul Tempone. Computational Finance. Technical University of Berlin, 2021. URL https://www.wias-berlin.de/people/bayerc/files/ lecture.pdf. Lecture notes.   \n[37] Terry J. Lyons. Differential equations driven by rough signals. Revista Matem\u00e1tica Iberoamericana, 14(2):215\u2013310, 1998.   \n[38] James Davidson and Nigar Hashimzade. Type I and type II fractional Brownian motions: A reconsideration. Computational Statistics & Data Analysis, 53(6):2089\u20132106, 2009. ISSN 0167-9473. The Fourth Special Issue on Computational Econometrics.   \n[39] Aaron Lou and Stefano Ermon. Reflected diffusion models. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 22675\u201322701. PMLR, 2023.   \n[40] Dang Huy. A remark on non-Markov property of a fractional Brownian motion. Vietnam Journal of Mathematics, 31, 01 2003.   \n[41] Sebastien Darses and Bruno Saussereau. Time Reversal for Drifted Fractional Brownian Motion with Hurst Index $H>1/2$ . Electronic Journal of Probability, 12(none):1181 \u2013 1211, 2007. doi: 10.1214/EJP.v12-439.   \n[42] Philipp Harms. Strong convergence rates for Markovian representations of fractional processes. Discrete and Continuous Dynamical Systems - B, 26(10):5567\u20135579, 2021. ISSN 1531-3492.   \n[43] Simo S\u00e4rkk\u00e4 and Arno Solin. Applied Stochastic Differential Equations, volume 10. Cambridge University Press, 2019.   \n[44] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Advances in Neural Information Processing Systems, volume 35, pages 26565\u201326577. Curran Associates, Inc., 2022.   \n[45] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.   \n[46] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and Xi Chen. Improved techniques for training GANs. In Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016.   \n[47] Dan Friedman and Adji Bousso Dieng. The Vendi Score: A diversity evaluation metric for machine learning. Transactions on Machine Learning Research, 2023. ISSN 2835-8856.   \n[48] Tuomas Kynk\u00e4\u00e4nniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \n[49] Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R. Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. In The Eleventh International Conference on Learning Representations, 2023.   \n[50] Raghav Singhal, Mark Goldstein, and Rajesh Ranganath. Where to diffuse, how to diffuse, and how to get back: Automated learning for multivariate diffusions. In The Eleventh International Conference on Learning Representations, 2023.   \n[51] Bowen Jing, Gabriele Corso, Renato Berlinghieri, and Tommi Jaakkola. Subspace diffusion generative models. In Lecture Notes in Computer Science, pages 274\u2013289. Springer Nature Switzerland, 2022.   \n[52] Dongjun Kim, Byeonghu Na, Se Jung Kwon, Dongsoo Lee, Wanmo Kang, and Il-chul Moon. Maximum likelihood training of implicit nonlinear diffusion model. In Advances in Neural Information Processing Systems, volume 35, pages 32270\u201332284. Curran Associates, Inc., 2022.   \n[53] Chin-Wei Huang, Milad Aghajohari, Joey Bose, Prakash Panangaden, and Aaron C Courville. Riemannian diffusion models. In Advances in Neural Information Processing Systems, volume 35, pages 2750\u20132761. Curran Associates, Inc., 2022.   \n[54] Charlotte Bunne, Ya-Ping Hsieh, Marco Cuturi, and Andreas Krause. The Schr\u00f6dinger bridge between Gaussian measures has a closed form. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2023.   \n[55] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 32211\u201332252. PMLR, 2023.   \n[56] Giannis Daras, Mauricio Delbracio, Hossein Talebi, Alex Dimakis, and Peyman Milanfar. Soft diffusion: Score matching with general corruptions. Transactions on Machine Learning Research, 2023. ISSN 2835-8856.   \n[57] Emiel Hoogeboom and Tim Salimans. Blurring diffusion models. In The Eleventh International Conference on Learning Representations, 2023.   \n[58] Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Cold diffusion: Inverting arbitrary image transforms without noise. In Advances in Neural Information Processing Systems, volume 36, pages 41259\u201341282. Curran Associates, Inc., 2023.   \n[59] Severi Rissanen, Markus Heinonen, and Arno Solin. Generative modelling with inverse heat dissipation. In The Eleventh International Conference on Learning Representations, 2023.   \n[60] Kohei Hayashi and Kei Nakagawa. Fractional SDE-Net: Generation of time series data with long-term memory. In 2022 IEEE 9th International Conference on Data Science and Advanced Analytics (DSAA), pages 1\u201310, 2022.   \n[61] Christian Bayer, Peter K. Friz, and Nikolas Tapia. Stability of deep neural networks via discrete rough paths. SIAM Journal on Mathematics of Data Science, 5(1):50\u201376, 2023.   \n[62] P Kidger. On neural differential equations. PhD thesis, University of Oxford, 2021.   \n[63] Shujian Liao, Terry Lyons, Weixin Yang, and Hao Ni. Learning stochastic differential equations using RNN with log signature features. arXiv preprint arXiv:1908.08286, 2019.   \n[64] James Morrill, Cristopher Salvi, Patrick Kidger, and James Foster. Neural rough differential equations for long time series. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 7829\u20137838. PMLR, 2021.   \n[65] Fischer Black and Myron Scholes. The pricing of options and corporate liabilities. The Journal of Political Economy, 81(3):637\u2013654, 1973.   \n[66] Christoph Czichowsky, R\u00e9mi Peyre, Walter Schachermayer, and Junjian Yang. Shadow prices, fractional Brownian motion, and portfolio optimisation under transaction costs. Finance and Stochastics, 22:161\u2013180, 2018.   \n[67] Paolo Guasoni, Zsolt Nika, and Mikl\u00f3s R\u00e1sonyi. Trading fractional Brownian motion. SIAM journal on financial mathematics, 10(3):769\u2013789, 2019.   \n[68] Christian Bayer, Peter Friz, and Jim Gatheral. Pricing under rough volatility. Quantitative Finance, 16(6):887\u2013904, 2016.   \n[69] Jim Gatheral, Thibault Jaisson, and Mathieu Rosenbaum. Volatility is rough. In Commodities, pages 659\u2013690. Chapman and Hall/CRC, 2022.   \n[70] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 234\u2013241. Springer, 2015.   \n[71] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, December 2014. arXiv:1412.6980 [cs.LG].   \n[72] Leslie N. Smith and Nicholay Topin. Super-convergence: very fast training of neural networks using large learning rates. In Defense $^+$ Commercial Sensing, 2018.   \n[73] Marjorie G. Hahn, Kei Kobayashi, and Sabir Umarov. Fokker-Planck-Kolmogorov equations associated with time-changed fractional Brownian motion. arXiv: Mathematical Physics, 139: 691\u2013705, 2011. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A The mathematical framework of generative fractional diffusion models 17   \nA.1 A Markovian representation of fractional Brownian motion . 17   \nA.2 The forward model 18   \nA.3 Estimating the score via augmented score matching loss . . 21   \nB Forward sampling 23   \nC Implementation details 26   \nD Additional experiments 26   \nE Illustration of generated data 28   \nF Computational cost of augmenting processes 30   \nG Likelihood computation 32   \nH Challenges in the attempt to generalize 32   \nI Notational conventions 34 ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A The mathematical framework of generative fractional diffusion models ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section we provide the mathematical details of the score-based generative model defined in the main paper. The driving noise of the underlying stochastic process is based on the affine representation of fractional processes from Harms and Stefanovits [26] and further simplified by the closed form expression to determine optimal approximation coefficients of Daems et al. [27]. ", "page_idx": 16}, {"type": "text", "text": "A.1 A Markovian representation of fractional Brownian motion ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We begin with the definition of type I fractional Brownian motion, defined on the whole real line, possessing correlated increments that are in contrast to type $\\mathrm{II}$ fractional Brownian motion stationary. ", "page_idx": 16}, {"type": "text", "text": "Definition A.1 (Type I Fractional Brownian Motion [23]). Let $\\textstyle(\\Omega,{\\mathcal{F}},\\mathbb{P})$ be a complete probability space equipped with a complete and right continuous flitration $\\{\\mathcal{F}_{t}\\}$ and $\\Gamma$ the Gamma function. For two standard independent $\\{\\mathcal{F}_{t}\\}$ -Brownian motions (BMs) $\\tilde{B}$ and $B$ the centered Gaussian process $W^{H}=(W_{t}^{H})_{t\\in\\mathbb{R}}$ with ", "page_idx": 16}, {"type": "equation", "text": "$$\nW_{t}^{H}:=\\frac{1}{\\Gamma(H+\\frac{1}{2})}\\int_{-\\infty}^{0}((t-s)^{H-\\frac{1}{2}}-(-s)^{H-\\frac{1}{2}})\\mathrm{d}\\tilde{B}_{s}+\\frac{1}{\\Gamma(H+\\frac{1}{2})}\\int_{0}^{t}(t-s)^{H-\\frac{1}{2}}\\mathrm{d}B_{s}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "uniquely characterized in law by its covariances ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[W_{t}^{H}W_{s}^{H}\\right]=\\frac{1}{2}\\left[t^{2H}+s^{2H}-(t-s)^{2H}\\right],\\quad t\\geq s>0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "is called type I fractional Brownian motion (fBM) with Hurst index $H\\in(0,1)$ . ", "page_idx": 16}, {"type": "text", "text": "Type II fBM from the main paper is retrieved by setting the additionally defined BM $\\tilde{B}$ on the negative real line to zero. Therefore, the difference to type II fBM is the stochastic integral w.r.t. B\u02dc that yields stationary increments and a non trivial distribution at $t=0$ . For $H=0.5$ , the process is a BM and has thus independent increments. For $H\\in(0,1)\\setminus\\{{\\frac{1}{2}}\\}$ , the process possesses correlated increments and, compared to BM, smoother paths for $H>0.5$ due to positively correlated increments (superdiffusion) and rougher paths for $H<0.5$ due to negatively correlated increments (sub-diffusion). These three regimes reflect for type I fBM in the same change of quadratic variation from $t$ to zero quadratic variation in the smooth regime and to infinite quadratic variation in the rough regime [30]. To prepare the approximation of the non-Markovian and non-semimartingale fBM [25] via Markovian semimartingales, define for every $\\gamma\\in(0,\\infty)$ the Ornstein-Uhlenbeck process $Y^{\\gamma}$ given by ", "page_idx": 16}, {"type": "equation", "text": "$$\nY_{t}^{\\gamma}:=Y_{0}^{\\gamma}e^{-t\\gamma}+\\int_{0}^{t}e^{-\\gamma(t-s)}d B_{s},\\quad t\\geq0,\\quad Y_{0}:=\\int_{-\\infty}^{0}e^{s\\gamma}d\\tilde{B}_{s},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with speed of mean reversion $\\gamma$ and non trivial starting value in contrast to the OU processes defined in eq. (6) of the main paper. By It\u00f4\u2019s product rule [30], the process $Y^{\\gamma}$ solves the same SDE ", "page_idx": 16}, {"type": "equation", "text": "$$\nd Y_{t}^{\\gamma}=-\\gamma Y_{t}^{\\gamma}d t+d B_{t},\\;\\;\\;Y_{0}=\\int_{-\\infty}^{0}e^{s\\gamma}d\\tilde{B}_{s},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with different starting value. According to Harms and Stefanovits [26] we represent fBm by an integral over the predefined family of Ornstein-Uhlenbeck processes. ", "page_idx": 16}, {"type": "text", "text": "Theorem A.2 (Markovian representation of fBM [26, 27]). The non-Markovian process $W^{H}$ permits the infinite-dimensional Markovian representation ", "page_idx": 16}, {"type": "equation", "text": "$$\nW_{t}^{H}=\\left\\{\\begin{array}{l l}{\\int_{0}^{\\infty}\\left(Y_{t}^{\\gamma}-Y_{0}^{\\gamma}\\right)\\nu_{1}(\\gamma)\\mathrm{d}\\gamma,}&{H\\leq\\frac{1}{2}}\\\\ {-\\int_{0}^{\\infty}\\partial_{\\gamma}\\left(Y_{t}^{\\gamma}-Y_{0}^{\\gamma}\\right)\\nu_{2}(\\gamma)\\mathrm{d}\\gamma,}&{H>\\frac{1}{2}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nu_{1}(\\gamma)=\\frac{\\gamma^{-(H+\\frac{1}{2})}}{\\Gamma(H+\\frac{1}{2})\\Gamma(\\frac{1}{2}-H)}\\;\\;\\;\\;a n d\\;\\;\\;\\frac{\\nu_{2}(\\gamma)=\\gamma^{-(H-\\frac{1}{2})}}{(\\Gamma(H+\\frac{1}{2})\\Gamma(\\frac{3}{2}-H))}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that we follow Daems et al. [27] in replacing the process $\\begin{array}{r}{Z_{t}^{\\gamma}:=Z_{0}^{\\gamma}e^{-t\\gamma}+\\int_{0}^{t}e^{-(t-s)\\gamma}Y_{s}^{\\gamma}d s}\\end{array}$ from the original theorem throughout this work by $Z_{t}^{\\gamma}=-\\partial_{\\gamma}Y_{t}^{\\gamma}+\\left(\\partial_{\\gamma}Y_{0}^{\\gamma}+Z_{0}^{\\gamma}\\right)e^{-t\\gamma}$ . This is justified by Harms and Stefanovits [26, Remark 3.5] and simplifies for $H>\\textstyle{\\frac{1}{2}}$ the approximation of fBM and the definition of our generative model, since we only have to reverse the $Y^{\\gamma}$ processes instead of the pairs $(Y^{\\gamma},Z^{\\gamma})$ . For $Y_{0}^{\\gamma}\\;=\\;0$ eq. (26) yields an infinite-dimensional Markovian representation of type II fBM [27]. The MA-fBM from Definition 3.2 in the main paper becomes for type I fBM ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{B}_{t}^{H}=\\sum_{k=1}^{K}\\omega_{k}\\left(Y_{t}^{k}-Y_{0}^{k}\\right),\\quad H\\in(0,1),\\quad t\\geq0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with non trivial $\\mathbf{Y}_{0}~=~(Y_{0}^{1},...,Y_{0}^{1})$ that is a centered multivariate Gaussian with covariances $\\mathbb{E}\\left[Y_{0}^{k}Y_{0}^{l}\\right]=1/(\\gamma_{k}+\\gamma_{l})$ [27]. Theorem 3.3 holds true for type I fBM as well with optimal approximation coefficients given in Daems et al. [27, Proposition 5]. For more details on the properties and distinction of type I and type II fBM we refer the reader to Daems et al. [27]. ", "page_idx": 17}, {"type": "text", "text": "A.2 The forward model ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We define in the following a score-based generative model approximating a fractional diffusion process driven by type I fBM. For the remainder of Appendix A we assume $\\begin{array}{r}{\\bar{Y}_{0}^{k}=\\int_{-\\infty}^{0}e^{s\\gamma_{k}}\\mathrm{d}\\tilde{B}_{s}}\\end{array}$ for all $1\\le k\\le K$ where the setting from the main paper with type $\\mathrm{II}$ fBM is recovered by choosing $Y_{0}^{k}=0$ instead. Let $\\hat{\\mathbf{B}}^{H}$ be a $D$ -dimensional MA-fBM with Hurst index $H\\in(0,1)$ . For continuous functions $\\mu:[0,T]\\to\\mathbb{R}$ and $g:[0,T]\\rightarrow\\mathbb{R}$ we define the forward process $\\mathbf{X}=(\\mathbf{X}_{t})_{t\\in[0,T]}$ by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{X}_{t}=\\mu(t)\\mathbf{X}_{t}\\mathrm{d}t+g(t)\\mathrm{d}\\hat{\\mathbf{B}}_{t}^{H},\\quad\\mathbf{X}_{0}=\\mathbf{x}_{0}\\sim p_{0},\\quad t\\in[0,T]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $p_{0}$ is an unknown data distribution from which we aim to sample from. Using eq. (25) we note ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{d}\\hat{\\mathbf{B}}_{t}^{H}=-\\sum_{k=1}^{K}\\omega_{k}\\gamma_{k}Y_{t}^{k}d t+\\sum_{k=1}^{K}\\omega_{k}\\mathrm{d}\\mathbf{B}_{t},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathbf{B}=(B_{1},...,B_{d})$ is a multivariate BM. With $\\begin{array}{r}{\\bar{\\omega}:=\\sum_{k=1}^{K}\\omega_{k}}\\end{array}$ we rewrite the dynamics of the forward process as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{X}_{t}=\\left[\\mu(t)\\mathbf{X}_{t}-g(t)\\sum_{k=1}^{K}\\omega_{k}\\gamma_{k}\\mathbf{Y}_{t}^{k}\\right]\\mathrm{d}t+\\bar{\\omega}g(t)\\mathrm{d}\\mathbf{B}_{t},\\quad t\\in[0,T],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Taking into account the dynamics of the OU processes, we define the augmented forward process ${\\bf Z}=({\\bf\\bar{Z}}_{t})_{t\\in[0,T]}$ by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{Z}_{t}=\\left(X_{t,1},Y_{t,1}^{1},...,Y_{t,1}^{K},X_{t,2},Y_{t,2}^{1},...,Y_{t,2}^{K},...,..,...,X_{t,D},Y_{t,D}^{1},...Y_{t,D}^{K}\\right)\\in\\mathbb{R}^{D\\left(K+1\\right)}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "following the dynamics ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{Z}_{t}=\\mathbf{F}(t)\\mathbf{Z}_{t}d t+\\mathbf{G}(t)\\mathrm{d}\\mathbf{B}_{t}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with $\\mathbf{F}(t)=d i a g(\\mathbf{R}(t),...,\\mathbf{R}(t))\\in\\mathbb{R}^{D(K+1),D(K+1)},$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{R}(t)={\\binom{\\mu(t)}{\\mathbf{0}_{K}}}\\begin{array}{r l r}{-g(t)\\omega_{1}\\gamma_{1}}&{\\hdots}&{-g(t)\\omega_{K}\\gamma_{K}}\\\\ &{-d i a g(\\gamma_{1},...,\\gamma_{K})}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf G(t)=\\left(\\bar{\\omega}g(t)I_{D}\\quad I_{D}\\quad.\\ldots\\quad I_{D}\\right)^{T}\\in\\mathbb{R}^{D(K+1),D}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For each dimension $1\\leq d\\leq D$ , the dynamics of the process transforming $\\mathbf{x}_{0,d}$ reduce to those of the augmented forward process with $D=1$ , given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{Z}_{t}=\\mathbf{F}(t)\\mathbf{Z}_{t}\\mathrm{d}t+\\mathbf{G}(t)\\mathrm{d}B_{t},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the $K+1$ processes that transform $\\mathbf{x}_{0,d}$ are all driven by the same one-dimensional $\\mathbf{B}\\mathbf{M}\\,B$ The augmented forward process $\\mathbf{Z}$ conditioned on $\\mathbf{y}_{0}^{1},...,\\mathbf{y}_{0}^{K}$ and a data sample $\\mathbf{x}_{0}\\sim p_{0}$ is a linear transformation of BM and hence a Gaussian process and so is X [43]. Since the integral w.r.t BM has zero mean, the mean vector of the augmenting processes is $\\mathbb{E}\\left[\\mathbf{Y}_{t}^{k}\\right]=\\mathbf{0}_{d}$ for all $1\\le k\\le K$ and the mean of the conditional forward process is the solution of the ODE ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\partial_{t}\\mathbb{E}\\left[\\mathbf{X}_{t}|\\mathbf{x}_{0}\\right]=\\mu(t)\\mathbb{E}\\left[\\mathbf{X}_{t}|\\mathbf{x}_{0}\\right]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and hence the marginal mean ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathbf{X}_{t}|\\mathbf{x}_{0}\\right]=c(t)\\mathbf{x}_{0}\\quad w i t h\\quad c(t)=\\exp\\left(\\int_{0}^{t}\\mu(s)d s\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "is not affected by changing the driving noise to MA-fBM. The marginal covariance matrix $\\Sigma_{t}$ of the conditional augmented forward process can be approximated numerically by solving an ODE, see Appendix B for details. In addition we present a continuous reparameterization of the forward process, resulting for some forward dynamics in a closed form solution of the marginal covariance matrix. Our result generalizes the explicit formula for the perturbation kernel $p_{0t}(\\mathbf{x}|\\mathbf{x}_{0})=$ $\\mathcal{N}(\\mathbf{x};c(t)\\mathbf{x}_{0},c^{2}(t)\\sigma^{2}(t)\\mathbf{I}_{d})$ given in Karras et al. [44]. ", "page_idx": 18}, {"type": "text", "text": "Proposition A.3 (Continuous Reparameterization Trick). Let $\\mathbf{x}_{\\mathrm{0}}$ be a fixed realisation drawn from $p_{0}$ . The forward process $\\mathbf{X}=(\\mathbf{X}_{t})_{t\\in[0,T]}$ conditioned on $\\mathbf{x}_{\\mathrm{0}}$ admits the continuous reparameterization ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\bf X}_{t}=c(t)\\left({\\bf x}_{0}+\\int_{0}^{t}\\alpha(t,s)\\mathrm{d}{\\bf B}_{s}\\right)+\\underbrace{c(t)\\sum_{k=1}^{K}\\omega_{k}\\gamma_{k}\\int_{0}^{t}\\frac{g(s)}{c(s)}e^{-s\\gamma_{k}}\\mathrm{d}s{\\bf Y}_{0}^{k}}_{=0\\;f o r\\;t y p e\\;I I g B M\\;s i n c e\\;{\\bf Y}_{0}^{k}=0}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with $\\begin{array}{r}{c(t)=\\exp\\left(\\int_{0}^{t}\\mu(s)\\mathrm{d}s\\right)}\\end{array}$ and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\alpha(t,s)=-\\sum_{k=1}^{K}\\omega_{k}\\gamma_{k}\\int_{s}^{t}\\frac{g(u)}{c(u)}e^{-\\gamma_{k}(u-s)}\\mathrm{d}u+\\bar{\\omega}\\frac{g(s)}{c(s)}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "such that $\\mathbf{X}_{t}|\\mathbf{x}_{0}\\sim\\mathcal{N}\\left(c(t)\\mathbf{x}_{0},\\left[c^{2}(t)\\sigma^{2}(t)+\\sigma_{K}^{2}(t)\\right]\\mathbf{I}_{d}\\right)$ is a Gaussian random vector for all $t\\in$ $(0,T]$ with ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sigma^{2}(t)=\\int_{0}^{t}\\alpha^{2}(t,s)d s\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sigma_{K}^{2}=c^{2}(t)\\sum_{k=1}^{K}\\frac{\\gamma_{k}}{2}\\left[\\omega_{k}\\int_{0}^{t}\\frac{g(s)}{c(u)}\\mathrm{d}u\\right]^{2}}\\\\ {\\displaystyle+\\,2c^{2}(t)\\sum_{k<l}\\frac{\\omega_{k}\\omega_{l}\\gamma_{k}\\gamma_{l}}{\\gamma_{k}+\\gamma_{l}}\\int_{0}^{t}\\frac{g(s)}{c(s)}e^{-s\\gamma_{k}}\\mathrm{d}s\\int_{0}^{t}\\frac{g(s)}{c(s)}e^{-s\\gamma_{l}}\\mathrm{d}s}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "vanishing for an underlying type II fBM. ", "page_idx": 18}, {"type": "text", "text": "Proof. By continuity, the functions $\\mu$ and $\\sigma$ are bounded. Moreover, the processes $Y_{j}^{1},...,Y_{j}^{K}$ posses continuous, hence bounded, paths and thus ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\int_{0}^{t}|\\mu(u)|\\mathrm{d}u<\\infty,\\quad\\int_{0}^{t}\\sigma^{2}(u)\\mathrm{d}u<\\infty\\quad a n d\\quad\\int_{0}^{t}|\\sum_{k}^{K}\\omega_{k}\\gamma_{k}\\mathbf{Y}_{t}^{k}|\\mathrm{d}u<\\infty\\quad\\mathbb{P}-a.s.,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last integral is understood entrywise. Hence, by Cohen and Elliott [30, Theorem 16.6.1], the unique solution of the SDE eq. (31) is given explicitly as ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\bf X}_{t}=c(t)\\left({\\bf x}_{0}-\\int_{0}^{t}\\frac{g(u)}{c(u)}\\left[\\sum_{k=1}^{K}\\omega_{k}\\gamma_{k}{\\bf Y}_{u}^{k}\\right]\\mathrm{d}u+\\bar{\\omega}\\int_{0}^{t}\\frac{g(u)}{c(u)}\\mathrm{d}{\\bf B}_{u}\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with $\\begin{array}{r}{c(t)=\\exp\\left(\\int_{0}^{t}\\mu(s)d s\\right)}\\end{array}$ . Define ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{J}\\left({\\mathbf{Y}}_{0}^{[K]},t\\right):=\\sum_{k=1}^{K}\\omega_{k}\\gamma_{k}\\int_{0}^{t}\\frac{g(s)}{c(s)}e^{-s\\gamma_{k}}\\mathrm{d}s{\\mathbf{Y}}_{0}^{k}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and by the definition of $Y_{j}^{k}$ in (24) we calculate using the Stochastic Fubini Theorem [26] ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle\\int_{0}^{t}\\frac{g(u)}{c(u)}\\left[\\sum_{k=1}^{K}\\omega_{k}\\gamma_{k}\\mathbf{Y}_{u}^{k}\\right]\\,\\mathrm{d}u=\\sum_{k=1}^{K}\\omega_{k}\\gamma_{k}\\displaystyle\\int_{0}^{t}\\int_{0}^{u}\\frac{g(u)}{c(u)}e^{-\\gamma_{k}(u-s)}\\mathrm{d}\\mathbf{B}_{s}\\mathrm{d}u+\\mathbf{J}(\\mathbf{Y}_{0}^{[K]},t)}\\\\ &{}&{\\displaystyle=\\int_{0}^{t}\\sum_{k=1}^{K}\\omega_{k}\\gamma_{k}\\displaystyle\\int_{s}^{t}\\frac{g(u)}{c(u)}e^{-\\gamma_{k}(u-s)}\\mathrm{d}u\\mathrm{d}\\mathbf{B}_{s}+\\mathbf{J}\\left(\\mathbf{Y}_{0}^{[K]},t\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and hence ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{X}_{t}=c(t)\\left(\\mathbf{x}_{0}-\\int_{0}^{t}\\frac{g(u)}{c(u)}\\left[\\displaystyle\\sum_{k=1}^{K}\\omega_{k}\\gamma_{k}\\mathbf{Y}_{u}^{k}\\right]\\mathrm{d}u+\\bar{\\omega}\\int_{0}^{t}\\frac{g(u)}{c(u)}\\mathrm{d}\\mathbf{B}_{u}\\right)}\\\\ &{\\quad=c(t)\\left(\\mathbf{x}_{0}-\\int_{0}^{t}\\displaystyle\\sum_{k=1}^{K}\\omega_{k}\\gamma_{k}\\int_{s}^{t}\\frac{g(u)}{c(u)}e^{-\\gamma_{k}(u-s)}\\mathrm{d}u\\mathrm{d}\\mathbf{B}_{s}+\\bar{\\omega}\\int_{0}^{t}\\frac{g(u)}{c(u)}\\mathrm{d}\\mathbf{B}_{u}-\\mathbf{J}\\left(\\mathbf{Y}_{0}^{[K]},t\\right)\\right)}\\\\ &{\\quad=c(t)\\left(\\mathbf{x}_{0}+\\int_{0}^{t}\\left[-\\displaystyle\\sum_{k=1}^{K}\\omega_{k}\\gamma_{k}\\int_{s}^{t}\\frac{g(u)}{c(u)}e^{-\\gamma_{k}(u-s)}\\mathrm{d}u+\\bar{\\omega}\\frac{g(s)}{c(s)}\\right]\\mathrm{d}\\mathbf{B}_{s}-\\mathbf{J}\\left(\\mathbf{Y}_{0}^{[K]},t\\right)\\right)}\\\\ &{\\quad=c(t)\\mathbf{x}_{0}+c(t)\\int_{0}^{t}\\int_{0}^{t}\\alpha(t,s)\\mathrm{d}\\mathbf{B}_{s}-c(t)\\mathbf{J}\\left(\\mathbf{Y}_{0}^{[K]},t\\right)}\\end{array}\\quad\\mathrm{()}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\alpha(t,s)=-\\sum_{k=1}^{K}\\omega_{k}\\gamma_{k}\\int_{s}^{t}\\frac{g(u)}{c(u)}e^{-\\gamma_{k}(u-s)}\\mathrm{d}u+\\bar{\\omega}\\frac{g(s)}{c(s)}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $\\alpha(t,\\cdot)$ is continuous for every fixed $t\\in[0,T]$ we have $\\textstyle\\int_{0}^{t}\\alpha^{2}(t,s)\\mathrm{d}s_{.}<\\infty$ . Using that the integral of a bounded deterministic function w.r.t. Brownian motion is a Gaussian process we have by It\u00f4\u2019s isometry ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int_{0}^{t}\\alpha(t,s)\\mathrm{d}\\mathbf{B}_{s}\\sim\\mathcal{N}\\left(\\mathbf{0}_{d},\\sigma^{2}(t)\\mathbf{I}_{d}\\right)\\quad w i t h\\quad\\sigma^{2}(t)=\\int_{0}^{t}\\alpha^{2}(t,s)\\mathrm{d}s.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, conditional on $\\mathbf{x}_{\\mathrm{0}}$ , the random vector $\\mathbf{X}_{t}$ is Gaussian with mean vector ", "page_idx": 19}, {"type": "equation", "text": "$$\n{m}_{t}^{\\mathbf{x}}=c(t)\\pmb{x}_{0}+\\underbrace{\\mathbb{E}\\left[\\mathbf{J}(\\mathbf{Y}_{0}^{[K]})\\right]}_{=0}=\\mathbf{x}_{0}\\exp\\left(\\int_{0}^{t}\\mu(s)\\mathrm{d}s\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Moreover, ${\\tilde{B}}_{j}$ and $B_{j}$ corresponding to the entries of $\\tilde{\\mathbf{B}}=(\\tilde{B}_{1},...,\\tilde{B}_{d})$ and $\\mathbf{B}=(B_{1},...,B_{d})$ are independent by Theorem A.1 resulting in the entrywise variance ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\bf{\\Sigma}}_{{\\bf{{\\tau}}},j,j}^{\\bf{{x}}}=c^{2}(t)\\int_{0}^{t}{\\alpha^{2}}(t,s)\\mathrm{d}s+\\sigma_{K}^{2}(t)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sigma_{K}^{2}(t)=\\mathbb{V}\\left[\\mathbf{J}(\\mathbf{Y}_{0}^{[K]})_{j}\\right]=c^{2}(t)\\sum_{k=1}^{K}\\frac{\\gamma_{k}}{2}\\left[\\omega_{k}\\int_{0}^{t}\\frac{g(s)}{c(u)}\\mathrm{d}u\\right]^{2}}\\\\ {\\displaystyle\\qquad\\qquad\\qquad+\\,2c^{2}(t)\\sum_{k<l}\\frac{\\omega_{k}\\omega_{l}\\gamma_{k}\\gamma_{l}}{\\gamma_{k}+\\gamma_{l}}\\int_{0}^{t}\\frac{g(s)}{c(s)}e^{-s\\gamma_{k}}\\mathrm{d}s\\int_{0}^{t}\\frac{g(s)}{c(s)}e^{-s\\gamma_{l}}\\mathrm{d}s,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we used again It\u00f4\u2019s isometry to calculate ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[Y_{0,j}^{k}Y_{0,j}^{l}\\right]=\\mathbb{E}\\left[\\int_{-\\infty}^{0}e^{\\gamma_{k}s}\\mathrm{d}\\tilde{B}_{s,j}\\int_{-\\infty}^{0}e^{\\gamma_{l}s}\\mathrm{d}\\tilde{B}_{s,j}\\right]=\\int_{-\\infty}^{0}e^{(\\gamma_{k}+\\gamma_{l})s}\\mathrm{d}s=\\frac{1}{\\gamma_{k}+\\gamma_{l}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since the entries of $\\mathbf{B}$ are independent, we find the covariance matrix ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Sigma_{t}^{\\mathbf{x}}=\\left[c^{2}(t)\\sigma^{2}(t)+\\sigma_{K}^{2}(t)\\right]{\\bf I}_{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The preceding proposition generalizes the \u201creparameterization trick\u201d3 from discrete time to continuous-time in the sense that ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\bf X}_{t_{n}}=\\sqrt{\\bar{\\alpha}_{t_{n}}}{\\bf x}_{0}+\\sqrt{1-\\bar{\\alpha}_{t_{n}}}{\\epsilon},\\quad\\epsilon\\sim\\mathcal{N}({\\bf0}_{d},{\\bf I}_{d})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "used in discrete time [2] with time steps $0=t_{0}<...<t_{N}=T$ is replaced by our continuous-time reparameterization ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\bf X}_{t}=c(t)\\left({\\bf x}_{0}+\\int_{0}^{t}\\alpha(t,s)\\mathrm{d}{\\bf B}_{s}\\right)+c(t)\\sum_{k=1}^{K}\\omega_{k}\\gamma_{k}\\int_{0}^{t}\\frac{g(s)}{c(s)}e^{-s\\gamma_{k}}\\mathrm{d}s{\\bf Y}_{0}^{k},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "enabling to directly sample $\\mathbf{X}_{t}|\\mathbf{x}_{0}\\sim\\mathcal{N}(c(t)\\mathbf{x}_{0}+\\left[c^{2}(t)\\sigma^{2}(t)+\\sigma_{K}^{2}(t)\\right]\\mathbf{I}_{D})$ for a given data sample $\\mathbf{x}_{\\mathrm{0}}$ and time point $t\\in(0,T]$ , in case that $\\sigma^{2}(t)$ and $\\sigma_{K}^{2}(t)$ have a closed form solution. For a complete characterization of the marginal covariance matrix $\\Sigma_{t}$ of the conditioned augmented forward process we calculate by It\u00f4 isometry with $X=X_{j}$ and $Y^{l}=Y_{j}^{l}$ for all $1\\le j\\le D$ , $1\\le l\\le K$ and any $t\\in[0,T]$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[X_{t}Y_{t}^{l}\\right]=c(t)\\int_{0}^{t}\\alpha(t,s)e^{-\\gamma_{k}(t-s)}\\mathrm{d}s+c(t)\\sum_{l=1}^{K}\\frac{\\omega_{k}\\gamma_{k}}{\\gamma_{k}+\\gamma_{l}}e^{-\\gamma_{l}t}\\int_{0}^{t}\\frac{g(s)}{c(s)}e^{-s\\gamma_{k}}d s\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[Y_{t}^{k}Y_{t}^{l}\\right]=\\frac{e^{-(\\gamma_{k}+\\gamma_{l})s}}{\\gamma_{k}+\\gamma_{l}}+\\frac{1-e^{-(\\gamma_{k}+\\gamma_{l})t}}{\\gamma_{k}+\\gamma_{l}}=\\frac{1}{\\gamma_{k}+\\gamma_{l}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "reducing for type $\\mathrm{II}$ fBM to ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[X_{t}Y_{t}^{l}\\right]=c(t)\\int_{0}^{t}\\alpha(t,s)e^{-\\gamma_{k}(t-s)}\\mathrm{d}s\\quad a n d\\quad\\mathbb{E}\\left[Y_{t}^{k}Y_{t}^{l}\\right]=\\frac{1-e^{-(\\gamma_{k}+\\gamma_{l})t}}{\\gamma_{k}+\\gamma_{l}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We denote in the following the stacked vector of the augmenting processes by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{Y}_{t}^{[K]}=(Y_{t,1}^{1},Y_{t,1}^{2},...,Y_{t,1}^{K},Y_{t,2}^{1},Y_{t,2}^{2},...,Y_{t,2}^{K},...,Y_{t,D}^{1},Y_{t,D}^{2},...,Y_{t,D}^{K})\\in\\mathbb{R}^{D(K+1)}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The random vector $\\mathbf{Y}_{t}^{[K]}$ is a centered Gaussian process with covariance matrix ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{\\boldsymbol{\\Lambda}}_{t}=d i a g(\\mathbf{\\boldsymbol{\\Sigma}}_{t}^{\\mathbf{y}},...,\\mathbf{\\boldsymbol{\\Sigma}}_{t}^{\\mathbf{y}})\\in\\mathbb{R}^{D\\cdot K,D\\cdot K},\\quad\\mathbf{\\boldsymbol{\\Sigma}}_{t}^{\\mathbf{y}}\\in\\mathbb{R}^{K,K},\\quad\\left[\\mathbf{\\boldsymbol{\\Sigma}}_{t}^{\\mathbf{y}}\\right]_{k,l}=\\mathbb{E}\\left[Y_{t}^{k}Y_{t}^{l}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\Sigma_{t}^{\\mathbf{y}}$ does not depend on the dimension $1\\le j\\le D$ and we write $q_{t}$ for the multivariate Gaussian density of Y[tK]. Since we know the distribution of Y[0K], we can directly calculate the corresponding score function by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{\\mathbf{y}^{[K]}}\\log q_{t}\\left(\\mathbf{Y}_{t}^{[K]}\\right)=-\\mathbf{A}_{t}^{-1}\\mathbf{Y}_{t}^{[K]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "A.3 Estimating the score via augmented score matching loss ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Conditioning $\\mathbf{Z}_{t}$ on $\\mathbf{x}_{0}\\sim p_{0}$ and a realisation $\\mathbf{y}_{t}^{[K]}$ of the stacked augmenting processes $\\mathbf{Y}_{t}^{[K]}$ defined in eq. (64) at fixed time $t\\in[0,T]$ results in the Gaussian vector $\\tilde{\\mathbf{X}}_{t}\\sim\\mathcal{N}(\\tilde{\\pmb{m}}_{t},\\tilde{\\Sigma}_{t})$ with mean ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tilde{m}_{t}=c(t)\\mathbf{x}_{0}+\\sum_{k=1}^{K}\\eta_{t}^{k}\\mathbf{y}_{t}^{k},\\quad w h e r e\\quad\\eta_{t}^{k}=\\sum_{l=1}^{K}\\mathbb{E}\\left[X_{t}Y_{t}^{l}\\right]\\left[\\left(\\boldsymbol{\\Sigma}_{t}^{\\mathbf{y}}\\right)^{-1}\\right]_{l,k}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and covariance ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tilde{\\Sigma}_{t}=\\left(c^{2}(t)\\sigma^{2}(t)-\\tau_{t}^{2}\\right)\\mathbf{I}_{d},\\quad w h e r e\\quad\\tau_{t}^{2}=\\sum_{k=1}^{K}\\eta_{t}^{k}\\mathbb{E}\\left[X_{t}Y_{t}^{k}\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We denote with $\\nabla_{\\mathbf{x}}\\log{p_{0t}}$ the conditional score function of $\\tilde{\\mathbf{X}}_{t}$ and calculate for the gradient w.r.t. $\\mathbf{x}=(x_{1},...,x_{D})\\in\\mathbb{R}^{L}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{x}}\\log p_{0t}(\\mathbf{x}|\\mathbf{y}_{t}^{[K]},\\mathbf{x}_{0})=-\\tilde{\\Sigma}_{t}^{-1}(\\mathbf{x}-\\tilde{m}_{t})=-\\frac{(\\mathbf{x}-\\tilde{m}_{t})}{(c^{2}(t)\\sigma^{2}(t)-\\tau_{t}^{2})}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and for the gradient w.r.t. $\\mathbf{y}^{k}=(y_{1}^{k},...,y_{D}^{k})\\in\\mathbb{R}^{D}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\nabla_{\\mathbf y^{k}}\\log p_{0t}(\\mathbf x|\\mathbf y_{t}^{[K]},\\mathbf x_{0})=-\\frac12\\nabla_{\\mathbf y^{k}}\\left[(\\mathbf x-\\tilde{m}_{t})^{T}\\tilde{\\Sigma}_{t}^{-1}(\\mathbf x-\\tilde{m}_{t})\\right]}\\\\ &{}&{=-\\eta_{t}^{k}\\nabla_{\\mathbf x}\\log p_{0t}(\\mathbf x|\\mathbf y_{t}^{[K]},\\mathbf x_{0}).\\quad\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Deploying this relation of $\\nabla_{\\mathbf{x}}\\log{p_{0t}}$ and $\\nabla_{\\mathbf{y}^{k}}\\log{p_{0t}}$ we derive the augmenting score matching loss that reduces the dimensionality of the score model we have to learn to the dimensionality of the data distribution and results in a score model guided by the the known score function $\\nabla_{\\mathbf{y}^{[K]}}\\log{q_{t}}$ . ", "page_idx": 21}, {"type": "text", "text": "Proposition A.4 (Optimal Score Model). Assume that $s_{\\theta}$ is optimal w.r.t. the augmented score matching loss $\\mathcal{L}$ . The score model ", "page_idx": 21}, {"type": "equation", "text": "$$\nS_{\\theta}(\\mathbf{Z}_{t},t):=\\left(s_{\\theta}(\\mathbf{X}_{t}-\\sum_{k}\\eta_{t}^{k}\\mathbf{Y}_{t}^{k},t),-\\eta_{t}^{1}s_{\\theta}(\\mathbf{X}_{t}-\\sum_{k}\\eta_{t}^{k}\\mathbf{Y}_{t}^{k},t),...,-\\eta_{t}^{K}s_{\\theta}(\\mathbf{X}_{t}-\\sum_{k}\\eta\\mathbf{Y}_{t}^{k},t)\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "yields the optimal $L^{2}(\\mathbb{P})$ approximation of $\\nabla_{\\mathbf{z}}\\log p_{t}(\\mathbf{Z}_{t})$ via ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S_{\\theta}(\\mathbf{Z}_{t},t)+\\nabla_{\\mathbf{z}}\\log q_{t}(\\mathbf{Y}_{t}^{[K]})\\approx\\nabla_{\\mathbf{z}}\\log p_{t}(\\mathbf{Z}_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Fix $t\\in[0,T]$ . We write $p_{t}^{a u g}$ for the density of $\\mathbf{Z}_{t},\\,p_{0t}^{a u g}$ for the conditional density of $\\mathbf{Z}_{t}$ on X0, p0t for the density of X\u02dct and q0t for the conditional density of Y[tK] on X0. First note that Y[tK] and $\\mathbf{X}_{0}$ are independent by assumption and hence $q_{t}=q_{0t}$ . By direct calculations we find ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\mathbf x}\\log p_{t}^{a u_{\\theta}}(\\mathbf Z_{t})=\\mathbb{E}_{(\\mathbf x_{0}|\\mathbf X_{t},Y_{t}^{[K]})}\\left[\\nabla_{\\mathbf x}\\log p_{0t}^{a u_{\\theta}}(\\mathbf Z_{t}|\\mathbf X_{0})\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{(\\mathbf x_{0}|\\mathbf X_{t},Y_{t}^{[K]})}\\left[\\nabla_{\\mathbf x}\\log\\left(p_{0t}(\\mathbf X_{t}|\\mathbf Y_{t}^{[K]},\\mathbf X_{0})q_{0t}(\\mathbf Y_{t}^{[K]}|\\mathbf X_{0})\\right)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{(\\mathbf x_{0}|\\mathbf X_{t},\\mathbf Y_{t}^{[K]})}\\left[\\nabla_{\\mathbf x}\\log p_{0t}(\\mathbf X_{t}|\\mathbf Y_{t}^{[K]},\\mathbf X_{0})+\\underbrace{\\nabla_{\\mathbf x}\\log q_{t}(\\mathbf Y_{t}^{[K]})}_{=\\mathbf{0}_{a}}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{(\\mathbf x_{0}|\\mathbf X_{t},\\mathbf Y_{t}^{[K]})}\\left[\\nabla_{\\mathbf x}\\log p_{0t}(\\mathbf X_{t}|\\mathbf Y_{t}^{[K]},\\mathbf X_{0})\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\stackrel{(\\mathrm{gs})}{=}\\mathbb{E}_{(\\mathbf x_{0}|\\mathbf X_{t},\\mathbf Y_{t}^{[K]})}\\left[\\frac{\\mathbf X_{t}-\\sum_{k}\\eta_{k}^{k}\\mathbf Y_{t}^{k}-c(t)\\mathbf X_{0}}{c^{2}(t)c^{2}(t)-\\tau_{k}^{2}}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Hence the best $L^{2}(\\mathbb{P})$ -approximation of $\\nabla_{\\mathbf{x}}\\log p_{t}^{a u g}(\\mathbf{Z}_{t})$ is a minimizer of the augmented score matching loss by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\mathbf{x}}\\log p_{t}^{a u g}(\\mathbf{Z}_{t})\\stackrel{(77)}{=}\\mathbb{E}_{(\\mathbf{X}_{0}|\\mathbf{X}_{t},\\mathbf{Y}_{t}^{[K]})}\\left[\\frac{\\mathbf{X}_{t}-\\sum_{k}\\eta_{t}^{k}\\mathbf{Y}_{t}^{k}-c(t)\\mathbf{X}_{0}}{c^{2}(t)\\sigma^{2}(t)-\\tau_{t}^{2}}\\right]}\\\\ &{\\quad=\\underset{s_{\\theta}}{\\mathrm{arg}\\,\\mathrm{min}}\\,\\mathbb{E}_{(\\mathbf{X}_{0},\\mathbf{Y}_{t}^{[K]})}\\mathbb{E}_{(\\mathbf{X}_{t}|\\mathbf{Y}_{t}^{[K]},\\mathbf{X}_{0})}\\left[\\left\\|s_{\\theta}(\\mathbf{X}_{t}-\\sum_{k=1}^{K}\\eta_{t}^{k}\\mathbf{Y}_{t}^{k},t)-\\frac{\\mathbf{X}_{t}-\\sum_{k}\\eta_{t}^{k}\\mathbf{Y}_{t}^{k}-c(t)\\mathbf{X}_{0}}{c^{2}(t)\\sigma^{2}(t)-\\tau_{t}^{2}}\\right\\|^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n{}^{\\!\\!()}\\arg\\operatorname*{min}_{s_{\\theta}}\\mathbb{E}_{(\\mathbf{X}_{0},\\mathbf{Y}_{t}^{[K]})}\\mathbb{E}_{(\\mathbf{X}_{t}|\\mathbf{Y}_{t}^{[K]},\\mathbf{X}_{0})}\\left[\\left\\|s_{\\theta}(\\mathbf{X}_{t}-\\sum_{k=1}^{K}\\eta_{t}^{k}\\mathbf{Y}_{t}^{k},t)-\\nabla_{\\mathbf{x}}\\log p_{0t}(\\mathbf{X}_{t}|\\mathbf{Y}_{t}^{[K]},\\mathbf{X}_{0})\\right\\|^{2}\\right]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Assume now that $s_{\\theta}$ is a minimizer of the augmented score matching loss. Similar to the calculation above we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\mathbf{y}^{k}}\\log p_{t}^{a u g}(\\mathbf{Z}_{t})=\\mathbb{E}_{(\\mathbf{X}_{0}|\\mathbf{X}_{t},\\mathbf{Y}_{t}^{[K]})}\\left[\\nabla_{\\mathbf{y}^{k}}\\log p_{0t}^{a u g}(\\mathbf{Z}_{t}|\\mathbf{X}_{0})\\right]}\\\\ &{\\phantom{=}=\\mathbb{E}_{(\\mathbf{X}_{0}|\\mathbf{X}_{t},\\mathbf{Y}_{t}^{[K]})}\\left[\\nabla_{\\mathbf{y}^{k}}\\log\\Big(p_{0t}(\\mathbf{X}_{t}|\\mathbf{Y}_{t}^{[K]},\\mathbf{X}_{0})q_{0t}(\\mathbf{Y}_{t}^{[K]}|\\mathbf{X}_{0})\\Big)\\right]}\\\\ &{\\phantom{=}=\\mathbb{E}_{(\\mathbf{X}_{0}|\\mathbf{X}_{t},\\mathbf{Y}_{t}^{[K]})}\\left[\\nabla_{\\mathbf{y}^{k}}\\log p_{0t}(\\mathbf{X}_{t}|\\mathbf{Y}_{t}^{[K]},\\mathbf{X}_{0})+\\nabla_{\\mathbf{y}^{k}}\\log q_{t}(\\mathbf{Y}_{t}^{[K]})\\right]}\\\\ &{\\phantom{=}\\stackrel{(70)}{=}-\\eta_{t}^{k}\\mathbb{E}_{(\\mathbf{X}_{0}|\\mathbf{X}_{t},\\mathbf{Y}_{t}^{[K]})}\\left[\\nabla_{\\mathbf{x}}\\log p_{0t}(\\mathbf{X}_{t}|\\mathbf{Y}_{t}^{[K]},\\mathbf{X}_{0})\\right]+\\nabla_{\\mathbf{y}^{k}}\\log q_{t}(\\mathbf{Y}_{t}^{[K]})}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and hence $\\begin{array}{r l r}{-\\eta_{t}^{k}s_{\\theta}(\\mathbf{X}_{t}\\ -\\ \\sum_{k}\\eta_{t}^{k}\\mathbf{Y}_{t}^{k})}&{{}\\!+\\ \\nabla_{\\mathbf{y}^{k}}\\log q_{t}(\\mathbf{Y}_{t}^{[K]})}&{}\\end{array}$ is the best approximation of $\\nabla_{\\mathbf{y}^{k}}\\log p_{t}^{a u g}(\\mathbf{Z}_{t})$ in $L^{2}(\\mathbb{P})$ and the score model ", "page_idx": 22}, {"type": "equation", "text": "$$\nS_{\\theta}(\\mathbf{Z}_{t},t):=\\left(s_{\\theta}(\\mathbf{X}_{t}-\\sum_{k}\\eta_{t}^{k}\\mathbf{Y}_{t}^{k},t),-\\eta_{t}^{1}s_{\\theta}(\\mathbf{X}_{t}-\\sum_{k}\\eta_{t}^{k}\\mathbf{Y}_{t}^{k},t),...,-\\eta_{t}^{K}s_{\\theta}(\\mathbf{X}_{t}-\\sum_{k}\\eta\\mathbf{Y}_{t}^{k},t)\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "yields the best $L^{2}(\\mathbb{P})$ -approximator of $\\nabla_{\\mathbf{z}}\\log{p_{t}}$ via ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S_{\\theta}(\\mathbf{Z}_{t},t)+\\nabla_{\\mathbf{z}}\\log q_{t}(\\mathbf{Y}_{t}^{[K]})\\approx\\nabla_{\\mathbf{z}}\\log p_{t}(\\mathbf{Z}_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "B Forward sampling ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We assume throughout this section type II fBM. Given the marginal covariance matrix $\\Sigma_{t}$ of $\\mathbf{Z}_{t}\\vert\\mathbf{x}_{0}$ we uniformly sample first a time point $t\\in(0,T]$ and second $\\bar{\\mathbf Z_{t}}\\sim\\mathcal{N}(\\hat{\\mathbf z}_{t},\\pmb{\\Sigma}_{t})$ with ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{z}}_{t}=(c(t)\\mathbf{x}_{0,1},0,...,0,c(t)\\mathbf{x}_{0,2},0,...,0,...,...,c(t)\\mathbf{x}_{0,D},0,...0)\\in\\mathbb{R}^{D(K+1)}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we use $\\mathbb{E}\\left[\\mathbf{X}_{t}|\\mathbf{x}_{0}\\right]=c(t)\\mathbf{x}_{0}$ and $\\mathbb{E}\\left[\\mathbf{Y}_{t}^{k}\\right]=\\mathbf{0}_{D}$ . In the following we characterize further the entries of the marginal covariance matrix $\\Sigma_{t}$ . The calculations in this section are straightforward; nevertheless, we present them in full detail to facilitate easy understanding for the interested reader. We begin with rewriting $\\sigma^{2}$ from Proposition 4.2 given by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sigma^{2}(t)=c^{2}(t)\\int_{0}^{t}\\alpha^{2}(t,s)d s\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\alpha(t,s)=\\bar{\\omega}\\frac{g(s)}{c(s)}-\\sum_{k=1}^{K}\\omega_{k}\\gamma_{k}\\int_{s}^{t}\\frac{g(u)}{c(u)}e^{-\\gamma_{k}(u-s)}d u}\\\\ {\\displaystyle=\\sum_{k=1}^{K}\\omega_{k}\\underbrace{\\left(\\frac{g(s)}{c(s)}-\\gamma_{k}\\int_{s}^{t}\\frac{g(u)}{c(u)}e^{-\\gamma_{k}(u-s)}d u\\right)}_{=:\\alpha_{k}(t,s)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "With ", "page_idx": 22}, {"type": "equation", "text": "$$\nf_{k}(u,s):=\\frac{g(u)}{c(u)}e^{-\\gamma_{k}(u-s)}\\quad a n d\\quad I_{k}(t,s):=\\int_{s}^{t}f_{k}(u,s)d u\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\eta_{1}^{\\prime\\prime}=e^{2}(t)\\int_{0}^{t}\\sum_{u=1}^{\\infty}\\beta d u}\\\\ {=}&{\\eta_{1}^{\\prime\\prime}\\left(\\prod_{j=1}^{K}\\left(\\frac{\\eta_{1}^{\\prime\\prime}(u)}{\\alpha(u)}-\\gamma_{1}\\int_{x}^{t}\\bar{f}_{u}(u,s)i u\\right)\\right)^{2}d s}\\\\ &{=e^{2}(t)\\int_{0}^{t}\\left(\\sum_{u=1}^{K}\\alpha_{1}(t,u)\\right)^{2}d s}\\\\ &{=e^{2}(t)\\int_{0}^{t}\\left(\\sum_{u=1}^{K}\\alpha_{1}(t,u)\\right)^{2}d s}\\\\ &{=e^{2}(t)\\int_{0}^{t}\\sum_{u=1}^{K}\\alpha_{1}\\mu_{2}(u,s)(t,u)\\mu_{1}(s)d s}\\\\ &{=\\displaystyle\\sum_{k=1}^{K}\\alpha_{1}\\mu_{2}e^{2}(t)\\int_{0}^{t}\\alpha_{1}(t,s)\\mu_{2}(t,s)d s}\\\\ &{=\\displaystyle\\sum_{k=1}^{K}\\frac{\\alpha_{1}\\mu_{2}\\sigma_{1}^{\\prime\\prime}(t)}{\\alpha_{1}\\mu_{2}}\\int_{0}^{t}\\left(\\frac{\\eta_{2}^{\\prime\\prime}(u)}{\\alpha_{1}^{\\prime}}-\\gamma_{1}t,(u,s)\\right)\\left(\\frac{\\eta_{1}^{\\prime\\prime}(u)}{\\alpha(u)}-\\gamma_{1}t,(u,s)\\right)d s}\\\\ &{=\\displaystyle\\sum_{k=1}^{K}\\alpha_{1}\\mu_{2}\\left\\{\\begin{array}{l}{\\alpha_{1}(t)-\\gamma_{1}(t,u)}\\\\ {\\alpha_{2}(t)}\\end{array}\\right\\}\\left(\\sum_{u=1}^{K}\\alpha_{1}(t,u)\\right)\\left(\\sum_{u=1}^{K}\\alpha_{2}(t,u)\\right)-\\gamma_{1}t}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{var}_{B}(t)=c^{2}(t)\\int_{0}^{t}\\frac{g^{2}(s)}{c^{2}(s)}d s\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "corresponds to the purely Brownian marginal variance, explicitly calculated for VE and VP in Song et al. [16]. Using the above derivation, we derive the closed form variance schedule for FVE dynamics. ", "page_idx": 23}, {"type": "text", "text": "Fractional Variance Exploding Fix $\\sigma_{\\mathrm{max}}>\\sigma_{\\mathrm{min}}>0$ and define $\\begin{array}{r}{r:=\\frac{\\sigma_{\\mathrm{max}}}{\\sigma_{\\mathrm{min}}}}\\end{array}$ . Following Song et al. [16] we set ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mu(t)\\equiv0\\quad a n d\\quad g(t)=a r^{t}\\quad w i t h\\quad a=\\sigma_{m i n}\\sqrt{2\\log(r)}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "such that $c(t)=\\exp(0)=1$ and calculate ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{I_{k}(t,s)=\\int_{s}^{t}f_{k}(u,s)d u=\\int_{s}^{t}a r^{u}e^{-\\gamma_{k}(u-s)}d u=F(t)-F(s)}}\\\\ &{}&{=\\underbrace{\\frac{a}{\\mathrm{ln}(r)-\\gamma_{k}}\\left(e^{\\mathrm{in}(r)t-\\gamma_{k}t+\\gamma_{k}s}-e^{\\mathrm{ln}(r)s}\\right)}_{a_{k}}=a_{k}\\left(r^{t}e^{-\\gamma_{k}(t-s)}-r^{s}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "since the derivative of $F(u)=a_{k}r^{u}e^{-\\gamma_{k}(u-s)}$ is given by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{\\displaystyle\\frac{d}{d u}F(u)=\\frac{d}{d u}\\left[a_{k}r^{u}e^{-\\gamma_{k}(u-s)}\\right]=a_{k}r^{u}\\ln(r)e^{-\\gamma_{k}(u-s)}+a_{k}r^{u}e^{-\\gamma_{k}(u-s)}(-\\gamma_{k})}}}\\\\ {{{\\displaystyle=\\frac{a}{\\ln(r)-\\gamma_{k}}(\\ln(r)-\\gamma_{k})(r^{u}e^{-\\gamma_{k}(u-s)})=a r^{u}e^{-\\gamma_{k}(u-s)}.}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We calculate for the variance of $X_{t}|\\mathbf{x}_{0}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{V}\\left[X_{t}|\\mathbf{x}_{0}\\right]=\\sum_{i,j=1}^{K}\\omega_{i}\\omega_{j}\\{v a r_{B}(t)-a\\gamma_{i}\\underbrace{\\int_{0}^{t}r^{s}I_{i}(t,s)d s}_{J_{i}(t)}-a\\gamma_{j}\\underbrace{\\int_{0}^{t}r^{s}I_{j}(t,s)d s}_{J_{j}(t)}}}\\\\ &{}&{+\\,\\gamma_{i}\\gamma_{j}\\underbrace{\\int_{0}^{t}I_{i}(t,s)I_{j}(t,s)d s}_{=J_{i,j}(t)}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "with ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{J_{k}(t)=a_{k}\\displaystyle\\int_{0}^{t}r^{s}\\left(r^{t}e^{-\\gamma_{k}(t-s)}-r^{s}\\right)d s=a_{k}\\displaystyle\\int_{0}^{t}r^{t+s}e^{-\\gamma_{k}(t-s)}d s-a_{k}\\displaystyle\\int_{0}^{t}r^{2s}d s}\\\\ &{}&{=a_{k}\\left[F_{1}(t)-F_{1}(0)\\right]-a_{k}\\left[F_{2}(t)-F_{2}(0)\\right]=a_{k}\\left[\\displaystyle\\frac{r^{2t}-r^{t}\\mathrm{e}^{-\\gamma_{k}t}}{\\ln{(r)}+\\gamma_{k}}-\\displaystyle\\frac{r^{2t}-1}{2\\ln{(r)}}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "since ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{d}{d s}F_{1}(s)=\\frac{\\left(r^{t+s}\\ln(r)e^{-\\gamma_{k}(t-s)}+r^{t+s}e^{-\\gamma_{k}(t-s)}(\\gamma_{k})\\right)}{\\ln(r)+\\gamma_{k}}=r^{t+s}e^{-\\gamma_{k}(t-s)},}\\\\ &{\\frac{d}{d s}F_{2}(s)=\\frac{d}{d s}\\left[\\frac{r^{2s}}{2\\ln(r)}\\right]=\\frac{r^{2s}\\ln(r)2}{2\\ln(r)}=r^{2s}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Finally ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{i,j}(t)=a_{i}a_{j}\\displaystyle\\int_{0}^{t}\\left(r^{t}e^{-\\gamma_{i}(t-s)}-r^{s}\\right)\\left(r^{t}e^{-\\gamma_{j}(t-s)}-r^{s}\\right)d s}\\\\ &{\\qquad=a_{i}a_{j}\\left[\\left(\\displaystyle\\frac{r^{2t}\\left(1-e^{-t(\\gamma_{i}+\\gamma_{j})}\\right)}{\\gamma_{i}+\\gamma_{j}}\\right)-\\displaystyle\\frac{r^{2t}-r^{t}e^{-\\gamma_{i}t}}{\\gamma_{i}+\\ln(r)}-\\displaystyle\\frac{r^{2t}-r^{t}e^{-\\gamma_{j}t}}{\\gamma_{j}+\\ln(r)}+\\displaystyle\\frac{r^{2t}-1}{2\\ln(r)}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "image", "img_path": "B9qg3wo75g/tmp/fb1f13706737eabddbc00ae86258f926f6b561127a963742cd92afe196fe626f.jpg", "img_caption": ["(a) Variance schedule of the forward FVE process. (b) Variance schedule of the augmenting processes. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 5: Analytical solution (blue) used by our method for FVE dynamics with $K=5$ and $H=0.5$ compared to the approximated solution (dashed red) resulting from solving ODE (119). ", "page_idx": 24}, {"type": "text", "text": "We calculate the covariance of $X_{t}|\\mathbf{x}_{0}$ and $Y_{t}^{l}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\cos(X_{i}|x_{0},Y_{i}^{t})=c(t)\\displaystyle\\int_{0}^{t}e(t,s)e^{-x_{i}(t-s)}d s}&{\\quad{\\mathrm{(II)}}}\\\\ {=\\displaystyle\\int_{0}^{t}\\sum_{k=1}^{\\infty}\\sin\\left[e(s)-\\gamma_{k}\\int_{s}^{t}f_{k}(s,s)d s\\right]e^{-x_{i}(t-s)}d s}&{\\quad{\\mathrm{(II)}}}\\\\ {=\\displaystyle\\sum_{k=1}^{\\infty}\\sin\\left[e\\int_{s}^{t}r e^{-x_{i}(t-s)}d s-\\gamma_{k}\\int_{s}^{t}\\int_{s}^{t}f_{k}(s,s)d s e^{-x_{i}(t-s)}d s\\right]}&{\\quad{\\mathrm{(II)}}}\\\\ {=\\displaystyle\\sum_{k=1}^{\\infty}\\sin\\left[e\\int_{s}^{t}r e^{-x_{i}(t-s)}d s-\\gamma_{k}\\int_{s}^{t}\\left(r^{t}e^{-x_{i}(t-s)}-r^{*}\\right)e^{-x_{i}(t-s)}d s\\right]}\\\\ {=\\displaystyle\\sum_{k=1}^{\\infty}e\\int_{s}^{t}{\\mathrm{(a}_{k}\\int_{s}^{t}\\left(r^{t}e^{-x_{i}(t-s)}-r\\right)d s}+\\gamma_{k}\\int_{s}^{t}\\left(r^{t}e^{-x_{i}(t-s)}-r^{*}\\right)e^{-x_{i}(t-s)}d s\\right]}\\\\ {=\\displaystyle\\sum_{k=1}^{\\infty}\\sin\\left[e^{-x_{i}\\int_{s}^{t}\\frac{r^{*}e^{-x_{i}(t)}d s}{r^{*}e^{-x_{i}(t-s)}-r^{*}e^{x_{i}(t-s)}}d s}\\int_{s}^{t}\\left(r^{t}e^{-r^{*}(s-1)}-r^{*}\\right)e^{-x_{i}(t-s)}d s\\right]}\\\\ {=\\displaystyle\\sum_{k=1}^{\\infty}\\left[(a+a\\gamma_{k})\\frac{(r^{*}e^{-x_{i}(t-s)}-r)}{r^{*}e+h(r)}-\\gamma_{k}e^{x_{i}\\int_{s}^{t}\\left(1-e^{-x_{i}(t+s)}-r\\right)}\\right].\\qquad{\\mathrm{(II)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Fractional Variance Preserving To the best of our knowledge, there is no closed form solution for $\\textstyle\\int_{s}^{t}f_{k}(u,s)d u$ for the dynamics of FVP. In this case, we numerically solve an ODE to determine the marginal covariance matrix of the conditional augmented forward process. ", "page_idx": 24}, {"type": "text", "text": "General Dynamics. The covariance matrix of the conditional augmented forward process with dynamics ", "page_idx": 24}, {"type": "equation", "text": "$$\nd\\mathbf{Z}_{t}=\\mathbf{F}(t)\\mathbf{Z}_{t}d t+\\mathbf{G}(t)\\mathrm{d}\\mathbf{B}_{t},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "solves the ODE ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\partial_{t}\\pmb{\\Sigma}_{t}=\\mathbf{F}(t)\\pmb{\\Sigma}_{t}+\\pmb{\\Sigma}_{t}\\mathbf{F}(t)^{T}+\\mathbf{G}(t)\\mathbf{G}(t)^{T},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "lacking in general a closed form solution [43] in contrast to the setting of Song et al. [16]. This approach is applicable for any choice of $\\mu$ and $g$ in the forward dynamics, but depending on the choice of drift and diffusion function it might not yield a numerically stable solution. We empirically observe in Figure 5 that the analytical solution for FVE and the numerical approximation of the variance schedule, determined by solving eq. (119) do not differ significantly. ", "page_idx": 24}, {"type": "text", "text": "Variance schedules. We normalize the variance schedule of FVE and FVP dynamics such that the variance at $t=0$ and at $t=T$ is equal to the variance used in the purely Brownian setting of VE and VP dynamics. For both FVE and FVP dynamics we calculate $\\tilde{\\omega}$ according to Proposition 3.3 and determine $\\tilde{\\sigma}_{T}^{2}$ and define $\\omega=\\tilde{\\omega}/\\tilde{\\sigma}_{T}^{2}$ to weight the OU-processes. By doing so, the terminal variance remains the same throughout different choices of $H$ , as empirically confirmed in Figure 6. ", "page_idx": 24}, {"type": "image", "img_path": "B9qg3wo75g/tmp/68d043baf2dcfd4cd0f197f2016ef785a8cde9823523156ec54ea3ef0780aa3a.jpg", "img_caption": ["(a) Variance schedules of the forward FVE process. (b) Variance schedules of the forward FVP process. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 6: Normalized variance schedules for $K\\,=\\,5$ over time. (a) Variance schedules of FVE dynamics, calculated in closed form according to the derived formulas. The shape of the schedule is preserved throughout different values of $H$ . (b) Variance schedules of FVP dynamics numerically approximated. The shape of the schedule is shifted for different values of $H$ . ", "page_idx": 25}, {"type": "text", "text": "In Figure 6 we observe for FVE dynamics that not only the terminal variance is the same across different choices of $H$ but also the shape of the variance schedule. For FVP dynamics, the shape of the variance schedule shifts with different values of $H$ , approaching a nearly linear schedule for $H=0.1$ , while $H=0.9$ offers a decreasing variance towards the end near $t=T$ . ", "page_idx": 25}, {"type": "text", "text": "C Implementation details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We used for all experiments a conditional U-Net [70] architecture and the Adam optimizer [71] with PyTorchs OneCylce learning rate scheduler [72]. On MNIST we trained without exponential moving average (EMA) while on CIFAR10 we conducted experiments with and without EMA. ", "page_idx": 25}, {"type": "text", "text": "Set up on MNIST. We used an attention resolution of [4, 2], 3 resnet blocks and a channel multiplication of $[1,2,2,2,2]$ and trained with a maximal learning rate of $10^{-4}$ for $50k$ iterations and a batch size of 1024. For all MNIST training runs we used one A100 GPU per run, taking approximately 17 hours. ", "page_idx": 25}, {"type": "text", "text": "Set up on CIFAR10. We used an attention resolution of [8], 4 resnet blocks and a channel multiplication of $[1,2,2,2,2]$ . For the experiments without EMA, we used the same setup as with MNIST, but trained the models in parallel on two A100 GPUs for $300k$ iterations with an effective batch size of 1024. When training with EMA, we followed the set up of Song et al. [16] using an EMA decay of 0.9999 for all FVP dynamics and an EMA decay of 0.999 for all FVE dynamics. In contrast to Song et al. [16] we used PyTorchs OneCycleLR learning rate scheduler with a maximal learning rate of $2\\cdot10^{-4}$ and trained only for 1mio iterations instead of the 1.3mio iterations in Song et al. [16]. ", "page_idx": 25}, {"type": "text", "text": "D Additional experiments ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In addition to the experiments presented in the main part, we provide additional results here, including a full evaluation of FVE dynamics on MNIST, as well as training on CIFAR10 without EMA. ", "page_idx": 25}, {"type": "text", "text": "Evaluation of different Hurst indices of FVE dynamics on MNIST. In Table 4 we provide the evaluation of FVE dynamics. For the ease of comparisan, we include the quantitative results on FVP dynamics already presented in the main part. For FVE dynamics both, the super-diffusive regime and the sub-diffusive regime achieve a higher FID as the purely Brownian dynamics for $K=1,2$ throughout all tested Hurst indices and for $K=3$ throughout all tested Hurst indices except for $H=0.9$ with a higher pixel-wise diversity in the sub-diffusive regime of $H<0.5$ . ", "page_idx": 25}, {"type": "text", "text": "Training on CIFAR10 without EMA. As Song et al. [16] point out, the empirically optimal EMA decay rate for VP dynamics differs from that for VE dynamics. Since we do not have the computational resources to optimize the EMA decay rate for every configuration of our framework, we evaluated it in line with Song et al. [16] using a consistent EMA decay rate of 0.999 across all configurations of FVE dynamics and 0.9999 across all configurations of FVP dynamics. Nevertheless, because the optimal EMA decay rate appears to depend on the dynamics of the underlying stochastic ", "page_idx": 25}, {"type": "table", "img_path": "B9qg3wo75g/tmp/d0233283a8aae778a467c41edeb4e574761288dc9d1f636422ce30f065f52456.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "B9qg3wo75g/tmp/add0c356bff108a11f768d2a284256ca7521e20c5544e0f59250c3de5a5038f9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Table 4: FID and pixel-wise diversity scores of GFDM compared to the original setting of purely Brownian driven dynamics VE and VP. In bold the scores that are better than both purely Brownian driven dynamics VE and VP. The overall best scores within the experiment are boxed in. ", "page_idx": 26}, {"type": "table", "img_path": "B9qg3wo75g/tmp/4ec856732f1d6c6de99b550a13a3d21d25986612b9e9546460403bb618520db1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Table 5: Quantitative results for FVE dynamics and varying Hurst index on CIFAR10 trained without EMA. In bold the scores that are better than both purely Brownian driven dynamics VE and VP. The overall best scores within the experiment are boxed in. ", "page_idx": 26}, {"type": "text", "text": "process, we also evaluate our framework without EMA. In Table 5 we observe that the best performing configuration in terms of FID is FVP( ${\\cal H}=0.9,K=2)$ with an FID of 8.99 and FVP( $\\stackrel{\\triangledown}{H}=0.1,K=$ 1) with an FID of 8.93 compared to the purely Brownian dynamics VE with an FID of 9.38 and VP with an FID of 17.29. Due to limited computational resources we only compared the best purely Brownian dynamics (VE) with the performance of corresponding augmented FVE dynamic of GFDM. As to be expected, using EMA for training of GFDM results in improved performnace w.r.t. image quality measured by FID obervable in Table 2b. ", "page_idx": 26}, {"type": "text", "text": "Effect of the number of augmenting processes in the super-diffusive regime. Additionally, in Figure 7, we show the FID evolution of the super-diffusive regime for various numbers of augmenting processes, showing a similar pattern that either that $K=2$ or $K=3$ yields the best performance across different datasets and dynamics. ", "page_idx": 26}, {"type": "image", "img_path": "B9qg3wo75g/tmp/5f851b5e292e569a0cb880c622ba9c7a347f9660dd3598cff70f90f5f4341251.jpg", "img_caption": ["Table 6: Averaged FID values for different NFEs of the super-diffusive regime compared to purely Brownian dynamics. "], "img_footnote": ["(a) Conditional image generation on MNIST with FVP. (b) Conditional image generation on MNIST with FVE. "], "page_idx": 26}, {"type": "text", "text": "Figure 7: Dynamics driven by MA-fBM with super-diffusive Hurst index $H=0.9$ and $K=0.7$ perform in all four experiments we conducted better than the original purely Brownian driven dynamics, where either $K=2$ or $K=3$ yields the best performance. ", "page_idx": 26}, {"type": "table", "img_path": "B9qg3wo75g/tmp/38ac26c953c3c21a0a0b2cc18cf344123bb50b2b4edde2968008c32bf404b9e4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Visual comparison of generated CIFAR10 images sampled with SDE dynamics. ", "page_idx": 27}, {"type": "image", "img_path": "B9qg3wo75g/tmp/87b26989e395dd4ff14077334a0b7de42ad525bc892633dbe333acf4439302db.jpg", "img_caption": ["Figure 8: (LHS) Images generated with the purely Brownian driven VP dynamics sampled with SDE dynamics, a FID of 4.85 and a pixel-wise diversity of 3.42. (RHS) Images generated with $\\mathrm{FVP}(\\dot{H}=0.9,K=2)$ dynamics sampled with SDE, a FID of 3.77 and a pixel-wise diversity of 3.60. ", "(a) Purely Brownian VP sample. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "B9qg3wo75g/tmp/8f29281e6468629932c06a8249ccdfebf08ad66a980c22b73d722ecbed0e38be.jpg", "img_caption": ["(b) Super-diffusive regime of MA-fBM with $H=0.9$ . "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "B9qg3wo75g/tmp/6a33c8d50ad1bafc9c1c0b74f95344937c7065c27f8984cd3bc1201029dadc17.jpg", "img_caption": ["Figure 9: (RHS) Images generated with the purely Brownian driven VP dynamics sampled with PF ODE, a FID of 5.63 and pixel-wise diversity of 3.91. (LHS) Images generated with FVP( $H=$ $0.9,K=2)$ ) dynamics sampled with PF ODE, a FID of 12.36 and pixel-wise diversity of 4.89. ", "(a) Purely Brownian VP sample. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "B9qg3wo75g/tmp/5c48972a78e1fae6492912d9b67a96b1737632dd6add45795595db1a0659a67a.jpg", "img_caption": ["(b) Super-diffusive regime of MA-fBM with $H=0.9$ . "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "B9qg3wo75g/tmp/eaa8f65833cfc27e3de2e87562a0ba88f91facecc33b7a87bc3581807965c522.jpg", "img_caption": ["(c) $\\mathrm{FVP}(K=5,H=0.9)$ with $\\mathrm{{FID}=2.17}$ and $\\mathrm{VS}_{p}=25.15$ . "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 10: Diversifying effect of the augmenting processes with FVP dynamics on MNIST. The super-diffusive regime with $H=0.9$ : For $K=5$ instead of $K=3$ augmenting processes the pixel VS increases from 24.18 to 25.15. ", "page_idx": 29}, {"type": "text", "text": "F Computational cost of augmenting processes ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section we compare the computation time of GFDM to the purely Brownian setting of traditional diffusion models. For a given Hurst index $H\\,\\in\\,(0,1)$ and a given $K$ , the optimal coefficients $\\omega_{1},...,\\omega_{K}$ are calculated only once before training. For completeness of our quantitative compute time evaluation, we provide the average computation time in seconds, needed to compute $\\omega_{1},...,\\omega_{K}$ on a GPU Tesla V100 with 32 GB RAM. We randomly sample 1000 times $H\\sim\\mathcal{U}[0.1,0.9]$ for a given $K\\in\\{1,2,3,4,5\\}$ and report the average computation time in Table 7. ", "page_idx": 29}, {"type": "table", "img_path": "B9qg3wo75g/tmp/074a9a7aadab09f37ef22d01acce10195d503deb41d8805d7ec4b7e040e7908d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "Table 7: Averaged time in seconds needed before training to calculate for a given $K$ the optimal approximation coefficients using the approach of Daems et al. [27]. ", "page_idx": 29}, {"type": "text", "text": "Computation time during training The computational difference during training consists of the computation of the covariance matrix $\\Sigma_{t}$ instead of the marginal variance and sampling from a multivariate Gaussian instead of a univariate Gaussian. Note however, that we only need to calculate $\\Sigma_{t}$ for $D=1$ and also sample only once for a given time $t$ and a given data point. In Table 8 and Table 9 we report the average time of one training step measured in seconds calculated over 1000 training steps on CIFAR10. The underlying conditional U-Net has 58.7mio and EMA is applied. The batch size is 128 and all computation have been carried out on a GPU Tesla V100 with 32 GB RAM. ", "page_idx": 29}, {"type": "text", "text": "We observe that the computation time depends only minimaly increases when switching from the original model to the augmented system and increases across FVE and FVP dynamics by at most 11/1000 seconds, while the choice of the Hurst index $H$ has no effect on the computation time. ", "page_idx": 29}, {"type": "text", "text": "Computation time during sampling Since the augmented system depends for fixed $K$ only on the approximating coefficients $\\omega_{1},...,\\omega_{K}$ it would suffice to report the average sampling time for FVP and FVE dynamics for varying $K$ . Nevertheless, we report for $H\\in\\{0.9,0.5,0.\\dot{1}\\}$ in Table 10 and Table 11 the average time to sample a batch of 1000 images over 1000 discretization steps of the reverse-time SDE over 10 trials. We observe that the average time in seconds for one sampling step in the reverse dynamics of FVE and FVP dynamics increases for $K\\leq4$ by at most $2/100$ seconds. Only for $K=5$ we observe a significant increase of average sampling time of roughly $4/10$ seconds. ", "page_idx": 29}, {"type": "table", "img_path": "B9qg3wo75g/tmp/db69dc8273df78d08221fc82366e12ce8afab1cc0fab2c0bd6e8019fb51f6237.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "table", "img_path": "B9qg3wo75g/tmp/d031f349851f1f75dc240cdefc2c4a5ee8a416cf93ed4c40602e890fa314001c.jpg", "table_caption": ["Table 8: Average time in seconds for one training step with FVE dynamics on CIFAR10 with a batch size of 128, a conditional U-Net with 58.7mio parameters and EMA. "], "table_footnote": [], "page_idx": 30}, {"type": "table", "img_path": "B9qg3wo75g/tmp/dd14416bfd2ebcf80adcc2eef36bdd7922ebd44305c445bf3b30bab75a8096e9.jpg", "table_caption": ["Table 9: Average time in seconds for one training step with FVP dynamics on CIFAR10 with a batch size of 128, a conditional U-Net with 58.7mio parameters and EMA. "], "table_footnote": [], "page_idx": 30}, {"type": "table", "img_path": "B9qg3wo75g/tmp/8b8cccd18c76207f4f995fa1ded9fca97084724b430d1c35f25ff7de543fd389.jpg", "table_caption": ["Table 10: Average time in seconds for one sampling step in the reverse dynamics of FVE to generate data of dimension (3, 32, 32) with a batch size of 1000 using a conditional U-Net with 58.7mio and EMA. "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Table 11: Average time in seconds for one sampling step in the reverse dynamics of FVP to generate data of dimension (3, 32, 32) with a batch size of 1000 using a conditional U-Net with 58.7mio parameters and EMA. ", "page_idx": 30}, {"type": "text", "text": "G Likelihood computation ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Given the approximate PF ODE corresponding to the augmented forward process ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{z}_{t}=\\underbrace{\\left\\{\\mathbf{F}(t)\\mathbf{z}_{t}-\\frac{1}{2}\\mathbf{G}(t)\\mathbf{G}(t)^{T}\\left[S_{\\theta}(\\mathbf{z}_{t},t)+\\nabla_{\\mathbf{z}}\\log q_{t}(\\mathbf{y}_{t}^{[K]})\\right]\\right\\}}_{:=\\tilde{\\mathbf{f}}_{\\theta}(\\mathbf{z}_{t},t)}d t,\\quad t\\in[0,T]\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "we estimate according to Song et al. [16] the log-likelihoods of test data $\\mathbf{z}_{\\mathrm{0}}$ under the learned density $\\tilde{p}_{0}^{a u g}$ via ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\log\\tilde{p}_{0}^{a u g}(\\mathbf{z}_{0})=\\log\\tilde{p}_{T}^{a u g}(\\mathbf{z}_{T})+\\int_{0}^{T}\\nabla\\tilde{\\mathbf{f}}_{\\theta}(\\mathbf{z}_{t},t)\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "According to Song et al. [16], we integrate over $[\\epsilon,T]$ rather than $[0,T]$ , using the same value of $\\epsilon=10^{-3}$ , which has been empirically shown to yield the best performance when simulating the SDE. For $\\epsilon\\neq0$ and type II fBM we need to adjust the starting value of the augmenting processes from zero to a jointly sampled vector $\\mathbf{y}_{\\epsilon}=(y_{\\epsilon}^{1},...,y_{\\epsilon}^{K})\\sim\\mathcal{N}(\\breve{\\mathbf{0}}_{K},\\mathbf{A}_{\\epsilon})$ with ", "page_idx": 31}, {"type": "equation", "text": "$$\n(\\Lambda_{\\epsilon})_{k,l}=\\mathbb{E}\\left[y_{\\epsilon}^{k}y_{\\epsilon}^{l}\\right]=\\int_{0}^{\\epsilon}e^{-(\\gamma_{k}+\\gamma_{l})(\\epsilon-s)}\\mathrm{d}s=\\frac{1-e^{-(\\gamma_{k}+\\gamma_{l})\\epsilon}}{\\gamma_{k}+\\gamma_{l}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Using the exact likelihood of $\\mathbf{y}_{\\epsilon}$ and the independence of $\\mathbf{y}_{\\epsilon}$ and $\\mathbf{x}_{\\mathrm{0}}$ we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\log\\tilde{p}_{0}^{a u g}(\\mathbf{z}_{\\epsilon})=\\log\\tilde{p}_{0}(\\mathbf{x}_{0})+\\log q_{\\epsilon}(\\mathbf{y}_{\\epsilon})\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\tilde{p}_{0}$ is the learned density of $\\mathbf{x}_{\\mathrm{0}}$ corresponding to $\\pmb{\\theta}$ . Hence in total ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\log\\tilde{p}_{0}(\\mathbf{x}_{0})\\stackrel{(121)}{=}\\log\\tilde{p}_{T}^{a u g}(\\mathbf{z}_{T})+\\int_{0}^{T}\\nabla\\tilde{\\mathbf{f}}_{\\theta}(\\mathbf{z}_{t},t)\\mathrm{d}t-\\log q_{\\epsilon}(\\mathbf{y}_{\\epsilon})\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and we define the negative log-likelihoods $N L L s$ of test data $\\mathbf{x}_{\\mathrm{0}}$ under the learned density by ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{N L L s(\\mathbf{x}_{0},\\pmb{\\theta}):=-\\log\\tilde{p}_{0}^{a u g}(\\mathbf{z}_{0})+\\log q_{\\epsilon}(\\mathbf{y}_{\\epsilon}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "H Challenges in the attempt to generalize ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this work, we seek to determine the extent to which the continuous-time framework of score-based generative models can be generalized from an underlying BM to an underlying fBM. For a fBM $W^{H}$ it is not straightforward to define the forward process ", "page_idx": 31}, {"type": "equation", "text": "$$\nX_{t}=X_{0}+\\int_{0}^{t}f(X_{s},s)\\mathrm{d}s+\\int_{0}^{t}g(X_{s},s)\\mathrm{d}W_{s}^{H},\\quad t\\in[0,T]\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "driven by fBM, since fBM is neither a Markov process nor a semimartingale [25], and hence It\u00f4 calculus may not be applied, to define the second integral. However, a definition of the integral w.r.t. fBM is established [25, 73] such that the remaining problem is the derivation of the reversetime model. Following the second and more intuitive derivation of the reverse-time model for BM from Anderson [32], the conditional backward Kolmogorov equation and the unconditional forward Kolmogorov equation are applied. Starting point of the derivation is to rewrite $p(x_{t},t,x_{s},s)\\,=$ $p(x_{s},s|\\bar{x}_{t},t)p(\\bar{x}_{t},t)$ with Bayes theorem to calculate with the product rule ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{\\partial p(x_{t},t,x_{s},s)}{\\partial t}=\\frac{\\partial p(x_{s},s|x_{t},t)}{\\partial t}p(x_{t},t)+\\frac{\\partial p(x_{t},t)}{\\partial t}p(x_{s},s|x_{t},t),\\quad s\\geq t.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Replacing $\\frac{\\partial p(x_{t},t)}{\\partial t}$ with the RHS of the unconditional forward Kolmogorov equation and $\\frac{\\partial p(x_{s},s|x_{t},t)}{\\partial t}$ with the RHS of the conditional backward Kolmogorov equation one derives an equation that only depends on the joint density $p(x_{t},t,x_{s},s)$ . Using Bayes theorem again leads to a conditional backward Kolmogorov equation for $p(x_{t},t|x_{s},s)$ that defines the dynamics of the reverse process by the one-to-one correspondence between the conditional backward Kolmogorov equation and the reverse-time SDE [32]. Following these steps for fBM, starting from eq. (127) and deploying the one-to-one correspondence of fBM and the evolution of its density [73], we could replace $\\textstyle\\frac{\\partial p(x_{t},t)}{\\partial t}$ in (127) by the RHS of ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "equation", "text": "$$\n{\\frac{\\partial p(x,t)}{\\partial t}}=\\sum_{i=1}^{d}f_{i}(t,x){\\frac{\\partial p(t,x)}{\\partial x_{i}}}+H t^{2H-1}\\sum_{i,j=1}^{d}g_{i j}(x,t){\\frac{\\partial^{2}p(t,x)}{\\partial x_{i}\\partial x_{j}}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The missing part is however an analogous to the conditional backward Kolmogorov equation to replace $\\frac{\\partial p(x_{s},s|x_{t},t)}{\\partial t}$ in eq. (127). The derivation of such an equation is to the best of our knowledge yet unsolved problem and hence the limiting factor in the generalization of continuous-time score-based generative models from an underlying BM to an underlying fBM. ", "page_idx": 32}, {"type": "text", "text": "I Notational conventions ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "[0, T] Time horizon with terminal time $T>0$   \nX = (Xt)t\u2208[0,T ] Stochastic forward process taking values in $\\mathbb{R}$   \nD \u2208N Data dimension   \nX Vector valued stochastic forward process $\\mathbf{X}=(\\mathbf{X}_{t})_{t\\in[0,T]}$ with $\\mathbf{X}_{t}=(X_{t,1},...,X_{t,D})$   \n$\\overline{{\\mathbf{X}}}$ Reverse time stochastic process with $\\overline{{\\mathbf{X}}}_{t}=\\mathbf{X}_{T-t}$   \nf Vector valued function f $\\mathbf{\\dot{\\rho}}:\\mathbb{R}^{D}\\times[0,T]\\rightarrow\\mathbb{R}^{D}$   \n$\\mu,g$ Functions $\\mu,g:[0,T]\\rightarrow\\mathbb{R}$   \nf Reverse time function with $\\overline{{\\mathbf{f}}}(\\mathbf{x},t)=\\mathbf{f}(\\mathbf{x},T-t)$   \n$\\bar{\\mu},\\bar{g}$ Reverse time functions with $\\bar{\\mu}(t)=\\mu(T-t)$ and $\\bar{g}(t)=g(T-t)$   \n$p_{0}$ Data distribution   \n$p_{t}$ Marginal density of (augmented) forward process at $t\\in[0,T]$   \n$B$ Brownian motion (BM)   \n$H$ Hurst index $H\\in(0,1)$   \n$W^{H}$ Type I fractional Brownian motion (fBM)   \nBH Type II fractional Brownian motion (fBM)   \nY \u03b3 = (Y t\u03b3 )t\u2208[0,T ] Ornstein\u2013Uhlenbeck (OU) process with speed of mean reversion $\\gamma\\in\\mathbb{R}$   \nK \u2208N Number of augmenting processes   \n\u03b31, ..., \u03b3K Geometrically spaced grid   \n\u03c91, ..., \u03c9K Approximation coefficients   \n\u03c9 Optimal approximation coefficients $\\pmb{\\omega}=(\\omega_{1}^{\\star},...,\\omega_{K}^{\\star})$   \n$\\bar{\\omega}$ Sum of optimal approximation coefficients   \n$\\hat{B}^{H}$ Markov-approximate fractional Brownian motion (MA-fBM)   \n$k$ $k\\in\\mathbb{N}$ with $1\\le k\\le K$   \n$Y^{k}$ OU processes $Y^{k}=Y^{\\gamma_{k}}$   \n$\\mathbf{Y}^{1},...,\\mathbf{Y}^{K}$ Augmenting processes with $\\mathbf{Y}^{k}=(Y^{k},...,Y^{k})$   \n$\\mathbf{F},\\mathbf{G}$ Vector valued functions $\\mathbf{F},\\mathbf{G}:[0,T]\\rightarrow\\mathbb{R}^{D\\cdot(K+1)}$   \n$\\overline{{\\mathbf{F}}},\\overline{{\\mathbf{G}}}$ Reverse time vector valued functions with $\\overline{{\\mathbf{F}}}(t)=\\mathbf{F}(T-t)$ and $\\overline{{\\mathbf{G}}}(t)=\\mathbf{G}(T-t)$   \nZ By $\\mathbf{Y}^{1},...,\\mathbf{Y}^{K}$ augmented forward process   \n$\\mathbf{Y}^{[K]}$ Stacked vector of augmenting processes   \n$q_{t}$ Marginal density of $\\mathbf{Y}^{[K]}$ at $t\\in[0,T]$   \n\u03b8 Weight vector of a neural network ", "page_idx": 33}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: We summarize the contribution of our work in the introduction and in the abstract. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 34}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: We include a section on limitations of our work where we discuss the limitations of our results. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We give a complete proof for our own theoretical results and refer to complete proofs for he theoretical results of others. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We give implementation details in the appendix revealing the used model architecture and training procedures. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We release our code upon publication. Together with the implementation details given in the paper our results can be reproduced. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: We describe the hyperparameters in our section on implementation details in the appednix. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [No] ", "page_idx": 36}, {"type": "text", "text": "Justification: Unfortunately we don not have the computational resources to run all experiments a sufficient number of times to provide statistical certainty. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 36}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We reveal the hardware specification we use and report the number of hours of training in our section on implementation details. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Our work fully confirms with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We include a broader impact statement at the end of our paper discussing potential misuse. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 37}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: We only feature experiments on small scale dataset up to size $3\\times32\\times32$ .   \nThe models trained in this work do not pose such risks. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We properly cite all research works we build on and use the code of others only according to its license. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 39}, {"type": "text", "text": "[Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: We release our code upon publication alongside a proper documentation under the MIT license. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: our work does not involve crowdsourcing nor research with human subjects. ", "page_idx": 39}, {"type": "text", "text": "16. Depending on the country in ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 39}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 40}]