[{"figure_path": "B9qg3wo75g/tables/tables_7_1.jpg", "caption": "Table 1: Effect of augmenting processes on conditional image generation on MNIST for FVE and FVP dynamics.", "description": "This table shows the impact of increasing the number of augmenting processes (K) on the quality of generated images, specifically focusing on two different dynamics, Fractional Variance Exploding (FVE) and Fractional Variance Preserving (FVP), using the MNIST dataset.  Metrics evaluated include FID (Frech\u00e9t Inception Distance) for image quality, NLLs Test for likelihood, and VSp (Pixel Vendi Score) for pixel diversity.  The results show how adding more processes can affect the balance between image quality and diversity at different Hurst indices (H).", "section": "Experiments"}, {"figure_path": "B9qg3wo75g/tables/tables_8_1.jpg", "caption": "Table 3: The class-wise image quality and class-wise distribution coverage of the super-diffusive regime FVP(H = 0.9, K = 2) compared to the purely Brownian VP dynamics.", "description": "This table presents a comparison of class-wise image quality and distribution coverage between the super-diffusive regime of the fractional diffusion model (FVP with H=0.9, K=2) and the standard Brownian VP dynamics.  The metrics used for evaluation are FID (Fr\u00e9chet Inception Distance), which measures image quality, and Recall, which quantifies the distribution coverage across different classes.  Lower FID values indicate better image quality, while higher Recall values indicate better class coverage.", "section": "Experiments"}, {"figure_path": "B9qg3wo75g/tables/tables_8_2.jpg", "caption": "Table 2: FID and pixel-wise diversity VS of GFDM compared to the original setting of purely Brownian driven VE and VP. In bold the scores that are better than both purely Brownian driven dynamics. The overall best scores within the experiment are boxed in, indicating that the highest scores on both datasets are achieved in the super-diffusive regime for H = 0.9.", "description": "This table compares the performance of the Generative Fractional Diffusion Models (GFDM) against the standard Variance Exploding (VE) and Variance Preserving (VP) models.  It shows the Fr\u00e9chet Inception Distance (FID), Inception Score (IS), and Pixel Vendi Score (VSp) for various settings of GFDM, defined by the Hurst exponent (H) and number of augmenting processes (K). The best results are highlighted, indicating the superior performance of GFDM, especially in the super-diffusive regime (H > 0.5).", "section": "Experiments"}, {"figure_path": "B9qg3wo75g/tables/tables_26_1.jpg", "caption": "Table 2: FID and pixel-wise diversity VS of GFDM compared to the original setting of purely Brownian driven VE and VP. In bold the scores that are better than both purely Brownian driven dynamics. The overall best scores within the experiment are boxed in, indicating that the highest scores on both datasets are achieved in the super-diffusive regime for H = 0.9.", "description": "This table compares the performance of the proposed Generative Fractional Diffusion Models (GFDM) against traditional Variance Exploding (VE) and Variance Preserving (VP) diffusion models.  It shows the Fr\u00e9chet Inception Distance (FID) and pixel-wise diversity (VSp) scores for both MNIST and CIFAR10 datasets.  The results are broken down by the Hurst index (H) and the number of augmenting processes (K), highlighting that GFDM generally achieves better performance (lower FID and higher VSp), particularly in the super-diffusive regime (H > 0.5).", "section": "Experiments"}, {"figure_path": "B9qg3wo75g/tables/tables_26_2.jpg", "caption": "Table 4: FID and pixel-wise diversity scores of GFDM compared to the original setting of purely Brownian driven dynamics VE and VP. In bold the scores that are better than both purely Brownian driven dynamics VE and VP. The overall best scores within the experiment are boxed in.", "description": "This table compares the Fr\u00e9chet Inception Distance (FID) and pixel-wise diversity (VSp) scores of Generative Fractional Diffusion Models (GFDM) against traditional Variance Exploding (VE) and Variance Preserving (VP) models using purely Brownian motion. It shows the FID and VSp for different values of the Hurst index (H) and the number of augmenting processes (K). The bold values indicate that the GFDM outperforms both VE and VP. The boxed values represent the overall best scores for each dataset.", "section": "Experiments"}, {"figure_path": "B9qg3wo75g/tables/tables_26_3.jpg", "caption": "Table 5: Quantitative results for FVE dynamics and varying Hurst index on CIFAR10 trained without EMA. In bold the scores that are better than both purely Brownian driven dynamics VE and VP. The overall best scores within the experiment are boxed in.", "description": "This table presents the results of experiments using Fractional Variance Exploding (FVE) dynamics on the CIFAR10 dataset without Exponential Moving Average (EMA) for training.  It compares the performance of the FVE model with different Hurst indices (H) and numbers of augmenting processes (K) against the original Variance Exploding (VE) and Variance Preserving (VP) models using purely Brownian motion.  The FID (Fr\u00e9chet Inception Distance) and VSp (pixel-wise diversity) are reported as metrics to evaluate image quality and diversity respectively. Bold values indicate performance superior to both VE and VP baselines.", "section": "Experiments"}, {"figure_path": "B9qg3wo75g/tables/tables_26_4.jpg", "caption": "Table 6: Averaged FID values for different NFEs of the super-diffusive regime compared to purely Brownian dynamics.", "description": "This table compares the FID values obtained using the purely Brownian driven dynamics (VE and VP) against those obtained using the MA-fBM driven dynamics (FVP) with H = 0.7 and H = 0.9, and K = 2.  The comparison is done for different numbers of function evaluations (NFEs) (250, 500, 750, and 1000). Lower FID indicates better image quality.", "section": "Experiments"}, {"figure_path": "B9qg3wo75g/tables/tables_29_1.jpg", "caption": "Table 7: Averaged time in seconds needed before training to calculate for a given K the optimal approximation coefficients using the approach of Daems et al. [27].", "description": "This table shows the average computation time required to calculate the optimal approximation coefficients (\u03c9\u2081, ..., \u03c9\u03ba) for different values of K (number of augmenting processes) before the training process begins.  The computation time is measured in seconds and obtained using a GPU Tesla V100 with 32 GB of RAM.  The Hurst index H was randomly sampled 1000 times from a uniform distribution between 0.1 and 0.9 for each K.", "section": "F Computational cost of augmenting processes"}, {"figure_path": "B9qg3wo75g/tables/tables_30_1.jpg", "caption": "Table 8: Average time in seconds for one training step with FVE dynamics on CIFAR10 with a batch size of 128, a conditional U-Net with 58.7mio parameters and EMA.", "description": "This table shows the average computation time for one training step using FVE dynamics on the CIFAR10 dataset.  The experiment uses a conditional U-Net with approximately 58.7 million parameters and exponential moving average (EMA) for training. The batch size is 128.  The table breaks down the average time for different Hurst indices (H) and numbers of augmenting processes (K).", "section": "D Additional experiments"}, {"figure_path": "B9qg3wo75g/tables/tables_30_2.jpg", "caption": "Table 8: Average time in seconds for one training step with FVE dynamics on CIFAR10 with a batch size of 128, a conditional U-Net with 58.7mio parameters and EMA.", "description": "This table shows the average computation time for a single training step using the Fractional Variance Exploding (FVE) dynamics on the CIFAR10 dataset.  The experiment uses a conditional U-Net architecture with approximately 58.7 million parameters and Exponential Moving Average (EMA) for model training. The batch size was 128.  The results are broken down by Hurst index (H) values of 0.9, 0.5, and 0.1 and include the average across these values.  The values represent the average time across multiple training steps.", "section": "Implementation details"}, {"figure_path": "B9qg3wo75g/tables/tables_30_3.jpg", "caption": "Table 10: Average time in seconds for one sampling step in the reverse dynamics of FVE to generate data of dimension (3, 32, 32) with a batch size of 1000 using a conditional U-Net with 58.7mio parameters and EMA.", "description": "This table shows the average time it takes to perform one sampling step in the reverse dynamics of the Fractional Variance Exploding (FVE) model for different Hurst exponents (H) and numbers of augmenting processes (K).  The experiment uses a conditional U-Net with 58.7 million parameters and exponential moving average (EMA). The batch size is 1000. The data dimensionality is 3x32x32.", "section": "F Computational cost of augmenting processes"}, {"figure_path": "B9qg3wo75g/tables/tables_30_4.jpg", "caption": "Table 1: Effect of augmenting processes on conditional image generation on MNIST for FVE and FVP dynamics.", "description": "This table shows the impact of increasing the number of augmenting processes (K) on the quality and diversity of generated images from MNIST dataset. Two different dynamics, Fractional Variance Exploding (FVE) and Fractional Variance Preserving (FVP), are compared, and the results are evaluated using FID (Fr\u00e9chet Inception Distance), NLLs (negative log-likelihoods of test data), and VSp (pixel Vendi Score). For each combination of dynamics (FVE or FVP) and number of augmenting processes (K=1 to 5), the table shows the FID, NLLs, and VSp metrics.  Lower FID values indicate better image quality, lower NLLs values indicate better model fit, and higher VSp values indicate better pixel-wise diversity.", "section": "Experiments"}]