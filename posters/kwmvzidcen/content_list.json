[{"type": "text", "text": "Multi-Scale Representation Learning for Protein Fitness Prediction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zuobai Zhang1,2,\\* Pascal Notin3,\\* Yining Huang3 Aur\u00e9lie Lozano5 Vijil Chenthamarakshan5 Debora Marks3,4 Payel Das5,\u2020 Jian Tang1,6,7,\u2020 ", "page_idx": 0}, {"type": "text", "text": "equal contribution \u2020corresponding author 1Mila - Qu\u00e9bec AI Institute, 2Universit\u00e9 de Montr\u00e9al, 3Harvard Medical School, 4Broad Institute, 5IBM Research, 6HEC Montr\u00e9al, 7CIFAR AI Chair zuobai.zhang@mila.quebec, pascal_notin@hms.harvard.edu, daspa@us.ibm.com, jian.tang@hec.ca ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Designing novel functional proteins crucially depends on accurately modeling their fitness landscape. Given the limited availability of functional annotations from wet-lab experiments, previous methods have primarily relied on self-supervised models trained on vast, unlabeled protein sequence or structure datasets. While initial protein representation learning studies solely focused on either sequence or structural features, recent hybrid architectures have sought to merge these modalities to harness their respective strengths. However, these sequence-structure models have so far achieved only incremental improvements when compared to the leading sequence-only approaches, highlighting unresolved challenges effectively leveraging these modalities together. Moreover, the function of certain proteins is highly dependent on the granular aspects of their surface topology, which have been overlooked by prior models. To address these limitations, we introduce the Sequence-Structure-Surface Fitness (S3F) model \u2014 a novel multimodal representation learning framework that integrates protein features across several scales. Our approach combines sequence representations from a protein language model with Geometric Vector Perceptron networks encoding protein backbone and detailed surface topology. The proposed method achieves state-of-the-art fitness prediction on the ProteinGym benchmark encompassing 217 substitution deep mutational scanning assays, and provides insights into the determinants of protein function. Our code is at https://github.com/DeepGraphLearning/S3F. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Proteins carry out a diverse range of functions in nature \u2013 from catalyzing chemical reactions to supporting cellular structures, transporting molecules or transmitting signals. These functions are uniquely determined by their amino acid sequences and three-dimensional structures. The ability to design these sequences and structures presents significant opportunities to tackle critical challenges in sustainability, new material, and healthcare. This optimization process typically begins by learning the relationship between protein sequences or structures and their function, referred to as a fitness landscape. This multivariate function describes how mutations impact protein fitness \u2013 the more accurately we model these landscapes, the better we can engineer proteins with desired traits [1, 2]. ", "page_idx": 0}, {"type": "text", "text": "A significant challenge in modeling the fitness landscape is the scarcity of experimentally collected functional labels relative to the vastness of protein space [3]. As a result, self-supervised approaches to protein representation learning have become crucial for predicting mutation effect [4, 5]. While initial methods focused on learning a family-specific distribution over homologous protein sequences retrieved with a Multiple Sequence Alignment [4, 6\u20139], subsequent methods have sought to learn general functional patterns across protein families, giving rise to \u2018protein language models\u2019 or \u2018familyagnostic models\u2019 [10\u201312]. Recently, hybrid models have achieved state-of-the-art fitness prediction performance by leveraging the relative strengths of both types of approaches [13\u201315]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Although sequence-based methods are effective in recapitulating certain aspects of protein structure [16, 13], several protein functions and tasks crucially benefit from using a more granular representation of the protein structures and surfaces [17, 18]. To bridge this gap, recent studies have exploited advances in protein structure representation learning [19\u201321]. For instance, inverse folding models that learn a distribution over protein sequences conditioned on a protein backbone have shown enhanced performance in stability prediction [21\u201325]. Recent efforts have also focused on integrating sequence-based and structure-based approaches [26, 27]. A prominent example, AlphaMissense, employs structural prediction losses to distill structural information into a hybrid model, highlighting the value from structural features [28]. However, these hybrid sequence-structure methods have thus far only achieved modest improvements over leading sequence-based models, or have not made their model weights publicly available. Furthermore, current methodologies fall short in effectively modeling protein surfaces, which are essential for deciphering protein interactions and capturing broader structural details [29]. ", "page_idx": 1}, {"type": "text", "text": "In this work, we introduce a multi-scale protein representation learning framework that integrates comprehensive levels of protein information for zero-shot protein fitness prediction (Fig. 1). We begin with a Sequence-Structure Fitness Model (S2F) by combining a protein language model with a structure-based encoder. S2F utilizes the output of the protein language model as node features for a structure encoder, specifically a Geometric Vector Perceptron (GVP) [19], which enables message passing among spatially close neighborhoods. Building on this, we develop a SequenceStructure-Surface Fitness Model (S3F), which enhances S2F by adding a protein surface encoder that represents surfaces as point clouds and facilitates message passing between neighboring points. These multi-scale protein encoders are pre-trained using a residue type prediction loss on the CATH dataset [30], enabling zero-shot prediction of mutation effects. ", "page_idx": 1}, {"type": "text", "text": "Our methods are rigorously evaluated using the comprehensive ProteinGym benchmark [31], which includes 217 substitution deep mutational scanning assays and over 2.4 million mutated sequences across more than 200 diverse protein families. Our experimental results show that S2F achieves competitive results with prior methods, while S3F reaches state-of-the-art performance after incorporating surface features. When further augmented with alignment information, our method improves the current state-of-the-art by $8.5\\%$ in terms of Spearman\u2019s rank correlation. Additionally, our methods have substantially fewer trainable parameters compared to other baselines, reducing pre-training time to several days on commodity hardware. Being both lightweight and agnostic of the model used to obtain initial node embeddings, they can be readily adapted to augment forthcoming, more advanced protein language models. To better understand the impact of multi-scale representation learning, we perform a breakdown analysis on different types of assays. Our results demonstrate the consistent improvements from multi-scale learning and show that incorporating structure and surface features can potentially correct biases in sequence-based methods, enhance accuracy in structure-related functions, and improve the ability to capture epistatic effects. We summarize our contributions as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We develop a general and modular framework to learn multi-scale protein representations $(\\S\\ 3)$ ; \u2022 We introduce two instances of this framework $-\\,S2F\\,(\\S\\;3.3)$ and S3F (\u00a7 3.4), augmenting protein language model embeddings with structure and surface features for superior fitness prediction; \u2022 We thoroughly evaluate our methods on the 217 assays from the ProteinGym benchmark, demonstrating their state-of-the-art performance and fast pre-training efficiency $(\\S\\,4.2)$ ; \u2022 We perform a breakdown analysis for different types of assays to deep dive into the determinants of functions enabled by our multi-scale representation $(\\S\\,4.3)$ . ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Protein Representation Learning. Previous research in protein representation learning has explored diverse modalities including sequences, multiple sequence alignments (MSAs), structures, and surfaces [32, 13, 20, 33]. Sequence-based methods treat protein sequences as a fundamental biological language, employing large-scale pre-training on billions of sequences to capture complex biological functions and evolutionary signals [34, 35, 11]. Alignment-based approaches, such as MSA Transformer [13], enhance representations by incorporating MSAs, improving the capture of evolutionary relationships. Recent advancements in structure prediction tools have shifted focus toward explicitly using protein structures for representation learning [36, 20, 19, 37]. These methods employ diverse self-supervised learning algorithms like contrastive learning, self-prediction, denoising, and masked structure token prediction to train structure encoders [20, 38, 39, 26]. Additionally, extracting features from protein surfaces has shown promise in uncovering critical chemical and geometric patterns important for biomolecular interactions [29, 40, 41]. The growing availability of diverse protein data has also spurred the development of hybrid methods that combine multiple modalities for a holistic view [42, 43, 33, 44, 45]. Despite these advances, direct application of these models to zero-shot protein ftiness prediction is still challenging and requires further design. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Protein Fitness Prediction. Learning a fitness landscape has traditionally been approached as a discriminative supervised learning task, where models are trained to predict specific targets using labeled datasets [46\u201349]. Recently, unsupervised fitness predictors has shown promise in surpassing these traditional methods by overcoming the limitations and biases associated with sparse labels. These unsupervised models, often designed as protein language models, are trained on vast evolutionary datasets comprising millions of protein sequences, aiming to capture a general distribution across all proteins [50, 10, 14, 11, 51]. In contrast, alignment-based models focus on specific protein families, learning from multiple sequence alignments (MSAs) to capture nuanced distribution patterns [4, 6, 8, 7, 9]. Additionally, hybrid sequence models integrate broad protein information with detailed family-specific data from alignments, enhancing the robustness of loglikelihood estimations and achieving top-tier performance [13, 15, 52]. ", "page_idx": 2}, {"type": "text", "text": "The incorporation of structural information into protein fitness prediction has marked a promising direction in the field, inspired by recent advances in structure representation learning. Based on the concept of protein language models, SaProt utilizes structure tokens from Foldseek [53] to generalize sequence-based methods to structural data [26]. However, these structure-based methods only achieve limited improvement over the best sequence models. The latest method, AlphaMissense, integrates structural prediction losses into a hybrid model, thereby enhancing predictive accuracy [28]. Nevertheless, its high performance relies heavily on fine-tuning with weak supervision on human missense variants and it still lacks public accessibility to its model weights. Moreover, to the best of our knowledge, no existing work has employed surface-based methods for fitness prediction. To fill this gap, this work aims to integrate multi-scale protein information for fitness prediction. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Proteins. Proteins are macromolecules that form through the linkage of residues via peptide bonds. The three-dimensional (3D) structures of proteins are determined by the specific sequence of these residues. A protein with $n_{r}$ residues (amino acids) and $n_{a}$ atoms can be represented as a sequencestructure tuple $(S,X)$ . The sequence is denoted by $S=[s_{1},s_{2},\\cdot\\cdot\\cdot\\,,s_{n_{r}}]$ , where $s_{i}\\in\\{1,...,20\\}$ represents the type of the $i$ -th residue. The structure is represented by $X=[\\pmb{x}_{1},\\pmb{x}_{2}...,\\pmb{x}_{n_{a}}]\\in\\mathbb{R}^{n_{a}\\times3}$ , with $\\pmb{x}_{i}$ specifying the Cartesian coordinates of the $i^{\\th}$ -th atom. For simplification, we only consider the alpha carbon atoms and ignore the side-chain variations induced by mutations. ", "page_idx": 2}, {"type": "text", "text": "Protein Fitness Landscape. The ability of a protein to perform a specific function, often referred to as protein fitness, is encoded by its sequence via spontaneous folding into structures. The effects of sequence mutations on protein function form a fitness landscape, which can be quantitatively measured through deep mutational scanning (DMS) experiments [54]. Modeling these landscapes is challenging due to the complicated relationship between sequences, structures and functions. ", "page_idx": 2}, {"type": "text", "text": "Problem Definition. Unsupervised models that predict mutational effects are becoming fundamental tools in drug discovery, addressing the challenge of data scarcity. In this paper, we explore the task of zero-shot prediction of mutational effects using structural information. For simplicity, we focus solely on substitutions, leaving the study of insertions and deletions (indels) to future work. ", "page_idx": 2}, {"type": "text", "text": "Formally, for each DMS assay, we start with a wild-type protein $(\\mathbf{S}^{\\mathrm{wt}},\\mathbf{X}^{\\mathrm{wt}})$ and generate a set of mutants by selecting specific mutation sites and randomly replacing the original residue type with a new one. For a mutant $(\\mathbf{S}^{\\mathrm{mt}},\\mathbf{X}^{\\mathrm{mt}})$ with multiple mutations $T$ , the sequence changes such that $s_{t}^{\\mathrm{mt}}\\neq s_{t}^{\\mathrm{wt}}$ if $t\\in T$ ; otherwise, $s_{t}^{\\mathrm{mt}}=s_{t}^{\\mathrm{wt}}$ . We assume that the backbone structures remain unchanged post-mutation $\\mathbf{X}^{\\mathrm{mt}}=\\mathbf{X}^{\\mathrm{wt}}$ ). The objective is to develop an unsupervised model that can predict a score for each mutant to quantify the changes in fitness values relative to the wild-type. ", "page_idx": 2}, {"type": "image", "img_path": "kWMVzIdCEn/tmp/63b1bf5589692a25e6541cabfb3651e82e021010feddfdcba587cee3007f18f4.jpg", "img_caption": ["Figure 1: Multi-scale Pre-training and Inference Frameworks for Protein Fitness Prediction. During pre-training, protein sequences and structures are sampled from a database, with $15\\%$ of residue types randomly masked. These sequences are fed into a protein language model, ESM-2- 650M. Then, the output residue representations are used to initialize node features in our structure and surface encoders. Through message passing on structure and surface graphs, our methods, S2F (blue) and S3F (green), accurately predict the residue type distribution at each masked position. This distribution is subsequently used for mutation preferences in downstream fitness prediction tasks. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3.2 Protein Language Models for Mutational Effect Prediction ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Protein language models trained using the masked language modeling objective are designed to predict the likelihood of a residue\u2019s occurrence at a specific position within a protein, based on the surrounding context [11, 32]. As demonstrated in [5], this method can score sequence variations using the log odds ratio between mutant and wild-type proteins for given mutations $T$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{t\\in T}\\log p(s_{t}=s_{t}^{m t}|S_{\\setminus T})-\\log p(s_{t}=s_{t}^{w t}|S_{\\setminus T}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $S_{\\setminus T}$ denotes the input sequence masked at each mutated position in $T$ and an additive model is assumed over multiple mutation sites. In the zero-shot setting, inference is performed directly on the sequence under evaluation using only forward passes of the model. ", "page_idx": 3}, {"type": "text", "text": "3.3 Sequence-Structure Model for Fitness Prediction (S2F) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Although these protein language models are effective in predicting mutational effects, they do not incorporate explicit structural information during pre-training, which is crucial for determining protein functions. Building on the ESM-2-650M model, we next propose Sequence-Structure Fitness Model (S2F), which integrates structural data into the sequence-based predictive framework. ", "page_idx": 3}, {"type": "text", "text": "A major challenge in applying structure-based methods to fitness prediction is modeling how mutations impact protein structures. To bypass this problem, we adopt a simplified assumption: the backbone structures of proteins remain unchanged post-mutation. Additionally, we choose to omit side-chain information, which reveals residue types and could potentially lead to information leakage. ", "page_idx": 3}, {"type": "text", "text": "Geometric Message Passing. We build a radius graph for each protein with nodes representing alpha carbons. Two nodes are connected if their Euclidean distance is less than $10\\mathring\\mathrm{A}$ . We use Geometric Vector Perceptrons (GVP) [19] to perform message passing across the graph. GVPs replace standard Multi-Layer Perceptrons (MLPs) in Graph Neural Networks, operating on scalar and geometric features that transform as vectors under spatial coordinate rotations. ", "page_idx": 3}, {"type": "text", "text": "We represent the hidden state of residue $i$ at the $l$ -th layer by $h_{i}^{(l)}\\in\\mathbb{R}^{d}\\times\\mathbb{R}^{d^{\\prime}\\times3}$ with $d$ -dim scalar features and $d^{\\prime}$ -dim vector features. The initial node feature of residue $i$ is set using the protein language model embeddings, specifically, $\\pmb{h}_{i}^{(0)}=(\\mathrm{ESM}(s_{i}|\\pmb{S}_{\\backslash T}),\\mathbf{0})$ . The edge features are given by $\\pmb{e}_{(j,i)}=(\\mathrm{rbf}(\\pmb{x}_{j}-\\pmb{x}_{i}),\\pmb{x}_{j}-\\pmb{x}_{i})$ , where $\\operatorname{rbf}(\\cdot)$ computes pairwise distance features with Radial Basis Function (RBF) kernels [55]. In the network, node and edge features are concatenated to facilitate message passing via the GVP module on the (scalar, vector) representations. Each message passing layer is followed by a feed-forward network. Formally, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{h}_{i}^{(l+0.5)}=\\pmb{h}_{i}^{(l)}+\\frac{1}{\\left|\\mathcal{N}(i)\\right|}\\sum_{j\\in\\mathcal{N}(i)}\\mathbf{G}\\mathbf{V}\\mathbf{P}\\left(\\pmb{h}_{j}^{(l)},\\pmb{e}_{(j,i)}\\right),}\\\\ &{\\pmb{h}_{i}^{(l+1)}=\\pmb{h}_{i}^{(l+0.5)}+\\mathbf{G}\\mathbf{V}\\mathbf{P}\\left(\\pmb{h}_{i}^{(l+0.5)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{N}(i)$ represents the set of neighbors of node $i$ . The GVP module ensures SE(3)-invariance for scalar features and SE(3)-equivariance for vector features. The scalar features at the last layer $h_{i}^{(L)}$ of each node $i$ are fed into a separate linear layer for predicting the residue type. Practically, we utilize $L=5$ layers of GVP, with $d^{\\bar{\\prime}}=16$ vector and $d=100$ scalar hidden representations. ", "page_idx": 4}, {"type": "text", "text": "3.4 Sequence-Structure-Surface Model for Fitness Prediction (S3F) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Besides protein sequences and structures, protein surfaces\u2014defined by neighboring amino acids\u2014are characterized by distinct patterns of geometric and chemical properties. For instance, within a folded protein, hydrophobic residues tend to cluster inside the core, while hydrophilic residues are exposed to water solvent on its surface. These patterns provide crucial insights into protein function and potential molecular interactions. Now we integrate this aspect into our S2F model to propose a new model called Sequence-Structure-Surface Fitness Model (S3F). ", "page_idx": 4}, {"type": "text", "text": "Surface Processing. We employ dMaSIF to generate the surface based on the backbone structure of each protein [40]. The surface is represented as a point cloud $\\{\\tilde{x}_{1},\\tilde{x}_{2},...,\\tilde{x}_{n_{s}}\\}\\in\\mathbb{R}^{3}$ , consisting of $n_{s}$ (6K-20K) points. These points are sampled based on the levels of a smooth distance function defined over each atom. Each surface point $i$ is associated with geometric features $\\Tilde{f_{i}}$ , specifically Gaussian curvatures [56] and Heat Kernel Signatures [57], providing a detailed characterization of the surface topology. In the sequel, we will use the notations\u02dc\u00b7 with tilde for surface points to distinguish with those for nodes in structure graphs. ", "page_idx": 4}, {"type": "text", "text": "Surface Feature Initialization. After generating protein surfaces, we build a mapping between the structure and surface graphs, projecting residue features onto the surface points. For each surface point $i$ , we identify its 3 nearest neighboring residues, denoted as $\\mathcal{N}_{\\mathrm{surf-res}}(i)$ , each initialized with its ESM feature. These features are concatenated with their Euclidean distances to point $i$ and processed through an MLP. The average features of these neighbors are then combined with the geometric features $\\pmb{f}_{i}$ . Formally, the scalar feature for surface node $i$ is initialized as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\pmb{h}}_{i}^{(0)}=\\mathrm{MLP}\\left(\\pmb{f}_{i},\\frac{1}{3}\\sum_{j\\in\\mathcal{N}_{\\mathrm{surfres}(i)}}\\mathrm{MLP}(\\pmb{h}_{j}^{(0)},\\lVert\\tilde{\\pmb{x}}_{i}-\\pmb{x}_{j}\\rVert_{2})\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with the vector feature is initialized as the zero vector 0. ", "page_idx": 4}, {"type": "text", "text": "Surface Message Passing. To perform message passing on protein surfaces, we construct a surface graph with a $\\boldsymbol{\\mathrm{k}}$ -nearest neighbor graph, wherein each surface point is linked to its 16 nearest neighbors on the surface. For each edge $(j,i)$ in this graph, the edge feature $\\tilde{e}(j,i)$ is initialized using RBF kernels and represented as $\\tilde{e}(j,i)=(\\mathrm{rbf}(\\tilde{x}_{j}-\\tilde{x}_{i}),\\tilde{x}_{j}-\\tilde{x}_{i})$ . ", "page_idx": 4}, {"type": "text", "text": "Similar to S2F, we employ GVP to perform message passing on the scalar and vector representations: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\boldsymbol{h}}_{i}^{(l+0.5)}=\\tilde{\\boldsymbol{h}}_{i}^{(l)}+\\frac{1}{|\\mathcal{N}_{\\mathrm{surf}}(i)|}\\sum_{j\\in\\mathcal{N}_{\\mathrm{surf}}(i)}\\mathbf{G}\\mathbf{V}\\mathbf{P}\\left(\\tilde{\\boldsymbol{h}}_{j}^{(l)},\\tilde{\\boldsymbol{e}}_{(j,i)}\\right),}\\\\ &{\\tilde{\\boldsymbol{h}}_{i}^{(l+1)}=\\tilde{\\boldsymbol{h}}_{i}^{(l+0.5)}+\\mathbf{G}\\mathbf{V}\\mathbf{P}\\left(\\tilde{\\boldsymbol{h}}_{i}^{(l+0.5)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{N}_{\\mathrm{surf}}(i)$ represents the neighbors of surface node $i$ . Similar with the approach on structure graphs, we utilize another five layers of GVP, with 16 vector and 100 scalar hidden representations. ", "page_idx": 4}, {"type": "text", "text": "Residue Representation Aggregation. After performing message passing on both the structure and surface graphs, we combine the structure representations, $\\pmb{h}^{(L)}$ , with the surface representations, $\\tilde{h}^{(L)}$ . ", "page_idx": 4}, {"type": "text", "text": "For each residue $i$ , we identify the 20 nearest surface points, denoted as $\\mathcal{N}_{\\mathrm{res-surf}}(i)$ , and compute their mean representations to enhance the residue\u2019s representation. Here we use more neighbors for mapping the surface points back to residues, as there are typically much more surface points than residues. Specifically, we update each residue representation as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{h}_{i}^{(L)}\\leftarrow\\pmb{h}_{i}^{(L)}+\\frac{1}{20}\\sum_{j\\in\\mathcal{N}_{\\mathrm{res-surf}}(i)}\\tilde{\\pmb{h}}_{j}^{(L)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\leftarrow$ indicates that the left-hand side is updated with the value from the right-hand side. The updated representation $h_{i}^{(L)}$ is then input into a separate linear layer for predicting the residue type. ", "page_idx": 5}, {"type": "text", "text": "3.5 Pre-Training and Inference ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Following the pre-training methodology in [58], we select $15\\%$ of residues at random for prediction. If the $i$ -th residue is selected, we manipulate the $i$ -th token by replacing it with: (1) the [MASK] token $80\\%$ of the time, (2) a random residue type $10\\%$ of the time, and (3) leaving the i-th token unchanged $10\\%$ of the time. The final hidden state $\\pmb{h}^{(L)}$ is then used to predict the original residue type using cross-entropy loss. For S3F, we avoid information leakage from surfaces by removing the top 20 closest surface points for each selected residue. During pre-training, the weights of the ESM2-650M model are frozen, and only the GVP layers for structure and surface graphs are trainable. This strategy helps preserve sequence representations and improves pre-training efficiency. Our models are pre-trained on a non-redundant subset of CATH v4.3.0 dataset (CC BY 4.0 license) [30], which contains 30,948 experimental structures with less than $40\\%$ sequence identity. S2F and S3F are trained with batch sizes of 128 and 8, respectively, for 100 epochs on four A100 GPUs. The pre-training time for S2F and S3F are 9 hours and 58 hours, respectively. ", "page_idx": 5}, {"type": "text", "text": "During inference, we adopt a strategy similar to that used in ESM-1v [5]. For each mutation, we mask the residue type at all mutation sites and remove the corresponding surface points, similar as the pre-training process. We use AlphaFold2 to predict the wild-type structures. To mitigate the influence of low-quality structures, for mutations on residues with a predicted Local Distance Difference Test (pLDDT) score below 70, we use the output scores from the baseline model, ESM-2-650M. For residues with pLDDT score no less than 70, we use the outputs from our S2F or S3F models. ", "page_idx": 5}, {"type": "text", "text": "4 Experiment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Setup", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Evaluation Dataset. To assess the performance of our method on zero-shot protein ftiness prediction, we run experiments on the ProteinGym benchmark (MIT License) [31]. This benchmark contains a comprehensive collection of Deep Mutational Scanning (DMS) assays, which covers a variety of functional properties such as thermostability, ligand binding, viral replication, and drug resistance. Specifically, we use 217 substitution assays that include both single and multiple mutations. ", "page_idx": 5}, {"type": "text", "text": "Metric. Given the complex, non-linear relationship between protein function and organism ftiness [59], we select the Spearman\u2019s rank correlation coefficient as a primary metric for evaluating model prediction against experimental measurements. Besides, to ensure a comprehensive assessment, we include other metrics from the official ProteinGym benchmark: the Area Under the ROC Curve (AUC) and the Matthews Correlation Coefficient (MCC), which compare model scores with binarized experimental data. We also report the Normalized Discounted Cumulative Gains (NDCG) and Top $10\\%$ Recall with the aim to identify the most functionally effective proteins. ", "page_idx": 5}, {"type": "text", "text": "Baseline. We select a subset of baselines from the ProteinGym benchmark for comparison, categorizing them based on whether the model requires multiple sequence alignments (MSA) as input. In the category without MSAs, we include three protein language models\u2014ProGen2 XL [50], CARP640M [51], and ESM-2-650M [32]; three inverse folding models\u2014ProteinMPNN [22], MIF [23], and ESM-IF [21]; and three sequence-structure hybrid models\u2014MIF-ST [23], ProtSSN [27], and SaProt [26]. For models utilizing alignments, we choose three family-specific models\u2014DeepSequence [6], EVE [7], and GEMME [8], as well as three hybrid models that combine family-agnostic and specific approaches, including MSA Transformer [13], Tranception L with retrieval [14], and TranceptEVE [15]. All baseline results are taken directly from the ProteinGym benchmark [31]. ", "page_idx": 5}, {"type": "table", "img_path": "kWMVzIdCEn/tmp/f61d47ae4ec2773f6dd6c4c83412fd60f44f456ce6d545910a9bff8317678fbf.jpg", "table_caption": ["Table 1: Overall Results on ProteinGym. Models are categorized into two groups based on their reliance on MSA inputs. The types of input information and the number of trainable parameters are listed for each model. The best models within each category are highlighted in red. "], "table_footnote": ["Since the numbers of trainable parameters for family-specific models depend on the length of protein sequences, we use the average length (400 AAs) of all ProteinGym sequences for estimation. "], "page_idx": 6}, {"type": "text", "text": "We report the zero-shot performance of our methods, S2F and S3F. To benchmark against alignmentbased models, we further enhance our models by ensembling them with EVE predictions through the summation of their z-scores, resulting in two variants named S2F-MSA and S3F-MSA. ", "page_idx": 6}, {"type": "text", "text": "4.2 Benchmark Result ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We report the benchmark results and model information for all baselines in Table 1. Among the methods that do not require MSAs, S2F achieves competitive results, surpassing protein language models and inverse folding models, and only slightly lagging behind SaProt. When augmented with surface features, S3F becomes the best model in this category, even outperforming the top alignmentbased model, TranceptEVE, by a significant margin in terms of Spearman\u2019s rank correlation. Despite relying on protein language models, our methods have substantially fewer trainable parameters compared to other baselines (20M in S3F v.s. 650M in SaProt). Hence, our methods finish pretraining within several days, whereas some large-scale baselines require months. Additionally, when enhanced with alignment information, S3F-MSA improve the current state-of-the-art method, SaProt, by $8.5\\%$ in terms of Spearman\u2019s rank correlation, further demonstrating the potential of our approach. These results highlight both the effectiveness and the parameter efficiency of our proposed methods. ", "page_idx": 6}, {"type": "text", "text": "4.3 Breakdown Analysis for Multi-Scale Learning ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "From Table 1, we observe the progressive improvements achieved by gradually incorporating various protein aspects into the model, as demonstrated by comparing the results of ESM2, S2F, S3F, and S3F-MSA. To better understand the impact of this multi-scale representation learning, we conduct a breakdown analysis on different types of assays. In Fig. 2(a-d), we report the performance of these four methods on assays grouped by function types, MSA depths, taxon, and mutation depths. Our analysis reveals the following key points: ", "page_idx": 6}, {"type": "text", "text": "1. Overall: Consistent improvements are observed across all types of assays in different categories when structure, surface, and alignment information are added to the model. ", "page_idx": 6}, {"type": "text", "text": "2. Function type: Introducing structure and surface features significantly enhances performance on binding and stability assays. This aligns with our intuition that binding and stability are closely related to structural features, suggesting that structure-based methods have an advantage in identifying structure-disrupting mutations. This observation is consistent with previous studies [24], while our methods also maintain competitive performance across other function types. ", "page_idx": 6}, {"type": "image", "img_path": "kWMVzIdCEn/tmp/9512b0983fa36a8ab5a98439ebdd03bdc560e0c0b9044deb9b4ec19179be2af2.jpg", "img_caption": ["Figure 2: Results of ESM-2-650M, S2F, S3F, and S3F-MSA for Analyzing Contributions of Sequences, Structures, Surfaces, and Alignments. (a-d) Breakdown performance (Spearman\u2019s rank correlation) on assays grouped by function type (a), MSA depth ${\\bf(b)}$ , taxon (c), and mutation depth (d). (e-f) Impact of protein structure quality on performance. (e) Breakdown performance on assays with low, medium, and high-quality structures. (f) Results using five groups of AlphaFold2-predicted structures ranked by pLDDT (0 for the highest pLDDT, 4 for the lowest pLDDT). (g) Results on all assays and out-of-distribution assays with low sequence similarity to the pre-training dataset. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "3. MSA Depth: As shown in Fig. 2(b), protein language models perform poorly on assays with low MSA depths but excel on those with high MSA depths. This may be because proteins with low MSA depth are underrepresented in the ESM2 pre-training dataset, potentially reducing the diversity of specific families in protein language models. Introducing structure features partially mitigates this issue, and explicitly including family-specific training, like EVE, results in significant improvements. ", "page_idx": 7}, {"type": "text", "text": "4. Taxon: Leveraging structure and surface-based features consistently improves the ftiness prediction performance across taxa. Critically, when the underlying protein language model is poor for a given taxon (eg., ESM on viral proteins [31]), incorporating structure and surface components provides inductive biases that help overcome these limitations. Developing taxa-specific models with higher quality node representations would likely results in further performance gains. ", "page_idx": 7}, {"type": "text", "text": "5. Mutation Depth: Most methods perform better on single mutations than on multiple mutations, likely due to our simplified additive assumption between mutations. As mutation depth increases, the performance gains from structure and surface encoding become more pronounced. This suggests that structure- and surface-based models are better at capturing epistatic effects. ", "page_idx": 7}, {"type": "text", "text": "In conclusion, multi-scale representation learning consistently improves performance. Incorporating structure and surface features can potentially correct biases in sequence-based methods, enhance accuracy in structure-related functions, and improve the ability to capture epistatic effects. ", "page_idx": 7}, {"type": "text", "text": "4.4 Impact of Structure Quality ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To facilitate the usage of our methods on proteins without experimentally determined structures, we employ AlphaFold2 to generate structures for ProteinGym assays. By default, five structures are generated for each assay, with the best one selected based on pLDDT scores. Now we study how the quality of these predicted structures influences performance through two experiments. ", "page_idx": 7}, {"type": "image", "img_path": "kWMVzIdCEn/tmp/86a29aab1f537d88c8a02a6b6c971e7a3e38e49b6bf3a624c488b664a3c81616.jpg", "img_caption": ["Figure 3: Case Study on GB1. (a-c) For each pair of mutation sites, we plot the Spearman\u2019s rank correlation between the experimental values and model-predicted scores for all 361 mutations: ESM (a), S2F (b), and S3F (c). The epistasis between residues 234-252 and residues 266-282 (in the black rectangle) are better captured by S2F and S3F. (d) Visualization of the predicted structure for GB1. Mutation regions 234-252 and 266-282 are highlighted in red and blue, respectively. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "First, we categorize the 217 assays into three groups based on the pLDDT of their predicted structures: 95 assays with pLDDT over 90 are classified as high-quality, 18 assays with pLDDT less than 70 as low-quality, and the remaining 104 assays as medium-quality. We report the average results for these categories in Fig. 2(e). The results indicate that performance for all four baselines positively correlates with structure quality, even for the sequence-based method, ESM. This may suggest our limited prior knowledge about these proteins with low-quality structures for all models, including AlphaFold2 and ESM2. Additionally, while improvements from structure and surface features are observed across all assays, those with high-quality structures benefti more significantly, especially for S3F. This underscores the importance of accurate structures for fitness prediction. ", "page_idx": 8}, {"type": "text", "text": "Next, we analyze the impact of structure quality using the five AlphaFold2-predicted structures. We perform ftiness predictions five times, each time using a different group of structures ranked by their pLDDT scores. Specifically, we test S2F and S3F using the top 1, 2, 3, 4, and 5 predicted structures for all assays, respectively. The results, plotted in Fig. 2(f), show clear performance drops when lower-quality structures are used. This further highlights the reliance on high-quality structures. ", "page_idx": 8}, {"type": "text", "text": "4.5 Generalization Ability to Unseen Protein Families ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In contrast to large-scale models pre-trained on UniRef100 or the AlphaFold Database, our method employs a much smaller dataset, CATH, which contains only 31,000 structures after clustering. This raises the question of whether the benefits of structure- and surface-based methods can generalize well to unseen protein families, despite the limited scale of the pre-training data. To address this, we select 23 out-of-distribution assays from ProteinGym, whose sequences have less than $30\\%$ similarity to the pre-training dataset. We plot the average performance of the four methods on these assays in Fig. $2(\\mathrm{g})$ . Although the absolute performance of all four methods decreases compared to their overall performance, we still observe consistent improvements from the structure- and surface-based methods, as evidenced by the performance gains of S2F over ESM and S3F over S2F. This proves the generalization ability of our methods to unseen protein families. ", "page_idx": 8}, {"type": "text", "text": "4.6 Case Study: Epistatic effects in the IgG-binding domain of protein G ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We illustrate the advantages of our method by focusing on the results from the high-throughput assay of the IgG-binding domain of protein G (GB1) introduced in [60]. The assay quantifies the effects of all single and double mutations, thereby providing detailed insights into the epistatic relationships between residue pairs in the domain. We report in Fig. 3 the pair-specific Spearman\u2019s rank correlation between the experimental measurements and predictions from ESM, S2F and S3F, respectively. Leveraging structure and surface features leads to a superior ability in predicting mutation effects \u2013 in particular the epistatic effects between residue pairs that are far in sequence space but close in the tertiary structure of GB1 (off-diagonal terms in Fig. 3(a-c) and colored residues in Fig. 3(d)). ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work introduces S3F, a novel multi-scale representation learning framework for protein fitness prediction that achieves state-of-the-art performance on the ProteinGym benchmark with a lightweight model. The breakdown analysis provides insights into how different protein modalities contribute to predicting various fitness landscapes. However, there are still several limitations in this work. First, our models are pre-trained on relatively small set of experimentally determined protein structures and may further benefti from leveraging more diverse structure sets, such as the AlphaFold database. Second, we make simplifying assumptions, such as ignoring side-chain information and assuming that backbone structures remain unchanged after mutations, which may limit the model\u2019s capacity. Third, our approach is limited to substitutions effects, and does not handle insertions nor deletions. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors would like to thank Jiarui Lu and Chenqing Hua for their helpful discussions. ", "page_idx": 9}, {"type": "text", "text": "This project is supported by AIHN IBM-MILA partnership program, the Natural Sciences and Engineering Research Council (NSERC) Discovery Grant, the Canada CIFAR AI Chair Program, collaboration grants between Microsoft Research and Mila, Samsung Electronics Co., Ltd., Amazon Faculty Research Award, Tencent AI Lab Rhino-Bird Gift Fund, a NRC Collaborative R&D Project (AI4D-CORE-06) as well as the IVADO Fundamental Research Project grant PRF-2019-3583139727. ", "page_idx": 9}, {"type": "text", "text": "P.N. was supported by a Chan Zuckerberg Initiative Award (Neurodegeneration Challenge Network, CZI2018-191853). D.M. holds a Ben Barres Early Career Award from the Chan Zuckerberg Initiative as part of the Neurodegeneration Challenge Network (CZI2018-191853) and is supported by a NIH Transformational Research Award (TR01 1R01CA260415). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Philip A. Romero and Frances H. Arnold. Exploring protein fitness landscapes by directed evolution. Nature Reviews Molecular Cell Biology, 10:866\u2013876, 2009.   \n[2] Pascal Notin, Nathan J. Rollins, Yarin Gal, Chris Sander, and Debora Marks. Machine learning for functional protein design. Nature Biotechnology, 42:216\u2013228, 2024.   \n[3] Surojit Biswas, Grigory Khimulya, Ethan C Alley, Kevin M Esvelt, and George M Church. Low-n protein engineering with data-efficient deep learning. Nature methods, 18(4):389\u2013396, 2021.   \n[4] Thomas A Hopf, John B Ingraham, Frank J Poelwijk, Charlotta PI Sch\u00e4rfe, Michael Springer, Chris Sander, and Debora S Marks. Mutation effects predicted from sequence co-variation. Nature biotechnology, 35(2):128\u2013135, 2017.   \n[5] Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alex Rives. Language models enable zero-shot prediction of the effects of mutations on protein function. Advances in neural information processing systems, 34:29287\u201329303, 2021.   \n[6] Adam J Riesselman, John B Ingraham, and Debora S Marks. Deep generative models of genetic variation capture the effects of mutations. Nature methods, 15(10):816\u2013822, 2018.   \n[7] Jonathan Frazer, Pascal Notin, Mafalda Dias, Aidan Gomez, Joseph K Min, Kelly Brock, Yarin Gal, and Debora S Marks. Disease variant prediction with deep generative models of evolutionary data. Nature, 599(7883):91\u201395, 2021.   \n[8] Elodie Laine, Yasaman Karami, and Alessandra Carbone. Gemme: a simple and fast global epistatic model predicting mutational effects. Molecular biology and evolution, 36(11):2604\u2013 2619, 2019.   \n[9] Jung-Eun Shin, Adam J Riesselman, Aaron W Kollasch, Conor McMahon, Elana Simon, Chris Sander, Aashish Manglik, Andrew C Kruse, and Debora S Marks. Protein design and variant prediction using autoregressive generative models. Nature communications, 12(1):2403, 2021.   \n[10] Ethan C Alley, Grigory Khimulya, Surojit Biswas, Mohammed AlQuraishi, and George M Church. Unified rational protein engineering with sequence-based deep representation learning. Nature methods, 16(12):1315\u20131322, 2019.   \n[11] Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C Lawrence Zitnick, Jerry Ma, et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences, 118(15), 2021.   \n[12] Ali Madani, Bryan McCann, Nikhil Vijay Naik, Nitish Shirish Keskar, Namrata Anand, Raphael R. Eguchi, Po-Ssu Huang, and Richard Socher. Progen: Language modeling for protein generation. bioRxiv, 2020.   \n[13] Roshan M Rao, Jason Liu, Robert Verkuil, Joshua Meier, John Canny, Pieter Abbeel, Tom Sercu, and Alexander Rives. Msa transformer. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8844\u20138856. PMLR, 18\u201324 Jul 2021.   \n[14] Pascal Notin, Mafalda Dias, Jonathan Frazer, Javier Marchena Hurtado, Aidan N Gomez, Debora Marks, and Yarin Gal. Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval. In International Conference on Machine Learning, pages 16990\u201317017. PMLR, 2022.   \n[15] Pascal Notin, Lood Van Niekerk, Aaron W Kollasch, Daniel Ritter, Yarin Gal, and Debora S Marks. Trancepteve: Combining family-specific and family-agnostic models of protein sequences for improved fitness prediction. bioRxiv, pages 2022\u201312, 2022.   \n[16] Faruck Morcos, Andrea Pagnani, Bryan Lunt, Arianna Bertolino, Debora S. Marks, Chris Sander, Riccardo Zecchina, Jos\u00e9 Nelson Onuchic, Terence Hwa, and Martin Weigt. Directcoupling analysis of residue coevolution captures native contacts across many protein families. Proceedings of the National Academy of Sciences, 108:E1293 \u2013 E1301, 2011.   \n[17] John Ingraham, Vikas K. Garg, Regina Barzilay, and T. Jaakkola. Generative models for graph-based protein design. In DGS@ICLR, 2019.   \n[18] G Baumann, Cornelius Fr\u00f6mmel, and Chris Sander. Polarity as a criterion in protein design. Protein engineering, 2 5:329\u201334, 1989.   \n[19] Bowen Jing, Stephan Eismann, Pratham N. Soni, and Ron O. Dror. Learning from protein structure with geometric vector perceptrons. In International Conference on Learning Representations, 2021.   \n[20] Zuobai Zhang, Minghao Xu, Arian Jamasb, Vijil Chenthamarakshan, Aurelie Lozano, Payel Das, and Jian Tang. Protein representation learning by geometric structure pretraining. In The Eleventh International Conference on Learning Representations, 2023.   \n[21] Chloe Hsu, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu, Adam Lerer, and Alexander Rives. Learning inverse folding from millions of predicted structures. In International conference on machine learning, pages 8946\u20138970. PMLR, 2022.   \n[22] Justas Dauparas, Ivan Anishchenko, Nathaniel Bennett, Hua Bai, Robert J Ragotte, Lukas F Milles, Basile IM Wicky, Alexis Courbet, Rob J de Haas, Neville Bethel, et al. Robust deep learning\u2013based protein sequence design using proteinmpnn. Science, 378(6615):49\u201356, 2022.   \n[23] Kevin K Yang, Niccol\u00f2 Zanichelli, and Hugh Yeh. Masked inverse folding with sequence transfer for protein representation learning. Protein Engineering, Design and Selection, 36:gzad015, 2023.   \n[24] Steffanie Paul, Aaron Kollasch, Pascal Notin, and Debora Marks. Combining structure and sequence for superior ftiness prediction. In NeurIPS 2023 Generative AI and Biology (GenBio) Workshop, 2023.   \n[25] Matteo Cagiada, Sergey Ovchinnikov, and Kresten Lindorff-Larsen. Predicting absolute protein folding stability using generative models. bioRxiv, 2024.   \n[26] Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, and Fajie Yuan. Saprot: Protein language modeling with structure-aware vocabulary. In The Twelfth International Conference on Learning Representations, 2024.   \n[27] Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, and Liang Hong. Semantical and topological protein encoding toward enhanced bioactivity and thermostability. bioRxiv, pages 2023\u201312, 2023.   \n[28] Jun Cheng, Guido Novati, Joshua Pan, Clare Bycroft, Akvil\u02d9e \u017demgulyt\u02d9e, Taylor Applebaum, Alexander Pritzel, Lai Hong Wong, Michal Zielinski, Tobias Sargeant, et al. Accurate proteomewide missense variant effect prediction with alphamissense. Science, 381(6664):eadg7492, 2023.   \n[29] Pablo Gainza, Freyr Sverrisson, Frederico Monti, Emanuele Rodola, D Boscaini, Michael M Bronstein, and Bruno E Correia. Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning. Nature Methods, 17(2):184\u2013192, 2020.   \n[30] Natalie L Dawson, Tony E Lewis, Sayoni Das, Jonathan G Lees, David Lee, Paul Ashford, Christine A Orengo, and Ian Sillitoe. Cath: an expanded resource to predict protein function through structure and sequence. Nucleic acids research, 45(D1):D289\u2013D295, 2017.   \n[31] Pascal Notin, Aaron Kollasch, Daniel Ritter, Lood Van Niekerk, Steffanie Paul, Han Spinner, Nathan Rollins, Ada Shaw, Rose Orenbuch, Ruben Weitzman, et al. Proteingym: largescale benchmarks for protein fitness prediction and design. Advances in Neural Information Processing Systems, 36, 2023.   \n[32] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, et al. Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, 379(6637):1123\u20131130, 2023.   \n[33] Vignesh Ram Somnath, Charlotte Bunne, and Andreas Krause. Multi-scale representation learning on proteins. Advances in Neural Information Processing Systems, 34:25244\u201325255, 2021.   \n[34] Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Xi Chen, John Canny, Pieter Abbeel, and Yun S Song. Evaluating protein transfer learning with tape. In Advances in Neural Information Processing Systems, 2019.   \n[35] Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Wang Yu, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, Debsindhu Bhowmik, and Burkhard Rost. Prottrans: Towards cracking the language of lifes code through self-supervised deep learning and high performance computing. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 1\u20131, 2021.   \n[36] Vladimir Gligorijevi\u00b4c, P Douglas Renfrew, Tomasz Kosciolek, Julia Koehler Leman, Daniel Berenberg, Tommi Vatanen, Chris Chandler, Bryn C Taylor, Ian M Fisk, Hera Vlamakis, et al. Structure-based protein function prediction using graph convolutional networks. Nature communications, 12(1):1\u201314, 2021.   \n[37] Pedro Hermosilla, Marco Sch\u00e4fer, Mate\u02c7j Lang, Gloria Fackelmann, Pere Pau V\u00e1zquez, Barbora Kozl\u00edkov\u00e1, Michael Krone, Tobias Ritschel, and Timo Ropinski. Intrinsic-extrinsic convolution and pooling for learning on 3d protein structures. International Conference on Learning Representations, 2021.   \n[38] Can (Sam) Chen, Jingbo Zhou, Fan Wang, Xue Liu, and Dejing Dou. Structure-aware protein self-supervised learning. Bioinformatics, 39, 2022.   \n[39] Zuobai Zhang, Minghao Xu, Aurelie Lozano, Vijil Chenthamarakshan, Payel Das, and Jian Tang. Pre-training protein encoder via siamese sequence-structure diffusion trajectory prediction. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[40] Freyr Sverrisson, Jean Feydy, Bruno E Correia, and Michael M Bronstein. Fast end-to-end learning on protein surfaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15272\u201315281, 2021.   \n[41] Vincent Mallet, Souhaib Attaiki, and Maks Ovsjanikov. Atomsurf: Surface representation for learning on protein structures. arXiv preprint arXiv:2309.16519, 2023.   \n[42] Zichen Wang, Steven A Combs, Ryan Brand, Miguel Romero Calvo, Panpan Xu, George Price, Nataliya Golovach, Emmanuel O Salawu, Colby J Wise, Sri Priya Ponnapalli, et al. Lm-gvp: an extensible sequence and structure informed deep learning framework for protein property prediction. Scientific reports, 12(1):6832, 2022.   \n[43] Zuobai Zhang, Chuanrui Wang, Minghao Xu, Vijil Chenthamarakshan, Aurelie Lozano, Payel Das, and Jian Tang. A systematic study of joint representation learning on protein sequences and structures. arXiv preprint arXiv:2303.06275, 2023.   \n[44] Youhan Lee, Hasun Yu, Jaemyung Lee, and Jaehoon Kim. Pre-training sequence, structure, and surface features for comprehensive protein representation learning. In The Twelfth International Conference on Learning Representations, 2024.   \n[45] Fang Wu, Lirong Wu, Dragomir Radev, Jinbo Xu, and Stan Z Li. Integration of pre-trained protein language models into geometric deep learning networks. Communications Biology, 6(1):876, 2023.   \n[46] Kevin K Yang, Zachary Wu, and Frances H Arnold. Machine-learning-guided directed evolution for protein engineering. Nature methods, 16(8):687\u2013694, 2019.   \n[47] Sam Gelman, Sarah A Fahlberg, Pete Heinzelman, Philip A Romero, and Anthony Gitter. Neural networks to learn protein sequence\u2013function relationships from deep mutational scanning data. Proceedings of the National Academy of Sciences, 118(48):e2104878118, 2021.   \n[48] Christian Dallago, Jody Mou, Kadina E Johnston, Bruce J Wittmann, Nicholas Bhattacharya, Samuel Goldman, Ali Madani, and Kevin K Yang. Flip: Benchmark tasks in ftiness landscape inference for proteins. bioRxiv, pages 2021\u201311, 2021.   \n[49] Pascal Notin, Ruben Weitzman, Debora Marks, and Yarin Gal. Proteinnpt: improving protein property prediction and design with non-parametric transformers. Advances in Neural Information Processing Systems, 36, 2023.   \n[50] Erik Nijkamp, Jeffrey A Ruffolo, Eli N Weinstein, Nikhil Naik, and Ali Madani. Progen2: exploring the boundaries of protein language models. Cell systems, 14(11):968\u2013978, 2023.   \n[51] Kevin K Yang, Nicolo Fusi, and Alex X Lu. Convolutions are competitive with transformers for protein sequence pretraining. Cell Systems, 15(3):286\u2013294, 2024.   \n[52] Timothy Truong Jr and Tristan Bepler. Poet: A generative model of protein families as sequences-of-sequences. Advances in Neural Information Processing Systems, 36, 2023.   \n[53] Michel Van Kempen, Stephanie S Kim, Charlotte Tumescheit, Milot Mirdita, Jeongjae Lee, Cameron LM Gilchrist, Johannes S\u00f6ding, and Martin Steinegger. Fast and accurate protein structure search with foldseek. Nature Biotechnology, 42(2):243\u2013246, 2024.   \n[54] Douglas M Fowler and Stanley Fields. Deep mutational scanning: a new style of protein science. Nature methods, 11(8):801\u2013807, 2014.   \n[55] Carl Edward Rasmussen and Hannes Nickisch. Gaussian processes for machine learning (gpml) toolbox. The Journal of Machine Learning Research, 11:3011\u20133015, 2010.   \n[56] Yueqi Cao, Didong Li, Huafei Sun, Amir H Assadi, and Shiqiang Zhang. Efficient curvature estimation for oriented point clouds. stat, 1050:26, 2019.   \n[57] Jian Sun, Maks Ovsjanikov, and Leonidas Guibas. A concise and provably informative multiscale signature based on heat diffusion. In Computer graphics forum, volume 28, pages 1383\u20131392. Wiley Online Library, 2009.   \n[58] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \n[59] Jeffrey I Boucher, Daniel NA Bolon, and Dan S Tawfik. Quantifying and understanding the ftiness effects of protein mutations: Laboratory versus nature. Protein Science, 25(7):1219\u20131226, 2016.   \n[60] C. Anders Olson, Nicholas C. Wu, and Ren Sun. A comprehensive biophysical description of pairwise epistasis throughout an entire protein domain. Current Biology, 24(22):2643\u20132651, November 2014. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "table", "img_path": "kWMVzIdCEn/tmp/f6389221ea86b4371d33c76ab3b6c19fd5185fbdef05ab79a44419fee9787917.jpg", "table_caption": ["Table 2: Average Spearman and Std. Error of Difference to Best Score. The best performing model is S3F-MSA, highlighted in red. $\\Delta$ Spearman is the difference between the baseline models and the S3F-MSA. We followed the ProteinGym benchmarks to compute the std. error of difference to best score. In this table, it is the bootstrapped std. error of the differences between the Spearman of each model and the Spearman of S3F-MSA. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A Broader Impact ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The main objective of this project is to model protein fitness landscape more accurately with multiscale representation learning. Our approach utilizes structural information in the CATH dataset to build a multi-scale encoder with both sequence, structure and surface features. This advantage allows for more comprehensive analysis of protein research and holds potential benefits for various real-world applications, including protein engineering, sequence and structure design. It is important to acknowledge that powerful fitness prediction models can potentially be misused for harmful purposes, such as the design of dangerous drugs. We anticipate that future studies will address and mitigate these concerns. ", "page_idx": 14}, {"type": "image", "img_path": "kWMVzIdCEn/tmp/57e303b2f44e8e706f053051015ecdd77a1998434d5bd2b12ab84d73523223c6.jpg", "img_caption": ["Figure 4: Spearmanr\u2019s rank correlation for ESM-2-650M, S2F, S3F and S3F-MSA on 19 proteins with less than $30\\%$ sequence similarity to the pre-training dataset. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Additional Experimental Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Performance on OOD test sets ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Section 4.5, we select 23 out-of-distribution assays from ProteinGym by filtering with sequence similarity. Here we report the performance on these assays in Figure 4. The results on assays with the same proteins are grouped together. ", "page_idx": 15}, {"type": "text", "text": "B.2 Statistical Significance Analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To quantify the statistical significance of the performance, we follow the same methodology as in ProteinGym and compute the non-parametric bootstrap standard error of the difference between the Spearman performance of a given model and that of the best overall model (10k bootstrap samples). Our performance delta with prior methods are all statistically significant, as shown in Table 2. ", "page_idx": 15}, {"type": "text", "text": "B.3 Ablation Study ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We provide ablation study results in Table 3, confirming the performance lift from the various modalities involved. The ablation dropping both structure and surface features corresponds to ESM2 in Table 1. Note that surface message-passing is designed to capture fine-grained structural aspects that enhance the coarse-grained features learned by our S2F (sequence+structure) model. However, relying solely on these fine-grained features without the context from structural features, as we do in the ablation removing structural inputs, appears to be detrimental to performance. ", "page_idx": 15}, {"type": "table", "img_path": "kWMVzIdCEn/tmp/a745a95a0d9e0cba76b7230a790e31596c802500263ee7847b56d8e1c022b103.jpg", "table_caption": ["Table 3: Ablation Study for S3F. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The main claims about the experimental results of our methods are supported in Section 4. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The limitations of this paper are discussed in Section 5. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: This paper does not include theoretical results. ", "page_idx": 16}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] Justification: The implementation details are provided in Section 3. ", "page_idx": 16}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 16}, {"type": "text", "text": "Answer: [No] ", "page_idx": 16}, {"type": "text", "text": "Justification: The code, data and pre-trained model weights will be released upon acceptance. ", "page_idx": 16}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The evaluation setup are provided in Sections 3.5 and 4.1. ", "page_idx": 16}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 16}, {"type": "text", "text": "Answer: [No] ", "page_idx": 16}, {"type": "text", "text": "Justification: The statistical significance will be reported when the results are released on the ProteinGym benchmark. ", "page_idx": 16}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The information about compute resources is provided in Section 3.5. ", "page_idx": 16}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The research conform with the NeurIPS Code of Ethics. ", "page_idx": 17}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The broader impacts are discussed in Section A. ", "page_idx": 17}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] Justification: The paper poses no such risks. ", "page_idx": 17}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The license of used datasets are provided. ", "page_idx": 17}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not release new assets. ", "page_idx": 17}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 17}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 17}, {"type": "text", "text": "Answer: [No] Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 17}]