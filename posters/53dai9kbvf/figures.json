[{"figure_path": "53daI9kbvf/figures/figures_0_1.jpg", "caption": "Figure 1: By integrating reward feedback during consistency distillation from VideoCrafter2 [Chen et al., 2024], our T2V-Turbo (VC2) can generate high-quality videos with 4-8 inference steps, breaking the quality bottleneck of a VCM [Wang et al., 2023a]. Appendix F includes the corresponding text prompts.", "description": "This figure shows a comparison of video quality generated by a Video Consistency Model (VCM) and the proposed T2V-Turbo model.  The top row displays the results from a VCM using 4 and 8 inference steps, showcasing the quality limitations of this faster approach. In contrast, the bottom row demonstrates the improved video quality generated by T2V-Turbo using the same 4 and 8 inference steps.  The improved quality is attributed to the integration of reward feedback during training.  Appendix F of the paper provides further details and associated text prompts.", "section": "Abstract"}, {"figure_path": "53daI9kbvf/figures/figures_1_1.jpg", "caption": "Figure 2: Overview of the training pipeline of our T2V-Turbo. We integrate reward feedback from both an image-text RM and a video-text RM into the VCD procedures by backpropagating gradient through the single-step generation process of our T2V-Turbo.", "description": "The figure illustrates the training process of T2V-Turbo.  It shows how mixed reward feedback from both image-text and video-text reward models is integrated into the consistency distillation process. The gradient from reward maximization and the gradient from distillation are shown separately.  A single-step generation is used, and the overall process aims to improve the video generation quality by optimizing for both visual appeal and text alignment.", "section": "Training T2V-Turbo with Mixed Reward Feedback"}, {"figure_path": "53daI9kbvf/figures/figures_6_1.jpg", "caption": "Figure 3: Human evaluation results with the 700 prompts from EvalCrafter [Liu et al., 2023]. We compare the 4-step and 8-step generations from our T2V-Turbo with their teacher T2V model and their baseline VCM. Top: results for T2V-Turbo (VC2). Bottom: results for T2V-Turbo (MS).", "description": "This figure shows the results of human evaluation comparing the 4-step and 8-step generations of T2V-Turbo with their respective teacher models (50-step DDIM samples) and baseline VCMs (4-step).  The evaluation used 700 prompts from the EvalCrafter benchmark, assessing visual quality, text-video alignment, and overall preference.  The top half displays results for T2V-Turbo (VC2), and the bottom half shows results for T2V-Turbo (MS).  Each bar graph shows the percentage of human preferences for each model, broken down by the three evaluation metrics.", "section": "4.2 Human Evaluation with 700 EvalCrafter Prompts"}, {"figure_path": "53daI9kbvf/figures/figures_6_2.jpg", "caption": "Figure 4: Qualitative comparisons between the 4-step VCM, 50-step teacher T2V, 4-step T2V-Turbo and 8-step T2V-Turbo generations. Left: (VC2), Right: (MS).", "description": "This figure presents a qualitative comparison of video generations from four different models: a baseline Video Consistency Model (VCM) with 4 inference steps, the teacher T2V model with 50 inference steps, and the proposed T2V-Turbo model with both 4 and 8 inference steps.  The left column shows results from distilling the T2V-Turbo model from VideoCrafter2, while the right column displays results from distilling it from ModelScopeT2V. Each row shows a different video generation task, with each column demonstrating the differences in video quality between the four models. The image aims to visually showcase the improved video quality of the T2V-Turbo model over the baseline VCM and teacher model, especially within the fewer steps.", "section": "4.2 Human Evaluation with 700 EvalCrafter Prompts"}, {"figure_path": "53daI9kbvf/figures/figures_19_1.jpg", "caption": "Figure 2: Overview of the training pipeline of our T2V-Turbo. We integrate reward feedback from both an image-text RM and a video-text RM into the VCD procedures by backpropagating gradient through the single-step generation process of our T2V-Turbo.", "description": "This figure illustrates the training process of the T2V-Turbo model. It shows how reward feedback from both image-text and video-text reward models is integrated into the consistency distillation (CD) process.  Instead of backpropagating gradients through the entire iterative sampling process, T2V-Turbo directly optimizes rewards associated with single-step generations, thereby improving efficiency and video quality.", "section": "Training T2V-Turbo with Mixed Reward Feedback"}, {"figure_path": "53daI9kbvf/figures/figures_21_1.jpg", "caption": "Figure 6: Ablation study on the choice of the Rimg. We compare the 4-step generations from each methods. The three Rimg we tested can all improve the video generation quality compare to the baseline VCM (VC2).", "description": "This figure presents an ablation study on the impact of different image-text reward models (Rimg) on the quality of video generation.  Four sets of video frames are shown for the same prompt: a baseline VCM (VC2), and three versions of the VCM with added reward feedback from three different Rimg models: HPSv2.1, PickScore, and ImgRwd.  The results illustrate that integrating any of these reward models improves the generated video quality compared to the baseline VCM. The visual differences highlight how each Rimg model affects the visual details and consistency of the generated video frames.", "section": "3.1 Optimizing Human Preference on Individual Video Frames"}, {"figure_path": "53daI9kbvf/figures/figures_21_2.jpg", "caption": "Figure 6: Ablation study on the choice of the Rimg. We compare the 4-step generations from each methods. The three Rimg we tested can all improve the video generation quality compare to the baseline VCM (VC2).", "description": "This figure shows an ablation study comparing the performance of different image-text reward models (Rimg) when integrated into a video consistency model (VCM).  Four different versions of the VCM are shown: one baseline VCM and three versions where different Rimg models (HPSv2.1, PickScore, and ImgRwd) are incorporated.  Each model generates a sequence of video frames depicting a dog wearing sunglasses.  The results demonstrate that adding any of the reward models improves the generated video's quality compared to the baseline VCM. The specific improvements are visually apparent in the frames.", "section": "3.1 Optimizing Human Preference on Individual Video Frames"}, {"figure_path": "53daI9kbvf/figures/figures_21_3.jpg", "caption": "Figure 6: Ablation study on the choice of the Rimg. We compare the 4-step generations from each methods. The three Rimg we tested can all improve the video generation quality compare to the baseline VCM (VC2).", "description": "This figure shows an ablation study on the choice of image-text reward models (Rimg) used in the T2V-Turbo model.  It compares the video generation quality of four models: a baseline VCM (Video Consistency Model), and three variations of the VCM that incorporate feedback from three different Rimg models (HPSv2.1, PickScore, and ImgRwd).  The results indicate that incorporating reward feedback from any of these Rimg models leads to improved video generation quality compared to the baseline VCM.", "section": "3.1 Optimizing Human Preference on Individual Video Frames"}, {"figure_path": "53daI9kbvf/figures/figures_22_1.jpg", "caption": "Figure 1: By integrating reward feedback during consistency distillation from VideoCrafter2 [Chen et al., 2024], our T2V-Turbo (VC2) can generate high-quality videos with 4-8 inference steps, breaking the quality bottleneck of a VCM [Wang et al., 2023a]. Appendix F includes the corresponding text prompts.", "description": "This figure shows a comparison of video quality generated by the T2V-Turbo model and a Video Consistency Model (VCM). The left side shows videos generated by a VCM using 4 and 8 steps inference while the right side shows videos generated by T2V-Turbo. The T2V-Turbo model achieved significantly better video quality than the VCM model, demonstrating the effectiveness of the proposed approach.", "section": "Abstract"}, {"figure_path": "53daI9kbvf/figures/figures_22_2.jpg", "caption": "Figure 1: By integrating reward feedback during consistency distillation from VideoCrafter2 [Chen et al., 2024], our T2V-Turbo (VC2) can generate high-quality videos with 4-8 inference steps, breaking the quality bottleneck of a VCM [Wang et al., 2023a]. Appendix F includes the corresponding text prompts.", "description": "This figure shows the results of video generation using T2V-Turbo and a baseline Video Consistency Model (VCM).  The top row displays results from a Video Consistency Model, demonstrating lower image quality. The bottom row shows T2V-Turbo's significantly improved video quality with only 4-8 inference steps, in comparison to VCM's 8-step approach. The text prompts used to generate the videos are provided in Appendix F.", "section": "Abstract"}, {"figure_path": "53daI9kbvf/figures/figures_22_3.jpg", "caption": "Figure 4: Qualitative comparisons between the 4-step VCM, 50-step teacher T2V, 4-step T2V-Turbo and 8-step T2V-Turbo generations. Left: (VC2), Right: (MS).", "description": "This figure shows a qualitative comparison of video generations from four different models using two different prompts.  The models compared are a 4-step Video Consistency Model (VCM), a 50-step teacher T2V model, a 4-step T2V-Turbo model, and an 8-step T2V-Turbo model.  The left side shows results for the VC2 variant of T2V-Turbo, while the right side shows results for the MS variant. Each row represents the outputs from the different models for a specific prompt.", "section": "4.2 Human Evaluation with 700 EvalCrafter Prompts"}, {"figure_path": "53daI9kbvf/figures/figures_23_1.jpg", "caption": "Figure 2: Overview of the training pipeline of our T2V-Turbo. We integrate reward feedback from both an image-text RM and a video-text RM into the VCD procedures by backpropagating gradient through the single-step generation process of our T2V-Turbo.", "description": "This figure illustrates the training process of the T2V-Turbo model.  It highlights the integration of reward feedback from both image-text and video-text reward models.  Instead of backpropagating gradients through the entire iterative sampling process, T2V-Turbo directly optimizes rewards associated with single-step generations, making the training process more efficient and memory-friendly. The diagram shows how gradients from reward maximization and distillation are combined and used to optimize the single-step generation within the consistency distillation process.", "section": "Training T2V-Turbo with Mixed Reward Feedback"}, {"figure_path": "53daI9kbvf/figures/figures_23_2.jpg", "caption": "Figure 4: Qualitative comparisons between the 4-step VCM, 50-step teacher T2V, 4-step T2V-Turbo and 8-step T2V-Turbo generations. Left: (VC2), Right: (MS).", "description": "This figure showcases a qualitative comparison of video generation results from four different methods: a baseline Video Consistency Model (VCM) with 4 inference steps, the original teacher T2V model with 50 steps, and the proposed T2V-Turbo method with both 4 and 8 steps.  The left side shows results when using VideoCrafter2 as the teacher model, and the right side uses ModelScopeT2V.  The comparison highlights the visual quality improvements achieved by T2V-Turbo, demonstrating its ability to match or even surpass the quality of the 50-step teacher model while significantly reducing computational cost.", "section": "4.2 Human Evaluation with 700 EvalCrafter Prompts"}, {"figure_path": "53daI9kbvf/figures/figures_23_3.jpg", "caption": "Figure 4: Qualitative comparisons between the 4-step VCM, 50-step teacher T2V, 4-step T2V-Turbo and 8-step T2V-Turbo generations. Left: (VC2), Right: (MS).", "description": "This figure shows a qualitative comparison of video generations from four different models using two example prompts.  The models compared are a 4-step Video Consistency Model (VCM), a 50-step teacher text-to-video (T2V) model, a 4-step T2V-Turbo model, and an 8-step T2V-Turbo model.  The left side shows results using the VideoCrafter2 teacher model, while the right side shows results using the ModelScopeT2V teacher model.  The image visually demonstrates the improvements in video quality achieved by T2V-Turbo, especially considering its significantly faster inference speed.", "section": "4 Experimental Results"}, {"figure_path": "53daI9kbvf/figures/figures_24_1.jpg", "caption": "Figure 1: By integrating reward feedback during consistency distillation from VideoCrafter2 [Chen et al., 2024], our T2V-Turbo (VC2) can generate high-quality videos with 4-8 inference steps, breaking the quality bottleneck of a VCM [Wang et al., 2023a]. Appendix F includes the corresponding text prompts.", "description": "This figure showcases the results of using the T2V-Turbo model to generate videos.  The top row shows videos generated using VideoCrafter2, a model known for generating high-quality videos but requiring many inference steps (8-step).  The bottom row shows videos produced by T2V-Turbo, demonstrating that it can generate comparable quality with fewer inference steps (4-step). This highlights T2V-Turbo's ability to maintain high-quality video generation despite significantly faster inference times. The examples illustrate the impact of the mixed reward feedback integrated into the T2V-Turbo training process.", "section": "Abstract"}, {"figure_path": "53daI9kbvf/figures/figures_24_2.jpg", "caption": "Figure 4: Qualitative comparisons between the 4-step VCM, 50-step teacher T2V, 4-step T2V-Turbo and 8-step T2V-Turbo generations. Left: (VC2), Right: (MS).", "description": "This figure shows a qualitative comparison of video generation results from four different methods.  The first column shows videos generated by a baseline Video Consistency Model (VCM) using only 4 inference steps. The second column shows videos generated by the teacher model (a pre-trained text-to-video diffusion model) using 50 inference steps.  The third and fourth columns show the results from the proposed T2V-Turbo method using 4 and 8 steps respectively. Two different versions of T2V-Turbo are compared, (VC2) and (MS). The left half of the figure shows the VC2 results and the right shows the MS results. The purpose is to visually demonstrate the quality improvement and speedup achieved by T2V-Turbo compared to the baseline VCM and the teacher model.", "section": "4.2 Human Evaluation with 700 EvalCrafter Prompts"}, {"figure_path": "53daI9kbvf/figures/figures_25_1.jpg", "caption": "Figure 10: Additional qualitative comparison results for our T2V-Turbo (MS).", "description": "This figure shows additional qualitative comparison results for T2V-Turbo (MS) model.  It presents visual comparisons of video generation outputs for two different prompts: \"Mickey Mouse is dancing on white background\" and \"a man looking at a distant mountain in Sci-fi style\".  The comparison includes outputs from the ModelScopeT2V (50-step), VCM (MS, 4-step), T2V-Turbo (MS, 4-step), and T2V-Turbo (MS, 8-step) models. The purpose is to visually demonstrate the improvements in video quality and coherence achieved by the T2V-Turbo approach compared to baseline VCM and the teacher model.", "section": "4.2 Human Evaluation with 700 EvalCrafter Prompts"}, {"figure_path": "53daI9kbvf/figures/figures_25_2.jpg", "caption": "Figure 10: Additional qualitative comparison results for our T2V-Turbo (MS).", "description": "This figure provides additional qualitative comparisons of video generation results from different models. It specifically compares the 4-step and 8-step generations from T2V-Turbo (MS), the 50-step generations from its teacher model (ModelScopeT2V), and the 4-step generations from the baseline VCM (MS). Two video prompts are used: \"Mickey Mouse is dancing on white background\" and \"a man looking at a distant mountain in Sci-fi style\".  The image shows several frames from the resulting videos, allowing for a visual comparison of video quality and fidelity across the different methods.", "section": "4.2 Human Evaluation with 700 EvalCrafter Prompts"}, {"figure_path": "53daI9kbvf/figures/figures_26_1.jpg", "caption": "Figure 6: Ablation study on the choice of the Rimg. We compare the 4-step generations from each methods. The three Rimg we tested can all improve the video generation quality compare to the baseline VCM (VC2).", "description": "This figure shows an ablation study comparing the performance of different image-text reward models (Rimg) when incorporated into the training process of a Video Consistency Model (VCM). Three different Rimg models were tested: HPSv2.1, PickScore, and ImageReward, and their outputs compared against the baseline VCM. The results demonstrate that all three Rimg models improve the video generation quality compared to the baseline VCM.", "section": "3.1 Optimizing Human Preference on Individual Video Frames"}, {"figure_path": "53daI9kbvf/figures/figures_26_2.jpg", "caption": "Figure 2: Overview of the training pipeline of our T2V-Turbo. We integrate reward feedback from both an image-text RM and a video-text RM into the VCD procedures by backpropagating gradient through the single-step generation process of our T2V-Turbo.", "description": "This figure illustrates the training process of T2V-Turbo.  It highlights the integration of reward feedback from both image-text and video-text reward models. Instead of backpropagating gradients through the entire iterative sampling process, T2V-Turbo optimizes rewards directly associated with single-step generations, thus overcoming memory constraints.", "section": "Training T2V-Turbo with Mixed Reward Feedback"}, {"figure_path": "53daI9kbvf/figures/figures_26_3.jpg", "caption": "Figure 6: Ablation study on the choice of the Rimg. We compare the 4-step generations from each methods. The three Rimg we tested can all improve the video generation quality compare to the baseline VCM (VC2).", "description": "This figure shows an ablation study comparing the performance of different image-text reward models (Rimg) when integrated into the video consistency model (VCM). Three different models (HPSv2.1, PickScore, and ImageReward) were tested, and the results were compared to the baseline VCM and the proposed T2V-Turbo. Each image shows a sequence of four frames generated for a given prompt.", "section": "4.3 Ablation Studies"}, {"figure_path": "53daI9kbvf/figures/figures_27_1.jpg", "caption": "Figure 6: Ablation study on the choice of the Rimg. We compare the 4-step generations from each methods. The three Rimg we tested can all improve the video generation quality compare to the baseline VCM (VC2).", "description": "This figure shows an ablation study comparing the performance of different image-text reward models (Rimg) when integrated into a video consistency model (VCM).  The baseline VCM is compared to three variations incorporating HPSv2.1, PickScore, and ImageReward models.  The results demonstrate that integrating any of these three models improves the video generation quality compared to the baseline VCM.", "section": "3.1 Optimizing Human Preference on Individual Video Frames"}, {"figure_path": "53daI9kbvf/figures/figures_27_2.jpg", "caption": "Figure 6: Ablation study on the choice of the Rimg. We compare the 4-step generations from each methods. The three Rimg we tested can all improve the video generation quality compare to the baseline VCM (VC2).", "description": "This figure shows an ablation study comparing the performance of different image-text reward models (Rimg) when combined with a video consistency model (VCM).  Three different Rimg models (HPSv2.1, PickScore, and ImageReward) were tested and compared to a baseline VCM without an added Rimg.  The results show that adding any of the three Rimg models improved the generated video quality compared to the baseline VCM, suggesting that incorporating feedback from an image-text reward model enhances the video generation process.", "section": "4.3 Ablation Studies"}, {"figure_path": "53daI9kbvf/figures/figures_27_3.jpg", "caption": "Figure 6: Ablation study on the choice of the Rimg. We compare the 4-step generations from each methods. The three Rimg we tested can all improve the video generation quality compare to the baseline VCM (VC2).", "description": "This figure shows an ablation study comparing the performance of different image-text reward models (Rimg) in improving the quality of video generation. The results demonstrate that using any of the three tested Rimg models (HPSv2.1, PickScore, and ImageReward) results in higher-quality video generation compared to the baseline Video Consistency Model (VCM).", "section": "3.1 Optimizing Human Preference on Individual Video Frames"}, {"figure_path": "53daI9kbvf/figures/figures_28_1.jpg", "caption": "Figure 6: Ablation study on the choice of the Rimg. We compare the 4-step generations from each methods. The three Rimg we tested can all improve the video generation quality compare to the baseline VCM (VC2).", "description": "This figure presents an ablation study comparing the performance of different image-text reward models (Rimg) in improving video generation quality when integrated into a Video Consistency Model (VCM).  Three different Rimg models (HPSv2.1, PickScore, and ImageReward) are compared against a baseline VCM without any Rimg. The results showcase that incorporating any of the tested Rimg models leads to a significant improvement in the quality of 4-step video generations compared to the baseline VCM.", "section": "4.3 Ablation Studies"}, {"figure_path": "53daI9kbvf/figures/figures_28_2.jpg", "caption": "Figure 6: Ablation study on the choice of the Rimg. We compare the 4-step generations from each methods. The three Rimg we tested can all improve the video generation quality compare to the baseline VCM (VC2).", "description": "This figure shows an ablation study on the effect of different image-text reward models (Rimg) on video generation quality.  Three different Rimg models (HPSv2.1, PickScore, and ImageReward) are compared against a baseline video consistency model (VCM).  The results demonstrate that incorporating feedback from any of these Rimg models improves the quality of the generated 4-step videos compared to the baseline VCM.", "section": "3.1 Optimizing Human Preference on Individual Video Frames"}]