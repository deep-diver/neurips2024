[{"figure_path": "EKN8AGS1wG/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of our PFM framework. In the typical RLHF scenarios (left), we first sample preference data from the supervised fine-tuned (SFT) reference model. A reward model is learned from the collected dataset, either implicitly (as in DPO) or explicitly. The reward model is then used to fine-tune the reference policy to obtain the final model. Our method directly learns the preference flow from the collected preference data, where the flow is represented as a vector field ve (middle). For inference, we again sample a point from the reference policy, and improve the quality of alignment by using the trained flow matching model, without the need of fine-tuning the existing reference model (right).", "description": "This figure illustrates the Preference Flow Matching (PFM) framework, comparing it to traditional Reinforcement Learning from Human Feedback (RLHF) methods.  RLHF methods involve sampling from a pre-trained model, training a reward model, and then fine-tuning the pre-trained model. In contrast, PFM directly learns a preference flow, transforming less preferred data points into preferred ones without the need for explicit reward models or model fine-tuning. The figure shows this process in three stages: RLHF, PFM training, and PFM inference.", "section": "1 Introduction"}, {"figure_path": "EKN8AGS1wG/figures/figures_4_1.jpg", "caption": "Figure 2: Comparison of RLHF, DPO, and PFM on a 2-dimensional toy experiment. We generate preference labels from a ground truth reward in (a) and a pre-trained Gaussian reference policy (c). Both the RLHF (e) and DPO (f) methods struggle to align with the preferences, due to the overfitted reward model (b), even with the presence of KL regularizer (\u03b2 = 1). PFM is able to mimic the distribution of the positively-labeled samples (d), and therefore achieves the highest performance (g). Repeating PFM iteratively to the marginal samples can further improve the alignment with the preference (h).", "description": "This figure compares the performance of RLHF, DPO, and PFM on a simple 2D toy problem where the goal is to learn a preference from samples generated by a pre-trained model.  The ground truth reward function (a) is shown, along with the learned reward model by RLHF which overfits (b), the pre-trained model samples (c), preferred samples (d), and the results of RLHF (e), DPO (f), PFM (g), and iterative PFM (h). PFM shows superior performance because it directly learns the preference flow rather than relying on a reward model, thus avoiding overfitting.", "section": "3.2 Illustrative Example: Preference Generated from 8-Gaussians Density"}, {"figure_path": "EKN8AGS1wG/figures/figures_6_1.jpg", "caption": "Figure 3: Comparison of RLHF, DPO, and PFM on a conditional MNIST image generation task. Numbers represent the preference score. PFM (d) demonstrates superior sample quality and preference alignment compared to RLHF (e) and DPO (f), where DPO collapses with a small size of \u03b2 (g). The iterative PFM with only two iterations (h) results in almost perfectly aligning with the preferences.", "description": "This figure compares the performance of three different preference alignment methods: RLHF, DPO, and the proposed PFM, on a conditional MNIST image generation task.  The images generated by a pre-trained model and then modified by each method are shown, along with their associated preference scores. The results visually demonstrate that PFM produces higher-quality images that better align with human preferences than RLHF and DPO, especially when using an iterative approach.  The DPO method's performance significantly degrades with a smaller beta value, highlighting its sensitivity to hyperparameter tuning. In contrast, the iterative PFM shows nearly perfect alignment, suggesting the effectiveness and robustness of the proposed method.", "section": "5.1 Conditional Image Generation"}, {"figure_path": "EKN8AGS1wG/figures/figures_7_1.jpg", "caption": "Figure 4: Distribution of preference scores for each method. Left visualizes the distribution of scores for the pre-trained reference policy and PFM-attached policy. Without fine-tuning the reference policy, PFM can obtain substantially better results by only adding a small flow-matching module. Right visualizes the preference score distribution of the RLHF (PPO) fine-tuned policy, and the PFM added policy to the PPO fine-tuned policy. Note that PFM is trained with the original dataset, not by the dataset generated from the PPO fine-tuned policy.", "description": "This figure shows the distribution of preference scores obtained by different methods: the pre-trained reference model, the reference model with PFM, and the reference model with iterative PFM (5 iterations).  The left panel compares the pre-trained model (SFT) against the same model enhanced with PFM, highlighting the improvement in preference scores after adding the PFM module.  Similarly, the right panel compares the RLHF (PPO) fine-tuned model and the same model enhanced with PFM, showcasing the robustness of PFM to overfitting.", "section": "5 Experimental Results"}, {"figure_path": "EKN8AGS1wG/figures/figures_8_1.jpg", "caption": "Figure 5: Analysis of a sample episode of a DPO fine-tuned model on the MuJoCo ant environment. DPO fine-tuned model often overestimates the reward due to reward overfitting (e.g., t = 196). This can cause the policy to choose problematic actions. Here, the implicit reward estimation is \n\u0109e(s, a) = \u03b2log(\u03c0\u03c1(as)/\u03c0ref(as)).", "description": "Figure 5 shows an example of a DPO (Direct Preference Optimization) model's behavior during an episode in the MuJoCo Ant environment.  The figure demonstrates the consequences of reward overfitting, where the DPO model consistently overestimates the reward (as indicated by the red line exceeding the true reward in the graph, specifically around time step 196). This overestimation leads to poor policy choices, resulting in unstable and ultimately unsuccessful actions by the ant agent. The implicit reward estimation used in this process is shown to be  \u0109e(s, a) = \u03b2log(\u03c0\u03c1(as)/\u03c0ref(as)).", "section": "Experimental Results"}, {"figure_path": "EKN8AGS1wG/figures/figures_13_1.jpg", "caption": "Figure 1: Illustration of our PFM framework. In the typical RLHF scenarios (left), we first sample preference data from the supervised fine-tuned (SFT) reference model. A reward model is learned from the collected dataset, either implicitly (as in DPO) or explicitly. The reward model is then used to fine-tune the reference policy to obtain the final model. Our method directly learns the preference flow from the collected preference data, where the flow is represented as a vector field ve (middle). For inference, we again sample a point from the reference policy, and improve the quality of alignment by using the trained flow matching model, without the need of fine-tuning the existing reference model (right).", "description": "The figure illustrates the Preference Flow Matching (PFM) framework, comparing it to standard Reinforcement Learning from Human Feedback (RLHF) methods.  RLHF methods (left) use a reward model, trained on sampled preferences, to fine-tune a pre-trained model. PFM (middle and right) directly learns a preference flow, a vector field, that transforms less preferred outputs into preferred ones, without requiring fine-tuning of the pre-trained model.", "section": "1 Introduction"}]