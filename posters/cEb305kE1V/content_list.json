[{"type": "text", "text": "Deep Implicit Optimization for Robust and Flexible Image Registration ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Deep Learning in Image Registration (DLIR) methods have been tremendously   \n2 successful in image registration due to their speed and ability to incorporate weak   \n3 label supervision at training time. However, DLIR methods forego many of the   \n4 benefits of classical optimization-based methods. The functional nature of deep   \n5 networks do not guarantee that the predicted transformation is a local minima   \n6 of the registration objective, the representation of the transformation (displace  \n7 ment/velocity field/affine) is fixed, and the networks are not robust to domain shift.   \n8 Our method aims to bridge this gap between classical and learning methods by   \n9 incorporating optimization as a layer in a deep network. A deep network is trained   \n10 to predict multi-scale dense feature images that are registered using a black box   \n11 iterative optimization solver. This optimal warp is then used to minimize image and   \n12 label alignment errors. By implicitly differentiating end-to-end through an iterative   \n13 optimization solver, our learned features are registration and label-aware, and the   \n14 warp functions are guaranteed to be local minima of the registration objective   \n15 in the feature space. Our framework shows excellent performance on in-domain   \n16 datasets, and is agnostic to domain shift such as anisotropy and varying inten  \n17 sity profiles. For the first time, our method allows switching between arbitrary   \n18 transformation representations (free-form to diffeomorphic) at test time with zero   \n19 retraining. End-to-end feature learning also facilitates interpretability of features,   \n20 and out-of-the-box promptability using additional label-fidelity terms at inference. ", "page_idx": 0}, {"type": "text", "text": "21 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "22 Deformable Image Registration (DIR) refers to the local, non-linear alignment of images by estimating   \n23 a dense displacement field. Many workflows in medical image analysis require images to be in a   \n24 common coordinate system for comparison, analysis, and visualization, including comparing inter  \n25 subject data in neuroimaging [53, 104, 97, 38, 89, 94], biomechanics and dynamics of anatomical   \n26 structures including myocardial motions, airflow and pulmonary function in lung imaging, organ   \n27 motion tracking in radiation therapy [78, 77, 11, 70, 29, 105, 50, 18, 71, 84], and life sciences   \n28 research [112, 104, 99, 80, 98, 72, 17].   \n29 Classical DIR methods are based on solving a variational optimization problem, where a similarity   \n30 metric is optimized to find the best transformation that aligns the images. However, these methods are   \n31 typically slow, and cannot leverage learning to incorporate a training set containing weak supervision   \n32 such as anatomical landmarks or expert annotations. The quality of the registration is therefore   \n33 limited by the fidelity of the intensity image. Deep Learning for Image Registration (DLIR) is an   \n34 interesting paradigm to overcome these challenges. DLIR methods take a pair of images as input   \n35 to a neural network and output a warp field that aligns the images, and their associated anatomical   \n36 landmarks. The neural network parameters are trained to minimize the alignment loss over image   \n37 pairs and landmarks in a training set. A benefit of this method is the ability to incorporate weak   \n38 supervision like anatomical landmarks or expert annotations during training, which performs better   \n39 landmark alignment without access to landmarks at inference time.   \n40 Motivation. However, DLIR methods face several limitations. First, the prediction paradigm of   \n41 deep learning implies the feature learning and amortized optimization steps are fused; transformations   \n42 predicted at test-time may not even be a local minima of the alignment loss between the fixed and   \n43 moving image. The end-to-end prediction also implies that the representation of the transforma  \n44 tion is fixed (as a design choice of the network), and the model cannot switch between different   \n45 representations like free-form, stationary velocity, geodesic, LDDMM, B-Splines, or affine at test   \n46 time without additional finetuning, in sharp contrast to the flexibility of classical methods. Typical   \n47 registration workflows require a practitioner to try different parameterizations of the transformation   \n48 (free-form, stationary velocity, geodesic, LDDMM, B-Splines, affine) to determine the representation   \n49 most suitable for their application and additional retraining becomes expensive. Moreover, design   \n50 decisions like sparse keypoint learning for affine registration [103, 16, 69, 40] do not facilitate dense   \n51 deformable registration. Furthermore, DLIR methods do not allow interactive registration using   \n52 additional landmarks or label maps at test time, which is crucial for clinical applications. Hyper  \n53 parameter tuning for regularization is also expensive for DLIR methods. Although recent methods   \n54 propose conditional registration [44, 67] to amortize over the hyperparameter search during training,   \n55 the family of regularization is fixed in such cases, and space of hyperparameters becomes exponential   \n56 in the number of hyperparameter families considered. Lastly, current DLIR methods are not robust   \n57 to minor domain shifts like varying anisotropy and voxel resolutions, different image acquisition   \n58 and preprocessing protocols [62, 53, 70, 43]. Robustness to domain shift is imperative to biomedical   \n59 and clinical imaging where volumes are acquired with different scanners, protocols, and resolutions,   \n60 where the applicability of DLIR methods is limited to the training domain.   \n61 Contributions. We introduce DIO, a generic differentiable implicit optimization layer to a   \n62 learnable feature network for image registration. By decoupling feature learning and optimization,   \n63 our framework incorporates weak supervision like anatomical landmarks into the learned   \n64 features during training, which improves the fidelity of the feature images for registration. Feature   \n65 learning also leads to dense feature images, which smoothens the optimization landscape compared   \n66 to intensity-based registration due to homogenity present in most medical imaging modalities. Since   \n67 optimization frameworks are agnostic to spatial resolutions and feature distortions, DIO is extremely   \n68 robust to domain shifts like varying anisotropy, difference in sizes of fixed and moving images, and   \n69 different image acquisition and preprocessing protocols, even when compared to models trained   \n70 on contrast-agnostic synthetic data [43]. Moreover, our framework allows zero-cost plug-and  \n71 play of arbitrary transformation representations (free-form, geodesics, B-Spline, affine, etc.) and   \n72 regularization at test time without additional training and loss of accuracy. This also paves the way for   \n73 practitioners to perform quick and interactive registration, and use additional arbitrary \u2018prompts\u2019   \n74 such as new landmarks or label maps out-of-the-box at test time, as part of the optimization layer. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "75 2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "76 Deep Learning for Image Registration DIR refers to the alignment of a fixed image $I_{f}$ with a   \n77 moving image $I_{m}$ using a transformation $\\varphi\\in T$ where $T$ is a family of transformations. Classical   \n78 methods formulate a variational optimization problem to find the optimal $\\varphi$ that aligns the images [15,   \n79 4, 7, 5, 6, 2, 15, 25, 24, 23, 27, 39, 63, 102, 101, 100, 46, 60, 61, 76, 33, 32, 12]. In contrast,   \n80 earliest DLIR methods used supervised learning [19, 55, 82, 88] to predict the transformation $\\varphi$ .   \n81 Voxelmorph [13] was the first unsupervised method utilizing a UNet [83] for unsupervised registration   \n82 on brain MRI data. Recent works considered different architectural designs [21, 56, 48, 66],   \n83 cascade-based architectures and loss functions [116, 115, 49, 26, 68, 114, 79, 20], and symmetric   \n84 or inverse consistency-based formulations [65, 51, 52, 92, 116]. [67, 44] inject the hyperparameter   \n85 as input and perform amortized optimization over different values of the hyperparameter. Domain   \n86 randomization and finetuning [43, 96, 73, 30] are also proposed to improve robustness of registration   \n87 to domain shift, that is a core necessity in medical imaging since different institutions follow   \n88 varying acquisition and preprocessing pipelines. Foundational models are also proposed to improve   \n89 registration accuracy [57, 93]. Another line of work propose to use the implicit priors of deep   \n90 learning [95] within an optimization framework [110, 106, 49, 45]. We refer the reader to [36, 41, 28]   \n91 for other detailed reviews.   \n92 Iterative methods for DLIR Owing to the success of iterative optimization methods, few DLIR   \n93 methods propose emulating the iterative optimization within a network. [115, 116] use a cascade of   \n94 networks to iteratively predict a warp field, and use the warped moving image as the input to the next   \n95 layer in the cascade. TransMorph-TVF [20] uses a recurrent network to predict a time-dependent   \n96 velocity field. [114] use a shared weights encoder to output feature images at multiple scales, and a   \n97 deformation field estimator utilizing a correlation layer. RAFT [91] similarly builds a 4D correlation   \n98 volume from two 2D feature maps, and updates the optical flow field using a recurrent unit that   \n99 performs lookup on the correlation volume. However, such recursive formulations have a large   \n100 memory footprint due to explicit backpropagation through the entire cascade [8], and are not adaptive   \n101 or optimal with respect to the inputs. In contrast, DIO uses optimization as a layer \u2013 guaranteeing   \n102 convergence to a local minima, and implicit differentiation avoids storing the entire computation   \n103 graph making the framework both memory and time efficient.   \n104 Feature Learning for Image Registration [103, 16, 69, 40] learn keypoints from images which   \n105 is then used to compute the optimal affine transform using a closed form solution. However, these   \n106 methods are restricted to transformations that can be represented by differentiable closed-form   \n107 analytical solutions, making backpropagation trivial. These sparse keypoints cannot be reused for   \n108 dense deformable registration either. On the other hand, dense deformable registration (diffeomorphic   \n109 or otherwise) is almost universally solved using iterative optimization methods. This motivates the   \n110 need to perform implicit differentiation through an iterative optimization solver to perform feature   \n111 learning for registration. Other approaches learn image features to perform registration [108, 59, 107,   \n112 81], but do not perform feature learning and registration end-to-end, i.e., the features obtained are not   \n113 task-aware and may not be optimal for registration, especially for anatomical landmarks. Learned   \n114 features are either fed into a functional form to compute the transformation end-to-end, or are learned   \n115 using unsupervised learning in a stagewise manner. In contrast, by implicitly differentiating through   \n116 a black-box iterative solver, and minimizing the image and label alignment losses end-to-end, DIO   \n117 learns features that are registration-aware, label-aware, and dense. The optimization routine also   \n118 guarantees that the transformation is a local minima of the alignment of high-fidelity feature images.   \n119 Deep Equilibrium models Deep Equilibrium (DEQ) models [9, 34] have emerged as an interesting   \n120 alterative to recurrent architectures. DEQ layers solve a fixed-point equation of a layer to find its   \n121 equilibrium state without unrolling the entire computation graph. This leads to high expressiveness   \n122 without the need for memory-intensive backpropagation through time [10, 8, 31, 75, 37, 111].   \n123 PIRATE [45] uses DEQ to finetune the PnP denoiser network for registration, but unlike our work,   \n124 the data-fidelity term comes from the intensity images. However, these methods use DEQ to emulate   \n125 an infinite-layer network, which typically consists of learnable parameters within the recurrent layer.   \n126 Conceptually, our work does not aim to simply emulate such an infinite cascade, but rather use   \n127 DEQ to decouple feature learning and optimization in an end-to-end registration framework.   \n128 This inherits all the robustness and agnosticity of optimization-based methods, while retaining the   \n129 fidelity of learned features. DEQ allows us to avoid the layer-stacking paradigm for cascades, and use   \n130 optimization as a black box layer without storing the entire computation graph, leading to constant   \n131 memory footprint and faster convergence. This allows learnable features to be registration-aware   \n132 since gradients are backpropagated to the feature images through the optimization itself. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "133 3 Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "134 The registration problem is formulated as a variational optimization problem: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\varphi^{*}=\\arg\\operatorname*{min}_{\\varphi}L(I_{f},I_{m}\\circ\\varphi)+R(\\varphi)=\\arg\\operatorname*{min}_{\\varphi}C(\\varphi,I_{f},I_{m})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "135 where $I_{f}$ and $I_{m}$ are fixed and moving images respectively, $L$ is a loss function that measures   \n136 the dissimilarity between the fixed image and the transformed moving image, and $R$ is a suitable   \n137 regularizer that enforces desirable properties of the transformation $\\varphi$ . We call this the image matching   \n138 objective. If the images $I_{f}$ and $I_{m}$ are supplemented with anatomical label maps $L_{f}$ and $L_{m}$ , we call   \n139 this the label matching objective. Classical methods perform image matching on the intensity images,   \n140 but the label matching performance is bottlenecked by the fidelity of image gradients with respect to   \n141 the label matching objective, and dynamics of the optimization algorithm. Deep learning methods   \n142 mitigate this by injecting label matching objectives (for example, Dice score) into the objective   \n143 Eq. (1) and using a deep network with parameters $\\theta$ to predict $\\varphi$ for every image pair as input. In   \n144 essence, learning-based problems solve the following objective: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\theta^{*}=\\arg\\operatorname*{min}_{\\theta}\\sum_{f,m}L(I_{f},I_{m}\\circ\\varphi_{\\theta})+D(S_{f},S_{m}\\circ\\varphi_{\\theta})+R(\\varphi_{\\theta})=\\arg\\operatorname*{min}_{\\theta}\\sum_{f,m}T(\\varphi_{\\theta},I_{f},I_{m},S_{f},S_{m})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "cEb305kE1V/tmp/b26a5c22234fa9bc1ea022f0b4e064acad7fcb8e19b02c267897e0487634ffdc.jpg", "img_caption": ["Figure 1: Overview of our framework. (a) A neural network extracts multi-scale features from the input images. (b)These features are used to optimize warp fields using a multi-scale differentiable optimization solver. (c) The optimized transform is used to warp the moving image and labels. (d) The warped image/label are compared with the fixed image/label using a similarity metric. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "145 where $\\varphi_{\\theta}(I_{f},I_{m})$ is abbreviated to $\\varphi_{\\theta}$ . This leads to learned transformations $\\varphi_{\\theta}$ that perform both   \n146 good image and label matching. However, the feature learning and optimization are coupled, and the   \n147 learned features are optimized only for a specific training domain. This limitation primarily marks   \n148 the difference between DIO and existing DLIR methods.   \n149 Fig. 1 shows the overview of our method. Our goal is to learn feature images such that regis  \n150 tration in this feature space corresponds to both image and label matching performance, by   \n151 disentangling feature learning and optimization. We do this by using a feature network to extract   \n152 dense features from the intensity image, that are used to solve Eq. (1) using a black-box optimization   \n153 solver, and obtain an optimal transform $\\varphi^{*}$ . Once $\\varphi^{*}$ is obtained, this is plugged into Eq. (2) to obtain   \n154 gradients with respect to $\\varphi^{*}$ . Since $\\varphi^{*}$ is a function of the feature images, we implicitly differentiate   \n155 through the optimization to backpropagate gradients to the feature images and to the deep network.   \n156 We discuss the details of our method in the following sections. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "157 3.1 Feature Extractor Network ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "158 The first component of our framework is a feature network that extracts dense features from the   \n159 intensity images. This network is parameterized by $\\theta$ , and takes an image $I\\in\\mathbb{R}^{H\\times W\\times D\\times C_{i n}}$ as   \n160 input and outputs a feature map $\\stackrel{\\triangledown}{\\boldsymbol{F}}\\in\\mathbb{R}^{H\\times W\\times D\\times C}$ , where $C$ is the number of feature channels, i.e.   \n161 $F=g_{\\theta}(I)$ . Unlike existing DLIR methods where moving and fixed images are concatenated and   \n162 passed to the network, our feature network processes the images independently. This allows the fixed   \n163 and moving images to be of different voxel sizes. The feature network can also output multi-feature   \n164 feature maps $\\mathcal{F}\\,=\\,g_{\\theta}(I)\\,=\\,[F^{0},F^{1},...\\,,F^{N}]$ , where $F^{k}\\,\\in\\,\\mathbb{R}^{H/2^{k}\\times W/2^{k}\\times D/2^{k}\\times C_{k}}$ , which can be   \n165 used by multi-scale optimization solvers. The feature network is agnostic to architecture choice, and   \n166 we ablate on different architectures in the experiments. ", "page_idx": 3}, {"type": "text", "text": "167 3.2 Implicit Differentiation through Optimization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "168 Given the feature maps $F_{f}$ and $F_{m}$ extracted from the fixed and moving images, an optimization   \n169 solver optimizes Eq. (1) to obtain the transformation $\\varphi^{*}$ . This can be written by modifying Eq. (1) to   \n170 use the feature maps $F$ ; i.e. $\\varphi^{*}=\\arg\\operatorname*{min}_{\\varphi}C(F_{f},F_{m}\\circ\\varphi)$ . A local minima of this equation satisfies: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\varrho(\\varphi^{*},F_{f},F_{m})=\\frac{\\partial C}{\\partial\\varphi}\\Bigg|_{\\varphi^{*}}=0\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "171 This $\\varphi^{*}$ is used to compute the loss Eq. (2) to minimize image and label matching objective. To   \n172 propagate derivatives from $\\varphi^{*}$ to the feature images $F_{f},F_{m}$ , we invoke the Implicit Function Theo  \n173 rem [54]:   \n174 Theorem 1 For a function $\\varrho~:~\\mathbb{R}^{n}~\\times~\\mathbb{R}^{m_{1}+m_{2}}~~\\rightarrow~\\mathbb{R}^{n}$ that is continuously differentiable, if   \n175 $\\varrho(\\varphi^{*},F_{f},F_{m})\\,=\\,0$ and $\\left|\\frac{\\partial\\varrho}{\\partial\\varphi}\\right|\\rvert\\varphi^{*}\\ \\neq\\ 0.$ , then there exist open sets $U,V_{f},V_{m}$ containing $\\varphi^{*},F_{f},F_{m}$ ,   \n176 and a function $\\varphi^{*}(F_{f},F_{m})$ defined on these open sets such that $\\varrho(\\varphi^{*}(F_{f},F_{m}),F_{f},F_{m})=0$ .   \n177 Given the Implicit Function Theorem, we write $\\varrho(\\varphi^{*}(F_{f},F_{m}),F_{f},F_{m})=0$ and differentiate with   \n178 respect to $F_{f}$ to obtain: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\frac{d\\varrho}{d F_{f}}}={\\frac{\\partial\\varrho}{\\partial\\varphi}}{\\frac{\\partial\\varphi}{\\partial F_{f}}}+{\\frac{\\partial\\varrho}{\\partial F_{f}}}=0\\implies{\\frac{\\partial\\varphi}{\\partial F_{f}}}=-\\left({\\frac{\\partial\\varrho}{\\partial\\varphi}}\\right)^{-1}{\\frac{\\partial\\varrho}{\\partial F_{f}}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "179 The gradients of $\\varphi$ come from Eq. (2) (i.e. $\\textstyle{\\frac{\\partial T}{\\partial\\varphi}})$ , and the gradients of $F_{f}$ w.r.t. Eq. (2) are obtained as   \n180 $\\begin{array}{r}{\\frac{\\partial T}{\\partial F_{f}}=-\\frac{\\partial T}{\\partial\\varphi}\\left(\\frac{\\partial\\varrho}{\\partial\\varphi}\\right)^{-1}\\frac{\\partial\\varrho}{\\partial F_{f}}}\\end{array}$ . The gradients of $F_{m}$ are obtained similarly.   \n181 This design ensures that optimal registration in the feature space corresponds to optimal registration   \n182 both in the image and label spaces. Furthermore, the optimization layer ensures that the $\\varphi^{*}$ is a local   \n183 minima of this high-fidelity feature matching objective, i.e., the features obtained by the network.   \n184 Jacobian-Free Backprop In practice, the Jacobian $\\frac{\\partial\\varrho}{\\partial\\varphi}$ is expensive to compute, given the high   \n185 dimensionality of $\\varphi$ and $\\varrho$ . Following [31], we substitute the Jacobian to identity, and compute   \n186 $\\begin{array}{r}{\\frac{\\partial\\hat{T}}{\\partial F_{f}}\\approx-\\frac{\\partial T}{\\partial\\varphi}\\frac{\\partial\\varrho}{\\partial F_{f}}}\\end{array}$ \u2212\u2202\u2202T\u03c6\u2202\u2202F\u03f1f . This leads to much less memory and stable training dynamics compared to other   \n187 estimates of Jacobian like phantom gradients, damped unrolling, or Neumann series [35, 34].   \n202 Figure 2: Dense feature learning leads to flatter loss landscapes.   \n203 Top row shows the intensity image with the corresponding multi  \n204 scale features predicted by the deep network, where the $\\bar{L^{\\mathrm{th}}}$ level   \n205 denotes a feature of size $\\dot{H_{/2}}^{k}\\!\\times\\!W\\dot{/}2^{k}\\!\\times\\!C_{k}$ . Bottom row shows the   \n206 loss landscape as a function of the relative translation between the   \nsquares in the fixed and moving image. Note the flat maxima which   \n207 occurs when there is no overlap between the fixed and moving   \n208 image, making optimization impossible if there is no overlap of the   \n209 squares. On the contrary, the loss landscape for learned features is   \n210 smooth, even at the finest scale, leading to much faster convergence   \n211 even when there is no overlap between the intensity images.   \n212 ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "cEb305kE1V/tmp/161869eadab8ec7871807040d17a37f9926b467f42c24ee6421f540370f7d160.jpg", "img_caption": ["188 3.3 Multi-scale optimization "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Optimization based methods typically use a multi-scale approach to improve convergence and avoid local minima with the image matching objective [7, 5, 3, 15]. However, the downsampling of intensity images leads to indiscriminate blurring and loss of details at the coarser scales. We adopt a multi-scale approach by using pyramidal features from the network, which are naturally built into many convolutional architectures. We perform optimization at the coarsest scale, and use the result as initialization for the next finer scale (Algorithm 2). This is similar to optimization methods, but our multi-scale features obtained from different layers in the network correspond to different semantic content, in contrast to classical methods where the multi-scale features are simply downsampled versions of the original images. This allows the multi-scale registration to align different anatomical regions at different scales, which may be hard to ", "page_idx": 4}, {"type": "text", "text": "213   \n214 align at other finer or coarser scales. ", "page_idx": 4}, {"type": "text", "text": "215 4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "216 4.1 DIO learns dense features from sparse images ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "217 A key strength of DIO is the ability to learn interpretable dense features from sparse intensity images   \n218 for accurate and robust image matching. This is especially relevant for medical image registration,   \n219 which typically contain a lot of homogenity in the intensity images, making registration difficult.   \n220 We design a toy task to isolate and demonstrate this behavior. The fixed and moving images are   \n221 generated by placing a square of size $32\\!\\times\\!32$ pixels on an image of $128\\!\\times\\!128$ pixels. The squares in   \n222 the fixed and moving images overlap with a $50\\%$ chance. The task is to find an affine transformation   \n223 to align the two images. However, classical optimization methods will fail this task $50\\%$ of the time,   \n224 because when the squares do not overlap, there is no gradient of the loss function, illustrated by the   \n225 flat loss landscape in Fig. 2. However, deep networks discover features that significantly flatten this   \n226 loss landscape in the feature matching space. To show this, we train a network to output multi-scale   \n227 feature maps that is used to optimize Eq. (1) to recover an affine transform. We choose a 2D UNet   \n228 architecture, and the multi-scale feature maps are recovered from different layers of the decoder path   \n229 of the UNet. Since the features are trained to maximize label matching, the loss landscape is much   \n230 flatter, and the network is able to recover the affine transform with $>99\\%$ overlap (Appendix A.4).   \n231 End-to-end learning enables learning of features that are most conducive to registration, unlike   \n232 existing work [108, 59, 107, 81] that may not contain discriminative registration-aware features   \n233 about anatomical labels due to lack of task-awareness. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "234 4.2 Results on brain MRI registration ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "235 Setup: We evaluated our method on inter-subject registration on the OASIS dataset [62]. The   \n236 OASIS dataset contains 414 T1-weighted MRI scans of the brain with label maps containing 35   \n237 subcortical structures extracted from automatic segmentation with FreeSurfer and SAMSEG. We use   \n238 the preprocessed version from the Learn2Reg challenge [42] where all the volumes are skull-stripped,   \n239 intensity-corrected and center-cropped to $160\\!\\times\\!192\\!\\times\\!224$ . We use the same training and validation   \n240 sets as provided in the Learn2Reg challenge to enable fair comparison with other methods.   \n241 Architectures: We consider four architec  \n242 tures for the task, representing different in  \n243 ductive biases in the network. We use a   \n244 3D UNet architecture (denoted as UNet in   \n245 experiments), and a large-kernel UNet (de  \n246 noted as $L K U$ ) [48]. To extract multi-scale   \n247 features from the networks, we attach sin  \n248 gle convolutional layers to the feature of the   \n249 desired scales from the decoder path. For   \n250 each of these architectures, we also consider   \n251 \u201cEncoder-Only\u201d versions by discarding the de  \n252 coder path, and creating independent encoders   \n253 for each scale Fig. 9, denoted as UNet- $E$ and   \n254 LKU-E. We choose Encoder-Only versions to   \n255 ablate the performance using shared features   \n256 from the decoder path versus independent fea  \n257 ture extraction at each scale.   \n258 Results: We compare our method with ex  \n259 isting methods on the Learn2Reg OASIS chal  \n260 lenge (Table 1). We compare with state-of  \n261 the-art classical methods [5, 46, 64, 100], and   \n262 deep networks [58, 87, 67, 14, 22, 48]. DIO   \n263 is highly competitive with existing methods, ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Table 1: Performance on OASIS validation set. DIO is highly competitive with state-of-the-art DLIR methods in the in-distribution setting. Our feature learning incorporates label-aware features, which is evident from the superior performance compared to four SOTA optimization-based classical methods. ", "page_idx": 5}, {"type": "table", "img_path": "cEb305kE1V/tmp/86f6635187233d757e547de9331261a7918cd72c9f85e2172900fcc0ffffa35e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "264 especially with TransMorph which uses up to two orders of magnitude more trainable parameters   \n265 than DIO to achieve a similar performance. We note that the Large Kernel UNet architecture performs   \n266 better than the standard UNet architecture, which is consistent with the findings in [48], even for   \n267 dense feature extraction. This is due to the larger receptive field of LKUNet, which is able to capture   \n268 more context in the image. Moreover, the Encoder-Only versions of the network perform slightly   \n269 worse than the full networks, showing that sharing features across scales is beneficial for the task. ", "page_idx": 5}, {"type": "text", "text": "270 4.3 Optimization-in-the-loop introduces robustness to domain shift ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "271 A key requirement of registration algorithms is to generalize over a spectrum of acquisition and   \n272 preprocessing protocols, since medical images are rarely acquired with the same configuration.   \n273 Existing DLIR methods are extremely sensitive to domain shift, and catastrophically fail on other   \n274 brain datasets. On the contrary, DIO inherits the domain agnosticism of the optimization solver, and   \n275 is robust under feature distortions introduced by domain shift.   \n276 We evaluate the robustness of the trained models on three brain datasets: LPBA40, IBSR18, and   \n277 CUMC12 datasets [85, 1, 53]. Contrary to the OASIS dataset, these datasets were obtained on   \n278 different scanners, aligned to different atlases (MNI305, Talairach) with varying algorithms used   \n279 for skull-stripping, bias correction (BrainSuite, autoseg), and different manual labelling protocols   \n280 of different anatomical regions (as opposed to automatically generated Freesurfer labels in OASIS).   \n281 Unlike the OASIS dataset, these datasets have different volume sizes, and IBSR18 and CUMC12   \n282 datasets are not 1mm isotropic. More details about the datasets are provided in Appendix A.6.   \n283 Results. We evaluate across a variety of configurations \u2013 (i) preserving the anisotropy of the   \n284 volumes or resampling to 1mm isotropic (denoted as anisotropic or isotropic), and (ii) center-cropping   \n85 the volumes to match the size of the OASIS dataset (denoted as Crop and No Crop). The results for all   \n8 three datasets are shown in Fig. 3 sorted by mean Dice score; quantitative comparison is also shown   \n287 in Appendix Table 4. Note that TransMorph, VoxelMorph, and SynthMorph do not work for sizes that   \n8 are different than the OASIS dataset, therefore they only work in the Crop setting. The IBSR18 dataset   \n289 also has volumes with different spatial sampling, and resampling to 1mm isotropic leads to different   \n290 voxel sizes. These volumes cannot be concatenated along the channel dimension, consequently every   \n291 DLIR method cannot run under this configuration (Fig. 3(a)). Since our method takes as input only a   \n292 single volume, and the convolutional architecture preserves the volume size, the fixed and moving   \n293 images can have different voxel sizes, i.e. feature extraction is not contingent on the voxel sizes of   \n294 the moving and fixed images being equal. The optimization solver can also handle different voxel   \n295 sizes for the fixed and moving volumes \u2013 which is useful in applications like multimodal registration   \n296 (in-vivo to ex-vivo, histology to 3D, MRI to microscopy). This unprecedented flexibility brings forth   \n297 a new operational paradigm in deep learning for registration that was unavailable before, widening   \n298 the scope of applications for registration with deep features.   \n299 We compare our method with a variety of DLIR baselines, trained with and without label super  \n300 vision (the former denoted as \u2018w/ Dice sup.\u2019 in Fig. 3). Our method performs substantially better   \n301 than all the baselines with a significantly narrower interquartile range on the IBSR18 and CUMC12   \n302 datasets. The differences are significant \u2013 on IBSR18 and CUMC12, our median performance is   \n303 higher than the third quartile of almost all baselines. The sturdy performance against domain shift   \n304 provides a strong motivation for using optimization-in-the-loop for learnable registration. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "cEb305kE1V/tmp/48a07840e4546eab2ce470c04df1319f5d6915499e2e70be6a095fb2de8fd66e.jpg", "img_caption": ["Figure 3: Boxplots of Dice scores for three out-of-distribution datasets. DIO performs significantly better across three datasets without additional finetuning. Contrary to other baselines that output warp fields considering 1mm isotropic data, leading to a performance drop with anisotropic volumes, DIO performs better with anisotropic data due to the optimization\u2019s resolution-agnostic nature. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "305 4.4 Robust feature learning enables zero-shot performance by switching optimizers at 306 test-time ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "307 Another major advantage of our framework is that we can switch the optimizer at test time without   \n308 any retraining. This is useful when the registration constraints change over time (i.e. initially   \n309 diffeomorphic transforms were required but now non-diffeomorphic transforms are acceptable), or   \n310 when the registration is used in a pipeline where different parameterizations (freeform, diffeomorphic,   \n311 geodesic, B-spline) may be compared. Since our framework decouples the feature learning from the   \n312 optimization, we can switch the optimizer arbitrarily at test time, at no additional cost. A crucial   \n313 requirement is that learned features should not be too sensitive to the training optimizer.   \n314 To demonstrate this functionality, we use the val  \n315 idation set of the OASIS dataset and the four net  \n316 works trained in Section 4.2. The networks were   \n317 initially trained on the SGD optimizer without any   \n318 additional constraints on the warp field. At test   \n319 time, we switch the optimizer to the FireANTs   \n320 optimizer [46], that uses a Riemannian Adam op  \n321 timizer for multi-scale diffeomorphisms. Results   \n322 in Table 2 compare the Dice score, 95th percentile   \n323 of the Haussdorf distance (denoted as HD95) and   \n324 percentage of volume with negative Jacobians (de  \n325 noted as $\\%(\\|J\\|<0)]$ ) for the two optimizers. The   \n326 SGD optimizer introduces anywhere from $0.79\\%$   \n327 to $1.1\\%$ of singularities in the registration, while   \n328 the FireANTs optimizer does not introduce any sin  \n329 gularities. A slight drop in performance can be at  \n330 tributed to the additional constraints imposed by dif  \n331 feomorphic transforms. However, the high-fidelity   \n332 features lead to a much better label overlap than   \n333 FireANTs run with image features (Table 1). Our   \n334 framework introduces an unprecedented amount   \n335 of flexibility at test time that is an indispensible   \n336 feature in deep learning for registration, and can   \n337 be useful in a variety of applications where the reg  \n338 istration requirements change over time, without   \n339 expensive retraining. ", "page_idx": 7}, {"type": "table", "img_path": "cEb305kE1V/tmp/8b41ceb253483452791318cd6d1364d596c2a6853c839148cf4216f1e0b69eb1.jpg", "table_caption": [], "table_footnote": ["Table 2: Zero shot performance by switching optimizers at test-time. Our method is trained on the OASIS dataset with the SGD optimizer to obtain the warp field. At inference time, we use an SGD optimizer for no constraint on the warp field, and the FireANTs optimizer to ensure diffeomorphic warps. Across all architectures, the Dice Score remains robust, with only a slight dip attributed to the constraints introduced by diffeomorphic mappings. The SGD optimization introduces ${\\sim}1\\%$ singularities, while FireANTs shows no singularities. "], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "cEb305kE1V/tmp/ff99c2598473f1189aa4c39f65240a06f433694b6e509383af2163a0a1e89d84.jpg", "img_caption": ["Figure 4: Examples of multi-scale features learned by the feature extractor. Scale-space features (bottom row) obtained by downsampling the image downsample all image features indiscriminately. Our features (top row) preserve necessary anatomical information at all scales, and introduce inhomogenity in the feature space for better optimization (watershed effect and enhanced contrast near gyri and a halo around the outer surface to delineaate background from gray matter). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "340 4.5 Interpretability of features ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "341 Decoupling of feature learning and optimization allows us to examine the feature images obtained at   \n342 each scale to understand what feature help in the registration task. Classical methods use scale-space   \n343 images (smoothened and downsampled versions of the original image) to avoid local minima, but   \n344 lose discriminative image features at lower resolutions. Moreover, intensity images may not provide   \n345 sufficient details to perform label-aware registration. Since our method learns dense features to   \n346 minimize label matching losses, we can observe which features are necessary to enable label-aware   \n347 registration. Fig. 4 highlights differences between scale-space images and features learned by our   \n348 network. At all scales, the features introduces heterogeneity using a watershed effect and enhanced   \n349 contrast to improve label matching performance. ", "page_idx": 8}, {"type": "text", "text": "350 4.6 Inference time ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "351 DLIR methods have been very popular due to their fast inference time by performing amortized   \n352 optimization [14]. Classical methods generally focus on robustness and reproducubility, and do have   \n353 GPU implementations for fast inference. However, modern optimization toolkits [60, 46] utilize   \n354 massively parallel GPU computing to register images in seconds, and scale very well to ultrahigh   \n355 resolution imaging. A concern with optimization-in-the-loop methods is the inference time. Table   \n356 Table 3 shows the inference time for our method for all four architectures. These inference times are   \n357 fast for a lot of applications, and the plug-and-play nature of our framework makes DIO amenable to   \n358 rapid experimentation and hyperparameter tuning. ", "page_idx": 8}, {"type": "text", "text": "359 5 Conclusion and Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "360 Architecture Neural net Optimization Conclusion DLIR methods provide several bene  \n361 UNet 0.444 1.693 fits such as amortized optimization, integration of   \n362 UNet-E 0.433 1.555 weak supervision, and the ability to learn from large   \n363 LKU 0.795 1.463 (labeled) datasets. However, coupling of the feature   \n364 LKU-E 2.281 1.457 learning and optimization steps in DLIR methods lim  \n365 its the flexibility and robustness of the deep networks.   \n366 Table 3: Inference time for various architec- In this paper, we we introduce a novel paradigm tures. A multi-scale optimization takes only $\\sim1.5$   \n336678 smeackoinndgs  itt os uriutna ballel  fitoerr amtioosnt sa (pnploi ceaatriloyn ss.t oTphpiisn igs) tbhaaste idn fcroarmpoerwatoersk so. ptTihmiis zpaatiroand-iagsm-a r-eltaayienrs  faolrl  ltehaer nfilenxgi--   \n369 compared to the time for neural network\u2019s feature bility and robustness of classical multi-scale methods   \n370 extraction which is architecture dependent. while leverging large scale weak supervision such as   \n371 anatomical landmarks into high-fidelity, registration  \n372 aware feature learning. Our paradigm allows \u201cpromptable\u201d registration out-of-the-box as part of the plug-and-play optimization, where additional supervision such as labelmaps or landmarks can   \n374 be added to the optimization loss at test time. Our fast implementation allows for implementation   \n375 of optimization-as-a-layer in deep learning, which was previously thought to be infeasible, due to existing optimization frameworks being prohibitively slow. Densification of features from our   \n377 method also leads to better optimization landscapes, and our method is robust to unseen anisotropy and domain shift. To our knowledge, our method is the first to switch between transformation   \n379 representations (free-form to diffeomorphic) at test time without any retraining. This comes with fast   \n380 inference runtimes, and interpretability of the features used for optimization. Potential future work can explore multimodal registration, online hyperparameter tuning and few-shot learning. ", "page_idx": 8}, {"type": "text", "text": "382 Limitations The first limitation is unlike existing DLIR methods that concatenate the fixed and   \n383 moving images to feed into the network, DIO processes the images independently. The features   \n384 extracted from an image are therefore trained to marginalize the label matching performance over all   \n385 possible moving images, and cannot adapt to the moving image. This leads to slightly asymptotically   \n386 lower in-domain performance than methods like [48]. The second limitation is the implicit bias of   \n387 the optimization algorithm. Implicit bias in SGD restricts the space of solutions for optimization   \n388 problems that are overparameterized, such as deep networks [113, 90, 47, 74, 109]. In deformable   \n389 registration, the implicit bias of SGD restricts the direction of the gradient of the particle at $\\varphi(x)$ ,   \n390 which is always parallel to $\\nabla F_{m}(\\varphi(x))$ , independent of the fixed image and dissimilarity function.   \n391 This limits the degrees of freedom of the optimization by $N$ -fold for $N$ -D images. This is unlike DLIR   \n392 methods where the warp is not constrained to move along $\\nabla F_{m}(\\varphi(x))$ . This behavior is explored in   \n393 more detail in Appendix A.1. Future work aims to mitigate this implicit bias for better performance. ", "page_idx": 8}, {"type": "text", "text": "394 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "395 [1] Internet brain segmentation repository (IBSR). http://www.cma.mgh.harvard.edu/   \n396 ibsr/.   \n397 [2] V. Arsigny, O. Commowick, X. Pennec, and N. Ayache. A Log-Euclidean Framework for   \n398 Statistics on Diffeomorphisms. In R. Larsen, M. Nielsen, and J. Sporring, editors, Medical   \n399 Image Computing and Computer-Assisted Intervention \u2013 MICCAI 2006, Lecture Notes in   \n400 Computer Science, pages 924\u2013931, Berlin, Heidelberg, 2006. Springer.   \n401 [3] J. Ashburner. A fast diffeomorphic image registration algorithm. Neuroimage, 38(1):95\u2013113,   \n402 2007.   \n403 [4] B. Avants and J. C. Gee. Geodesic estimation for large deformation anatomical shape averaging   \n404 and interpolation. NeuroImage, 23:S139\u2013S150, Jan. 2004.   \n405 [5] B. B. Avants, C. L. Epstein, M. Grossman, and J. C. Gee. Symmetric diffeomorphic image   \n406 registration with cross-correlation: evaluating automated labeling of elderly and neurodegener  \n407 ative brain. Medical Image Analysis, 12(1):26\u201341, Feb. 2008.   \n408 [6] B. B. Avants, C. L. Epstein, M. Grossman, and J. C. Gee. Symmetric diffeomorphic image   \n409 registration with cross-correlation: Evaluating automated labeling of elderly and neurodegen  \n410 erative brain. Medical Image Analysis, 12(1):26\u201341, Feb. 2008.   \n411 [7] B. B. Avants, P. T. Schoenemann, and J. C. Gee. Lagrangian frame diffeomorphic image   \n412 registration: Morphometric comparison of human and chimpanzee cortex. Medical Image   \n413 Analysis, 10(3):397\u2013412, June 2006.   \n414 [8] S. Bai, Z. Geng, Y. Savani, and J. Z. Kolter. Deep Equilibrium Optical Flow Estimation. In   \n415 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages   \n416 610\u2013620, New Orleans, LA, USA, June 2022. IEEE.   \n417 [9] S. Bai, J. Z. Kolter, and V. Koltun. Deep equilibrium models. Advances in neural information   \n418 processing systems, 32, 2019.   \n419 [10] S. Bai, V. Koltun, and J. Z. Kolter. Multiscale deep equilibrium models. Advances in neural   \n420 information processing systems, 33:5238\u20135250, 2020.   \n421 [11] W. Bai, H. Suzuki, J. Huang, C. Francis, S. Wang, G. Tarroni, F. Guitton, N. Aung, K. Fung,   \n422 S. E. Petersen, et al. A population-based phenome-wide association study of cardiac and aortic   \n423 structure and function. Nature medicine, 26(10):1654\u20131662, 2020.   \n424 [12] R. Bajcsy, R. Lieberson, and M. Reivich. A computerized system for the elastic matching   \n425 of deformed radiographic images to idealized atlas images. Journal of computer assisted   \n426 tomography, 7(4):618\u2013625, 1983.   \n427 [13] G. Balakrishnan, A. Zhao, M. R. Sabuncu, J. Guttag, and A. V. Dalca. VoxelMorph: A   \n428 Learning Framework for Deformable Medical Image Registration. IEEE Transactions on   \n429 Medical Imaging, 38(8):1788\u20131800, Aug. 2019. arXiv:1809.05231 [cs].   \n430 [14] G. Balakrishnan, A. Zhao, M. R. Sabuncu, J. Guttag, and A. V. Dalca. Voxelmorph: a learning   \n431 framework for deformable medical image registration. IEEE transactions on medical imaging,   \n432 38(8):1788\u20131800, 2019.   \n433 [15] M. F. Beg, M. I. Miller, A. Trouv\u00e9, and L. Younes. Computing large deformation metric   \n434 mappings via geodesic flows of diffeomorphisms. International journal of computer vision,   \n435 61:139\u2013157, 2005.   \n436 [16] B. Billot, D. Moyer, N. Dey, M. Hoffmann, E. A. Turk, B. Gagoski, E. Grant, and P. Golland.   \n437 Se (3)-equivariant and noise-invariant 3d motion tracking in medical images. arXiv preprint   \n438 arXiv:2312.13534, 2023.   \n439 [17] B. E. Brezovec, A. B. Berger, Y. A. Hao, F. Chen, S. Druckmann, and T. R. Clandinin.   \n440 Mapping the neural dynamics of locomotion across the drosophila brain. Current Biology,   \n441 34(4):710\u2013726, 2024.   \n442 [18] K. K. Brock, S. Mutic, T. R. McNutt, H. Li, and M. L. Kessler. Use of image registration   \n443 and fusion algorithms and techniques in radiotherapy: Report of the aapm radiation therapy   \n444 committee task group no. 132. Medical physics, 44(7):e43\u2013e76, 2017.   \n445 [19] X. Cao, J. Yang, J. Zhang, D. Nie, M. Kim, Q. Wang, and D. Shen. Deformable image   \n446 registration based on similarity-steered cnn regression. In Medical Image Computing and   \n447 Computer Assisted Intervention- MICCAI 2017: 20th International Conference, Quebec City,   \n448 QC, Canada, September 11-13, 2017, Proceedings, Part I 20, pages 300\u2013308. Springer, 2017.   \n449 [20] J. Chen, E. C. Frey, and Y. Du. Unsupervised learning of diffeomorphic image registration   \n450 via transmorph. In International Workshop on Biomedical Image Registration, pages 96\u2013102.   \n451 Springer, 2022.   \n452 [21] J. Chen, E. C. Frey, Y. He, W. P. Segars, Y. Li, and Y. Du. TransMorph: Transformer for   \n453 unsupervised medical image registration. Medical Image Analysis, 82:102615, Nov. 2022.   \n454 [22] J. Chen, E. C. Frey, Y. He, W. P. Segars, Y. Li, and Y. Du. TransMorph: Transformer for   \n455 unsupervised medical image registration. Medical Image Analysis, 82:102615, Nov. 2022.   \n456 arXiv:2111.10480 [cs, eess].   \n457 [23] G. E. Christensen and H. J. Johnson. Consistent image registration. IEEE transactions on   \n458 medical imaging, 20(7):568\u2013582, 2001.   \n459 [24] G. E. Christensen, S. C. Joshi, and M. I. Miller. Volumetric transformation of brain anatomy.   \n460 IEEE transactions on medical imaging, 16(6):864\u2013877, 1997.   \n461 [25] G. E. Christensen, R. D. Rabbitt, and M. I. Miller. Deformable templates using large deforma  \n462 tion kinematics. IEEE transactions on image processing, 5(10):1435\u20131447, 1996.   \n463 [26] B. D. De Vos, F. F. Berendsen, M. A. Viergever, H. Sokooti, M. Staring, and I. I\u0161gum. A deep   \n464 learning framework for unsupervised affine and deformable image registration. Medical image   \n465 analysis, 52:128\u2013143, 2019.   \n466 [27] F. Dru, P. Fillard, and T. Vercauteren. An ITK Implementation of the Symmetric Log-Domain   \n467 Diffeomorphic Demons Algorithm. The Insight Journal, Sept. 2010.   \n468 [28] Y. Fu, Y. Lei, T. Wang, W. J. Curran, T. Liu, and X. Yang. Deep learning in medical image   \n469 registration: a review. Physics in Medicine & Biology, 65(20):20TR01, Oct. 2020.   \n470 [29] Y. Fu, Y. Lei, T. Wang, K. Higgins, J. D. Bradley, W. J. Curran, T. Liu, and X. Yang. LungReg  \n471 Net: an unsupervised deformable image registration method for 4D-CT lung. Medical physics,   \n472 47(4):1763\u20131774, Apr. 2020.   \n473 [30] Y. Fu, Y. Lei, J. Zhou, T. Wang, S. Y. David, J. J. Beitler, W. J. Curran, T. Liu, and X. Yang.   \n474 Synthetic ct-aided mri-ct image registration for head and neck radiotherapy. In Medical   \n475 Imaging 2020: Biomedical Applications in Molecular, Structural, and Functional Imaging,   \n476 volume 11317, pages 572\u2013578. SPIE, 2020.   \n477 [31] S. W. Fung, H. Heaton, Q. Li, D. McKenzie, S. Osher, and W. Yin. JFB: Jacobian-Free   \n478 Backpropagation for Implicit Networks, Dec. 2021. arXiv:2103.12803 [cs].   \n479 [32] J. C. Gee and R. K. Bajcsy. Elastic matching: Continuum mechanical and probabilistic analysis.   \n480 Brain warping, 2:183\u2013197, 1998.   \n481 [33] J. C. Gee, M. Reivich, and R. Bajcsy. Elastically deforming a three-dimensional atlas to match   \n482 anatomical brain images. 1993.   \n483 [34] Z. Geng and J. Z. Kolter. TorchDEQ: A Library for Deep Equilibrium Models, Oct. 2023.   \n484 arXiv:2310.18605 [cs].   \n485 [35] Z. Geng, X.-Y. Zhang, S. Bai, Y. Wang, and Z. Lin. On training implicit models. Advances in   \n486 Neural Information Processing Systems, 34:24247\u201324260, 2021.   \n487 [36] A. Gholipour, N. Kehtarnavaz, R. Briggs, M. Devous, and K. Gopinath. Brain functional   \n488 localization: a survey of image registration techniques. IEEE transactions on medical imaging,   \n489 26(4):427\u2013451, 2007.   \n490 [37] D. Gilton, G. Ongie, and R. Willett. Deep equilibrium architectures for inverse problems in   \n491 imaging. IEEE Transactions on Computational Imaging, 7:1123\u20131133, 2021.   \n492 [38] M. Goubran, C. Crukley, S. De Ribaupierre, T. M. Peters, and A. R. Khan. Image registration   \n493 of ex-vivo mri to sparsely sectioned histology of hippocampal and neocortical temporal lobe   \n494 specimens. Neuroimage, 83:770\u2013781, 2013.   \n495 [39] U. Grenander and M. I. Miller. Computational anatomy: An emerging discipline. Quarterly of   \n496 applied mathematics, 56(4):617\u2013694, 1998.   \n497 [40] G. Haskins, J. Kruecker, U. Kruger, S. Xu, P. A. Pinto, B. J. Wood, and P. Yan. Learning deep   \n498 similarity metric for 3d mr\u2013trus image registration. International journal of computer assisted   \n499 radiology and surgery, 14:417\u2013425, 2019.   \n500 [41] G. Haskins, U. Kruger, and P. Yan. Deep learning in medical image registration: a survey.   \n501 Machine Vision and Applications, 31(1):8, Jan. 2020.   \n502 [42] A. Hering, L. Hansen, T. C. Mok, A. C. Chung, H. Siebert, S. H\u00e4ger, A. Lange, S. Kuckertz,   \n503 S. Heldmann, W. Shao, et al. Learn2reg: comprehensive multi-task medical image registration   \n504 challenge, dataset and evaluation in the era of deep learning. IEEE Transactions on Medical   \n505 Imaging, 42(3):697\u2013712, 2022.   \n506 [43] M. Hoffmann, B. Billot, D. N. Greve, J. E. Iglesias, B. Fischl, and A. V. Dalca. Synthmorph:   \n507 learning contrast-invariant registration without acquired images. IEEE transactions on medical   \n508 imaging, 41(3):543\u2013558, 2021.   \n509 [44] A. Hoopes, M. Hoffmann, B. Fischl, J. Guttag, and A. V. Dalca. Hypermorph: Amortized   \n510 hyperparameter learning for image registration. In Information Processing in Medical Imaging:   \n511 27th International Conference, IPMI 2021, Virtual Event, June 28\u2013June 30, 2021, Proceedings   \n512 27, pages 3\u201317. Springer, 2021.   \n513 [45] J. Hu, W. Gan, Z. Sun, H. An, and U. S. Kamilov. A Plug-and-Play Image Registration   \n514 Network, Mar. 2024. arXiv:2310.04297 [eess].   \n515 [46] R. Jena, P. Chaudhari, and J. C. Gee. Fireants: Adaptive riemannian optimization for multi  \n516 scale diffeomorphic registration. arXiv preprint arXiv:2404.01249, 2024.   \n517 [47] Z. Ji and M. Telgarsky. Gradient descent aligns the layers of deep linear networks. arXiv   \n518 preprint arXiv:1810.02032, 2018.   \n519 [48] X. Jia, J. Bartlett, T. Zhang, W. Lu, Z. Qiu, and J. Duan. U-net vs transformer: Is u-net   \n520 outdated in medical image registration? arXiv preprint arXiv:2208.04939, 2022.   \n521 [49] A. Joshi and Y. Hong. Diffeomorphic Image Registration using Lipschitz Continuous Residual   \n522 Networks. page 13.   \n523 [50] M. L. Kessler. Image registration and data fusion in radiation therapy. The British journal of   \n524 radiology, 79(special_issue_1):S99\u2013S108, 2006.   \n525 [51] B. Kim, D. H. Kim, S. H. Park, J. Kim, J.-G. Lee, and J. C. Ye. Cyclemorph: cycle consistent   \n526 unsupervised deformable image registration. Medical image analysis, 71:102036, 2021.   \n527 [52] B. Kim, J. Kim, J.-G. Lee, D. H. Kim, S. H. Park, and J. C. Ye. Unsupervised deformable image   \n528 registration using cycle-consistent cnn. In Medical Image Computing and Computer Assisted   \n529 Intervention\u2013MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13\u201317,   \n530 2019, Proceedings, Part VI 22, pages 166\u2013174. Springer, 2019.   \n531 [53] A. Klein, J. Andersson, B. A. Ardekani, J. Ashburner, B. Avants, M.-C. Chiang, G. E. Chris  \n532 tensen, D. L. Collins, J. Gee, P. Hellier, J. H. Song, M. Jenkinson, C. Lepage, D. Rueckert,   \n533 P. Thompson, T. Vercauteren, R. P. Woods, J. J. Mann, and R. V. Parsey. Evaluation of 14   \n534 nonlinear deformation algorithms applied to human brain MRI registration. NeuroImage,   \n535 46(3):786\u2013802, July 2009.   \n536 [54] S. G. Krantz and H. R. Parks. The implicit function theorem: history, theory, and applications.   \n537 Springer Science & Business Media, 2002.   \n538 [55] J. Krebs, T. Mansi, H. Delingette, L. Zhang, F. C. Ghesu, S. Miao, A. K. Maier, N. Ayache,   \n539 R. Liao, and A. Kamen. Robust non-rigid registration through agent-based action learning.   \n540 In Medical Image Computing and Computer Assisted Intervention- MICCAI 2017: 20th   \n541 International Conference, Quebec City, QC, Canada, September 11-13, 2017, Proceedings,   \n542 Part I 20, pages 344\u2013352. Springer, 2017.   \n543 [56] L. Lebrat, R. Santa Cruz, F. de Gournay, D. Fu, P. Bourgeat, J. Fripp, C. Fookes, and O. Salvado.   \n544 CorticalFlow: A Diffeomorphic Mesh Transformer Network for Cortical Surface Reconstruc  \n545 tion. In Advances in Neural Information Processing Systems, volume 34, pages 29491\u201329505.   \n546 Curran Associates, Inc., 2021.   \n547 [57] F. Liu, K. Yan, A. P. Harrison, D. Guo, L. Lu, A. L. Yuille, L. Huang, G. Xie, J. Xiao, X. Ye,   \n548 and D. Jin. SAME: Deformable Image Registration Based on Self-supervised Anatomical   \n549 Embeddings. In M. de Bruijne, P. C. Cattin, S. Cotin, N. Padoy, S. Speidel, Y. Zheng, and   \n550 C. Essert, editors, Medical Image Computing and Computer Assisted Intervention \u2013 MICCAI   \n551 2021, Lecture Notes in Computer Science, pages 87\u201397, Cham, 2021. Springer International   \n552 Publishing.   \n553 [58] J. Lv, Z. Wang, H. Shi, H. Zhang, S. Wang, Y. Wang, and Q. Li. Joint progressive and   \n554 coarse-to-fine registration of brain mri via deformation field integration and non-rigid feature   \n555 fusion. IEEE Transactions on Medical Imaging, 41(10):2788\u20132802, 2022.   \n556 [59] J. Ma, X. Jiang, A. Fan, J. Jiang, and J. Yan. Image matching from handcrafted to deep   \n557 features: A survey. International Journal of Computer Vision, 129(1):23\u201379, 2021.   \n558 [60] A. Mang, A. Gholami, C. Davatzikos, and G. Biros. CLAIRE: A distributed-memory solver for   \n559 constrained large deformation diffeomorphic image registration. SIAM Journal on Scientific   \n560 Computing, 41(5):C548\u2013C584, Jan. 2019. arXiv:1808.04487 [cs, math].   \n561 [61] A. Mang and L. Ruthotto. A lagrangian gauss\u2013newton\u2013krylov solver for mass-and intensity  \n562 preserving diffeomorphic image registration. SIAM Journal on Scientific Computing,   \n563 39(5):B860\u2013B885, 2017.   \n564 [62] D. S. Marcus, T. H. Wang, J. Parker, J. G. Csernansky, J. C. Morris, and R. L. Buckner.   \n565 Open access series of imaging studies (oasis): cross-sectional mri data in young, middle aged,   \n566 nondemented, and demented older adults. Journal of cognitive neuroscience, 19(9):1498\u20131507,   \n567 2007.   \n568 [63] M. I. Miller, A. Trouv\u00e9, and L. Younes. On the Metrics and Euler-Lagrange Equations of   \n569 Computational Anatomy. Annual Review of Biomedical Engineering, 4(1):375\u2013405, 2002.   \n570 _eprint: https://doi.org/10.1146/annurev.bioeng.4.092101.125733.   \n571 [64] M. Modat, G. R. Ridgway, Z. A. Taylor, M. Lehmann, J. Barnes, D. J. Hawkes, N. C. Fox, and   \n572 S. Ourselin. Fast free-form deformation using graphics processing units. Computer methods   \n573 and programs in biomedicine, 98(3):278\u2013284, 2010.   \n574 [65] T. C. Mok and A. Chung. Fast symmetric diffeomorphic image registration with convolutional   \n575 neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern   \n576 recognition, pages 4644\u20134653, 2020.   \n577 [66] T. C. Mok and A. Chung. Affine medical image registration with coarse-to-fine vision   \n578 transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern   \n579 Recognition, pages 20835\u201320844, 2022.   \n580 [67] T. C. Mok and A. C. Chung. Conditional deformable image registration with convolutional   \n581 neural network. pages 35\u201345, 2021.   \n582 [68] T. C. W. Mok and A. C. S. Chung. Large Deformation Diffeomorphic Image Registration with   \n583 Laplacian Pyramid Networks, June 2020. arXiv:2006.16148 [cs, eess].   \n584 [69] D. Moyer, E. Abaci Turk, P. E. Grant, W. M. Wells, and P. Golland. Equivariant filters   \n585 for efficient tracking in 3d imaging. In Medical Image Computing and Computer Assisted   \n586 Intervention\u2013MICCAI 2021: 24th International Conference, Strasbourg, France, September   \n587 27\u2013October 1, 2021, Proceedings, Part IV 24, pages 193\u2013202. Springer, 2021.   \n588 [70] K. Murphy, B. Van Ginneken, J. M. Reinhardt, S. Kabus, K. Ding, X. Deng, K. Cao, K. Du,   \n589 G. E. Christensen, V. Garcia, et al. Evaluation of registration methods on thoracic ct: the   \n590 empire10 challenge. IEEE transactions on medical imaging, 30(11):1901\u20131920, 2011.   \n591 [71] S. Oh and S. Kim. Deformable image registration in radiation therapy. Radiation oncology   \n592 journal, 35(2):101, 2017.   \n593 [72] H. Peng, P. Chung, F. Long, L. Qu, A. Jenett, A. M. Seeds, E. W. Myers, and J. H. Simpson.   \n594 Brainaligner: 3d registration atlases of drosophila brains. Nature methods, 8(6):493\u2013498,   \n595 2011.   \n596 [73] J. P\u00e9rez de Frutos, A. Pedersen, E. Pelanis, D. Bouget, S. Survarachakan, T. Lang\u00f8, O.-J. Elle,   \n597 and F. Lindseth. Learning deep abdominal ct registration through adaptive loss weighting and   \n598 synthetic data generation. Plos one, 18(2):e0282110, 2023.   \n599 [74] S. Pesme, L. Pillaud-Vivien, and N. Flammarion. Implicit bias of sgd for diagonal linear   \n600 networks: a provable benefit of stochasticity. Advances in Neural Information Processing   \n601 Systems, 34:29218\u201329230, 2021.   \n602 [75] A. Pokle, Z. Geng, and J. Z. Kolter. Deep equilibrium approaches to diffusion models.   \n603 Advances in Neural Information Processing Systems, 35:37975\u201337990, 2022.   \n604 [76] Y. Qiao, B. P. Lelieveldt, and M. Staring. An efficient preconditioner for stochastic gra  \n605 dient descent optimization of image registration. IEEE transactions on medical imaging,   \n606 38(10):2314\u20132325, 2019.   \n607 [77] C. Qin, S. Wang, C. Chen, W. Bai, and D. Rueckert. Generative Myocardial Motion Tracking   \n608 via Latent Space Exploration with Biomechanics-informed Prior, June 2022. arXiv:2206.03830   \n609 [cs, eess].   \n610 [78] C. Qin, S. Wang, C. Chen, H. Qiu, W. Bai, and D. Rueckert. Biomechanics-informed Neural   \n611 Networks for Myocardial Motion Tracking in MRI, July 2020. arXiv:2006.04725 [cs, eess].   \n612 [79] H. Qiu, C. Qin, A. Schuh, K. Hammernik, and D. Rueckert. Learning diffeomorphic and   \n613 modality-invariant registration using b-splines. 2021.   \n614 [80] L. Qu, F. Long, and H. Peng. 3-d registration of biological images and models: registration   \n615 of microscopic images and its uses in segmentation and annotation. IEEE Signal Processing   \n616 Magazine, 32(1):70\u201377, 2014.   \n617 [81] D. Quan, H. Wei, S. Wang, R. Lei, B. Duan, Y. Li, B. Hou, and L. Jiao. Self-distillation feature   \n618 learning network for optical and sar image registration. IEEE Transactions on Geoscience and   \n619 Remote Sensing, 60:1\u201318, 2022.   \n620 [82] M.-M. Roh\u00e9, M. Datar, T. Heimann, M. Sermesant, and X. Pennec. Svf-net: learning de  \n621 formable image registration using shape matching. In Medical Image Computing and Com  \n622 puter Assisted Intervention- MICCAI 2017: 20th International Conference, Quebec City, QC,   \n623 Canada, September 11-13, 2017, Proceedings, Part I 20, pages 266\u2013274. Springer, 2017.   \n624 [83] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image   \n625 segmentation. In Medical image computing and computer-assisted intervention\u2013MICCAI   \n626 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part   \n627 III 18, pages 234\u2013241. Springer, 2015.   \n628 [84] J. G. Rosenman, E. P. Miller, and T. J. Cullip. Image registration: an essential part of radiation   \n629 therapy treatment planning. International Journal of Radiation Oncology\\* Biology\\* Physics,   \n630 40(1):197\u2013205, 1998.   \n631 [85] D. W. Shattuck, M. Mirza, V. Adisetiyo, C. Hojatkashani, G. Salamon, K. L. Narr, R. A.   \n632 Poldrack, R. M. Bilder, and A. W. Toga. Construction of a 3d probabilistic atlas of human   \n633 cortical structures. Neuroimage, 39(3):1064\u20131080, 2008.   \n634 [86] A. Siarohin. cuda-gridsample-grad2. GitHub Repository, 2023.   \n635 [87] H. Siebert, L. Hansen, and M. P. Heinrich. Fast 3d registration with accurate optimisation and   \n636 little learning for learn2reg 2021. In International Conference on Medical Image Computing   \n637 and Computer-Assisted Intervention, pages 174\u2013179. Springer, 2021.   \n638 [88] H. Sokooti, B. De Vos, F. Berendsen, B. P. Lelieveldt, I. I\u0161gum, and M. Staring. Nonrigid image   \n639 registration using multi-scale 3d convolutional neural networks. In Medical Image Computing   \n640 and Computer Assisted Intervention- MICCAI 2017: 20th International Conference, Quebec   \n641 City, QC, Canada, September 11-13, 2017, Proceedings, Part I 20, pages 232\u2013239. Springer,   \n642 2017.   \n643 [89] J. H. Song, G. E. Christensen, J. A. Hawley, Y. Wei, and J. G. Kuhl. Evaluating image   \n644 registration using nirep. In Biomedical Image Registration: 4th International Workshop, WBIR   \n645 2010, L\u00fcbeck, Germany, July 11-13, 2010. Proceedings 4, pages 140\u2013150. Springer, 2010.   \n646 [90] D. Soudry, E. Hoffer, M. S. Nacson, S. Gunasekar, and N. Srebro. The implicit bias of gradient   \n647 descent on separable data. Journal of Machine Learning Research, 19(70):1\u201357, 2018.   \n648 [91] Z. Teed and J. Deng. RAFT: Recurrent All-Pairs Field Transforms for Optical Flow, Aug.   \n649 2020. arXiv:2003.12039 [cs].   \n650 [92] L. Tian, H. Greer, F.-X. Vialard, R. Kwitt, R. S. J. Est\u00e9par, R. J. Rushmore, N. Makris,   \n651 S. Bouix, and M. Niethammer. Gradicon: Approximate diffeomorphisms via gradient inverse   \n652 consistency. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern   \n653 Recognition, pages 18084\u201318094, 2023.   \n654 [93] L. Tian, Z. Li, F. Liu, X. Bai, J. Ge, L. Lu, M. Niethammer, X. Ye, K. Yan, and D. Jin. SAME $^{++}$ :   \n655 A Self-supervised Anatomical eMbeddings Enhanced medical image registration framework   \n656 using stable sampling and regularized transformation, Nov. 2023. arXiv:2311.14986 [cs].   \n657 [94] A. W. Toga and P. M. Thompson. The role of image registration in brain mapping. Image and   \n658 vision computing, 19(1-2):3\u201324, 2001.   \n659 [95] D. Ulyanov, A. Vedaldi, and V. Lempitsky. Deep Image Prior. International Journal of   \n660 Computer Vision, 128(7):1867\u20131888, July 2020. arXiv:1711.10925 [cs, stat].   \n661 [96] H. Uzunova, M. Wilms, H. Handels, and J. Ehrhardt. Training cnns for image registration   \n662 from few samples with model-based data augmentation. In Medical Image Computing and   \n663 Computer Assisted Intervention- MICCAI 2017: 20th International Conference, Quebec City,   \n664 QC, Canada, September 11-13, 2017, Proceedings, Part I 20, pages 223\u2013231. Springer, 2017.   \n665 [97] D. C. Van Essen, H. A. Drury, S. Joshi, and M. I. Miller. Functional and structural mapping of   \n666 human cerebral cortex: solutions are in the surfaces. Proceedings of the National Academy of   \n667 Sciences, 95(3):788\u2013795, 1998.   \n668 [98] E. Varol, A. Nejatbakhsh, R. Sun, G. Mena, E. Yemini, O. Hobert, and L. Paninski. Statistical   \n669 atlas of c. elegans neurons. In Medical Image Computing and Computer Assisted Intervention\u2013   \n670 MICCAI 2020: 23rd International Conference, Lima, Peru, October 4\u20138, 2020, Proceedings,   \n671 Part V 23, pages 119\u2013129. Springer, 2020.   \n672 [99] V. Venkatachalam, N. Ji, X. Wang, C. Clark, J. K. Mitchell, M. Klein, C. J. Tabone, J. Flor  \n673 man, H. Ji, J. Greenwood, et al. Pan-neuronal imaging in roaming caenorhabditis elegans.   \n674 Proceedings of the National Academy of Sciences, 113(8):E1082\u2013E1088, 2016.   \n675 [100] T. Vercauteren, X. Pennec, A. Perchant, and N. Ayache. Symmetric Log-Domain Diffeomor  \n676 phic Registration: A Demons-Based Approach. In D. Metaxas, L. Axel, G. Fichtinger, and   \n677 G. Sz\u00e9kely, editors, Medical Image Computing and Computer-Assisted Intervention \u2013 MICCAI   \n678 2008, Lecture Notes in Computer Science, pages 754\u2013761, Berlin, Heidelberg, 2008. Springer.   \n679 [101] T. Vercauteren, X. Pennec, A. Perchant, and N. Ayache. Diffeomorphic demons: Efficient   \n680 non-parametric image registration. NeuroImage, 45(1):S61\u2013S72, Mar. 2009.   \n681 [102] T. Vercauteren, X. Pennec, A. Perchant, N. Ayache, et al. Diffeomorphic demons using itk\u2019s   \n682 finite difference solver hierarchy. The Insight Journal, 1, 2007.   \n683 [103] A. Q. Wang, M. Y. Evan, A. V. Dalca, and M. R. Sabuncu. A robust and interpretable deep   \n684 learning framework for multi-modal registration via keypoints. Medical Image Analysis,   \n685 90:102962, 2023.   \n686 [104] Q. Wang, S.-L. Ding, Y. Li, J. Royall, D. Feng, P. Lesnar, N. Graddis, M. Naeemi, B. Facer,   \n687 A. Ho, T. Dolbeare, B. Blanchard, N. Dee, W. Wakeman, K. E. Hirokawa, A. Szafer, S. M.   \n688 Sunkin, S. W. Oh, A. Bernard, J. W. Phillips, M. Hawrylycz, C. Koch, H. Zeng, J. A. Harris,   \n689 and L. Ng. The Allen Mouse Brain Common Coordinate Framework: A 3D Reference Atlas.   \n690 Cell, 181(4):936\u2013953.e20, May 2020.   \n691 [105] Y. Wang, X. Wei, F. Liu, J. Chen, Y. Zhou, W. Shen, E. K. Fishman, and A. L. Yuille. Deep   \n692 Distance Transform for Tubular Structure Segmentation in CT Scans. In 2020 IEEE/CVF   \n693 Conference on Computer Vision and Pattern Recognition (CVPR), pages 3832\u20133841, Seattle,   \n694 WA, USA, June 2020. IEEE.   \n695 [106] J. M. Wolterink, J. C. Zwienenberg, and C. Brune. Implicit Neural Representations for   \n696 Deformable Image Registration. page 11.   \n697 [107] G. Wu, M. Kim, Q. Wang, Y. Gao, S. Liao, and D. Shen. Unsupervised deep feature learning   \n698 for deformable registration of mr brain images. In Medical Image Computing and Computer  \n699 Assisted Intervention\u2013MICCAI 2013: 16th International Conference, Nagoya, Japan, Septem  \n700 ber 22-26, 2013, Proceedings, Part II 16, pages 649\u2013656. Springer, 2013.   \n701 [108] G. Wu, M. Kim, Q. Wang, B. C. Munsell, and D. Shen. Scalable high-performance image reg  \n702 istration framework by unsupervised deep feature representations learning. IEEE transactions   \n703 on biomedical engineering, 63(7):1505\u20131516, 2015.   \n704 [109] J. Wu, D. Zou, V. Braverman, and Q. Gu. Direction matters: On the implicit bias of stochastic   \n705 gradient descent with moderate learning rate. arXiv preprint arXiv:2011.02538, 2020.   \n706 [110] Y. Wu, T. Z. Jiahao, J. Wang, P. A. Yushkevich, M. A. Hsieh, and J. C. Gee. NODEO: A   \n707 Neural Ordinary Differential Equation Based Optimization Framework for Deformable Image   \n708 Registration. arXiv:2108.03443 [cs], Feb. 2022. arXiv: 2108.03443.   \n709 [111] Z. Yang, T. Pang, and Y. Liu. A closer look at the adversarial robustness of deep equilibrium   \n710 models. Advances in Neural Information Processing Systems, 35:10448\u201310461, 2022.   \n711 [112] I. Yoo, D. G. Hildebrand, W. F. Tobin, W.-C. A. Lee, and W.-K. Jeong. ssemnet: Serial-section   \n712 electron microscopy image registration using a spatial transformer network with learned   \n713 features. pages 249\u2013257, 2017.   \n714 [113] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning (still)   \n715 requires rethinking generalization. Communications of the ACM, 64(3):107\u2013115, 2021.   \n716 [114] L. Zhang, L. Zhou, R. Li, X. Wang, B. Han, and H. Liao. Cascaded feature warping network   \n717 for unsupervised medical image registration. In 2021 IEEE 18th International Symposium on   \n718 Biomedical Imaging (ISBI), pages 913\u2013916. IEEE, 2021.   \n719 [115] S. Zhao, Y. Dong, E. I.-C. Chang, and Y. Xu. Recursive cascaded networks for unsupervised   \n720 medical image registration. In Proceedings of the IEEE/CVF International Conference on   \n721 Computer Vision (ICCV), October 2019.   \n722 [116] S. Zhao, T. Lau, J. Luo, I. Eric, C. Chang, and Y. Xu. Unsupervised 3d end-to-end medical   \n723 image registration with volume tweening network. IEEE journal of biomedical and health   \n724 informatics, 24(5):1394\u20131404, 2019. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "725 A Appendix ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "726 A.1 Implicit bias of optimization for registration ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "727 Model based systems, such as deep networks are not immune to inductive biases due to architecture,   \n728 loss functions, and optimization algorithms used to train them. Functional forms of the deep   \n729 network induce constraints on the solution space, but optimization algorithms are not excluded   \n730 from such biases either. The implicit bias for Gradient Descent is a well-studied phenomena for   \n731 overparameterized linear and shallow networks. Gradient Descent for linear systems leads to an   \n732 optimum that is in the span of the input data starting from the initialization [113, 90, 47, 74, 109].   \n733 This bias is also dependent on the chosen representation, since that defines the functional relationship   \n734 of the gradients with the parameters and inputs. This limits the reachable set of solutions by the   \n735 optimization algorithm when multiple local minima exist.   \n736 In the case of image registration, the optimization limits the space of solutions (warps) that can be   \n737 obtained by the SGD algorithm. To show this, we consider the transformation $\\varphi$ as a set of particles   \n738 in a Langrangian frame that are displaced by the optimization algorithm to align the moving image to   \n739 the fixed image. Consider a regular grid of particles, whose locations specify the warp field. Let the   \n740 location of $i$ -th particle at iteration $t$ be $\\varphi^{(t)}\\bar{(}{\\bf x}_{i})$ . For a fixed feature image $F_{f}$ , moving image $F_{m}$ and   \n741 current iterate $\\varphi^{(t)}$ , the gradient of the registration loss with respect to particle $i$ at iteration $t$ is given   \n742 by ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial C(F_{f},F_{m}\\circ\\varphi^{(t)})}{\\partial\\varphi^{(t)}(\\mathbf{x}_{i})}=C_{i}^{\\prime}(F_{f},F_{m}\\circ\\varphi^{(t)})\\nabla F_{m}(\\varphi^{(t)}(\\mathbf{x}_{i}))\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where ", "page_idx": 16}, {"type": "equation", "text": "$$\nC_{i}^{\\prime}(F_{f},F_{m}\\circ\\varphi^{(t)})=\\frac{\\partial C(F_{f},F_{m}\\circ\\varphi^{(t)})}{\\partial M(\\varphi^{(t)}(\\mathbf{x}_{i}))}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "743 is the (scalar) derivative of scalar loss $C$ with respect to the intensity of $i$ -th particle computed at   \n744 the current iterate, and $\\nabla F_{m}(\\varphi^{(t)}(\\mathbf{x}_{i}))$ is the spatial gradient of the moving image at the location of   \n745 the particle. Note that the direction of the gradient of particle $i$ is independent of the fixed image,   \n746 loss function, and location of other particles \u2013 it only depends on the spatial gradient of the moving   \n747 image at the location of the particle. This restricts the movement of a particle located at any given   \n748 location along a 1D line whose direction is the spatial gradient of the moving image at that location.   \n749 Since $F_{f}$ and $F_{m}$ are computed independently of each other (and therefore no information of $F_{f}$ and   \n750 $F_{m}$ is contained in each other), the space of solutions of $\\varphi$ is restricted by this implicit bias. This   \n751 is restrictive because the similarity function and fixed image do not influence the direction of the   \n752 gradient, and the optimization algorithm is biased towards solutions that are in the direction of the   \n753 gradient of the moving image.   \n754 We show this bias empirically \u2013 we perform multi-scale optimization algorithm using feature maps   \n755 obtained from the network. We keep track of two gradients, one obtained by the loss function, and   \n756 another obtained by the gradient of a surrogate loss $\\begin{array}{r}{C_{\\mathrm{surrogate}}(F_{m},\\varphi^{(t)})=\\dot{\\sum_{i}}\\,F_{m}(\\varphi^{(t)}(\\mathbf{x}_{i}))}\\end{array}$ . Note   \n757 that Csurrogate does not depend on the fixed image or the loss function. The gradient of $C_{\\mathrm{surrogate}}$ with   \n758 respect to the $i$ -th particle is given by $\\nabla F_{m}(\\varphi^{(t)}(\\mathbf{x}_{i}))$ . At each iteration, we compute the magnitude   \n759 of cosine similarly between the gradients of $C$ and $C_{:}$ surrogate. Fig. 5 shows that the loss converges, and   \n760 the per-pixel gradients can be predicted by $C_{\\mathrm{surrogate}}$ alone, as depicted by the magnitude and standard   \n761 deviation of cosine similarity between $C$ and Csurrogate. This limits the movement of each particle   \n762 along a 1D line in an $N$ -D space, and limits the degrees of freedom of the optimization by $N$ -fold   \n763 for $N$ -D images. Future work will aim at alleviating this implicit bias to allow for more flexible   \n764 solutions. ", "page_idx": 16}, {"type": "image", "img_path": "cEb305kE1V/tmp/42f85bb36f2a3ab3a7c337d5babd5a3976fcda81acd5a6bee25f8589e88a2834.jpg", "img_caption": ["Figure 5: Implicit bias in SGD for image registration. The plot shows the loss curves for a multi-scale optimization of two feature images. Each plot also shows the absolute cosine similarity of per-pixel gradients obtained by $C$ and $C_{\\mathrm{surrogate}}$ at each iteration. Note that over the course of optimization, the cosine similarity is always 1 \u2013 demonstrating the implicit bias of the optimization for registration. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "765 A.2 Algorithm details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "766 DIO is a learnable framework that leverages implicit differentiation of an arbitrary black-box optimiza  \n767 tion solver to learn features such that registration in this feature space corresponds to good registration   \n768 of the images and additional label maps. This additional indirection leads to learnable features that   \n769 are registration-aware, interpretable, and the framework inherits the optimization solver\u2019s versatility   \n770 to variability in the data like difference in contrast, anisotropy, and difference in sizes of the fixed and   \n771 moving images. We contrast our approach with a typical classical optimization-based registration   \n772 algorithm in Fig. 6. A classical multi-scale optimization routine indiscriminately downsamples the   \n773 intensity images, and does not retain discriminative information that is useful for registration. Since   \n774 our method is trained to maximize label alignment from all scales, multi-scale features obtained from   \n775 our method are more discriminative and registration-aware. We also compare DIO with a typical   \n776 DLIR method in Fig. 7. Note that the fixed end-to-end architecture and functional form of a deep   \n777 network subsumes the representation choice into the architecture as well, limiting its ability to switch   \n778 to arbitrary transformation representations at inference time without additional retraining. Our frame  \n779 work therefore combines the benefits of both classical (robustness to out-of-distribution datasets,   \n780 and zero-shot transfer to other optimization routines) and learning-based methods (high-fidelity,   \n781 label-aware, and registration-aware). ", "page_idx": 17}, {"type": "text", "text": "782 A.3 Implementation Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "783 For all experiments, we use downsampling scales of $1,2,4$ for the multi-scale optimization. All our   \n784 methods are implemented in PyTorch, and use the Adam optimizer for learning the parameters of the   \n785 feature network. Note that in Eq. (3), $\\varrho$ is the partial derivative of the loss function $C$ with respect   \n786 to the transformation $\\varphi$ , which contains a $\\nabla(F_{m}\\circ\\varphi)$ term, which is the backward transform of the   \n787 grid_sample operator in PyTorch. Since this operation is not implemented using PyTorch primitives,   \n788 a backward pass for the gradient operation does not exist in PyTorch. We use the gridsample_grad2   \n789 library [86] to compute the gradients of the backward pass of the grid_sample operator, used in   \n790 Eq. (3). All experiments are performed on a single NVIDIA A6000 GPU. ", "page_idx": 17}, {"type": "text", "text": "791 A.4 Toy example ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "792 Fig. 8 shows the loss curves for the toy dataset described in Section 4.1. An image-based optimization   \n793 algorithm would correspond to the green curve being a flat line at 1 due to the flat landscape of the   \n794 intensity-based loss function. ", "page_idx": 17}, {"type": "text", "text": "795 A.5 Quantitative Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "796 Table 4 shows the quantitative results of our method for out-of-distribution performance on the   \n797 IBSR18, CUMC12, and LPBA40 datasets. In 9 out of 10 cases, DIO demonstrates the best accuracy   \n798 with fairly lower standard deviations, highlighting the robustness of the model. DIO therefore serves   \n799 as a strong candidate for out-of-distribution performance, and can be used in a variety of settings   \n800 where the training and test distributions differ. ", "page_idx": 17}, {"type": "text", "text": "801 A.6 Datasets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "802 We consider four brain MRI datasets in this paper: OASIS dataset for in-distribution performance,   \n803 and LPBA40, IBSR18, and CUMC12 datasets for out-of-distribution performance [85, 1, 53, 62].   \n804 More details about the datasets are provided below.   \n805 \u2022 OASIS. The Open Access Series of Imaging Studies (OASIS) dataset contains 414 T1-weighted   \n806 brain images in Young, Middle Aged, Nondemented, and Demented Older adults. The images are   \n807 skull-stripped and bias-corrected, followed by a resampling and afine alignment to the FreeSurfer\u2019s   \n808 Talairach atlas. Label segmentations of 35 subcortical structures were obtained using automatic   \n809 segmentation using Freesurfer software. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Algorithm 1 Classical registration pipeline ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1: Input: Fixed image $I_{f}$ , Moving image $I_{m}$   \n2: Scales $\\left[s_{1},s_{2},\\ldots,s_{n}\\right]$ , Iterations $[T_{1},T_{2},\\ldots]_{n}]$ , $_n$ levels.   \n3: Initialize $\\varphi=\\mathbf{Id}_{s_{1}}$ . $\\triangleright$ Initialize warp to identity at first scale   \n4: Initialize $l=1$ . \u25b7Initialize current scale   \n5: while $l\\leq n$ do   \n6: Initialize $i=0$   \n7: Initialize $I_{f}^{l},I_{m}^{l}=\\mathrm{downsample}(I_{f},s_{l})$ , downsample(Im, sl)   \n8: while $i<\\check{T}_{l}$ do   \n9: Li = C(Ifl, Ilm \u25e6\u03c6i)   \n10: Compute \u2207\u03c6L   \n11: Update $\\varphi^{(i+1)}=\\mathrm{Optimize}(\\varphi^{i},\\nabla_{\\varphi}L_{i})$ \u25b7Optimization algorithm   \n12: $i=i+1$   \n13: end while   \n14: if $l<n$ then   \n15: $\\varphi={\\mathrm{Upsample}}(\\varphi,s_{(l+1)})$ \u25b7Upsample warp to next level   \n16: end if   \n17: $l=l+1$   \n18: end while   \nAlgorithm 2 Differentiable Implicit Optimization for Registration (Our algorithm)   \n1: Input: Fixed features $\\mathcal{F}_{f}=[F_{f}^{1},F_{f}^{2}\\cdot\\cdot\\cdot F_{f}^{n}]$ , Moving features $\\mathcal{F}_{f}=[F_{f}^{1},F_{f}^{2}\\cdot\\cdot\\cdot F_{f}^{n}]$   \n2: Scales $\\left[s_{1},s_{2},\\ldots,s_{n}\\right]$ , Iterations $[T_{1},T_{2},\\dotsc.T_{n}]$ , $_n$ levels.   \n3: Initialize $\\varphi=\\mathbf{Id}_{s_{1}}$ . $\\triangleright$ Initialize warp to identity at first scale   \n4: Initialize $l=1$ . \u25b7Initialize current scale   \n5: Outputs $=[]$ . \u25b7Save intermediate outputs for backpropagation   \n6: while $l\\leq n$ do   \n7: Initialize $i=0$   \n8: Initialize $I_{f}^{l},I_{m}^{l}=F_{f}^{l},F_{m}^{l}$   \n9: while $i<\\dot{T}_{l}$ do   \n10: Li = C(Ifl, Ilm \u25e6\u03c6i)   \n11: Compute \u2207\u03c6L   \n12: Update $\\varphi^{(i+1)}=\\mathrm{Optimize}(\\varphi^{i},\\nabla\\varphi L_{i})$ \u25b7Optimization algorithm   \n13: i = i + 1   \n14: end while   \n15: Outputs.append $\\left(\\varphi^{(T_{l})}\\right)$ \u25b7Save final warp at this level for backpropagation   \n16: if $l<n$ then   \n17: $\\varphi={\\mathrm{Upsample}}(\\varphi,s_{(l+1)})$ \u25b7Upsample warp for next level   \n18: end if   \n19: l = l + 1   \n20: end while   \n810 \u2022 LPBA40. 40 brain images and their labels are used to construct the LONI Probabilistic Brain Atlas   \n811 (LPBA40) dataset at the Laboratory of Neuroimaging (LONI) at UCLA [85]. All volumes are   \n812 preprocessed according to LONI protocols to produce skull-stripped volumes. These volumes are   \n813 aligned to the MNI305 atlas \u2013 this is relevant since existing DLIR methods may be biased towards   \n814 images that are aligned to the Talairach and Tournoux (1988) atlas which is used to align the images   \n815 in the OASIS dataset. This is followed by a custom manual labelling protocol of 56 structures from   \n816 each of the volumes. Bias correction is perfrmed using the BrainSuite\u2019s Bias Field Corrector.   \n817 \u2022 IBSR18. the Internet Brain Segmentation Repository contains 18 different brain images acquired   \n818 at different laboratories as IBSRv2.0. The dataset consists of T1-weighted brains aligned to the   \n819 Talairach and Tournoux (1988) atlas, and manually segmented into 84 labelled regions. Bias   \n820 correction of the images are performed using the \u2018autoseg\u2019 bias field correction algorithm.   \n\u2022 CUMC12. The Columbia University Medical Center dataset contains $12\\,\\mathrm{T}1$ -weighted brain images   \n822 with manual segmentation of 128 regions. The images were scanned on a $1.5\\mathrm{T}$ GE scanner, and the   \n823 images were resliced coronally to a slice thickness of $3\\mathrm{mm}$ , rotated into cardinal orientation, and   \n824 segmented by a technician trained according to the Cardviews labelling scheme. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "image", "img_path": "cEb305kE1V/tmp/a36c2ff87d3cb2046c8da3dc2dbd8e5d2d2bbcaeebea65d23f0bacb25d3b4d67.jpg", "img_caption": ["Figure 7: Comparison of typical DLIR method and our method. (a) shows the pipeline of a typical deep network. The neural network architecture takes the channelwise concatenation of the fixed and moving images as input, and outputs a warp field, which has a fixed transformation representation (SVF, free-form, B-splines, affine, etc. denoted as the blue locked layer). This representation is fixed throughout training and cannot be switched at test-time, without additional finetuning of the network. (b) shows our framework wherein the fixed and moving images are input separately into a feature extraction network that outputs multi-scale features. These features are then passed onto an iterative black-box solver than can be implicitly differentiated to backpropagate the gradients from the optimized warp field back to the feature network. This allows for a more flexible transformation representation, and the optimization solver can be switched at test-time with zero finetuning. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "image", "img_path": "cEb305kE1V/tmp/e6bdd540c8ed0a3924af53ae43f4b23b0ca4f6dc9e9554a72ed7781f3cb40534.jpg", "img_caption": ["Validation score on toy square alignment task ", "Figure 8: Loss curves for toy dataset. Plot shows three curves - the Dice score for (a) all validation image pairs, (b) image pairs that have non-zero overlap in the image space (therefore a gradient-based affine solver will recover a transform from intensity images), and (c) image pairs that have zero overlap in the image space (therefore any gradient-based solver using intensity images will fail). Our feature network recovers dense multi-scale features (see Fig. 2) which allows all subsets to be registered with $>\\!0.99$ Dice score. "], "img_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "cEb305kE1V/tmp/1550a39d1afe5db76063212ba7ce5cec8a1dceab71af187cc5af96a5505e1382.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 4: Quantitative evaluation on out-of-distribution performance on IBSR18, CUMC12, and LPBA40 datasets. We compare DIO with other state-of-the-art DLIR methods. The \u2018Dice supervision\u2019 column shows if the method is trained with label matching on the OASIS dataset. We evaluate the performance of the methods with and without isotropic and anisotropic data resampling. The results are reported as mean $\\pm$ standard deviation. $=$ First, $=$ Second, $=$ Third best result. ", "page_idx": 21}, {"type": "image", "img_path": "cEb305kE1V/tmp/cd6d2f2c2321b1a02c5b862551fe2119aaa35008c82c0e5b52cce617a4f8274f.jpg", "img_caption": ["Figure 9: Architecture details. (a) illustrates the UNet and Large Kernel U-Net (LKUNet) architecture designs, which consists of encoder blocks (red) and decoder blocks (purple) linked using skip connections. Multi-scale features are extracted from the intermediate decoder layers using a single convolutional layer. This design leads to shared features across multiple scales. UNet and LKUNet differ in the kernel parameters within each encoder and decoder blocks. (b) illustrates the \u2018Encoder-Only\u2019 versions of the same networks. The decoder path is entirely discarded, and each feature image is extracted using a separate encoder. This design enables independent learning of each multi-scale feature. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "825 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "27 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n28 paper\u2019s contributions and scope?   \n29 Answer: [Yes]   \n30 Justification: Yes. Experiments are shown on community-standard, out-of-distribution   \n31 datasets for demonstrating robustness. Zero-shot performance by switching optimizers at   \n32 test time is shown.   \n33 Guidelines:   \n34 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n35 made in the paper.   \n36 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n37 contributions made in the paper and important assumptions and limitations. A No or   \n38 NA answer to this question will not be perceived well by the reviewers.   \n39 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n40 much the results can be expected to generalize to other settings.   \n41 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n42 are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: An implicit bias of the representation and optimization algorithm is discussed in the Discussion and Appendix. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "78 Answer: [Yes]   \n79 Justification: Only Implicit Function Theorem is used with all its assumptions.   \n80 Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "891 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "892 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n893 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n894 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Code contains scripts to reproduce all experiments of the paper. Appendix contains algorithm details. Code will be published to Github upon acceptance, with additional documentation, tutorials and instructions. Data is publicly available. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "931 5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "32 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n33 tions to faithfully reproduce the main experimental results, as described in supplemental   \n34 material?   \n35 Answer: [Yes]   \n36 Justification: Code is provided in the supplemental material. Data is publicly available and   \n37 instructions are provided in the supplemental material.   \n38 Guidelines:   \n39 \u2022 The answer NA means that paper does not include experiments requiring code.   \n40 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n41 public/guides/CodeSubmissionPolicy) for more details.   \n42 \u2022 While we encourage the release of code and data, we understand that this might not be   \n43 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n44 including code, unless this is central to the contribution (e.g., for a new open-source   \n45 benchmark).   \n46 \u2022 The instructions should contain the exact command and environment needed to run to   \n47 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n48 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n49 \u2022 The authors should provide instructions on data access and preparation, including how   \n50 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n51 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n52 proposed method and baselines. If only a subset of experiments are reproducible, they   \n53 should state which ones are omitted from the script and why.   \n54 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n55 versions (if applicable).   \n56 \u2022 Providing as much information as possible in supplemental material (appended to the   \n57 paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Implementation details are provided in Appendix and supplemental material. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "970 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "971 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n972 information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: All results are reported either with an error bar of one standard deviation, or boxplots with interquartile ranges and outliers are reported. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). ", "page_idx": 25}, {"type": "text", "text": "984 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n985 call to a library function, bootstrap, etc.)   \n986 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n987 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n988 of the mean.   \n989 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n990 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n991 of Normality of errors is not verified.   \n992 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n993 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n994 error rates).   \n995 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n996 they were calculated and reference the corresponding figures or tables in the text.   \n997 8. Experiments Compute Resources   \n998 Question: For each experiment, does the paper provide sufficient information on the com  \n999 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n1000 the experiments?   \n1001 Answer: [Yes]   \n1002 Justification: Compute resources are provided in the Appendix.   \n1003 Guidelines:   \n1004 \u2022 The answer NA means that the paper does not include experiments.   \n1005 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n1006 or cloud provider, including relevant memory and storage.   \n1007 \u2022 The paper should provide the amount of compute required for each of the individual   \n1008 experimental runs as well as estimate the total compute.   \n1009 \u2022 The paper should disclose whether the full research project required more compute   \n1010 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n1011 didn\u2019t make it into the paper).   \n1012 9. Code Of Ethics   \n1013 Question: Does the research conducted in the paper conform, in every respect, with the   \n1014 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n1015 Answer: [Yes]   \n1016 Justification: No research is performed involving new human subjects, animals, or environ  \n1017 mental impact. Existing datasets comply with Code of Ethics. The proposed research is   \n1018 theoretical and computational. The proposed research has no immediate negative societal   \n1019 impact.   \n1020 Guidelines:   \n1021 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n1022 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n1023 deviation from the Code of Ethics.   \n1024 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n1025 eration due to laws or regulations in their jurisdiction).   \n1026 10. Broader Impacts   \n1027 Question: Does the paper discuss both potential positive societal impacts and negative   \n1028 societal impacts of the work performed?   \n1029 Answer: [No]   \n1030 Justification: Medical image registration has no immediate negative societal impact necessi  \n1031 tating a dedicated discussion.   \n1032 Guidelines:   \n1033 \u2022 The answer NA means that there is no societal impact of the work performed.   \n1034 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n1035 impact or why the paper does not address societal impact.   \n1036 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n1037 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n1038 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n1039 groups), privacy considerations, and security considerations.   \n1040 \u2022 The conference expects that many papers will be foundational research and not tied   \n1041 to particular applications, let alone deployments. However, if there is a direct path to   \n1042 any negative applications, the authors should point it out. For example, it is legitimate   \n1043 to point out that an improvement in the quality of generative models could be used to   \n1044 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n1045 that a generic algorithm for optimizing neural networks could enable people to train   \n1046 models that generate Deepfakes faster.   \n1047 \u2022 The authors should consider possible harms that could arise when the technology is   \n1048 being used as intended and functioning correctly, harms that could arise when the   \n1049 technology is being used as intended but gives incorrect results, and harms following   \n1050 from (intentional or unintentional) misuse of the technology.   \n1051 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n1052 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n1053 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n1054 feedback over time, improving the efficiency and accessibility of ML).   \n1055 11. Safeguards   \n1056 Question: Does the paper describe safeguards that have been put in place for responsible   \n1057 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n1058 image generators, or scraped datasets)?   \n1059 Answer: [NA]   \n1060 Justification: [NA]   \n1061 Guidelines:   \n1062 \u2022 The answer NA means that the paper poses no such risks.   \n1063 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n1064 necessary safeguards to allow for controlled use of the model, for example by requiring   \n1065 that users adhere to usage guidelines or restrictions to access the model or implementing   \n1066 safety filters.   \n1067 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n1068 should describe how they avoided releasing unsafe images.   \n1069 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n1070 not require this, but we encourage authors to take this into account and make a best   \n1071 faith effort.   \n1072 12. Licenses for existing assets   \n1073 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n1074 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n1075 properly respected?   \n1076 Answer: [Yes]   \n1077 Justification: Appropriate citations are provided for existing code and data.   \n1078 Guidelines:   \n1079 \u2022 The answer NA means that the paper does not use existing assets.   \n1080 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n1081 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n1082 URL.   \n1083 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n1084 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n1085 service of that source should be provided.   \n6 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n7 package should be provided. For popular datasets, paperswithcode.com/datasets   \n88 has curated licenses for some datasets. Their licensing guide can help determine the   \n89 license of a dataset.   \n0 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n91 the derived asset (if it has changed) should be provided.   \n92 \u2022 If this information is not available online, the authors are encouraged to reach out to   \nthe asset\u2019s creators.   \n093   \n094 13. New Assets   \n095 Question: Are new assets introduced in the paper well documented and is the documentation   \n096 provided alongside the assets?   \n097 Answer: [Yes]   \n098 Justification: Code is reasonably commented for a new reader to understand the implemen  \n099 tation.   \n100 Guidelines:   \n101 \u2022 The answer NA means that the paper does not release new assets.   \n102 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n103 submissions via structured templates. This includes details about training, license,   \n104 limitations, etc.   \n105 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n106 asset is used.   \n107 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n108 create an anonymized URL or include an anonymized zip file.   \n109 14. Crowdsourcing and Research with Human Subjects   \n10 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n11 include the full text of instructions given to participants and screenshots, if applicable, as   \n12 well as details about compensation (if any)?   \n13 Answer: [NA]   \n114 Justification: [NA]   \n15 Guidelines:   \n16 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n17 human subjects.   \n18 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n19 tion of the paper involves human subjects, then as much detail as possible should be   \n120 included in the main paper.   \n121 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n122 or other labor should be paid at least the minimum wage in the country of the data   \n123 collector.   \n24 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n25 Subjects   \n126 Question: Does the paper describe potential risks incurred by study participants, whether   \n1127 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n128 approvals (or an equivalent approval/review based on the requirements of your country or   \n1129 institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]