[{"figure_path": "cDS8WxnMVP/figures/figures_1_1.jpg", "caption": "Figure 1: Many convolution-related routines can be expressed as TNs and evaluated with einsum. We illustrate this for the input-based factor of KFAC for convolutions [KFC, 27], whose standard implementation (top) requires unfolding the input (high memory). The TN (middle) enables internal optimizations inside einsum (e.g. with contraction path optimizers like opt_einsum [66]). (Bottom) In many cases, the TN further simplifies due to structures in the index pattern, which reduces cost.", "description": "This figure demonstrates how various convolution-related operations can be represented as tensor networks (TNs) and efficiently computed using einsum.  It focuses on the input-based Kronecker factor (KFC) for KFAC (Kronecker-factored approximate curvature), a second-order optimization method. The figure compares three approaches: a standard implementation using im2col (requiring unfolding and thus high memory), a TN representation which utilizes einsum for efficient tensor multiplications (and allows for internal optimization within einsum), and a further simplified TN leveraging structural properties of the index pattern for even greater efficiency. The figure highlights the simplicity and expressiveness of the TN approach for convolutions.", "section": "1 Introduction"}, {"figure_path": "cDS8WxnMVP/figures/figures_2_1.jpg", "caption": "Figure 2: TNs of (a) 2d convolution and (b,c) connections to its matrix multiplication view. The connectivity along each dimension is explicit via an index pattern tensor \u03a0.", "description": "This figure illustrates the tensor network (TN) representation of a 2D convolution and how it relates to the matrix multiplication view.  Panel (a) shows the TN diagram for convolution, clearly depicting the connections between the input tensor (X), kernel (W), and output (Y). The index pattern tensors, \u03a0(1) and \u03a0(2), explicitly represent the connectivity along each spatial dimension. Panels (b) and (c) demonstrate how input unfolding (im2col) and kernel unfolding can be viewed as specific TN structures derived from the main convolution TN in (a). This highlights the paper's core idea of representing convolutions as TNs for easier analysis and manipulation.", "section": "2 Preliminaries"}, {"figure_path": "cDS8WxnMVP/figures/figures_4_1.jpg", "caption": "Figure 3: TN differentiation as graphical manipulation. (a) Differentiating convolution w.r.t. W is cutting it out of the diagram and yields the weight Jacobian. (b) Same procedure applied to the Jacobian w.r.t. X. (c) VJP for the weight and (d) input Jacobian (transpose convolution). Jacobians are shaded, only their contraction with V(Y) is highlighted.", "description": "This figure demonstrates the process of TN differentiation as a graphical manipulation. It shows how differentiating a tensor network (TN) diagram for convolution w.r.t weights (W) or inputs (X) results in simpler diagrams.  Panel (a) shows differentiating w.r.t W (weight Jacobian), and (b) shows the differentiation w.r.t X (input Jacobian). Panels (c) and (d) depict the vector-Jacobian products (VJPs) for weight and input, respectively, highlighting the connection to transpose convolutions.  Shaded areas represent Jacobians, and only their contractions with V(Y) are highlighted.", "section": "3 TNs for Convolution Operations"}, {"figure_path": "cDS8WxnMVP/figures/figures_5_1.jpg", "caption": "Figure 4: TNs of input-based Kronecker factors for KFAC approximations of the Fisher/GGN (no batching, no groups). The unfolded input is shaded, only additional contractions are highlighted. (a) \u03a9 (KFC/KFAC-expand) from Grosse & Martens [27], (b) \u03a9 (KFAC-reduce) from Eschenhagen et al. [23] (vectors of ones effectively amount to sums).", "description": "This figure shows the tensor network (TN) diagrams for the input-based Kronecker factors in two popular Kronecker-factored approximate curvature methods: KFAC-expand and KFAC-reduce.  The diagrams illustrate how these factors are computed using tensor network operations.  The unfolded input is represented by a shaded area, emphasizing its role in the calculations. The diagrams highlight the differences in connectivity patterns between the KFAC-expand and KFAC-reduce approaches, reflecting variations in how they approximate the Fisher/Generalized Gauss-Newton (GGN) matrix.", "section": "3.3 Kronecker-factored Approximate Curvature"}, {"figure_path": "cDS8WxnMVP/figures/figures_6_1.jpg", "caption": "Figure 1: Many convolution-related routines can be expressed as TNs and evaluated with einsum. We illustrate this for the input-based factor of KFAC for convolutions [KFC, 27], whose standard implementation (top) requires unfolding the input (high memory). The TN (middle) enables internal optimizations inside einsum (e.g. with contraction path optimizers like opt_einsum [66]). (Bottom) In many cases, the TN further simplifies due to structures in the index pattern, which reduces cost.", "description": "This figure illustrates how tensor networks (TNs) can simplify the computation of convolution-related routines, particularly the input-based factor of Kronecker-factored approximate curvature (KFAC). The figure compares three approaches: a standard implementation using unfolding (im2col), a TN implementation using einsum, and a simplified TN implementation leveraging index pattern structures.  The TN approach avoids memory-intensive unfolding, allows for internal optimizations within einsum, and can lead to further simplifications based on the structure of the index pattern, resulting in significant computational cost reduction.", "section": "1 Introduction"}, {"figure_path": "cDS8WxnMVP/figures/figures_7_1.jpg", "caption": "Figure 6: Run time ratios of TN (w/o simplifications) vs. standard implementation for dense convolutions of 9 CNNs. With simplifications, convolution and input VJP achieve median ratios slightly above 1, and the TN implementation is faster for weight VJP, KFC & KFAC-reduce. The code in Figure 1 corresponds to default, TN, and simplified TN KFC implementation.", "description": "This figure compares the performance (run time) of tensor network (TN) implementations of various convolution-related operations with their standard PyTorch counterparts.  It shows that TNs, even without simplifications, are comparable in speed for forward and input VJP, but significantly outperform standard PyTorch for weight VJP, KFC and KFAC-reduce, especially after simplifications are applied.  The comparison uses dense convolutions from nine different Convolutional Neural Networks (CNNs).", "section": "5.1 Run Time Evaluation"}, {"figure_path": "cDS8WxnMVP/figures/figures_8_1.jpg", "caption": "Figure 7: Extra memory used by the standard versus our TN implementation (simplifications enabled) of KFAC-reduce. Each point represents a convolution from 9 CNNs, clipped below by 1 MiB. TNs consistently use less memory than the standard implementation (one exception), and often no extra memory at all. We observe memory savings up to 3 GiB.", "description": "This figure compares the auxiliary memory usage of the standard implementation versus the tensor network (TN) implementation of the KFAC-reduce algorithm for various convolutional layers in nine different Convolutional Neural Networks (CNNs).  The y-axis represents the auxiliary memory used by the TN implementation, while the x-axis represents the auxiliary memory used by the standard implementation. Each point corresponds to a specific convolutional layer in one of the nine CNNs, and the values are clipped at a minimum of 1 MiB. A line of best fit is included to emphasize the trend of reduced memory usage for the TN implementation. The results indicate that the TN implementation consistently uses less memory than the standard implementation, with some cases showing a reduction of up to 3 GiB.", "section": "5 Experiments"}, {"figure_path": "cDS8WxnMVP/figures/figures_9_1.jpg", "caption": "Figure 8: Sampling spatial axes is more effective than channels on both (a) real-world and (b) synthetic data. We take the untrained All-CNN-C [68] for CIFAR-100 with cross-entropy loss, disable dropout, and modify the convolutions to use a fraction p of X when computing the weight gradient via Bernoulli-CRS. For mini-batches of size 128, we compute the deterministic gradients for all kernels, then flatten and concatenate them into a vector g; likewise for its proxy \u011d. CRS is described by (pcin, Pi1, Pi2), the keep rates along the channel and spatial dimensions. We compare channel and spatial sampling with same memory reduction, i.e. (p, 1, 1) and (1, \u221aP, \u221aP). To measure approximation quality, we use the normalized residual norm ||\u011d-g||2/||g||2 and report mean and standard deviation of 10 different model and batch initializations.", "description": "The figure shows that sampling spatial axes is more effective than channels in reducing approximation error when using Bernoulli-CRS for stochastic gradient approximation.  This is demonstrated on both real-world and synthetic data using the untrained All-CNN-C model for CIFAR-100.  The results show that for the same memory reduction, spatial sampling achieves lower error compared to channel sampling.  This suggests a more efficient strategy for reducing computational cost in stochastic gradient approximation.", "section": "5 Experiments"}, {"figure_path": "cDS8WxnMVP/figures/figures_16_1.jpg", "caption": "Figure 3: TN differentiation as graphical manipulation. (a) Differentiating convolution w.r.t. W is cutting it out of the diagram and yields the weight Jacobian. (b) Same procedure applied to the Jacobian w.r.t. X. (c) VJP for the weight and (d) input Jacobian (transpose convolution). Jacobians are shaded, only their contraction with V(Y) is highlighted.", "description": "This figure demonstrates how taking derivatives of a tensor network (TN) representation of a convolution can be done via simple graphical manipulations.  It shows four diagrams, each representing a different operation:\n\n(a) Differentiating with respect to the kernel (W) results in a cut to the network that isolates the Jacobian of the convolution with respect to the kernel.\n(b) Differentiating with respect to the input (X) similarly results in a cut showing the Jacobian with respect to the input.\n(c) The vector-Jacobian product (VJP) for the weights illustrates how the gradient flows back through the network.\n(d) The VJP for the input shows the equivalent for the input, which is the transpose convolution (also demonstrating that the transpose convolution is cleanly described as the corresponding VJP).\n\nThe shaded areas highlight the Jacobians, and the connection to the V(Y) vector emphasizes how the contraction produces the final result.", "section": "3 TNs for Convolution Operations"}, {"figure_path": "cDS8WxnMVP/figures/figures_16_2.jpg", "caption": "Figure 3: TN differentiation as graphical manipulation. (a) Differentiating convolution w.r.t. W is cutting it out of the diagram and yields the weight Jacobian. (b) Same procedure applied to the Jacobian w.r.t. X. (c) VJP for the weight and (d) input Jacobian (transpose convolution). Jacobians are shaded, only their contraction with V(Y) is highlighted.", "description": "This figure illustrates how the tensor network (TN) representation of a convolution simplifies the process of computing derivatives.  It shows that differentiating a convolution with respect to its weights (W) or inputs (X) involves a simple graphical manipulation: cutting the corresponding tensor out of the TN diagram. The resulting diagrams visually represent the weight Jacobian (\u2202Y/\u2202W), input Jacobian (\u2202Y/\u2202X), weight vector-Jacobian product (VJP for W), and input VJP (which is equivalent to a transpose convolution). Shaded areas highlight the Jacobian tensors, showing how they contract with the vector V(Y) during backpropagation.", "section": "3 TNs for Convolution Operations"}, {"figure_path": "cDS8WxnMVP/figures/figures_17_1.jpg", "caption": "Figure 3: TN differentiation as graphical manipulation. (a) Differentiating convolution w.r.t. W is cutting it out of the diagram and yields the weight Jacobian. (b) Same procedure applied to the Jacobian w.r.t. X. (c) VJP for the weight and (d) input Jacobian (transpose convolution). Jacobians are shaded, only their contraction with V(Y) is highlighted.", "description": "This figure demonstrates how tensor network (TN) diagrams can be used to visualize and simplify the process of differentiating convolutions.  It shows how taking the derivative with respect to weights (W) or inputs (X) can be represented graphically as cutting those parts from the TN diagrams. The resulting diagrams represent the Jacobians (weight Jacobian, input Jacobian), which capture the sensitivity of the output to changes in weights and inputs. The figure also illustrates the computation of vector-Jacobian products (VJPs), a crucial step in backpropagation, by showing how they can be obtained by contracting the Jacobians with the gradient of the loss function (V(Y)).", "section": "3 TNs for Convolution Operations"}, {"figure_path": "cDS8WxnMVP/figures/figures_17_2.jpg", "caption": "Figure 1: Many convolution-related routines can be expressed as TNs and evaluated with einsum. We illustrate this for the input-based factor of KFAC for convolutions [KFC, 27], whose standard implementation (top) requires unfolding the input (high memory). The TN (middle) enables internal optimizations inside einsum (e.g. with contraction path optimizers like opt_einsum [66]). (Bottom) In many cases, the TN further simplifies due to structures in the index pattern, which reduces cost.", "description": "This figure demonstrates how convolution-related operations can be represented and computed using tensor networks (TNs) and the einsum function.  It uses the example of the Kronecker-factored curvature (KFC) for convolutions, comparing three different approaches: a standard implementation (requiring memory-intensive unfolding of the input), a TN representation (allowing for internal optimizations within einsum), and a simplified TN representation (further reducing computational cost due to exploiting structural patterns in the index pattern).", "section": "1 Introduction"}, {"figure_path": "cDS8WxnMVP/figures/figures_19_1.jpg", "caption": "Figure 2: TNs of (a) 2d convolution and (b,c) connections to its matrix multiplication view. The connectivity along each dimension is explicit via an index pattern tensor \u03a0.", "description": "This figure illustrates the tensor network (TN) representations of a 2D convolution and how it relates to matrix multiplication.  Panel (a) shows the TN representation of the convolution, highlighting the connections between input, output, and kernel tensors through index pattern tensors. Panels (b) and (c) illustrate how the input and kernel tensors can be unfolded and reshaped to allow for matrix multiplication.  The figure emphasizes the use of index pattern tensors (\u03a0) to explicitly define the connectivity within each dimension, making the TN representation a powerful tool for analysis and manipulation of convolutions.", "section": "2 Preliminaries"}, {"figure_path": "cDS8WxnMVP/figures/figures_20_1.jpg", "caption": "Figure 2: TNs of (a) 2d convolution and (b,c) connections to its matrix multiplication view. The connectivity along each dimension is explicit via an index pattern tensor \u03a0.", "description": "This figure illustrates the tensor network (TN) representation of a 2D convolution.  Panel (a) shows the TN diagram of a 2D convolution, highlighting the connections between the input tensor (X), the kernel (W), and the output tensor (Y). The connections are explicitly represented by index pattern tensors \u03a0(1) and \u03a0(2), which encode the spatial relationships between the input, kernel, and output along the respective dimensions (I1, I2, O1, O2, K1, K2). Panel (b) shows how the input is unfolded (im2col) to form a matrix, explicitly illustrating the connectivity. Panel (c) shows an analogous unfolding for the kernel, again emphasizing the connectivity in the matrix multiplication. This depiction helps bridge the intuitive understanding of a convolution's sliding window approach with a more formal tensor network analysis.", "section": "2 Preliminaries"}, {"figure_path": "cDS8WxnMVP/figures/figures_21_1.jpg", "caption": "Figure 3: TN differentiation as graphical manipulation. (a) Differentiating convolution w.r.t. W is cutting it out of the diagram and yields the weight Jacobian. (b) Same procedure applied to the Jacobian w.r.t. X. (c) VJP for the weight and (d) input Jacobian (transpose convolution). Jacobians are shaded, only their contraction with V(Y) is highlighted.", "description": "This figure illustrates how to perform differentiation in a tensor network.  It shows that differentiating a tensor network diagram is equivalent to a simple graphical manipulation.  The diagrams (a) through (d) illustrate how to compute weight Jacobian, input Jacobian, and Jacobian-vector products (VJPs) graphically by removing parts of the diagram.  Shaded areas represent the Jacobians, and their contraction with V(Y) is highlighted.", "section": "3 TNs for Convolution Operations"}, {"figure_path": "cDS8WxnMVP/figures/figures_21_2.jpg", "caption": "Figure B9: TNs of the (a) forward pass, (b, c) Jacobians, and (d, e) VJPs with batch axis and channel groups. They generalize Figures 2 and 3 from the main text. For the VJPs, the Jacobians are shaded.", "description": "This figure illustrates the tensor network (TN) diagrams for various operations related to 2D convolutions, including the forward pass, Jacobians (weight and input), and vector-Jacobian products (VJPs, weight and input).  The diagrams incorporate batching and channel groups, adding complexity compared to the simpler diagrams in Figures 2 and 3. Shading is used in the VJP diagrams to highlight the Jacobian tensors. The figure extends the concepts presented in the main text to more realistic scenarios by incorporating these additional factors.", "section": "B Visual Tour of Tensor Network Operations for Convolutions"}, {"figure_path": "cDS8WxnMVP/figures/figures_21_3.jpg", "caption": "Figure 3: TN differentiation as graphical manipulation. (a) Differentiating convolution w.r.t. W is cutting it out of the diagram and yields the weight Jacobian. (b) Same procedure applied to the Jacobian w.r.t. X. (c) VJP for the weight and (d) input Jacobian (transpose convolution). Jacobians are shaded, only their contraction with V(Y) is highlighted.", "description": "This figure demonstrates how to compute the derivatives of a convolutional layer using tensor network diagrams.  Panel (a) shows how differentiating with respect to the weights (W) is represented graphically; cutting the weight tensor from the network yields the weight Jacobian. Panel (b) shows the same process for differentiating with respect to the input (X), resulting in the input Jacobian. Panels (c) and (d) illustrate the vector-Jacobian product (VJP) for weights and the input Jacobian (related to transpose convolution), respectively.  The Jacobians are highlighted in gray, and their contractions with V(Y) are emphasized.", "section": "3 TNs for Convolution Operations"}, {"figure_path": "cDS8WxnMVP/figures/figures_21_4.jpg", "caption": "Figure 3: TN differentiation as graphical manipulation. (a) Differentiating convolution w.r.t. W is cutting it out of the diagram and yields the weight Jacobian. (b) Same procedure applied to the Jacobian w.r.t. X. (c) VJP for the weight and (d) input Jacobian (transpose convolution). Jacobians are shaded, only their contraction with V(Y) is highlighted.", "description": "This figure demonstrates how tensor network (TN) diagrams simplify the process of differentiation in convolutions.  Each sub-figure (a-d) shows a TN representation of a different autodiff operation: (a) differentiating the convolution with respect to the kernel (weight Jacobian), (b) differentiating with respect to the input (input Jacobian), (c) calculating the vector-Jacobian product (VJP) for the weights, and (d) calculating the VJP for the input (transpose convolution). The shaded areas represent the Jacobian tensors, and their contractions with the vector V(Y) are highlighted.", "section": "3 TNs for Convolution Operations"}, {"figure_path": "cDS8WxnMVP/figures/figures_23_1.jpg", "caption": "Figure 1: Many convolution-related routines can be expressed as TNs and evaluated with einsum. We illustrate this for the input-based factor of KFAC for convolutions [KFC, 27], whose standard implementation (top) requires unfolding the input (high memory). The TN (middle) enables internal optimizations inside einsum (e.g. with contraction path optimizers like opt_einsum [66]). (Bottom) In many cases, the TN further simplifies due to structures in the index pattern, which reduces cost.", "description": "This figure illustrates how to represent and compute Kronecker factors for convolutions using Tensor Networks (TNs). The standard method (top) uses im2col, which requires unfolding the input tensor, leading to high memory usage.  The TN representation (middle) allows for internal optimizations within the einsum function (especially using contraction path optimizers like opt_einsum), improving efficiency.  The bottom part shows that, in many situations, the TN can be simplified even further due to the specific structure of the index patterns of convolutions, again resulting in memory and computational savings.", "section": "1 Introduction"}, {"figure_path": "cDS8WxnMVP/figures/figures_29_1.jpg", "caption": "Figure 1: Many convolution-related routines can be expressed as TNs and evaluated with einsum. We illustrate this for the input-based factor of KFAC for convolutions [KFC, 27], whose standard implementation (top) requires unfolding the input (high memory). The TN (middle) enables internal optimizations inside einsum (e.g. with contraction path optimizers like opt_einsum [66]). (Bottom) In many cases, the TN further simplifies due to structures in the index pattern, which reduces cost.", "description": "This figure illustrates how tensor networks (TNs) and einsum can simplify the computation of convolution-related routines. The example shown is the input-based factor of Kronecker-factored curvature (KFC) for convolutions.  The standard implementation is memory-intensive because it requires unfolding the input. The TN representation allows for internal optimizations within the einsum function, such as contraction path optimization. Finally, the TN representation can be further simplified based on structural properties of the index patterns, leading to additional computational savings.", "section": "1 Introduction"}, {"figure_path": "cDS8WxnMVP/figures/figures_30_1.jpg", "caption": "Figure 1: Many convolution-related routines can be expressed as TNs and evaluated with einsum. We illustrate this for the input-based factor of KFAC for convolutions [KFC, 27], whose standard implementation (top) requires unfolding the input (high memory). The TN (middle) enables internal optimizations inside einsum (e.g. with contraction path optimizers like opt_einsum [66]). (Bottom) In many cases, the TN further simplifies due to structures in the index pattern, which reduces cost.", "description": "This figure demonstrates how Kronecker-factored curvature (KFC) approximation for convolutions can be represented as tensor networks (TNs) and efficiently evaluated using einsum.  It compares three approaches: a standard implementation using im2col (requiring input unfolding and thus high memory), a TN representation which allows for einsum's internal optimizations, and a simplified TN representation that leverages index pattern structures to reduce computational cost.  The figure visually illustrates these methods with diagrams and highlights the memory and computational advantages of using TNs.", "section": "1 Introduction"}, {"figure_path": "cDS8WxnMVP/figures/figures_31_1.jpg", "caption": "Figure 6: Run time ratios of TN (w/o simplifications) vs. standard implementation for dense convolutions of 9 CNNs. With simplifications, convolution and input VJP achieve median ratios slightly above 1, and the TN implementation is faster for weight VJP, KFC & KFAC-reduce. The code in Figure 1 corresponds to default, TN, and simplified TN KFC implementation.", "description": "This figure presents a comparison of the runtime performance between a Tensor Network (TN) implementation and a standard PyTorch implementation for various convolution operations. The results are shown for nine different Convolutional Neural Networks (CNNs), focusing on dense convolutions. The x-axis represents different operations (Forward, Input VJP, Weight VJP, KFC, KFAC-reduce), and the y-axis shows the ratio of TN runtime to the standard PyTorch runtime. The figure illustrates that without simplifications, the TN implementation is generally slower than PyTorch's implementation; however, when simplifications are applied, the TN's performance improves significantly, becoming comparable or even faster than PyTorch's in several cases.", "section": "5.1 Run Time Evaluation"}, {"figure_path": "cDS8WxnMVP/figures/figures_34_1.jpg", "caption": "Figure 6: Run time ratios of TN (w/o simplifications) vs. standard implementation for dense convolutions of 9 CNNs. With simplifications, convolution and input VJP achieve median ratios slightly above 1, and the TN implementation is faster for weight VJP, KFC & KFAC-reduce. The code in Figure 1 corresponds to default, TN, and simplified TN KFC implementation.", "description": "This figure compares the performance of the Tensor Network (TN) implementation of convolutions against the standard PyTorch implementation for dense convolutions across 9 different Convolutional Neural Networks (CNNs).  It shows run-time ratios, indicating how much faster or slower the TN method is compared to the standard method. Two sets of results are presented: one without any simplifications applied to the TN approach and another with simplifications enabled. The results show that with simplifications, the TN method is either comparable to, or faster than, the standard PyTorch implementation for most operations, highlighting the efficiency gains achieved.", "section": "5.1 Run Time Evaluation"}, {"figure_path": "cDS8WxnMVP/figures/figures_37_1.jpg", "caption": "Figure 6: Run time ratios of TN (w/o simplifications) vs. standard implementation for dense convolutions of 9 CNNs. With simplifications, convolution and input VJP achieve median ratios slightly above 1, and the TN implementation is faster for weight VJP, KFC & KFAC-reduce. The code in Figure 1 corresponds to default, TN, and simplified TN KFC implementation.", "description": "This figure compares the performance of the Tensor Network (TN) implementation of dense convolutions against the standard PyTorch implementation.  It shows run time ratios for various operations including forward pass, input VJP, weight VJP, KFC, and KFAC-reduce.  The results indicate that with simplifications, the TN implementation is competitive with or even outperforms PyTorch for some operations, highlighting the efficiency of the TN approach, especially for more complex computations like weight VJP, KFC, and KFAC-reduce.", "section": "5.1 Run Time Evaluation"}, {"figure_path": "cDS8WxnMVP/figures/figures_40_1.jpg", "caption": "Figure 6: Run time ratios of TN (w/o simplifications) vs. standard implementation for dense convolutions of 9 CNNs. With simplifications, convolution and input VJP achieve median ratios slightly above 1, and the TN implementation is faster for weight VJP, KFC & KFAC-reduce. The code in Figure 1 corresponds to default, TN, and simplified TN KFC implementation.", "description": "This figure presents the performance comparison of the Tensor Network (TN) approach with the standard implementation for computing the Kronecker factors in KFAC (KFC and KFAC-reduce).  It shows the run-time ratios for various operations in dense convolutions across 9 different Convolutional Neural Networks (CNNs). Two sets of results are shown: those without TN simplifications (blue boxes) and those with simplifications (magenta boxes). The results indicate that TN implementations with simplifications are comparable or faster than the standard approach for most operations, demonstrating efficiency gains for weight VJP and the approximation of KFAC.", "section": "5.1 Run Time Evaluation"}, {"figure_path": "cDS8WxnMVP/figures/figures_43_1.jpg", "caption": "Figure 6: Run time ratios of TN (w/o simplifications) vs. standard implementation for dense convolutions of 9 CNNs. With simplifications, convolution and input VJP achieve median ratios slightly above 1, and the TN implementation is faster for weight VJP, KFC & KFAC-reduce. The code in Figure 1 corresponds to default, TN, and simplified TN KFC implementation.", "description": "This figure shows the performance comparison between the standard PyTorch implementation and the proposed Tensor Network (TN) implementation for dense convolutions across 9 different Convolutional Neural Networks (CNNs).  The results are presented as the ratio of TN run time to PyTorch run time, with lower ratios indicating that TN is faster. Two sets of box plots are shown, one for the TN implementation without simplifications (showing that TN is often slower without optimizations) and one for the TN implementation with the simplifications described in the paper.  The simplifications lead to improved performance, particularly for weight VJP, KFC and KFAC-reduce.  The figure also highlights the equivalence between three different implementations of the KFC (Kronecker-factored curvature) routine (default, TN, and simplified TN) to show how the TN approach is superior.", "section": "F Run Time Evaluation Details (GPU)"}]