[{"type": "text", "text": "GSDF: 3DGS Meets SDF for Improved Neural Rendering and Reconstruction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mulin $\\mathbf{Y_{u}}^{1*}$ Tao $\\mathbf{L}\\mathbf{u}^{1*}$ Linning $\\mathbf{Xu^{2}}$ Lihan Jiang4,1 Yuanbo Xiangli3\u2020 Bo Dai5,1 1Shanghai Artificial Intelligence Laboratory, 2The Chinese University of Hong Kong, 3Cornell University, 4 University of Science and Technology of China, 5 The University of Hong Kong ", "page_idx": 0}, {"type": "image", "img_path": "r6V7EjANUK/tmp/b472308f8ccdb93ef32b2d6147b9d6a99b5a30f61b75e43fdc2d3d303edad4dc.jpg", "img_caption": ["Figure 1: Conceptual Illustration of GSDF. Rendering and reconstruction tasks have traditionally involved trade-offs in neural representation methods. While 3D-GS achieves high-fidelity viewdependent rendering, it often compromises on geometric accuracy. Recent approaches [13, 10] use explicit regularization to align Gaussian primitives near surfaces, but this can reduce model capacity for high-fidelity visuals. Our GSDF introduces a dual-branch framework with specialized GS- and SDF-branches for rendering and geometry tasks. We propose three mutual guidances (detailed in Sec. 3.2) to enhance the quality of both tasks. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Representing 3D scenes from multiview images remains a core challenge in computer vision and graphics, requiring both reliable rendering and reconstruction, which often confilcts due to the mismatched prioritization of image quality over precise underlying scene geometry. Although both neural implicit surfaces and explicit Gaussian primitives have advanced with neural rendering techniques, current methods impose strict constraints on density fields or primitive shapes, which enhances the affinity for geometric reconstruction at the sacrifice of rendering quality. To address this dilemma, we introduce GSDF, a dual-branch architecture combining 3D Gaussian Splatting (3DGS) and neural Signed Distance Fields (SDF). Our approach leverages mutual guidance and joint supervision during the training process to mutually enhance reconstruction and rendering. Specifically, our method guides the Gaussian primitives to locate near potential surfaces and accelerates the SDF convergence. This implicit mutual guidance ensures robustness and accuracy in both synthetic and real-world scenarios. Experimental results demonstrate that our method boosts the SDF optimization process to reconstruct more detailed geometry, while reducing floaters and blurry edge artifacts in rendering by aligning Gaussian primitives with the underlying geometry. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Recent advancements in neural scene representations have showcased superior rendering capabilities [23, 24, 17], these advancements have sparked significant interest in neural surface reconstruction [39, 31, 19, 32, 10, 13]. They seek to develop unified representations, which simultaneously support highfidelity rendering and accurate geometric reconstruction, to better support downstream applications such as robotics [16, 27], physical simulations [35, 8], and XR applications [36, 15]. ", "page_idx": 1}, {"type": "text", "text": "Although both neural implicit surfaces and explicit Gaussian primitives have advanced with neural rendering techniques, current methods often suffer from a mismatched prioritization between appearance and geometry. Imposing regularization or constraints on the unified representation may boost one task, while unavoidably deteriorating the performance of the other. Recent approaches [10, 5], have explored using flat Gaussian primitives for surface modeling. Enforcing binary opacity [10] and jointly learned NeuS models [5] to regularize attributes have led to degraded rendering quality due to primitive constraints. However, works such as Adaptive Shell [34], Binary Occupancy Field [26], and Scaffold-GS [21] have demonstrated that incorporating geometry guidance significantly enhances rendering quality by producing well-regularized spatial structures with hybrid representations. ", "page_idx": 1}, {"type": "text", "text": "Building on these insights, we propose a synchronously optimized dual-branch system, addressing rendering and reconstruction with hybrid representations, to bypass the confilcts and further achieve mutual enhancements, as illustrated in Fig. 1. Our method leverages mutual guidance and joint supervision to balance rendering and reconstruction without compromising their intrinsic advantages. Specifically, our system features a 3D Gaussian Splatting (3DGS) branch for rendering and a Signed Distance Field (SDF) branch for surface reconstruction. The key innovations of our approach include: (1) Utilizing rasterized depth from the GS-branch to guide ray sampling in the SDF-branch, enhancing volume rendering efficiency and avoiding local minima. (2) Applying SDF-guidance for density control in 3DGS, directing the growth of 3D Gaussians in near-surface regions and pruning elsewhere. (3) Aligning geometry properties (depth and normal) estimated from both branches. This unified system overcomes the limitations inherent in each method due to differences in rendering techniques (rasterization vs. dense ray sampling) and scene representation (discrete primitives vs. continuous fields). Moreover, our framework is designed to accommodate future advancements in each branch. ", "page_idx": 1}, {"type": "text", "text": "Extensive experiments demonstrate that our dual-branch design allows: 1) The GS-branch to generate structured primitives closely aligned with the surface, reducing floaters and improving detail and edge quality in view synthesis. 2) Accelerated convergence in the SDF-branch, resulting in superior geometric accuracy and enhanced surface details. Our results confirm that an integrated blend of both reconstruction and rendering is achievable, enhancing overall robustness and performance. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Advanced Neural Rendering Techniques. Neural Radiance Fields (NeRFs) [23] have achieved remarkable photorealistic rendering with view-dependent effects. They use Multi-Layer Perceptrons (MLPs) to map 3D spatial locations to color and density, which are then aggregated into pixel colors through neural volumetric rendering. This approach excels in novel view synthesis but is slow due to the need for extensive point sampling along each ray and the global MLP architecture\u2019s scalability limitations. Recent research [24, 37, 4, 9] has shifted the learning burden to locally optimized spatial features, enhancing scalability. Alternative methods like rasterizing geometric primitives (e.g., point clouds) [2, 41] offer efficiency and flexibility but struggle with discontinuities and outliers. Augmenting points with neural features and incorporating volumetric rendering [38] improves quality but adds computational overhead. Recently, 3D Gaussian Splatting (3DGS) [17] revolutionized neural rendering by using anisotropic 3D Gaussians as primitives, sorted by depth and rasterized onto a 2D screen with $\\alpha$ -blending. This method achieves high-quality, detailed results at real-time frame rates. Scaffold-GS [21] enhanced 3DGS by introducing a hierarchical structure that aligns anchors with scene geometry, improving rendering quality and memory efficiency. However, these methods prioritize view synthesis over accurate scene geometry, often resulting in fuzzy volumetric density fields that hinder the extraction of high-quality surfaces ", "page_idx": 1}, {"type": "text", "text": "Neural Surface Reconstruction. The success of neural rendering has sparked significant interest in neural surface reconstruction [25, 39, 31, 19, 32, 40]. These methods typically use coordinate-based networks to encode scene geometry through occupancy fields or Signed Distance Field (SDF) values. ", "page_idx": 1}, {"type": "image", "img_path": "r6V7EjANUK/tmp/86c85f2f78484b30efabec6aa23a2c520d3c13fd9a406a73afd69c7caea734e0.jpg", "img_caption": ["Figure 2: Overview of Dual-branch Guidance. Our dual-branch framework includes a GS-branch for rendering and an SDF-branch for learning neural surfaces. This design preserves the efficiency and fidelity of Gaussian primitive for rendering [17, 21] while accurately approximating scene surfaces from an SDF field adapted from NeuS [31]. Specifically: (1) The GS-branch renders depth maps to guide SDF-branch ray sampling, querying absolute SDF values $|s|$ and sampling points within $2k|s|$ (e.g., $k=4$ ). (2) Predicted SDF values guide GS-branch density control, growing Gaussians near surfaces and pruning deviated ones. (3) Mutual geometry consistency is enforced by comparing depth and normal maps from both branches, ensuring coherent alignment between Gaussians and surfaces. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "While MLPs with volume rendering produce smooth and complete surfaces, they often lack highfidelity details and are slow to optimize. Recent works [19, 34, 26] have leveraged multi-resolution hashed feature grids from iNGP [24] to enhance representation power, achieving state-of-the-art results. Hybrid approaches combining surface and volume rendering [30, 34, 26] have also emerged to maintain rendering speed and quality. More recently, methods have explored integrating 3D Gaussian Splatting for surface learning. For example, SuGaR [10] aligns 3D Gaussians with potential surfaces by approximating them to 2D planar primitives with binary opacity. Similarly, 2D Gaussian Splatting [13] replaces 3D Gaussians with 2D Gaussians for more accurate ray-splat intersection and regularizes depth maps to enforce a more condensed Gaussian primitive distribution. Another approach [7] optimizes Gaussian surfels constrained by a normal prior from a pre-trained monocular estimator, enhancing surface representation. The concurrent work NeuSG [5] jointly optimizes 3D Gaussian Splatting with NeuS [31] by encouraging flat 3D Gaussians as well, with normals aligned with NeuS predictions. 3DGSR [22] also aligns the geometry from 3DGS with a SDF field. Despite advances in neural surface reconstruction, a fidelity gap persists due to the explicit regularization of Gaussian primitives, which hampers rendering quality compared to 3DGS. Our method optimizes primitives aligned with surfaces using geometric clues, enhancing structural integrity without losing representational power. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We present a dual-branch framework with GS- and SDF-branches, jointly optimized to provide mutual guidance, as shown in Fig. 2. Our approach aligns Gaussian primitives with surfaces using geometric clues rather than constraining their shapes, enhancing both structure and representational power. These improved primitives yield more accurate and detailed scene surfaces. ", "page_idx": 2}, {"type": "text", "text": "In Sec.3.1, we briefly review neural rendering methods[17, 21] and neural implicit surface representations [31, 19]. Sec.3.2 details our framework and the three proposed mutual guidance techniques. Sec.3.3 outlines our training strategy and loss design. ", "page_idx": 2}, {"type": "text", "text": "3.1 Preliminary ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3D Gaussian Splatting. 3D Gaussian Splatting (3DGS) [17] represents scenes using 3D Gaussian primitives, achieving state-of-the-art rendering quality and speed with a tile-based rasterizer. Each Gaussian is defined by its mean $\\mu\\in\\mathbb{R}^{3}$ and covariance $\\Sigma\\in\\mathbb{R}^{3\\times3}$ with $G(x)=e^{-{\\frac{1}{2}}(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu)}$ , where $x$ is a 3D position in the scene. The rasterizer efficiently sorts and $\\alpha$ -blends Gaussians onto the 2D image plane [43]. The adaptive control of Gaussians based on gradients is also critical for improving scene representation accuracy, reducing redundancy, and refining Gaussians to better match the underlying geometry and appearance. ", "page_idx": 3}, {"type": "text", "text": "Scaffold-GS [21] improves 3DGS by enhancing scene structure fidelity and robustness to viewdependent effects. It uses a hierarchical 3D Gaussian representation, with anchor points encoding local scene information and generating local neural Gaussians. Each anchor, optimized with a feature vector, predicts the color, center, variance, and opacity of the neural Gaussians. ", "page_idx": 3}, {"type": "text", "text": "Neural Implicit SDFs. NeuS [31] integrates signed distance functions (SDFs) with NeRF\u2019s volumetric rendering, converting SDF values into volume densities to maintain geometric accuracy. SDF values are converted to opacities using a logistic function $\\begin{array}{r}{\\alpha_{i}=\\operatorname*{max}\\left(\\frac{\\Phi_{s}(f(\\mathbf{x}_{i}))-\\Phi_{s}(f(\\mathbf{x}_{i+1}))}{\\Phi_{s}(f(\\mathbf{x}_{i}))},0\\right)}\\end{array}$ The color of a ray $r$ is also calculated by accumulating weighted colors of the sample points: $\\begin{array}{r}{\\mathbf{C}(\\mathbf{r})=\\sum_{i=1}^{P}T_{i}\\alpha_{i}\\mathbf{c}_{i}}\\end{array}$ , and $\\begin{array}{r}{T_{i}\\,=\\,\\exp\\left(-\\sum_{j=1}^{i-1}\\alpha_{j}\\delta_{j}\\right)}\\end{array}$ , where $\\delta_{j}$ is the interval between sampled points. Inspired by [24], [11] introduces a multi-resolution hash grid to enhance representation power and accelerate rendering and training. ", "page_idx": 3}, {"type": "text", "text": "3.2 GSDF: Dual-branch for Rendering and Reconstruction ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To bridge the gap between rendering and geometric accuracy, we propose a novel approach that combines the strengths of Gaussian-based and SDF-based methods. By leveraging the high-quality rendering capabilities of 3DGS and the precise geometric representation of SDFs, our method aims to achieve superior results in both tasks. ", "page_idx": 3}, {"type": "text", "text": "As illustrated in Fig. 2, our dual-branch design integrates a GS-branch and an SDF-branch. We use Scaffold-GS [21] and NeuS [31] with adapted hash-encoding implementation [11] as the backbones, chosen for their effectiveness and simplicity. Importantly, our framework is versatile and can be easily adapted to incorporate future advanced methods for each branch. ", "page_idx": 3}, {"type": "text", "text": "3.2.1 $\\mathbf{GS}\\rightarrow\\mathbf{SDF}$ : Depth Guided Ray Sampling ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To address the computational expenses of ray-sampling, techniques such as hierarchical sampling [23], occupancy grids [20, 4, 24], early stopping [24], and proposal networks [3, 26] are widely used with CUDA accelerations. When depth maps are available, either from sensors or monocular estimation, samples can be strategically placed around surface regions, which is crucial for effective optimization of the Signed Distance Field (SDF). Unlike neural implicit SDF-based methods that rely on their own predicted SDF values for ray sampling [31, 19, 28], we employ the GS-branch to provide surface proximity, avoiding the chicken-and-egg dilemma. Inspired by [37], where a more efficient branch provides coarse geometry guidance, we leverage depth maps from the GS-branch to refine the ray sampling range for the SDF-branch. While Gaussian primitives may be less precise, they are efficient and flexible, offering sufficient geometric clues without significant overhead. ", "page_idx": 3}, {"type": "text", "text": "Concretely, for a ray emitted from a camera center $\\vec{o}$ in the direction $\\vec{v}$ , the depth value $D$ from the GS-branch is: ", "page_idx": 3}, {"type": "equation", "text": "$$\nD=\\sum_{i\\in N}d_{i}\\sigma_{i}\\prod_{j=1}^{i-1}(1-\\sigma_{j}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $N$ is the number of 3D Gaussians encountered, $\\sigma_{i}$ is the opacity, and $d_{i}$ is the distance from the $i$ -th Gaussian to the camera. For SDF-branch optimization, points are sampled around ${\\vec{o}}+D\\cdot{\\vec{v}}$ The sampling range adapts based on predicted SDF values $s$ at various depths: ", "page_idx": 3}, {"type": "equation", "text": "$$\ns=\\mathcal{F}_{s}(\\vec{o}+D\\cdot\\vec{v}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ${\\mathcal{F}}_{s}$ is a two-layer MLP predicting the SDF value at a given position. The sampling range is defined as $r=[\\vec{o}+(D-k|s|\\bar{)}\\cdot\\vec{v},\\vec{o}+\\bar{(}D+k|s|)\\cdot\\vec{v}]$ . As shown in Fig. 2, inspired from NeRF\u2019s ", "page_idx": 3}, {"type": "text", "text": "hierarchical sampling strategy [23], we use coarse and fine ranges with $k=3$ and $k=1$ , respectively, we uniformly sample $M$ points along the ray within each range. ", "page_idx": 4}, {"type": "text", "text": "3.2.2 $\\mathbf{S}\\mathbf{D}\\mathbf{F}\\rightarrow\\mathbf{G}\\mathbf{S}$ : Geometry-aware Gaussian Density Control ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Many previous and concurrent methods [13, 10, 7] have attempted to learn scene surfaces from 3DGS by flattening 3D Gaussians along the normal direction and enforcing nearly binary opacity, treating them like surface primitives similar to mesh triangles. However, this approach often leads to degraded rendering quality and incomplete surfaces. Our approach, instead of putting additional regularization on the 3D Gaussian primitives, enhances the distribution of Gaussian primitives using a geometry-aware density control strategy. Building on the original gradient-based density control, we leverage the zero-level set of the SDF-branch to determine the proximity of Gaussian primitives to the surface. By querying the SDF-branch with the positions of Gaussian primitives, we are able to identify close-to-surface Gaussian primitives (i.e. with smaller absolute SDF values), allowing for more precise control over the placement and density of Gaussians. ", "page_idx": 4}, {"type": "text", "text": "Growing Operator. For each Gaussian primitive at position $c$ , we obtain its SDF value $s=\\mathcal{F}_{s}(c)$ from the SDF-branch. The growth criteria for Gaussians are then defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\epsilon_{g}=\\nabla_{g}+\\omega_{g}\\mu(s),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\nabla_{g}$ is the averaged gradient of Gaussian primitives accumulated over $K$ training iterations, as in Scaffold-GS [21]. $\\mu(s)=\\exp\\left(-s^{2}/(2\\sigma^{2})\\right)$ is a Gaussian function converting the SDF value to a positive factor, decreasing monotonically with distance from the zero level set. $\\omega_{g}$ controls the influence of geometric guidance. New Gaussian primitives are added if $\\epsilon_{g}$ are greater than a predefined $\\tau_{g}$ . ", "page_idx": 4}, {"type": "text", "text": "Pruning Operator. Beyond the original opacity-based criteria, we prune Gaussian primitives far from the surface, indicated by large SDF values. The pruning criteria are: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\epsilon_{p}=\\sigma_{a}-\\omega_{p}(1-\\mu(s)),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\sigma_{a}$ is the aggregated opacity over $K$ iterations. The weight $\\omega_{p}$ balances the contributions of transparency and SDF. Primitives with $\\epsilon_{p}$ less than a predefined $\\tau_{p}$ are pruned. ", "page_idx": 4}, {"type": "text", "text": "3.2.3 $\\mathbf{GS}\\leftrightarrow\\mathbf{SDF}$ : Mutual Geometry Supervision ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To enhance both rendering and reconstruction outcomes, our framework incorporates mutual supervision between the two branches, using depth and normal maps as key geometric features to facilitate this interconnection. ", "page_idx": 4}, {"type": "text", "text": "For the SDF-branch, a per-view depth map $D_{s}$ is rendered using volumetric rendering principles, and a normal map $N_{s}$ is derived by rendering the gradients of the SDF volumetrically. For the GS-branch, we compute a per-view depth map $D_{g}$ following Eq. 1. The normal of each 3D Gaussian is determined by the direction of its smallest scaling factor, as in [10, 5, 6]. To render the normal map for each camera view, we accumulate the normals of the 3D Gaussians using $\\alpha$ -blending $\\begin{array}{r}{N_{g}=\\sum_{i\\in N}n_{i}\\sigma_{i}\\prod_{j=1}^{i-1}(1-\\sigma_{j})}\\end{array}$ , where $n_{i}$ is the estimated normal of the $i$ -th 3D Gaussian. ", "page_idx": 4}, {"type": "text", "text": "This mutual geometry supervision ensures that the depth and normal maps from both branches are consistently aligned. This consistent alignment is crucial for maintaining coherence between the rendering and reconstruction processes, as it helps to prevent discrepancies that could lead to artifacts and inaccuracies when performing mutual guidance. This consistency is especially important for achieving high-quality results in complex scenes, where precise geometric alignment can significantly enhance the visual and structural integrity of the output. ", "page_idx": 4}, {"type": "text", "text": "3.3 Training Strategy and Loss Design ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The GS-branch is supervised by rendering losses ${\\mathcal{L}}_{1}$ and $\\mathcal{L}_{\\mathrm{SSIM}}$ , which measure the difference between the rendered RGB images and ground truth images, and a volume regularization term $\\mathcal{L}_{v o l}$ as in [21]. The complete loss function for GS-branch is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{g}}=\\lambda_{1}\\mathcal{L}_{1}+(1-\\lambda_{1})\\mathcal{L}_{\\mathrm{SSIM}}+\\lambda_{v o l}\\mathcal{L}_{v o l},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda_{1}$ and $\\lambda_{v o l}$ are weighting coefficients. Similarly, the SDF-branch is supervised by the ${\\mathcal{L}}_{1}$ rendering loss, supplemented with Eikonal and curvature penalties to ensure accurate geometry reconstruction. The loss function for the SDF-branch is: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{s}}=\\mathcal{L}_{\\mathrm{1}}+\\lambda_{\\mathrm{eik}}\\mathcal{L}_{\\mathrm{eik}}+\\lambda_{\\mathrm{curv}}\\mathcal{L}_{\\mathrm{curv}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{L}_{\\mathrm{eik}}$ represents the Eikonal loss, ensuring that the gradients of the predicted SDF field are normalized, and $\\mathcal{L}_{\\mathrm{curv}}$ denotes the curvature loss, promoting surface smoothness as described in [19, 28]. The coefficients $\\lambda_{\\mathrm{eik}}$ and $\\lambda_{\\mathrm{curv}}$ balance the influence of these loss terms. ", "page_idx": 5}, {"type": "text", "text": "The mutual geometry supervision includes depth and normal consistency losses applied to both branches. For unbounded scenes with distant backgrounds, we only apply the mutual geometric supervision to foreground regions, as these geometries are more reliable and of primary interest [11]. The mutual loss is formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{mutual}}=\\lambda_{d}\\mathcal{L}_{d}+\\lambda_{n}\\mathcal{L}_{n}=\\lambda_{d}\\|D_{g s}-D_{s}\\|+\\lambda_{n}\\left(1-\\frac{\\vert N_{g s}\\cdot N_{s}\\vert}{\\Vert N_{g s}\\Vert\\,\\Vert N_{s}\\Vert}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{L}_{d}$ and ${\\mathcal{L}}_{n}$ represent the depth and normal discrepancies between the two branches, and $\\lambda_{d}$ and $\\lambda_{n}$ balance their importance. Finally, the total loss for joint learning is defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}=\\mathcal{L}_{\\mathrm{g}}+\\mathcal{L}_{\\mathrm{s}}+\\mathcal{L}_{\\mathrm{mutual}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The hyper-parameter settings are detailed in the supplementary material. ", "page_idx": 5}, {"type": "text", "text": "4 Experiment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets. We evaluated results using 26 real-world and synthetic scenes from various datasets: 7 from Mip-NeRF360 [3], 2 from DeepBlending [12], 2 from Tanks&Temples [18], and 15 from DTU [14], featuring a wide range of indoor, outdoor, and object-centric scenarios. ", "page_idx": 5}, {"type": "text", "text": "Implementation Details. We implemented our dual-branch model based on 1) Scaffold-GS [21] and 2) an enhanced version of NeuS [31] with a hash-grid variant [11], following the practice of [19]. The hash grid resolution spans from $2^{5}$ to $2^{11}$ with 16 levels, each entry having a feature dimension of 4 and a maximum of $2^{2\\bar{1}}$ entries per level. The coarsest 4 layers were activated initially for the DTU [1], and 8 layers for other datasets, with finer levels added every $2k$ iterations. We trained the GS-branch for $15\\mathrm{k}$ iterations, followed by joint training of both branches for 30k iterations. The SDF-branch was warmed up for 2k iterations on the DTU and $5\\mathrm{k}$ on other datasets without depth-guided ray sampling. Note that our system is adaptable with various existing or future rendering and reconstruction models. ", "page_idx": 5}, {"type": "text", "text": "Evaluations. We evaluated our method against state-of-the-art rendering and reconstruction approaches. For rendering comparison, we compared with Scaffold-GS [21], 3D-GS [17], NeuS [31], 2D-GS [13], and SuGaR [10] using PSNR, SSIM [33], and LPIPS [42] for quantitative comparisons. We trained 3D-GS and Scaffold-GS for $45k$ iterations to align with our configurations. For reconstruction, we compared with NeuS [31], Instant-NSR [11] (our representative SDF-branch), SuGaR [10] and 2D-GS [13]. We use Chamfer distance for quantitative evaluation. For all datasets, we used $1/8$ of the images as test sets and the other $7/8$ as training sets. All experiments were conducted on a single NVIDIA A100 GPU with 80G memory. ", "page_idx": 5}, {"type": "text", "text": "4.2 Results Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.2.1 Rendering Comparisons ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our GSDF retained high rendering quality compared to state-of-the-art 3D-GS and SDF-based methods, as shown in Table 1. Quantitative metrics against 3D-GS [17], 2D-GS [13], and ScaffoldGS [21] show that GSDF consistently outperformed these methods across all four datasets, showcasing improvements in all metrics. Notably, the prominent improvements in the LPIPS metric indicate that our method effectively captures high-frequency scene details and renders perceptually superior results compared to baselines. ", "page_idx": 5}, {"type": "table", "img_path": "r6V7EjANUK/tmp/7b1076754abdddbbb24f0a16f4799c6499694f06404631584003494eb2a29820.jpg", "table_caption": ["Table 1: Rendering and reconstruction comparisons against baselines over four benchmark scenes. 3D-GS [17], Scaffold-GS [21], 2D-GS [13], SuGaR [10] and GSDF initialized the Gaussian primitives with COLMAP [29] sparse points. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "r6V7EjANUK/tmp/f1610f1f68e5fce87a935256de02c81e6c2c185e0279c5bc3f36920a3a15bfec.jpg", "table_caption": ["Table 2: Rendering comparisons with random initialization against baselines over three benchmark scenes. Scaffold-GS (rand) [21], 2D-GS (rand) [13] and GSDF (rand) initialized the Gaussian primitives with random points. "], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "r6V7EjANUK/tmp/aac65d35b6a30f0ae233a6d155847aafeece899f783879fb0dd4b17e604cfd6c.jpg", "img_caption": ["Figure 3: Qualitative comparisons of GSDF against popular Gaussian-based baselines [17, 21, 13] across diverse 3D scene datasets [18, 3, 12]. As highlighted, GSDF excels in modeling delicate geometries (1st & 2nd rows) and handling texture-less and sparsely observed regions (3rd & 4th rows), which are commonly presented in larger scenes where baseline approaches struggle to address. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Enchanced Detail and Fidelity. Notably, our method excelled in achieving high rendering quality in texture-less areas, as showcased in Fig.3, which is aligned with our design motivation for geometryaware density control. Specifically, in texture-less areas where vanilla 3D Gaussians struggled due to small accumulated gradients, our method overcame this limitation by growing anchors in surface regions which brings enhanced accuracy and scene details. ", "page_idx": 6}, {"type": "text", "text": "Robustness with Noisy Gaussians. To test the robustness, we experimented with randomly initialized Gaussian primitives on 2D-GS, Scaffold-GS, and our method. Quantitative results in Table 2 highlight the advantages of our geometric guidance, demonstrating superior performance and stability even with random input. Further visual results are provided in the supplementary material. ", "page_idx": 6}, {"type": "image", "img_path": "r6V7EjANUK/tmp/3c21996b9a4021dc6de3565a910f7f55544640e88a3493ae112a1997eee93812.jpg", "img_caption": ["Figure 4: Reconstruction Comparison. We visualize the reconstructed meshes from InstantNSR [11] (our SDF-branch), SuGaR [10], 2D-GS [13], and Ours. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.2.2 Reconstruction Comparisons ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We quantitatively evaluated the reconstruction ability on the DTU dataset, where our method achieved the best Chamfer distance as shown in Table 1. ", "page_idx": 7}, {"type": "text", "text": "Enhanced Geometry Accuracy and Completeness. As illustrated in Fig.4, our method reconstructed more complete and detailed meshes compared to baseline methods. Our approach effectively bypassed local minima, preventing holes in the meshes. Notably, our method outperformed Instant-NSR, delivering smooth, continuous meshes with well-preserved high-frequency details. In contrast, meshes extracted from SuGaR [10] were non-manifold with broken topological relationships. 2DGS [13] extracts mesh from fused depth, which leads to over-smooth geometry. ", "page_idx": 7}, {"type": "text", "text": "Accerelated Convergence and Rendering. Benefited from the GS-branch, our method optimized the SDF field significantly faster in terms of training iterations than previous methods with accurate sample placements, which often required slow and exhaustive training and rendering. For instance, NeuS [31] required about 8 hours of optimization on a single GPU for the DTU dataset, whereas our method achieved comparable or better results within just 2 hours on a single GPU. ", "page_idx": 7}, {"type": "table", "img_path": "r6V7EjANUK/tmp/89d51f93fc10535d788a7589c508654d52ed62d44ae546f7365107c60cfc8e41.jpg", "table_caption": ["Table 3: Quantitative Results on Ablation Studies. We separately listed the rendering metrics for each ablation described in Sec. 4.3. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "r6V7EjANUK/tmp/aa21d9736200f321d488008c2582ebde8ae573a84ee7541d219e6afefe7e33a5.jpg", "img_caption": ["Figure 5: Ablation results. Visualizations of reconstructed meshes and rendered images from 1) our full method, 2) ours w/o depth-guided ray sampling, 3) ours w/o geometry-aware density control, and 4) ours w/o geometric supervision. We highlight the degradation of quality using numbered patches. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we examine the effectiveness of each individual module. Quantitative metrics and qualitative visualizations are provided in Table 3 and Fig. 5. ", "page_idx": 8}, {"type": "text", "text": "Depth-Guided Ray Sampling. To evaluate the effectiveness of our depth-guided ray sampling detailed in Sec 3.2, we conducted an ablation on depth-guided ray sampling using the original stratified ray sampling approach [23]. The results show that this ablated setting produced overly smoothed surfaces, failing to capture finer geometry details such as the heater and doors (Fig. 5, column 2, patches 2 & 3). The resulting less accurate SDF also impacted rendering quality, making details like the sticker on the wall appear more blurred compared to our full model. ", "page_idx": 8}, {"type": "text", "text": "Geometry-aware Gaussian Density Control. We replaced our geometry-aware Gaussian density control with the pruning and growing strategy from Scaffold-GS [21] to evaluate its efficacy. As shown in Fig. 5 (3rd column), this change led to missed details like the sticker on the wall (patch 4) due to small accumulated gradients on the texture-less surface. The reconstruction results also showed the absence of thin objects such as table legs and the chandelier (patch 1), highlighting the limitations of the vanilla strategy. ", "page_idx": 8}, {"type": "text", "text": "Mutual Geometric Supervision. Lastly, we ablated the proposed mutual geometric supervision by setting both $\\lambda_{d}$ and $\\lambda_{n}$ to 0 (Eq. 7). This resulted in a significant decay in surface quality and omission of details in rendering. These findings indicate that neural rendering is more tolerant of deviations than neural surface reconstruction. Without explicitly aligning Gaussians with SDF-derived geometry during optimization, the two branches can diverge, leading to sub-optimal results for both. ", "page_idx": 8}, {"type": "text", "text": "4.4 Limitations and Future Works ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Currently, our method is not tailored to handle challenging scenes with reflections and intense lighting variations, such as those found in indoor environments. However, we have observed that employing more structured and surface-aligned Gaussian primitives holds promise in capturing such view-dependent appearance changes with improved scene geometry. Our framework requires more memory usage because it simultaneously considers two representations. Furthermore, the performance of our SDF-branch significantly lags behind the GS-branch, leading to extended training durations compared to Scaffold-GS [21] only. Hence, improving the efficiency of the MLP-based ", "page_idx": 8}, {"type": "text", "text": "SDF-branch is a crucial direction for future research. Still, training time is not the highest priority compared to inference, where primitives are stored and can be accessed efficiently. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we introduced a dual-branch framework that leverages the strengths of both 3D-GS and SDF, showcasing its potential to achieve both enhanced rendering and reconstruction quality. The inherent differences in two representations, rendering approaches, and supervision loss pose a challenge to the seamless integration. We therefore integrate a bidirectional mutual guidance approach during the training to circumvent these restrictions. Three types of guidance have been introduced and validated in our framework, namely: 1) depth guided sampling $(\\mathrm{GS}{\\rightarrow}\\mathrm{SDF})$ ), 2) geometryaware Gaussian density control $\\mathrm{SDF}{\\rightarrow}\\mathrm{GS})$ ); and 3) mutual geometry supervision $\\mathrm{(GS\\leftrightarrowSDF)}$ ). Our extensive results demonstrate the efficiency and joint performance improvement on both tasks. As the two branches maintain their original architectures, we keep their efficiency during inference, allowing room for potential enhancements by substituting each branch with more advanced models in the future. We envision our model benefiting applications demanding high-quality rendering and geometry, including embodied environments, physical simulation, and immersive VR experiences. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is funded in part by the National Key R&D Program of China (2022ZD0160201), and Shanghai Artificial Intelligence Laboratory. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Henrik Aan\u00e6s, Rasmus Ramsb\u00f8l Jensen, George Vogiatzis, Engin Tola, and Anders Bjorholm Dahl. Large-scale data for multiple-view stereopsis. International Journal of Computer Vision, pages 1\u201316, 2016.   \n[2] Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry Ulyanov, and Victor Lempitsky. Neural point-based graphics. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXII 16, pages 696\u2013712. Springer, 2020.   \n[3] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Mipnerf 360: Unbounded anti-aliased neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5470\u20135479, 2022.   \n[4] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. TensoRF: Tensorial radiance fields. 2022.   \n[5] Hanlin Chen, Chen Li, and Gim Hee Lee. Neusg: Neural implicit surface reconstruction with 3d gaussian splatting guidance. arXiv preprint arXiv:2312.00846, 2023.   \n[6] Kai Cheng, Xiaoxiao Long, Kaizhi Yang, Yao Yao, Wei Yin, Yuexin Ma, Wenping Wang, and Xuejin Chen. Gaussianpro: 3d gaussian splatting with progressive propagation. arXiv preprint arXiv:, 2024.   \n[7] Pinxuan Dai, Jiamin Xu, Wenxiang Xie, Xinguo Liu, Huamin Wang, and Weiwei Xu. Highquality surface reconstruction using gaussian surfels. 2024.   \n[8] Yutao Feng, Xiang Feng, Yintong Shang, Ying Jiang, Chang Yu, Zeshun Zong, Tianjia Shao, Hongzhi Wu, Kun Zhou, Chenfanfu Jiang, et al. Gaussian splashing: Dynamic fluid synthesis with gaussian splatting. arXiv preprint arXiv:2401.15318, 2024.   \n[9] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5501\u20135510, 2022.   \n[10] Antoine Gu\u00e9don and Vincent Lepetit. Sugar: Surface-aligned gaussian splatting for efficient 3d mesh reconstruction and high-quality mesh rendering. arXiv preprint arXiv:2311.12775, 2023.   \n[11] Yuan-Chen Guo. Instant neural surface reconstruction, 2022. https://github.com/bennyguo/instant-nsr-pl.   \n[12] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Gabriel Brostow. Deep blending for free-viewpoint image-based rendering. 37(6):257:1\u2013257:15, 2018.   \n[13] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance fields. ArXiv, abs/2403.17888, 2024.   \n[14] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola, and Henrik Aan\u00e6s. Large scale multi-view stereopsis evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 406\u2013413, 2014.   \n[15] Ying Jiang, Chang Yu, Tianyi Xie, Xuan Li, Yutao Feng, Huamin Wang, Minchen Li, Henry Lau, Feng Gao, Yin Yang, et al. Vr-gs: A physical dynamics-aware interactive gaussian splatting system in virtual reality. arXiv preprint arXiv:2401.16663, 2024.   \n[16] Nikhil Keetha, Jay Karhade, Krishna Murthy Jatavallabhula, Gengshan Yang, Sebastian Scherer, Deva Ramanan, and Jonathon Luiten. Splatam: Splat, track & map 3d gaussians for dense rgb-d slam. arXiv preprint arXiv:2312.02126, 2023.   \n[17] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00fchler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4), 2023.   \n[18] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics, 36(4), 2017.   \n[19] Zhaoshuo Li, Thomas M\u00fcller, Alex Evans, Russell H Taylor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin. Neuralangelo: High-fidelity neural surface reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8456\u20138465, 2023.   \n[20] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. Advances in Neural Information Processing Systems, 33:15651\u201315663, 2020.   \n[21] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. Scaffoldgs: Structured 3d gaussians for view-adaptive rendering, 2023.   \n[22] Xiaoyang Lyu, Yang-Tian Sun, Yi-Hua Huang, Xiuzhe Wu, Ziyi Yang, Yilun Chen, Jiangmiao Pang, and Xiaojuan Qi. 3dgsr: Implicit surface reconstruction with 3d gaussian splatting. arXiv preprint arXiv:2404.00409, 2024.   \n[23] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020.   \n[24] Thomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. ACM Transactions on Graphics (ToG), 41(4):1\u2013 15, 2022.   \n[25] Michael Oechsle, Songyou Peng, and Andreas Geiger. Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5589\u20135599, 2021.   \n[26] Christian Reiser, Stephan Garbin, Pratul P Srinivasan, Dor Verbin, Richard Szeliski, Ben Mildenhall, Jonathan T Barron, Peter Hedman, and Andreas Geiger. Binary opacity grids: Capturing fine geometric detail for mesh-based view synthesis. arXiv preprint arXiv:2402.12377, 2024.   \n[27] Antoni Rosinol, John J Leonard, and Luca Carlone. Nerf-slam: Real-time dense monocular slam with neural radiance fields. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 3437\u20133444. IEEE, 2023.   \n[28] Radu Alexandru Rosu and Sven Behnke. Permutosdf: Fast multi-view reconstruction with implicit surfaces using permutohedral lattices. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8466\u20138475, 2023.   \n[29] Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4104\u20134113, 2016.   \n[30] Haithem Turki, Vasu Agrawal, Samuel Rota Bul\u00f2, Lorenzo Porzi, Peter Kontschieder, Deva Ramanan, Michael Zollh\u00f6fer, and Christian Richardt. Hybridnerf: Efficient neural rendering via adaptive volumetric surfaces. arXiv preprint arXiv:2312.03160, 2023.   \n[31] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv preprint arXiv:2106.10689, 2021.   \n[32] Yiming Wang, Qin Han, Marc Habermann, Kostas Daniilidis, Christian Theobalt, and Lingjie Liu. Neus2: Fast learning of neural implicit surfaces for multi-view reconstruction. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 3272\u20133283, 2022.   \n[33] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600\u2013612, 2004.   \n[34] Zian Wang, Tianchang Shen, Merlin Nimier-David, Nicholas Sharp, Jun Gao, Alexander Keller, Sanja Fidler, Thomas M\u00fcller, and Zan Gojcic. Adaptive shells for efficient neural radiance field rendering. arXiv preprint arXiv:2311.10091, 2023.   \n[35] Tianyi Xie, Zeshun Zong, Yuxin Qiu, Xuan Li, Yutao Feng, Yin Yang, and Chenfanfu Jiang. Physgaussian: Physics-integrated 3d gaussians for generative dynamics. arXiv preprint arXiv:2311.12198, 2023.   \n[36] Linning Xu, Vasu Agrawal, William Laney, Tony Garcia, Aayush Bansal, Changil Kim, Samuel Rota Bul\u00f2, Lorenzo Porzi, Peter Kontschieder, Alja\u017e Bo\u017ei\u02c7c, et al. Vr-nerf: High-fidelity virtualized walkable spaces. In SIGGRAPH Asia 2023 Conference Papers, pages 1\u201312, 2023.   \n[37] Linning Xu, Yuanbo Xiangli, Sida Peng, Xingang Pan, Nanxuan Zhao, Christian Theobalt, Bo Dai, and Dahua Lin. Grid-guided neural radiance fields for large urban scenes. 2023.   \n[38] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Point-based neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5438\u20135448, 2022.   \n[39] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces. Advances in Neural Information Processing Systems, 34:4805\u20134815, 2021.   \n[40] Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin, Pratul P Srinivasan, Richard Szeliski, Jonathan T Barron, and Ben Mildenhall. Bakedsdf: Meshing neural sdfs for real-time view synthesis. arXiv preprint arXiv:2302.14859, 2023.   \n[41] Wang Yifan, Felice Serena, Shihao Wu, Cengiz \u00d6ztireli, and Olga Sorkine-Hornung. Differentiable surface splatting for point-based geometry processing. ACM Transactions on Graphics (TOG), 38(6):1\u201314, 2019.   \n[42] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.   \n[43] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and Markus Gross. Ewa volume splatting. In Proceedings Visualization, 2001. VIS\u201901., pages 29\u2013538. IEEE, 2001. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Supplementary Material ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The following sections are organized as follows: 1) The first section elaborates on the implementation details of GSDF, covering hyper-parameters and curvature loss. 2) We then show additional experimental results. 3) Lastly, we also delve into limitations and future directions. More results and video demos can be found in supplementary webpage. ", "page_idx": 12}, {"type": "text", "text": "A.1 Implementation details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1.1 Configurations. ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Below, we enumerate the hyper-parameters used in our experiments: ", "page_idx": 12}, {"type": "text", "text": "\u2022 The variance $\\sigma^{2}$ for the Gaussian function in Eq. 3 and 4 is set to 0.005.   \n\u2022 For the rendering loss discussed in Sec. 3.3, we set $\\lambda_{1}=0.2$ and $\\lambda_{v o l}=0.01$ , in consistency with the configurations specified in Scaffold-GS [21].   \n\u2022 For the SDF-branch, we set $\\lambda_{e i k}\\,=\\,0.1$ and implement an adaptive scheme for $\\lambda_{c u r v}$ . Specifically, $\\lambda_{c u r v}$ increases linearly from 0 to 1 (0.5 on DTU) over the first 2000 iterations, after which it remains at 0.05 (0.01 on DTU) for subsequent iterations. This strategy is based on our observation that increasing the weight of the curvature loss significantly encourages the convergence of the SDF to a geometric outline.   \n\u2022 For the mutual geometry loss, we typically assign $\\lambda_{d}=0.5$ and $\\lambda_{n}=0.01$ . ", "page_idx": 12}, {"type": "text", "text": "A.1.2 Curvature Loss. ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "While Instant-NSR [11] derives the curvature loss from a discrete Laplacian, we follow a more robust and explicit method as described in PermutoSDF [28]. For any given point, we randomly perturb it within the tangent plane orthogonal to its normal. The curvature loss is then measured by the cosine similarity of normals between the original point and its perturbed counterpart. Note that, we applied the same curvature loss for both Instant-NSR and GSDF. ", "page_idx": 12}, {"type": "text", "text": "A.2 More experiments ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.2.1 Reconstruction with $500k$ iterations. ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "As discussed in Sec. 4, optimizing the SDF is a time-intensive process. While our GS-branch assists in the convergence of the SDF-branch, we conjecture that further improvements could be made through additional iterations. Hence, we train our method for $500k$ iterations. As shown in Fig.6, our method achieves superior reconstruction quality compared to Instant-NSR [11] (our SDF-branch) with the same training iterations. ", "page_idx": 12}, {"type": "text", "text": "A.2.2 Rendering comparison with random initialization. ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In Sec. 3.2, we discussed how the SDF-branch can improve the rendering quality of the GS-branch. This enhancement is particularly noticeable when Gaussian primitives are randomly initialized, as shown in Fig. 7 and Fig. 8. ", "page_idx": 12}, {"type": "text", "text": "A.2.3 Per-scene Results. ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We list the per-scene quality metrics (PSNR, SSIM, LPIPS, and Chamfer Distance) used in our rendering and reconstruction evaluation in Sec. 4 for all considered methods, as shown from Tab. 4 to Tab. 6. ", "page_idx": 12}, {"type": "text", "text": "Table 4: Quantitative results for DTU scenes [1]. Note that we tried our best, but NeuS [31] failed to reconstruct geometries in scenes 69 and 105 with $\\frac{7}{8}$ training images. ", "page_idx": 13}, {"type": "table", "img_path": "r6V7EjANUK/tmp/70f7f4622fc1b317033059a8d92f8f8b5d354515417c6fb9f99c8abfb3dc6434.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "table", "img_path": "r6V7EjANUK/tmp/8554f65360e28b53a94ce3bd1e3f6eef955b5a07c10ad52590cc364183aa903b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "table", "img_path": "r6V7EjANUK/tmp/c9d5fb00c5e9832e2e2207a7e0d4bb003e9c1e4e313eda7d555e1c11a3eeb83b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "table", "img_path": "r6V7EjANUK/tmp/5f955a117dc483749b0e6bf7c8fb36206727bd0c4a7e03a0a1e2f43354b0d4e8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "table", "img_path": "r6V7EjANUK/tmp/47943156da92c3c1a20d1489d9c548bf2e579cd38b7a47586c89603b9b0972c0.jpg", "table_caption": ["Table 5: Quantitative results for Mip-NeRF 360 scenes [3]. "], "table_footnote": [], "page_idx": 13}, {"type": "table", "img_path": "r6V7EjANUK/tmp/26535b8e439d17e7081d6ac94dc2909692e2c9a40b1a7b091e87952becf47c58.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "table", "img_path": "r6V7EjANUK/tmp/abba6bb1d0d98fc8ee6281694649a2df72e769836396d97f5e83b77b53a9611b.jpg", "table_caption": ["Table 6: Quantitative results for Tanks&Temples [18] and Deep Blending [12] scenes. "], "table_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "r6V7EjANUK/tmp/24cf0879e9d907bc97500dff131b46fb348562d105e56b8782d82636237e13e9.jpg", "img_caption": ["Figure 6: Reconstruction comparison with $500\\mathbf{k}$ training iterations. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "r6V7EjANUK/tmp/4efeffa14e6d86b1874d8555582d41f5d4e001480815492bcf7d06f0d37a9fd4.jpg", "img_caption": ["Figure 7: Rendering Comparison with random initialization (Part 1). We compare rendering results between Scaffold-GS [21] and our method. The highlighted patches indicate that our method is superior in expressing finer details in both geometry and appearance. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "r6V7EjANUK/tmp/3256e676082286a3278123b05191ba036cadaa03830f0a15c7d7ed21f1777d7e.jpg", "img_caption": ["Figure 8: Rendering Comparison with random initialization (Part 2) - Cont. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We introduce GSDF, a dual-branch structure combining the strengths of 3D-GS and SDF, for improving both rendering and reconstruction. Please see our abstract and introduction for further details. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: Our framework still struggles with the dilemma like most SDF-based reconstruction methods. Please see the limitation section (Sec. 4.4) for further details. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: In the method and experiment sections, we provide a detailed discussion and substantial evidence to demonstrate each result. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: GSDF is a robust framework that comprises a GS-branch dedicated to rendering and an SDF-branch focusing on learning neural surfaces. By leveraging the mutual guidance between these two branches, our method achieves state-of-the-art performance on both rendering and reconstruction tasks. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: We will release the code after acceptance. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: In the Experimental Setup section (Sec. 4.1) and Implementation details section (Sec. A.1), we provide comprehensive details, including the data splits and hyperparameters used in our experiments. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: The experimental result for redering and reconstruction tasks from dense views are very stable, the standard deviation is negligible, so we follow the previous work\u2019s practice and not report the error bar. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: In Experiment section (Sec. 4), We provide sufficient information on the computer resources, including the type of compute workers GPU and training time. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We have carefully reviewed the NeurIPS Code of Ethics and are committed to strictly adhering to its guidelines. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Neural rendering and surface reconstruction are crucial tasks with important downstream applications in areas like robotics, physical simulations, and augmented reality. We believe our GSDF system has the potential to advance progress in this domain and support the development of these applications. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: There is no such risks in the rendering and reconstruction tasks. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: In the Experimental Setup section (Sec. 4.1), we cite each used code package and public datasets. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [No] ", "page_idx": 22}, {"type": "text", "text": "Justification: We plan to release the code upon acceptance of the paper. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper does not involve any crowdsourcing or research with human subjects. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper does not involve any crowdsourcing or research with human subjects. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]