{"references": [{"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-00-00", "reason": "This paper introduced Vision Transformers (ViTs), a foundational model that the current paper builds upon and improves."}, {"fullname_first_author": "Albert Gu", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "publication_date": "2023-00-00", "reason": "This paper introduced the Mamba model, which directly inspired the current paper's approach."}, {"fullname_first_author": "Kaiming He", "paper_title": "Deep residual learning for image recognition", "publication_date": "2016-00-00", "reason": "This paper introduced the ResNet architecture, a significant contribution in computer vision, which is benchmarked against in the current paper."}, {"fullname_first_author": "Ze Liu", "paper_title": "Swin Transformer: Hierarchical vision transformer using shifted windows", "publication_date": "2021-00-00", "reason": "This paper introduced the Swin Transformer, a highly influential model in computer vision, that is compared to the current paper's approach."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Training data-efficient image transformers & distillation through attention", "publication_date": "2021-00-00", "reason": "This paper explores training data-efficient image transformers, a topic highly relevant to the current paper's focus on efficiency."}]}