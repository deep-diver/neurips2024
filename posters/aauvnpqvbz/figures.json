[{"figure_path": "aaUVnpQvbZ/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of ground-truth optimal transport maps with different costs h, for the same base function g. In this experiment, g is the negative of a random ICNN with 2-dimensional inputs, 3 layers and hidden dimensions of sizes [8, 8, 8]. All plots display the level lines of g. The optimal transport map Th are recomputed four times using Prop. 1, with four different costs h, displayed above each plot. (left) When h is the usual l2 cost, we observe a typical OT map that follows from each x\u2081, minus the gradient of g. With the l\u2081 sparsity-inducing regularizer (middle-left), we obtain sparse displacements: most arrows follow either of the two canonical axes, yet some points do not move at all. (middle-right) This is slightly different when using the k-overlap norm, which exhibits less shrinkage. With a cost that penalizes displacements that are orthogonal to a vector b, we obtain displacements that push further to the bottom than in the (left) plot, as in the (right) plot, where displacements are almost parallel to b. When b is not known beforehand, and both source and target samples are given, we present a procedure to learn adaptively such a parameter in \u00a7 5.", "description": "This figure shows four examples of optimal transport maps with different cost functions (h). Each example uses the same base function (g), which is visualized by its level lines.  The differences in the maps illustrate how the choice of cost function influences the structure of the resulting transport map.  Different costs induce different patterns of displacements, ranging from smooth (l2 cost) to sparse (l1 cost) and directional (cost that penalizes displacements orthogonal to vector b).", "section": "3 On Ground-Truth Monge Maps for Elastic Costs"}, {"figure_path": "aaUVnpQvbZ/figures/figures_4_1.jpg", "caption": "Figure 2: Illustration of the h-transform computation in 2d. (left): base concave potential g, here a negative quadratic. (other figures) Level lines of the corresponding h-transform \u011dh for different choices of h. The h-transform is computed using the iterations described in Prop. 2.", "description": "This figure shows the computation of the h-transform for different cost functions (h). It starts with a base concave potential function g (a negative quadratic in this case) and then computes the h-transform using the iterative method described in Proposition 2. The figure displays the level lines of the h-transform for each of the four cost functions considered.  Different cost functions lead to substantially different h-transforms, and, as a consequence, the optimal transport maps for those costs. ", "section": "3 On Ground-Truth Monge Maps for Elastic Costs"}, {"figure_path": "aaUVnpQvbZ/figures/figures_7_1.jpg", "caption": "Figure 3: Performance of the MBO estimator on two ground-truth tasks involving the \u03c4 = l\u2081 and \u03c4A+ = ||A+z||2 structured costs, where p = 2 in dimension d = 5 (two figures to the left) and dimension d = 10 (two figures to the right). We display the MSE ratio between the MSE estimated with a regularizer strength \u03b3 > 0 and that in the absence of regularization (i.e., \u03b3 = 0). The level of regularization used for generating the ground-truth data is \u03b3*, whereas performance are shown varying w.r.t. \u03b3. We display curves \u00b1 s.t.d. estimated over 10 random seeds.", "description": "The figure displays the performance of the Monge-Bregman-Occam (MBO) estimator on synthetic datasets. Two different regularizers (l1 and ||A+z||2) are used in two different dimensions (d=5 and d=10).  The Mean Squared Error (MSE) ratio (with regularization vs. without) is plotted against various regularization strength (\u03b3), showing improved performance with appropriate regularization, especially in higher dimensions.", "section": "6.1 MBO on Synthetic Ground-Truth Displacement"}, {"figure_path": "aaUVnpQvbZ/figures/figures_8_1.jpg", "caption": "Figure 4: Error averaged over 5 seeded runs (lower is better) in [0, 1] of the  p\u02c6 \u00d7 d orthogonal matrix \u00c2 recovered by our algorithm, compared to the ground-truth p* \u00d7 d cost matrix A*. Error bars are not shown for compactness, but are negligible since all quantities are bounded below and close to 0. Dimensions d, p* vary in each of these 6 plots, whereas p\u02c6 is fixed to either p* (top row) or 1.25p* (bottom row). Error is quantified as the normalized squared-residual error obtained when projecting the p* basis vectors of A* onto the span of \u00c2. From left to right, the regularization strength \u03b3* increases to ensure that 50%, 70% and 90% of the total inertia of all displacements generated by the ground-truth Monge map are borne by the p* highest singular values. As expected, recovery is easier when p\u02c6 is slightly larger than p* (bottom) compared to being exactly equal (top). It is also easier as the share of inertia captured by p* increases.", "description": "This figure displays the results of an experiment evaluating the ability of the algorithm to recover the ground truth subspace parameters in elastic costs.  It shows the error in recovering a low-dimensional subspace (represented by matrix A) using different dimensions for the data (d), the true subspace (p*), and the estimated subspace (p\u0302). The experiment was conducted with varying percentages (50%, 70%, 90%) of inertia of displacements in the true subspace, and the results indicate improved recovery performance with higher inertia percentages and slightly overestimated subspace dimensions.", "section": "6.2 Recovery of Ground-Truth Subspace Parameters in Elastic Costs"}, {"figure_path": "aaUVnpQvbZ/figures/figures_9_1.jpg", "caption": "Figure 5: Predictive performance of the MBO estimator on single-cell datasets, d = 256, using either the naive baseline l2 cost (black dotted line) or elastic subspace cost (13), with varying \u03b3 and p. Remarkably, promoting displacements to happen in a subspace of much lower dimension improves predictions, even when measured in the squared-Euclidean distance.", "description": "This figure displays the predictive performance of the Monge-Bregman-Occam (MBO) estimator on single-cell datasets with 256 dimensions.  It compares the performance using the standard l2 cost with that of the elastic subspace cost. The results show that promoting displacements within a lower-dimensional subspace significantly improves predictive performance, even when evaluating using the squared Euclidean distance. The x-axis represents varying values of the regularization parameter (\u03b3), and the different colored lines indicate varying subspace dimensions (p).", "section": "6.3 Learning Displacement Subspaces for Single-Cell Transport"}]