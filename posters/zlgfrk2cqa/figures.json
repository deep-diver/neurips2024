[{"figure_path": "zlgfRk2CQa/figures/figures_0_1.jpg", "caption": "Figure 1: Recurrent-based model architectures for learning algorithms with input x and output y. F, G and H are convolutional networks that work on any size input. A scratchpad & serves as the working memory during computation. As described in Section 2 the original DT model didn't include recall, denoted by the dotted line. The improved DT-R and our DT-L model include this connection.", "description": "This figure shows three different model architectures used for learning algorithms.  The input is represented by 'x' and the output by 'y'.  The models use convolutional networks (F, G, H) and a scratchpad (\u03a6) to store intermediate computations. The key difference between the models is the inclusion of 'recall' (dotted line), a connection from the original input 'x' to the recurrent function G, which is present in DT-R and DT-L but absent in the original DT model.  This allows the model to access the original input throughout the iterative process.", "section": "1 Introduction"}, {"figure_path": "zlgfRk2CQa/figures/figures_2_1.jpg", "caption": "Figure 2: Distribution of spectral norms of reshaped weight matrices for the different convolutional layers in the recurrent part of DT-R. 30 prefix-sum-solving models with width w = 32 were sampled. Given a convolutional layer with weights shaped (Cout, Cin, n), where n is the number of elements (e.g. the kernel height times width in a 2D convolution) we can flatten the weights into a matrix of shape Cout \u00d7 Cinn; the spectral norm is the largest singular value of this matrix.", "description": "This figure shows the distribution of spectral norms for the convolutional layers in the recurrent part of the DT-R model.  The spectral norm is a measure of how much the magnitude of the output of a layer changes relative to its input; a value of 1 indicates no change.  The violin plots show the distribution across 30 different prefix-sum solving models (width=32).  The plot indicates that in most cases, the spectral norm is greater than 1, meaning the magnitude of the activations increases as it passes through each layer. This contributes to instability in the training and extrapolation behavior of the DT-R model.", "section": "3 Analysis of Deep Thinking Networks"}, {"figure_path": "zlgfRk2CQa/figures/figures_3_1.jpg", "caption": "Figure 3: Mean training (cross-entropy) loss at each epoch for prefix-sums-solving models of varying width w. For small w training is stable, but not all models converge; larger w has a higher chance of models reaching a small loss, but the training process has very large spikes in the loss which causes some models to explode. Each curve is measured from a different random initialization of the model throughout training, for 10 models of each width", "description": "This figure shows the training stability of Deep Thinking with Recall (DT-R) models for solving prefix sums problems with varying model widths (w).  It plots the mean cross-entropy loss across 10 different model initializations at each training epoch for widths w = 16, 32, 64, 128, and 256.  Narrower models (small w) exhibit more stable training, although convergence isn't guaranteed, while wider models show a higher likelihood of reaching a low loss but also experience significant instability and even model failure (NaN loss) due to loss spikes.", "section": "3.1 Training Stability"}, {"figure_path": "zlgfRk2CQa/figures/figures_6_1.jpg", "caption": "Figure 4: Comparison between Deep Thinking with Recall and Deep Thinking with Lipschitz Constraints on the prefix sums problem. Two left plots show the solution accuracy of inference-time runs on 512-bit problems for 30 individual models each. Each line corresponds to the performance of a network trained from scratch with different randomly initial weights. The accuracy is measured on 10000 problem instances. The right plot shows the mean of all 30 for each. Models have a channel width of w = 32. Shaded areas show 95% confidence intervals.", "description": "This figure compares the performance of two models, DT-R and DT-L, on the prefix sums problem.  DT-R shows unstable performance across multiple runs, with some models failing to reach high accuracy, while DT-L demonstrates much more consistent and reliable performance, achieving high accuracy across nearly all runs.  The rightmost plot shows the mean accuracy for each model, highlighting the significant improvement in stability and accuracy provided by DT-L.", "section": "5 Results on Easy-to-Hard Problems"}, {"figure_path": "zlgfRk2CQa/figures/figures_7_1.jpg", "caption": "Figure 4: Comparison between Deep Thinking with Recall and Deep Thinking with Lipschitz Constraints on the prefix sums problem. Two left plots show the solution accuracy of inference-time runs on 512-bit problems for 30 individual models each. Each line corresponds to the performance of a network trained from scratch with different randomly initial weights. The accuracy is measured on 10000 problem instances. The right plot shows the mean of all 30 for each. Models have a channel width of w = 32. Shaded areas show 95% confidence intervals.", "description": "This figure compares the performance of two different models, DT-R and DT-L, on the prefix sums problem. The left two plots show the accuracy of 30 different models for each architecture, trained with different random initial weights. The right plot displays the mean performance for each architecture. DT-L demonstrates better and more consistent accuracy across all models.  The shaded areas represent 95% confidence intervals.", "section": "5 Results on Easy-to-Hard Problems"}, {"figure_path": "zlgfRk2CQa/figures/figures_7_2.jpg", "caption": "Figure 4: Comparison between Deep Thinking with Recall and Deep Thinking with Lipschitz Constraints on the prefix sums problem. Two left plots show the solution accuracy of inference-time runs on 512-bit problems for 30 individual models each. Each line corresponds to the performance of a network trained from scratch with different randomly initial weights. The accuracy is measured on 10000 problem instances. The right plot shows the mean of all 30 for each. Models have a channel width of w = 32. Shaded areas show 95% confidence intervals.", "description": "This figure compares the performance of Deep Thinking with Recall (DT-R) and Deep Thinking with Lipschitz Constraints (DT-L) models on a prefix sum problem.  Thirty models of each type were trained independently from random initializations. The left plots show the accuracy of each model on 512-bit test instances, plotted against the number of test-time iterations. The right plot presents the average accuracy for each model type. The results show DT-L's improved stability and accuracy on larger problems.", "section": "5 Results on Easy-to-Hard Problems"}, {"figure_path": "zlgfRk2CQa/figures/figures_12_1.jpg", "caption": "Figure 1: Recurrent-based model architectures for learning algorithms with input x and output y. F, G and H are convolutional networks that work on any size input. A scratchpad & serves as the working memory during computation. As described in Section 2 the original DT model didn't include recall, denoted by the dotted line. The improved DT-R and our DT-L model include this connection.", "description": "This figure shows three different model architectures for learning algorithms.  The input is x, and the output is y. All models use convolutional neural networks (F, G, H) that can handle variable-sized inputs. A scratchpad (phi) acts as a working memory. The main difference between the models is how they handle recall of the original input. The original Deep Thinking (DT) model did not include recall, while the Deep Thinking with Recall (DT-R) and Deep Thinking with Lipschitz Constraints (DT-L) models do.", "section": "1 Introduction"}, {"figure_path": "zlgfRk2CQa/figures/figures_12_2.jpg", "caption": "Figure 1: Recurrent-based model architectures for learning algorithms with input x and output y. F, G and H are convolutional networks that work on any size input. A scratchpad  serves as the working memory during computation. As described in Section 2 the original DT model didn't include recall, denoted by the dotted line. The improved DT-R and our DT-L model include this connection.", "description": "This figure shows three different recurrent neural network architectures used for learning algorithms.  The input x is processed by a convolutional network F to produce an initial state. This state is iteratively updated by another convolutional network G (with the original input x also included in DT-R and DT-L).  The updated state is further processed by a final convolutional network H to produce the output y.  The scratchpad \u03c6 represents working memory during this iterative process. The differences between DT, DT-R, and DT-L lie in the inclusion (DT-R and DT-L) or exclusion (DT) of a connection from the original input x to the state update function G, which is represented by a dotted line in the diagram.  This 'recall' mechanism is a key improvement.", "section": "Analysis of Deep Thinking Networks"}, {"figure_path": "zlgfRk2CQa/figures/figures_12_3.jpg", "caption": "Figure 1: Recurrent-based model architectures for learning algorithms with input x and output y. F, G and H are convolutional networks that work on any size input. A scratchpad \u03c6 serves as the working memory during computation. As described in Section 2 the original DT model didn't include recall, denoted by the dotted line. The improved DT-R and our DT-L model include this connection.", "description": "This figure shows three different recurrent neural network architectures for learning algorithms.  The input is denoted by 'x' and the output by 'y'.  The networks use convolutional layers (F, G, H) and a scratchpad (\u03c6) to store intermediate computations.  The key difference between the three architectures is the incorporation of 'recall'. The original DT model lacks recall, while the improved DT-R and the proposed DT-L model both include a recall connection (dotted line in DT-R, solid line in DT-L), allowing the network to access the original input at each iteration.", "section": "1 Introduction"}, {"figure_path": "zlgfRk2CQa/figures/figures_13_1.jpg", "caption": "Figure 1: Recurrent-based model architectures for learning algorithms with input x and output y. F, G and H are convolutional networks that work on any size input. A scratchpad \u03c6 serves as the working memory during computation. As described in Section 2 the original DT model didn't include recall, denoted by the dotted line. The improved DT-R and our DT-L model include this connection.", "description": "This figure shows three different model architectures for learning algorithms.  The input is x, and the output is y.  The models use convolutional neural networks (F, G, and H) and a scratchpad (\u03c6) to store intermediate results.  The key difference between the models is the inclusion or exclusion of a \"recall\" connection (dotted line).  The original Deep Thinking (DT) model lacked recall, while the improved Deep Thinking with Recall (DT-R) and the Deep Thinking with Lipschitz Constraints (DT-L) models include it.  This connection allows the models to use the original input (x) at every iteration, which improves performance.", "section": "1 Introduction"}, {"figure_path": "zlgfRk2CQa/figures/figures_16_1.jpg", "caption": "Figure D4: Extended results of Figure 3. Mean training (cross-entropy) loss at each epoch for prefix-sums-solving models of varying width w. Each curve is measured from a different random initialization of the model throughout training, for 10 models of each width. Dotted vertical lines indicate loss becoming NaN or infinite, where the model does not recover.", "description": "This figure shows the mean training loss for prefix-sum solving models with different widths (w). Each line represents the average training loss across 10 models, each initialized randomly. The dotted vertical lines highlight points where the training loss becomes NaN (Not a Number) or infinite, indicating model failure.  The figure extends the results shown in Figure 3 of the main paper, providing additional data points to illustrate the effect of model width on training stability for Deep Thinking with Recall models.", "section": "D.1 DT-R Stability"}, {"figure_path": "zlgfRk2CQa/figures/figures_17_1.jpg", "caption": "Figure 5: Comparison between Deep Thinking with Recall and Deep Thinking with Lipschitz Constraints on the mazes problem for small models. Two left plots show the solution accuracy of inference-time runs on 33 \u00d7 33 mazes for 14 different models each. The right plot shows the mean of all 14 for each. Models have a channel width of w = 32. Shaded areas show 95% confidence intervals.", "description": "This figure compares the performance of Deep Thinking with Recall (DT-R) and Deep Thinking with Lipschitz Constraints (DT-L) models on the mazes problem.  Smaller models (width w=32) were trained on smaller mazes (17x17) and then tested on larger mazes (33x33) to evaluate their ability to extrapolate. The plots show the solution accuracy for 14 different model runs on the larger mazes, with different iterations (M) at inference time. The right-hand plot shows the average accuracy across these 14 runs.  Shaded areas represent 95% confidence intervals.", "section": "5 Results on Easy-to-Hard Problems"}, {"figure_path": "zlgfRk2CQa/figures/figures_17_2.jpg", "caption": "Figure 4: Comparison between Deep Thinking with Recall and Deep Thinking with Lipschitz Constraints on the prefix sums problem. Two left plots show the solution accuracy of inference-time runs on 512-bit problems for 30 individual models each. Each line corresponds to the performance of a network trained from scratch with different randomly initial weights. The accuracy is measured on 10000 problem instances. The right plot shows the mean of all 30 for each. Models have a channel width of w = 32. Shaded areas show 95% confidence intervals.", "description": "This figure compares the performance of Deep Thinking with Recall (DT-R) and Deep Thinking with Lipschitz Constraints (DT-L) models on a prefix sum problem.  The left two subplots show the accuracy of 30 independently trained DT-R and DT-L models on 512-bit test instances, illustrating the variability in model performance. The right subplot shows the average accuracy across all 30 models for both DT-R and DT-L.  The shaded regions represent 95% confidence intervals.  This figure highlights DT-L's improved stability and performance compared to DT-R in solving larger problem instances.", "section": "5 Results on Easy-to-Hard Problems"}, {"figure_path": "zlgfRk2CQa/figures/figures_18_1.jpg", "caption": "Figure D7: Mean solution accuracy at different M for (a) 5 instances of DT-R (without batch normalization) and 5 instances of DT-R (with batch normalization), and for (b) 5 instances of DT-L (without batch normalization) and 5 instances of DT-L (with batch normalization). All models have width w = 32. Extrapolation performance tested on 512-bit prefix sum problem instances. Shaded areas show 95% confidence intervals.", "description": "This figure compares the performance of DT-R and DT-L models with and without batch normalization on the prefix sum task.  The x-axis represents the number of iterations (M) at inference time, while the y-axis shows the solution accuracy.  The shaded regions indicate the 95% confidence intervals. The results demonstrate the impact of batch normalization on model stability and accuracy in extrapolating to larger problems.", "section": "D.2 Ablation Studies"}]