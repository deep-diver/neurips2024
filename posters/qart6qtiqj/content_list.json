[{"type": "text", "text": "The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ezra Edelman\\* Nikolaos Tsilivis\\* University of Pennsylvania New York Universityt ezrae@cis.upenn.edu nt2231@nyu.edu ", "page_idx": 0}, {"type": "text", "text": "Benjamin L. Edelman Eran Malach Surbhi Goel Harvard University Harvard University University of Pennsylvania bedelman@g.harvard.edu emalach@g.harvard.edu surbhig@cis.upenn.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models have the ability to generate text that mimics patterns in their inputs. We introduce a simple Markov Chain sequence modeling task in order to study how this in-context learning capability emerges. In our setting, each example is sampled from a Markov chain drawn from a prior distribution over Markov chains. Transformers trained on this task form statistical induction heads which compute accurate next-token probabilities given the bigram statistics of the context. During the course of training, models pass through multiple phases: after an initial stage in which predictions are uniform, they learn to sub-optimally predict using in-context single-token statistics (unigrams); then, there is a rapid phase transition to the correct in-context bigram solution. We conduct an empirical and theoretical investigation of this multi-phase process, showing how successful learning results from the interaction between the transformer's layers, and uncovering evidence that the presence of the simpler unigram solution may delay formation of the final bigram solution. We examine how learning is affected by varying the prior distribution over Markov chains, and consider the generalization of our in-context learning of Markov chains (ICL-MC) task to $n$ gramsfor $n>2$ ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) exhibit a remarkable ability to perform in-context learning (ICL) from patterns in their input context [12, 16]. The ability of LLMs to adaptively learn from context is profoundly useful, yet the underlying mechanisms of this emergent capability are not fully understood. ", "page_idx": 0}, {"type": "text", "text": "In an effort to better understand ICL, some recent works propose to study ICL in controlled synthetic settings\u2014-in particular, training transformers on mathematically defined tasks which require learning from the input context. For example, a recent line of works studies the ability of transformers to perform ICL of standard supervised learning problems such as linear regression [3, 20, 26, 41j.  Studying these well-understood synthetic learning tasks enables fine-grained control over the data distribution, allows for comparisons with established supervised learning algorithms, and facilitates the examination of the in-context \u201calgorithm\u201d implemented by the network. That said, these supervised settings are reflective specifically of few-shot learning, which is only a special case of the more general phenomenon of networks incorporating patterns from their context into their predictions. A few recent works [8, 42] go beyond the case of cleanly separated in-context inputs and outputs, studying in-context learning on distributions based on discrete stochastic processes. ", "page_idx": 0}, {"type": "image", "img_path": "qaRT6QTIqJ/tmp/ba3fdb2bc25d8335502b82a594053d92c597088cab5dcbcaaf5c3ac094565aca.jpg", "img_caption": ["Figure 1: (left) We train small transformers to perform in-context learning of Markov chains (ICLMC). Each training sequence is generated by sampling a transition matrix from a prior distribution, and then sampling a sequence from this Markov chain. (right) Distance of a transformer's output distribution to several well-defined strategies over the course of training on our in-context Markov chain task. The model passes through three stages: (1) predicting a uniform distribution (blue region), (2) predicting based on in-context unigram statistics (orange region), (3) predicting based on in-context bigram statistics (green region). Shading is based on the minimum of the curves. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The goal of this work is to propose and analyze a simple synthetic setting for studying ICL. To achieve this, we consider $n$ -gram models [11, 15, 37], one of the simplest and oldest methods for language modeling. An $n$ -gram language model predicts the probability of a token based on the preceding $n-1$ tokens, using fixed-size chunks ( $\\ln$ -grams) of text data to capture linguistic patterns. Our work studies ICL of $n$ -gram models, where the network needs to compute the conditional probability of the next token based on the statistics of the tokens observed in the input context, rather than on the statistics of the entire training data. We mainly focus on the simple case of $n=2$ ; i.e., bigram models, which can be represented as Markov chains. We therefore consider ICL of Markov chains (ICL-MC): we train two layer attention-only transformers on sequences of tokens, where each sequence is produced by a different Markov chain, generated using a different transition matrix (see Figure 1 (left). ", "page_idx": 1}, {"type": "text", "text": "By studying ICL-MC, we are able to replicate and study multiple phenomena that have been observed in ICL for LLMs, and identify new ones. We demonstrate our findings using a combination of empirical observations on transformers trained from scratch on ICL-MC and theoretical analysis of a simplified linear transformer. Our key findings are summarized below: ", "page_idx": 1}, {"type": "text", "text": "(1) Transformers learn statistical induction heads to optimally solve ICL-MC. Prior work studying ICL in transformers revealed the formation of induction heads [18], a circuit that looks for recent occurrence(s) of the current token, and boosts the probabilities of tokens which followed in the input context. We show that in order to solve ICL-MC, transformers learn statistical induction heads that are able to compute the correct conditional (posterior) probability of the next token given all previous occurrences of the prior token (see attention patterns in Figure 2). We show that these statistical induction heads lead to the transformer achieving performance approaching that of the Bayes-optimal predictor. ", "page_idx": 1}, {"type": "text", "text": "(2) Transformers learn predictors of increasing complexity and undergo a phase transition when increasing complexity. We observe that transformers display phase transitions when learning Markov chainslearning appears to be separated into phases, with fast drops in loss between the phases. We are able to show that different phases correspond to learning models of increased complexity\u2014\u2014unigrams, then bigrams (see Figure 1)\u2014\u2014and characterize the transition between the phases. We also consider the $n$ -gram generalization of our setting, where the next token is generated based on the previous $n-1$ tokens, and observe a similar multi-stage learning process. ", "page_idx": 1}, {"type": "text", "text": "(3) Simplicity bias may slow down learning. We provide evidence that the model's inherent bias towards simpler solutions (in particular, in-context unigrams) causes learning of the optimal solution to be delayed. Changing the distribution of the in-context examples to remove the usefulness of in-context unigrams leads to faster convergence, even when evaluated on the original distribution. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "(4) Alignment of layers is crucial.We show that the transition from a phase of learning the simple-but-inadequate solution to the complex-and-correct solution happens due to an alignment between the layers of the model: the learning signal for the first layer is tied to the extent to which the second layer approaches its correct weights. ", "page_idx": 2}, {"type": "text", "text": "1.1 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In-Context Learning. In [13], the authors discuss how properties of the data distribution promote ICL. Xie et al. [42] suggest a Bayesian interpretation of ICL and studies how ICL emerges when the training distribution comes from a Hidden Markov Model (HMM). Abernethy et al. [2] study the ability of transformers to segment the context into pairs of examples and labels and provide learning guarantees when the labeling is of the form of a sparse function. The work of Bietti et al. [8] studies the dynamics of training transformers on a task that is reminiscent of our Markov chain setting but has additional complexities. Instead of drawing a fresh Markov chain for each sequence, in their task all sequences are sampled from the same Markov chain; after certain \u2018trigger? tokens, the following \u2018output\u2019 token is chosen deterministically within a sequence. Thus, successful prediction requires incorporating both global bigram statistics and in-context deterministic bigram copying, unlike in our setting where the patterns computed by statistical induction heads are necessary and sufficient. As in our work, the authors identify multiple distinct stages of training and show how multiple top-down gradient steps lead to a solution. ", "page_idx": 2}, {"type": "text", "text": "Induction Heads. Elhage et al. [18] relates ICL with the formation of induction heads, subcomponents of transformers that match previous occurrences of the current token, retrieving the token that succeeds the most recent occurrence. Reddy [34] studies the formation of induction heads and their role in ICL, showing empirically that a three layer network exhibits a sudden formation of induction heads towards solving some ICL problem of interest. Bietti et al. [8] study the effect of specific trigger tokens on the formation of induction heads. ", "page_idx": 2}, {"type": "text", "text": "Phase Transitions. It has been observed in different contexts that neural networks and language models display a sudden drop in loss during their training process. This phase transition is often related to emergence of new capabilities in the network. The work of Power et al. [32] observed the \u201cgrokking\u201d phenomenon, where the test loss of neural networks sharply drops, long after the network overfits the training data. Chen et al. [14] shows another example of a phase transition in language model training, where the formation of specific attention mechanisms happen suddenly in training, causing the loss to quickly drop. Barak et al. [7] observe that neural networks trained on complex learning problems display a phase transition when converging to the correct solution. Several works [25, 27] attribute these phase transitions to rapid changes in the inductive bias of networks, while Merrill et al. [29] argue that the models are sparser after the phase change. Schaeffer et al. [35] warn that phenomena in deep learning that seem to be discontinuous can actually be understood to evolve continuously once seen through the right lens. ", "page_idx": 2}, {"type": "text", "text": "Simplicity Bias. Various works observed that neural networks have a \u201csimplicity bias\", which causes them to \u201cprioritize\" learning simple patterns first [5, 39]. The work of Kalimeris et al. [23] shows that SGD learns functions of increased complexity, first fitting a linear concept to the data before moving to more complex functions. [36] shows that the simplicity bias of neural networks can sometimes be harmful, causing them to ignore important features of the data. Chen et al. [14] demonstrate the effect of simplicity bias on language tasks that require understanding of syntactic structure. Abbe et al. [1] provide a theoretical framework for understanding how the simplicity of the target function can govern the convergence time of SGD, describing how simple partial solutions can speed up learning; in contrast, in our setting, the unigram solution appears likely to be a distractor which delays learning of the correct solution. ", "page_idx": 2}, {"type": "text", "text": "Concurrent works. In parallel to this work, there have been a number of papers devoted to the study of similar questions regarding in-context learning or Markov chains: Akyirek et al. [4] empirically compare the ability of different architectures to perform in-context learning of regular languages. Their experiments with synthetic languages motivate architectural changes which improve natural language modeling in large scale datasets. Hoogland et al. [21] observe similar stage-wise learning behaviors on transformers trained on language or synthetic linear regression tasks. Makkuva et al. ", "page_idx": 2}, {"type": "text", "text": "[28] study the loss landscape of transformers trained on sequences sampled from a single Markov Chain. Perhaps closest to our work, Nichani et al. [30] introduces a general family of in-context learning tasks with causal structure, a special case of which is in-context Markov chains. The authors prove that a simplified transformer architecture (similar to the one we introduce in Section 2.2) can learn to identify the causal relationships by training via gradient descent, and also characterize the ability of the trained models to adapt to out-of-distribution data. The focus of our work, instead, is on the different stages of training and how they relate to specific, well-defined, strategies. ", "page_idx": 3}, {"type": "text", "text": "2 Setup", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we describe our learning problem and present the neural network architectures that we will use for learning. ", "page_idx": 3}, {"type": "text", "text": "ICL-MC Task. Our learning task consists of sequences generated from Markov Chains with random transition matrices. The goal is to in-context estimate the transition probabilities from sampled sequences, in order to predict the next state. Formally, each sample sequence is generated by a Markov Chain with state space $S=\\{1,\\ldots,k\\}$ and a transition matrix $\\mathcal{P}$ sampled from a prior distribution, with $x_{1}$ drawn from some other prior distribution (potentially dependent on $\\mathcal{P}$ ), and the rest of $\\pmb{x}=(x_{1},\\dots,x_{T})$ drawn from the Markov Chain. We primarily focus on the case where each row of the matrix is sampled from the Dirichlet distribution with concentration parameter $_{\\alpha}$ , i.e. $\\mathcal{P}_{i,}$ $\\sim\\operatorname*{Dir}(\\alpha)$ . We want to learn a predictor that, given context $x_{1},\\ldots,x_{T}$ predicts the next token, $x_{T+1}$ . Note that this is an inherently non-deterministic task, even provided full information about the transition matrix, and as such it can better capture certain properties of language than previous in-context learning modeling approaches, such as linear regression [20]. We focus on the case of the fat Dirichlet distribution, with $\\mathbf{\\dot{\\alpha}}=(1,\\ldots,1)^{\\top}$ , that corresponds to uniform transition probabilities between states. We draw the initial state $x_{1}$ from the stationary distribution $\\pi$ of the chain (which exists almost surely). We primarily consider the case where the number of states $k$ is 2 or 3. ", "page_idx": 3}, {"type": "text", "text": "In subsection 3.3, we consider the generalization of this setting to $n$ -grams for $n>2$ . Instead of the distribution of $x_{T}$ being determined by $x_{T-1}$ , we let it be determined by $x_{T-n+1},\\ldots,x_{T-1}$ according to a conditional distribution $\\mathcal{P}$ which is uniform over the possible states3. ", "page_idx": 3}, {"type": "text", "text": "2.1 Potential Strategies for (Partially) Solving ICL-MC ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We adopt the Bayesian interpretation of in-context learning [42], in which a prior distribution is provided by the training data, and, at test time, the model updates this prior given the in-context sequence. In this framework, we focus on two strategies for Bayesian inference: a (suboptimal) unigram strategy which assumes tokens in each sequence are i.i.d. samples (and counts the frequency of the states in the sequence so far), and the bigram strategy which correctly takes into account dependencies among adjacent tokens (and counts frequency of pairs of tokens). ", "page_idx": 3}, {"type": "text", "text": "1st strategy: Unigrams.  Since we initialize the Markov chain at its stationary distribution (which exists a.s.), the optimal strategy across unigrams is just to count frequency of states and form a posterior belief about the stationary distribution. Unfortunately, the stationary distribution of this random Markov chain does not admit a simple analytical characterization when there is a finite number of states, but it can be estimated approximately. At the limit of $k\\rightarrow\\infty$ ,thestationary distribution converges to the uniform distribution [10]. ", "page_idx": 3}, {"type": "text", "text": "2nd strategy: Bigrams. For any pair of states $i$ and $j$ , let $\\mathcal{P}_{i j}$ be the probability of transitioning from $i$ to $j$ . On each sample $\\textbf{\\em x}$ , we can focus on the transitions from the $i$ -th state, which follow a categorical distribution with probabilities equal to $(\\mathcal{P}_{i1},\\ldots,\\mathcal{P}_{i k})$ . If we observe the incontext empirical counts $\\{c_{i j}\\}_{j=1}^{k}$ of the transitions, then $\\mathcal{P}_{i j}$ is given by: $\\left(\\mathcal{P}_{i1},...\\,,\\mathcal{P}_{i k}\\right)|x\\rrangle\\sim$ $\\operatorname{Dir}(k,c_{i1}+\\alpha_{1},\\ldots,c_{i k}+\\alpha_{k})$ , where $\\alpha_{1},\\ldots,\\alpha_{k}$ are the Dirichlet concentration parameters of the prior. Hence, each $\\mathcal{P}_{i j}$ has a (marginal) distribution that is actually a Beta distribution: $\\mathcal{P}_{i j}|\\pmb{x}\\sim\\mathrm{Beta}\\left(c_{i j}+\\alpha_{j},\\sum_{j}\\alpha_{j}+N_{i}-\\alpha_{j}-c_{i j}\\right)$ where $N_{i}$ is the total number of observed transitions from statei. Assuch, oubest point estmate foreachstate jisgivenby: [P] = For the uniform Dirichle, \u03b1 = (1, ., 1)T,it is E[Pilel = Nk - ", "page_idx": 3}, {"type": "image", "img_path": "qaRT6QTIqJ/tmp/36ebaba961e727c34f0d3b1d25fbe5d150e298756ce24e998772cc7e7dc691b4.jpg", "img_caption": ["Figure 2: Attention patterns that correspond to the last token of the sequence for a transformer trained to perform ICL-MC. The intensity of each blue line signifies the strength of the corresponding attention value. As the model gets trained, we observe that the attention weights mimic the construction of Proposition 2.1. Specifically, at the end of training $(r i g h t)$ , each token in the first layer is attending to the previous token. In the second layer, the last token, a \u00b0 $\\cdots$ , is attending to tokens that followed \u201c2\"'s, allowing bigram statistics to be calculated. See also Figure 9 for full attention matrices during the course of training. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "2.2  Architectures: Transformers and Simplifications ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We are interested in investigating how transformers [40] can succeed in in-context learning this task. We focus on attention-only transformers with 2 layers with causal masking which is a popular architecture for language modeling. Given an input sequence $\\textbf{\\em x}$ , the output of an $n$ -layer attention-only transformer4 is: ", "page_idx": 4}, {"type": "equation", "text": "$$\nT F(E)=P\\circ(A t t n_{n}+I)\\cdot\\cdot\\cdot\\circ(A t t n_{1}+I)\\circ E,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $E\\in\\mathbb{R}^{T\\times d}$ is an embedding of $\\pmb{x}\\in\\mathbb{R}^{d}$ $P\\in\\mathbb{R}^{d\\times k}$ is a linear projection to the output logits, and $A t t n(x)$ is masked self attention with relative position embeddings [38], which is parameterized by $W_{Q},\\dot{W_{K}},W_{V}\\in\\mathbb{R}^{d\\times d},v\\in\\mathbb{R}^{T\\times d}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\nA t t n(z)=\\mathrm{softmax}(\\mathrm{mask}(A))z W_{V},\\qquad A_{i,j}=\\frac{(z_{i}W_{Q})(z_{j}W_{K}+v_{i-j+1})^{\\top}}{\\sqrt{d}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In general, transformers often contain an MLP module, but for this task they are not necessary (see Appendix A and Figure 10 for additional experiments with transformers with MLPs). During training, weminimize the loss: ", "page_idx": 4}, {"type": "equation", "text": "$$\nL(\\theta)=\\underset{\\mathcal{P}\\sim\\mathrm{Dir}(\\alpha)^{k}}{\\mathbb{E}}\\left[\\frac{1}{t}\\sum_{i=1}^{T}l\\left(T F(\\mathbf{x};\\theta)_{i},x_{i+1}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\theta$ denotes the parameters of the model and $l$ is the cross entropy loss. Notice that we provide supervision in all positions, as standard in language modeling. ", "page_idx": 4}, {"type": "text", "text": "We now show how a two-layer transformer of the above architecture can represent the optimal bigrams solution. ", "page_idx": 4}, {"type": "text", "text": "Proposition 2.1 (Transformer Construction). A single-head two layer attention-only transformer can find the bigram statistics in the in-context learning Markov chain task. ", "page_idx": 4}, {"type": "text", "text": "Intuitively, the first layer of the transformer copies the previous token at each position, and in the second layer each token sums the embeddings of all the tokens whose output from the first layer matches itself. The full proof can be found in Appendix B.1. ", "page_idx": 4}, {"type": "text", "text": "Simplified Transformer Architecture. As we see from the construction, there are two main ingredients in the solution realized by the transformer; (1st layer) the ability to look one token back and (2nd layer) the ability to attend to itself. For this reason, we define a minimal model that is expressive enough to be able to represent such a solution, but also simple enough to be amenable to analysis. Let $e_{x_{i}}$ denote the one-hot embedding that corresponds to the state at position $i\\in[T]$ , and let $E$ be the $\\mathbb{R}^{(\\dot{T}+1)\\times k}$ one-hot embedding matrix. Then the model is parameterized by $W\\in\\mathbb{R}^{k\\times k}$ and $\\boldsymbol{v}\\in\\mathbb{R}^{T+1}$ and defined as: ", "page_idx": 4}, {"type": "image", "img_path": "qaRT6QTIqJ/tmp/bfe26e9910b842fce84b87718f7d0b84859b8c6591eb41948964edfcca26a145.jpg", "img_caption": ["Figure 3: A two layer transformer $(t o p)$ and a minimal model (bottom) trained on our in-context Markov Chain task. A comparison of the two layer attention-only transformer and minimal model (4) (with $v$ and $W$ initialized to O). The graphs on the left are test loss measured by KL-Divergence from the underlying truth. The orange line shows the loss of the unigram strategy, and the green line shows the loss of the bigram strategy. The middle graph shows the effective positional encoding (for the transformer, these are for the first layer, and averaged over all tokens). The graph on the right shows the KL-divergence between the outputs of the models and three strategy. The lower the KL-divergence, the more similar the model is to that strategy. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f(E)=\\operatorname*{max}(E W(\\operatorname{Softmax}(M)E)^{\\top})E,\\quad M=\\left(\\begin{array}{l l l l}{v_{0}}&{-\\infty}&{\\cdots}&{-\\infty}\\\\ {v_{1}}&{v_{0}}&{\\cdots}&{-\\infty}\\\\ {\\vdots}&{\\vdots}&{\\cdots}&{\\vdots}\\\\ {v_{T}}&{v_{T-1}}&{\\cdots}&{v_{0}}\\end{array}\\right)\\in\\mathbb{R}^{(T+1)\\times(T+1)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Wwhere maske $(\\cdot)$ $\\begin{array}{r}{\\operatorname{lax}(M)_{i,j}=\\frac{\\exp(M_{i,j})}{\\sum_{t=0}^{T}\\exp(M_{-},j)}}\\end{array}$ $W$ to mimic the attention mechanism of the second layer and the role of $v$ is that of the relative positional embeddings. This model can be seen as a simplified version of a two-layer linear attention-only transformer. See also Appendix B.2 for a discussion. ", "page_idx": 5}, {"type": "text", "text": "Fact 2.2. Both the bigrams strategy and the unigrams strategy can be expressed by the minimal model with a simple choice of weights. ", "page_idx": 5}, {"type": "text", "text": "\u00b7Bigrams: Let $\\begin{array}{r l r l r l}{\\boldsymbol{v}}&{{}=}&{\\left(0\\quad c\\quad0\\quad\\ldots\\quad0\\right)^{\\top}}\\end{array}$ and $\\begin{array}{r l r}{W}&{{}=}&{I_{k\\times k}}\\end{array}$ \uff0c then $\\begin{array}{r l}{f(E)_{T,s}}&{{}=}\\end{array}$$\\begin{array}{r}{\\sum_{t=2}^{T}\\mathbb{1}\\left\\{x_{t}=s\\right\\}\\mathbb{1}\\left\\{x_{t-1}=x_{T}\\right\\}+O\\left(\\frac{k T^{2}}{\\exp(c)}\\right).}\\end{array}$", "page_idx": 5}, {"type": "text", "text": "See Section B for the proofs. ", "page_idx": 5}, {"type": "text", "text": "3  Empirical Findings and Theoretical Validation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we present our empirical findings on how transformers succeed in in-context learning Markov Chains, we demonstrate the different learning stages during training and the sudden transitions between them, and draw analytical and empirical insights from the minimal model. ", "page_idx": 5}, {"type": "image", "img_path": "qaRT6QTIqJ/tmp/8a3d9b3fa05316e7264f3cde1a3349c3bd76e8580a9c82927dc0e1f2cb6394c6.jpg", "img_caption": ["Figure 4: (left) Unigrams slow down optimization: Comparison of two-layer attention only transformers trained on two distributions; one with a uniformly random doubly stochastic transition matrix and another with a mixture of the doubly stochastic and unigrams distribution (see Appendix A.1 for details). We see that in absence of unigrams \u201csignal' the model minimizes the loss (evaluated on the full distribution) much faster. (center, right) Training of the minimal model on ICL-MC with $k=2$ states: (center) The heatmap of the second layer ( $\\mathcal{W}$ matrix) that learns to be close to diagonal. (right) The values of the positional embeddings (1st layer) that display a curious even/odd pattern. This is before any softmax is applied to the positional embeddings. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "3.1  Transformers In-Context Learn Markov Chains Hierarchically ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We focus on attention-only transformers with 2 layers with causal masking and relative positional encodings and train them with the Adam optimizer on ICL-MC (see Section A for experimental details). As can be seen in Figure 3, all the models converge near the Bayesian optimal solution, suggesting that they learn to implement the bigram strategy. Curiously, however, learning seems to be happening in stages; there is an initial rapid drop and the model quickly finds a better than random solution. Afterwards, there is a long period of only slight improvement before a second rapid drop brings the model close to the Bayes optimal loss. We observe that training a 1-layer transformer fails to undergo a phase transition or converge to the right solution, even if trained for $10\\mathrm{x}$ the amount of time - see Figure 7 in the Appendix. ", "page_idx": 6}, {"type": "text", "text": "Interestingly, as can be seen from the horizontal lines in Figure 3, the intermediate plateau corresponds to a phase when the model reaches the unigram baseline. We provide evidence that this is not a coincidence, and that after the initial drop in loss, the model's strategy is very similar to the unigram strategy, before eventually being overtaken by the bigram strategy. Some of the strongest such evidence is on the right in Figure 3, where we plot the KL divergence between model's prediction and the two different strategies. For both the strategies, their KL divergence from the model quickly goes down, with the unigram solution being significantly lower. Around the point of the second loss drop, the KL divergence between the model and the bigram solution decreases, while the other one increases, making it clear that the model transitions from the one solution to the other. This final drop is what has been associated to prior work with induction heads formation [31]; special dedicated heads inside a transformer are suddenly being formed to facilitate in-context learning. Similar observation hold for Markov Chains with a larger number of states - see Figures 8 and 11. ", "page_idx": 6}, {"type": "text", "text": "Mechanistic evidence for solutions found by transformer.  To confirm how the two layer attentiononly transformer solves ICL-MC, we inspected the attention in each layer throughout training. Figure 2 shows the attention for a particular input during different parts of training. By the end of training, the attention patterns match that of our construction in Proposition 2.1, with the first layer attending to tokens one in the past, and the second layer attending to tokens that follow the same token as the currentone. ", "page_idx": 6}, {"type": "text", "text": "Varying the data distribution - Unigrams slow down learning. There are several interesting phenomena in the learning scenario that we just described, but it is the second drop (and the preceding plateau) that warrants the most investigation. In particular, one can ask the question: is the unigram solution helpful for the eventual convergence of the model, or is it perhaps just a by-product of the learning procedure? To answer these questions, we define distributions over Markov chains that are in between the distribution where unigrams is Bayes optimal, and the distribution where unigrams is as good as uniform - see Appendix A for more details. As we see in Figure 4, the transformers that are being trained on the distribution where there is no unigrams \u201csignal\" train much faster. And even more tellingly, giving additional \u201cunigram samples\" curiously slows down learning. See also Figure 12 in the Appendix that displays how the models perform on different parts of the distribution during training. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "3.2  Theoretical Insights from the Minimal Model ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To abstract away some of the many complicated components from the transformer architecture, we focus our attention now to the minimal model of Section 2.2. We train minimal models of eq. (4), starting from a deterministic constant initialization, by minimizing the cross entropy loss with SGD. Full experimental details can be found in Appendix A. Figure 3 (bottom) displays the training curves for the minimal model. Similar to the transformer, learning occurs in two stages and the models eventually converge close to the optimal solution. ", "page_idx": 7}, {"type": "text", "text": "We now provide theoretical insights on how training progresses stage by stage and how this is achieved by the synergy between the two layers. As it turns out, there need to be at least two steps of gradient descent in order for both elements of the solution to be formed. ", "page_idx": 7}, {"type": "text", "text": "Proposition 3.1. Let the model be defined as in eq. (4) and initialized with $W^{(0)}\\,=\\,{\\bf0},v^{(0)}\\,=$ O.Suppose thetransitionmatrix $P\\,\\in\\,\\mathbb{R}^{k\\times k}$ is sampled from one of the following two types of distribution: ", "page_idx": 7}, {"type": "text", "text": "1. $k=2$ and $P$ is sampled from the uniform distribution over the set of $2\\times2$ stochastic matrices. 2.For any constant $k$ and $0~<~\\alpha~<~1$ \uff0cwith probability $\\alpha$ .sample the matrix $P$ uniformly from a\u201cbigram-only\u201d distribution\u2014the set of $k\\,\\times\\,k$ doubly stochastic matrices; and with probability $1-\\alpha$ use $a$ \"unigram-only distribution\": draw a vector u uniformly from the set $\\widetilde{\\{u\\in\\mathbb{R}_{\\geq0}^{k}:\\|u\\|_{1}=1\\}}$ and let $P=\\mathbf{1}\\dot{u}^{\\top}$ ", "page_idx": 7}, {"type": "text", "text": "Then after one step of population gradient descent with step size $\\eta>0$ ", "page_idx": 7}, {"type": "equation", "text": "$$\nW^{(1)}=\\Theta(\\eta T)I+\\Theta(\\eta T){\\bf11^{\\top}}+E\\,a n d\\,v^{(1)}={\\bf0}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\|E\\|\\le O(\\eta\\log T)$ In other words, for large $T$ the second layer weights are a mixture of the correct solution $I$ anduniform attention $\\mathbf{11^{\\top}}$ ", "page_idx": 7}, {"type": "text", "text": "Assuming in the first step $\\begin{array}{r}{\\eta_{1}=O\\left(\\frac{1}{T^{2}}\\right)}\\end{array}$ , then $W^{(2)}$ has the same structure as $W^{(1)}$ (up to scaling). Furthermore, ", "page_idx": 7}, {"type": "equation", "text": "$$\nv_{1}^{(2)}=\\Omega(\\eta_{2}\\log T),\\;a n d\\;v_{n}^{(2)}\\leq c v_{1}^{(2)}\\;\\forall n\\neq1,c<1\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\eta_{2}$ is the step size for the second step. ", "page_idx": 7}, {"type": "text", "text": "$I f\\,\\eta_{2}=\\Omega(T)$ then the output of the model will be a weighted sum of bigrams and unigrams strategy. Formally, ", "page_idx": 7}, {"type": "equation", "text": "$$\nf(E)_{T,s}=\\Theta(\\eta_{2}T)\\sum_{i=1}^{T}\\mathbb{1}\\left[x_{i-1}=x_{t},x_{i}=s\\right]+\\Theta(\\eta_{2}T)\\sum_{i=1}^{T}\\mathbb{1}\\left[x_{i}=s\\right]+O(\\log T)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Note  In the frst distribution (uniformly random $2\\times2$ ) or the second distribution with $k>6$ , at the end of the two steps, the weight on bigrams is greater than that of the weight on unigrams strategy. ", "page_idx": 7}, {"type": "text", "text": "Proof Overview. The idea of the proof is that a first step of gradient descent with a small learning rate can align the second layer, while a second step can learn to identify the correct relative positional embedding. The identity bias of $W$ in the second layer ensures there is a strong signal in the gradient to look back one in the first layer. Without a bias in $W$ , the gradient for the positional encodings, $v$ is zero. ", "page_idx": 7}, {"type": "text", "text": "We get additional intuition from the second distribution (mixture of unigrams and doubly stochastic): in the first step, effectively all of the gradient comes from the examples where the unigram strategy is optimal, while in the second step effectively all of the gradient comes from the examples where the bigram strategy is optimal. ", "page_idx": 7}, {"type": "text", "text": "Remark 3.2. It is worth noting that, while this is a simplified setting, the analysis goes beyond NTK-based [22] analyses where the representations do not change much and it crucially involves more than one step which has been a standard tool in the analysis of feature learning [6]. ", "page_idx": 7}, {"type": "text", "text": "We summarize the key theoretical implications: ", "page_idx": 7}, {"type": "image", "img_path": "qaRT6QTIqJ/tmp/673bf1367c496b01ed7c23cd98d938ce0de7902843a6f32aabc4961939b79496.jpg", "img_caption": ["Figure 5: Three-headed transformer trained on In-Context Learning 3-grams (trigrams), with context length 200. (left) Loss during training. The model hierarchically converges close to the Bayes optimal solution. (right) KL divergence between the model and different strategies during training. As we observe, there are 4 stages of learning, each of them corresponding to a different algorithm implemented by the model. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Learning occurs in two phases. Both in the theoretical and experimental models, training has two phases that work at very different speeds. The first phase is fast in both cases; in the theoretical setting, even a single step with step size $O\\left({\\frac{1}{T}}\\right)$ is sufficient for learning the second layer. In the second phase, a much larger step size of $\\Omega(1)$ is needed in order to learn the positional encodings (in Onestep). ", "page_idx": 8}, {"type": "text", "text": "Second layer is learned first. It has been observed before in a similar bigram learning setting with a two-layer transformer that the model might be learning first the second layer [8]. We also make similar observations in our experiments with the minimal model and the transformers (see Figure 2). For the minimal model, the gradient calculations, clearly suggest that starting from a default initialization, it is only the second layer that quickly \u201cpicks up\" the right solution. ", "page_idx": 8}, {"type": "text", "text": "Even/odd pattern in positional encodings emerges. We notice in the experiments, that the positional embeddings of both the transformer and minimal model displayed an intriguing even/odd oscillating pattern - see Figure 3 (top, center) and Figure 4 (right). We believe that a careful analysis the gradient of $v$ in the second step will recover this pattern, which is likely related to the moments of the eigenvalues of the transition matrix. ", "page_idx": 8}, {"type": "text", "text": "3.3 Beyond Bigrams: $n$ -gram Statistics ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Finally, we investigate the performance of transformers on learning in-context $n$ gramsfor $n>2$ in particular, 3-grams. We train attention-only transformers with three heads in each layer? by minimizing the in-context cross entropy loss with the Adam optimizer. As can be seen in Figure 5 (left), the model eventually converges to the Bayes optimal solution. Interestingly, as in the case of Markov Chains, the model displays a \u201chierarchical learning\" behavior characterized by long plateaus and sudden drops. In this setup, the different strategies correspond to unigrams, bigrams and trigrams, respectively. This is presented clearly on the right of Figure 5, where we plot the similarity of the model with the different strategies and it exhibits the same clear pattern as in the case of $n=2$ .We leave a more thorough investigation of $n$ grams for future work. ", "page_idx": 8}, {"type": "text", "text": "4   Conclusion and Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we have introduced a simple learning problem which serves as a controlled setting for understanding in-context learning and the emergence of (statistical) induction heads. Through a combination of theoretical analysis and empirical investigation, we identify different stages during learning, which we were able to precisely characterize. These validate similar observations from training large-scale language models. ", "page_idx": 8}, {"type": "text", "text": "The main limitation of our work is that our analysis relies on a simplified transformer architecture and our learning task is synthetic. Yet, we see the simplicity of our modeling as a positive, since it allows to make rigorous predictions about the mechanisms behind in-context learning abilities of a transformer. On the theoretical front, it would be interesting to extend our analysis to handle higher number of symbols and more complex models for language generation (beyond Markov chains). ", "page_idx": 9}, {"type": "text", "text": "On the empirical front, it would be worthwhile to understand similar stage-wise learning with natural language data, and use insights from our minimal model to improve formation of induction heads. In particular, it would be great to understand if better data curriculum could remove the undesirable simplicity bias we observe from unigrams. Such simple but incomplete solutions may be commonplace in language modeling and other rich learning settings; for any such solution, one can ask to what extent its presence speeds up or slows down the formation of more complex circuits with higher accuracy. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements. EE thanks Alan Yan for helpful conversations. NT acknowledges support through the National Science Foundation under NSF Award 1922658. NT would like to thank Boaz Barak, Cengiz Pehlevan and the whole ML Foundations Group at Harvard for their hospitality during Fall 2023 when most of this work was done. BE acknowledges funding from the ONR under award N00014-22-1-2377 and the NSF under award IS 2229881. SG acknowledges support through the OpenAI SuperAlignment Fast Grants. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. Sgd learning on neural networks: leap complexity and saddle-to-saddle dynamics. In The Thirty Sixth Annual Conference on Learning Theory, pages 2552-2623. PMLR, 2023.   \n[2] Jacob D. Abernethy, Alekh Agarwal, Teodor V. Marinov, and Manfred K. Warmuth. A mechanism for sample-efficient in-context learning for sparse retrieval tasks. CoRR, abs/2305.17040, 2023.   \n[3] Ekin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022.   \n[4] Ekin Akyurek, Bailin Wang, Yoon Kim, and Jacob Andreas. In-context language learning: Architectures and algorithms. CoRR, abs/2401.12973, 2024. doi: 10.48550/ARXIV.2401.12973. URL https: //doi.org/10.48550/arXiv.2401.12973.   \n[5] Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio. Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. In International conference on machine learning, pages 233-242. PMLR, 2017.   \n[6] Jimmy Ba, Murat A. Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang. Highdimensional asymptotics of feature learning: How one gradient step improves the representation. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9,2022,2022.   \n[7] Boaz Barak, Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang. Hidden progress in deep learning: Sgd learns parities near the computational limit. Advances in Neural Information Processing Systems, 35:21750-21764, 2022.   \n[8]  Alberto Bieti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou. Birth of a transformer: A memory viewpoint, 2023.   \n[9] Garrett Birkhoff. Three observations on linear algebra. Univ. Nac. Tacuman, Rev. Ser. A, 5: 147-151, 1946.   \n[10] Charles Bordenave, Pietro Caputo, and Djalil Chafai. Circular law theorem for random markov matrices. Probability Theory and Related Fields, 152, 08 2008.   \n[11] Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. Class-based n-gram models of natural language. Comput. Linguist., 18(4):467-479, dec 1992. ISSN 0891-2017.   \n[12]  Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc' Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.   \n[13] Stephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre Richemond, James McClelland, and Felix Hill. Data distributional properties drive emergent in-context learning in transformers. Advances in Neural Information Processing Systems, 35:18878-18891, 2022.   \n[14]  Angelica Chen, Ravid Shwartz-Ziv, Kyunghyun Cho, Matthew L. Leavitt, and Naomi Saphra. Sudden drops in the loss: Syntax acquisition, phase transitions, and simplicity bias in mlms, 2023.   \n[15]  Noam Chomsky. Three models for the description of language. IRE Transactions on information theory, 2(3):113-124, 1956.   \n[16]  Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022.   \n[17] Benjamin L. Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation in self-attention mechanisms, 2022.   \n[18] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for transformer circuits. Transformer Circuits Thread, 1, 2021.   \n[19] Robert Gallager. Discrete Stochastic Processes (Draft of 2nd Edition). MIT OpenCouseWare, 2011.   \n[20] Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn in-context? A case study of simple function classes. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022.   \n[21] Jesse Hoogland, George Wang, Matthew Farrugia-Roberts, Liam Carroll, Susan Wei, and Daniel Murfet. The developmental landscape of in-context learning. CoRR, abs/2402.02364, 2024. doi: 10.48550/ARXIV.2402.02364. URL https : //doi . org/10 .48550/arXiv . 2402 . 02364.   \n[22] Arthur Jacot, Cl\u00e9ment Hongler, and Franck Gabriel.  Neural tangent kernel: Convergence and generalization in neural networks. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 3l: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, pages 8580-8589, 2018. URL https: //proceedings.neurips.cc/paper/2018/hash/ 5a4be1fa34e62bb8a6ec6b91d2462f5a-Abstract.html.   \n[23] Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan Yang, Boaz Barak, and Haofeng Zhang. Sgd on neural networks learns functions of increasing complexity. Advances in neural information processing systems, 32, 2019.   \n[24] Andrej Karpathy. Mingpt. https: //github. com/karpathy/minGPT/tree/master, 2023.   \n[25] Tanishq Kumar, Blake Bordelon, Samuel J. Gershman, and Cengiz Pehlevan. Groking as the transition from lazy to rich training dynamics. CoRR, abs/2310.06110, 2023. doi: 10.48550/ARXIV.2310.06i10. URL https://doi . org/10.48550/arXiv.2310.06110.   \n[26] Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning. In International Conference on Machine Learning, pages 19565-19594. PMLR, 2023.   \n[27]  Kaifeng Lyu, Jikai Jin, Zhiyuan Li, Simon S. Du, Jason D. Lee, and Wei Hu. Dichotomy of early and late phase implicit biases can provably induce grokking. CoRR, abs/2311.18817, 2023.   \n[28]  Ashok Vardhan Makkuva, Marco Bondaschi, Adway Girish, Alliot Nagle, Martin Jaggi, Hyeji Kim, and Michael Gastpar. Attention with markov: A framework for principled analysis of transformers via markov chains. CoRR, abs/2402.04161, 2024. doi: 10.48550/ARXIV.2402. 04161.   \n[29] William Merrill, Nikolaos Tsilivis, and Aman Shukla. A tale of two circuits: Grokking as competition of sparse and dense subnetworks. CoRR, abs/2303.11873, 2023.   \n[30] Eshaan Nichani, Alex Damian, and Jason D Lee. How transformers learn causal structure with gradient descent. arXiv preprint arXiv:2402.14735, 2024.   \n[31] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022. https://ransformer-circuits.pub/2022/in-context-learning-and-inductionheads/index.html.   \n[32]  Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177, 2022.   \n[33]  Nived Rajaraman, Marco Bondaschi, Kannan Ramchandran, Michael Gastpar, and Ashok Vardhan Makkuva. Transformers on markov data: Constant depth suffices. arXiv preprint arXiv:2407.17686, 2024.   \n[34]  Gautam Reddy. The mechanistic basis of data dependence and abrupt learning in an in-context classification task, 2023.   \n[35] Rylan Schaeffer Brando Miranda and Sanmi Koyejo. Are emergent abilities of large language models a mirage? CoRR, abs/2304.15004, 2023.   \n[36] Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. The pitall of simplicitybias in neural networks.Advances in Neural Information Processing Systems, 33:9573-9585, 2020.   \n[37]  Claude Elwood Shannon. A mathematical theory of communication. The Bell system technical journal, 27(3):379-423, 1948.   \n[38]  Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-atention with relative position representations. In Marilyn A. Walker, Heng Ji, and Amanda Stent, editors, Proceedings of the 2018 Conference of the North American Chapter of theAssociation forComputational Linguistics: Human Language Technologies, NAACL-HLT, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 2 (Short Papers), pages 464-468. Association for Computational Linguistics, 2018.   \n[39] Guillermo Valle-Perez, Chico Q Camargo, and Ard A Louis. Deep learning generalizes because the parameter-function map is biased towards simple functions. arXiv preprint arXiv:1805.08522, 2018.   \n[40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Ilia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett,editors,Advances inNeural InformationProcessing Systems 30:Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998-6008, 2017.   \n[41] Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and Peter L Bartlett. How many pretraining tasks are needed for in-context learning of linear regression? arXiv preprint arXiv:2310.08391, 2023.   \n[42] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A  Experimental Details and Additional Experiments ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1  Experimental details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We train transformers of the form (1) with the AdamW optimizer with learning rate $3\\times10^{-5}$ (for 3-grams a learning rate of $3\\times10^{-2}$ was used), batch size 64, and hidden dimension 16. The sequence length of the examples is 100 tokens. The minimal model was trained with SGD, with batch size 64, and learning rate $3^{\\star}\\times10^{-4}$ .We use PyTorch 2.1.2. ", "page_idx": 12}, {"type": "text", "text": "The data was generated in an online fashion, using numpy.random.dirichlet to generate each row of the transition matrices. At each epoch, we generate a new transition matrix, and, then, a sequence from the induced Markov chain (starting from the stationary distribution of the chain). Both the model initialization (for the transformers) and the data were randomized based on the seed (in a perfectly reproducible manner). ", "page_idx": 12}, {"type": "text", "text": "Some of the training and model code was based on minGPT [24]. The experiments all measure the outputs of the models at the last token. ", "page_idx": 12}, {"type": "text", "text": "All of the experiments were performed with a single NVIDIA GeForce GTX 1650 Ti GPU with 4 gigabytes of vram with 32 gigabytes of system memory. Each training run took under ten minutes. ", "page_idx": 12}, {"type": "text", "text": "The code used can be found at: https://github.com/EzraEdelman/Evolution-of-Statistical-Induction-Heads. ", "page_idx": 12}, {"type": "text", "text": "Details for experiment on the left in Figure 4  In this experiment, we compared the test loss of the model trained in too different ways. Consider two distributions, each uniform over their support (subsets of the space of $3\\times3$ transition matrices for this particular experiment): ", "page_idx": 12}, {"type": "text", "text": "1. The \u201cdoubly stochastic\" distribution is uniform over the space of doubly stochastic matrices, that is, transition matrices for which each row or column has entries summing to 1. This is equivalent to the space of Markov Chains with a uniform stationary distribution (that is, $\\scriptstyle\\pi\\ ={\\frac{1}{k}}\\mathbb{I}.$ ), which means that the unigram and uniform strategy are the same. 2. The \u201cunigram distribution\u201d is uniform over the space of stochastic matrices $P$ such that for any tworows $P_{i},P_{j}$ in $P$ $P_{i}=P_{j}$ . This is equivalent to the distribution over markov chains for which the distribution of the next state doesn't change depending on the previous state. Notice that the unigram strategy is asymptotically optimal on any markov chain from this distribution. ", "page_idx": 12}, {"type": "text", "text": "We first train models on a number of samples from the doubly stochastic distribution, and plot their training loss as the blue line. Then, we trained models on a random mixture of the two distributions, with $75\\%$ of the samples coming from doubly stochastic distribution, and the remaining $25\\%$ coming from the unigram distribution. Importantly, we train this second batch of models on more total samples, so that they see the same number of samples from the doubly stochastic distribution, and then plot their test loss in orange with the $\\mathbf{X}$ -axis being the number of samples from the doubly stochastic distribution seen. This means that every point on the ${\\bf X}$ -axis in the graph, the models in the orange line have seen more samples than seen by those in the blue line, and yet still take longer to converge. ", "page_idx": 12}, {"type": "text", "text": "We would also like to note that the test loss measured is in the distribution uniform over all stochastic matrices, which the models in the orange line do seem to generalize slightly better to after convergence. ", "page_idx": 12}, {"type": "text", "text": "Note on KL-divergence In our experiments, we used KL divergence to measure the difference between the probabilities predicted by the model and various other probability distributions. For test loss (depicted, for instance, in Figure 3 (left)), this other distribution comes from the appropriate rows of the transition matrices used to generate the test examples. ", "page_idx": 12}, {"type": "text", "text": "Formally, let $f(x_{1:T-1})$ be the softmax distribution of the transformer's output, given the input sequence $\\pmb{x}_{1:T-1}$ . In our standard setting, we measured ", "page_idx": 12}, {"type": "equation", "text": "$$\nd_{K L}(\\mathcal{P}_{\\mathbf{x}_{T-1}}||f(\\mathbf{x}_{1:T-1}))\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\mathcal{P}_{\\mathbf{x}_{T-1}}$ is the true distribution of the next state $x_{T}$ given the previous state, under the true Markov chain $\\mathcal{P}$ .Note that $\\mathcal{P}$ varies from sequence to sequence (it is drawn from a prior over transition matrices) and is not directly observable by the learner\u2014this is what needs to be learned in-context. ", "page_idx": 12}, {"type": "image", "img_path": "qaRT6QTIqJ/tmp/cbb7885ca3332b16f542dc2f4970bb2a190467b606550c5942fbf8aeb46e5c4c.jpg", "img_caption": ["Figure 6: In distribution test loss for 10 two layer attention only transformers, with random seeds $0,1,\\ldots9$ (randomness affects initialization and the training data). The training dynamics are consistent for each model, though the exact position of the phase transitions changes. "], "img_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "qaRT6QTIqJ/tmp/744d74d94d2e0852ffab1a6e3d8c0c965066ce2b9eab0585b3b5722583716130.jpg", "img_caption": ["Figure 7: Graphs of test loss showing that a single layer transformer can not achieve good performance on ICL-MC. This result holds for transformers with or without MLPs, and with absolute or relative positional encodings. These graphs show that even trained 8 times longer, there is no notable increase in performance beyond the unigrams strategy (orange line). "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "For measuring how close the model was to various strategies (for instance, in Figure 3 (right), we computed the predicted probabilities given by said strategies, and used those as the base distribution. Note that the output of the bigrams strategy (which is Bayes-optimal for our base setting) is different from the aforementioned ground-truth $\\mathcal{P}_{\\mathbf{x}_{T-1}}$ ). Instead, as described in Section 2, it is a Bayesian posterior distribution of the next state given the observed sequence, with the prior determined by the prior distribution of transition matrices. Formally, this corresponds to $\\mathbb{E}[\\mathcal{P}_{\\mathbf{x}_{T-1}}|\\mathbf{x}_{1:T-1}]$ where the expectation is taken over the draw of Markov chain transition matrix. ", "page_idx": 13}, {"type": "text", "text": "A.2  Additional experiments ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In the main text, we mainly show experiments with one seed per experiment, in order to keep presentation simple. Figure 6 justifies this choice: it plots the test loss of two-layer transformers (multiple random seeds) trained on ICL-MC for chains with 3 states. Randomness slightly affects the duration of the plateau, but not the qualitative, two-stage, process of learning. ", "page_idx": 13}, {"type": "text", "text": "Figure 7 demonstrates the inability of one layer attention only transformers to in-context learn Markov Chains with 3 states. ", "page_idx": 13}, {"type": "text", "text": "Figure 8 shows that our results extend for Markov Chains with larger state spaces (here, $k=8$ ).As the number of states grows, larger sequence lengths are needed for learning (this is to be expected as a larger transition matrix needs to be estimated in context - roughly, sequence length needs to be $\\Omega(k^{2}))$ ", "page_idx": 13}, {"type": "image", "img_path": "qaRT6QTIqJ/tmp/ba73c30e24c2f52f51f28b2e1ab7ef356560b788d3de2fb71213051eb1d3f637.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 8: Our results extend to more symbols than $k=2$ Or $k=3$ .TheKL-divergencebetween the transformer and strategies over training. This required a sequence length greater than 100 (200 in this case) for the difference between unigrams and uniform to be large enough for the unigram phase to be visible (in either case there was a plateau before the final drop in test loss). ", "page_idx": 14}, {"type": "image", "img_path": "qaRT6QTIqJ/tmp/ee0a1cc780a76ca8528492ba9a904977ab30e683bca9b984468be29a85a05227.jpg", "img_caption": ["Figure 9: A two layer attention-only transformer trained with cross entropy loss on ICL-MC. The heatmaps on the right represent part of the attention for the transformer at various time steps, specifically the values of the matrix $A$ from (2). The top row are showing $A$ from the first layer, and the bottom row from the second layer. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "We also know that, in this particular family of Markov Chains, the stationary distribution approaches the uniform distribution as the number of states grows. As a result, we expect the difference between uniform and unigram solutions to be less noticeable. ", "page_idx": 14}, {"type": "text", "text": "Figure 9 shows the attention patterns for the two layers of a transformer during training. ", "page_idx": 14}, {"type": "text", "text": "In Figure 10, we demonstrate that our observations extend to two-layer transformers, which have an additional fully-connected MLP on top of the attention layers. ", "page_idx": 14}, {"type": "text", "text": "Finally, in Figure 12 we plot the performance of the transformer and the minimal model in various distributions over the course of training. ", "page_idx": 14}, {"type": "text", "text": "B Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we present our theoretical results on in-context learning Markov Chains of Section 2.2. ", "page_idx": 14}, {"type": "image", "img_path": "qaRT6QTIqJ/tmp/b944e6c7ae9d8c76b0959989c887e40bd6da84d1c018767f88ee6602fa572af1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 10: A two layer relative position encoding transformer with MLPs trained on ICL-MC with $\\scriptstyle\\mathbf{k}=3$ symbols. Notice while slightly noisier, the overall trend and observations made regarding the attention only transformer still hold. ", "page_idx": 15}, {"type": "image", "img_path": "qaRT6QTIqJ/tmp/1568eb36e85fc17596e944b151d6da6288a7fe21c45be0f2a9c3ce23272b8f22.jpg", "img_caption": ["Two Layer Transformer: 3 Symbols ", "Figure 11: A comparison of the two layer attention only transformer and minimal model for $k=3$ symbols. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "B.1  Transformer Construction ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof of Proposition 2.1. Set the internal dimension $d\\ \\ =\\ \\ 3k$ , and choose $\\mathbf{e}_{x}$ to be one-hot embeddings\u2014-that is, $\\mathbf{e}_{\\pmb{x}_{i}}~=~\\delta_{\\mathbf{x}_{i}}$ , where $\\delta$ is the Kronecker delta. We will call the parameters o ateationlayer $i$ $W_{Q}^{(i)},W_{K}^{(i)},W_{V}^{(i)},v^{(i)}$ Let ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\boldsymbol{v}^{(1)}=\\left(\\!\\!\\begin{array}{c}{\\delta_{2}\\mathbf{1}_{k}^{\\intercal}}\\\\ {\\mathbf{0}}\\\\ {\\mathbf{0}}\\end{array}\\!\\!\\right)\\quad\\boldsymbol{W}_{Q}^{(1)}=\\left(\\!\\!\\begin{array}{c c c}{c I^{k\\times k}}&{\\mathbf{0}}&{\\mathbf{0}}\\\\ {\\mathbf{0}}&{\\mathbf{0}}&{\\mathbf{0}}\\\\ {\\mathbf{0}}&{\\mathbf{0}}&{\\mathbf{0}}\\end{array}\\!\\!\\right)\\quad\\boldsymbol{W}_{K}^{(1)}=\\mathbf{0}\\quad\\boldsymbol{W}_{V}^{(1)}=\\left(\\!\\!\\begin{array}{c c c}{\\mathbf{0}}&{I^{k\\times k}}&{\\mathbf{0}}\\\\ {\\mathbf{0}}&{\\mathbf{0}}&{\\mathbf{0}}\\\\ {\\mathbf{0}}&{\\mathbf{0}}&{\\mathbf{0}}\\end{array}\\!\\!\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "So, ", "page_idx": 15}, {"type": "equation", "text": "$$\nA_{i,j}^{(1)}=\\frac{(e_{i}W_{Q}^{(1)})(v_{i-j+1}^{(1)})^{\\top}}{\\sqrt{d}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Notice that $A_{i,j}^{(1)}\\,=\\,c\\mathbb{1}[j\\,=\\,i\\,-\\,1]$ . So, softmax $(\\operatorname*{mask}(A))_{i,j}^{(1)}\\approx\\mathbb{1}[j=i-1]$ for large enough $c$ So, for any $2\\,\\le\\,i\\,<\\,T,1\\,\\le\\,j\\,<\\,k,\\,A t t n_{1}(e)_{i,j+k}\\,=\\,e_{i-1,j}$ Effectively, the first layer appends the embedding of the previous token after the embedding of the current token, so that the output at position $i$ is approximately $\\begin{array}{r l r}{(e_{x_{i}}}&{{}e_{x_{i-1}}}&{\\mathbf{0})}\\end{array}$ ", "page_idx": 15}, {"type": "image", "img_path": "qaRT6QTIqJ/tmp/a312384138c46460c99b7af49cac7960c16c9acecaf947f9d40073d91e4e391c.jpg", "img_caption": ["Figure 12: A two layer attention-only transformer (top) and minimal model (4) (bottom), trained on the main task with ICL-MC with cross entropy loss, test loss measured by KL-Divergence from the underlying truth (labels based on transition probabilities, not samples). The distributions test loss is measured in are (from left to right) in-distribution, a distribution where each token is sampled i.i.d., and a distribution over uniformly random doubly stochastic transition matrices (that is, a setting where the stationary distribution is the uniform distribution, which implies that unigram based guesses are as good as guessing uniform probability). For both models, the in distribution test loss quickly drops to the level of the unigram algorithm. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "The second layer is defined as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\nv^{(2)}=\\mathbf{0}\\quad W_{Q}^{(2)}=\\left(\\!\\!\\begin{array}{c c c}{{c I^{k\\times k}}}&{{\\mathbf{0}}}&{{\\mathbf{0}}}\\\\ {{\\mathbf{0}}}&{{\\mathbf{0}}}&{{\\mathbf{0}}}\\\\ {{\\mathbf{0}}}&{{\\mathbf{0}}}&{{\\mathbf{0}}}\\end{array}\\!\\!\\right)\\quad W_{K}^{(2)}=\\left(\\!\\!\\begin{array}{c c c}{{\\mathbf{0}}}&{{\\mathbf{0}}}&{{\\mathbf{0}}}\\\\ {{I^{k\\times k}}}&{{\\mathbf{0}}}&{{\\mathbf{0}}}\\\\ {{\\mathbf{0}}}&{{\\mathbf{0}}}&{{\\mathbf{0}}}\\end{array}\\!\\!\\right)\\quad W_{V}^{(2)}=\\left(\\!\\!\\begin{array}{c c c}{{\\mathbf{0}}}&{{\\mathbf{0}}}&{{I^{k\\times k}}}\\\\ {{\\mathbf{0}}}&{{\\mathbf{0}}}&{{\\mathbf{0}}}\\\\ {{\\mathbf{0}}}&{{\\mathbf{0}}}&{{\\mathbf{0}}}\\end{array}\\!\\!\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that $z=e+A t t n_{1}(e)$ , then ", "page_idx": 16}, {"type": "equation", "text": "$$\nA_{i,j}^{(2)}=\\frac{(z_{i}W_{Q}^{(2)})(z_{j}W_{K}^{(2)})^{\\top}}{\\sqrt{d}}=\\frac{c e_{x_{i}}(e_{x_{j-1}})^{\\top}}{\\sqrt{d}}=\\frac{c}{\\sqrt{d}}\\Im[x_{j-1}=x_{i}].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "So, for all $\\begin{array}{r}{j\\,<\\,i,\\,\\mathrm{softmax}(\\mathrm{mask}(A))_{i,j}\\,\\approx\\,\\frac{\\mathbb{1}[x_{j-1}=x_{i}]}{\\sum_{h=1}^{i}\\mathbb{1}[x_{h-1}=x_{i}]}}\\end{array}$ for large enough $c$ . For any $2\\leq i<$ $T,1\\leq j<k$ ", "page_idx": 16}, {"type": "equation", "text": "$$\nA t t n_{2}(e)_{i,j+2k}=\\sum_{h=1}^{3k}\\frac{\\mathbb{1}[x_{h-1}=x_{i}]}{\\sum_{g=1}^{i}\\mathbb{1}[x_{g-1}=x_{i}]}(z W_{V}^{(2)})_{h,j}=\\frac{\\sum_{h=1}^{k}\\mathbb{1}[x_{h-1}=x_{i}]\\mathbb{1}[x_{h}=j]}{\\sum_{g=1}^{i}\\mathbb{1}[x_{g-1}=x_{i}]}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This is exactly the empirical bigram statistics (that is, the number of times $x_{i}\\rightarrow j$ appearsbefore position $i$ ). In order to make this the output, we set $P=\\left(\\!\\!{\\begin{array}{c}{{{\\bf0}}}\\\\ {{{\\bf0}}}\\\\ {{I^{k\\times k}}}\\end{array}}\\!\\!\\right)\\,6$ \u53e3 ", "page_idx": 16}, {"type": "text", "text": "B.2 ICL-MC with Minimal Model ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Setup and notation  Our data consists of sequences of length $T+1$ \uff0c $\\pmb{x}=(x_{0},\\dots,x_{T})$ drawn from a Markov Chain with state space $S=\\{1,\\ldots,k\\}$ (i.e., $x_{j}\\in\\{1,\\ldots,k\\}$ for all $j\\in[T])$ , and a transition matrix $P$ . Each row of the matrix is sampled from a fat Dirichlet distribution, i.e. $P_{i}\\sim\\operatorname{Dir}({\\bf1})$ , corresponding drawing the row from a uniform distribution over the simplex. Let ", "page_idx": 16}, {"type": "text", "text": "$E\\,\\in\\,\\{0,1\\}^{(T+1)\\,\\times\\,k}$ be the one hot embedding matrix of $x$ ,that is, $E_{i,x_{i}}\\,=\\,1\\$ and for all $s\\neq x_{i}$ $E_{i,s}=0$ . A crucial difference with parallel work which also studies in-context learning of Markov Chains [30] is that the whole sequence is sampled from the Markov Chain (whilst in [30], the penultimate token is sampled from the uniform distribution). ", "page_idx": 17}, {"type": "text", "text": "Model  We define our model as a simplified sequence to sequence transformer $f\\,:\\,\\mathbb{R}^{T\\times k}\\,\\rightarrow$ $\\mathbb{R}^{(T+1)\\times k}$ with $f(E)=\\operatorname*{mask}(E W(\\mathrm{Softmax}(M)E)^{\\top})E$ The trainable parameters are $W\\in\\mathbb{R}^{k\\times k}$ and $\\boldsymbol{v}\\in\\mathbb{R}^{T+1}$ . We define $M\\in\\mathbb{R}^{(T+1)\\times(T+1)}$ as $M=\\left(\\begin{array}{c c c c}{v_{0}}&{-\\infty}&{...}&{-\\infty}\\\\ {v_{1}}&{v_{0}}&{...}&{-\\infty}\\\\ {\\vdots}&{\\vdots}&{...}&{\\vdots}\\\\ {v_{T}}&{v_{T-1}}&{...}&{v_{0}}\\end{array}\\right),$ that is, for all $T\\,\\geq\\,i\\,\\geq\\,j\\,\\geq\\,0$ $M_{i,j}\\,=\\,v_{i-j}$ and if $i\\,>\\,j,\\,M_{j,i}\\,=\\,-\\infty$ .Furthermore, $v\\,=\\,[v_{0},v_{2},\\dots,v_{T}]\\,\\in$ $\\mathbb{R}^{t\\times T+1}$ . Softmax is defined as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{Softmax}(M)_{i,j}=\\frac{\\exp\\left(M_{i,j}\\right)}{\\sum_{T=1}^{T}\\exp\\left(M_{i,j}\\right)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The logit for symbol $s$ at position $T$ for our model is: ", "page_idx": 17}, {"type": "equation", "text": "$$\nf(E)_{T,s}=\\sum_{u=1}^{k}\\sum_{i=0}^{T}\\sum_{j=0}^{i}W_{x_{T},u}\\mathbb{1}[x_{i-j}=u\\wedge x_{i}=s]\\frac{\\exp(v_{j})}{\\sum_{\\ell=0}^{i}\\exp(v_{\\ell})}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The model can represent the unigrams and bigrams solutions as following: ", "page_idx": 17}, {"type": "text", "text": "\u00b7 Construction for bigrams: $\\boldsymbol{v}~~=~(0,c,0,\\ldots,0)^{\\top}$ and ${W\\mathrm{~\\,~}=\\,\\,\\,I_{k\\times k}}$ , then $f(E)_{T,s}\\;\\;=\\;\\;$ $\\begin{array}{r}{\\sum_{i=0}^{T}\\mathbb{1}\\left[x_{i}=s\\wedge x_{i-1}=x_{T}\\right]+O\\left(\\frac{T^{3}}{\\exp(c)}\\right)}\\end{array}$ As $c$ tendstoit, s \u00b7 Construction for unigrams: $v={\\bf0}$ and $\\begin{array}{r}{W=\\frac{1}{k}\\mathbf{1}^{\\top}\\mathbf{1}}\\end{array}$ , then $\\begin{array}{r}{f(E)_{T,s}=\\sum_{i=0}^{T}\\mathbb{1}\\left[x_{i}=s\\right]\\!.}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "We prove the above claims. ", "page_idx": 17}, {"type": "text", "text": "Proof of Fact 2.2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. We will first prove the unigrams construction. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle f(E)_{T,s}=\\sum_{u=1}^{k}\\sum_{i=0}^{T}\\sum_{j=0}^{i}W_{x_{T},u}\\mathbb{I}\\big[x_{i-j}=u\\wedge x_{i}=s\\big]\\frac{\\exp(v_{j})}{\\sum_{\\ell=0}^{i}\\exp(v_{\\ell})}}\\\\ {\\displaystyle}&{=\\sum_{u=1}^{k}\\sum_{i=0}^{T}\\sum_{j=0}^{i}\\mathbb{I}\\big[x_{i-j}=u\\wedge x_{i}=s\\big]\\frac{1}{i}}\\\\ {\\displaystyle}&{=\\frac{1}{k}\\sum_{i=0}^{T}\\sum_{j=0}^{i}\\mathbb{I}\\big[x_{i}=s\\big]\\frac{k}{i}}\\\\ {\\displaystyle}&{=\\sum_{i=0}^{T}\\mathbb{I}\\big[x_{i}=s\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which is exactly the unigrams solution. ", "page_idx": 17}, {"type": "text", "text": "Now consider the bigrams construction. As $c$ grows, the softmax of $v$ very quickly becomes one hot. Formally, by Lemma B.7 in [17], for any $i>0$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\frac{\\exp(v_{j})}{\\sum_{\\ell=0}^{i}\\exp(v_{\\ell})}}=\\mathbb{1}\\left[j=1\\right]+O\\left({\\frac{T}{\\exp(c)}}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "So, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle(E)_{T,\\alpha}=\\sum_{i=1}^{k}\\displaystyle\\sum_{m=1}^{r}\\displaystyle\\sum_{n=1}^{i}\\|E_{\\alpha,\\gamma,n}\\|_{\\mathbb{H}_{r}^{1},\\cdot=j}=u\\wedge x_{i}=s_{i}\\displaystyle\\sum_{\\sum_{i=0}^{i-1}\\alpha_{i}\\exp(y_{i})}^{\\exp(y_{i})}}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}&{\\displaystyle=\\sum_{i=0}^{T}\\displaystyle\\sum_{j=0}^{i}\\|\\mathbb{I}_{\\tilde{H}_{r}^{i}-j}=x_{T}\\wedge x_{i}=s_{j}\\displaystyle\\sum_{\\sum_{i=0}^{i-1}\\alpha_{i}\\exp(y_{i})}^{\\exp(y_{i})}}\\\\ {\\displaystyle}\\\\ &{\\displaystyle=\\|[x_{T}=x_{0}\\circ s]+\\sum_{i=1}^{r}\\displaystyle\\sum_{j=1}^{i}\\|[x_{T}=x_{i-j}\\wedge x_{i}=s_{i}]-\\displaystyle\\sum_{i=1}^{\\exp(y_{i})}}\\\\ {\\displaystyle}\\\\ &{\\displaystyle=1[x_{T}=x_{0}\\circ s]+\\sum_{i=1}^{r}\\displaystyle\\sum_{m=1}^{i-1}\\|x_{T}=x_{i-j}\\wedge x_{i}=s_{i}\\Big(\\underbrace{1[j=1]+O\\Big(\\frac{2T}{\\exp(e)}\\Big)}_{t_{1}[j=1]/\\alpha_{1}\\exp(e)}\\Big)\\ (\\mathrm{Lemms})}\\\\ {\\displaystyle}\\\\ &{\\displaystyle=1[x_{T}=x_{0}\\circ s]+\\sum_{i=1}^{r}\\|[x_{T}=x_{i-1}\\wedge x_{i}=s]+\\sum_{i=1}^{r}\\displaystyle\\sum_{j=0}^{i}\\|[x_{T}=x_{i-j}\\wedge x_{i}=s]O\\Big(\\frac{2T}{\\exp(y_{i})}}\\\\ {\\displaystyle}\\\\ &{\\displaystyle=\\sum_{i=1}^{T}\\|x_{T}=x_{i-1}\\wedge x_{i}=s\\Big)+\\sum_{i=1}^{r}O\\Big(\\frac{T}{\\exp(e)}\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This simplified model was constructed by taking a two layer transformer with relative positional encodings and simplifying it. Our construction for how transformers would form induction heads (corroborated with experiments such as the viewing of attention patterns in figure 2) implies that the MLPs and the value matrices could just be identity functions, and the first layer query matrix, and the second layer positional embeddings were zero matricies, so in the simplified model we froze these parameters to there final states. We also remove the softmax on the attention in the first layer. Despite these changes, the training dynamics, our main interest, stay remarkably similar. ", "page_idx": 18}, {"type": "text", "text": "Training We analyze gradient descent with the cross entropy loss $\\begin{array}{r l}{L_{T}(f,E,x_{T+1})}&{{}=}\\end{array}$ -s=1 log Softmax $\\left(f(E)\\right)_{T,s}P_{X_{T},s}{}^{7}$ ", "page_idx": 18}, {"type": "text", "text": "B.3 Gradient Calculations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For use in the proofs, here we show the calculations of the gradients of the model with respect to the parameters, and the loss with respect to the model. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial f({\\cal E})_{T,s}}{\\partial W_{a,b}}=\\sum_{u=1}^{k}\\sum_{i=0}^{T}\\sum_{j=0}^{i}\\mathbb{I}\\big[x_{T}=a\\wedge b=u\\big]\\mathbb{I}\\big[x_{i-j}=u\\wedge x_{i}=s\\big]\\frac{\\exp(v_{j})}{\\sum_{\\ell=0}^{i}\\exp(v_{\\ell})}}\\\\ {\\displaystyle\\qquad=\\sum_{i=0}^{T}\\sum_{j=0}^{i}\\mathbb{I}\\big[x_{T}=a\\big]\\mathbb{I}\\big[x_{i-j}=b\\wedge x_{i}=s\\big]\\frac{\\exp(v_{j})}{\\sum_{\\ell=0}^{i}\\exp(v_{\\ell})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial f({\\cal E})_{T,s}}{\\partial v_{a}}}\\\\ {\\displaystyle=\\sum_{u=1}^{k}\\sum_{i=0}^{T}\\sum_{j=0}^{i}W_{x_{T},u}\\mathbb{I}\\big[x_{i-j}=u\\wedge x_{i}=s\\big]\\left(\\mathbb{I}[j=a]\\frac{\\exp(v_{a})}{\\sum_{\\ell=0}^{i}\\exp(v_{\\ell})}-\\mathbb{I}[a\\leq i]\\frac{\\exp(v_{j})}{\\sum_{\\ell=0}^{i}\\exp(v_{\\ell})}\\frac{\\exp(\\ln(j-i)\\phi)}{\\sum_{\\ell=0}^{i}\\exp(v_{\\ell})}\\right)}\\\\ {\\displaystyle=\\sum_{u=1}^{k}\\sum_{i=0}^{T}\\frac{\\exp(v_{a})}{\\sum_{\\ell=0}^{i}\\exp(v_{\\ell})}\\sum_{j=0}^{i}W_{x_{T},u}\\left(\\mathbb{I}[x_{i-a}=u\\wedge x_{i}=s]\\mathbb{I}[j=a]-\\mathbb{I}[x_{i-j}=u\\wedge x_{i}=s]\\mathbb{I}[a\\leq i]\\mathbb{I}[j=a]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "7In practice, one could use the empirical value of $x_{T+1}$ rather than its distribution $P_{X_{T},s}$ , but in full batch gradient descent this is in fact equivalent. This is because conditional on $x_{T}$ and $P$ $x_{T+1}$ is independent of $x_{1},\\cdot\\cdot\\cdot x_{T-1}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle=\\sum_{u=1}^{k}\\sum_{i=0}^{T}\\frac{\\exp(v_{a})}{\\sum_{\\ell=0}^{i}\\exp(v_{\\ell})}W_{x_{T},u}\\left(\\mathbb{1}[x_{i-a}=u\\wedge x_{i}=s]\\mathbb{1}[a\\leq i]-\\sum_{j=0}^{i}\\mathbb{1}[x_{i-j}=u\\wedge x_{i}=s]\\mathbb{1}[a\\leq i]\\mathbb{1}[a\\leq i]\\right)}\\\\ {\\displaystyle=\\sum_{u=1}^{k}\\sum_{i=a}^{T}\\frac{\\exp(v_{a})}{\\sum_{\\ell=0}^{i}\\exp(v_{\\ell})}W_{x_{T},u}\\left(\\mathbb{1}[x_{i-a}=u\\wedge x_{i}=s]-\\sum_{j=0}^{i}\\mathbb{1}[x_{i-j}=u\\wedge x_{i}=s]\\frac{\\exp(v_{j})}{\\sum_{\\ell=0}^{i}\\exp(v_{\\ell})}\\right)}\\\\ {\\displaystyle=\\sum_{u=1}^{k}\\sum_{i=a}^{T}\\frac{\\exp(v_{a})}{\\sum_{\\ell=0}^{i}\\exp(v_{\\ell})}W_{x_{T},u}\\mathbb{1}[x_{i}=s]\\left(\\mathbb{1}[x_{i-a}=u]-\\sum_{j=0}^{i}\\mathbb{1}[x_{i-j}=u]\\frac{\\exp(v_{j})}{\\sum_{\\ell=0}^{i}\\exp(v_{\\ell})}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\partial L_{T}}{\\partial f(E)_{T,s}}=\\mathrm{Softmax}(f(E))_{T,s}-P_{x_{T},s}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B.4 Proof of lemma 3.1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. Recall that at initialization, $v={\\bf0}$ and $W={\\bf0}$ , implying further that $\\boldsymbol{f}(E)=\\mathbf{0}$ ", "page_idx": 19}, {"type": "text", "text": "First step. ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "First consider the gradient of the loss with respect to $W$ . By chain rule, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{1}{\\sqrt{\\pi C}\\beta}(E)=\\frac{\\hbar}{C}\\frac{\\hbar}{C}\\frac{\\partial E}{\\partial E(E)}\\frac{\\partial E(E)}{\\partial E_{\\mathrm{s}}}}\\\\ {\\partial\\hbar\\alpha_{\\mathrm{s}}=}&{\\frac{1}{\\sqrt{\\pi}C}\\frac{\\partial E(E)}{\\partial E_{\\mathrm{s}}}\\frac{\\partial E}{\\partial E_{\\mathrm{s}}}\\alpha_{\\mathrm{s}}\\,,}\\\\ &{=\\sum_{i}^{\\infty}\\mathrm{Stana}\\{E(E)\\}_{\\mathrm{re}}-\\hbar\\gamma_{\\mathrm{s}}\\sum_{i}^{\\infty}\\mathrm{I}[\\varepsilon_{i}\\,\\gamma_{i}-\\varepsilon_{i}\\,][\\varepsilon_{i}\\,\\gamma_{i}-\\hbar\\,k\\,\\chi_{i}-\\hbar\\,\\chi_{i}\\frac{\\partial E(E)}{\\partial E_{\\mathrm{s}}\\partial E(E)}}\\\\ &{-\\frac{\\hbar}{\\sqrt{\\pi}C}\\left(\\frac{1}{\\hbar}\\,k-\\hbar\\,\\gamma_{\\mathrm{s}}\\,,\\right)[\\varepsilon_{i}-\\hbar\\,\\frac{\\gamma_{\\mathrm{s}}}{\\sqrt{\\pi}C_{p}}]\\frac{1}{\\sqrt{\\pi}\\varepsilon_{i}}\\,\\hbar\\gamma_{x}-\\hbar\\,k\\,\\chi_{i}\\,\\frac{1}{\\sqrt{\\pi}}\\frac{1}{\\sqrt{\\pi}}\\frac{\\partial E(E)}{\\partial E_{\\mathrm{s}}}}\\\\ &{=\\frac{1}{\\hbar}[\\varepsilon_{i}-\\hbar\\,\\frac{\\gamma_{\\mathrm{s}}}{\\sqrt{\\pi}C_{p}}]\\frac{\\Gamma}{\\sqrt{\\pi}}\\Bigg(\\frac{\\hbar}{\\gamma_{\\mathrm{s}}}\\,\\hbar\\varepsilon_{i}\\,\\beta_{i}-\\hbar\\frac{\\gamma_{\\mathrm{s}}}{\\varepsilon_{i}+1}\\frac{\\Gamma}{\\sqrt{\\pi}C_{p}}\\,\\hbar\\varepsilon_{i}\\,\\gamma_{\\mathrm{s}}\\,\\alpha_{i}\\Bigg)\\frac{\\hbar}{\\sqrt{\\pi}}\\Bigg[\\varepsilon_{i}-\\hbar\\,k\\,\\gamma_{\\mathrm{s}}-\\hbar\\,k\\,\\frac{\\gamma_{\\mathrm{s}}}{\\varepsilon_{i}+1}}\\\\ &{=\\frac{\\hbar}{\\hbar}[\\varepsilon_{i}-\\hbar\\,\\frac{\\gamma_{\\mathrm{s}}}{\\sqrt{\\pi}C_{p}}]\\frac{\\Gamma}{\\sqrt{\\pi}}\\,\\hbar\\varepsilon_{i}\\,\\gamma_{\\mathrm{s}}-\\frac{\\hbar}{\\sqrt{\\pi}}\\,\\hbar\\varepsilon_{i}\\,\\Gamma_{T}= \n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Where the last line follows from the Markov property. ", "page_idx": 19}, {"type": "text", "text": "Now we take the expectation over $x,x_{T+1}$ conditioned on the transition matrix $P$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}_{x|P}\\left[\\frac{\\partial{\\cal L}_{T}}{\\partial W_{a,b}}\\right]=\\pi_{b}\\sum_{i=0}^{T}\\left(\\frac{1}{k}\\left(P^{T-i}\\right)_{b,a}-\\displaystyle\\sum_{s=1}^{k}P_{a,s}\\frac{1}{i+1}\\left(P^{T-i}\\right)_{s,a}\\sum_{j=0}^{i}\\left(P^{j}\\right)_{b,s}\\right)}\\\\ {\\displaystyle=\\pi_{b}\\sum_{i=0}^{T}\\left(\\frac{1}{k}\\left(P^{i}\\right)_{b,a}-\\displaystyle\\sum_{s=1}^{k}P_{a,s}\\frac{1}{T-i+1}\\left(P^{i}\\right)_{s,a}\\sum_{j=0}^{T-i}\\left(P^{j}\\right)_{b,s}\\right)}\\\\ {\\displaystyle=\\pi_{b}\\pi_{a}(T+1)\\left(\\frac{1}{k}-\\displaystyle\\sum_{s=1}^{k}P_{a,s}\\pi_{s}\\right)+{\\cal O}(\\log T)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Where the last step follows from Lemma B.6. Then, by applying Lemma B.1 (for the uniform over all $2\\times2$ transition matrices case) or lemmas B.3 and B.4 (for the mixture of distributions case), there exist positive constants (potentially depending on $k$ , but not $T$ $A,B$ such that for all $a$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x|P}\\left[{\\frac{\\partial L_{T}}{\\partial W_{a,a}}}\\right]=-(A+B)T+O(\\log T)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and for all $a\\neq b$ \uff0c ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x|P}\\left[{\\frac{\\partial L_{T}}{\\partial W_{a,b}}}\\right]=-B T+O(\\log T)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The updated Wab after the gradient step is just -aP] (because $W$ is initialized at 0).   \nChoose $\\begin{array}{r}{\\eta_{1}=\\Theta\\left(\\frac{1}{T}\\right)}\\end{array}$ , so that $W$ will be $O(1)$ with respect to $T$ after the first step. ", "page_idx": 20}, {"type": "text", "text": "For the gradient with respect to $v$ ,since $W={\\bf0}$ \uff0c ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\partial F(E)_{T,s}}{\\partial v}=\\sum_{u=1}^{k}\\sum_{i=a}^{T}\\frac{\\exp(v_{a})}{\\sum_{\\ell=0}^{i}\\exp(v_{\\ell})}W_{x_{T},u}\\left(\\mathbb{I}\\left[x_{i-a}=u\\wedge x_{i}=s\\right]-\\sum_{j=0}^{i}\\mathbb{I}\\left[x_{i-j}=u\\wedge x_{i}=s\\right]\\frac{\\partial F(x_{i-a})}{\\partial v}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "So, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\partial L_{T}(E)}{\\partial v}=\\sum_{s=1}^{k}\\frac{\\partial L_{T}(E)}{\\partial f(E)_{T,s}}\\frac{\\partial F(E)_{T,s}}{\\partial v}=0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Completing the first step calculations. ", "page_idx": 20}, {"type": "text", "text": "Second step. ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "After the first step, $W=\\eta_{1}\\left(A I+B\\mathbf{1}^{\\top}\\mathbf{1}\\right)$ . Now let us bound the output of the model ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{f(E)_{T,\\tau_{s}}=}&{\\left|\\displaystyle\\sum_{u=1}^{k}\\sum_{u=0}^{T}\\sum_{\\alpha=0}^{t}W_{x,\\tau_{s}}\\mathbf{f}[\\tilde{x}_{i-\\beta}=u\\Delta_{i}=s]\\frac{\\exp(y_{\\tau})}{\\sum_{\\alpha=0}^{t}\\exp(y_{\\tau})}\\right|}\\\\ &{=\\left|\\displaystyle\\sum_{u=1}^{k}\\sum_{\\alpha=0}^{T}\\sum_{\\alpha=0}^{t}\\left|\\left(A I+B\\Pi^{\\top}\\right)_{x,\\tau_{s}}\\mathbf{I}[\\tilde{x}_{i-\\beta}=u\\Delta_{i}=s]\\frac{1}{i}\\right|}\\\\ &{\\leq\\eta\\left|\\displaystyle\\sum_{u=1}^{k}\\sum_{\\alpha=0}^{t-1}\\sum_{\\beta=0}^{t}(A+B)\\mathbf{I}[\\tilde{x}_{i-\\beta}=u\\Delta_{i}=s]\\frac{1}{i}\\right|}\\\\ &{\\leq\\eta\\left|\\displaystyle\\sum_{u=1}^{k}\\sum_{\\alpha=0}^{t-1}\\sum_{\\beta=0}^{t}(A+B)\\mathbf{I}[\\tilde{x}_{i-\\beta}=u]\\frac{1}{i}\\right|}\\\\ &{\\leq\\eta\\left|\\displaystyle\\sum_{\\alpha=0}^{t-1}\\sum_{\\beta=0}^{t-1}(A+B)\\frac{1}{i}\\right|}\\\\ &{\\leq\\eta\\left|\\displaystyle\\sum_{\\alpha=0}^{t-1}(A+B)\\frac{1}{i}\\right|}\\\\ &{\\leq\\eta\\left|T\\right|_{\\displaystyle\\Delta}+B\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "So, using the first order approximation of softmax, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial{\\cal L}_{T}(E)}{\\partial f(E)_{T,s}}=\\mathrm{Softmax}(f(E))_{T,s}-\\mathbb{1}\\left[x_{T+1}=s\\right]}\\\\ {\\displaystyle=\\frac{1}{k}+\\frac{f(E)_{T,s}}{k}-\\frac{\\sum_{u=1}^{k}f(E)_{T,u}}{k^{2}}+O(f(E)_{T,s}^{2})-\\mathbb{1}\\left[x_{T+1}=s\\right]}\\\\ {\\displaystyle=\\frac{1}{k}+O\\left(\\eta_{1}\\frac{T}{k}(A+B)\\right)+O(\\eta_{1}^{2}T^{2}(A+B)^{2})-\\mathbb{1}\\left[x_{T+1}=s\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{=\\displaystyle\\frac{1}{k}+O\\left(\\eta_{1}\\frac{T}{k}(A+B)\\right)+O(\\eta_{1}^{2}T^{2}(A+B)^{2})-\\mathbb{1}\\left[x_{T+1}=s\\right]}}\\\\ {{=\\displaystyle\\frac{1}{k}-\\mathbb{1}\\left[x_{T+1}=s\\right]+O\\left(\\frac{1}{T}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Where the last step follows since for the first step, $\\begin{array}{r}{e t a=O\\left(\\frac{1}{T^{2}}\\right)}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "Now, we can begin to analyze the gradients with respect to the parameters. For $W$ , the gradient is approximately the same as in the last step. Notice that af(E)T does not depend on W, and v is unchanged, so f(E)Ts is unchanged. Furthermore, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{\\partial f({\\boldsymbol E})_{T,s}}{\\partial W_{a,b}}=\\sum_{s=1}^{k}\\sum_{i=0}^{T}\\sum_{j=0}^{i}\\mathbb{I}[\\boldsymbol x_{T}=a]\\mathbb{I}[\\boldsymbol x_{i-j}=b\\wedge\\boldsymbol x_{i}=s]\\frac{\\exp(\\boldsymbol y_{j})}{\\sum_{\\ell=0}^{i}\\exp(\\boldsymbol v_{\\ell})}}\\\\ &{=\\displaystyle\\sum_{i=0}^{T}\\sum_{j=0}^{i}\\mathbb{I}[\\boldsymbol x_{T}=a]\\mathbb{I}[\\boldsymbol x_{i-j}=b]\\frac{1}{i}}\\\\ &{\\le\\displaystyle\\sum_{i=0}^{T}\\sum_{j=0}^{i}\\frac{1}{i}}\\\\ &{=T}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We will now show that the gradient is approximately the same as in the first gradient step: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial{\\cal L}_{T}(E)}{\\partial W_{a,b}}=\\sum_{s=1}^{k}\\displaystyle\\frac{\\partial{\\cal L}_{T}}{\\partial f(E)_{T,s}}\\displaystyle\\frac{\\partial f(E)_{T,s}}{\\partial W_{a,b}}}\\\\ {\\displaystyle~~~~~~~~~~=\\sum_{s=1}^{k}\\left(\\displaystyle\\frac{1}{k}-1\\left[x_{T+1}=s\\right]+O\\left(\\displaystyle\\frac{1}{T}\\right)\\right)\\displaystyle\\frac{\\partial f(E)_{T,s}}{\\partial W_{a,b}}}\\\\ {\\displaystyle~~~~~~~~=\\sum_{s=1}^{k}\\left(\\displaystyle\\frac{1}{k}-1\\left[x_{T+1}=s\\right]\\right)\\displaystyle\\frac{\\partial f(E)_{T,s}}{\\partial W_{a,b}}+O\\left(\\displaystyle\\frac{1}{T}\\right)\\displaystyle\\frac{\\partial f(E)_{T,s}}{\\partial W_{a,b}}}\\\\ {\\displaystyle~~~~~~~=\\pi_{b}\\pi_{a}(T+1)\\left(\\displaystyle\\frac{1}{k}-\\sum_{s=1}^{k}P_{a,s}\\pi_{s}\\right)+O(\\log T)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Where the last lines follows from the gradient calculations in the first step. ", "page_idx": 21}, {"type": "text", "text": "Now we will consider the gradient with respect to $v$ . First, notice that the uniform component of $W$ $B\\mathbf{1}^{\\top}\\mathbf{1}$ , has no affect on the gradient of $v$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{\\mathcal{Y}f(E)_{T,s}}{\\partial v_{n}}=\\displaystyle\\sum_{u=1}^{k}\\sum_{i=0}^{T}W_{v,n}\\frac{\\exp(v_{n})}{\\sum_{i=0}^{k}\\exp(v_{n})}\\mathbf{1}[x_{i}=s]\\left(1[x_{i-\\alpha}=u]-\\displaystyle\\sum_{j=0}^{k}\\frac{\\exp(v_{j})}{\\sum_{i=0}^{k}\\exp(v_{n})}\\mathbf{1}[x_{i-j}=s]\\right.}\\\\ {\\displaystyle}&{\\left.=\\displaystyle\\sum_{u=1}^{k}\\sum_{i=0}^{T}\\left(m I+B^{\\frac{1}{1}}T\\right)_{x_{T},u}\\frac{1}{i+1}\\mathbf{1}[x_{i}=s]\\left(\\mathbf{1}[x_{i-\\alpha}=u]-\\displaystyle\\sum_{j=0}^{k}\\frac{1}{i+1}\\mathbf{1}[x_{i-j}=u]\\right)}\\\\ &{=\\displaystyle\\sum_{u=1}^{k}\\sum_{i=0}^{T}\\left(A\\mathbf{1}[x_{T}=u]+B\\right)\\frac{1}{i+1}\\mathbf{1}[x_{i}=s]\\left(\\mathbf{1}[x_{i-\\alpha}=u]-\\displaystyle\\sum_{j=0}^{k}\\frac{1}{i+1}\\mathbf{1}[x_{i-j}=u]\\right)}\\\\ &{=A\\displaystyle\\sum_{u=1}^{k}\\sum_{i=0}^{T}\\mathbf{1}[x_{T}=u]\\frac{1}{i+1}\\mathbf{1}[x_{i}=s]\\left(\\mathbf{1}[x_{i-\\alpha}=u]-\\displaystyle\\sum_{j=0}^{k}\\frac{1}{i+1}\\mathbf{1}[x_{i-j}=u]\\right)}\\\\ &{+B\\displaystyle\\sum_{u=1}^{k}\\sum_{i=0}^{T}\\frac{1}{i+1}\\mathbf{1}[x_{i}=s]\\left(\\mathbf{1}[x_{i-\\alpha}=u]-\\displaystyle\\sum_{j=0}^{k}\\frac{1}{i+1}\\mathbf{1}[x_{i-j}=u]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle=A\\sum_{i=1}^{k}\\sum_{i=n}^{T}\\|x_{T}=u\\|\\frac{1}{i+1}\\|x_{i}=s\\bigg(\\underbrace{\\mathbf{1}[x_{i-\\alpha}=u]}_{j=0}\\sum_{i=0}^{k}\\frac{1}{i+1}\\mathbf{1}[x_{i-j}=u]\\bigg)}\\\\ {\\displaystyle+B\\sum_{i=\\alpha}^{k}\\frac{1}{i+1}\\mathbf{1}[x_{i}=s]\\left(\\sum_{i=1}^{k}\\|x_{i-\\alpha}=u\\|-\\sum_{j=0}^{i}\\frac{1}{i+1}\\sum_{u=1}^{k}\\mathbb{1}[x_{i-j}=u]\\right)}\\\\ {\\displaystyle=A\\sum_{i=1}^{k}\\sum_{i=n}^{T}\\|x_{T}=u\\|\\frac{1}{i+1}\\mathbb{1}[x_{i}=s]\\left(\\mathbb{1}[x_{i-\\alpha}=u]-\\sum_{j=0}^{i}\\frac{1}{i+1}\\mathbb{1}[x_{i-j}=u]\\right)}\\\\ {\\displaystyle+B\\sum_{i=\\alpha}^{k}\\frac{1}{i+1}\\mathbf{1}[x_{i}=s]\\left(1-\\sum_{j=0}^{i}\\frac{1}{i+1}\\right)}\\\\ {\\displaystyle=A\\sum_{i=1}^{k}\\sum_{i=1}^{T}\\|x_{T}=u\\|\\frac{1}{i+1}\\mathbb{1}[x_{i}=s]\\left(\\mathbb{1}[x_{i-\\alpha}=u]-\\sum_{j=0}^{i}\\frac{1}{i+1}\\mathbb{1}[x_{i-j}=u]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By chain rule, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial{\\cal L}_{T}}{\\partial v_{a}}=\\sum_{s=1}^{k}\\displaystyle\\frac{\\partial{\\cal L}_{T}}{\\partial f(E)_{T,s}}\\displaystyle\\frac{\\partial f(E)_{T,s}}{\\partial v_{a}}}\\\\ {\\displaystyle\\qquad=\\sum_{s=1}^{k}\\left(\\frac{1}{k}-P_{x_{T},s}+O\\left(\\frac{1}{T}\\right)\\right)\\sum_{u=1}^{k}\\sum_{i=a}^{T}{\\bf1}[x_{T}=u]\\displaystyle\\frac{1}{i+1}\\Im[x_{i}=s]\\left(\\mathbb{1}[x_{i-a}=u]-\\sum_{j=0}^{i}\\frac{1}{i+1}\\Re[x_{i-j}=u]\\right)}\\\\ {\\displaystyle\\qquad=\\sum_{s=1}^{k}\\left(\\frac{1}{k}-P_{x_{T},s}\\right)\\sum_{u=1}^{k}\\sum_{i=a}^{T}\\mathbb{1}[x_{T}=u]\\displaystyle\\frac{1}{i+1}\\Im[x_{i}=s]\\left(\\mathbb{1}[x_{i-a}=u]-\\sum_{j=0}^{i}\\frac{1}{i+1}\\Im[x_{i-j}=u]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Where the last step follows because ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{i=a}^{T}\\Im[x_{T}=u]\\frac{1}{i+1}\\Im[x_{i}=s]\\left(\\Im[x_{i-a}=u]-\\displaystyle\\sum_{j=0}^{i}\\frac{1}{i+1}\\Im[x_{i-j}=u]\\right)\\Bigg|\\leq\\displaystyle\\left|\\sum_{u=1}^{k}\\sum_{i=a}^{T}\\Im[x_{T}=u]\\frac{1}{i+1}\\right|}&{}\\\\ &{\\quad\\displaystyle=\\left|\\displaystyle\\sum_{i=a}^{T}\\frac{1}{i+1}\\right|}\\\\ &{\\quad\\displaystyle\\leq\\log T}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In expectation over the values of $x$ , conditioned on the choice of $P$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\xi_{x|P}\\left[\\frac{\\partial{\\cal L}_{T}}{\\partial v_{a}}\\right]=\\sum_{s=1}^{k}\\sum_{u=1}^{k}\\left(\\frac{1}{k}-{\\cal P}_{u,s}\\right)\\sum_{i=a}^{T}\\frac{\\pi_{u}}{i+1}\\left(P^{T-i}\\right)_{s,u}\\left((P^{a})_{u,s}-\\frac{1}{i+1}\\sum_{j=0}^{i}\\left(P^{i-j}\\right)_{u,s}\\right)+{\\cal O}\\left(1\\right)}}\\\\ {{\\displaystyle\\ ~~~~~~~~=\\sum_{s=1}^{k}\\sum_{u=1}^{k}\\left(\\frac{1}{k}-{\\cal P}_{u,s}\\right)\\sum_{i=a}^{T}\\frac{\\pi_{u}}{T-i+1}\\left(P^{i}\\right)_{s,u}\\left((P^{a})_{u,s}-\\frac{1}{T-i+1}\\sum_{j=0}^{T-i}\\left(P^{j}\\right)_{u,s}\\right)+}}\\\\ {{\\displaystyle~~~~~~~=\\left(\\log\\left(T+1\\right)-\\log\\left(a+1\\right)\\right)\\sum_{s=1}^{k}\\sum_{u=1}^{k}\\pi_{u}^{2}{\\cal P}_{u,s}\\left(\\pi_{s}-(P^{a})_{u,s}\\right)+{\\cal O}(1)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Where the last step follows from Lemma B.7. Then, by applying Lemma B.2 or lemmas B.3 and B.4 (depending on the distribution assumption on $P$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x|P}\\left[\\frac{\\partial L_{T}}{\\partial v_{1}}\\right]<\\mathbb{E}_{x|P}\\left[\\frac{\\partial L_{T}}{\\partial v_{a}}\\right]\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x|P}\\left[{\\frac{\\partial L_{T}}{\\partial v_{1}}}\\right]<0\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, after the step is taken, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v_{1}=\\Theta(\\eta_{2}\\log T)}\\\\ {v_{1}-v_{n}=\\eta_{2}\\Omega(\\log T)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Finally, we can consider the state of the model after the second step. Assume that the step size for $v$ in the second step is O(T), and the step size for W is T(A+B) ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\left\\langle E\\right\\rangle_{T,s}=\\frac{k}{1+B}\\sum_{i=1}^{B}\\left\\langle E_{\\pi,\\pi,\\mathbf{u}}^{i}\\mathbb{L}_{\\pi^{i},j}=\\boldsymbol{u}\\wedge\\boldsymbol{\\lambda}_{i}=\\boldsymbol{s}\\right\\rangle\\frac{\\exp(y_{i})}{\\sum_{i=1}^{B}\\exp(y_{i})}}}\\\\ {{\\displaystyle=\\frac{1}{A}+\\frac{k}{B}\\sum_{i=1}^{B}\\frac{1}{\\log(1-\\log(\\frac{k}{T}))}\\left\\langle\\boldsymbol{A}\\big+B\\mathbf{I}^{\\pi}\\mathbf{I}^{\\pi}+O\\left(\\frac{\\log T}{T}\\right)\\right\\rangle_{\\gamma,\\pi^{s}}\\mathbf{I}\\big\\vert_{\\boldsymbol{s}^{\\angle},j}=\\boldsymbol{u}\\wedge\\boldsymbol{\\lambda}_{i}=\\boldsymbol{s}\\big\\rangle\\left(\\mathbf{I}\\left\\{j=1\\right\\}\\right.}}\\\\ {{\\displaystyle=\\frac{1}{A}+\\frac{k}{B}\\sum_{i=1}^{B}\\frac{1}{\\log(\\frac{k}{T})}\\left(\\boldsymbol{A}\\big+B\\mathbf{I}^{\\pi}\\mathbf{I}^{\\pi}+O\\left(\\frac{\\log T}{T}\\right)\\right)_{\\gamma,\\pi^{s}}\\mathbf{I}\\left\\{\\boldsymbol{s}_{\\angle},j=\\boldsymbol{u}\\wedge\\boldsymbol{\\lambda}_{\\pi}=\\boldsymbol{s}\\right\\}\\left(\\mathbf{I}\\left\\{j=1\\right\\}\\right.}}\\\\ {{\\displaystyle=\\frac{1}{A}+\\frac{k}{B}\\sum_{i=1}^{B}\\left(\\boldsymbol{A}\\big+B\\mathbf{I}^{\\pi}\\mathbf{I}\\big)_{\\mathcal{A}\\pi,\\pi}\\mathbf{I}\\left\\{\\boldsymbol{s}_{i}-\\mathbf{1}=\\boldsymbol{u}\\wedge\\boldsymbol{\\lambda}_{i}=\\delta\\right\\}O\\left(\\log T\\right)}}\\\\ {{\\displaystyle=\\frac{\\mathrm{~d~}}{A}+\\frac{\\mathrm{~d~}}{A}\\sum_{i=1}^{B}\\left[\\big|\\boldsymbol{\\lambda}_{i}[-\\pi_{T}\\wedge\\boldsymbol{\\lambda}_{i}=\\boldsymbol{s}]+\\frac{B}{A}\\sum_{i=1}^{k}\\sum_{i=1}^{B}\\mathbf{I}\\big|\\big\\vert\\boldsymbol{\\lambda}_{i}-\\boldsymbol{u}\\wedge\\boldsymbol{\\lambda}_{i}=\\delta\\big\\vert O\\big(\\log T\\big)}}\\\\ {{\\displaystyle=\\frac{\\mathrm{~d~}}{A}+\\frac{\\mathrm{~d~}}{B}\\sum_{i=1}^{B}\\left[\\big|\\boldsymbol{\\lambda} \n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since $A$ and $B$ are constant in terms of $T$ , we can recover the desired statements, completing the proof. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "B.5Inequality lemmas for $k=2$ ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Lemma B.1. If $P$ is a uniformly random stochastic $2\\times2$ matrix,and $\\pi$ is the stationary distribution of $P$ then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\pi_{a}^{2}\\left(\\frac{1}{k}-\\sum_{s=1}^{k}P_{a,s}\\pi_{s}\\right)\\right]=\\frac{5}{12}-\\frac{2}{3}\\log(2)\\approx-0.045\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "andforany $b\\neq a$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\pi_{a}\\pi_{b}\\left(\\frac{1}{k}-\\sum_{s=1}^{k}P_{a,s}\\pi_{s}\\right)\\right]=-\\frac{7}{6}+\\frac{5}{3}\\log(2)\\approx-0.011\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. We have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\Xi\\Bigg[\\pi_{a}^{2}\\left(\\frac{1}{k}-\\sum_{s=1}^{k}P_{a,s}\\pi_{s}\\right)\\Bigg]=\\mathbb{E}_{a,b}\\left[\\frac{(b-1)^{2}}{(a+b-2)^{2}}\\left[\\frac{1}{2}-\\frac{a(b-1)}{a+b-2}-\\frac{(1-a)(a-1)}{a+b-2}\\right]\\right]}\\\\ {\\displaystyle=\\frac{1}{2}\\int_{0}^{1}\\int_{0}^{1}\\frac{(b-1)^{2}}{(a+b-2)^{2}}d a d b-\\int_{0}^{1}\\int_{0}^{1}\\frac{a(b-1)^{3}}{(a+b-2)^{3}}d a d b+\\int_{0}^{1}\\int_{0}^{1}\\frac{(b-1)^{2}(a-1)^{2}}{(a+b-2)^{3}}d a d b}\\\\ {\\displaystyle=\\frac{1}{2}\\left(1-\\ln2\\right)-\\frac{1}{2}\\left(1-\\ln2\\right)+\\frac{5}{12}\\left(5-8\\ln2\\right)=\\frac{5}{12}-\\frac{2}{3}\\ln2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For the non-diagonal elements, it holds: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left[\\pi_{a}\\pi_{b}\\left(\\frac{1}{k}-\\sum_{s=1}^{k}P_{a,s}\\pi_{s}\\right)\\right]}\\\\ {\\displaystyle\\qquad=\\frac{1}{2}\\int_{0}^{1}\\int_{0}^{1}\\frac{(b-1)(a-1)}{(a+b-2)^{2}}d a d b-\\int_{0}^{1}\\int_{0}^{1}\\frac{a(b-1)^{2}(a-1)}{(a+b-2)^{3}}d a d b+\\int_{0}^{1}\\int_{0}^{1}\\frac{(b-1)(a-1)^{3}}{(a+b-2)^{3}}d a d b}\\\\ {\\displaystyle\\qquad=\\frac{1}{2}\\left(\\ln2-\\frac{1}{2}\\right)-\\frac{1}{6}\\left(1-\\ln2\\right)+\\left(\\ln2-\\frac{3}{4}\\right)=\\frac{5}{3}\\ln2-\\frac{7}{6}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lemma B.2. If $P$ is a uniformly random stochastic $2\\times2$ matrix, and $\\pi$ is the stationary distribution of $P$ ,then, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{s=1}^{k}\\sum_{u=1}^{k}\\pi_{u}^{2}P_{u,s}\\left(\\pi_{s}-P_{u,s}\\right)\\right]=-7/2+5\\log(2)\\approx-0.034\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and for any $n\\not=1$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{s=1}^{k}\\sum_{u=1}^{k}\\pi_{u}^{2}P_{u,s}\\left(\\pi_{s}-P_{u,s}\\right)\\right]\\leq\\mathbb{E}\\left[\\sum_{s=1}^{k}\\sum_{u=1}^{k}\\pi_{u}^{2}P_{u,s}\\left(\\pi_{s}-\\left(P^{n}\\right)_{u,s}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. We have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\Bigg[\\displaystyle\\sum_{s=1}^{k}\\sum_{u=1}^{k}\\pi_{u}^{2}P_{u,s}\\left(\\pi_{s}-P_{u,s}\\right)\\Bigg]=}\\\\ &{\\Bigg[\\frac{1/6x^{4}+(x+y)(6x y(-4x^{2}+2x+1)+6y^{4}+y^{3}(20-24x)}{12(x+y)}}\\\\ &{\\quad+\\left.\\frac{y^{2}(12x^{2}-12x-3)+\\log((x+y)^{6x^{2}(4x^{2}+2x-1)}(x+y)^{6y^{2}(4y^{2}+2y-1)}))}{12(x+y)}\\right]_{0}^{1}}\\\\ &{=-7/2+5l o g(2)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For the inequality, we have an intuition that doesn't depend on $k$ ,notice that: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{s=1}^{k}\\displaystyle\\sum_{u=1}^{k}\\pi_{u}^{2}P_{u,s}\\left(\\pi_{s}-(P^{n})_{u,s}\\right)\\geq-\\displaystyle\\sum_{s=1}^{k}\\displaystyle\\sum_{u=1}^{k}\\pi_{u}^{2}P_{u,s}\\left|\\pi_{s}-(P^{a})_{u,s}\\right|}\\\\ {\\displaystyle\\geq-\\displaystyle\\sum_{s=1}^{k}\\displaystyle\\sum_{u=1}^{k}\\pi_{u}^{2}P_{u,s}\\alpha^{n}}\\\\ {\\displaystyle}&{=-\\displaystyle\\sum_{u=1}^{k}\\pi_{u}^{2}\\alpha^{n}}\\\\ {\\displaystyle}&{>-\\alpha^{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "As long as $\\alpha$ isn't concentrated around 1, then this shows that the magnitude of the RHS is bounded by a term that shrinks exponentially in $n$ .For $k=2$ , we will find a similar bound, and then show separately that for all $n$ for which the bound fails, the inequality still holds true. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{k}\\sum_{u=1}^{k}\\pi_{u}^{2}P_{u,s}\\left(\\pi_{s}-(P^{n})_{u,s}\\right)=\\frac{P_{1,2}P_{2,1}(4P_{1,2}P_{2,1}-P_{1,2}-P_{2,1})}{(P_{1,2}+P_{2,1})^{3}}(1-P_{1,2}-P_{2,1})^{n}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We can show that for any choice of $P_{1,2}$ and $P_{2,1}$ on the unit square, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left|\\frac{P_{1,2}P_{2,1}(4P_{1,2}P_{2,1}-P_{1,2}-P_{2,1})}{(P_{1,2}+P_{2,1})^{3}}\\right|\\leq\\frac{1}{4}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "To see why this is true, observe that, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{(4P_{1,2}P_{2,1}-P_{1,2}-P_{2,1})^{2}}\\\\ &{=16P_{1,2}^{2}P_{2,1}^{2}+(P_{1,2}+P_{2,1})^{2}-8(P_{1,2}+P_{2,1})P_{1,2}P_{2,1}}\\\\ &{\\leq16P_{1,2}^{2}P_{2,1}^{2}+(P_{1,2}+P_{2,1})^{2}-4(P_{1,2}+P_{2,1})^{2}P_{1,2}P_{2,1}}&{\\mathrm{since}~P_{1,2}+P_{2,1}\\leq1}\\\\ &{=16P_{1,2}^{2}P_{2,1}^{2}+(P_{1,2}+P_{2,1})^{2}-4P_{1,2}P_{2,1}((P_{1,2}+P_{2,1})^{2}-4P_{1,2}P_{2,1})}\\\\ &{=(P_{1,2}+P_{2,1})^{2}-4P_{1,2}P_{2,1}(P_{1,2}-P_{2,1})^{2}}\\\\ &{\\leq(P_{1,2}+P_{2,1})^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Using the above, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\langle\\underbrace{P_{1,2}P_{2,1}(4P_{1,2}P_{2,1}-P_{1,2}-P_{2,1})}_{(P_{1,2}+P_{2,1})^{3}}\\rangle^{2}\\underbrace{P_{1,2}^{2}P_{2,1}^{2}(P_{1,2}+P_{2,1})^{2}}_{(P_{1,2}+P_{2,1})^{6}}}\\\\ &{=\\frac{P_{1,2}^{2}P_{2,1}^{2}}{(P_{1,2}+P_{2,1})^{4}}}\\\\ &{\\leq\\frac{P_{1,2}^{2}P_{2,1}^{2}}{16P_{1,2}^{2}P_{2,1}^{2}}\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{using}\\ (P_{1,2}+P_{2,1})^{2}\\geq4P_{1,2}P_{2,1}}\\\\ &{=\\frac{1}{16}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "So, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\sum_{s=1}^{k}\\sum_{u=1}^{k}\\pi_{u}^{2}P_{u,s}\\left(\\pi_{s}-(P^{n})_{u,s}\\right)\\|\\leq\\frac{1}{4}\\left|1-P_{1,2}-P_{2,1}\\right|^{n}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[-1\\frac{1}{4}\\left|1-P_{1,2}-P_{2,1}\\right|^{n}\\right]=-\\frac{1}{4}\\int_{0}^{1}\\int_{0}^{1}\\left|1-x-y\\right|^{n}}\\\\ {\\displaystyle=-\\frac{1}{4}\\frac{2}{(n+1)(n+2)}}\\\\ {\\displaystyle=-\\frac{1}{2(n+1)(n+2)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Notice that thisdecrases in $n$ and at $n=3$ $\\textstyle{\\frac{1}{2(3+1)(3+2)}}={\\frac{1}{40}}=0.025$ whichis es in magnitude than the value we proved at $n=1$ $|-7/2+5\\log2|\\approx0.034$ . So, solving for (verified by a symbolic algebra program) ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\chi\\left[\\frac{P_{1,2}P_{2,1}\\left(-P_{1,2}-P_{2,1}+1\\right)^{2}\\cdot\\left(2P_{1,2}P_{2,1}+P_{1,2}\\left(P_{2,1}-1\\right)+P_{2,1}\\left(P_{1,2}-1\\right)\\right)}{\\left(P_{1,2}+P_{2,1}\\right)^{3}}\\right]=-\\frac{413}{60}+\\frac{145}{3}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Which is not only greater than $-7/2+5\\log2$ , but positive. Lastly, we simply need to show that the inequality holds at $n=0$ , and we are done. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\frac{P_{1,2}P_{2,1}\\left(-P_{1,2}-P_{2,1}+1\\right)^{0}\\cdot\\left(2P_{1,2}P_{2,1}+P_{1,2}\\left(P_{2,1}-1\\right)+P_{2,1}\\left(P_{1,2}-1\\right)\\right)}{\\left(P_{1,2}+P_{2,1}\\right)^{3}}\\right]}\\\\ &{=-\\mathbb{E}\\left[\\frac{P_{1,2}P_{2,1}\\cdot\\left(2P_{1,2}P_{2,1}+P_{1,2}\\left(P_{2,1}-1\\right)+P_{2,1}\\left(P_{1,2}-1\\right)\\right)}{\\left(P_{1,2}+P_{2,1}\\right)^{3}}\\right]}\\\\ &{\\quad=-7/6+5*l o g(2)/3\\approx-0.0114}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Which is greater than $-7/2+5\\log2$ , completing our proof. ", "page_idx": 25}, {"type": "text", "text": "Lemma B.3. If $P$ is a uniformly random doubly stochastic matrix, then, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\pi_{a}^{2}\\left(\\frac{1}{k}-\\sum_{s=1}^{k}P_{a,s}\\pi_{s}\\right)\\right]=\\mathbb{E}\\left[\\pi_{a}\\pi_{b}\\left(\\frac{1}{k}-\\sum_{s=1}^{k}P_{a,s}\\pi_{s}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for all $a,b$ and ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{s=1}^{k}\\sum_{u=1}^{k}\\pi_{u}^{2}P_{u,s}\\left(\\pi_{s}-P_{u,s}\\right)\\right]<\\mathbb{E}\\left[\\sum_{s=1}^{k}\\sum_{u=1}^{k}\\pi_{u}^{2}P_{u,s}\\left(\\pi_{s}-\\left(P^{a}\\right)_{u,s}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For all non-negative $a\\ne1$ and ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{s=1}^{k}\\sum_{u=1}^{k}\\pi_{u}^{2}P_{u,s}\\left(\\pi_{s}-P_{u,s}\\right)\\right]<0\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. We will use the fact that for doubly stochastic matrices, the stationary distribution is the uniform vector $\\scriptstyle{\\frac{1}{k}}\\mathbf{1}$ ", "page_idx": 26}, {"type": "text", "text": "The frst equality follows directly from $\\begin{array}{r}{\\pi_{a}=\\frac{1}{k}=\\pi_{b}}\\end{array}$ . Now we will prove the first inequality. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathbb{E}\\left[\\sum_{s=1}^{k}\\sum_{u=1}^{k}\\pi_{u}^{2}P_{u,s}\\left(\\pi_{s}-(P^{a})_{u,s}\\right)\\right]=\\mathbb{E}\\left[\\sum_{s=1}^{k}\\sum_{u=1}^{k}\\frac{1}{k^{2}}P_{u,s}\\left(\\frac{1}{k}-(P^{a})_{u,s}\\right)\\right]}}\\\\ {{\\displaystyle=\\frac{1}{k^{2}}-\\frac{1}{k^{2}}\\sum_{s=1}^{k}\\sum_{u=1}^{k}\\mathbb{E}\\left[P_{u,s}\\left(P^{a}\\right)_{u,s}\\right]}}\\\\ {{\\displaystyle=\\frac{1}{k^{2}}-\\frac{1}{k^{2}}\\mathbb{E}\\left[\\langle P,P^{a}\\rangle_{F}\\right]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Where $\\langle.,.\\rangle_{F}$ is the Frobenius inner product. We will first consider the case where $a>1$ . Notice that it is sufficient to prove that for any doubly stochastic $P$ (excluding the measure zero case of where $P=P^{2}$ , where equality is reached), $\\langle P,\\dot{P}^{a}\\rangle_{F}<\\|P\\|_{F}^{2}$ . First, by Cauchy-Schwarz, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\langle P,P^{a}\\rangle_{F}<\\|P\\|_{F}\\|P^{a}\\|_{F}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We can use strictly less than because Cauchy Schwarz is only tight when $P$ and $P^{a}$ are linearly dependent, and since both $P$ and $P^{a}$ are doubly stochastic, linear dependence implies equality, which is only the case when $P=P^{a}$ . Then, For now assume $a>0$ , then, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{=\\|P\\|_{F}\\|P^{a}\\|_{F}}}\\\\ &{=\\|P\\|_{F}\\|P P^{a-1}\\|_{F}}\\\\ &{=\\|P\\|_{F}\\|\\sum_{i}\\alpha_{i}\\Lambda_{i}P^{a-1}\\|_{F}}\\\\ &{\\leq\\displaystyle\\sum_{i}\\alpha_{i}\\|P\\|_{F}\\|\\Lambda_{i}P^{a-1}\\|_{F}}\\\\ &{=\\displaystyle\\sum_{i}\\alpha_{i}\\|P\\|_{F}\\|P^{a-1}\\|_{F}}\\\\ &{=\\|P\\|_{F}\\|P^{a-1}\\|_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The third step used the well known Birkhoff-Von Neumann Theorem [9] that any doubly stochastic matrix $P$ is the convex combination of permutation matrices, so $\\textstyle P=\\sum_{i}\\alpha_{i}\\Lambda_{i}$ for some permutation matrices $\\Lambda_{i}$ and constants $\\alpha_{i}>0$ with $\\textstyle\\sum_{i}\\alpha_{i}=1$ . The inequality step uses the triangle inequality. Induction onpositive $a$ yields the desired inequality for positive $a$ ", "page_idx": 26}, {"type": "text", "text": "Now consider the remaining case, $a=0$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{1}{k^{2}}-\\frac{1}{k^{2}}\\sum_{s=1}^{k}\\sum_{u=1}^{k}\\mathbb{E}\\left[P_{u,s}\\left(P^{0}\\right)_{u,s}\\right]=\\frac{1}{k^{2}}-\\frac{1}{k^{2}}\\sum_{s=1}^{k}\\mathbb{E}\\left[P_{s,s}\\right]\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n=\\frac{1}{k^{2}}-\\frac{1}{k^{2}}=0.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "While at $a=1$ , for any $P$ that isn't $\\mathbb{1}\\mathbb{1}^{\\top}$ \uff0c $\\|P\\|_{F}>1$ ,sO ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{1}{k^{2}}-\\frac{1}{k^{2}}\\sum_{s=1}^{k}\\sum_{u=1}^{k}\\mathbb{E}\\left[P_{u,s}^{2}\\right]<\\frac{1}{k^{2}}-\\frac{1}{k^{2}}=0.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Completing the proof of both inequalities ", "page_idx": 27}, {"type": "text", "text": "Lemma B.4. If $P$ is auniformlyrandom $k\\times k$ stochastic matrix subject to each row being the same, then, ", "text_level": 1, "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\pi_{a}^{2}\\left(\\frac{1}{k}-\\sum_{s=1}^{k}P_{a,s}\\pi_{s}\\right)\\right]<\\mathbb{E}\\left[\\pi_{a}\\pi_{b}\\left(\\frac{1}{k}-\\sum_{s=1}^{k}P_{a,s}\\pi_{s}\\right)\\right]<0\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{\\mathbb{E}\\left[\\pi_{a}^{2}\\left(\\frac{1}{k}-\\sum_{s=1}^{k}P_{a,s}\\pi_{s}\\right)\\right]}{\\mathbb{E}\\left[\\pi_{a}\\pi_{b}\\left(\\frac{1}{k}-\\sum_{s=1}^{k}P_{a,s}\\pi_{s}\\right)\\right]}\\ge\\frac{8}{5}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for all a and b and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{s=1}^{k}\\sum_{u=1}^{k}\\pi_{u}^{2}P_{u,s}\\left(\\pi_{s}-\\left(P^{a}\\right)_{u,s}\\right)\\right]=0\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For all $a$ ", "page_idx": 27}, {"type": "text", "text": "Proof. The equality statement follows from the facts that for such transition matrices, $P^{a}=P$ for all natural $a>0$ , and that the stationary distribution matches the rows, that is, for any $a,b,\\pi_{b}=P_{a,b}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{\\Sigma}\\left[\\displaystyle\\sum_{s=1}^{k}\\sum_{u=1}^{k}\\pi_{u}^{2}P_{u,s}\\left(\\pi_{s}-(P^{a})_{u,s}\\right)\\right]=\\mathbb{E}\\left[\\displaystyle\\sum_{s=1}^{k}\\sum_{u=1}^{k}\\pi_{u}^{2}P_{u,s}\\left(\\pi_{s}-P_{u,s}\\right)\\right]}&{=\\mathbb{E}\\left[\\displaystyle\\sum_{s=1}^{k}\\sum_{u=1}^{k}\\pi_{u}^{2}P_{u,s}\\left(\\pi_{s}-P_{u,s}\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Now we will do the inequalities. We will also use the following facts derived from the moments of the Dirichlet distribution, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{E\\left[\\|\\pi\\|_{2}^{2}\\right]={\\cfrac{2}{k+1}}}\\\\ {E\\left[\\|\\pi\\|_{2}^{4}\\right]={\\cfrac{4(k+5)}{(k+1)(k+2)(k+3)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "So, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\pi_{a}^{2}\\left(\\displaystyle\\frac{1}{k}-\\displaystyle\\sum_{s=1}^{k}P_{a,s}\\pi_{s}\\right)\\right]=\\mathbb{E}\\left[\\pi_{a}^{2}\\left(\\displaystyle\\frac{1}{k}-\\displaystyle\\sum_{s=1}^{k}\\pi_{s}^{2}\\right)\\right]}&{}\\\\ {=\\displaystyle\\frac{1}{k}\\mathbb{E}\\left[\\|\\pi\\|_{2}^{2}\\left(\\displaystyle\\frac{1}{k}-\\|\\pi\\|_{2}^{2}\\right)\\right]}&{}\\\\ {=\\displaystyle\\frac{1}{k^{2}}\\mathbb{E}\\left[\\|\\pi\\|_{2}^{2}\\right]-\\displaystyle\\frac{1}{k}\\mathbb{E}\\left[(\\|\\pi\\|_{2}^{4})\\right]}&{}\\\\ {=\\displaystyle\\frac{2}{k^{2}(k+1)}-\\frac{4(k+5)}{k(k+1)(k+2)(k+3)}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Which is negative for all $k\\geq2$ . And, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\pi_{a}\\pi_{b}\\left(\\frac{1}{k}-\\sum_{s=1}^{k}P_{a,s}\\pi_{s}\\right)\\right]=\\!\\mathbb{E}\\left[\\pi_{a}\\pi_{b}\\left(\\frac{1}{k}-\\sum_{s=1}^{k}\\pi_{s}^{2}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle=\\frac{1}{k^{2}}\\mathbb{E}\\left[\\left(\\frac{1}{k}-\\|\\pi\\|_{2}^{2}\\right)\\right]}\\\\ {\\displaystyle=\\frac{1}{k^{3}}-\\frac{1}{k^{2}}\\mathbb{E}\\left[\\|\\pi\\|_{2}^{2}\\right]}\\\\ {\\displaystyle=\\frac{1}{k^{3}}-\\frac{2}{k^{2}(k+1)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Which is also negative for all $k\\geq2$ . Finally, notice that ", "page_idx": 28}, {"type": "equation", "text": "$$\n{\\frac{{\\frac{2}{k^{2}(k+1)}}-{\\frac{4(k+5)}{k(k+1)(k+2)(k+3)}}}{{\\frac{1}{k^{3}}}-{\\frac{2}{k^{2}(k+1)}}}}\\geq{\\frac{8}{5}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For all $k\\geq2$ ", "page_idx": 28}, {"type": "text", "text": "B.6 Approximation Lemmata ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The following lemma is a well known property of stochastic matrices, (see Lemma 3.3.2 Gallager [19] for example). ", "page_idx": 28}, {"type": "text", "text": "Lemma B.5. Let $\\alpha=1-2\\operatorname*{min}_{i,j}P_{i,j}$ . Then, for any $i,j$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left|(P^{n})_{i,j}-\\pi_{j}\\right|\\leq\\alpha^{n}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Lemma B.6 and Lemma B.7 both share similar intuitions and proofs. They largely rely on Lemma B.5, which shows that ${(P^{n})}_{i,j}$ approaches $\\pi_{j}$ exponentially fast with respect to $n$ , to show that over the course of summations over $n$ the stationary distribution dominates, allowing us to simplify the expressions. ", "page_idx": 28}, {"type": "text", "text": "Lemma B.6. Let $P$ be a stochastic matrix with all positive entries, and let $a,b$ be states.Assume that $\\operatorname*{min}_{i,j}P_{i,j}$ is positive and doesn't dependend on $T$ Then, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi_{b}\\displaystyle\\sum_{i=0}^{T}\\left(\\frac{1}{k}\\left(P^{i}\\right)_{b,a}-\\displaystyle\\sum_{s=1}^{k}P_{a,s}\\frac{1}{T-i+1}\\left(P^{i}\\right)_{s,a}\\displaystyle\\sum_{j=0}^{T-i}\\left(P^{j}\\right)_{b,s}\\right)}\\\\ &{\\qquad=\\pi_{b}\\pi_{a}(T+1)\\left(\\frac{1}{k}-\\displaystyle\\sum_{s=1}^{k}P_{a,s}\\pi_{s}\\right)+O(\\log T).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. Let us bound the magnitude of the difference between the two expressions. ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Bigg|\\left|\\pi_{S}\\!\\!\\sum_{i=0}^{T}\\!\\!\\left(\\!\\frac{1}{K}(P)_{h_{s}}-\\frac{K}{\\gamma}P_{s_{i}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}_{n,-1}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\le r_{\\frac{1}{n}}\\displaystyle\\sum_{s=1}^{r}\\left(\\frac{1}{k^{n}}\\right)^{s}\\Bigg(l^{\\frac{1}{n}}\\!+\\!\\frac{1}{\\gamma}\\!\\!-\\!s\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The last step follows from our assumption, completing the proof. ", "page_idx": 29}, {"type": "text", "text": "Lemma B.7. Let $P$ be a stochastic matrix with all positive entries, and let $a,b$ be states.Assume that $\\operatorname*{min}_{i,j}P_{i,j}$ is positive and doesn't depend on $T$ Then, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{s=1}^{k}\\sum_{u=1}^{k}\\left(\\frac{1}{k}-P_{u,s}\\right)\\sum_{i=a}^{T}\\frac{\\pi_{u}}{T-i+1}\\left(P^{i}\\right)_{s,u}\\left((P^{a})_{u,s}-\\frac{1}{T-i+1}\\sum_{j=0}^{T-i}\\left(P^{j}\\right)_{u,s}\\right)}}\\\\ &{}&{=\\left(\\log\\left(T+1\\right)-\\log\\left(a+1\\right)\\right)\\sum_{s=1}^{k}\\sum_{u=1}^{k}\\pi_{u}^{2}P_{u,s}\\left(\\pi_{s}-(P^{a})_{u,s}\\right)+O(1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. First notice that, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\sum_{u=1}^{k}{\\displaystyle\\sum_{u=1}^{k}{\\left(\\frac{1}{k}-P_{u,s}\\right)\\pi_{u}^{2}\\left({\\left(P^{a}\\right)}_{u,s}-\\pi_{s}\\right)}}=\\displaystyle\\sum_{s=1}^{k}{\\displaystyle\\sum_{u=1}^{k}{\\pi_{u}^{2}\\left({\\frac{1}{k}\\left({\\left(P^{a}\\right)}_{u,s}-\\pi_{s}\\right)-P_{u,s}\\left({\\left(P^{a}\\right)}_{u,s}-\\pi_{s}\\right)}\\right)}}}\\\\ {\\displaystyle=\\sum_{u=1}^{k}{\\pi_{u}^{2}\\left({\\sum_{s=1}^{k}{\\frac{1}{k}\\left({\\left(P^{a}\\right)}_{u,s}-\\pi_{s}\\right)}-\\sum_{s=1}^{k}{P_{u,s}\\left({\\left(P^{a}\\right)}_{u,s}-\\pi_{s}\\right)}}\\right)}}\\\\ {\\displaystyle=\\sum_{u=1}^{k}{\\pi_{u}^{2}\\left({\\frac{1}{k}\\left(1-1\\right)-\\displaystyle\\sum_{s=1}^{k}{P_{u,s}\\left({\\left(P^{a}\\right)}_{u,s}-\\pi_{s}\\right)}}\\right)}}\\\\ {\\displaystyle=\\sum_{u=1}^{k}{\\pi_{u}^{2}\\sum_{s=1}^{k}{P_{u,s}\\left(\\pi_{s}-{\\left(P^{a}\\right)}_{u,s}\\right)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Wewillbondtedistane btwen( Pus)(P)u, \u03c0\uff09 andlP] Define $\\begin{array}{r}{\\alpha=1-2\\operatorname*{min}_{i,j}P_{i,j}}\\end{array}$ as in lemma B.5. ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\displaystyle{\\sum_{j=1}^{k}\\sum_{i=1}^{k}\\left(\\frac{1}{k}-\\gamma_{w,i}\\right)\\sum_{s=T-i+1}^{k}\\left(t^{p}\\right)_{s,s}=\\frac{1}{T}\\sum_{s=T-i+1}^{k}\\left(t^{p}\\right)_{s,s}=\\frac{1}{T}\\sum_{s=T-i+1}^{k-1}\\left(t^{p}\\right)_{s,s}=0\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\sum_{s=T-i+1}^{k-1}\\left(\\frac{1}{k}-\\gamma_{s,s}\\right)\\sum_{s=T-i+1}^{k}\\left(t^{p}\\right)_{s,s}=\\gamma_{s,s}\\right)}\\\\ &{=\\left|\\displaystyle{\\sum_{j=1}^{k-1}\\sum_{i=1}^{k}\\left(\\frac{1}{k}-\\gamma_{s,i}\\right)\\sum_{s=T-i+1}^{k}\\left(t^{p}\\right)_{s,s}=\\left(t^{p}\\right)_{s,s}=\\frac{1}{T-i+1}\\sum_{s=T-i+1}^{k-1}\\left(t^{p}\\right)_{s,s}=\\gamma_{s,s}\\left(t^{p}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\left(t^{p}\\right)_{s,s}=\\frac{1}{T}\\sum_{s=T-i+1}^{k-1}\\left(t^{p}\\right)_{s,s}=\\gamma_{s,s}\\left(t^{p}\\right)_{s,s}=0\\right)}\\\\ &{\\leq\\displaystyle{\\sum_{j=1}^{k-1}\\sum_{i=1}^{k}\\sum_{s=T-i+1}^{k}\\left(t^{p}\\right)_{s,s}\\left(t^{p}\\right)_{s,s}=-\\frac{1}{T}\\sum_{s=t-i+1}^{k-1}\\sum_{s=t}^{k}\\left(t^{p}\\right)_{s,s}=-\\left(t^{p}\\right)_{s,s}=0\\right)}\\\\ &{\\leq\\displaystyle{\\sum_{j=1}^{k-1}\\sum_{i=1}^{k}\\sum_{s=t-i+1}^{k}\\left(t^{p}\\right)_{s,s}=\\frac{1}{T}\\sum_{s=t-i+1}^{k}\\left(t^{p}\\right)_{s,s}=\\left(t^{p}\\right)_{s,s}=-\\alpha_{s,s}\\left(t^{p}\\right)_{s,s}=0}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lesssim\\underset{m=1}{\\overset{...}{\\sum}}\\sum_{n=1}^{\\infty}\\underset{r=t+1}{\\overset{...}{\\sum}}\\underset{\\tau=t+1}{\\overset{...}{\\sum}}\\left(\\nu^{t}\\right)\\underset{s=t+1}{\\overset{...}{\\sum}}\\left(\\nu^{t}\\right)\\underset{s=t+1}{\\overset{...}{\\sum}}\\frac{1}{\\nu^{t}}\\left(\\nu^{t}\\right)\\underset{\\sim}{,...,s^{\\prime}}\\left(\\nu^{t}\\right)-\\left(\\nu^{t}\\right)\\underset{s=t+1}{\\overset{...}{\\sum}}\\left(\\nu^{t}\\right)\\underset{s=t+1}{\\overset{...}{\\sum}}\\left(\\nu^{t}\\right)}\\\\ &{\\lesssim\\underset{m=1}{\\overset{...}{\\sum}}\\sum_{n=1}^{\\infty}\\underset{r=t+1}{\\overset{...}{\\sum}}\\left(\\nu^{t}\\right)\\underset{s=t+1}{\\overset{...}{\\sum}}\\underset{\\tau=t+1}{\\overset{...}{\\sum}}\\left(\\nu^{t}-\\left(\\nu^{t}\\right)_{s=t}\\right)-\\left(\\nu^{t}\\right)\\underset{s=t+1}{\\overset{...}{\\sum}}\\left(\\nu^{t}\\right)\\underset{s=t+1}{\\overset{...}{\\sum}}\\left(\\nu^{t}\\right)}\\\\ &{\\underset{s=t+1}{\\overset{...}{\\sum}}\\sum_{n=1}^{\\infty}\\underset{r=t+1}{\\overset{...}{\\sum}}\\left(\\nu^{t}\\right)\\underset{s=t+1}{\\overset{...}{\\sum}}\\underset{\\tau=t+1}{\\overset{...}{\\sum}}\\left(\\nu^{t}\\right)\\underset{s=t+1}{\\overset{...}{\\sum}}\\left(\\nu^{t}\\right)\\underset{s=t+1}{\\overset{...}{\\sum}}\\left(\\nu^{t}\\right)\\underset{s=t+1}{\\overset{...}{\\sum}}\\left(\\nu^{t}\\right)\\underset{s=t+1}{\\overset{...}{\\sum}}\\left(\\nu^{t}\\right)\\underset{s=t+1}{\\overset{...}{\\sum}}\\left(\\nu^{t}\\right)}\\\\ &{\\overset{=}{\\underset{m=1}{\\overset{...}{\\sum}}}\\underset{s=t+1}{\\overset{...}{\\sum}}\\underset{s=t+1}{\\overset{...}{\\sum}}\\\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: All claims made in our paper are supported by theoretical proofs or rigorous empirical evaluations. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: The purpose of this work is to introduce a novel task to study in-context learning in transformers. Since it focuses on a theoretical understanding, certain assumptions must be made. We mention in the introduction that our theoretical results apply to a simplified model of a transformer, while we precisely define our experimental setup. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: Our main paper may have some informal theorem statements for ease of readability, we give formal versions of all the statements with full proofs in the appendix. We also make sure to clarify the assumptions under which our results hold. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We provide all training details in Appendix A, and information on the data generating process and architecture in Section 2. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 32}, {"type": "text", "text": "some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Justification:\u3000 Our code is shared at https://github.com/EzraEdelman/ Evolution-of-Statistical-Induction-Heads. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: The details are available in Appendix A Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our main results regard the multiphase nature of training, and we include Figure 6 which shows that for the seeds $0,1,\\ldots9$ the model each time has the same patterns in the training curve. The curves were shown individually instead of through error bars since the main purpose of the loss curves is to show the shape, but because the phase transition doesn't occur at a consistent time, adding error bars smooths out the curve making the phase transition look less sharp. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The resources are described in Appendix A. Only a single computer was used, and none of the training runs took longer than ten minutes. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We read the code of ethics in its entirety and strongly believe that our research abides by the stated code. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPs Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: Our work focuses on understanding the internal mechanisms of Transformer models on synthetic tasks. We do not foresee any direct societal impact of this work. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: Our work does not release any data or models that poses safety risks. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We cite the Github repository we use as the codebase for our Transformer models. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets. \u00b7 The authors should cite the original paper that produced the code package or dataset. \u00b7 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode. com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: We do not introduce any new assets. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) wereobtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 36}, {"type": "text", "text": "\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]