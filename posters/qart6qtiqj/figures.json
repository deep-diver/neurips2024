[{"figure_path": "qaRT6QTIqJ/figures/figures_1_1.jpg", "caption": "Figure 1: (left) We train small transformers to perform in-context learning of Markov chains (ICL-MC). Each training sequence is generated by sampling a transition matrix from a prior distribution, and then sampling a sequence from this Markov chain. (right) Distance of a transformer's output distribution to several well-defined strategies over the course of training on our in-context Markov chain task. The model passes through three stages: (1) predicting a uniform distribution (blue region), (2) predicting based on in-context single-token statistics (unigrams); then, there is a rapid phase transition to the correct in-context bigram solution. We conduct an empirical and theoretical investigation of this multi-phase process, showing how successful learning results from the interaction between the transformer's layers, and uncovering evidence that the presence of the simpler unigram solution may delay formation of the final bigram solution. We examine how learning is affected by varying the prior distribution over Markov chains, and consider the generalization of our in-context learning of Markov chains (ICL-MC) task to n-grams for n > 2.", "description": "The figure shows the training process of small transformers on an in-context learning task involving Markov chains. The left panel illustrates the task setup, where sequences are generated from randomly sampled Markov chains. The right panel displays the KL divergence between the model's predictions and three strategies (uniform, unigram, bigram) across training iterations. It reveals a multi-phase learning process, starting with uniform predictions, then transitioning to unigram-based predictions, and finally converging to the optimal bigram solution.", "section": "2 Setup"}, {"figure_path": "qaRT6QTIqJ/figures/figures_4_1.jpg", "caption": "Figure 2: Attention patterns that correspond to the last token of the sequence for a transformer trained to perform ICL-MC. The intensity of each blue line signifies the strength of the corresponding attention value. As the model gets trained, we observe that the attention weights mimic the construction of Proposition 2.1. Specifically, at the end of training (right), each token in the first layer is attending to the previous token. In the second layer, the last token, a \u201c2\u201d, is attending to tokens that followed \u201c2\u201ds, allowing bigram statistics to be calculated. See also Figure 9 for full attention matrices during the course of training.", "description": "This figure shows the attention patterns of a transformer trained on the ICL-MC task at different stages of training. The intensity of the blue lines represents the attention weights. At the beginning of training, the attention is diffuse. As training progresses, the model learns to focus its attention on relevant tokens, mimicking the bigram calculation process. At the end of training, each token in the first layer attends to the preceding token, while the last token in the second layer attends to the tokens that previously followed it.", "section": "Architectures: Transformers and Simplifications"}, {"figure_path": "qaRT6QTIqJ/figures/figures_5_1.jpg", "caption": "Figure 3: A two layer transformer (top) and a minimal model (bottom) trained on our in-context Markov Chain task. A comparison of the two layer attention-only transformer and minimal model (4) (with v and W initialized to 0). The graphs on the left are test loss measured by KL-Divergence from the underlying truth. The orange line shows the loss of the unigram strategy, and the green line shows the loss of the bigram strategy. The middle graph shows the effective positional encoding (for the transformer, these are for the first layer, and averaged over all tokens). The graph on the right shows the KL-divergence between the outputs of the models and three strategy. The lower the KL-divergence, the more similar the model is to that strategy.", "description": "This figure compares the performance of a two-layer transformer and a simplified minimal model on the ICL-MC task.  The leftmost graphs show the test loss (KL-divergence from the true distribution) over the course of training, for both models. The orange and green lines represent the loss of the unigram and bigram strategies respectively, providing a benchmark. The center graphs display the positional encoding weights learned by each model. The rightmost graphs show the KL-divergence between the model predictions and the three strategies (uniform, unigram, bigram) across training epochs. This illustrates how the models learn to approximate these strategies over time, with a clear phase transition from unigram to bigram indicated by shading.", "section": "3 Empirical Findings and Theoretical Validation"}, {"figure_path": "qaRT6QTIqJ/figures/figures_6_1.jpg", "caption": "Figure 4: (left) Unigrams slow down optimization: Comparison of two-layer attention only transformers trained on two distributions; one with a uniformly random doubly stochastic transition matrix and another with a mixture of the doubly stochastic and unigrams distribution (see Appendix A.1 for details). We see that in absence of unigrams \u201csignal\u201d the model minimizes the loss (evaluated on the full distribution) much faster. (center, right) Training of the minimal model on ICL-MC with k = 2 states: (center) The heatmap of the second layer (W matrix) that learns to be close to diagonal. (right) The values of the positional embeddings (1st layer) that display a curious even/odd pattern. This is before any softmax is applied to the positional embeddings.", "description": "This figure shows that the presence of unigram signals slows down the training process of a two-layer attention-only transformer on the ICL-MC task.  The left panel compares the test loss of transformers trained on a purely doubly stochastic distribution and a mixture of doubly stochastic and unigram distributions. The central and right panels illustrate the minimal model's training dynamics by showing the weight matrix (W) and positional embeddings (v) respectively; illustrating how the minimal model learns to approach the bigram solution, with the W matrix resembling an identity matrix and v showing an alternating positive/negative pattern.", "section": "3 Empirical Findings and Theoretical Validation"}, {"figure_path": "qaRT6QTIqJ/figures/figures_8_1.jpg", "caption": "Figure 5: Three-headed transformer trained on In-Context Learning 3-grams (trigrams), with context length 200. (left) Loss during training. The model hierarchically converges close to the Bayes optimal solution. (right) KL divergence between the model and different strategies during training. As we observe, there are 4 stages of learning, each of them corresponding to a different algorithm implemented by the model.", "description": "This figure shows the training results of a three-headed transformer on a trigram prediction task.  The left panel displays the training loss, which demonstrates a multi-stage learning process with distinct phases of learning. The right panel shows the KL divergence between the model's predictions and various baselines (uniform, unigram, bigram, and trigram) over the course of training.  The KL divergence plot visually illustrates the transition between these stages, confirming that the model progressively learns more complex patterns (from unigram to trigram) during the training process.", "section": "3.3 Beyond Bigrams: n-gram Statistics"}, {"figure_path": "qaRT6QTIqJ/figures/figures_13_1.jpg", "caption": "Figure 6: In distribution test loss for 10 two layer attention only transformers, with random seeds 0,1,... 9 (randomness affects initialization and the training data). The training dynamics are consistent for each model, though the exact position of the phase transitions changes.", "description": "This figure displays the test loss curves for ten different two-layer attention-only transformers trained on the in-context learning of Markov chains task. Each transformer used a different random seed, resulting in variations in the initialization and training data.  Despite these variations, the overall training dynamics remain consistent across all ten models, showing a two-phase learning process. The first phase involves a rapid initial drop in the loss, followed by a prolonged period of slow improvement.  The second phase is characterized by a sudden, sharp drop in loss, converging towards the optimal solution. Although the exact timing of the phase transitions varies across models, the overall pattern of the two-phase learning process remains consistent.", "section": "3.1 Transformers In-Context Learn Markov Chains Hierarchically"}, {"figure_path": "qaRT6QTIqJ/figures/figures_13_2.jpg", "caption": "Figure 7: Graphs of test loss showing that a single layer transformer can not achieve good performance on ICL-MC. This result holds for transformers with or without MLPs, and with absolute or relative positional encodings. These graphs show that even trained 8 times longer, there is no notable increase in performance beyond the unigrams strategy (orange line).", "description": "The figure shows the training loss curves for single-layer transformers trained on the in-context learning Markov chain task.  Two different single-layer transformer models are shown, each trained for different numbers of epochs. The results demonstrate that single-layer transformers fail to achieve performance better than a simple unigram baseline, even with substantially increased training time.  This is evidence that multiple layers are necessary for successfully learning the more complex bigram solution.", "section": "3 Empirical Findings and Theoretical Validation"}, {"figure_path": "qaRT6QTIqJ/figures/figures_14_1.jpg", "caption": "Figure 8: Our results extend to more symbols than k = 2 or k = 3. The KL-divergence between the transformer and strategies over training. This required a sequence length greater than 100 (200 in this case) for the difference between unigrams and uniform to be large enough for the unigram phase to be visible (in either case there was a plateau before the final drop in test loss).", "description": "This figure shows the KL-divergence between a transformer model's predictions and three different strategies (uniform, unigram, and bigram) during training on a Markov Chain task with 8 symbols.  The x-axis represents the number of training examples seen, and the y-axis shows the KL-divergence. The figure demonstrates that the model initially learns a suboptimal unigram strategy before transitioning to a more complex bigram strategy.  A longer sequence length (200) was needed for the unigram phase to be clearly observable.", "section": "3.3 Beyond Bigrams: n-gram Statistics"}, {"figure_path": "qaRT6QTIqJ/figures/figures_14_2.jpg", "caption": "Figure 2: Attention patterns that correspond to the last token of the sequence for a transformer trained to perform ICL-MC. The intensity of each blue line signifies the strength of the corresponding attention value. As the model gets trained, we observe that the attention weights mimic the construction of Proposition 2.1. Specifically, at the end of training (right), each token in the first layer is attending to the previous token. In the second layer, the last token, a \u201c2\u201d, is attending to tokens that followed \u201c2\u201ds, allowing bigram statistics to be calculated. See also Figure 9 for full attention matrices during the course of training.", "description": "This figure visualizes the attention patterns in a two-layer transformer trained on the In-Context Learning Markov Chains (ICL-MC) task.  It shows how the attention weights evolve during training, demonstrating a shift from simpler to more complex strategies.  In the initial stage, attention is local, focusing on the previous token. Later, the second layer learns to attend to tokens that followed the same token as the last one, effectively capturing bigram statistics.", "section": "Architectures: Transformers and Simplifications"}, {"figure_path": "qaRT6QTIqJ/figures/figures_15_1.jpg", "caption": "Figure 3: A two layer transformer (top) and a minimal model (bottom) trained on our in-context Markov Chain task. A comparison of the two layer attention-only transformer and minimal model (4) (with v and W initialized to 0). The graphs on the left are test loss measured by KL-Divergence from the underlying truth. The orange line shows the loss of the unigram strategy, and the green line shows the loss of the bigram strategy. The middle graph shows the effective positional encoding (for the transformer, these are for the first layer, and averaged over all tokens). The graph on the right shows the KL-divergence between the outputs of the models and three strategy. The lower the KL-divergence, the more similar the model is to that strategy.", "description": "The figure compares the performance of a two-layer transformer and a simplified minimal model on an in-context learning task for Markov chains.  It shows the test loss over training, highlighting multiple phases in the learning process, with the transition between simpler (unigram) and more complex (bigram) solutions. The effective positional encodings from the transformer and the KL-divergence between models and different strategies are also visualized.", "section": "3 Empirical Findings and Theoretical Validation"}, {"figure_path": "qaRT6QTIqJ/figures/figures_15_2.jpg", "caption": "Figure 1: (left) We train small transformers to perform in-context learning of Markov chains (ICL-MC). Each training sequence is generated by sampling a transition matrix from a prior distribution, and then sampling a sequence from this Markov chain. (right) Distance of a transformer's output distribution to several well-defined strategies over the course of training on our in-context Markov chain task. The model passes through three stages: (1) predicting a uniform distribution (blue region), (2) predicting based on in-context unigram statistics (orange region), (3) predicting based on in-context bigram statistics (green region). Shading is based on the minimum of the curves.", "description": "The left panel shows the experimental setup for in-context learning of Markov chains. The right panel shows the KL divergence between the model's predictions and three different strategies (uniform, unigram, bigram) during training. The figure shows that the model progresses through three phases: initially predicting uniformly, then using unigram statistics, and finally using bigram statistics.", "section": "2 Setup"}, {"figure_path": "qaRT6QTIqJ/figures/figures_16_1.jpg", "caption": "Figure 3: A two layer transformer (top) and a minimal model (bottom) trained on our in-context Markov Chain task. A comparison of the two layer attention-only transformer and minimal model (4) (with v and W initialized to 0). The graphs on the left are test loss measured by KL-Divergence from the underlying truth. The orange line shows the loss of the unigram strategy, and the green line shows the loss of the bigram strategy. The middle graph shows the effective positional encoding (for the transformer, these are for the first layer, and averaged over all tokens). The graph on the right shows the KL-divergence between the outputs of the models and three strategy. The lower the KL-divergence, the more similar the model is to that strategy.", "description": "This figure compares the performance of a two-layer transformer and a simplified minimal model on the in-context learning of Markov chains task. It shows the test loss (KL-divergence from the true distribution), the effective positional encoding, and the KL-divergence between model predictions and three strategies (uniform, unigram, and bigram). The results indicate a multi-stage learning process for both models, where they initially learn the simpler unigram strategy before transitioning to the optimal bigram strategy.", "section": "3 Empirical Findings and Theoretical Validation"}]