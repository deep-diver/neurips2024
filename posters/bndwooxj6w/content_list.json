[{"type": "text", "text": "On the Complexity of Identification in Linear Structural Causal Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Julian Do\u00a8rfler\u2217 Benito van der Zander\u2217 Saarland University University of L\u00a8ubeck ", "page_idx": 0}, {"type": "text", "text": "Markus Bla\u00a8ser\u2020 Maciej Lis\u00b4kiewicz\u2020 Saarland University University of L\u00a8ubeck ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Learning the unknown causal parameters of a linear structural causal model is a fundamental task in causal analysis. The task, known as the problem of identification, asks to estimate the parameters of the model from a combination of assumptions on the graphical structure of the model and observational data, represented as a non-causal covariance matrix. In this paper, we give a new sound and complete algorithm for generic identification which runs in polynomial space. By a standard simulation result, namely ${\\mathsf{P S P A C E}}\\subseteq{\\mathsf{E X P}}$ , this algorithm has exponential running time which vastly improves the state-of-the-art double exponential time method using a Gro\u00a8bner basis approach. The paper also presents evidence that parameter identification is computationally hard in general. In particular, we prove, that the task asking whether, for a given feasible correlation matrix, there are exactly one or two or more parameter sets explaining the observed matrix, is hard for $\\forall\\mathbb{R}$ , the co-class of the existential theory of the reals. In particular, this problem is coNP-hard. To our best knowledge, this is the first hardness result for some notion of identifiability. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recognizing and predicting the causal effects and distinguishing them from purely statistical correlations is an important task of empirical sciences. For example, identifying the causes of disease and health outcomes is of great significance in developing new disease prevention and treatment strategies. A common approach for establishing causal effects is through randomized controlled trials (Fisher, [20]) \u2013 called the gold standard of experimentation \u2013 which, however, requires physical intervention in the examined system. Therefore, in many practical applications, experimentation is not always possible due to cost, ethical constraints, or technical feasibility. E.g., to learn the effects of radiation on human health one cannot conduct interventional studies involving human participants. In such cases, a researcher may use an alternative approach and establish cause-effect relationships by combining existing observed data with the knowledge of the structure of the system under study. This is called the problem of identification in causal inference (Pearl, [32]) and the approach is commonly used in various fields, including modern ML. ", "page_idx": 0}, {"type": "text", "text": "A key ingredient of this framework is the way the underlying structure models the true mechanism behind the system. In general, this is done using structural causal models (SCMs) [32, 3]. In this work, we focus on the problem of identification in linear SCMs, known also as structural equation models (SEMs) [7, 18]. They represent the causal relationships between observed random variables assuming that each variable $X_{i}$ , with $i=1,\\hdots,n$ is linearly dependent on the remaining variables and an unobserved error term $\\varepsilon_{i}$ of normal distribution with zero mean: $\\begin{array}{r}{X_{j}=\\sum_{i}\\lambda_{i,j}X_{i}^{-}+\\varepsilon_{j}}\\end{array}$ . The model implies the existence of some covariance matrix $\\Omega=(\\omega_{i,j})$ between t he error terms. In this paper, we consider mainly recursive models, i.e., we assume that, for all $i>j$ , we have $\\lambda_{i,j}\\,=\\,0$ , nonetheless we discuss how our methods can be extended to the general case. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Linear SCMs can be represented as a graph with nodes $\\{1,\\ldots,n\\}$ corresponding to variables $X_{1},\\ldots,X_{n}$ and with directed and bidirected edges. A directed edge $i\\;\\rightarrow\\;j$ represents a linear influence $\\lambda_{i,j}\\neq0$ of a parent node $i$ on its child $j$ . A bidirected edge $i\\leftrightarrow j$ represents a correlation $\\omega_{i,j}\\neq0$ between error terms $\\varepsilon_{i}$ and $\\varepsilon_{j}$ (cf. Figure 1). ", "page_idx": 1}, {"type": "text", "text": "Writing the coefficients of all directed edges as an adjacency matrix $\\Lambda=(\\lambda_{i,j})$ and the coefficients of all bidirected edges as an adjacency matrix $\\Omega=(\\omega_{i,j})$ , the covariances $\\sigma_{i,j}$ between each pair of observed variables $X_{i}$ and $X_{j}$ can be calculated as matrix $\\Sigma=\\left(\\sigma_{i,j}\\right)$ : ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\Sigma=(I-\\Lambda)^{-T}\\Omega(I-\\Lambda)^{-1},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $I$ is the identity matrix [21]. Knowledge of the parameters $\\lambda_{i,j}$ allows for the prediction of all causal effects in the system. The key task here is to learn $\\Lambda$ from the observed covariances $\\Sigma$ assuming $\\Omega$ remains unknown. This leads to the formulation of the identification problem in linear SCMs as solving for the parameter $\\Lambda$ using equation (1). If the problem asks to find symbolic solutions involving only symbols $\\sigma_{i,j}$ , we call it the problem of symbolic identification. In the case when the parameter can be determined uniquely almost everywhere using $\\Sigma$ alone, we call the instance to be generically identifiable (for a formal definition, see Sec. 2). If the goal is to find, for a given $\\Sigma$ of rational numbers, the numerical solutions, we call it the problem of numerical identification. In this paper, we study the computational complexity of both variants of the problem. ", "page_idx": 1}, {"type": "text", "text": "Previous Work. The identification in linear SCMs and its applications have been a subject of research interest for many decades, including the early work in econometrics and agricultural sciences [42, 41, 19, 8]. Currently, it seems, that one of the most challenging tasks in this field is providing efficient computational methods to find solutions, both for symbolic and for numeric variants, or providing evidence that the problems are computationally intractable. ", "page_idx": 1}, {"type": "text", "text": "The generic identification can be computed using standard algebraic tools for solving symbolic polynomial equations (1). Such an approach provides a sound and complete method, i.e., it is guaranteed to identify all identifiable instances. However, common algorithms for solving such equations usually use Gro\u00a8bner basis computation, whose time complexity is doubly exponential in the worst case [22]. So far, it has remained widely open whether the double exponential function is a sharp upper bound on the computational complexity of the generic identifiability. ", "page_idx": 1}, {"type": "text", "text": "Most approaches to solving the problem in practice are based on instrumental variables, in which the causal direct effect is identified as a fraction of two covariances [41, 8]. For example, in the linear model shown in Figure 1, one can calculate first $\\lambda_{1,2}=\\sigma_{1,2}$ and then $\\begin{array}{r}{\\lambda_{2,3}=\\frac{\\lambda_{1,2}\\bar{\\lambda}_{2,3}}{\\lambda_{1,2}}=\\frac{\\sigma_{1,3}}{\\sigma_{1,2}}}\\end{array}$ variable $X_{1}$ is then called an instrumental variable (IV). This method is sound but not complete, that is, when it identifies a parameter, then it is always correct. However, when the method fails due to a missing IV, then the parameter might still be identified by other means. This approach has inspired intensive research aimed at providing computational methods that may not be complete but enable efficient algorithms and identify a significantly large number of cases. ", "page_idx": 1}, {"type": "text", "text": "Conditional IVs (cIVs) are one of the most natural extensions of simple IVs [8, 31]. The corresponding identification method is based on an efficient, polynomial time algorithm for finding conditional IVs [38]. More complex criteria and methods proposed in the literature, which are also accompanied by polynomial time algorithms, involve instrumental sets (IS) [10] half-treks (HTC) [21], instrumental cutsets (ICs) [28], auxiliary instrumental variables (aIVs) [15]. The generalized HTC $\\mathrm{(gHTC)}$ [13, 40] and auxiliary variables (AVS) [13, 14] can be implemented in polynomial time provided ", "page_idx": 1}, {"type": "image", "img_path": "bNDwOoxj6W/tmp/3712b80fb13e8bed124d707bc653f969f652cc5bf7922e749eae61fbb2409223.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 2: Methods for generic identification in linear SCMs. An arrow from methods $A\\rightarrow B$ means that $B$ subsume all methods $A$ , i.e., any instance that can be identified by any of methods $A$ can be identified by method $B$ and this inclusion is proper. Green boxes mean there exist polynomial-time algorithms to apply the method, a red box means no such algorithm is known or the method has been proven to be NP-hard. The blue box includes the complete methods. ", "page_idx": 2}, {"type": "text", "text": "that the number of incoming edges to each node in the causal graph is bounded. The methods based on generalized IS (gIS), a simplified version of the criterion (scIS), and generalized AVS (gAVS) appeared to be computationally intractable [10, 9, 11, 37, 14]. The auxiliary cutsets (ACID) algorithm [29] subsumes all the above methods in the sense that it covers all the instances identified by them. Recently [39, 24] provide the TreeID algorithm for identification in tree-shaped linear models. TreeID is incomparable since it is complete for the subclass of tree-like SCMs. However, TreeID does not work for general SCMs, which is the focus of our work. Figure 2 summarizes these results. ", "page_idx": 2}, {"type": "text", "text": "Numerical parameter identification, known in the literature as the estimation of the parameters of structural equation models, has been the subject of a considerable amount of research, which has resulted in significant progress in theoretical understanding and development of estimation methods [2, 26, 12, 30, 7, 25]. Currently, in practical applications (e.g. in econometrics, psychometrics, or biometrics), methods based on maximum likelihood (ML) or generalized least squares estimator (GLS) are commonly used to find model parameters. However, despite the great importance of this problem and considerable effort in method development, the computational complexity of the parameter estimation problem remains unexplored. In our work, we provide, to our knowledge, the first hardness result for a very basic variant of the SEM parameter estimation problem, which we call numerical identification. ", "page_idx": 2}, {"type": "text", "text": "Our Contribution. We improve significantly the best-known upper bound on the computational complexity for generic identification and provide evidence that parameter identification is computationally hard in general. In more detail, our contributions are as follows: ", "page_idx": 2}, {"type": "text", "text": "\u2022 We provide a polynomial-space algorithm for sound and complete generic parameter identification in linear models. This gives an exponential running time which vastly improves the state-of-the-art double exponential time method using Gro\u00a8bner basis. In our approach, we formulate generic identifiability as a formula over real variables with both existential and universal quantifiers and then use Renegar\u2019s algorithm [33].   \n\u2022 Our constructive technique allows us to prove an $\\exists\\forall\\mathbb{R}$ (and $\\forall\\exists\\mathbb{R}$ ) upper bound on generic identifiability, for the well-studied complexity class $\\exists\\forall\\mathbb{R}$ (see Sec. 2.2 for definitions). It is an intermediate class between NP and PSPACE.   \n\u2022 We prove that numerical identification is hard for the complexity class $\\forall\\mathbb{R}$ . In particular, the problem is coNP-hard. Our complexity characterization is quite precise since we show a (promise) $\\forall\\mathbb{R}$ upper bound for numerical identification. To the best of our knowledge, this is the first hardness result for some notion of identifiability.   \n\u2022 On the other hand, we show that numerical identifiability can be decided in polynomial space.   \n\u2022 If an instance is non-identifiable, then an important task is to identify as many model parameters as possible. We are particularly interested in identifying the parameters of specific edges in the graph representing the linear model. In the paper, we obtain, for \u201cedge identifiability\u201d, the same results as for the common identifiability problem. Since these proofs are essentially the same, they can be found in Appendix A. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "2.1 The Problems of Identification in Linear Causal Models ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A mixed graph is a triple $G\\;=\\;(V,D,B)$ where $V\\ :=\\ \\{1,\\ldots,n\\}$ is a finite set of nodes and $D\\subseteq V\\times V$ and $B\\subseteq\\left({V_{2}\\atop1}\\right)$ are two sets of directed and bidirected edges, respectively. Let $\\mathbb{R}^{D}$ be the set of matrices $\\Lambda=(\\lambda_{i,j})\\in\\mathbb{R}^{n\\times n}$ with $\\lambda_{i,j}=0$ if $i\\rightarrow j$ is not in $D$ and let $\\mathrm{PD}(n)$ denote the cone of positive definite $n\\times n$ matrices. Let $\\operatorname{PD}(B)$ be the set of matrices $\\Omega\\,=\\,(\\omega_{i,j})\\,\\in\\,\\mathrm{PD}(n)$ with $\\omega_{i,j}=0$ if $i\\neq j$ and $i\\leftrightarrow j$ is not an edge in $B$ . For now, we will only consider recursive models, i.e. we assume that, for all $i>j$ , we have $\\lambda_{i,j}=0$ (in Sec. 7 we will discuss how our methods can be extended to general graphs allowing cycles). Thus, the directed graph $(V,D)$ accompanied with the model is acyclic. We will assume w.l.o.g. that the nodes are topologically sorted, i.e., there are no edges $i\\rightarrow j$ with $i>j$ . ", "page_idx": 3}, {"type": "text", "text": "Denote by $\\mathcal{N}_{n}(\\mu,\\Sigma)$ the multivariate normal distribution with mean $\\boldsymbol{\\mu}\\in\\mathbb{R}^{n}$ and covariance matrix $\\Sigma$ . The linear SCMs $\\mathcal{M}(G)$ associated with $G\\,=\\,(V,D,B)$ is the family of multivariate normal distributions $\\mathcal{N}_{n}(0,\\Sigma)$ with $\\Sigma$ satisfying equation (1), for $\\mathring{\\Lambda}\\,\\in\\,\\mathbb{R}^{D}$ and $\\Omega\\,\\in\\,\\mathrm{PD}(B)$ . A model in $\\mathcal{M}(G)$ is specified in a natural way in terms of a system of linear structural equations: $X_{j}\\,=$ $\\begin{array}{r}{\\sum_{i\\in\\mathrm{pa}(j)}\\lambda_{i,j}X_{i}\\!+\\!\\varepsilon_{j}}\\end{array}$ , for $j=1,\\dots,n$ , where $\\mathtt{p a}(j)$ denote the parents of $j$ in $G$ . If $\\boldsymbol{\\varepsilon}=(\\varepsilon_{1},\\dots,\\bar{\\varepsilon}_{n})$ is a random vector with the multivariate normal distribution $\\mathcal{N}_{n}(0,\\Sigma)$ and $\\Lambda\\in\\mathbb{R}^{D}$ , then the random vector $\\boldsymbol{X}=(X_{1},\\ldots,X_{n})$ is well defined as a solution to the equation system and follows a centered multivariate normal distribution with covariance matrix $(I-\\mathring{\\Lambda})^{-T}\\Omega(\\mathring{I}-\\Lambda)^{-1}$ (see, e.g. [17]). ", "page_idx": 3}, {"type": "text", "text": "For a given (acyclic) mixed graph $G=(V,D,B)$ , define the parametrization map ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\phi_{G}:(\\Lambda,\\Omega)\\mapsto(I-\\Lambda)^{-T}\\Omega(I-\\Lambda)^{-1}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and let $\\Theta:=\\mathbb{R}^{D}\\times\\operatorname{PD}(B)$ . We say that $G$ is globally identifiable if $\\phi_{G}$ is injective on $\\Theta$ [17]. ", "page_idx": 3}, {"type": "text", "text": "Global identification can be decided easily, see [17, Theorem 2]. However, it is a very strong property. For instance, as seen in the introduction, in Figure 1, we can recover the parameter $\\lambda_{2,3}$ as $\\frac{\\sigma_{1,\\overset{.}{3}}}{\\sigma_{1,2}}$ . If $\\sigma_{1,2}=0$ , then the identification fails, so the instance is not globally identifiable. But identification fails only in the (very unlikely) case that $\\sigma_{1,2}\\,=\\,0$ . This leads to the concept of generic identifiability: ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Generic Identifiability, [21]). The mixed graph $G$ is said to be generically identifiable $i f\\,\\phi_{G}$ is injective on the complement $\\Theta\\setminus\\mathcal{V}$ of a proper (i.e., strict) algebraic subset $\\nu\\subset\\Theta$ . ", "page_idx": 3}, {"type": "text", "text": "Given matrices $\\Lambda_{0}\\in\\mathbb{R}^{D}$ and $\\Omega_{0}\\in\\mathrm{PD}(B)$ , the corresponding fiber is defined by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{F}_{G}(\\Lambda_{0},\\Omega_{0})=\\{(\\Lambda,\\Omega)\\mid\\phi_{G}(\\Lambda,\\Omega)=\\phi_{G}(\\Lambda_{0},\\Omega_{0}),\\Lambda\\in\\mathbb{R}^{D},\\Omega\\in\\mathrm{PD}(B)\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "A fiber contains all pairs of matrices that induce the same observed covariance matrix $\\Sigma$ . For $\\Sigma\\in$ $\\operatorname{im}\\phi_{G}$ , we also write ${\\mathcal{F}}_{G}(\\Sigma)$ for the fiber belonging to $\\Sigma$ . We can phrase identifiability in terms of fibers: ", "page_idx": 3}, {"type": "text", "text": "\u2022 $G$ is globally identifiable, if $\\left|\\mathcal{F}_{G}(\\Lambda_{0},\\Omega_{0})\\right|=1$ for all $\\Lambda_{0}\\in\\mathbb{R}^{D}$ and $\\Omega_{0}\\in\\mathrm{PD}(B)$ . \u2022 $G$ is generically identifiable, if $|\\mathcal{F}_{G}(\\Lambda_{0},\\Omega_{0})|\\;=\\;1$ for Zariski almost all $\\Lambda_{0}\\,\\in\\,\\mathbb{R}^{D}$ and $\\Omega_{0}\\in\\mathrm{{PD}}(B)$ . ", "page_idx": 3}, {"type": "text", "text": "Generic identifiability asks whether all parameters are almost always identifiable in the Zariski sense, that is, everywhere except for a lower dimensional algebraic set. For generic identifiability, we only consider the parameters $\\lambda_{i,j}$ since the parameters $\\omega_{k,l}$ can be recovered from the $\\lambda_{i,j}$ and $\\sigma_{i,j}$ using (1), see also [16]. ", "page_idx": 3}, {"type": "text", "text": "It is also of interest to ask whether a single parameter $\\lambda_{i,j}$ is almost always identifiable. For this, we consider the projection of the fiber on the single parameter, which we will also call an edge fiber: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{F}_{G}^{i,j}(\\Lambda_{0},\\Omega_{0})=\\{\\Lambda_{i,j}\\ |\\ (\\Lambda,\\Omega)\\in\\mathcal{F}_{G}(\\Lambda_{0},\\Omega_{0})\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "(Above $\\Lambda_{i,j}$ denotes the entry of $\\Lambda$ in the position $(i,j)$ , that is, $\\lambda_{i,j}$ .) ", "page_idx": 3}, {"type": "text", "text": "Definition 2. The parameter $\\lambda_{i,j}$ is generically edge identifiable, i $f\\,|\\mathcal{F}_{G}^{i,j}(\\Lambda_{0},\\Omega_{0})|=1$ for Zariski almost all $\\Lambda_{0}\\in\\mathbb{R}^{D}$ and $\\Omega_{0}\\in\\mathrm{PD}(B)$ . ", "page_idx": 3}, {"type": "text", "text": "Global and generic identifiability are properties of the given mixed graph. In this work, we also study identification as a property of the observed numerical data, i.e., of the observed covariance matrix $\\Sigma$ . ", "page_idx": 4}, {"type": "text", "text": "Definition 3 (Numerical Identifiability). Given an acyclic mixed graph $G=(V,D,B)$ and a feasible matrix $\\Sigma$ , decide whether the parameters are uniquely identifiable, i.e. $\\begin{array}{r}{\\dot{\\i}f\\left|\\mathcal{F}_{G}(\\Sigma)\\right|=1?}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "Note that this is a promise problem. We assume that $\\Sigma$ is feasible, i.e., in the image of $\\phi_{G}$ . Therefore, we shall also study the feasibility problem: Given $\\Sigma$ , is it contained in $\\operatorname{im}\\phi_{G}$ ? ", "page_idx": 4}, {"type": "text", "text": "Similarly we can also define numerical edge identifiability: For a given feasible $\\Sigma$ , test whether the edge fiber $\\Sigma$ belongs to has size 1 or $>1$ . ", "page_idx": 4}, {"type": "text", "text": "2.2 The (Existential) Theory of the Reals ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The existential theory of the reals (ETR) is the set of true sentences of the form ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\exists x_{1}\\dots\\exists x_{n}\\;\\varphi(x_{1},\\ldots,x_{n}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\varphi$ is a quantifier-free Boolean formula over the basis $\\{\\lor,\\land,\\lnot\\}$ and a signature consisting of the constants 0 and 1, the functional symbols $^+$ and \u00b7, and the relational symbols $<,\\,\\leq$ , and $=$ . The sentence is interpreted over the real numbers in the standard way. The theory forms its own complexity class $\\exists\\mathbb{R}$ which is defined as the closure of ETR under polynomial-time manyone reductions. Many natural problems have been shown to be complete for $\\mathrm{ETR}$ , for instance the computation of Nash equilibria [35], the famous art gallery problem [1], or training neural networks [5], just to mention a few. See the recent compendium [34] for a complete overview. ", "page_idx": 4}, {"type": "text", "text": "It turns out that one can simplify the form of an ETR-instance. We can get rid of the relations $<$ and $\\leq$ and it is sufficient to consider only Boolean conjunctions. More precisely, the following problem is $\\exists\\mathbb{R}$ -complete: Given polynomials $p_{1},\\ldots,p_{m}$ in variables $x_{1},\\ldots,x_{n}$ , decide whether there is a $\\xi\\in\\mathbb{R}^{n}$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{1}(\\xi)=\\cdots=p_{m}(\\xi)=0.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "By Tseitin\u2019s trick, we can assume that all polynomials are of one of the forms ", "page_idx": 4}, {"type": "equation", "text": "$$\na b-c,\\;\\;a+b-c,\\;\\;a-b,\\;\\;a-1,\\;\\;a\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and all variables in each of the polynomials are distinct. Note that all polynomials in (4) have degree at most two. Therefore, this problem is also called the feasibility problem of quadratic equations QUAD. For a proof, see e.g. [35]. ", "page_idx": 4}, {"type": "text", "text": "Universal Quantification. If, instead of considering existentially quantified true sentences, we consider universally quantified true sentences of the form ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\forall x_{1}\\ldots\\forall x_{n}\\;\\varphi(x_{1},\\ldots,x_{n}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\varphi$ is again a quantifier-free Boolean formula, and form the closure under polynomial-time many-one reductions, we obtain the complexity class $\\forall\\mathbb{R}$ . Using De Morgan\u2019s law, it is easy to see the well-known fact that $\\forall\\mathbb{R}=\\mathsf{c o-\\exists\\mathbb{R}}$ , i.e. it is the complement class of $\\exists\\mathbb{R}$ . ", "page_idx": 4}, {"type": "text", "text": "It is also possible to alternate quantifiers, giving rise to a whole hierarchy, comparable to the wellknown polynomial time hierarchy, see [36]. We call the corresponding classes $\\mathbf{\\bar{\\beta}}\\forall\\mathbb{R},\\forall\\exists\\mathbb{R},$ ... ", "page_idx": 4}, {"type": "text", "text": "Complexity of $\\exists\\mathbb{R}$ and $\\forall\\mathbb{R}$ . It is easy to see that quantification over real variables can be used to simulate quantification over Boolean variables by adding the constraint $x(x-1)=0$ . This way we can convert 3SAT-formulas to ETR-formulas, proving the well-known containment ${\\mathsf{N P}}\\subseteq\\exists\\mathbb{R}$ . ", "page_idx": 4}, {"type": "text", "text": "With his celebrated result about quantifier elimination, Renegar [33] proved that the truth of any sentence over the reals with a constant amount of quantifier alternations is decidable in PSPACE. This in particular implies ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\mathsf{N P}}\\subseteq{\\exists\\mathbb{R}\\subseteq{\\mathsf{P S P A C E}}\\qquad{\\mathrm{and}}\\qquad{\\mathsf{c o N P}}\\subseteq{\\mathsf{V I R}}\\subseteq{\\mathsf{P S P A C E}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "While all these inclusions are believed to be strict, it is unknown for all of them. ", "page_idx": 4}, {"type": "text", "text": "3 Finding Another Solution ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Numerical identification is a promise problem, i.e., we assume that the given input is feasible. Being a promise problem means that an algorithm for numerical identification should output the correct answer whenever the input is feasible. But it can output anything when the input is not feasible. We give some further information about promise problems in Appendix B for the reader\u2019s convenience. ", "page_idx": 5}, {"type": "text", "text": "For our hardness proof, we need to look at instances of ETR or QUAD that are satisfiable. Of course, deciding whether a satisfiable instance is satisfiable is a trivial task. So the task will be to decide whether the satisfiable instance has another solution. We call the corresponding promise problems $\\mathrm{ETR^{++}}$ and $\\mathrm{QUAD^{++}}$ . ", "page_idx": 5}, {"type": "text", "text": "It turns out that these promise problems are $\\exists\\mathbb{R}$ -hard. Since $\\mathrm{QUAD^{++}}$ is a special case of $\\mathrm{ETR^{++}}$ , it suffices to prove this for $\\mathrm{QUAD^{++}}$ . Let $y$ be an extra variable. We will plant an extra solution into the system (3): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{y(y-1)=0\\quad}&{{}}\\\\ {y x_{i}=0\\quad}&{{}i=1,\\ldots,n}\\\\ {(y-1)p_{j}=0\\quad}&{{}j=1,\\ldots,m}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Lemma 1. The system above has the following solutions: ", "page_idx": 5}, {"type": "text", "text": "1. $y=1$ , $x_{1}=\\cdot\\cdot=x_{n}=0$   \n2. $y=0$ , $x_{1}=\\xi_{1},\\ldots,x_{n}=\\xi_{n},$ , where $\\xi\\in\\mathbb{R}^{n}$ is any solution to the original instance. ", "page_idx": 5}, {"type": "text", "text": "In particular, the system always has a solution. It has more than one solution iff the original QUADinstance is satisfiable. ", "page_idx": 5}, {"type": "text", "text": "Proof. The first equation (7) constrains $y$ to be $\\{0,1\\}$ -valued. If $y=0$ , then the equations (8) are trivially satisfied and (9) reduces to the original instance (3). If $y=1$ , then the equations (9) are trivially satisfied and (8) reduces to $x_{1}\\,=\\,\\cdot\\,\\cdot\\,=\\,x_{n}\\,=\\,0$ . Note that in both cases we always get different solutions since the $y$ -value differs. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "Using the transformation in the lemma above, we can map any QUAD-instance into a $\\mathrm{QUAD^{++}}$ - instance and obtain ", "page_idx": 5}, {"type": "text", "text": "Corollary 1. ETR $^{++}$ and QUAD $^{++}$ are \u2203R-hard.", "page_idx": 5}, {"type": "text", "text": "4 Hardness of Numerical Identifiability ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "This section is dedicated to proving: ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. Numerical identifiability is $\\forall\\mathbb{R}$ -hard. ", "page_idx": 5}, {"type": "text", "text": "The proof consists of building a polynomial-time reduction from the complement of $\\mathrm{QUAD^{++}}$ to numerical identifiability, i.e., we construct an acyclic mixed graph $G$ and a $\\Sigma\\,\\in\\,\\mathrm{im}\\,\\phi_{G}$ , such that the fiber ${\\mathcal{F}}_{G}(\\Sigma)$ has size 1 iff the given $\\mathrm{QUAD^{++}}$ -instance has only one solution. For this, we use the following characterization of fibers due to [16]: ", "page_idx": 5}, {"type": "text", "text": "Lemma 3. Let $G=(V,D,B)$ be an acyclic mixed graph, and let $\\Sigma\\in\\operatorname{im}\\phi_{G}$ . The fiber ${\\mathcal{F}}_{G}(\\Sigma)$ is isomorphic to the set of matrices $\\Lambda\\in\\mathbb{R}^{\\tilde{D}}$ that solve the equation system ", "page_idx": 5}, {"type": "equation", "text": "$$\n[(I-\\Lambda)^{T}\\Sigma(I-\\Lambda)]_{i,j}=0,\\qquad\\qquad\\qquad i\\not=j,i\\leftrightarrow j\\not\\in B\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We construct $G$ as follows: The directed edges form a bipartite graph with edges going from the bottom layer to the top layer. Every node at the bottom layer has outdegree one. Moreover bidirected edges exist between all pairs of nodes, except for certain pairs of nodes of the top layer. See Figure 3 (left-hand side) for an illustration. ", "page_idx": 5}, {"type": "text", "text": "This missing edge in Figure 3 corresponds to the equation ", "page_idx": 5}, {"type": "equation", "text": "$$\n0=\\sigma_{i,j}-\\sum_{\\ell=1}^{n}\\sigma_{a_{\\ell},j}\\lambda_{a_{\\ell},i}-\\sum_{k=1}^{m}\\sigma_{b_{k},i}\\lambda_{b_{k},j}+\\sum_{\\ell=1}^{n}\\sum_{k=1}^{m}\\sigma_{a_{\\ell},b_{k}}\\lambda_{a_{\\ell},i}\\lambda_{b_{k},j}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "in Lemma 3. ", "page_idx": 5}, {"type": "image", "img_path": "bNDwOoxj6W/tmp/0d481847906c9e622b8fd3ea517934e67916ae965a58368f5fa1a0d2f7872d64.jpg", "img_caption": ["Figure 3: Left: A single missing edge on the top layer. Right: The gadget storing the value of each variable. $\\lambda_{i,r}$ corresponds to $x_{i}$ . "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "bNDwOoxj6W/tmp/937990f5672d41c072587da2d74791a51e3f3331e61a5f40a8c31c30f68db3ce.jpg", "img_caption": ["Figure 4: Left: Gadget for affine linear constraints. Right: Gadget for multiplicative constraints. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Observation 4. All $\\sigma$ values that appear in (11) cannot appear in any other missing edge equation of missing edges in the top layer. Parameters $\\sigma_{a_{\\ell},j}$ can only appear in an equation of a missing edge that contains the node $j$ and another node $h$ such that there is a directed edge from a\u2113to $h$ . However, $(a_{\\ell},i)$ is the only such edge, since the nodes in the bottom layer only have outdegree one. The same is true for $\\sigma_{b_{k},j}.\\ \\sigma_{a_{\\ell},b_{\\ell}}$ can only appear in an equation of a missing edge $h\\leftrightarrow h^{\\prime}$ if there are directed edges $(a_{\\ell},h)$ and $(b_{k},h^{\\prime})$ . By the same argument, $i\\leftrightarrow j$ is the only such missing edge. Furthermore $\\sigma_{i,j}$ can obviously only appear in this equation. ", "page_idx": 6}, {"type": "text", "text": "The above observation means that we can freely \u201cprogram\u201d the equations, that is, we can freely choose the $\\sigma$ -values in each missing edge equation without interfering with any other missing edge equation. ", "page_idx": 6}, {"type": "text", "text": "We start with a gadget with one node $r$ in the top layer and $n$ nodes in the bottom layer connected to it. It is used to store the value of each variable of our ETR instance, $\\lambda_{1,r}$ corresponds to $x_{1}$ , $\\lambda_{2,r}$ corresponds to $x_{2}$ , etc, see Figure 3 (right-hand side) for an illustration. ", "page_idx": 6}, {"type": "text", "text": "By assuming all polynomials in our $\\mathrm{QUAD^{++}}$ -instance are of the forms (4), we need to be able to encode products and affine linear forms. We start by showing how to encode an arbitrary affine lviinae aa r mciosnsistnrga iendt $\\textstyle\\sum_{\\ell=1}^{n}\\alpha_{\\ell}x_{\\ell}=\\beta$ using a single additional node $i$ in the top layer, \u201cconnected\u201d to $r$ ", "page_idx": 6}, {"type": "text", "text": "Setting $\\sigma_{r,i}=\\beta$ and $\\sigma_{\\ell,i}=\\alpha_{\\ell}$ , $1\\leq\\ell\\leq n$ , makes (11) together with $\\lambda_{\\ell,r}=x_{\\ell}$ directly equivalent to $\\textstyle\\sum_{\\ell=1}^{n}{\\dot{\\alpha}}\\ell{\\mathcal{x}}_{\\ell}=\\beta$ . ", "page_idx": 6}, {"type": "text", "text": "Encoding a product $x_{a}=x_{b}\\cdot x_{c}$ requires two additional nodes $i$ and $j$ in the top layer, with missing bidirectional edges between them and $r$ . Furthermore we introduce two nodes $i^{\\prime}$ and $j^{\\prime}$ in the bottom layer, connected to $i$ and $j$ respectively, see Figure 4 (right-hand side). This introduces three equations. The missing edge $r\\leftrightarrow i$ enforces $\\lambda_{i^{\\prime},i}=\\lambda_{c,r}$ by setting $\\sigma_{r,i}=\\sigma_{1,i^{\\prime}}=...=\\sigma_{n,i^{\\prime}}=0.$ , $\\sigma_{r,i^{\\prime}}=-1$ , $\\sigma_{c,i}=1$ , and $\\sigma_{\\ell,i}=0$ , for all $\\ell\\in\\{1,\\ldots,n\\}\\setminus\\{c\\}$ in (11). We use the missing edge $i\\leftrightarrow j$ to further ensure $\\lambda_{j^{\\prime},j}\\,=\\,\\lambda_{i^{\\prime},i}\\,=\\,\\lambda_{c,r}$ , for which we set $\\sigma_{i,j}\\,=\\,\\sigma_{i^{\\prime},j^{\\prime}}\\,=\\,0$ , $\\sigma_{i^{\\prime},j}=1$ , and $\\sigma_{i,j^{\\prime}}=-1$ . After having copied $\\lambda_{c,r}$ twice, we are finally able to enforce the multiplication itself using the missing edge $r\\leftrightarrow j$ . We set $\\sigma_{r,j}\\,=\\,\\sigma_{r,j^{\\prime}}\\,=\\,0$ , $\\sigma_{a,j}\\,=\\,1\\$ , $\\sigma_{b,j^{\\prime}}=1$ , $\\sigma_{\\ell,j}\\,=\\,0$ , for all $\\ell\\in\\}\\bar{\\{}},\\ldots,n\\}\\setminus\\bar{\\{}a\\}$ , and $\\sigma_{\\ell,j^{\\prime}}=0$ , for all $\\ell\\in\\{1,\\ldots,n\\}\\setminus\\{b\\}$ . We need to copy the parameter $\\lambda_{c,r}$ twice to be able to \u201cprogram\u201d the equation corresponding to the missing edge $r\\leftrightarrow j$ . ", "page_idx": 6}, {"type": "text", "text": "Proof of Theorem 2. Let polynomials $p_{1},\\ldots,p_{m}$ in variables $x_{1},\\ldots,x_{n}$ be a $\\mathrm{QUAD^{++}}$ -instance with all polynomials being one of the forms in (4). Let the number of affine linear polynomials among $p_{1},\\ldots,p_{m}$ be $k$ . Then the graph $G=(V,D,B)$ constructed above has $\\ell:=1+n+k+$ $4(m-k)=1+n+4m-3k$ nodes. Using Observation 4, we see that the construction induces a well-defined partial matrix $\\Sigma\\in\\mathbb{R}^{\\ell\\times\\ell}$ . Every entry of $\\Sigma$ not defined by the construction is set to ", "page_idx": 6}, {"type": "text", "text": "0 if it is off-diagonal and $\\ell$ if it is on the diagonal. Since all $\\sigma_{i,j}$ set in the construction are from $\\{-1,0,1\\}$ and off-diagonal, $\\Sigma$ strictly diagonally dominant by our choice of $\\ell$ and thus positive definite by the Gershgorin circle theorem [23]. ", "page_idx": 7}, {"type": "text", "text": "Remains to prove $\\Sigma\\,\\in\\,\\mathrm{im}\\,\\phi_{G}$ . Let $\\xi\\in\\mathbb{R}^{n}$ be any solution with $p_{1}(\\xi)=\\cdots=p_{m}(\\xi)=0$ . The existence of $\\xi$ is guaranteed by the promise of $\\mathrm{QUAD^{++}}$ . Create $\\Lambda\\in\\mathbb{R}^{\\ell\\times\\ell}$ as follows: $\\lambda_{i,r}=\\xi_{i}$ for $i\\in\\{1,\\dotsc,n\\}$ and $\\lambda_{i^{\\prime},i}=\\lambda_{j^{\\prime},j}=\\xi_{c}$ whenever the vertices $i,i^{\\prime},j,j^{\\prime}$ are the vertices added by the construction due to a multiplication. All other entries of $\\Lambda$ are 0. Then $I-\\Lambda$ is invertible and we have $\\Sigma=\\phi_{G}(\\Lambda,\\Omega)$ for $\\Omega\\,\\dot{^{\\prime}}=(I-\\Lambda)^{T}\\Sigma(I-\\Lambda)$ . Furthermore $\\Omega$ is positive definite due to $\\Sigma$ being positive definite and $\\Omega\\in\\operatorname{PD}(B)$ . ", "page_idx": 7}, {"type": "text", "text": "By Lemma 3, this implies that $|\\mathcal{F}_{G}(\\Sigma)|$ is precisely the number of solutions of our $\\mathrm{QUAD^{++}}$ - instance. So if the $\\mathrm{QUAD^{++}}$ -instance is a yes-instance, that is, has more than one solution, then our constructed instance is not numerically identifiable. If the $\\mathrm{QUAD^{++}}$ -instance is a no-instance, that is, has only one solution, then our constructed instance is numerically identifiable. So we have a reduction from the complement of $\\mathrm{QUAD^{++}}$ . The theorem now follows, since by Corollary 1, $\\mathrm{QUAD^{++}}$ is $\\exists\\mathbb{R}$ -hard and the complement of an $\\exists\\mathbb{R}$ -hard problem is $\\forall\\mathbb{R}$ -hard. \u53e3 ", "page_idx": 7}, {"type": "text", "text": "5 Upper Bound for Numerical Identifiability ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we show a $\\forall\\mathbb{R}$ upper bound for numerical identifiability and thus, combined with Theorem 2, prove an almost3 matching lower and upper bound. We start with the following lemma. The $\\exists\\mathbb{R}$ part will be needed in the next section. ", "page_idx": 7}, {"type": "text", "text": "Lemma 5. Membership in $\\mathrm{PD}(n)$ and $\\mathrm{PD}(B)$ can be expressed in $\\exists\\mathbb{R}$ and $\\forall\\mathbb{R}$ . 4 ", "page_idx": 7}, {"type": "text", "text": "Proof. For the $\\exists\\mathbb{R}$ expression, we use the fact that every real positive definite matrix $A\\,\\in\\,\\mathbb{R}^{n\\times n}$ has a Cholesky decomposition $A\\,=\\,L L^{T}$ where $L$ is a real lower triangular matrix with positive diagonal entries. We can thus express $A\\in\\operatorname{PD}(n)$ as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\exists L\\in\\mathbb{R}^{n\\times n}:A=L L^{T}\\land\\bigwedge_{i\\in\\{1,\\dotsc,n\\}}(L_{i,i}>0\\quad\\land\\bigwedge_{j\\in\\{i+1,\\dotsc,n\\}}L_{i,j}=0).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We quantify over matrices and consider matrix equations in (12). But this can be easily rewritten as an ETR-instance by quantifying over all entries of the matrix and having one individual equation for each entry of the matrix equation. ", "page_idx": 7}, {"type": "text", "text": "For the $\\forall\\mathbb{R}$ expression, we directly use the definition of positive definite matrices to express $A\\in$ $\\mathrm{PD}(n)$ as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\forall x\\in\\mathbb{R}^{n}:x\\neq0\\implies x^{T}A x>0.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "For membership $A\\in\\mathrm{{PD}}(B)$ , in both $\\exists\\mathbb{R}$ and $\\forall\\mathbb{R}$ , we add the constraint $\\bigwedge_{(i,j)\\notin B\\wedge i\\neq j}A_{i,j}=0$ to (12) and (13), respectively. \u53e3 ", "page_idx": 7}, {"type": "text", "text": "We remind the reader that numerical identifiability is a promise problem with the promise that the input $\\Sigma\\in\\operatorname{im}\\phi_{G}$ , so it suffices to check whether all elements in the fiber ${\\mathcal{F}}_{G}(\\Sigma)$ are identical. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\forall\\Lambda_{1},\\Lambda_{2}\\in\\mathbb R^{D},\\Omega_{1},\\Omega_{2}\\in\\mathrm{PD}(B):\\qquad\\qquad\\qquad\\qquad}\\\\ {\\phi_{G}(\\Lambda_{1},\\Omega_{1})=\\phi_{G}(\\Lambda_{2},\\Omega_{2})=\\Sigma\\Rightarrow(\\Lambda_{1}=\\Lambda_{2}\\wedge\\Omega_{1}=\\Omega_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The checks $\\phi_{G}(\\Lambda_{i},\\Omega_{i})=\\Sigma$ are implemented using $\\Omega_{i}=(I-\\Lambda_{i})^{T}\\Sigma(I-\\Lambda_{i})$ , which is equivalent due to $I-\\Lambda_{i}$ being invertible for any $\\boldsymbol{\\Lambda}_{i}\\in\\mathbb{R}^{D}$ . This proves the following: ", "page_idx": 7}, {"type": "text", "text": "Theorem 6. Numerical identifiability is in (the promise version of) $\\forall\\mathbb{R}$ . ", "page_idx": 7}, {"type": "text", "text": "Remark 7. Strictly speaking, numerical identifiability is not contained in $\\forall\\mathbb{R}$ , since it is a promise problem, that is, the outcome is not specified for $\\Sigma$ that are not feasible. $\\forall\\mathbb{R}$ consists by definition only of classical decision problems, where the outcome is specified for all inputs. So the corresponding complexity class is Promise- $\\forall\\mathbb{R}$ . Section $B$ contains some more information on promise problems for the reader\u2019s convenience. ", "page_idx": 7}, {"type": "text", "text": "However, we can express feasibility in $\\exists\\mathbb{R}$ : ", "page_idx": 8}, {"type": "text", "text": "Lemma 8. Membership in $\\operatorname{im}\\phi_{G}$ can be expressed in $\\exists\\mathbb{R}$ . ", "page_idx": 8}, {"type": "text", "text": "Proof. We use the expression $\\exists\\Lambda\\in\\mathbb{R}^{D},\\Omega\\in\\mathrm{PD}(B):(I-\\Lambda)^{T}\\Sigma(I-\\Lambda)=\\Omega$ , where we use Lemma 5 to express $\\Omega\\in\\operatorname{PD}(B)$ in $\\exists\\mathbb{R}$ , that is, we quantify over an arbitrary matrix $\\Omega$ first and add the ETR expression from Lemma 5 to ensure that $\\Omega$ is in $\\mathrm{PD}(B)$ . \u53e3 ", "page_idx": 8}, {"type": "text", "text": "Hence, we can check in $\\exists\\mathbb{R}$ whether the input $\\Sigma$ is feasible and then in $\\forall\\mathbb{R}$ whether the fiber has only one element. Using Renegar\u2019s algorithm, we get: ", "page_idx": 8}, {"type": "text", "text": "Corollary 2. Numerical identifiability can be decided in polynomial space. ", "page_idx": 8}, {"type": "text", "text": "6 Generic Identifiability is in PSPACE ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Let DIM denote the following problem: Given an encoding of a semi-algebraic set $S$ and a number $d$ , decide whether $\\dim S\\geq d$ . ", "page_idx": 8}, {"type": "text", "text": "Lemma 9 (Koiran $[27]^{5}$ ). The problem DIM is $\\exists\\mathbb{R}$ -complete. Moreover, this is even true when the set is given by an existentially quantified formula as in (2). ", "page_idx": 8}, {"type": "text", "text": "We use the same notation as in Section 2.1. Let $G\\,=\\,(V,D,B)$ be a mixed graph. Let $S_{G}\\,=$ $\\{(\\Lambda,\\Omega)\\ |\\ |\\mathcal{F}_{G}(\\Lambda,\\Omega)|>1,\\Lambda\\in\\mathbb{R}^{D},\\Omega\\in\\mathrm{PD}(B)\\}$ . ", "page_idx": 8}, {"type": "text", "text": "Observation 10. $G$ is generically identifiable iff $\\dim\\mathbb{R}^{D}+\\dim\\operatorname{PD}(B)>\\dim S_{G}.$ ", "page_idx": 8}, {"type": "text", "text": "Proof. As $S_{G}\\subseteq\\mathbb{R}^{D}\\times\\operatorname{PD}(B)$ and $\\dim(\\mathbb{R}^{D}\\times\\mathrm{PD}(B))=\\dim\\mathbb{R}^{D}+\\dim\\mathrm{PD}(B)$ , the right-hand side is just the definition of being generically identifiable. \u53e3 ", "page_idx": 8}, {"type": "text", "text": "We postpone the proof that membership in $S_{G}$ can be expressed in $\\exists\\mathbb{R}$ , in favor of first giving our algorithm to decide generic identifiability, using this observation: ", "page_idx": 8}, {"type": "text", "text": "Theorem 11. Generic identifiability is both in $\\forall\\exists\\mathbb{R}$ and $\\exists\\forall\\mathbb{R}$ . ", "page_idx": 8}, {"type": "text", "text": "Proof. Let $G$ be the given mixed graph. Formulate membership in $\\mathbb{R}^{D},\\operatorname{PD}(B)$ , and $S_{G}$ as instances of ETR using Lemmas 5 and 12. Note that the number of variables and the sizes of these instances are polynomial in the size of $G$ . Now we can check whether $G$ is generically identifiable by checking the condition in Observation 10. ", "page_idx": 8}, {"type": "text", "text": "We first assume that we have oracle access to ETR, that is, we can query ETR a polynomial number of times. We decide whether $G$ is generically identifiable as follows: ", "page_idx": 8}, {"type": "text", "text": "1. Use Koiran\u2019s algorithm (see Lemma 9) repeatedly to compute $\\mathrm{dim}\\,S_{G}$ by checking whether di $\\operatorname{m}S_{G}\\geq d$ for $d=0,\\ldots,2n^{2}$ . (We could even use binary search.) 2. Compute $\\mathrm{dim}\\,\\mathbb{R}^{D}$ and $\\dim\\operatorname{PD}(B)$ in the same way.6 Here it suffices to check up to the maximum possible dimension of $d=n^{2}$ . 3. Accept if $\\dim\\mathbb{R}^{D}+\\dim\\operatorname{PD}(B)>\\dim S_{G}$ , and reject otherwise. ", "page_idx": 8}, {"type": "text", "text": "The algorithm is correct by Observation 10. The algorithm above would already show the PSPACE upper bound for generic identifiability. ", "page_idx": 8}, {"type": "text", "text": "However, we can implement the algorithm above by a single formula by replacing the repeated use of Koiran\u2019s algorithm by a big disjunction: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\bigvee_{d_{1}=0}^{n^{2}}\\bigvee_{d_{2}=0}^{n^{2}}(\\dim\\mathbb{R}^{D}\\ge d_{1}\\wedge\\dim\\mathrm{PD}(B)\\ge d_{2}\\wedge\\dim S_{G}<d_{1}+d_{2})\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "5see Appendix D for why this statement follows from [27]. 6While we could compute these dimensions more directly, this is not necessary to obtain PSPACE algorithm. However for implementing this algorithm in practice, we would advise computing them more efficiently. ", "page_idx": 8}, {"type": "text", "text": "Note that the check $\\dim S_{G}<d_{1}{+}d_{2}$ needs to be implemented as $\\neg(\\dim S_{G}\\geq d_{1}\\!+\\!d_{2})$ , thus being in $\\forall\\mathbb{R}$ by De Morgan\u2019s laws. The existential and universal quantifiers are however independent, giving upper bounds of both $\\forall\\exists\\mathbb{R}$ and $\\exists\\forall\\mathbb{R}$ . \u53e3 ", "page_idx": 9}, {"type": "text", "text": "Using Renegar\u2019s algorithm this implies: ", "page_idx": 9}, {"type": "text", "text": "Corollary 3. Generic identifiability can be decided in PSPACE. ", "page_idx": 9}, {"type": "text", "text": "It only remains to show how to express membership in $S_{G}$ as an ETR-formula.   \nLemma 12. Membership in $S_{G}$ can be expressed in $\\exists\\mathbb{R}$ . ", "page_idx": 9}, {"type": "text", "text": "Proof. The membership of some $(\\Lambda,\\Omega)$ in $S_{G}$ can be expressed as ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Lambda\\in\\mathbb{R}^{D}\\wedge\\Omega\\in\\mathrm{PD}(B)\\wedge\\exists\\Sigma\\in\\mathbb{R}^{n\\times n},\\Lambda^{\\prime}\\in\\mathbb{R}^{D},\\Omega^{\\prime}\\in\\mathrm{PD}(B):(I-\\Lambda)^{T}\\Sigma(I-\\Lambda)=\\Omega}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\wedge\\left(I-\\Lambda^{\\prime}\\right)^{T}\\Sigma(I-\\Lambda^{\\prime})=\\Omega^{\\prime}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\wedge\\left(\\Lambda\\neq\\Lambda^{\\prime}\\vee\\Omega\\neq\\Omega^{\\prime}\\right).\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Remark 13. The algorithm of Theorem 11 as is only tests generic identifiability. Since the problem has a high degree, one cannot expect that the solutions have easy expressions. However, Renegar\u2019s algorithm shows that the solutions are the linear factors of a certain polynomial, see $[33].$ . ", "page_idx": 9}, {"type": "text", "text": "7 A Note on Cyclic Graphs ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our results so far have depended on the fact that every matrix $I-\\Lambda$ with $\\Lambda\\in\\mathbb{R}^{D}$ is invertible if the graph is acyclic. However, if the graph is cyclic, $I-\\Lambda$ is not necessarily invertible. So in this case, we need to explicitly consider the subset $\\mathbb{R}_{\\mathrm{reg}}^{\\bullet_{D}}$ of matrices $\\Lambda\\in\\mathbb{R}^{D}$ such that $I-\\Lambda$ is invertible. ", "page_idx": 9}, {"type": "text", "text": "For matrices $\\Lambda_{0}\\in\\mathbb{R}_{\\mathrm{reg}}^{D}$ and $\\Omega_{0}\\in\\mathrm{PD}(B)$ , [21] define fibers as ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\mathcal{F}_{G}(\\Lambda_{0},\\Omega_{0})=\\{(\\Lambda,\\Omega)\\mid\\phi_{G}(\\Lambda,\\Omega)=\\phi_{G}(\\Lambda_{0},\\Omega_{0}),\\Lambda\\in\\mathbb{R}_{\\mathrm{reg}}^{D},\\Omega\\in\\mathrm{PD}(B)\\}.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "They determine generic identifiability for cyclic graphs in terms of these fibers. That is a mixed (cyclic) graph $G$ is said to be generically identifiable if $|\\mathcal{F}_{G}(\\Lambda_{0},\\Omega_{0})|\\,=\\,1$ for Zariski almost all $\\dot{\\Lambda_{0}}\\in\\dot{\\mathbb{R}_{\\mathrm{reg}}^{\\breve{D}}}$ and $\\Omega_{0}\\in\\mathrm{PD}(B)$ . ", "page_idx": 9}, {"type": "text", "text": "This is the same criterion used for acyclic graphs, except $\\mathbb{R}^{D}$ has been replaced by $\\mathbb{R}_{\\mathrm{reg}}^{D}$ twice. Matrix invertibility can be easily expressed in , using the definition of invertibility: ", "page_idx": 9}, {"type": "equation", "text": "$$\nA{\\mathrm{~is~invertible}}\\iff\\exists B\\in\\mathbb{R}^{n\\times n}:A B=I.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Hence, all our upper bounds also hold for general graphs. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Due to double exponential runtime, the state-of-the-art algorithm for the generic identification problem is often too slow to solve instances of reasonable size. For example, Garc\u00b4\u0131a-Puente et al. [22] report that the runtime varies between seconds and 75 days for graphs with four nodes. An interesting topic for future work would be to implement the (theoretical) algorithm presented in our paper. ", "page_idx": 9}, {"type": "text", "text": "We have given a new upper on the complexity of generic identifiability, namely PSPACE. More precisely, we showed that it is in $\\exists\\forall\\mathbb{R}$ and $\\forall\\exists\\mathbb{R}$ . This can be even improved to $\\forall\\mathbb{R}$ . It is not necessary to express the dimension of $\\mathbb{R}^{D}$ and $\\mathrm{PD}(B)$ in terms of the theory of the reals, but they can be calculated directly, $\\mathrm{dim}\\mathbb{R}^{D}=|D|$ as well as $\\dim\\operatorname{PD}(B)=n+|{\\dot{B}}|$ . In the light of our hardness proofs for the new notion of numerical identifiability, we conjecture that generic identifiability is hard for $\\forall\\mathbb{R}$ , too. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was supported by the Deutsche Forschungsgemeinschaft (DFG) grant 471183316 (ZA 1244/1-1). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Mikkel Abrahamsen, Anna Adamaszek, and Tillmann Miltzow. The art gallery problem is $\\exists\\mathbb{R}$ - complete. In Proc. ACM SIGACT Symposium on Theory of Computing (STOC), pages 65\u201373, 2018. [2] T.W. Anderson and Herman Rubin. Statistical inference in factor analysis. In Proc. of the Berkeley Symposium on Mathematical Statistics and Probability, page 111. University of California Press, 1956.   \n[3] Elias Bareinboim, Juan D. Correa, Duligur Ibeling, and Thomas Icard. On Pearl\u2019s Hierarchy and the Foundations of Causal Inference, pages 507\u2013556. Association for Computing Machinery, New York, NY, USA, 2022. [4] Saugata Basu, Richard Pollack, and Marie-Franc\u00b8oise Roy. Algorithms in Real Algebraic Geometry. Springer, 2003. [5] Daniel Bertschinger, Christoph Hertrich, Paul Jungeblut, Tillmann Miltzow, and Simon Weber. Training fully connected neural networks is $\\exists\\mathbb{R}$ -complete. In Proc. Conference on Neural Information Processing Systems (NeurIPS), 2023. [6] J. Bochnak, M. Coste, and M.F. Roy. Real Algebraic Geometry. Ergebnisse der Mathematik und ihrer Grenzgebiete. 3. Folge / A Series of Modern Surveys in Mathematics. Springer Berlin Heidelberg, 2013. [7] Kenneth A. Bollen. Structural equations with latent variables. John Wiley & Sons, 1989. [8] R.J. Bowden and D.A. Turkington. Instrumental variables. Cambridge University Press, 1984. [9] Carlos Brito. Instrumental sets. In Rina Dechter, Hector Geffner, and Joseph Y Halpern, editors, Heuristics, Probability and Causality. A Tribute to Judea Pearl, chapter 17, pages 295\u2013308. College Publications, 2010.   \n[10] Carlos Brito and Judea Pearl. Generalized instrumental variables. In Proc. Conference on Uncertainty in Artificial Intelligence (UAI), pages 85\u201393, 2002.   \n[11] Carlos Brito and Judea Pearl. A graphical criterion for the identification of causal effects in linear models. In Proc. AAAI Conference on Artificial Intelligence (AAAI), pages 533\u2013538, 2002.   \n[12] Michael W. Browne. Generalized least squares estimators in the analysis of covariance structures. South African Statistical Journal, 8(1):1\u201324, 1974.   \n[13] Bryant Chen. Identification and overidentification of linear structural equation models. In Proc. International Conference on Neural Information Processing Systems (NeurIPS), pages 1587\u20131595, 2016.   \n[14] Bryant Chen, Daniel Kumor, and Elias Bareinboim. Identification and model testing in linear structural equation models using auxiliary variables. In Proc. International Conference on Machine Learning (ICML), pages 757\u2013766. PMLR, 2017.   \n[15] Bryant Chen, Judea Pearl, and Elias Bareinboim. Incorporating knowledge into structural equation models using auxiliary variables. In Proc. International Joint Conference on Artificial Intelligence (IJCAI), pages 3577\u20133583, 2015.   \n[16] Mathias Drton. Algebraic problems in structural equation modeling. In The 50th anniversary of Gr\u00a8obner bases, pages 35\u201386. Mathematical Society of Japan, 2018.   \n[17] Mathias Drton, Rina Foygel, and Seth Sullivant. Global identifiability of linear structural equation models. The Annals of Statistics, 39(2):865\u2013886, 2011.   \n[18] Otis Dudley Duncan. Introduction to structural equation models. Academic Press, 1975.   \n[19] Franklin M. Fisher. The identification problem in econometrics. McGraw-Hill, 1966.   \n[20] Ronald Aylmer Fisher. Design of experiments. British Medical Journal, 1(3923):554, 1936.   \n[21] Rina Foygel, Jan Draisma, and Mathias Drton. Half-trek criterion for generic identifiability of linear structural equation models. The Annals of Statistics, 40(3):1682\u20131713, 2012.   \n[22] Luis D. Garc\u00b4\u0131a-Puente, Sarah Spielvogel, and Seth Sullivant. Identifying causal effects with computer algebra. In Proc. Conference on Uncertainty in Artificial Intelligence (UAI), pages 193\u2013200. AUAI Press, 2010.   \n[23] S Gers\u02c7gorin. U\u00a8ber die Abgrenzung der Eigenwerte einer Matrix. Bulletin de l\u2019Acad\u00b4emie des Sciences de l\u2019URSS. Classe des sciences math\u00b4ematiques et na, 6:749\u2013754, 1931.   \n[24] Aaryan Gupta and Markus Bla\u00a8ser. Identification for tree-shaped structural causal models in polynomial time. In Proc. AAAI Conference on Artificial Intelligence (AAAI), pages 20404\u2013 20411, 2024.   \n[25] Li-tze Hu, Peter M Bentler, and Yutaka Kano. Can test statistics in covariance structure analysis be trusted? Psychological bulletin, 112(2):351, 1992.   \n[26] Karl G. Jo\u00a8reskog. A general approach to confirmatory maximum likelihood factor analysis. Psychometrika, 34(2):183\u2013202, 1969.   \n[27] Pascal Koiran. The real dimension problem is $\\mathbf{NP}_{\\mathbb{R}}$ -complete. J. Complex., 15(2):227\u2013238, 1999.   \n[28] Daniel Kumor, Bryant Chen, and Elias Bareinboim. Efficient identification in linear structural causal models with instrumental cutsets. In Proc. Advances in Neural Information Processing Systems (NeurIPS), pages 12477\u201312486, 2019.   \n[29] Daniel Kumor, Carlos Cinelli, and Elias Bareinboim. Efficient identification in linear structural causal models with auxiliary cutsets. In Proc. International Conference on Machine Learning (ICML), pages 5501\u20135510. PMLR, 2020.   \n[30] Bengt Muthen. Latent variable structural equation modeling with categorical data. Journal of Econometrics, 22(1-2):43\u201365, 1983.   \n[31] Judea Pearl. Parameter identification: A new perspective. Technical Report R-276, UCLA, 2001.   \n[32] Judea Pearl. Causality. Cambridge University Press, 2009.   \n[33] James Renegar. On the computational complexity and geometry of the first-order theory of the reals. parts i-iii. Journal of symbolic computation, 13(3):255\u2013352, 1992.   \n[34] Marcus Schaefer, Jean Cardinal, and Tillmann Miltzow. The existential theory of the reals as a complexity class: A compendium. CoRR, abs/2407.18006, 2024.   \n[35] Marcus Schaefer and Daniel Stefankovic. Fixed points, Nash equilibria, and the existential theory of the reals. Theory Comput. Syst., 60(2):172\u2013193, 2017.   \n[36] Marcus Schaefer and Daniel Stefankovic. Beyond the existential theory of the reals. Theory Comput. Syst., 68(2):195\u2013226, 2024.   \n[37] Benito van der Zander and Maciej Lis\u00b4kiewicz. On searching for generalized instrumental variables. In Proc. International Conference on Artificial Intelligence and Statistics (AISTATS), pages 1214\u20131222, 2016.   \n[38] Benito van der Zander, Johannes Textor, and Maciej Lis\u00b4kiewicz. Efficiently finding conditional instruments for causal inference. In Proc. International Joint Conference on Artificial Intelligence (IJCAI), pages 3243\u20133249, 2015.   \n[39] Benito van der Zander, Marcel Wieno\u00a8bst, Markus Bla\u00a8ser, and Maciej Lis\u00b4kiewicz. Identification in tree-shaped linear structural causal models. In Proc. International Conference on Artificial Intelligence and Statistics (AISTATS), pages 6770\u20136792. PMLR, 2022.   \n[40] Luca Weihs, Bill Robinson, Emilie Dufresne, Jennifer Kenkel, Kaie Kubjas, Reginald L. McGee II, Nhan Nguyen, Elina Robeva, and Mathias Drton. Determinantal generalizations of instrumental variables. Journal of Causal Inference, 6(1), 2018.   \n[41] Philip G. Wright. Tariff on animal and vegetable oils. Macmillan Company, New York, 1928.   \n[42] Sewall Wright. Correlation and causation. Journal of agricultural research, 20(7):557, 1921. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Edge Identifiability ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For edge identifiability, we obtain the same results as for identifiability itself. ", "page_idx": 13}, {"type": "text", "text": "A.1 Hardness of Numerical Edge Identifiability ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "If we analyze the reduction in Theorem 2 in a bit more detail, we can use the same reduction to show Corollary 4. Numerical edge identifiability is $\\forall\\mathbb{R}$ -hard. ", "page_idx": 13}, {"type": "text", "text": "Proof. If instead of starting with an arbitrary $\\mathrm{QUAD^{++}}$ -instance, we start with a $\\mathrm{QUAD^{++}}$ - instance generated by Lemma 1 and Corollary 1. These instances have a distinguished variable $y$ , such that there is always a single solution with $y=1$ and possibly multiple solutions with $y=0$ . W.l.o.g. let $x_{1}$ be this distinguished variable. Following the reduction in Theorem 2, we construct a graph $G$ and a $\\Sigma\\in\\operatorname{im}\\phi_{G}$ , such that the fiber ${\\mathcal{F}}_{G}(\\Sigma)$ is isomorphic to the solutions our $\\mathrm{QUAD^{++}}$ - instance. In particular the value of $\\lambda_{1,r}$ in the elements of ${\\mathcal{F}}_{G}(\\Sigma)$ is exactly the value of $x_{1}$ in the solutions to the $\\mathrm{QUAD^{++}}$ -instance. Thus $|\\mathcal{F}_{G}^{1,r}(\\Sigma)|=1$ iff the $\\mathrm{QUAD^{++}}$ instance has exactly one solution, otherwise $|\\mathcal{F}_{G}^{1,r}(\\Sigma)|=2$ . \u53e3 ", "page_idx": 13}, {"type": "text", "text": "A.2 Upper Bound for Numerical Edge Identifiability ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Similarly to (14), we can express numerical edge identifiability as the $\\forall\\mathbb{R}$ -formula ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\forall\\Lambda_{1},\\Lambda_{2}\\in\\mathbb{R}^{D},\\Omega_{1},\\Omega_{2}\\in\\mathrm{PD}(B):\\qquad\\qquad\\qquad\\qquad}\\\\ {\\phi_{G}(\\Lambda_{1},\\Omega_{1})=\\phi_{G}(\\Lambda_{2},\\Omega_{2})=\\Sigma\\Rightarrow(\\Lambda_{1})_{i,j}=(\\Lambda_{2})_{i,j}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This yields ", "page_idx": 13}, {"type": "text", "text": "Theorem 14. Numerical edge identifiability is in (the promise version of) $\\forall\\mathbb{R}$ . ", "page_idx": 13}, {"type": "text", "text": "Again using Renegar\u2019s algorithm we also get Corollary 5. Numerical edge identifiability can be decided in polynomial space. ", "page_idx": 13}, {"type": "text", "text": "A.3 Generic Edge Identifiability is in PSPACE ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We modify the algorithm of Section 6 to work with generic edge identifiability for some $\\lambda_{i,j}$ rather than generic identifiability. Let $S_{G}^{i,j}=\\{(\\Lambda,\\Omega)\\ |\\ |\\mathcal{F}_{G}^{i,j}(\\Lambda,\\Omega)|>1,\\Lambda\\in\\mathbb{R}^{D},\\Omega\\in\\mathrm{PD}(B)\\}$ . Then we have the following analog to Observation 10: ", "page_idx": 13}, {"type": "text", "text": "Observation 15. $\\lambda_{i,j}$ is generically edge identifiable iff $\\dim\\mathbb{R}^{D}+\\dim\\mathrm{PD}(B)>\\dim S_{G}^{i,j}.$ ", "page_idx": 13}, {"type": "text", "text": "Lemma 16. Membership in $S_{G}^{i j}$ can be expressed in $\\exists\\mathbb{R}$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. We use a similar formula to Lemma 12: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Lambda\\in\\mathbb{R}^{D}\\wedge\\Omega\\in\\mathrm{PD}(B)\\wedge\\exists\\Sigma\\in\\mathbb{R}^{m\\times m},\\Lambda^{\\prime}\\in\\mathbb{R}^{D},\\Omega^{\\prime}\\in\\mathrm{PD}(B):(I-\\Lambda)^{T}\\Sigma(I-\\Lambda)=\\Omega}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\wedge\\left(I-\\Lambda^{\\prime}\\right)^{T}\\Sigma(I-\\Lambda^{\\prime})=\\Omega^{\\prime}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\wedge\\left(\\Lambda_{i,j}\\neq\\Lambda_{i,j}^{\\prime}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Theorem 17. Generic edge identifiability is both in \u2200\u2203R and $\\exists\\forall\\mathbb{R}$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. We use the algorithm of Theorem 11, but replace the set $S_{G}$ by $S_{G}^{i,j}$ . ", "page_idx": 13}, {"type": "text", "text": "Corollary 6. Generic edge identifiability can be decided in PSPACE. ", "page_idx": 13}, {"type": "text", "text": "B Promise problems ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We give some background information on promise problems for the readers convenience. In a classical decision problem $L$ , we are given an input and we have to decide whether $x\\in L$ (the so-called yes-instances) or $x\\notin L$ (the no-instances). For instance, in the classical SAT problem, we are given a Boolean formula $F$ in CNF. The yes-instances are the satisfiable formulas and the no-instances are the unsatisfiable one. ", "page_idx": 14}, {"type": "text", "text": "Promise problems have a third type of instances, the so-called do-not-care-instances. On these instances, an algorithm can do what it wants and give any output. For instance, consider the problem $\\mathrm{SAT^{++}}$ , where we ask the question of whether a satisfiable formula in CNF has another satisfying assignment. The yes-instances are all $F$ with at least two satisfying assignments, the no-instances are all $F$ with exactly one satisfying assignment, and the do-not-care-instances are all unsatisfiable $F$ . An algorithm solving $\\mathrm{SAT^{++}}$ has to output \u201cyes\u201d on every $F$ with at least two satisfying assignments and \u201cno\u201d on every $F$ with exactly one satisfying assignment. On unsatisfiable formulas, it can output whatever it wants. Note that every classical decision problem is also a promise problem with the do-not-care-instances being the empty set. ", "page_idx": 14}, {"type": "text", "text": "We can also define many-one reductions for promise problems: A function $f:\\{0,1\\}^{*}\\to\\{0,1\\}^{*}$ is called a many-one reduction from a promise problem $L$ to another promise problem $L^{\\prime}$ , if $f$ maps yes-instances of $L$ to yes-instances of $L^{\\prime}$ and no-instances of $L$ to no-instances of $L^{\\prime}$ . $f$ can map do-not-care-instances of $L$ to any instance of $L^{\\prime}$ . By using a similar trick of encoding an additional satisfying assignment like in the case of $\\mathrm{ETR^{++}}$ , one can show that $\\mathrm{SAT^{++}}$ is NP-hard, since we can reduce SAT to it. This reduction maps the unsatisfiable formulas (no-instances of SAT) to formulas with one satisfying assignment (no-instances of $\\mathrm{SAT^{++}}$ ) and satisfiable formulas (yesinstances of SAT) to formulas with two or more satisfying assignments (yes-instances of $\\mathrm{SAT}^{\\ddagger+}$ ). Since SAT is a classical decision problem, there are no do-not-care-instances. $\\mathrm{SAT^{++}}$ is, however, not contained in NP for formal reasons, because NP only contains classical decision problems. ", "page_idx": 14}, {"type": "text", "text": "C Semialgebraic sets ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For the reader\u2019s convenience, we give a brief introduction to semialgebraic sets and discuss the notations important for this work. For details and proofs, we refer to the book [4]. ", "page_idx": 14}, {"type": "text", "text": "A semialgebraic set in $\\mathbb{R}^{n}$ is a finite Boolean combination (finite number of unions and intersections) of sets of the form $\\{(x_{1},\\ldots,x_{n})\\mid f(x_{1},\\ldots,x_{n})>0\\}$ and $\\{(x_{1},\\ldots,x_{n})\\mid g(x_{1},\\ldots,x_{n})\\ge0\\}$ . Here $f$ and $g$ are real polynomials in $n$ variables. ", "page_idx": 14}, {"type": "text", "text": "A semialgebraic function is a function $\\mathbb{R}^{n}\\to\\mathbb{R}^{n^{\\prime}}$ with a semialgebraic graph, that is, the set of all $\\{(x,f(x))\\mid x\\in\\mathbf{\\mathrm{\\bar{R}}}^{n}\\}$ is a semialgebraic set. ", "page_idx": 14}, {"type": "text", "text": "From this definition of semialgebraic sets, it is easy to see that semialgebraic sets are the solutions of ETR-instances, that is, all $(x_{1},\\ldots,x_{n})$ satisfying $\\varphi(x_{1},\\ldots,x_{n})$ in (2) form a semialgebraic set. From Tarski\u2019s theorem (see [4]), it follows that semialgebraic sets allow quantifier elimination, that is, all $(x_{1},\\ldots,x_{n})$ satisfying ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\exists y_{1}\\,.\\,.\\,.\\,\\exists y_{t}\\psi(x_{1},\\,.\\,.\\,.\\,,x_{n},y_{1},\\,.\\,.\\,.\\,,y_{t})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "form a semialgebraic set, where $\\psi$ (like $\\varphi$ ) is a quantifier-free Boolean formula over the basis $\\{\\lor,\\land,\\lnot\\}$ and a signature consisting of the constants 0 and 1, the functional symbols $^+$ and \u00b7, and the relational symbols $<,\\,\\leq$ , and $=.\\;\\;\\psi$ depends on two sets of variables. It is clear that all $(x_{1},\\ldots,x_{n},y_{1},\\ldots,y_{t})$ satisfying $\\psi$ form a semialgebraic set. Tarski\u2019s theorem tells us that we still get a semialgebraic set when we are projecting the $y_{1},\\ldots,y_{t}$ away using the existential quantifiers. This is used frequently in our proofs. ", "page_idx": 14}, {"type": "text", "text": "Definition 4. A semialgebraic set $S$ has dimension $d$ if there exists a $d$ -dimensional coordinate subspace such that the image of $S$ under the canonical projection onto this subspace has a nonempty interior and there is no such subspace of dimension $d+1$ . ", "page_idx": 14}, {"type": "text", "text": "Above, by a $d_{\\cdot}$ -dimensional coordinate subspace, we mean the subspace of $\\mathbb{R}^{n}$ of all points $x$ such that $x_{i}=0$ for $i\\notin I$ for some subset $I\\subseteq\\{1,\\ldots,n\\}$ and $|I|=d$ . ", "page_idx": 14}, {"type": "text", "text": "Theorem 18 (see e.g. [6]). Let $S$ be a semialgebraic set. Then its dimension as a semialgebraic set (as in Definition 4) equals the dimension of its Zariski closure as an algebraic set. ", "page_idx": 14}, {"type": "text", "text": "D Koiran\u2019s algorithm ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Koiran mainly works in the so-called BSS-model of real computation. In this model, one is also allowed to use arbitrary real constants in the algorithm as well as real inputs. In $\\exists\\mathbb{R}$ , we only allow the constants 0 and 1 and the inputs are given by some binary encoding. However, Koiran also considers the bit model. [27, Theorem 6] proves the computational equivalence of DIM and 4FEAS in the bit model. Since 4FEAS is $\\exists\\mathbb{R}$ -complete [35], this implies that DIM is $\\exists\\mathbb{R}$ -complete. Note that at the time Koiran wrote his paper, the class $\\exists\\mathbb{R}$ was not formally defined and therefore, Koiran does not mention it explicitly. ", "page_idx": 15}, {"type": "text", "text": "For the moreover part, note that [27, Section 1.1] discusses the representations of semi-algebraic sets that are supported by his proof. There he mentions existentially quantified formulas explicitly. ", "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: The two main claims from the abstract can be found as Corollary 3 and Theorem 2. In the introduction, we mention five items in the subsection \u201cOur contributions\u201d. They correspond to Corollary 3, Theorem 11, Theorem 2, Corollary 2, and the results in Appendix A. All results are formally proved. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: Our main algorithm is (provably) complete and thus does not have limitations regarding the correctness of the output. This is an algorithms theory paper, so we consider classical worst case complexity. Therefore, there are no limitations regarding the inputs. The complexity upper bounds are formally proved. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The paper contains all proofs in the main part, with the exception of the proofs about edge identifiability, which are almost exactly the same as the proofs from the main part with slight modifications. They can be found in Appendix A instead. All proofs are formal and the assumptions are clearly stated. All theorems, formulas, and proofs in the paper are numbered and cross-referenced. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: This paper does not contain experimental results. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 17}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not contain any experiments. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 18}, {"type": "text", "text": "\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: This is a purely theory based paper with no experiments. No data sets were used. No crowdsourcing or contract work was done. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: This is a purely theory based paper. We do not expect an improvement in the complexity of identification algorithms to have any negative societal impact. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This is a theory paper with no data or models. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not contain any assets. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not contain any experiments or research with human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not contain any experiments or research with human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]