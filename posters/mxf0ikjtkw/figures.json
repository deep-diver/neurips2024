[{"figure_path": "MxF0IKJtKW/figures/figures_3_1.jpg", "caption": "Figure 1: The figure illustrates Batched Greedy Pruning on attention blocks, where W is a output matrix and H is the corresponding Hessian. Different colors represent distinct attention heads and gray indicates the pruned weights.", "description": "This figure illustrates the process of Batched Greedy Pruning, a method used to efficiently prune attention heads in large language models. It shows how the algorithm reorders columns of the weight matrix (W) and the inverse Hessian matrix (H\u207b\u00b9), performs a grouped Cholesky decomposition, and then prunes columns based on calculated errors. Finally, it shows how the columns are restored and compensated for after pruning.", "section": "4.2 Batched Greedy Pruning"}, {"figure_path": "MxF0IKJtKW/figures/figures_5_1.jpg", "caption": "Figure 2: Per-layer FFN output error between the original LLaMA-7B and three distinct pruned models. The pruned models each implement a first-layer reduction of 25%, 50%, and 75%, respectively. The PPL of original model is 12.63. For ease of visualization, the layer index has been truncated to 25.", "description": "This figure shows the error accumulation in layer-wise pruning. Three models are pruned with different first-layer pruning ratios (25%, 50%, 75%). The y-axis represents the squared error between the outputs of the original and pruned models for each layer. The x-axis represents the layer index. The figure demonstrates that the error increases with model depth and accumulates at a rate exceeding linear progression as the initial layer\u2019s pruning ratio increases.", "section": "4.3 Incremental Pruning Ratio"}, {"figure_path": "MxF0IKJtKW/figures/figures_9_1.jpg", "caption": "Figure 3: Effects of Calibration Sample Size & Sequence Length.", "description": "This figure shows the impact of calibration sample size and sequence length on the performance of SlimGPT.  The left subplot (a) illustrates how both perplexity (PPL) and average zero-shot performance improve as the calibration sample size increases from 128 to 2048.  The right subplot (b) shows a similar trend for increasing sequence lengths from 64 to 2048 tokens, indicating that larger and more diverse calibration data leads to better model compression and performance. ", "section": "5.3 Ablation Study"}, {"figure_path": "MxF0IKJtKW/figures/figures_16_1.jpg", "caption": "Figure 4: Layer-wise pruning ratio on LLaMA-7B with total pruning ratio 50%.", "description": "This figure compares the layer-wise pruning ratios employed by two different methods: SlimGPT and LLM-Pruner, both aiming for a 50% overall pruning rate.  The x-axis represents the layer index, and the y-axis shows the pruning ratio applied to each individual layer. SlimGPT adopts a logarithmic increase strategy, starting with a lower pruning ratio in the initial layers and gradually increasing it towards the deeper layers. In contrast, LLM-Pruner employs a heuristic approach with a uniform pruning ratio in the intermediate layers while avoiding pruning in the initial and final layers. This visualization highlights the distinct layer-wise pruning strategies and how they differ in distributing the overall pruning ratio across the different layers of the model.", "section": "5.3 Ablation Study"}, {"figure_path": "MxF0IKJtKW/figures/figures_16_2.jpg", "caption": "Figure 5: Alpaca train loss & Wikitext2 evaluation loss.", "description": "This figure shows the training loss on the Alpaca dataset and the evaluation loss on the Wikitext2 dataset during the fine-tuning stage.  The training loss decreases and converges normally, while the evaluation loss shows no significant fluctuations, indicating that fine-tuning is not overfitting to the Wikitext2 dataset.", "section": "5.3 Ablation Study"}]