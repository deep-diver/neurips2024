[{"figure_path": "MxF0IKJtKW/tables/tables_6_1.jpg", "caption": "Table 1: PPL & Commonsense Reasoning zero-shot performance of the pruned LLaMA-7B. The average score is computed across seven datasets. The bolded results represent the optimal results, while the underlined ones is the sub-optimal results. The asterisk-marked (*) results are those replicated within a consistent experimental framework, which slightly differ from the original source.", "description": "This table presents the results of the perplexity (PPL) and commonsense reasoning zero-shot performance evaluation on seven datasets for the pruned LLaMA-7B model.  It compares different pruning methods (LLM-Pruner, Compresso, LoraPrune, and SlimGPT) at various pruning percentages (20%, 25%, 33%, 50%).  The table highlights the best performing method for each pruning percentage and dataset.", "section": "5.2 Main Result"}, {"figure_path": "MxF0IKJtKW/tables/tables_7_1.jpg", "caption": "Table 1: PPL & Commonsense Reasoning zero-shot performance of the pruned LLaMA-7B. The average score is computed across seven datasets. The bolded results represent the optimal results, while the underlined ones is the sub-optimal results. The asterisk-marked (*) results are those replicated within a consistent experimental framework, which slightly differ from the original source.", "description": "This table presents the results of the pruned LLaMA-7B model on seven different datasets, including perplexity (PPL) and commonsense reasoning zero-shot performance.  The table compares SlimGPT to several baseline methods (LLM-Pruner, Compresso, LoraPrune) across various pruning percentages (20%, 25%, 33%, 50%).  Results with and without fine-tuning are shown for SlimGPT.  The best and near-best performing methods are highlighted.", "section": "5.2 Main Result"}, {"figure_path": "MxF0IKJtKW/tables/tables_7_2.jpg", "caption": "Table 3: Pruning Runtime and Memory Usage", "description": "This table presents the runtime and GPU memory usage of the SlimGPT pruning method on LLaMA 7B and 13B models.  The runtime is broken down for two different pruning ratios: 20% and 50%. The memory usage represents the peak memory consumption during the pruning process.", "section": "5 Experiment"}, {"figure_path": "MxF0IKJtKW/tables/tables_7_3.jpg", "caption": "Table 4: Inference Latency and Memory Usage", "description": "This table presents the inference latency and memory usage of the pruned LLaMA-7B models. The models were pruned by 20% and 50%, resulting in parameter counts of 5.4B and 3.4B, respectively. The maximum output length was set to 512, and the results are averages from 50 inference trials.  The table shows the reduction in memory usage and latency achieved by pruning.", "section": "5.2 Main Result"}, {"figure_path": "MxF0IKJtKW/tables/tables_8_1.jpg", "caption": "Table 5: Pruning results under different strategies of SlimGPT. \u2018-DGS\u2019 means removing Dynamic Group Size for FFN while \u2018-GCD\u2019 means removing grouped Cholesky decomposition for attention blocks.", "description": "This table presents the ablation study results for SlimGPT, showing the impact of removing key components: Dynamic Group Size (DGS) for Feed-Forward Networks (FFNs) and Grouped Cholesky Decomposition (GCD) for attention blocks.  The table compares the performance of the full SlimGPT model against versions with DGS removed (-DGS) and GCD removed (-GCD), providing PPL and Zero-shot Avg. metrics to assess the effect of each component on the overall model performance.  The numbers in parentheses indicate the change in performance compared to the full SlimGPT model.", "section": "5.3 Ablation Study"}, {"figure_path": "MxF0IKJtKW/tables/tables_8_2.jpg", "caption": "Table 6: Pruning results with different pruning ratio strategies.", "description": "This table presents the results of experiments using various pruning ratio strategies.  It compares the performance (PPL and Zero-shot Avg) of models pruned using a logarithmic increase strategy (SlimGPT's default), linear increase, uniform, logarithmic decrease, and linear decrease strategies. The results highlight the effectiveness of SlimGPT's logarithmic increase strategy compared to other methods. Note that the numbers in parentheses represent the difference compared to SlimGPT's default method.", "section": "5.2 Main Result"}, {"figure_path": "MxF0IKJtKW/tables/tables_13_1.jpg", "caption": "Table 1: PPL & Commonsense Reasoning zero-shot performance of the pruned LLaMA-7B. The average score is computed across seven datasets. The bolded results represent the optimal results, while the underlined ones is the sub-optimal results. The asterisk-marked (*) results are those replicated within a consistent experimental framework, which slightly differ from the original source.", "description": "This table presents the results of the pruned LLaMA-7B model's performance on various tasks.  It compares the performance of SlimGPT with other state-of-the-art structured pruning methods (LLM-Pruner, Compresso, LoraPrune) across different pruning percentages (20%, 25%, 33%, 50%). The metrics used are Perplexity (PPL) for language modeling and average scores across seven commonsense reasoning datasets.  The table highlights SlimGPT's superior performance and the impact of fine-tuning on the results.", "section": "5.2 Main Result"}, {"figure_path": "MxF0IKJtKW/tables/tables_14_1.jpg", "caption": "Table 8: PPL & Commonsense Reasoning zero-shot performance of the pruned Vicuna-7B", "description": "This table presents the results of evaluating the pruned Vicuna-7B model on the Wikitext2 dataset (perplexity) and seven common sense reasoning datasets (zero-shot performance).  It compares the performance of the original Vicuna-7B model against LLM-Pruner and SlimGPT (with and without fine-tuning) at different pruning percentages (20% and 50%).  The table shows the number of parameters remaining, perplexity scores, and average scores across the common sense reasoning datasets for each model and pruning level.", "section": "5.2 Main Result"}, {"figure_path": "MxF0IKJtKW/tables/tables_14_2.jpg", "caption": "Table 9: PPL & Commonsense Reasoning zero-shot performance of the pruned LLaMA2-7B", "description": "This table shows the performance of pruned LLaMA2-7B models on the WikiText2 perplexity task and seven commonsense reasoning tasks.  It compares the performance of the original, unpruned model to models pruned using LLM-Pruner and SlimGPT (with and without fine-tuning).  The table presents the number of parameters, perplexity scores, and average scores across the commonsense reasoning tasks for different pruning percentages (20% and 50%).", "section": "5.2 Main Result"}, {"figure_path": "MxF0IKJtKW/tables/tables_14_3.jpg", "caption": "Table 10: MMLU 5-shot performance of the pruned LLaMA2-7b", "description": "This table presents the results of the MMLU (Massive Multitask Language Understanding) benchmark for pruned versions of the LLaMA2-7B model.  The evaluation uses a 5-shot setting, meaning five examples are provided for each task before the model makes a prediction.  The table compares the performance of the original model, a model pruned using LLM-Pruner, and two models pruned using SlimGPT (one fine-tuned with LoRA and one without). The performance is measured across four categories: Humanities, Social Sciences, STEM, and Other, with an average across all four.  This allows for an assessment of how well SlimGPT's pruning method preserves the model's knowledge across different domains and the impact of LoRA fine-tuning.", "section": "5.2 Main Result"}, {"figure_path": "MxF0IKJtKW/tables/tables_15_1.jpg", "caption": "Table 1: PPL & Commonsense Reasoning zero-shot performance of the pruned LLaMA-7B. The average score is computed across seven datasets. The bolded results represent the optimal results, while the underlined ones is the sub-optimal results. The asterisk-marked (*) results are those replicated within a consistent experimental framework, which slightly differ from the original source.", "description": "This table presents the performance of different pruned versions of the LLaMA-7B model on several metrics.  It compares the performance of SlimGPT with other state-of-the-art structured pruning methods such as LLM-Pruner and Compresso. The metrics include Perplexity (PPL) on the WikiText2 dataset, and average zero-shot performance across seven commonsense reasoning datasets (BoolQ, PIQA, HellaSwag, WinoGrande, ARC-easy, ARC-challenge, OpenbookQA).  The table shows results for different pruning percentages (20%, 25%, 33%, 50%), highlighting the impact of pruning on model size and performance.", "section": "5.2 Main Result"}, {"figure_path": "MxF0IKJtKW/tables/tables_15_2.jpg", "caption": "Table 12: MMLU 5-shot performance of the pruned Baichuan-7B", "description": "This table presents the results of the Massive Multitask Language Understanding (MMLU) benchmark on the pruned Baichuan-7B model.  It shows the performance (5-shot) across four categories: Humanities, Social Sciences, STEM, and Other, for different pruning percentages (20%) and methods (LLM-Pruner, SlimGPT with and without finetuning).  The table helps to evaluate the effectiveness of the pruning techniques on a complex multi-task language understanding benchmark.", "section": "5.2 Main Result"}, {"figure_path": "MxF0IKJtKW/tables/tables_15_3.jpg", "caption": "Table 13: LongBench evaluation results of the pruned Mistral-7B-Instruct-V2.0", "description": "This table presents the results of evaluating the pruned Mistral-7B-Instruct-V2.0 model on the LongBench benchmark.  It shows the performance (in terms of accuracy) on four different subtasks: Single-Doc QA, Multi-Doc QA, Summarization, and Few-shot.  The table compares the performance of the original, unpruned model (6.7B parameters) with the performance of the model pruned by SlimGPT to 5.4B parameters (20% pruning). The results indicate how well the pruned model maintains its performance on these complex long-context understanding tasks.", "section": "5 Experiment"}, {"figure_path": "MxF0IKJtKW/tables/tables_16_1.jpg", "caption": "Table 14: Pruning results with various calibration datasets", "description": "This table presents the results of pruning experiments conducted using different calibration datasets.  The performance of SlimGPT is evaluated by comparing the perplexity (PPL) scores on WikiText2 and average zero-shot scores across seven commonsense reasoning datasets. Three different calibration datasets were used: C4 (the default for SlimGPT), Alpaca, and GPT4-Alpaca.  The table shows that the choice of calibration dataset impacts the final results, suggesting that the dataset used for calibration should be carefully selected depending on the downstream task (language modeling versus commonsense reasoning).", "section": "5.3.1 Impact of Batched Greedy Pruning Strategy"}, {"figure_path": "MxF0IKJtKW/tables/tables_17_1.jpg", "caption": "Table 1: PPL & Commonsense Reasoning zero-shot performance of the pruned LLaMA-7B. The average score is computed across seven datasets. The bolded results represent the optimal results, while the underlined ones is the sub-optimal results. The asterisk-marked (*) results are those replicated within a consistent experimental framework, which slightly differ from the original source.", "description": "This table presents the results of the perplexity (PPL) and commonsense reasoning zero-shot performance for the pruned LLaMA-7B model at different pruning percentages (20%, 25%, 33%, 50%).  It compares the performance of SlimGPT (with and without fine-tuning) against other state-of-the-art methods like LLM-Pruner, Compresso, and LoraPrune.  The average scores are calculated across seven different commonsense reasoning datasets.  Optimal and suboptimal results are highlighted.", "section": "5.2 Main Result"}]