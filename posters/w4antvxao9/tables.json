[{"figure_path": "w4AnTVxAO9/tables/tables_4_1.jpg", "caption": "Table 1: Dataset statistics.", "description": "This table presents the number of data samples used for training, in-domain testing, out-of-domain easy testing, and out-of-domain hard testing for each of the three reasoning tasks: Analog of Algebra, Multi-digit Addition, and Directional Reasoning.  The sizes reflect the varying difficulty levels incorporated into the experimental design. The 'in-domain' sets are used for evaluating the model's ability to solve problems similar to those seen during training, while the 'out-of-domain' sets, categorized as 'easy' and 'hard', assess its generalization capability on unseen and more complex problems, respectively.", "section": "4.1 Datasets"}, {"figure_path": "w4AnTVxAO9/tables/tables_5_1.jpg", "caption": "Table 2: Step number following ability of the initialized Llama2 models across different tasks. \"# Skipping\" represents the number of instances where n \u2212 i > 0. \u201cStep Consistency\u201d quantifies the match between the actual number of steps taken and the number indicated in the input. \u201cAnswer Accuracy\u201d calculates the percentage of correct final answers out of the \u201c# Skipping\u201d cases. \u201cAverage Step\u201d reflects the mean number of steps across all predictions within the dataset.", "description": "This table presents the results of the step-number-following ability test on the initialized Llama2 model across three different reasoning tasks.  It shows the number of times the model skipped steps, how consistently the model followed the requested number of steps, the accuracy of the answers when the model skipped steps, and the average number of steps used in each task.  These metrics offer insights into the model's ability to adhere to instructions regarding the number of reasoning steps to be included in its answers.", "section": "5 Results"}, {"figure_path": "w4AnTVxAO9/tables/tables_6_1.jpg", "caption": "Table 3: Performance comparison of models from different phases. Avg steps denotes the average number of steps taken in the prediction. With the skipped step data, models achieve even better generalization performance with fewer steps.", "description": "This table compares the performance of language models trained with different data across various iterations.  It shows the accuracy and average number of steps taken by the models on in-domain and out-of-domain datasets. The in-domain data is the original data used for training, while the out-of-domain data represents new data with varying difficulty levels.  The table highlights the impact of including skipped reasoning steps within the dataset, demonstrating improved efficiency and often better generalization performance.", "section": "5 Results"}, {"figure_path": "w4AnTVxAO9/tables/tables_9_1.jpg", "caption": "Table 3: Performance comparison of models from different phases. Avg steps denotes the average number of steps taken in the prediction. With the skipped step data, models achieve even better generalization performance with fewer steps.", "description": "This table compares the performance of language models trained with different methods (cold start, warm start, and iterative training with skipped steps) across three reasoning tasks.  It shows the accuracy and average number of reasoning steps used by the models on in-domain and out-of-domain datasets.  The results demonstrate the effectiveness of incorporating skipped steps into the training data, leading to improved generalization and efficiency in solving problems.", "section": "5 Results"}, {"figure_path": "w4AnTVxAO9/tables/tables_15_1.jpg", "caption": "Table 3: Performance comparison of models from different phases. Avg steps denotes the average number of steps taken in the prediction. With the skipped step data, models achieve even better generalization performance with fewer steps.", "description": "This table compares the performance of language models across different phases (iterations) of training.  It shows the accuracy and average number of steps taken by the models to solve problems in three different scenarios: in-domain (standard test set), OOD-easy (out-of-distribution, easier), and OOD-hard (out-of-distribution, harder).  The comparison highlights the impact of incorporating step-skipping data during training on the model's efficiency and generalization ability.", "section": "5 Results"}, {"figure_path": "w4AnTVxAO9/tables/tables_16_1.jpg", "caption": "Table 3: Performance comparison of models from different phases. Avg steps denotes the average number of steps taken in the prediction. With the skipped step data, models achieve even better generalization performance with fewer steps.", "description": "This table compares the performance of language models across different training phases (iterations) and dataset types (in-domain and out-of-domain). It shows that incorporating skipped reasoning steps into the training data leads to improved accuracy and efficiency (fewer steps) in both in-domain and out-of-domain tasks, highlighting the models' ability to generalize well and solve problems more effectively.  The table specifically indicates the average number of steps used in predictions across the various phases and dataset types.", "section": "5 Results"}, {"figure_path": "w4AnTVxAO9/tables/tables_16_2.jpg", "caption": "Table 3: Performance comparison of models from different phases. Avg steps denotes the average number of steps taken in the prediction. With the skipped step data, models achieve even better generalization performance with fewer steps.", "description": "This table compares the performance of language models across different training phases (cold start, warm start, and iterations) for three different tasks (Analog of Algebra, Multi-digit Addition, and Directional Reasoning).  It shows the accuracy and average number of steps used by the model in both in-domain and out-of-domain (OOD-easy and OOD-hard) settings. The results demonstrate how the introduction of skipped reasoning steps, generated through the iterative training process, affects the models' performance, showing an improvement in both efficiency (fewer steps) and generalization capabilities.", "section": "5 Results"}, {"figure_path": "w4AnTVxAO9/tables/tables_16_3.jpg", "caption": "Table 8: Ablation of different data mixing choices on Analog of Algebra.", "description": "This table presents the results of an ablation study that investigates the impact of different data mixing strategies on the performance of a model trained on the Analog of Algebra task.  Specifically, it compares the performance of models trained using only generated skipping data (Skipping) versus models trained using a combination of cold-start data and generated skipping data (Skipping w/ Cold start). The results are presented in terms of accuracy and average number of steps for in-domain, OOD-easy, and OOD-hard datasets, illustrating the effect of different data combinations on model performance and generalization ability.", "section": "5.5 Analysis on the Influence of Training Steps"}, {"figure_path": "w4AnTVxAO9/tables/tables_17_1.jpg", "caption": "Table 9: Cross-domain generalization of step-skipping capability in the phi-3-mini model. In the specified \"Withheld Task\" setting, step-skipping data is excluded from one specific task, while the \"All\" setting includes only full-step data across three tasks.", "description": "This table presents the results of an experiment designed to evaluate the cross-domain generalization of the step-skipping ability learned by the model.  The experiment withheld step-skipping data from one task at a time, while training on full-step data and step-skipping data from the other two tasks.  The \"All\" setting serves as a baseline, using only full-step data for training across all three tasks. The table compares the in-domain, OOD-easy, and OOD-hard performance of the model across different evaluation tasks (Analog of Algebra, Multi-digit Addition, and Directional Reasoning), showing how the absence of step-skipping data in a specific task impacts overall performance.", "section": "5 Results"}, {"figure_path": "w4AnTVxAO9/tables/tables_19_1.jpg", "caption": "Table 10: Dataset split for GSM8K.", "description": "This table shows the dataset split used for GSM8K experiments in the paper. The GSM8K dataset was split into in-domain and out-of-domain sets based on the number of steps required to solve the problems. The in-domain set contained problems requiring no more than 4 steps, while the out-of-domain set contained the rest. The table shows the number of examples in the training and testing sets for both in-domain and out-of-domain problems.", "section": "B.7 Experiments on GSM8K"}, {"figure_path": "w4AnTVxAO9/tables/tables_19_2.jpg", "caption": "Table 11: Performance comparison across different iterations. The table shows accuracy and average steps for various test and training datasets.", "description": "This table presents the results of experiments conducted on the GSM8K dataset across five iterations (including a cold start).  The accuracy and average number of steps are shown for in-domain (Test-ID) and out-of-domain (Test-OOD and Train-OOD) data. The results demonstrate how the model's step-skipping behavior evolves over iterations and how it performs on different data conditions.", "section": "B.7 Experiments on GSM8K"}]