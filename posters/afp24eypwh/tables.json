[{"figure_path": "aFP24eYpWh/tables/tables_2_1.jpg", "caption": "Figure 11: Incorporating symmetries improves data efficiency. Importance-weighted lower bound (IWLB) (mean and std. err. over 3 random seeds) on rotated MNIST for a standard VAE (w. and w.o. data aug.) and two VAE variants that incorporate symmetries via our SGM. Improved data efficiency is demonstrated by better performance with less training data and less sensitivity to added rotation.", "description": "This figure compares the performance of four different models on the rotated MNIST dataset: a standard VAE, a VAE with data augmentation, AugVAE (a VAE that uses the proposed Symmetry-aware Generative Model (SGM) to resample transformed examples), and InvVAE (a VAE that uses the SGM to convert each example to its prototype).  The models are evaluated across different amounts of training data and varying degrees of added rotation. The results show that AugVAE demonstrates improved data efficiency, showing little performance degradation when training data is reduced or the amount of added rotation is increased, unlike the standard VAE. InvVAE achieves even higher likelihoods, showing high robustness to both data reduction and added rotation.  The results highlight the benefits of incorporating symmetries into generative models for improved efficiency and robustness.", "section": "4.2 VAE Data Efficiency"}, {"figure_path": "aFP24eYpWh/tables/tables_3_1.jpg", "caption": "Algorithm 1 Learning", "description": "This algorithm details the learning process for the Symmetry-aware Generative Model (SGM). It's a two-stage process: first, self-supervised learning (SSL) is used to learn the transformation inference function (fw), which maps an observation to its prototype. Then, maximum likelihood estimation (MLE) is used to learn the distribution over transformations (p\u03c8). The algorithm iteratively updates the parameters of fw and p\u03c8 until convergence.", "section": "2.1 Learning"}, {"figure_path": "aFP24eYpWh/tables/tables_4_1.jpg", "caption": "Figure 5: Idealised examples of simple and flexible learned distributions over angles p\u03c8(\u03b7|x)\u2014given the true distribution p(\u03b7|x) = \u03a3\u03c7\u03b5{8,..,8,...,8} p(\u03b7|x, x).", "description": "This table shows examples of simple and flexible learned distributions over angles given the true distribution.  It illustrates the concept of learning a distribution over transformations that captures symmetries by comparing simple unimodal Gaussian family against a more flexible bimodal mixture-of-Gaussian family.", "section": "3 Practical Considerations and Further Motivations"}, {"figure_path": "aFP24eYpWh/tables/tables_5_1.jpg", "caption": "Figure 6: Examples of learned distributions over angles p\u03c8 (\u00b7)\u2014with and without dependence on x, given the true distribution p(\u03b7 | x) = \u03a3x\u03b5{8,...,8,...,8} p(\u03b7 | x, x).", "description": "This table shows examples of learned distributions over angles, p\u03c8(\u03b7|x), comparing cases with and without dependence on x.  It illustrates the impact of considering the prototype x when modeling the distribution of transformations.  The true distribution, p(\u03b7|x), is a mixture of delta functions reflecting the discrete nature of rotations in the idealized data-generating process.  The table demonstrates that modeling the dependence on x leads to a more accurate and flexible representation of the distribution compared to an approach which ignores this dependency.", "section": "3 Practical Considerations and Further Motivations"}, {"figure_path": "aFP24eYpWh/tables/tables_5_2.jpg", "caption": "Figure 7: Examples of learned distributions over angles p\u03c8 (\u03b7 | x)\u2014with different degrees of invariance in the prototype x, given the true p (\u03b7 | x).", "description": "This table shows three scenarios of learning prototype x with different invariance levels.  The first column shows the FULL invariance case where a single prototype x is used for all variations. The PARTIAL invariance case shows that the model learned to use multiple prototypes for the same digit.  The NONE invariance shows that the model has learned a different prototype for each example.", "section": "3.2 Modelling Choices"}, {"figure_path": "aFP24eYpWh/tables/tables_17_1.jpg", "caption": "We compare rotation inference nets\u2014with hidden layers of dimensions [2048, 1024, 512, 256, 128] trained for 2k steps using the AdamW optimizer with a constant learning rate of 3 \u00d7 10\u22124 and a batch size of 256\u2014trained on fully rotated MNIST digits using both X-space and H-space SSL objectives:", "description": "This table shows the mean squared error (MSE) for x and \u03b7 on fully rotated MNIST digits when using either X-space or H-space self-supervised learning (SSL) objectives. The X-space objective measures the distance in the observation space between the original and transformed images.  The H-space objective uses a different transformation parameterization.  The table also provides the average MSE of both methods.", "section": "Learning"}, {"figure_path": "aFP24eYpWh/tables/tables_17_2.jpg", "caption": "Averaging multiple samples for the SSL loss. Just as we found averaging the MLE loss over multiple samples to improve performance, so too is averaging the SSL loss.", "description": "This table shows the results of an experiment to determine the optimal number of samples to use when averaging the self-supervised learning (SSL) loss.  The experiment used a rotation inference net with hidden layers of dimensions [2048, 1024, 512, 256, 128], trained for 2k steps using the AdamW optimizer with a cosine decay learning rate schedule, and a batch size of 256.  The table shows the mean x-mse for different numbers of samples (1, 3, 5, 10, and 30). The results show that the x-mse decreases until saturating around 5 samples, indicating that using 5 samples is a good trade-off between improved performance and increased compute time.", "section": "2.1 Learning"}]