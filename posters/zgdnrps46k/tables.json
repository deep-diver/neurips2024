[{"figure_path": "ZgDNrpS46k/tables/tables_14_1.jpg", "caption": "Figure 7: Comparison of the performance (final test accuracy) and fraction of dead ReLUs (inactive activations) across different settings. The learning rate in the initial phase of 5 epochs is scaled by a factor of either 1, 8, or 128. Standard (S) denotes normal training, while Frozen Biases (fb) involves freezing the biases at the onset of training. The Random (R) approach employs random gradient directions at the start of training, and Leaky ReLU replaces the ReLUs in standard models with Leaky ReLUs using a scaling factor of \u03b1 = 0.1. We observe a notable correspondence between large initial updates, higher ratios of dead ReLUs in ResNet-20, and performance degradation.", "description": "This table presents the results of an experiment comparing different training methods on a ResNet-20 model with varying initial update sizes.  The methods include standard training, training with frozen biases, training with random gradients, and training with leaky ReLU activations. Each method is tested with three scaling factors for the initial learning rate (1, 8, and 128). The table shows the final test accuracy and the fraction of dead ReLUs for each method and scaling factor. The results highlight the negative impact of large initial updates on performance and the mitigating effect of leaky ReLUs.", "section": "A The Detrimental Effects of Large Updates"}]