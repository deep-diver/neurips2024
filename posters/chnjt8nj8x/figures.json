[{"figure_path": "chnJT8Nj8X/figures/figures_3_1.jpg", "caption": "Figure 1: The methodology framework of Transformer Doctor. It begins by analyzing the dynamic integration of inter-token information in MHSA and the static integration of intra-token information in FFN, Subsequently, conjunction errors within them are diagnosed, and finally treated to enhance model performance.", "description": "This figure illustrates the methodology of Transformer Doctor, which consists of two main stages: diagnosing and treating.  The diagnosing stage involves analyzing the dynamic integration of information between tokens in the Multi-Head Self-Attention (MHSA) module and the static integration of information within tokens in the Feed-Forward Network (FFN) module.  This analysis helps identify conjunction errors (errors in the integration of information). The treating stage involves applying heuristic dynamic integration constraint methods and rule-based static integration constraint methods to correct these errors, ultimately improving model performance. The figure visually represents the information flow and processing steps in each module of the Transformer, highlighting the key components and steps involved in the proposed framework.", "section": "Information Integration Hypothesis based Transformer Treatments"}, {"figure_path": "chnJT8Nj8X/figures/figures_4_1.jpg", "caption": "Figure 2: Visual comparison of integration weights a in MHSA. (a) and (b) respectively present visualizations of weights a at different depths of blocks for high-confidence images and the overlay of reshaped and resized rows of a onto the original image. Similarly, (c) and (d) depict visualizations of weights a for low-confidence images and their overlay onto the original image.", "description": "This figure compares the integration weights (a) in the multi-head self-attention (MHSA) module of a vision transformer at different depths for both high-confidence and low-confidence images. The visualizations show how the weights change as the model processes the image, highlighting the dynamic integration of information among tokens.  High-confidence images show a diagonal pattern in shallower blocks, transitioning to a vertical pattern in deeper blocks, indicating a shift from local to global feature integration. Low-confidence images exhibit less consistent patterns, suggesting issues in the integration process. The overlay shows how these integration weights map onto the original images.", "section": "4 Information Integration Hypothesis based Transformer Diagnosis"}, {"figure_path": "chnJT8Nj8X/figures/figures_5_1.jpg", "caption": "Figure 2: Visual comparison of integration weights a in MHSA. (a) and (b) respectively present visualizations of weights a at different depths of blocks for high-confidence images and the overlay of reshaped and resized rows of a onto the original image. Similarly, (c) and (d) depict visualizations of weights a for low-confidence images and their overlay onto the original image.", "description": "This figure compares the integration weights (a) in the multi-head self-attention (MHSA) module of a vision transformer model at different depths (shallow vs. deep blocks) for both high and low-confidence image classifications.  It visually represents these weights as heatmaps, overlaid onto the original image to show the spatial distribution of attention. The comparison highlights differences in attention patterns between high-confidence (correctly classified) and low-confidence (incorrectly classified) predictions.  In high-confidence samples, the weights show a clear focus on relevant image features, whereas low-confidence images exhibit less coherent attention patterns.", "section": "4 Information Integration Hypothesis based Transformer Diagnosis"}, {"figure_path": "chnJT8Nj8X/figures/figures_8_1.jpg", "caption": "Figure 2: Visual comparison of integration weights a in MHSA. (a) and (b) respectively present visualizations of weights a at different depths of blocks for high-confidence images and the overlay of reshaped and resized rows of a onto the original image. Similarly, (c) and (d) depict visualizations of weights a for low-confidence images and their overlay onto the original image.", "description": "This figure visually compares the attention weights (integration weights 'a') within the multi-head self-attention (MHSA) module of a vision transformer at different depths (layers) for both high and low-confidence image classifications.  The left column shows the matrices of integration weights, revealing patterns that change as the network deepens.  The right column overlays a reshaped version of these weights onto the original input image as heatmaps, making it easier to understand which parts of the image were focused on during attention.", "section": "4 Information Integration Hypothesis based Transformer Diagnosis"}, {"figure_path": "chnJT8Nj8X/figures/figures_8_2.jpg", "caption": "Figure 1: The methodology framework of Transformer Doctor. It begins by analyzing the dynamic integration of inter-token information in MHSA and the static integration of intra-token information in FFN, Subsequently, conjunction errors within them are diagnosed, and finally treated to enhance model performance.", "description": "This figure illustrates the methodology of Transformer Doctor.  It's a flowchart showing the two main diagnostic steps (analyzing dynamic and static information integration) and treatment steps (applying dynamic and static integration constraints) to correct conjunction errors in Vision Transformers, ultimately improving model performance.", "section": "Information Integration Hypothesis based Transformer Treatments"}, {"figure_path": "chnJT8Nj8X/figures/figures_14_1.jpg", "caption": "Figure 2: Visual comparison of integration weights a in MHSA. (a) and (b) respectively present visualizations of weights a at different depths of blocks for high-confidence images and the overlay of reshaped and resized rows of a onto the original image. Similarly, (c) and (d) depict visualizations of weights a for low-confidence images and their overlay onto the original image.", "description": "This figure visualizes the integration weights (a) within the Multi-Head Self-Attention (MHSA) module of a Transformer model at various depths.  Panels (a) and (b) show the weights for high-confidence images, illustrating a transition from diagonal patterns in shallower layers to vertical patterns in deeper layers. The overlay on the original images helps to understand the spatial focus of these weights. Panels (c) and (d) repeat this visualization for low-confidence images, revealing a difference in weight patterns that is indicative of errors.", "section": "4 Information Integration Hypothesis based Transformer Diagnosis"}, {"figure_path": "chnJT8Nj8X/figures/figures_15_1.jpg", "caption": "Figure 7: Visualization comparison of integration weights a in MHSA. (a) and (b) correspond to high-confidence and low-confidence samples, respectively. (1) and (2) show the visualizations of integration weights a in blocks from shallow to deep from left to right, as well as the visualization of reshaped and resized rows of a superimposed onto the original image.", "description": "This figure compares the integration weights (a) in the Multi-Head Self-Attention (MHSA) module of a Transformer model for both high and low confidence image samples. It shows the visualization of these weights at different depths within the model.  The top row shows high confidence samples while the bottom shows low confidence samples. The left column for each shows the integration weights themselves at various depths. The right column shows the weights overlaid onto the original images for visualization.  The difference highlights how the model processes information differently depending on confidence in its prediction, with correct predictions (high-confidence) showing a consistent integration pattern and incorrect predictions showing inconsistent and erroneous patterns.", "section": "4.1 Inter-token Information Dynamic Integration"}, {"figure_path": "chnJT8Nj8X/figures/figures_16_1.jpg", "caption": "Figure 7: Visualization comparison of integration weights a in MHSA. (a) and (b) correspond to high-confidence and low-confidence samples, respectively. (1) and (2) show the visualizations of integration weights a in blocks from shallow to deep from left to right, as well as the visualization of reshaped and resized rows of a superimposed onto the original image.", "description": "This figure compares the integration weights (a) in the Multi-Head Self-Attention (MHSA) module of a Transformer model for high-confidence and low-confidence image samples.  It visualizes these weights at different depths within the model, showing how they change from shallow to deep layers.  The visualizations overlay the reshaped integration weights on the original images to show the spatial regions of focus.", "section": "4 Information Integration Hypothesis based Transformer Diagnosis"}, {"figure_path": "chnJT8Nj8X/figures/figures_17_1.jpg", "caption": "Figure 7: Visualization comparison of integration weights a in MHSA. (a) and (b) correspond to high-confidence and low-confidence samples, respectively. (1) and (2) show the visualizations of integration weights a in blocks from shallow to deep from left to right, as well as the visualization of reshaped and resized rows of a superimposed onto the original image.", "description": "This figure compares the integration weights (a) in the multi-head self-attention (MHSA) module of a Transformer model for high-confidence and low-confidence image samples.  It shows how the weights change across different depths (shallow, middle, deep) within the Transformer blocks. The visualization includes both a matrix representation of the weights and an overlay of reshaped and resized rows of the weights onto the original image to demonstrate their spatial relationship.", "section": "4 Information Integration Hypothesis based Transformer Diagnosis"}, {"figure_path": "chnJT8Nj8X/figures/figures_19_1.jpg", "caption": "Figure 2: Visual comparison of integration weights a in MHSA. (a) and (b) respectively present visualizations of weights a at different depths of blocks for high-confidence images and the overlay of reshaped and resized rows of a onto the original image. Similarly, (c) and (d) depict visualizations of weights a for low-confidence images and their overlay onto the original image.", "description": "This figure visually compares the integration weights (a) from the Multi-Head Self-Attention (MHSA) module of a Transformer model at different depths (layers) for both high-confidence and low-confidence image classifications.  Panels (a) and (b) show the weights for high-confidence images, with (b) overlaying a reshaped and resized representation of the weights onto the original image as a heatmap.  Panels (c) and (d) repeat this process for low-confidence images.  The visualizations aim to demonstrate the dynamic integration of information among tokens in MHSA as the model processes information at different depths.", "section": "4 Information Integration Hypothesis based Transformer Diagnosis"}, {"figure_path": "chnJT8Nj8X/figures/figures_19_2.jpg", "caption": "Figure 7: Visualization comparison of integration weights a in MHSA. (a) and (b) correspond to high-confidence and low-confidence samples, respectively. (1) and (2) show the visualizations of integration weights a in blocks from shallow to deep from left to right, as well as the visualization of reshaped and resized rows of a superimposed onto the original image.", "description": "This figure visually compares the integration weights (a) in the Multi-Head Self-Attention (MHSA) module of a Transformer model. It shows how these weights change across different depths (shallow to deep) for both high-confidence and low-confidence image samples. The visualization helps understand how information is integrated among tokens at various stages of the network, highlighting differences between correct and incorrect predictions.", "section": "4.1 Inter-token Information Dynamic Integration"}, {"figure_path": "chnJT8Nj8X/figures/figures_20_1.jpg", "caption": "Figure 1: The methodology framework of Transformer Doctor. It begins by analyzing the dynamic integration of inter-token information in MHSA and the static integration of intra-token information in FFN, Subsequently, conjunction errors within them are diagnosed, and finally treated to enhance model performance.", "description": "This figure illustrates the workflow of the Transformer Doctor framework. It starts by analyzing the dynamic information integration among tokens (inter-token) within the Multi-Head Self-Attention (MHSA) module and the static information integration within tokens (intra-token) in the Feed-Forward Network (FFN) module of a vision transformer.  The framework then diagnoses conjunction errors in both MHSA and FFN. Finally, it applies treatment methods (heuristic dynamic integration constraints for MHSA and rule-based static integration constraints for FFN) to correct these errors and improve model performance.", "section": "Information Integration Hypothesis based Transformer Diagnosis"}, {"figure_path": "chnJT8Nj8X/figures/figures_21_1.jpg", "caption": "Figure 1: The methodology framework of Transformer Doctor. It begins by analyzing the dynamic integration of inter-token information in MHSA and the static integration of intra-token information in FFN, Subsequently, conjunction errors within them are diagnosed, and finally treated to enhance model performance.", "description": "This figure illustrates the methodology of Transformer Doctor.  It's a flowchart showing the diagnostic and treatment steps for vision transformers.  First, the model analyzes dynamic integration of information between tokens (inter-token) within the Multi-Head Self-Attention (MHSA) module and static information integration within each token (intra-token) within the Feed-Forward Network (FFN).  Then, it diagnoses conjunction errors that occur during the integration of information. Finally, it applies methods to correct these errors, enhancing overall model performance.", "section": "Information Integration Hypothesis based Transformer Treatments"}]