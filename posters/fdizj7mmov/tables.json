[{"figure_path": "fDiZJ7mmOV/tables/tables_23_1.jpg", "caption": "Table 1: Comparison of methods, computational cost, and memory requirements", "description": "This table compares the computational cost and memory requirements for different methods discussed in the paper.  The methods include standard SGD, Soft Reset with various configurations (drift parameter per layer or per parameter, with or without proximal updates), and Bayesian Soft Reset Proximal (also with variations). The cost is expressed in Big O notation, showing how it scales with relevant factors (S= cost of SGD backward pass, K= number of updates, M= number of Monte Carlo samples, P=number of parameters, L= number of layers). This helps to understand the computational trade-offs between the different approaches.", "section": "E Computational complexity"}, {"figure_path": "fDiZJ7mmOV/tables/tables_23_2.jpg", "caption": "Table 1: Comparison of methods, computational cost, and memory requirements", "description": "This table compares the computational cost and memory requirements of different methods discussed in the paper, including SGD, Soft Resets with various configurations (gamma per layer, gamma per parameter, with and without proximal updates), and Bayesian Soft Reset Proximal.  The cost is expressed in big O notation, reflecting the dominant terms as the problem size increases.  The memory requirements describe the space complexity of each method.", "section": "E Computational complexity"}, {"figure_path": "fDiZJ7mmOV/tables/tables_24_1.jpg", "caption": "Table 1: Comparison of methods, computational cost, and memory requirements", "description": "This table compares the computational cost and memory requirements of different methods for non-stationary learning.  The methods include standard SGD, Soft Resets with different parameterizations (per layer and per parameter), Soft Resets with proximal updates (with different iterations), and Bayesian Soft Reset with proximal updates.  The computational cost is given in Big O notation, considering the number of parameters (P), layers (L), SGD backward passes (S), Monte Carlo samples for drift parameter (My), Monte Carlo samples for parameter updates (Me), number of updates for drift parameters (Ky), and number of NN parameter updates (Ke).  The memory requirements are also expressed in Big O notation.", "section": "E Computational complexity"}]