[{"figure_path": "fDiZJ7mmOV/figures/figures_2_1.jpg", "caption": "Figure 1: Left: graphical model for data generating process in the (a) stationary case and (b) non-stationary case with drift model p(\u03b8t+1|\u03b8t, Yt). Right: (c) In a stationary online learning regime, the Bayesian posterior (red dashed circles) in the long run will concentrate around \u03b8* (red dot). (d) In a non-stationary regime where the optimal parameters suddenly change from current value \u03b8t to new value \u03b8t+1 (blue dot) online Bayesian estimation can be less data efficient and take time to recover when the change-point occurs. (e) The use of p(\u03b8t+1|\u03b8t, Yt) and the estimation of Yt allows to increase the uncertainty, by soft resetting the posterior to make it closer to the prior (green dashed circle), so that the updated Bayesian posterior pt+1(\u03b8) (blue dashed circle) can faster track \u03b8t+1.", "description": "This figure visually explains the concept of soft parameter reset using a Bayesian inference example. The left side shows the graphical models for the data generating process under stationary (a) and non-stationary (b) assumptions. The right side illustrates how the Bayesian posterior evolves under different scenarios. In the stationary case (c), the posterior concentrates around the optimal parameters. In the non-stationary case without a dynamical model (d), the posterior adapts slowly to sudden changes in optimal parameters.  However, by incorporating a drift model (e), the posterior is softly reset, allowing for faster adaptation to changes.", "section": "3 Online non-stationary learning with learned parameter resets"}, {"figure_path": "fDiZJ7mmOV/figures/figures_7_1.jpg", "caption": "Figure 2: Plasticity benchmarks. Left: performance on permuted MNIST. Center: performance on random-label MNIST (data efficient). Right: performance on random-label CIFAR-10 (memorization). The x-axis is the task id and the y-axis is the per-task training accuracy (25).", "description": "This figure compares the performance of different continual learning methods (Online SGD, L2 init, Hard Reset, Shrink and Perturb, Soft Reset, and Bayesian Soft Reset) across three different plasticity benchmarks: permuted MNIST, data-efficient random-label MNIST, and memorization random-label CIFAR-10.  Each benchmark presents a unique challenge to continual learning, testing the ability of the methods to maintain performance across tasks.  The x-axis represents the task ID, and the y-axis shows the per-task training accuracy.  The results demonstrate that the Soft Reset and Bayesian Soft Reset methods significantly outperform the baselines in preserving performance across tasks, highlighting their ability to maintain plasticity.", "section": "5 Experiments"}, {"figure_path": "fDiZJ7mmOV/figures/figures_7_2.jpg", "caption": "Figure 2: Plasticity benchmarks. Left: performance on permuted MNIST. Center: performance on random-label MNIST (data efficient). Right: performance on random-label CIFAR-10 (memorization). The x-axis is the task id and the y-axis is the per-task training accuracy (25).", "description": "This figure compares the performance of different methods on three different plasticity benchmarks: permuted MNIST, data-efficient random-label MNIST, and memorization random-label CIFAR-10.  The x-axis represents the task ID, and the y-axis shows the average per-task training accuracy.  The results demonstrate the effectiveness of the proposed Soft Reset method in maintaining plasticity compared to other approaches, especially in the data-efficient and memorization settings.", "section": "5 Experiments"}, {"figure_path": "fDiZJ7mmOV/figures/figures_8_1.jpg", "caption": "Figure 4: Left: the minimum encountered \u03b3t for each layer on random-label MNIST and CIFAR-10. Center: the dynamics of \u03b3t on the first 20 tasks on MNIST. Right: the same on CIFAR-10.", "description": "This figure visualizes the behavior of the learned drift parameter \u03b3t. The left panel shows the minimum value of \u03b3t encountered for each layer across all tasks, separately for MNIST and CIFAR-10 datasets. The center and right panels display the dynamics of \u03b3t for the first 20 tasks on MNIST and CIFAR-10, respectively, focusing on the first layer.  The plots illustrate how \u03b3t changes over time and across different layers, providing insights into the adaptive nature of the soft reset mechanism in response to non-stationarity.", "section": "5 Experiments"}, {"figure_path": "fDiZJ7mmOV/figures/figures_9_1.jpg", "caption": "Figure 5: (a) the x-axis denotes the layer, the y-axis denotes the minimum encountered \u03b3t for each convolutional and fully-connected layer when trained on permuted Patches MNIST, color is the patch size. The impact of non-stationarity on performance on random-label MNIST of Online SGD and Hard Reset is shown in (b) while the one of Soft Resets is shown in (c). The x-axis denotes the number of epochs each task lasts, while the marker and line styles denote the percentage of random labels within each task, circle (solid) represents 20%, rectangle(dashed) 40%, while rhombus (dashed and dot) 60%. The y-axis denotes the average performance (over 3 seeds) on the stream of 200 tasks.", "description": "Figure 5 presents an analysis of non-stationarity effects. (a) shows how the minimum \u03b3t (minimum drift parameter value encountered) varies across layers in a permuted patch MNIST setup with different patch sizes. This reveals insights into how much the parameters drift across various network layers and with changing data non-stationarity levels (patch size). (b) plots the average task accuracy of Online SGD and Hard Reset across different numbers of epochs per task and varying random label percentages. It indicates how these methods handle varying levels of non-stationarity. (c) depicts the average task accuracy of Soft Reset methods (with and without Bayesian variants) across different epochs per task and random label percentages, showing their performance compared to baselines in non-stationary scenarios.", "section": "5 Experiments"}, {"figure_path": "fDiZJ7mmOV/figures/figures_9_2.jpg", "caption": "Figure 2: Plasticity benchmarks. Left: performance on permuted MNIST. Center: performance on random-label MNIST (data efficient). Right: performance on random-label CIFAR-10 (memorization). The x-axis is the task id and the y-axis is the per-task training accuracy (25).", "description": "This figure presents a comparison of different methods' performance on three plasticity benchmarks: permuted MNIST, data-efficient random-label MNIST, and memorization random-label CIFAR-10.  Each benchmark assesses the ability of a model to learn new tasks without forgetting previously learned ones, under different data conditions. The x-axis represents the task ID (indicating a sequence of tasks), while the y-axis shows the training accuracy achieved on each task.  The results demonstrate the relative performance of several continual learning methods including Soft Reset and its variants, compared to existing approaches such as Online SGD, Hard Reset, L2-Init and Shrink & Perturb.", "section": "5 Experiments"}, {"figure_path": "fDiZJ7mmOV/figures/figures_22_1.jpg", "caption": "Figure 2: Plasticity benchmarks. Left: performance on permuted MNIST. Center: performance on random-label MNIST (data efficient). Right: performance on random-label CIFAR-10 (memorization). The x-axis is the task id and the y-axis is the per-task training accuracy (25).", "description": "This figure presents the results of plasticity benchmarks on three different datasets: permuted MNIST, random-label MNIST (data-efficient), and random-label CIFAR-10 (memorization).  The results compare the performance of several continual learning methods, including the proposed Soft Reset algorithm and baselines like Online SGD, L2-init, Hard Reset, and Shrink and Perturb. For each dataset, the x-axis represents the task ID, and the y-axis displays the per-task training accuracy. The plots show the ability of each method to maintain plasticity (the ability to learn new tasks without forgetting previously learned tasks) in different non-stationary scenarios.", "section": "5 Experiments"}, {"figure_path": "fDiZJ7mmOV/figures/figures_22_2.jpg", "caption": "Figure 2: Plasticity benchmarks. Left: performance on permuted MNIST. Center: performance on random-label MNIST (data efficient). Right: performance on random-label CIFAR-10 (memorization). The x-axis is the task id and the y-axis is the per-task training accuracy (25).", "description": "This figure shows the results of plasticity benchmarks on three different datasets: permuted MNIST, random-label MNIST (data efficient), and random-label CIFAR-10 (memorization).  Each subfigure represents a different dataset and compares the performance of several methods, including Soft Reset and several baselines.  The x-axis represents the task ID, indicating the order in which the tasks were presented to the model, and the y-axis represents the per-task training accuracy.", "section": "5 Experiments"}, {"figure_path": "fDiZJ7mmOV/figures/figures_24_1.jpg", "caption": "Figure 2: Plasticity benchmarks. Left: performance on permuted MNIST. Center: performance on random-label MNIST (data efficient). Right: performance on random-label CIFAR-10 (memorization). The x-axis is the task id and the y-axis is the per-task training accuracy (25).", "description": "This figure compares the performance of different continual learning methods on three benchmark tasks: permuted MNIST, data-efficient random-label MNIST, and memorization random-label CIFAR-10.  Each task involves learning a new permutation of MNIST digits (left), a new random assignment of labels to MNIST digits (center - data efficient, meaning the model does not easily memorize), or learning to memorize new random labels assigned to CIFAR-10 images (right). The x-axis indicates the task ID, and the y-axis shows the training accuracy for each task. This illustrates the ability of the proposed Soft Reset method to maintain plasticity (the ability to learn new tasks without forgetting previous ones) compared to several baseline methods (Online SGD, L2 Init, Shrink and Perturb).", "section": "5 Experiments"}, {"figure_path": "fDiZJ7mmOV/figures/figures_25_1.jpg", "caption": "Figure 2: Plasticity benchmarks. Left: performance on permuted MNIST. Center: performance on random-label MNIST (data efficient). Right: performance on random-label CIFAR-10 (memorization). The x-axis is the task id and the y-axis is the per-task training accuracy (25).", "description": "This figure compares the performance of different continual learning methods (Online SGD, L2 init, Hard Reset, Shrink and Perturb, Soft Reset, and Bayesian Soft Reset) across three different benchmarks: permuted MNIST, data-efficient random-label MNIST, and memorization random-label CIFAR-10.  Each benchmark presents a unique challenge for continual learning, testing the algorithms' ability to retain previous knowledge while adapting to new tasks. The x-axis represents the task ID, indicating the sequence of tasks, while the y-axis shows the per-task accuracy. The results illustrate the effectiveness of the Soft Reset methods, particularly the Bayesian variant, in preserving plasticity and maintaining high accuracy across the tasks compared to standard continual learning algorithms. ", "section": "5 Experiments"}, {"figure_path": "fDiZJ7mmOV/figures/figures_25_2.jpg", "caption": "Figure 2: Plasticity benchmarks. Left: performance on permuted MNIST. Center: performance on random-label MNIST (data efficient). Right: performance on random-label CIFAR-10 (memorization). The x-axis is the task id and the y-axis is the per-task training accuracy (25).", "description": "This figure displays the results of plasticity benchmarks on three different datasets: permuted MNIST, random-label MNIST (data-efficient), and random-label CIFAR-10 (memorization).  Each subfigure shows the per-task training accuracy plotted against the task ID.  The results compare the performance of Soft Reset and Bayesian Soft Reset against several baseline methods (Online SGD, L2 Init, Shrink and Perturb, and Hard Reset), demonstrating the effectiveness of the proposed Soft Reset approaches in maintaining plasticity and preventing catastrophic forgetting.", "section": "5 Experiments"}, {"figure_path": "fDiZJ7mmOV/figures/figures_26_1.jpg", "caption": "Figure 2: Plasticity benchmarks. Left: performance on permuted MNIST. Center: performance on random-label MNIST (data efficient). Right: performance on random-label CIFAR-10 (memorization). The x-axis is the task id and the y-axis is the per-task training accuracy (25).", "description": "This figure compares the performance of different continual learning methods on three benchmark tasks.  The tasks assess the ability of the models to maintain plasticity (the ability to learn new tasks without forgetting previously learned ones) under different levels of non-stationarity.  The x-axis shows the task ID, and the y-axis represents the accuracy achieved on each task.  The left panel shows permuted MNIST, where the pixels of the images are randomly permuted for each task. The center panel shows random-label MNIST (data-efficient), where random labels are assigned to MNIST images.  The right panel shows random-label CIFAR-10 (memorization),  a more challenging task.  The results demonstrate that the proposed Soft Reset method achieves better performance compared to standard online SGD and other baselines (L2-init, Hard Reset, Shrink and Perturb) across all three tasks.", "section": "5 Experiments"}, {"figure_path": "fDiZJ7mmOV/figures/figures_26_2.jpg", "caption": "Figure 2: Plasticity benchmarks. Left: performance on permuted MNIST. Center: performance on random-label MNIST (data efficient). Right: performance on random-label CIFAR-10 (memorization). The x-axis is the task id and the y-axis is the per-task training accuracy (25).", "description": "This figure compares the performance of different continual learning methods on three benchmark tasks: permuted MNIST, data-efficient random label MNIST, and memorization random label CIFAR-10.  The x-axis represents the task ID, indicating the sequence of learning tasks. The y-axis shows the per-task training accuracy, providing a measure of how well each method maintains plasticity and avoids catastrophic forgetting as new tasks are introduced.  The results demonstrate the effectiveness of the proposed Soft Reset method in handling non-stationary data distributions, particularly when compared to traditional methods like Online SGD, L2-Init, Shrink and Perturb, and Hard Reset.", "section": "5 Experiments"}, {"figure_path": "fDiZJ7mmOV/figures/figures_26_3.jpg", "caption": "Figure 2: Plasticity benchmarks. Left: performance on permuted MNIST. Center: performance on random-label MNIST (data efficient). Right: performance on random-label CIFAR-10 (memorization). The x-axis is the task id and the y-axis is the per-task training accuracy (25).", "description": "This figure compares the performance of different continual learning methods on three benchmark datasets: permuted MNIST, data-efficient random-label MNIST, and memorization random-label CIFAR-10.  Each dataset presents a different challenge in terms of non-stationarity. The x-axis represents the task ID, indicating the progression through a sequence of learning tasks. The y-axis represents the per-task training accuracy, measuring the model's ability to learn and retain knowledge across tasks. The figure shows that Soft Reset consistently outperforms other methods across all three benchmarks, highlighting its effectiveness in maintaining plasticity (the ability to learn new tasks without forgetting previously learned ones).", "section": "5 Experiments"}, {"figure_path": "fDiZJ7mmOV/figures/figures_27_1.jpg", "caption": "Figure 2: Plasticity benchmarks. Left: performance on permuted MNIST. Center: performance on random-label MNIST (data efficient). Right: performance on random-label CIFAR-10 (memorization). The x-axis is the task id and the y-axis is the per-task training accuracy (25).", "description": "This figure compares the performance of different continual learning methods (Online SGD, L2 init, Hard Reset, Shrink and Perturb, Soft Reset, and Bayesian Soft Reset) on three different plasticity benchmarks: permuted MNIST, data-efficient random-label MNIST, and memorization random-label CIFAR-10.  The x-axis represents the task ID, and the y-axis shows the per-task training accuracy.  The results demonstrate the effectiveness of Soft Reset and Bayesian Soft Reset in maintaining plasticity, especially compared to methods that employ hard resets.", "section": "5 Experiments"}, {"figure_path": "fDiZJ7mmOV/figures/figures_27_2.jpg", "caption": "Figure 16: Perfect soft-resets on data-efficient random-label MNIST. Left, Soft Reset method does not use higher learning rate when \u03b3 < 1. Right, Soft Reset increases the learning rate when \u03b3 < 1, see (18). The x-axis represents task id, whereas the y-axis is the average training accuracy on the task.", "description": "This figure compares the performance of different variants of Soft Reset on data-efficient random-label MNIST.  The left panel shows Soft Reset with a constant learning rate, while the right panel shows Soft Reset with a higher learning rate at task switches (when \u03b3 < 1).  The results demonstrate the impact of increasing the learning rate at the task boundaries for improving the learning efficiency and plasticity. The plot shows that Soft Reset with a higher learning rate outperforms the baselines, and it shows the impact of different values of gamma (\u03b3) on plasticity in this setting.", "section": "H.1 Perfect Soft Resets"}, {"figure_path": "fDiZJ7mmOV/figures/figures_28_1.jpg", "caption": "Figure 2: Plasticity benchmarks. Left: performance on permuted MNIST. Center: performance on random-label MNIST (data efficient). Right: performance on random-label CIFAR-10 (memorization). The x-axis is the task id and the y-axis is the per-task training accuracy (25).", "description": "This figure shows the results of plasticity benchmarks on three different datasets: permuted MNIST, random-label MNIST (data efficient), and random-label CIFAR-10 (memorization).  Each subfigure represents a different dataset and compares the performance of several methods, including Soft Reset and baselines such as Online SGD, L2-init, Hard Reset, and Shrink and Perturb. The x-axis represents the task ID, indicating the sequence of tasks in the continual learning setting. The y-axis shows the per-task training accuracy, which is a measure of how well each method performs on each individual task.  The figure demonstrates the Soft Reset method's ability to maintain plasticity (the ability to learn new tasks without forgetting previously learned tasks) across the different datasets and task sequences compared to the baselines.", "section": "5 Experiments"}, {"figure_path": "fDiZJ7mmOV/figures/figures_28_2.jpg", "caption": "Figure 2: Plasticity benchmarks. Left: performance on permuted MNIST. Center: performance on random-label MNIST (data efficient). Right: performance on random-label CIFAR-10 (memorization). The x-axis is the task id and the y-axis is the per-task training accuracy (25).", "description": "This figure compares the performance of different methods (Online SGD, L2 init, Hard Reset, Shrink and Perturb, Soft Reset, and Bayesian Soft Reset) on three different plasticity benchmarks: permuted MNIST, data-efficient random-label MNIST, and memorization random-label CIFAR-10.  The x-axis represents the task ID (a sequence of learning tasks), and the y-axis shows the per-task training accuracy. The results demonstrate the effectiveness of the proposed Soft Reset method in maintaining plasticity, especially when compared to traditional methods like Online SGD and Hard Reset.", "section": "5 Experiments"}, {"figure_path": "fDiZJ7mmOV/figures/figures_28_3.jpg", "caption": "Figure 2: Plasticity benchmarks. Left: performance on permuted MNIST. Center: performance on random-label MNIST (data efficient). Right: performance on random-label CIFAR-10 (memorization). The x-axis is the task id and the y-axis is the per-task training accuracy (25).", "description": "This figure compares the performance of different methods (Online SGD, L2 Init, Hard Reset, Shrink and Perturb, Soft Reset, and Bayesian Soft Reset) on three different plasticity benchmarks: permuted MNIST, data-efficient random-label MNIST, and memorization random-label CIFAR-10.  Each benchmark presents a unique challenge in continual learning due to varying degrees of data distribution shift and task similarity. The x-axis represents the task ID, while the y-axis shows the per-task training accuracy. The results demonstrate that the proposed Soft Reset methods, especially the Bayesian Soft Reset, significantly improve performance compared to standard methods on these challenging benchmarks. ", "section": "5 Experiments"}, {"figure_path": "fDiZJ7mmOV/figures/figures_29_1.jpg", "caption": "Figure 2: Plasticity benchmarks. Left: performance on permuted MNIST. Center: performance on random-label MNIST (data efficient). Right: performance on random-label CIFAR-10 (memorization). The x-axis is the task id and the y-axis is the per-task training accuracy (25).", "description": "This figure displays the results of plasticity benchmarks on three different datasets: permuted MNIST, random-label MNIST (data-efficient setting), and random-label CIFAR-10 (memorization setting).  The x-axis represents the task ID, indicating the sequence of tasks. The y-axis shows the per-task training accuracy.  Several methods, including Online SGD, L2 Init, Shrink and Perturb, Hard Reset, Soft Reset, and Bayesian Soft Reset, are compared to evaluate their ability to maintain plasticity (the ability to learn new tasks without forgetting previous ones) under non-stationary conditions.  Each dataset and setting presents a different challenge, and the results highlight the relative performance of each method.", "section": "5 Experiments"}, {"figure_path": "fDiZJ7mmOV/figures/figures_29_2.jpg", "caption": "Figure 2: Plasticity benchmarks. Left: performance on permuted MNIST. Center: performance on random-label MNIST (data efficient). Right: performance on random-label CIFAR-10 (memorization). The x-axis is the task id and the y-axis is the per-task training accuracy (25).", "description": "This figure compares the performance of different continual learning methods (Online SGD, L2 init, Hard Reset, Shrink and Perturb, Soft Reset, and Bayesian Soft Reset) on three different plasticity benchmarks.  Each benchmark involves a sequence of learning tasks where the data distribution changes between tasks. The left panel shows the performance on the permuted MNIST dataset, the center panel shows performance on a data-efficient version of random-label MNIST, and the right panel shows performance on a memorization version of random-label CIFAR-10.  The x-axis shows the task ID, indicating the order in which tasks were presented. The y-axis shows the per-task training accuracy, representing the model's performance on each task.  The figure demonstrates the effectiveness of the Soft Reset methods in maintaining plasticity across multiple tasks, outperforming baseline methods in preventing catastrophic forgetting.", "section": "5 Experiments"}, {"figure_path": "fDiZJ7mmOV/figures/figures_29_3.jpg", "caption": "Figure 1: Left: graphical model for data generating process in the (a) stationary case and (b) non-stationary case with drift model p(\u03b8t+1|\u03b8t, Yt). Right: (c) In a stationary online learning regime, the Bayesian posterior (red dashed circles) in the long run will concentrate around \u03b8* (red dot). (d) In a non-stationary regime where the optimal parameters suddenly change from current value \u03b8 to new value \u03b8t+1 (blue dot) online Bayesian estimation can be less data efficient and take time to recover when the change-point occurs. (e) The use of p(\u03b8|\u03b8t, \u03b3t) and the estimation of \u03b3t allows to increase the uncertainty, by soft resetting the posterior to make it closer to the prior (green dashed circle), so that the updated Bayesian posterior pt+1(\u03b8) (blue dashed circle) can faster track \u03b8t+1.", "description": "This figure illustrates the data generating process in both stationary and non-stationary scenarios, highlighting the benefits of the proposed drift model in handling non-stationarity. The left side shows the graphical models for both cases, comparing i.i.d and non-i.i.d assumptions.  The right side provides a visual representation of Bayesian posterior updates under these scenarios and how the drift model improves efficiency and speed in adapting to changes.", "section": "3 Online non-stationary learning with learned parameter resets"}, {"figure_path": "fDiZJ7mmOV/figures/figures_30_1.jpg", "caption": "Figure 23: Impact of specific initialization \u03b80 as a mean of po(\u03b8) in Soft Resets. The x-axis represents task id. The y-axis represents the average task accuracy with standard deviation computed over 3 random seeds. The task is random label MNIST - data efficient.", "description": "This figure shows the impact of using a specific initialization (\u03b80) in the prior distribution (p0(\u03b8)) for the Soft Reset algorithm. Two variants of Soft Reset are compared: one where the prior's mean is set to a specific initialization (\u03b80), and another where the prior's mean is set to 0. The y-axis shows the average task accuracy across multiple tasks, with error bars representing the standard deviation across 3 random seeds. The results show similar performance for both variants, suggesting that the choice of initialization in the prior may not be as crucial for the Soft Reset algorithm's performance.", "section": "H.6 Impact of specific initialization"}, {"figure_path": "fDiZJ7mmOV/figures/figures_35_1.jpg", "caption": "Figure 24: Non-stationary mean tracking with SGD.", "description": "This figure shows a comparison of different SGD approaches for tracking a non-stationary mean in a toy problem.  The true mean switches between -2 and 2 every 50 timesteps. The figure compares standard SGD with two different learning rates (0.05 and 0.15) against SGD methods that include parameter resets at the switch points with different reset learning rates. The results illustrate how parameter resets with appropriate learning rate scheduling can significantly improve adaptation to non-stationarity, allowing for faster convergence to the new mean compared to standard SGD.", "section": "K Toy illustrative example for SGD underperformance in the non-stationary regime"}]