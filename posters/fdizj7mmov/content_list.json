[{"type": "text", "text": "Non-Stationary Learning of Neural Networks with Automatic Soft Parameter Reset ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Alexandre Galashov\u2217   \nUCL Gatsby   \nGoogle DeepMind   \nagalashov@google.com ", "page_idx": 0}, {"type": "text", "text": "Michalis K. Titsias Google DeepMind mtitsias@google.com ", "page_idx": 0}, {"type": "text", "text": "Andr\u00e1s Gy\u00f6rgy Google DeepMind agyorgy@google.com ", "page_idx": 0}, {"type": "text", "text": "Clare Lyle Google DeepMind clarelyle@google.com ", "page_idx": 0}, {"type": "text", "text": "Razvan Pascanu Google DeepMind razp@google.com ", "page_idx": 0}, {"type": "text", "text": "Yee Whye Teh Google DeepMind University of Oxford ywteh@google.com ", "page_idx": 0}, {"type": "text", "text": "Maneesh Sahani UCL Gatsby maneesh@gatsby.ucl.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Neural networks are traditionally trained under the assumption that data come from a stationary distribution. However, settings which violate this assumption are becoming more popular; examples include supervised learning under distributional shifts, reinforcement learning, continual learning and non-stationary contextual bandits. In this work we introduce a novel learning approach that automatically models and adapts to non-stationarity, via an Ornstein-Uhlenbeck process with an adaptive drift parameter. The adaptive drift tends to draw the parameters towards the initialisation distribution, so the approach can be understood as a form of soft parameter reset. We show empirically that our approach performs well in non-stationary supervised and off-policy reinforcement learning settings. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Neural networks (NNs) are typically trained using algorithms like stochastic gradient descent (SGD), assuming data comes from a stationary distribution. This assumption fails in scenarios such as continual learning, reinforcement learning, non-stationary contextual bandits, and supervised learning with distribution shifts [20, 53]. A phenomenon occurring in non-stationary settings is the loss of plasticity [12, 2, 13], manifesting either as a failure to generalize to new data despite reduced training loss [4, 2], or as an inability to reduce training error as the data distribution changes [13, 37, 1, 42, 34]. ", "page_idx": 0}, {"type": "text", "text": "In [38], the authors argue for two factors that lead to the loss of plasticity: preactivation distribution shift, leading to dead or dormant neurons [47], and parameter norm growth causing training instabilities. To address these issues, strategies often involve hard resets based on heuristics like detecting dormant units [47], assessing neuron utility [13, 12], or simply after a fixed number of steps [43]. Though effective at increasing plasticity, hard resets can be inefficient as they can discard valuable knowledge captured by the parameters. ", "page_idx": 0}, {"type": "text", "text": "We propose an algorithm that implements a mechanism of soft parameter resets, in contrast to the hard resets discussed earlier. A soft reset partially moves NN parameters towards the initialization while keeping them close to their previous values. It also increases learning rate of the learning algorithm, allowing new NN parameters to adapt faster to the changing data. The amount by which the parameters move towards the initialization and the amount of learning rate increase are controlled by the drift parameters which are learned online. The exact implementation of soft reset mechanism is based on the use of a drift model in NN parameters update before observing new data. Similar ideas which modify the starting point of SGD as well as increase the learning rate of SGD depending on non-stationarity were explored (see [21, 29]) in an online convex optimization setting. Specifically, in [21], the authors assume that the optimal parameter of SGD changes according to some dynamical model out of a finite family of models. They propose an algorithm to identify this model and a way to leverage this model in a modified SGD algorithm. Compared to these works, we operate in a general non-convex setting. Proposed drift model can be thought as a dynamical Bayesian prior over Neural Network parameters, which is adapted online to new data. We make a specific choice of drift model which implements soft resets mechanism. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Our contributions can be summarized as follows. First, we propose an explicit model for the drift in NN parameters and describe the procedure to estimate the parameters of this model online from the stream of data. Second, we describe how the estimated drift model is incorporated in the learning algorithm. Third, we empirically demonstrate the effectiveness of this approach in preventing the loss of plasticity as well as in an off-policy reinforcement learning setting. ", "page_idx": 1}, {"type": "text", "text": "2 Non-stationary learning with Online SGD ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In a non-stationary learning setting with changing data distributions $p_{t}(x,y)$ , where $x\\in\\mathbb{R}^{L},y\\in\\mathbb{R}^{K}$ , we define the loss function for parameters $\\boldsymbol{\\theta}\\in\\breve{\\mathbb{R}}^{D}$ as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathcal{L}_{t}(\\theta)=\\mathbb{E}_{(x_{t},y_{t})\\sim p_{t}}\\mathcal{L}_{t}(\\theta,x_{t},y_{t})\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Our goal is to find a parameter sequence $\\Theta=(\\theta_{1},\\dots,\\theta_{T})$ that minimizes the dynamic regret: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R_{T}(\\Theta,\\Theta^{\\star})=\\frac{1}{T}\\sum_{t=1}^{T}\\left(\\mathcal{L}_{t}(\\theta_{t})-\\mathcal{L}_{t}(\\theta_{t}^{\\star})\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "with a reference sequence $\\Theta^{\\star}=(\\theta_{1}^{\\star},\\cdot\\cdot\\cdot,\\theta_{T}^{\\star})$ , satisfying $\\theta_{t}^{\\star}=\\arg\\operatorname*{min}_{\\theta}\\mathcal{L}_{t}(\\theta)$ . A common approach to the online learning problem is online stochastic gradient descent (SGD) [23]. Starting from initial parameters $\\theta_{0}$ , the method updates these parameters sequentially for each batch of data $\\widetilde{\\{(x_{t}^{i},y_{t}^{i})\\}_{i=1}^{B}}$ s.t. $(x_{t}^{i},y_{t}^{i})\\sim p_{t}(x_{t},y_{t})$ . The update rule is: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\theta_{t+1}=\\theta_{t}-\\alpha_{t}\\nabla_{\\theta}\\mathcal{L}_{t+1}(\\theta_{t}),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\begin{array}{r}{\\nabla_{\\theta}\\mathcal{L}_{t+1}(\\theta_{t})=\\frac{1}{B}\\sum_{i=1}^{B}\\nabla_{\\theta}\\mathcal{L}_{t+1}(\\theta_{t},x_{t}^{i},y_{t}^{i})}\\end{array}$ and $\\alpha_{t}$ is learning rate. See also Appendix $\\mathrm{G}$ for the connection of SGD to proximal optimization. ", "page_idx": 1}, {"type": "text", "text": "Convex Setting. In the convex setting, online SGD with a fixed learning rate $\\alpha$ can handle nonstationarity [56]. By selecting $\\alpha$ appropriately \u2013 potentially using additional knowledge about the reference sequence\u2014we can optimize the dynamic regret in (2). In general, algorithms that adapt to the observed level of non-stationarity can outperform standard online SGD. For example, in [29], the authors propose to adjust the learning rate $\\alpha_{t}$ , while in [21] and in [29], the authors suggest modifying the starting point of SGD from $\\theta_{t}$ to an adjusted $\\theta_{t}^{\\prime}$ proportional to the level of non-stationarity. ", "page_idx": 1}, {"type": "text", "text": "Non-Convex Setting. Non-stationary learning with NNs is more complex, since now there is a changing set of local minima as the data distribution changes. Such changes can lead to a loss of plasticity and other pathologies. Alternative optimization methods like Adam [30], do not fully resolve this issue [13, 37, 1, 42, 34]. Parameter resets [13, 48, 12] partially mitigate the problem, but could be too aggressive if the data distributions are similar. ", "page_idx": 1}, {"type": "text", "text": "3 Online non-stationary learning with learned parameter resets ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Notation. We denote by ${\\mathcal{N}}(\\theta;\\mu,\\sigma^{2})$ a Gaussian distribution on $\\theta$ with mean $\\mu$ and variance $\\sigma^{2}$ . We denote $\\theta^{i}$ the $i$ -the component of the vector $\\theta\\,=\\,(\\theta^{1},\\ldots,\\theta^{D})$ . Unless explicitly mentioned, we assume distributions are defined per NN parameter and we omit the index $i$ . We denote as $\\mathcal{L}_{t+1}(\\theta)=-\\log p(y_{t+1}\\vert x_{t+1},\\theta)$ the negative log likelihood on $(y_{t+1},x_{t+1})$ for parameters $\\theta$ . ", "page_idx": 1}, {"type": "text", "text": "We introduce Soft Resets, an approach that enhances learning algorithms on non-stationary data distributions and prevents plasticity loss. The main idea is to assume that the data is generated in non-i.i.d. fashion such that a change in the data distribution is modeled by the drift in the parameters $p(\\theta_{t+1}|\\theta_{t},\\gamma_{t})$ at every time $t+1$ before new data is observed. We assume a class of drift models $p(\\theta_{t+1}|\\theta_{t},\\gamma_{t})$ which encourages the parameters to move closer to the initialization. The amount of drift (and level of non-stationarity) is controlled by $\\gamma_{t}$ which are estimated online from the data. ", "page_idx": 1}, {"type": "image", "img_path": "fDiZJ7mmOV/tmp/59b58308c461ca4fd9dcdf68916ab5ca6688d0e49a8e43cb2bca1b84e7dc576c.jpg", "img_caption": ["Figure 1: Left: graphical model for data generating process in the (a) stationary case and (b) nonstationary case with drift model $p(\\theta_{t+1}|\\theta_{t},\\gamma_{t})$ . Right: (c) In a stationary online learning regime, the Bayesian posterior (red dashed circles) in the long run will concentrate around $\\theta^{*}$ (red dot). (d) In a non-stationary regime where the optimal parameters suddenly change from current value $\\theta_{t}^{*}$ to new value $\\theta_{t+1}^{*}$ (blue dot) online Bayesian estimation can be less data efficient and take time to recover when the change-point occurs. (e) The use of $p(\\theta|\\theta_{t},\\gamma_{t})$ and the estimation of $\\gamma_{t}$ allows to increase the uncertainty, by soft resetting the posterior to make it closer to the prior (green dashed circle), so that the updated Bayesian posterior $p_{t+1}(\\theta)$ (blue dashed circle) can faster track $\\theta_{t+1}^{*}$ . "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "As can be seen below, in the context of SGD, this approach adjusts the starting point $\\theta_{t}$ of the update to a point $\\tilde{\\theta}_{t}(\\gamma_{t})$ , which is closer to the initialization and increases the learning rate proportionally to the drift. In the context of Bayesian inference, this approach shrinks the mean of the estimated posterior towards the prior and increases the variance proportional to $\\gamma_{t}$ . This approach is inspired by prior work in online convex optimization for non-stationary environments [e.g., 25, 21, 8, 18, 29]. ", "page_idx": 2}, {"type": "text", "text": "3.1 Toy illustration of the advantage of drift models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Consider online Bayesian inference with 2-D observations $y_{t}=\\theta^{\\star}+\\epsilon_{t}$ , where $\\theta^{\\star}\\;\\in\\;\\mathbb{R}^{2}$ are unknown true parameters and $\\epsilon_{t}\\sim\\mathcal{N}(0;\\sigma^{2}I)$ is Gaussian noise with variance $\\sigma^{2}$ . Starting from a Gaussian prior $p_{0}(\\theta)\\,=\\,\\mathcal{N}(\\theta;\\mu_{0};\\Sigma_{0})$ , the posterior distribution $p_{t+1}(\\theta)\\,=\\,p(\\theta|y_{1},\\dots,y_{t})\\,=$ $\\mathcal{N}(\\theta;\\mu_{t+1},\\Sigma_{t+1})$ is updated using Bayes\u2019 rule ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{t+1}(\\theta)\\sim p(y_{t+1}|\\theta)p_{t}(\\theta).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The posterior update (3) comes from the i.i.d. assumption on the data generation process (Figure 1a), since $\\begin{array}{r}{p_{t+1}(\\theta)\\stackrel{}{\\sim}p_{0}(\\theta)\\prod_{s=1}^{t+1}p(y_{s}|\\theta)}\\end{array}$ . By the Central Limit Theorem (CLT), the posterior mean $\\mu_{t}$ converges to $\\theta^{\\star}$ and the covariance matrix $\\Sigma_{t}$ shrinks to zero (the radius of red circle in Figure 1c). ", "page_idx": 2}, {"type": "text", "text": "Suppose now that the true parameters $\\theta_{t}^{\\star}$ (kept fixed before $t$ ) change to new parameters $\\theta_{t+1}^{\\star}$ at time $t+1$ . The i.i.d. assumption (Figure 1a) is violated and the update (3) becomes problematic because the low uncertainty (small radius of red dashed circle in Figure 1d) in $p_{t}(\\theta)$ causes the posterior $p_{t+1}(\\theta)$ (see blue circle) to adjust slowly towards $\\theta_{t+1}^{\\star}$ (blue dot) as illustrated in Figure 1d. ", "page_idx": 2}, {"type": "text", "text": "To address this issue, we assume that before observing new data, the parameters drift according to $p(\\theta_{t+1}|\\theta_{t},\\gamma_{t})$ where the amount of drift is controlled by $\\gamma_{t}$ . The corresponding conditional independence structure is shown in Figure 1b. The posterior update then becomes: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{t+1}(\\theta)\\sim p(y_{t+1}|\\theta)\\int p(\\theta|\\theta_{t}^{\\prime},\\gamma_{t})p_{t}(\\theta_{t}^{\\prime})d\\theta_{t}^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For a suitable choice of drift model $p(\\theta_{t+1}|\\theta_{t},\\gamma_{t})$ , this modification allows $p_{t+1}(\\theta)$ (blue circle) to adjust more rapidly towards the new $\\theta_{t+1}^{\\star}$ (blue dot), see Figure 1e. This is because the new prior $\\begin{array}{r}{\\int p(\\theta|\\theta_{t}^{\\prime},\\gamma_{t})p_{t}(\\theta_{t}^{\\prime})d\\theta_{t}^{\\prime}}\\end{array}$ has larger variance (green circle) than $p_{t}(\\theta)$ and its mean is closer to the center of the circle. Ideally, the parameter $\\gamma_{t}$ should capture the underlying non-stationarity in the data distribution in order to control the impact of the prior $\\begin{array}{r}{\\int p(\\theta|\\theta_{t}^{\\prime},\\dot{\\gamma}_{t})\\bar{p}_{t}(\\theta_{t}^{\\prime})d\\theta_{t}^{\\prime}}\\end{array}$ . For example, if at some point the non-stationarity disappears, we want the drift model to exhibit no-drift to recover the posterior update (3). This highlights the importance of the adaptive nature of the drift model. ", "page_idx": 2}, {"type": "text", "text": "3.2 Ornstein-Uhlenbeck parameter drift model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We motivate the specific choice of the drift model which is useful for maintaining plasticity. We assume that our Neural Network has enough capacity to learn any stationary dataset in a fixed number of iterations starting from a good initialization $\\theta_{0}\\sim p_{0}(\\theta)$ [see, e.g., 24, 16]. Informally, we call the initialization $\\theta_{0}$ plastic and the region around $\\theta_{0}$ a plastic region. ", "page_idx": 3}, {"type": "text", "text": "Consider now a piecewise stationary datastream that switches between a distribution $p_{a}$ , with a set of local minima $\\mathcal{M}_{a}$ of the negative likelihood ${\\mathcal{L}}(\\theta)$ , to a distribution $p_{b}$ at time $t+1$ , with a set of local minima $M_{b}$ . If $M_{b}$ is far from $M_{a}$ , then hard reset might be beneficial, but if $M_{b}$ is close to $M_{a}$ , resetting parameters is suboptimal. Furthermore, since $\\theta$ is high-dimensional, different dimensions might need to be treated differently. We want a drift model that can capture all of these scenarios. ", "page_idx": 3}, {"type": "text", "text": "Drift model. The drift model $p(\\theta_{t+1}|\\theta_{t},\\gamma_{t})$ which exhibits the above properties is given by ", "page_idx": 3}, {"type": "equation", "text": "$$\np(\\theta|\\theta_{t},\\gamma_{t})=\\mathcal{N}(\\theta;\\gamma_{t}\\theta_{t}+(1-\\gamma_{t})\\mu_{0};(1-\\gamma_{t}^{2})\\sigma_{0}^{2}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which is separately defined for every parameter dimension $\\theta^{i}$ where $p_{0}(\\theta_{0}^{i})\\sim\\mathcal{N}(\\theta_{0}^{i};\\mu_{0}^{i};\\left[\\sigma_{0}^{i}\\right]^{2})$ is the per-parameter prior distribution and $\\gamma_{t}=(\\gamma_{t}^{1},\\ldots,\\gamma_{t}^{D})$ . The model is a discretized OrnsteinUhlenbeck (OU) process [50] (see Appendix A for the derivation). ", "page_idx": 3}, {"type": "text", "text": "The parameter $\\gamma_{t}\\in[0,1]$ is a drift parameter and controls the amount of non-stationarity in each parameter. For $\\gamma_{t}=1$ , there is no drift and for $\\gamma_{t}=0$ , the drift model reverts the parameters back to the prior. A value of $\\gamma_{t}\\in(0,1)$ interpolates between these two extremities. A remarkable property of (5) is that starting from the current parameter $\\theta_{t}$ , if we simulate a long trajectory, as $T\\rightarrow\\infty$ , the distribution of $p(\\boldsymbol{\\theta}_{T}|\\boldsymbol{\\theta}_{t})$ will converge to the prior $p(\\ensuremath{\\boldsymbol{\\theta}}_{0})$ . This is only satisfied (for $\\gamma_{t}\\in(0,1))$ ) due to the variance $\\sigma_{0}^{2}(1-\\gamma_{t}^{2})$ . Replacing it by an arbitrary variance $\\sigma^{2}$ would result in the variance of $p(\\boldsymbol{\\theta}_{T}|\\boldsymbol{\\theta}_{t})$ either going to 0 or growing to $\\infty$ , harming learning. Thus, the model (5) encourages parameters to move towards plastic region (initialization). In Appendix B, we discuss this further and other potential choices for the drift model. ", "page_idx": 3}, {"type": "text", "text": "3.3 Online estimation of drift model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The drift model $p(\\theta_{t+1}|\\theta_{t},\\gamma_{t})$ quantifies prior belief about the change in parameters before seeing new data. A suitable choice of an objective to select $\\gamma_{t}$ is predictive likelihood which quantifies the probability of new data under our current parameters and drift model. From Bayesian perspective, it means selecting the prior distribution which explains the future data the best. ", "page_idx": 3}, {"type": "text", "text": "We derive the drift estimation procedure in the context of approximate online variational inference [7] with Bayesian Neural Networks (BNN). Let $\\Gamma_{t}\\;=\\;(\\gamma_{1},\\ldots,\\gamma_{t})$ be the history of observed parameters of the drift model and ${\\cal S}_{t}~=~\\{(x_{1},y_{1}),\\dots,(x_{t},y_{t})\\}$ be the history of observed data. The objective of approximate online variational inference is to propagate an approximate posterior $\\bar{q}_{t}(\\theta|S_{t},\\Gamma_{t-1}\\bar{)}$ over parameters, such that it is constrained to some family $\\mathcal{Q}$ of probability distributions. In the context of BNNs, it is typical [5] to assume a family $\\begin{array}{r}{\\mathcal{Q}=\\dot{\\{q(\\theta):q(\\theta)\\sim\\prod_{i=1}^{D}\\!N(\\theta^{i};\\mu^{i},\\left[\\sigma^{i}\\right]^{2});\\theta=(\\theta^{1},\\dots,\\theta^{D})\\}}}\\end{array}$ of Gaussian mean-field distributions over parameters $\\boldsymbol\\theta\\in\\mathbb{R}^{D}$ (separate Gaussian per parameter). For simplicity of notation, we omit the index $i$ . Let $q_{t}(\\theta)\\triangleq q_{t}(\\theta|S_{t},\\Gamma_{t-1})\\in\\mathcal{Q}$ be the Gaussian approximate posterior at time $t$ with mean $\\mu_{t}$ and variance $\\sigma_{t}^{2}$ for every parameter. The new approximate posterior $q_{t+1}(\\theta)\\in\\mathcal{Q}$ is found by ", "page_idx": 3}, {"type": "equation", "text": "$$\nq_{t+1}(\\theta)=\\arg\\operatorname*{min}_{q}\\mathbb{K L}\\left[q(\\theta)||p(y_{t+1}|x_{t+1},\\theta)q_{t}(\\theta|\\gamma_{t})\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the prior term is the approximate predictive look-ahead prior given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q_{t}(\\theta|\\gamma_{t})=\\int q_{t}(\\theta_{t})p(\\theta|\\theta_{t},\\gamma_{t})d\\theta_{t}=\\mathcal{N}(\\theta;\\tilde{\\mu}_{t}(\\gamma_{t}),\\tilde{\\sigma}_{t}^{2}(\\gamma_{t}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "that has parameters $\\tilde{\\mu}_{t}(\\gamma_{t})=\\gamma_{t}\\mu_{t}+(1-\\gamma_{t})\\mu_{0},\\tilde{\\sigma}_{t}^{2}(\\gamma_{t})=\\gamma_{t}^{2}\\sigma_{t}^{2}+(1-\\gamma_{t}^{2})\\sigma_{0}^{2},$ , see Appendix I.1 for derivation. The form of this prior $q_{t}(\\theta|\\gamma_{t})$ comes from the non i.i.d. assumption (see Figure 1b) and the form of the drift model (5). For new batch of data $(x_{t+1},y_{t+1})$ at time $t+1$ , the approximate predictive log-likelihood equals to ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log q_{t}(y_{t+1}|x_{t+1},\\gamma_{t})=\\log\\int p(y_{t+1}|x_{t+1},\\theta)q_{t}(\\theta|\\gamma_{t})d\\theta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The log-likelihood (8) allows us to quantify predictions on new data $(x_{t+1},y_{t+1})$ given our current distribution $q_{t}(\\theta)$ and the drift model from (5). We want to find such $\\gamma_{t}^{\\star}$ that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\gamma_{t}^{\\star}\\approx\\arg\\operatorname*{max}_{\\gamma_{t}}\\log q_{t}(y_{t+1}|x_{t+1},\\gamma_{t})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Using $\\gamma_{t}^{\\star}$ in (5) modifies the prior distribution (7) to fit the most recent observations the best by putting more mass on the region where the new parameter could be found (see Figure 1,right). ", "page_idx": 4}, {"type": "text", "text": "Gradient-based optimization for $\\gamma_{t}$ . The approximate predictive prior in (7) is Gaussian which allows us to use the so-called reparameterisation trick to optimize (8) via gradient descent. Starting from an initial value of drift parameter $\\gamma_{t}^{0}$ at time $t$ , we perform $K$ updates with learning rate $\\eta_{\\gamma}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\gamma_{t,k+1}=\\gamma_{t,k}+\\eta_{\\gamma}\\nabla_{\\gamma}\\log\\int p(y_{t+1}|x_{t+1},\\tilde{\\mu}_{t}(\\gamma_{t,k})+\\epsilon\\tilde{\\sigma}_{t}(\\gamma_{t,k}))\\mathcal{N}(\\epsilon;0,I)d\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The integral is evaluated by Monte-Carlo (MC) using $M$ samples $\\epsilon_{i}\\sim\\mathcal{N}(\\epsilon;0,I),i=1,\\ldots,M$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\int p(y_{t+1}|x_{t+1},\\tilde{\\mu}_{t}(\\gamma_{t,k})+\\epsilon\\tilde{\\sigma}_{t}(\\gamma_{t,k}))\\mathcal{N}(\\epsilon;0,I)d\\epsilon\\approx\\frac{1}{M}\\sum_{i=1}^{M}p(y_{t+1}|x_{t+1},\\tilde{\\mu}_{t}(\\gamma_{t,k})+\\epsilon_{i}\\tilde{\\sigma}_{t}(\\gamma_{t,k})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Inductive bias in the drift model is captured by $\\gamma_{t}^{0}$ , where $\\gamma_{t,0}=1$ encourages stationarity, while $\\gamma_{t,0}=\\gamma_{t-1,K}$ promotes temporal smoothness. In practice, we found $\\gamma_{t,0}=1$ was the most effective. ", "page_idx": 4}, {"type": "text", "text": "Structure in the drift model. The drift model can be defined to be shared across different subsets of parameters which reduces the expressivity of the drift model but also provides regularization to (10). We consider $\\gamma_{t}$ to be either defined for each parameter or for each layer. See Section 5 for details as well as corresponding results in Appendix $_\\mathrm{H}$ . ", "page_idx": 4}, {"type": "text", "text": "Interpretation of $\\gamma_{t}$ . By linearising $\\log p(y_{t+1}|x_{t+1},\\theta)$ around $\\mu_{t}$ , we can compute (8) in a closed form and get the following loss for $\\gamma_{t}$ (see Appendix $\\textbf{J}$ for the proof) optimizing (9) ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{F}(\\gamma_{t})=0.5(\\sigma_{t}^{2}(\\gamma_{t})\\odot g_{t+1})^{\\top}g_{t+1}-(\\gamma_{t}\\odot\\mu_{t}+(1-\\gamma_{t})\\odot\\mu_{0})^{\\top}g_{t+1},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $g_{t}=\\nabla\\mathcal{L}_{t+1}{(\\mu_{t})}$ and $\\mathcal{L}_{t+1}(\\theta)=-\\log p(y_{t+1}|x_{t+1},\\mu_{t})$ , and where $\\odot$ denotes element-wise product performed only over parameters for which $\\gamma_{t}$ is shared (see paragraph about structure in drift model). The transpose operation is also defined on a subset of parameters for which $\\gamma_{t}$ is shared. Adding the $\\ell_{2}$ penalty $\\textstyle{\\frac{1}{2}}\\lambda\\rceil|\\gamma_{t}-\\gamma_{t}^{0}||^{2}$ encoding the starting point $\\dot{\\gamma}_{t}^{0}$ , gives us the closed form for $\\gamma_{t}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\gamma_{t}=\\frac{\\left(\\mu_{t}-\\mu_{0}\\right)^{T}g_{t+1}+\\lambda\\gamma_{t}^{0}}{\\left(\\left(\\sigma_{0}^{2}-\\sigma_{t}^{2}\\right)\\cdot g_{t+1}\\right)^{T}g_{t+1}+\\lambda},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where we also clip parameters $\\gamma_{t}$ to $[0,1]$ . The expression (12) gives us the geometric interpretation for $\\gamma_{t}$ . The value of $\\gamma_{t}$ depends on the angle between $\\left(\\mu_{t}-\\mu_{0}\\right)$ and $g_{t+1}$ When these vectors are aligned, $\\gamma_{t}$ is high and is low otherwise. When these vectors are orthogonal or the gradient $g_{t+1}\\approx0$ the value of $\\gamma_{t}$ is heavily influenced by $\\gamma_{t}^{0}$ . Moreover, when $g_{t+1}\\approx0$ , we can interpret it as being close to a local minimum, i.e., stationary, which means that we want $\\gamma_{t}\\approx1$ , therefore adding the $\\ell_{2}$ penalty is important. Also, when the norm of the gradients $g_{t+1}$ is high, the value of $\\gamma_{t}$ is encouraged to decrease, introducing the drift. This means that using $\\gamma_{t}$ in the parameter update (see Section 3.5) encourages the norm of the gradient to stay small. In practice, we found that update (12) was unstable suggesting that linearization of the log-likelihood might not be a good approximation for learning $\\gamma_{t}$ ", "page_idx": 4}, {"type": "text", "text": "3.4 Approximate Bayesian update of posterior $q_{t}(\\theta)$ with BNNs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The optimization problem (6) for the per-parameter Gaussian $q(\\theta)=\\mathcal{N}(\\theta;\\mu,\\sigma^{2})$ with Gaussian prior $q_{t}(\\theta)\\dot{=}\\mathcal{N}(\\theta;\\mu_{t},\\dot{\\sigma}_{t}^{2})$ , both defined for every parameter of NN, can be written (see Appendix I.1) to minimize the following loss ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\mathcal{F}}_{t}(\\mu,\\sigma,\\gamma_{t})=\\mathbb{E}_{\\epsilon\\sim N(0;I)}\\left[\\mathcal{L}_{t+1}(\\mu+\\epsilon\\sigma)\\right]+\\sum_{i=1}^{D}\\lambda_{t}^{i}\\left[\\frac{(\\mu^{i}-\\tilde{\\mu}_{t}^{i}(\\gamma_{t}))^{2}+\\left[\\sigma^{i}\\right]^{2}}{2\\left[\\tilde{\\sigma}_{t}^{i}(\\gamma_{t})\\right]^{2}}-\\frac{1}{2}\\log\\left[\\sigma^{i}\\right]^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda_{t}^{i}\\,>\\,0$ are per-parameter temperature coefficients. The use of small temperature $\\lambda\\,>\\,0$ parameter (shared for all NN parameters) was shown to improve empirical performance of Bayesian Neural Networks [54]. Given that in (13), the variance $\\tilde{\\sigma}_{t}^{2}(\\gamma_{t})$ can be small, in order to control the strength of the regularization, we propose to use per parameter temperature $\\lambda_{t}^{i}=\\lambda\\times\\left[\\sigma_{t}^{i}\\right]^{2}$ , where $\\lambda>0$ is a global constant. This leads to the following objective ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\tau}_{t}(\\mu,\\sigma,\\gamma_{t})=\\mathbb{E}_{\\epsilon\\sim N(0;I)}\\left[\\mathcal{L}_{t+1}(\\mu+\\epsilon\\sigma)\\right]+\\frac{\\lambda}{2}\\sum_{i}r_{t}^{i}\\left[(\\mu_{i}-\\tilde{\\mu}_{t}^{i}(\\gamma_{t}))^{2}+[\\sigma^{i}]^{2}-\\left[\\tilde{\\sigma}_{t}^{i}(\\gamma_{t})\\right]^{2}\\log[\\sigma^{i}]^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the quantity $r_{t}^{i}=[\\sigma_{t}^{i}]^{2}/[\\sigma_{t}^{i}(\\gamma_{t})]^{2}$ is a relative change in the posterior variance due to the drift. The ratio $r_{t}^{i}=1$ when $\\gamma_{t}=1$ . For $\\gamma_{t}<1$ since typically $\\bar{\\sigma}_{t}^{2}<\\sigma_{0}^{2}$ , the ratio is $r_{t}^{i}<1$ . Thus, as long as there is non-stationarity $(\\gamma_{t}<1)$ , the objective (14) favors the data term $\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0;I)}\\left[\\mathcal{L}_{t+1}(\\mu+\\epsilon\\sigma)\\right]$ ", "page_idx": 4}, {"type": "text", "text": "Input: Data-stream ${\\cal S}_{T}=\\{(x_{t},y_{t})\\})_{t=1}^{T}$   \nNeural Network (NN) initializing distribution $p_{i n i t}(\\theta)$ and specific initialization $\\theta_{0}\\sim p_{i n i t}(\\theta)$   \nLearning rate $\\alpha_{t}$ for parameters and $\\eta_{\\gamma}$ for drift parameters   \nNumber of gradient updates $K_{\\gamma}$ on drift parameter $\\gamma_{t}$   \nNN initial standard deviation (STD) scaling $p\\leq1$ (see (23)) and ratio $\\begin{array}{r}{s=\\frac{\\sigma_{t}}{p\\sigma_{0}}}\\end{array}$ .   \nfor step $t=0,1,2,\\ldots,T$ do For $(x_{t+1},y_{t+1})$ , predict $\\hat{y}_{t+1}=f(x_{t+1}|\\theta_{t})$ Compute performance metric based on $(y_{t+1},\\hat{y}_{t+1})$ Initialize drift parameter $\\gamma_{t,0}=1$ for step $k=0,1,2,\\ldots,K_{\\gamma}\\;\\cdot$ do Sample $\\theta_{0}^{\\prime}\\sim p_{i n i t}(\\theta)$ Stochastic update (21) on drift parameter using specific initialization (24) $\\gamma_{t,k+1}=\\gamma_{t,k}\\dot{\\gamma}_{\\gamma}\\nabla_{\\gamma}\\left[\\log p(y_{t+1}|x_{t+1},\\gamma_{t}\\theta_{t}+(1-\\gamma_{t})\\theta_{0}+\\theta_{0}^{\\prime}p\\sqrt{1-\\gamma_{t}^{2}+\\gamma_{t}^{2}s^{2}})\\right]_{\\gamma_{t}=\\gamma_{t,k}},$ end for Get $\\tilde{\\theta}_{t}(\\gamma_{t,K})$ with (17) and $\\tilde{\\alpha}_{t}(\\gamma_{t,K})$ with (18) Update parameters $\\theta_{t+1}=\\tilde{\\theta}_{t}(\\gamma_{t,K})-\\tilde{\\alpha}_{t}(\\gamma_{t,K})\\circ\\nabla_{\\theta}\\mathcal{L}_{t+1}(\\tilde{\\theta}_{t}(\\gamma_{t,K}))$   \nend for ", "page_idx": 5}, {"type": "text", "text": "allowing the optimization to respond faster to changes in the data distribution. To find new parameters, let $\\mu_{t+1,0}=\\tilde{\\mu}_{t}(\\gamma_{t})$ and $\\sigma_{t+1,0}=\\tilde{\\sigma}_{t}(\\gamma_{t})$ , and perform updates $K$ on (14) ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu_{t+1,k+1}=\\mu_{t+1,k}-\\alpha_{\\mu}\\hat{F}_{t}(\\mu_{t+1,k},\\sigma_{t+1,k},\\gamma_{t}),\\;\\;\\sigma_{t+1,k+1}=\\sigma_{t+1,k}-\\alpha_{\\sigma}\\hat{F}_{t}(\\mu_{t+1,k},\\sigma_{t+1,k},\\gamma_{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\alpha_{\\mu}$ and $\\alpha_{\\sigma}$ are learning rates for the mean and for the standard deviation correspondingly. All derivations are provided in Appendix I.1. The full procedure is described in Algorithm 2. ", "page_idx": 5}, {"type": "text", "text": "3.5 Fast MAP update of posterior $q_{t}(\\theta)$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As a faster alternative to propagating the posterior (6), we do MAP updates with the prior $p_{0}(\\theta)=$ $\\mathcal{N}(\\theta;\\mu_{0};\\sigma_{0}^{2})$ and the approximate posterior $q_{t}(\\theta)\\;=\\;{\\mathcal N}(\\theta;\\theta_{t};\\sigma_{t}^{2}\\;=\\;s^{2}\\sigma_{0}^{2})$ , where $s~\\leq~1$ is a hyperparameter controlling the variance $\\sigma_{t}^{2}$ of $q_{t}(\\theta)$ . Since a fixed $s$ may not capture the true parameters variance, using a Bayesian method (see Section 3.4) is preferred but comes at a high computational cost (see Appendix $\\boldsymbol{\\mathrm E}$ for discussion). The MAP update is given by (see Appendix I.2 for derivations) finding a minimum of the following proximal objective ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{G(\\theta)=\\mathcal{L}_{t+1}(\\theta)+\\frac{1}{2}\\sum_{i=1}^{D}\\frac{|\\theta^{i}-\\tilde{\\theta}_{t}^{i}(\\gamma_{t})|^{2}}{\\tilde{\\alpha}_{t}^{i}(\\gamma_{t})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the regularization target for the parameter dimension $i$ is given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{\\theta}_{t}^{i}(\\gamma_{t})=\\gamma_{t}^{i}\\theta_{t}^{i}+(1-\\gamma_{t}^{i})\\mu_{0}^{i}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and the per-parameter learning rate is given as (assuming that $\\alpha_{t}$ the base SGD learning rate) ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\alpha}_{t}^{i}(\\gamma_{t})=\\alpha_{t}\\left((\\gamma_{t}^{i})^{2}+\\frac{1-(\\gamma_{t}^{i})^{2}}{s^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Linearising $\\mathcal{L}_{t+1}(\\theta)$ around $\\tilde{{\\boldsymbol{\\theta}}}_{t}(\\gamma_{t})$ and optimizing (16) for $\\theta$ leads to (see Appendix I.2) ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\theta_{t+1}=\\tilde{\\theta}_{t}(\\gamma_{t})-\\tilde{\\alpha}_{t}(\\gamma_{t})\\circ\\nabla_{\\theta}\\mathcal{L}_{t+1}\\big(\\tilde{\\theta}_{t}(\\gamma_{t})\\big),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\circ$ is element-wise multiplication. For $\\gamma_{t}=1$ , we recover the ordinary SGD update, while the values $\\gamma_{t}~<~1$ move the starting point of the modified SGD closer to the initialization as well as increase the learning rate. Algorithm 1 describes the full procedure. In Appendix C we describe additional practical choices made for the Soft Resets algorithm. Similarly to the Bayesian approach (15), we can do multiple updates on (16). We describe this Soft Resets Proximal algorithm in Appendix I.2 and full procedure is given in Algorithm 3. ", "page_idx": 5}, {"type": "text", "text": "4 Related Work ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Plasticity loss in Neural Networks. Our model shares similarities with reset-based approaches such as Shrink & Perturb (S&P) [2] and L2-Init [33]; however, whereas we learn drift parameters from data, these methods do not, leaving them vulnerable to mismatch between assumed non-stationarity and the actual realized non-stationarity in the data. Continual Backprop [13] or ReDO [47] apply resets in a data-dependent fashion, e.g. either based on utility or whether units are dead. But they use hard resets, and cannot amortize the cost of removing entire features. Interpretation (12) of $\\gamma_{t}$ connects to the notion of parameters utility from [14], but this quantity is used to prevent catastrophic forgetting by decreasing learning rate for high $\\gamma_{t}$ . Our method increases the learning rate for low $\\gamma_{t}$ to maximize adaptability, and is not designed to prevent catastrophic forgetting. ", "page_idx": 6}, {"type": "text", "text": "Non-stationarity. Non-stationarity arises naturally in a variety of contexts, the most obvious being continual and reinforcement learning. The structure of non-stationarity may vary from problem to problem. At one extreme, we have a piece-wise stationary setting, for example a change in the location of a camera generating a stream of images, or a hard update to the learner\u2019s target network in value-based deep RL algorithms. This setting has been studied extensively due to its propensity to induce catastrophic forgetting [e.g. 31, 45, 51, 10] and plasticity loss [13, 39, 38, 34]. At the other extreme, we can consider more gradual changes, for example due to improvements in the policy of an RL agent [40, 46, 42, 13] or shifts in the data generating process [36, 55, 20, 53]. Further, these scenarios might be combined, for example in continual reinforcement learning [31, 1, 13] where the reward function or transition dynamics could change over time. ", "page_idx": 6}, {"type": "text", "text": "Non-stationary online convex optimization. Non-stationary prediction has a long history in online convex optimization, where several algorithms have been developed to adapt to changing data [see, e.g., 25, 8, 22, 17, 21, 18, 29]. Our approach takes an inspiration from these works by employing a drift model as, e.g., [25, 21] and by changing learning rate as [29, 52]. Further, our OU drift model bears many similarities to the implicit drift model introduced in the update rule of [25] (see also [8, 17]), where the predictive distribution is mixed with a uniform distribution to ensure the prediction could change quickly enough if the data changes significantly, where in our case $p_{0}$ plays the same role as the uniform distribution. ", "page_idx": 6}, {"type": "text", "text": "Bayesian approaches to non-stationary learning. A standard approach is Variational Continual Learning [41], which focuses on preventing catastrophic forgetting and is an online version of \u201cBayes By Backprop\u201d [5]. This method does not incorporate dynamical parameter drift components. In [35], the authors applied variational inference (VI) on non-stationary data, using the OU-process and Bayesian forgetting, but unlike in our approach, their drift parameter is not learned. Further, in [49], the authors considered an OU parameter drift model similar to ours, with an adaptable drift scalar $\\gamma$ and analytic Kalman filter updates, but is applied over the final layer weights only, while the remaining weights of the network were estimated by online SGD. In [28], the authors propose to deal with non-stationarity by assuming that each parameter is a finite sum of random variables following different OU process. They derive VI updates on the posterior of these variables. Compared to this work, we learn drift parameters for every NN parameter rather than assuming a finite set of drift parameters. A different line of research assumes that the drift model is known and use different techniques to estimate the hidden state (the parameters) from the data: in [9], the authors use Extended Kalman Filter to estimate state and in [3], they propagate the MAP estimate of the hidden state distribution with $K$ gradient updates on a proximal objective similar to (43), whereas in Bayesian Online Natural Gradient (BONG) [27], the authors use natural gradient for the variational parameters. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Soft reset methods. There are multiple variations of our method. We call the method implemented by Algorithm 1 with 1 gradient update on the drift parameter Soft Reset, while other versions show different parameter choices: Soft Reset $K_{\\gamma}=10\\$ ) is a version with 10 updates on the drift parameter, while Soft Reset $X_{\\gamma}=10$ , $K_{\\theta}=10)$ ) is the method of Algorithm 3 in Appendix I.2 with 10 updates on drift parameter, followed by 10 updates on NN parameters. Bayesian Soft Reset $\\langle K_{\\gamma}\\,=\\,10$ , $K_{\\theta}=10)$ ) is a method implemented by Algorithm 2 with 10 updates on drift parameter followed by 10 updates on the mean $\\mu_{t}$ and the variance $\\sigma_{t}^{2}$ (uncertainty) for each NN parameter. Bayesian method performed the best overall but required higher computational complexity (see Appendix E). Unless specified, $\\gamma_{t}$ is shared for all the parameters in each layer (separately for weight and biases). ", "page_idx": 6}, {"type": "image", "img_path": "fDiZJ7mmOV/tmp/32ddd914365b19ccfef3098fc8969cb54ebded5394e5a28ad909bcdc78c2ad55.jpg", "img_caption": ["Figure 2: Plasticity benchmarks. Left: performance on permuted MNIST. Center: performance on random-label MNIST (data efficient). Right: performance on random-label CIFAR-10 (memorization). The x-axis is the task id and the y-axis is the per-task training accuracy (25). "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "fDiZJ7mmOV/tmp/653a6253dacc0221b45093823f168a10df946bd485e8f0a801344873da64e0b6.jpg", "img_caption": ["Figure 3: Different variants of Soft Resets. Left: performance on permuted MNIST. Center: performance on random-label MNIST (data efficient). Right: performance on random-label CIFAR$I O$ (memorization). The $\\mathbf{X}$ -axis is the task id and the y-axis is the per-task training accuracy (25). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Loss of plasticity. We analyze the performance of our method on plasticity benchmarks [34, 39, 38]. Here, we have a sequence of tasks, where each task consists of a fixed (for all tasks) subset of 10000 images images from either CIFAR-10 [32] or MNIST, where either pixels are permuted or the label for each image is randomly chosen. Several papers [34, 39, 38] study a memorization random-label setting where $S G D$ can perfectly learn each task from scratch. To highlight the data-efficiency of our approach, we study the data-efficient setting where $S G D$ achieves only $50\\%$ accuracy on each task when trained from scratch. Here, we expect that algorithms taking into account similarity in the data, to perform better. To study the impact of the non-stationarity of the input data, we consider permuted MNIST where pixels are randomly permuted within each task (the same task as considered by 34). As baselines, we use Online $S G D$ and Hard Reset at task boundaries. We also consider $L2$ init [34], which adds $L2$ penalty $||\\theta-\\theta_{0}||^{2}$ to the fixed initialization $\\theta_{0}$ as well as Shrink&Perturb [2], which multiplies each parameter by a scalar $\\lambda\\leq1$ and adds random Gaussian noise with fixed variance $\\sigma$ . See Appendix D.1 for all details. As metrics, we use average per-task online accuracy (25), which is ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{A}_{t}=\\frac{1}{N}\\sum_{i=1}^{N}a_{i}^{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $a_{i}^{t}$ are the online accuracies collected on the task $t$ via $N$ timesteps, corresponding to the duration of the task. In Figure 5, we also use average accuracy over all $T$ tasks, i.e. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{A}_{T}=\\frac{1}{T}\\sum_{t=1}^{T}\\mathcal{A}_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The results are provided in Figure 2. We observe that Soft Reset is always better than Hard Reset and most baselines despite the lack of knowledge of task boundaries. The gap is larger in the data efficient regime. Moreover, we see that $L2$ Init only performs well in the memorization regime, and achieves comparable performance to Hard Reset in the data efficient one. The method $L2$ Init could be viewed as an instantiation of our Soft Reset Proximal method optimizing (16) with $\\gamma_{t}=0$ at every step, which is sub-optimal when there is similarity in the data. Bayesian Soft Reset demonstrates significantly better performance overall, see also discussion below. ", "page_idx": 7}, {"type": "text", "text": "In Figure 3, we compare different variants of Soft Reset. We observe that adding more compute for estimating $\\gamma_{t}$ (thus, estimating non-stationarity, $K_{\\gamma}=10$ ) as well as doing more updates on NN parameters (thus, more accurately adapting to non-staionarity, $K_{\\theta}=10)$ ) leads to better performance. All variants of Soft Reset $\\gamma_{t}$ parameters are shared for each NN layer, except for the Bayesian method. This variant is able to take advantage of a more complex per-parameter drift model, while other variants performed considerably worse, see Appendix H.4. We hypothesize this is due to the NN parameters uncertainty estimates $\\sigma_{t}$ which Bayesian method provide, while others do not, which leads to a more accurate drift model estimation, since uncertainty is used in this update (10). But, this approach comes at a higher computational cost, see Appendix E. In Appendix H, we provide ablations of the structure of the drift model, as well as of the impact of learning the drift parameter. ", "page_idx": 7}, {"type": "image", "img_path": "fDiZJ7mmOV/tmp/87d7a25d6877ae6552da73b4fe0b47d3a1a86e76dba89d37193b08970437187b.jpg", "img_caption": ["Figure 4: Left: the minimum encountered $\\gamma_{t}$ for each layer on random-label MNIST and CIFAR-10. Center: the dynamics of $\\gamma_{t}$ on the first 20 tasks on MNIST. Right: the same on CIFAR-10. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Qualitative behavior of Soft Resets. For Soft Reset, we track the values of $\\gamma_{t}$ for the first MLP layer when trained on random-label tasks studied above (only 20 tasks), as well as the minimum encountered value of $\\gamma_{t}$ for each layer, which highlights the maximum amount of resets. Figure 4b,c shows $\\gamma_{t}$ as a function of $t$ , and suggests that $\\gamma_{t}$ aggressively decreases at task boundaries (red dashed lines). The range of values of $\\gamma_{t}$ depends on the task and on the layer, see Figure 4a. Overall, $\\gamma_{t}$ changes more aggressively for long duration (memorization) random-label CIFAR-10 and less for shorter (data-efficient) random-label MNIST. See Appendix H.2 for more detailed results. ", "page_idx": 8}, {"type": "text", "text": "To study the behavior of Soft Reset under input distribution non-stationarity, we consider a variant of Permuted MNIST where each image is partitioned into patches of a given size. The non-stationarity is controlled by permuting the patches (not pixels). Figure 5a shows the minimum encountered $\\gamma_{t}$ for each layer for different patch sizes. As the patch size increases and the problem becomes more stationary, the range of values for $\\gamma_{t}$ is less aggressive. See Appendix H.3 for more detailed results. ", "page_idx": 8}, {"type": "text", "text": "Impact of non-stationarity. We consider a variant of random-label MNIST where for each task, an image has either a random or a true label. The label assignment is kept fixed throughout the task and is changed at task boundaries. We consider cases of $\\bar{20\\%}$ , $40\\%$ and $60\\%$ of random labels and we control the duration of each task (number of epochs). In total, the stream contains 200 tasks. In Figure 5b, we show performance of Online SGD, Hard Reset and in Figure 5c, the one of Soft Reset and of Bayesian Soft Reset. See Appendix D.2 for more details. The results suggest that for the shortest duration of the tasks, the performance of all the methods is similar. As we increase the duration of each of the task (moving along the $\\mathbf{X}_{\\mathrm{}}$ -axis), we see that both Soft Resets variants perform better than SGD and the gap widens as the duration increases. This implies that Soft Resets is more effective with infrequent data distribution changes. We also observe that Bayesian method performs better in all the cases, highlighting the importance of estimating uncertainty for NN parameters. ", "page_idx": 8}, {"type": "text", "text": "5.1 Reinforcement learning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Reinforcement learning experiments. We conduct Reinforcement Learning (RL) experiments in the highly off-policy regime, similarly to [43], since in this setting loss of plasticity was observed. We ran SAC [19] agent with default parameters from Brax [15] on the Hopper-v5 and Humanoid-v4 GYM [6] environments (from Brax [15]). To reproduce the setting from [43], we control the off-policyness of the agent by setting the off-policy ratio $M$ such that for every 128 environment steps, we do $128M$ gradient steps with batch size of 256 on the replay buffer. As baselines we consider ordinary SAC, hard-coded Hard Reset where we reset all the parameters $K=5$ times throughout training (every 200000 steps), while keeping the replay buffer fixed (similarly to [43]). We employ our Soft Reset method as follows. After we have collected fresh data from the environment, we do one gradient update on $\\gamma_{t}$ (shared for all the parameters within each layer) with batch size of 128 on this new chunk of data and the previously collected one, i.e., two chunks of data in total. Then we initialize $\\tilde{\\theta}_{t}(\\gamma_{t})$ and we employ the update rule (43) where the regularization $\\tilde{\\theta}_{t}(\\gamma_{t})$ is kept constant for all the off-policy gradient updates on the replay buffer. See Appendix D.3 for more details. ", "page_idx": 8}, {"type": "image", "img_path": "fDiZJ7mmOV/tmp/ec735402ad3c450a8dd867a11854d495e0ebedc0fb3ac964611f9c648959119b.jpg", "img_caption": ["Figure 5: (a) the $\\mathbf{X}$ -axis denotes the layer, the y-axis denotes the minimum encountered $\\gamma_{t}$ for each convolutional and fully-connected layer when trained on permuted Patches MNIST, color is the patch size. The impact of non-stationarity on performance on random-label MNIST of Online SGD and Hard Reset is shown in ${\\bf(b)}$ while the one of Soft Resets is shown in (c). The $\\mathbf{X}$ -axis denotes the number of epochs each task lasts, while the marker and line styles denote the percentage of random labels within each task, circle (solid) represents $20\\%$ , rectangle(dashed) $40\\%$ , while rhombus (dashed and dot) $60\\%$ . The y-axis denotes the average performance (over 3 seeds) on the stream of 200 tasks. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "fDiZJ7mmOV/tmp/25f4443f150d0ac6c710762124818cc28aadd9cd834553dc67c88e5bafb1343c.jpg", "img_caption": ["Figure 6: RL results. First row is humanoid, second is hopper. Each column corresponds to different replay ratio. The $\\mathbf{X}$ -axis is the number of total timesteps, the y-axis the average reward. The shaded area denotes the standard deviation across 3 random seeds and the solid line indicates the mean. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "The results are given in Figure 6. As the off-policy ratio increases, Soft Reset becomes more efficient than the baselines. This is consistent with our finding in Figure 5b,c, where we showed that the performance of Soft Reset is better when the data distribution is not changing fast. Figure 8 in Appendix D.3 shows the value of learned $\\gamma_{t}$ . It shows $\\gamma_{t}$ mostly change for the value function and not for the policy indicating that the main source of non-stationarity comes from the value function. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Learning efficiently on non-stationary distributions is critical to a number of applications of deep neural networks, most prominently in reinforcement learning. In this paper, we have proposed a new method, Soft Resets, which improves the robustness of stochastic gradient descent to nonstationarities in the data-generating distribution by modeling the drift in Neural Network (NN) parameters. The proposed drift model implements soft reset mechanism where the amount of reset is controlled by the drift parameter $\\gamma_{t}$ . We showed that we could learn this drift parameter from the data and therefore we could learn when and how far to reset each Neural Network parameter. We incorporate the drift model in the learning algorithm which improves learning in scenarios with plasticity loss. The variant of our method which models uncertainty in the parameters achieves the best performance on plasticity benchmarks so far, highlighting the promise of the Bayesian approach. Furthermore, we found that our approach is particularly effective either on data distributions with a lot of similarity or on slowly changing distributions. Our findings open the door to a variety of exciting directions for future work, such as investigating the connection to continual learning and deepening our theoretical analysis of the proposed approach. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Zaheer Abbas, Rosie Zhao, Joseph Modayil, Adam White, and Marlos C. Machado. Loss of plasticity in continual deep reinforcement learning, 2023.   \n[2] Jordan T. Ash and Ryan P. Adams. On warm-starting neural network training, 2020.   \n[3] Gianluca M. Bencomo, Jake C. Snell, and Thomas L. Grifftihs. Implicit maximum a posteriori filtering via adaptive optimization, 2023.   \n[4] Tudor Berariu, Wojciech Czarnecki, Soham De, Jorg Bornschein, Samuel Smith, Razvan Pascanu, and Claudia Clopath. A study on the plasticity of neural networks. arXiv preprint arXiv:2106.00042, 2021.   \n[5] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 1613\u20131622, Lille, France, 07\u201309 Jul 2015. PMLR.   \n[6] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.   \n[7] Tamara Broderick, Nicholas Boyd, Andre Wibisono, Ashia C Wilson, and Michael I Jordan. Streaming variational bayes. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013.   \n[8] N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press, Cambridge, 2006.   \n[9] Peter G. Chang, Gerardo Dur\u00e1n-Mart\u00edn, Alexander Y Shestopaloff, Matt Jones, and Kevin Murphy. Low-rank extended kalman filtering for online learning of neural networks from streaming data, 2023.   \n[10] Zhiyuan Chen and Bing Liu. Lifelong machine learning. Synthesis Lectures on Artificial Intelligence and Machine Learning, 12(3):1\u2013207, 2018.   \n[11] Hugh Dance and Brooks Paige. Fast and scalable spike and slab variable selection in highdimensional gaussian processes, 2022.   \n[12] Shibhansh Dohare, J. Fernando Hernandez-Garcia, Qingfeng Lan, Parash Rahman, Ashique Rupam Mahmood, and Richard S. Sutton. Loss of plasticity in deep continual learning. Nature, 632:768 \u2013 774, 2024.   \n[13] Shibhansh Dohare, Richard S. Sutton, and A. Rupam Mahmood. Continual backprop: Stochastic gradient descent with persistent randomness, 2022.   \n[14] Mohamed Elsayed and A. Rupam Mahmood. Addressing loss of plasticity and catastrophic forgetting in continual learning, 2024.   \n[15] C. Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, and Olivier Bachem. Brax - a differentiable physics engine for large scale rigid body simulation, 2021. http://github.com/google/brax.   \n[16] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Yee Whye Teh and Mike Titterington, editors, Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, volume 9 of Proceedings of Machine Learning Research, pages 249\u2013256, Chia Laguna Resort, Sardinia, Italy, 13\u201315 May 2010. PMLR.   \n[17] A. Gy\u00f6rgy, T. Linder, and G. Lugosi. Efficient tracking of large classes of experts. IEEE Transactions on Information Theory, IT-58(11):6709\u20136725, Nov. 2012.   \n[18] Andr\u00e1s Gy\u00f6rgy and Csaba Szepesv\u00e1ri. Shifting regret, mirror descent, and matrices. In Proceedings of The 33rd International Conference on Machine Learning, pages 2943\u20132951, 2016.   \n[19] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor, 2018.   \n[20] Raia Hadsell, Dushyant Rao, Andrei A. Rusu, and Razvan Pascanu. Embracing change: Continual learning in deep neural networks. Trends in Cognitive Sciences, 24(12):1028\u20131040, 2020.   \n[21] Eric Hall and Rebecca Willett. Dynamical models and tracking regret in online convex programming. In International Conference on Machine Learning, pages 579\u2013587. PMLR, 2013.   \n[22] E. Hazan and C. Seshadhri. Efficient learning algorithms for changing environments. In Proc. 26th Annual International Conference on Machine Learning, pages 393\u2013400. ACM, 2009.   \n[23] Elad Hazan. Introduction to online convex optimization, 2023.   \n[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification, 2015.   \n[25] M. Herbster and M. K. Warmuth. Tracking the best expert. Machine Learning, 32(2):151\u2013178, 1998.   \n[26] Hemant Ishwaran and J. Sunil Rao. Spike and slab variable selection: Frequentist and bayesian strategies. The Annals of Statistics, 33(2), April 2005.   \n[27] Matt Jones, Peter Chang, and Kevin Murphy. Bayesian online natural gradient (bong), 2024.   \n[28] Matt Jones, Tyler R. Scott, and Michael Curtis Mozer. Human-like learning in temporally structured environments. In AAAI Spring Symposia, 2024.   \n[29] Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar. Adaptive gradient-based metalearning methods, 2019.   \n[30] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.   \n[31] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521\u20133526, March 2017.   \n[32] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009. https://www.cs.toronto.edu/\\~kriz/ learning-features-2009-TR.pdf.   \n[33] Aviral Kumar, Rishabh Agarwal, Dibya Ghosh, and Sergey Levine. Implicit underparameterization inhibits data-efficient deep reinforcement learning, 2021.   \n[34] Saurabh Kumar, Henrik Marklund, and Benjamin Van Roy. Maintaining plasticity in continual learning via regenerative regularization, 2023.   \n[35] Richard Kurle, Botond Cseke, Alexej Klushyn, Patrick van der Smagt, and Stephan G\u00fcnnemann. Continual learning with bayesian neural networks for non-stationary data. In International Conference on Learning Representations, 2020.   \n[36] Zhiqiu Lin, Jia Shi, Deepak Pathak, and Deva Ramanan. The clear benchmark: Continual learning on real-world imagery. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021.   \n[37] Clare Lyle, Mark Rowland, and Will Dabney. Understanding and preventing capacity loss in reinforcement learning, 2022.   \n[38] Clare Lyle, Zeyu Zheng, Khimya Khetarpal, Hado van Hasselt, Razvan Pascanu, James Martens, and Will Dabney. Disentangling the causes of plasticity loss in neural networks, 2024.   \n[39] Clare Lyle, Zeyu Zheng, Evgenii Nikishin, Bernardo Avila Pires, Razvan Pascanu, and Will Dabney. Understanding plasticity in neural networks, 2023.   \n[40] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning, 2013.   \n[41] Cuong V. Nguyen, Yingzhen Li, Thang D. Bui, and Richard E. Turner. Variational continual learning. In International Conference on Learning Representations, 2018.   \n[42] Evgenii Nikishin, Junhyuk Oh, Georg Ostrovski, Clare Lyle, Razvan Pascanu, Will Dabney, and Andr\u00e9 Barreto. Deep reinforcement learning with plasticity injection, 2023.   \n[43] Evgenii Nikishin, Max Schwarzer, Pierluca D\u2019Oro, Pierre-Luc Bacon, and Aaron Courville. The primacy bias in deep reinforcement learning, 2022.   \n[44] Neal Parikh and Stephen Boyd. Proximal algorithms. Found. Trends Optim., 1(3):127\u2013239, jan 2014.   \n[45] German I. Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan, and Stefan Wermter. Continual lifelong learning with neural networks: A review. Neural Networks, 113:54\u201371, 2019.   \n[46] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017.   \n[47] Ghada Sokar, Rishabh Agarwal, Pablo Samuel Castro, and Utku Evci. The dormant neuron phenomenon in deep reinforcement learning. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 32145\u201332168. PMLR, 23\u201329 Jul 2023.   \n[48] Ghada Sokar, Rishabh Agarwal, Pablo Samuel Castro, and Utku Evci. The dormant neuron phenomenon in deep reinforcement learning, 2023.   \n[49] Michalis K. Titsias, Alexandre Galashov, Amal Rannen-Triki, Razvan Pascanu, Yee Whye Teh, and Jorg Bornschein. Kalman filter for online classification of non-stationary data, 2023.   \n[50] G. E. Uhlenbeck and L. S. Ornstein. On the theory of the brownian motion. Phys. Rev., 36:823\u2013841, Sep 1930.   \n[51] Gido M van de Ven and Andreas S Tolias. Three scenarios for continual learning. arXiv preprint arXiv:1904.07734, 2019.   \n[52] Tim van Erven, Wouter M. Koolen, and Dirk van der Hoeven. Metagrad: Adaptation using multiple learning rates in online learning. Journal of Machine Learning Research, 22(161):1\u201361, 2021.   \n[53] Eli Verwimp, Rahaf Aljundi, Shai Ben-David, Matthias Bethge, Andrea Cossu, Alexander Gepperth, Tyler L. Hayes, Eyke H\u00fcllermeier, Christopher Kanan, Dhireesha Kudithipudi, Christoph H. Lampert, Martin Mundt, Razvan Pascanu, Adrian Popescu, Andreas S. Tolias, Joost van de Weijer, Bing Liu, Vincenzo Lomonaco, Tinne Tuytelaars, and Gido M. van de Ven. Continual learning: Applications and the road forward, 2024.   \n[54] Florian Wenzel, Kevin Roth, Bastiaan S. Veeling, Jakub \u00b4Swi a\u02dbtkowski, Linh Tran, Stephan Mandt, Jasper Snoek, Tim Salimans, Rodolphe Jenatton, and Sebastian Nowozin. How good is the bayes posterior in deep neural networks really?, 2020.   \n[55] Runtian Zhai, Stefan Schroedl, Aram Galstyan, Anoop Kumar, Greg Ver Steeg, and Pradeep Natarajan. Online continual learning for progressive distribution shift (OCL-PDS): A practitioner\u2019s perspective, 2023.   \n[56] Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the Twentieth International Conference on International Conference on Machine Learning, ICML\u201903, page 928\u2013935. AAAI Press, 2003. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: We outline main contributions of the paper in the introduction and abstract. Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 13}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Justification: We discuss limitations in the experimental section ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 13}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 13}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 13}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 14}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: We disclose the experimental information in Experimental and Appendix sections. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 14}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 14}, {"type": "text", "text": "Answer: [No] ", "page_idx": 15}, {"type": "text", "text": "Justification: Unfortunately, due to IP constrains, we cannot release the code for the paper. Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 15}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: We provide experimental details in the appendix. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 15}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: We specify that we report results with 3 random seeds with mean and standard deviation. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We provide information about compute resources required in the appendix. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 16}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: Based on our understanding, our work conforms to the every aspect of NeurIPS Code of Ethics. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 16}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 17}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We cite the works which introduced the publicly available datasets ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 17}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 18}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 18}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 18}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 18}, {"type": "text", "text": "A Ornstein-Uhlenbeck process ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We make use that the Ornstein-Uhlenbeck process [50] defines a SDE that can be solved explicitly and written as a time-continuous Gaussian Markov process with transition density ", "page_idx": 19}, {"type": "equation", "text": "$$\np(x_{t}|x_{s})=\\mathcal{N}(x_{s}e^{-(t-s)},(1-e^{-2(t-s)})\\sigma_{0}^{2}I),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for any pair of times $t>s$ . Based on this as a drift model for the parameters $\\theta_{t}$ (so $\\theta_{t}$ is the state $x_{t}$ ) we use the conditional density ", "page_idx": 19}, {"type": "equation", "text": "$$\np(\\theta_{t+1}|\\theta_{t})=\\mathcal{N}(\\theta_{t}\\gamma_{t},(1-\\gamma_{t}^{2})\\sigma_{0}^{2}I),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\gamma_{t}=e^{-\\delta_{t}}$ and $\\delta_{t}\\geq0$ corresponds to the learnable discretization time step. In other words, by learning $\\gamma_{t}$ online we equivalently learn the amount of a continuous \u201ctime shift\u201d $\\delta_{t}$ between two consecutive states in the OU process. This essentially models parameter drift since e.g. if $\\gamma_{t}=1$ , then $\\delta_{t}=0$ and there is no \u201ctime shift\u201d which means that the next state/parameter remains the same as the previous one, i.e. $\\theta_{t+1}=\\theta_{t}$ . ", "page_idx": 19}, {"type": "text", "text": "B Other choices of drift model ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we discuss alternative choices of a drift model instead of (5). ", "page_idx": 19}, {"type": "text", "text": "Independent mean and variance of the drift. We consider the drift model where the mean and the variance are not connected, i.e., ", "page_idx": 19}, {"type": "equation", "text": "$$\np(\\theta_{t+1}|\\theta_{t},\\gamma_{t},\\beta_{t})=\\mathcal{N}(\\theta_{t+1};\\gamma_{t}\\theta_{t}+(1-\\gamma_{t})\\mu_{0};\\beta_{t}^{2}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\gamma_{t}\\,\\in\\,[0,1]$ is the parameters controlling the mean of the distribution and $\\beta_{t}$ is the learned variance. When $\\beta$ is fixed, this would be similar to our experiment in Figure 16 where we assumed known task boundaries and we do not estimate the drift parameters but assume it as a hyperparameter. Figure 16, left corresponds to the case when $\\beta_{t}$ is a fixed parameter independent from $\\gamma_{t}$ whereas Figure 16, right corresponds to the case when $\\beta_{t}=\\sqrt{1-\\gamma_{t}^{2}}\\sigma_{0}$ , i.e., when we use the drift model (5). We see from the results, using drift model (5) leads to a better performance. In case when $\\beta_{t}$ are learned, estimating the parameters of this model will likely overfti to the noise since there is a lot of degrees of freedom. ", "page_idx": 19}, {"type": "text", "text": "Shrink & Perturb [2]. When we do not use the mean of the initialization, we can use the following drift model ", "page_idx": 19}, {"type": "equation", "text": "$$\np(\\theta_{t+1}|\\theta_{t},\\lambda_{t},\\beta_{t})=\\mathcal{N}(\\theta_{t+1};\\lambda_{t}\\theta_{t};\\beta_{t}^{2})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Similarly to the case of (20), estimating both parameters $\\lambda_{t}$ and $\\beta_{t}$ from the data will likely overfti to the noise. ", "page_idx": 19}, {"type": "text", "text": "Arbitrary linear model. We can use the arbitrary linear model of the form ", "page_idx": 19}, {"type": "equation", "text": "$$\np(\\theta_{t+1}|\\theta_{t},A_{t},B_{t})=\\mathcal{N}(\\theta_{t+1};A_{t}\\theta_{t};B_{t}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "but estimating the parameters $A_{t}$ and $B_{t}$ has too many degrees of freedom and will certainly overfti. ", "page_idx": 19}, {"type": "text", "text": "Gaussian Spike & Slab We consider a Gaussian [11] approximation to Spike & Slab [26] prior ", "page_idx": 19}, {"type": "equation", "text": "$$\np(\\theta_{t+1}|\\theta_{t},\\gamma_{t})=\\gamma_{t}p(\\theta_{t+1}|\\theta_{t})+(1-\\gamma_{t})p_{0}(\\theta_{t+1}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which is a mixture of two distributions - a Gaussian $p(\\theta_{t+1}|\\theta_{t})=\\mathcal{N}(\\theta_{t+1};\\theta_{t},\\sigma^{2})$ centered around the previous parameter $\\theta_{t}$ and an initializing distribution $p_{0}(\\theta_{t+1})=\\mathcal N(\\theta_{t+1};\\mu_{0},\\sigma_{0}^{2})$ . This model, however, implements the mechanism of Hard reset as opposed to the soft ones. Moreover, estimating such a model and incorporating it into a learning update is more challenging since the mixture of Gaussian is not conjugate with respect to a Gaussian which will make the KL term (34) to be computed only approximately via Monte Carlo updates. ", "page_idx": 19}, {"type": "text", "text": "C Practical implementations of the drift model estimation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Stochastic approximation for drift parameters estimation In practice, we use $M=1$ , which leads to the stochastic approximation ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\int p(y_{t+1}|x_{t+1},\\mu_{t}(\\gamma_{t}^{k})+\\epsilon\\sigma_{t}(\\gamma_{t}^{k}))\\mathcal{N}(\\epsilon;0,I)d\\epsilon\\approx p\\left(y_{t+1}|x_{t+1},\\mu_{t}(\\gamma_{t}^{k})+\\epsilon\\sigma_{t}(\\gamma_{t}^{k})\\right)\\mathcal{N}(\\epsilon,\\mu_{t}(\\gamma_{t}^{k}))\\mathcal{N}(\\epsilon,\\mu_{t}(\\gamma_{t}^{k}))\\delta\\left(x_{t+1}^{k}-x_{t}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using NN initializing distribution. In the drift model (5), we assume that the initial distribution over parameters is given by $p_{0}(\\theta)=\\mathcal{N}(\\theta;\\mu_{0};\\sigma_{0}^{2})$ . In practice, we have access to the NN initializer $p_{i n i t}(\\r_{\\theta})\\,=\\mathcal{N}(\\r_{\\theta};\\r_{0};\\r_{\\sigma_{0}^{2}})$ where $\\mu_{0}\\,=\\,0$ (for most of the NNs). This means that we can replace $\\epsilon$ from (10) by $\\scriptstyle{\\frac{1}{\\sigma_{0}}}\\theta_{0}^{\\prime}$ where $\\theta_{0}^{\\prime}\\sim p_{i n i t}(\\theta)$ . This means that the term in (21) can be replaced by ", "page_idx": 20}, {"type": "equation", "text": "$$\np\\left(y_{t+1}|x_{t+1},\\mu_{t}(\\gamma_{t}^{k})+\\epsilon\\sigma_{t}(\\gamma_{t}^{k})\\right)=p\\left(y_{t+1}|x_{t+1},\\mu_{t}(\\gamma_{t}^{k})+\\theta_{0}^{\\prime}\\sqrt{1-\\gamma_{t}^{2}+\\gamma_{t}^{2}\\frac{\\sigma_{t}^{2}}{\\sigma_{0}^{2}}}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we used the fact that $\\sigma_{t}^{2}(\\gamma_{t})=\\gamma_{t}^{2}\\sigma_{t}^{2}+(1-\\gamma_{t}^{2})\\sigma_{0}^{2}$ . Note that in (22), we only need to know $\\frac{\\sigma^{2}}{\\sigma_{0}^{2}}$ underlying algorithm. Finally, in practice, we can tie $p_{0}(\\theta)$ to the specific initialization $\\theta_{0}\\sim p_{i n i t}(\\theta)$ . It was observed empricially [34] that using a specific initialization in gradient updates led to better performance than using samples from the initial distribution. This would imply that ", "page_idx": 20}, {"type": "equation", "text": "$$\np_{0}(\\theta)=\\mathcal{N}(\\theta;\\theta_{0},\\tilde{\\sigma}_{0}^{2}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with $\\tilde{\\sigma}_{0}^{2}=p^{2}\\sigma_{0}^{2}$ . The parameter $p\\leq1$ accounts for the fact that the distribution $p_{0}(\\theta)$ should have lower than $p_{i n i t}(\\theta)$ variance since it uses the specific initializaiton from the prior. This modification would imply the following modification on the drift model term (22) ", "page_idx": 20}, {"type": "equation", "text": "$$\np\\left(y_{t+1}|x_{t+1},\\mu_{t}(\\gamma_{t}^{k})+\\epsilon\\sigma_{t}(\\gamma_{t}^{k})\\right)=p\\left(y_{t+1}|x_{t+1},\\mu_{t}(\\gamma_{t}^{k})+\\theta_{0}^{\\prime}p\\sqrt{1-\\gamma_{t}^{2}+\\gamma_{t}^{2}\\frac{\\sigma_{t}^{2}}{\\sigma_{0}^{2}}}\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "D Experimental details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "D.1 Plasticity experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Tasks In this section we provide experimental details. As plasticity tasks, we use a randomly selected subset of size 10000 from CIFAR-10 [32] and from MNIST. This subset is fixed for all the tasks. Within each task, we randomly permute labels for every image; we call such problems random-label classification problems. We study two regimes \u2013 data efficient, where we do 400 epochs on a task with a batch size of 128, and memorization, a regime where we do only 70 epochs with a batch size of 128. As the main backbone architecture, we use MLP with 4 hidden layers each having a hidden dimension of 256 hidden units. We use ReLU activation function and do not use any batch or layer normalization. For the incoming data, we apply random crop, for MNIST to produce images of size $24\\times24$ and for CIFAR-10 to produce images of size $28\\times28$ . We normalize images to be within [0, 1] range by dividing by 255. On top of that, we consider permuted MNIST task with a similar training ragime as in [34] \u2013 we consider a subset of 10000 images, with batch size 16 and each task is one epoch. As a backbone, we still use MLP with ReLU activation and 4 hidden layers. Moreover, we considered permuted Patch MNIST, where we permute patches, not individual pixels. In this case, we used a simple 4 layer convolutional neural network with 2 fully connected layers at the end. ", "page_idx": 20}, {"type": "text", "text": "Metrics We use online accuracy as first metric with results reported in Appendix H. Moreover we use per-task Average Online Accuracy which is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\boldsymbol{A}_{t}=\\frac{1}{N}\\sum_{i=1}^{N}a_{i}^{t},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $a_{i}^{t}$ are the online accuracies collected on the task $t$ via $N$ timesteps. ", "page_idx": 20}, {"type": "text", "text": "Baselines First baseline is Online SGD which sequentially learns over the sequence of task, with a fixed learning rate. Hard Reset is the Online $S G D$ which resets all the parameters at task boundaries. $L2$ init [34] adds a regularizer $\\lambda||\\theta-\\theta_{0}||^{2}$ term to each Online $S G D$ update where the regularization strength $\\lambda$ is a hyperparameter. Shrink & Perturb applies the transformation $\\lambda\\theta_{t}+\\sigma\\epsilon,\\epsilon\\stackrel{\\cdot}{\\sim}\\mathcal{N}(\\epsilon;0,I)$ to each parameter before the gradient update. The hyperparameters are $\\lambda$ and $\\sigma$ . ", "page_idx": 20}, {"type": "text", "text": "Soft Reset corresponds to one update (10) starting from 1 using 1 Monte Carlo estimate. We always use 1 Monte Carlo estimate for updating $\\gamma_{t}$ as we found that it worked well in practice on these tasks. The hyperparameters of the method \u2013 $\\sigma_{0}^{2}$ initial variance of the prior, which we set to be equal to $p^{2}{\\frac{1}{N}}$ where $N$ is the width of the hidden layer and $p$ is a constant (hyperparameter). It always equals to $\\bar{p}=0.1$ . On top of that the second hyperparameter is $s$ , such that $\\sigma_{t}=s\\sigma_{0}$ , which controls the relative decrease of the constant posterior variance. This is the hyperparameter over which we sweep over. Another hyperparameter is the learning rate for learning $\\gamma_{t}$ . For Soft Reset Proximal, we also have a proximal coefficient regularization constant $\\lambda$ . Besides that, we also sweep over the learning rate for the parameter. For the Bayesian Soft Reset, we just add an additional learning rate for the variance $\\alpha_{\\sigma}$ and we do 1 Monte Carlo sample for each ELBO update. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Hyper parameters selection and evaluation For all the experiments, we run a sweep over the hyperparameters. We select the best hyperparameters based on the smallest cumulative error (sum of all $\\bar{1}-a_{i}^{t}$ throughout the training). We then report the mean and the standard deviation across 3 seeds in all the plots. ", "page_idx": 21}, {"type": "text", "text": "Hyperparameter ranges . Learning rate $\\alpha$ which is used to update parameters, for all the methods, is selected from $\\{1e{-4},5e{-4},1e{-3},5e{-3},1e{-2},5e{-2},1e{-1},5e{-1},1.0\\}$ . The $\\lambda_{i n i t}$ parameter in $L2$ Init, is selected from $\\{10.0,1.0,0.0,1e-1,5e-1,1e-2,5e-2,1e-3,\\ldots$ $\\k_{1,\\,1e-2,\\,5e-2,\\,1e-3,\\,5e-3,\\,1e-4,\\,5e-4,\\,1e-$ 5, 5e\u22125, 1e\u22126, 5e\u22126, 1e\u2212 $\\cdot\\,7,5e-7,1e-8,5e-8,1e-9,5e-9,1e-10,\\,\\}.$ . For S&P, the shrink parameter $\\lambda$ is selected from $\\{\\downarrow.0,0.99999,0.9999,0.999,0.99,0.9,0.8,0.7,0.$ 5, 0.3, 0.2, 0.1}, and the perturbation parameter $\\sigma$ is from $\\{1e-1,1e-2,1e-3,1e-4,1e-5,1e-6\\}$ . As noise distribution, we use the Neural Network initial distribution. For Soft Resets, the learning rate for $\\gamma_{t}$ is selected from $\\{0.5,0.1,0.05,0.01,0.005,0.001,0.0005,0.0001\\}$ , the constant $s$ is selected from $\\{1.0,0.95,0.9,0.8,0.7,0.6,0.5,0.3,0.1\\}$ , the proximal cost $\\tilde{\\lambda}$ in (41) is selected from $\\{1.0,0.1,0.0\\dot{1}\\}$ , the same is true for the proximal cost in the Bayesian method (38). On top of that for the Bayesian method, we always use $p$ (see Algorithm 2) equal to $p=0.05$ and $s=0.9$ , i.e. the posterior is always slightly smaller than the prior. Finally for the Bayesian method we had to learn the variance with learning rate from $\\{0.01,0.1,1,10\\}$ range. ", "page_idx": 21}, {"type": "text", "text": "In practice, we found that there is one learning rate of 0.1, which was always the best in practice for most of the methods and only proximal Soft Resets on memorization CIFAR-10 required smaller learning rate 0.01. This allowed us to significantly reduce the hyperparameter sweep. ", "page_idx": 21}, {"type": "text", "text": "D.2 Impact of non-stationarity experiments ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this experiment, we consider a subset of 10000 images from MNIST (fixed throughtout all experiment) and a sequence of tasks. Each task is constructed by assigning either a true or a random label to each image from MNIST, where the probability of assignment is controlled by the experiment. The duration of each is controlled by the number of epochs with batch size of 128. As backbone we use MLP with 4 hidden layers and 256 hidden units and ReLU activation. For all the methods, the learning rate is 0.1. For Soft Resets, we use $s=0.9$ and $p=1$ and $\\eta_{\\gamma}=0.01$ . Bayesian method uses proximal cost $\\lambda=0.01$ . Detailed results are given in Figure 7. ", "page_idx": 21}, {"type": "text", "text": "D.3 Reinforcement learning experiments ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We conduct experiments in the RL environments. We take the canonical implementation of Soft-Actor Critic(SAC) from Brax [15] repo in github, which uses 2 layer MLPs for both policy and Q-function. It employs ReLU activation functions for both. On top of that, it uses 2 MLP networks to parameterize $Q$ -function (see Brax [15]) for more details. To employ Soft Reset, we do the following. After we have collected a chunk of data (128) time-steps, we do one update (10) on $\\gamma_{t}$ starting from 1 at every update of $\\gamma_{t}$ , where $\\gamma_{t}$ is shared for all the parameters within each layer of a Neural Network, separately for weights and biases. On top of that, since we have policy and value function networks, we have separate $\\gamma_{t}$ for each of these. After the update on $\\gamma_{t}$ , we compute $\\theta_{t}(\\gamma_{t})$ and $\\alpha_{t}(\\gamma_{t})$ , see Section 3.5. After that, we employ the proximal objective (41) with a fixed regularization target $\\theta_{t}(\\gamma_{t})$ . Concretely, we use the update rule (43) where for each update the gradient is estimate on the batch of data from the replay buffer. This is not exactly the same as what we did with plasticity benchmarks since there the update was applied to the same batch of data, multiple times. Nevertheless, we found this strategy effective and easy to implement on top of a SAC algorithm. In practice, we swept over the parameter $s$ (similar for both, policy and the value function) which controls the relative learning rate increase in (18). Moreover, we swept over the proximal regularization constant $\\tilde{\\lambda}$ from eqn. (41), which was different for the policy and for the value function. In practice, we found that using proximal constant of 0 for the policy led to the best empirical results. The range for the proximal constants $\\tilde{\\lambda}$ was $\\{0.1,0.01,0.001\\}$ and for $s$ was $\\{0.8,0.9,0.95,0.97,1.0\\}$ . We used $p=1$ for all the experiments. For each experiment, we used a 3 hours of the $A100\\;\\mathrm{GPU}$ with $^\\mathrm{40\\,Gb}$ of memory. ", "page_idx": 21}, {"type": "image", "img_path": "fDiZJ7mmOV/tmp/440305c5401de793bb4a5862f298a5c7cbcfa65567085ac649087fda3f34d2e9.jpg", "img_caption": ["Figure 7: Non-stationarity impact. The $\\mathbf{X}$ -axis denotes task id, each column denotes the duration, whereas a row denotes the amount of label noise. Each color denotes the method studied. The y-axis denotes average over 3 seeds online accuracy. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "fDiZJ7mmOV/tmp/d16bc85ed6e4bb451bf318c35470190f8fae3b20994c3629ed0d1df61c730a25.jpg", "img_caption": ["Figure 8: Visualization of the $\\gamma_{t}$ dynamics for the run on Humanoid environment. Each column corresponds to the replay ratio studied. First row denotes the $\\gamma_{t}$ for the policy $\\pi$ . The second and the third rows denote the $\\gamma_{t}$ for the two $Q$ -functions. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "E Computational complexity ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We provide the study of computational cost for all the proposed methods. Notations: ", "page_idx": 23}, {"type": "text", "text": "\u2022 $P$ be the number of parameters in the Neural Network   \n\u2022 $L$ is the number of layers   \n\u2022 $O(S)$ is the cost of SGD backward pass.   \n\u2022 $M_{\\gamma}$ - number of Monte Carlo samples for the drift model   \n\u2022 $M_{\\theta}$ - number of Monte Carlo samples for the parameter updates (Bayesian Method). \u2022 $K_{\\gamma}$ - number of updates for the drift parameter   \n\u2022 $K_{\\theta}$ - number of NN parameter updates. ", "page_idx": 23}, {"type": "table", "img_path": "fDiZJ7mmOV/tmp/c975f08b034c34f85b2ba1c8501c1b3c323cec39163540678c438684687dc711.jpg", "table_caption": ["Table 1: Comparison of methods, computational cost, and memory requirements "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "The general theoretical cost of all the proposed approaches is given in Table 1. In practice, for all the experiments, we assume that $M_{\\gamma}=1$ and $M_{\\theta}=1$ . Moreover, we used $K_{\\gamma}=1$ and $K_{\\theta}=1$ for Soft Reset, $K_{\\gamma}=10$ and $K_{\\theta}=1$ for Soft Reset with more computation. On top of that, for Soft Reset proximal and all Bayesian methods, we used $K_{\\gamma}=10$ and $K_{\\theta}=10$ . Table 2, quantifying the complexity of all the methods from Figure 2. ", "page_idx": 23}, {"type": "table", "img_path": "fDiZJ7mmOV/tmp/0e671730c138166ff37bb32513f366a03c025a27176720af8860af0a552b54e5.jpg", "table_caption": [], "table_footnote": ["Table 2: Comparison of methods, computational cost, and memory requirements for methods in Figure 2. "], "page_idx": 23}, {"type": "text", "text": "The complexity $O(2S)$ of Soft Resets comes from one update on drift parameter and one updat eon NN parameters. The memory complexity requires storing $O(L)$ parameters gamma (one for each layer), parameters $\\theta_{t}$ with $O(P)$ and sampled parameters for drift model update which requires $O(P)$ . ", "page_idx": 23}, {"type": "text", "text": "Note that as Figure 9 suggests, it is beneficial to spend more computational cost on optimizing gamma and on doing multiple updates on parameters. However, even the cheapest version of our method Soft Resets still leads to a good performance as indicated in Figure 2. ", "page_idx": 23}, {"type": "text", "text": "The complexity of soft resets in reinforcement learning setting requires only one gradient update on $\\gamma$ after each new chunk of fresh data from the environment. In SAC, we do $G$ gradient updates on parameters for every new chunk of data. Assuming that complexity of one gradient update in SAC is $O(S)$ , soft reset only requires doing one additional gradient update to fit $\\gamma$ parameter. ", "page_idx": 23}, {"type": "text", "text": "The computation complexity of Soft Reset in Reinforcement Learning is marginally higher than SAC but leads to better empirical performance in a highly off-policy regime, see Appendix D.3. ", "page_idx": 23}, {"type": "text", "text": "Figure 9: Compute-performance tradeoff. The $\\mathbf{X}$ -axis indicates the method going from the cheapest (left) to the most expensive (right). See Table 2 for complexity analysis. The y-axis is the average performance on all the tasks across the stream. ", "page_idx": 24}, {"type": "image", "img_path": "fDiZJ7mmOV/tmp/41693941bfafc215578a9011bb8b1a65d7e62a6345c404782c9a6529e919d57f.jpg", "img_caption": ["Table 3: Comparison of methods, computational cost, and memory requirements for methods in for RL. "], "img_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "fDiZJ7mmOV/tmp/0f2bc9403d997b4bc9759efeac14cd0248312eaf3efaf432f858475196a206b8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "F Sensitivity analysis ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We study the sensitivity of Soft Resets where $\\gamma$ is defined per layer when trained on random-label MNIST (data efficient). We fix the learning rate to $\\alpha=0.1$ . We study the sensitivity of learning rate for the drift parameter, $\\eta_{\\gamma}$ , as well as $p-$ initial prior standard deviation rescaling, and $s$ \u2013 posterior standard deviation rescaling parameter. ", "page_idx": 24}, {"type": "text", "text": "On top of that, we conduct the sensitivity analysis of L2 Init [34] and Shrink&Perturb [2] methods. The $\\mathbf{X}$ -axis of each plot denotes one of the studied hyperparameters, whereas y-axis is the average performance across all the tasks (see Experiments section for tasks definition). The standard deviation is reported over 3 random seeds. A color indicates a second hyperparameter which is studied, if available. In the title of each plot, we write hyperparameters which are fixed. The analysis is provided in Figure 10 for Soft Resets and in Figure 11 for the baselines. ", "page_idx": 24}, {"type": "text", "text": "The most important parameter is the learning rate of the drift model $\\eta_{\\gamma}$ . For each method, there exists a good value of this parameter and performance is sensitive to it. This makes sense since this parameter directly impacts how we learn the drift model. ", "page_idx": 24}, {"type": "text", "text": "The performance of Soft Resets is robust with respect to the posterior standard deviation scaling $s$ parameter as long as it is $s\\geq0.5$ . For $s<0.5$ , the performance degrades. This parameter is defined from $\\sigma_{t}=s\\sigma_{0}$ and affects relative increase in learning rate given by $\\frac{1}{\\gamma^{2}\\!+\\!(1\\!-\\!\\gamma^{2})/s^{2})}$ which could be ill-behaved for small $s$ . ", "page_idx": 24}, {"type": "text", "text": "We also study the sensitivity of the baseline methods. We find that L2 Init [34] is very sensitive to the parameter $\\lambda$ , which is a penalty term for $\\lambda||\\theta-\\theta_{0}||^{2}$ . In fact, Figure 11, left shows that there is only one good value of this parameter which works. Shrink&Perturb [2] is very sensitive to the shrink parameter $\\lambda$ . Similar to $L2\\,\\mathrm{Init}$ , there is only one value which works, 0.9999 while values 0.999 and values 0.99999 lead to bad performance. This method however, is not very sensitive to the perturb parameter $\\sigma$ provided that $\\sigma\\le0.001$ . ", "page_idx": 24}, {"type": "text", "text": "Compared to the baselines, our method is more robust to the hyperparameters choice. Below, we also add sensitivity analysis for other method variants. Figure 12 shows sensitivity of Soft Resets, $K_{\\gamma}=10$ , Figure 13 shows sensitivity of Soft Resets, $K_{\\gamma}=10$ , $K_{\\theta}=10$ , Figure 14 shows sensitivity of Bayesian Soft Resets, $K_{\\gamma}\\;=\\;10$ , $K_{\\theta}~=~10$ with $\\gamma_{t}$ per layer, Figure 15 shows sensitivity of Bayesian Soft Resets, $K_{\\gamma}=10$ , $K_{\\theta}=10$ with $\\gamma_{t}$ per parameter. ", "page_idx": 25}, {"type": "image", "img_path": "fDiZJ7mmOV/tmp/c2ec9f506d723f1704f83b22635241572bb57b8cab96eaa9ad9356026ba4baa1.jpg", "img_caption": ["Figure 10: Soft Reset, sensitivity analysis of performance with respect to the hyperparameters on data-efficient random-label MNIST. The ${\\bf X}$ -axis denotes the studied hyperparameter, whereas the y-axis denotes the average performance across the tasks. The standard deviation is computed over 3 random seeds. The color indicates additional studied hyperparameter. (Left) shows sensitivity analysis where the $\\mathbf{X}$ -axis is the posterior standard deviation scaling $s$ and the color indicates the drift model learning rate $\\eta_{\\gamma}$ . (Right) shows sensitivity of Soft Reset where the $\\mathbf{X}$ -axis is the posterior standard deviation scaling $s$ and the color indicates initial prior standard deviation scaling $p$ . "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "fDiZJ7mmOV/tmp/b916f6ffda3876618b07f923835fa1c95da396480215ab5c6645f7b0d0b5b0bc.jpg", "img_caption": ["Figure 11: L2 Init and Shrink&Perturb sensitivity analysis of performance with respect to the hyperparameters on data-efficient random-label MNIST. The $\\mathbf{X}$ -axis denotes the studied hyperparameter, whereas the y-axis denotes the average performance across the tasks. The standard deviation is computed over 3 random seeds. The color optionally indicates additional studied hyperparameter. (Left) shows sensitivity of $L2$ Init with respect to the $L2$ penalty regularization cost $\\lambda$ applied to $||\\theta-\\theta_{0}||^{2}$ term. We do not use an additional hyperparameter, therefore there is only one color. (Right) shows sensitivity of Shrink&Perturb method where the $\\mathbf{X}$ -axis is the perturb parameter $\\sigma$ while the color indicates the shrink parameter $\\lambda$ . "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "G Proximal SGD ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Each step of online SGD can be seen in terms of a regularized minimization problem referred to as the proximal form [44]: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\theta}_{t+1}=\\arg\\operatorname*{min}_{\\theta}\\mathcal{L}_{t+1}(\\theta)+\\frac{1}{2\\alpha_{t}}||\\theta-\\theta_{t}||^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In general, we cannot solve (26) directly, so we consider a Taylor expansion of $\\mathcal{L}_{t+1}$ around $\\theta_{t}$ , giving ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\theta_{t+1}=\\arg\\operatorname*{min}_{\\theta}\\nabla_{\\theta}\\mathcal{L}_{t+1}(\\theta_{t})^{\\top}(\\theta-\\theta_{t})+\\frac{1}{2\\alpha_{t}}||\\theta_{t}-\\theta||^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Here we see the role of $\\alpha_{t}>0$ as both enforcing that the Taylor expansion around $\\theta_{t}$ is accurate, and regularising $\\theta_{t+1}$ towards the old parameters $\\theta_{t}$ (hence ensuring that the learning from past data is not forgotten). Solving (27) naturally leads to the well known SGD update: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\theta_{t+1}=\\theta_{t}-\\alpha_{t}\\nabla_{\\theta}\\mathcal{L}_{t+1}(\\theta_{t}),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\alpha_{t}$ can now also be interpreted as the learning rate. ", "page_idx": 25}, {"type": "image", "img_path": "fDiZJ7mmOV/tmp/1dde3c34384173d18118b0c5a2cef7fc39f852554673a576d0bcd844f4d12a88.jpg", "img_caption": ["Figure 12: Soft Reset, $K_{\\gamma}=10$ , sensitivity analysis of performance with respect to the hyperparameters on data-efficient random-label MNIST. The $\\mathbf{X}_{\\mathrm{}}$ -axis denotes the studied hyperparameter, whereas the $\\mathbf{y}$ -axis denotes the average performance across the tasks. The standard deviation is computed over 3 random seeds. The color indicates additional studied hyperparameter. (Left) shows sensitivity analysis where the x-axis is the posterior standard deviation scaling $s$ and the color indicates the drift model learning rate $\\eta_{\\gamma}$ . (Right) shows sensitivity analysis where the $\\mathbf{X}$ -axis is the posterior standard deviation scaling $s$ and the color indicates initial prior standard deviation scaling $p$ . "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "fDiZJ7mmOV/tmp/b99accfac523d715ba17da106a95e292caed15d9b927d4eefd993b1800e4a0f4.jpg", "img_caption": ["Figure 13: Soft Reset, $K_{\\gamma}\\;=\\;10,K_{\\theta}\\;=\\;10$ , sensitivity analysis of performance with respect to the hyperparameters on data-efficient random-label MNIST. The $\\mathbf{X}$ -axis denotes the studied hyperparameter, whereas the $\\mathrm{y}$ -axis denotes the average performance across the tasks. The standard deviation is computed over 3 random seeds. The color indicates additional studied hyperparameter. (Left) shows sensitivity analysis where the $\\mathbf{X}$ -axis is the posterior standard deviation scaling $s$ and the color indicates the drift model learning rate $\\eta_{\\gamma}$ . (Right) shows sensitivity analysis where the $\\mathbf{X}$ -axis is the posterior standard deviation scaling $s$ and the color indicates initial prior standard deviation scaling $p$ . "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "fDiZJ7mmOV/tmp/2043f77a5a6db3405c0e6d4302c2e5a840616528824ed6eeaef7f8ade4f8408b.jpg", "img_caption": ["Figure 14: Bayesian Soft Reset, $K_{\\gamma}\\,=\\,10,K_{\\theta}\\,=\\,10$ with $\\gamma_{t}$ per layer, sensitivity analysis of performance with respect to the hyperparameters on data-efficient random-label MNIST. The $\\mathbf{X}_{\\mathrm{~}}$ -axis denotes the studied hyperparameter, whereas the y-axis denotes the average performance across the tasks. The standard deviation is computed over 3 random seeds. The color indicates additional studied hyperparameter. (Left) shows sensitivity analysis where the ${\\bf X}$ -axis is the prior standard deviation initial scaling $p$ and the color indicates the drift model learning rate $\\eta_{\\gamma}$ . (Right) shows sensitivity analysis where the ${\\bf X}$ -axis is the KL divergence coefficient $\\lambda$ while the color indicates the learning rate \u03b7\u03b3. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "fDiZJ7mmOV/tmp/a0f3bd0358dfabe125c3d60a047cdc9188618ba0c5b8b90bb746c65a28529f21.jpg", "img_caption": ["Figure 15: Bayesian Soft Reset, $K_{\\gamma}=10$ , $K_{\\theta}=10$ with $\\gamma_{t}$ per parameter, sensitivity analysis of performance with respect to the hyperparameters on data-efficient random-label MNIST. The $\\mathbf{X}$ -axis denotes the studied hyperparameter, whereas the y-axis denotes the average performance across the tasks. The standard deviation is computed over 3 random seeds. The color indicates additional studied hyperparameter. (Left) shows sensitivity analysis where the ${\\bf X}$ -axis is the prior standard deviation initial scaling $p$ and the color indicates the drift model learning rate $\\eta_{\\gamma}$ . (Right) shows sensitivity analysis where the ${\\bf X}$ -axis is the KL divergence coefficient $\\lambda$ while the color indicates the learning rate \u03b7\u03b3. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "fDiZJ7mmOV/tmp/eedf25cad95c6ff12864bd30226ece561fe76d1356b08ff6e434883a84f9f1eb.jpg", "img_caption": ["Figure 16: Perfect soft-resets on data-efficient random-label MNIST. Left, Soft Reset method does not use higher learning rate when $\\gamma<1$ . Right, Soft Reset increases the learning rate when $\\gamma<1$ , see (18). The ${\\bf X}$ -axis represents task id, whereas the y-axis is the average training accuracy on the task. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "H Qualitative behavior of soft resets and additional results on Plasticity benchmarks ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "H.1 Perfect Soft Resets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "To understand the impact of drift model (5), we study the data efficient random-label MNIST setting where task boundaries are known. We run Online SGD, Hard Reset which resets all parameters at task boundaries, and Hard Reset (only last) which resets only the last layer. We use Soft Reset method (19) where $\\gamma_{t}=1$ all the time and becomes $\\gamma_{t}=\\hat{\\gamma_{t}}$ (with manually chosen $\\hat{\\gamma}_{t}$ ) at task boundaries. We consider constant learning rate $\\alpha_{t}(\\gamma_{t})$ and increasing learning rate (18) at task boundary for Soft Reset. On top of that, we run Soft Reset method unaware of task boundaries which learns $\\gamma_{t}$ . We report Average training task accuracy metric in Figure 16. See Appendix D.1 for details. The results suggest that with the appropriate choice of $\\hat{\\gamma}_{t}$ , Soft Reset is much more efficient than Hard Reset and the effect becomes stronger if the learning rate $\\alpha_{t}(\\gamma_{t})$ increases. We also see that Soft Reset could learn an appropriate $\\gamma_{t}$ without the knowledge of task boundary. ", "page_idx": 27}, {"type": "text", "text": "H.2 Qualitative Behaviour on Soft Resets on random-label tasks. ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We observe what values of $\\gamma_{t}$ we get as we train Soft Reset method on random-label MNIST (dataefficient) and CIFAR-10 (memorization). The results are given in Figure 17 for MNIST and in Figure 18 for CIFAR-10. We report these for the first 20 tasks. ", "page_idx": 27}, {"type": "image", "img_path": "fDiZJ7mmOV/tmp/1cc2d9677d4346b3b0c77ba5ef82e9268b0e2719028c8406afa584f3f93aff63.jpg", "img_caption": ["Figure 17: Behaviour of $\\gamma_{t}$ for different layers on random-label MNIST (data efficient) for the first 20 tasks. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "fDiZJ7mmOV/tmp/ed29399fd51716879d14eb5bf2b6ab5779ce80fbba4656a8b85b499ef5e4f80a.jpg", "img_caption": ["Figure 18: Behaviour of $\\gamma_{t}$ for different layers on random-label CIFAR-10 (memorization) for the first 20 tasks. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "H.3 Qualitative Behaviour on Soft Resets on permuted patches of MNIST. ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We consider a version of permuted MNIST where instead of permuting all the pixels, we permute patches of pixels with a patch size varying from 1 to 14. The patch size of 1 corresponds to permututed MNIST and therefore the most non-stationary case, while patch size of 14 corresponds to least nonstationary case. We use a convolutional Neural Network in this case. In Figure 19, we report the behavior of $\\gamma$ for different convolutional and fully connected layers on first few tasks. ", "page_idx": 28}, {"type": "text", "text": "H.4 Bayesian method is better than non-Bayesian ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "As discussed in Section 5, we found that in practice Soft Reset and Soft Reset Proximal where $\\gamma$ is learned per-parameter, did not perform well on the plasticity benchmarks. However, the Bayesian variant described in Section I.1, actually benefited from specifying $\\gamma$ for every parameter in Neural Network. We report these additional results in Figure 20. We see that the non Bayesian variants where $\\gamma_{t}$ is specified per parameter, do not perform well. The fact that the Bayesian method performs better here suggests that it is important to have a good uncertainty estimate $\\sigma_{t}^{2}$ for the update (10) on $\\gamma_{t}$ . When, however, we regularize $\\gamma_{t}$ to be shared across all parameters within each layer, this introduces useful inductive bias which mitigates the lack of uncertainty estimation in the parameters. This is because for non-Bayesian methods, we assume that the uncertainty is fixed, given by a hyperparameter \u2013 assumption which would not always hold in practice. ", "page_idx": 28}, {"type": "text", "text": "H.5 Qualitative behavior of soft resets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we zoom-in in the data-efficient experiment on random-label MNIST. We use Soft Reset Proximal $\\gamma$ per layer) method with separate $\\gamma$ for layer (different for each weight and for each bias) and run it for 20 tasks on random-label MNIST. In Figure 21 we show the online accuracy as we learn over this sequence of tasks. In Figure 22, we visualize the dynamics of parameters $\\gamma$ for each layer. First of all, we see that $\\gamma_{t}$ seems to accurately capture the task boundaries. Second, we see that the amount by which each $\\gamma_{t}$ changes depends on the parameter type \u2013 weights versus biases, and it depends on the layer. The architecture in this setting starts form linear and goes up to linear4, which represent the 4 MLP hidden layers with a last layer linear4. ", "page_idx": 28}, {"type": "image", "img_path": "fDiZJ7mmOV/tmp/52cc8a854b0a15e4bc83104ed75f0f4217920ed6c609404dda92fe8f44f53f83.jpg", "img_caption": ["Figure 19: Behaviour of $\\gamma_{t}$ for different layers on permuted MNIST "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "fDiZJ7mmOV/tmp/9a0d6922718ad4adcba40ce078dc4ec0394c149cffce53a2bfaf957f5b972ef6.jpg", "img_caption": ["Figure 20: Performance of $\\gamma$ per-parameter methods "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "fDiZJ7mmOV/tmp/2d267ae2fe908895cb4ddd12f6472839ecf064599b2e644b18f35dae2f039c51.jpg", "img_caption": ["Figure 21: Visualization of accuracy when trained on data efficient random-label MNIST task. The dashed red lines correspond to a task boundary. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "fDiZJ7mmOV/tmp/4145da30eb6fe541a55b07ae8864919966f27981b8bc69c23721fe839d2765f9.jpg", "img_caption": ["Figure 22: Visualization of $\\gamma$ and task boundaries on data-efficient Random-label MNIST. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "H.6 Impact of specific initialization ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section, we study the impact of using specific initialization $\\theta_{0}\\sim p_{i n i t}(\\theta)$ in $p_{0}(\\theta)$ as discussed in Appendix C. Using the specific initialization in Soft Resets leads to fixing the mean of the $p_{0}(\\theta)$ to be $\\theta_{0}$ , see (23). This, in turn, leads to the predictive distribution (24). In case when we are not using specific initialization $\\theta_{0}$ , the mean of $p_{0}(\\bar{\\theta})$ is 0 and the predictive distribution is given by (22). To understand the impact of this design decision, we conduct an experiment on random label MNIST with Soft Reset, where we either use the specific initialization or not. For each of the variants, we do a hyperparameters sweep. The results are given in Figure 23. We see that both variants perform similarly. ", "page_idx": 29}, {"type": "text", "text": "I Learning parameters with estimated drift models ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section, we provide a Bayesian Neural Network algorithm to learn the distributions of NN parameters when there is a drift in the data distribution. Moreover, we provide a MAP-like inference ", "page_idx": 29}, {"type": "image", "img_path": "fDiZJ7mmOV/tmp/3def6932a040f660c1ff9819445d7e00cdba4b367ffe01f4967252a7bbf4eb4c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure 23: Impact of specific initialization $\\theta_{0}$ as a mean of $p_{0}(\\theta)$ in Soft Resets. The $\\mathbf{X}_{\\mathrm{}}$ -axis represents task id. The y-axis represents the average task accuracy with standard deviation computed over 3 random seeds. The task is random label MNIST \u2013 data efficient. ", "page_idx": 30}, {"type": "text", "text": "algorithm which does not require to learn the distributions over parameters, but simply propagates the MAP estimate over these. ", "page_idx": 30}, {"type": "text", "text": "I.1 Bayesian Neural Networks algorithm ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In this section, we describe an algorithm for parameters update based on Bayesian Neural Networks (BNN). It is based on the online variational Bayes setting described below. ", "page_idx": 30}, {"type": "text", "text": "Let the family of distributions over parameters be ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathcal{Q}=\\{q(\\theta):q(\\theta)\\sim\\prod_{i=1}^{D}\\mathcal{N}(\\theta^{i};\\mu_{i},\\sigma_{i}^{2});\\theta=(\\theta^{1},\\dots,\\theta^{D})\\},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which is the family of Gaussian mean-field distributions over parameters $\\boldsymbol\\theta\\in\\mathbb{R}^{D}$ (separate Gaussian per parameter). For simplicity of notation, we omit the index $i$ . Let $\\Gamma_{t}\\,=\\,(\\gamma_{1},.\\,.\\,.\\,,\\gamma_{t})$ be the history of observed parameters of the drift model and ${\\cal S}_{t}=\\{(x_{1},y_{1}),\\dots,(x_{t},y_{t})\\}$ be the history of observed data. We denote by $q_{t}(\\theta)\\triangleq q_{t}(\\theta|S_{t},\\Gamma_{t-1})\\in\\mathcal{Q}$ the Gaussian approximate posterior at time $t$ with mean $\\mu_{t}$ and variance $\\boldsymbol{\\sigma}_{t}^{2}$ for every parameter. The approximate predictive look-ahead prior is given by ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q_{t}(\\theta|\\gamma_{t})=\\int q_{t}(\\theta_{t})p(\\theta|\\theta_{t},\\gamma_{t})d\\theta_{t}=\\mathcal{N}(\\theta;\\mu_{t}(\\gamma_{t}),\\sigma_{t}^{2}(\\gamma_{t})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "that has parameters $\\mu_{t}(\\gamma_{t})=\\gamma_{t}\\mu_{t}+(1-\\gamma_{t})\\mu_{0},\\sigma_{t}^{2}(\\gamma_{t})=\\gamma_{t}^{2}\\sigma_{t}^{2}+(1-\\gamma_{t}^{2})\\sigma_{0}^{2}$ . To see this, we will use the law of total expectation and the law total variance. For two random variables $X$ and $Y$ defined on the same space, law of total expectation says ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}[Y]=\\mathbb{E}[\\mathbb{E}[Y|X]]\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and the law of total variance says ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{V}[Y]=\\mathbb{E}[\\mathbb{V}[Y|X]]+\\mathbb{V}[\\mathbb{E}[Y|X]]\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "In our case, from the drift model (5), we have the conditional distribution ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\theta|\\theta_{t}=\\gamma_{t}\\theta_{t}+(1-\\gamma_{t})\\mu_{0}+\\sqrt{(1-\\gamma_{t}^{2})\\sigma_{0}^{2}}\\epsilon,\\epsilon\\sim\\mathcal{N}(0;I)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "From (30), we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\theta|\\theta_{t}]=\\gamma_{t}\\theta_{t}+(1-\\gamma_{t})\\mu_{0}}\\\\ {\\mathbb{V}[\\theta|\\theta_{t}]=(1-\\gamma_{t}^{2})\\sigma_{0}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "From here, we have that the mean is given by ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\theta]=\\mathbb{E}[\\mathbb{E}[\\theta|\\theta_{t}]]=\\gamma_{t}\\mu_{t}+(1-\\gamma_{t})\\mu_{0}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and the variance is given by ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{V}[\\theta]=\\mathbb{E}[\\mathbb{V}[\\theta\\vert\\theta_{t}]]+\\mathbb{V}[\\mathbb{E}[\\theta\\vert\\theta_{t}]]}\\\\ {\\mathbb{V}[\\theta]=(1-\\gamma_{t}^{2})\\sigma_{0}^{2}+\\gamma_{t}^{2}\\theta_{t}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Now, we note that $q_{t}(\\theta|\\gamma_{t})$ is a Gaussian and its parameters are given by $\\mathbb{E}[\\theta]=\\gamma_{t}\\mu_{t}+(1-\\gamma_{t})\\mu_{0}$ from (31) and by $\\hat{\\mathbb{V}}[\\dot{\\theta}]\\ddot{=}\\,(1-\\gamma_{t}^{2})\\sigma_{0}^{2}+\\gamma_{t}^{2}\\theta_{t}^{2}$ from (32). Then, for new data $(x_{t+1},y_{t+1})$ at time $t+1$ , the approximate predictive log-likelihood equals to ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log q_{t}(y_{t+1}|x_{t+1},\\gamma_{t})=\\log\\int p(y_{t+1}|x_{t+1},\\theta)q_{t}(\\theta|\\gamma_{t})d\\theta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We are looking for a new approximate posterior $q_{t+1}(\\theta)$ such that ", "page_idx": 31}, {"type": "equation", "text": "$$\nq_{t+1}(\\theta)=\\arg\\operatorname*{min}_{q}\\mathbb{K L}\\left[q(\\theta)||p(y_{t+1}|x_{t+1},\\theta)q_{t}(\\theta|\\gamma_{t})\\right]\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The optimization problem (33) can be written as minimization of the following loss ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{F}_{t}(\\theta,\\gamma_{t})=\\mathbb{E}_{q}\\left[\\mathcal{L}_{t+1}(\\theta)\\right]+\\mathbb{K}\\mathbb{L}\\left[q(\\theta)||q_{t}(\\theta|\\gamma_{t})\\right],\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "since $\\mathcal{L}_{t+1}(\\theta)\\,=\\,-\\log p(y_{t+1}\\vert x_{t+1},\\theta)$ . Using the fact that we are looking for a member $q\\,\\in\\,\\mathcal{Q}$ from (28), we can write the objective (34) as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{F}_{t}(\\mu,\\sigma,\\gamma_{t})=\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0;I)}\\left[\\mathcal{L}_{t+1}(\\mu+\\epsilon\\sigma)\\right]+\\mathbb{K}\\mathbb{L}\\left[q(\\theta)||q_{t}(\\theta|\\gamma_{t})\\right],\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where we used the reparameterisation trick for the loss term. We now expand the regularization term to get ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{F}_{t}(\\mu,\\sigma,\\gamma_{t})=\\mathbb{E}_{\\epsilon\\sim N(0;I)}\\left[\\mathcal{L}_{t+1}(\\mu+\\epsilon\\sigma)\\right]+\\sum_{i}\\left[\\frac{(\\mu_{i}-\\mu_{t,i}(\\gamma_{t}))^{2}+\\sigma_{i}^{2}}{2\\sigma_{t,i}^{2}(\\gamma_{t})}-\\frac{1}{2}\\log\\sigma_{i}^{2}\\right]\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Since the posterior variance of NN parameters may become small, the optimization of (35) may become numerically unstable due to division by $\\dot{\\sigma}_{t,i}^{2}(\\gamma_{t})$ . It was shown [54] that using small temperature on the prior led to better empirical results when using Bayesian Neural Networks, a phenomenon known as cold posterior. Here, we define a temperature per-parameter, i.e., $\\lambda_{t,i}>0$ for every time-step $t$ , such that the objective above becomes ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{F}}_{t}(\\mu,\\sigma,\\gamma_{t};\\{\\lambda_{t,i}\\}_{i})=\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0;I)}\\left[\\mathcal{L}_{t+1}(\\mu+\\epsilon\\sigma)\\right]+\\sum_{i}\\lambda_{t,i}\\left[\\frac{(\\mu_{i}-\\mu_{t,i}(\\gamma_{t}))^{2}+\\sigma_{i}^{2}}{2\\sigma_{t,i}^{2}(\\gamma_{t})}-\\frac{1}{2}\\log\\sigma_{i}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "As said above, it is common to use the same temperature $\\lambda_{t,i}=\\lambda$ for all the parameters. In this work, we propose the specific choice of the temperature to be ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\lambda_{t,i}=\\lambda\\sigma_{t,i}^{2},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\lambda>0$ is some globally chosen temperature parameter. This leads to the following objective ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{r}}_{t}(\\mu,\\sigma,\\gamma_{t};\\{r t,i\\}_{i})=\\mathbb{E}_{\\epsilon\\sim N(0;I)}\\left[\\mathcal{L}_{t+1}(\\mu+\\epsilon\\sigma)\\right]+\\frac{1}{2}\\sum_{i}r_{t,i}\\left[(\\mu_{i}-\\mu_{t,i}(\\gamma_{t}))^{2}+\\sigma_{i}^{2}-\\sigma_{t,i}^{2}(\\gamma_{t})\\log\\sigma_{i}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the quantity $\\boldsymbol{r}_{t,i}$ is defined as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{r_{t,i}=\\frac{\\sigma_{t,i}^{2}}{\\sigma_{t}^{2}(\\gamma_{t})}=\\frac{\\sigma_{t,i}^{2}}{\\gamma_{t}^{2}\\sigma_{t,i}^{2}+(1-\\gamma_{t}^{2})\\sigma_{0}^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "which represents the relative change in the posterior variance due to the drift. In the exact stationary case, when $\\gamma_{t}=1$ , this ratio is $r_{t,i}=1$ while for $\\gamma_{t}<1$ , since typically $\\sigma_{t}^{2}<\\sigma_{0}^{2}$ , we have $r_{t,i}<1$ . This means that in the non-stationary case, the strength of the regularization in (38) in favor of the data term $\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0;I)}\\left[\\mathcal{L}_{t+1}(\\mu+\\epsilon\\sigma)\\right]$ , allowing the optimization to respond faster to the change in the data distribution. In practice, this data term is approximated via Monte-Carlo, i.e. ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0;I)}\\left[\\mathcal{L}_{t+1}(\\mu+\\epsilon\\sigma)\\right]\\sim\\frac{1}{M}\\sum_{i=1}^{M}\\mathcal{L}_{t+1}(\\mu+\\epsilon_{i}\\sigma)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "To find new parameters, $\\mu_{t+1}$ and $\\sigma_{t+1}$ , we let $\\mu_{t+1}^{0}\\,=\\,\\mu_{t}(\\gamma_{t})$ and $\\sigma_{t+1}^{0}\\,=\\,\\sigma_{t}(\\gamma_{t})$ and perform multiple updates on (38) ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mu_{t+1}^{k+1}=\\mu_{t+1}^{k}-\\alpha_{\\mu}\\hat{\\mathcal{F}}_{t}(\\mu_{t+1}^{k},\\sigma_{t+1}^{k},\\gamma_{t},\\{r t,i\\}_{i}),\\;\\;\\sigma_{t+1}^{k+1}=\\sigma_{t+1}^{k}-\\alpha_{\\sigma}\\hat{\\mathcal{F}}_{t}(\\mu_{t+1}^{k},\\sigma_{t+1}^{k},\\gamma_{t},\\{r t,i\\}_{i}),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\alpha_{\\mu}$ and $\\alpha_{\\sigma}$ are corresponding learning rates. The full algorithm of learning the drift parameters $\\gamma_{t}$ as well as learning the Bayesian Neural Network parameters using the procedure above is given in Algorithm 2. ", "page_idx": 31}, {"type": "text", "text": "Input: Data-stream ${\\cal S}_{T}=\\{(x_{t},y_{t})\\})_{t=1}^{T}$   \nNeural Network initial variance for every parameter $\\sigma_{0}^{2}$ coming from standard NN library   \nNN initializer $p_{i n i t}(\\theta)$   \nProximal cost $\\lambda\\geq0$ .   \nInitial prior variance rescaling $p\\in[0,1]$ .   \nInitial posterior variance rescaling $f\\in[0,1]$ .   \nLearning rate for the mean $\\alpha_{\\mu}$ and for the standard deviation $\\alpha_{\\sigma}$   \nNumber of gradient updates $K_{\\theta}$ to be applied on $\\mu$ and $\\sigma$   \nNumber of Monte-Carlo samples $M_{\\theta}$ for estimating $\\mu$ and $\\sigma$ in (39)   \nNumber of gradient updates $K_{\\gamma}$ on drift parameter $\\gamma_{t}$ in (10)   \nNumber of Monte-Carlo samples $M_{\\gamma}$ to estimate $\\gamma_{t}$ in (11)   \nLearning rate $\\eta_{\\gamma}$ for drift parameter   \nInitial drift parameters $\\gamma_{0}=1$ for every iteration.   \nInitialization:   \nInitialize NN parameters $\\theta_{0}\\sim p_{i n i t}(\\theta)$   \nInitialize prior distribution $p_{0}(\\theta)=\\mathcal{N}(\\theta;0;p^{2}\\sigma_{0}^{2})$ to be used for drift model (5).   \nInitialize posterior $q_{0}(\\theta)$ to be $\\mathcal{N}(\\theta;\\theta_{0};\\sigma_{i n i t}^{2})$ , where $\\sigma_{i n i t}^{2}=f^{2}p^{2}\\sigma_{0}^{2}$ .   \nfor step $t=0,1,2,\\dots,T\\,\\mathrm{{e}}$ do Current posterior $q_{t}=\\mathcal{N}(\\theta;\\mu_{t},\\sigma_{t}^{2})$ For $(x_{t+1},y_{t+1})$ , predict $\\hat{y}_{t+1}=f(x_{t+1}|\\mu_{t})$ with current posterior mean parameters $\\mu_{t}$ Compute performance metric based on $(y_{t+1},\\hat{y}_{t+1})$ Estimating the drift Initialize drift parameter $\\gamma_{t}^{0}=\\gamma_{0}$ . Compute $\\mu_{t}(\\gamma_{t})=\\gamma_{t}\\mu_{t}$ and $\\sigma^{2}(\\gamma_{t})=\\gamma_{t}^{2}\\sigma_{t}^{2}+(1-\\gamma_{t}^{2})\\sigma_{0}^{2}$ for $k=0,\\ldots,K_{\\gamma}-1$ do $\\begin{array}{r}{\\gamma_{t}^{k+1}=\\gamma_{t}^{k}\\eta_{\\gamma}\\nabla_{\\gamma}\\log\\frac{1}{M_{\\gamma}}\\sum_{i=1}^{M_{\\gamma}}p(y_{t+1}|x_{t+1},\\mu_{t}(\\gamma_{t}^{k})+\\epsilon_{i}\\sigma_{t}(\\gamma_{t}^{k}))}\\end{array}$ end for Updating variational posterior Let $\\mu_{t+1}^{0}\\stackrel{.}{=}\\gamma_{t}\\mu_{t},{\\sigma_{t+1}^{0}}\\stackrel{.}{=}\\sqrt{\\gamma_{t}^{2}\\sigma_{t}^{2}+(1-\\gamma_{t}^{2})\\sigma_{0}^{2}}$   \nLet $\\begin{array}{r}{r_{t,i}=\\frac{\\sigma_{t,i}^{2}}{\\sigma_{t}^{2}(\\gamma_{t})}=\\frac{\\sigma_{t,i}^{2}}{\\gamma_{t}^{2}\\sigma_{t,i}^{2}+(1-\\gamma_{t}^{2})\\sigma_{0}^{2}}}\\end{array}$ to be used in for $k=0,\\ldots,K_{\\theta}-1$ do $\\mu_{t+1}^{k+1}=\\mu_{t+1}^{k}-\\alpha_{\\mu}\\hat{\\mathcal{F}}_{t}(\\mu_{t+1}^{k},\\sigma_{t+1}^{k},\\gamma_{t},\\{r_{t,i}\\}_{i},\\lambda)$ $\\sigma_{t+1}^{k+1}=\\sigma_{t+1}^{k}-\\alpha_{\\sigma}\\hat{\\mathcal{F}}_{t}(\\mu_{t+1}^{k},\\sigma_{t+1}^{k},\\gamma_{t},\\{r_{t,i}\\}_{i},\\lambda)$ end for   \nend for ", "page_idx": 32}, {"type": "text", "text": "I.2 Modified SGD with drift model ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Instead of propagating the posterior (6), we do MAP updates on (4) with the prior $p_{0}(\\theta)~=$ $\\mathcal{N}(\\theta;\\mu_{0};\\sigma_{0}^{2})$ and the posterior $q_{t}(\\theta)\\,=\\,\\mathcal{N}(\\theta;\\theta_{t};s^{2}\\sigma_{0}^{2})$ , where $s\\leq1$ is hyperparameter controlling the variance $\\sigma_{t}^{2}$ of the posterior $q_{t}(\\theta)$ . Since fixed $s$ may not capture the true parameters variance, using Bayesian method (see Appendix I.1) is preferred but comes at a high computational cost. Instead of Bayesian update (33), we consider maximum a-posteriori (MAP) update ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\theta}\\log p(y_{t+1}|x_{t+1},\\theta)+\\log q_{t}(\\theta|\\gamma_{t}),\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "with $q_{t}(\\theta|\\gamma_{t})$ given by (29). Denoting $\\mathcal{L}_{t+1}(\\theta)\\,=\\,\\log p(y_{t+1}\\vert x_{t+1},\\theta)$ and using the definition of $q_{t}(\\theta|\\gamma_{t})$ , we get the following problem ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\theta}-\\mathcal{L}_{t+1}(\\theta)-\\sum_{i}\\lambda_{t,i}\\left[\\frac{(\\mu_{i}-\\mu_{t,i}(\\gamma_{t}))^{2}}{2\\sigma_{t,i}^{2}(\\gamma_{t})}\\right],\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where similarly to (36), we use a per-parameter temperature $\\lambda_{t,i}\\geq0$ . We choose temperature to be equal to ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\lambda_{t,i}=s^{2}\\sigma_{0,i}^{2}\\lambda,\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Input: Data-stream ${\\cal S}_{T}=\\{(x_{t},y_{t})\\})_{t=1}^{T}$   \nNeural Network (NN) initializing distribution $p_{i n i t}(\\theta)$ and specific initialization $\\theta_{0}\\sim p_{i n i t}(\\theta)$   \nLearning rate $\\alpha_{t}$ for parameters and $\\eta_{\\gamma}$ for drift parameters   \nNumber of gradient updates $K_{\\gamma}$ on drift parameter $\\gamma_{t}$   \nNumber of gradient updates $K_{\\theta}$ on NN parameters   \nProximal term cost $\\lambda\\geq0$   \nNN initial standard deviation (STD) scaling $p\\leq1$ (see (23)) and ratio $\\begin{array}{r}{s=\\frac{\\sigma_{t}}{p\\sigma_{0}}}\\end{array}$   \nfor step $t=0,1,2,\\ldots,T$ do For $(x_{t+1},y_{t+1})$ , predict $\\hat{y}_{t+1}=f(x_{t+1}|\\theta_{t})$ Compute performance metric based on $(y_{t+1},\\hat{y}_{t+1})$ Initialize drift parameter $\\gamma_{t}^{0}=1$ for step $k=0,1,2,\\ldots,\\bar{K_{\\gamma}}$ do Sample $\\theta_{0}^{\\prime}\\sim p_{i n i t}(\\theta)$ Stochastic update (21) on drift parameter using specific initialization (24) $\\begin{array}{r}{\\gamma_{t}^{k+1}=\\gamma_{t}^{k}+\\eta_{\\gamma}\\nabla_{\\gamma}\\left[\\log p(y_{t+1}|x_{t+1},\\gamma_{t}\\theta_{t}+(1-\\gamma_{t})\\theta_{0}+\\theta_{0}^{\\prime}p\\sqrt{1-\\gamma_{t}^{2}+\\gamma_{t}^{2}s^{2}})\\right]_{\\gamma_{t}=\\gamma_{t}^{k}}}\\end{array}$ end for Initialize $\\theta_{t+1}^{0}=\\theta_{t}(\\gamma_{t}^{K})$ with (42) and use $\\begin{array}{r}{\\alpha_{t}(\\gamma_{t}^{K})=\\alpha_{t}\\left((\\gamma_{t}^{i})^{2}+\\frac{1-(\\gamma_{t}^{i})^{2}}{s^{2}}\\right)}\\end{array}$ with (44) for step $k=0,1,2,\\ldots,K_{\\theta}$ do ${\\theta}_{t+1}^{k+1}={\\theta}_{t+1}^{k}-\\alpha_{t}(\\gamma_{t})\\circ\\nabla_{\\theta}G_{t+1}({\\theta}_{t+1}^{k};\\lambda)$ end for   \nend for ", "page_idx": 33}, {"type": "text", "text": "where $\\lambda$ is some constant. Such choice of temperature is motivated by the same logic as in (37) \u2013 it is a constant multiplied by the posterior variance $\\sigma_{t,i}^{2}=s^{2}\\sigma_{0,i}^{2}$ . With such choice of temperature, maximizing (40) is equivalent to minimizing ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{G(\\theta;\\lambda)=\\mathcal{L}_{t+1}(\\theta)+\\frac{\\lambda}{2}\\sum_{i=1}^{D}\\frac{|\\theta^{i}-\\theta_{t}^{i}(\\gamma_{t})|^{2}}{r_{t,i}(\\gamma)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the regularization target for the dimension $i$ is ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\theta_{t}^{i}(\\gamma_{t}^{i})=\\gamma_{t}^{i}\\theta_{t}^{i}+(1-\\gamma_{t}^{i})\\mu_{0}^{i}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and the constant $r_{t,i}(\\gamma)$ is given by ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{r_{t,i}(\\gamma_{t})=\\left((\\gamma_{t}^{i})^{2}+\\frac{1-(\\gamma_{t}^{i})^{2}}{s^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We can perform $K$ gradient updates (41) with a learning rate $\\alpha_{t}$ starting from $\\theta_{t+1}^{0}=\\theta_{t}(\\gamma_{t})$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\theta_{t+1}^{k+1}=\\theta_{t+1}^{k}-\\alpha_{t}(\\gamma_{t})\\circ\\nabla_{\\theta}G_{t+1}(\\theta_{t+1}^{k};\\lambda),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the vector-valued learning rate $\\alpha_{t}(\\gamma_{t})$ is given by ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\alpha_{t}(\\gamma_{t})=\\alpha_{t}r_{t,i}(\\gamma_{t})=\\alpha_{t}\\left((\\gamma_{t}^{i})^{2}+\\frac{1-(\\gamma_{t}^{i})^{2}}{s^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "with $\\alpha_{t}$ the base learning rate. Note that doing one update is equivalent to modified SGD method (19). Doing multiple updates on (43) allows us to perform multiple computations on the same data. The corresponding algorithm is given in Algorithm 3. ", "page_idx": 33}, {"type": "text", "text": "J Proof of linearisation ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Interpretation of $\\gamma_{t}$ . By linearising $\\log p(y_{t+1}|x_{t+1},\\theta)$ around $\\mu_{t}$ , we can simplify (8) to get ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{F}(\\gamma_{t})=(\\gamma_{t}\\odot\\mu_{t}+(1-\\gamma_{t})\\odot\\mu_{0})^{T}g_{t+1}-0.5(\\sigma_{t}^{2}(\\gamma_{t})\\odot g_{t+1})^{T}g_{t+1}-\\lambda\\sum_{i=1}^{K}(\\gamma_{t,i}-\\gamma_{t,i}^{0})^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\odot$ denotes elementwise product, $g_{t}=-\\nabla{\\mathcal{L}}_{t+1}(\\mu_{t})$ is the negative gradient of the loss (1) evaluated at $\\mu_{t}$ and we added the $\\ell_{2}$ -penalty $\\textstyle{\\frac{1}{2}}\\lambda(\\gamma_{t,i}-\\gamma_{t,i}^{0})^{2}$ to take into account the initialization. ", "page_idx": 33}, {"type": "text", "text": "Proof. We assume that the following linearisation is correct ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\log p(y_{t+1}|x_{t+1},\\theta)\\sim\\log p(y_{t+1}|x_{t+1},\\mu_{t})+g_{t+1}^{T}(\\theta-\\mu_{t}),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where ", "page_idx": 34}, {"type": "equation", "text": "$$\ng_{t+1}=-\\nabla_{\\theta}\\log p(y_{t+1}|x_{t+1},\\theta=\\mu_{t})=\\nabla_{\\theta}\\mathcal{L}_{t+1}(\\mu_{t})\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\np(y_{t+1}|x_{t+1},\\theta)\\sim p(y_{t+1}|x_{t+1},\\mu_{t})\\exp^{g_{t+1}^{T}(\\theta-\\mu_{t})}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Let\u2019s write the integral from (8) ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\log\\int p(y_{t+1}|x_{t+1},\\theta)\\exp^{-\\frac{1}{2}(\\theta-\\mu_{t}(\\gamma_{t}))^{T}\\Sigma_{t}^{-1}(\\gamma_{t})(\\theta-\\mu_{t}(\\gamma_{t}))}\\,d\\theta\\frac{1}{\\sqrt{(2\\pi)^{D}}|\\Sigma_{t}(\\gamma_{t})|}=}\\\\ &{}&{\\log\\int p(y_{t+1}|x_{t+1},\\mu_{t})\\exp^{g_{t+1}^{T}(\\theta-\\mu_{t})}\\exp^{-\\frac{1}{2}(\\theta-\\mu_{t}(\\gamma_{t}))^{T}\\Sigma_{t}^{-1}(\\gamma_{t})(\\theta-\\mu_{t}(\\gamma_{t}))}\\,d\\theta\\frac{1}{\\sqrt{(2\\pi)^{D}}|\\Sigma_{t}(\\gamma_{t})|}=}\\\\ &{}&{\\mathrm{sg}\\,p(y_{t+1}|x_{t+1},\\mu_{t})+\\log\\int\\exp^{g_{t+1}^{T}(\\theta-\\mu_{t})}\\exp^{-\\frac{1}{2}(\\theta-\\mu_{t}(\\gamma_{t}))^{T}\\Sigma_{t}^{-1}(\\gamma_{t})(\\theta-\\mu_{t}(\\gamma_{t}))}\\,d\\theta\\frac{1}{\\sqrt{(2\\pi)^{D}}|\\Sigma_{t}(\\gamma_{t})|}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Consider only the exp term inside the integral: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{g_{t+1}^{T}(\\theta-\\mu_{t})-\\frac{1}{2}(\\theta-\\mu_{t}(\\gamma_{t}))^{T}\\Sigma_{t}^{-1}(\\gamma_{t})(\\theta-\\mu_{t}(\\gamma_{t}))=}\\\\ &{}&{g_{t+1}^{T}\\theta-g_{t+1}^{T}\\mu_{t}-\\frac{1}{2}\\theta^{T}\\Sigma_{t}^{-1}(\\gamma_{t})\\theta+\\theta^{T}\\Sigma_{t}^{-1}(\\gamma_{t})\\mu_{t}(\\gamma_{t})-\\frac{1}{2}\\mu_{t}(\\gamma_{t})^{T}\\Sigma_{t}^{-1}(\\gamma_{t})\\mu_{t}(\\gamma_{t})=}\\\\ &{}&{-\\frac{1}{2}\\theta^{T}\\Sigma_{t}^{-1}\\theta+\\theta^{T}(\\Sigma_{t}^{-1}(\\gamma_{t})\\mu_{t}(\\gamma_{t})+g_{t+1})-g_{t+1}^{T}\\mu_{t}-\\frac{1}{2}\\mu_{t}(\\gamma_{t})^{T}\\Sigma_{t}^{-1}(\\gamma_{t})\\mu_{t}(\\gamma_{t})=}\\\\ &{}&{-\\frac{1}{2}\\left(\\theta^{T}\\Sigma_{t}^{-1}\\theta-2\\theta^{T}(\\Sigma_{t}^{-1}(\\gamma_{t})\\mu_{t}(\\gamma_{t})+g_{t+1})\\right)-g_{t+1}^{T}\\mu_{t}-\\frac{1}{2}\\mu_{t}(\\gamma_{t})^{T}\\Sigma_{t}^{-1}(\\gamma_{t})\\mu_{t}(\\gamma_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Let\u2019s focus on this term ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}&{}&{-{\\frac{1}{2}}\\left(\\theta^{T}\\Sigma_{t}^{-1}\\theta-2\\theta^{T}(\\Sigma_{t}^{-1}(\\gamma_{t})\\mu_{t}(\\gamma_{t})+g_{t+1})\\right)=}\\\\ {\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}&{}&{-{\\frac{1}{2}}\\left(\\theta^{T}\\Sigma_{t}^{-1}\\theta-2\\theta^{T}\\Sigma_{t}^{-1}b(\\gamma_{t})\\right)=}\\\\ {\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}&{}&{-{\\frac{1}{2}}\\left(\\theta^{T}\\Sigma_{t}^{-1}\\theta-2\\theta^{T}\\Sigma_{t}^{-1}b(\\gamma_{t})\\right)=}\\\\ {\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}&{}&{-{\\frac{1}{2}}\\left(\\theta^{T}\\Sigma_{t}^{-1}\\theta-2\\theta^{T}\\Sigma_{t}^{-1}b(\\gamma_{t})+b(\\gamma_{t})^{T}\\Sigma_{t}^{-1}b(\\gamma_{t})-b(\\gamma_{t})^{T}\\Sigma_{t}^{-1}b(\\gamma_{t})\\right)=}\\\\ {\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}&{}&{-{\\frac{1}{2}}\\left(\\theta^{T}\\Sigma_{t}^{-1}\\theta-2\\theta^{T}\\Sigma_{t}^{-1}b(\\gamma_{t})+b(\\gamma_{t})^{T}\\Sigma_{t}^{-1}b(\\gamma_{t})\\right)+{\\frac{1}{2}}b(\\gamma_{t})^{T}\\Sigma_{t}^{-1}b(\\gamma_{t})=}\\\\ {\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}&{}&{-{\\frac{1}{2}}(\\theta-b(\\gamma_{t}))^{T}\\Sigma_{t}^{-1}(\\theta-b(\\gamma_{t}))+{\\frac{1}{2}}b(\\gamma_{t})^{T}\\Sigma_{t}^{-1}b(\\gamma_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where ", "page_idx": 34}, {"type": "equation", "text": "$$\nb(\\gamma_{t})=\\Sigma_{t}(\\gamma_{t})\\left[\\Sigma_{t}^{-1}(\\gamma_{t})\\mu_{t}(\\gamma_{t})+g_{t+1}\\right]\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Therefore, the integral could be written as ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log\\int\\exp^{g_{t+1}^{T}(\\theta-\\mu_{t})}\\exp^{-\\frac{1}{2}(\\theta-\\mu_{t}(\\gamma_{t}))^{T}\\Sigma_{t}^{-1}(\\gamma_{t})(\\theta-\\mu_{t}(\\gamma_{t}))}d\\theta\\frac{1}{\\sqrt{(2\\pi)^{D}}|\\Sigma_{t}^{-1}(\\theta-\\mu_{t}(\\gamma_{t}))|^{2}},}\\\\ {{}^{r}\\Sigma_{t}^{-1}b(\\gamma_{t})+\\log\\int\\exp^{-\\frac{1}{2}(\\theta-b(\\gamma_{t}))^{T}\\Sigma_{t}^{-1}(\\theta-b(\\gamma_{t}))}d\\theta\\frac{1}{\\sqrt{(2\\pi)^{D}}|\\Sigma_{t}(\\gamma_{t})|}-g_{t+1}^{T}\\mu_{t}-\\frac{1}{2}\\mu_{t}(\\gamma_{t})^{T}\\Sigma_{t}^{-1}(\\gamma_{t}),}\\\\ {\\frac{1}{2}b(\\gamma_{t})^{T}\\Sigma_{t}^{-1}(\\gamma_{t})b(\\gamma_{t})-g_{t+1}^{T}\\mu_{t}-\\frac{1}{2}\\mu_{t}(\\gamma_{t})^{T}\\Sigma_{t}^{-1}(\\gamma_{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Now, we only keep the terms depending on $\\gamma_{t}$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\frac{1}{2}b(\\gamma_{t})^{T}\\Sigma_{t}^{-1}(\\gamma_{t})b(\\gamma_{t})-\\frac{1}{2}\\mu_{t}(\\gamma_{t})^{T}\\Sigma_{t}^{-1}(\\gamma_{t})\\mu_{t}(\\gamma_{t})=}\\\\ &{}&{\\frac{1}{2}\\left[\\Sigma_{t}^{-1}(\\gamma_{t})\\mu_{t}(\\gamma_{t})+g_{t+1}\\right]^{T}\\Sigma_{t}(\\gamma_{t})\\left[\\Sigma_{t}^{-1}(\\gamma_{t})\\mu_{t}(\\gamma_{t})+g_{t+1}\\right]-\\frac{1}{2}\\mu_{t}(\\gamma_{t})^{T}\\Sigma_{t}^{-1}(\\gamma_{t})\\mu_{t}(\\gamma_{t})=}\\\\ &{}&{\\frac{1}{2}\\mu_{t}(\\gamma_{t})^{T}\\Sigma_{t}^{-1}(\\gamma_{t})\\mu_{t}(\\gamma_{t})+g_{t+1}^{T}\\mu_{t}(\\gamma_{t})+g_{t+1}^{T}\\frac{1}{2}\\Sigma_{t}(\\gamma_{t})g_{t+1}-\\frac{1}{2}\\mu_{t}(\\gamma_{t})^{T}\\Sigma_{t}^{-1}(\\gamma_{t})\\mu_{t}(\\gamma_{t})=}\\\\ &{}&{g_{t+1}^{T}\\mu_{t}(\\gamma_{t})+\\frac{1}{2}g_{t+1}^{T}\\Sigma_{t}(\\gamma_{t})g_{t+1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Since $\\Sigma_{t}(\\gamma_{t})=d i a g(\\sigma_{t}^{2}\\gamma_{t}^{2}+(1-\\gamma_{t}^{2})\\sigma_{0}^{2})$ , we recover ", "page_idx": 35}, {"type": "equation", "text": "$$\ng_{t+1}^{T}\\big(\\gamma_{t}\\odot\\mu_{t}+(1-\\gamma_{t})\\odot\\mu_{0}\\big)+\\frac12g_{t+1}^{T}\\left((\\sigma_{t}^{2}\\gamma_{t}^{2}+(1-\\gamma_{t}^{2})\\sigma_{0}^{2}\\big)\\odot g_{t+1}\\right)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Now, we add an l2-penalty $\\frac{\\lambda}{2}\\vert\\vert\\gamma_{t}-\\gamma_{t,i}^{0}\\vert\\vert^{2}$ and we get ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\boldsymbol{\\mathcal{G}}(\\gamma_{t})=\\boldsymbol{g}_{t+1}^{T}\\big(\\gamma_{t}\\odot\\mu_{t}+(1-\\gamma_{t})\\odot\\mu_{0}\\big)+\\frac{1}{2}\\boldsymbol{g}_{t+1}^{T}\\left((\\sigma_{t}^{2}\\gamma_{t}^{2}+(1-\\gamma_{t}^{2})\\sigma_{0}^{2})\\odot\\boldsymbol{g}_{t+1}\\right)-\\frac{\\lambda}{2}||\\gamma_{t}-\\gamma_{t,i}^{0}||^{2}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Let\u2019s take the gradient wrt $\\gamma_{t}$ , we get ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\nabla F(\\gamma_{t})=g_{t+1}^{T}(\\mu_{t}-\\mu_{0})+g_{t+1}^{T}\\left((\\sigma_{t}^{2}\\gamma_{t}-\\sigma_{0}^{2}\\gamma_{t})\\odot g_{t+1}\\right)-\\lambda(\\gamma_{t}-\\gamma_{t,i}^{0})=}\\\\ &{}&{g_{t+1}^{T}(\\mu_{t}-\\mu_{0})+\\lambda\\gamma_{t,i}^{0}-\\gamma_{t}(\\lambda+g_{t+1}^{T}\\left((\\sigma_{0}^{2}-\\sigma_{t}^{2})\\odot g_{t+1}\\right)=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\gamma_{t}=\\frac{g_{t+1}^{T}(\\mu_{t}-\\mu_{0})+\\lambda\\gamma_{t,i}^{0}}{\\lambda+g_{t+1}^{T}\\left((\\sigma_{0}^{2}-\\sigma_{t}^{2}\\right)\\odot g_{t+1}\\right)}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "If $\\gamma_{t}$ is defined per parameter, this becomes ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\gamma_{t}=\\frac{g_{t+1}(\\mu_{t}-\\mu_{0})+\\lambda\\gamma_{t,i}^{0}}{\\lambda+g_{t+1}^{2}(\\sigma_{0}^{2}-\\sigma_{t}^{2})}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "K Toy illustrative example for SGD underperformance in the non-stationary regime ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Illustrative example of SGD on a non-stationary stream. We consider a toy problem of tracking a changing mean value. Let the observations in the stream $\\mathcal{S}_{t}$ follow $y_{t}=\\mu_{t}+\\sigma\\epsilon$ , where $\\epsilon\\sim\\mathcal{N}(0,\\bar{1})$ , $\\sigma=0.01$ . Every 50 timesteps the mean $\\mu_{t}$ switches from $-2$ to 2. We fit a 3-layer MLP with layer sizes $(10,5,1)$ and ReLU activations, using SGD with two different choices for the learning rate: $\\alpha=0.05$ and $\\alpha=0.15$ . Moreover, given that we know when a switch of the mean happens, we reset (or not reset) all the parameters at every switch as we run SGD. Only during the reset, we use different learning rate $\\beta=0.05$ or $\\beta=0.15$ . Using higher learning rate during reset allows SGD to learn faster from new data. We also ran SGD with $\\alpha=0.05$ and $\\beta=0.15$ , where the higher learning rate is used during task switch but we do not reset the parameters. We found that it performed the same as SGD with $\\alpha=0.05$ , which highlights the benefit of reset. ", "page_idx": 35}, {"type": "image", "img_path": "fDiZJ7mmOV/tmp/db337faf55a083e7921fc65530ebd8d7f62e823b43689fde475efe9cb77e5e61.jpg", "img_caption": ["Figure 24: Non-stationary mean tracking with SGD. "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "We report the predicted mean $\\hat{\\mu}_{t}$ for all SGD variants in Figure 24. We see that after the first switch of the mean, the SGD without reset takes more time to learn the new mean compared to the version with parameters reset. Increasing the learning rate speeds up the adaptation to new data, but it still remains slower during the mean change from 2 to $-2$ compared to the version that resets parameters. This example highlights that resets could be highly beneficial for improving the performance of SGD which could be slowed down by the implicit regularization towards the previous parameters $\\theta_{t}$ and the impact of the regularization strength induced by the learning rate. ", "page_idx": 35}, {"type": "text", "text": "L Using arbitrary drift models ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "L.1 Using arbitrary drift models ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "The approach described in section 3.5 provides a general strategy of incorporating arbitrary Gaussian drift models $p(\\theta|\\theta_{t};\\psi_{t})=\\mathcal{N}(\\theta;f(\\theta_{t};\\bar{\\psi}_{t});g^{2}(\\theta_{t};\\bar{\\psi}_{t}))$ which induces proximal optimization problem ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\theta_{t+1}=\\arg\\operatorname*{min}_{\\theta}\\mathcal{L}_{t+1}(\\theta)+\\frac{1}{2g(\\theta_{t};\\psi_{t})}||\\theta-f(\\theta_{t};\\psi_{t})||^{2}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The choice of $f(\\theta_{t};\\psi_{t})$ and $g(\\theta_{t};\\psi_{t})$ affects the behavior of the estimate $\\theta_{t+1}$ from (45) and ultimately depends on the problem in hand. The objective function of the form (45) was studied in context of online convex optimization in [21],[29], where the underlying algorithms estimated the deterministic drift model online. These worked demonstrated improved regret bounds depending on model estimation errors. This approach could also be used together with a Bayesian Neural Network (BNN). ", "page_idx": 36}]