[{"figure_path": "cFTi3gLJ1X/tables/tables_1_1.jpg", "caption": "Table 1: Preferable properties of a powerful monocular depth estimation model.", "description": "This table summarizes the desirable properties of an effective monocular depth estimation model.  It compares three models: Marigold, Depth Anything V1, and Depth Anything V2 (the authors' model).  For each model, it shows whether it exhibits each of the listed preferable properties. The properties include the ability to capture fine details, accurately estimate depth of transparent objects and reflections, handle complex scenes, and achieve both efficiency and transferability.", "section": "1 Introduction"}, {"figure_path": "cFTi3gLJ1X/tables/tables_7_1.jpg", "caption": "Table 2: Zero-shot relative depth estimation. Better: AbsRel\u2193, \u03b4\u2191. Solely from the metrics, Depth Anything V2 is better than MiDaS, but merely comparable with V1. But indeed, the focus and strengths of our V2 (e.g., fine-grained details, robust to complex layouts, transparent objects, etc.) cannot be correctly reflected on these benchmarks. Similar results (i.e., better model but worse score) are also observed in [7, 28].", "description": "This table presents a comparison of zero-shot relative depth estimation performance across different methods on five unseen test datasets (KITTI, NYU-D, Sintel, ETH3D, and DIODE).  It highlights the limitations of using standard metrics to fully capture the improvements achieved by Depth Anything V2, which excels in fine-grained details and robustness to complex scenes, aspects not fully reflected in the presented metrics.  Depth Anything V2 shows comparable or slightly better results than Depth Anything V1 and MiDaS V3.1 based on the given metrics.", "section": "Zero-Shot Relative Depth Estimation"}, {"figure_path": "cFTi3gLJ1X/tables/tables_7_2.jpg", "caption": "Table 3: Performance on our proposed DA-2K evaluation benchmark, which encompasses eight representative scenarios. Even our most lightweight model is superior to all other community models.", "description": "This table presents the accuracy results of different depth estimation models on the DA-2K benchmark.  The DA-2K benchmark is a newly proposed, more comprehensive and accurate evaluation benchmark that includes diverse scenarios not well-represented in previous benchmarks.  The table shows that Depth Anything V2 significantly outperforms existing state-of-the-art models, even with its smallest model.", "section": "7 Experiment"}, {"figure_path": "cFTi3gLJ1X/tables/tables_8_1.jpg", "caption": "Table 2: Zero-shot relative depth estimation. Better: AbsRel\u2193, \u03b41 \u2191. Solely from the metrics, Depth Anything V2 is better than MiDaS, but merely comparable with V1. But indeed, the focus and strengths of our V2 (e.g., fine-grained details, robust to complex layouts, transparent objects, etc.) cannot be correctly reflected on these benchmarks. Similar results (i.e., better model but worse score) are also observed in [7, 28].", "description": "This table presents the results of zero-shot relative depth estimation on several benchmark datasets (KITTI, NYU-D, Sintel, ETH3D, DIODE).  It compares the performance of Depth Anything V1, Depth Anything V2, and MiDaS V3.1.  While the quantitative metrics suggest that Depth Anything V2 is slightly better or comparable to existing methods, the authors emphasize that the table does not fully capture the qualitative improvements of Depth Anything V2, such as improved robustness to complex scenes and finer details.  They note that this limitation is common to several benchmark datasets.", "section": "Zero-Shot Relative Depth Estimation"}, {"figure_path": "cFTi3gLJ1X/tables/tables_8_2.jpg", "caption": "Table 2: Zero-shot relative depth estimation. Better: AbsRel\u2193, \u03b4\u2191. Solely from the metrics, Depth Anything V2 is better than MiDaS, but merely comparable with V1. But indeed, the focus and strengths of our V2 (e.g., fine-grained details, robust to complex layouts, transparent objects, etc.) cannot be correctly reflected on these benchmarks. Similar results (i.e., better model but worse score) are also observed in [7, 28].", "description": "This table presents the results of zero-shot relative depth estimation on several benchmark datasets (KITTI, NYU-D, Sintel, ETH3D, DIODE). It compares the performance of Depth Anything V2 with MiDaS V3.1 and Depth Anything V1.  While numerical results show Depth Anything V2 is slightly better or comparable to the others, the authors highlight that these metrics do not fully capture the improvements in fine-grained details and robustness to complex scenes which are key features of Depth Anything V2.", "section": "Zero-Shot Relative Depth Estimation"}, {"figure_path": "cFTi3gLJ1X/tables/tables_9_1.jpg", "caption": "Table 6: Comparison between originally manual label and our produced pseudo label on the DIML dataset [14]. Our produced pseudo labels are of much higher quality than the manual labels provided by DIML.", "description": "This table compares the performance of depth estimation models trained using manually labeled data from the DIML dataset versus models trained using pseudo labels generated by the authors' method.  The results demonstrate a significant improvement in performance when using pseudo labels, highlighting the effectiveness of the authors' approach in addressing the challenges of noisy manual labels in real-world datasets.", "section": "8 Related Work"}, {"figure_path": "cFTi3gLJ1X/tables/tables_17_1.jpg", "caption": "Table 7: Our training data sources.", "description": "This table lists the datasets used for training Depth Anything V2.  It's divided into two parts: Precise Synthetic Images and Pseudo-labeled Real Images.  The Precise Synthetic Images section shows five datasets used to create highly accurate depth labels, with the number of images in each dataset specified.  The Pseudo-labeled Real Images section shows eight large-scale datasets that were used to enhance the model's generalization ability, with pseudo labels generated by the model.  The table indicates whether each dataset contains indoor and/or outdoor scenes.", "section": "5.2 Details"}, {"figure_path": "cFTi3gLJ1X/tables/tables_17_2.jpg", "caption": "Table 2: Zero-shot relative depth estimation. Better: AbsRel\u2193, \u03b41 \u2191. Solely from the metrics, Depth Anything V2 is better than MiDaS, but merely comparable with V1. But indeed, the focus and strengths of our V2 (e.g., fine-grained details, robust to complex layouts, transparent objects, etc.) cannot be correctly reflected on these benchmarks. Similar results (i.e., better model but worse score) are also observed in [7, 28].", "description": "This table presents the results of a zero-shot relative depth estimation experiment.  It compares Depth Anything V2 against MiDaS V3.1 on several benchmark datasets (KITTI, NYU-D, Sintel, ETH3D, DIODE).  While quantitative metrics suggest Depth Anything V2 performs similarly to or slightly better than V1 and MiDaS, the authors highlight that these metrics don't fully capture the improvements in fine-grained details and robustness to complex scenes achieved by V2.", "section": "Zero-Shot Relative Depth Estimation"}, {"figure_path": "cFTi3gLJ1X/tables/tables_17_3.jpg", "caption": "Table 2: Zero-shot relative depth estimation. Better: AbsRel\u2193, \u03b41 \u2191. Solely from the metrics, Depth Anything V2 is better than MiDaS, but merely comparable with V1. But indeed, the focus and strengths of our V2 (e.g., fine-grained details, robust to complex layouts, transparent objects, etc.) cannot be correctly reflected on these benchmarks. Similar results (i.e., better model but worse score) are also observed in [7, 28].", "description": "This table presents zero-shot relative depth estimation results on several benchmarks (KITTI, NYU-D, Sintel, ETH3D, DIODE).  It compares Depth Anything V2 with MiDaS V3.1 and Depth Anything V1. While the metrics show Depth Anything V2 as slightly better than MiDaS and comparable to Depth Anything V1, the authors highlight that these metrics don't fully capture the improvements in fine-grained details and robustness to complex scenes that Depth Anything V2 offers.  They argue that the existing benchmarks are inadequate for fully evaluating the model's strengths.", "section": "Zero-Shot Relative Depth Estimation"}, {"figure_path": "cFTi3gLJ1X/tables/tables_18_1.jpg", "caption": "Table 2: Zero-shot relative depth estimation. Better: AbsRel\u2193, \u03b4\u2191. Solely from the metrics, Depth Anything V2 is better than MiDaS, but merely comparable with V1. But indeed, the focus and strengths of our V2 (e.g., fine-grained details, robust to complex layouts, transparent objects, etc.) cannot be correctly reflected on these benchmarks. Similar results (i.e., better model but worse score) are also observed in [7, 28].", "description": "This table presents the zero-shot relative depth estimation results on five unseen test datasets (KITTI, NYU-D, Sintel, ETH3D, DIODE) using different encoders (ViT-S, ViT-B, ViT-L, ViT-G).  The results are compared against MiDaS V3.1 and Depth Anything V1.  It highlights the limitations of using standard metrics to fully capture the improvements achieved in Depth Anything V2, particularly regarding fine-grained details and robustness to complex scenes.", "section": "Zero-Shot Relative Depth Estimation"}, {"figure_path": "cFTi3gLJ1X/tables/tables_18_2.jpg", "caption": "Table 2: Zero-shot relative depth estimation. Better: AbsRel\u2193, \u03b4\u2191. Solely from the metrics, Depth Anything V2 is better than MiDaS, but merely comparable with V1. But indeed, the focus and strengths of our V2 (e.g., fine-grained details, robust to complex layouts, transparent objects, etc.) cannot be correctly reflected on these benchmarks. Similar results (i.e., better model but worse score) are also observed in [7, 28].", "description": "This table compares the performance of Depth Anything V2 with other zero-shot relative depth estimation models on standard benchmarks such as KITTI, NYU-D, Sintel, ETH3D, and DIODE.  The metrics used are Absolute Relative Error (AbsRel) and \u03b41. While Depth Anything V2 shows competitive results according to these metrics, the authors emphasize that these metrics don't fully capture the model's advantages, especially its improved robustness and detail in complex scenes, compared to other models.", "section": "Zero-Shot Relative Depth Estimation"}, {"figure_path": "cFTi3gLJ1X/tables/tables_18_3.jpg", "caption": "Table 12: Results under different models and strategies in the NTIRE 2024 Transparent Surface Challenge [54].", "description": "This table shows the performance comparison of different models on the NTIRE 2024 Transparent Surface Challenge.  It compares the performance of MiDaS V3.1, Depth Anything V1, Depth Anything V2 (both zero-shot and with simple fine-tuning) and DINOv2 on the \u03b41 metric, showing Depth Anything V2 achieves the highest score with simple fine-tuning.", "section": "B.5 Performance on transparent or reflective surfaces"}, {"figure_path": "cFTi3gLJ1X/tables/tables_19_1.jpg", "caption": "Table 2: Zero-shot relative depth estimation. Better: AbsRel\u2193, \u03b41 \u2191. Solely from the metrics, Depth Anything V2 is better than MiDaS, but merely comparable with V1. But indeed, the focus and strengths of our V2 (e.g., fine-grained details, robust to complex layouts, transparent objects, etc.) cannot be correctly reflected on these benchmarks. Similar results (i.e., better model but worse score) are also observed in [7, 28].", "description": "This table presents the zero-shot relative depth estimation results on five unseen test datasets (KITTI, NYU-D, Sintel, ETH3D, and DIODE) for three different models: MiDaS V3.1, Depth Anything V1, and Depth Anything V2.  The results are evaluated using two metrics: AbsRel (absolute relative error) and \u03b41 (percentage of pixels with relative error less than 1.25).  Depth Anything V2 shows improved performance over MiDaS but only comparable performance to Depth Anything V1 based solely on these metrics. The authors highlight that the table does not fully capture the improvements in fine-grained details and robustness to complex scenes achieved by Depth Anything V2.", "section": "Zero-Shot Relative Depth Estimation"}, {"figure_path": "cFTi3gLJ1X/tables/tables_21_1.jpg", "caption": "Table 14: Per-scenario accuracy (%) of Depth Anything V2 on our proposed benchmark DA-2K.", "description": "This table presents the per-scenario accuracy of the Depth Anything V2 model on the DA-2K benchmark.  It shows the model's performance across different scenarios (Indoor, Outdoor, Non-real, Transparent, Adverse style, Aerial, Underwater, Object) and two encoders (with and without pseudo-labeled real images) for different model sizes (ViT-S, ViT-B, ViT-L). The mean accuracy across all scenarios is also provided for each model configuration. This allows for a detailed analysis of the model's strengths and weaknesses in various contexts.", "section": "C DA-2K Evaluation Benchmark"}, {"figure_path": "cFTi3gLJ1X/tables/tables_22_1.jpg", "caption": "Table 2: Zero-shot relative depth estimation. Better: AbsRel\u2193, \u03b41 \u2191. Solely from the metrics, Depth Anything V2 is better than MiDaS, but merely comparable with V1. But indeed, the focus and strengths of our V2 (e.g., fine-grained details, robust to complex layouts, transparent objects, etc.) cannot be correctly reflected on these benchmarks. Similar results (i.e., better model but worse score) are also observed in [7, 28].", "description": "This table presents a comparison of zero-shot relative depth estimation performance across different models on five unseen test datasets (KITTI, NYU-D, Sintel, ETH3D, DIODE).  It compares Depth Anything V1 and V2 against MiDaS V3.1, highlighting the limitations of using standard metrics to capture the improvements in fine-grained details and robustness offered by Depth Anything V2, especially for complex layouts and transparent objects.", "section": "Zero-Shot Relative Depth Estimation"}]