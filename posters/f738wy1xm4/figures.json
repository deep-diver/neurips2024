[{"figure_path": "F738WY1Xm4/figures/figures_1_1.jpg", "caption": "Figure 1: Training a deep linear network on a univariate regression task with quadratic loss. The weight matrices are initialized as Gaussian random variables, whose standard deviation is the x-axis of plots 1a and 1b. Experimental details are given in Appendix C.", "description": "This figure shows the results of training a deep linear network on a univariate regression task using different learning rates and initialization scales. Plot (a) displays the squared distance of the trained network to the optimal regressor, revealing a critical learning rate below which training succeeds, independent of the initialization scale. Plot (b) illustrates the sharpness (largest eigenvalue of the Hessian) at initialization and after training, highlighting that training is possible even when initializing beyond the 2/\u03b7 threshold. Plots (c) and (d) showcase the evolution of the squared distance to the optimal regressor and sharpness during training, demonstrating training dynamics under different conditions.", "section": "1 Introduction"}, {"figure_path": "F738WY1Xm4/figures/figures_1_2.jpg", "caption": "Figure 1: Training a deep linear network on a univariate regression task with quadratic loss. The weight matrices are initialized as Gaussian random variables, whose standard deviation is the x-axis of plots 1a and 1b. Experimental details are given in Appendix C.", "description": "This figure visualizes the training of a deep linear network on a univariate regression task using a quadratic loss function.  The weight matrices are initialized with Gaussian random variables.  The plots (a) and (b) show the relationship between the initialization scale (standard deviation of the Gaussian random variables) and the training results, showing the squared distance of the trained network from the optimal regressor and the sharpness (largest eigenvalue of the Hessian) at initialization and after training. Plots (c) and (d) illustrate the training dynamics (evolution of the squared distance and sharpness over training steps) for different learning rates to demonstrate the concept of the \"edge of stability\".", "section": "1 Introduction"}, {"figure_path": "F738WY1Xm4/figures/figures_2_1.jpg", "caption": "Figure 2: Squared distance of the trained network to the empirical risk minimizer, for various learning rates and depth. For each depth, learning succeeds if the learning rate is below a threshold, which corresponds to the theoretical value 2 ~ (||w*||32- La)\u2212\u00b9 of Theorem 1 (dashed vertical line).", "description": "This figure shows the relationship between the learning rate and the squared distance to the optimal regressor for various depths of the neural network. The dashed vertical lines indicate the theoretical threshold of the learning rate.  The results confirm that for a given depth, successful training occurs only when the learning rate is below the threshold.", "section": "To deepen our understanding of the training dynamics, we aim at quantifying the sharpness of the minimizer found by gradient descent (when it succeeds): we know that it has to be larger than Smin, but is it close to Smin or is it much larger?"}, {"figure_path": "F738WY1Xm4/figures/figures_43_1.jpg", "caption": "Figure 3: Probability of divergence of gradient descent for a Gaussian initialization of the weight matrices, depending on the initialization scale and the learning rate.", "description": "This figure shows the probability of divergence of the gradient descent algorithm for training deep linear networks for regression, using a Gaussian initialization of the weight matrices. The x-axis represents the scale of the initialization (standard deviation of Gaussian random variables), and the y-axis represents the probability of divergence.  Different lines correspond to different learning rates.  The plot reveals that for a fixed learning rate, there is a critical scale of initialization beyond which the network fails to learn (diverges). This critical scale depends on the learning rate; larger learning rates lead to divergence at smaller initialization scales.", "section": "Analysis of gradient flow from small initialization"}, {"figure_path": "F738WY1Xm4/figures/figures_44_1.jpg", "caption": "Figure 1: Training a deep linear network on a univariate regression task with quadratic loss. The weight matrices are initialized as Gaussian random variables, whose standard deviation is the x-axis of plots 1a and 1b. Experimental details are given in Appendix C.", "description": "The figure shows the results of training deep linear networks on a univariate regression task. Plots (a) and (b) show the relationship between the initialization scale (standard deviation of Gaussian random variables used for initialization) and the learning rate's effect on the training success and sharpness (largest eigenvalue of the Hessian) after training. Plot (c) shows a training run where the learning rate is below the critical value (no edge of stability), and (d) shows a run where the learning rate is above this value (edge of stability). The results illustrate the paper's main findings of implicit regularization towards flat minima and a linear relationship between depth and minimal sharpness.", "section": "Introduction"}, {"figure_path": "F738WY1Xm4/figures/figures_44_2.jpg", "caption": "Figure 3: Probability of divergence of gradient descent for a Gaussian initialization of the weight matrices, depending on the initialization scale and the learning rate.", "description": "This figure shows the probability that the gradient descent diverges during training of a deep linear network for regression. The x-axis represents the scale of the Gaussian initialization of the weight matrices, while the different lines represent different learning rates.  As the learning rate increases, the probability of divergence also increases, and for higher learning rates, divergence becomes more likely even with smaller initialization scales.  The plot highlights the impact of the interplay between learning rate and initialization scale on the success of training.", "section": "Analysis of gradient flow from small initialization"}, {"figure_path": "F738WY1Xm4/figures/figures_45_1.jpg", "caption": "Figure 6: Experiment with a deep linear network and a degenerate data covariance matrix, where the number of data n is less than the dimension d.", "description": "This figure shows the results of experiments performed on a deep linear network with a degenerate data covariance matrix (number of data points less than the dimension of the data).  Panel (a) displays how sharpness at initialization and after training varies with the initialization scale for different learning rates.  A key observation is a similar connection between learning rate, initialization scale, and sharpness to what was observed in the case of a full-rank data matrix.  Panel (b) illustrates the relationship between generalization performance (generalization gap) and sharpness after training, exhibiting a positive correlation (linear regression has a slope of 0.42).", "section": "Underdetermined regression and link to generalization"}, {"figure_path": "F738WY1Xm4/figures/figures_46_1.jpg", "caption": "Figure 1: Training a deep linear network on a univariate regression task with quadratic loss. The weight matrices are initialized as Gaussian random variables, whose standard deviation is the x-axis of plots 1a and 1b. Experimental details are given in Appendix C.", "description": "This figure shows the results of training deep linear networks on a univariate regression task. The x-axis represents the scale of the Gaussian initialization of the weight matrices (standard deviation).  Plot (a) displays the squared distance of the trained network to the optimal regressor for various learning rates and initialization scales. Plot (b) shows the sharpness (largest eigenvalue of the Hessian) at initialization and after training, again for various learning rates and initialization scales.  Plots (c) and (d) show the evolution during training of the squared distance to the optimal regressor and sharpness for specific learning rates and initialization scales. These plots illustrate the relationship between learning rate, initialization scale, and the sharpness of the resulting network minima. The plot helps explain the existence of a critical learning rate above which the network fails to train, a phenomenon that does not depend on the initial scale.", "section": "Introduction"}]