[{"figure_path": "axX62CQJpa/tables/tables_5_1.jpg", "caption": "Table 2: Results on VideoChatGPT benchmark [50].", "description": "This table presents the results of VideoStreaming and other state-of-the-art methods on the VideoChatGPT benchmark.  The benchmark evaluates video understanding models on five aspects: Correctness of Information (CI), Detailed Orientation (DO), Contextual Understanding (CU), Temporal Understanding (TU), and Consistency (CO).  Higher scores indicate better performance on each aspect.  The table shows that VideoStreaming achieves superior performance to other models, especially in temporal understanding.", "section": "4 Experiments"}, {"figure_path": "axX62CQJpa/tables/tables_6_1.jpg", "caption": "Table 3: Results on the fullset test split of EgoSchema [51].", "description": "This table presents the results of various methods on the EgoSchema benchmark's fullset test split.  The benchmark focuses on long-form video understanding.  The table compares the performance of several methods, showing their parameters (model size) and their performance score on the fullset.  The \"Ours\" row indicates the performance of the proposed VideoStreaming model.", "section": "4 Experiments"}, {"figure_path": "axX62CQJpa/tables/tables_6_2.jpg", "caption": "Table 4: Results on the validation set of Next-QA [79]. \u0421, T, D denotes causal, temporal and descriptive splits.", "description": "This table presents the results of several methods on the validation set of the Next-QA benchmark.  The benchmark consists of multiple-choice questions about videos, categorized into three types: causal, temporal, and descriptive.  The table shows the performance of each method across these categories and overall.", "section": "4 Experiments"}, {"figure_path": "axX62CQJpa/tables/tables_6_3.jpg", "caption": "Table 5: Results on Next-GQA [80]. Acc@GQA is defined as the percentage of questions that are both correctly answered and visually grounded with IoP > 0.5.", "description": "This table presents the results of different methods on the Next-GQA benchmark.  The metrics evaluated include mean Intersection over Prediction (mIoP), Intersection over Prediction at 0.5 threshold (IoP@0.5), mean Intersection over Union (mIoU), mIoU@0.5, and Accuracy at GQA (Acc@GQA). Acc@GQA specifically measures the percentage of questions correctly answered and visually grounded with an IoP above 0.5, indicating both accurate answers and precise visual grounding. The table compares the performance of various methods, including those with and without specialized grounding modules, highlighting the superior performance of the proposed VideoStreaming model.", "section": "4.2 Main Results"}, {"figure_path": "axX62CQJpa/tables/tables_7_1.jpg", "caption": "Table 7: Results on MovieNet-QA [68]. We present the used modality, the average number of tokens input to LLM and the average inference latency per question for comprehensive comparison.", "description": "This table presents a comparison of different methods on the MovieNet-QA benchmark.  It shows the performance of three different models, including LLaMA-VID, MovieLLM, and the proposed VideoStreaming model, in terms of overview, plot, and temporal understanding of long videos.  The table also provides information on whether text and vision modalities were used, the number of tokens processed, and the inference latency.  The results highlight the efficiency of the VideoStreaming model in processing and accurately understanding long videos.", "section": "4 Experiments"}, {"figure_path": "axX62CQJpa/tables/tables_8_1.jpg", "caption": "Table 8: Ablation studies on the effects of memory selection and historical memory in streaming encoding.", "description": "This table presents the ablation study results focusing on the impact of memory selection and historical memory within the streaming encoding process of the VideoStreaming model. It shows the performance (Fullset, Global Acc., Break. Acc.) under different configurations: with/without propagated memory and with/without adaptive memory selection.  The results highlight the contribution of each component to the overall performance of the model on long video understanding tasks.", "section": "3.4 Short-to-Long Training"}, {"figure_path": "axX62CQJpa/tables/tables_8_2.jpg", "caption": "Table 9: Ablation studies on clip- and frame-based sampling strategy.", "description": "This table presents the results of ablation studies comparing two sampling strategies for video processing: clip-based sampling and frame-based sampling.  The results are evaluated on two metrics: Accuracy (Acc.) and the number of Frames used in each method.  The comparison is performed on two datasets, Fullset and MovieNet, to assess the impact of sampling on performance across different video lengths and complexities.", "section": "4. Experiments"}, {"figure_path": "axX62CQJpa/tables/tables_9_1.jpg", "caption": "Table 10: Ablation studies on the streaming encoder architecture.", "description": "This table presents the results of an ablation study on the streaming encoder architecture, comparing different encoder models (MC-ViT, Phi, Vicuna) with varying numbers of layers and parameters. The performance is evaluated on three metrics: Fullset, Next-QA, and MovieChat-1K's global and breakpoint accuracy.  The results show how the choice of encoder model and its complexity affect the performance on the various video understanding tasks.", "section": "4.3 Ablation Study"}, {"figure_path": "axX62CQJpa/tables/tables_15_1.jpg", "caption": "Table 11: Results on the test set of IntentQA [41].", "description": "This table presents the performance comparison of different methods on the IntentQA dataset.  IntentQA is a long-form video understanding dataset consisting of 4.3K videos with 16K multiple-choice questions categorized into three types: why, how, and before/after.  The table shows the accuracy of each method for each question type and overall accuracy.  The results highlight the performance of the proposed VideoStreaming model in comparison to several other recent state-of-the-art models.", "section": "4 Experiments"}, {"figure_path": "axX62CQJpa/tables/tables_17_1.jpg", "caption": "Table 13: Ablation study on the number of summarization tokens and selected timestamps. We report the results on EgoSchema [51] and Next-GQA [80].", "description": "This table presents the results of ablation studies conducted to analyze the impact of varying the number of summarization tokens (P) and the number of selected timestamps (V) on the model's performance.  The studies were performed on two benchmark datasets: EgoSchema [51] and Next-GQA [80]. The table shows how changes in these parameters affect the model's performance, measured by the Fullset and Acc@GQA metrics. Different combinations of P and V were tested, resulting in varying numbers of tokens used as input to the Language Model (LLM). The results reveal the optimal balance between the number of summarization tokens, selected timestamps, and overall model performance.", "section": "4.3 Ablation Study"}, {"figure_path": "axX62CQJpa/tables/tables_17_2.jpg", "caption": "Table 14: Ablation study on the formulation of time prompts. We report the results on EgoSchema [51], Next-GQA [80] and MovieChat-1K [66].", "description": "This table presents the ablation study results on different prompt formulations in the VideoStreaming model.  It compares the model's performance across four scenarios: no prompt, using only the current clip's timestamp in the prompt, using only the historical memory's timestamps, and using both the current clip and historical memory timestamps. The results are presented for three different metrics: Fullset accuracy, Acc@GQA, and breakpoint accuracy for EgoSchema, Next-GQA and MovieChat-1K benchmarks.", "section": "4. Experiments"}, {"figure_path": "axX62CQJpa/tables/tables_17_3.jpg", "caption": "Table 15: Ablation study on the similarity measurement. We report the results on EgoSchema [51], Next-GQA [80] and MovieChat-1K [66]", "description": "This table presents the ablation study comparing two different similarity measurements used in the adaptive memory selection process: cosine similarity and dot product.  The results are reported across three different benchmarks: EgoSchema, Next-GQA, and MovieChat-1K.  Each benchmark measures different aspects of video understanding.  The table shows the impact of the choice of similarity metric on the overall performance. ", "section": "4.3 Ablation Study"}, {"figure_path": "axX62CQJpa/tables/tables_18_1.jpg", "caption": "Table 16: Ablation study of different settings, including memory propagation, temporal selection, the number of summarization tokens and the number of selected clips, on hour-long MovieNet-QA benchmark from three perspectives.", "description": "This table presents the ablation study results focusing on four key aspects of the VideoStreaming model on the MovieNet-QA benchmark, which contains hour-long videos.  The aspects studied are memory propagation, temporal selection, the number of summarization tokens (P), and the number of selected clips (V). For each configuration, the table shows the performance across three perspectives: overview, plot, and temporal understanding. This helps analyze the individual and combined effects of these design choices on the model's overall performance in handling long videos.", "section": "4.2 Main Results"}]