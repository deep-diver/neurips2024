[{"figure_path": "axX62CQJpa/figures/figures_3_1.jpg", "caption": "Figure 1: Fig. 1a shows an overview of VideoStreaming, where we segment a long video into short clips and iteratively encode each clip into compact memories. Then, according to specific questions, we select a constant number of subsets of relevant memories as input to an LLM to produce responses. The \n and X respectively denote selected and unselected memories. Fig. 1b illustrates the detailed process of each streaming encoding iteration. We encode current clip features with reference to specific timestamps and historical memory from the preceding clip into a condensed representation.", "description": "This figure illustrates the VideoStreaming framework.  Part (a) shows the overall architecture: a long video is segmented into short clips, each encoded into a compact memory representation.  For a given question, relevant memory subsets are selected and fed to a large language model (LLM) to generate the answer. Part (b) details a single iteration of the streaming encoding process, showing how current clip features, historical memory from the previous clip, and a time prompt are combined to create an updated memory representation for the current time step.", "section": "3 VideoStreaming"}, {"figure_path": "axX62CQJpa/figures/figures_3_2.jpg", "caption": "Figure 1: Fig. 1a shows an overview of VideoStreaming, where we segment a long video into short clips and iteratively encode each clip into compact memories. Then, according to specific questions, we select a constant number of subsets of relevant memories as input to an LLM to produce responses. The \u0124 and X respectively denote selected and unselected memories. Fig. 1b illustrates the detailed process of each streaming encoding iteration. We encode current clip features with reference to specific timestamps and historical memory from the preceding clip into a condensed representation.", "description": "This figure illustrates the VideoStreaming framework.  (a) shows the overall process: a long video is segmented into short clips, each encoded into a compact memory representation.  Based on a question, relevant memory subsets are selected and fed to a Large Language Model (LLM) for answering. (b) details a single iteration of the streaming encoding process, showing how the current clip features are combined with the previous clip's memory and fed to a small language model. The output is a new condensed memory representing the video up to that point, and a clip indicator token for later memory selection.", "section": "3 VideoStreaming"}, {"figure_path": "axX62CQJpa/figures/figures_7_1.jpg", "caption": "Figure 1: Fig. 1a shows an overview of VideoStreaming, where we segment a long video into short clips and iteratively encode each clip into compact memories. Then, according to specific questions, we select a constant number of subsets of relevant memories as input to an LLM to produce responses. The  and X respectively denote selected and unselected memories. Fig. 1b illustrates the detailed process of each streaming encoding iteration. We encode current clip features with reference to specific timestamps and historical memory from the preceding clip into a condensed representation.", "description": "This figure provides a high-level overview of the VideoStreaming framework.  (a) shows the overall process:  a long video is segmented into clips, each clip is encoded into a compact memory representation, and a subset of relevant memories is selected based on the question and fed into an LLM for response generation. (b) zooms into a single iteration of the streaming encoding process, highlighting the integration of current clip features, historical memory from the previous clip, and a summarization token to create a condensed representation of the video content up to that point.", "section": "3 VideoStreaming"}, {"figure_path": "axX62CQJpa/figures/figures_8_1.jpg", "caption": "Figure 1: Fig. 1a shows an overview of VideoStreaming, where we segment a long video into short clips and iteratively encode each clip into compact memories. Then, according to specific questions, we select a constant number of subsets of relevant memories as input to an LLM to produce responses. The \u0124 and X respectively denote selected and unselected memories. Fig. 1b illustrates the detailed process of each streaming encoding iteration. We encode current clip features with reference to specific timestamps and historical memory from the preceding clip into a condensed representation.", "description": "This figure shows an overview of the VideoStreaming framework.  Part (a) illustrates the system's pipeline: long videos are segmented into short clips, each encoded into a compact memory representation.  Based on a user's question, a subset of these memories are selected and fed to a large language model (LLM) for answering.  Part (b) details a single iteration of the streaming encoding process, showing how the current clip's features are combined with historical memory and fed to a smaller language model to produce a condensed representation.", "section": "3 VideoStreaming"}, {"figure_path": "axX62CQJpa/figures/figures_8_2.jpg", "caption": "Figure 1: Fig. 1a shows an overview of VideoStreaming, where we segment a long video into short clips and iteratively encode each clip into compact memories. Then, according to specific questions, we select a constant number of subsets of relevant memories as input to an LLM to produce responses. The \u0124 and X respectively denote selected and unselected memories. Fig. 1b illustrates the detailed process of each streaming encoding iteration. We encode current clip features with reference to specific timestamps and historical memory from the preceding clip into a condensed representation.", "description": "This figure illustrates the VideoStreaming framework.  (a) shows an overview of the system, segmenting a long video into short clips, iteratively encoding them into compact memories, and then selecting relevant memories based on the question to feed into an LLM for response generation. (b) details a single iteration of the streaming encoding process, showing how current clip features are encoded with reference to preceding clip's encoded results (historical memory), specific timestamps, and the current clip's summarization tokens. The result is a condensed representation of the video up to that point.", "section": "3 VideoStreaming"}, {"figure_path": "axX62CQJpa/figures/figures_15_1.jpg", "caption": "Figure 1: Fig. 1a shows an overview of VideoStreaming, where we segment a long video into short clips and iteratively encode each clip into compact memories. Then, according to specific questions, we select a constant number of subsets of relevant memories as input to an LLM to produce responses. The \u0124 and X respectively denote selected and unselected memories. Fig. 1b illustrates the detailed process of each streaming encoding iteration. We encode current clip features with reference to specific timestamps and historical memory from the preceding clip into a condensed representation.", "description": "This figure shows an overview of the VideoStreaming framework.  (a) illustrates the process of segmenting a long video into short clips, iteratively encoding each clip into a compact memory representation, and then selecting relevant memories based on specific questions to feed into a large language model (LLM) for generating responses. (b) provides a detailed breakdown of the streaming encoding process for a single clip, showing how current clip features, historical memory from previous clips, timestamps, and summarization tokens are combined to produce a condensed representation.", "section": "3 VideoStreaming"}, {"figure_path": "axX62CQJpa/figures/figures_16_1.jpg", "caption": "Figure 1: Fig. 1a shows an overview of VideoStreaming, where we segment a long video into short clips and iteratively encode each clip into compact memories. Then, according to specific questions, we select a constant number of subsets of relevant memories as input to an LLM to produce responses. The  and X respectively denote selected and unselected memories. Fig. 1b illustrates the detailed process of each streaming encoding iteration. We encode current clip features with reference to specific timestamps and historical memory from the preceding clip into a condensed representation.", "description": "This figure illustrates the VideoStreaming framework.  (a) shows the overall process: a long video is segmented into short clips, each encoded into a compact memory representation. Relevant memories are selected based on the question and fed to an LLM. (b) details one iteration of the streaming encoding, showing how current clip features are combined with historical memory and fed into a small language model to produce a condensed representation.", "section": "3 VideoStreaming"}, {"figure_path": "axX62CQJpa/figures/figures_16_2.jpg", "caption": "Figure 1: Fig. 1a shows an overview of VideoStreaming, where we segment a long video into short clips and iteratively encode each clip into compact memories. Then, according to specific questions, we select a constant number of subsets of relevant memories as input to an LLM to produce responses. The  and X respectively denote selected and unselected memories. Fig. 1b illustrates the detailed process of each streaming encoding iteration. We encode current clip features with reference to specific timestamps and historical memory from the preceding clip into a condensed representation.", "description": "This figure shows the VideoStreaming framework.  (a) provides a high-level overview of the process: long videos are segmented into short clips, which are encoded into compact memories.  A language model (LLM) then uses a subset of these memories (selected based on the question) to generate an answer. (b) zooms in on a single iteration of the streaming encoding process, showing how current clip features, historical memory, and a summarization token are used to create a condensed representation of the video content up to that point.", "section": "3 VideoStreaming"}]