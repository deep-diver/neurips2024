{"references": [{"fullname_first_author": "Mohammad Gheshlaghi Azar", "paper_title": "Minimax regret bounds for reinforcement learning", "publication_date": "2017-00-00", "reason": "This paper provides foundational theoretical results on minimax regret bounds for reinforcement learning, which are highly relevant to the analysis of the proposed algorithms in this paper."}, {"fullname_first_author": "Arnaud Robert", "paper_title": "Sample complexity of goal-conditioned hierarchical reinforcement learning", "publication_date": "2023-00-00", "reason": "This paper studies sample complexity in goal-conditioned HRL, directly addressing a key theoretical question related to scalable oversight in the context of hierarchical reinforcement learning."}, {"fullname_first_author": "Zheng Wen", "paper_title": "On efficiency in hierarchical reinforcement learning", "publication_date": "2020-00-00", "reason": "This paper analyzes the efficiency of hierarchical reinforcement learning, offering insights into the sample complexity of goal-conditioned HRL and its relationship to the structure of the problem."}, {"fullname_first_author": "Wenhao Zhan", "paper_title": "Provable reward-agnostic preference-based reinforcement learning", "publication_date": "2023-00-00", "reason": "This paper develops a preference-based reinforcement learning algorithm with provable guarantees, which is highly relevant to the ordinal feedback setting considered in this paper."}, {"fullname_first_author": "Aldo Pacchiano", "paper_title": "Dueling RL: reinforcement learning with trajectory preferences", "publication_date": "2021-00-00", "reason": "This paper introduces a novel framework for reinforcement learning based on trajectory preferences, which is relevant to the approach in this paper for learning from ordinal feedback."}]}