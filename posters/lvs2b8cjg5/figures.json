[{"figure_path": "lvS2b8CjG5/figures/figures_3_1.jpg", "caption": "Figure 1: The EEGPT structure involves patching the input EEG signal as p<sub>i,j</sub> through masking (50% time and 80% channel patches), creating masked part M and unmasked part M and then embedding as token<sub>i,j</sub> by local spatio-temporal embedding. The encoder processes the masked part, extracting features (enc<sub>j</sub>) consisting of {e<sup>j</sup>}<sub>i=1</sub> for each time segment in the M part with summary tokens {s<sup>j</sup>}<sub>i=1</sub>. The predictor predicts features (pred<sub>j</sub>) for all time segments, aligning with the Momentum Encoder output (menc<sub>j</sub>). Based on features extracted by the predictor and encoder, the reconstructor generates rec<sub>i,j</sub> to reconstruct the EEG signal of the M part.", "description": "This figure shows the architecture of the EEGPT model. The input EEG signal is first divided into patches and embedded. Then, the patches are split into masked and unmasked parts. The encoder processes the masked parts to extract features, while the predictor predicts the features for the unmasked parts. The Momentum Encoder maintains a moving average of the encoder's parameters. The reconstructor reconstructs the EEG signal from the predicted and encoded features. The alignment loss and reconstruction loss are used for training the model.", "section": "EEG Pretrained Transformer (EEGPT)"}, {"figure_path": "lvS2b8CjG5/figures/figures_4_1.jpg", "caption": "Figure 2: Illustration of local spatio-temporal embedding. The EEG signal is divided into equally sized patches in the spatio-temporal dimensions. Each patch represents a time segment for a specific channel without overlap. The patches are linearly embedded and incorporated with channel embedding information to obtain a corresponding feature.", "description": "The figure illustrates the process of local spatio-temporal embedding in the EEGPT model.  The EEG signal is divided into small patches across both time and channels. Each patch then undergoes linear embedding and is combined with channel-specific embedding information, resulting in a feature vector that captures both spatial (channel) and temporal information within that patch. This is a crucial step in EEGPT as it transforms the raw EEG into a format suitable for processing by the transformer model.", "section": "2.3 Local Spatio-Temporal Embedding"}, {"figure_path": "lvS2b8CjG5/figures/figures_5_1.jpg", "caption": "Figure 1: The EEGPT structure involves patching the input EEG signal as pi,j through masking (50% time and 80% channel patches), creating masked part M and unmasked part M and then embedding as tokeni,j by local spatio-temporal embedding. The encoder processes the masked part, extracting features (encj) consisting of {e}=1 for each time segment in the M part with summary tokens {$i}1. The predictor predicts features (predj) for all time segments, aligning with the Momentum Encoder output (mencj). Based on features extracted by the predictor and encoder, the reconstructor generates reci,j to reconstruct the EEG signal of the M part.", "description": "This figure illustrates the architecture of the EEGPT model.  The input EEG signal is first divided into patches and embedded. Then, a masking process creates masked and unmasked portions. The masked part is processed by the encoder to extract features (encj), while the predictor predicts features (predj) for the whole signal, aligning them with the momentum encoder output. Finally, the reconstructor uses these features to reconstruct the original masked EEG signal, creating a dual self-supervised learning task.", "section": "2 Method"}, {"figure_path": "lvS2b8CjG5/figures/figures_8_1.jpg", "caption": "Figure 1: The EEGPT structure involves patching the input EEG signal as pi,j through masking (50% time and 80% channel patches), creating masked part M and unmasked part M and then embedding as tokeni,j by local spatio-temporal embedding. The encoder processes the masked part, extracting features (encj) consisting of {e}i=1 for each time segment in the M part with summary tokens {s}i=1. The predictor predicts features (predj) for all time segments, aligning with the Momentum Encoder output (mencj). Based on features extracted by the predictor and encoder, the reconstructor generates reci,j to reconstruct the EEG signal of the M part.", "description": "The figure shows the architecture of the EEGPT model.  The input EEG signal is divided into patches, and then a masking process is performed, separating patches into masked and unmasked parts.  The masked patches are fed into an encoder, which extracts features, while the unmasked patches are used in a reconstruction task. A predictor network predicts the features of the masked patches, which are aligned with the output of a momentum encoder. Finally, a reconstructor uses both the predicted features and the encoder's output to reconstruct the original EEG signal of the masked patches. This dual self-supervised learning approach is key to the model's performance.", "section": "2 Method"}, {"figure_path": "lvS2b8CjG5/figures/figures_13_1.jpg", "caption": "Figure 1: The EEGPT structure involves patching the input EEG signal as  through masking (50% time and 80% channel patches), creating masked part  and unmasked part  and then embedding as  by local spatio-temporal embedding. The encoder processes the masked part, extracting features ( ) consisting of  for each time segment in the  part with summary tokens  . The predictor predicts features ( ) for all time segments, aligning with the Momentum Encoder output ( ). Based on features extracted by the predictor and encoder, the reconstructor generates  to reconstruct the EEG signal of the  part.", "description": "The figure illustrates the architecture of the EEGPT model, a transformer-based model for EEG feature extraction. The input EEG signal is first patched and split into masked and unmasked parts.  The masked parts are processed by an encoder, which extracts features. A predictor then predicts features for the whole signal and these are aligned with the output of a momentum encoder. Finally, a reconstructor uses the encoder and predictor outputs to reconstruct the masked parts of the EEG signal.  The whole process is a dual self-supervised learning approach with spatio-temporal representation alignment.", "section": "2 Method"}, {"figure_path": "lvS2b8CjG5/figures/figures_14_1.jpg", "caption": "Figure 5: Scaling laws with EEGPT parameter size N. Axes are all on a logarithmic scale.", "description": "This figure shows the scaling laws observed when varying the model size (number of parameters) of the EEGPT model.  The x-axis represents the number of parameters (on a logarithmic scale), and the y-axis shows two metrics: test accuracy and test loss (also on logarithmic scales).  The plot demonstrates a positive correlation between model size and performance, indicating that larger models generally achieve higher accuracy and lower loss. The lines represent the trendlines fitted to the data, illustrating the relationship between model size and these performance metrics.", "section": "3.5 Pretrain Experiment Results"}, {"figure_path": "lvS2b8CjG5/figures/figures_14_2.jpg", "caption": "Figure 5: Scaling laws with EEGPT parameter size N. Axes are all on a logarithmic scale.", "description": "This figure shows the scaling laws observed when training the EEGPT model with varying parameter sizes.  The x-axis represents the number of parameters in the model (log scale), while the y-axis shows both test accuracy and test loss (log scale).  The plot demonstrates a positive correlation between model size and performance, indicating that larger models generally achieve higher accuracy and lower loss.  The lines represent regression fits to the data, and the shaded regions represent confidence intervals.  This figure supports the claim that the model scales well with increasing size, leading to improved performance.", "section": "3.5 Pretrain Experiment Results"}, {"figure_path": "lvS2b8CjG5/figures/figures_15_1.jpg", "caption": "Figure 5: Scaling laws with EEGPT parameter size N. Axes are all on a logarithmic scale.", "description": "This figure shows the scaling laws observed when varying the EEGPT model's parameter size (N).  The x-axis represents the parameter size on a logarithmic scale, while the y-axis shows both test accuracy and test loss, also on a logarithmic scale. The plot shows that as the model size increases, the test accuracy improves and test loss decreases, following clear scaling trends.  These trends are quantified with the equations provided in the caption, demonstrating the relationship between model size and performance.", "section": "3.5 Pretrain Experiment Results"}, {"figure_path": "lvS2b8CjG5/figures/figures_15_2.jpg", "caption": "Figure 5: Scaling laws with EEGPT parameter size N. Axes are all on a logarithmic scale.", "description": "This figure shows the scaling laws observed during experiments on the effect of model size on EEGPT's performance.  The x-axis represents the number of parameters (model size) on a logarithmic scale, and the y-axis shows both test accuracy and test loss, also on a logarithmic scale. The lines represent the fitted curves from the experiments. This visualization demonstrates the relationship between model size and both the model's accuracy and loss, suggesting that larger models generally lead to higher accuracy but also that larger models do not always reduce loss.  The data points with error bars show that larger models yield better performance in downstream tasks.", "section": "3.5 Pretrain Experiment Results"}, {"figure_path": "lvS2b8CjG5/figures/figures_22_1.jpg", "caption": "Figure 1: The EEGPT structure involves patching the input EEG signal as pi,j through masking (50% time and 80% channel patches), creating masked part M and unmasked part M and then embedding as tokeni,j by local spatio-temporal embedding. The encoder processes the masked part, extracting features (encj) consisting of {e}i=1 for each time segment in the M part with summary tokens {s}i=1. The predictor predicts features (predj) for all time segments, aligning with the Momentum Encoder output (mencj). Based on features extracted by the predictor and encoder, the reconstructor generates reci,j to reconstruct the EEG signal of the M part.", "description": "This figure illustrates the architecture of the EEGPT model, a pretrained transformer designed for universal and reliable EEG signal representation. The model processes EEG signals by first patching and embedding them, then applying a masking strategy to create masked and unmasked parts. The masked part is fed into an encoder to extract spatio-temporal features which are then aligned with predictions from a predictor. Finally, a reconstructor utilizes both encoder and predictor outputs to reconstruct the original EEG signal, fostering dual self-supervised learning.", "section": "2 Method"}, {"figure_path": "lvS2b8CjG5/figures/figures_22_2.jpg", "caption": "Figure 1: The EEGPT structure involves patching the input EEG signal as pi,j through masking (50% time and 80% channel patches), creating masked part M and unmasked part M and then embedding as tokeni,j by local spatio-temporal embedding. The encoder processes the masked part, extracting features (encj) consisting of {e}i=1 for each time segment in the M part with summary tokens {s}i=1. The predictor predicts features (predj) for all time segments, aligning with the Momentum Encoder output (mencj). Based on features extracted by the predictor and encoder, the reconstructor generates reci,j to reconstruct the EEG signal of the M part.", "description": "This figure illustrates the architecture of the EEGPT model, showing the process of patching, embedding, masking, encoding, predicting and reconstructing EEG signals using the dual self-supervised learning method. The model takes EEG signals as input, processes them through the encoder and predictor, and then uses a reconstructor to generate reconstructed signals.  The spatio-temporal representation alignment and the mask-based reconstruction are also illustrated.", "section": "2 Method"}, {"figure_path": "lvS2b8CjG5/figures/figures_23_1.jpg", "caption": "Figure 13: BCIC2A Channel and Class Pearson Correlation Diagram", "description": "The figure shows the correlation between channels and motor imagery classes detected using the channel perturbation method after training on the BCIC2A dataset. Gaussian multiplicative random noise is randomly added to the signal amplitude of each channel, and the Pearson correlation between the noise intensity and changes in the corresponding class logits is calculated and presented as a heatmap.  Symmetric relationships are observed for electrodes related to left and right hand movements, with bilateral electrodes corresponding to foot movements, and distinct channels corresponding to the four different classes.", "section": "G.2 BCIC2A Experiment Results Visualization"}, {"figure_path": "lvS2b8CjG5/figures/figures_23_2.jpg", "caption": "Figure 1: The EEGPT structure involves patching the input EEG signal as p<sub>i,j</sub> through masking (50% time and 80% channel patches), creating masked part M and unmasked part M and then embedding as token<sub>i,j</sub> by local spatio-temporal embedding. The encoder processes the masked part, extracting features (enc<sub>j</sub>) consisting of {e}<sup>M</sup><sub>i=1</sub> for each time segment in the M part with summary tokens {s}<sup>S</sup><sub>i=1</sub>. The predictor predicts features (pred<sub>j</sub>) for all time segments, aligning with the Momentum Encoder output (menc<sub>j</sub>). Based on features extracted by the predictor and encoder, the reconstructor generates rec<sub>i,j</sub> to reconstruct the EEG signal of the M part.", "description": "This figure illustrates the architecture of the proposed EEGPT model.  It shows how the input EEG signal is processed through patching, embedding, masking, and then fed into an encoder, predictor, and reconstructor. The encoder processes masked patches to extract features, the predictor predicts features aligning with the momentum encoder output, and the reconstructor uses these features to reconstruct the masked parts of the signal. The overall process is designed for efficient and effective feature extraction from EEG signals.", "section": "2 Method"}, {"figure_path": "lvS2b8CjG5/figures/figures_24_1.jpg", "caption": "Figure 15: BCIC2A Confusion Matrix Diagram", "description": "This confusion matrix visualizes the performance of the model on the BCIC2A dataset for classifying four motor imagery tasks: left hand, right hand, foot, and tongue. Each cell (i, j) represents the number of samples from class i that were predicted as class j.  The diagonal elements show correct classifications. Off-diagonal elements indicate misclassifications. The color intensity reflects the magnitude of the values.", "section": "G.2 BCIC2A Experiment Results Visualization"}, {"figure_path": "lvS2b8CjG5/figures/figures_24_2.jpg", "caption": "Figure 16: P300 spatio-temporal Attention Diagram", "description": "This figure visualizes the model's attention distribution for the P300 task.  The top section shows attention in the time period from -0.1 to 1 second, while the bottom section shows attention from 1 to 2 seconds. For each time period, it displays temporal attention, mean attention, and difference attention across various time windows. The spatial attention patterns are displayed as scalp maps for each of these attention types across the different time windows, revealing which brain regions are most relevant at different moments during the P300 task.", "section": "G.3 PhysioP300 Experiment Results Visualization"}, {"figure_path": "lvS2b8CjG5/figures/figures_24_3.jpg", "caption": "Figure 1: The EEGPT structure involves patching the input EEG signal as  through masking (50% time and 80% channel patches), creating masked part  and unmasked part  and then embedding as  by local spatio-temporal embedding. The encoder processes the masked part, extracting features ( ) consisting of  for each time segment in the  part with summary tokens . The predictor predicts features ( ) for all time segments, aligning with the Momentum Encoder output ( ). Based on features extracted by the predictor and encoder, the reconstructor generates  to reconstruct the EEG signal of the  part.", "description": "This figure illustrates the architecture of EEGPT, a pretrained transformer model for EEG feature extraction. It shows how the input EEG signal is processed through patching, masking, embedding, encoding (by encoder), prediction (by predictor), and reconstruction (by reconstructor), involving spatio-temporal representation alignment and mask-based reconstruction.  The dual self-supervised learning method and hierarchical structure are also visually represented.", "section": "2 Method"}, {"figure_path": "lvS2b8CjG5/figures/figures_25_1.jpg", "caption": "Figure 15: BCIC2A Confusion Matrix Diagram", "description": "This confusion matrix visualizes the performance of the EEGPT model on the BCIC2A dataset for classifying four different motor imagery tasks: left hand, right hand, feet, and tongue. Each cell in the matrix represents the number of samples belonging to a particular class that were predicted as a specific class. The diagonal elements show the number of correctly classified samples, while the off-diagonal elements represent the misclassifications.", "section": "G.2 BCIC2A Experiment Results Visualization"}]