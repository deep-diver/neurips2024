[{"figure_path": "KSyTvgoSrX/figures/figures_8_1.jpg", "caption": "Figure 1: Expected approximation error of BSR, AOF and baseline factorizations for two different hyperparameter settings (left: \u03b1 = 0.999, \u03b2 = 0, right: \u03b1 = 1, \u03b2 = 0.9) with repeated participation (b = 100, k= n/100). See Section 4 for details.", "description": "The figure shows the expected approximation error for different matrix factorization methods (BSR, AOF, Id, A) with two different sets of hyperparameters for SGD optimization (\u03b1 and \u03b2). The left plot uses \u03b1 = 0.999 and \u03b2 = 0, while the right plot uses \u03b1 = 1 and \u03b2 = 0.9. Both plots consider the setting of repeated participation where b = 100 and k = n/100. The x-axis represents the workload matrix size (n), and the y-axis represents the expected approximation error. The plot demonstrates that BSR achieves approximation error comparable to AOF, while significantly outperforming the baseline factorizations (Id and A).", "section": "Experiments"}, {"figure_path": "KSyTvgoSrX/figures/figures_8_2.jpg", "caption": "Figure 2: Classification accuracy (mean and standard deviation over 5 runs with different random seeds) on CIFAR-10 for BSR, AOF, and baselines for (\u03b5, \u03b4) = (4, 10\u207b\u2075) for independent training runs. Left: one epoch, different batch sizes. Right: different number of epochs, constant batch size.", "description": "This figure compares the classification accuracy of different matrix factorization methods (BSR, AOF, Id, A) on the CIFAR-10 dataset.  The left plot shows the accuracy when training for a single epoch with varying batch sizes. The right plot illustrates the results obtained when using a fixed batch size but changing the number of epochs.", "section": "Experiments"}, {"figure_path": "KSyTvgoSrX/figures/figures_12_1.jpg", "caption": "Figure 1: Expected approximation error of BSR, AOF and baseline factorizations for two different hyperparameter settings (left: \u03b1 = 0.999, \u03b2 = 0, right: \u03b1 = 1, \u03b2 = 0.9) with repeated participation (b = 100, k= n/100). See Section 4 for details.", "description": "This figure compares the expected approximation error for four different matrix factorization methods (BSR, AOF, Id, A) used in differentially private SGD model training. The comparison is done for two different sets of hyperparameters (\u03b1 and \u03b2 representing weight decay and momentum, respectively), each with repeated data participation (each data batch contributes multiple times). The x-axis represents the size of the workload matrix (n), and the y-axis represents the approximation error. The figure demonstrates that BSR achieves comparable accuracy to the best-performing existing method (AOF) while having much lower computational overhead. The baselines (Id and A) show significantly higher error.", "section": "Experiments"}, {"figure_path": "KSyTvgoSrX/figures/figures_13_1.jpg", "caption": "Figure 1: Expected approximation error of BSR, AOF and baseline factorizations for two different hyperparameter settings (left: \u03b1 = 0.999, \u03b2 = 0, right: \u03b1 = 1, \u03b2 = 0.9) with repeated participation (b = 100, k= n/100). See Section 4 for details.", "description": "This figure compares the expected approximation error of four different matrix factorization methods (BSR, AOF, Id, and A) for two different sets of hyperparameters (\u03b1 and \u03b2) in the context of repeated participation in stochastic gradient descent (SGD) model training. The x-axis shows the size of the workload matrix, which increases as the number of SGD steps or training epochs increases. The y-axis shows the expected approximation error. The left plot uses hyperparameters \u03b1 = 0.999 and \u03b2 = 0, while the right plot uses \u03b1 = 1 and \u03b2 = 0.9. The results indicate that BSR has an approximation error comparable to that of AOF, and both perform better than the baseline methods.", "section": "Experiments"}, {"figure_path": "KSyTvgoSrX/figures/figures_14_1.jpg", "caption": "Figure 1: Expected approximation error of BSR, AOF and baseline factorizations for two different hyperparameter settings (left: \u03b1 = 0.999, \u03b2 = 0, right: \u03b1 = 1, \u03b2 = 0.9) with repeated participation (b = 100, k= n/100). See Section 4 for details.", "description": "The figure shows the expected approximation error for four different matrix factorization methods: BSR, AOF, Identity-left, and Identity-right.  Two different hyperparameter settings are compared (\u03b1=0.999, \u03b2=0 and \u03b1=1, \u03b2=0.9).  The x-axis represents the size of the workload matrix (n), and the y-axis represents the expected approximation error.  The plot shows that BSR and AOF consistently outperform the baseline methods (Identity-left and Identity-right) across different matrix sizes and hyperparameters. BSR and AOF have similar approximation errors; BSR sometimes yields slightly better values than AOF, potentially due to numerical limitations in solving AOF for larger matrices.", "section": "Experiments"}, {"figure_path": "KSyTvgoSrX/figures/figures_20_1.jpg", "caption": "Figure 1: Expected approximation error of BSR, AOF and baseline factorizations for two different hyperparameter settings (left: \u03b1 = 0.999, \u03b2 = 0, right: \u03b1 = 1, \u03b2 = 0.9) with repeated participation (b = 100, k= n/100). See Section 4 for details.", "description": "The figure shows the expected approximation error for different matrix factorization methods used in differentially private SGD.  Two sets of hyperparameters are shown: one with weight decay (\u03b1 = 0.999, \u03b2 = 0) and another without (\u03b1 = 1, \u03b2 = 0.9).  The error is plotted against the size of the workload matrix (n). The results compare the banded square root (BSR) factorization to the approximately optimal factorization (AOF) and two baseline methods.", "section": "Experiments"}, {"figure_path": "KSyTvgoSrX/figures/figures_29_1.jpg", "caption": "Figure 1: Expected approximation error of BSR, AOF and baseline factorizations for two different hyperparameter settings (left: \u03b1 = 0.999, \u03b2 = 0, right: \u03b1 = 1, \u03b2 = 0.9) with repeated participation (b = 100, k = n/100). See Section 4 for details.", "description": "This figure compares the expected approximation errors of four different matrix factorization methods for differentially private SGD model training.  Two different hyperparameter settings (momentum and weight decay) are tested in the context of repeated data participation. The methods compared are the proposed Banded Square Root (BSR) factorization, the Approximately Optimal Factorization (AOF), and two baseline methods (A = A * Id and A = Id * A). The figure shows that BSR achieves comparable performance to AOF while having a significantly lower computational cost.", "section": "Experiments"}, {"figure_path": "KSyTvgoSrX/figures/figures_30_1.jpg", "caption": "Figure 1: Expected approximation error of BSR, AOF and baseline factorizations for two different hyperparameter settings (left: \u03b1 = 0.999, \u03b2 = 0, right: \u03b1 = 1, \u03b2 = 0.9) with repeated participation (b = 100, k= n/100). See Section 4 for details.", "description": "This figure compares the expected approximation errors of four different matrix factorization methods for differentially private SGD model training: BSR (banded square root), AOF (approximately optimal factorization), Id (identity matrix baseline), and A (workload matrix baseline). Two distinct hyperparameter settings are considered: one with weight decay (\u03b1=0.999, \u03b2=0) and another without (\u03b1=1, \u03b2=0.9). The results illustrate that BSR achieves comparable approximation error to AOF across a range of workload matrix sizes, while substantially outperforming the baselines. The repeated participation setting (b=100, k=n/100) implies that each data batch can contribute multiple times to the model updates, mirroring realistic multi-epoch training scenarios.", "section": "Experiments"}]