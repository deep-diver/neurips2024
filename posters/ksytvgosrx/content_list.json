[{"type": "text", "text": "Banded Square Root Matrix Factorization for Differentially Private Model Training ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Nikita Kalinin Christoph Lampert Institute of Science and Technology (ISTA) Institute of Science and Technology (ISTA) Klosterneuburg, Austria Klosterneuburg, Austria nikita.kalinin@ist.ac.at chl@ist.ac.at ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Current state-of-the-art methods for differentially private model training are based on matrix factorization techniques. However, these methods suffer from high computational overhead because they require numerically solving a demanding optimization problem to determine an approximately optimal factorization prior to the actual model training. In this work, we present a new matrix factorization approach, BSR, which overcomes this computational bottleneck. By exploiting properties of the standard matrix square root, BSR allows to efficiently handle also large-scale problems. For the key scenario of stochastic gradient descent with momentum and weight decay, we even derive analytical expressions for BSR that render the computational overhead negligible. We prove bounds on the approximation quality that hold both in the centralized and in the federated learning setting. Our numerical experiments demonstrate that models trained using BSR perform on par with the best existing methods, while completely avoiding their computational overhead. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the problem of differentially private $(D P)$ model training with stochastic gradient descent $(S G D)$ in the setting of either federated or centralized learning. This task has recently emerged as one of the most promising ways to train powerful machine learning models but nevertheless guarantee the privacy of the used data, which led to a number of studies, both theoretical as well as application-driven [Abadi et al., 2016, Yu et al., 2020, Zhang et al., 2021, Kairouz et al., 2021, Denisov et al., 2022]. The state of the art in the field are approaches based on the matrix factorization (MF) mechanism [Li et al., 2015, Henzinger et al., 2024], which combines theoretical guarantees with practical applicability [Choquette-Choo et al., 2023a,c,b, 2024]1It is based on the observation that all iterates of SGD are simply linear combinations of model gradients, which are computed at intermediate time steps. Consequently, the iterates can be written formally as the result of multiplying the matrix of coefficients, called workload matrix, with the row-stacked gradient vectors. To preserve the privacy of the training data in this process one adds suitably scaled Gaussian noise at intermediate steps of the computation. The MF mechanism provides a way to select the noise covariance structure based on a factorization of the workload matrix into two matrices. ", "page_idx": 0}, {"type": "text", "text": "Identifying the minimal amount of noise necessary to achieve a desired privacy level requires solving an optimization problem over all possible factorizations, subject to data participation constraints. For some specific settings, the optimal solutions have been characterized: for streaming learning, when each data batch contributes at most once to the gradients, Li et al. [2015] presented a formulation of this problem as a semi-definite program. Henzinger et al. [2024] proved that a square root factorization of the workload matrix is asymptotically optimal for different linear workloads, including continual summation and decaying sums. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, our focus lies on the settings that are most relevant to machine learning tasks: workload matrices that reflect SGD-like optimization, and participation schemes in which each data batch can potentially contribute to more than one gradient vector, as it is the case for standard multi-epoch training. Assuming that this happens at most once every $b$ steps, for some value $b\\geq1$ , leads to the problem of optimal matrix factorization in the context of $b$ -min-separated participation sensitivity. Unfortunately, as shown in Choquette-Choo et al. [2023a], finding the optimal matrix factorization in this setting is computationally intractable. Instead, the authors proposed an approximate solution by posing additional constraints on the solution set. The result is a semi-definite program that is tractable, but still has high computational cost, making it practical only for small to medium-sized problem settings. ", "page_idx": 1}, {"type": "text", "text": "Subsequent work concentrated on improving or better understanding the factorizations for specific algorithms, such as plain SGD without gradient clipping, momentum, or weight decay [Koloskova et al., 2023] or specific, e.g. convex, objective functions [Choquette-Choo et al., 2024]. Often, streaming data was asssumed, i.e. each data item can contribute at most to one model update, which is easier to analyze theoretical, but further removed from real-world applications [Dvijotham et al., 2024]. A concurrent line of works has also focused on scaling matrix factorizations for large-scale training. McMahan et al. [2024] extended the Buffered Linear Toeplitz (BLT) mechanism to support multi-participation in federated learning, improving privacy-utility tradeoffs, while ensuring memory efficiency. McKenna [2024] improved DP Banded Matrix Factorization\u2019s scalability, enabling it to handle millions of iterations and large models efficiently. These advancements enhance the practicality of differentially private training in real-world applications. ", "page_idx": 1}, {"type": "text", "text": "Our work aims at a general-purpose solution that covers as many realistic scenarios as possible. Our ultimate goal is to make general-purpose differentially private model training as simple and efficient to use as currently dominating non-private technique. Our main contributions are: ", "page_idx": 1}, {"type": "text", "text": "1. We introduce a new factorization, the banded squared root (BSR), which is efficiently computable even for large workload matrices and agnostic to the underlying training objective. For matrices stemming from SGD optimization potentially with momentum and/or weight decay, we even provide closed form expressions.   \n2. We provide general lower bounds on the approximate error for any factorization, along with specific upper and lower bounds for the BSR, Square Root, and baseline factorizations, in the contexts of both single participation (streaming) and repeated participation (e.g., multi-epoch) training.   \n3. We demonstrate experimentally that BSR\u2019s approximation error is comparable to the state-of-the-art method, and that both methods also perform comparably in real-world training tasks. ", "page_idx": 1}, {"type": "text", "text": "Overall, the proposed BSR factorization achieves training high-accuracy models with provable privacy guarantees while staying computationally efficient even for large-scale training tasks. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our work falls into the areas of differentially private (stochastic) optimization, of which we remind the reader here, following mostly the description of Denisov et al. [2022] and Choquette-Choo et al. [2023a]. The goal is to estimate a sequence of parameter vectors, $\\Theta=(\\theta_{1},\\dots,\\bar{\\theta}_{n})\\in\\mathbb{R}^{d}$ , where each $\\theta_{i}$ is a linear combination of update vectors, $x_{1},\\ldots,x_{i}\\in\\mathbb{R}^{d}$ , that were computed in previous steps, typically as gradients of a model with respect to training data that is meant to stay private. We assume that all $x_{i}$ have a bounded norm, $\\|x_{i}\\|\\leq\\zeta$ . Compactly, we write $\\Theta=A X$ , where the lower triangular workload matrix $A$ contains the coefficients and $X\\,\\in\\,\\mathbb{R}^{n\\times d}$ is formed by stacking the update vectors as rows. With different choices of $A$ , the setting then reflects many popular first-order optimization algorithms, in particular stochastic gradient descent (SGD), potentially with momentum and/or weight decay. Depending on how exactly $x_{1},\\ldots,x_{n}$ are obtained, the setting can express different centralized as well as federated training paradigms. To formalize this aspect, we adopt the concept of $b$ -min-separated participation [Choquette-Choo et al., 2023a]. For some integer $b\\geq1$ , it states that if a data item (e.g. a single training example in central training, or a client batch in ", "page_idx": 1}, {"type": "text", "text": "Algorithm 1 Differentially Private SGD with Matrix Factorization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Input: Initial model $\\theta_{0}\\ \\in\\ \\mathbb{R}^{d}$ , dataset $D$ , batchsize $b$ , matrix $C\\;\\in\\;\\mathbb{R}^{n\\times n}$ , model loss $\\ell(\\theta,d)$ , clipnorm $\\zeta$ , noise matrix $Z\\in\\mathbb{R}^{n\\times d}$ with i.i.d. entries $\\sim\\mathcal{N}(0,s^{2})$ , where $s=\\sigma\\operatorname{sens}_{k,b}(C)$ . ", "page_idx": 2}, {"type": "text", "text": "for $i=1,2,\\dots,n$ do $\\begin{array}{r l}&{S_{i}\\gets\\{d_{1},\\dotsc,d_{m}\\}\\subseteq D\\quad\\mathrm{select~a~data~batch,respecting~th}}\\\\ &{g_{i}\\gets\\nabla_{\\theta}\\ell(\\theta_{i-1},d_{j}))\\quad\\mathrm{for~}j=1,\\dotsc,m}\\\\ &{x_{i}\\gets\\sum_{j=1}^{m}\\mathrm{clip}_{\\zeta}(g_{j})\\quad\\mathrm{where~clip}_{\\zeta}(d)=\\operatorname*{min}(1,\\zeta/||d||)d}\\\\ &{\\hat{x}_{i}\\gets x_{i}+\\zeta[C^{-1}Z]_{[i,\\cdot]}}\\\\ &{\\theta_{i}\\gets\\mathrm{update}(\\theta_{i-1},\\hat{x}_{i}),\\qquad//\\mathrm{\\;SGD~model~updates}}\\end{array}$ select a data batch, respecting the data participation constraints ", "page_idx": 2}, {"type": "text", "text": "Output: $\\Theta=(\\theta_{1},\\ldots,\\theta_{n})$ ", "page_idx": 2}, {"type": "text", "text": "federated learning) contributed to an update $x_{i}$ , the earliest it can contribute again is the update $x_{i+b}$ . Additionally, let $\\begin{array}{r}{1\\leq k\\leq\\frac{n}{b}}\\end{array}$ be the maximal number any data point can contribute. In particular, this notion also allows us to treat in a unified way streaming data ( $b=n$ or $k=1$ ), as well as unrestricted access patterns ( $k=n$ with $b=1$ ), but also intermediate settings, such as multi-epoch training on a fixed-size dataset. ", "page_idx": 2}, {"type": "text", "text": "The matrix factorization approach [Li et al., 2015] adopts a factorization $A=B C$ of the workload matrix and computes $\\Theta^{\\mathrm{MF}}\\,{\\overset{\\triangledown}{=}}\\,B(C X+Z)$ , where $Z$ is Gaussian noise that is chosen appropriately to make the intermediate result $C X+Z$ private to the desired level. Algorithm 1 shows the resulting algorithm in pseudocode. It exploits the fact that instead of explicit multiplication by $C$ and $B$ , standard optimization toolboxes can be employed with suitably modified update vectors, because also $\\Theta^{\\mathrm{MF}}\\,\\dot{=}\\,A(X+C^{-1}Z)$ , and multiplication by $A$ corresponds to performing the optimization. ", "page_idx": 2}, {"type": "text", "text": "Different factorizations recover different algorithms from the literature. For example, $B=A$ , $C=\\operatorname{Id}$ recovers DP-SGD [Abadi et al., 2016], where noise is added directly to the gradients. Conversely, $B=\\mathrm{Id}$ , $C=A$ simply adds noise to each iterate of the optimization [Dwork et al., 2006]. However, better choices than these baselines are possible, in the sense that they can guarantee the same levels of privacy with less added noise, and therefore potentially with higher retained accuracy. The reason lies in the fact that $B$ and $C$ play different roles: $B$ acts as a post-processing operation of already private data. Hence, it has no further effect on privacy, but it influences to what amount the added noise affects the expected error in the approximation of $\\Theta$ . Specifically, for $Z\\sim\\mathcal{N}(0;s\\operatorname{Id})$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{Z}\\|\\Theta-\\Theta^{\\mathrm{MF}}\\|_{F}^{2}=\\mathbb{E}_{Z}\\|B Z\\|_{F}^{2}=s^{2}\\|B\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In contrast, $C X$ is the quantity that is meant to be made private. Doing so requires noise of a strength proportional to $C$ \u2019s sensitivity, $\\mathrm{sens}(C):=\\,\\operatorname*{sup}_{X\\sim X^{\\prime}}\\,\\lceil|C X-C X^{\\prime}|\\rceil_{F}$ , where the neighborhood relation, $X\\sim X^{\\prime}$ , indicates that the two sequences of update vectors differ only in those entries that correspond to a single data item2 As shown in Choquette-Choo et al. [2023a], in the setting of $b$ -min-separated repeated participation, it holds that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname{sens}_{k,b}(C)\\leq\\operatorname*{max}_{\\pi\\in\\Pi_{k,b}}\\sqrt{\\sum_{i,j\\in\\pi}\\left\\vert(C^{\\top}C)_{[i,j]}\\right\\vert},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\Pi_{k,b}=\\big\\{\\;\\pi\\subset\\{1,\\dots,n\\}\\;:\\;|\\pi|\\leq k\\wedge\\big(\\{i,j\\}\\subset\\pi\\;\\Rightarrow\\;i=j\\;\\vee\\;|i-j|\\geq b\\big)\\;\\big\\},$ is the set of possible $b$ -min-separated index sets with at most $k$ participation. Furthermore, (2) holds even with equality if all entries of $C^{\\top}C$ are non-negative. ", "page_idx": 2}, {"type": "text", "text": "Combining (1) with $s=\\operatorname{sens}_{k,b}(C)$ yields a quantitative measure for the quality of a factorization. ", "page_idx": 2}, {"type": "text", "text": "Definition 1. For any factorization $A=B C$ , its expected approximation error is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{E}(B,C):=\\sqrt{\\mathbb{E}_{Z}\\|\\Theta-\\Theta^{M F}\\|_{F}^{2}/n}=\\frac{1}{\\sqrt{n}}\\operatorname{sens}_{k,b}(C)\\|B\\|_{F},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the $1/\\sqrt{n}$ factor is meant to make the quantity comparable across different problem sizes. ", "page_idx": 2}, {"type": "text", "text": "The optimal factorization by this reasoning would be the one of smallest expected approximation error.   \nUnfortunately, minimizing (3) across all factorizations it is generally computationally intractable.   \nInstead, Choquette-Choo et al. [2023a] propose an approximately optimal factorization. ", "page_idx": 3}, {"type": "text", "text": "Definition 2. For a workload matrix $A$ , let $S$ be the solution to the optimization problem ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{min}_{S\\in S_{+}^{n}}\\mathrm{trace}[A^{\\top}A\\,S^{-1}]\\quad s u b j e c t\\,t o\\quad\\mathrm{diag}(S)=1\\quad a n d\\;\\;S_{[i,j]}=0\\;f o r\\,\\;|i-j|\\ge b,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $S_{+}^{n}$ is the cone of positive definite $n\\times n$ matrices. Then, $A=B C$ is called the approximately optimal factorization (AOF), $i f C$ is lower triangular and fulfills $C^{\\top}C=S$ . ", "page_idx": 3}, {"type": "text", "text": "The optimization problem (4) is a semi-definite program (SDP), and can therefore be solved numerically using standard packages. However, this is computationally costly, and for large problems (e.g. $n>5000]$ ) computing the AOF solution is impractical. This poses a problem for real-world training tasks, where the number of update steps are commonly thousands or tens of thousands. ", "page_idx": 3}, {"type": "text", "text": "Solving (4) itself only approximately can mitigate this problem to some extent, but as we will discuss in Section 4, this can lead to robustness problems, especially because the recovery of $C$ from $S$ in Definition 2, e.g. by a Cholesky decomposition, tends to be sensitive to numerical errors. ", "page_idx": 3}, {"type": "text", "text": "3 Banded Square Root Factorization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the following section, we introduce our main contribution: a general-purpose factorization for the task of differentially private stochastic optimization that can be computed efficiently even for large problem sizes. ", "page_idx": 3}, {"type": "text", "text": "Definition 3 (Banded Square Root Factorization). Let $A\\in\\mathbb{R}^{n\\times n}$ be a lower triangular workload matrix with strictly positive diagonal entries. Then, we call $A=C^{2}$ the square root factorization (SR), when $C$ denotes the unique matrix square root that also has strictly positive diagonal entries. Furthermore, for any bandwidth $p\\in\\{1,\\ldots,n\\}$ , we define the banded square root factorization of bandwidth $p$ ( $\\acute{p}$ -BSR) of $A$ , as ", "page_idx": 3}, {"type": "equation", "text": "$$\nA=B^{|p|}C^{|p|}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $C^{|p|}$ is created from $C$ by setting all entries below the $p$ -th diagonal to 0, ", "page_idx": 3}, {"type": "equation", "text": "$$\nC_{[i,j]}^{|p|}=\\left\\{\\!\\!\\begin{array}{l l}{C_{[i,j]}}&{\\quad i f\\,i-j<p,}\\\\ {0}&{\\quad o t h e r w i s e.}\\end{array}\\right.\\quad\\quad\\quad\\quad a n d\\quad B^{|p|}=A(C^{|p|})^{-1}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that determining the SR, and therefore any $p$ -BSR, is generally efficient even for large workload matrices, because explicit recursive expressions exist for computing the square root of a lower triangular matrix [Bj\u00f6rck and Hammarling, 1983, Deadman et al., 2012]. ", "page_idx": 3}, {"type": "text", "text": "In the rest of this work, we focus on the case where the workload matrix stems from SGD with momentum and/or weight decay, and we show that then even closed form expressions for the entries of $C^{|p|}$ exist that renders the computational cost negligible. ", "page_idx": 3}, {"type": "text", "text": "3.1 Banded Square Root Factorization for SGD with Momentum and Weight Decay ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We recall the update steps of SGD with momentum and weight decay: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\theta_{i}=\\alpha\\theta_{i-1}-\\eta m_{i}\\qquad\\mathrm{for}\\qquad m_{i}=\\beta m_{i-1}+x_{i}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $x_{1},\\ldots,x_{n}$ are the update vectors, $\\eta>0$ is the learning rate and $0\\leq\\beta<1$ is the momentum strength and $0<\\alpha\\le1$ is the weight decay parameter. Note that our results also hold for $\\beta=0$ , i.e. without momentum, and for $\\alpha=1$ , i.e., without weight decay. In line with real algorithms and to avoid degenerate cases, we assume $\\beta<\\alpha$ throughout this work. The update vectors are typically gradients of the model with respect to its parameters, but additional operations such as normalization or clipping might have been applied. ", "page_idx": 3}, {"type": "text", "text": "Unrolling the recursion, we obtain an expression for $\\theta_{i}$ as a linear combination of update vectors as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\theta_{i}=\\eta\\sum_{j=1}^{i}x_{j}\\Big(\\sum_{k=j}^{i}\\alpha^{i-k}\\beta^{k-j}\\Big).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Consequently, the workload matrix has the explicit form $A=\\eta A_{\\alpha,\\beta}$ for ", "page_idx": 4}, {"type": "equation", "text": "$$\nA_{\\alpha,\\beta}=\\left(\\begin{array}{c c c c c}{{a_{0}}}&{{0}}&{{0}}&{{\\ldots}}&{{0}}\\\\ {{a_{1}}}&{{a_{0}}}&{{0}}&{{\\ldots}}&{{0}}\\\\ {{a_{2}}}&{{a_{1}}}&{{a_{0}}}&{{\\ldots}}&{{0}}\\\\ {{\\vdots}}&{{\\vdots}}&{{\\ddots}}&{{\\ddots}}&{{\\vdots}}\\\\ {{a_{n-1}}}&{{a_{n-2}}}&{{\\ldots}}&{{a_{1}}}&{{a_{0}}}\\end{array}\\right)\\quad\\mathrm{with~}a_{j}=\\sum_{i=0}^{j}\\alpha^{i}\\beta^{j-i}=\\frac{\\alpha^{j+1}-\\beta^{j+1}}{\\alpha-\\beta}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As one can see, $A_{\\alpha,\\beta}$ is a lower triangular Toeplitz-matrix, so it is completely determined by the entries of its first column. In the following, we use the notation LDToep $(m_{1},\\ldots,m_{n})$ to denote a lower triangular Toeplitz matrix with first column $m_{1},\\ldots,m_{n}$ , i.e. $A_{\\alpha,\\beta}=\\mathrm{LDToep}(a_{0},\\ldots,a_{n-1})$ . ", "page_idx": 4}, {"type": "text", "text": "Our first result is an explicit expression for the positive square root of $A_{\\alpha,\\beta}$ (and thereby its $p$ -BSR). Theorem 1 (Square-Root of SGD Workload Matrix). Let $A_{\\alpha,\\beta}$ be the workload matrix (9). Then $A_{\\alpha,\\beta}=C_{\\alpha,\\beta}^{2}$ for $C_{\\alpha,\\beta}=\\mathrm{LDToep}(c_{0},\\ldots,c_{n-1})$ , with $c_{0}=1$ and $\\begin{array}{r}{c_{j}=\\sum_{i=0}^{j}\\alpha^{j-i}r_{j-i}r_{i}\\beta^{i}}\\end{array}$ for $j=1,\\cdot\\cdot\\cdot,n-1$ with coefficients $r_{i}=|\\!\\left({^{-1/2}_{i}}\\right)|$ . For any $p\\in\\{1,\\ldots,n\\}$ , the $p$ -banded BSR matrix C|\u03b1p,|\u03b2 is obtained from this by setting all coefficients cj = 0 for j \u2265p. ", "page_idx": 4}, {"type": "text", "text": "Proof sketch. The proof be found in Appendix F.1. Its main idea is to factorize $A_{\\alpha,\\beta}$ into a product of two simpler lower triangular matrices, each of which has a closed-form square root. We show that the two roots commute and that the matrix $C_{\\alpha,\\beta}$ is their product, which implies the theorem. \u53e3 ", "page_idx": 4}, {"type": "text", "text": "3.2 Efficiency ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We first establish that the $p$ -BSR for SGD can be computed efficiently even for large problem sizes. ", "page_idx": 4}, {"type": "text", "text": "Lemma 1 (Efficiency of BSR). The entries of $C_{\\alpha,\\beta}^{|p|}$ can be determined in runtime $O(p\\log p)$ , i.e., in particular independent of $n$ . ", "page_idx": 4}, {"type": "text", "text": "Proof sketch. As a lower triangular Toeplitz matrix, $C_{\\alpha,\\beta}^{|p|}$ is fully determined by the values of its first column. By construction $c_{p+1},\\ldots,c_{n}=0$ , so only the complexity of computing $c_{1},\\dotsc,c_{p-1}$ matters. These can be computed efficiently by writing them as the convolution of vectors $(\\alpha^{i}r_{i})_{i=0,...,p-1}$ and $(\\beta^{i}r_{i})_{i=0,...,p-1}$ and, e.g., employing the fast Fourier transform. \u53e3 ", "page_idx": 4}, {"type": "text", "text": "Note that for running Algorithm 1, the matrix $B$ of the factorization is not actually required. However, one needs to know the sensitivity of C|\u03b1p,|\u03b2, as this determines the necessary amount of noise. The following theorem establishes that for a large class of matrices, including the BSR in the SGD setting, this is possible exactly and in closed form. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2 (Sensitivity for decreasing non-negative Toeplitz matrices). Let $\\begin{array}{r l}{M}&{{}=}\\end{array}$ $\\mathrm{LDToep}(m_{0},\\ldots,m_{n-1})$ be a lower triangular Toeplitz matrix with decreasing non-negative entries, i.e. ", "page_idx": 4}, {"type": "text", "text": "$m_{0}\\geq m_{1}\\geq m_{2}\\geq...\\,m_{n-1}\\geq0$ . Then its sensitivity (2) in the setting of $b$ -min-separation is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{sens}_{k,b}(M)=\\Big\\|\\sum_{j=0}^{k-1}M_{[\\cdot,1+j b]}\\Big\\|=\\left(\\sum_{i=0}^{n-1}\\left(\\sum_{j=0}^{\\operatorname*{min}\\{k-1,i/b\\}}m_{i-j b}\\right)^{2}\\right)^{1/2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $M_{[\\cdot,1+j b]}$ denotes the $(1+j b)$ -th column of $M$ . ", "page_idx": 4}, {"type": "text", "text": "Proof sketch. The proof can be found in Appendix F.2. It builds on the identity (2), which holds with equality because of the non-negative entries of $M$ . Using the fact that the entries of $M$ are non-increasing one establishes that an optimal $b$ -separated index set is $\\{1,1+b,\\cdots,1+(k-1)b\\}$ . From this, the identity (10) follows. \u53e3 ", "page_idx": 4}, {"type": "text", "text": "Corollary 1. The sensitivity of the $p$ -BSR for SGD can be computed using formula (10). ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Proof sketch. It suffices to show that the coefficients $c_{0},\\ldots,c_{n-1}$ of Theorem 1 are monotonically decreasing. We do so by an explicit computation, see Appendix F.3. \u53e3 ", "page_idx": 4}, {"type": "text", "text": "3.3 Approximation Quality \u2013 Single Participation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Having established the efficiency of BSR, we now demonstrate its suitability for high-quality model training. To avoid corner cases, we assume that $\\frac{n}{b}$ is an integer, which does not affect the asymptotic behavior. We also discuss only the case in which the update vectors have bounded norm $\\zeta=1$ . Results for general $\\zeta$ can readily be derived using the linearity of the sensitivity with respect to $\\zeta$ . ", "page_idx": 5}, {"type": "text", "text": "We first discuss the case of model training with single participation $\\;k=1\\;\\;$ ), where more precise results are possible than the general case. Our main result are bounds on the expected approximation error of the square root factorization that, in particular, prove its asymptotic optimality. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3 (Expected approximation error with single participation). Let $A_{\\alpha,\\beta}\\,\\in\\,\\mathbb{R}^{n\\times n}$ be the workload matrix (9) of SGD with momentum $0\\leq\\beta<1$ and weight decay parameter $0<\\alpha\\leq1$ , where $\\alpha>\\beta$ . Assume that each data item can contribute at most once to an update vector (e.g. single participation, $k=1$ ). Then, the expected approximation error of the square root factorization, $\\bar{A_{\\alpha,\\beta}}\\bar{=C_{\\alpha,\\beta}^{2}}$ , fulfills ", "page_idx": 5}, {"type": "equation", "text": "$$\n1\\leq\\mathcal{E}(C_{\\alpha,\\beta},C_{\\alpha,\\beta})\\leq\\frac{1}{(\\alpha-\\beta)^{2}}\\log\\frac{1}{1-\\alpha^{2}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for $\\alpha<1$ , and ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left\\{1,\\frac{\\log(n+1)-1}{4}\\right\\}\\leq\\mathcal{E}(C_{1,\\beta},C_{1,\\beta})\\leq\\frac{1+\\log(n)}{(1-\\beta)^{2}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proof sketch. For the proof, we establish a relations between $\\operatorname{sens}_{1,n}(C)$ and $\\|C_{\\alpha,\\beta}\\|_{F}$ , and then we bound the resulting expressions by an explicit analysis of the norm. For details, see Appendix F.5. ", "page_idx": 5}, {"type": "text", "text": "The following two results provide context for the interpretation of Theorem 3. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4. Assume the setting of Theorem 3. Then, for any factorization $A_{\\alpha,\\beta}~=~B C$ with $C^{\\top}C\\geq0$ , the expected approximation error fulfills ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{E}(B,C)=\\left\\{\\Omega(1)\\qquad\\begin{array}{l l}{{f o r\\,\\alpha<1,}}\\\\ {{f o r\\,\\alpha=1.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proof sketch. The theorem is the special case $k=1$ of Theorem 8, which we state in the next section and prove in Section F.9. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "Theorem 5. Assume the setting of Theorem 3. Then, the baseline factorizations $A_{\\alpha,\\beta}=A_{\\alpha,\\beta}\\cdot\\mathrm{Id}$ and $A_{\\alpha,\\beta}=\\operatorname{Id}\\cdot A_{\\alpha,\\beta}$ fulfill, for $\\alpha<1$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\xi(A_{\\alpha,\\beta},\\mathrm{H})=\\frac{\\sqrt{1+\\alpha\\beta}}{\\sqrt{(1-\\alpha\\beta)(1-\\alpha^{2})(1-\\beta^{2})}}+o(1)\\quad a n d\\quad\\varepsilon(A_{1,\\beta},\\mathrm{H})\\leq\\frac{\\sqrt{n}}{\\sqrt{2}(1-\\beta)}+o(\\sqrt{n})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{E}(\\mathrm{Id},A_{\\alpha,\\beta})=\\frac{\\sqrt{1+\\alpha\\beta}}{\\sqrt{(1-\\alpha\\beta)(1-\\alpha^{2})(1-\\beta^{2})}}\\,+o(1)\\quad a n d\\quad\\mathcal{E}(\\mathrm{Id},A_{1,\\beta})\\leq\\frac{\\sqrt{n}}{1-\\beta}\\,+o(\\sqrt{n}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proof sketch. The result follows from an explicit analysis of the coefficients, see Appendix F.6. ", "page_idx": 5}, {"type": "text", "text": "Discussion. Theorems 3 to 5 provide a full characterization of the approximation quality of the square root factorization as well as its alternatives: 1) the square root factorization has asymptotically optimal approximation quality, because the upper bounds in Equation (12) match the lower bounds in Equation (13); 2) the AOF from Definition 2 also fulfills the conditions of Theorem 4. Therefore, it must also adhere to the lower bound (13) and cannot be asymptotically better than the square root factorization; 3) the approximation qualities of the baseline factorizations in Equation (14) and (15) are asymptotically worse than optimal in the $\\alpha=1$ setting, and worse by a constant factor for $\\alpha<1$ . The BSR factorization can be applied even in more general scenarios, such as with varying learning rates. However, in this case, the workload matrix will no longer be Toeplitz. This makes it difficult to provide analytical guarantees for the matrix, but it can still be applied numerically. ", "page_idx": 5}, {"type": "text", "text": "3.4 Approximation Quality \u2013 Repeated Participation. ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now provide mostly asymptotic statements about the approximation quality of BSR and baselines in the setting where data items can contribute more than once to the update vectors. ", "page_idx": 6}, {"type": "text", "text": "Theorem 6 (Approximation error of BSR). Let $A_{\\alpha,\\beta}\\in\\mathbb{R}^{n\\times n}$ be the workload matrix (9) of SGD with momentum $0\\leq\\beta<1$ and weight decay $0<\\alpha\\le1$ , with $\\alpha>\\beta$ . Let $A_{\\alpha,\\beta}=B_{\\alpha,\\beta}^{|p|}C_{\\alpha,\\beta}^{|p|};$ , be its banded square root factorization as in Definition 3. Then, for any $b\\in\\{1,\\ldots,n\\}$ , $p\\leq b$ , and $k\\in\\{1,\\ldots,\\frac{\\bar{n}}{b}\\}$ it holds: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{E}(B_{\\alpha,\\beta}^{|p|},C_{\\alpha,\\beta}^{|p|})=\\left\\{\\begin{array}{l l}{\\displaystyle O_{\\beta}\\left(\\sqrt{\\frac{n k\\log p}{p}}\\right)+O_{\\beta,p}(\\sqrt{k})\\quad}&{\\mathrm{~}f o r\\;\\alpha=1,}\\\\ {\\displaystyle O_{\\beta,p,\\alpha}\\left(\\sqrt{k}\\right)\\quad}&{\\mathrm{~}f o r\\;\\alpha<1.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Proof sketch. For the proof, we separately bound the sensitivity of $C_{\\alpha,\\beta}^{|p|}$ and the Frobenius norm of B|\u03b1p,|\u03b2. The former is straightforward because of the matrix\u2019s band structure. The latter requires an in-depth analysis of the inverse matrix\u2019 coefficient. Both steps are detailed in Appendix F.7. \u53e3 ", "page_idx": 6}, {"type": "text", "text": "The following results provide context for the interpretation of Theorem 6. ", "page_idx": 6}, {"type": "text", "text": "Theorem 7 (Approximation error of Square Root Factorization). Let $A_{\\alpha,\\beta}\\in\\mathbb{R}^{n\\times n}$ be the workload matrix (9) of $S G D$ with momentum $0\\leq\\beta<1$ and weight decay $0\\,<\\,\\alpha\\,\\leq\\,1$ , with $\\alpha\\,>\\,\\beta$ . Let $A_{\\alpha,\\beta}=C_{\\alpha,\\beta}^{2}$ be its square root factorization. Then, for any $b\\in\\{1,\\ldots,n\\}$ and $\\begin{array}{r}{k=\\frac{n}{b}}\\end{array}$ it holds: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{E}(C_{\\alpha,\\beta},C_{\\alpha,\\beta})=\\left\\{\\Theta_{\\beta}\\left(k\\sqrt{\\log n}+\\sqrt{k}\\log n\\right)\\qquad\\begin{array}{l l}{f o r\\,\\alpha=1,}\\\\ {\\Theta_{\\alpha,\\beta}\\bigl(\\sqrt{k}\\bigr)}&{\\qquad f o r\\,\\alpha<1.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Proof sketch. We bound $\\mathrm{sens}_{k,b}(C_{\\alpha,\\beta})$ and $\\|C_{\\alpha,\\beta}\\|_{F}$ using the explicit entries for $C_{\\alpha,\\beta}$ from Theorem 1. Details are provided in Appendix F.8. \u53e3 ", "page_idx": 6}, {"type": "text", "text": "Theorem 8. Assume the setting of Theorem 6. Then, for any factorization $A_{\\alpha,\\beta}~=~B C$ with $C^{\\top}C\\geq0$ , the approximation error fulfills ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal E(B,C)\\geq\\left\\{\\!\\!\\begin{array}{l l}{\\sqrt k\\log n}&{\\qquad f o r\\,\\alpha=1,}\\\\ {\\sqrt k}&{\\qquad f o r\\,\\alpha<1,}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Proof sketch. The proof is based on the observation that $\\|X\\|_{F}\\|Y\\|_{F}\\geq\\|X Y\\|_{*}$ for any matrices $X,Y$ , where $\\|\\cdot\\|_{*}$ denotes the nuclear norm. To derive (18), we show that $\\operatorname{sens}_{k,b}(C)$ is lower bounded by ${\\frac{\\sqrt{k}}{n}}\\|C\\|_{F}$ , and derive explicit bounds on the singular values of $A_{\\alpha,\\beta}$ . \u53e3 ", "page_idx": 6}, {"type": "text", "text": "Theorem 9. Assume the setting of Theorem 6. Then, the baseline factorizations $A_{\\alpha,\\beta}=A_{\\alpha,\\beta}\\cdot\\mathrm{Id}$ and $A_{\\alpha,\\beta}=\\operatorname{Id}\\cdot A_{\\alpha,\\beta}$ fulfill ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal E(A_{\\alpha,\\beta},\\mathrm{Id})\\geq\\left\\{\\sqrt{\\frac{n k}{2}}\\begin{array}{l l}{\\qquad}&{f o r\\,\\alpha=1,}\\\\ &{\\qquad\\,\\,f o r\\,\\alpha<1.}\\end{array}\\right.\\qquad\\mathcal E(\\mathrm{Id},A_{\\alpha,\\beta})\\geq\\left\\{\\begin{array}{l l}{\\!\\frac{k\\sqrt{n}}{\\sqrt{3}}}&{\\,f o r\\,\\alpha=1,}\\\\ {\\!\\sqrt{k}}&{\\,f o r\\,\\alpha<1.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Proof sketch. The proof relies on the fact that the workload matrices can be lower bounded componentwise by simpler matrices: $A_{\\alpha,\\beta}\\geq A_{\\alpha,0}$ and $A_{\\alpha,0}\\geq\\mathrm{Id}$ . For the simpler matrices, the bounds (19) can then be derived analytically, and the general case follows by monotonicity. \u53e3 ", "page_idx": 6}, {"type": "text", "text": "Discussion. Analogously to the case of single participation, Theorems 6 to 9 again establish that the proposed BSR is asymptotically superior to the baseline factorizations if $\\alpha=1$ . A comparison of Theorems 6 and 7 suggests that, at least for maximal participation, $\\begin{array}{r}{k=\\frac{n}{b}}\\end{array}$ and $p=b$ , the bandedness of the $p$ -BSR improves the approximation quality, specifically in the practically relevant regime where $b\\ll n$ . While none of the methods match the lower bound of Theorem 6, we conjecture that this is not because any asymptotically better methods would exist, but rather a sign of Equation (18) is not tight. Both theoretical consideration and experiments suggest that a term linear in $k$ should appear there. For $\\alpha<1$ , all studied methods are asymptotically identical and, in fact, optimal. ", "page_idx": 7}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To demonstrate that BSR can achieve high accuracy not only in theory but also in practice, we compare it to AOF and baselines in numerical experiments. Our results show that BSR achieves quality comparable to the AOF, but without the computational overhead, and it clearly outperforms the baseline factorizations. The privacy guarantees are identical for all methods, so we do not discuss them explicitly. ", "page_idx": 7}, {"type": "text", "text": "Implementation and computational cost. We implement BSR by the closed-form expressions of Theorem (1). For single data participation, we use the square root decomposition directly. For repeated data participation we use $p$ -BSR with $p=b$ . Using standard python/numpy code, computing the BSR as dense matrices are memory-bound rather than compute-bound. Even sizes of $n=10,000$ or more take at most a few seconds. Computing only the Toeplitz coefficients is even faster, of course. ", "page_idx": 7}, {"type": "text", "text": "To compute AOF, we solve the optimization problem (4) using the cvxpy package with SCS backend, see Algorithm B for the source code3. With the default numerical tolerance, $10^{\\bar{-}4}$ , each factorization took a few minutes $n\\leq100)$ ) to hours $n\\le500)$ to several days $(n\\geq700)$ of CPU time. Note that this overhead reappears for any change in the number of update steps, $n$ , weight decay, $\\alpha$ , or momentum, $\\beta$ , as these induce different workload matrices. In our experiments, when the optimization for AOF did not terminate within 10 days, we reran the optimization problem with the tolerance increased by a factor of 10. The runtime depends not only on the matrix size but also on the entries. In particular, we observe matrices with momentum to be harder to factorize than without. For large matrix sizes we frequently encountered numerical problems: the intermediate matrices, $S$ , in (4), often did not fulfill the positive definiteness condition required to solve the subsequent Cholesky decomposition for $C$ . Unfortunately, simply projecting the intermediates back to the cone of positive semi-definite matrices is not enough, because the resulting $C$ matrices also have to be invertible and not too badly conditioned. Ultimately, we adopted a postprocessing step for $S$ that ensures that all its eigenvalues were at least of value $\\sqrt{1/n}$ , which we find to be a reasonable modification to ensure the stability of the convergence. Enforcing this empirically found value leads to generally good results, as our experiments below show, but it does add an undesirable extra level of complexity to the process. In contrast, due to its analytic expressions, BSR does not suffer from numerical problems. It also does not possess additional hyperparameters, such as a numeric tolerance or the number of optimization steps. ", "page_idx": 7}, {"type": "text", "text": "Apart from the factorization itself, the computational cost of BSR and AOF are nearly identical. Both methods produce (banded) lower triangular matrices, so computing the inverse matrices or solving linear systems can be done within milliseconds to seconds using forward substitution. Note that, in principle, one could even exploit the Toeplitz structure of $p$ -BSR, but we found this not to yield any practical benefti in our experiments. Computing the sensitivity is trivial for $p$ -BSR using Corollary 1, and it is still efficient for AOF by the dynamic program proposed in Choquette-Choo et al. [2023a]. ", "page_idx": 7}, {"type": "text", "text": "Expected Approximation Error. As a first numeric experiment, we evaluate the expected approximation error for workload matrices that reflect different SGD settings. Specifically, we use workload matrices (9) for $n\\in\\{100,200,\\dots,1000,1500,2000\\}$ , with $\\alpha=\\{\\bar{0}.99\\bar{,}0.999,0.9999,1\\}$ , and $\\beta\\;\\in\\;\\{0,0.9\\}$ , either with single participation, $k\\,=\\,1$ , or repeated participation, $b\\,=\\,100$ , $k=n/100$ . Figure 1 shows the expected approximate error, $\\mathcal{E}(B,C)$ , of the proposed BSR, AOF, as well as the baseline factorizations, $A=A\\cdot\\operatorname{Id}$ and $A=\\operatorname{Id}\\cdot A$ in two exemplary cases. Additional results for other privacy levels can be found in Appendix C. ", "page_idx": 7}, {"type": "image", "img_path": "KSyTvgoSrX/tmp/963b591f35b70fa0ae001fed2abc77a1ee75df99efecf09527c2d0b472fbbbd8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 1: Expected approximation error of BSR, AOF and baseline factorizations for two different hyperparameter settings (left: $\\alpha=0.999,\\beta=0$ , right: $\\alpha=1,\\beta=0.9)$ ) with repeated participation $(\\dot{b}=\\dot{1}00,k=n/100)$ . See Section 4 for details. ", "page_idx": 8}, {"type": "image", "img_path": "KSyTvgoSrX/tmp/2437ff8a1b2810a38b0497a869577b6ee574f66a542a337cfbd0c27a465bfe53.jpg", "img_caption": ["Figure 2: Classification accuracy (mean and standard deviation over 5 runs with different random seeds) on CIFAR-10 for BSR, AOF, and baselines for $(\\epsilon,\\delta)=(4,10^{-5})$ for independent training runs. Left: one epoch, different batch sizes. Right: different number of epochs, constant batch size. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "The results confirm our expectations from the theoretical analysis: in particular, BSR\u2019s expected approximation error is quite close to AOF\u2019s, typically within a few percent (left plot). Both methods are clearly superior to the naive factorizations. For large matrix sizes, BSR sometimes even yields slightly better values than AOF (right plot). However, we believe this to be a numeric artifact of us having to solve AOF with less-than-perfect precision. ", "page_idx": 8}, {"type": "text", "text": "Private Model Training on CIFAR-10. To demonstrate the usefulness of BSR in practical settings, we follow the setup of Kairouz et al. [2021] and report results for training a simple ConvNet on the CIFAR-10 dataset (see Table 1 in Appendix C for the architecture). Specifically, we adapt Google\u2019s reference implementation of DP-SGD in jax Bradbury et al. [2018] to work with the different matrix factorizations: BSR, AOF, and the two baselines. To reflect the setting of single-participation training, we split the 50,000 training examples into batches of size $m\\in\\{1000,50\\bar{0},250,20\\bar{0},100,50,25\\}$ , resulting in $n\\in\\{100,200,400,500,1000,2000\\}$ update steps. For repeated participation, we fix the batch size to 500 and run $k\\in\\{1,2,\\ldots,10,15,20\\}$ epoch of training, i.e. $n=100k$ and $b=100$ . In both cases, $20\\%$ of the training examples are used as validation sets to determine the learning rate $\\eta\\in\\{0.01,0.05,0.1,0.5,1\\}$ , weight decay parameters $\\alpha\\in\\{0.99,0.999,0.9999,1\\}$ , and momentum $\\beta\\in\\{0,0.9\\}$ . Figure 2 shows the test set accuracy of the model trained with hyperparameters that achieved the highest validation accuracy.4 One can see the expected effect that in DP model training, more update steps/epochs do not necessary lead to higher accuracy due to the need to add more noise. The quality of models trained with BSR is mostly identical to AOF. When training for a large number of epochs it achieves even better slightly results, but this could also be an artifact of us having to solve AOF with reduced precision in this regime. Both methods are clearly superior to the baselines. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion and Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduce an efficient and effective approach to the matrix factorization mechanism for SGDbased model training with differential privacy. The proposed banded square root factorization (BSR) factorization achieves results on par with the previous state-of-the-art, and clearly superior to baseline methods. At the same time, it does not suffer from the previous method\u2019s computational overhead, thereby making differentially private model training practical even for large scale problems. ", "page_idx": 9}, {"type": "text", "text": "Despite the promising results, some open questions remain. On the theoretical side, the asymptotic optimality of BSR without weight decay is still unresolved because the current upper bounds on the expected approximation error do not match the provided lower bounds. Based on the experimental results, we believe this discrepancy lies with the lower bounds, which we suspect should be linear in the number of participations. We observe that BSR achieves results comparable to AOF, although we cannot currently prove this due to the insufficient understanding of AOF\u2019s theoretical properties; nonetheless, we consider it a promising research direction. On the practical side, it would be interesting to extend the guarantees to even more learning scenarios, such as variable learning rates. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was supported by the Scientific Service Units (SSU) of ISTA through resources provided by Scientific Computing (SciComp). We thank Monika Henzinger and Jalaj Upadhyay for their valuable comments on the earlier versions of this manuscript. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep learning with differential privacy. In ACM Special Interest Group on Security, Audit and Control (SIGSAC), 2016.   \nN. Batir, H. K\u00fcc\u00fck, and S. Sorgun. Convolution identities involving the central binomial coefficients and Catalan numbers. Transactions on Combinatorics, 2021.   \n\u00c5. Bj\u00f6rck and S. Hammarling. A Schur method for the square root of a matrix. Linear Algebra and its Applications, 1983.   \nJ. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.   \nC. A. Choquette-Choo, A. Ganesh, R. McKenna, H. B. McMahan, J. K. Rush, A. G. Thakurta, and X. Zheng. (Amplified) banded matrix factorization: A unified approach to private training. In Conference on Neural Information Processing Systems (NeurIPS), 2023a.   \nC. A. Choquette-Choo, A. Ganesh, T. Steinke, and A. Thakurta. Privacy amplification for matrix mechanisms. In International Conference on Learning Representations (ICLR), 2023b.   \nC. A. Choquette-Choo, H. B. McMahan, K. Rush, and A. Thakurta. Multi-epoch matrix factorization mechanisms for private machine learning. International Conference on Machine Learing (ICML), 2023c.   \nC. A. Choquette-Choo, K. Dvijotham, K. Pillutla, A. Ganesh, T. Steinke, and A. Thakurta. Correlated noise provably beats independent noise for differentially private learning. In International Conference on Learning Representations (ICLR), 2024.   \nE. Deadman, N. J. Higham, and R. Ralha. Blocked schur algorithms for computing the matrix square root. In International Workshop on Applied Parallel Computing (PARA), 2012.   \nS. Denisov, H. B. McMahan, J. Rush, A. Smith, and G. A. Thakurta. Improved Differential Privacy for SGD via optimal private linear operators on adaptive streams. In Conference on Neural Information Processing Systems (NeurIPS), 2022.   \nK. Dvijotham, H. B. McMahan, K. Pillutla, T. Steinke, and A. Thakurta. Efficient and near-optimal noise generation for streaming differential privacy, 2024. arXiv:2404.16706 [cs.DS].   \nC. Dwork. Differential privacy. In International colloquium on automata, languages, and programming (ICALP), 2006.   \nC. Dwork and A. Roth. The algorithmic foundations of differential privacy. Foundations and Trends in Theoretical Computer Science, 2014.   \nC. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography Conference (TCC), 2006.   \nJ. F. Elliott. The characteristic roots of certain real symmetric matrices, 1953. M. S. thesis, University of Tennessee,.   \nS. A. Gershgorin. \u00dcber die Abgrenzung der Eigenwerte einer Matrix. Proceedings of the USSR Academy of Sciences. Mathematics Series, (6):749\u2013754, 1931.   \nF. Granqvist, C. Song, \u00c1ine Cahill, R. van Dalen, M. Pelikan, Y. S. Chan, X. Feng, N. Krishnaswami, V. Jina, and M. Chitnis. pfl-research: simulation framework for accelerating research in private federated learning, 2024. URL https://arxiv.org/abs/2404.06430.   \nD. H. Greene and D. E. Knuth. Mathematics for the Analysis of Algorithms. Springer, 1990.   \nM. Henzinger, J. Upadhyay, and S. Upadhyay. A unifying framework for differentially private sums under continual observation. In Symposium on Discrete Algorithms (SODA), 2024.   \nP. Kairouz, B. McMahan, S. Song, O. Thakkar, A. Thakurta, and Z. Xu. Practical and private (deep) learning without sampling or shuffilng. In International Conference on Machine Learing (ICML), 2021.   \nA. Koloskova, R. McKenna, Z. Charles, J. Rush, and H. B. McMahan. Gradient descent with linearly correlated noise: Theory and applications to differential privacy. In Conference on Neural Information Processing Systems (NeurIPS), 2023.   \nA. Kurakin, S. Song, S. Chien, R. Geambasu, A. Terzis, and A. Thakurta. Toward training at imagenet scale with differential privacy, 2022. arXiv:2201.12328.   \nC. Li, G. Miklau, M. Hay, A. McGregor, and V. Rastogi. The matrix mechanism: Optimizing linear counting queries under Differential Privacy. International Conference on Very Large Data Bases (VLDB), 2015.   \nZ. Li, B. Ding, C. Zhang, N. Li, and J. Zhou. Federated matrix factorization with privacy guarantee. International Conference on Very Large Data Bases (VLDB), 2021.   \nF. J. MacWilliams and N. J. A. Sloane. The theory of error-correcting codes. Elsevier, 1977.   \nR. McKenna. Scaling up the banded matrix factorization mechanism for differentially private ml, 2024. arXiv:2405.15913 [cs.DS].   \nH. B. McMahan, Z. Xu, and Y. Zhang. A hassle-free algorithm for private learning in practice: Don\u2019t use tree aggregation, use blts, 2024. arXiv:2408.08868 [cs.DS].   \nT. V. Nguyen, Y. Mori, and T. Mori. Relaxed monotonic conditions for Schur stability of real polynomials. IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences, 90(10):2326\u20132328, 2007.   \nN. Papernot and T. Steinke. Hyperparameter tuning with renyi differential privacy. In International Conference on Learning Representations (ICLR), 2021.   \nN. Ponomareva, H. Hazimeh, A. Kurakin, Z. Xu, C. Denison, H. B. McMahan, S. Vassilvitskii, S. Chien, and A. G. Thakurta. How to DP-fy ML: A practical guide to machine learning with differential privacy. Journal of Artificial Intelligence Research (JAIR), 77:1113\u20131201, 2023.   \nSebastienB. Trace norm of a triangular matrix with only ones above the diagonal, 2017. URL https://math.stackexchange.com/questions/1857078/ trace-norm-of-a-triangular-matrix-with-only-ones-above-the-diagonal.   \nH. Shin, S. Kim, J. Shin, and X. Xiao. Privacy enhanced matrix factorization for recommendation with local differential privacy. IEEE Transactions on Knowledge and Data Engineering, 30(9), 2018.   \nS. Vadhan. The complexity of differential privacy. Tutorials on the Foundations of Cryptography, 2017.   \nD. Yu, H. Zhang, W. Chen, J. Yin, and T.-Y. Liu. Gradient perturbation is underrated for differentially private convex optimization. In International Joint Conference on Artificial Intelligence (IJCAI), 2020.   \nX. Zhang, J. Ding, M. Wu, S. T. Wong, H. Van Nguyen, and M. Pan. Adaptive privacy preserving deep learning algorithms for medical data. In Winter Conference on Applications of Computer Vision (WACV), 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A General introduction to differential privacy. ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Differential privacy [Dwork, 2006] is a robust framework designed to provide strong privacy guarantees for statistical analyses and data sharing. It aims to protect individual data points in a dataset while still allowing meaningful aggregate information to be extracted. Unlike traditional data anonymization techniques, which might involve removing identifiers or aggregating data, differential privacy offers a mathematical definition of privacy that quantifies the amount of privacy loss and ensures that the risk of identifying any individual\u2019s data remains low, even when combined with other data sources.To formalize this concept, a randomized mechanism $M$ is said to provide $(\\varepsilon,\\delta)$ -differential privacy if, for all data sets $D$ and $D^{\\prime}$ that differ in one element, and for all subsets of the mechanism\u2019s output space $S$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[M(D)\\in S]\\leq e^{\\varepsilon}\\cdot\\operatorname*{Pr}[M(D^{\\prime})\\in S]+\\delta\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "For a detailed introduction to differential privacy, we recommend the books \"The Algorithmic Foundations of Differential Privacy\" by Dwork and Roth [2014] and \"The Complexity of Differential Privacy\" by Vadhan [2017]. ", "page_idx": 12}, {"type": "text", "text": "B Source code for computing AOF ", "text_level": 1, "page_idx": 12}, {"type": "image", "img_path": "KSyTvgoSrX/tmp/c0ae7edab67a521466990de134cc1a728f7f2dd2ea36de6fdaa93908f9298880.jpg", "img_caption": [], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "C Network architecture for CIFAR-10 experiments ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Table 1: ConvNet architecture for CIFAR-10 experiments ", "page_idx": 12}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/6ce434e80b434fd8b53d6874ac0a0f75227225dd28fe1d2fa34b364781fd4a8a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "D Additional Experimental Results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section we provide additional experiments comparing BSR, AOF and baselines: Figures 3 and 4 and following tables show their expected approximation error (lower is better) for workload matrices stemming from SGD with different hyperparameter settings. Figure 5) and following tables show the accuracy of resulting classifiers on CIFAR-10 (higher is better) for different privacy levels. ", "page_idx": 13}, {"type": "text", "text": "The results show the same trends as the one in Section 4. BSR achieves almost identical expected approximation error as AOF, and results in equally good classifiers. In some cases, results for BSR even improve over AOF\u2019s. Presumably this is because of numerical issues in solving the optimization problem for AOF. ", "page_idx": 13}, {"type": "image", "img_path": "KSyTvgoSrX/tmp/9d62b6c0fe1c5d7cb79b00efda96d46386905d9e11254f23b2361d7bd400654d.jpg", "img_caption": ["Figure 3: Expected approximation error of $p$ -BSR, AOF and baseline factorizations with lmultiple participations and $p=b=100$ . "], "img_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "KSyTvgoSrX/tmp/13d74b7808e3a1380003203481e9b10c361f1148057e9380e1fe7d35ec2391bc.jpg", "img_caption": ["Figure 4: Expected approximation error of BSR, AOF and baseline factorizations with single participation $\\mathrm{\\Delta}k=1$ , $p=b=n!$ ). "], "img_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/b0ad0a51dfa752e82faa3430f0da16f5de77e92eaa7ac98f2207eaa656a67a36.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/42ef5af9afba39376caebac0f86f0bb22dd073982a1837e9d29e6b2e0767a80a.jpg", "table_caption": ["Table 2: Numeric results for Figure 3 as well as a plain square root decomposition: $\\alpha=1$ , $\\beta=0$ , $k=1$ , $b=k/n$ "], "table_footnote": ["Table 3: Numeric results for Figure 3 as well as a plain square root decomposition: $\\alpha=1$ , $\\beta=0.9$ , $k=1$ , $b=k/n$ "], "page_idx": 15}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/8ae330fbb42d960dbc71ab33bb1674cf47d0fdda3031c0ede77c0aba97b86f5f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/b370f1883c84cb662c344b082b32b90887614d4a353d06daf426bf19dbf4f6ba.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/e52f7ff20e953f62d20cd2f1d3808f97dea4a66baa9f3384215bebdba8c4dae2.jpg", "table_caption": ["Table 5: Numeric results for Figure 3 as well as a plain square root decomposition: $\\alpha=0.9999$ , $\\beta=0.9,k=1,b=k/n$ "], "table_footnote": ["Table 6: Numeric results for Figure 3 as well as a plain square root decomposition: $\\alpha\\,=\\,0.999$ , $\\beta=0,k=1,b=k/n$ "], "page_idx": 16}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/f03810c65effef609516e7c728f0ffa88a606f32a6a36490e1ea81d3387fa417.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/1d4df1ebeebf554f0c2b6f4e9adbbb75cbec9c897c0cbc058affe077983c071f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/4f22fb345ba1daf5a5c23a5857bde35c580651418382eac9869d9f59e56d6e39.jpg", "table_caption": [], "table_footnote": ["Table 9: Numeric results for Figure 3 as well as a plain square root decomposition: $\\alpha\\,=\\,0.99$ , $\\beta=0.9,k=1,b=k/n$ "], "page_idx": 17}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/787642285ea902e527fd414b018c51dae76310ac824d0acf4010971091b7a017.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 10: Numeric results for Figure 4 as well as a plain square root decomposition: $\\alpha=1$ , $\\beta=0$ , $b=0$ ", "page_idx": 17}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/33cea6cb0d584a700c674ffe537a547e12051dd0820d2b7573f157c784f0cb66.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 11: Numeric results for Figure 4 as well as a plain square root decomposition: $\\alpha=1$ , $\\beta=0.9$ , $b=0$ ", "page_idx": 18}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/63bda03fde6e997696e61c5bf2af38a594d2cf489253bf4ed8e93906f9273828.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/7cc5fe4e8da20084a0d576bf510d9f08cab2cfdeb17fba5706a5fd1d5b31e7ec.jpg", "table_caption": ["Table 12: Numeric results for Figure 4 as well as a plain square root decomposition: $\\alpha=0.9999$ , $\\beta=0,b=0$ "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/8b032f39ef6261bc9f74b11491d407b92ff9ff768079aa4a28de850f2b05f67b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/136fb0f191c1b3175cdfb19f95e53f7a59ac8d9e7ba8bc37ab356e84b0aed4db.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 15: Numeric results for Figure 4 as well as a plain square root decomposition: $\\alpha=0.999$ , $\\beta=0.9$ , $b=0$ ", "page_idx": 19}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/ba49582231bf2195c51c962e9d680f656afedaa9137f4c5d324baba745e9c10a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/33943dffe750432e6e29793ad23782dc6ce2499f5e4524a5cd11d1b81ec6d332.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "KSyTvgoSrX/tmp/25f34d5865d094ca3683d42bd2c1e12d19a999f6e4210036a338b80c3a66400a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 5: Classification accuracy (mean and standard deviation over 5 runs with different random seeds) on CIFAR-10 for BSR, AOF, and baselines for independent training runs. Top row: classification accuracy on CIFAR-10 with $(\\epsilon,\\delta)=(2,10^{-5})$ . Bottom row: classification accuracy on CIFAR-10 with $\\left(\\epsilon,\\delta\\right)=\\left(8,10^{-5}\\right)$ . Left plots: one epoch, different batch sizes. Right plots: different number of epochs, constant batch size. ", "page_idx": 20}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/d1b9041bcd4bad926bbc219076cf2f317557c013518112642d1f45f63fe5a93e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "", "img_caption": [], "img_footnote": ["Table 18: Numeric values for results in Table 2 left plot (CIFAR-10, single participation, $\\left(\\epsilon,\\delta\\right)=$ $(4,10^{-5})$ . "], "page_idx": 20}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/65de89eab9b4796d99dee805c1452b842e9aafc9b0b145269d2af6712446c6a2.jpg", "table_caption": [], "table_footnote": ["Table 19: Numeric values for results in Table 2 right plot (CIFAR-10, repeated participation, $\\left(\\epsilon,\\delta\\right)=$ $(4,10^{-5})$ . "], "page_idx": 21}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/e9fb53c12c5b2dfc3de20a55780c384b477fc83d6c4ae58b4d61221f705d770f.jpg", "table_caption": [], "table_footnote": ["Table 20: Numeric values for results in Table 5 top left plot (CIFAR-10, single participation, $(\\epsilon,\\delta)=(2,10^{-5})$ . "], "page_idx": 21}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/e2d8e7ed81d94fa12977a1083ac7a9f0ec6947022642cbee71419330c8ab51bb.jpg", "table_caption": [], "table_footnote": ["Table 21: Numeric values for results in Table 5 top right plot (CIFAR-10, repeated participation, $(\\epsilon,\\delta)=(2,10^{-5})$ . "], "page_idx": 21}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/7041d569ee1b0713fcdac0342d70ed16ad992e42079efc293f755d9016137f42.jpg", "table_caption": [], "table_footnote": ["Table 22: Numeric values for results in Table 5 bottom left plot (CIFAR-10, repeated participation, $(\\epsilon,\\delta)=(8,10^{-5})$ . "], "page_idx": 22}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/d692928275f41f0a06474ecbc3dfab2673a4bad81f07fed22ca958e0f24e17c7.jpg", "table_caption": [], "table_footnote": ["Table 23: Numeric values for results in Table 5 bottom right plot (CIFAR-10, repeated participation, $(\\epsilon,\\delta)=(8,10^{-5})$ . "], "page_idx": 22}, {"type": "text", "text": "E Experimental Results for Different Optimizers ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section we report on experimental results when different optimizers are used to (approximately) solve the AOF optimization problem (4). Besides cvxpy (CVX) these are standard gradient descent $(G D)$ and the Limited-Memory Broyden-Fletcher-Goldfarb-Shanno algorithm (LBFGS). The latter two we implement in jax using the optax toolbox. Similar to [Granqvist et al., 2024, ftrl_mechanism.py], we use an adaptive line-search for the step size of the gradient-based methods, which at the same time ensures the positive definiteness constraints of the optimization problem. Our implementation differs from theirs, however, in that our learning rate is not restricted to shrink monotonically, thereby avoiding premature termination. ", "page_idx": 22}, {"type": "text", "text": "E.1 Runtime ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We report the runtimes for the different methods in Tables 24 to 39. For comparison, we also include results for BSR and the CVX optimizers with three tolerance levels in the same settings, where practically feasible. Note that while the experiments for BSR and CVX used a single-core CPU-only environment, the experiments for GD and LBFGS were run on an NVIDIA H100 GPU with 16 available CPU cores. As a consequence, the absolute runtimes are not directly comparable between the methods, but they should rather be seen as illustrations of the scaling behavior of the method for different workload types and problem sizes. ", "page_idx": 22}, {"type": "text", "text": "Indeed, the results show a clear trend: BSR is the fastest, with almost no overhead. Even for the largest problem sizes of $n\\,=\\,10\\,000$ , BSR never took more than 2.5s to despite running in the single-core CPU-only setup. GD and LBFGS benefit strongly from the GPU hardware. In the multiple participation setting $(p=100,k\\,=\\,n/p)$ , they solve most workload sizes within a few minutes, except the largest ones, which for GD can take a few hours. In the single participation setting $\\left.k=1\\right.$ ), LBFGS also occasionally need several hours to converge. In general, stronger weight decay (smaller $\\alpha$ ) tends to lead to lower runtimes, while the use of momentum $\\beta=0.9)$ to higher times until convergence. CVX (on weak hardware) is orders of magnitude slower than the other methods. Furthermore, its runtime grow approximately cubic with the problem size, whereas for GD and LBFGS the relation is not too far from linear. Note that despite the stable patterns described above, all runtime results should be taken with caution, because internal parameters of the optimization, such as the convergence criterion and the specific implementation of the line search can substantially influence the overall runtime as well. ", "page_idx": 22}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/722fb30cb6519fce04cdcb20925369da89983e3767fc4bc9e5d5a1c473009516.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/0fbfc8f896392aa436155c903f0ffebcffb4f2343a1f500311174009818dafaf.jpg", "table_caption": ["Table 25: $\\alpha=1.0$ , $\\beta=0.0$ , p = 100, k = n/p "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "E.2 Expected Approximation Error ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Figures 6 and 7 report the expected approximation errors achieved by the different optimizers of AOF (4) and by BSR. For CVX, we report the smallest error value across all tolerance levels for which the optimization converged. ", "page_idx": 23}, {"type": "text", "text": "The curves show several clear trends. GD and LBFGS generally perform similarly, and achieve expected approximation errors slightly (at most a few percent) lower than BSR. An exception are the problems with momentum $\\beta=0.9)$ ) in the single participation setting, where it appears that SGD occasionally fails to find the optimum for large problem sizes $n\\ge1500]$ ). CVX performs comparably to the other methods for problems without momentum $\\ p=0$ ). With momentum, however, the solutions it find are often worse than the other methods, especially in the single-participation setting and for medium to large problem sizes $n\\geq500)$ . Presumably, even smaller tolerance values would be require here, which, however, would result in even longer runtimes. ", "page_idx": 23}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/4d1125f4e763e075ce35ec3855247297f4c26f4ddf6ee63d30d1acc1013d1920.jpg", "table_caption": ["Table 26: $\\alpha=0.9999$ , $\\beta=0.9$ , $p=100$ , $k=n/p$ "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/e27d507d6804778b2d4222d986415eae82827c9c694cd8a822e49fd134f7d1a6.jpg", "table_caption": ["Table 27: $\\alpha=0.9999$ , $\\beta=0.0$ , $p=100$ , k = n/p "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/408165d5b8fb77c2981bd0576864c3bfbe79518b3ebc099c89b4640f49a6eb2c.jpg", "table_caption": ["Table 28: $\\alpha=0.999$ , $\\beta=0.9$ , $p=100$ , $k=n/p$ "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/6d8930ee1c8f3ec1d795e37882e6f31443a9411d50781c14e24bb897b6f163ef.jpg", "table_caption": ["Table 29: $\\alpha=0.999$ , $\\beta=0.0$ , $p=100$ , $k=n/p$ "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/3895843b6947397d7fd7fc3085436c5b5a7a851449d4a90ba188f075a71615b6.jpg", "table_caption": ["Table 30: $\\alpha=0.99$ , $\\beta=0.9$ , p = 100, k = n/p "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/d0905eeda2ed0bd2589fc787fdcbe55a4453a09ee0c104c492184de3e2a7781a.jpg", "table_caption": ["Table 31: $\\alpha=0.99$ , $\\beta=0.0$ , p = 100, k = n/p "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/987451511fd5615ce01df06fde689121d494162d70275dacebf581ea2a1bec4c.jpg", "table_caption": ["Table 32: $\\alpha=1.0$ , $\\beta=0.9$ , $k=1$ "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/255dafc8674d4fa44eba5ea2b40fc97cc1da6b3e9dd995182695524895b0922e.jpg", "table_caption": ["Table 33: $\\alpha=1.0$ , $\\beta=0.0$ , k = 1 "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/7fbf006a3c63e2d27607fc49450e73e2c58afcb76a1ec9f9851f82564b9a0985.jpg", "table_caption": ["Table 34: $\\alpha=0.9999$ , $\\beta=0.9$ , $k=1$ "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/552af562b441bc4b4bc5d21296600925e0ccf2f60463bf70479054c94544a9ef.jpg", "table_caption": ["Table 35: $\\alpha=0.9999$ , $\\beta=0.0$ , $k=1$ "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Table 36: $\\alpha=0.999$ , $\\beta=0.9$ , k = 1 ", "text_level": 1, "page_idx": 27}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/c8dbf6027a2d84e1e369c27ee308dd3d5264d3de188fede9e044e33988f348d9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/7a95fad81bab2d226e2e6f709e22ca272012a87d079de8e0bac0ac791f1fe286.jpg", "table_caption": ["Table 37: $\\alpha=0.999$ , \u03b2 = 0.0, $k=1$ "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/f319dfe8f95e127a6397bd3fb3873082b4d4c28b66118113b54c29932aae4632.jpg", "table_caption": ["Table 38: $\\alpha=0.99$ , $\\beta=0.9$ , $k=1$ "], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "KSyTvgoSrX/tmp/0204a2a2ed8aec73402241dfb40fca9f61175ff95074cc72ea80520ee85ce1a0.jpg", "table_caption": ["Table 39: $\\alpha=0.99$ , $\\beta=0.0$ , k = 1 "], "table_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "KSyTvgoSrX/tmp/e6daefadcb2915d18971cdd76efd09189a8b59ede291b8c4efb9caf87b78f929.jpg", "img_caption": ["Figure 6: Expected approximation error for AOF with GD, LBFGS and CVX optimizers as well as for BSR in the multiple participations setting. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "KSyTvgoSrX/tmp/80267ace99a51199100e5fdeb32eb6ca01ca2838e3e882cf5a87d2920656f593.jpg", "img_caption": ["Figure 7: Expected approximation error for AOF with GD, LBFGS and CVX optimizers as well as for BSR in the single participation setting. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "F Complete Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section, we provide the complete proofs for our results from the main manuscript. For the convenience of the reader, we also restate the statements themselves. ", "page_idx": 31}, {"type": "text", "text": "F.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Theorem 1 (Square-Root of SGD Workload Matrix). Let $A_{\\alpha,\\beta}$ be the workload matrix (9). Then $A_{\\alpha,\\beta}=C_{\\alpha,\\beta}^{2}$ for $C_{\\alpha,\\beta}=\\mathrm{LDToep}(c_{0},\\ldots,c_{n-1})$ , with $c_{0}=1$ and $\\begin{array}{r}{c_{j}=\\sum_{i=0}^{j}\\alpha^{j-i}r_{j-i}r_{i}\\beta^{i}}\\end{array}$ for $j=1,\\cdot\\cdot\\cdot,n-1$ with coefficients $r_{i}=|\\!\\left({^{-1/2}_{i}}\\right)|$ . For any $p\\in\\{1,\\ldots,n\\}$ , the $p$ -banded BSR matrix C|\u03b1p,|\u03b2 is obtained from this by setting all coefficients cj = 0 for j \u2265p. ", "page_idx": 31}, {"type": "text", "text": "Proof. We observe that $A_{\\alpha,\\beta}$ can be written as ", "page_idx": 31}, {"type": "equation", "text": "$$\nA_{\\alpha,\\beta}=\\left(\\begin{array}{c c c c}{{1}}&{{0}}&{{\\ldots}}&{{0}}\\\\ {{\\alpha}}&{{1}}&{{\\ldots}}&{{0}}\\\\ {{\\vdots}}&{{\\vdots}}&{{\\ddots}}&{{\\vdots}}\\\\ {{\\alpha^{n-1}}}&{{\\alpha^{n-2}}}&{{\\ldots}}&{{1}}\\end{array}\\right)\\times\\left(\\begin{array}{c c c c}{{1}}&{{0}}&{{\\ldots}}&{{0}}\\\\ {{\\beta}}&{{1}}&{{\\ldots}}&{{0}}\\\\ {{\\vdots}}&{{\\vdots}}&{{\\ddots}}&{{\\vdots}}\\\\ {{\\beta^{n-1}}}&{{\\beta^{n-2}}}&{{\\ldots}}&{{1}}\\end{array}\\right)=:E_{\\alpha}\\times E_{\\beta}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Relying on the result from Henzinger et al. [2024], that E11/2 $:E_{1}^{1/2}\\,=\\,\\left(\\begin{array}{c c c c}{{1}}&{{0}}&{{\\hdots\\hdots}}&{{0}}\\\\ {{r_{1}}}&{{1}}&{{\\hdots\\hdots}}&{{0}}\\\\ {{\\vdots}}&{{\\vdots}}&{{\\ddots}}&{{\\vdots}}\\\\ {{r_{n-1}}}&{{r_{n-2}}}&{{\\hdots}}&{{1}}\\end{array}\\right)\\,\\mathrm{with}$ ", "page_idx": 31}, {"type": "text", "text": "$r_{k}=\\left|\\binom{-1/2}{k}\\right|$ , one can check that the square roots of the matrices $E_{\\alpha},E_{\\beta}$ are: ", "page_idx": 31}, {"type": "equation", "text": "$$\nE_{\\alpha}^{1/2}=\\left(\\begin{array}{c c c c}{{1}}&{{0}}&{{\\cdots}}&{{0}}\\\\ {{\\alpha r_{1}}}&{{1}}&{{\\cdots}}&{{0}}\\\\ {{\\vdots}}&{{\\vdots}}&{{\\ddots}}&{{\\vdots}}\\\\ {{\\alpha^{n-1}r_{n-1}}}&{{\\alpha^{n-2}r_{n-2}}}&{{\\cdots}}&{{1}}\\end{array}\\right)\\quad E_{\\beta}^{1/2}=\\left(\\begin{array}{c c c c}{{1}}&{{0}}&{{\\cdots}}&{{0}}\\\\ {{\\beta r_{1}}}&{{1}}&{{\\cdots}}&{{0}}\\\\ {{\\vdots}}&{{\\vdots}}&{{\\ddots}}&{{\\vdots}}\\\\ {{\\beta^{n-1}r_{n-1}}}&{{\\beta^{n-2}r_{n-2}}}&{{\\cdots}}&{{1}}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "An explicit check yields that these matrices commute, i.e. $E_{\\alpha}^{1/2}E_{\\beta}^{1/2}=E_{\\beta}^{1/2}E_{\\alpha}^{1/2}$ . Therefore ", "page_idx": 31}, {"type": "equation", "text": "$$\nC_{\\alpha,\\beta}=A_{\\alpha,\\beta}^{1/2}=E_{\\alpha}^{1/2}\\times E_{\\beta}^{1/2}=\\left(\\begin{array}{l l l l}{1}&{0}&{\\cdots}&{0}\\\\ {c_{1}}&{1}&{\\cdots}&{0}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {c_{n-1}}&{c_{n-2}}&{\\cdots}&{1}\\end{array}\\right),\\ \\mathrm{with}\\ c_{k}=\\sum_{i=0}^{k}\\alpha^{i}r_{i}r_{k-i}\\beta^{k-i}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "F.2 Proof of Theorem 2 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Theorem 2 (Sensitivity for decreasing non-negative Toeplitz matrices). Let $\\begin{array}{r l}{M}&{{}=}\\end{array}$ $\\mathrm{LDToep}(m_{0},\\ldots,m_{n-1})$ be a lower triangular Toeplitz matrix with decreasing non-negative entries, i.e. ", "page_idx": 31}, {"type": "text", "text": "$m_{0}\\geq m_{1}\\geq m_{2}\\geq...\\,m_{n-1}\\geq0.$ . Then its sensitivity (2) in the setting of $b$ -min-separation is ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathrm{sens}_{k,b}(M)=\\Big\\|\\sum_{j=0}^{k-1}M_{[\\cdot,1+j b]}\\Big\\|=\\left(\\sum_{i=0}^{n-1}\\left(\\sum_{j=0}^{\\operatorname*{min}\\{k-1,i/b\\}}m_{i-j b}\\right)^{2}\\right)^{1/2},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $M_{[\\cdot,1+j b]}$ denotes the $(1+j b)$ -th column of $M$ . ", "page_idx": 31}, {"type": "text", "text": "Proof. Because all entries of $M$ are positive, so are the entries of $M^{\\top}M$ . Therefore, the condition is fulfilled such that (2) holds with equality, and ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathrm{ens}_{k,b}^{2}(M)=\\operatorname*{max}_{\\pi\\in\\Pi_{k,b}}\\sum_{i,j\\in\\pi}(M^{\\top}M)_{[i,j]}=\\operatorname*{max}_{\\pi\\in\\Pi_{k,b}}f(\\pi,\\pi)\\quad\\mathrm{for}\\quad f(\\pi,\\pi^{\\prime})=\\sum_{i\\in\\pi}\\sum_{j\\in\\pi^{\\prime}}\\langle M_{[:,i]},M_{[:,j]}\\rangle,\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\Pi_{k,b}=\\{\\;\\pi\\subset\\{1,\\ldots,n\\}\\;:\\;|\\pi|\\leq k\\wedge(\\{i,j\\}\\subset\\pi\\;\\Rightarrow\\;i=j\\;\\vee\\;|i-j|\\geq b)\\;\\}.$ We now establish that $\\{1,1+b,\\ldots,1+(k-1)b\\}$ is an optimal index set, which implies the statement of the theorem. ", "page_idx": 32}, {"type": "text", "text": "To see this, let $\\pi^{*}$ be any optimal solution and let $i^{*}\\in\\pi^{*}$ be a column index that is not as far left as sppolsits $\\pi^{*}$ e,i ni.teo. ,t $\\pi^{*}\\setminus\\{i^{*}\\}\\cup\\{i^{*}-1\\}$ awreo uslmd abllee ra  tvhaalni $i^{*}$ ,n daenxd  stehte  irne $\\Pi_{k,b}$ .i nIfg ,s ruicghh ta, no innedse: $\\pi^{*}=\\pi_{L}^{*}\\dot{\\cup}\\pi_{R}^{*}$ $i^{*}$ wwihtihc $\\pi_{L}^{*}=\\{i\\mid i\\in\\pi\\land i<i^{*}\\}$ u, $\\pi_{R}^{*}=\\{i\\mid i\\in\\pi\\wedge i\\geq i^{*}\\}$ y.  oTnhee np,o switei ocno tnos ttrhuec lt eaft :n $\\pi^{\\prime}=\\pi_{L}^{*}\\cup\\overleftarrow{\\pi}_{R}^{*}$ with $\\overleftarrow{\\pi}_{R}^{*}=\\{i-1\\mid i\\in\\pi_{R}^{*}\\}$ . By the condition on $i^{*}$ , we know $\\bar{\\pi^{\\prime}}\\in\\Pi_{k,b}$ . ", "page_idx": 32}, {"type": "text", "text": "We now prove that $f(\\pi^{\\prime},\\pi^{\\prime})\\ge f(\\pi,\\pi)$ , so $\\pi^{\\prime}$ must also be optimal. First, we observe two inequalities: for any $i,j>1$ : ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\langle M_{[\\cdot,i-1]},M_{[\\cdot,j-1]}\\rangle=\\langle M_{[\\cdot,i]},M_{[\\cdot,j]}\\rangle+m_{n-i+1}m_{n-j+1}\\geq\\langle M_{[\\cdot,i]},M_{[\\cdot,j]}\\rangle,\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and for $i\\geq1,j>1$ : ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\langle M_{[\\cdot,i]},M_{[\\cdot,j-1]}\\rangle=\\displaystyle\\sum_{l=1}^{n}M_{[l,i]},M_{[l,j-1]}=\\displaystyle\\sum_{l=j-1}^{n}m_{l-i}m_{l-j+1}}&{}\\\\ {=\\displaystyle\\sum_{l=j-1}^{n-1}m_{l-i}m_{l-j+1}}&{+}\\\\ {\\ge\\displaystyle\\sum_{l=j}^{n}m_{l-i-1}m_{l-j}}&{}\\\\ {\\ge\\displaystyle\\sum_{l=j}^{n}m_{l-i}m_{l-j}=\\displaystyle\\sum_{l=1}^{n}M_{[l,i]},M_{[l,j]}=\\langle M_{[\\cdot,i]},M_{[\\cdot,j]}\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the last inequality holds because by assumption $m_{l-i-1}\\geq m_{l-i}$ for $l\\geq i+1$ . ", "page_idx": 32}, {"type": "text", "text": "Now, we split $f(\\pi^{\\prime},\\pi^{\\prime})$ and $f(\\pi^{\\prime},\\pi^{\\prime})$ into three terms: the inner products of indices below $i^{*}$ , the ones of terms above $i^{*}$ and the ones between both, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\pi^{*},\\pi^{*})=f(\\pi_{L}^{*},\\pi_{L}^{*})+f(\\pi_{R}^{*},\\pi_{R}^{*})+2f(\\pi_{L}^{*},\\pi_{R}^{*}).}\\\\ &{\\ f(\\pi^{\\prime},\\pi^{\\prime})=f(\\pi_{L}^{*},\\pi_{L}^{*})+f(\\overleftarrow{\\pi}_{R}^{*},\\overleftarrow{\\pi}_{R}^{*})+2f(\\pi_{L}^{*},\\overleftarrow{\\pi}_{R}^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The first term appears identically in both expressions. The second term fulfills ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\overleftarrow{\\pi}_{R}^{*},\\overleftarrow{\\pi}_{R}^{*})=\\displaystyle\\sum_{i,j\\in\\{\\pi}_{R}^{*}}\\langle M_{[\\cdot,i]},M_{[\\cdot,j]}\\rangle=\\displaystyle\\sum_{i,j\\in\\pi_{R}^{*}}\\langle M_{[\\cdot,i-1]},M_{[\\cdot,j-1]}\\rangle}\\\\ &{\\qquad\\qquad\\geq\\displaystyle\\sum_{i,j\\in\\pi_{R}^{*}}\\langle M_{[\\cdot,i]},M_{[\\cdot,j]}\\rangle=f(\\pi_{R}^{*},\\pi_{R}^{*})}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "by Equation (24). The third term fulfills ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\pi_{L}^{*},\\overleftarrow{\\pi}_{R}^{*})=\\displaystyle\\sum_{i\\in\\pi_{L}^{*}}\\sum_{j\\in\\overleftarrow{\\pi}_{R}^{*}}\\left<M_{[\\cdot,i]},M_{[\\cdot,j]}\\right>=\\displaystyle\\sum_{i\\in\\pi_{L}^{*}}\\sum_{j\\in\\pi_{R}^{*}}\\left<M_{[\\cdot,i]},M_{[\\cdot,j-1]}\\right>}\\\\ &{\\qquad\\qquad\\ge\\displaystyle\\sum_{i\\in\\pi_{L}^{*}}\\sum_{j\\in\\pi_{R}^{*}}\\left<M_{[\\cdot,i]},M_{[\\cdot,j]}\\right>=f(\\pi_{L}^{*},\\pi_{R}^{*})}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "by Equation (28). In combination, this establishes $f(\\pi^{\\prime},\\pi^{\\prime})\\geq f(\\pi^{*},\\pi^{*})$ , and since $\\pi^{*}$ was already optimal, the same must hold for $\\pi^{\\prime}$ . ", "page_idx": 32}, {"type": "text", "text": "Using the above construction, we can create a new optimal index sets, $\\pi^{*}$ , until reaching one that does not contain any index $i^{*}$ as described anymore. Then $\\pi^{*}=\\{1,1\\!+\\!b,\\ldots,1\\!+\\!(l\\!-\\!1)b\\}$ for some $l\\in\\mathbb N$ must hold. If $l=k$ , the statement of Theorem 2 is confirmed. Otherwise, $\\pi^{\\prime}=\\{1,\\ldots,1+(k-1)b\\}$ is superset of $\\pi^{*}$ , so because of the positivity of entries, $f(\\pi^{\\prime},\\pi^{\\prime})\\,\\geq\\,f(\\pi^{*},{\\bar{\\pi}}^{*})$ must hold. Once again, because $\\pi^{*}$ was optimal, the same must hold for $\\pi^{\\prime}$ , which concludes the proof. \u53e3 ", "page_idx": 33}, {"type": "text", "text": "F.3 Proof of Corollary 1 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Corollary 1. The sensitivity of the $p$ -BSR for SGD can be computed using formula (10). ", "page_idx": 33}, {"type": "text", "text": "Proof. From (1) we know that $C_{\\alpha,\\beta}$ is a Toeplitz matrix with coefficients $(1,c_{1},\\ldots,c_{n-1})$ , where $\\begin{array}{r}{c_{j}=\\sum_{i=0}^{j}\\alpha^{i}r_{i}r_{j-i}\\beta^{j-i}}\\end{array}$ for $0\\leq\\beta<\\alpha\\leq1$ , with $\\begin{array}{r}{r_{i}\\,=\\,|\\bigl({-}\\!\\!1/2\\bigr)|\\,=\\,\\frac{B_{i}}{4^{i}}}\\end{array}$ , where $B_{i}\\,=\\,\\binom{2i}{i}$ is the $i$ -cen tral binomial coefficient. It suffices to show that $c_{j}\\geq c_{j+1}$ for any $j\\in\\{1,\\ldots,n-1\\}$ . ", "page_idx": 33}, {"type": "text", "text": "First, we show for the $r_{i}$ coefficients: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{r_{i}-r_{i+1}=\\displaystyle\\frac{1}{4^{i}}\\binom{2i}{i}-\\displaystyle\\frac{1}{4^{i+1}}\\binom{2i+2}{i+1}=\\displaystyle\\frac{1}{4^{i}}\\frac{(2i)!}{i!\\,i!}-\\displaystyle\\frac{1}{4^{i+1}}\\frac{(2i+2)!}{(i+1)!\\,(i+1)!}}}\\\\ {{=\\displaystyle\\frac{1}{4^{i}}\\frac{(2i)!}{i!\\,i!}\\Big(1-\\displaystyle\\frac{1}{4}\\frac{(2i+2)(2i+1)}{(i+1)\\,(i+1)}\\Big)=r_{i}\\Big(1-\\displaystyle\\frac{2i+1}{2(i+1)}\\Big)}}\\\\ {{=\\displaystyle\\frac{r_{i}}{2(i+1)}=\\displaystyle\\frac{1}{4^{i}\\cdot2}C_{i+1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\begin{array}{r}{C_{j}=\\frac{1}{j+1}B_{j}=\\frac{1}{j+1}{\\binom{2j}{j}}}\\end{array}$ is the $j$ -th Catalan number. ", "page_idx": 33}, {"type": "text", "text": "Now, we study the case $\\alpha=1$ . If $\\beta=0$ , then $c_{1}=c_{2}=\\cdot\\cdot\\cdot=c_{n}=1$ , so monotonicity is fulfilled. Otherwise, i.e. $0<\\beta<1$ , we write ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{c_{k}-c_{k+1}=\\displaystyle\\sum_{i=0}^{k}r_{i}(r_{k-i}-r_{k+1-i})\\beta^{i}-r_{k+1}r_{0}\\beta^{k+1}}}\\\\ {{\\mathrm{}}}\\\\ {{\\displaystyle{\\geq\\frac{1}{4^{k}}\\sum_{i=0}^{k}\\frac{1}{2}B_{i}C_{k-i}\\beta^{k-i}-\\displaystyle\\frac{1}{4^{k+1}}B_{k+1}\\beta^{k+1}}}}\\\\ {{\\mathrm{}}}\\\\ {{\\displaystyle{=\\frac{\\beta^{k}}{4^{k+1}}\\left[2\\sum_{i=0}^{k}B_{i}C_{k-i}\\beta^{-i}-B_{k+1}\\beta\\right]}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Using the classic identity between Catalan numbers, $\\begin{array}{r}{2\\sum_{i=0}^{k}B_{k-i}C_{i}=B_{k+1}}\\end{array}$ , e.g. [Batir et al., 2021, Identity 4.2] we obtain ", "page_idx": 33}, {"type": "equation", "text": "$$\n=\\frac{\\beta^{k}}{4^{k+1}}\\left[2\\sum_{i=0}^{k}B_{i}C_{k-i}(\\beta^{-i}-\\beta)\\right]>0,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the last inequality follow from the fact that $\\beta^{-i}-\\beta>0$ for each $i=0,\\dots,k$ and any $\\beta<1$ .   \nThis proves the monotonicity of $c_{k}$ . ", "page_idx": 33}, {"type": "text", "text": "For $\\alpha<1$ , we observe that $\\begin{array}{r}{c_{j}=\\alpha^{j}\\sum_{i=1}^{j}r_{i}r_{k-i}\\;\\gamma^{j-i}}\\end{array}$ for $\\textstyle\\gamma={\\frac{\\alpha}{\\beta}}<1$ . Clearly, the sequence $\\alpha^{j}$ is decreasing, and by the above argument, the sum is decreasing, too. Consequently, $c_{j}$ is the product of two decreasing sequences, so it is also decreasing, which concludes the proof. \u53e3 ", "page_idx": 33}, {"type": "text", "text": "F.4 Useful Lemmas ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Before the remaining proofs, we establish a number of useful lemmas. ", "page_idx": 33}, {"type": "text", "text": "Lemma 2. For any $C\\in\\mathbb{R}^{n\\times n}$ with $C^{\\top}C\\geq0$ it holds for any $b\\in\\{1,\\ldots,n\\}$ that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathrm{sens}_{1,b}^{2}(C)=\\|C\\|_{2,\\infty},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\begin{array}{r}{\\|C\\|_{2,\\infty}^{2}=\\operatorname*{max}_{i=1,\\ldots,n}\\|C_{[\\cdot,i]}\\|^{2}}\\end{array}$ . ", "page_idx": 33}, {"type": "text", "text": "Proof. This follows directly from Theorem 2: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname{sens}_{1,b}^{2}(C)=\\operatorname*{max}_{\\pi\\in\\Pi_{1,b}}\\sum_{i,j\\in\\pi}[C^{\\top}C]_{i,j}=\\operatorname*{max}_{i=1,\\ldots,n}[C^{\\top}C]_{i,i}=\\operatorname*{max}_{i=1,\\ldots,n}\\|C_{[\\cdot,i]}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Lemma 3. For any $C\\in\\mathbb{R}^{n\\times n}$ with $C^{\\top}C\\geq0$ it holds for any $b\\in\\{1,\\ldots,n\\}$ and $k\\in\\{1,\\ldots,\\frac{b}{n}\\}$ that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{k}{n}\\|C\\|_{F}^{2}\\leq\\mathrm{sens}_{k,b}^{2}(C)\\leq k\\|C\\|_{F}^{2},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof. We first show the upper bound. Observe that for any $\\pi\\subset[n]$ : ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i,j\\in\\pi}[C^{\\top}C]_{i,j}=\\displaystyle\\sum_{i,j\\in\\pi}\\langle C_{[\\cdot,i]},C_{[\\cdot,j]}\\rangle\\leq\\displaystyle\\sum_{i,j\\in\\pi}\\|C_{[\\cdot,i]}\\|\\|C_{[\\cdot,j]}\\|}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad=\\big(\\displaystyle\\sum_{i\\in\\pi}\\|C_{[\\cdot,i]}\\|)^{2}\\leq|\\pi|\\displaystyle\\sum_{i\\in\\pi}\\|C_{[\\cdot,i]}\\|^{2}\\leq|\\pi|\\|C\\|_{F}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Therefore, using Theorem 2: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname{sens}^{2}(C)=\\operatorname*{max}_{\\pi\\in\\Pi_{k,b}}\\sum_{i,j\\in\\pi}[C^{\\top}C]_{i,j}\\leq k\\|C\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "For the lower bound, we introduce some additional notation. Let $\\tilde{\\Pi}_{k}$ be the set of $b$ -separated index sets with exactly $k$ elements. Then, from Theorem 2, we obtain ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathrm{sens}_{k,b}^{2}(C)=\\operatorname*{max}_{\\pi\\in\\Pi_{k,b}}\\sum_{i,j\\in\\pi}[C^{\\top}C]_{i j}\\ge\\operatorname*{max}_{\\pi\\in\\Pi_{k,b}}\\sum_{i\\in\\pi}[C^{\\top}C]_{i i}=\\operatorname*{max}_{\\pi\\in\\Pi_{k,b}}S(\\pi)\\ge\\operatorname*{max}_{\\pi\\in\\tilde{\\Pi}_{k}}S(\\pi),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "with the notation $\\begin{array}{r}{S(I)=\\sum_{i\\in I}\\|C_{[\\cdot,i]}\\|^{2}}\\end{array}$ for any index set $I\\subset\\{1,\\ldots,n\\}$ . ", "page_idx": 34}, {"type": "text", "text": "Now, we prove by backwards induction over $k=1,\\ldots,\\frac{n}{b}$ : ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi\\in\\tilde{\\Pi}_{k}}S(\\pi)\\geq\\frac{k}{n}\\|C\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "As base case, let $\\begin{array}{r}{k=\\frac{n}{b}}\\end{array}$ . Denote by $\\pi_{i}:=\\{i,i+b,i+2b,\\ldots,i+(n-b)\\}$ for $i=1,\\dots,b$ the uniformly spaced index sets. By construction they all fulflil $\\pi_{i}\\in\\tilde{\\Pi}_{n/b}$ and $\\textstyle\\bigcup_{i=1}^{n}\\pi_{i}=[n]$ , where the union is disjoint. Therefore ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi\\in\\tilde{\\Pi}_{n/b}}S(\\pi)\\geq\\operatorname*{max}_{i=1,\\ldots,b}S(\\pi_{i})\\geq\\frac{1}{b}\\sum_{i=1}^{b}S(\\pi_{i})=\\frac{1}{b}S([n])=\\frac{1}{b}\\|C\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This proves the statement (49), because $\\textstyle{\\frac{1}{b}}={\\frac{k}{n}}$ in this case. ", "page_idx": 34}, {"type": "text", "text": "As an induction step, we prove that if (49) holds for some value $k\\ \\leq\\ {\\frac{n}{b}}$ , then it also holds for $k-1\\geq1$ . ", "page_idx": 34}, {"type": "text", "text": "Let $\\pi^{*}\\;\\in\\;\\mathrm{argmax}_{\\pi\\in\\tilde{\\Pi}_{k}}\\,S(\\pi)$ and $j^{*}\\,=\\,\\mathrm{argmin}_{j\\in\\pi^{*}}\\,S(\\{j\\})$ , such that we know that $S(\\{j\\})\\le$ $\\scriptstyle{\\frac{1}{k}}S(\\pi^{*})$ . Now, set $\\pi^{\\prime}=\\pi^{*}\\setminus\\{j^{*}\\}$ . Because $\\pi^{\\prime}\\in\\tilde{\\Pi}_{k-1}$ , it follows that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi\\in\\tilde{\\Pi}_{k-1}}S(\\pi)\\geq S(\\pi^{\\prime})=S(\\pi^{*})-S(\\{j\\})\\geq\\frac{k-1}{k}S(\\pi^{*})\\geq\\frac{k-1}{n}\\|C\\|_{F}^{2},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where in the last step we used the induction hypothesis. This concludes the proof. ", "page_idx": 34}, {"type": "text", "text": "Lemma 4. For $C_{\\alpha,\\beta}$ as in (1), and $k=1$ , it holds that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{j=1}^{n}\\sum_{i=0}^{n-j}c_{i}^{2}\\le\\mathcal{E}(C_{\\alpha,\\beta},C_{\\alpha,\\beta})\\le\\sum_{i=0}^{n-1}c_{i}^{2}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof. From Lemmas 3 and 2 we obtain ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathcal{E}(C_{\\alpha,\\beta},C_{\\alpha,\\beta})\\leq\\frac{1}{\\sqrt{n}}\\|C_{\\alpha,\\beta}\\|_{F}\\,\\mathrm{sens}_{1,b}(C_{\\alpha,\\beta})\\leq\\Big(\\mathrm{sens}_{1,b}(C_{\\alpha,\\beta})\\Big)^{2}\\leq\\|C\\|_{2,\\infty}^{2}=\\sum_{i=0}^{n-1}c_{i}^{2},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the last identify follows from the explicit form of $C_{\\alpha,\\beta}$ . The lower bound follows from ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathcal{E}(C_{\\alpha,\\beta},C_{\\alpha,\\beta})=\\frac{1}{\\sqrt{n}}\\|C_{\\alpha,\\beta}\\|_{F}\\operatorname{sens}_{1,b}(C_{\\alpha,\\beta})\\geq\\frac{1}{n}\\|C\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and again the explicit form of $\\|C\\|_{F}^{2}$ . ", "page_idx": 35}, {"type": "text", "text": "Lemma 5. For $\\begin{array}{r}{r_{j}=|\\binom{-1/2}{j}|=\\frac{1}{4^{j}}\\binom{2j}{j}}\\end{array}$ it holds that: ", "page_idx": 35}, {"type": "equation", "text": "$$\nr_{0}=1\\;\\;\\;\\;\\;\\;\\;a n d\\;\\;\\;\\;\\;\\;r_{1}=\\frac{1}{2}\\;\\;\\;\\;\\;\\;\\;a n d\\;i n\\;g e n e r a l\\;\\;\\;\\;\\;\\;\\;\\frac{1}{2\\sqrt{j}}\\leq r_{j}\\leq\\frac{1}{\\sqrt{\\pi j}}\\;\\;\\;\\;\\;\\;\\;f o r\\;j\\geq1.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. The double inequality is a particular case of a more general pair of binomial inequalities when $k=j$ and $m=2j$ : ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{m}{8k(m-k)}}2^{m H(k/m)}\\leq\\binom{m}{k}\\leq\\sqrt{\\frac{m}{2\\pi k(m-k)}}2^{m H(k/m)},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $H(k/m)$ is the binary entropy function, with $H(1/2)=1$ . The proof of the general result (56), can be found in MacWilliams and Sloane [1977, Chapter 10, Lemma 7, p309]. \u53e3 ", "page_idx": 35}, {"type": "text", "text": "Lemma 6. Let $\\begin{array}{r}{c_{k}=\\sum_{j=0}^{k}\\alpha^{j}r_{j}r_{k-j}\\beta^{k-j}}\\end{array}$ as in (1). Then $c_{0}=1$ , and for $j\\geq1$ : ", "page_idx": 35}, {"type": "equation", "text": "$$\n{\\frac{\\alpha^{j}}{2\\sqrt{j+1}}}\\leq c_{j}\\leq{\\frac{\\alpha^{j}}{(1-{\\frac{\\beta}{\\alpha}})\\sqrt{j+1}}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. We exploit the upper and lower bounds from Lemma 5. First, we write $\\mathit{c}_{k}=$ $\\begin{array}{r}{\\alpha^{k}\\sum_{j=0}^{k}r_{j}r_{k-j}\\gamma^{j}}\\end{array}$ with $\\begin{array}{r}{\\gamma:=\\frac{\\beta}{\\alpha}}\\end{array}$ \u03b1 . Then we check immediately that $c_{0}=1$ and $\\begin{array}{r}{c_{1}=\\frac{1}{2}(\\alpha+\\beta)=}\\end{array}$ 2\u03b1 (1 + \u03b3) \u2264 2\u03b11\u22121\u03b3 . ", "page_idx": 35}, {"type": "text", "text": "For $j\\geq2$ we derive the upper bound by ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{c_{j}}{\\alpha^{j}}=r_{j}(1+\\gamma^{j})+\\displaystyle\\sum_{i=1}^{j-1}r_{i}r_{j-i}\\dot{r}^{i}\\le\\frac{1+\\gamma^{j}}{\\sqrt{\\pi}j}+\\displaystyle\\sum_{i=1}^{j-1}\\frac{\\gamma^{i}}{\\pi\\sqrt{i(j-i)}}}\\\\ &{\\quad\\le\\frac{1+\\gamma^{j}}{\\sqrt{\\pi}j}+\\displaystyle\\sum_{i=1}^{j-1}\\frac{\\gamma^{i}}{\\pi\\sqrt{j-1}}\\le\\frac{\\sqrt{\\pi}-1}{\\pi\\sqrt{j}}+\\frac{\\sqrt{\\pi}-1}{\\pi\\sqrt{j}}\\gamma^{j}+\\frac{1}{\\pi\\sqrt{j-1}}\\displaystyle\\sum_{i=0}^{j}\\gamma^{i}}\\\\ &{\\quad=\\frac{\\sqrt{\\pi}-1}{\\pi\\sqrt{j}}(1+\\gamma^{j})+\\displaystyle\\frac{1}{\\pi\\sqrt{j-1}}\\displaystyle\\frac{1}{(1-\\beta)}}\\\\ &{\\quad=\\underbrace{\\left(\\frac{\\sqrt{\\pi}-1}{\\sqrt{j}}\\right)\\sqrt{j+1}}_{\\le1}\\frac{(1+\\gamma^{j})}{\\pi\\sqrt{j+1}}+\\underbrace{\\sqrt{j+1}}_{\\le7-1}\\frac{1}{\\pi\\sqrt{j+1}}\\frac{1}{(1-\\gamma)}}\\\\ &{\\quad\\le\\frac{3}{\\pi\\sqrt{j+1}}\\displaystyle\\frac{1}{(1-\\gamma)}\\le\\frac{1}{\\sqrt{j+1}}\\displaystyle\\frac{1}{(1-\\gamma)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "which proves the upper bound on $a_{j}$ . The lower bound for $j\\geq1$ follows trivially from ", "page_idx": 35}, {"type": "equation", "text": "$$\nc_{j}\\geq\\alpha^{j}r_{j}\\geq\\frac{\\alpha^{j}}{2\\sqrt{j}}\\geq\\frac{\\alpha^{j}}{2\\sqrt{j+1}}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Lemma 7. For $j\\in\\{1,\\ldots,n\\}$ it holds ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\frac{\\log(j+1)}{4}\\leq\\sum_{i=0}^{j-1}c_{i}^{2}\\leq\\frac{1+\\log j}{(1-\\beta)^{2}}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "for $\\alpha=1$ , and otherwise ", "page_idx": 36}, {"type": "equation", "text": "$$\n1\\leq\\sum_{i=0}^{j-1}c_{i}^{2}\\leq\\frac{1}{(\\alpha-\\beta)^{2}}\\log\\left(\\frac{1}{1-\\alpha^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. We first prove the result for $\\alpha=1$ . Combining Lemmas 4 and 6 we obtain ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{i=0}^{j-1}c_{i}^{2}\\leq\\frac{1}{(1-\\beta)^{2}}\\sum_{i=0}^{j-1}\\frac{1}{i+1}=\\frac{1}{(1-\\beta)^{2}}\\sum_{i=1}^{j}\\frac{1}{i}\\leq\\frac{1+\\log j}{(1-\\beta)^{2}}.}\\\\ {\\displaystyle\\sum_{i=0}^{j-1}c_{i}^{2}\\geq\\frac{1}{4}\\sum_{i=0}^{j-1}\\frac{1}{i+1}=\\frac{1}{4}\\sum_{i=1}^{j}\\frac{1}{i}\\geq\\frac{\\log(j+1)}{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "For $\\alpha<1$ , if follows analogously: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{i=0}^{j-1}c_{i}^{2}\\leq\\frac{1}{(1-\\frac{\\alpha}{\\beta})^{2}}\\sum_{i=0}^{j-1}\\frac{\\alpha^{2i}}{i+1}\\leq\\frac{1}{(\\alpha-\\beta)^{2}}\\sum_{i=1}^{\\infty}\\frac{\\alpha^{2i}}{i}=\\frac{1}{(\\alpha-\\beta)^{2}}\\log\\left(\\frac{1}{1-\\alpha^{2}}\\right).}\\\\ {\\displaystyle\\sum_{i=0}^{j-1}c_{i}^{2}\\geq\\frac{1}{4}\\sum_{i=0}^{j-1}\\frac{\\alpha^{2i}}{i+1}=\\frac{1}{4\\alpha^{2}}\\sum_{i=1}^{j}\\frac{\\alpha^{2i}}{i}=\\frac{1}{4\\alpha^{2}}\\Big[\\displaystyle\\sum_{i=1}^{\\infty}\\frac{\\alpha^{2i}}{i}-\\displaystyle\\sum_{i=j+1}^{\\infty}\\frac{\\alpha^{2i}}{i}\\Big]}\\\\ {\\displaystyle~~~~~~~~~~\\geq\\frac{1}{4\\alpha^{2}}\\Big[\\log\\left(\\displaystyle\\frac{1}{1-\\alpha^{2}}\\right)-\\displaystyle\\frac{\\alpha^{2(j+1)}}{(j+1)(1-\\alpha^{2})}\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the last term emerges from $\\begin{array}{r}{\\sum_{i=j+1}^{\\infty}\\frac{\\alpha^{2i}}{i}\\ge\\frac{\\alpha^{2(j+1)}}{j+1}\\sum_{i=0}^{\\infty}\\alpha^{2i}=\\frac{\\alpha^{2(j+1)}}{j+1}\\frac{1}{1-\\alpha^{2}}.}\\end{array}$ ", "page_idx": 36}, {"type": "text", "text": "Lemma 8. Let $0\\leq\\beta<\\alpha\\leq1$ . Let $\\sigma_{1}\\geq\\cdot\\cdot\\geq\\sigma_{n}$ be the sorted list of singular values of $A_{\\alpha,\\beta}$ . If $\\alpha<1$ , then for $j=1,\\dots,n$ : ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\frac{1}{(1+\\alpha)(1+\\beta)}\\leq\\sigma_{j}\\leq\\frac{1}{(1-\\alpha)(1-\\beta)}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and ", "page_idx": 36}, {"type": "equation", "text": "$$\nn\\leq\\|A_{\\alpha,\\beta}\\|_{*}\\leq\\frac{n}{(1-\\alpha)(1-\\beta)}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "If $\\alpha=1$ , then for $j=1,\\dots,n,$ , ", "page_idx": 36}, {"type": "equation", "text": "$$\n{\\frac{2}{\\pi}}{\\frac{1}{1+\\beta}}{\\frac{n}{j}}\\leq\\sigma_{j}\\leq{\\frac{1}{1-\\beta}}{\\frac{n}{j}}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and consequently ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\frac{2}{\\pi}\\frac{(n+1)\\log(n+1)}{1+\\beta}\\leq\\|A_{1,\\beta}\\|_{*}\\leq\\frac{(n+1)(1+\\log n)}{1+\\beta}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. The statements on the singular values follow from the following Lemma 9, because $A_{\\alpha,\\beta}=$ $E_{\\alpha}E_{\\beta}$ . Because $E_{\\alpha}$ and $E_{\\beta}$ are diagonalizable and they commute, we have $\\sigma_{n}(E_{\\beta})\\sigma_{j}(E_{\\alpha})\\,\\le$ $\\sigma_{j}(E_{\\alpha}E_{\\beta})\\le\\sigma_{1}(E_{\\beta})\\sigma_{j}(E_{\\alpha})$ . For $\\alpha<1$ the lower bound follows from $\\|A_{\\alpha,\\beta}\\|_{*}\\geq\\mathrm{trace}\\,\\bar{A}_{\\alpha,\\beta}$ , and the upper bound follows from the identity $\\begin{array}{r}{\\|A_{\\alpha,\\beta}\\|_{*}=\\sum_{j=1}^{n}\\sigma_{j}}\\end{array}$ . ", "page_idx": 36}, {"type": "text", "text": "For $\\alpha=1$ , the bounds follow from the same identity together with the fact that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\log(n+1)\\leq\\sum_{j=1}^{n}{\\frac{1}{j}}\\leq\\log(n)+1.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Lemma 9 (Singular values of $E_{t}$ ). For $0\\le t\\le1$ , let $E_{t}=\\mathrm{LDToep}(1,t,\\cdot\\cdot\\cdot,t^{n-1})\\in\\mathbb{R}^{n\\times n}$ . Then the singular values $\\sigma_{1}(E_{t})\\geq\\cdot\\cdot:\\geq\\sigma_{n}(E_{t})$ fulfill for $i=1,\\hdots,n$ : ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{1}{1+t}\\leq\\sigma_{i}(E_{t})\\leq\\frac{1}{1-t}\\quad f o r\\,0\\leq t<1,\\qquad a n d\\qquad\\sigma_{i}(E_{1})=\\frac{1}{\\sin\\left(\\frac{i-\\frac{1}{2}}{n+\\frac{1}{2}}\\frac{\\pi}{2}\\right)}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. We follow the steps of SebastienB [2017], and use that the singular values of $E_{t}$ are the reciprocals of the singular values of $E_{t}^{-1}$ , which themselves are the eigenvalues of $(E_{t})^{-1}((E_{t})^{-1})^{\\top}=:T$ , i.e., for $i=1,\\hdots,n$ : ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sigma_{j}(E_{t})=\\frac{1}{\\sqrt{\\lambda_{n+1-j}(T)}}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "The $E_{t}^{-1}$ and $T$ can be computed explicitly as ", "page_idx": 37}, {"type": "equation", "text": "$$\nE_{t}^{-1}=\\left(\\begin{array}{c c c c c c}{1}&{0}&{0}&{0}&{\\ldots}&{0}\\\\ {-t}&{1}&{0}&{0}&{\\ldots}&{0}\\\\ {0}&{-t}&{1}&{0}&{\\ldots}&{0}\\\\ {\\vdots}&{\\ddots}&{\\ddots}&{\\ddots}&{\\vdots}&{\\vdots}\\\\ {0}&{\\ldots}&{0}&{-t}&{1}&{0}\\\\ {0}&{\\ldots}&{0}&{0}&{-t}&{1}\\end{array}\\right),\\qquad T=\\left(\\begin{array}{c c c c c c}{1}&{-t}&{0}&{\\ldots}&{0}\\\\ {-t}&{1+t^{2}}&{-t}&{\\ldots}&{0}\\\\ {0}&{\\ddots}&{\\ddots}&{\\vdots}&{\\vdots}\\\\ {0}&{\\ldots}&{-t}&{1+t^{2}}&{-t}\\\\ {0}&{\\ldots}&{0}&{-t}&{1+t^{2}}\\end{array}\\right)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Lemma 10. All eigenvalues, $\\mu,$ of $T$ fulfill ", "page_idx": 37}, {"type": "equation", "text": "$$\n(1-t)^{2}\\leq\\mu\\leq(1+t)^{2}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. By Gershgorin\u2019s circle theorem [Gershgorin, 1931], we know that $\\mu$ fulflils i) $|1-\\mu|\\leq t$ , i.e. $1-t\\leq t\\leq\\mu\\leq1+t$ or ii) $|1+t^{2}-\\bar{\\mu}|\\leq2\\dot{t}$ , i.e. $1-2t+t^{2}\\le\\mu\\le1+2t+t^{2}$ . For $t\\in[0,1]$ the first condition implies the second, so (79) must hold. \u53e3 ", "page_idx": 37}, {"type": "text", "text": "Case I: For $t<1$ , the statement (76) follows from Lemma 10 in combination with (77). ", "page_idx": 37}, {"type": "text", "text": "Case II: For $t=1$ the matrix simplifies to $T=\\left(\\begin{array}{c c c c c}{{1}}&{{-1}}&{{0}}&{{\\ldots}}&{{0}}\\\\ {{-1}}&{{2}}&{{-1}}&{{\\ldots}}&{{0}}\\\\ {{0}}&{{\\vdots}}&{{\\ddots}}&{{\\vdots}}&{{\\vdots}}\\\\ {{0}}&{{\\ldots}}&{{-1}}&{{2}}&{{-1}}\\\\ {{0}}&{{\\ldots}}&{{0}}&{{-1}}&{{2}}\\end{array}\\right).$ Note that $T$ is not exactly Toeplitz, because of the top left entry, so closed-form expressions for the eigenvalues of tridiagonal Toeplitz matrices do not apply to it. Instead, we can compute its eigenvalues explicitly. Matrices of this form have been studied by Elliott [1953]; for completeness, we provide a full proof here. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "Let $\\mu$ be an eigenvalue of $T$ with eigenvector $\\Psi=\\left(\\Psi_{0},\\ldots,\\Psi_{n-1}\\right)$ . From the eigenvector equation $T\\Psi=\\mu\\Psi$ we obtain ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c}{\\mu\\Psi_{0}=\\Psi_{0}-\\Psi_{1}}\\\\ {\\mu\\Psi_{k}=-\\Psi_{k-1}+2\\Psi_{k}-\\Psi_{k+1}}\\\\ {\\mu\\Psi_{n-1}=-\\Psi_{n-2}+2\\Psi_{n-1}}\\end{array}}\\end{array}\\quad\\begin{array}{r}{\\mathrm{for}\\,k=1,\\dots,n-2}\\\\ {\\mathrm{for}\\,k=1,\\dots,n-2}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "which yields a linear recurrence relation ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\Psi_{k+1}=(2-\\mu)\\Psi_{k}-\\Psi_{k-1}\\quad{\\mathrm{for~}}k=1,\\dots,n-2\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "with two boundary conditions ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\Psi_{1}=(1-\\mu)\\Psi_{0}}\\\\ {\\Psi_{n-2}=(2-\\mu)\\Psi_{n-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We solve the recurrence relation using the polynomial method [Greene and Knuth, 1990]. The characteristic polynomial of (83) is $P(\\bar{z})=\\bar{z}^{2}+(\\mu-2)z+1$ . Its roots are ", "page_idx": 37}, {"type": "equation", "text": "$$\nr_{\\pm}={\\frac{2-\\mu}{2}}\\pm{\\sqrt{{\\left({\\frac{2-\\mu}{2}}\\right)}^{2}-1}}={\\frac{(2-\\mu)\\pm i{\\sqrt{4-(2-\\mu)^{2}}}}{2}}=e^{\\pm i\\theta}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "for some value $\\theta\\in[0,2\\pi)$ . Note that the expression under the second square root is positive, because of Lemma 10. The last equation is a consequence of, $|r_{\\pm}|^{2}={\\textstyle{\\frac{1}{4}}}\\left((2-\\mu)^{2}+(4-(2-\\mu)^{2})\\right)=1$ . Consequently, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mu=2-2\\Re(e^{i\\theta})=2-2\\cos\\theta.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "From standard results on linear recurrence, it follows that any solution to (83) has the form $\\Psi_{j}=$ $c_{1}(r_{+})^{j}+c_{2}(r_{-})^{j}$ for some constants $c_{1},c_{2}\\in\\mathbb{C}$ . The fact that $\\Psi_{j}$ must be real-valued implies that $c_{1}=c_{2}=:\\alpha e^{i\\phi}$ for some values $\\alpha\\in\\mathbb{R},\\phi\\in[0,2\\pi)$ . Dropping the normalization constant (which we could recover later if needed), we obtain ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\Psi_{j}=e^{i(\\phi+j\\theta)}+e^{-i(\\phi+j\\theta)}=2\\cos(\\phi+j\\theta).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Next, we use the boundary conditions to establish values for $\\phi$ and $\\theta$ . ", "page_idx": 38}, {"type": "text", "text": "Equation (85) can be rewritten as ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\cos(\\phi+(n-2)\\theta)=2\\cos(\\theta)\\cos((n-1)\\theta)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "which, using $2\\cos(\\alpha+\\beta)=\\cos(a+b)+\\cos(a-b)$ , simplifies to ", "page_idx": 38}, {"type": "equation", "text": "$$\n0=\\cos(\\phi+n\\theta)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Consequently, $\\phi+n\\theta={\\textstyle{\\frac{1}{2}}}\\pi+k\\pi$ must hold for some $k\\in\\mathbb{N}$ . ", "page_idx": 38}, {"type": "text", "text": "Equation (84) can be rewritten as ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\cos(\\phi+\\theta)=(2\\cos(\\theta)-1)\\cos(\\phi)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "which simplifies to ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\cos(\\phi)=\\cos(\\theta-\\phi)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "One solution to this would be $\\theta=0$ , but that would implies $\\mu=0$ , which is inconsistent with $T$ being an invertible matrix. So instead, it must hold that $\\begin{array}{r}{\\phi=\\frac{\\theta}{2}+k\\pi}\\end{array}$ for some $k\\in\\mathbb{N}$ . ", "page_idx": 38}, {"type": "text", "text": "Combining both conditions and solving for $\\theta$ we obtain ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\theta={\\frac{{\\frac{1}{2}}+k}{n+{\\frac{1}{2}}}}\\pi={\\frac{1}{2}}\\pi+k\\pi\\qquad{\\mathrm{for~some~}}k\\in\\mathbb{N}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Each such value $\\theta_{k}$ for $k\\in\\{0,\\ldots,n-1\\}$ yields an eigenvector with associated eigenvalue $\\mu=$ $2-2\\cos\\theta_{k}=4\\sin^{2}(\\theta_{k}/2)$ . Now, (76) follows from this in combination with (77). \u53e3 ", "page_idx": 38}, {"type": "text", "text": "F.5 Proof of Theorem 3 ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Theorem 3 (Expected approximation error with single participation). Let $A_{\\alpha,\\beta}\\,\\in\\,\\mathbb{R}^{n\\times n}$ be the workload matrix (9) of SGD with momentum $0\\leq\\beta<1$ and weight decay parameter $0<\\alpha\\leq1$ , where $\\alpha>\\beta$ . Assume that each data item can contribute at most once to an update vector (e.g. single participation, $k=1$ ). Then, the expected approximation error of the square root factorization, $\\bar{A_{\\alpha,\\beta}}\\bar{=C_{\\alpha,\\beta}^{2}}$ , fulfills ", "page_idx": 38}, {"type": "equation", "text": "$$\n1\\leq\\mathcal{E}(C_{\\alpha,\\beta},C_{\\alpha,\\beta})\\leq\\frac{1}{(\\alpha-\\beta)^{2}}\\log\\frac{1}{1-\\alpha^{2}}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "for $\\alpha<1$ , and ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left\\{1,\\frac{\\log(n+1)-1}{4}\\right\\}\\leq\\mathcal{E}(C_{1,\\beta},C_{1,\\beta})\\leq\\frac{1+\\log(n)}{(1-\\beta)^{2}}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. The proof consists of a combination of Lemmas 4 and 7. Because in the single participation $k=1$ , so we need just the first column of matrix $C_{\\alpha,\\beta}$ : ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathcal E(C_{\\alpha,\\beta},C_{\\alpha,\\beta})\\leq\\sum_{i=0}^{n-1}c_{i}^{2}\\leq\\left\\{\\begin{array}{l l}{\\displaystyle\\frac{1+\\log n}{(1-\\beta)^{2}}}&{\\quad\\mathrm{otherwise}.}\\\\ {\\displaystyle\\frac{1}{(\\alpha-\\beta)^{2}}\\log\\frac{1}{1-\\alpha^{2}}}&{\\quad\\mathrm{otherwise}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "which proves the upper bounds. For the lower bounds, for any $\\alpha\\leq1$ : ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\displaystyle\\mathcal{E}(C_{\\alpha,\\beta},C_{\\alpha,\\beta})\\ge\\frac{1}{n}\\sum_{j=0}^{n-1}\\sum_{i=0}^{j}c_{i}^{2}\\ge\\frac{1}{n}\\sum_{j=0}^{n-1}c_{0}=1.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Also, for $\\alpha=1$ : ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathcal{E}(C_{1,\\beta},C_{1,\\beta})\\geq\\frac{1}{n}\\sum_{j=1}^{n}\\sum_{i=0}^{j-1}c_{i}^{2}\\geq\\frac{1}{4n}\\sum_{j=1}^{n}\\log(j+1)=\\frac{\\log(\\,(n+1)!\\,)}{4n}\\geq\\frac{\\log(n+1)-1}{4},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "because $\\log({(n+1)!})\\geq(n+1)\\log(n+1)-n.$ . ", "page_idx": 39}, {"type": "text", "text": "F.6 Proof of Theorem 5 ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Theorem 5. Assume the setting of Theorem 3. Then, the baseline factorizations $A_{\\alpha,\\beta}=A_{\\alpha,\\beta}\\cdot\\mathrm{Id}$ and $A_{\\alpha,\\beta}=\\operatorname{Id}\\cdot A_{\\alpha,\\beta}$ fulfill, for $\\alpha<1$ , ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\xi(A_{\\alpha,\\beta},\\mathrm{Id})=\\frac{\\sqrt{1+\\alpha\\beta}}{\\sqrt{(1-\\alpha\\beta)(1-\\alpha^{2})(1-\\beta^{2})}}+o(1)\\quad a n d\\quad\\varepsilon(A_{1,\\beta},\\mathrm{Id})\\leq\\frac{\\sqrt{n}}{\\sqrt{2}(1-\\beta)}+o(\\sqrt{n})\n$$", "text_format": "latex", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathcal{E}(\\mathrm{Id},A_{\\alpha,\\beta})=\\frac{\\sqrt{1+\\alpha\\beta}}{\\sqrt{(1-\\alpha\\beta)(1-\\alpha^{2})(1-\\beta^{2})}}\\,+o(1)\\quad a n d\\quad\\mathcal{E}(\\mathrm{Id},A_{1,\\beta})\\leq\\frac{\\sqrt{n}}{1-\\beta}\\,+o(\\sqrt{n}).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Proof. For $\\alpha=1$ , by Lemma 2 we have: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\mathrm{sens}_{1,b}^{2}(A_{1,\\beta})=\\displaystyle\\sum_{i=0}^{n-1}a_{i}^{2}=\\displaystyle\\sum_{i=0}^{n-1}\\left(\\frac{1-\\beta^{i+1}}{1-\\beta}\\right)^{2}=\\displaystyle\\frac{n}{(1-\\beta)^{2}}-\\frac{2\\beta}{(1-\\beta)^{2}}\\displaystyle\\sum_{i=0}^{n-1}\\beta^{i}+\\frac{\\beta^{2}}{(1-\\beta)^{2}}\\displaystyle\\sum_{i=0}^{n-1}\\beta^{i}}}\\\\ {{=\\displaystyle\\frac{n}{(1-\\beta)^{2}}-\\frac{2\\beta^{n+1}}{(1-\\beta)^{3}}+\\frac{\\beta^{2n+2}}{(1-\\beta)^{2}(1-\\beta^{2})}=\\displaystyle\\frac{n}{(1-\\beta)^{2}}(1+o(1)).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "For $\\alpha<1$ : ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{sens}_{1,b}^{2}(A_{\\alpha,\\beta})=\\displaystyle\\sum_{i=0}^{n-1}a_{i}^{2}=\\displaystyle\\sum_{i=0}^{n-1}\\frac{(\\alpha^{i+1}-\\beta^{i+1})^{2}}{(\\alpha-\\beta)^{2}}}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\frac{1}{\\alpha-\\beta}\\Big[\\alpha^{2}\\sum_{i=0}^{n-1}(\\alpha^{2})^{i}-2\\alpha\\beta\\sum_{i=0}^{n-1}(\\alpha\\beta)^{i}+\\beta^{2}\\sum_{i=0}^{n-1}(\\beta^{2})^{i}\\Big]}\\\\ &{\\qquad\\qquad=\\displaystyle\\frac{1}{(\\alpha-\\beta)^{2}}\\left[\\frac{\\alpha^{2}}{1-\\alpha^{2}}-\\frac{2\\alpha\\beta}{1-\\alpha\\beta}+\\frac{\\beta^{2}}{1-\\beta^{2}}\\right](1+o(1))}\\\\ &{\\qquad\\qquad=\\displaystyle\\frac{1+\\alpha\\beta}{(1-\\alpha\\beta)(1-\\alpha^{2})(1-\\beta^{2})}(1+o(1)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Together with ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle||A_{\\beta}||_{F}^{2}/n=\\frac{1}{n}\\sum_{j=0}^{n-1}(n-j)\\left[\\sum_{i=0}^{j}\\beta^{i}\\right]^{2}=\\frac{1}{n}\\sum_{j=0}^{n-1}(n-j)\\left[\\frac{1-\\beta^{j+1}}{1-\\beta}\\right]^{2}}\\\\ {\\displaystyle=\\frac{1}{n(1-\\beta)^{2}}\\sum_{j=0}^{n-1}(n-j)(1-2\\beta^{j+1}+\\beta^{2j+2})}\\\\ {\\displaystyle=\\frac{(n+1)}{2(1-\\beta)^{2}}+O(1)=\\frac{n}{2(1-\\beta)^{2}}(1+o(1))}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "as $\\beta<1$ and the sum $\\textstyle\\sum_{j=0}^{n-1}j\\beta^{j+1}$ is uniformly bounded by $\\begin{array}{r}{\\beta\\sum_{j=0}^{\\infty}j\\beta^{j}=\\frac{\\beta^{2}}{(1-\\beta)^{2}}}\\end{array}$ For $\\alpha<1$ : ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle||A_{\\alpha,\\beta}||_{F}^{2}/n=\\frac{1}{n}\\sum_{j=0}^{n-1}(n-j)\\frac{(\\alpha^{j+1}-\\beta^{j+1})^{2}}{(\\alpha-\\beta)^{2}}=\\sum_{j=0}^{n-1}\\frac{(\\alpha^{j+1}-\\beta^{j+1})^{2}}{(\\alpha-\\beta)^{2}}+o(1)}}\\\\ {{\\displaystyle=\\frac{1+\\alpha\\beta}{(1-\\alpha\\beta)(1-\\alpha^{2})(1-\\beta^{2})}(1+o(1))}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where the second equality due to the fact that we average over the sequence $j x^{j+1}$ which converges to 0 for $|x|<1$ . \u53e3 ", "page_idx": 40}, {"type": "text", "text": "F.7 Proof of Theorem 6 ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Theorem 6 (Approximation error of BSR). Let $A_{\\alpha,\\beta}\\in\\mathbb{R}^{n\\times n}$ be the workload matrix (9) of SGD with momentum $0\\leq\\beta<1$ and weight decay $0<\\alpha\\le1$ , with $\\alpha>\\beta$ . Let $A_{\\alpha,\\beta}=B_{\\alpha,\\beta}^{|p|}C_{\\alpha,\\beta}^{|p|}$ B\u03b1,\u03b2C\u03b1,\u03b2, be its banded square root factorization as in Definition $^3$ . Then, for any $b\\in\\{1,\\ldots,n\\}$ , $p\\leq b,$ , and $k\\in\\{1,\\ldots,\\frac{n}{b}\\}$ it holds: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathcal{E}(B_{\\alpha,\\beta}^{|p|},C_{\\alpha,\\beta}^{|p|})=\\left\\{\\begin{array}{l l}{\\!O_{\\beta}\\left(\\sqrt{\\frac{n k\\log p}{p}}\\right)+O_{\\beta,p}(\\sqrt{k})\\quad}&{\\mathrm{~}f o r\\,\\alpha=1,}\\\\ {\\!O_{\\beta,p,\\alpha}\\left(\\sqrt{k}\\right)\\quad}&{\\mathrm{~}f o r\\,\\alpha<1.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proof. Consider a Lower Triangular Toeplitz (LTT) matrix multiplication: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\!\\!\\begin{array}{c c c c c}{a_{1}}&{0}&{\\ldots}&{0}\\\\ {a_{2}}&{a_{1}}&{\\ldots}&{0}\\\\ {\\ldots}&{\\ldots}&{\\ldots}&{\\ldots}\\\\ {a_{n}}&{a_{n-1}}&{\\ldots}&{a_{1}}\\end{array}\\!\\!\\right)\\times\\left(\\!\\!\\begin{array}{c c c c c}{b_{1}}&{0}&{\\ldots}&{0}\\\\ {b_{2}}&{b_{1}}&{\\ldots}&{0}\\\\ {\\ldots}&{\\ldots}&{\\ldots}&{\\ldots}\\\\ {b_{n}}&{b_{n-1}}&{\\ldots}&{b_{1}}\\end{array}\\!\\!\\right)=\\left(\\!\\!\\begin{array}{c c c c c}{c_{1}}&{0}&{\\ldots}&{0}\\\\ {c_{2}}&{c_{1}}&{\\ldots}&{0}\\\\ {\\ldots}&{\\ldots}&{\\ldots}&{\\ldots}\\\\ {c_{n}}&{c_{n-1}}&{\\ldots}&{c_{1}}\\end{array}\\!\\!\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where $\\begin{array}{r}{c_{j}=\\sum_{i=1}^{j}a_{i}b_{n+1-i}}\\end{array}$ so the LTT structure is preserved with multiplication that allows us to work with sequences and their convolutions rather than matrix multiplication. For instance, we would write the previous product in the form: ", "page_idx": 40}, {"type": "equation", "text": "$$\n(a_{1},\\ldots,a_{n})*(b_{1},\\ldots,b_{n})=(c_{1},\\ldots,c_{n}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "The inverse of the Lower Triangular Toeplitz matrix remains a Lower Triangular Toeplitz (LTT) matrix because we can find a unique sequence $\\left(c_{1},\\ldots,c_{n}\\right)$ such that: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{}}&{{(c_{1},\\dotsc,...,c_{n})*(a_{1},\\dotsc...,a_{n})=(1,0,\\dotsc...,0)}}\\\\ {{}}&{{c_{j}=-\\displaystyle\\frac{1}{a_{1}}\\sum_{i=1}^{j-1}c_{j}a_{j+1-i},\\mathrm{~and~}\\ c_{1}=\\displaystyle\\frac{1}{a_{1}},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "with the restriction that $a_{1}\\neq0$ ; otherwise, the original matrix was not invertible. We consider the banded square root factorization $A_{\\alpha,\\beta}=B_{\\alpha,\\beta}^{|p|}C_{\\alpha,\\beta}^{|p|}$ B|\u03b1p,|\u03b2C|\u03b1p,|\u03b2 which is characterized by the following identity: ", "page_idx": 40}, {"type": "equation", "text": "$$\n(b_{0},\\ldots,b_{n-1})*(1,c_{1},\\ldots,c_{p-1},0,\\ldots,0)=(1,c_{1},\\ldots,c_{n-1})*(1,c_{1},\\ldots,c_{n-1}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "We will bound the Frobenius norm of the LTT matrix $(b_{0},\\ldots,b_{n-1})$ . By the uniqueness of the solution, we obtain that for the first $p$ values we have $b_{i}=c_{i}$ . For the next $p$ values we have the following formula: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{b_{p+j}+\\cdot\\cdot+b_{p}c_{j}+\\cdot\\cdot\\cdot+b_{j+1}c_{p-1}=c_{p+j}+\\cdot\\cdot\\cdot+c_{p+1}c_{p-1}+\\cdot\\cdot\\cdot+c_{p+j}}}\\\\ {{b_{p+j}+b_{p+j-1}c_{1}+\\cdot\\cdot\\cdot+b_{p}c_{j}=2\\left(c_{p+j}+\\cdot\\cdot\\cdot+c_{p}c_{j}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "By induction argument, we can see that $b_{p+j}=2c_{p+j}$ for $0\\leq j\\leq p-1$ . For the remaining $n-2p$ values we will prove convergence to a constant. ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\sum_{j=0}^{p-1}b_{j-i}c_{i}=a_{j}=\\frac{\\alpha^{j+1}-\\beta^{j+1}}{\\alpha-\\beta}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We make an ansatz for the solution of the linear recurrence in the form: ", "page_idx": 41}, {"type": "equation", "text": "$$\nb_{j}=\\frac{\\alpha^{j+1}}{(\\alpha-\\beta)\\sum_{i=0}^{p-1}c_{i}\\alpha^{-i}}-\\frac{\\beta^{j+1}}{(\\alpha-\\beta)\\sum_{i=0}^{p-1}c_{i}\\beta^{-i}}+\\alpha^{j}y_{j},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $y_{j}$ represents the error terms, which will be proven to converge to 0. The sequence $y_{j}$ satisfies the following recurrence formula: ", "page_idx": 41}, {"type": "equation", "text": "$$\ny_{j}=-\\sum_{i=1}^{p-1}y_{j-i}c_{i}\\alpha^{-i}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We denote $w_{j}=c_{j}\\alpha^{-j}$ which is a decreasing sequence because the values correspond to the $C_{1,\\beta/\\alpha}$ matrix. We rewrite the recurrence in matrix notation: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\begin{array}{c c c c c c c}{-w_{1}}&{-w_{2}}&{-w_{3}}&{\\ldots}&{-w_{p-2}}&{-w_{p-1}}\\\\ {1}&{0}&{0}&{\\ldots}&{0}&{0}\\\\ {0}&{1}&{0}&{\\ldots}&{0}&{0}\\\\ {0}&{0}&{1}&{\\ldots}&{0}&{0}\\\\ {\\cdots}&{\\cdots}&{\\cdots}&{\\cdots}&{\\cdots}&{\\cdots}\\\\ {0}&{0}&{0}&{\\ldots}&{1}&{0}\\end{array}\\right)\\,\\left(\\begin{array}{c}{y_{k-1}}\\\\ {y_{k-2}}\\\\ {y_{k-3}}\\\\ {y_{k-4}}\\\\ {\\ldots}\\\\ {y_{k-b}}\\end{array}\\right)=\\left(\\begin{array}{c}{y_{k}}\\\\ {y_{k-1}}\\\\ {y_{k-2}}\\\\ {y_{k-3}}\\\\ {\\ldots}\\\\ {y_{k-b+1}}\\end{array}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "To show that the error terms $y_{j}$ goes to 0 as $j$ goes to infinity, we first study the characteristic polynomial of the associate homogeneous relations: ", "page_idx": 41}, {"type": "equation", "text": "$$\ng(\\lambda)=\\lambda^{p-1}+w_{1}\\lambda^{p-2}+\\cdots+w_{p-2}\\lambda+w_{p-1}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Because $1\\,>\\,w_{1}\\,>\\,w_{2}\\,>\\,\\cdot\\,\\cdot\\,>\\,w_{b-1}\\,>\\,0$ , it follows from Schur\u2019s (relaxed) stability condition [Nguyen et al., 2007, Theorem 1] that all its (complex) roots lie inside of the open unit circle. Therefore, all solutions to the homogeneous relation converge to zero at a rate exponential in $j$ and yj = o(1) and j\u221e=0 yj2 = O\u03b1,\u03b2,p(1). Then we can bound the Frobenious norm of the matrix B|\u03b1p,|\u03b2 as: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\|B_{\\alpha,\\beta}^{|p|}\\|_{F}^{2}\\le\\sum_{j=0}^{n-1}b_{j}^{2}\\le\\sum_{j=0}^{p-1}c_{j}^{2}+\\sum_{j=p}^{n-1}\\frac{\\alpha^{2j+2}}{(\\alpha-\\beta)^{2}\\left[\\sum_{i=0}^{p-1}w_{i}\\right]^{2}}+\\alpha^{2j}y_{j}^{2}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We use the following lower bound for the sum of $w_{j}$ : ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\sum_{j=0}^{p-1}w_{j}\\geq\\frac{1}{2}\\sum_{j=0}^{p-1}\\frac{1}{\\sqrt{j+1}}\\geq\\sqrt{p+1}-1.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Combining these bounds we can upper bound the Frobenious norm of the matrix $B_{\\alpha,\\beta}^{|p|}$ the following way: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\|B_{\\alpha,\\beta}^{|p|}\\|_{F}^{2}/n\\leq\\left\\{\\frac{1}{(\\alpha-\\beta)^{2}}\\log\\left(\\frac{1}{1-\\alpha^{2}}\\right)+\\frac{\\alpha^{2}}{(\\sqrt{p+1}-1)^{2}(\\alpha-\\beta)^{2}}+O_{\\alpha,\\beta,p}(1)\\right.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Simplifying for the leading terms in asymptotics, we have: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\|B_{\\alpha,\\beta}^{|p|}\\|_{F}^{2}/n=\\left\\{{\\cal O}_{\\alpha,\\beta,p}(1)\\begin{array}{l l}{{}}&{{\\mathrm{for}\\;\\alpha<1}}\\\\ {{}}&{{}}\\\\ {{{\\cal O}_{\\beta}\\left(\\frac{n}{p}\\right)+{\\cal O}_{p,\\beta}(1)}}&{{\\mathrm{for}\\;\\alpha=1.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Sensitivity of $C_{\\beta}^{|p|}$ . For the $b$ -min-separation participation sensitivity we have the following bound for any : ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\displaystyle\\mathrm{sens}_{k,b}^{2}(C_{\\alpha,\\beta}^{|p|})\\le k\\sum_{j=0}^{p-1}c_{j}^{2}\\le\\left\\{\\begin{array}{l l}{\\displaystyle\\frac{k}{(\\alpha-\\beta)^{2}}\\log\\left(\\frac{1}{1-\\alpha^{2}}\\right)}&{\\qquad\\mathrm{for}\\ \\alpha<1}\\\\ {\\displaystyle k\\frac{1+\\log(p)}{(1-\\beta)^{2}}}&{\\qquad\\mathrm{for}\\ \\alpha=1.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Combining sensitivity with the upper bound for the Frobenious norm we obtain: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathcal{E}(B_{\\alpha,\\beta}^{|p|},C_{\\alpha,\\beta}^{|p|})=\\left\\{\\begin{array}{l l}{\\displaystyle O_{p,\\alpha,\\beta}(\\sqrt{k})}&{\\mathrm{for}\\ \\alpha<1}\\\\ {\\displaystyle O_{\\beta}\\left(\\sqrt{\\frac{n k\\log p}{p}}\\right)+O_{\\beta,p}(\\sqrt{k})}&{\\mathrm{for}\\ \\alpha=1.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "F.8 Proof of Theorem 7 for Square Root Factorization ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Theorem 7 (Approximation error of Square Root Factorization). Let $A_{\\alpha,\\beta}\\in\\mathbb{R}^{n\\times n}$ be the workload matrix (9) of $S G D$ with momentum $0\\leq\\beta<1$ and weight decay $0\\,<\\,\\alpha\\,\\leq\\,1$ , with $\\alpha\\,>\\,\\beta$ . Let $A_{\\alpha,\\beta}=C_{\\alpha,\\beta}^{2}$ be its square root factorization. Then, for any $b\\in\\{1,\\ldots,n\\}$ and $\\begin{array}{r}{k=\\frac{n}{b}}\\end{array}$ it holds: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathcal{E}(C_{\\alpha,\\beta},C_{\\alpha,\\beta})=\\left\\{\\Theta_{\\beta}\\left(k\\sqrt{\\log n}+\\sqrt{k}\\log n\\right)\\qquad\\begin{array}{l l}{f o r\\,\\alpha=1,}\\\\ {\\Theta_{\\alpha,\\beta}\\bigl(\\sqrt{k}\\bigr)}&{\\qquad f o r\\,\\alpha<1.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof. We prove the case without weight decay $\\alpha=1$ ) and with weight decay $(\\alpha<1)$ ) separately. ", "page_idx": 42}, {"type": "text", "text": "Case 1) no weight decay $\\left.\\alpha=1\\right.$ ). ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "We start by bounding the $b$ -min-separation sensitivity: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\operatorname{sens}_{k,b}^{2}(C_{1,\\beta})=\\sum_{i=0}^{k-1}\\sum_{j=0}^{k-1}\\langle(C_{1,\\beta})_{[\\cdot,i b]},(C_{1,\\beta})_{[\\cdot,j b]}\\rangle.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Consider a scalar product for a general pair of indices, $j>i$ : ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\langle(C_{1,\\beta})_{[\\cdot,i]},(C_{1,\\beta})_{[\\cdot,j]}\\rangle=\\sum_{t=0}^{n-1-j}c_{t}c_{j-i+t}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Using the bounds on $c_{k}$ (6) for $\\alpha=1$ we can lower and upper bound this sum by: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=0}^{n-1-j}c_{t}c_{j-i+t}\\leq\\frac{1}{(1-\\beta)^{2}}\\sum_{t=1}^{n-j}\\frac{1}{\\sqrt{t(j-i+t)}}\\leq\\frac{1}{(1-\\beta)^{2}}\\int_{0}^{n-j}\\frac{d x}{\\sqrt{x(j-i+x)}}}\\\\ {\\displaystyle\\sum_{t=0}^{n-1-j}c_{t}c_{j-i+t}\\geq\\frac{1}{4}\\sum_{t=1}^{n-j}\\frac{1}{\\sqrt{t(j-i+t)}}\\geq\\frac{1}{4}\\int_{1}^{n-j}\\frac{d x}{\\sqrt{x(j-i+x)}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "We can compute the indefinite integral explicitly: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\int{\\frac{d x}{\\sqrt{x(j-i+x)}}}=F\\left({\\frac{j-i}{x}}\\right)+C\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "for $\\begin{array}{r}{F(a)=2\\log\\left(\\sqrt{\\frac{1}{a}+1}+\\sqrt{\\frac{1}{a}}\\right)}\\end{array}$ . In combination, we obtain the upper and lower bound for (126): ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\frac{1}{4}f\\left(\\frac{j-i}{n-j}\\right)-\\frac{1}{4}f(j-i)\\leq\\langle(C_{\\beta})_{i},(C_{\\beta})_{j}\\rangle\\leq\\frac{1}{(1-\\beta)^{2}}f\\left(\\frac{j-i}{n-j}\\right).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Now we are ready to bound the sensitivity of the matrix $C_{1,\\beta}$ : ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{sens}_{k,b}^{2}(C_{1,\\beta})=\\displaystyle\\sum_{i=0}^{k-1}\\langle(C_{1,\\beta})_{i b},(C_{1,\\beta})_{i b}\\rangle+2\\displaystyle\\sum_{i=0}^{k-1}\\sum_{j=i+1}^{k-1}\\langle(C_{1,\\beta})_{i b},(C_{1,\\beta})_{j b}\\rangle}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{1}{(1-\\beta)^{2}}\\sum_{i=0}^{k-1}(\\log(n-i b)+1)+\\displaystyle\\frac{2}{(1-\\beta)^{2}}\\sum_{i=0}^{k-1}\\sum_{j=i+1}^{k-1}f\\left(\\frac{j-i}{k-j}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "and, analogously ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathrm{sens}_{k,b}^{2}(C_{1,\\beta})\\geq\\frac{1}{4}\\sum_{i=0}^{k-1}\\log(n-i b)+\\frac{1}{2}\\sum_{0\\leq i<j\\leq k-1}\\left[f\\left(\\frac{j-i}{k-j}\\right)-f(b(j-i))\\right]\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Firstly, using $(\\frac{k}{e})^{k}\\leq k!\\leq k^{k}$ , we bound the sum of the logarithms: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{j=0}^{k-1}\\log(n-j b)=\\sum_{j=1}^{k}[\\log b+\\log j]=k\\log b+\\log k!\\leq k\\log b+k\\log k=k\\log n,}\\\\ &{\\displaystyle\\sum_{j=0}^{k-1}\\log(n-j b)=k\\log b+\\log k!\\geq k\\log b+k\\log k-k=k\\log n-k.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "To upper bound the last term in sensitivity lower bound (134), we use the auxiliary inequality $\\begin{array}{r}{f(a)\\stackrel{}{=}2\\log\\left(\\sqrt{\\frac{1}{a}+1}+\\sqrt{\\frac{1}{a}}\\right)\\leq\\frac{4}{\\sqrt{a}}}\\end{array}$ to derive: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\stackrel{1}{\\sum}_{0\\leq i<j\\leq k-1}f(b(j-i))\\leq\\frac{2}{\\sqrt{b}}\\sum_{0\\leq i<j\\leq k-1}\\frac{1}{\\sqrt{j-i}}=\\frac{2}{\\sqrt{b}}\\sum_{j=1}^{k-1}\\sum_{t=1}^{j}\\frac{1}{\\sqrt{t}}\\leq\\frac{4}{\\sqrt{b}}\\sum_{j=1}^{k-1}\\sqrt{j}\\leq\\frac{8}{3\\sqrt{b}}k^{3/2}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "To bound the final term we establish the following inequalities for $f(a)$ : ", "page_idx": 43}, {"type": "equation", "text": "$$\nf(a)=2\\log\\left(\\sqrt{\\frac{1}{a}+1}+\\sqrt{\\frac{1}{a}}\\right)=\\log\\left(\\frac{1}{a}+1\\right)+2\\log\\left(1+\\frac{1}{\\sqrt{a+1}}\\right)\n$$", "text_format": "latex", "page_idx": 43}, {"type": "equation", "text": "$$\n\\log\\left({\\frac{1}{a}}+1\\right)<f(a)<\\log\\left({\\frac{1}{a}}+1\\right)+2\\log2\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Then we can bound the first double sum in (134) as ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\sum_{0\\leq i<j\\leq k-1}\\log\\left(\\frac{k-i}{j-i}\\right)\\leq\\sum_{0\\leq i<j\\leq k-1}f\\left(\\frac{j-i}{k-j}\\right)\\leq\\sum_{0\\leq i<j\\leq k-1}\\log\\left(\\frac{k-i}{j-i}\\right)+2k^{2}\\log2.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "To bound the term $\\begin{array}{r}{\\sum_{0\\leq i<j\\leq k-1}\\log\\left(\\frac{k-i}{j-i}\\right)}\\end{array}$ we use the following identities: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{0\\leq i<j\\leq k-1}{\\sum}\\log\\left(\\frac{k-i}{j-i}\\right)=\\log\\frac{k-2}{\\prod_{i=0}^{k-2}\\left(k-i\\right)!}}&{}\\\\ {=\\log\\frac{k^{-1}\\left(k-1\\right)^{k-2}\\dots\\,2^{1}}{1!\\cdot2!\\cdot3!\\dots\\,(k-1)!}=\\log\\frac{2^{1}\\cdot3^{2}\\cdot4^{3}\\cdot\\dots\\,k^{k-1}}{1^{k-1}\\cdot2^{k-2}\\cdot\\dots\\,(k-1)^{1}}}\\\\ &{=\\log\\frac{k-1}{\\prod_{i=0}^{k}\\left(\\frac{j}{k}-j\\right)!}\\qquad\\underset{j=i}{\\overset{k-1}{\\prod_{i=0}^{k}}}\\log(j+1)-\\underset{j=1}{\\overset{k-1}{\\sum}}j\\log(k-j)}\\\\ &{=\\underset{j=1}{\\overset{k}{\\sum}}(j-1)\\log(j)-\\underset{j=1}{\\overset{k-1}{\\sum}}(k-j)\\log(j)}\\\\ &{=\\underset{j=1}{\\overset{k-1}{\\sum}}j\\log(j)-\\underset{j=1}{\\overset{k}{\\sum}}(k)\\log(k)}\\\\ &{=2\\underset{j=1}{\\overset{k-1}{\\sum}}j\\log(j)-\\log k!+2k\\log k-k\\log k!}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Now, using that $x\\log x$ is a monotonically increasing function, ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{0\\leq i<j\\leq k-1}\\log\\left(\\frac{k-i}{j-i}\\right)\\leq2\\int_{1}^{k}x\\log x d x+k\\log k+k-k^{2}\\log k+k^{2}}}\\\\ &{=k^{2}\\log k-\\frac{k^{2}}{2}+k\\log k-k-k^{2}\\log k+k^{2}}\\\\ &{\\leq\\frac{3}{2}k^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "As a lower bound, we obtain ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{0\\le i<j\\le k-1}\\log\\left(\\frac{k-i}{j-i}\\right)\\ge2\\displaystyle\\int_{1}^{k-1}x\\log x d x-k\\log k+k\\log k-k(k-1)\\log(k-1)}&{}\\\\ {\\displaystyle=(k-1)^{2}\\log(k-1)-\\frac{(k-1)^{2}}{2}+k\\log k-k(k-1)\\log(k-1)}&{}\\\\ {\\displaystyle=-(k-1)\\log(k-1)-\\frac{(k-1)^{2}}{2}+k\\log k}&{}\\\\ {\\displaystyle\\ge-\\frac{k^{2}}{2}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Therefore, combining the upper bound (146) and the lower bound (149) yields ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\sum_{0\\leq i<j\\leq k-1}f\\left({\\frac{j-i}{k-j}}\\right)\\leq(2\\log2+3/2)k^{2}\\leq3k^{2},\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Combining all three terms together we obtain the following bounds for the squared sensitivity (134): ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathrm{sens}_{k,b}^{2}(C_{1,\\beta})\\leq\\frac{k}{(1-\\beta)^{2}}(\\log n+1)+\\frac{6}{(1-\\beta)^{2}}k^{2}}\\\\ {\\displaystyle\\mathrm{sens}_{k,b}^{2}(C_{1,\\beta})\\geq\\frac{k}{4}(\\log n-1)-\\frac{8}{3\\sqrt{b}}k^{3/2}+\\frac{2}{5}k^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Now, we recall the bounds for the Frobenius norm of the matrix $C_{1,\\beta}\\;^{7}$ and (96): ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\frac{\\log(n+1)-1}{4}\\leq\\|C_{1,\\beta}\\|_{F}^{2}/n\\leq\\frac{\\log n+1}{(1-\\beta)^{2}}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "With the auxiliary inequality $\\textstyle{\\sqrt{\\frac{a}{2}}}+{\\sqrt{\\frac{b}{2}}}\\leq{\\sqrt{a+b}}\\leq{\\sqrt{a}}+{\\sqrt{b}}$ and combining (155) and the bounds on Frobenius norm (7) and (96) we get that: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathcal{E}(C_{1,\\beta},C_{1,\\beta})\\leq\\frac{\\sqrt{k}}{(1-\\beta)^{2}}(\\log n+1)+\\frac{\\sqrt{5}k}{(1-\\beta)^{2}}\\sqrt{\\log n+1}}}\\\\ {{\\displaystyle\\mathcal{E}(C_{1,\\beta},C_{1,\\beta})\\geq\\frac{1}{4\\sqrt{2}}\\sqrt{k}(\\log n-1)+\\frac{k}{2\\sqrt{5}}\\sqrt{\\log(n)-1}\\sqrt{1-\\frac{20}{3\\sqrt{n}}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Making the lower bound well-defined requires $\\textit{n}\\geq\\textit{45}$ , otherwise one can simply take $\\mathcal{E}(C_{1,\\beta},C_{1,\\beta})\\,\\geq\\,1$ . As a final step, we combine both inequalities in the following asymptotic statement: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathcal{E}(C_{1,\\beta},C_{1,\\beta})=\\Theta_{\\beta}\\left(\\sqrt{k}\\log n+k\\sqrt{\\log n}\\right),\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "which concludes the proof of the case without weight decay. ", "page_idx": 45}, {"type": "text", "text": "Case 2) with weight decay $(\\alpha<1)$ ). ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "As above, we first express the $b$ -min-separation sensitivity of the matrix $C_{\\alpha,\\beta}$ in terms of inner products, ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\operatorname{sens}_{k,b}^{2}(C_{\\alpha,\\beta})=\\sum_{i=0}^{k-1}\\sum_{j=0}^{k-1}\\langle(C_{\\alpha,\\beta})_{i b},(C_{\\alpha,\\beta})_{j b}\\rangle.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "and then consider a scalar product for a general pair of indexes $j>i$ : ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\langle(C_{\\alpha,\\beta})_{i},(C_{\\alpha,\\beta})_{j}\\rangle=\\sum_{t=0}^{n-1-j}c_{t}c_{j-i+t}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Now, we use the bounds on $c_{t}$ from Lemma 6 for $\\alpha<1$ , to upper and lower bound this sum with the following expression, where $\\textstyle\\gamma={\\frac{\\beta}{\\alpha}}$ ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\langle(C_{\\alpha,\\beta})_{i},(C_{\\alpha,\\beta})_{j}\\rangle\\le\\frac{\\alpha^{j-i}}{(1-\\gamma)^{2}}\\sum_{t=0}^{n-1-j}\\frac{\\alpha^{2t}}{\\sqrt{(t+1)(j-i+t+1)}}\\le\\frac{\\alpha^{j-i}}{(1-\\gamma)^{2}(1-\\alpha^{2})\\sqrt{j-i}}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "equation", "text": "$$\n\\langle(C_{\\alpha,\\beta})_{i},(C_{\\alpha,\\beta})_{j}\\rangle\\geq\\frac{\\alpha^{j-i}}{4}\\sum_{t=0}^{n-1-j}\\frac{\\alpha^{2t}}{\\sqrt{(t+1)(j-i+t+1)}}\\geq\\frac{\\alpha^{j-i}}{4\\sqrt{j-i+1}}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "We substitute these bounds into Equation (161) to obtain the following upper bound for sensitivity of matrix $C_{\\alpha,\\beta}$ : ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{sens}_{k,b}^{2}(C_{\\alpha,\\beta})\\leq\\displaystyle\\sum_{i=0}^{k-1}\\langle(C_{\\alpha,\\beta})_{i b},(C_{\\alpha,\\beta})_{i b}\\rangle+\\displaystyle\\frac{2}{(1-\\gamma)^{2}(1-\\alpha^{2})\\sqrt{b}}\\sum_{j>i}\\frac{\\alpha^{b(j-i)}}{\\sqrt{j-i}}}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{k}{(\\alpha-\\beta)^{2}}\\log\\displaystyle\\frac{1}{1-\\alpha^{2}}+\\displaystyle\\frac{2}{(1-\\gamma)^{2}(1-\\alpha^{2})\\sqrt{b}}\\sum_{j>i}\\alpha^{b(j-i)}}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{k}{(\\alpha-\\beta)^{2}}\\log\\displaystyle\\frac{1}{1-\\alpha^{2}}+\\displaystyle\\frac{2k\\alpha^{b}}{(1-\\gamma)^{2}(1-\\alpha^{2})(1-\\alpha^{b})\\sqrt{b}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where the second inequality is due to Equation (95). ", "page_idx": 46}, {"type": "text", "text": "A lower bound for the sensitivity follows directly from Lemma 3: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\mathrm{sens}_{k,b}^{2}(C_{\\alpha,\\beta})\\geq k\\|C_{\\alpha,\\beta}\\|_{F}\\geq k.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "The Frobenius norm of the matrix $C_{\\alpha,\\beta}$ is the same as that for one round participation; thus, we could reuse Inequalities (95): ", "page_idx": 46}, {"type": "equation", "text": "$$\n1\\leq\\Vert C_{\\alpha,\\beta}\\Vert_{F}^{2}/n\\leq\\frac{1}{(\\alpha-\\beta)^{2}}\\log\\frac{1}{1-\\alpha^{2}}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "By merging the bounds for sensitivity and Frobenius norm, we derive the following bounds for error: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}(C_{\\alpha,\\beta},C_{\\alpha,\\beta})\\leq\\sqrt{k}\\left[\\frac{1}{(\\alpha-\\beta)^{2}}\\log\\frac{1}{1-\\alpha^{2}}+\\frac{2\\alpha^{b}}{(1-\\gamma)^{2}(1-\\alpha^{2})(1-\\alpha^{b})\\sqrt{b}}\\right]}\\\\ &{\\mathcal{E}(C_{\\alpha,\\beta},C_{\\alpha,\\beta})\\geq\\sqrt{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "The combination of these results yields the following asymptotic statement: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\mathcal{E}(C_{\\alpha,\\beta},C_{\\alpha,\\beta})=\\Theta_{\\alpha,\\beta}(\\sqrt{k}),\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 46}, {"type": "text", "text": "F.9 Proof of Theorem 8 ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Theorem 8. Assume the setting of Theorem $^{\\sc6}$ . Then, for any factorization $A_{\\alpha,\\beta}~=~B C$ with $C^{\\top}C\\geq0,$ , the approximation error fulfills ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\mathcal E(B,C)\\geq\\left\\{\\!\\!\\begin{array}{l l}{\\sqrt k\\log n}&{\\qquad f o r\\,\\alpha=1,}\\\\ {\\sqrt k}&{\\qquad f o r\\,\\alpha<1,}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Proof. Let $A_{\\alpha,\\beta}=B C$ be any factorization with $C C^{\\top}\\geq0$ . From Lemma 3 it follows that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\mathcal E(B,C)=\\frac{1}{\\sqrt n}\\|B\\|_{F}\\operatorname{sens}_{k,b}(C)\\geq\\frac{\\sqrt k}{n}\\|B\\|_{F}\\|C\\|_{F}\\geq\\frac{\\sqrt k}{n}\\|A_{\\alpha,\\beta}\\|_{*},\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where $\\|\\cdot\\|_{*}$ denotes the nuclear norm, and the last inequality follows from its variational form, $\\begin{array}{r}{\\|M\\|_{*}=\\operatorname*{min}_{\\{X,Y:X Y^{T}=M\\}}\\|X\\|_{F}\\|Y\\|_{F}}\\end{array}$ . The statement of the Theorem follows by inserting the corresponding bounds on $\\|A_{\\alpha,\\beta}\\|_{*}$ from Lemma 8. \u53e3 ", "page_idx": 46}, {"type": "text", "text": "F.10 Proof of Theorem 9 ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Theorem 9. Assume the setting of Theorem 6. Then, the baseline factorizations $A_{\\alpha,\\beta}=A_{\\alpha,\\beta}\\cdot\\mathrm{Id}$ and $A_{\\alpha,\\beta}=\\operatorname{Id}\\cdot A_{\\alpha,\\beta}$ fulfill ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\mathcal E(A_{\\alpha,\\beta},\\mathrm{Id})\\geq\\left\\{\\sqrt{\\frac{n k}{2}}\\begin{array}{l l}{\\qquad}&{f o r\\,\\alpha=1,}\\\\ &{\\qquad\\,\\,f o r\\,\\alpha<1.}\\end{array}\\right.\\qquad\\mathcal E(\\mathrm{Id},A_{\\alpha,\\beta})\\geq\\left\\{\\begin{array}{l l}{\\!\\frac{k\\sqrt{n}}{\\sqrt{3}}}&{\\,f o r\\,\\alpha=1,}\\\\ {\\!\\sqrt{k}}&{\\,f o r\\,\\alpha<1.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Proof. Case 1) $A_{\\alpha,\\beta}=B C$ with $B=A_{\\alpha,\\beta}$ and $C=\\operatorname{Id}$ . It is easy to check that $\\mathrm{sens}_{k,b}(C)=$ $\\sqrt{k}$ , so $\\begin{array}{r}{\\mathcal{E}(B,C)=\\sqrt{\\frac{k}{n}}\\|A_{\\alpha,\\beta}\\|_{F}}\\end{array}$ . Because $\\begin{array}{r}{A_{\\alpha,0}\\leq A_{\\alpha,\\beta}\\leq\\frac{1}{\\alpha-\\beta}A_{\\alpha,0}}\\end{array}$ componentwise, we have for $\\alpha=1$ , ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\frac{n(n+1)}{2}=\\|A_{1,0}\\|_{F}^{2}\\leq\\|A_{1,\\beta}\\|_{F}^{2}\\leq\\frac{1}{(1-\\beta)^{2}}\\|A_{1,0}\\|_{F}^{2}=\\frac{n(n+1)}{2(1-\\beta)^{2}},\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "which implies the corresponding statement of the theorem. For $0<\\alpha<1$ , we use that $A_{1,0}>\\mathrm{Id}$ componentwise, so $\\|A_{\\alpha,0}\\|_{F}^{2}>\\|\\operatorname{Id}\\|_{F}^{2}=n$ , which conclude the proof of this case. ", "page_idx": 47}, {"type": "text", "text": "Case 2) $A_{\\alpha,\\beta}=B C$ with $B=\\mathrm{Id}$ and $C=A_{\\alpha,\\beta}$ . We observe that $\\|\\operatorname{Id}\\|_{F}={\\sqrt{n}}$ , so ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\varepsilon(B,C)=\\operatorname{sens}_{k,b}(A_{\\alpha,\\beta}).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Again, we use the fact that $\\begin{array}{r}{A_{\\alpha,0}\\leq A_{\\alpha,\\beta}\\leq\\frac{1}{\\alpha-\\beta}A_{\\alpha,0}}\\end{array}$ . Now $A_{\\alpha,0}$ fulflils the conditions of Theorem 2, so from Equation (10) we know ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\operatorname{sens}_{k,b}(A_{\\alpha,0})=\\Big\\|\\sum_{j=0}^{k-1}(A_{\\alpha,0})_{[\\cdot,1+j b]}\\Big\\|,\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "We first study (176) for $\\alpha=1$ . Then, from the explicit structure of $A_{1,0}=\\mathrm{LDToep}(1,1,\\ldots,1)$ one sees that the vectors inside the norm have a block structure ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\sum_{j=0}^{k-1}(A_{1,0})_{[\\cdot,1+j b]}=\\left(\\begin{array}{c}{v_{1}}\\\\ {\\vdots}\\\\ {v_{k}}\\\\ {v^{\\prime}}\\end{array}\\right)\\qquad\\mathrm{with}\\qquad v_{i}=\\left(\\begin{array}{c}{i}\\\\ {\\vdots}\\\\ {i}\\end{array}\\right)\\in\\mathbb{R}^{b}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "for $i=1,\\ldots,k$ , and $v^{\\prime}={\\binom{k}{\\vdots}}\\in\\mathbb{R}^{n-b k}$ , appears only if $\\begin{array}{r}{k<\\frac{n}{b}}\\end{array}$ . Now we check ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|v^{\\prime}\\|^{2}+\\sum_{i=0}^{k}\\|v_{i}\\|_{2}^{2}=(n-b k)k^{2}+b\\Big(\\displaystyle\\sum_{i=1}^{k}i^{2}\\Big)}}\\\\ &{\\qquad\\qquad=n k^{2}-b k^{3}+b\\displaystyle\\frac{k(k+1)(2k+1)}6}\\\\ &{\\qquad\\qquad\\geq n k^{2}-\\frac23b k^{3}\\geq\\frac13n k^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "because $b k\\geq n$ . Consequently ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathrm{sens}_{k,b}(A_{1,\\beta})\\geq\\frac{k\\sqrt{n}}{\\sqrt{3}},\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "which concludes the proof of this case. For $\\alpha<1$ , $A_{\\alpha,\\beta}\\geq\\operatorname{Id}$ componentwise readily implies ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\operatorname{sens}_{k,b}(A_{\\alpha,\\beta})\\geq{\\sqrt{k}},\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "which implies the statement of the theorem. ", "page_idx": 47}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: Our abstract and introduction clearly states our claims, the contributions made in the paper, the main assumptions and limitations. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 48}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Justification: We discuss the limitation of the approximately optimal matrix factorization methods. We emphasize that the bounds for the banded square root factorization are asymptotic and should be perceived together with the numerical experiments. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 48}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: All lemmas and theorem state the necessary assumptions. We provide proof sketches in the main text and proofs for all results in the appendix. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 49}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: For the factorization error comparison, which we consider to be the main experimental results, we have explicit descriptions of the procedures together with the code included in the appendix. For the model training, we adapted the public code from prior work [Choquette-Choo et al., 2023c], and we describe the changes in the appendix. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 49}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: We provide the code for the approximately optimal matrix factorization. The code for model training is adapted from the publicly accessible code, see above. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 50}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: We use the same setting as Choquette-Choo et al. [2023c] and report to have the same hyperparameters for the training. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 50}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: we report error bars and indicate that experiments were performed multiple times to ensure reliability and statistical significance. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 50}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 51}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 51}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: Our experiments run on commodity hardware. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 51}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Justification: We have acquainted with the guidelines and preserve the anonymity of the authors in the paper. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 51}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: Our work is about model training while preserving data privacy. We mention this in the manuscript. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 51}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 52}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 52}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: We have cited all the papers which have results and code used in our paper. Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 52}, {"type": "text", "text": "", "page_idx": 53}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 53}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 53}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 53}]