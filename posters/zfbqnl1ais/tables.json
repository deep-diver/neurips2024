[{"figure_path": "ZFbqnL1AiS/tables/tables_5_1.jpg", "caption": "Table 1: ARI scores (mean \u00b1 standard deviation across 5 seeds) for CAE, CAE++, CtCAE, RF, SynCx and SlotAttention on Tetrominoes, dSprites and CLEVR. Results for CAE, CAE++ and CtCAE baselines taken from Stani\u0107 et al. [24] and for SlotAttention taken from Locatello et al. [34].", "description": "This table presents the Adjusted Rand Index (ARI) scores achieved by different models on three datasets: Tetrominoes, dSprites, and CLEVR. ARI is used to measure the accuracy of object grouping.  The models compared include CAE, CAE++, CtCAE, RF (all state-of-the-art synchrony-based models), SynCx (the proposed model), and SlotAttention (a state-of-the-art slot-based model). The results show SynCx's performance compared to other models across the datasets and its relative strengths and weaknesses.", "section": "Unsupervised Object Discovery"}, {"figure_path": "ZFbqnL1AiS/tables/tables_6_1.jpg", "caption": "Table 2: Bottleneck ablation on Tetrominoes.", "description": "This table presents the result of an ablation study on the effect of bottlenecks in the SynCx model on the Tetrominoes dataset.  It compares the mean squared error (MSE) and Adjusted Rand Index (ARI) for two model variants: SynCx with a bottleneck (the original model) and SynCx without a bottleneck (where the spatial resolution of feature maps is preserved). The results show a significant drop in ARI for the model without a bottleneck despite a lower MSE, suggesting that the bottleneck is crucial for effective object grouping by SynCx.", "section": "Results"}, {"figure_path": "ZFbqnL1AiS/tables/tables_7_1.jpg", "caption": "Table 3: Ablation on number of iterations used for training on dSprites.", "description": "This table presents the results of an ablation study on the number of iterations used during training of the SynCx model on the dSprites dataset.  It shows the Mean Squared Error (MSE) and Adjusted Rand Index (ARI) for different numbers of iterations (1, 2, 3, and 4). The MSE measures the reconstruction error, while the ARI quantifies the accuracy of object grouping.  Lower MSE indicates better reconstruction, and higher ARI suggests more accurate grouping.  The results show that increasing the number of iterations generally improves grouping performance (ARI) but that the improvement diminishes after a certain point.", "section": "Results"}, {"figure_path": "ZFbqnL1AiS/tables/tables_7_2.jpg", "caption": "Table 4: Ablation on number of iterations used at test-time on dSprites.", "description": "This table shows the effect of increasing the number of iterations at test time on the model's performance.  The model was trained using 3 iterations, and the test was performed with 4, 5, and 6 iterations to see if performance degraded. The results show that increasing the number of iterations at test time did not negatively impact performance.", "section": "Results"}, {"figure_path": "ZFbqnL1AiS/tables/tables_7_3.jpg", "caption": "Table 5: Phase init. ablation on Tetrominoes.", "description": "This ablation study compares the performance of three variants of the SynCx model with different phase initialization methods: zero, uniform, and von-Mises.  The results show the mean and standard deviation of the MSE and ARI scores across 5 different random seeds for each phase initialization method.  The von-Mises distribution shows the best performance, indicating that the variance of the noise distribution is important for phase synchronization in this model.", "section": "Results"}, {"figure_path": "ZFbqnL1AiS/tables/tables_7_4.jpg", "caption": "Table 6: Comparison of parameter counts (rounded up to the nearest thousand) of various synchrony-based models. Total number of parameters expressed in terms of number of real-valued floats.", "description": "This table compares the number of parameters (in thousands) for different synchrony-based models across three datasets: Tetrominoes, dSprites, and CLEVR.  The models compared are CAE++, CtCAE, RF, and the proposed SynCx model.  The table highlights the significantly fewer parameters used in SynCx compared to the others.", "section": "Results"}, {"figure_path": "ZFbqnL1AiS/tables/tables_7_5.jpg", "caption": "Table 7: Wall-clock training times for SynCx and RF models on a P100 GPU.", "description": "This table shows the training time in hours and minutes for the SynCx and RF models on a P100 GPU for the Tetrominoes and dSprites datasets.  It highlights the significant time savings achieved by SynCx compared to RF.", "section": "Results"}, {"figure_path": "ZFbqnL1AiS/tables/tables_8_1.jpg", "caption": "Table 8: Grayscale CLEVR.", "description": "This table presents a comparison of the performance of RF and SynCx models on a grayscale version of the CLEVR dataset.  It shows the mean squared error (MSE) and Adjusted Rand Index (ARI) for each model, highlighting the impact of removing color information as a shortcut cue on model performance.  SynCx shows significantly better performance indicating it is less reliant on color cues for grouping compared to RF.", "section": "Results"}, {"figure_path": "ZFbqnL1AiS/tables/tables_15_1.jpg", "caption": "Table 1: ARI scores (mean \u00b1 standard deviation across 5 seeds) for CAE, CAE++, CtCAE, RF, SynCx and SlotAttention on Tetrominoes, dSprites and CLEVR. Results for CAE, CAE++ and CtCAE baselines taken from Stani\u0107 et al. [24] and for SlotAttention taken from Locatello et al. [34].", "description": "This table presents the Adjusted Rand Index (ARI) scores achieved by different models on three datasets: Tetrominoes, dSprites, and CLEVR.  ARI measures the similarity of the object groupings produced by the models compared to the ground truth. The table compares the performance of SynCx against other state-of-the-art synchrony-based models (CAE, CAE++, CtCAE, RF) and a leading slot-based model (SlotAttention).  The scores are averaged over 5 different random seeds, with standard deviations also included, to provide a measure of robustness.  The baseline model results from Stani\u0107 et al. [24] and Locatello et al. [34] are included for comparison purposes.", "section": "Unsupervised Object Discovery"}, {"figure_path": "ZFbqnL1AiS/tables/tables_15_2.jpg", "caption": "Table 10: Training hyperparameters for SynCx.", "description": "This table lists the hyperparameters used for training the SynCx model on three different datasets: Tetrominoes, dSprites, and CLEVR.  The hyperparameters include the number of training steps, batch size, learning rate, gradient norm clipping, number of iterations, and phase initialization method. The values for each hyperparameter vary slightly depending on the dataset.", "section": "Model & Training Details"}, {"figure_path": "ZFbqnL1AiS/tables/tables_17_1.jpg", "caption": "Table 1: ARI scores (mean \u00b1 standard deviation across 5 seeds) for CAE, CAE++, CtCAE, RF, SynCx and SlotAttention on Tetrominoes, dSprites and CLEVR. Results for CAE, CAE++ and CtCAE baselines taken from Stani\u0107 et al. [24] and for SlotAttention taken from Locatello et al. [34].", "description": "This table presents a comparison of the performance of several models on three different datasets (Tetrominoes, dSprites, and CLEVR) using the Adjusted Rand Index (ARI) as a metric for evaluating unsupervised object discovery.  The models compared include CAE, CAE++, CtCAE, RF, SynCx, and SlotAttention.  The table shows the mean and standard deviation of ARI scores across five different seeds for each model and dataset.  The results from Stani\u0107 et al. [24] and Locatello et al. [34] are included for comparison.", "section": "Unsupervised Object Discovery"}]