[{"type": "text", "text": "Recurrent Complex-Weighted Autoencoders for Unsupervised Object Discovery ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anand Gopalakrishnan1\u2217Aleksandar Stanic\u00b42\u2020 J\u00fcrgen Schmidhuber1,3 Michael Curtis Mozer2 1The Swiss AI Lab, IDSIA, USI & SUPSI, Lugano, Switzerland 2Google DeepMind 3AI Initiative, KAUST, Thuwal, Saudi Arabia ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Current state-of-the-art synchrony-based models encode object bindings with complex-valued activations and compute with real-valued weights in feedforward architectures. We argue for the computational advantages of a recurrent architecture with complex-valued weights. We propose a fully convolutional autoencoder, SynCx, that performs iterative constraint satisfaction: at each iteration, a hidden layer bottleneck encodes statistically regular configurations of features in particular phase relationships; over iterations, local constraints propagate and the model converges to a globally consistent configuration of phase assignments. Binding is achieved simply by the matrix-vector product operation between complex-valued weights and activations, without the need for additional mechanisms that have been incorporated into current synchrony-based models. SynCx outperforms or is strongly competitive with current models for unsupervised object discovery. SynCx also avoids certain systematic grouping errors of current models, such as the inability to separate similarly colored objects without additional supervision. 3 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "When shown arrays of simple visual elements, people have a natural tendency to perceive the arrays in disjoint groups based on their color, shape, spatial configuration, etc. Grouping behavior was first studied and systematized by Gestalt psychologists [1\u20133] who proposed a set of principles according to which perception operates, e.g., grouping by proximity, similarity, closure, good continuation and common fate. Although these Gestalt principles\u2014also called laws\u2014were assumed to be innate, statistics of the environment may explain some forms of grouping [4], and people rapidly learn new grouping principles when they are exposed to novel environments [5]. Grouping abilities are acquired early in an infant\u2019s developmental cycle [6] and serve as a building block in the human ability to form concepts [7], build abstractions and categorize [8], and reason about the physical world [9]. ", "page_idx": 0}, {"type": "text", "text": "Perceptual grouping has been modeled in deep learning under the guise of object-centric learning. (See Greff et al. [10] for an overview.) Object-centric models discover modular and compositional representations that can facilitate stronger generalization and relational reasoning capabilities on downstream visual tasks such as question answering [11], game playing [12\u201314] and robotics [15\u201317]. ", "page_idx": 0}, {"type": "text", "text": "Both human and AI object-centric learning aim to solve the binding problem [18, 19]\u2014determining how to integrate visual information in a flexible, dynamic manner to form unified wholes. The predominant design strategy for implementing binding in deep learning systems to achieve perceptual grouping is to maintain a set of latent activation vectors (slots), each of which encodes features of just one object. Slot-based models differ from one another in the procedures used to partition information from the inputs to each slot [10]. Far less investigation has gone into an alternative paradigm for perceptual grouping based on synchrony. In this paradigm, bindings between features are expressed in terms of the relative phases of complex-valued neural activities. The earliest demonstration of a synchrony-based model in neural networks focused on a supervised setting with teacher-specified target phases for hard-coded one-hot features of contour types [20], although consideration was given to an unsupervised extension obtained by phase clustering [21]. Recent synchrony-based models [22\u201325] using complex-valued activations to learn suitable features have made progress in unsupervised grouping performance on synthetic and more naturalistic scenes. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "However, current state-of-the-art unsupervised synchrony-based models such as CAE ([23]), CtCAE ([24]), and RF ([25]) have a number of limitations. They employ real-valued weights to process complex-valued activations by weight sharing across the real and imaginary components. As a result, the models do not exploit the constructive or destructive interference of complex-valued activations that occur naturally via multiplication and addition operations (cf., [26]). And the models require use of additional inductive biases such as gating mechanisms ( $\\chi$ -binding [22, 23, 25, 24]) to implement binding. Further, it remains unclear how $\\chi$ -binding mechanism works in more general scenarios where the complex-valued activations are not exactly out-of-phase or norms of weights and features do not satisfy certain conditions as noted by L\u00f6we et al. [27]. Cosine binding [27] was proposed as a more computationally motivated and interpretable alternative that easily handles such scenarios. However, this model continues to use real-valued weights to process complex-valued activations. Therefore, it uses cosine distance between inputs and intermediate outputs to implement binding which has a large memory overhead. ", "page_idx": 1}, {"type": "text", "text": "These state-of-the-art synchrony-based models [25, 24] are not as robust as one might hope. As we show later, these models tend to exploit color as a shortcut feature, a helpful heuristic when different objects have different colors but a harmful one when color is an unreliable shortcut feature. And on more naturalistic datasets, RF largely learns semantic-level groups (see, e.g., Figures 2 & 20 in [25]) rather than instance-level groups which is the central focus of object-centric learning. These state-of-the-art models require either an additional contrastive loss (see, e.g., Figure 8 in [24]) or pre-trained features from self-supervised vision transformers or \u2018depth masks\u2019 even on synthetic datasets (see, e.g., Figure 6 in [25]) to partially resolve such grouping errors. ", "page_idx": 1}, {"type": "text", "text": "Many of the complexities introduced in recent models seem to be moving the field away from the core intuition that initially motivated a synchrony-based approach to binding [20]. We step back and describe the original conception of synchrony-based grouping mechanisms from Mozer et al. [20]. ", "page_idx": 1}, {"type": "text", "text": "Consider the shapes composed of horizontal and vertical bars in Figure 1: the letters $\\mathbb{T}$ and $\\mathbb{H}$ and a pair of overlapping squares with occlusion. Perceptual grouping involves determining which of the bars belong together. The pairs highlighted in green are part of the same object; the pairs highlighted in red are parts of different objects. A feedforward convolutional architecture is not well suited to this task because spatially local patches are ambiguous with regard to grouping. While a feedforward, fully connected architectures may work in principle, it is also problematic because statistical regularities in images are local and fairly low order (i.e., involve a subset of image features). However, if we drop the feedforward restriction, a convolutional model can iteratively converge on a solution: each iteration suggests soft constraints on how features should be grouped, and over iterations, constraints can propagate from one region in the image to adjacent regions. This methodology was adopted in classical AI vision models [28] and in early synchrony-based models [20]. In the latter, phases\u2014 ", "page_idx": 1}, {"type": "image", "img_path": "ZFbqnL1AiS/tmp/057d2906e9f391a1ea1a311ac7aede3f65dcd40f68e1128cf56f8dd7f2c0df7a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Local feature configurations are insufficient to determine whether features belong to the same object: highlighted horizontal and vertical configurations sometimes belong to the same object (green) and at others to different objects (red). ", "page_idx": 1}, {"type": "text", "text": "representing object binding\u2014are initialized to random values and each iteration, the phases are updated to be consistent with constraints from neighboring patches. After multiple iterations the model converges on an interpretation that assigns each image location to a particular phase (object). ", "page_idx": 1}, {"type": "text", "text": "Complex weights are a natural choice to instantiate this function in our synchrony formulation outlined above. Just as hidden units in a standard architecture detect configurations of features, hidden units in a complex-valued analogue detect configurations of features in a particular relative phase arrangement. A hidden unit that is tuned to the top of a $\\mathbb{T}$ will expect the horizontal and vertical bar to have the same phase; a hidden unit that is tuned to occlusion (the red patches in Figure 1) will expect the horizontal and vertical bars to be out of phase. In this manner, the hidden units can encode alternative hypotheses concerning object groupings, and iterative processing will attempt to find global configurations that are locally consistent. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Drawing on these intuitions we design a model and training strategy for the fully unsupervised setting. Figure 2 shows our proposed model, Synchronous Complex Network $({\\bf S y n C x})$ , which uses complex-valued weights (detectors) to process complex-valued activations (features) in every layer of a fully convolutional autoencoder. The matrix vector product between complex-valued weights and activations ensures that weights process inputs not only based on their features (magnitudes $\\rightarrow$ features) but also their phase relationships (phases $\\rightarrow$ object bindings). The addition and multiplication operators in complex algebra mechanistically implement the binding mechanism in our model removing the need for additional gating mechanisms $(\\chi$ [22] / cosine binding [27]) or contrastive training [24]. The spatial locality of constraints are ensured via 2D convolutions used at every layer in the autoencoder. The model is initialized with a random phase map (prior on object bindings) and learns to (de)synchronize groups of features based on their (dis)similarity iteratively. The autoencoder weights are shared across iterations and predicted output phases (top-down feedback) from the previous iteration are used as the input phases at the next iteration while the pixel values are the input magnitudes at each step. Iteratively updating the phases allows spatially local constraints to be slowly propagated across the spatial axes and in principle should lead to the system settling to a fixed point. The autoencoder is trained to simply reconstruct the input image at each iteration. ", "page_idx": 2}, {"type": "text", "text": "In contrast to the synchrony formulation of Mozer et al. [20], all recent state-of-the-art models use purely feedforward processing of inputs. Therefore, these models lack the ability to propagate information about local constraints to align phases which we show $\\mathbb{T}$ and $\\mathbb{H}$ junction examples) plays a key role in achieving binding via synchrony. ", "page_idx": 2}, {"type": "text", "text": "We show that binding in synchrony-based models can be mechanistically implemented simply by matrix-vector products between complex-valued weights and activations to iteratively process inputs. We do not need any additional mechanisms like $\\chi$ -binding [22, 23, 25] or cosine binding [27] or contrastive training [24] as in prior work to achieve the same. Our conceptually simpler model (SynCx) designed from first principles outperforms (on Tetrominoes) or competitive with (on dSprites and CLEVR) state-of-the-art synchrony-based models for unsupervised object discovery. RF groups objects using largely color cues requiring supervision via additional features (\u2018depth masks\u2019) to segregate objects with similar colors. Whereas SynCx more gracefully separates objects of the same color as it relies on features (color/shape/texture) and the local spatial context removing the need for \u2018depth masks\u2019. We visualize the phase maps across iterations to qualitatively inspect the binding process. Lastly, we discuss some remaining practical limitations of current state-of-the-art synchrony-based models. ", "page_idx": 2}, {"type": "image", "img_path": "ZFbqnL1AiS/tmp/b41b350381e25fbfc3fda55346d4f19e6a3aefdd2de48ba40c3b007e9d610e0d.jpg", "img_caption": ["Figure 2: SynCx is a fully convolutional autoencoder that iteratively processes an input image. It starts with a randomly initialized phase $\\phi_{x}^{1}$ and the input image $\\pmb{\\mu}_{x}$ in the magnitude-component updates the phases in a stateful manner, i.e., output phase at iteration 1 fed as input at iteration 2 $(\\dot{\\phi}_{x}^{2}\\leftarrow\\phi_{z}^{1})$ and so on. The magnitude-component at the input is always clamped to the input image $\\pmb{\\mu}_{x}$ . SynCx is trained to reconstruct $\\pmb{\\mu}_{x}$ using the output magnitude-component $\\pmb{\\mu}_{z}^{n}$ at every step. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "2 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We describe the intuitions behind our model, its architecture and training procedure below. ", "page_idx": 3}, {"type": "text", "text": "Basic Intuition. Our autoencoder model is trained to reconstruct the input image using a representational bottleneck in the hidden layer(s). This requires the model to learn an efficient coding scheme to compress the input image. Images are typically composed of objects which are modular (reusable) units of feature information (shape, color, texture etc.) For a network with complex-valued activations, one intuitive coding scheme is using the phase components to express relationships between lower-order features (edges/textures/shape) in order to compress them into appropriate higher-order features (objects/parts). As the image is processed by the encoder it progressively compresses the input by storing and processing modular \u2018parts\u2018 together as higher-order features while the decoder learns the inverse mapping to recover the input from this compressed format. It is then possible to characterize objects as the collection of features with similar phases across the entire spatial map. ", "page_idx": 3}, {"type": "text", "text": "Model. Our model (SynCx) is a fully convolutional autoencoder architecture. It uses complexvalued weights to manipulate complex-valued activations at each layer. Let $h,w,h^{\\prime},w^{\\prime},c,d_{\\mathrm{in}},d_{\\mathrm{out}}$ and $p$ denote positive integers. Every layer of the network is a parametric function $f_{\\mathbf{W}}:\\mathbb{C}^{h\\times w\\times d_{\\mathrm{in}}}\\rightarrow$ $\\mathbb{C}^{h^{\\prime}\\times w^{\\prime}\\times d_{\\mathrm{out}}}$ which maps complex-valued inputs $\\mathbf{x}$ to complex-valued outputs $\\mathbf{h}$ using complex-valued weights $\\mathbf{W}\\in\\mathbb{C}^{p}$ . First, we compute the complex-valued pre-activation response y: ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\bf y}=f_{\\bf W}({\\bf x}),\\quad\\mathrm{where~}f_{\\bf W}\\;\\mathrm{denotes~a~2D~convolution}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Then, we apply the modReLU activation rule [29] element-wise only on the magnitude component of $\\mathbf{y}$ (i.e., $\\pmb{\\mu}_{y}.$ ) to obtain the complex-valued layer output $\\mathbf{h}$ as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{h}=\\mathrm{modReLU}(\\mathbf{y})=\\mathrm{ReLU}(\\mu_{y}+b)\\odot e^{i\\phi_{y}}\\ \\ \\mathrm{with}\\ \\mathbf{y}\\equiv\\mu_{y}\\odot e^{i\\phi_{y}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\odot$ denotes a Hadamard product and $b\\in\\mathbb{R}^{d_{\\mathrm{out}}}$ is a learnable parameter. We use modReLU instead of ReLU because the nonlinearity is applied only to the magnitude component (strictly nonnegative). Consequently, the standard ReLU activation rule would operate only in its linear region. The modReLU activation rule [29] was introduced specifically for a setting such as ours where the activation function is applied only to the magnitude components of complex-valued activations. The encoder module progressively downsamples the resolution of the complex-valued spatial feature map using strided 2D convolutions. The decoder module upsamples the resolution of the spatial feature map using the upsampling function independently on its magnitude and phase components. ", "page_idx": 3}, {"type": "text", "text": "Training and Loss Function. Given an image $\\mu_{x}\\in\\mathbb{R}^{h\\times w\\times c}$ of height $h$ , width $w$ , and $c$ channels (3 for color images) with non-negative pixel values. We construct a complex-valued input with magnitudes $\\pmb{\\mu}_{x}$ and phases $\\phi_{x}$ which are randomly initialized at every spatial location and channel of the image (see Figure 2). For each vector-valued feature at a given location, its phase component encodes the network\u2019s current hypothesis about its object binding; random initialization reflects lack of knowledge initially. The autoencoder processes the input image to update its hypotheses about object bindings at every iteration $n\\in\\{1,...,N\\}$ . The complex-valued input $\\mathbf{x}^{n}\\in\\mathbb{C}^{h\\times w\\times c}$ to the autoencoder at the $n^{\\mathrm{th}}$ iteration is: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{x}^{n}=\\pmb{\\mu}_{x}\\odot e^{i\\phi_{x}^{n}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Notice in Equation (3), that the magnitude components are always clamped to the image $(\\pmb{\\mu}_{x})$ while the phase components $(\\phi_{x}^{n})$ are fed back. At the $n^{\\mathrm{th}}$ iteration, $\\mathbf{x}^{n}$ is processed by the encoder and decoder layers, denoted as $\\mathbf{Net}(\\mathbf{x}^{n})$ , to compute a complex-valued output $\\mathbf{z}^{n}\\,\\in\\,\\mathbb{C}^{h\\times w\\times c}$ . The magnitude component of $\\mathbf{z}^{n}$ , i.e. $\\pmb{\\mu}_{z}^{n}$ , is the reconstruction of input image $\\pmb{\\mu}_{x}$ . The phase component of $\\mathbf{z}^{n}$ , i.e. $\\phi_{z}^{n}$ , is used to initialize the complex-valued input $\\mathbf{x}^{\\bar{n+1}}$ at the next iteration: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{z}^{n}=\\mathbf{Net}(\\mathbf{x}^{n})\\quad;\\quad\\phi_{x}^{n+1}\\leftarrow\\phi_{z}^{n}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This way the phase maps act as the state of a recurrent function which processes a clamped input image at all steps as shown in Figure 2. After $N$ such iterative updates to the phase maps by the ", "page_idx": 3}, {"type": "text", "text": "autoencoder, it is trained to minimize the average pixel-wise reconstruction loss across all iterations: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\frac{1}{N}\\sum_{n=1}^{N}||\\pmb{\\mu}_{x}-\\pmb{\\mu}_{z}^{n}||^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We use the weight initialization scheme outlined by Trabelsi et al. [30] for the complex-valued weights W at every layer of the encoder and decoder modules. This initialization scheme derives the variances of complex-valued weights to satisfy the initialization criterion of He et al. [31]. The magnitude components of the complex-valued weights at every layer are sampled from a Rayleigh distribution with $\\sigma=1/\\mathrm{fan}_{\\mathrm{in}}$ while the phase components are sampled from a uniform distribution between 0 to $2\\pi$ . ", "page_idx": 4}, {"type": "text", "text": "Through ablation studies in the following section, we quantify the effects of key components such as random initialization of the phase map, presence of a bottleneck at latent layer(s) and the number of iterative updates have on the performance of our system. ", "page_idx": 4}, {"type": "text", "text": "Extracting Object Assignments. Here, we outline the processing steps used to extract discrete object assignments for every pixel given its continuous phase values. We start by taking the output complex-valued feature map $\\mathbf{\\check{h}}^{N}\\,\\in\\,\\mathbb{C}^{h\\times w\\times d_{\\mathrm{out}}}$ from the penultimate decoder layer at the last iteration $N$ . We use latent-level complex-valued features instead of output-level so as to use high-order features (texture/shape/color etc.) than simply color cues (RGB space) to determine their object constituency. Then, we construct a complex-valued feature map with unit magnitudes (i.e., $\\mu_{h}^{\\tilde{N}}\\gets\\mathbf{1}\\in\\mathbb{R}^{h\\times w^{\\star}d_{\\mathrm{out}}})$ and phases the same as that of $\\mathbf{h}^{N}$ (i.e., $\\phi_{h}^{N}$ ). Background regions in this constructed feature map are masked out by setting their magnitudes to zero and then converted to the Cartesian form element-wise. Finally, these features in Cartesian form are clustered using $k$ -Means to compute the object assignments for every location in the spatial map. Using unit magnitudes for complex-valued activations ensures that object assignments are solely determined by their orientations. We note that it would be most natural to cluster the phases from the output layer at each location to assign it to an object. However, we observed that the object separation shown in the output phases is weaker than that from the high-dimensional latent layer. Therefore, for all results presented here we choose to cluster the higher dimensional latent representations (from the penultimate decoder layer) by location. For more details on the object assignment process refer to Appendix A. ", "page_idx": 4}, {"type": "text", "text": "3 Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "First, we describe details of the datasets, our model/baselines, training and evaluation procedures. We visualize the phase maps and qualitatively characterize the grouping behavior shown by our model. We quantitatively compare the grouping performance on the unsupervised object discovery task of our model (SynCx) against state-of-the-art synchrony-based baselines (CAE, $\\mathrm{CAE++}$ , CtCAE, and RF). We also quantify the effects of key components of our model using ablation experiments. We conclude with some practical limitations shown by the binding mechanisms in these synchrony-based models. ", "page_idx": 4}, {"type": "text", "text": "Datasets. We evaluate models on three datasets from the multi-object suite [32] namely Tetrominoes, dSprites and CLEVR used by prior work in object-centric learning [33\u201335]. For CLEVR, we use a filtered version [35] which consists of images containing less than seven objects. In all experiments we use image resolutions identical to Emami et al. [35], i.e., $35\\mathrm{x}35$ for Tetrominoes, 64x64 for dSprites and $96\\mathrm{x}96$ for CLEVR (center crop of 192x192 resized to 96x96). In Tetrominoes and dSprites the number of training images is 60K whereas in CLEVR it is 50K. All three datasets have 320 test images on which we report the evaluation metrics. For further details about datasets and preprocessing, we refer to Appendix A. ", "page_idx": 4}, {"type": "text", "text": "Model & Training Details. For more details about the encoder and decoder architecture of our model we refer to Appendix A. We train our model for 40K steps on Tetrominoes, and 100K steps on dSprites and CLEVR with Adam optimizer [36] with a constant learning rate of 5e-4, i.e., no warmup schedules or decay (all hyperparameter details are given in Appendix A). The phase initialization at each spatial location and channel of the input image are independent samples from a von-Mises distribution with a mean of 0 and concentration of 1. ", "page_idx": 4}, {"type": "text", "text": "Evaluation Metrics. We use the same evaluation protocol as prior work [33\u201335, 23] which compares the grouping performance of models using the Adjusted Rand Index (ARI) [37, 38]. The ARI scores are measured only for the foreground pixels common practice in the object-centric literature. ", "page_idx": 4}, {"type": "image", "img_path": "ZFbqnL1AiS/tmp/13d656d0bb5b120bd18428b8a19c261c8d030258950cc7b8846f1a101df1e2d6.jpg", "img_caption": ["Figure 3: Evolution of phase maps in radial and heatmap form (colors matched) across iterations in SynCx for two inputs from Tetrominoes (row 1) and dSprites (row 2). "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Phase Map Visualization. We seek to qualitatively inspect phase components of the output complex-valued feature map $\\mathbf{h}\\in\\mathbb{C}^{h^{\\prime}\\times w^{\\prime}\\times d_{\\mathrm{out}}}$ from an intermediate decoder layer to see if our model has indeed learned phases specialized in an object-centric manner. We visualize the phase component of the high-dimensional feature map as a heatmap in two dimensions. We assign unit magnitudes and retain the phase components of $\\mathbf{h}$ and elementwise convert these to the cartesian form as we did previously to extract object assignments. We apply t-SNE [39] to perform dimensionality reduction on the complex-valued feature map of an input image to recover a scalar \u2018composite\u2019 phase value at every spatial location (see Appendix A and Figure 10 for more details). This \u2018composite\u2019 phase map is visualized as a two dimensional heatmap and radial plot with colors in the former corresponding to orientations in the latter (see columns 2 & 3 in Figure 3). From Figure 3 shows how the phase maps evolve across the iterations in SynCx grouping process. In the sample image from Tetrominoes (row 1 in Figure 3), we can see how the phase \u2018bands\u2019 corresponding to each tetris block are progressively separated in their orientations. Such a visualization gives an interpretable artefact that allows a qualitative inspection of the phase synchronization process. ", "page_idx": 5}, {"type": "text", "text": "Unsupervised Object Discovery. Table 1 compares the performance of recent state-of-the-art synchrony-based baselines (CAE, $\\mathrm{CAE++}$ , CtCAE, and RF) against ours (SynCx) on the unsupervised object discovery task. On Tetrominoes, we observe that SynCx outperforms all baselines on grouping performance. SynCx more gracefully separates objects of the same color (see Figure 4) compared to RF which in addition to complex-valued activations requires $\\chi$ -binding mechanism. SynCx also outperforms the CtCAE baseline which in addition to complex-valued activations requires $\\chi$ -binding and contrastive training to separate similarly colored objects. On dSprites, SynCx significantly outperforms CAE, $\\mathrm{CAE++}$ and CtCAE baselines while being strongly competitive with RF. Similarly on CLEVR, SynCx strongly outperforms CAE, $\\mathrm{CAE++}$ and CtCAE baselines while being competitive with RF. Overall, SynCx despite its simple binding mechanism (no gates) and training strategy (no contrastive training) outperforms or is strongly competitive with all recent state-of-the-art synchrony-based models. However, there still remains a significant gap in grouping performance between synchrony-based models w.r.t dominant slot-based approaches such as SlotAttention [34] especially on CLEVR. We refer to Appendix B for the model comparisons that includes the MSE loss as well. We refer to Appendix C for additional qualitative grouping examples from our model. ", "page_idx": 5}, {"type": "text", "text": "Table 1: ARI scores (mean $\\pm$ standard deviation across 5 seeds) for CAE, $\\mathrm{CAE++}$ , CtCAE, RF, SynCx and SlotAttention on Tetrominoes, dSprites and CLEVR. Results for CAE, $\\mathrm{CAE++}$ and CtCAE baselines taken from Stanic\u00b4 et al. [24] and for SlotAttention taken from Locatello et al. [34]. ", "page_idx": 5}, {"type": "table", "img_path": "ZFbqnL1AiS/tmp/acdce29871f1c0138e456dc18db05b48427ca7fb4e9f35cc98040d074caa4e06.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "ZFbqnL1AiS/tmp/72058495348a96a4a36adaacc60359b928f78fff96be05913be789e195ec88c7.jpg", "img_caption": ["Figure 4: Comparison between RF and SynCx grouping on Tetrominoes, dSprites and CLEVR. RF tends to systematically group similarly colored objects together while SynCx is more adept at separating them such as blue tetris blocks (left), green square and heart (middle) and yellow cylinders (right). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Below, we quantify the effects of key components of our model such as representational bottlenecks, number of iterations and phase initialization have on its grouping performance. ", "page_idx": 6}, {"type": "text", "text": "Effect of Bottlenecks. We measure the effect of having representational bottlenecks (i.e. spatial resolution of feature maps) in the hidden layers of our model. To do so, we design a variant of our model (denoted as SynCx w/o bottleneck in Table 2) that preserves the spatial resolution of feature maps in the encoder (using stride of 1 in all layers) thereby removing all bottlenecks. We ensure that both these variants (SynCx and ", "page_idx": 6}, {"type": "text", "text": "SynCx w/o bottleneck) have the same number of parameters. Instead we only vary the spatial resolution of feature maps, so the bottleneck is w.r.t spatial resolution and not the number of model parameters. We observe a sharp drop in grouping performance for our ablated ", "page_idx": 6}, {"type": "table", "img_path": "ZFbqnL1AiS/tmp/326a6a0771c6600fbb3bb37ab7d9f945665773069400494f6c88c51748dfcac2.jpg", "table_caption": ["Table 2: Bottleneck ablation on Tetrominoes. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "model variant without any bottlenecks despite it achieving a lower test loss (see Figure 5). This result supports our initial intuition that representational bottlenecks force an autoencoder with complex-valued activations to compress spatial regularities in features by using phase components to capture relationships between them. Since objects are modular units containing highly regular features within their boundaries, this leads to phases strongly specializing (synchronizing) towards them to facilitate better image compression. ", "page_idx": 6}, {"type": "image", "img_path": "ZFbqnL1AiS/tmp/d8611c6639a41185db3f701faf24b2d7a254bf9aa06c1710b8790423abf0a0a5.jpg", "img_caption": ["Figure 5: Reconstruction, object masks, radial phase plot and phase heatmaps (colors matched between columns 5 & 6) for SynCx without the bottleneck (row 1) and the full SynCx model (row 2). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Effect of Iterations. We measure the effect of multiple iterations to update phases on the grouping performance of our model. Table 3 shows the grouping performance of our model with increasing number of iterations used during training. We observe a general trend of increasing phase specialization towards objects and better reconstructions as we increase the number of iterations. This supports our qualitative characterization of the models\u2019 operation \u2014 starting ", "page_idx": 7}, {"type": "table", "img_path": "ZFbqnL1AiS/tmp/d7c1926dd8afa058263d04d8b477d4ae4a2ce8fc49903428c2f69292e1b86a16.jpg", "table_caption": ["Table 3: Ablation on number of iterations used for training on dSprites. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "with random object assignments it iteratively refines the hypotheses for the assignment for each pixel. ", "page_idx": 7}, {"type": "text", "text": "We also measure the effect of increasing the number of iterations at test-time compared to that used during training. Table 4 shows the grouping performance of our model as we increase the number of iterations from 4 to 6 while during training the model used 3 iterations. We observe no drop in object separation performance of our model when we extrapolate the number of iterations used at test-time. ", "page_idx": 7}, {"type": "table", "img_path": "ZFbqnL1AiS/tmp/bb213fa1faf29067665009c5e3123192909dd5fc7d4c48bbddb8d7c574a679f4.jpg", "table_caption": ["Table 4: Ablation on number of iterations used at test-time on dSprites. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Effect of Phase Initialization. We measure the effect that the initialization of phases has on the grouping performance of our model. To achieve this, we compare (see Table 5) the grouping performance of three variants \u2014 i) initial phases sampled from a uniform distribution between - $-\\pi$ and $\\pi$ ii) initial phases sampled from a von-Mises distribution with mean of 0 and concentration of 1 and iii) all values in the initial phase map are set to zero. ", "page_idx": 7}, {"type": "text", "text": "Rest of the model and training hyperparameters are kept the same between these two variants. We observe that the variant using von-Mises distribution to sample initial input phases achieves the best test loss and improved grouping scores among all variants. This suggests the variance of the noise distribution is an important factor to tune for phase synchronization. The von-Mises ", "page_idx": 7}, {"type": "table", "img_path": "ZFbqnL1AiS/tmp/9f765e00786002567e411e6430e80db5fdf51e4a4a9ea9f4cabcff9de8386255.jpg", "table_caption": ["Table 5: Phase init. ablation on Tetrominoes. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "distribution with a mean of 0 and concentration of 1 is the circular analogue of the Normal distribution and therefore samples \u201cnoisy\u201d phase values with an intermediate level of variance compared to the other two alternatives which have maximum and zero variance. We use the von-Mises variant as the default phase initialization for input phases in all experiments. ", "page_idx": 7}, {"type": "table", "img_path": "ZFbqnL1AiS/tmp/19d634bea6040c6a2d345e8eb13cc7fbbe0429f932340f8084162c7fa0fff8ce.jpg", "table_caption": ["Table 6: Comparison of parameter counts (rounded up to the nearest thousand) of various synchronybased models. Total number of parameters expressed in terms of number of real-valued floats. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Parameter and Training Efficiency Table 6 shows the parameters counts for the various synchrony-based models. We see that our model, SynCx, is $6{-}23\\mathrm{x}$ more parameter efficient compared to the other synchrony-based alternatives. Table 7 shows the wall-clock training times for convergence of SynCx against the strongest synchrony baseline, RF, on Tetrominoes and dSprites. We see that our model on these datasets consistently takes lesser wall-clock time for training in these settings. ", "page_idx": 7}, {"type": "table", "img_path": "ZFbqnL1AiS/tmp/67ecb2c2ff192c80edfcb3ae6996152841a402b3bd9bc6d8391ec6c662dc757c.jpg", "table_caption": ["Table 7: Wall-clock training times for SynCx and RF models on a P100 GPU. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "ZFbqnL1AiS/tmp/90884180ec047245afbaa106ec44b72493db8402ffe915d8247034c1cc595402.jpg", "img_caption": ["Figure 6: Reconstruction and object masks for RF and SynCx on grayscale CLEVR. While both models reconstruct well, RF struggles to group the objects without access to color unlike SynCx. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "These results highlight the computational beneftis of the simple ideas of complex-valued weights and recurrence that are the essence of our model design. ", "page_idx": 8}, {"type": "text", "text": "Limitations. We test how reliant on color cues are the two best performing synchrony-based models from Table 1, i.e., RF and SynCx, for unsupervised object discovery. We train and evaluate them on a grayscale version of the CLEVR dataset. To separate objects in this dataset, the models\u2019 would need to rely more on other features like shape, texture, etc. From Table 8, we see that the grouping performance of RF on grayscale CLEVR drops sharply compared to the original (see Table 1) indicating its heavy reliance on color cues. Our models\u2019 grouping performance also drops but to a much lesser degree compared to RF. This indicates a weaker dependence on color cues in the binding mechanism of our model. In fact, on grayscale CLEVR SynCx significantly outperforms RF despite having slightly worse performance on the original colored version. The binding mechanism (i.e., matrix-vector product) in our ", "page_idx": 8}, {"type": "table", "img_path": "ZFbqnL1AiS/tmp/1772ee91adf7fd24080fbd018e74b9bd846fac78b24a92429040fdff88628690.jpg", "table_caption": ["Table 8: Grayscale CLEVR. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "model updates phases based on agreement between feature detectors (complex-valued weights) and high-dimensional attributes (complex-valued activations) that model color, edges, texture, shape etc. while taking it account their phase relationships (local context). This facilitates it to bind to objects more robustly and not simply rely on color. Overall, we can see that despite its simple architecture and training procedure our model shows good grouping performance on a wider range of visual environments. However, binding mechanisms in state-of-the-art synchrony models are far from perfect. They are unable to reliably capture objects even on synthetic datasets like CLEVR containing simple 3D shapes with metallic or matte textures under camera lighting with a non-textured background. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Slot-based Binding. Recent years have seen a growing number of models for unsupervised perceptual grouping (see Greff et al. [10] for a summary). These models maintain the separation of information using a separate set of activations (\u2018slots\u2019) but differ in the segregation mechanism to infer their contents. Representational symmetry in \u2018slots\u2019 is broken via temporal ordering [40, 41], spatial ordering [42\u201344], type-specificity [45], iterative routing procedures [46, 47, 33, 34, 48] (cf. earlier work [49]) or combinations thereof [50\u201352]. Extensions of slot-based grouping models for videos [53\u201356], novel view synthesis [57\u201359] as well as other modalities than vision including speech [60], music [61] and actions [62] have been explored. Conceptual limitations of slot-based models include separation only maintained at one level, binding information stored in hard-wired architectural components unsuited for gradient-based adaptation, uniform capacity, inability to store object-level relational factors and high computational cost for training as noted by Stanic\u00b4 et al. [24]. ", "page_idx": 8}, {"type": "text", "text": "Synchrony-based Binding. Synchrony-based models could in principle address the above shortcomings of slot-based models but have so far received comparatively little attention. They resolve the binding problem by augmenting each activation with additional grouping features. They can be broadly divided into two classes \u2014 temporal and complex, based on the type of coding strategy used for the augmentation. Models using temporal codes rely on spiking neurons with rhythmic firing behavior [63\u201365]. In such models, features of an object are expressed by neurons that fire in-sync. However, spiking neurons are non-differentiable and therefore incompatible with gradientbased learning, requiring specialized learning algorithms difficult to scale with current hardware accelerators for training. Models using complex-valued codes [20, 21, 66, 22, 23] are based on complex-valued activations suitable for scalable training using backpropagation. These models have been empirically benchmarked only on binarized images of simple geometric shapes or MNIST digits. More recent models [25, 24] show improved unsupervised grouping performance on more visually challenging color images from the multi-object suite [32]. However, these recent models heavily rely on supervision in the form of \u2018depth masks\u2019, gating mechanisms, contrastive training or a combination thereof. In contrast, our model is simply a fully convolutional autoencoder with complex-valued weights that iteratively updates phases to reconstruct an input image. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Temporal Correlation Hypothesis. Synchrony-based models draw functional inspiration from the temporal correlation hypothesis [67, 68] in neuroscience. It posits that the brain uses synchronized dynamics of neuronal firings to bind together distributed feature information computed in parallel at different areas into coherent percepts. Neural synchrony is also believed to convey information about relationships between features needed for dynamic and context-dependent binding by \u2018relational coding\u2019 [69]. This is in contrast to \u2018labeled line coding\u2019 where each unit has a fixed label attached to it indicating the static feature conjuction being active. Our use of complex-valued weights to process complex-valued activations is akin to the \u2018relational coding\u2019 scheme. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We explored synchrony-based binding with an architecture, SynCx, that differs from current models in three respects: i) SynCx\u2019s weights are complex valued, allowing it to encode joint feature-phase configurations in the weights; ii) SynCx is recurrent and stateful, allowing it to perform iterative constraint propagation; and iii) SynCx has an internal representational bottleneck, which require it to use the phases of complex-valued activations to encode statistical regularities in images. These three properties are crucial to extending object-centric phase synchronization behavior to the fully unsupervised setting. Our conceptually elegant model outperforms all state-of-the-art baselines on Tetrominoes and overcomes a common failure mode of these baselines in which similarly colored objects are grouped together. Overreliance on color cues for grouping by current models is further highlighted on a grayscale variant of the CLEVR dataset where we see that the RF model struggles to group objects using other features (shape and texture), in contrast to SynCx. Our model shows strong performance compared to more sophisticated synchrony-based baselines [25, 24] on dSprites and CLEVR without the need for additional supervision such as \u2018depth masks\u2019, gating mechanisms or contrastive training. Starting with our model as a simple template, future directions to explore include the use of a temporal difference style loss to weight reconstruction targets and incorporate spatial priors into the binding mechanism. The process of extracting discrete object assignments from continuous phase maps by clustering could be improved by accounting for outliers, non-Gaussian distributed phase values with unequal sizes and variances. While recent synchrony-based models have been steadily closing the gap in grouping performance to well-established slot-based approaches such as SlotAttention [34] on simple synthetic datasets (Tetrominoes and dSprites) the gap remains significant on CLEVR. Further, they are yet to be scaled up to more challenging multi-object visual benchmarks (e.g. MOVi [70]) which offers an open challenge for future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments. We thank Kazuki Irie for numerous discussions and guidance. We also thank Thomas Kipf for giving valuable feedback on our manuscript. This research was funded by Swiss National Science Foundation grant: 200021_192356, project NEUSYM and partially by the ERC Advanced grant no: 742870, AlgoRNN. This work was also supported by the following grants from the Swiss National Supercomputing Centre (CSCS) under project ID s1205 and s1294. We also thank NVIDIA Corporation for donating DGX machines as part of the Pioneers of AI Research Award. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Max Wertheimer. Untersuchungen zur lehre von der gestalt. ii. Psychologische Forschung, 4(1): 301\u2013350, 1923.   \n[2] Kurt Koffka. Principles of gestalt psychology. Philosophy and Scientific Method, 32(8), 1935.   \n[3] Wolfgang K\u00f6hler. Gestalt psychology. Psychologische Forschung, 31(1), 1967.   \n[4] B. Kim, E. Reif, M Wattenberg, S. Bengio, and M. C. Mozer. Neural networks trained on natural scenes exhibit gestalt closure. Computational Brain & Behavior, 4:251\u2013263, 2021.   \n[5] Richard S Zemel, Marlene Behrmann, Michael C Mozer, and Daphne Bavelier. Experiencedependent perceptual grouping and object-based attention. Journal of Experimental Psychology: Human Perception and Performance, 28(1):202, 2002.   \n[6] Elizabeth S. Spelke. Principles of object perception. Cognitive Science, 14(1):29\u201356, 1990.   \n[7] Elizabeth S Spelke and Katherine D Kinzler. Core knowledge. Developmental science, 10(1): 89\u201396, 2007.   \n[8] Charles Kemp and Joshua B. Tenenbaum. The discovery of structural form. Proceedings of the National Academy of Sciences, 105(31):10687\u201310692, 2008.   \n[9] Peter W. Battaglia, Jessica B. Hamrick, and Joshua B. Tenenbaum. Simulation as an engine of physical scene understanding. Proceedings of the National Academy of Sciences, 110(45): 18327\u201318332, 2013.   \n[10] Klaus Greff, Sjoerd van Steenkiste, and J\u00fcrgen Schmidhuber. On the binding problem in artificial neural networks. Preprint arXiv:2012.05208, 2020.   \n[11] David Ding, Felix Hill, Adam Santoro, Malcolm Reynolds, and Matthew Botvinick. Attention over learned object embeddings enables complex visual reasoning. In Proc. Advances in Neural Information Processing Systems (NeurIPS), 2021.   \n[12] Victor Bapst, Alvaro Sanchez-Gonzalez, Carl Doersch, Kimberly L. Stachenfeld, Pushmeet Kohli, Peter W. Battaglia, and Jessica B. Hamrick. Structured agents for physical construction. In Proc. Int. Conf. on Machine Learning (ICML), 2019.   \n[13] Aleksandar Stanic\u00b4, Yujin Tang, David Ha, and J\u00fcrgen Schmidhuber. Learning to generalize with object-centric agents in the open world survival game crafter. IEEE Transactions on Games, pages 1\u201320, 2023.   \n[14] Jaesik Yoon, Yi-Fu Wu, Heechul Bae, and Sungjin Ahn. An investigation into pre-training object-centric representations for reinforcement learning. In Proc. Int. Conf. on Machine Learning (ICML), 2023.   \n[15] Priyanka Mandikal and Kristen Grauman. Learning dexterous grasping with object-centric visual affordances. In 2021 IEEE international conference on robotics and automation (ICRA), 2021.   \n[16] Yizhe Wu, Oiwi Parker Jones, Martin Engelcke, and Ingmar Posner. Apex: Unsupervised, objectcentric scene segmentation and tracking for robot manipulation. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021.   \n[17] Andrii Zadaianchuk, Maximilian Seitzer, and Georg Martius. Self-supervised visual reinforcement learning with object-centric representations. In Int. Conf. on Learning Representations (ICLR), 2021.   \n[18] Anne Treisman. The binding problem. Current opinion in neurobiology, 6(2):171\u2013178, 1996.   \n[19] Adina L Roskies. The binding problem. Neuron, 24(1):7\u20139, 1999.   \n[20] Michael C. Mozer, Richard S. Zemel, Marlene Behrmann, and Christopher K. I. Williams. Learning to Segment Images Using Dynamic Feature Binding. Neural Computation, 4(5): 650\u2013665, 1992.   \n[21] Michael C Mozer. A principle for unsupervised hierarchical decomposition of visual scenes. In Proc. Advances in Neural Information Processing Systems (NIPS), 1998.   \n[22] David P Reichert and Thomas Serre. Neuronal synchrony in complex-valued deep networks. In Int. Conf. on Learning Representations (ICLR), 2014.   \n[23] Sindy L\u00f6we, Phillip Lippe, Maja Rudolph, and Max Welling. Complex-valued autoencoders for object discovery. Transactions on Machine Learning Research, 2022.   \n[24] Aleksandar Stanic\u00b4, Anand Gopalakrishnan, Kazuki Irie, and J\u00fcrgen Schmidhuber. Contrastive training of complex-valued autoencoders for object discovery. In Proc. Advances in Neural Information Processing Systems (NeurIPS), 2023.   \n[25] Sindy L\u00f6we, Phillip Lippe, Francesco Locatello, and Max Welling. Rotating features for object discovery. In Proc. Advances in Neural Information Processing Systems (NeurIPS), 2023.   \n[26] Geoffrey Hinton. How to Represent Part-Whole Hierarchies in a Neural Network. Neural Computation, 35(3):413\u2013452, 2023.   \n[27] Sindy L\u00f6we, Francesco Locatello, and Max Welling. Binding dynamics in rotating features. ArXiv, abs/2402.05627, 2024.   \n[28] David Waltz. Generating semantic descriptions from drawings of scenes with shadows. Technical report, Massachusetts Institute of Technology, MIT MAC-TR 271, 1972.   \n[29] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In Proc. Int. Conf. on Machine Learning (ICML), 2016.   \n[30] Chiheb Trabelsi, Olexa Bilaniuk, Ying Zhang, Dmitriy Serdyuk, Sandeep Subramanian, Joao Felipe Santos, Soroush Mehri, Negar Rostamzadeh, Yoshua Bengio, and Christopher J Pal. Deep complex networks. In Int. Conf. on Learning Representations (ICLR), 2018.   \n[31] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2015.   \n[32] Rishabh Kabra, Chris Burgess, Loic Matthey, Raphael Lopez Kaufman, Klaus Greff, Malcolm Reynolds, and Alexander Lerchner. Multi-object datasets. https://github.com/deepmind/multiobject-datasets/, 2019.   \n[33] Klaus Greff, Rapha\u00ebl Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess, Daniel Zoran, Loic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object representation learning with iterative variational inference. In Proc. Int. Conf. on Machine Learning (ICML), 2019.   \n[34] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention. In Proc. Advances in Neural Information Processing Systems (NeurIPS), 2020.   \n[35] Patrick Emami, Pan He, Sanjay Ranka, and Anand Rangarajan. Efficient iterative amortized inference for learning symmetric and disentangled multi-object representations. In Proc. Int. Conf. on Machine Learning (ICML), 2021.   \n[36] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Int. Conf. on Learning Representations (ICLR), 2015.   \n[37] William M Rand. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical association, 66(336):846\u2013850, 1971.   \n[38] Lawrence Hubert and Phipps Arabie. Comparing partitions. Journal of classification, 2:193\u2013218, 1985.   \n[39] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(86):2579\u20132605, 2008.   \n[40] SM Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Geoffrey E Hinton, et al. Attend, infer, repeat: Fast scene understanding with generative models. In Proc. Advances in Neural Information Processing Systems (NIPS), 2016.   \n[41] Adam Kosiorek, Hyunjik Kim, Yee Whye Teh, and Ingmar Posner. Sequential attend, infer, repeat: Generative modelling of moving objects. In Proc. Advances in Neural Information Processing Systems (NeurIPS), 2018.   \n[42] Eric Crawford and Joelle Pineau. Spatially invariant unsupervised object detection with convolutional neural networks. In Proc. AAAI Conf. on Artificial Intelligence, 2019.   \n[43] Aleksandar Stani\u00b4c, Sjoerd van Steenkiste, and J\u00fcrgen Schmidhuber. Hierarchical relational inference. In Proc. AAAI Conf. on Artificial Intelligence, 2021.   \n[44] Zhixuan Lin, Yi-Fu Wu, Skand Vishwanath Peri, Weihao Sun, Gautam Singh, Fei Deng, Jindong Jiang, and Sungjin Ahn. SPACE: unsupervised object-oriented scene representation via spatial attention and decomposition. In Int. Conf. on Learning Representations (ICLR), 2020.   \n[45] Geoffrey E Hinton, Sara Sabour, and Nicholas Frosst. Matrix capsules with EM routing. In Int. Conf. on Learning Representations (ICLR), 2018.   \n[46] Klaus Greff, Antti Rasmus, Mathias Berglund, Tele Hao, Harri Valpola, and J\u00fcrgen Schmidhuber. Tagger: Deep unsupervised perceptual grouping. In Proc. Advances in Neural Information Processing Systems (NIPS), 2016.   \n[47] Klaus Greff, Sjoerd van Steenkiste, and J\u00fcrgen Schmidhuber. Neural expectation maximization. In Proc. Advances in Neural Information Processing Systems (NIPS), 2017.   \n[48] Yi-Fu Wu, Klaus Greff, Gamaleldin Fathy Elsayed, Michael Curtis Mozer, Thomas Kipf, and Sjoerd van Steenkiste. Inverted-attention transformers can learn object representations: Insights from slot attention. In UniReps: the First Workshop on Unifying Representations in Neural Models, NeurIPS, 2023.   \n[49] J\u00fcrgen Schmidhuber. Learning to control fast-weight memories: An alternative to recurrent nets. Neural Computation, 4(1):131\u2013139, 1992.   \n[50] Christopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt Botvinick, and Alexander Lerchner. Monet: Unsupervised scene decomposition and representation. arXiv preprint arXiv:1901.11390, 2019.   \n[51] Martin Engelcke, Adam R. Kosiorek, Oiwi Parker Jones, and Ingmar Posner. Genesis: Generative scene inference and sampling with object-centric latent representations. In Int. Conf. on Learning Representations (ICLR), 2020.   \n[52] Gautam Singh, Yeongbin Kim, and Sungjin Ahn. Neural systematic binder. In Int. Conf. on Learning Representations (ICLR), 2022.   \n[53] Thomas Kipf, Gamaleldin Fathy Elsayed, Aravindh Mahendran, Austin Stone, Sara Sabour, Georg Heigold, Rico Jonschkowski, Alexey Dosovitskiy, and Klaus Greff. Conditional objectcentric learning from video. In Int. Conf. on Learning Representations (ICLR), 2022.   \n[54] Gamaleldin F. Elsayed, Aravindh Mahendran, Sjoerd van Steenkiste, Klaus Greff, Michael C. Mozer, and Thomas Kipf. $\\mathrm{SAVi++}$ : Towards end-to-end object-centric learning from real-world videos. In Proc. Advances in Neural Information Processing Systems (NeurIPS), 2022.   \n[55] Gautam Singh, Yi-Fu Wu, and Sungjin Ahn. Simple unsupervised object-centric learning for complex and naturalistic videos. In Proc. Advances in Neural Information Processing Systems (NeurIPS), 2022.   \n[56] Andrii Zadaianchuk, Maximilian Seitzer, and Georg Martius. Object-centric learning for real-world videos by predicting temporal feature similarities. In Proc. Advances in Neural Information Processing Systems (NeurIPS), 2023.   \n[57] Nanbo Li, Cian Eastwood, and Robert Fisher. Learning object-centric representations of multiobject scenes from multiple views. In Proc. Advances in Neural Information Processing Systems (NeurIPS), 2020.   \n[58] Karl Stelzner, Kristian Kersting, and Adam R Kosiorek. Decomposing 3d scenes into objects via unsupervised volume segmentation. arXiv:2104.01148, 2021.   \n[59] Mehdi S. M. Sajjadi, Daniel Duckworth, Aravindh Mahendran, Sjoerd van Steenkiste, Filip Paveti\u00b4c, Mario Lu\u02c7ci\u00b4c, Leonidas J. Guibas, Klaus Greff, and Thomas Kipf. Object Scene Representation Transformer. In Proc. Advances in Neural Information Processing Systems (NeurIPS), 2022.   \n[60] Pradyumna Reddy, Scott Wisdom, Klaus Greff, John R. Hershey, and Thomas Kipf. Audioslots: A slot-centric generative model for audio separation. In IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW), 2023.   \n[61] Joonsu Gha, Vincent Herrmann, Benjamin Grewe, J\u00fcrgen Schmidhuber, and Anand Gopalakrishnan. Unsupervised musical object discovery from audio. In Workshop on Machine Learning for Audio, NeurIPS, 2023.   \n[62] Anand Gopalakrishnan, Kazuki Irie, J\u00fcrgen Schmidhuber, and Sjoerd van Steenkiste. Unsupervised Learning of Temporal Abstractions With Slot-Based Transformers. Neural Computation, 35(4):593\u2013626, 2023.   \n[63] Christoph von der Malsburg and Werner Schneider. A neural cocktail-party processor. Biological cybernetics, 54(1):29\u201340, 1986.   \n[64] Christoph von der Malsburg and Joachim Buhmann. Sensory segmentation with coupled neural oscillators. Biological cybernetics, 67(3):233\u2013242, 1992.   \n[65] DeLiang Wang. The time dimension for scene analysis. IEEE Transactions on Neural Networks, 16(6):1401\u20131426, 2005.   \n[66] A. Ravishankar Rao, Guillermo A. Cecchi, Charles C. Peck, and James R. Kozloski. Unsupervised segmentation with dynamical units. IEEE Transactions on Neural Networks, 19(1): 168\u2013182, 2008.   \n[67] Christoph von der Malsburg. Binding in models of perception and brain function. Current opinion in neurobiology, 5(4):520\u2013526, 1995.   \n[68] Wolf Singer and Charles M Gray. Visual feature integration and the temporal correlation hypothesis. Annual review of neuroscience, 18(1):555\u2013586, 1995.   \n[69] Wolf Singer. Neuronal synchrony: A versatile code for the definition of relations? Neuron, 24 (1):49\u201365, 1999.   \n[70] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, Thomas Kipf, Abhijit Kundu, Dmitry Lagun, Issam Laradji, Hsueh-Ti (Derek) Liu, Henning Meyer, Yishu Miao, Derek Nowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Radwan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi, Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun, Suhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi, Fangcheng Zhong, and Andrea Tagliasacchi. Kubric: a scalable dataset generator. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2022.   \n[71] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011.   \n[72] Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018.   \n[73] Leland McInnes, John Healy, Nathaniel Saul, and Lukas Grossberger. Umap: Uniform manifold approximation and projection. The Journal of Open Source Software, 3(29):861, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Experimental Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Datasets. We evaluate all models on a subset of three datasets from the multi-object suite [32] namely: Tetrominoes which consists of colored tetris blocks on a black background, dSprites with colored sprites of various shapes like heart, square, oval, etc., on a grayscale background, and lastly, CLEVR, a dataset from a synthetic 3D environment. For CLEVR, we use the flitered version [35] which consists of images containing less than seven objects sometimes referred to as CLEVR6 as in Locatello et al. [34]. We normalize all input RGB images to have pixel values in the range $[0,1]$ consistent with prior work [23]. To generate the grayscale variant of the CLEVR dataset we apply the color to grayscale conversion function from the Pillow library as part of the data preprocessing pipeline. ", "page_idx": 15}, {"type": "text", "text": "Models. Table 9 shows the architecture specifications such as number of layers, kernel sizes, stride lengths, number of channels etc., for the convolution layers used by the encoder and decoder modules in SynCx. We reproduce unsupervised object discovery results of the RF baseline by adapting the source code released by the authors. 4 Similarly, we reproduce unsupervised object discovery for the CAE, $\\mathrm{CAE++}$ and CtCAE by adapting the source code released by the authors. 5 ", "page_idx": 15}, {"type": "table", "img_path": "ZFbqnL1AiS/tmp/0c88d532b1def8af9b182516d56b12b3801bd90280ef7211ddf9324c2547ec3c.jpg", "table_caption": ["Table 9: Encoder and Decoder architecture specifications for SynCx. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Training Details. Table 10 shows the hyperparameter configurations used to report unsupervised object discovery results (Table 1) for SynCx. To reproduce the unsupervised object discovery results for RF in Table 1 we use the hyperparameter configurations reported by the authors [25] listed in Appendix D.6. ", "page_idx": 15}, {"type": "table", "img_path": "ZFbqnL1AiS/tmp/a3033141f6572387700b83a404d59c315a31b0b4b4c57bfe36461b6bd864f046.jpg", "table_caption": ["Table 10: Training hyperparameters for SynCx. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Computational Efficiency. We report the training and inference time (wall-clock) for our models across 3 image resolutions for the 3 datasets used in this work from the multi-object suite. Inference on the test set containing 320 images of 35x35 resolution takes 7.87 seconds using 3 iterations, for 64x64 images it takes 38.96 seconds using 3 iterations and for the 96x96 images it takes 112.75 seconds using 4 iterations on a single NVIDIA GTX1080Ti GPU. Training time(s) on the other hand differs depending on the image resolution, number of iterations and model size. To train our model for 40k steps on $35\\mathrm{x}35$ resolution images from Tetrominoes took 1.7 hours on a NVIDIA Tesla P100 GPU. To train our model for 100k steps on 64x64 resolution images from dSprites took 4.25 hours on a NVIDIA Tesla P100 GPU. To train our model for $100\\mathbf{k}$ steps on $96\\mathrm{x}96$ resolution images from CLEVR took 17.87 hours on a NVIDIA Tesla V100-SXM2 GPU. To reproduce all the results/tables (mean and std-dev across 5 seeds) reported in this work we estimate the compute requirement to be 390 GPU hours in total for training models (RF and SynCx). Further, we estimate that the total compute used in this project is roughly 10-15 times more than the above figure, used for experiments in the prototyping phase. Note, that the figure above does not account for the phase map visualization and evaluation (clustering) procedures to extract object masks using the trained models primarily executed on CPUs. ", "page_idx": 16}, {"type": "text", "text": "Phase Map Visualization. We seek to visualize the complex-valued feature map $\\mathbf{h}\\in\\mathbb{C}^{h^{\\prime}\\times w^{\\prime}\\times d_{\\mathrm{{out}}}}$ at some intermediate layer of the decoder module as an image. We apply dimensionality reduction along the channels of $\\mathbf{h}$ so as to represent it as an one dimensional feature map. We start by constructing a complex-valued feature map in polar form with phase components $\\left(\\phi_{h}\\right)$ and magnitude components being the identity, i.e., $\\pmb{\\mu}_{h}\\leftarrow\\mathbf{1}\\in\\mathbb{R}^{h^{\\prime}\\times w^{\\prime}\\times d_{\\mathrm{out}}}$ . Then, we convert the feature map obtained in the previous step from polar to Cartesian form. Next, we apply t-SNE to project the Cartesian domain feature map in two dimensions at each spatial location. We use the t-SNE implementation in the scikit-learn library [71] (sklearn.manifold.TSNE) with n_iter set to 500, metric set to Euclidean and perplexity set to 20. We also experimented with UMAP [72] for the dimensionality reduction computation but found it to be inferior to t-SNE qualitatively (see Figure 10) and processing time per image (UMAP takes $\\approx24$ seconds whereas t-SNE takes $\\approx8$ seconds). We use the UMAP implementation in the umap-learn library [73] (umap.UMAP) with default arguments. Therefore, we use t-SNE for dimensionality reduction in the phase visualization process. We recover \u2018composite\u2019 phase $\\overline{{\\phi}}_{h}\\in\\mathbb{R}^{h^{\\prime}\\times w^{\\prime}}$ by converting the two dimensional feature back to the polar form. The \u2018composite\u2019 magnitude $\\overline{{\\mu}}_{h}\\in\\mathbb{R}^{h^{\\prime}\\times w^{\\prime}}$ at each spatial location of the map is the Euclidean norm (along channels) of the vector-valued features (i.e., magnitude components of $\\mathbf{h}$ ) at that location. The \u2018composite\u2019 phase at every spatial location is visualized as the heatmap value and the \u2018composite\u2019 magnitude is its distance from the center in the radial plot. The colors of points are matched across the heatmap and the radial plot such that the color of a pixel in the former correspond to its orientation in the latter. Lastly, the magnitude of background regions are masked out as was the case while extracting object assignments from phase maps. ", "page_idx": 16}, {"type": "text", "text": "Evaluation Details. To evaluate our model for the unsupervised object discovery task we require discrete object assignments at every location in the image. We cluster the continuous phase components of the complex-valued feature maps from an intermediate layer (second to last by default) of the decoder to compute these object assignments for every pixel. We use the $k$ -means implementation in the scikit-learn library [71] (sklearn.clustering.KMeans) with n_clusters set using the ground-truth value for each dataset and n_init set to 5. ", "page_idx": 16}, {"type": "text", "text": "B Additional Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Table 11: MSE and ARI scores (mean $\\pm$ standard deviation across 5 seeds) for CAE, $\\mathrm{CAE++}$ , CtCAE, RF and SynCx models for Tetrominoes, dSprites and CLEVR. Results for CAE, $\\mathrm{CAE++}$ and CtCAE baselines are taken from Stanic\u00b4 et al. [24]. ", "page_idx": 17}, {"type": "table", "img_path": "ZFbqnL1AiS/tmp/b881a903a58f1cc5854947ad94ad9dad3dc2a090f55d0137073c0cad7483d7bd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "ZFbqnL1AiS/tmp/20d342c8b98340019babd031bc2f584b91c8379a668b27ad6c751cb35ff80148.jpg", "img_caption": ["C Additional Visualizations ", "Figure 7: Collection of images grouped by SynCx from the Tetrominoes dataset. Rows 1-3 show cases where SynCx perfectly groups the 3 tetris blocks even with mulitple objects of the same color. Row 4 shows a failure mode where it imperfectly partitions the two red blocks into one straight parts and two $\\mathbb{L}$ -shaped parts. It is plausible decomposition since the dataset contains many tetris blocks with such $\\mathbb{L}$ -shaped bends. Rows 5 and 6 show another failure mode where it fails to decompose the two similarly colored tetris blocks at all. Rather SynCx learns specialized phases for the edges and interior portions of each of the similarly colored tetris blocks. This could be a caused by a particularly poor initialization of phase maps by random sampling. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "ZFbqnL1AiS/tmp/faaecb66d0abd9087717dec751637c93b65cbdedd41cf6185f90803613710e49.jpg", "img_caption": ["Figure 8: Collection of images grouped by SynCx from the dSprites dataset. Rows 1-3 show cases where SynCx perfectly groups images containing 3-5 objects. We can also see that the pairs of phase map and radial plot visualizations show discernible specialization in phase values towards an object. Row 4 shows a grouping failure where the square and heart in light blue are incorrectly grouped together. However, on closer inspection of the corresponding phase plots we can see that the phases do show specialization (see row 4 column 6) towards both the square (light blue) and heart (dark blue). This grouping failure is a limitation in the clustering process (KMeans) to extract object assignments and assumes clusters have equal angular variance which need not hold true in practice. Similarly, grouping failures in rows 5 and 6, where blue oval and purple square (row 5) or light blue heart and cobalt square (row 6) are incorrectly grouped together despite showing enough phase specialization in each case. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "ZFbqnL1AiS/tmp/33273f18d26452e9799170e950fb25ad9bdd8168f56f5e2c5e8acf89aab0a526.jpg", "img_caption": ["Figure 9: Collection of images grouped by SynCx from the CLEVR dataset. Rows 1-4 show cases where $\\mathrm{{SynCx}}$ groups scenes containing three to six objects reasonably well. Row 5 shows a failure mode where the gray, red and purple objects are incorrectly grouped together despite showing noticeable (see column 6). This can be attributed to the limitations in the clustering process similar to the ones highlighted previously with regards to assumption of equal angular variance of clusters. Similar effects can be observed with the grouping errors in row 6. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "ZFbqnL1AiS/tmp/79464e662835dd467d67c5e0e2520d00bea2f2a17df5e394d07eb140e7edd1b7.jpg", "img_caption": ["Figure 10: Collection of samples comparing the use of t-SNE versus UMAP for the dimensionality reduction computation in the visualization process. Panels 1 and 5 show cases where the clusters of projected phase maps of both t-SNE and UMAP correlate with the number of objects in the image. Panels 2, 3 and 4 show cases where the clusters of phases projected using t-SNE correlate with the number of objects in the image whereas the clusters of phases projected using UMAP do not. Overall, we find that t-SNE produces qualitatively better projections than UMAP, i.e., phase clusters correlate better to the number of objects in the image. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: In the abstract we make the following claims: \"recurrent architecture with complex-valued weights perform equally or better at unsupervised object discovery compared to architectures that have complex activations but real-valued weights\" which is supported by superior performance in the experiments; we claim that SynCx (our model) avoids certain systematic grouping errors of current models, such as the inability to separate similarly colored objects without additional supervision, which is also supported by experiments with grayscale data. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We discuss limitations of our method in a dedicated paragraph (last paragraph of the Experiments section), in particular we show its robustness to grouping in the absence of color information. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide all architectural and hyperparameter details in the paper. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Data we used is already public and upon acceptance we will publish our code. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: All training and test details are provided in Appendix A. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: All experimental results are provided with mean and standard deviation across 5 seeds. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide sufficient information (see Appendix A) of the hardware resources required to reproduce the main results of the paper as well resources used during the prototyping phase. ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics and confirm that our work respects it, i.e. it does not involve human subjects, the data we use is synthetic (no privacy or ethical concerns) and we foresee no harmful consequences of our work as the models are small scale and training and evaluation data is all synthetic. ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: We foresee neither negative nor positive societal impacts of our work as the visual scenes we experiment on are highly synthetic (toy tasks) nowhere near the the complexity or scale of naturalistic images/videos. We also only use very small-scale models (less than 1M params) which are not mature yet for use in production. ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: We do not use language models, image generators or datasets scraped from the internet in our work, so we foresee no risk of misuse. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The SynCx model is an original contribution of ours. The baseline models CAE, $\\mathrm{CAE++}$ , CtCAE and RF have been properly credited with citations to the research papers and code repositories. We also properly cite the data source for the multi-object suite. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: We don\u2019t introduce any new assets. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: We do not have any crowd sourcing or research with human subjects. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] Justification: We do not have any research with human subjects in our work. ", "page_idx": 24}]