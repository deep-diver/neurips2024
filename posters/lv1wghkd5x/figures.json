[{"figure_path": "lV1wGHKd5x/figures/figures_1_1.jpg", "caption": "Figure 1: (left) The training of the CLAP model for learning cross-modal representations. (right) Zero-shot classification with the CLAP model.", "description": "This figure illustrates the training and inference processes of the CLAP model. The left panel shows the training phase where the text encoder processes text prompts, the audio encoder processes audio waveforms, and a contrastive loss function learns a joint embedding space for text and audio. The right panel shows the zero-shot classification process, where a new text prompt is encoded, and its similarity to the embeddings of different audio waveforms determines the classification result.", "section": "2 Preliminaries"}, {"figure_path": "lV1wGHKd5x/figures/figures_2_1.jpg", "caption": "Figure 2: LMAC-ZS architecture. The input spectrogram (linear frequency) X\u2081 (the i-th audio in the batch) first of all passes through the transformations (InputTf block) to make it compatible with the input domain (e.g. Mel Spectra) of the audio encoder faudio(.), which yields the latent representations hi. These representations along with the text representation tj (the j-th text prompt within the batch) are then fed to the decoder Me(., .). The resulting mask is then element-wise multiplied with the input spectrogram X\u2081. The masked spectrogram M X\u2081 is then converted back to the input domain of the audio encoder, and the similarity score te faudio (Mo(ti, hj) Xaudio,j) is calculated, which is used in the overall training objective Lzs(0). The listenable explanation is produced by simply inverting the masked spectrogram through the inverse-STFT by incorporating the phase spectrogram of the input Xphase.", "description": "This figure illustrates the architecture of the Listenable Maps for Zero-Shot Audio Classifiers (LMAC-ZS) method.  The input audio spectrogram undergoes transformations to match the encoder's input format, then gets processed by the audio encoder.  Simultaneously, text input is processed by the text encoder. Both encodings are fed to a decoder which generates a mask. This mask modifies the original spectrogram. The modified spectrogram is then processed to produce a listenable explanation. The entire process is guided by a novel loss function that aims to preserve the original audio-text similarity.", "section": "2.2 Saliency Maps For Fixed Set Audio Classifiers"}, {"figure_path": "lV1wGHKd5x/figures/figures_5_1.jpg", "caption": "Figure 3: (left) Mask-Mean vs Similarity for LMAC-ZS, (middle) Mask-Mean vs Similarity for GradCam++, (right) Model Randomization Test for LMAC-ZS and GradCam++.", "description": "This figure presents three plots visualizing the results of experiments comparing LMAC-ZS and GradCAM++. The left plot shows the relationship between the mean of the mask values (Mask-Mean) and the similarity scores between audio and text for LMAC-ZS. The middle plot displays the same relationship but for GradCAM++. The right plot illustrates the results of a model randomization test, showing the structural similarity index measure (SSIM) values for both methods across different convolutional block IDs. These plots demonstrate the faithfulness and robustness of LMAC-ZS in comparison to GradCAM++.", "section": "4 Experiments"}, {"figure_path": "lV1wGHKd5x/figures/figures_8_1.jpg", "caption": "Figure 4: Qualitative Comparisons of Explanations given by LMAC-ZS, and GradCAM++, for two different classes. We see that LMAC-ZS shuts-off the explanation depending on the similarity of the given prompt with the input audio, whereas GradCAM++ remains insensitive to the class label.", "description": "This figure provides a qualitative comparison of the explanations generated by the proposed method LMAC-ZS and the baseline GradCAM++.  Two audio examples are shown, one of a 'cat' sound and one of 'glass breaking'. For each, the input spectrogram is shown along with the explanations (saliency maps) generated by both methods, conditioned on both the correct class label and an incorrect class label. LMAC-ZS demonstrates sensitivity to the input-prompt similarity; when the similarity is high, the saliency map highlights relevant audio regions, while a low similarity leads to a nearly empty map. GradCAM++, in contrast, produces relatively consistent saliency maps regardless of the prompt.", "section": "4.4 Qualitative Comparison and Sanity Checks"}, {"figure_path": "lV1wGHKd5x/figures/figures_14_1.jpg", "caption": "Figure 5: Visualization of Explanations after Cascading Model Randomization. Left column is the input, second column is the original explanation, and more we go towards the right more layers are randomized. Top row is for LMAC-ZS, and the bottom row is for GradCAM++.", "description": "This figure shows a visualization of how the explanations change when layers of the model are randomly initialized. The leftmost column shows the input spectrogram. The second column represents the original explanation generated by each method (LMAC-ZS and GradCAM++). As we move to the right, more and more layers of the model are randomly reinitialized, demonstrating how the explanations change as the model's internal representations are altered. LMAC-ZS explanations quickly disappear as layers are randomized, while GradCAM++ explanations are more resilient to these changes.", "section": "A.2 Qualitative Analysis of Model Randomization Test"}, {"figure_path": "lV1wGHKd5x/figures/figures_14_2.jpg", "caption": "Figure 4: Qualitative Comparisons of Explanations given by LMAC-ZS, and GradCAM++, for two different classes. We see that LMAC-ZS shuts-off the explanation depending on the similarity of the given prompt with the input audio, whereas GradCAM++ remains insensitive to the class label.", "description": "This figure compares the qualitative results of explanation methods LMAC-ZS and GradCAM++, applied to audio classification tasks.  Two examples are shown, each with two different text prompts.  The left column of each example shows the spectrogram of the input audio. The middle column shows the saliency map generated by the explanation method when the text prompt matches the audio class. The right column shows the saliency map when the text prompt does *not* match the audio class. LMAC-ZS demonstrates sensitivity to the text prompt, producing a blank saliency map when there is little similarity between the audio and prompt. GradCAM++, on the other hand, is shown to be insensitive to prompt selection, providing similar saliency maps regardless of the prompt used.", "section": "4.4 Qualitative Comparison and Sanity Checks"}, {"figure_path": "lV1wGHKd5x/figures/figures_14_3.jpg", "caption": "Figure 4: Qualitative Comparisons of Explanations given by LMAC-ZS, and GradCAM++, for two different classes. We see that LMAC-ZS shuts-off the explanation depending on the similarity of the given prompt with the input audio, whereas GradCAM++ remains insensitive to the class label.", "description": "This figure displays a comparison of qualitative results for LMAC-ZS and GradCAM++.  Two audio samples are shown, one labeled \"Cat\" and one labeled \"Glass breaking.\" For each audio sample, the input spectrogram is displayed along with the explanations generated by each method when prompted with both the correct label and an incorrect label.  The key observation is that LMAC-ZS's explanations are sensitive to the similarity between the input audio and the given prompt; when the prompt is irrelevant to the audio, the explanation is largely suppressed.  In contrast, GradCAM++ produces explanations that do not adapt to the prompt, providing outputs that are largely consistent regardless of the prompt's relevance.", "section": "4.4 Qualitative Comparison and Sanity Checks"}, {"figure_path": "lV1wGHKd5x/figures/figures_15_1.jpg", "caption": "Figure 8: Explanations obtained with the additional diversity term (Eq 7).", "description": "This figure shows qualitative results comparing explanations generated by LMAC-ZS with and without the additional diversity term (Equation 7) in the paper.  The top row displays spectrograms of input audio, the middle row shows the generated explanations (saliency masks) when the diversity term is included, and the bottom row shows the explanations without the diversity term.  Each column represents a different audio-text pair, with the original and masked similarities noted. The figure demonstrates how the additional diversity term makes the generated explanations more sensitive to the text prompts, resulting in more focused and relevant saliency maps.", "section": "B Explanation sensitivity to Audio-Text Similarity"}, {"figure_path": "lV1wGHKd5x/figures/figures_15_2.jpg", "caption": "Figure 8: Explanations obtained with the additional diversity term (Eq 7).", "description": "This figure showcases the qualitative results comparing the explanations generated by LMAC-ZS with and without the additional diversity term (Equation 7).  The top row shows the input spectrogram and the original audio-text similarity. The middle row shows the explanation generated with the additional diversity term applied. The bottom row shows the similarity after masking the spectrogram with the generated mask. The figure demonstrates how the additional diversity term enhances the sensitivity of the generated masks to different text prompts.", "section": "B Explanation sensitivity to Audio-Text Similarity"}, {"figure_path": "lV1wGHKd5x/figures/figures_16_1.jpg", "caption": "Figure 10: Audio-text similarity after audio masking, without the diversity term (left), with the diversity term (right).", "description": "This figure shows 2D histograms comparing mask means and audio-text similarities after masking audio. The left panel shows results without the diversity term (Equation 7), and the right panel shows results with it.  The color intensity represents the frequency of data points in each bin. The x-axis shows the similarity between the text prompt and masked audio, while the y-axis shows the average value (mean) of the mask. The figure demonstrates how the additional diversity term enhances the sensitivity of the generated explanations to different text prompts, resulting in a clearer relationship between similarity and mask means.", "section": "B Explanation sensitivity to Audio-Text Similarity"}]