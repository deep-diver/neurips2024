[{"figure_path": "UiQkFXLfbu/tables/tables_3_1.jpg", "caption": "Table 3: Full task list and ground truth Base reward functions.", "description": "This table lists 16 different tasks (numbered 0-15) used to evaluate the proposed DLM. Each task is described by a natural language prompt specifying a desired policy outcome.  The table shows the complete prompt for each task and the corresponding ground truth reward function used as a baseline for comparison in the evaluation. This baseline represents a direct implementation of the desired outcome. The reward function is a lambda function, using the current state and feature vector of each arm to generate a reward score.", "section": "5.1 Simulated Public Health Setting"}, {"figure_path": "UiQkFXLfbu/tables/tables_8_1.jpg", "caption": "Table 1: Average precision/recall of features used in LLM-proposed reward functions compared to ground truth Base reward function. Comparison between zeroshot DLM (No Reflection) and DLM (Reflection). Cells in yellow showed improvement from Reflection with p < 0.1; cells in green showed improvement from Reflection with p < 0.05. Results indicate LLMs are very effective feature extractors for reward function generation. Furthermore, the Reflection module is particularly useful for improving recall rates, as 13/16 tasks showed significant recall improvement with Reflection.", "description": "This table shows the precision and recall of features used by LLMs in generating reward functions, compared to the ground truth Base reward functions.  It compares the performance of the zeroshot DLM (without reflection) and the DLM with reflection.  Cells highlighted in yellow indicate improvements from reflection with a p-value less than 0.1, while green cells show improvements with a p-value less than 0.05. The results suggest LLMs are effective at extracting features for reward function design, and that the reflection module is especially helpful in enhancing recall.", "section": "5.5 LLM-Generated Rewards and Reflection in Public Health Settings"}, {"figure_path": "UiQkFXLfbu/tables/tables_15_1.jpg", "caption": "Table 3: Full task list and ground truth Base reward functions.", "description": "This table lists sixteen different tasks used to evaluate the proposed DLM model.  Each task is defined by a human-language prompt describing a specific policy goal (e.g., prioritizing older mothers or low-income beneficiaries).  The table also shows the corresponding ground truth Base reward function for each task, which represents the ideal reward function aligned with the prompt's intention. This table is crucial for understanding the evaluation setup, showcasing the range of objectives addressed and the corresponding reward functions used as a baseline to compare the performance of the LLM-generated reward functions.", "section": "5.1 Simulated Public Health Setting"}, {"figure_path": "UiQkFXLfbu/tables/tables_17_1.jpg", "caption": "Table 3: Full task list and ground truth Base reward functions.", "description": "This table lists sixteen different tasks used to evaluate the proposed DLM model. Each task represents a specific policy objective in maternal healthcare, focusing on various demographic groups such as older mothers, low-income families, and those with limited access to technology. The \"Full Prompt\" column provides the natural language description used to guide the DLM model, while the \"Base Reward Function\" column specifies the ground truth reward function corresponding to each task, expressed as a Python lambda function.", "section": "5.1 Simulated Public Health Setting"}, {"figure_path": "UiQkFXLfbu/tables/tables_18_1.jpg", "caption": "Table 2: Recall of logical combinations of features for multi-feature prompts. We consider multi-feature prompts 4\u201315, and report the recall compared to Base reward for accurately emulating behavior of Base reward. Note that we consider only LLM generations with high feature recall, i.e. those proposed rewards that include, at minimum, the features used in the corresponding ground truth Base reward.", "description": "This table presents the recall of logical combinations of features for prompts 4-15 in the experiment.  The recall is a measure of how well the LLM-generated reward functions capture the features used in the ground truth (Base) reward functions.  Only LLM generations that included at least the features in the Base reward are considered.", "section": "5 Experimental Evaluation"}, {"figure_path": "UiQkFXLfbu/tables/tables_21_1.jpg", "caption": "Table 3: Full task list and ground truth Base reward functions.", "description": "This table presents 16 different tasks used in the experiments. Each task is defined by a prompt that describes a specific policy goal. The prompts are designed to target different features such as age, income, language, education, and call times.  Each task also includes the corresponding ground truth base reward function. The base reward functions are used to evaluate the performance of the proposed DLM and are considered the \"gold standard\".", "section": "5.1 Simulated Public Health Setting"}, {"figure_path": "UiQkFXLfbu/tables/tables_22_1.jpg", "caption": "Table 3: Full task list and ground truth Base reward functions.", "description": "This table lists 16 different tasks (prompts) used to evaluate the DLM model. Each task describes a specific policy goal for resource allocation, focusing on different demographic subpopulations (e.g., older mothers, low-income families, Hindi speakers). The table also provides the corresponding ground truth Base reward function for each task, which serves as the target for the LLM to achieve through reward function generation.", "section": "5.1 Simulated Public Health Setting"}, {"figure_path": "UiQkFXLfbu/tables/tables_23_1.jpg", "caption": "Table 4: Full prompts and numerical results.", "description": "This table presents the results of the experiments conducted in the paper. It shows the mean normalized reward (MNR) for each of the 16 tasks (prompts) for different methods: Base reward, No Action, Default, DLM (No Reflection), and DLM (Reflection). The Base reward is the ground truth reward, No Action represents a policy that does not take any action, Default is the original (fixed) reward function, DLM (No Reflection) is the proposed method without the self-reflection stage, and DLM (Reflection) is the proposed method with self-reflection. The table also includes statistical significance tests comparing MNR scores.", "section": "5 Experimental Evaluation"}, {"figure_path": "UiQkFXLfbu/tables/tables_24_1.jpg", "caption": "Table 3: Full task list and ground truth Base reward functions.", "description": "This table lists the 16 different tasks used in the experiments. For each task, it provides a description of the prompt given to the LLM,  along with the corresponding base reward function used for evaluation. The prompts vary in terms of prioritizing specific demographic groups (e.g., older mothers, low-income mothers) or other aspects like preferred call times. The base reward functions are used to measure the performance of the LLM-generated reward functions against a human-defined benchmark, where a reward of 1 represents a perfect alignment with the human goal.", "section": "5.1 Simulated Public Health Setting"}]