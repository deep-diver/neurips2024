[{"figure_path": "SpcEwP6EYt/figures/figures_1_1.jpg", "caption": "Figure 1: The overall workflow of the proposed method. To enhance the output feature representation, the LAF loss is used to transmit the important information of output feature of the ANN to that of the SNN counterpart and the last LIF activation neuron layer is replaced by ReLU activation in our SNN structures.", "description": "This figure illustrates the proposed EnOF-SNN method.  It shows three network structures: a trained ANN, a vanilla SNN, and the proposed improved SNN. The ANN is used as a teacher network, its output features guiding the training of the SNN via a Kullback-Leibler (KL) divergence loss (LAF loss).  The proposed SNN modifies the vanilla SNN by replacing the last LIF activation layer with a ReLU activation, aiming to create a richer output feature representation. This results in a full precision output feature for the SNN, improving its performance.", "section": "1 Introduction"}, {"figure_path": "SpcEwP6EYt/figures/figures_5_1.jpg", "caption": "Figure 2: t-SNE visualization of the output feature for random 2000 samples in CIFAR-10. Every color represents a different class. It can be clearly seen that our method can learn better representations.", "description": "This figure shows the t-SNE visualization of output features from four different models trained on the CIFAR-10 dataset.  The four models are: a vanilla SNN, an SNN trained with the LAF loss, an SNN with the RepAct modification, and an SNN trained with both LAF loss and RepAct. Each point represents a feature vector from a randomly selected sample, and the color of the point indicates the class of the sample. The figure demonstrates how the proposed methods (LAF loss and RepAct) improve the quality of the learned feature representations, resulting in more distinct clusters for each class and better class separability.", "section": "4 Ablation Study"}]