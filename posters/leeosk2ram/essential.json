{"importance": "This paper is crucial for researchers working on **large vision-language models (LVLMs)** and **retrieval-augmented generation (RAG)**. It introduces a novel framework to tackle the limitations of LVLMs in handling up-to-date information, bridging the gap between static LVLMs and dynamic internet knowledge. The proposed framework and dataset are valuable resources, opening avenues for further investigation in improving LVLMs' capabilities to process real-time information.", "summary": "SearchLVLMs: A plug-and-play framework efficiently augments large vision-language models with up-to-date internet knowledge via hierarchical filtering, significantly improving accuracy on visual question answering.", "takeaways": ["SearchLVLMs framework efficiently integrates existing LVLMs with current internet knowledge, enhancing visual question answering.", "A novel hierarchical filtering model effectively selects relevant information from web search results to prompt LVLMs.", "The UDK-VQA dataset, automatically generated from news sources, provides a benchmark for evaluating LVLM's performance on up-to-date knowledge."], "tldr": "Large vision-language models (LVLMs) often struggle with up-to-date information due to infrequent updates.  This paper addresses this limitation by proposing a novel framework, SearchLVLMs. The core problem is that LVLMs are trained on massive datasets that cannot be constantly updated. This makes them unable to answer questions requiring information generated after their training.  The challenge is to incorporate new information without retraining the entire model which is computationally expensive and time-consuming. \nSearchLVLMs tackles this challenge by using a **hierarchical filtering model** to efficiently retrieve and filter relevant information from the internet, specifically focusing on visual question answering (VQA). The model first filters websites based on titles and snippets, then selects the most informative content segments from the chosen websites.  This filtered information is then used to prompt the LVLM, improving its ability to answer questions about recent events. The paper also introduces a new dataset, **UDK-VQA**, specifically designed for testing models on up-to-date knowledge which further enhances the value of this work.", "affiliation": "Shanghai AI Laboratory", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "leeosk2RAM/podcast.wav"}