[{"figure_path": "pGOBEYcXzs/tables/tables_4_1.jpg", "caption": "Table 1: Comparison of memory requirements for deploying Float16 and binarized models, with the number in parentheses denoting the compression ratio of binarized models over Float16 models.", "description": "This table compares the memory usage of large language models (LLMs) when using different quantization methods.  It shows the memory footprint in GB for Float16 (full precision) and four different binarization techniques (PB-LLM, BiLLM, OneBit, and BinaryMoS) across two different LLaMA model sizes (7B and 13B parameters).  The numbers in parentheses indicate the compression ratio achieved by each binarization method compared to the full-precision Float16 model.  This highlights the space savings offered by various binarization approaches, demonstrating the efficiency of BinaryMoS in reducing the memory needed for LLMs while maintaining performance.", "section": "3.3 Impact of BinaryMoS on LLM Compression"}, {"figure_path": "pGOBEYcXzs/tables/tables_6_1.jpg", "caption": "Table 2: The impact of the numbers of scaling experts on the proposed BinaryMoS. Quick assessment conducted using the LLaMA-1-7B model trained on one-third of the training data.", "description": "This table presents the results of an experiment to determine the optimal number of scaling experts for the BinaryMoS model. The experiment used the LLaMA-1-7B model and only one-third of the training data for faster assessment. The table shows the perplexity and zero-shot accuracy results for different numbers of scaling experts (1, 2, 4, and 8).  Lower perplexity values and higher zero-shot accuracy indicate better model performance. The results suggest that using 4 scaling experts provides the optimal balance between performance improvement and increased model complexity.", "section": "4.2 Analysis on the Number of Scaling Experts"}, {"figure_path": "pGOBEYcXzs/tables/tables_7_1.jpg", "caption": "Table 3: Perplexity and zero-shot accuracy results of Float16 and binarized LLMs.", "description": "This table presents a comparison of the performance of Float16 and various binarized LLMs across different language modeling tasks.  It shows perplexity scores (lower is better) on Wiki2 and C4 datasets and zero-shot accuracy (higher is better) on several benchmark datasets (BoolQ, PIQA, HellaSwag, Winogrande, ARC-e, ARC-c). The table allows for a detailed comparison of the effectiveness of different binarization techniques (PB-LLM, BiLLM, OneBit, and BinaryMoS) in maintaining performance while reducing model size.", "section": "4.4 Perplexity and Accuracy Results of Binarized Models"}, {"figure_path": "pGOBEYcXzs/tables/tables_8_1.jpg", "caption": "Table 4: Perplexity and zero-shot accuracy results for 2-bit quantization methods and BinaryMoS.", "description": "This table compares the performance of BinaryMoS against two other 2-bit quantization methods (GPTQ and OmniQuant) across various LLMs.  It shows perplexity scores on the WikiText2 and C4 datasets, as well as average zero-shot accuracy across several downstream tasks. The results demonstrate that BinaryMoS achieves lower perplexity and higher zero-shot accuracy compared to the other 2-bit quantization methods, highlighting its effectiveness in improving the accuracy of binarized LLMs.", "section": "4.5 Comparison between BinaryMoS and 2-bit Quantization"}, {"figure_path": "pGOBEYcXzs/tables/tables_12_1.jpg", "caption": "Table 5: Evaluation of binarized LLaMA-1-7B model trained with various training datasets. We train the model on a subset of the dataset with the same training step. \u2020: Generated dataset synthesized by LLaMA-1-7B model. \u2021: Mixed dataset of Wikitext2 and C4.", "description": "This table presents the results of an ablation study on the choice of training dataset for a binarized LLaMA-1-7B language model.  It compares the performance of models trained on three different datasets: the WikiText2 dataset, the C4 dataset, and a synthetic dataset generated by the LLaMA-1-7B model.  For each dataset, the table shows perplexity scores on the WikiText2 and C4 datasets, as well as zero-shot accuracy on various common sense reasoning tasks (BoolQ, PIQA, HellaSwag, WinoGrande, ARC-e, ARC-c). The table helps determine the optimal dataset for training binarized models and demonstrates that a mixed dataset comprising both the C4 and WikiText2 datasets provides the best balance of performance across different tasks.", "section": "4.1 Experimental Settings"}, {"figure_path": "pGOBEYcXzs/tables/tables_12_2.jpg", "caption": "Table 6: Latency (\u00b5sec) of linear layer in LLaMA-1/2-7B and LLaMA-1/2-13B.", "description": "This table presents the latency in microseconds (\u00b5sec) of the linear layer operations for various LLM binarization methods, including Float16 (baseline), PB-LLM, BiLLM, OneBit, and BinaryMoS.  The latency is measured for two different LLaMA models: LLaMA-1/2-7B and LLaMA-1/2-13B, each with three different weight sizes.  The results show the impact of the different binarization techniques on inference speed.", "section": "A.2 Latency Measurement"}, {"figure_path": "pGOBEYcXzs/tables/tables_13_1.jpg", "caption": "Table 7: Perplexity and zero-shot accuracy results of Float16 and binarized LLMs for LLaMA-1-30B", "description": "This table presents the results of perplexity and zero-shot accuracy for the LLaMA-1-30B model using different binarization methods (PB-LLM, BiLLM, BinaryMoS) and the Float16 baseline.  It shows the perplexity scores on the Wiki2 and C4 datasets, and zero-shot accuracy scores across various reasoning tasks (BoolQ, PIQA, HellaSwag, WinoGrande, ARC-e, ARC-c).  The results demonstrate the relative performance of each method in terms of maintaining linguistic capabilities while achieving model compression.", "section": "A.3 Experimental Results for LLaMA-1-30B"}]