[{"figure_path": "YZWvf58dBS/figures/figures_2_1.jpg", "caption": "Figure 2: TransLLM pipeline.", "description": "This figure illustrates the pipeline of the TransLLM model, which consists of four main steps: Model Extension, Target Language Pre-training, Translation Pre-training, and Transfer Fine-tuning.  The Model Extension step involves adding LoRA modules and extending the input embedding and LM head. The Target Language Pre-training step pre-trains the model on monolingual target language data. The Translation Pre-training step trains the model on bidirectional translation tasks between English and the target language, incorporating English language modeling to protect English embeddings. Finally, the Transfer Fine-tuning step fine-tunes the model using TCOT data, recovery KD data, and translation data. The figure also shows examples of data used in each step and highlights the key components of the model, such as the extended LM head, LoRA modules, and extended input embedding.", "section": "3 Method"}, {"figure_path": "YZWvf58dBS/figures/figures_12_1.jpg", "caption": "Figure 3: TH Pre-Training loss.", "description": "This figure shows the training loss curve for the Thai (TH) pre-training stage of the TransLLM model. The x-axis represents the number of training steps, and the y-axis represents the training loss.  The curve demonstrates a rapid decrease in loss during the initial phase of training, indicating effective learning by the model. The loss eventually plateaus around 1.42, suggesting the model has converged.", "section": "3.2 Training"}, {"figure_path": "YZWvf58dBS/figures/figures_12_2.jpg", "caption": "Figure 2: TransLLM pipeline.", "description": "This figure shows the pipeline of the TransLLM model.  It is composed of four steps. The first step is Model Extension, which extends the model with LoRA modules and fine-grained target language vocabulary. The second step is Target Language Pre-training, which pre-trains the chat LLM on the monolingual target language data. The third step is Translation Pre-training, where the LLM is further trained with a bi-directional translation task between English and the target language, and an English language modeling task. The fourth and final step is Transfer Fine-tuning, where the LLM is fine-tuned on TCOT, recovery KD, and translation data.", "section": "3 Method"}, {"figure_path": "YZWvf58dBS/figures/figures_15_1.jpg", "caption": "Figure 2: TransLLM pipeline.", "description": "This figure shows the pipeline of the TransLLM model for transferring knowledge from English to Thai. The pipeline consists of four steps: model extension, target language pre-training, translation pre-training, and transfer fine-tuning. In the model extension step, the model is extended with LoRA modules and a fine-grained Thai vocabulary. In the target language pre-training step, the model is pre-trained on monolingual Thai data. In the translation pre-training step, the model is pre-trained on parallel data between English and Thai. In the transfer fine-tuning step, the model is fine-tuned on TCOT, recovery KD, and translation data.", "section": "3 Method"}]