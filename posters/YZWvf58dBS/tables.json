[{"figure_path": "YZWvf58dBS/tables/tables_0_1.jpg", "caption": "Table 1: Comparison between our model and strong LLMs on MT-bench under human evaluation. We provide the 95% confidence interval in brackets.", "description": "This table presents the results of a human evaluation comparing the performance of the proposed TransLLM model against two strong Large Language Models (LLMs), ChatGPT and GPT-4, on the MT-bench benchmark.  The evaluation focuses on a multi-turn conversation task, and the table shows the win, tie, and loss rates for each model in both the first and second turns of the conversation.  The 95% confidence intervals are also provided to indicate the uncertainty in the results.", "section": "4 Experiments"}, {"figure_path": "YZWvf58dBS/tables/tables_5_1.jpg", "caption": "Table 1: Comparison between our model and strong LLMs on MT-bench under human evaluation. We provide the 95% confidence interval in brackets.", "description": "This table presents the results of a human evaluation comparing the performance of the authors' model against ChatGPT and GPT-4 on the MT-bench benchmark.  The evaluation focuses on multi-turn conversations and uses a win/tie/loss metric to compare performance.  Confidence intervals are provided to indicate the uncertainty in the results.", "section": "4.2.1 Human Evaluation Results"}, {"figure_path": "YZWvf58dBS/tables/tables_5_2.jpg", "caption": "Table 2: Agreement between GPT-4 and humans. \u201cR=\" denotes the expect agreement between random judges. \u2020 EN results are from Zheng et al. (2024).", "description": "This table shows the agreement rate between GPT-4 evaluations and human evaluations on the MT-bench dataset for both Thai and English languages. The table presents the agreement rate with and without considering ties in the evaluations. The results indicate that GPT-4 and human annotators show considerable agreement in evaluating the helpfulness of different LLMs in both languages, especially in Thai.", "section": "4.1 Settings"}, {"figure_path": "YZWvf58dBS/tables/tables_6_1.jpg", "caption": "Table 3: Result for different models on safety benchmark AdvBenchmark under human evaluation. \u2020 GPT-4 results are from Yong et al. (2023).", "description": "This table presents the results of a human evaluation on the safety benchmark, AdvBenchmark, for various language models.  It shows the percentage of queries where the model bypassed safety measures (Bypass), rejected harmful queries (Reject), and produced unclear responses (Unclear).  The results compare the performance of ChatGPT, GPT-4, the proposed model using GPT-4 knowledge distillation (Ours w/ GPT-4 KD), the proposed model (Ours), and the original English models for context.  The table highlights the superior safety performance of the proposed model compared to others.", "section": "4.2 Main Results"}, {"figure_path": "YZWvf58dBS/tables/tables_6_2.jpg", "caption": "Table 4: Comparison between our model and different methods on MT-Bench under GPT-4 evaluation.", "description": "This table presents the results of a comparative study evaluating the performance of various large language models (LLMs) on the MT-bench benchmark.  The models are compared based on their performance across two turns of conversation, using GPT-4 as the evaluation metric.  The results show the win rate, tie rate, loss rate, and the difference between win and loss rates for each model. This comparison helps to highlight the relative strengths and weaknesses of different models for multi-turn conversation in a non-English language.", "section": "4.2.2 GPT-4 Evaluation Results"}, {"figure_path": "YZWvf58dBS/tables/tables_6_3.jpg", "caption": "Table 5: Comparison between our model and different methods on Alpaca-Eval under GPT-4 evaluation.", "description": "This table presents the results of comparing the performance of the proposed TransLLM model against several other strong baselines on the Alpaca-Eval benchmark using GPT-4 for evaluation.  The comparison is made based on win rate, tie rate, and loss rate, with a final delta score calculated to show the overall performance difference compared to the baselines. The table demonstrates TransLLM's significant outperformance against the compared models.", "section": "4.2.2 GPT-4 Evaluation Results"}, {"figure_path": "YZWvf58dBS/tables/tables_7_1.jpg", "caption": "Table 7: The difference of generation probabilities.", "description": "This table shows the difference in generation probabilities on a hold-out validation set of recovery KD data in English between the original LLaMA2-Chat model and three variations of the TransLLM model: one without transfer fine-tuning, one with GPT-4 KD, and the final TransLLM model.  The difference reflects how much original knowledge is lost and subsequently recovered during the training process. ", "section": "5.2 TransLLM Recover the Original Knowledge"}, {"figure_path": "YZWvf58dBS/tables/tables_7_2.jpg", "caption": "Table 8: Translation performance on Flores-200.", "description": "This table presents the results of evaluating the translation performance of different models on the Flores-200 benchmark.  The benchmark assesses translation quality in both English-to-Thai (EN-TH) and Thai-to-English (TH-EN) directions.  The models compared are ChatGPT, NLLB (a multilingual model), and the authors' proposed TransLLM model.  The evaluation metrics used are COMET and BLEU scores, which are commonly used for measuring machine translation quality.  Higher scores indicate better translation performance.", "section": "5.3 Why TransLLM is better than translation-as-a-bridge?"}, {"figure_path": "YZWvf58dBS/tables/tables_8_1.jpg", "caption": "Table 9: Fluency on MT-Bench.", "description": "This table presents a human evaluation of the fluency of different models' responses in Thai on the MT-Bench benchmark.  The scores range from 5 to 7, with higher scores indicating greater fluency. The models evaluated include NLLB-bridge, GPT4, ChatGPT, and the authors' proposed model.  The table highlights the relative fluency of different large language models when generating Thai text, with the authors' model showing similar fluency to ChatGPT.", "section": "4.2.1 Human Evaluation Results"}, {"figure_path": "YZWvf58dBS/tables/tables_11_1.jpg", "caption": "Table 10: Model details.", "description": "This table provides a summary of the different large language models (LLMs) used in the experiments, including their base architecture, pre-training data, fine-tuning data, and model size.  The table helps to clarify the variations in model configurations, allowing for a better understanding of the comparison between the proposed TransLLM model and existing state-of-the-art models.", "section": "A Experiment Details"}, {"figure_path": "YZWvf58dBS/tables/tables_13_1.jpg", "caption": "Table 12: Rating criterion.", "description": "This table shows the rating criteria used for evaluating the helpfulness of the model's responses in human evaluation.  The criteria are broken down into five levels (Very Poor to Excellent), each with a description of the characteristics of responses at that level. The criteria cover adherence to instructions, expression fluency, and style.", "section": "A.4.1 Human Evaluation"}, {"figure_path": "YZWvf58dBS/tables/tables_14_1.jpg", "caption": "Table 1: Comparison between our model and strong LLMs on MT-bench under human evaluation. We provide the 95% confidence interval in brackets.", "description": "This table presents the results of a human evaluation comparing the performance of the proposed TransLLM model against ChatGPT and GPT-4 on the MT-bench benchmark.  The evaluation focuses on multi-turn conversations and provides win, tie, and loss percentages for both the first and second turns of the conversation.  Confidence intervals are included to show the statistical significance of the results.", "section": "4.2.1 Human Evaluation Results"}, {"figure_path": "YZWvf58dBS/tables/tables_14_2.jpg", "caption": "Table 1: Comparison between our model and strong LLMs on MT-bench under human evaluation. We provide the 95% confidence interval in brackets.", "description": "This table presents the results of a human evaluation comparing the performance of the authors' model against ChatGPT and GPT-4 on the MT-bench benchmark.  The evaluation assesses performance across two turns of conversation.  The win rate, tie rate, and loss rate are shown for each model, along with 95% confidence intervals, indicating the statistical significance of the differences between models.", "section": "4.2.1 Human Evaluation Results"}, {"figure_path": "YZWvf58dBS/tables/tables_14_3.jpg", "caption": "Table 1: Comparison between our model and strong LLMs on MT-bench under human evaluation. We provide the 95% confidence interval in brackets.", "description": "This table presents the results of a human evaluation comparing the performance of the proposed TransLLM model against strong Large Language Models (LLMs) such as ChatGPT and GPT-4 on the MT-bench benchmark.  The evaluation focuses on multi-turn conversations in Thai. The table shows the win, tie, and loss rates for each model across both the first and second turns of the conversation, providing a clear comparison of their relative performance.", "section": "4.2.1 Human Evaluation Results"}, {"figure_path": "YZWvf58dBS/tables/tables_15_1.jpg", "caption": "Table 16: GPT-4 evaluation scores on Alpaca-Eval for different models.", "description": "This table presents the results of evaluating different language models on the Alpaca-Eval benchmark using GPT-4 as the evaluator.  The models are compared across five different subsets of the benchmark (Helpful-Base, Koala, Oasst, Self-Instruct, Vicuna) and an overall average score. The scores likely reflect the models' abilities to follow instructions and generate helpful and safe responses, in a way that is comparable to human outputs.", "section": "4.2.2 GPT-4 Evaluation Results"}, {"figure_path": "YZWvf58dBS/tables/tables_16_1.jpg", "caption": "Table 1: Comparison between our model and strong LLMs on MT-bench under human evaluation. We provide the 95% confidence interval in brackets.", "description": "This table presents the results of a human evaluation comparing the performance of the proposed TransLLM model against ChatGPT and GPT-4 on the MT-bench benchmark.  The evaluation focuses on multi-turn conversations, and the results show the percentage of wins, ties, and losses for each model in both the first and second turns of a conversation. Confidence intervals are provided to indicate the uncertainty in the results.", "section": "4.2.1 Human Evaluation Results"}, {"figure_path": "YZWvf58dBS/tables/tables_16_2.jpg", "caption": "Table 1: Comparison between our model and strong LLMs on MT-bench under human evaluation. We provide the 95% confidence interval in brackets.", "description": "This table presents the results of a human evaluation comparing the performance of the proposed TransLLM model against ChatGPT and GPT-4 on the MT-bench benchmark.  The evaluation focuses on multi-turn conversations and provides win/tie/loss percentages for both the first and second turns of the conversation.  Confidence intervals are included to indicate the statistical reliability of the results.", "section": "4.2 Human Evaluation Results"}, {"figure_path": "YZWvf58dBS/tables/tables_16_3.jpg", "caption": "Table 1: Comparison between our model and strong LLMs on MT-bench under human evaluation. We provide the 95% confidence interval in brackets.", "description": "This table presents the results of a human evaluation comparing the performance of the proposed TransLLM model against strong LLMs like ChatGPT and GPT-4 on the MT-bench benchmark.  The evaluation focuses on multi-turn conversations and provides win, tie, and loss rates for both the first and second turns of the conversation.  Confidence intervals are included to indicate the uncertainty in the results.", "section": "4.2.1 Human Evaluation Results"}]