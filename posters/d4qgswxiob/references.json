{"references": [{"fullname_first_author": "Alethea Power", "paper_title": "Grokking: Generalization beyond overfitting on small algorithmic datasets", "publication_date": "2022-01-27", "reason": "This paper introduces the concept of \"grokking,\" a phenomenon central to the current paper's investigation of implicit reasoning in transformers."}, {"fullname_first_author": "Zeyuan Allen-Zhu", "paper_title": "Physics of language models: Part 3.2, knowledge manipulation", "publication_date": "2023-09-14", "reason": "This paper provides a theoretical framework for understanding the limitations of large language models in knowledge manipulation, a key concept discussed in the current work."}, {"fullname_first_author": "Mor Geva", "paper_title": "Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space", "publication_date": "2022-12-01", "reason": "This study helps interpret the inner workings of transformers, providing a foundation for the mechanistic analysis conducted in the current paper."}, {"fullname_first_author": "Jason Wei", "paper_title": "Chain of thought prompting elicits reasoning in large language models", "publication_date": "2022-12-01", "reason": "This research explores prompting techniques for eliciting reasoning in LLMs, providing a useful background for the study of implicit reasoning in transformers."}, {"fullname_first_author": "Mostafa Dehghani", "paper_title": "Universal transformers", "publication_date": "2019-01-01", "reason": "This paper introduces the Universal Transformer architecture, which is relevant to the current paper's investigation of transformer architectures and their capabilities in implicit reasoning."}]}