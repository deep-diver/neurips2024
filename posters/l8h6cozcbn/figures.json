[{"figure_path": "L8h6cozcbn/figures/figures_2_1.jpg", "caption": "Figure 1: Illustration of how Transformers are trained to do in-context linear regression.", "description": "This figure illustrates the in-context learning process for linear regression using Transformers.  The input consists of a sequence of in-context examples, where each example is a pair of input features (x) and its corresponding output label (y).  These examples are presented to the transformer model, which then learns to make predictions on new unseen input data (xt+1) without explicit parameter updates.  The figure shows how the Transformer processes the input examples and generates a prediction for the new input data, effectively performing in-context learning.", "section": "Problem Setup"}, {"figure_path": "L8h6cozcbn/figures/figures_4_1.jpg", "caption": "Figure 2: Convergence of Algorithms. Similar to Iterative Newton and GD, Transformer's performance improve over the layer index l. When n > d, the Transformer model, from layers 3 to 8, demonstrates a superlinear convergence rate, similar to Iterative Newton, while GD, with fixed step size, is sublinear. Later layers of Transformers show a slower convergence rate, and we hypothesize they have little incentive to implement the algorithm precisely since the error is already very small. A 24-layer Transformer model exhibits the same superlinear convergence (Figure 25 in \u00a7A.4.2).", "description": "The figure compares the convergence rates of three algorithms: Transformers, Iterative Newton's Method, and Gradient Descent.  It shows that Transformers and Iterative Newton's Method exhibit superlinear convergence (meaning that the error decreases faster than linearly with each iteration or layer), while Gradient Descent shows sublinear convergence. The convergence rate of Transformers is similar to that of Iterative Newton's Method, significantly faster than that of Gradient Descent, especially in the middle layers.  Later Transformer layers show a slower convergence, potentially due to the error already being very small, and thus they have little incentive to continue precise optimization.", "section": "3.3 Measuring Algorithmic Similarity"}, {"figure_path": "L8h6cozcbn/figures/figures_6_1.jpg", "caption": "Figure 2: Convergence of Algorithms. Similar to Iterative Newton and GD, Transformer's performance improve over the layer index l. When n > d, the Transformer model, from layers 3 to 8, demonstrates a superlinear convergence rate, similar to Iterative Newton, while GD, with fixed step size, is sublinear. Later layers of Transformers show a slower convergence rate, and we hypothesize they have little incentive to implement the algorithm precisely since the error is already very small. A 24-layer Transformer model exhibits the same superlinear convergence (Figure 25 in \u00a7A.4.2).", "description": "The figure compares the convergence rates of three algorithms: Transformers, Iterative Newton's Method, and Gradient Descent, for in-context linear regression.  It shows how the error decreases as the number of layers (Transformers), iterations (Iterative Newton), or steps (Gradient Descent) increases.  The results demonstrate that Transformers achieve a superlinear convergence rate similar to Iterative Newton's Method, which is significantly faster than the sublinear convergence rate of Gradient Descent, especially in the middle layers.  Later Transformer layers show slower convergence because the error is already very small.", "section": "3.3 Measuring Algorithmic Similarity"}, {"figure_path": "L8h6cozcbn/figures/figures_7_1.jpg", "caption": "Figure 4: Transformers performance on ill-conditioned data. Given 40 in-context examples, Transformers and Iterative Newton converge similarly and they both can converge to the OLS solution quickly whereas GD suffers.", "description": "This figure compares the convergence performance of Transformers, Iterative Newton's method, and Gradient Descent on ill-conditioned data for linear regression.  The x-axis represents the number of steps/layers, and the y-axis shows the error.  It demonstrates that Transformers and Iterative Newton converge much faster (superlinearly) to the optimal solution (Ordinary Least Squares) than Gradient Descent (sublinearly), even when dealing with ill-conditioned data where Gradient Descent struggles.", "section": "4.3 Transformers perform well on ill-conditioned data"}, {"figure_path": "L8h6cozcbn/figures/figures_7_2.jpg", "caption": "Figure 2: Convergence of Algorithms. Similar to Iterative Newton and GD, Transformer's performance improve over the layer index l. When n > d, the Transformer model, from layers 3 to 8, demonstrates a superlinear convergence rate, similar to Iterative Newton, while GD, with fixed step size, is sublinear. Later layers of Transformers show a slower convergence rate, and we hypothesize they have little incentive to implement the algorithm precisely since the error is already very small. A 24-layer Transformer model exhibits the same superlinear convergence (Figure 25 in \u00a7A.4.2).", "description": "This figure compares the convergence rates of three algorithms: Transformers, Iterative Newton's Method, and Gradient Descent, for in-context linear regression.  The x-axis represents the number of layers (Transformers), steps (Iterative Newton's Method), or steps (Gradient Descent), and the y-axis represents the mean squared error.  The figure demonstrates that Transformers and Iterative Newton's Method achieve a superlinear convergence rate that's significantly faster than Gradient Descent, particularly when the number of in-context examples exceeds the data dimensionality (n > d). The plots also show that the predictions of successive Transformer layers closely match Iterative Newton's method at different iteration steps, indicating that Transformer layers efficiently approximate steps of the second-order optimization method. For comparison, later transformer layers show a slight decrease in convergence speed as the error nears zero.  A 24-layer model shows similar behaviour as described in Appendix A.4.2.", "section": "3.3 Measuring Algorithmic Similarity"}, {"figure_path": "L8h6cozcbn/figures/figures_8_1.jpg", "caption": "Figure 6: Ablation on Transformer's Hidden Size. For linear regression problems with d = 20, Transformers need O(d) hidden dimension to mimic OLS solutions.", "description": "This figure shows the ablation study on the hidden size of the Transformer model for linear regression problems with dimension d=20.  The results indicate that Transformers need a hidden dimension of O(d) to accurately approximate the ordinary least squares (OLS) solutions.  Smaller hidden sizes lead to significantly higher errors.", "section": "4.4 Transformers Require O(d) Hidden Dimension"}, {"figure_path": "L8h6cozcbn/figures/figures_16_1.jpg", "caption": "Figure 2: Convergence of Algorithms. Similar to Iterative Newton and GD, Transformer's performance improve over the layer index l. When n > d, the Transformer model, from layers 3 to 8, demonstrates a superlinear convergence rate, similar to Iterative Newton, while GD, with fixed step size, is sublinear. Later layers of Transformers show a slower convergence rate, and we hypothesize they have little incentive to implement the algorithm precisely since the error is already very small. A 24-layer Transformer model exhibits the same superlinear convergence (Figure 25 in \u00a7A.4.2).", "description": "This figure compares the convergence rates of three algorithms: Transformers, Iterative Newton's Method, and Gradient Descent.  It shows that for in-context linear regression tasks where the number of data points exceeds the data dimension, Transformers exhibit superlinear convergence, similar to Iterative Newton's Method, and significantly faster than Gradient Descent's sublinear convergence.  The convergence speed of Transformers slows in later layers, likely because there is less incentive to compute the algorithm precisely when the error is already small.", "section": "3.3 Measuring Algorithmic Similarity"}, {"figure_path": "L8h6cozcbn/figures/figures_17_1.jpg", "caption": "Figure 2: Convergence of Algorithms. Similar to Iterative Newton and GD, Transformer's performance improve over the layer index l. When n > d, the Transformer model, from layers 3 to 8, demonstrates a superlinear convergence rate, similar to Iterative Newton, while GD, with fixed step size, is sublinear. Later layers of Transformers show a slower convergence rate, and we hypothesize they have little incentive to implement the algorithm precisely since the error is already very small. A 24-layer Transformer model exhibits the same superlinear convergence (Figure 25 in \u00a7A.4.2).", "description": "This figure compares the convergence rates of three different algorithms for linear regression: Transformers, Iterative Newton's method, and Gradient Descent.  It shows that for a sufficiently large number of in-context examples (n > d), the Transformer's error decreases superlinearly with the layer index, similar to Iterative Newton's method. Gradient descent, on the other hand, shows sublinear convergence.  The figure also suggests that the later layers of the transformer exhibit slower convergence because the error is already very small, thus reducing the incentive to precisely implement the algorithm.", "section": "3.3 Measuring Algorithmic Similarity"}, {"figure_path": "L8h6cozcbn/figures/figures_17_2.jpg", "caption": "Figure 2: Convergence of Algorithms. Similar to Iterative Newton and GD, Transformer's performance improve over the layer index l. When n > d, the Transformer model, from layers 3 to 8, demonstrates a superlinear convergence rate, similar to Iterative Newton, while GD, with fixed step size, is sublinear. Later layers of Transformers show a slower convergence rate, and we hypothesize they have little incentive to implement the algorithm precisely since the error is already very small. A 24-layer Transformer model exhibits the same superlinear convergence (Figure 25 in \u00a7A.4.2).", "description": "The figure compares the convergence rates of three algorithms (Transformers, Iterative Newton's Method, and Gradient Descent) for in-context linear regression.  It shows that Transformers exhibit a superlinear convergence rate similar to Iterative Newton's Method, significantly faster than the sublinear rate of Gradient Descent, especially in the middle layers (3-8).  Later Transformer layers show slower convergence, likely because the error is already small.", "section": "3.3 Measuring Algorithmic Similarity"}, {"figure_path": "L8h6cozcbn/figures/figures_18_1.jpg", "caption": "Figure 2: Convergence of Algorithms. Similar to Iterative Newton and GD, Transformer's performance improve over the layer index l. When n > d, the Transformer model, from layers 3 to 8, demonstrates a superlinear convergence rate, similar to Iterative Newton, while GD, with fixed step size, is sublinear. Later layers of Transformers show a slower convergence rate, and we hypothesize they have little incentive to implement the algorithm precisely since the error is already very small. A 24-layer Transformer model exhibits the same superlinear convergence (Figure 25 in \u00a7A.4.2).", "description": "This figure compares the convergence rates of three algorithms: Transformers, Iterative Newton's Method, and Gradient Descent.  It shows that for linear regression problems, as the number of layers in a Transformer increases, its prediction error decreases superlinearly, similarly to Iterative Newton's Method.  Gradient Descent, on the other hand, exhibits a much slower sublinear convergence rate. This suggests that Transformers internally approximate second-order optimization methods.", "section": "3.3 Measuring Algorithmic Similarity"}, {"figure_path": "L8h6cozcbn/figures/figures_20_1.jpg", "caption": "Figure 2: Convergence of Algorithms. Similar to Iterative Newton and GD, Transformer's performance improve over the layer index l. When n > d, the Transformer model, from layers 3 to 8, demonstrates a superlinear convergence rate, similar to Iterative Newton, while GD, with fixed step size, is sublinear. Later layers of Transformers show a slower convergence rate, and we hypothesize they have little incentive to implement the algorithm precisely since the error is already very small. A 24-layer Transformer model exhibits the same superlinear convergence (Figure 25 in \u00a7A.4.2).", "description": "The figure compares the convergence rates of three algorithms: Transformers, Iterative Newton's Method, and Gradient Descent.  It shows that Transformers exhibit a superlinear convergence rate similar to Iterative Newton's Method, significantly outperforming Gradient Descent, especially in the middle layers.  Later Transformer layers show slower convergence, likely because the error is already small.", "section": "3.3 Measuring Algorithmic Similarity"}, {"figure_path": "L8h6cozcbn/figures/figures_20_2.jpg", "caption": "Figure 2: Convergence of Algorithms. Similar to Iterative Newton and GD, Transformer's performance improve over the layer index l. When n > d, the Transformer model, from layers 3 to 8, demonstrates a superlinear convergence rate, similar to Iterative Newton, while GD, with fixed step size, is sublinear. Later layers of Transformers show a slower convergence rate, and we hypothesize they have little incentive to implement the algorithm precisely since the error is already very small. A 24-layer Transformer model exhibits the same superlinear convergence (Figure 25 in \u00a7A.4.2).", "description": "The figure shows the convergence speed comparison of three algorithms: Transformer, Iterative Newton's Method, and Gradient Descent.  It plots the error against the number of layers (Transformer), steps (Iterative Newton's Method), and steps (Gradient Descent).  The results demonstrate that the Transformer's convergence rate is superlinear and similar to Iterative Newton's Method, significantly faster than Gradient Descent's sublinear rate, especially in the initial layers.  This suggests that Transformers learn second-order optimization strategies.", "section": "3.3 Measuring Algorithmic Similarity"}, {"figure_path": "L8h6cozcbn/figures/figures_21_1.jpg", "caption": "Figure 2: Convergence of Algorithms. Similar to Iterative Newton and GD, Transformer's performance improve over the layer index l. When n > d, the Transformer model, from layers 3 to 8, demonstrates a superlinear convergence rate, similar to Iterative Newton, while GD, with fixed step size, is sublinear. Later layers of Transformers show a slower convergence rate, and we hypothesize they have little incentive to implement the algorithm precisely since the error is already very small. A 24-layer Transformer model exhibits the same superlinear convergence (Figure 25 in \u00a7A.4.2).", "description": "This figure compares the convergence rates of three algorithms: Transformers, Iterative Newton's Method, and Gradient Descent.  It shows that as the number of layers increases in the Transformer model (and the number of steps in Iterative Newton's method), the error decreases superlinearly. Conversely, Gradient Descent shows a much slower, sublinear convergence.  The results suggest that Transformers learn to approximate a second-order optimization method, similar to Iterative Newton's Method, rather than a first-order method like Gradient Descent.", "section": "3.3 Measuring Algorithmic Similarity"}, {"figure_path": "L8h6cozcbn/figures/figures_21_2.jpg", "caption": "Figure 14: Similarity of induced weights over varying number of in-context examples, on three layer indices of Transformers, indexed as 2, 3 and 12. We find that initially at layer 2, the Transformers model hasn't learned so it has zero similarity to all candidate algorithms. As we progress to the next layer number 3, we find that Transformers start to learn, and when provided few examples, Transformers are more similar to OLS but soon become most similar to the Iterative Newton's Method. Layer 12 shows that Transformers in the later layers converge to the OLS solution when provided more than 1 example. We also find there is a dip around n = d for similarity between Transformers and OLS but not for Transformers and Newton, and this is probably because OLS has a more prominent double-descent phenomenon than Transformers and Newton.", "description": "This figure shows the cosine similarity between the induced weight vectors of Transformers and three other algorithms (Iterative Newton's Method, Gradient Descent, and Ordinary Least Squares) across different numbers of in-context examples.  It demonstrates how the similarity of Transformers' weights changes as it processes more data, illustrating its transition from resembling OLS to Iterative Newton's Method in later layers, which is consistent with a second-order optimization approach.  The dip around n=d (data dimension) for OLS similarity highlights a double descent phenomenon that is less prominent in Transformers and Iterative Newton.", "section": "A.2.5 Additional Results on Similarity of Induced Weights"}, {"figure_path": "L8h6cozcbn/figures/figures_22_1.jpg", "caption": "Figure 2: Convergence of Algorithms. Similar to Iterative Newton and GD, Transformer's performance improve over the layer index l. When n > d, the Transformer model, from layers 3 to 8, demonstrates a superlinear convergence rate, similar to Iterative Newton, while GD, with fixed step size, is sublinear. Later layers of Transformers show a slower convergence rate, and we hypothesize they have little incentive to implement the algorithm precisely since the error is already very small. A 24-layer Transformer model exhibits the same superlinear convergence (Figure 25 in \u00a7A.4.2).", "description": "This figure compares the convergence rates of three different algorithms: Transformers, Iterative Newton's Method, and Gradient Descent.  It shows that as the number of layers increases in a Transformer model (when the number of data points exceeds the number of features), its performance improves at a superlinear rate, similar to Iterative Newton's Method which is also a second-order optimization method, but much faster than Gradient Descent (a first-order optimization method).  The convergence rate of Gradient Descent is sublinear; as the number of steps increase, the improvement becomes gradually smaller.  The Transformer's convergence slows in later layers because the error is already very small, diminishing the incentive for the model to precisely implement the optimization algorithm.", "section": "3.3 Measuring Algorithmic Similarity"}, {"figure_path": "L8h6cozcbn/figures/figures_22_2.jpg", "caption": "Figure 2: Convergence of Algorithms. Similar to Iterative Newton and GD, Transformer's performance improve over the layer index l. When n > d, the Transformer model, from layers 3 to 8, demonstrates a superlinear convergence rate, similar to Iterative Newton, while GD, with fixed step size, is sublinear. Later layers of Transformers show a slower convergence rate, and we hypothesize they have little incentive to implement the algorithm precisely since the error is already very small. A 24-layer Transformer model exhibits the same superlinear convergence (Figure 25 in \u00a7A.4.2).", "description": "The figure compares the convergence speed of three algorithms: Transformers, Iterative Newton's Method, and Gradient Descent, for in-context linear regression tasks.  It shows that Transformers and Iterative Newton's Method exhibit similar superlinear convergence rates, significantly faster than Gradient Descent's sublinear convergence. The plots illustrate how the error decreases as the number of layers (Transformers), steps (Iterative Newton), or steps (Gradient Descent) increases.  The results suggest that Transformers learn an optimization strategy closer to second-order methods than to first-order methods.", "section": "3.3 Measuring Algorithmic Similarity"}, {"figure_path": "L8h6cozcbn/figures/figures_23_1.jpg", "caption": "Figure 2: Convergence of Algorithms. Similar to Iterative Newton and GD, Transformer's performance improve over the layer index l. When n > d, the Transformer model, from layers 3 to 8, demonstrates a superlinear convergence rate, similar to Iterative Newton, while GD, with fixed step size, is sublinear. Later layers of Transformers show a slower convergence rate, and we hypothesize they have little incentive to implement the algorithm precisely since the error is already very small. A 24-layer Transformer model exhibits the same superlinear convergence (Figure 25 in \u00a7A.4.2).", "description": "This figure compares the convergence rates of three algorithms: Transformers, Iterative Newton's Method, and Gradient Descent.  It shows that the error decreases as the number of layers (for Transformers), iterations (for Iterative Newton's Method), and steps (for Gradient Descent) increases.  Importantly, it demonstrates that Transformers exhibit a superlinear convergence rate similar to Iterative Newton's Method for in-context linear regression problems where the number of examples (n) exceeds the dimensionality of the data (d), whereas Gradient Descent converges sublinearly.  The plot suggests that Transformers internally approximate a second-order optimization method, unlike the first-order Gradient Descent.", "section": "3.3 Measuring Algorithmic Similarity"}, {"figure_path": "L8h6cozcbn/figures/figures_23_2.jpg", "caption": "Figure 2: Convergence of Algorithms. Similar to Iterative Newton and GD, Transformer's performance improve over the layer index l. When n > d, the Transformer model, from layers 3 to 8, demonstrates a superlinear convergence rate, similar to Iterative Newton, while GD, with fixed step size, is sublinear. Later layers of Transformers show a slower convergence rate, and we hypothesize they have little incentive to implement the algorithm precisely since the error is already very small. A 24-layer Transformer model exhibits the same superlinear convergence (Figure 25 in \u00a7A.4.2).", "description": "This figure compares the convergence rate of three different algorithms: Transformers, Iterative Newton's Method, and Gradient Descent.  It shows how the error decreases as the number of layers (for Transformers), iterations (for Iterative Newton), and steps (for Gradient Descent) increases.  The results demonstrate that Transformers converge at a similar rate to Iterative Newton's Method, which is significantly faster than Gradient Descent, especially when the number of data points exceeds the data dimension (n>d).  The superlinear convergence of Transformers and Iterative Newton's method is highlighted, while Gradient Descent's convergence is shown to be sublinear.  The later layers of the transformer show a slower convergence because the error is already small.", "section": "3.3 Measuring Algorithmic Similarity"}, {"figure_path": "L8h6cozcbn/figures/figures_24_1.jpg", "caption": "Figure 2: Convergence of Algorithms. Similar to Iterative Newton and GD, Transformer's performance improve over the layer index l. When n > d, the Transformer model, from layers 3 to 8, demonstrates a superlinear convergence rate, similar to Iterative Newton, while GD, with fixed step size, is sublinear. Later layers of Transformers show a slower convergence rate, and we hypothesize they have little incentive to implement the algorithm precisely since the error is already very small. A 24-layer Transformer model exhibits the same superlinear convergence (Figure 25 in \u00a7A.4.2).", "description": "The figure compares the convergence rates of three algorithms: Transformers, Iterative Newton's Method, and Gradient Descent, for in-context linear regression.  It shows that Transformers and Iterative Newton's method exhibit a superlinear convergence rate (significantly faster than linear), while Gradient Descent converges sublinearly. The Transformer's convergence rate is similar to Iterative Newton's, particularly in the middle layers.  Later layers of the Transformer show slower convergence, likely because the error is already small and there is less incentive for precise algorithm implementation.", "section": "3.3 Measuring Algorithmic Similarity"}, {"figure_path": "L8h6cozcbn/figures/figures_25_1.jpg", "caption": "Figure 2: Convergence of Algorithms. Similar to Iterative Newton and GD, Transformer's performance improve over the layer index l. When n > d, the Transformer model, from layers 3 to 8, demonstrates a superlinear convergence rate, similar to Iterative Newton, while GD, with fixed step size, is sublinear. Later layers of Transformers show a slower convergence rate, and we hypothesize they have little incentive to implement the algorithm precisely since the error is already very small. A 24-layer Transformer model exhibits the same superlinear convergence (Figure 25 in \u00a7A.4.2).", "description": "This figure compares the convergence rates of three algorithms (Transformers, Iterative Newton's Method, and Gradient Descent) for in-context linear regression.  It shows that Transformers and Iterative Newton's Method exhibit similar superlinear convergence, significantly faster than the sublinear convergence of Gradient Descent. The plots demonstrate that performance improves progressively as the number of layers (Transformers), steps (Iterative Newton), or steps (Gradient Descent) increase.", "section": "3.3 Measuring Algorithmic Similarity"}, {"figure_path": "L8h6cozcbn/figures/figures_25_2.jpg", "caption": "Figure 2: Convergence of Algorithms. Similar to Iterative Newton and GD, Transformer's performance improve over the layer index l. When n > d, the Transformer model, from layers 3 to 8, demonstrates a superlinear convergence rate, similar to Iterative Newton, while GD, with fixed step size, is sublinear. Later layers of Transformers show a slower convergence rate, and we hypothesize they have little incentive to implement the algorithm precisely since the error is already very small. A 24-layer Transformer model exhibits the same superlinear convergence (Figure 25 in \u00a7A.4.2).", "description": "This figure compares the convergence rates of three algorithms: Transformers, Iterative Newton's Method, and Gradient Descent, on the task of in-context linear regression.  The x-axis represents the number of layers for Transformers and the number of steps for Iterative Newton and Gradient Descent. The y-axis shows the error rate.  The figure shows that Transformers and Iterative Newton's Method exhibit similar, superlinear convergence rates (meaning the error decreases faster than linearly), while Gradient Descent demonstrates a slower, sublinear convergence rate.  The superlinear convergence of Transformers is particularly evident in the middle layers (3-8).  Later layers show reduced convergence as the error becomes very small.", "section": "3.3 Measuring Algorithmic Similarity"}, {"figure_path": "L8h6cozcbn/figures/figures_26_1.jpg", "caption": "Figure 2: Convergence of Algorithms. Similar to Iterative Newton and GD, Transformer's performance improve over the layer index l. When n > d, the Transformer model, from layers 3 to 8, demonstrates a superlinear convergence rate, similar to Iterative Newton, while GD, with fixed step size, is sublinear. Later layers of Transformers show a slower convergence rate, and we hypothesize they have little incentive to implement the algorithm precisely since the error is already very small. A 24-layer Transformer model exhibits the same superlinear convergence (Figure 25 in \u00a7A.4.2).", "description": "This figure compares the convergence rates of three algorithms: Transformers, Iterative Newton's Method, and Gradient Descent, for in-context linear regression.  The x-axis represents the number of layers (Transformers), steps (Iterative Newton), or steps (Gradient Descent), while the y-axis shows the error. The figure demonstrates that Transformers and Iterative Newton's Method exhibit a similar superlinear convergence rate, significantly faster than the sublinear convergence of Gradient Descent. This supports the paper's claim that Transformers learn to approximate second-order optimization methods.", "section": "3.3 Measuring Algorithmic Similarity"}, {"figure_path": "L8h6cozcbn/figures/figures_26_2.jpg", "caption": "Figure 2: Convergence of Algorithms. Similar to Iterative Newton and GD, Transformer's performance improve over the layer index l. When n > d, the Transformer model, from layers 3 to 8, demonstrates a superlinear convergence rate, similar to Iterative Newton, while GD, with fixed step size, is sublinear. Later layers of Transformers show a slower convergence rate, and we hypothesize they have little incentive to implement the algorithm precisely since the error is already very small. A 24-layer Transformer model exhibits the same superlinear convergence (Figure 25 in \u00a7A.4.2).", "description": "This figure compares the convergence rate of three algorithms: Transformers, Iterative Newton's Method, and Gradient Descent.  It shows that the error decreases as the number of layers (for Transformers), steps (for Iterative Newton's Method), and steps (for Gradient Descent) increases.  The key finding is that Transformers exhibit a superlinear convergence rate, similar to Iterative Newton's Method, which is significantly faster than the sublinear rate of Gradient Descent, especially when the number of data points exceeds the data dimension.", "section": "3.3 Measuring Algorithmic Similarity"}, {"figure_path": "L8h6cozcbn/figures/figures_27_1.jpg", "caption": "Figure 3: Heatmaps of Similarity. The best matching steps are highlighted in yellow. Transformers layers show a linear trend with Iterative Newton steps but an exponential trend with GD. This suggests Transformers and Iterative Newton have the same convergence rate that is exponentially faster than GD. See Figure 10 for an additional heatmap where GD's steps are shown in log scale: on that plot there is a linear correspondence between Transformers and GD's steps. This further strengthens the claim that Transformers have an exponentially faster rate of convergence than GD.", "description": "This figure presents heatmaps visualizing the similarity between Transformers and two optimization algorithms: Iterative Newton's Method and Gradient Descent.  The yellow highlights indicate the best-matching steps between the algorithms. The heatmaps show that Transformers' performance improves linearly with the number of Iterative Newton steps and exponentially with the number of Gradient Descent steps, which indicates that Transformers learn a second-order optimization method similar to Iterative Newton's Method and significantly faster than Gradient Descent.", "section": "3.3 Measuring Algorithmic Similarity"}, {"figure_path": "L8h6cozcbn/figures/figures_27_2.jpg", "caption": "Figure 2: Convergence of Algorithms. Similar to Iterative Newton and GD, Transformer's performance improve over the layer index l. When n > d, the Transformer model, from layers 3 to 8, demonstrates a superlinear convergence rate, similar to Iterative Newton, while GD, with fixed step size, is sublinear. Later layers of Transformers show a slower convergence rate, and we hypothesize they have little incentive to implement the algorithm precisely since the error is already very small. A 24-layer Transformer model exhibits the same superlinear convergence (Figure 25 in \u00a7A.4.2).", "description": "This figure compares the convergence rates of three algorithms: Transformers, Iterative Newton's Method, and Gradient Descent.  It shows that the Transformer's error decreases superlinearly with the number of layers, similar to Iterative Newton's Method, but much faster than Gradient Descent.  The plot suggests that Transformers may internally perform an optimization algorithm similar to a second-order method rather than gradient descent.", "section": "4 Experimental Evidence"}, {"figure_path": "L8h6cozcbn/figures/figures_28_1.jpg", "caption": "Figure 2: Convergence of Algorithms. Similar to Iterative Newton and GD, Transformer's performance improve over the layer index l. When n > d, the Transformer model, from layers 3 to 8, demonstrates a superlinear convergence rate, similar to Iterative Newton, while GD, with fixed step size, is sublinear. Later layers of Transformers show a slower convergence rate, and we hypothesize they have little incentive to implement the algorithm precisely since the error is already very small. A 24-layer Transformer model exhibits the same superlinear convergence (Figure 25 in \u00a7A.4.2).", "description": "The figure compares the convergence rates of three algorithms: Transformers, Iterative Newton's Method, and Gradient Descent, for in-context linear regression.  It shows that Transformers' performance improves superlinearly with the number of layers, similar to Iterative Newton's Method, and significantly faster than Gradient Descent.  The plots demonstrate how the predictions of successive Transformer layers closely approximate those of Iterative Newton's Method after a corresponding number of iterations, supporting the claim that Transformers learn to approximate second-order optimization methods.", "section": "3.3 Measuring Algorithmic Similarity"}, {"figure_path": "L8h6cozcbn/figures/figures_35_1.jpg", "caption": "Figure 2: Convergence of Algorithms. Similar to Iterative Newton and GD, Transformer's performance improve over the layer index l. When n > d, the Transformer model, from layers 3 to 8, demonstrates a superlinear convergence rate, similar to Iterative Newton, while GD, with fixed step size, is sublinear. Later layers of Transformers show a slower convergence rate, and we hypothesize they have little incentive to implement the algorithm precisely since the error is already very small. A 24-layer Transformer model exhibits the same superlinear convergence (Figure 25 in \u00a7A.4.2).", "description": "This figure compares the convergence rates of three different algorithms for linear regression: Transformers, Iterative Newton's method, and Gradient Descent.  It shows that as the number of layers increases in the Transformer model, the error decreases superlinearly (much faster than a linear rate), similar to Iterative Newton's method.  Gradient Descent, in contrast, shows a sublinear convergence rate. The results suggest that Transformers learn to approximate second-order optimization methods.", "section": "3.3 Measuring Algorithmic Similarity"}]