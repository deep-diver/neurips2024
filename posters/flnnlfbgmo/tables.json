[{"figure_path": "FLNnlfBGMo/tables/tables_3_1.jpg", "caption": "Table 1: Prompt Optimization and MAB.", "description": "This table draws a parallel between the prompt optimization problem and the multi-armed bandit (MAB) problem, specifically focusing on the fixed-budget best arm identification (BAI-FB) setting.  It maps concepts from prompt optimization, such as the pool of prompts, interacting with the LLM using a prompt, the score function, randomness in inputs and outputs, and the overall performance of a prompt, to their corresponding equivalents within the MAB framework.  The final row emphasizes that the prompt optimization problem under a limited budget aligns directly with the BAI-FB objective of identifying the best arm efficiently under a fixed budget.", "section": "3 Connecting Prompt Optimization with Best Arm Identification"}, {"figure_path": "FLNnlfBGMo/tables/tables_5_1.jpg", "caption": "Table 2: Averaged performance ranks of baselines and TRIPLE on the selected tasks using GPT-3.5, which are computed separately for methods using embeddings or not. The rank of BO is computed with the highest performance from BO-EI and BO-PI. The highest ranked methods are marked bold.", "description": "This table presents a comparison of the average performance ranks of different prompt selection methods (baselines and TRIPLE variants) across various tasks using the GPT-3.5 language model.  The results are divided into two groups: methods that do and do not use prompt embeddings.  Within each group, the average rank (lower is better) and standard deviation are shown for each method. The table highlights the superior performance of TRIPLE methods compared to the baselines.", "section": "5.1 Evaluating TRIPLE with Fixed Prompt Pools"}, {"figure_path": "FLNnlfBGMo/tables/tables_7_1.jpg", "caption": "Table 2: Averaged performance ranks of baselines and TRIPLE on the selected tasks using GPT-3.5, which are computed separately for methods using embeddings or not. The rank of BO is computed with the highest performance from BO-EI and BO-PI. The highest ranked methods are marked bold.", "description": "This table presents the average performance ranks of different prompt selection methods (Uniform, UCB, SH, CR, BO, NeuralUCB, CLST, GSE) across multiple tasks, using GPT-3.5.  The results are separated into two groups: methods without prompt embeddings and methods with prompt embeddings.  The rank is based on the average performance across the tasks, with lower ranks indicating better performance.  The highest-performing methods in each category are bolded.", "section": "5.1 Evaluating TRIPLE with Fixed Prompt Pools"}, {"figure_path": "FLNnlfBGMo/tables/tables_8_1.jpg", "caption": "Table 3: Performances of integrating TRIPLE in the end-to-end pipelines using GPT-3.5. The baseline methods reported in the original implementations are labeled as (b). For each task, the best score across two pipelines is marked as red, and the best score in the remaining pipeline is highlighted as yellow. TRIPLE-CR are selected over TRIPLE-SH due to its better performance observed in the previous experiments. TRILE-CLST is ignored in the tests with APO, as it is ineffective to cluster only 10 prompts.", "description": "This table shows the performance of integrating TRIPLE into two end-to-end prompt optimization pipelines: APE and APO.  It compares TRIPLE's performance against baseline methods (Uniform and UCB for APE, and UCB for APO) across twelve tasks. The best performing method for each task in each pipeline is highlighted.  The results demonstrate that TRIPLE consistently improves performance across various tasks.", "section": "5.2 Integrating TRIPLE into End-to-End Pipelines"}, {"figure_path": "FLNnlfBGMo/tables/tables_9_1.jpg", "caption": "Table 4: Performance comparisons of various example selection methods on different tasks using GPT-3.5 with |G| = 50 candidate examples, budget N = 100, and length M = 4. The tasks are numbered according to Table 3. For each task, the best score across is marked as red, and the second best as yellow.", "description": "This table presents the performance comparison of four different example selection methods across twelve tasks using the GPT-3.5 language model.  The methods are Random, Uniform, SAR, and CSAR.  Each method's performance is evaluated across four metrics:  Average Performance Rank, and the performance on each individual task.  The best performing method for each task, and the second-best performing method is highlighted for clarity.", "section": "6 Extension: Selections of Examples for Few-shot Prompts"}, {"figure_path": "FLNnlfBGMo/tables/tables_22_1.jpg", "caption": "Table 5: Averaged scores of baselines and TRIPLE on the task \u201cCola\u201d (from the GLUE dataset) and the GSM8K dataset using GPT-3.5, with |P| = 30 candidates and budget N = 150, where the highest ranked methods are marked bold.", "description": "This table presents the average scores achieved by different prompt selection methods (Uniform, UCB, SH, CR, BO-EI, NeuralUCB, CLST, GSE) on two datasets: GLUE's \"Cola\" task and GSM8K.  The results are obtained using GPT-3.5 with a fixed prompt pool size of 30 and a budget of 150. The table highlights the superior performance of TRIPLE methods by bolding the best-performing methods for each dataset. The \"Cola\" task assesses linguistic acceptability, while GSM8K focuses on mathematical reasoning problems.", "section": "F.3 Performances on Additional Datasets"}, {"figure_path": "FLNnlfBGMo/tables/tables_24_1.jpg", "caption": "Table 2: Averaged performance ranks of baselines and TRIPLE on the selected tasks using GPT-3.5, which are computed separately for methods using embeddings or not. The rank of BO is computed with the highest performance from BO-EI and BO-PI. The highest ranked methods are marked bold.", "description": "This table presents the average performance ranks of different prompt selection methods (Uniform, UCB, SH, CR, BO, NeuralUCB, CLST, GSE) across multiple tasks using the GPT-3.5 language model.  The ranks are calculated separately for methods that use prompt embeddings and those that don't.  A lower rank indicates better performance. The table shows that TRIPLE methods consistently outperform the baseline methods.", "section": "5.1 Evaluating TRIPLE with Fixed Prompt Pools"}, {"figure_path": "FLNnlfBGMo/tables/tables_25_1.jpg", "caption": "Table 7: Clusters for \"rhymes\": the best prompt overall is marked in red, and the best prompt in each cluster in yellow.", "description": "This table shows the result of clustering 30 prompts for the \"rhymes\" task. Each row represents a cluster of prompts with similar characteristics, and the best performing prompt in each cluster is highlighted in yellow, with the overall best prompt marked in red.  This illustrates the effectiveness of prompt clustering for efficient selection, as similar prompts are grouped together, allowing for faster identification of top performers.", "section": "Experiments"}, {"figure_path": "FLNnlfBGMo/tables/tables_32_1.jpg", "caption": "Table 8: The ratios of different methods outputting a good prompt with GPT-3.5 from large prompt pools |P| = 30.", "description": "This table presents the success rates of different prompt selection methods in identifying a good prompt (either the optimal prompt or one achieving at least 95% of the optimal prompt's performance) from a pool of 30 candidate prompts. The success rate is measured across various tasks and budgets (5, 10, and 20 evaluations per prompt).  It showcases the performance of Uniform, UCB, SH, CR, CLST, and GSE algorithms under different budget constraints.", "section": "F.1 Selection of Budgets"}]