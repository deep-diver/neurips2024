{"references": [{"fullname_first_author": "Mihai Anitescu", "paper_title": "Degenerate nonlinear programming with a quadratic growth condition", "publication_date": "2000-00-00", "reason": "This paper establishes a foundational condition for optimization problems which is extended in the current work to characterize loss landscapes of neural networks."}, {"fullname_first_author": "John Duchi", "paper_title": "Adaptive subgradient methods for online learning and stochastic optimization", "publication_date": "2011-00-00", "reason": "This paper introduces adaptive gradient methods that are empirically successful in training deep neural networks, a topic investigated by the current work."}, {"fullname_first_author": "Chaoyue Liu", "paper_title": "Loss landscapes and optimization in over-parameterized non-linear systems and neural networks", "publication_date": "2022-00-00", "reason": "This paper provides a comprehensive analysis of loss landscapes and optimization in overparameterized systems and directly inspires the research of the current work."}, {"fullname_first_author": "Hadi Daneshmand", "paper_title": "Escaping saddles with stochastic gradients", "publication_date": "2018-00-00", "reason": "This paper provides insights into the dynamics of gradient-based optimization methods in non-convex settings, a key focus of this paper."}, {"fullname_first_author": "Nicolas Loizou", "paper_title": "Stochastic Polyak step-size for SGD: An adaptive learning rate for fast convergence", "publication_date": "2021-00-00", "reason": "This paper proposes an adaptive learning rate method for SGD which is analyzed in the context of the new function class introduced in the current work."}]}