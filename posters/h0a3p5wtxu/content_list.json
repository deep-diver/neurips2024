[{"type": "text", "text": "Loss landscape Characterization of Neural Networks without Over-Parametrization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Rustem Islamov1 Niccol\u00f3 Ajroldi2 Antonio Orvieto2,3,4 Aurelien Lucchi1 ", "page_idx": 0}, {"type": "text", "text": "1University of Basel 2Max Planck Institute for Intelligent Systems 3 ELLIS Institute T\u00fcbingen 4T\u00fcbingen AI Center ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Optimization methods play a crucial role in modern machine learning, powering the remarkable empirical achievements of deep learning models. These successes are even more remarkable given the complex non-convex nature of the loss landscape of these models. Yet, ensuring the convergence of optimization methods requires specific structural conditions on the objective function that are rarely satisfied in practice. One prominent example is the widely recognized Polyak-\u0141ojasiewicz (PL) inequality, which has gained considerable attention in recent years. However, validating such assumptions for deep neural networks entails substantial and often impractical levels of over-parametrization. In order to address this limitation, we propose a novel class of functions that can characterize the loss landscape of modern deep models without requiring extensive over-parametrization and can also include saddle points. Crucially, we prove that gradient-based optimizers possess theoretical guarantees of convergence under this assumption. Finally, we validate the soundness of our new function class through both theoretical analysis and empirical experimentation across a diverse range of deep learning models. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The strides in empirical progress achieved by deep neural networks over the past decade have been truly remarkable. Central to the triumph of these techniques lies the effectiveness of optimization methods, which is particularly noteworthy given the non-convex nature of the objective functions under consideration. Worst-case theoretical results point to a pessimistic view since even a degree four polynomial can be NP-hard to optimize [30] and the loss landscape of some neural networks are known to include saddle points or bad local minima [6, 70, 88]. ", "page_idx": 0}, {"type": "text", "text": "Yet, empirical evidence has shown that gradient-based optimizers \u2013 including SGD, AdaGrad [21] and Adam [39] among many others \u2013 can effectively optimize the loss of modern deep-learning-based models. While some have pointed to the ability of gradient-based optimizers to deal with potentially complex landscapes, e.g. escaping saddle points [36, 17], another potential explanation is that the loss landscape itself is less complex than previously assumed [27, 51]. ", "page_idx": 0}, {"type": "text", "text": "Some key factors in this success include the choice of architecture [5, 48, 40, 18], as well as the over-parametrization [73, 12, 50, 51]. In the well-known infinite-width limit [86, 35, 32], neural networks are known to exhibit simple landscapes [49]. However, practical networks operate in a finite range, which still leaves a lot of uncertainty regarding the nature of the loss landscape. This is especially important given that the convergence guarantees of gradient-based optimizers are derived by assuming some specific structure on the objective function [37, 79, 71]. Consequently, an essential theoretical endeavor involves examining the class of functions that neural networks can represent. ", "page_idx": 0}, {"type": "text", "text": "In this work, we present a new class of functions that satisfy a newly proposed $\\alpha{-}\\beta$ -condition (see Eq. (2)). We theoretically and empirically demonstrate that these functions effectively characterize the loss landscape of neural networks. Furthermore, we derive theoretical convergence guarantees for commonly used gradient-based optimizers under the $\\alpha{-}\\beta{}.$ -condition. ", "page_idx": 0}, {"type": "table", "img_path": "h0a3p5WtXU/tmp/b8b67a95dc4463e8219124fc53d176c405ec6cfe73401910ac6674737dc3cd90.jpg", "table_caption": ["Table 1: Summary of existing assumptions on the problem (1) and their limitations. Here $\\boldsymbol{S}$ denotes the set of minimizers of $f$ and $f_{i}^{*}:=\\mathrm{argmin}_{x}\\;f_{i}(x)$ . Unlike earlier conditions, the $\\alpha{-}\\beta{}.$ -condition is specifically designed to capture local minima and saddle points. $\\mathrm{NN}=\\mathrm{N}$ eural Network. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In summary, we make the following contributions: ", "page_idx": 1}, {"type": "text", "text": "1. We introduce the $\\alpha{-}\\beta$ -condition and theoretically demonstrate its applicability to a wide range of complex functions, notably those that include local saddle points and local minima.   \n2. We empirically validate that the $\\alpha{-}\\beta{}.$ -condition is a meaningful assumption that captures a wide range of practical functions, including matrix factorization and neural networks (ResNet, LSTM, GNN, Transformer, and other architectures).   \n3. We analyze the theoretical convergence of several optimizers under $\\alpha{-}\\beta$ -condition, including vanilla SGD (Stochastic Gradient Descent), $\\mathsf{S P S}_{\\operatorname*{max}}$ (Stochastic Polyak Stepsize) [53], and NGN [63] (Non-negative Gauss-Newton).   \n4. We provide empirical and theoretical counter-examples where the weakest assumptions, such as the PL and Aiming conditions, do not hold, but the $\\alpha{-}\\beta$ -condition does. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Function classes in optimization ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Studying the convergence properties of gradient-based optimizers has a long history in the field of optimization and machine learning. Notably, one of the fundamental observations is the linear and sub-linear convergence exhibited by GD for strongly convex (SCvx) and general convex (Cvx) functions [61]. However, most modern Machine Learning models have non-convex loss landscapes, for which the existing convex theory is not applicable. Without assumptions on the loss functions (other than smoothness), one can only obtain weak convergence guarantees to a first-order critical point. This situation has led to the derivation of assumptions that are weaker than convexity but that are sufficient to guarantee convergence of GD-based optimizers. The list includes error bounds (EB) [54], essential strong convexity (ESC) [52], weak strong convexity (WSC) [60], the restricted secant inequality (RSI) [87], and the quadratic growth (QG) condition [1]. In the neighborhood of the minimizer set $\\boldsymbol{S}$ , EB, PL, and QG are equivalent if the objective is twice differentiable [68]. All of them, except QG, are sufficient to guarantee a global linear convergence of GD. However, among these less stringent conditions, the Polyak-\u0141ojasiewicz (PL) condition stands out as particularly renowned. Initially demonstrated by Polyak [66] to ensure linear convergence, it has recently experienced a resurgence of interest, in part because it accurately characterizes the loss landscape of heavilly over-parametrized neural networks [50]. It was also shown to be one of the weakest assumptions among the other known conditions outlined so far [37]. A generalized form of the PL condition for non-smooth optimization is the Kurdyka-\u0141ojasiewicz (KL) condition [43, 10] which is satisfied for a much larger class of functions [14, 77] than PL. The KL inequality has been employed to analyze the convergence of the classic proximal-point algorithm [3, 11, 47] and other optimization methods [4, 45]. ", "page_idx": 1}, {"type": "text", "text": "More recently, some new convex-like conditions have appeared in the literature such as star-convex (StarCvx) [62], quasar-convex (QCvx) [28], and Aiming [51]. These conditions are relaxations of ", "page_idx": 1}, {"type": "image", "img_path": "h0a3p5WtXU/tmp/f202543bda48be53996f1f1e3d351a9e44644f0b63bc01c03c86a4718a8608e3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: Training of 3 layer LSTM model that shows Aiming condition does not always hold. The term \u201cAngle\u201d in the figures refers to the angle $\\angle(\\nabla f(x^{k}),{\\check{x^{k}}}-x^{K})$ , and it should be positive if Aiming holds, while in a-b we observe that it is negative during the first part of the training. Figures c-d demonstrate that possible constant $\\mu$ in $\\mathrm{PL}$ condition should be small which makes theoretical convergence slow. ", "page_idx": 2}, {"type": "image", "img_path": "h0a3p5WtXU/tmp/b1fa16f955539f28deeda902fd37474c6b55fc52ef8240b8e0b0f3353da04313.jpg", "img_caption": ["Figure 2: Training for half-space learning problem with SGD. The term \u201cAngle\u201d in the figures refers to the angle $\\angle(\\nabla\\mathsf{\\check{f}}(x^{k}),x^{k}-x^{K})$ . "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "convexity and include non-convex functions. Within the domain of reinforcement learning, several works [84, 23] have also considered relaxations of the gradient domination condition, although these analyses are conducted specifically within the context of policy gradient methods, and therefore less relatable to StarCvx, QCvx or the Aiming condition. ", "page_idx": 2}, {"type": "text", "text": "We present a summary of some of these conditions in Table 1. There is no general implication between already existing assumptions such as QCvx, Aiming, PL, and the $\\alpha{-}\\beta{}.$ -condition. However, as we will later see, the $\\alpha{-}\\beta$ -condition can more generally characterize the landscape of neural networks without requiring unpractical amounts of over-parametrization. Notably, the $\\alpha{-}\\beta$ -condition is a condition that applies globally to the loss. However, we will demonstrate that convergence guarantees can still be established for commonly-used gradient-based optimizers, although these guarantees are weaker than those derived under the PL condition, which relies on much stronger assumptions. ", "page_idx": 2}, {"type": "text", "text": "2.2 Limitations of existing conditions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Next, we discuss the limitations of previous conditions to characterize the loss landscape of complex objective functions such as the ones encountered when training deep neural networks. ", "page_idx": 2}, {"type": "text", "text": "Necessity of Over-parameterization. When considering deep models, the theoretical justification of conditions such as Aiming [51] and PL [50] require a significant amount of over-parameterization. This implies that the neural network must be considerably large, often with the minimum layer\u2019s width scaling with the size of the dataset $n$ . However, various studies suggest that this setup may not always accurately model real-world training dynamics [13, 2]. To the best of our knowledge, the weakest requirements on the width of a network are sub-quadratic $\\widetilde{\\Omega}(n^{3/2})$ [20, 74], where $n$ is the size of a dataset. This implies that, even for a small dataset such as MNIST, a network should have billions of parameters which is not a realistic setting in practice. In contrast, the $\\alpha{-}\\beta$ -condition condition does not require such an over-parametrization condition to hold (e.g., see Example 5). In Section 5 we provide empirical results showing how our condition is affected by over-parameterization. ", "page_idx": 2}, {"type": "text", "text": "Necessity of Invexity. One limitation of prior assumptions is their inability to account for functions containing local minima or saddle points. Indeed, many of the weakest conditions, such as QCvx, PL, KL, and Aiming, require that any point where the gradient is zero must be deemed a global minimizer. However, such conditions are not consistent with the loss landscapes observed in practical neural networks. For example, finite-size MLPs can have spurious local points or saddle points [88, 80]. Another known example is the half-space learning problem which is known to have saddle points [17]. We refer the reader to Figure 2-a that illustrates this claim (it showcases the surface of the problem fixing all parameters except first 2), and also demonstrates that the Aiming and PL conditions fail to hold in such a setting. We present the results in Figure $2^{1}$ where we observe that $(i)$ the angle between the full gradient and direction to the minimizer $\\angle(\\nabla f(x^{k}),x^{k}-x^{*})$ can be negative implying that the Aiming condition does not hold in this case (since the angle should remain positive); $(i i)$ the gradient norm can become zero while we did not reach minimum (loss is still large) implying that the PL condition does not hold as well (since the inequality $\\|\\nabla f(x^{k})\\|^{2}\\geq2\\mu(f(\\Breve{x}^{k})-\\Breve{f}^{*})$ is not true for any positive $\\dot{\\mu_{,}}$ ). These observations suggest that the Aiming and PL conditions do not characterize well a landscape in the absence of invexity property. In contrast, we demonstrate in Example 2 and Figure 9 that our proposed assumption is preferable in this scenario. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Lack of Theory. As previously mentioned, most theoretical works apply to some infinite limit or neural networks of impractical sizes. In contrast, several works [89, 27, 75] have studied the empirical properties of the loss landscape of neural networks during training. They have shown that gradient-based optimization methods do not encounter significant obstacles that impede their progress. However, these studies fall short of providing theoretical explanations for this observed phenomenon. ", "page_idx": 3}, {"type": "text", "text": "Lack of Empirical Evidence. Several theoretical works [49, 51] prove results on the loss landscape of neural networks without supporting their claims using experimental validation on deep learning benchmarks. We demonstrate some practical counter-examples to these conditions proved in prior work. We train LSTM-based model2 with standard initialization on Wikitext-2 [57] and Penn Treebank (PTB) [58] datasets. In Figure 1 (a-b), we show that the angle between the full gradient and direction to the minimizer $\\angle(\\nabla f(x^{\\natural}),x^{k}-x^{*})$ can be negative in the first part of the training. This result implies that the Aiming condition does not hold in this setting (either we do not have enough over-parameterization or the initialization does not lie in the locality region where Aiming holds). Moreover, for the same setting in Figure 1 (c-d) we plot $2\\log(\\|\\nabla f(x^{\\bar{k}})\\|)^{\\breve{-}}{}^{2}/\\delta\\log(f(x^{k})-\\breve{f}(x^{K}))^{\\bar{3}}$ to measure the empirical value of $\\mathrm{PL}$ constant $\\log(2\\mu)$ (see derivations in Appendix A). We observe that the value of $\\mu$ that might satisfy $\\mathrm{PL}$ condition should be of order $10^{\\bar{-8}}-10^{-7}$ and leads to slow theoretical convergence [37]. These observations contradict with practical results. We defer to Appendix A for a more detailed discussion. In contrast, we demonstrate in Figure $7{-}(\\mathrm{g-h})$ that the proposed $\\alpha{-}\\beta$ -condition can be verified in this setting. ", "page_idx": 3}, {"type": "text", "text": "3 The proposed $\\alpha{-}\\beta$ -condition ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Setting. We consider the following Empirical Risk Minimization (ERM) problem that typically appears when training machine learning models: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\textstyle\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}\\left[f(x):=\\frac{1}{n}\\sum_{i=1}^{n}f_{i}(x)\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ denotes the parameters of a model we aim to train, $d$ is the number of parameters, and $n$ is the number of samples in the training dataset. Each $f_{i}(x)$ is the loss associated with the $i$ -th data point. We denote the minimum of the problem (1) by $f^{*}$ and the minimum of each individual function by $f_{i}^{*}:=\\operatorname*{min}_{x}f_{i}(x)$ , which we assume to be finite. Besides, the set $\\boldsymbol{S}$ denotes the set of all minimizers of $f$ . ", "page_idx": 3}, {"type": "text", "text": "A new class of functions. Next, we present a new condition that characterizes the interplay between individual losses $f_{i}$ and the set of minimizers of the global loss function $f$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 1 ( $\\alpha\\!-\\!\\beta.$ -condition ). Let $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ be a set and consider a function $f:\\mathcal{X}\\rightarrow\\mathbb{R}$ as defined in (1). Then $f$ satisfies the $\\alpha{-}\\beta$ -condition with positive parameters $\\alpha$ and $\\beta$ such that $\\alpha>\\beta$ if for any $x\\in\\mathscr{X}$ there exists $x_{p}\\in\\operatorname{Proj}(x,S)$ such that for all $i$ , ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\langle\\nabla f_{i}(x),x-x_{p}\\rangle\\geq\\alpha(f_{i}(x)-f_{i}(x_{p}))-\\beta(f_{i}(x)-f_{i}^{*}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The $\\alpha{-}\\beta{}.$ -condition recovers several existing assumptions as special cases. For example, the proposed assumption reduces to QCvx around $x^{*}$ if $\\alpha>0$ , $\\beta=0$ , and $\\boldsymbol{S}$ is a singleton $\\{x^{*}\\}$ . Importantly, the $\\alpha$ - $\\beta$ -condition is also applicable when the set $\\boldsymbol{S}$ contains multiple elements. ", "page_idx": 4}, {"type": "text", "text": "One can easily check that the pair of parameters $(\\alpha,\\beta)$ in (2) can not be unique. Indeed, if the assumption is satisfied for some $\\alpha$ and $\\beta$ , then due to inequality $f_{i}(x_{p})\\geq f_{i}^{*}$ it will be also satisfied for $(\\bar{\\alpha^{+}}\\delta,\\beta+\\delta)$ for any $\\delta\\geq0$ . ", "page_idx": 4}, {"type": "text", "text": "3.1 Theoretical verification of the $\\alpha{-}\\beta$ -condition ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To demonstrate the significance of Definition 1 as a meaningful condition for describing structural non-convex functions, we provide several examples below that satisfy (2). We do not aim to provide the tightest possible choice of $\\alpha$ and $\\beta$ such that Definition 1 holds. Instead, this section aims to offer a variety of examples that demonstrate specific desired characteristics when $\\alpha{-}\\beta{}.$ -condition holds, encompassing a broad range of functions. ", "page_idx": 4}, {"type": "text", "text": "The initial example illustrates that $\\boldsymbol{S}$ could potentially be infinite for the class of functions satisfying Definition 1. ", "page_idx": 4}, {"type": "text", "text": "Example 1. Let $f,f_{1},f_{2}\\colon\\mathbb{R}^{2}\\rightarrow\\mathbb{R}$ be such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f=\\frac{1}{2}(f_{1}+f_{2})\\quad\\mathrm{with}\\quad f_{1}(x,y)=\\frac{(x+y)^{2}}{(x+y)^{2}+1},\\quad f_{2}(x,y)=\\frac{(x+y+1)^{2}}{(x+y+1)^{2}+1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "then Definition 1 holds with $\\alpha\\in[5/2,+\\infty)$ and $\\beta\\in[4\\alpha/5,\\alpha)$ . ", "page_idx": 4}, {"type": "text", "text": "Next, we provide an example where $f$ satisfies Definition 1 even in the presence of saddle points. Example 2. Let $f,f_{1},f_{2}\\colon\\mathbb{R}^{2}\\rightarrow\\mathbb{R}$ be such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f=\\frac{1}{2}(f_{1}+f_{2})\\quad\\mathrm{with}\\quad f_{1}(x,y)=1-e^{-x^{2}-y^{2}},\\quad f_{2}(x,y)=1-e^{-(x-2)^{2}-(y-2)^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "then Definition 1 holds for some $\\alpha$ and $\\beta=\\alpha-8$ . ", "page_idx": 4}, {"type": "text", "text": "Remark 1. Examples 1 and 2 can be generalized for any number of functions $n$ and dimension $d$ as follows ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f_{i}(x)=\\frac{\\left(\\sum_{j=1}^{d}x_{j}+a_{i}\\right)^{2}}{\\left(\\sum_{j=1}^{d}x_{j}+a_{i}\\right)^{2}+1},\\quad f_{i}(x)=\\frac{\\sum_{j=1}^{d}(x_{j}-b_{i j})^{2}}{1+\\sum_{j=1}^{d}(x_{j}-b_{i j})^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for some properly chosen $\\{a_{i}\\}$ and $\\{b_{i j}\\},i\\in[n],j\\in[d]$ . ", "page_idx": 4}, {"type": "text", "text": "Example 3. Let $f,f_{1},f_{2}\\colon\\mathbb{R}^{2}\\rightarrow\\mathbb{R}$ be such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f=\\frac{1}{2}(f_{1}+f_{2})\\quad\\mathrm{with}\\quad f_{1}(x,y)=\\frac{1+x^{2}+y^{2}}{4+x^{2}+y^{2}},\\quad f_{2}(x,y)=\\frac{(x-2.5)^{2}+(y-2.5)^{2}}{4+(y-2.5)^{2}+(y-2.5)^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "then Definition 1 holds for some $\\alpha$ and $\\beta=\\alpha-1$ . ", "page_idx": 4}, {"type": "text", "text": "The three examples above demonstrate that functions satisfying Definition 1 can potentially be nonconvex with an unbounded set of minimizers $\\boldsymbol{S}$ (Example 1) and can have saddle points (Example 2) and local minima (Example 3). In contrast, the PL and Aiming conditions are not met in cases where a problem exhibits saddle points. For illustration purposes, we plot the loss landscapes of $f$ in Figure 3. ", "page_idx": 4}, {"type": "text", "text": "So far, we have presented simple examples to verify Definition 1. Next, we turn our attention to more practical examples in the field of machine learning. We start with the matrix factorization problem that is known to have saddle points [76] but can be shown to be PL after a sufficiently large number of iterations of alternating gradient descent and under a specific random initialization [81]. ", "page_idx": 4}, {"type": "text", "text": "Example 4. Let $f_{i},f_{i j}$ be such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f(W,S)=\\frac{1}{2n m}\\|X-W^{\\top}S\\|_{\\mathrm{F}}^{2}=\\frac{1}{2n m}\\sum_{i,j}(X_{i j}-w_{i}^{\\top}s_{j})^{2},\\quad f_{i j}(W,S)=\\frac{1}{2}(X_{i j}-w_{i}^{\\top}s_{j})^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $X\\,\\in\\,\\mathbb{R}^{n\\times m},W\\,=\\,(w_{i})_{i=1}^{n}\\,\\in\\,\\mathbb{R}^{k\\times n},S\\,=\\,(s_{j})_{j=1}^{m}\\,\\in\\,\\mathbb{R}^{k\\times m},$ , and $\\operatorname{rank}(X)\\,=\\,r\\,\\geq\\,k$ . We assume that $X$ is generated using matrices $W^{*}$ and $S^{*}$ with non-zero additive noise that minimize empirical loss, namely, $X=(W^{*})^{\\top}S^{*}{+}(\\varepsilon_{i j})_{i\\in[n],j\\in[m]}$ where $W^{*}$ $V^{*},S^{*}=\\mathrm{argmin}_{W,S}\\;f(W,S)$ . Let $\\mathcal{X}$ be any bounded set that contains $\\boldsymbol{S}$ . Then Definition 1 is satisfied with $\\alpha=\\beta+1$ and some $\\beta>0$ . ", "page_idx": 4}, {"type": "image", "img_path": "h0a3p5WtXU/tmp/741d9d7917558b3643784cbf52020f50025c4e3f9f0ff003df3b6fea522240aa.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 3: Loss landscape of $f$ that satisfy Definition 1. The analytical form of $f_{i}$ is given in Section 3.1. These examples demonstrate that the problem (1) that satisfies $\\alpha{-}\\beta$ -condition might have an unbounded set of minimizers $\\boldsymbol{S}$ (Example 1), a saddle point (Example 2), and local minima (Example 3) in contrast to the PL and Aiming conditions. The contour plots are presented in ??. ", "page_idx": 5}, {"type": "text", "text": "Example 5. Consider training a two-layer neural network with a logistic loss ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f=\\frac{1}{n}\\sum_{i=1}^{n}f_{i},\\quad f_{i}(W,v)=\\phi(y_{i}\\cdot v^{\\top}\\sigma(W x_{i}))+\\lambda_{1}\\|v\\|^{2}+\\lambda_{2}\\|W\\|_{\\mathrm{F}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for a classification problem where $\\phi(t):=\\log(1+\\exp(-t))$ , $W\\,\\in\\,\\mathbb{R}^{k\\times d},v\\,\\in\\,\\mathbb{R}^{k}$ , $\\sigma$ is a ReLU function applied coordinate-wise, $y_{i}\\in\\{-1,+1\\}$ is a label and $x_{i}\\in\\mathbb{R}^{d}$ is a feature vector. Let $\\mathcal{X}$ be any bounded set that contains $\\boldsymbol{S}$ . Then the $\\alpha{-}\\beta$ -condition holds in $\\mathcal{X}$ for some $\\alpha\\geq1$ and $\\beta=\\alpha-1$ . ", "page_idx": 5}, {"type": "text", "text": "Remark 2. The previous examples can be extended to any positive and convex function $\\phi$ (e.g., square loss) with the additional assumption that each individual loss $f_{i}$ does not have minimizers in $\\boldsymbol{S}$ , i.e. $\\nexists(W^{*},v^{*})\\in S$ such that $f_{i}(W^{*},v^{*})=f_{i}^{*}$ for some $i\\in[n]$ . ", "page_idx": 5}, {"type": "text", "text": "We highlight that Example 5 is applicable for a bounded set $\\mathcal{X}$ of an arbitrary size. Moreover, in practice, we typically add $\\ell_{1}$ or $\\ell_{2}$ regularization which can be equivalently written as a constrained optimization problem, and therefore, Example 5 holds in this scenario. In comparison, the results from previous works do not hold for an arbitrary bounded set around $\\boldsymbol{S}$ requiring initialization to be close enough to the solution set [51, 50]. The proofs for all examples are given in Appendix C.1. ", "page_idx": 5}, {"type": "text", "text": "4 Theoretical convergence of algorithms ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We conduct our analysis under the following smoothness assumption that is standard in the optimization literature. ", "page_idx": 5}, {"type": "text", "text": "Assumption 1. We assume that each $f_{i}$ is $L$ -smooth, i.e. for all $\\boldsymbol{x},\\boldsymbol{y}\\in\\mathbb{R}^{d}$ it holds $\\Vert\\nabla f_{i}(x)-$ $\\nabla f_{i}(y)\\Vert\\leq L\\Vert x-y\\Vert$ . ", "page_idx": 5}, {"type": "text", "text": "The next assumption, which is sometimes called functional dissimilarity [56], is standard in the analysis of SGD with adaptive stepsizes [53, 26, 24]. ", "page_idx": 5}, {"type": "text", "text": "Assumption 2. We assume that the interpolation error $\\sigma_{\\mathrm{int}}^{2}:=\\,\\mathbb{E}_{i}[f^{*}-f_{i}^{*}]$ is finite, where the expectation is taken concerning the randomness of indices i for a certain algorithm. ", "page_idx": 5}, {"type": "text", "text": "4.1 Convergence under the $\\alpha{-}\\beta.$ -condition ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Now we demonstrate that the $\\alpha{-}\\beta{}.$ -condition is sufficient for common optimizers to converge up to a neighbourhood of the set of minimizers $\\boldsymbol{S}$ . We provide convergence guarantees for SGD-based algorithms with fixed and adaptive stepsize (i.e., the update direction is of the form $-\\gamma_{k}\\nabla f_{i_{k}}(x^{k}))$ . In this section, we only present the main statements about convergence while the algorithms\u2019 description and the proofs are deferred to Appendix C.2. ", "page_idx": 5}, {"type": "text", "text": "Convergence of SGD. We start with the results for vanilla SGD with constant stepsize. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Assume that Assumptions 1-2 hold. Then the iterates of SGD (Alg. 1) with stepsize $\\begin{array}{r}{\\gamma\\le\\frac{\\alpha-\\beta}{2L}}\\end{array}$ satisfy ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{0\\leq k<K}\\mathbb{E}\\left[f(x^{k})-f^{*}\\right]\\leq\\frac{\\mathbb{E}\\left[\\mathrm{dist}(x^{0},S)^{2}\\right]}{K}\\frac{1}{\\gamma(\\alpha-\\beta)}+\\frac{2L\\gamma}{\\alpha-\\beta}\\sigma_{\\mathrm{int}}^{2}+\\frac{2\\beta}{\\alpha-\\beta}\\sigma_{\\mathrm{int}}^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Table 2: Summary of how the non-vanishing term $\\beta\\sigma_{\\mathrm{int}}^{2}$ (as appearing e.g. in Eq. (9)) increases $(\\nearrow)$ or decreases $(\\searrow)$ as a function of specific quantities of interest. ", "page_idx": 6}, {"type": "table", "img_path": "h0a3p5WtXU/tmp/8520f5c60a6aaebf83c5cf3ad03df2814e77302104eda0ca8c19e8cd5cf0712a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Theorem 1 shows that under the $\\alpha\\!\\cdot\\!\\beta.$ -condition , SGD converges with a rate $\\mathcal{O}(K^{-1/2})$ (the same rate obtained by SGD for convex functions [24]) up to a ball of size $O(\\beta\\sigma_{\\mathrm{int}}^{2})$ . We argue that the non-vanishing term $O(\\beta\\sigma_{\\mathrm{int}}^{2})$ must appear in the convergence rate for non-convex optimization for several reasons: $(i)$ This term arises directly from the use of the $\\alpha{-}\\beta$ condition in the analysis, without resorting to additional upper bounds or approximations. It reflects the potential existence of local minima that the $\\alpha{-}\\beta$ condition is designed to model. In the worst-case scenario, if SGD is initialized near local minima and uses a sufficiently small step size, it may fail to converge to the exact minimizer and can become trapped in suboptimal minima. This sub-optimality is modeled in the upper bound by the stepsize-independent quantity $O(\\beta\\sigma_{\\mathrm{int}}^{2})$ since we provide convergence guarantees for the function value sub-optimality rather than the squared gradient norm, which is more typical in the non-convex setting. $(i i)$ We also observe that the last term in (9) shrinks as a model becomes more over-parameterized (which is consistent with prior works such as [50] that require large amounts of over-parametrization); see Sections 5.1, 5.2, and D.7 for experimental validation of this claim. Further empirical observations are summarized in Table 2 and will be discussed in Section 5. Theoretically, if a model is sufficiently over-parameterized such that the interpolation condition $f_{i}^{*}=f^{*}$ holds, then the non-vanishing term is not present in the bound. $(i i i)$ The presence of a non-vanishing error term in the rate with the $\\alpha{-}\\beta$ condition is consistent with empirical observation as it is frequently observed when training neural networks (see Figure 8.1 in [25]). This is also observed during the training of language models where the loss is significantly larger than 0 (see Figure 19). This phenomenon suggests that reaching a critical point, which is a global minimizer, is not commonly observed practically. $(i v)$ Finally, we note a potential similarity with prior works that propose other conditions to describe the loss landscape of deep neural networks (e.g. gradient confusion [71]), and also obtain a non-vanishing term in the convergence rate (see Theorem 3.2 in [71]). ", "page_idx": 6}, {"type": "text", "text": "Convergence of $\\mathsf{S P S}_{\\mathrm{max}}$ . Next, we consider the $\\mathsf{S P S}_{\\mathrm{max}}$ algorithm. $\\mathsf{S P S}_{\\mathrm{max}}$ stepsize is given by $\\begin{array}{r}{\\gamma_{k}:=\\operatorname*{min}\\left\\{\\frac{f_{i_{k}}(x^{k})-f_{i_{k}}^{\\ast}}{c\\|\\nabla f_{i_{k}}(x^{k})\\|^{2}},\\gamma_{\\mathrm{b}}\\right\\}}\\end{array}$ where $c$ and $\\gamma_{\\mathrm{b}}$ are the stepsize hyperparameters. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. Assume that Assumptions 1-2 hold. Then the iterates of $\\mathsf{S P S}_{\\mathrm{max}}$ (Alg. 2) with a stepsize hyperparameter $\\begin{array}{r}{c>\\frac{1}{2(\\alpha-\\beta)}}\\end{array}$ 2(\u03b1\u2212\u03b2) satisfy ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{0\\leq k<K}\\mathbb{E}\\left[f(x^{k})-f^{*}\\right]\\leq\\frac{c_{1}}{K}\\mathbb{E}\\left[\\mathrm{dist}(x^{0},S)^{2}\\right]+2\\alpha c_{1}\\gamma_{\\mathrm{b}}\\sigma_{\\mathrm{int}}^{2},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where \u03b3min := min{1/2cL, \u03b3b} and c1 :=\u03b3min(2(\u03b1c\u2212\u03b2)c\u22121). ", "page_idx": 6}, {"type": "text", "text": "In the convex case, i.e. $\\alpha{-}\\beta$ -condition holds with $\\alpha=1,\\beta=0$ , we recover the rate of Loizou et al.   \n[53]. ", "page_idx": 6}, {"type": "text", "text": "Convergence of NGN. Finally, we turn to the analysis of NGN. This algorithm is proposed for minimizing positive functions $f_{i}$ which is typically the case for many practical choices. Its stepsize $\\begin{array}{r}{\\gamma_{k}:=\\frac{\\gamma}{1+\\frac{\\gamma}{2f_{i_{k}}(x^{k})}\\|\\nabla f_{i_{k}}(x^{k})\\|^{2}}}\\end{array}$ where $\\gamma$ is the stepsize hyperparameter. NGN stepsize differs from that of $\\mathsf{S P S}_{\\mathrm{max}}$ by replacing min operator by softer harmonic averaging of SPS stepsize and a constant $\\gamma$ . In addition to the already mentioned assumptions, we make a mild assumption that the positivity error $\\sigma_{\\mathrm{pos}}^{2}:=\\mathbb{E}\\left[f_{i}^{*}\\right]$ is finite. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3. Assume that Assumptions $^{\\,l}$ with $\\alpha\\ge\\beta+1$ and 1-2 hold. Assume that each function $f_{i}$ is positive and $\\sigma_{\\mathrm{pos}}^{2}<\\infty$ . Then the iterates of NGN (Alg. 3) with a stepsize parameter $\\gamma>0$ satisfy ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\operatorname*{min}_{0\\le k\\le K-1}\\mathbb{E}\\left[f(x^{k})-f^{*}\\right]}&{\\le}&{\\displaystyle\\frac{\\mathbb{E}\\left[\\mathrm{dist}(x^{0},S)^{2}\\right]}{2\\gamma K}\\frac{(1+2\\gamma L)^{2}}{c_{2}}+\\frac{3L\\gamma\\alpha(1+\\gamma L)\\sigma_{\\mathrm{int}}^{2}}{c_{2}}}\\\\ &&{\\displaystyle+\\,\\frac{\\gamma L}{a}\\operatorname*{max}\\left\\{2\\gamma L-1,0\\right\\}\\sigma_{\\mathrm{pos}}^{2}+\\frac{2\\beta\\sigma_{\\mathrm{int}}^{2}}{c_{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $c_{2}:=2\\gamma L(\\alpha-\\beta-1)+\\alpha-\\beta$ . ", "page_idx": 6}, {"type": "image", "img_path": "h0a3p5WtXU/tmp/97d5cb101b5d889629b288038db1b6ca72624f7cf63d0f7678086ef6b8bb838c.jpg", "img_caption": ["Figure 4: $\\alpha\\!-\\!\\beta.$ -condition in the training of 3 layer MLP model on Fashion-MNIST dataset varying the size of the second layer. Here $T(x_{k})\\stackrel{*}{=}\\langle\\nabla f_{i_{k}}^{\\phantom{\\dagger}}(x^{k}),x^{k}-x^{K}\\rangle-\\alpha(f_{i_{k}}(x^{k})-f_{i_{k}}(x^{K}))-\\dot{\\beta}f_{i_{k}}^{\\phantom{\\dagger}}(x^{k})$ assuming that $f_{i}^{*}=0$ . Minimum is taken across all runs and iterations for given pair of $(\\alpha,\\beta)$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "One of the main properties of NGN is its robustness to the choice of stepsize $\\gamma$ . Theorem 3 can be seen as an extension of this feature from the set of convex functions originally analyzed in [63] to the class of structured non-convex satisfying $\\alpha{-}\\beta{}.$ -condition. ", "page_idx": 7}, {"type": "text", "text": "Comparing the results of Theorems 2 and 3 we highlight several important differences. $(i)$ There is no restriction on the stepsize parameter $\\gamma$ for NGN. Conversely, $\\mathsf{S P S}_{\\mathrm{max}}$ requires $c$ to be lower bounded. $(i i)$ Both algorithms converge to a neighborhood of the solution with a fixed stepsize hyperparameter. However, the neighborhood size of $\\mathsf{S P S}_{\\mathrm{max}}$ is not controllable by the stepsize hyperparameter and remains constant even in the convex setting when $\\beta\\,=\\,0$ . In contrast, NGN converges to a ball whose size can be made smaller by choosing a small stepsize parameter, and the \u201cnon-vanishing\u201d term disappears in the convex setting $\\beta=0$ . ", "page_idx": 7}, {"type": "text", "text": "We note that our goal was not to achieve the tightest convergence guarantees for each algorithm, but rather to underscore the versatility of the $\\alpha{-}\\beta{}.$ -condition in deriving convergence guarantees for SGD-type algorithms, both for constant or adaptive stepsizes. In addition to the results of this section, we demonstrate the convergence guarantees for SGD, $\\mathsf{S P S}_{\\mathrm{max}}$ , and NGN with decreasing with $k$ stepsizes in Appendix C.2. Besides, in Appendix C.2.4 we present a convergence of a slightly modified version of Adagrad-norm method [82] under $\\alpha$ - $\\beta$ -condition. ", "page_idx": 7}, {"type": "text", "text": "5 Experimental validation of the $\\alpha{-}\\beta$ -condition ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we provide extensive numerical results supporting that the $\\alpha{-}\\beta$ -condition does hold in many practical applications for various tasks, model architectures, and datasets. The detailed experimental setting is described in Appendix D. ", "page_idx": 7}, {"type": "text", "text": "In all cases, we approximate $\\operatorname{Proj}(x^{k},S)$ as the last iterate $x^{K}$ in a run. After finding such an approximation, we start a second training run with the same random seed to measure all necessary quantities. To guarantee that the second training trajectory follows the same path as the first run, we disable non-deterministic CUDA operations while training on a GPU. For each task, we demonstrate possible values of pairs of $(\\alpha,\\beta)$ that work across all runs (might differ from one experiment to another) with different random seeds and satisfy $\\alpha\\ge\\beta+0.1$ . ", "page_idx": 7}, {"type": "text", "text": "5.1 MLP architecture ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "First, we test MLP neural networks with 3 fully connected layers on Fashion-MNIST [83] dataset. We fix the second layer of the network to be a square matrix and vary its dimension layer to investigate the effect of over-parameterization on $\\alpha{-}\\beta$ -condition. We test it for dimensions $\\{32,128,2049,4096\\}$ , and for each case, we run experiments for 4 different random seeds. In Figure 4 we demonstrate possible values of pairs of $(\\alpha,\\beta)$ that work across all 4 runs. We observe that minimum possible values of $\\alpha$ and $\\beta$ increase from small size to medium, and then tend to decrease again as the model becomes more over-parameterized. We defer more experimental results for MLP to Appendix D.3 to showcase this phenomenon. This observation leads to the fact that the neighborhood of convergence $O(\\beta\\sigma_{\\mathrm{int}}^{2})$ of SGD eventually becomes smaller with the size of the model as we expect (since it becomes more over-parameterized). ", "page_idx": 7}, {"type": "text", "text": "5.2 CNN architecture ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In our next experiment, we test convolutional neural networks with 2 convolution layers and 1 fully connected layer on CIFAR10 dataset [41]. We vary the number of convolutions in the second convolution layer to investigate the effect of over-parameterization on $\\alpha{-}\\beta$ -condition. We test it for $\\{32,128,512,2048\\}$ number of convolutions in the second layer, and for each case, we run experiments for 4 different random seeds. In Figure 5, we observe that the smallest possible values of $(\\alpha,\\beta)$ increase till 64 convolutions, and then decrease back. Second, the difference $\\alpha-\\beta$ for possible choice of $\\alpha$ and $\\beta$ decreases from Figure 5-a to Figure 5-b, but then it increases again. ", "page_idx": 7}, {"type": "image", "img_path": "h0a3p5WtXU/tmp/bae683385b8691e30867c9b345f114e830905ceebea11c024444caaad51894b5.jpg", "img_caption": ["Figure 5: $\\alpha$ - $\\beta$ -condition in the training of CNN model on CIFAR10 dataset varying the number of convolutions in the second layer. Here $T(x_{k})=\\langle\\nabla f_{i_{k}}(x^{k}),x^{k}-x^{K}\\rangle-\\alpha(f_{i_{k}}\\langle x^{k}\\rangle-f_{i_{k}}(x^{K}))-$ $\\beta f_{i_{k}}(x^{k})$ assuming that $f_{i}^{*}=0$ . Minimum is taken across all runs and iterations for a given pair of $(\\alpha,\\beta)$ . "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "h0a3p5WtXU/tmp/f925517f2267561959670c78e9485436880b667badc81d232a6f5fdb215d6698.jpg", "img_caption": ["Figure 6: $\\alpha{-}\\beta{}.$ -condition in the training of Resnet9 model on CIFAR100 dataset varying the batch size. Here $T(x_{k})\\overset{\\cdot}{=}\\langle\\nabla f_{i_{k}}(x^{k}),x^{k}-x^{K}\\overset{\\setminus}{\\rangle}-\\alpha(f_{i_{k}}(x^{k})-f_{i_{k}}(x^{K}))-\\beta f_{i_{k}}(x^{k})$ assuming that $f_{i}^{*}=0$ . Minimum is taken across all runs and iterations for a given pair of $(\\alpha,\\beta)$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.3 Resnet architecture ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Next, we switch to the Resnet architecture [29] with batch sizes in $\\{64,128,256,512\\}$ trained on CIFAR100 [41]. For each batch size, we run experiments for 4 different random seeds. In Figure 6, we plot the possible choice of pairs $(\\alpha,\\beta)$ that works across all runs. We observe that $\\alpha{-}\\beta$ -condition holds in all cases. Besides, there is a tendency for the minimum possible choice of $\\alpha$ and $\\beta$ to decrease with batch size. Moreover, for larger batches, the difference between $\\alpha$ and $\\beta$ also increases. From Theorem 1, this result suggests that we can use bigger stepsizes with larger batches. ", "page_idx": 8}, {"type": "text", "text": "5.4 Training of AlgoPerf workloads and transformers for language modeling ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We are now interested in assessing the validity of $\\alpha{-}\\beta$ -condition on modern Deep Learning architectures. Thereby, we consider tasks from the AlgoPerf benchmarking suite [16]. We consider four workloads from the competition: (i) DLRMsmall model [59] on Criteo 1TB dataset [44]; (ii) U-Net model [69] on FastMRI dataset [85] (iii) GNN model [7] on OGBG dataset [31]; (iv) Transformer-big [78] on WMT dataset [9]. To train the models, we use NAdamW optimizer $[19]^{4}$ , which achieves state-of-the-art performances on the current version of the benchmark. The hyperparameters of the optimizer are chosen to reach the validation target threshold set by the original competition. Moreover, we consider the pretraining of a decoder-only transformer architecture [78] for causal language modeling. We conduct our evaluation on two publicly available Pythia models [8], of sizes 70M and 160M, trained on 1.25B and 2.5B tokens respectively. For this study, we use the SlimPajama [72] dataset. Following the original Pythia recipes, we fix a sequence length of 2048 and train the language model to predict the next token in a self-supervised fashion. We refer to section Appendices D and D.8 for additional details. ", "page_idx": 8}, {"type": "text", "text": "The results are presented in Figure 7. We observe that $\\alpha$ - $\\beta$ -condition holds for a wide range of values of $\\alpha$ and $\\beta$ which demonstrates that our condition can be seen as a good characterization of the training of modern large models as well. One can notice that the possible values of $\\alpha$ and $\\beta$ for AlgoPerf workloads are higher than those for smaller models discussed in previous sections. ", "page_idx": 8}, {"type": "image", "img_path": "h0a3p5WtXU/tmp/4c4f257db0657d3069782cba25763cf7d89d76bef18e1b46402d22fda0e86f86.jpg", "img_caption": ["Figure 7: $\\alpha{-}\\beta$ -condition in the training of some large models from AlgoPerf, 3-layer LSTM, and Transformers for language modeling. Here $T(x_{k})=\\check{\\langle\\nabla}f_{i_{k}}(x^{k}),x^{k}\\!-\\!x^{K}\\!\\rangle\\!-\\!\\alpha(\\dot{f}_{i_{k}}(x^{\\check{k}})\\!-\\!f_{i_{k}}(x^{\\check{K}}))-$ $\\beta f_{i_{k}}(x^{k})$ assuming that $f_{i}^{*}=0$ . Minimum is taken across all runs and iterations for a given pair of $(\\alpha,\\beta)$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "This difference is attributable to smaller models interpolating the training data more effectively, resulting in significantly lower training losses compared to those observed in AlgoPerf experiments (see Appendix D for detailed results). However, we highlight that the convergence guarantees of the optimizers depend on a term ${\\mathcal O}(\\beta\\sigma_{\\mathrm{int}}^{2})$ which is stable across all experiments we provide. ", "page_idx": 9}, {"type": "text", "text": "5.5 Additional experiments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We defer the verification of $\\alpha{-}\\beta$ -condition by Adam and SGDM to Appendix D.6. The results in Figure 16 suggest that Adam explores the part of the landscape with smaller values of $\\alpha$ and $\\beta$ than those for SGDM. The same conclusions can be drawn when comparing SGDM and SGD. These observations might be of the reasons why diagonal preconditioning and momentum are helpful in the training of DL models. The verification of the proposed condition varying the depth of Resnet model can be found in Appendix D.7. The results from Figure 17 demonstrate that the values of $\\alpha$ and $\\beta$ decrease with the depth of Resnet model, i.e. the model becomes more over-parametrized. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion, potential extensions, and limitations. ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we introduce a new class of functions that more accurately characterize loss landscapes of neural networks. In particular, we provide several examples that satisfy the proposed condition, including 2-layer ReLU-neural networks. Additionally, we prove that several optimization algorithms converge under our condition. Finally, we provide extensive empirical verification showing that the proposed $\\alpha$ - $\\cdot\\beta$ -condition holds along the optimization trajectories of various large deep learning models. ", "page_idx": 9}, {"type": "text", "text": "It is also possible to further expand convergence guarantees upon the ones presented in Section 4, for instance, by considering momentum [67] which is widely used in practice. However, we defer the exploration of other possible extensions to future research endeavors. ", "page_idx": 9}, {"type": "text", "text": "One of the limitations of this work is the empirical validation of the $\\alpha{-}\\beta{}.$ -condition on neural networks. We are only able to verify this condition along the trajectories of specific optimizers. Even when performing checks with many random seeds, we cannot fully observe the entire loss landscape. Additionally, while our theoretical examples demonstrate that the proposed condition holds, we do not provide the most precise theoretical values of $\\alpha$ and $\\beta$ that satisfy Definition 1. Therefore, in future work, we aim to obtain stronger theoretical guarantees demonstrating that the $\\alpha{-}\\beta{}.$ -condition holds for neural networks with more precise values of $\\alpha$ and $\\beta$ . We also intend to explore how the $\\alpha$ - $\\beta$ -condition varies when changing the architecture or the number of parameters (i.e., theoretical exploration of over-parameterization). ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Rustem Islamov and Aurelien Lucchi acknowledge the financial support of the Swiss National Foundation, SNF grant No 207392. Antonio Orvieto acknowledges the financial support of the Hector Foundation. The authors thank the anonymous reviewers for their valuable comments and suggestions on improving the paper. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Mihai Anitescu. Degenerate nonlinear programming with a quadratic growth condition. SIAM Journal on Optimization, 2000. (Cited on page 2)   \n[2] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. In Advances in Neural Information Processing Systems, 2019. (Cited on page 3)   \n[3] Hedy Attouch and J\u00e9r\u00f4me Bolte. On the convergence of the proximal algorithm for nonsmooth functions involving analytic features. Mathematical Programming, 116:5\u201316, 2009. (Cited on page 2)   \n[4] Hedy Attouch, J\u00e9r\u00f4me Bolte, and Benar Fux Svaiter. Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms, forward\u2013backward splitting, and regularized gauss\u2013seidel methods. Mathematical Programming, 2013. (Cited on page 2)   \n[5] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arXiv preprint arXiv: 1607.06450, 2016. (Cited on page 1)   \n[6] Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from examples without local minima. Neural networks, 1989. (Cited on page 1)   \n[7] Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv: 1806.01261, 2018. (Cited on page 9)   \n[8] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, 2023. (Cited on pages 9 and 41)   \n[9] Ond\u02c7rej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Logacheva, et al. Findings of the 2017 conference on machine translation (wmt17). Association for Computational Linguistics, 2017. (Cited on page 9)   \n[10] J\u00e9r\u00f4me Bolte, Aris Daniilidis, Olivier Ley, and Laurent Mazet. Characterizations of lojasiewicz inequalities and applications. arXiv preprint arXiv:0802.0826, 2008. (Cited on page 2)   \n[11] J\u00e9r\u00f4me Bolte, Trong Phong Nguyen, Juan Peypouquet, and Bruce W. Suter. From error bounds to the complexity of first-order descent methods for convex functions. Mathematical Programming, 2017. (Cited on page 2)   \n[12] L\u00e9na\u00efc Chizat and Francis Bach. On the global convergence of gradient descent for overparameterized models using optimal transport. In Advances in Neural Information Processing Systems, 2018. (Cited on page 1)   \n[13] L\u00e9na\u00efc Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS 2019), 2019. (Cited on page 3) ", "page_idx": 10}, {"type": "text", "text": "[14] Michel Coste. An introduction to o-minimal geometry. Istituti editoriali e poligrafici internazionali Pisa, 2000. (Cited on page 2) ", "page_idx": 11}, {"type": "text", "text": "[15] Ashok Cutkosky and Harsh Mehta. Momentum improves normalized SGD. In Proceedings of the 37th International Conference on Machine Learning, 2020. (Cited on page 37)   \n[16] George E. Dahl, Frank Schneider, Zachary Nado, Naman Agarwal, Chandramouli Shama Sastry, Philipp Hennig, Sourabh Medapati, Runa Eschenhagen, Priya Kasimbeg, Daniel Suo, Juhan Bae, Justin Gilmer, Abel L. Peirson, Bilal Khan, Rohan Anil, Mike Rabbat, Shankar Krishnan, Daniel Snider, Ehsan Amid, Kongtao Chen, Chris J. Maddison, Rakshith Vasudev, Michal Badura, Ankush Garg, and Peter Mattson. Benchmarking neural network training algorithms. arXiv preprint arXiv: 2306.07179, 2023. (Cited on pages 9 and 40)   \n[17] Hadi Daneshmand, Jonas Kohler, Aurelien Lucchi, and Thomas Hofmann. Escaping saddles with stochastic gradients. In Proceedings of the 35th International Conference on Machine Learning, 2018. (Cited on pages 1, 3, 4, and 37)   \n[18] Hadi Daneshmand, Amir Joudaki, and Francis Bach. Batch normalization orthogonalizes representations in deep random networks. In Advances in Neural Information Processing Systems, 2021. (Cited on page 1)   \n[19] Timothy Dozat. Incorporating nesterov momentum into adam. 2016. URL https: //openreview.net/pdf?id $=$ OM0jvwB8jIp57ZJjtNEZ. (Cited on page 9)   \n[20] Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In Proceedings of the 36th International Conference on Machine Learning, 2019. (Cited on page 3)   \n[21] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 2011. (Cited on page 1)   \n[22] Ilyas Fatkhullin, Jalal Etesami, Niao He, and Negar Kiyavash. Sharp analysis of stochastic optimization under global kurdyka-lojasiewicz inequality. In Advances in Neural Information Processing Systems, 2022. (Cited on page 19)   \n[23] Ilyas Fatkhullin, Anas Barakat, Anastasia Kireeva, and Niao He. Stochastic policy gradient methods: Improved sample complexity for fisher-non-degenerate policies. In International Conference on Machine Learning, 2023. (Cited on page 3)   \n[24] Guillaume Garrigos and Robert Gower. Handbook of convergence theorems for (stochastic) gradient methods. arXiv preprint arXiv: 2301.11235, 2024. (Cited on pages 6, 7, 25, and 34)   \n[25] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. (Cited on page 7)   \n[26] Robert Gower, Othmane Sebbouh, and Nicolas Loizou. Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, 2021. (Cited on page 6)   \n[27] Charles Guille-Escuret, Hiroki Naganuma, Kilian Fatras, and Ioannis Mitliagkas. No wrong turns: The simple geometry of neural networks optimization paths. arXiv preprint arXiv: 2306.11922, 2023. (Cited on pages 1 and 4)   \n[28] Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical systems. Journal of Machine Learning Research, 2018. (Cited on page 2)   \n[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385, 2015. (Cited on page 9)   \n[30] Christopher J Hillar and Lek-Heng Lim. Most tensor problems are np-hard. Journal of the ACM (JACM), 2013. (Cited on page 1)   \n[31] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 2020. (Cited on page 9)   \n[32] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, and zhifeng Chen. Gpipe: Efficient training of giant neural networks using pipeline parallelism. In Proceedings of Advances in Neural Information Processing Systems, 2019. (Cited on page 1)   \n[33] Florian H\u00fcbler, Junchi Yang, Xiang Li, and Niao He. Parameter-agnostic optimization under relaxed smoothness. In Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, 2024. (Cited on pages 4 and 37)   \n[34] Rustem Islamov, Mher Safaryan, and Dan Alistarh. AsGrad: A sharp unified analysis of asynchronous-SGD algorithms. In Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, volume 238, pages 649\u2013657. PMLR, 2024. (Cited on page 19)   \n[35] Arthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 2018. (Cited on page 1)   \n[36] Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points efficiently. In International conference on machine learning, 2017. (Cited on page 1)   \n[37] Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximalgradient methods under the polyak-\u0142ojasiewicz condition. Machine Learning and Knowledge Discovery in Databases, 2016. (Cited on pages 1, 2, 4, and 19)   \n[38] Sarit Khirirat, Eduard Gorbunov, Samuel Horv\u00e1th, Rustem Islamov, Fakhri Karray, and Peter Richt\u00e1rik. Clip21: Error feedback for gradient clipping. arXiv preprint: arXiv 2305.18929, 2023. (Cited on page 19)   \n[39] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv: 1412.6980, 2017. (Cited on page 1)   \n[40] Jonas Kohler, Hadi Daneshmand, Aurelien Lucchi, Thomas Hofmann, Ming Zhou, and Klaus Neymeyr. Exponential convergence rates for batch normalization: The power of length-direction decoupling in non-convex optimization. In The 22nd International Conference on Artificial Intelligence and Statistics, 2019. (Cited on page 1)   \n[41] Alex Krizhevsky. Learning multiple layers of features from tiny images. Lecture Notes, 2009. (Cited on pages 8, 9, 38, and 39)   \n[42] Shailesh Kumar. Classifying cifar100 images using resnets, regularization and data augmentation in pytorch. https://jovian.com/kumar-shailesh1597/cifar100-resnet18, 2023. (Cited on page 39)   \n[43] Krzysztof Kurdyka. On gradients of functions definable in o-minimal structures. In Annales de l\u2019institut Fourier, volume 48, pages 769\u2013783, 1998. (Cited on page 2)   \n[44] Criteo A. I. Lab. Criteo 1tb click logs dataset. 2014. URL https://ailab.criteo.com/ download-criteo-1tb-click-logs-dataset/. (Cited on page 9)   \n[45] Christian Lageman. Convergence of gradient-like dynamical systems and optimization algorithms. PhD thesis, Universit\u00e4t W\u00fcrzburg, 2007. (Cited on page 2)   \n[46] Kfir Levy. Online to offilne conversions, universality and adaptive minibatch sizes. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2017. (Cited on page 35)   \n[47] Guoyin Li and Ting Kei Pong. Calculus of the exponent of kurdyka\u2013\u0142ojasiewicz inequality and its applications to linear convergence of first-order methods. Foundations of computational mathematics, 2018. (Cited on page 2)   \n[48] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. Advances in neural information processing systems, 31, 2018. (Cited on page 1)   \n[49] Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Toward a theory of optimization for overparameterized systems of non-linear equations: the lessons of deep learning. arXiv preprint arXiv:2003.00307, 2020. (Cited on pages 1 and 4)   \n[50] Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Loss landscapes and optimization in overparameterized non-linear systems and neural networks. Applied and Computational Harmonic Analysis, 2022. (Cited on pages 1, 2, 3, 6, and 7)   \n[51] Chaoyue Liu, Dmitriy Drusvyatskiy, Misha Belkin, Damek Davis, and Yian Ma. Aiming towards the minimizers: fast convergence of SGD for overparametrized problems. In Proceedings of Advances in Neural Information Processing Systems, 2023. (Cited on pages 1, 2, 3, 4, and 6)   \n[52] Ji Liu, Steve Wright, Christopher Re, Victor Bittorf, and Srikrishna Sridhar. An asynchronous parallel stochastic coordinate descent algorithm. In Proceedings of the 31st International Conference on Machine Learning, 2014. (Cited on page 2)   \n[53] Nicolas Loizou, Sharan Vaswani, Issam Hadj Laradji, and Simon Lacoste-Julien. Stochastic polyak step-size for sgd: An adaptive learning rate for fast convergence. In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, 2021. (Cited on pages 2, 6, 7, and 26)   \n[54] Zhi-Quan Luo and Paul Tseng. Error bounds and convergence analysis of feasible descent methods: a general approach. Annals of Operations Research, 1993. (Cited on page 2)   \n[55] Maksim Makarenko, Elnur Gasanov, Abdurakhmon Sadiev, Rustem Islamov, and Peter Richt\u00e1rik. Adaptive compression for communication-efficient distributed training. Transactions on Machine Learning Research, 2023. (Cited on page 19)   \n[56] Grigory Malinovsky, Konstantin Mishchenko, and Peter Richt\u00e1rik. Server-side stepsizes and sampling without replacement provably help in federated optimization. OPT2021: 13th Annual Workshop on Optimization for Machine Learning, 2022. (Cited on page 6)   \n[57] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In Proceedings of 4th International Conference on Learning Representations (ICLR 2016), 2016. (Cited on page 4)   \n[58] Tomas Mikolov, Martin Karafi\u00e1t, Lukas Burget, Jan Cernock\u00fd, and Sanjeev Khudanpur. Recurrent neural network based language model. Proceedings of the 11th Annual Conference of the International Speech Communication Association, INTERSPEECH 2010, 2010. (Cited on page 4)   \n[59] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G Azzolini, et al. Deep learning recommendation model for personalization and recommendation systems. arXiv preprint arXiv:1906.00091, 2019. (Cited on page 9)   \n[60] I. Necoara, Yu. Nesterov, and F. Glineur. Linear convergence of first order methods for nonstrongly convex optimization. Mathematical Programming, 2019. (Cited on page 2)   \n[61] Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer Academic Publishers, Dordrecht, The Netherlands, 2004. (Cited on page 2)   \n[62] Yurii Nesterov and Boris T. Polyak. Cubic regularization of newton method and its global performance. Mathematical Programming, 2006. (Cited on page 2)   \n[63] Antonio Orvieto and Lin Xiao. An adaptive stochastic gradient method with non-negative gauss-newton stepsizes. arXiv preprint arXiv:2407.04358, 2024. (Cited on pages 2, 8, 30, and 32)   \n[64] Antonio Orvieto, Simon Lacoste-Julien, and Nicolas Loizou. Dynamics of sgd with stochastic polyak stepsizes: Truly adaptive variants and convergence to exact solution. In Advances in Neural Information Processing Systems, 2022. (Cited on pages 27, 29, and 34)   \n[65] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Proceedings of Advances in Neural Information Processing Systems, 2019. (Cited on page 36)   \n[66] Boris T. Polyak. Gradient methods for the minimisation of functionals. USSR Computational Mathematics and Mathematical Physics, 1963. (Cited on page 2)   \n[67] B.T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical, 1964. (Cited on page 10)   \n[68] Quentin Rebjock and Nicolas Boumal. Fast convergence to non-isolated minima: four equivalent conditions for c2 functions. arXiv preprint arXiv:2303.00096, 2023. (Cited on page 2)   \n[69] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional Networks for Biomedical Image Segmentation. In Medical Image Computing and Computer-Assisted Intervention \u2013 MICCAI 2015, Cham, 2015. (Cited on page 9)   \n[70] Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks. In Proceedings of the 35th International Conference on Machine Learning, 2018. (Cited on page 1)   \n[71] Karthik A. Sankararaman, Soham De, Zheng Xu, W. Ronny Huang, and Tom Goldstein. The impact of neural network overparameterization on gradient confusion and stochastic gradient descent. In Proceedings of the 37st International Conference on Machine Learning, 2020. (Cited on pages 1 and 7)   \n[72] Zhiqiang Shen, Tianhua Tao, Liqun Ma, Willie Neiswanger, Joel Hestness, Natalia Vassilieva, Daria Soboleva, and Eric Xing. Slimpajama-dc: Understanding data combinations for llm training. arXiv preprint arXiv:2309.10818, 2023. (Cited on pages 9 and 41)   \n[73] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv: 1409.1556, 2015. (Cited on page 1)   \n[74] Chaehwan Song, Ali Ramezani-Kebrya, Thomas Pethick, Armin Eftekhari, and Volkan Cevher. Subquadratic overparameterization for shallow neural networks. In Advances in Neural Information Processing Systems, 2021. (Cited on page 3)   \n[75] Hoang Tran, Qinzi Zhang, and Ashok Cutkosky. Empirical tests of optimization assumptions in deep learning. arXiv preprint arXiv:2407.01825, 2024. (Cited on page 4)   \n[76] Hossein Valavi, Sulin Liu, and Peter Ramadge. Revisiting the landscape of matrix factorization. In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, 2020. (Cited on pages 5 and 22)   \n[77] Lou Van den Dries and Chris Miller. Geometric categories and o-minimal structures. 1996. (Cited on page 2)   \n[78] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 2017. (Cited on page 9)   \n[79] Sharan Vaswani, Francis Bach, and Mark Schmidt. Fast and faster convergence of sgd for overparameterized models (and an accelerated perceptron). In Proceedings of 22nd International Conference on Artificial Intelligence and Statistics (AISTATS 2019), 2019. (Cited on page 1)   \n[80] Luca Venturi, Afonso S. Bandeira, and Joan Bruna. Spurious valleys in one-hidden-layer neural network optimization landscapes. Journal of Machine Learning Research, 2019. (Cited on page 3)   \n[81] Rachel Ward and Tamara Kolda. Convergence of alternating gradient descent for matrix factorization. Advances in Neural Information Processing Systems, 36:22369\u201322382, 2023. (Cited on page 5)   \n[82] Rachel Ward, Xiaoxia Wu, and Leon Bottou. AdaGrad stepsizes: Sharp convergence over nonconvex landscapes. In Proceedings of the 36th International Conference on Machine Learning, 2019. (Cited on page 8)   \n[83] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv: 1708.07747, 2017. (Cited on pages 8 and 37)   \n[84] Rui Yuan, Robert M Gower, and Alessandro Lazaric. A general sample complexity analysis of vanilla policy gradient. In International Conference on Artificial Intelligence and Statistics, 2022. (Cited on page 3)   \n[85] Jure Zbontar, Florian Knoll, Anuroop Sriram, Matthew J. Muckley, Mary Bruno, Aaron Defazio, Marc Parente, Krzysztof J. Geras, Joe Katsnelson, Hersh Chandarana, Zizhao Zhang, Michal Drozdzal, Adriana Romero, Michael G. Rabbat, Pascal Vincent, James Pinkerton, Duo Wang, Nafissa Yakubova, Erich Owens, C. Lawrence Zitnick, Michael P. Recht, Daniel K. Sodickson, and Yvonne W. Lui. fastmri: An open dataset and benchmarks for accelerated MRI. arXiv preprint arXiv: 1811.08839. (Cited on page 9)   \n[86] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. Communications of the ACM, 2016. (Cited on page 1)   \n[87] Hui Zhang and Wotao Yin. Gradient methods for convex minimization: better rates under weaker conditions. arXiv preprint arXiv: 1303.4645, 2013. (Cited on page 2)   \n[88] Yi Zhou and Yingbin Liang. Critical points of linear neural networks: Analytical forms and landscape properties. In Proceedings of International Conference on Learning Representations (ICLR 2018), 2018. (Cited on pages 1 and 3)   \n[89] Yi Zhou, Junjie Yang, Huishuai Zhang, Yingbin Liang, and Vahid Tarokh. SGD converges to global minimum in deep learning via star-convex path. In Proceedings of International Conference on Learning Representations (ICLR 2019), 2019. (Cited on page 4) ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1 Introduction 1 ", "page_idx": 16}, {"type": "text", "text": "2 Related work 2   \n2.1 Function classes in optimization 2   \n2.2 Limitations of existing conditions 3 ", "page_idx": 16}, {"type": "text", "text": "3 The proposed $_{\\alpha}$ - $\\beta$ -condition 3.1 Theoretical verification of the $_{\\alpha}$ - $_\\beta$ -condition 5 ", "page_idx": 16}, {"type": "text", "text": "4 Theoretical convergence of algorithms 6   \n4.1 Convergence under the $_{\\alpha}$ - $_{\\cdot\\beta}$ -condition 6   \nExperimental validation of the $_{\\alpha}$ - $_\\beta$ -condition 8   \n5.1 MLP architecture 8   \n5.2 CNN architecture 8   \n5.3 Resnet architecture 9   \n5.4 Training of AlgoPerf workloads and transformers for language modeling 9   \n5.5 Additional experiments . 10 ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "6 Conclusion, potential extensions, and limitations. 10 ", "page_idx": 16}, {"type": "text", "text": "A Additional explanation on PL assumption 19 ", "page_idx": 16}, {"type": "text", "text": "B Additional examples 19 ", "page_idx": 16}, {"type": "text", "text": "C Missing proofs 19 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Proofs of examples satisfying definition 1 19   \nC.1.1 Proof of example 1 20   \nC.1.2 Proof of example 2 20   \nC.1.3 Proof of Example 3 21   \nC.1.4 Proof of example 6 22   \nC.1.5 Proof of example 4 22   \nC.1.6 Proof of example 5 23   \nC.2 Convergence of optimization algorithms under $_{\\alpha}$ - $\\beta$ -condition 24   \nC.2.1 Convergence of SGD 24   \nC.2.2 Convergence of SGD with Polyak Stepsize 26   \nC.2.3 Convergence of NGN 30   \nC.2.4 Convergence of AdaGrad-norm-max 34   \nD Additional experiments 36   \nD.1 Half space learning 37   \nD.2 Experiment setup from section 2.2 37   \nD.3 MLP architecture 37   \nD.4 CNN architecture 38   \nD.5 Resnet architecture 39   \nD.6 Verification of $_{\\alpha}$ - $\\beta$ -condition by different optimizers 39   \nD.7 Increasing the depth of Resnet architecture . 40   \nD.8 AlgoPerf experiments . 40   \nD.9 Pythia experiments 41 ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "42 ", "page_idx": 17}, {"type": "image", "img_path": "h0a3p5WtXU/tmp/db9aa9344fe848573111ef1137601cbfb3e5e2dad30a1c8390cb7c00e1605280.jpg", "img_caption": ["Figure 8: Training of 3 layer LSTM model that shows Aiming condition does not always hold. We plot possible values of PL constant for different powers $\\delta$ in (12). We observe that possible values of $\\mu$ are of order $10^{-9}-10^{-7}$ which implies slow theoretical convergence and contradicts practical observations. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "A Additional explanation on PL assumption ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Let us consider the relaxation of $\\mathrm{PL}$ condition for some $\\delta\\in[1,2]$ and $\\mu>0$ (Assumption 3 in [22]) ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\nabla f(x)\\|^{\\delta}\\geq(2\\mu)^{\\delta/2}(f(x)-f^{*})\\quad{\\mathrm{~for~all~}}x\\in\\mathbb{R}^{d}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Taking logarithms of both sides we continue ", "page_idx": 18}, {"type": "equation", "text": "$$\n2\\log(\\|\\nabla f(x)\\|)-{\\frac{2}{\\delta}}\\log(f(x)-f^{*})\\geq\\log(2\\mu).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To satisfy PL condition for some $\\delta$ , we need to find $\\mu$ that satisfies (13) for all iterations. In Figure 8 we plot LHS of (13). We observe that the values of $\\mu$ that satisfy (13) for all iterations should be of order $10^{-9}-10^{-7}$ that leads to slow theoretical convergence [37]. Therefore, we claim that PL condition does not hold globally for neural networks. Nevertheless, it might hold locally around the solution (the value of $\\mu$ closer to the end of the training are large enough). ", "page_idx": 18}, {"type": "text", "text": "B Additional examples ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we list additional examples that satisfy Definition 1 and might have non-optimal stationary points. The functions of this form are typically used to simulate non-convex optimization problems and test the performance of algorithms on small or synthetic datasets [34, 55, 38]. ", "page_idx": 18}, {"type": "text", "text": "Example 6. Let $f_{1},f_{2}\\colon\\ensuremath{\\mathbb{R}}^{2}\\to\\ensuremath{\\mathbb{R}}$ be such that ", "page_idx": 18}, {"type": "equation", "text": "$$\nf_{1}(x,y)=\\frac{x^{2}+y^{2}}{1+x^{2}+y^{2}},\\ \\ \\ f_{2}(x,y)=\\frac{(x-1)^{2}+(y-1)^{2}}{1+(x-1)^{2}+(y-1)^{2}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "then Definition 1 holds with $\\begin{array}{r}{\\alpha\\,\\geq\\,\\frac{2}{\\operatorname*{min}_{i}\\operatorname*{min}_{(z,t)\\in{\\mathcal S}}f_{i}(z,t)}\\,\\approx\\,41.369325}\\end{array}$ and $\\beta\\,=\\,\\alpha\\mathrm{~-~}1$ . Besides, $\\beta=0$ not satisfy Definition 1 with $\\beta=0$ ", "page_idx": 18}, {"type": "text", "text": "C Missing proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1 Proofs of examples satisfying definition 1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We highlight that we only show that $\\alpha$ - $\\beta$ -condition holds for some $\\alpha,\\beta$ without giving all possible values of them. ", "page_idx": 18}, {"type": "text", "text": "C.1.1 Proof of example 1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Example 1. Let $f,f_{1},f_{2}\\colon\\mathbb{R}^{2}\\rightarrow\\mathbb{R}$ be such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f=\\frac{1}{2}(f_{1}+f_{2})\\quad\\mathrm{with}\\quad f_{1}(x,y)=\\frac{(x+y)^{2}}{(x+y)^{2}+1},\\quad f_{2}(x,y)=\\frac{(x+y+1)^{2}}{(x+y+1)^{2}+1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "then Definition 1 holds with $\\alpha\\in[5/2,+\\infty)$ and $\\beta\\in[4\\alpha/5,\\alpha)$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Let $(z,t)=\\mathrm{Proj}((x,y),S)$ . One can show that the set of minimizers of $f$ in this example is $S=\\{(x,y)\\colon y=-x-1/2\\}$ . Moreover, $f_{1}^{*}=0$ for $x=-y$ . We will show that $\\alpha{-}\\beta$ -condition holds for $i=1$ . For $i=2$ it can be shown in similar way with change of variables. We have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\partial_{x}f_{1}(x,y)=\\partial_{y}f_{1}(x,y)=\\frac{2(x+y)}{(1+(x+y)^{2})^{2}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, we need to show that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{2(x+y)}{(1+(x+y)^{2})}(x-z+y-t)\\geq(\\alpha-\\beta)\\frac{(x+y)^{2}}{1+(x+y)^{2}}-\\alpha f_{i}(z,t).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note hat $\\begin{array}{r}{f_{i}(z,t)=\\frac{(z+t)^{2}}{1+(z+t)^{2}}=\\frac{(-1/2)^{2}}{1+(-1/2)^{2}}=\\frac{1}{5}}\\end{array}$ . Moreover, we can compute the projection operator onto $\\boldsymbol{S}$ . After simple derivations, the projection can be expressed as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname{Proj}((x,y),S)={\\frac{1}{2}}\\left({x-y-1/2}\\atop-x+y-1/2\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, we need to satisfy ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{2(x+y)}{(1+(x+y)^{2})^{2}}(x+y+1/2)\\geq(\\alpha-\\beta)\\frac{(x+y)^{2}}{1+(x+y)^{2}}-\\frac{\\alpha}{5}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This is equivalent to ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{2(x+y)(x+y+^{1/2})\\geq(\\alpha-\\beta)(x+y)^{2}(1+(x+y)^{2})-\\cfrac{\\alpha}{5}(1+(x+y)^{2})^{2}}\\\\ {\\Leftrightarrow2(x+y)^{2}+\\cfrac{\\alpha}{5}(1+2(x+y)^{2}+(x+y)^{4})\\geq(\\alpha-\\beta)((x+y)^{2}+(x+y)^{4})-(x+y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We should satisfy the above for all values of $x+y$ . First, we need the coefficient next to $(x+y)^{4}$ in LHS to be larger or equal of the corresponding coefficient in RHS. This gives us $\\textstyle{\\frac{\\alpha}{5}}\\geq\\alpha-\\beta$ . This shows that $\\beta$ can not be zero. Using $2a b\\leq a^{2}+b^{2}$ , (15) is satisfied as long as we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n2(x+y)^{2}+{\\frac{\\alpha}{5}}(1+2(x+y)^{2}+(x+y)^{4})\\geq(\\alpha-\\beta)((x+y)^{2}+(x+y)^{4})+{\\frac{1}{2}}(x+y)^{2}+{\\frac{1}{2}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, we should satisfy ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\mathrm{coefficient~next~to~}(x+y)^{4}:{\\frac{\\alpha}{5}}\\geq\\alpha-\\beta}\\\\ {\\mathrm{coefficient~next~to~}(x+y)^{2}:2+{\\frac{2\\alpha}{5}}\\geq\\alpha-\\beta+{\\frac{1}{2}}}\\\\ {\\mathrm{coefficient~next~to~}1:{\\frac{\\alpha}{5}}\\geq{\\frac{1}{2}}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that if the first inequality holds, then the second one is true as well. Therefore, from the first and third inequalities we get that the solution of the system of inequalities is $\\alpha\\geq\\frac52$ and $\\beta\\in[4\\alpha/5,\\alpha]$ . ", "page_idx": 19}, {"type": "text", "text": "C.1.2 Proof of example 2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Example 2. Let $f,f_{1},f_{2}\\colon\\mathbb{R}^{2}\\rightarrow\\mathbb{R}$ be such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f=\\frac{1}{2}(f_{1}+f_{2})\\quad\\mathrm{with}\\quad f_{1}(x,y)=1-e^{-x^{2}-y^{2}},\\quad f_{2}(x,y)=1-e^{-(x-2)^{2}-(y-2)^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "then Definition 1 holds for some $\\alpha$ and $\\beta=\\alpha-8$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Let $(z,t)=\\mathrm{Proj}((x,y),S)$ . Again, we show that $\\alpha{-}\\beta{}.$ -condition holds for $f_{1}$ ; for $f_{2}$ it can be proved with the same arguments with change of variables. Note that $f_{i}^{*}=0$ and $f^{*}>0$ . We also observe that $\\boldsymbol{S}$ contains two points in total, and both of them of the form $(t,t)$ where for one $t$ is close to 0 (it is around $t\\approx0.00067)$ and for second $t$ is close to 2 (it is around $t\\approx1.99932$ ). Besides, note that $c:=\\operatorname*{min}_{i}\\operatorname*{min}_{(t,t)\\in\\mathcal{S}}f_{i}(t,t)\\in(0,1)$ and $c\\approx9\\cdot10^{-7}$ , which means that we are close to interpolation. ", "page_idx": 20}, {"type": "text", "text": "We have in this case ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\partial_{x}f_{1}(x,y)=2x\\exp(-x^{2}-y^{2}),\\quad\\partial_{y}f_{1}(x,y)=2y\\exp(-x^{2}-y^{2}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, we need to satisfy ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\exp(-x^{2}-y^{2})(2x(x-t)+2y(y-t))\\geq(\\alpha-\\beta)(1-\\exp(-x^{2}-y^{2}))-\\alpha f_{1}(z,t).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that if $\\beta\\,=\\,0$ and $x,y\\to+\\infty$ , then the inequality is obviously can not be satisfied for all $x,y\\in\\;\\mathbb{R}$ . Indeed, assume $\\beta\\;=\\;0$ , then since $f_{1}\\bar{(z,t)}<\\,1$ and LHS goes to 0 while RHS to $\\alpha-\\alpha f_{1}(z,t)>0.$ . ", "page_idx": 20}, {"type": "text", "text": "Rearranging terms in (16), we should satisfy ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\exp(-x^{2}-y^{2})(2x^{2}+2y^{2}+\\alpha-\\beta)+\\alpha f_{1}(z,t)\\ge\\exp(-x^{2}-y^{2})(2x t+2y t)+(\\alpha-\\beta).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using $2a b\\leq a^{2}+b^{2}$ , the above is satisfied if the following inequality holds ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\exp(-x^{2}-y^{2})(2x^{2}+2y^{2}+\\alpha-\\beta)+\\alpha c\\geq\\exp(-x^{2}-y^{2})(x^{2}+y^{2}+2t^{2})+(\\alpha-\\beta)}\\\\ &{\\exp(-x^{2}-y^{2})(x^{2}+y^{2}+\\alpha-\\beta)+\\alpha c\\geq\\exp(-x^{2}-y^{2})\\cdot2t^{2}+(\\alpha-\\beta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $t\\in(0,2)$ we can choose $\\alpha-\\beta=8$ , so that $x^{2}+y^{2}-2t^{2}+\\alpha-\\beta\\geq x^{2}+y^{2}\\geq0$ . Finally, we choose $\\begin{array}{r}{\\alpha\\geq\\frac{8}{c}}\\end{array}$ , so that $\\alpha c\\geq\\alpha-\\beta=8$ . We can estimate that $\\alpha\\gtrsim72\\cdot10^{7}$ . \u53e3 ", "page_idx": 20}, {"type": "text", "text": "C.1.3 Proof of Example 3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Example 3. Let $f,f_{1},f_{2}\\colon\\mathbb{R}^{2}\\rightarrow\\mathbb{R}$ be such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f=\\frac{1}{2}(f_{1}+f_{2})\\quad\\mathrm{with}\\quad f_{1}(x,y)=\\frac{1+x^{2}+y^{2}}{4+x^{2}+y^{2}},\\quad f_{2}(x,y)=\\frac{(x-2.5)^{2}+(y-2.5)^{2}}{4+(y-2.5)^{2}+(y-2.5)^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "then Definition 1 holds for some $\\alpha$ and $\\beta=\\alpha-1$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. Let $(z,t)=\\mathrm{Proj}((x,y),S)$ . For this example, we have $f_{1}^{*}=\\textstyle{\\frac{1}{4}}$ (achieved at $(0,0),$ ), $f_{2}^{*}=0$ (achieved at (2.5, 2.5)), and $f^{*}\\approx=0.408$ (achieved at (2.471, 2.471)). Besides, we have $f(0,0)=$ 0.587963 and $f(2.5,2.5)=0.409091$ . Besides, $\\begin{array}{r}{c:=\\operatorname*{min}_{i}\\operatorname*{min}_{(z,t)\\in\\mathcal{S}}f_{i}(z,t)=f_{2}(2.471,2.471)=}\\end{array}$ 0.00167918. Let us show that $\\alpha{-}\\beta$ -condition holds for $f_{1}$ ; for $f_{2}$ it can be shown similarly. We have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\partial_{x}f_{1}(x,y)={\\frac{6x}{(4+x^{2}+y^{2})^{2}}},\\quad\\partial_{y}f_{1}(x,y)={\\frac{6y}{(4+x^{2}+y^{2})^{2}}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, we need to satisfy for some $\\alpha$ and $\\beta$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{6}{(4+x^{2}+y^{2})^{2}}(x(x-z)+y(y-t))\\ge(\\alpha-\\beta)\\frac{1+x^{2}+y^{2}}{4+x^{2}+y^{2}}-f_{1}(z,t)}\\\\ &{6x^{2}+6y^{2}-6x z-6y t\\ge(\\alpha-\\beta)(1+x^{2}+y^{2})(4+x^{2}+y^{2})-\\alpha f_{1}(z,t)(4+x^{2}+y^{2})^{2}}\\\\ &{6(x^{2}+y^{2})+\\alpha f_{1}(z,t)(16+8(x^{2}+y^{2})+(x^{2}+y^{2})^{2})\\ge6x z+6y t}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+(\\alpha-\\beta)(4+5(x^{2}+y^{2})+(x^{2}+y^{2})^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The above is satisfied for all $x,y\\in\\mathbb{R}$ and $\\alpha-\\beta=1$ if we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{6(x^{2}+y^{2})+\\alpha f_{1}(z,t)(16+8(x^{2}+y^{2})+(x^{2}+y^{2})^{2})\\geq3(x^{2}+y^{2})+3z^{2}+3t^{2}}\\\\ {+(4+5(x^{2}+y^{2})+(x^{2}+y^{2})^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "To satisfy the above for all $x,y\\in\\mathbb{R}$ we should have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\begin{array}{l l}{16\\alpha f_{1}(z,t)}&{\\geq4+3(z^{2}+t^{2})}\\\\ {6+8\\alpha f_{1}(z,t)}&{\\geq3+5}\\\\ {\\alpha f_{1}(z,t)}&{\\geq1,}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $z=t=2.471$ , and $f_{1}(z,t)=0.0016718$ . We can satisfy $\\alpha$ - $\\beta$ -condition with $\\alpha\\gtrsim1512.4586$ . ", "page_idx": 20}, {"type": "text", "text": "C.1.4 Proof of example 6 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Example 6. Let $f_{1},f_{2}\\colon\\ensuremath{\\mathbb{R}}^{2}\\to\\ensuremath{\\mathbb{R}}$ be such that ", "page_idx": 21}, {"type": "equation", "text": "$$\nf_{1}(x,y)=\\frac{x^{2}+y^{2}}{1+x^{2}+y^{2}},\\ \\ \\ f_{2}(x,y)=\\frac{(x-1)^{2}+(y-1)^{2}}{1+(x-1)^{2}+(y-1)^{2}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "then Definition 1 holds with $\\begin{array}{r}{\\alpha\\,\\geq\\,\\frac{2}{\\operatorname*{min}_{i}\\operatorname*{min}_{(z,t)\\in{\\mathcal S}}f_{i}(z,t)}\\,\\approx\\,41.369325}\\end{array}$ and $\\beta\\,=\\,\\alpha\\mathrm{~-~}1$ . Besides, Definition 1 does not hold with $\\beta=0$ not satisfy Definition 1 with $\\beta=0$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. Let $(z,t)=\\mathrm{Proj}((x,y),S)$ . We show that $\\alpha{-}\\beta$ -condition holds for $f_{1}$ ; for $f_{2}$ the proof is similar with change of variables. In this case, $f_{i}^{*}=0$ , while $f^{\\ast}\\,>\\,0$ $(f^{*}\\approx0.316988$ at points (0.159375, 0.159375) and (0.840625, 0.840625)). Besides, we have $c:=\\operatorname*{min}_{i}\\operatorname*{min}_{(z,t)\\in S}f_{i}(z,t)\\approx$ $0.048345>0.$ . Moreover, We have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\partial_{x}f_{1}(x,y)={\\frac{2x}{(1+x^{2}+y^{2})^{2}}},\\quad\\partial_{y}f_{1}(x,y)={\\frac{2y}{(1+x^{2}+y^{2})^{2}}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, we need to satisfy for some $\\alpha$ and $\\beta$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{(1+x^{2}+y^{2})^{2}}(2x(x-z)+2y(y-t))\\geq(\\alpha-\\beta)\\frac{x^{2}+y^{2}}{1+x^{2}+y^{2}}-\\alpha f_{1}(z,t)}\\\\ {\\displaystyle2x^{2}+2y^{2}-2x z-2y t\\geq(\\alpha-\\beta)(x^{2}+y^{2})(1+x^{2}+y^{2})-\\alpha f_{1}(z,t)(1+x^{2}+y^{2})^{2}}\\\\ {\\displaystyle2x^{2}+2y^{2}+\\alpha f_{1}(z,t)(1+2(x^{2}+y^{2})+(x^{2}+y^{2})^{2})\\geq2x z+2y t+}\\\\ {\\displaystyle(\\alpha-\\beta)(x^{2}+y^{2})(1+x^{2}+y^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that if we take $\\beta=0$ and $x,y\\to+\\infty$ , then LHS grows as $\\alpha f_{1}(z,t)(x^{2}+y^{2})^{2}$ and RHS grows as $\\alpha(x^{2}+y^{2})^{2}$ , therefore in the limit the RHS is larger. This implies that $\\beta\\neq0$ . Using $2a b\\leq a^{\\frac{\\vee}{2}}\\!+\\!b^{2}$ the above is satisfied if we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(2+2\\alpha f_{1}(z,t))(x^{2}+y^{2})+\\alpha f_{1}(z,t)(x^{2}+y^{2})^{2}+\\alpha f_{1}(z,t)\\geq(x^{2}+y^{2})(1+\\alpha-\\beta)+}\\\\ {z^{2}+t^{2}+(\\alpha-\\beta)(x^{2}+y^{2})^{2}.~~(1+\\alpha-\\beta)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let us take $\\alpha-\\beta=1$ and mini min(z,2t)\u2208S fi(z,t) \u224841.369325. With this choice, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z^{2}+t^{2}\\leq2\\leq\\alpha f_{1}(z,t),}\\\\ &{1+\\alpha-\\beta=2\\leq2+2\\alpha f_{1}(z,t),}\\\\ &{\\alpha-\\beta=1<2\\leq\\alpha f_{1}(z,t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, LHS is always larger than RHS in (17). ", "page_idx": 21}, {"type": "text", "text": "C.1.5 Proof of example 4 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Example 4. Let $f_{i},f_{i j}$ be such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f(W,S)=\\frac{1}{2n m}\\|X-W^{\\top}S\\|_{\\mathrm{F}}^{2}=\\frac{1}{2n m}\\sum_{i,j}(X_{i j}-w_{i}^{\\top}s_{j})^{2},\\quad f_{i j}(W,S)=\\frac{1}{2}(X_{i j}-w_{i}^{\\top}s_{j})^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $X\\,\\in\\,\\mathbb{R}^{n\\times m}$ $\\mathbf{\\Delta},W\\,=\\,(w_{i})_{i=1}^{n}\\,\\in\\,\\mathbb{R}^{k\\times n},S\\,=\\,(s_{j})_{j=1}^{m}\\,\\in\\,\\mathbb{R}^{k\\times m}$ , and $\\operatorname{rank}(X)\\,=\\,r\\,\\geq\\,k$ . We assume that $X$ is generated using matrices $W^{*}$ and $S^{*}$ with non-zero additive noise that minimize empirical loss, namely, $X=(W^{*})^{\\top}S^{*}{+}(\\varepsilon_{i j})_{i\\in[n],j\\in[m]}$ where $W^{*},S^{*}=\\mathrm{argmin}_{W,S}\\,f(W,S)$ . Let $\\mathcal{X}$ be any bounded set that contains $\\boldsymbol{S}$ . Then Definition 1 is satisfied with $\\alpha=\\beta+1$ and some $\\beta>0$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. Since $k\\le r$ , then the matrix factorization problem has a unique solution which can be found from SVD decomposition of $X$ [76]. ", "page_idx": 21}, {"type": "text", "text": "We have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\nabla_{w_{i}}f_{i j}(W,S)=(w_{i}^{\\top}s_{j}-X_{i j})s_{j},\\quad\\nabla_{s_{j}}f_{i j}(W,S)=(w_{i}^{\\top}s_{j}-X_{i j})w_{i}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, we need to show that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle(w_{i}^{\\top}s_{j}-X_{i j})s_{j},w_{i}-w_{i}^{*}\\right\\rangle+\\left\\langle(w_{i}^{\\top}s_{j}-X_{i j})w_{i},s_{j}-s_{j}^{*}\\right\\rangle\\geq\\frac{\\alpha-\\beta}{2}(X_{i j}-w_{i}^{\\top}s_{j})^{2}}\\\\ &{\\quad-\\;\\frac{\\alpha}{2}(X_{i j}-(w_{i}^{*})^{\\top}s_{j}^{*})^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "since $f_{i j}^{*}=0$ . Since $f_{i j}$ is convex w.r.t. $w_{i}$ , the above holds if we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac12(w_{i}^{\\top}s_{j}-X_{i j})^{2}-\\frac12(s_{j}^{\\top}w_{i}^{*}-X_{i j})^{2}+\\frac12(w_{i}^{\\top}s_{j}-X_{i j})^{2}-\\frac12(w_{i}^{\\top}s_{j}^{*}-X_{i j})^{2}\\geq}\\\\ &{\\displaystyle\\quad\\frac{\\alpha-\\beta}{2}(X_{i j}-w_{i}^{\\top}s_{j})^{2}-\\frac{\\alpha}{2}(X_{i j}-(w_{i}^{*})^{\\top}s_{j}^{*})^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let us take $\\alpha=\\beta+1$ , then we can simplify the above as follows ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{1}{2}(w_{i}^{\\top}s_{j}-X_{i j})^{2}+\\frac{\\alpha}{2}(X_{i j}-(w_{i}^{*})^{\\top}s_{j}^{*})^{2}\\geq\\frac{1}{2}(s_{j}^{\\top}w_{i}^{*}-X_{i j})^{2}+\\frac{1}{2}(w_{i}^{\\top}s_{j}^{*}-X_{i j})^{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since we consider $\\mathcal{X}$ to be bounded, then there exist $r\\geq0$ such that $\\|s_{j}\\|,\\|w_{i}\\|\\leq r$ for all $i$ and $j$ . Therefore, the RHS in (18) is bounded by some constant $C$ . From the data generation, we have that $c=\\mathrm{min}_{i j}(X_{i j}-(w_{i}^{*})^{\\top}s_{j}^{*})^{2}=\\mathrm{min}_{i j}\\,\\varepsilon_{i j}^{\\bar{2}}>0$ . Therefore, we can take $\\alpha\\geq\\frac{2C}{c}$ to verify (18). ", "page_idx": 22}, {"type": "text", "text": "C.1.6 Proof of example 5 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Example 5. Consider training a two-layer neural network with a logistic loss ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f=\\frac{1}{n}\\sum_{i=1}^{n}f_{i},\\quad f_{i}(W,v)=\\phi(y_{i}\\cdot v^{\\top}\\sigma(W x_{i}))+\\lambda_{1}\\|v\\|^{2}+\\lambda_{2}\\|W\\|_{\\mathrm{F}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for a classification problem where $\\phi(t):=\\log(1+\\exp(-t))$ , $W\\,\\in\\,\\mathbb{R}^{k\\times d},v\\,\\in\\,\\mathbb{R}^{k}$ , $\\sigma$ is a ReLU function applied coordinate-wise, $y_{i}\\in\\{-1,+1\\}$ is a label and $x_{i}\\in\\mathbb{R}^{d}$ is a feature vector. Let $\\mathcal{X}$ be any bounded set that contains $\\boldsymbol{S}$ . Then the $\\alpha{-}\\beta$ -condition holds in $\\mathcal{X}$ for some $\\alpha\\geq1$ and $\\beta=\\alpha-1$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. Let $(Z,z):=\\operatorname{Proj}((W,v),S)$ . We have the following derivations for gradients ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathrm{~}}&{\\mathbb{R}^{k\\times d}\\ni\\nabla_{W}f_{i}(W,v)=\\phi^{\\prime}(y_{i}v^{\\top}\\sigma(W x_{i}))\\cdot y_{i}(v\\circ\\mathbb{1}_{W x_{i}\\geq0})x_{i}^{\\top}+2\\lambda_{2}W,}\\\\ &{\\mathbb{R}^{k}\\ni\\nabla_{v}f_{i}(W,v)=\\phi^{\\prime}(y_{i}v^{\\top}\\sigma(W x_{i}))\\cdot y_{i}\\sigma(W x_{i})=\\phi^{\\prime}(y_{i}v^{\\top}\\sigma(W x_{i}))\\cdot y_{i}(W x_{i})\\circ\\mathbb{1}_{W x_{i}\\geq0}}&\\\\ &{\\quad}&{\\quad+2\\lambda_{1}v}&\\\\ &{\\quad}&{\\quad=\\phi^{\\prime}(y_{i}v^{\\top}\\sigma(W x_{i}))\\cdot y_{i}(W x_{i})\\circ e_{w}+2\\lambda_{1}v,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we denote $e_{w}\\,:=\\,\\mathbb{1}_{W x_{i}\\geq0}\\,\\in\\,\\mathbb{R}^{k}$ . Besides, we denote $e_{z}:=\\mathbb{1}_{Z x_{i}\\geq0}\\,\\in\\,\\mathbb{R}^{k}$ . Note that the optimal value of $f_{i}^{*}>0$ because of the L2 regularization. ", "page_idx": 22}, {"type": "text", "text": "Note that we have the following relations ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v^{\\top}\\sigma(W x_{i})=\\langle v,(W x_{i})\\circ\\mathbb{1}_{W x_{i}\\geq0}\\rangle=\\langle v\\circ\\mathbb{1}_{W x_{i}\\geq0},W x_{i}\\rangle=\\langle v\\circ e_{w},W x_{i}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that $\\boldsymbol{S}$ is bounded because of the L2 regularization. Since we assume that the interpolation does not hold, then $f$ is always strictly larger than $f_{i}^{*}$ in $\\mathcal{X}$ , and due to continuity there exists $c:=\\mathrm{min}_{i\\in[n]}\\,\\mathrm{min}_{(Z,z)\\in S}\\;f_{i}(Z,z)>0$ . ", "page_idx": 22}, {"type": "text", "text": "We need to show that there exist some $\\alpha$ and $\\beta$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\langle\\nabla_{W}f_{i}(W,v),W-Z\\rangle_{\\mathbb{F}}+\\langle\\nabla_{v}f_{i}(W,v),v-z\\rangle}\\\\ &{\\qquad\\qquad\\geq\\alpha\\big(f_{i}(W,v)-f_{i}(Z,z)\\big)-\\beta\\big(f_{i}(W,v)-f_{i}^{*}\\big)}\\\\ &{\\Rightarrow\\phi^{\\prime}\\big(y_{i}(v\\circ e_{w})^{\\top}W x_{i}\\big)y_{i}\\left(\\big\\langle(v\\circ e_{w})x_{i}^{\\top},W-Z\\big\\rangle+\\langle(W x_{i})\\circ e_{w},v-z\\rangle\\big)+2\\lambda_{1}\\langle v,v-z\\rangle}\\\\ &{\\qquad\\qquad\\geq\\alpha\\big[\\phi_{i}(v\\circ e_{w})^{\\top}W x_{i}\\big)+\\lambda_{1}\\|v\\|^{2}+\\lambda_{2}\\|W\\|_{\\mathbb{F}}^{2}-\\phi(y_{i}(z\\circ e_{z})^{\\top}Z x_{i})-\\lambda_{1}\\|z\\|^{2}-\\lambda_{2}\\|Z\\|_{\\mathbb{F}}^{2}\\,}\\\\ &{\\qquad\\qquad-\\beta\\left[\\phi(\\eta_{i}(v\\circ e_{w})^{\\top}W x_{i})+\\lambda_{1}\\|v\\|^{2}+\\lambda_{2}\\|W\\|_{\\mathbb{F}}^{2}-f_{i}^{*}\\right]}\\\\ &{\\Rightarrow\\phi^{\\prime}\\big(y_{i}(v\\circ e_{w})^{\\top}W x_{i}\\big)\\left(|y_{i}(v\\circ e_{w})^{\\top}W x_{i}-y_{i}(v\\circ e_{w})^{\\top}Z x_{i}\\big|+\\big[y_{i}(v\\circ e_{w})^{\\top}W x_{i}-y_{i}(z\\circ e_{w})^{\\top}W}\\\\ &{\\qquad\\qquad\\geq\\alpha\\big[\\phi(y_{i}(v\\circ e_{w})^{\\top}W x_{i})+\\lambda_{1}\\|v\\|^{2}+\\lambda_{2}\\|W\\|_{\\mathbb{F}}^{2}-\\phi(y_{i}(z\\circ e_{z})^{\\top}Z x_{i})-\\lambda_{1}\\|z\\|^{2}-\\lambda_{2}\\|Z\\|_{\\mathbb{F}}^{2}\\right]}\\\\ &{\\qquad\\qquad-\\beta\\left[\\phi(y_{i}(v\\circ e_{w})^{\\top}W x_{i})+\\lambda_{1}\\|v\\|^{2}+\\lambda_{2}\\|W\\|_{\\mathbb{F}}^{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Algorithm 1 SGD with constant stepsize ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1: Input: Stepsize $\\gamma$   \n2: for $k=0,1,2,\\ldots,K-1$ do   \n3: Sample $i_{k}\\sim\\mathrm{Unif}[n]$ and compute $\\nabla f_{i_{k}}(x^{k})$   \n4: Update model ", "page_idx": 23}, {"type": "equation", "text": "$$\nx^{k+1}=x^{k}-\\gamma\\nabla f_{i_{k}}(x^{k})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "5: end for ", "page_idx": 23}, {"type": "text", "text": "where we use (19). Since $\\phi$ is convex, then we have $\\phi^{\\prime}(x)(x-y)\\geq\\phi(x)-\\phi(y)$ . Therefore, using (19) again, we get that (20) is satisfied if we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{2\\phi(y_{i}(v\\circ e_{w})^{\\top}W x_{i})-\\phi(y_{i}(v\\circ e_{w})^{\\top}Z x_{i})-\\phi(y_{i}(z\\circ e_{w})^{\\top}W x_{i})+\\lambda_{1}\\langle v,v-z\\rangle+\\lambda_{2}\\langle W,W-Z_{i}(v\\circ e_{w})^{\\top}W x_{i}\\rangle}\\\\ &{}&{\\geq\\alpha\\left[\\phi(y_{i}(v\\circ e_{w})^{\\top}W x_{i})+\\lambda_{1}\\|v\\|^{2}+\\lambda_{2}\\|W\\|_{\\mathrm{F}}^{2}-\\phi(y_{i}(z\\circ e_{z})^{\\top}Z x_{i})-\\lambda_{1}\\|z\\|^{2}-\\lambda_{2}\\|Z\\|_{\\mathrm{F}}^{2}\\right]}\\\\ &{}&{\\qquad-\\beta\\left[\\phi(y_{i}(v\\circ e_{w})^{\\top}W x_{i})+\\lambda_{1}\\|v\\|^{2}+\\lambda_{2}\\|W\\|_{\\mathrm{F}}^{2}-f_{i}^{\\ast}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now we take $\\alpha=\\beta+1$ and simplify the above as follows ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi(y_{i}(v\\circ e_{w})^{\\top}W x_{i})+\\alpha\\phi(y_{i}(z\\circ e_{w})^{\\top}Z x_{i}))+\\lambda_{1}(2\\langle v,v-z\\rangle-\\|v\\|^{2}-\\|z\\|^{2})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\lambda_{2}(2\\langle W,W-Z\\rangle-\\|W\\|_{\\mathrm{F}}^{2}-\\|Z\\|_{\\mathrm{F}}^{2})+(\\alpha-1)(\\lambda_{1}\\|z\\|^{2}+}\\\\ &{\\qquad\\geq\\phi(y_{i}(v\\circ e_{w})^{\\top}Z x_{i})+\\phi(y_{i}(z\\circ e_{w})^{\\top}W x_{i})+(\\alpha-1)f_{i}^{*}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The above is satisfied if we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi(y_{i}(v\\circ e_{w})^{\\top}W x_{i})+\\alpha c+\\lambda_{1}\\|v-z\\|^{2}+\\lambda_{2}\\|W-Z\\|_{\\mathrm{F}}^{2}+(\\alpha-1)(\\|z\\|^{2}+\\|Z\\|_{\\mathrm{F}}^{2})}\\\\ &{\\geq\\phi(y_{i}(v\\circ e_{w})^{\\top}Z x_{i})+\\phi(y_{i}(z\\circ e_{w})^{\\top}W x_{i})+(\\alpha-1)f_{i}^{*},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "because we also have $\\phi(y_{i}(z\\circ e_{z})^{\\top}Z x_{i})\\geq c$ for all $i$ and $(Z,z)\\in S$ . Since $(W,v)\\in\\mathcal{X}$ , there exist constants $R,r\\geq0$ such that $\\|v\\|\\leq r$ and $\\|W\\|\\leq R$ , and RHS in (22) is bounded by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{i\\in[n]}{\\operatorname*{max}}\\log(1+\\exp(R r\\|x_{i}\\|))\\ge\\phi\\big(y_{i}(v\\circ e_{w})^{\\top}Z x_{i}\\big)+\\phi\\big(y_{i}(z\\circ e_{w})^{\\top}W x_{i}\\big).}\\\\ &{\\mathrm{,~we~can~take~}\\alpha\\ge\\Big\\{\\frac{2\\operatorname*{max}_{i\\in[n]}\\log(1+\\exp(R r\\|x_{i}\\|))}{c-f_{i}^{*}},1\\Big\\}\\;\\mathrm{and}\\;\\beta=\\alpha-1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Remark 3. The result suggest that the choice $\\beta=0$ is possible only if constants $r$ and $R$ are small, i.e. locally around $\\boldsymbol{S}$ only. In order to satisfy $\\alpha{-}\\beta{}.$ -condition in larger set $\\mathcal{X}$ , one needs to choose $\\beta>0$ . ", "page_idx": 23}, {"type": "text", "text": "C.2 Convergence of optimization algorithms under $\\alpha{-}\\beta$ -condition ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "C.2.1 Convergence of SGD ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Constant stepsize. In this section, we present the proof of convergence of SGD with constant stepsize under $\\alpha{-}\\beta$ -condition for completeness of the presentation. ", "page_idx": 23}, {"type": "text", "text": "Theorem 1. Assume that Assumptions 1-2 hold. Then the iterates of SGD (Alg. 1) with stepsize $\\begin{array}{r}{\\gamma\\le\\frac{\\alpha-\\beta}{2L}}\\end{array}$ satisfy ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{0\\leq k<K}\\mathbb{E}\\left[f(x^{k})-f^{*}\\right]\\leq\\frac{\\mathbb{E}\\left[\\mathrm{dist}(x^{0},S)^{2}\\right]}{K}\\frac{1}{\\gamma(\\alpha-\\beta)}+\\frac{2L\\gamma}{\\alpha-\\beta}\\sigma_{\\mathrm{int}}^{2}+\\frac{2\\beta}{\\alpha-\\beta}\\sigma_{\\mathrm{int}}^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Let $x_{p}^{k}\\in\\operatorname{Proj}(x^{k},S)$ satisfies Definition 1. Using smoothness we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{k}\\left[\\mathrm{dist}(x^{k+1},S)^{2}\\right]}&{\\leq\\;\\;\\mathbb{E}_{k}\\left[\\|x^{k+1}-x_{p}^{k}\\|^{2}\\right]}\\\\ &{=\\;\\;\\mathrm{dist}(x^{k},S)^{2}-2\\gamma\\mathbb{E}_{k}\\left[\\left\\langle\\nabla f_{i_{k}}(x^{k}),x^{k}-x_{p}^{k}\\right\\rangle\\right]+\\gamma^{2}\\mathbb{E}_{k}\\left[\\|\\nabla f_{i_{k}}(x^{k})\\|^{2}\\right]}\\\\ &{\\overset{(i)}{\\leq}\\;\\;\\mathrm{dist}(x^{k},S)^{2}-2\\alpha\\gamma\\mathbb{E}_{k}\\left[f_{i_{k}}(x^{k})-f_{i_{k}}(x_{p}^{k})\\right]+2\\beta\\gamma\\mathbb{E}_{k}\\left[f_{i_{k}}(x^{k})-f_{i_{k}}^{\\ast}\\right]}\\\\ &{\\;\\;+\\;2L\\gamma^{2}\\mathbb{E}_{k}\\left[f_{i_{k}}(x^{k})-f_{i_{k}}^{\\ast}\\right]}\\\\ &{=\\;\\;\\mathrm{dist}(x^{k},S)^{2}-2\\alpha\\gamma\\mathbb{E}_{k}\\left[f_{i_{k}}(x^{k})-f_{i_{k}}(x_{p}^{k})\\right]}\\\\ &{\\;\\;+\\;2\\gamma(\\beta+L\\gamma)\\mathbb{E}_{k}\\left[\\left(f_{i_{k}}(x^{k})-f_{i_{k}}(x_{p}^{k})\\right)+\\left(f_{i_{k}}(x_{p}^{k})-f_{i_{k}}^{\\ast}\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $(i)$ holds because of the $\\alpha{-}\\beta$ -condition and smoothness. Now we need to choose a stepsize $\\begin{array}{r}{\\gamma\\le\\frac{\\alpha-\\beta}{2L}}\\end{array}$ to get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}_{k}\\left[\\mathrm{dist}(x^{k+1},\\mathcal{S})^{2}\\right]}&{\\leq}&{\\mathrm{dist}(x^{k},\\mathcal{S})^{2}-\\gamma(\\alpha-\\beta)\\mathbb{E}_{k}\\left[f_{i_{k}}(x^{k})-f_{i_{k}}(x_{p}^{k})\\right]}\\\\ &&{+\\;2\\gamma(\\beta+L\\gamma)\\mathbb{E}_{k}\\left[f_{i_{k}}(x_{p}^{k})-f_{i_{k}}^{\\ast}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Taking full expectation, noting that $\\boldsymbol{x}_{p}^{k}$ is independent of the randomness of $i_{k}$ , and performing simple derivations, we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{0\\leq k<K}\\mathbb{E}\\left[f(x^{k})-f^{*}\\right]\\leq\\frac{\\mathbb{E}\\left[\\mathrm{dist}(x^{0},S)^{2}\\right]}{K}\\frac{1}{\\gamma(\\alpha-\\beta)}+\\frac{2L\\gamma}{\\alpha-\\beta}\\sigma_{\\mathrm{int}}^{2}+\\frac{2\\beta}{\\alpha-\\beta}\\sigma_{\\mathrm{int}}^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Decreasing stepsize. Now we present the results with decreasing stepsize. ", "page_idx": 24}, {"type": "text", "text": "Theorem 4. Assume that Assumptions 1-2 hold. Then the iterates of SGD with decreasing stepsize $\\begin{array}{r}{\\gamma_{k}=\\frac{\\gamma_{0}}{\\sqrt{k+1}}}\\end{array}$ where $\\begin{array}{r}{\\gamma_{0}\\leq\\frac{\\alpha-\\beta}{2L}}\\end{array}$ satisfy ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\operatorname*{min}_{0\\le k<K}\\mathbb{E}\\left[f(x^{k})-f^{*}\\right]}&{\\le}&{\\displaystyle\\frac{5\\mathbb{E}\\left[\\mathrm{dist}(x^{0},S)^{2}\\right]}{4(\\alpha-\\beta)\\gamma_{0}\\sqrt{K}}+\\frac{5\\gamma_{0}L\\sigma_{\\mathrm{int}}^{2}}{\\alpha-\\beta}\\frac{\\log(K+1)}{\\sqrt{K}}+\\frac{2\\beta}{\\alpha-\\beta}\\sigma_{\\mathrm{int}}^{2}}\\\\ &{=}&{\\displaystyle\\widetilde{\\mathcal{O}}\\left(\\frac{1}{\\sqrt{K}}+\\beta\\sigma_{\\mathrm{int}}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. Similarly to the proof of constant stepsize SGD we can obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{\\mathbb{E}_{k}\\left[\\mathrm{dist}(x^{k+1},S)^{2}\\right]}}&{{\\leq}}&{{\\mathrm{dist}(x^{k},S)^{2}-\\gamma_{k}(\\alpha-\\beta)\\mathbb{E}_{k}\\left[f_{i_{k}}(x^{k})-f_{i_{k}}(x_{p}^{k})\\right]}}\\\\ {{}}&{{}}&{{+\\;2\\gamma_{k}(\\beta+L\\gamma_{k})\\mathbb{E}_{k}\\left[f_{i_{k}}(x_{p}^{k})-f_{i_{k}}^{*}\\right],}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "since $\\begin{array}{r}{\\gamma_{k}\\,\\le\\,\\gamma_{0}\\,\\le\\,\\frac{\\alpha-\\beta}{2L}}\\end{array}$ for all $k$ . Now we follow standard proof techniques [24] for decreasing stepsize. Taking full expectation and dividing both sides by $\\alpha-\\beta$ we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\gamma_{k}\\mathbb{E}\\left[f(x^{k})-f^{*}\\right]\\leq\\frac{\\mathbb{E}\\left[\\mathrm{dist}(x^{k},S)^{2}\\right]-\\mathbb{E}\\left[\\mathrm{dist}(x^{k+1},S)^{2}\\right]}{\\alpha-\\beta}+\\frac{2\\beta}{\\alpha-\\beta}\\gamma_{k}\\sigma_{\\mathrm{int}}^{2}+\\frac{2L}{\\alpha-\\beta}\\gamma_{k}^{2}\\sigma_{\\mathrm{int}}^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Summing the above from iteration 0 to $K-1$ leads to ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{k=0}^{K-1}\\gamma_{k}\\mathbb{E}\\left[f(x^{k})-f^{*}\\right]\\leq\\frac{\\mathbb{E}\\left[\\mathrm{dist}(x^{0},S)^{2}\\right]}{\\alpha-\\beta}+\\frac{2\\beta}{\\alpha-\\beta}\\sigma_{\\mathrm{int}}^{2}\\sum_{k=0}^{K-1}\\gamma_{k}+\\frac{2L}{\\alpha-\\beta}\\sigma_{\\mathrm{int}}^{2}\\sum_{k=0}^{K-1}\\gamma_{k}^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{0\\geq k<K}\\mathbb{E}\\left[f(x^{k})-f^{*}\\right]\\leq\\frac{\\mathbb{E}\\left[\\mathrm{dist}(x^{0},S)^{2}\\right]}{(\\alpha-\\beta)\\sum_{k=0}^{K-1}\\gamma_{k}}+\\frac{2\\beta}{\\alpha-\\beta}\\sigma_{\\mathrm{int}}^{2}+\\frac{2L}{\\alpha-\\beta}\\sigma_{\\mathrm{int}}^{2}\\frac{\\sum_{k=0}^{K-1}\\gamma_{k}^{2}}{\\sum_{k=0}^{K_{1}}\\gamma_{k}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Using the results of Theorem 5.7 [24] we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{k=0}^{K-1}\\gamma_{k}^{2}\\leq2\\gamma_{0}^{2}\\log(K+1),\\quad\\sum_{k=0}^{K-1}\\gamma_{k}\\geq\\frac{4\\gamma_{0}}{5}\\sqrt{K}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, the final rate we get is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\operatorname*{min}_{0\\ge k<K}\\mathbb{E}\\left[f(x^{k})-f^{*}\\right]}&{\\le}&{\\displaystyle\\frac{\\mathbb{E}\\left[\\mathrm{dist}(x^{0},S)^{2}\\right]}{(\\alpha-\\beta)\\frac{4}{5}\\gamma_{0}\\sqrt{K}}+\\frac{2\\beta}{\\alpha-\\beta}\\sigma_{\\mathrm{int}}^{2}+\\frac{2L}{\\alpha-\\beta}\\sigma_{\\mathrm{int}}^{2}\\frac{2\\gamma_{0}^{2}\\log(K+1)}{\\frac{4\\gamma_{0}}{5}\\sqrt{K}}}\\\\ &{=}&{\\displaystyle\\frac{5\\mathbb{E}\\left[\\mathrm{dist}(x^{0},S)^{2}\\right]}{4(\\alpha-\\beta)\\gamma_{0}\\sqrt{K}}+\\frac{5\\gamma_{0}L\\sigma_{\\mathrm{int}}^{2}}{\\alpha-\\beta}\\frac{\\log(K+1)}{\\sqrt{K}}+\\frac{2\\beta}{\\alpha-\\beta}\\sigma_{\\mathrm{int}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Algorithm 2 SPSmax: Stochastic Polyak Stepsize ", "page_idx": 25}, {"type": "text", "text": "1: Input: Stepsize parameter $c$ and stepsize upper bound $\\gamma_{\\mathrm{b}}$   \n2: for $k=0,1,2,\\ldots,K-1$ do   \n3: Sample $i_{k}\\sim\\mathrm{Unif}[n]$ and compute $\\nabla f_{i_{k}}(x^{k})$   \n4: Compute Polyak stepsize ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\gamma_{k}:=\\operatorname*{min}\\left\\{\\frac{f_{i_{k}}(x^{k})-f_{i_{k}}^{*}}{c\\|\\nabla f_{i_{k}}(x^{k})\\|^{2}},\\gamma_{\\mathrm{b}}\\right\\}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "5: Update model ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\boldsymbol{x}^{k+1}=\\boldsymbol{x}^{k}-\\gamma_{k}\\nabla f_{i_{k}}(\\boldsymbol{x}^{k})\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "6: end for ", "page_idx": 25}, {"type": "text", "text": "C.2.2 Convergence of SGD with Polyak Stepsize ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Constant stepsize parameter. In this section, we present the proof of convergence of SGD with Polyak stepsize (with constant stepsize parameters) under $\\alpha{-}\\beta$ -condition for completeness of the presentation. ", "page_idx": 25}, {"type": "text", "text": "Lemma 1 (Lemma from [53]). The $\\mathsf{S P S}_{\\mathrm{max}}$ stepsize satisfy ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\gamma_{k}^{2}\\|\\nabla f_{i_{k}}(x^{k})\\|^{2}\\leq\\frac{\\gamma_{k}}{c}(f_{i_{k}}(x^{k})-f_{i_{k}}^{*}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma 2 (Lemma from [53]). Assume each $f_{i}$ is $L$ -smooth, then $\\mathsf{S P S}_{\\mathrm{max}}$ stepsize satisfy ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\gamma_{\\mathrm{min}}:=\\operatorname*{min}\\left\\{\\frac{1}{2c L},\\gamma_{\\mathrm{b}}\\right\\}\\leq\\gamma_{k}\\leq\\gamma_{\\mathrm{b}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now we present the proof of Theorem 2 with constant stepsize parameters. ", "page_idx": 25}, {"type": "text", "text": "Theorem 2. Assume that Assumptions 1-2 hold. Then the iterates of $\\mathsf{S P S}_{\\mathrm{max}}$ (Alg. 2) with a stepsize hyperparameter $\\begin{array}{r}{c>\\frac{1}{2(\\alpha-\\beta)}}\\end{array}$ 2(\u03b1\u2212\u03b2) satisfy ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{0\\leq k<K}\\mathbb{E}\\left[f(x^{k})-f^{*}\\right]\\leq\\frac{c_{1}}{K}\\mathbb{E}\\left[\\mathrm{dist}(x^{0},S)^{2}\\right]+2\\alpha c_{1}\\gamma_{\\mathrm{b}}\\sigma_{\\mathrm{int}}^{2},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where \u03b3min := min{1/2cL, \u03b3b} and c1 :=\u03b3min(2(\u03b1c\u2212\u03b2)c\u22121). ", "page_idx": 25}, {"type": "text", "text": "Proof. Let $x_{p}^{k}\\in\\operatorname{Proj}(x^{k},S)$ that satisfies Definition 1, then we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{k}\\left[\\mathrm{dist}(x^{k+1},S)^{2}\\right]}&{\\leq}&{\\mathbb{E}_{k}\\left[\\|x^{k+1}-x_{p}^{k}\\|^{2}\\right]}\\\\ &{=}&{\\|x^{k}-x_{p}^{k}\\|^{2}-2\\mathbb{E}_{k}\\left[\\gamma_{k}\\left\\langle\\nabla f_{i_{k}}(x^{k}),x^{k}-x_{p}^{k}\\right\\rangle\\right]+\\mathbb{E}_{k}\\left[\\gamma_{k}^{2}\\big\\|\\nabla f_{i_{k}}(x^{k})\\big\\|^{2}\\right]}\\\\ &{\\overset{(i)}{\\leq}}&{\\mathrm{dist}(x^{k},S)^{2}-2\\mathbb{E}_{k}\\left[\\gamma_{k}\\big(f_{i_{k}}(x^{k})-f_{i_{k}}(x_{p}^{k})\\big)\\right]}\\\\ &{\\quad+2\\beta\\mathbb{E}_{k}\\left[\\gamma_{k}\\big(f_{i_{k}}(x^{k})-f_{i_{k}}^{\\ast}\\big)\\right]+\\frac{1}{C}\\mathbb{E}_{k}\\left[\\gamma_{k}\\big(f_{i_{k}}(x^{k})-f_{i_{k}}^{\\ast}\\big)\\right]}\\\\ &{=}&{\\mathrm{dist}(x^{k},S)^{2}-2\\alpha\\mathbb{E}_{k}\\left[\\gamma_{k}\\big([f_{i_{k}}(x^{k})-f_{i_{k}}^{\\ast}]-[f_{i_{k}}(x_{p}^{k})-f_{i_{k}}^{\\ast}]\\big)\\right]}\\\\ &{\\quad+2\\beta\\mathbb{E}_{k}\\left[\\gamma_{k}\\big(f_{i_{k}}(x^{k})-f_{i_{k}}^{\\ast}\\big)\\right]+\\frac{1}{C}\\mathbb{E}_{k}\\left[\\gamma_{k}\\big(f_{i_{k}}(x^{k})-f_{i_{k}}^{\\ast}\\big)\\right]}\\\\ &{=}&{\\mathrm{dist}(x^{k},S)^{2}-\\mathbb{E}_{k}\\left[\\gamma_{k}\\left(2\\alpha-2\\beta-\\frac{1}{C}\\right)\\left(f_{i_{k}}(x^{k})-f_{i_{k}}^{\\ast}\\right)\\right]}\\\\ &{\\quad+2\\alpha\\mathbb{E}_{k}\\left[\\gamma_{k}\\big(f_{i_{k}}(x_{p}^{k})-f_{\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $(i)$ follows from Lemma 1 and $\\alpha$ - $\\beta.$ -condition . Now, since $\\begin{array}{r}{c>\\frac{1}{2(\\alpha-\\beta)}}\\end{array}$ , we get that $2\\alpha-2\\beta-$ $1/c>0$ . Therefore, using Lemma 2 and the fact that $f_{i_{k}}^{*}\\leq f_{i_{k}}(x_{p}^{k})$ and $f_{i_{k}}^{*}\\leq f_{i_{k}}(x^{k})$ we get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\frac{\\cdot}{c}_{k}\\left[\\mathrm{dist}(x^{k+1},S)^{2}\\right]}&{\\leq}&{\\mathrm{dist}(x^{k},S)^{2}-\\gamma_{\\operatorname*{min}}\\left(2\\alpha-2\\beta-\\frac{1}{c}\\right)\\mathbb{E}_{k}\\left[f_{i_{k}}(x^{k})-f_{i_{k}}^{\\ast}\\right]}\\\\ &&{+\\;2\\alpha\\gamma_{\\mathbb{b}}\\mathbb{E}_{k}\\left[(f_{i_{k}}(x_{p}^{k})-f_{i_{k}}^{\\ast})\\right]}\\\\ &{=}&{\\mathrm{dist}(x^{k},S)^{2}-\\gamma_{\\operatorname*{min}}\\left(2\\alpha-2\\beta-\\frac{1}{c}\\right)\\mathbb{E}_{k}\\left[f_{i_{k}}(x^{k})-f_{i_{k}}(x_{p}^{k})\\right]}\\\\ &&{-\\;\\gamma_{\\operatorname*{min}}\\left(2\\alpha-2\\beta-\\frac{1}{c}\\right)\\mathbb{E}_{k}\\left[f_{i_{k}}(x_{p}^{k})-f_{i_{k}}^{\\ast}\\right]+2\\alpha\\gamma_{\\mathbb{b}}\\mathbb{E}_{k}\\left[(f_{i_{k}}(x_{p}^{k})-f_{i_{k}}^{\\ast})\\right]}\\\\ &{\\leq}&{\\mathrm{dist}(x^{k},S)^{2}-\\gamma_{\\operatorname*{min}}\\left(2\\alpha-2\\beta-\\frac{1}{c}\\right)\\mathbb{E}_{k}\\left[f_{i_{k}}(x^{k})-f_{i_{k}}(x_{p}^{k})\\right]}\\\\ &&{+\\;2\\alpha\\gamma_{\\mathbb{b}}\\mathbb{E}_{k}\\left[(f_{i_{k}}(x_{p}^{k})-f_{i_{k}}^{\\ast})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, noticing that $\\mathbb{E}_{k}\\left[f_{i_{k}}(x_{p}^{k})\\right]=f^{*}$ since $\\boldsymbol{x}_{p}^{k}$ is independent of $i_{k}$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\gamma_{\\mathrm{min}}\\left(2\\alpha-2\\beta-\\displaystyle\\frac{1}{c}\\right)\\mathbb{E}_{k}\\left[f(\\boldsymbol{x}^{k})-f^{*}\\right]}&{\\leq}&{\\mathrm{dist}(\\boldsymbol{x}^{k},\\boldsymbol{S})^{2}-\\mathbb{E}_{k}\\left[\\mathrm{dist}(\\boldsymbol{x}^{k+1},\\boldsymbol{S})^{2}\\right]+2\\alpha\\gamma_{b}\\sigma_{\\mathrm{int}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Dividing both sides by $\\gamma_{\\mathrm{min}}(2\\alpha-2\\beta-1/c)$ and taking full expectation, we get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}\\left[f(x^{k})-f^{*}\\right]}&{\\leq}&{\\displaystyle\\frac{c}{\\gamma_{\\operatorname*{min}}(2(\\alpha-\\beta)c-1)}\\left(\\mathbb{E}\\left[\\mathrm{dist}(x^{k},S)^{2}\\right]-\\mathbb{E}\\left[\\mathrm{dist}(x^{k+1},S)^{2}\\right]\\right)}\\\\ &&{\\displaystyle+\\;\\frac{2\\alpha c\\gamma_{\\mathrm{b}}}{\\gamma_{\\operatorname*{min}}(2(\\alpha-\\beta)c-1)}\\sigma_{\\operatorname*{int}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Averaging for $k\\in\\{0,\\ldots,K-1\\}$ we get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\underset{0\\leq k<K}{\\operatorname*{min}}\\mathbb{E}\\left[f(x^{k})-f^{*}\\right]}&{\\leq}&{\\frac{c}{\\gamma_{\\operatorname*{min}}\\left(2\\left(\\alpha-\\beta\\right)c-1\\right)K}\\mathbb{E}\\left[\\mathrm{dist}(x^{0},S)^{2}\\right]}\\\\ &&{\\displaystyle+\\,\\frac{2\\alpha c\\gamma_{\\mathrm{b}}}{\\gamma_{\\operatorname*{min}}\\left(2\\left(\\alpha-\\beta\\right)c-1\\right)}\\sigma_{\\mathrm{int}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "that finalizes the proof. ", "page_idx": 26}, {"type": "text", "text": "Decreasing stepsize parameter. Now we switch to the analysis of $\\mathsf{S P S}_{\\mathrm{max}}$ with decreasing stepsize parameters. We consider the stepsize DecSPS introduced in [64] ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\gamma_{k}=\\frac{1}{c_{k}}\\operatorname*{min}\\left\\{\\frac{f_{i_{k}}(x^{k})-f_{i_{k}}^{*}}{\\|\\nabla f_{i_{k}}(x^{k})\\|^{2}},c_{k-1}\\gamma_{k-1}\\right\\}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "with $c_{-1}=c_{0}$ and $\\gamma_{-1}=\\gamma_{b}>0$ to get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\gamma_{0}=\\frac{1}{c_{0}}\\operatorname*{min}\\left\\{\\frac{f_{i_{0}}(x^{0})-f_{i_{0}}^{*}}{\\|\\nabla f_{i_{0}}(x^{0})\\|^{2}},c_{0}\\gamma_{b}\\right\\}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Lemma 3 (Lemma 1 from [64]). Let Assumption 1 holds. Let $\\{c_{k}\\}_{k\\ge0}$ be any non-decreasing positive sequence of real numbers. Then we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\left\\{\\frac{1}{2c_{k}L},\\frac{c_{0}\\gamma_{b}}{c_{k}}\\right\\}\\leq\\gamma_{k}\\leq\\frac{c_{0}\\gamma_{b}}{c_{k}},\\quad\\mathrm{~and~}\\quad\\gamma_{k}\\leq\\gamma_{k-1}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Theorem 5. Assume that Assumptions 1-2 hold. Let $\\{c_{k}\\}_{k\\ge0}$ be any positive non-decreasing sequence such that $\\begin{array}{r}{c_{k}\\geq\\frac{1}{\\alpha-\\beta}}\\end{array}$ . Let Then iterates of DecSPS satisfy ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\operatorname*{min}_{0\\le k<K}\\mathbb{E}\\left[f(x^{k})-f^{*}\\right]}&{\\le}&{\\displaystyle\\frac{2\\widetilde{L}D^{2}c_{K-1}}{K(\\alpha-\\beta)}+\\frac{2\\beta}{\\alpha-\\beta}\\sigma_{\\mathrm{int}}^{2}+\\frac{1}{K}\\sum_{k=0}^{K-1}\\frac{\\sigma_{\\mathrm{int}}^{2}}{(\\alpha-\\beta)c_{k}},}\\\\ {\\displaystyle D^{2}:=\\displaystyle\\operatorname*{max}_{0\\le k\\le K}\\mathrm{dist}(x^{k},S)^{2},\\widetilde{L}:=\\operatorname*{max}\\left\\{L,\\frac{1}{2c_{0}\\gamma_{b}}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. From the definition of the stepsize we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\gamma_{k}\\leq\\frac{1}{c_{k}}\\frac{f_{i_{k}}(x^{k})-f_{i_{k}}^{*}}{\\|\\nabla f_{i_{k}}(x^{k})\\|^{2}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\gamma_{k}^{2}\\|\\nabla f_{i_{k}}(x^{k})\\|^{2}\\leq\\frac{\\gamma_{k}}{c_{k}}(f_{i_{k}}(x^{k})-f_{i_{k}}^{*}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Let $x_{p}^{k}=\\operatorname{Proj}(x^{k},S)$ which satisfies Definition 1. Now we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{\\mathrm{dist}(x^{k+1},S)^{2}}&{\\leq}&{\\|x^{k+1}-x_{p}^{k}\\|^{2}}\\\\ &{\\leq}&{\\mathrm{dist}(x^{k},S)^{2}-2\\gamma_{k}\\left\\langle\\nabla f_{i_{k}}(x^{k}),x^{k}-x_{p}^{k}\\right\\rangle+\\frac{\\gamma_{k}}{C_{k}}(f_{i_{k}}(x^{k})-f_{i_{k}}^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Using $\\alpha{-}\\beta$ -condition we get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathrm{dist}(x^{k+1},\\mathcal{S})^{2}}&{\\leq}&{\\|x^{k+1}-x_{p}^{k}\\|^{2}}\\\\ &{\\leq}&{\\mathrm{dist}(x^{k},\\mathcal{S})^{2}-2\\alpha\\gamma_{k}\\big(f_{i_{k}}(x^{k})-f_{i_{k}}(x_{p}^{k})\\big)+2\\beta\\gamma_{k}\\big(f_{i_{k}}(x^{k})-f_{i_{k}}^{*}\\big)}\\\\ &&{\\displaystyle+\\,\\frac{\\gamma_{k}}{c_{k}}\\big(f_{i_{k}}(x^{k})-f_{i_{k}}^{*}\\big)}\\\\ &{=}&{\\mathrm{dist}(x^{k},\\mathcal{S})^{2}-\\gamma_{k}\\left(2\\alpha-2\\beta-1/c_{k}\\right)\\big(f_{i_{k}}(x^{k})-f_{i_{k}}(x_{p}^{k})\\big)}\\\\ &&{\\displaystyle+\\,\\gamma_{k}(2\\beta+1/c_{k})\\big(f_{i_{k}}(x_{p}^{k})-f_{i_{k}}^{*}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Let us divide both sides by $\\gamma_{k}>0$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\frac{\\mathrm{dist}(x^{k+1},S)^{2}}{\\gamma_{k}}}&{\\leq}&{\\displaystyle\\frac{\\mathrm{dist}(x^{k},S)^{2}}{\\gamma_{k}}-(2\\alpha-2\\beta-1/c_{k})\\,\\big(f_{i_{k}}(x^{k})-f_{i_{k}}(x_{p}^{k})\\big)}\\\\ &&{\\displaystyle+\\,\\big(2\\beta+1/c_{k}\\big)\\big(f_{i_{k}}(x_{p}^{k})-f_{i_{k}}^{*}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since by hypothesis $\\begin{array}{r}{c_{k}\\geq\\frac{1}{\\alpha-\\beta}}\\end{array}$ , we get $2\\alpha-2\\beta-1/c_{k}\\geq\\alpha-\\beta$ . Therefore, we get ", "page_idx": 27}, {"type": "equation", "text": "$$\nf_{i_{k}}(x^{k})-f_{i_{k}}(x_{p}^{k})\\quad\\leq\\quad\\frac{\\mathrm{dist}(x^{k},S)^{2}}{(\\alpha-\\beta)\\gamma_{k}}-\\frac{\\mathrm{dist}(x^{k+1},S)^{2}}{(\\alpha-\\beta)\\gamma_{k}}+\\frac{(2\\beta+1/c_{k})}{\\alpha-\\beta}(f_{i_{k}}(x_{p}^{k})-f_{i_{k}}^{*}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Summing from $k=0$ to $K-1$ we get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{k=0}^{K-1}f_{i_{k}}(x^{k})-f_{i_{k}}(x_{p}^{k})}&{\\le}&{\\displaystyle\\sum_{k=0}^{K-1}\\frac{\\mathrm{dist}(x^{k},S)^{2}}{(\\alpha-\\beta)\\gamma_{k}}-\\displaystyle\\sum_{k=0}^{K-1}\\frac{\\mathrm{dist}(x^{k+1},S)^{2}}{(\\alpha-\\beta)\\gamma_{k}}}\\\\ &{\\quad+\\displaystyle\\sum_{k=0}^{K-1}\\frac{2\\beta}{\\alpha-\\beta}(f_{i_{k}}(x_{p}^{k})-f_{i_{k}}^{*})+\\displaystyle\\sum_{k=0}^{K-1}\\frac{1}{(\\alpha-\\beta)c_{k}}(f_{i_{k}}(x_{p}^{k})-f_{i_{k}}^{*})}\\\\ {\\le}&{\\displaystyle\\frac{\\mathrm{dist}(x^{0},S)^{2}}{(\\alpha-\\beta)\\gamma_{0}}+\\displaystyle\\sum_{k=1}^{K-1}\\frac{\\mathrm{dist}(x^{k},S)^{2}}{(\\alpha-\\beta)\\gamma_{k}}-\\displaystyle\\sum_{k=0}^{K-2}\\frac{\\mathrm{dist}(x^{k+1},S)^{2}}{(\\alpha-\\beta)\\gamma_{k}}}\\\\ &{\\quad-\\displaystyle\\frac{\\mathrm{dist}(x^{K},S)^{2}}{(\\alpha-\\beta)\\gamma_{K}-2}+\\displaystyle\\sum_{k=0}^{K-1}\\frac{2\\beta}{\\alpha-\\beta}(f_{i_{k}}(x_{p}^{k})-f_{i_{k}}^{*})}\\\\ &{\\quad+\\displaystyle\\sum_{k=0}^{K-1}\\frac{1}{(\\alpha-\\beta)c_{k}}(f_{i_{k}}(x_{p}^{k})-f_{i_{k}}^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Here we use the fact that $\\gamma_{k}$ is a non-increasing sequence with $k$ . We continue as follows ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{k=0}^{K-1}f_{k}(x^{k})-f_{k}(x_{p}^{k})}&{\\le\\displaystyle\\frac{\\mathrm{dist}(x^{0},S)^{2}}{(\\alpha-\\beta)\\gamma_{0}}+\\displaystyle\\sum_{k=0}^{K-2}\\left(\\frac{1}{\\gamma_{1+1}}-\\frac{1}{\\gamma_{k}}\\right)\\displaystyle\\frac{\\mathrm{dist}(x^{k+1},S)^{2}}{\\alpha-\\beta}}\\\\ &{\\phantom{\\le\\displaystyle\\sum_{k=0}^{K-1}\\frac{2\\beta}{\\alpha-\\beta}}(f_{k}(x_{p}^{k})-f_{k}^{\\ast})+\\displaystyle\\sum_{k=0}^{K-1}\\frac{1}{(\\alpha-\\beta)c_{k}}(f_{k}(x_{p}^{k})-f_{k}^{\\ast})}\\\\ {\\displaystyle}&{\\le\\displaystyle\\sum_{\\alpha-\\beta}^{D^{2}}\\left(\\frac{1}{\\gamma_{0}}+\\displaystyle\\sum_{k=0}^{K-2}\\left(\\frac{1}{\\gamma_{k+1}}-\\frac{1}{\\gamma_{k}}\\right)\\right)}\\\\ &{\\phantom{\\le\\displaystyle\\sum_{k=0}^{K-1}\\frac{2\\beta}{\\alpha-\\beta}}(f_{k}(x_{p}^{k})-f_{k}^{\\ast})+\\displaystyle\\sum_{k=0}^{K-1}\\frac{1}{(\\alpha-\\beta)c_{k}}(f_{k}(x_{p}^{k})-f_{k}^{\\ast})}\\\\ {\\displaystyle}&{\\le\\displaystyle\\sum_{\\left(\\alpha-\\beta\\right)\\gamma_{K-1}}^{D^{2}}\\displaystyle\\sum_{k=0}^{K-1}\\sum_{\\alpha=-\\beta}^{2\\beta}(f_{k}(x_{p}^{k})-f_{k}^{\\ast})}\\\\ &{\\phantom{\\le\\displaystyle\\sum_{k=0}^{K-1}\\frac{1}{\\alpha-\\beta}}(\\alpha-\\beta)\\epsilon_{k}(x_{p}^{k})-f_{k}^{\\ast}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Here we use the fact that $1/\\gamma_{k+1}-1/\\gamma_{k}\\geq0$ . From Lemma 3 we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{1}{\\gamma_{k}}\\leq c_{k}\\underbrace{\\operatorname*{max}\\left\\{2L,\\frac{1}{c_{0}\\gamma_{b}}\\right\\}}_{:=\\tilde{L}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Therefore, we continue as follows ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\frac{1}{K}\\sum_{k=0}^{K-1}f_{i_{k}}(x^{k})-f_{i_{k}}(x_{p}^{k})}&{\\leq}&{\\displaystyle\\frac{2\\widetilde{L}D^{2}c_{K-1}}{K(\\alpha-\\beta)}+\\frac{1}{K}\\sum_{k=0}^{K-1}\\frac{2\\beta}{\\alpha-\\beta}(f_{i_{k}}(x_{p}^{k})-f_{i_{k}}^{*})}\\\\ &&{\\displaystyle+\\,\\frac{1}{K}\\sum_{k=0}^{K-1}\\frac{1}{(\\alpha-\\beta)c_{k}}(f_{i_{k}}(x_{p}^{k})-f_{i_{k}}^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Taking expectation we get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{0\\leq k<K}\\mathbb{E}\\left[f(x^{k})-f^{*}\\right]\\;\\;\\leq\\;\\;\\frac{2\\tilde{L}D^{2}c_{K-1}}{K(\\alpha-\\beta)}+\\frac{2\\beta}{\\alpha-\\beta}\\sigma_{\\mathrm{int}}^{2}+\\frac{1}{K}\\sum_{k=0}^{K-1}\\frac{\\sigma_{\\mathrm{int}}^{2}}{(\\alpha-\\beta)c_{k}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Corollary 1. Let $c_{k}=\\sqrt{k+1}$ with $c_{-1}=c_{0}$ , then iterates of DecSPS satisfy ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\operatorname*{min}_{0\\leq k<K}\\mathbb{E}\\left[f(x^{k})-f^{*}\\right]}&{\\leq}&{\\displaystyle\\frac{2\\widetilde{L}D^{2}+2\\sigma_{\\mathrm{int}}^{2}}{\\sqrt{K}(\\alpha-\\beta)}+\\frac{2\\beta}{\\alpha-\\beta}\\sigma_{\\mathrm{int}}^{2}}\\\\ &{=}&{\\tilde{\\mathcal{O}}\\left(\\displaystyle\\frac{1}{\\sqrt{K}}+\\beta\\sigma_{\\mathrm{int}}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. The proof d\u221airectly follows from Theorem 4 using the choice $c_{k}={\\sqrt{k+1}}$ and the fact that $\\begin{array}{r}{\\sum_{k=0}^{K-1}k^{-1/2}\\le2\\sqrt{K}}\\end{array}$ . \u53e3 ", "page_idx": 28}, {"type": "text", "text": "Remark 4. It turned out that removing the bounded iterates assumption for DecSPS is a challenging task. Nevertheless, we believe that this is rather the technicalities of Polyak stepsize, but not of our assumption. Moreover, we highlight that [64] removed bounded iterates assumption in restrictive strongly convex setting. ", "page_idx": 28}, {"type": "text", "text": "Algorithm 3 NGN: Non-negative Gauss Newton ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1: Input: Stepsize parameter $\\gamma$   \n2: for $k=0,1,2,\\ldots,K-1$ do   \n3: Sample $i_{k}\\sim\\mathrm{Unif}[n]$ and compute $\\nabla f_{i_{k}}(x^{k})$   \n4: Compute NGN stepsize ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\gamma_{k}:=\\frac{\\gamma}{1+\\frac{\\gamma}{2f_{i_{k}}(x^{k})}\\|\\nabla f_{i_{k}}(x^{k})\\|^{2}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "5: Update model ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\boldsymbol{x}^{k+1}=\\boldsymbol{x}^{k}-\\gamma_{k}\\nabla f_{i_{k}}(\\boldsymbol{x}^{k})\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "6: end for ", "page_idx": 29}, {"type": "text", "text": "C.2.3 Convergence of NGN ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Constant stepsize parameter. In this section we present the proof of convergence of NGN with constant stepsize parameter $\\gamma$ under $\\alpha{-}\\beta$ -condition for completeness of the presentation. ", "page_idx": 29}, {"type": "text", "text": "Lemma 4 (Lemma from [63]). Let $f_{i}$ be $L$ -smooth for all $i$ , then the stepsize of NGN satisfies ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\gamma_{k}\\in\\left[\\frac{\\gamma}{1+\\gamma L},\\gamma\\right].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Lemma 5 (Lemma from [63]). Let $f_{i}$ be $L$ -smooth for all $i$ , then the iterates of NGN satisfy ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\gamma_{k}^{2}\\|\\nabla f_{i_{k}}(x^{k})\\|^{2}\\leq\\frac{4\\gamma L}{1+2\\gamma L}\\gamma_{k}(f_{i_{k}}(x^{k})-f_{i_{k}}^{\\ast})+\\frac{2\\gamma^{2}L}{1+\\gamma L}\\operatorname*{max}\\left\\{\\frac{2\\gamma L-1}{2\\gamma L+1},0\\right\\}f_{i_{k}}^{\\ast}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Theorem 3. Assume that Assumptions $^{\\,l}$ with $\\alpha\\ge\\beta+1$ and 1-2 hold. Assume that each function $f_{i}$ is positive and $\\sigma_{\\mathrm{pos}}^{2}<\\infty$ . Then the iterates of NGN (Alg. 3) with a stepsize parameter $\\gamma>0$ satisfy ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\operatorname*{min}_{0\\le k\\le K-1}\\mathbb{E}\\left[f(x^{k})-f^{*}\\right]}&{\\le}&{\\displaystyle\\frac{\\mathbb{E}\\left[\\mathrm{dist}(x^{0},S)^{2}\\right]}{2\\gamma K}\\frac{(1+2\\gamma L)^{2}}{c_{2}}+\\frac{3L\\gamma\\alpha(1+\\gamma L)\\sigma_{\\mathrm{int}}^{2}}{c_{2}}}\\\\ &&{\\displaystyle+\\,\\frac{\\gamma L}{a}\\operatorname*{max}\\left\\{2\\gamma L-1,0\\right\\}\\sigma_{\\mathrm{pos}}^{2}+\\frac{2\\beta\\sigma_{\\mathrm{int}}^{2}}{c_{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $c_{2}:=2\\gamma L(\\alpha-\\beta-1)+\\alpha-\\beta$ . ", "page_idx": 29}, {"type": "text", "text": "Proof. Let $x_{p}^{k}\\in\\operatorname{Proj}(x^{k},S)$ satisfying Definition 1. Then we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{E}_{k}\\left[\\mathrm{dist}(x^{k+1},S)^{2}\\right]}&{\\leq}&{\\mathbb{E}_{k}\\left[\\|x^{k+1}-x_{p}^{k}\\|^{2}\\right]}\\\\ &{=}&{\\|x^{k}-x_{p}^{k}\\|^{2}-2\\mathbb{E}_{k}\\left[\\gamma_{k}\\langle\\nabla f_{i_{k}}(x^{k}),x^{k}-x_{p}^{k}\\rangle\\right]+\\mathbb{E}_{k}\\left[\\gamma_{k}^{2}\\|\\nabla f_{i_{k}}(x^{k})\\|^{2}\\right]}\\\\ &{\\leq}&{\\mathrm{dist}(x^{k},S)^{2}-2\\alpha\\mathbb{E}_{k}\\left[\\gamma_{k}(f_{i_{k}}(x^{k})-f_{i_{k}}(x_{p}^{k}))\\right]}\\\\ &{}&{+\\,2\\beta\\mathbb{E}_{k}\\left[\\gamma_{k}\\big(f_{i_{k}}(x^{k})-f_{i_{k}}^{\\ast}\\big)\\right]+\\mathbb{E}_{k}\\left[\\gamma_{k}^{2}\\|\\nabla f_{i_{k}}(x^{k})\\|^{2}\\right].\\qquad\\qquad(3\\delta^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "From Lemma 5 ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\gamma_{k}^{2}\\|\\nabla f_{i_{k}}(x^{k})\\|^{2}\\leq\\frac{4\\gamma L}{1+2\\gamma L}\\gamma_{k}(f_{i_{k}}(x^{k})-f_{i_{k}}^{\\ast})+\\frac{2\\gamma^{2}L}{1+\\gamma L}\\operatorname*{max}\\left\\{\\frac{2\\gamma L-1}{2\\gamma L+1},0\\right\\}f_{i_{k}}^{\\ast}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Plugging (35) in (34) we get ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}_{k}\\left[\\mathrm{dist}(x^{k+1},S)^{2}\\right]}&{\\leq}&{\\mathrm{dist}(x^{k},S)^{2}-2\\alpha\\mathbb{E}_{k}\\left[\\gamma_{k}(f_{i_{k}}(x^{k})-f_{i_{k}}(x_{p}^{k}))\\right]}\\\\ &&{\\displaystyle+\\;2\\beta\\mathbb{E}_{k}\\left[\\gamma_{k}(f_{i_{k}}(x^{k})-f_{i_{k}}^{\\ast})\\right]\\frac{4\\gamma L}{1+2\\gamma L}+\\mathbb{E}_{k}\\left[\\gamma_{k}(f_{i_{k}}(x^{k})-f_{i_{k}}^{\\ast})\\right]}\\\\ &&{\\displaystyle+\\;\\frac{2\\gamma^{2}L}{1+\\gamma L}\\operatorname*{max}\\left\\{\\frac{2\\gamma L-1}{2\\gamma L+1},0\\right\\}\\mathbb{E}_{k}\\left[f_{i_{k}}^{\\ast}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We have $f_{i_{k}}(x^{k})-f_{i_{k}}(x_{p}^{k})=(f_{i_{k}}(x^{k})-f_{i_{k}}^{*})-(f_{i_{k}}(x_{p}^{k})-f_{i_{k}}^{*})$ . Now we write $\\gamma_{k}=\\rho+\\epsilon_{k}$ where $\\rho$ is a constant independent of $i_{k}$ . Therefore, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{k}\\left[\\left.\\operatorname{dist}(x^{k+1},S)^{2}\\right]}&{\\le\\;\\operatorname{dist}(x^{k},S)^{2}-2\\alpha\\rho\\mathbb{E}_{k}\\left[f_{k}(x^{k})-f_{k}(x^{k})\\right]}\\\\ &{\\quad-2\\alpha\\mathbb{E}_{k}\\left[e_{k}(f_{k+1}(x^{k})-f_{k+1}^{\\ast})\\right]+2\\alpha\\mathbb{E}_{k}\\left[e_{k}(f_{k}(x_{p}^{k})-f_{k+1}^{\\ast})\\right]}\\\\ &{\\quad+2\\beta\\mathbb{E}_{k}\\left[\\gamma_{k}(f_{k+1}(x^{k})-f_{k+1}^{\\ast})\\right]+\\frac{4\\gamma L}{1+2\\gamma L}\\mathbb{E}_{k}\\left[\\gamma_{k}(f_{k+1}(x^{k})-f_{k}^{\\ast})\\right]}\\\\ &{\\quad+\\left.\\frac{2\\gamma^{2}L}{1+\\gamma L}\\operatorname*{max}\\left\\{\\frac{2\\gamma L-1}{2\\gamma L+1},0\\right\\}\\mathbb{E}_{k}\\left[f_{k+1}^{\\ast}\\right]}\\\\ {=}&{\\operatorname{dist}(x^{k},S)^{2}-2\\alpha\\rho\\mathbb{E}_{k}\\left[f_{k}(x^{k})-f_{k}(x_{p}^{k})\\right]}\\\\ &{\\quad-2\\mathbb{E}_{k}\\left[\\left(\\alpha\\epsilon_{k}-\\left(\\beta+\\frac{2\\gamma L}{1+2\\gamma L}\\right)\\gamma_{k}\\right)(f_{k+1}(x^{k})-f_{k}^{\\ast})\\right]}\\\\ &{\\quad+2\\alpha\\mathbb{E}_{k}\\left[e_{k}(f_{k+1}(x_{p}^{k})-f_{k}^{\\ast})\\right]}\\\\ &{\\quad+\\frac{2\\gamma^{2}L}{1+\\gamma L}\\operatorname*{max}\\left\\{\\frac{2\\gamma L-1}{2\\gamma L+1},0\\right\\}\\mathbb{E}_{k}\\left[f_{k+1}^{\\ast}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We need to find $\\rho$ such that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\alpha\\epsilon_{k}-\\left(\\beta+\\frac{2\\gamma L}{1+2\\gamma L}\\right)\\gamma_{k}\\ge0}\\\\ {\\displaystyle\\alpha(\\gamma_{k}-\\rho)-\\left(\\beta+\\frac{2\\gamma L}{1+2\\gamma L}\\right)\\gamma_{k}\\ge0}\\\\ {\\displaystyle\\gamma_{k}\\left(\\alpha-\\beta-\\frac{2\\gamma L}{1+2\\gamma L}\\right)\\ge\\alpha\\rho.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Note that since $\\begin{array}{r}{\\gamma_{k}\\geq\\frac{\\gamma}{1+\\gamma L}}\\end{array}$ from Lemma 4, then it is enough if $\\rho$ satisfies ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\frac{\\gamma}{1+\\gamma L}\\left(\\alpha-\\beta-\\frac{2\\gamma L}{1+2\\gamma L}\\right)\\ge\\alpha\\rho}\\\\ {\\displaystyle\\frac{\\gamma(2\\gamma L(\\alpha-\\beta-1)+\\alpha-\\beta)}{\\alpha(1+\\gamma L)(1+2\\gamma L)}\\ge\\rho.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Let us take this bound as a value of $\\rho$ . Note that since $\\alpha\\ge\\beta+1$ , then $\\rho\\geq0$ . Therefore, the bound for $\\epsilon_{k}$ is the following ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\epsilon_{k}}&{=}&{\\gamma_{k}-\\rho}\\\\ &{\\leq}&{\\gamma-\\frac{\\gamma\\left(2\\gamma L\\left(\\alpha-\\beta-1\\right)+\\alpha-\\beta\\right)}{\\alpha\\left(1+\\gamma L\\right)\\left(1+2\\gamma L\\right)}}\\\\ &{=}&{\\gamma\\left(\\frac{\\alpha\\left(1+\\gamma L\\right)\\left(1+2\\gamma L\\right)-2\\gamma L\\left(\\alpha-\\beta-1\\right)-\\left(\\alpha-\\beta\\right)}{\\alpha\\left(1+\\gamma L\\right)\\left(1+2\\gamma L\\right)}\\right)}\\\\ &{=}&{L\\gamma^{2}\\frac{\\alpha+2\\alpha\\gamma L+2\\left(\\beta+1\\right)}{\\alpha\\left(1+\\gamma L\\right)\\left(1+2\\gamma L\\right)}+\\gamma\\frac{\\beta}{\\alpha\\left(1+\\gamma L\\right)\\left(1+2\\gamma L\\right)}}\\\\ &{\\leq}&{\\displaystyle\\frac{3L\\gamma^{2}}{1+2\\gamma L}+\\frac{\\beta\\gamma}{\\alpha\\left(1+\\gamma L\\right)\\left(1+2\\gamma L\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, we get the following descent inequality ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}_{k}\\left[\\mathrm{dist}(x^{k+1},S)^{2}\\right]}&{\\leq}&{\\mathrm{dist}(x^{k},S)^{2}-\\underbrace{2\\alpha\\rho}_{\\geq b_{k}}\\mathbb{E}_{k}\\left[f_{i_{k}}(x^{k})-f_{i_{k}}(x_{p}^{k})\\right]}\\\\ &&{\\quad+2\\alpha\\mathbb{E}_{k}\\left[\\epsilon_{k}(f_{i_{k}}(x_{p}^{k})-f_{i_{k}}^{*})\\right]+\\underbrace{\\frac{2\\gamma^{2}L}{1+\\gamma L}\\operatorname*{max}\\left\\{\\frac{2\\gamma L-1}{2\\gamma L+1},0\\right\\}\\mathbb{E}_{k}\\left[f_{i_{k}}^{*}\\right]}_{:=T_{3}(\\gamma^{2})}}\\\\ &{\\leq}&{\\mathrm{dist}(x^{k},S)^{2}-T_{0}(\\gamma)\\mathbb{E}_{k}\\left[f_{i_{k}}(x^{k})-f_{i_{k}}(x_{p}^{k})\\right]}\\\\ &&{+T_{2}(\\gamma^{2})\\alpha\\mathbb{E}_{k}\\left[f_{i_{k}}(x_{p}^{k})-f_{i_{k}}^{*}\\right]+\\beta T_{1}(\\gamma)\\alpha\\mathbb{E}_{k}\\left[f_{i_{k}}(x_{p}^{k})-f_{i_{k}}^{*}\\right]}\\\\ &&{+T_{3}(\\gamma^{2})\\mathbb{E}_{k}\\left[f_{i_{k}}^{*}\\right]}\\\\ &{=}&{\\mathrm{dist}(x^{k},S)^{2}-T_{0}(\\gamma)(f(x^{k})-f^{*})+T_{2}(\\gamma^{2})\\alpha\\mathbb{E}_{k}\\left[f^{*}-f_{i_{k}}^{*}\\right]}\\\\ &&{+\\beta T_{1}(\\gamma)\\alpha\\mathbb{E}_{k}\\left[f^{*}-f_{i_{k}}^{*}\\right]+T_{3}(\\gamma^{2})\\mathbb{E}_{k}\\left[f_{i_{k}}^{*}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Here we use the fact that $\\boldsymbol{x}_{p}^{k}$ is a minimizer of $f$ and independent of $i_{k}$ . Unrolling the recursion, we get ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{l l l}{\\displaystyle\\frac{1}{K}\\left(\\mathbb{E}\\left[\\mathrm{dist}(x^{K},S)^{2}\\right]-\\mathbb{E}\\left[\\mathrm{dist}(x^{0},S)^{2}\\right]\\right)}&{\\leq}&{-\\displaystyle\\frac{T_{0}(\\gamma)}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[f(x_{k})-f^{*}\\right]}\\\\ &&{\\displaystyle+\\;E_{1}(\\gamma)+E_{2}(\\gamma^{2}),}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{E_{1}(\\gamma):=\\beta T_{1}(\\gamma)\\alpha\\underbrace{\\mathbb{E}\\left[f^{*}-f_{i}^{*}\\right]}_{\\sigma_{\\mathrm{int}}^{2}},\\quad E_{2}(\\gamma^{2}):=T_{2}(\\gamma^{2})\\alpha\\underbrace{\\mathbb{E}\\left[f^{*}-f_{i}^{*}\\right]}_{\\sigma_{\\mathrm{int}}^{2}}+T_{3}(\\gamma^{2})\\underbrace{\\mathbb{E}\\left[f_{i}^{*}\\right]}_{\\sigma_{\\mathrm{pos}}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Therefore, we get ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\operatorname*{min}_{0\\le k\\le K-1}\\mathbb{E}\\left[f(x^{k})-f^{*}\\right]}&{\\le}&{\\displaystyle\\frac{\\mathbb{E}\\left[\\mathrm{dist}(x^{0},S)^{2}\\right]}{K T_{0}(\\gamma)}+\\displaystyle\\frac{E_{1}(\\gamma)}{T_{0}(\\gamma)}+\\displaystyle\\frac{E_{2}(\\gamma^{2})}{T_{0}(\\gamma)}}\\\\ &{\\le}&{\\displaystyle\\frac{\\mathbb{E}\\left[\\mathrm{dist}(x^{0},S)^{2}\\right]}{2\\gamma K}\\displaystyle\\frac{(1+2\\gamma L)^{2}}{2\\gamma L(\\alpha-\\beta-1)+\\alpha-\\beta}}\\\\ &&{\\displaystyle+\\,\\frac{3L\\gamma\\alpha}{2\\gamma L(\\alpha-\\beta-1)+\\alpha-\\beta}(1+\\gamma L)\\sigma_{\\mathrm{int}}^{2}}\\\\ &&{\\displaystyle+\\,\\frac{\\gamma L}{2\\gamma L(\\alpha-\\beta-1)+\\alpha-\\beta}\\operatorname*{max}\\left\\{2\\gamma L-1,0\\right\\}\\sigma_{\\mathrm{po}}^{2}}\\\\ &&{\\displaystyle+\\,\\frac{\\beta\\sigma_{\\mathrm{int}}^{2}}{2\\gamma L(\\alpha-\\beta-1)+\\alpha-\\beta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "This finished the proof. ", "page_idx": 31}, {"type": "text", "text": "Remark 5. If all $f_{i}$ are convex, i.e. we can take $\\alpha=1,\\beta=0$ in $\\alpha{-}\\beta$ -condition, then we get ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\operatorname*{min}_{0\\le k\\le K-1}\\mathbb{E}\\left[f(x^{k})-f^{*}\\right]}&{\\le}&{\\displaystyle\\frac{\\mathbb{E}\\left[\\mathrm{dist}(x^{0},S)^{2}\\right]}{2\\gamma K}(1+2\\gamma L)^{2}+3L\\gamma(1+\\gamma L)\\sigma_{\\mathrm{int}}^{2}}\\\\ &&{\\displaystyle+\\,\\gamma L\\operatorname*{max}\\left\\{2\\gamma L-1,0\\right\\}\\sigma_{\\mathrm{pos}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "that coincides with the results in [63]. ", "page_idx": 31}, {"type": "text", "text": "Decreasing stepsize parameter. Now we present the results with decreasing stepsize parameter. Theorem 6. Assume that Assumptions $^{\\,l}$ with $\\alpha\\ge\\beta+1$ and 1-2 hold. Assume that each function $f_{i}$ is positive and $\\sigma_{\\mathrm{pos}}^{2}<\\infty$ . Then the iterates of NGN with decreasing stepsize of the form ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\gamma_{k}=\\frac{\\widetilde{\\gamma}_{k}}{1+\\frac{\\widetilde{\\gamma}_{k}}{2f_{i_{k}}(x^{k})}\\|\\nabla f_{i_{k}}(x^{k})\\|^{2}},\\quad\\widetilde{\\gamma}_{k}:=\\frac{\\gamma}{\\sqrt{k+1}}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "satisfy ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\operatorname*{min}_{0\\le k<K}\\mathbb{E}\\left[f(x^{k})-f^{*}\\right]}&{\\le}&{\\displaystyle\\frac{C_{1}}{\\sqrt{K}}\\mathbb{E}\\left[\\mathrm{dist}(x^{0},S)^{2}\\right]+\\displaystyle\\frac{C_{2}\\log(K+1)}{\\sqrt{K}}\\alpha\\sigma_{\\mathrm{int}}^{2}+\\displaystyle\\frac{C_{3}}{\\alpha-\\beta}\\beta\\sigma_{\\mathrm{int}}^{2}}\\\\ &&{\\displaystyle+\\,\\frac{C_{4}\\log(K+1)}{\\sqrt{K}}\\sigma_{\\mathrm{pos}}^{2}}\\\\ &{=}&{\\displaystyle\\tilde{\\mathcal{O}}\\left(\\frac{1}{\\sqrt{K}}+\\beta\\sigma_{\\mathrm{int}}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $C_{1},C_{2},C_{3}$ , and $C_{4}$ are defined in (43). ", "page_idx": 32}, {"type": "text", "text": "Proof. Similarly to the proof with constant stepsize parameter we can obtain (similar to (37)) ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}_{k}\\left[\\mathrm{dist}(x^{k+1},S)^{2}\\right]}&{\\leq}&{\\mathrm{dist}(x^{k},S)^{2}-T_{0}(\\widetilde{\\gamma}_{k})(f(x^{k})-f^{*})+T_{2}(\\widetilde{\\gamma}_{k}^{2})\\alpha\\sigma_{\\mathrm{int}}^{2}}\\\\ &&{\\displaystyle+\\;\\beta T_{1}(\\widetilde{\\gamma}_{k})\\alpha\\sigma_{\\mathrm{int}}^{2}+T_{3}(\\widetilde{\\gamma}_{k}^{2})\\sigma_{\\mathrm{pos}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Note that we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{T_{0}(\\widetilde{\\gamma}_{k})}&{=\\frac{2\\widetilde{\\gamma}_{k}(2\\widetilde{\\gamma}_{k}L(2\\alpha-\\beta-1)+\\alpha-\\beta)}{(1+\\widetilde{\\gamma}_{k}L)(1+2\\widetilde{\\gamma}_{k}L)}}\\\\ &{\\geq\\frac{2\\widetilde{\\gamma}_{k}(\\alpha-\\beta)}{(1+\\widetilde{\\gamma}_{k})(1+2\\widetilde{\\gamma}_{0}L)}=:\\overline{{\\Gamma}}_{0}(\\widetilde{\\gamma}_{k}),}\\\\ {T_{1}(\\widetilde{\\gamma}_{k})}&{=\\frac{2\\widetilde{\\gamma}_{k}^{2}}{\\alpha(1+\\widetilde{\\gamma}_{k})(1+2\\widetilde{\\gamma}_{k}L)}}\\\\ &{\\leq\\frac{2\\widetilde{\\gamma}_{k}}{\\alpha}=:\\widetilde{\\Gamma}_{1}(\\widetilde{\\gamma}_{k}),}\\\\ {T_{2}(\\widetilde{\\gamma}_{k}^{2})}&{=\\frac{6\\widetilde{L}_{2}^{2}}{1+2\\widetilde{\\gamma}_{k}L}}\\\\ &{\\leq\\epsilon L\\widetilde{\\gamma}_{k}^{2}=\\frac{2\\widetilde{\\gamma}_{k}L}{2},}\\\\ {T_{3}(\\widetilde{\\gamma}_{k}^{2})}&{=\\frac{2\\widetilde{\\gamma}_{k}L}{1+\\widetilde{\\gamma}_{k}L},}\\\\ {\\leq}&{1\\frac{2\\widetilde{\\gamma}_{k}L}{\\gamma L},}\\\\ &{\\leq\\frac{2\\widetilde{\\gamma}_{k}^{2}}{1+\\widetilde{\\gamma}_{k}}\\frac{\\Gamma_{2}}{L},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Therefore, we can continue as follows ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}\\left[\\mathrm{dist}(x^{k+1},\\mathcal{S})^{2}\\right]}&{\\leq}&{\\mathbb{E}\\left[\\mathrm{dist}(x^{k},\\mathcal{S})^{2}\\right]-\\widetilde{T}_{0}(\\widetilde{\\gamma}_{k})\\mathbb{E}\\left[f(x^{k})-f^{*}\\right]+\\widetilde{T}_{2}(\\widetilde{\\gamma}_{k}^{2})\\alpha\\sigma_{\\mathrm{int}}^{2}}\\\\ &&{\\:+\\:\\beta\\widetilde{T}_{1}(\\widetilde{\\gamma}_{k})\\alpha\\sigma_{\\mathrm{int}}^{2}+\\widetilde{T}_{3}(\\widetilde{\\gamma}_{k}^{2})\\sigma_{\\mathrm{pos}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "This leads to ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\sum_{k=0}^{K-1}\\widetilde{T}_{0}(\\widetilde{\\gamma}_{k})\\mathbb{E}\\left[f(x^{k})-f^{*}\\right]}&{\\le}&{\\displaystyle\\mathbb{E}\\left[\\mathrm{dist}(x^{0},S)^{2}\\right]+\\displaystyle\\sum_{k=0}^{K-1}\\widetilde{T}_{2}(\\widetilde{\\gamma}_{k}^{2})\\alpha\\sigma_{\\mathrm{int}}^{2}+\\displaystyle\\sum_{k=0}^{K-1}\\beta\\widetilde{T}_{1}(\\widetilde{\\gamma}_{k})\\alpha\\sigma_{\\mathrm{int}}^{2}}\\\\ &&{\\displaystyle+\\sum_{k=0}^{K-1}\\widetilde{T}_{3}(\\widetilde{\\gamma}_{k}^{2})\\sigma_{\\mathrm{pos}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Therefore, we have ", "text_level": 1, "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\operatorname*{min}_{0\\leq k<K}\\mathbb{E}\\left[f(x^{k})-f^{*}\\right]}&{\\leq}&{\\displaystyle\\frac{1}{\\sum_{k=0}^{K-1}\\widetilde{T}_{0}(\\widetilde{\\gamma}_{k})}\\mathbb{E}\\left[\\mathrm{dist}(x^{0},S)^{2}\\right]+\\displaystyle\\frac{\\sum_{k=0}^{K-1}\\widetilde{T}_{2}(\\widetilde{\\gamma}_{k}^{2})}{\\sum_{k=0}^{K-1}\\widetilde{T}_{0}(\\widetilde{\\gamma}_{k})}\\alpha\\sigma_{\\mathrm{int}}^{2}}\\\\ &&{\\displaystyle+\\;\\frac{\\beta\\sum_{k=0}^{K-1}\\widetilde{T}_{1}(\\widetilde{\\gamma}_{k})}{\\sum_{k=0}^{K-1}\\widetilde{T}_{0}(\\widetilde{\\gamma}_{k})}\\alpha\\sigma_{\\mathrm{int}}^{2}+\\displaystyle\\frac{\\sum_{k=0}^{K-1}\\widetilde{T}_{3}(\\widetilde{\\gamma}_{k}^{2})}{\\sum_{k=0}^{K-1}\\widetilde{T}_{0}(\\widetilde{\\gamma}_{k})}\\sigma_{\\mathrm{pos}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Following the results of [64] and [24] we get ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\sum_{k=0}^{K-1}\\widetilde{T}_{0}(\\widetilde{x}_{k})}&{=\\;\\;\\sum_{k=0}^{K-1}\\frac{2\\widetilde{\\gamma}_{k}}{(1+\\gamma\\widetilde{L})(1+2\\gamma)L}}\\\\ &{\\ge\\;\\;\\frac{8\\pi\\sqrt{K}}{(1+\\gamma\\widetilde{L})(1+2\\gamma)L},}\\\\ &{\\overset{K-1}{\\sum}\\widetilde{T}_{2}(\\widetilde{z}_{3}^{\\prime})}&{=\\;\\;\\sum_{k=0}^{K-2}6\\widetilde{L}_{2}^{2}}\\\\ &{\\le12L^{2}\\mathrm{'og}(K+1),}\\\\ {\\sum_{k=0}^{K-1}\\widetilde{T}_{3}(\\widetilde{z}_{3}^{\\prime})}&{=\\;\\;\\sum_{k=0}^{K-1}2\\widetilde{\\gamma}_{k}^{2}L\\mathrm{max}\\{2\\gamma L-1,0\\}}\\\\ &{\\le\\;4\\gamma^{2}L\\mathrm{max}\\{2\\gamma L-1,0\\}\\log(K+1),}\\\\ {\\sum_{k=0}^{K-1}\\frac{\\widetilde{T}_{3}(\\widetilde{z}_{3}^{\\prime})}{2\\widetilde{\\gamma}_{k}(\\widetilde{\\gamma}_{k})}}&{=\\;\\;\\sum_{k=0}^{K-1}\\frac{2\\widetilde{\\gamma}_{k}}{2\\widetilde{\\gamma}_{k}(1+2\\gamma)L}}\\\\ &{=\\;\\;\\sum_{k=0}^{K-1}\\frac{2\\widetilde{\\gamma}_{k}^{2}}{2\\widetilde{\\gamma}_{k}(\\widetilde{\\gamma}_{k})}}\\\\ &{=\\;\\;(1+\\gamma\\widetilde{L})(1+2\\gamma L).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus, the final result can be written as follows ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{0\\leq k<K}{\\operatorname*{min}}\\mathbb{E}\\left[f(x^{k})-f^{*}\\right]}&{\\leq\\begin{array}{r l}{\\frac{5(1+\\gamma L)(1+2\\gamma L)}{8\\gamma\\sqrt{K}}\\mathbb{E}\\left[\\operatorname*{dist}(x^{0},S)^{2}\\right]+\\frac{12L\\gamma^{2}\\log(K+1)}{\\frac{8\\gamma\\sqrt{K}}{9(1+\\gamma L)(1+2\\gamma L)}}\\alpha\\sigma_{\\operatorname*{int}}^{2}}\\end{array}}\\\\ &{\\quad+\\frac{\\sum_{k=0}^{K-1}\\tilde{T}_{1}(\\gamma_{k})}{\\sum_{k=0}^{K-1}\\tilde{T}_{0}(\\gamma_{k})}\\alpha\\sigma_{\\operatorname*{int}}^{2}+\\sum_{k=0}^{K-1}\\tilde{T}_{3}(\\tilde{\\gamma}_{k}^{2})\\sigma_{\\operatorname*{ps}}^{2}}\\end{array}}\\\\ &{=\\begin{array}{r l}{\\frac{5(1+\\gamma L)(1+2\\gamma L)}{8\\gamma\\sqrt{K}}\\mathbb{E}\\left[\\operatorname*{dist}(x^{0},S)^{2}\\right]}\\\\ &{+\\frac{15L\\gamma(1+\\gamma L)(1+2\\gamma L)\\log(K+1)}{2\\sqrt{K}}\\alpha\\sigma_{\\operatorname*{int}}^{2}}\\end{array}}\\\\ &{\\quad+\\frac{(15L\\gamma L)(1+2\\gamma L)\\beta\\sigma_{\\operatorname*{int}}^{2}}{2(\\alpha-\\beta)}}\\\\ &{\\quad+\\frac{5(1+\\gamma L)(1+2\\gamma L)\\gamma L}{(\\alpha-\\beta)}\\alpha\\sigma_{\\operatorname*{int}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Now it is left to use the definitions of constants $C_{1},C_{2}$ , and $C_{3}$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{C_{1}:=\\frac{5(1+\\gamma L)(1+2\\gamma L)}{8\\gamma},}\\\\ &{}&\\\\ &{}&{C_{2}:=\\frac{15L\\gamma(1+\\gamma L)(1+2\\gamma L)}{2},}\\\\ &{}&{C_{3}:=(1+\\gamma L)(1+2\\gamma L),}\\\\ &{}&{C_{4}:=\\frac{5(1+\\gamma L)(1+2\\gamma L)\\gamma L\\operatorname*{max}\\{2\\gamma L-1,0\\}}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "C.2.4 Convergence of AdaGrad-norm-max ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Theorem 7. Assume that Assumptions 1-2 hold. Assume that for all $k\\geq0$ stochastic gradients satisfy \u2225\u2207fik(xk)\u22252 \u2264G2 for some G > 0 and b\u22121 \u2265\u03b12L\u2212\u03b3\u03b2 . Let $D^{2}=\\operatorname*{max}_{i}\\|\\nabla f_{i}(x^{0})\\|^{2}$ . Then the ", "page_idx": 33}, {"type": "text", "text": "Algorithm 4 AdaGrad-norm-max ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1: Input: Stepsize parameter $\\begin{array}{r}{\\gamma>0,c_{-1}=0,b_{-1}\\geq\\frac{2L\\gamma}{\\alpha-\\beta}}\\end{array}$   \n2: for $k=0,1,2,\\ldots,K-1$ do   \n3: Sample $i_{k}\\sim\\mathrm{Unif}[n]$ and compute $\\nabla f_{i_{k}}(x^{k})$   \n4: Compute AdaGrad-norm-max stepsize ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{c_{k}^{2}}&{:=}&{\\operatorname*{max}\\big\\{c_{k-1}^{2},\\|\\nabla f_{i_{k}}(x^{k})\\|^{2}\\big\\}}\\\\ {b_{k}^{2}}&{:=}&{b_{k-1}^{2}+c_{k}^{2},\\quad\\gamma_{k}=\\displaystyle\\frac{\\gamma}{b_{k}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "5: Update model ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\boldsymbol{x}^{k+1}=\\boldsymbol{x}^{k}-\\gamma_{k}\\nabla f_{i_{k}}(\\boldsymbol{x}^{k})\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "6: end for ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "iterates of AdaGrad-norm-max (Alg. 4) satisfy ", "text_level": 1, "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\operatorname*{min}_{0\\le k<K}\\mathbb{E}\\left[f(x^{k})-f^{*}\\right]}&{\\le}&{\\displaystyle\\frac{\\mathrm{dist}(x^{k},S)^{2}}{\\gamma K(\\alpha-\\beta)}\\sqrt{b_{-1}^{2}+G^{2}K}}\\\\ &&{\\displaystyle+\\,\\frac{2\\alpha}{(\\alpha-\\beta)K D^{2}}\\sigma_{\\mathrm{int}}^{2}\\sqrt{b_{-1}^{2}+G^{2}K}\\sqrt{b_{-1}^{2}+D^{2}(K+1)}}\\\\ &{=}&{\\displaystyle\\mathcal{O}\\left(\\frac{1}{\\sqrt{K}}+\\alpha\\sigma_{\\mathrm{int}}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We see that if $K$ is large enough, then we recover the standard convex rate of Adagrad of order $\\widetilde{\\mathcal{O}}(K^{-1/2})$ [46]. ", "page_idx": 34}, {"type": "text", "text": "Proof. Let $x_{p}^{k}=\\operatorname{Proj}(x^{k},S)$ satysfying Definition 1. Then we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname{list}(x^{k+1},S)^{2}}&{\\leq\\ \\|x^{k+1}-x_{p}^{k}\\|^{2}}\\\\ &{=\\ \\|x^{k}-x_{p}^{k}\\|^{2}-2\\gamma_{k}\\langle\\nabla f_{k}(x^{k}),x^{k}-x_{p}^{k}\\rangle+\\gamma_{k}^{k}\\|\\nabla f_{k}(x^{k})\\|^{2}}\\\\ &{\\leq\\ \\operatorname{dist}(x^{k},S)^{2}-2\\alpha\\gamma_{k}\\langle f_{k}(x^{k})-f_{k_{k}}(x^{k})\\rangle+2\\beta\\gamma_{k}\\langle f_{k_{k}}(x^{k})-f_{i_{k}}^{\\ast}\\rangle}\\\\ &{\\quad+\\ \\gamma_{k}^{k}\\|\\nabla f_{k}(x^{k})\\|^{2}}\\\\ &{\\leq\\ \\operatorname{dist}(x^{k},S)^{2}-2\\alpha\\gamma_{k}\\langle f_{k_{k}}(x^{k})-f_{k_{k}}(x^{k})\\rangle+2\\beta\\gamma_{k}\\langle f_{k_{k}}(x^{k})-f_{i_{k}}^{\\ast}\\rangle}\\\\ &{\\quad+\\ 2\\gamma_{k}^{k}L(f_{k_{k}}(x^{k})-f_{k_{k}}^{\\ast})}\\\\ &{=\\ \\operatorname{dist}(x^{k},S)^{2}-2\\alpha\\gamma_{k}\\langle f_{k_{k}}(x^{k})-f_{k_{k}}^{\\ast}+f_{i_{k}}^{\\ast}-f_{i_{k}}(x_{p}^{k})\\rangle+2\\beta\\gamma_{k}\\langle f_{k_{k}}(x^{k})-f_{i_{k}}^{\\ast}\\rangle}\\\\ &{\\quad+\\ 2\\gamma_{k}^{k}L(f_{k_{k}}(x^{k})-f_{k_{k}}^{\\ast})}\\\\ &{=\\ \\operatorname{dist}(x^{k},S)^{2}-2\\gamma_{k}(\\alpha-\\beta-L\\gamma_{k})(f_{k_{k}}(x^{k})-f_{i_{k}}^{\\ast})}\\\\ &{\\quad+\\ 2\\alpha\\gamma_{k}(f_{k_{k}}(x_{p}^{k})-f_{k_{k}}^{\\ast}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Note that we choose $\\begin{array}{r}{b_{-1}\\geq\\,\\frac{2L\\gamma}{\\alpha-\\beta}}\\end{array}$ . Since $b_{k}$ is inreasing sequence, then we have for any $k$ that $\\begin{array}{r}{b_{k}\\geq\\frac{2L\\gamma}{\\alpha-\\beta}}\\end{array}$ . Therefore, ", "page_idx": 34}, {"type": "equation", "text": "$$\nL\\gamma_{k}=L\\frac{\\gamma}{b_{k}}\\le L\\frac{\\gamma}{b_{-1}}\\le L\\frac{\\gamma}{2L\\gamma\\slash\\alpha-\\beta}=\\frac{\\alpha-\\beta}{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This means that ", "page_idx": 34}, {"type": "equation", "text": "$$\n-2\\gamma_{k}(\\alpha-\\beta-L\\gamma_{k})\\le-2\\gamma_{k}(\\alpha-\\beta-\\alpha-\\beta/2)=-\\gamma_{k}(\\alpha-\\beta).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Thus, we can continue (45) as follows ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathrm{dist}(x^{k+1},\\mathcal{S})^{2}}&{\\leq}&{\\mathrm{dist}(x^{k},\\mathcal{S})^{2}-\\displaystyle\\frac{\\gamma}{b_{k}}(\\alpha-\\beta)\\big(f_{i_{k}}(x^{k})-f_{i_{k}}^{\\ast}\\big)+2\\alpha\\displaystyle\\frac{\\gamma}{b_{k}}\\big(f_{i_{k}}(x_{p}^{k})-f_{i_{k}}^{\\ast}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We know that $b_{k}$ is increasing sequence that satisfy ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sqrt{b_{-1}^{2}+D^{2}(k+1)}\\leq b_{k}\\leq b_{K-1}\\leq\\sqrt{b_{-1}^{2}+G^{2}K}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "This leads together with the fact that both $f_{i_{k}}(x^{k})-f_{i_{k}}^{*}$ and $f_{i_{k}}(x_{p}^{k})-f_{i_{k}}^{*}$ are non-negative to ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathrm{dist}(x^{k+1},S)^{2}}&{\\leq}&{\\mathrm{dist}(x^{k},S)^{2}-\\displaystyle\\frac{\\gamma}{\\sqrt{b_{-1}^{2}+G^{2}K}}(\\alpha-\\beta)(f_{i_{k}}(x^{k})-f_{i_{k}}^{*})}\\\\ &&{+\\displaystyle2\\alpha\\displaystyle\\frac{\\gamma}{\\sqrt{b_{-1}^{2}+D^{2}(k+1)}}(f_{i_{k}}(x_{p}^{k})-f_{i_{k}}^{*})}\\\\ &{=}&{\\mathrm{dist}(x^{k},S)^{2}-\\displaystyle\\frac{\\gamma(\\alpha-\\beta)}{\\sqrt{b_{-1}^{2}+G^{2}K}}(f_{i_{k}}(x^{k})-f_{i_{k}}(x_{p}^{k}))}\\\\ &&{+\\displaystyle\\frac{2\\alpha\\gamma}{\\sqrt{b_{-1}^{2}+D^{2}(k+1)}}(f_{i_{k}}(x_{p}^{k})-f_{i_{k}}^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Taking the conditional expectation we get ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{\\mathbb{E}_{k}\\left[f(x^{k})-f^{*}\\right]}{\\sqrt{b_{-1}^{2}+G^{2}K}}\\ \\ \\leq\\ \\ \\frac{\\mathrm{dist}(x^{k},\\mathcal{S})^{2}}{\\gamma(\\alpha-\\beta)}-\\frac{\\mathbb{E}_{k}\\left[\\mathrm{dist}(x^{k+1},\\mathcal{S})^{2}\\right]}{\\gamma(\\alpha-\\beta)}+\\frac{2\\alpha}{\\sqrt{b_{-1}^{2}+D^{2}(k+1)}(\\alpha-\\beta)}\\sigma_{\\mathrm{int}}^{2},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "which leads to the following bound ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}_{k}\\left[f(x^{k})-f^{*}\\right]}&{\\leq}&{\\displaystyle\\left(\\frac{\\mathrm{dist}(x^{k},\\mathcal{S})^{2}}{\\gamma(\\alpha-\\beta)}-\\frac{\\mathbb{E}_{k}\\left[\\mathrm{dist}(x^{k+1},\\mathcal{S})^{2}\\right]}{\\gamma(\\alpha-\\beta)}\\right)\\sqrt{b_{-1}^{2}+G^{2}K}}\\\\ &&{\\displaystyle+\\,\\frac{2\\alpha}{\\sqrt{b_{-1}^{2}+D^{2}(k+1)}(\\alpha-\\beta)}\\sigma_{\\mathrm{int}}^{2}\\sqrt{b_{-1}^{2}+G^{2}K}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Averaging over $K$ iterations we get ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{y\\leq k<K}{\\operatorname*{min}}\\ E\\left[f(x^{k})-f^{*}\\right]}&{\\leq}&{\\displaystyle\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[f(x^{k})-f^{*}\\right]}\\\\ &{\\leq}&{\\displaystyle\\frac{\\mathrm{dist}(x^{k},S)^{2}}{\\gamma K(\\alpha-\\beta)}\\sqrt{\\hat{\\nu}_{x^{-1}}^{2}+G^{2K}}}\\\\ &{\\quad+\\frac{2\\alpha}{(\\alpha-\\beta)K^{0}}\\sigma_{\\mathrm{in}}^{2}\\sqrt{\\hat{\\nu}_{x^{-1}}^{2}+G^{2K}}\\sum_{k=0}^{K-1}\\frac{1}{\\sqrt{\\hat{\\nu}_{x^{-1}}^{2}+D^{2}(k+1)}}}\\\\ &{\\leq}&{\\displaystyle\\frac{\\mathrm{dist}(x^{k},S)^{2}}{\\gamma K(\\alpha-\\beta)}\\sqrt{\\hat{\\nu}_{x^{-1}}^{2}+G^{2K}}}\\\\ &{\\quad+\\frac{2\\alpha}{(\\alpha-\\beta)K^{2}}\\sigma_{\\mathrm{in}}^{2}\\sqrt{\\hat{\\nu}_{x^{-1}}^{2}+G^{2K}}\\sqrt{\\hat{\\nu}_{x^{-1}}^{2}+D^{2}(K+1)}}\\\\ &{=}&{\\displaystyle\\mathcal{O}\\left(\\frac{1}{\\sqrt{K}}+\\alpha\\sigma_{\\mathrm{in}}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "D Additional experiments ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "For all the experiments, we make use of PyTorch [65] package. LSTM, MLP, CNN and Resnet experiments are performed using one NVIDIA GeForce RTX 3090 GPU with a memory of 24 GB. For training Algoperf and Pythia language models, we resort instead to 4xA100-SXM4 GPUs, with a memory of 40 GB each, and employ data parallelism for efficient distributed training. When necessary, we disable CUDA non-deterministic operations, to allow consistency between runs with the same random seed. ", "page_idx": 35}, {"type": "image", "img_path": "h0a3p5WtXU/tmp/53e6ae8ef09b42c33757da2a7a8f0ec7c2fd8ad052f56862ab8a686fb09c528f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "Figure 9: Training for half-space learning problem with SGD. Here $T(x_{k})=\\langle\\nabla f_{i_{k}}(x^{k}),x^{k}-x^{K}\\rangle-$ $\\alpha(f_{i_{k}}(x^{k})-f_{i_{k}}(x^{K}))-\\beta f_{i_{k}}(x^{k})$ assuming that $f_{i}^{*}\\approx0.000523853i$ ; angle denotes $\\angle(\\nabla f(x^{k}),x^{k}-$ $x^{K}$ ). ", "page_idx": 36}, {"type": "text", "text": "D.1 Half space learning ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Half Space Learning problem corresponds to the following optimization problem ", "page_idx": 36}, {"type": "equation", "text": "$$\nf(x)=\\frac{1}{n}\\sum_{i=1}^{n}\\sigma(-b_{i}x^{\\top}a_{i})+\\frac{\\lambda}{2}\\|x\\|^{2},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $\\{a_{i},b_{i}\\}_{i=1}^{n},a_{i}\\in\\mathbb{R}^{d},y_{i}\\in\\{0,1\\}$ is a given dataset, $\\lambda=10^{-5}$ , and $\\sigma$ is a sigmoid function. For the test, we create a synthetic dataset that contains 20 samples for both classes. We sample data points from normal distribution where each class has its own mean and variance value of 2. We use SGD with learning rate $\\textstyle{\\frac{1}{4}}$ and batch size 1 for minimization task.5 ", "page_idx": 36}, {"type": "text", "text": "We observe that the gradient norm becomes zero quite which means that SGD trajectory goes through saddle point. This leads to possible negative angle between full gradient and direction to minimizer. Nevertheless, we demonstrate that even for such highly non-convex landscape with many saddle points our $\\alpha{-}\\beta$ -condition holds for large enough values of $\\alpha$ and $\\beta$ . ", "page_idx": 36}, {"type": "text", "text": "D.2 Experiment setup from section 2.2 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "We use 3 layer LSTM based model from H\u00fcbler et al. $[33]^{6}$ . The model for PTB dataset has 35441600 parameters while for Wikitext-2 dataset the model has 44798534 parameters. To train the model, we choose NSGD with momentum [15] with decaying stepsize and momentum parameters according to the experiment section from [33]. We train the model for 1000 epochs with initial stepsize values 158 and 900 for PTB and Wikitext-2 datasets respectively. We switch off dropout during measuring stochastic gradients and losses for $\\alpha$ - $\\beta.$ -condition . We run the experiments for 7 different random seeds and plot the mean with maximum and minimum fluctuations. ", "page_idx": 36}, {"type": "text", "text": "In Figure 10, we plot empirical aiming coefficient $\\frac{\\langle\\nabla f(x^{k}),x^{k}-x^{K}\\rangle}{f(x^{k})}$ for full loss $f$ ; mean of stochastic losses across 7 runs along with maximum and minimum fluctuations from the mean; pairs of $(\\alpha,\\beta)$ that satisfy $\\alpha{-}\\beta$ -condition across all 7 runs. We observe that for both datasets, there is a plateau at the beginning of the training. This part of the training corresponds to possible negative values of the empirical coefficient Aiming condition. After this, it becomes stable and positive. ", "page_idx": 36}, {"type": "text", "text": "Besides, we demonstrate that possible values of pairs of $(\\alpha,\\beta)$ that satisfy $\\alpha{-}\\beta$ -condition are large. We believe, this happens because the full loss $f$ has a minimum value of around 3.5 while individual losses have $f_{i}^{*}$ , i.e. the model is far from the interpolation regime (when $f^{*}=f_{i}^{*}$ ). ", "page_idx": 36}, {"type": "text", "text": "D.3 MLP architecture ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "We use MLP model with 3 fully connected layers. We fix the dimensions of the second layer to be equal, i.e. the parameter matrix of this layer is square. After the first and second fully connected layers we use ReLU activation function. We train the model in all cases with fixed learning rate 0.09 for 1500 epochs and batch size 64 on Fashion-MNIST [83] dataset. In Figure 12, we plot the mean stochastic loss across 4 runs along with maximum and minimum fluctuations, and in Figure 11 possible values of $\\alpha$ and $\\beta$ that work over all random seeds and iterations satisfying $\\alpha\\ge\\beta+0.1$ . ", "page_idx": 36}, {"type": "image", "img_path": "h0a3p5WtXU/tmp/3f0696da614aee503371222000f7419850af947b39b459ae8ef0c27189edb21b.jpg", "img_caption": ["Figure 10: Training of 3 layer LSTM model on PTB (first row) and Wikitext-2 (second row) datasets. Here $T(x_{k})=\\langle\\nabla\\check{f}_{i_{k}}(x^{k}),x^{k}-x^{K}\\rangle-\\alpha(f_{i_{k}}(x^{k})-f_{i_{k}}(x^{K}))-\\beta f_{i_{k}}(x^{k})$ assuming that $f_{i}^{*}=0$ . "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "h0a3p5WtXU/tmp/028db0607cead8fd4cd56e08c8d8f12cc7dfdf0e9a0b1cb67c84845741946396.jpg", "img_caption": ["Figure 11: Values of $\\alpha$ and $\\beta$ during the training of 3 layer MLP model on Fashion-MNIST dataset varying the dimension of the second layer. Here $T(x\\overbar{\\mathbf{\\alpha}_{k}})=\\langle\\nabla f_{i_{k}}(x^{k}),x^{k}-x^{K}\\rangle-\\alpha(f_{i_{k}}(x^{k})-$ $f_{i_{k}}(x^{K}))-\\beta f_{i_{k}}(x^{k})$ assuming that $f_{i}^{*}=0$ . "], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "We observe that the magnitude of the smallest possible values of $\\alpha$ and $\\beta$ increase till up to the second layer size 512, but then it starts decreasing as the model becomes more over-parameterized. This leads to smaller values of neighborhood $\\bar{\\mathcal{O}}(\\beta\\sigma_{\\mathrm{int}}^{2})$ as we expect in this setting. ", "page_idx": 37}, {"type": "text", "text": "D.4 CNN architecture ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "We use CNN model with 2 convolution layers followed by a fully connected one. After each convolution layer, we use max-pooling and ReLU activation functions. We train the model with a cosine annealing learning rate scheduler with a maximum value 0.01 and batch size 64. We train the model on CIFAR10 dataset [41] for 1500 epochs. We run the experiments for 4 different random seeds. In Figure 13 we plot possible values of $\\alpha$ and $\\beta$ satisfying $\\alpha\\ge\\beta+0.1$ that work across all runs and iterations. ", "page_idx": 37}, {"type": "text", "text": "We observe that increasing the dimension of the second layer of the model makes the model closer to over-parameterization: values of stochastic losses decrease. We observe the same phenomenon as in Appendix D.3: minimum possible values of $\\alpha$ and $\\beta$ increase up to 128 number of convolutions, but then it decreases for a larger number of convolutions. This happens because the model becomes more over-parameterized. Moreover, the possible difference between $\\alpha$ and $\\beta$ tends to increase with number of convolutions starting from 128 convolutions. ", "page_idx": 37}, {"type": "image", "img_path": "h0a3p5WtXU/tmp/6fd3805e99409f2fb1fb456636e03958dbec470902e021ccbf1703a983e3ee4b.jpg", "img_caption": ["Figure 12: Values of stochastic loss during the training of 3 layer MLP model on Fashion-MNIST dataset varying the dimension of the second layer. "], "img_footnote": [], "page_idx": 38}, {"type": "image", "img_path": "h0a3p5WtXU/tmp/79b8ff8a1e9bea3ab5e826e4a0b8fb575266e4c87564506ee43cece817da1589.jpg", "img_caption": [], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "Figure 13: Values of $\\alpha$ and $\\beta$ during the training of CNN model on CIFAR10 dataset varying the number of convolutions in the second layer. Here $T(x_{k})=\\langle\\nabla f_{i_{k}}(x^{k}),x^{k}-x^{K}\\rangle-\\alpha(f_{i_{k}}^{\\'}(x^{\\overline{{k}}})-$ $f_{i_{k}}(x^{K}))-\\beta f_{i_{k}}(x^{k})$ assuming that $f_{i}^{*}=0$ . ", "page_idx": 38}, {"type": "text", "text": "D.5 Resnet architecture ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "We use the implementation from Kumar [42]. We train the model on CIFAR100 dataset [41] for 1000 epochs. We use one-cycle scheduler with a maximum learning rate 0.01. To compute dense stochastic gradients, we switch off dropout during evaluations of stochastic gradients and losses for $\\alpha$ - $\\beta$ -condition. We run the experiments for 4 different random seeds and plot the mean along with maximum and minimum fluctuations. ", "page_idx": 38}, {"type": "text", "text": "In Figure 15 we observe that the minimum value of stochastic loss increases with batch size which means that the model becomes further from over-parameterization. ", "page_idx": 38}, {"type": "text", "text": "D.6 Verification of $_{\\alpha}$ - $\\beta$ -condition by different optimizers ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Now we turn to another interesting question: how does the choice of an optimizer affect the practical verification of the $\\alpha{-}\\beta$ -condition? To explore this question, we train Resnet9 model with SGD, SGDM, and Adam. We report the results in Figure 16 varying the batch size used in the training. Comparing the values of $\\alpha$ and $\\beta$ for SGD (from Figure 6), SGDM, and Adam, we observe that the loss landscape explored by the Adam optimizer achieves smaller values of $\\alpha$ and $\\beta$ . Moreover, the values of $\\alpha$ and $\\beta$ found by SGDM are typically smaller than those found by SGD. This result may shed light on why momentum (from a comparison of SGDM against SGD) and adaptive stepsize (from a comparison of SGDM against Adam) are typically beneficial in practice: these more advanced algorithms explore better part of a loss landscape from the $\\alpha{-}\\beta$ -condition point of view. ", "page_idx": 38}, {"type": "image", "img_path": "h0a3p5WtXU/tmp/dceb6f0edf67be13e2b18df9b4919583094f85e56b1b47e50bb771b7d14e4fb0.jpg", "img_caption": ["Figure 14: Values of stochastic loss during the training of CNN model on CIFAR10 dataset varying the number of convolutions in the second layer. "], "img_footnote": [], "page_idx": 39}, {"type": "image", "img_path": "h0a3p5WtXU/tmp/25b7d04bdfd89ff495074bb26ded150337b06f51f13f6d2a8c29fa5afd6c134c.jpg", "img_caption": ["Figure 15: Training of Resnet9 model on CIFAR100 dataset varying the batch size. "], "img_footnote": [], "page_idx": 39}, {"type": "table", "img_path": "h0a3p5WtXU/tmp/c0a280a5842f33f3c5e39985d3934f2bfd1b84d26df3e4070071e8f1cf5293a5.jpg", "table_caption": ["Table 3: Training details of large models from Appendix D.8 and Appendix D.9 "], "table_footnote": [], "page_idx": 39}, {"type": "text", "text": "D.7 Increasing the depth of Resnet architecture ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "In our next experiment, we aim to investigate how the $\\alpha{-}\\beta$ -condition behaves increasing the depth of a model. To do so, in addition to training Resnet9 model, we test Resnet18 and Resnet34 models on CIFAR100 dataset. The results in Figure 17 suggest that the values of $\\alpha$ and $\\beta$ tend to decrease with the depth of a model. These observations align with those from Section 5.1 and Section 5.2 as the constants $\\alpha$ and $\\beta$ decrease as the depth of the model increases. ", "page_idx": 39}, {"type": "text", "text": "D.8 AlgoPerf experiments ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "For each of all aforementioned tasks, we repeat the training with 3 random seeds to create more stable results. The detailed model architectures are given in [16]. In Table 3 we provide the parameters of optimizers we use for each task. The loss curves are presented in Figure 18. For each workload, we run the experiments for 3 different random seeds to obtain more stable results. The hyperparameters of optimizer NadamW are chosen such that we can reach the validation threshold set by the organizers7. We employ a cosine annealing learning rate schedule that reduces the learning to $1e-10$ , with an initial linear warm-up. For each workload, we run the experiments sufficiently enough so that we reach the validation target threshold and the stochastic loss becomes sufficiently stable. ", "page_idx": 39}, {"type": "image", "img_path": "h0a3p5WtXU/tmp/cd2d3d5dae21762c08f6cb6d9d72756e23fd9f49e82a10a477d4c674cf30b7e6.jpg", "img_caption": ["Figure 16: Training of ResNet9 model on CIFAR100 dataset varying the batch size with SGDM (stepsize 0.01 and momentum 0.9 with OneCycle learning rate scheduler) and Adam (stepsize 0.0001 with default momentum parameters with OneCycle learning rate scheduler) optimizers. Here $T(x_{k})=\\langle\\nabla f_{i_{k}}(x^{k}),x^{k}-x^{K}\\rangle\\,\\dot{-}\\alpha\\big(f_{i_{k}}(x^{k})-f_{i_{k}}(x^{K})\\big)-\\beta f_{i_{k}}(x^{k})$ assuming that $f_{i}^{*}=0$ . Minimum is taken across all runs and iterations for a given pair of $(\\alpha,\\beta)$ . We plot values of $\\alpha,\\beta$ in $\\alpha{-}\\beta$ -condition for SGDM (first row) and Adam (second row). "], "img_footnote": [], "page_idx": 40}, {"type": "image", "img_path": "h0a3p5WtXU/tmp/7534a4b435ac9179980207c90306d3bf39cd98e6aad227174e5b2c129f613479.jpg", "img_caption": ["Figure 17: Training of ResNet18 and Resnet34 model on CIFAR100 dataset varying the batch size SGD (stepsize 0.01 and momentum 0.9 with OneCycle learning rate scheduler). Here $T(x_{k})\\,=$ $\\langle\\nabla f_{i_{k}}(x^{\\bar{k}}),x^{k}-x^{K}\\rangle-\\alpha(f_{i_{k}}(x^{k})-f_{i_{k}}(x^{K}))-\\beta f_{i_{k}}(x^{k})$ assuming that $f_{i}^{*}=0$ . Minimum is taken across all runs and iterations for a given pair of $(\\alpha,\\beta)$ . "], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "D.9 Pythia experiments ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "For each of all aforementioned tasks, we repeat the training with 3 random seeds to create more stable results. We train Pythia 70M and Pythia 160M [8] on publicly available Slim-Pajama-627B dataset [72]. Both models are trained on sequences of length 2048, and makes use of a batch size of $0.5\\mathrm{M}$ tokens, which amounts to a batch size of 256 samples. We use AdamW optimizer and a cosine annealing with linear warmup, with hyperparameters specified in Table 3. The stochastic loss and training perplexity are reported in Figure 19. ", "page_idx": 40}, {"type": "image", "img_path": "h0a3p5WtXU/tmp/6622d759dd5e1bcf23fb31223c13fbd3e8c74b2d07414f78c92d870ded72f934.jpg", "img_caption": ["Figure 18: Training of large models from AlgoPerf benchmark. "], "img_footnote": [], "page_idx": 41}, {"type": "image", "img_path": "h0a3p5WtXU/tmp/ab26e0aaaef0c8528c9ec68eb871c172c7b3983a3772425475305fbb6ef4980f.jpg", "img_caption": ["Figure 19: Training statistics for Pythia language models. "], "img_footnote": [], "page_idx": 41}, {"type": "text", "text": "E Checklist ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: In the abstract, we shortly list all the main results of the paper. In the introduction and related works, we demonstrate the limitations of previous works and explicitly list the contributions of our work. We support theoretical claims by extensive experimental results on various deep learning benchmarks. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 41}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: We provide a section in the end of the main body where we discuss several limitations of our work. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper. \u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. ", "page_idx": 41}, {"type": "text", "text": "\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 42}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: We explicitly state a new class of functions we propose in the paper as well as other assumptions on the smoothness and noise of ERM problem. We explicitly state all convergence guarantees for 4 different optimization algorithms. The proofs are deferred to the appendix. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 42}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: For most of the experiments we use existing open source Github repositories making small changes in the code to track necessary quantities during the training. We provide the links to all used repositories. Besides, we list all parameters of optimizers to reproduce the results. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: For each open source data and code we attach the link. All codes can be used almost immediately with small changes that are needed to track necessary quantities for validation of the proposed condition. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 43}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We provide all hyperparameters and training details in the corresponding section of the appendix. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 44}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: For each instance of the experiment section, we run tests for several random seeds (exact numbers are indicated in the experiment section of the appendix) to get more stable results. For each figure with training curves, we plot the mean and maximum and minimum fluctuations around it. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 44}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We provide information about GPU that we use for our experiments in the appendix. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 44}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 45}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We follow all the rules listed following the link above. In particular, we anonymized the paper and attached the code. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 45}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: we provide an impact statement in ??. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 45}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper poses no such risks. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 46}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We use open-source datasets and code. For all used codes we explicitly cite the corresponding paper or Github repository. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 46}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 46}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 47}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 47}]