[{"figure_path": "h0a3p5WtXU/figures/figures_2_1.jpg", "caption": "Figure 1: Training of 3 layer LSTM model that shows Aiming condition does not always hold. The term \u201cAngle\u201d in the figures refers to the angle \u2220(\u2207f(xk), xk \u2013 xK), and it should be positive if Aiming holds, while in a-b we observe that it is negative during the first part of the training. Figures c-d demonstrate that possible constant \u03bc in PL condition should be small which makes theoretical convergence slow.", "description": "This figure shows the results of training a 3-layer LSTM model on two datasets (PTB and Wikitext-2).  The plots illustrate that the Aiming condition, a commonly used assumption in optimization theory, does not always hold during training.  Specifically, the angle between the gradient and the direction to the minimum is negative at the beginning of the training process for both datasets. Additionally, the plots show that the Polyak-\u0141ojasiewicz (PL) constant (\u03bc) must be small for the PL condition to hold, resulting in slow convergence, whereas empirical evidence suggests fast convergence. This discrepancy highlights limitations of existing theoretical conditions in characterizing the loss landscape of deep neural networks.", "section": "Limitations of existing conditions"}, {"figure_path": "h0a3p5WtXU/figures/figures_2_2.jpg", "caption": "Figure 2: Training for half-space learning problem with SGD. The term \u201cAngle\u201d in the figures refers to the angle \u2220(f(xk), xk \u2212 xK).", "description": "This figure shows the training process of a half-space learning problem using SGD. It contains four subfigures that illustrate various aspects of the optimization process.  The 'Angle' refers to the angle between the gradient and the direction to the minimizer (\u2220(\u2207f(xk), xk \u2212 xK)). The plots showcase the angle over iterations, the full loss, the gradient norm, and the evolution of the angle over time. The half-space learning problem is known to have saddle points, and the figure seems to demonstrate the behavior of the optimizer in navigating this complex loss landscape.", "section": "2.2 Limitations of existing conditions"}, {"figure_path": "h0a3p5WtXU/figures/figures_5_1.jpg", "caption": "Figure 3: Loss landscape of f that satisfy Definition 1. The analytical form of fi is given in Section 3.1. These examples demonstrate that the problem (1) that satisfies \u03b1-\u03b2-condition might have an unbounded set of minimizers S (Example 1), a saddle point (Example 2), and local minima (Example 3) in contrast to the PL and Aiming conditions. The contour plots are presented in ??", "description": "This figure shows the loss landscape of three example functions that satisfy the \u03b1-\u03b2 condition, illustrating that the \u03b1-\u03b2 condition can capture functions with an unbounded set of minimizers, saddle points, and local minima, unlike other conditions like PL and Aiming.", "section": "3.1 Theoretical verification of the \u03b1-\u03b2-condition"}, {"figure_path": "h0a3p5WtXU/figures/figures_7_1.jpg", "caption": "Figure 4: \u03b1-\u03b2-condition in the training of 3 layer MLP model on Fashion-MNIST dataset varying the size of the second layer. Here T(xk) = (\u2207 fik (xk), xk \u2013 xK) \u2013 a(fik (xk) \u2013 fik (XK)) \u2013 \u1e9e fik (xk) assuming that f* = 0. Minimum is taken across all runs and iterations for given pair of (\u03b1, \u03b2).", "description": "The figure shows the \u03b1-\u03b2-condition during the training of a 3-layer MLP model on the Fashion-MNIST dataset. The size of the second layer is varied to investigate the effect of over-parametrization on the \u03b1-\u03b2-condition. The plots show the minimum value of T(x<sub>k</sub>) across all runs and iterations for given pairs of (\u03b1, \u03b2). The results show that the minimum possible values of \u03b1 and \u03b2 increase from small sizes to medium sizes and then decrease as the model becomes more over-parametrized.", "section": "5.1 MLP architecture"}, {"figure_path": "h0a3p5WtXU/figures/figures_8_1.jpg", "caption": "Figure 13: Values of \u03b1 and \u03b2 during the training of CNN model on CIFAR10 dataset varying the number of convolutions in the second layer. Here T(xk) = (\u2207fik(xk), xk \u2013 xK) \u2013 \u03b1(fik(xk) \u2013 fik(xK)) \u2013 \u03b2fik(xk) assuming that f* = 0.", "description": "This figure displays the values of \u03b1 and \u03b2 parameters obtained during the training of a Convolutional Neural Network (CNN) model on the CIFAR-10 dataset.  The x-axis represents the number of convolutions in the second layer of the CNN. The y-axis shows the minimum values of \u03b1 and \u03b2 parameters obtained across all runs and iterations during training, where these parameters satisfy the \u03b1-\u03b2-condition (a newly proposed condition in the paper) with \u03b1 > \u03b2.  The different subplots correspond to different numbers of convolutions, illustrating the relationship between model over-parameterization (as indicated by increasing convolution count) and the values of the \u03b1 and \u03b2 parameters required to satisfy the \u03b1-\u03b2-condition.", "section": "Experimental validation of the \u03b1-\u03b2-condition"}, {"figure_path": "h0a3p5WtXU/figures/figures_8_2.jpg", "caption": "Figure 6: \u03b1-\u03b2-condition in the training of Resnet9 model on CIFAR100 dataset varying the batch size. Here T(xk) = (\u2207fik (xk), xk \u2013 xK) \u2013 a(fik (xk) \u2013 fik (xK)) \u2013 \u03b2 fik (xk) assuming that f* = 0. Minimum is taken across all runs and iterations for a given pair of (\u03b1, \u03b2).", "description": "This figure displays the results of an experiment evaluating the \u03b1-\u03b2-condition on the training of a Resnet9 model using the CIFAR100 dataset.  The experiment varied the batch size during training. The plot shows the minimum value of the expression T(x) = (\u2207fik (xk), xk \u2013 xK) \u2013 \u03b1(fik (xk) \u2013 fik (xK)) \u2013 \u03b2 fik (xk) across all runs and iterations for each pair of \u03b1 and \u03b2 values.  The value of f* is assumed to be 0.  This illustrates how the \u03b1-\u03b2-condition holds under different batch sizes in this setting.", "section": "5.3 Resnet architecture"}, {"figure_path": "h0a3p5WtXU/figures/figures_9_1.jpg", "caption": "Figure 7: \u03b1-\u03b2-condition in the training of some large models from AlgoPerf, 3-layer LSTM, and Transformers for language modeling. Here T(xk) = (\u2207fik (xk), xk \u2212 xK) -a (fik (xk) - fik (xK)) - Bfin (xk) assuming that f* = 0. Minimum is taken across all runs and iterations for a given pair of (\u03b1, \u03b2).", "description": "This figure displays the results of applying the \u03b1-\u03b2-condition to several large language models and other deep learning models during training.  It shows how the angle between the gradient and the projection onto the set of minimizers varies across different models. The heatmaps illustrate the minimum values of \u03b1 and \u03b2 that satisfy the \u03b1-\u03b2-condition for each model across multiple training runs and iterations.  The results demonstrate the applicability of the \u03b1-\u03b2-condition to a diverse range of architectures and problem settings.", "section": "Experimental validation of the \u03b1-\u03b2-condition"}, {"figure_path": "h0a3p5WtXU/figures/figures_18_1.jpg", "caption": "Figure 8: Training of 3 layer LSTM model that shows Aiming condition does not always hold. We plot possible values of PL constant for different powers \u03b4 in (12). We observe that possible values of \u03bc are of order 10\u22129 \u2013 10\u22127 that leads to slow theoretical convergence and contradicts practical observations.", "description": "This figure shows the result of training a 3-layer LSTM model on PTB and Wikitext-2 datasets. It demonstrates that the widely recognized Polyak-\u0141ojasiewicz (PL) inequality, often used to guarantee convergence in optimization, is not always satisfied in practice. The plots show that the possible values for the PL constant \u03bc are very small (around 10^-9 to 10^-7), which implies slow convergence according to theoretical analysis. This contrasts with the observed fast convergence in practice. The figure visualizes the inadequacy of PL inequality for characterizing the loss landscape of deep neural networks without significant over-parametrization, motivating the need for the new \u03b1-\u03b2-condition proposed in the paper.", "section": "Additional explanation on PL assumption"}, {"figure_path": "h0a3p5WtXU/figures/figures_36_1.jpg", "caption": "Figure 9: Training for half-space learning problem with SGD. Here T(xk) = \u27e8\u2207fik(xk), xk \u2013 xK\u27e9 \u2013 \u03b1(fik(xk) \u2013 fik(xK)) \u2013 \u03b2fik(xk) assuming that f* \u2248 0.000523853; angle denotes \u2220(\u2207f(xk), xk \u2013 xK).", "description": "This figure shows the results of training a model on a half-space learning problem using SGD.  It displays multiple plots illustrating various aspects of the training process. The plot labeled \"Angle\" shows the angle between the gradient and the direction to the minimizer. This angle should be positive if the \"Aiming\" condition (a known condition used for convergence analysis) holds, and the figure shows how it does not always hold.  The plot labeled \"Full Loss\" shows the loss function, highlighting how it is decreasing as training progresses. The \"Gradient Norm\" plot shows the norm of the gradient, which indicates the magnitude of changes in the loss function. The plot labeled \"min T(xk)\" shows the minimum value of a particular function T(xk), which is a measure relevant to the newly introduced \u03b1-\u03b2 condition proposed in the paper. The figure demonstrates that while the Aiming condition doesn't always hold, the \u03b1-\u03b2 condition, proposed in the study, does provide a better characterization of the optimization landscape.", "section": "D.1 Half space learning"}, {"figure_path": "h0a3p5WtXU/figures/figures_37_1.jpg", "caption": "Figure 10: Training of 3 layer LSTM model on PTB (first row) and Wikitext-2 (second row) datasets. Here T(xk) = (\u2207fik (xk), xk \u2013 xK) \u2013 a(fik (xk) \u2013 fik (xK)) \u2013 \u03b2 fik (xk) assuming that f* = 0.", "description": "The figure shows the training process of a 3-layer LSTM model on two datasets: Penn Treebank (PTB) and WikiText-2.  It visualizes how the Aiming condition (a measure of gradient alignment with the direction to the minimizer) and the Polyak-\u0141ojasiewicz (PL) condition (a measure of the relationship between gradient norm and function value) behave during training.  The plots include the Aiming coefficient, full loss, stochastic loss, and the PL constant.  The goal is to show whether the Aiming and PL conditions, commonly used to analyze optimization algorithms, hold throughout the training process in these models. Note that the term \"Angle\" refers to the angle \u2220(\u2207f(xk), xk \u2013 xK).", "section": "Experimental validation of the \u03b1-\u03b2-condition"}, {"figure_path": "h0a3p5WtXU/figures/figures_37_2.jpg", "caption": "Figure 11: Values of \u03b1 and \u03b2 during the training of 3 layer MLP model on Fashion-MNIST dataset varying the dimension of the second layer. Here T(xk) = (\u2207fik(xk), xk \u2013 xK) \u2013 \u03b1(fik(xk) \u2013 fik(xK)) \u2013 \u03b2fik(xk) assuming that f* = 0.", "description": "This figure shows the values of \u03b1 and \u03b2 parameters of the \u03b1-\u03b2 condition during the training process of a 3-layer MLP model on the Fashion-MNIST dataset. The x-axis represents \u03b1, the y-axis represents \u03b2, and different subfigures show results for different sizes of the second layer (32, 128, 512, 1024, 2048, 4096). The color intensity represents the minimum value of T(xk) across all iterations and runs, where T(xk) is a function of the gradient and the distance to the minimizer, \u03b1 and \u03b2.  The figure aims to illustrate how the \u03b1-\u03b2 condition parameters change during the training, and how these changes are related to the over-parameterization of the model.", "section": "5.1 MLP architecture"}, {"figure_path": "h0a3p5WtXU/figures/figures_38_1.jpg", "caption": "Figure 11: Values of \u03b1 and \u03b2 during the training of 3 layer MLP model on Fashion-MNIST dataset varying the dimension of the second layer. Here T(xk) = (\u2207fik (xk), xk \u2013 xK) \u2013 a(fik (xk) \u2013 fik (xK)) \u2013 \u03b2fik (xk) assuming that f* = 0.", "description": "The figure shows the values of \u03b1 and \u03b2 parameters for the \u03b1-\u03b2-condition during the training of a 3-layer MLP model on the Fashion-MNIST dataset. The size of the second layer is varied to investigate the effect of over-parameterization. The plot shows that minimum possible values of \u03b1 and \u03b2 increase from small size to medium, and then tend to decrease again as the model becomes more over-parametrized. This observation leads to the fact that the neighborhood of convergence O(\u03b2\u03c3nt) of SGD eventually becomes smaller with the size of the model.", "section": "5.1 MLP architecture"}, {"figure_path": "h0a3p5WtXU/figures/figures_38_2.jpg", "caption": "Figure 11: Values of \u03b1 and \u03b2 during the training of 3 layer MLP model on Fashion-MNIST dataset varying the dimension of the second layer. Here T(xk) = (\u2207fik (xk), xk \u2013 xK) \u2013 \u03b1(fik (xk) \u2013 fik (xK)) \u2013 \u03b2fik (xk) assuming that f* = 0.", "description": "This figure shows the values of \u03b1 and \u03b2 parameters obtained during the training of a 3-layer Multilayer Perceptron (MLP) model on the Fashion-MNIST dataset. The size of the second layer was varied to investigate the effect of over-parameterization on the \u03b1-\u03b2 condition. The plot shows how the minimum possible values of \u03b1 and \u03b2 change with the size of the second layer.  It suggests that the minimum values increase initially with size, but start decreasing as the model becomes over-parametrized, leading to smaller neighborhood size O(\u03b2\u03c3int).", "section": "5.1 MLP architecture"}, {"figure_path": "h0a3p5WtXU/figures/figures_39_1.jpg", "caption": "Figure 12: Values of stochastic loss during the training of 3 layer MLP model on Fashion-MNIST dataset varying the dimension of the second layer.", "description": "The figure shows the training curves of stochastic loss for a 3-layer MLP model trained on the Fashion-MNIST dataset. The x-axis represents the training epoch, and the y-axis represents the stochastic loss. Multiple lines are shown, each corresponding to a different size of the second layer of the MLP model.  The goal is to observe how the loss changes depending on the model's width to investigate the effect of over-parameterization on the \u03b1-\u03b2 condition.", "section": "5.1 MLP architecture"}, {"figure_path": "h0a3p5WtXU/figures/figures_39_2.jpg", "caption": "Figure 15: Training of Resnet9 model on CIFAR100 dataset varying the batch size.", "description": "The figure shows the training curves of ResNet9 model trained on CIFAR100 dataset with different batch sizes (64, 128, 256, and 512).  Each curve represents the stochastic loss over epochs. The aim is to illustrate how the minimum value of the stochastic loss changes with the batch size. It shows that the minimum stochastic loss increases with the batch size, which indicates that the model is further from being over-parameterized as the batch size increases.", "section": "5.3 Resnet architecture"}, {"figure_path": "h0a3p5WtXU/figures/figures_40_1.jpg", "caption": "Figure 11: Values of \u03b1 and \u03b2 during the training of 3 layer MLP model on Fashion-MNIST dataset varying the dimension of the second layer. Here  T(xk) = (\u2207fik(xk),xk\u2212xK)\u2212\u03b1(fik(xk)\u2212fik(xK))\u2212\u03b2fik(xk) assuming that f* = 0.", "description": "The figure shows the values of \u03b1 and \u03b2 parameters during the training process of a 3-layer Multilayer Perceptron (MLP) model on the Fashion-MNIST dataset. The x-axis represents the size of the second layer, while the y-axis displays the minimum values of \u03b1 and \u03b2 that satisfy the \u03b1-\u03b2 condition, which is a novel condition proposed in the paper. The results suggest a relationship between the model's over-parametrization (second layer size) and the minimum \u03b1 and \u03b2 values that satisfy the \u03b1-\u03b2 condition. The minimum possible values of \u03b1 and \u03b2 first increase with the second layer size (indicating a more complex landscape) but then begin to decrease as the model becomes more over-parameterized, suggesting that a simpler loss landscape is observed with significant over-parametrization.", "section": "5.1 MLP architecture"}, {"figure_path": "h0a3p5WtXU/figures/figures_40_2.jpg", "caption": "Figure 4: \u03b1-\u03b2-condition in the training of 3 layer MLP model on Fashion-MNIST dataset varying the size of the second layer. Here T(xk) = (\u2207fik (xk), xk \u2013 xK) \u2013 a(fik (xk) \u2013 fik (xK)) \u2013 \u1e9efik (xk) assuming that f* = 0. Minimum is taken across all runs and iterations for a given pair of (\u03b1, \u03b2).", "description": "This figure displays the results of an experiment assessing the \u03b1-\u03b2 condition during the training of a 3-layer Multilayer Perceptron (MLP) model on the Fashion-MNIST dataset.  The experiment varies the size of the second layer of the MLP to investigate the impact of over-parameterization on the \u03b1-\u03b2 condition. The plot shows the minimum value of the expression T(xk) = (\u2207fik (xk), xk \u2013 xK) \u2013 a(fik (xk) \u2013 fik (xK)) \u2013 \u1e9efik (xk) across all runs and iterations, for given values of \u03b1 and \u03b2. The minimum is taken to check if the \u03b1-\u03b2 condition holds at any point during training.  The results are shown for different sizes of the second layer (32, 128, 2048, and 4096), providing insights into how the \u03b1-\u03b2 condition is affected by over-parameterization.", "section": "5.1 MLP architecture"}, {"figure_path": "h0a3p5WtXU/figures/figures_41_1.jpg", "caption": "Figure 18: Training of large models from AlgoPerf benchmark.", "description": "The figure shows the training curves of several large models from the AlgoPerf benchmark.  Each subfigure displays the stochastic loss over training iterations for a specific model and dataset, with multiple runs shown for each model to illustrate variability.  This provides empirical evidence for the \u03b1-\u03b2-condition.", "section": "5 Experimental validation of the \u03b1-\u03b2-condition"}, {"figure_path": "h0a3p5WtXU/figures/figures_41_2.jpg", "caption": "Figure 19: Training statistics for Pythia language models.", "description": "This figure shows the training statistics for Pythia language models with different sizes (70M and 160M) trained on the SlimPajama dataset. The plots show stochastic loss and perplexity for three different runs, visualizing the training progress and stability.", "section": "D.9 Pythia experiments"}]