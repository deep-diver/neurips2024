[{"type": "text", "text": "Upping the Game: How 2D U-Net Skip Connections Flip 3D Segmentation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xingru Huang1\u2217, Yihao $\\mathbf{Guo}^{1}\\colon$ \u2217, Jian Huang1\u2217, Tianyun Zhang1, Hong $\\mathbf{H}\\mathbf{e}^{1}$ \u2020, Shaowei Jiang1\u2020, Yaoqi Sun1\u2020 1Hangzhou Dianzi University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In the present study, we introduce an innovative structure for 3D medical image segmentation that effectively integrates 2D U-Net-derived skip connections into the architecture of 3D convolutional neural networks (3D CNNs). Conventional 3D segmentation techniques predominantly depend on isotropic 3D convolutions for the extraction of volumetric features, which frequently engenders inefficiencies due to the varying information density across the three orthogonal axes in medical imaging modalities such as computed tomography (CT) and magnetic resonance imaging (MRI). This disparity leads to a decline in axial-slice plane feature extraction efficiency, with slice plane features being comparatively underutilized relative to features in the time-axial. To address this issue, we introduce the U-shaped Connection (uC), utilizing simplified 2D U-Net in place of standard skip connections to augment the extraction of the axial-slice plane features while concurrently preserving the volumetric context afforded by 3D convolutions. Based on uC, we further present uC 3DU-Net, an enhanced 3D U-Net backbone that integrates the uC approach to facilitate optimal axial-slice plane feature utilization. Through rigorous experimental validation on five publicly accessible datasets\u2014FLARE2021, OIMHS, FeTA2021, AbdomenCT-1K, and BTCV, the proposed method surpasses contemporary state-of-the-art models. Notably, this performance is achieved while reducing the number of parameters and computational complexity. This investigation underscores the efficacy of incorporating 2D convolutions within the framework of 3D CNNs to overcome the intrinsic limitations of volumetric segmentation, thereby potentially expanding the frontiers of medical image analysis. Our implementation is available at https://github.com/IMOP-lab/U-Shaped-Connection. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "3D volumetric data segmentation extensively relies on the utilization of axial symmetrical 3D convolutions to extract features based on a volumetric representation. Imaging modalities such as CT [1, 2, 3, 4] and MRI [5, 6, 7, 8, 9] yield high-precision images along the slice plane and repeat this process along the temporal axis, resulting in non-uniform information density across the three axes. We visualize the difference in information density between the axial and the slice plane in 3D medical imaging in Fig. 1. The information density variance arising from non-simultaneous imaging in three dimensions of volumetric data engenders distinctiveness between time-axial and slice plane features, rendering them unsuitable for the same processing. Typically, features extracted from the slice plane possess higher information density and are more adept at delineating the finegrained, voxel-level neighboring structures. These localized features are essential in understanding the precise 3D structure of tissues, thereby surpassing the utility of time-axial features. However, traditional 3D convolutions [10, 11, 12], although excelling in time-axial information extraction, significantly increase computational complexity to accommodate time-axial information, leading to axial-slice plane performance drop-off in 3D CNNs. Thus, the structural constraints of 3D convolutions inherently weaken the rich local features of the slice plane. When confronted with sequences abundant in slice plane features, symmetric 3D convolutions face substantial inefficiencies and performance degradation. ", "page_idx": 0}, {"type": "image", "img_path": "QI1ScdeQjp/tmp/e5d327900e094ba60d102ae58607f5afc319d548bd69b6f7404661470fbabdc7.jpg", "img_caption": ["Figure 1: Visualization of the difference in information density between the time-axial (Green) and slice planes (Red). Panels (a), (b), and (c) respectively illustrate the information density differences in time-axial and slice planes for volumetric data of the abdomen, retina, and brain tissues. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To address the axial-slice plane performance drop-off issue, an efficient strategy involves increasing channel depth to enhance feature extraction performance. The depth of the channels, indicative of the high-dimensional feature richness, directly influences network performance. Increasing channel depth can enhance the network\u2019s capacity to capture more high-dimensional information and improve feature representation quality. However, the relationship between channel depth and network performance in 3D CNNs is intricate and nonlinear. While initially increasing channel depth improves model performance by introducing more detailed and abstract features, this improvement comes at the expense of ever-increasing computational overhead. Moreover, an excessive number of channel depths can complicate the model\u2019s information processing, potentially introducing noise and redundancy, culminating in dimensional explosion and performance decline. The linear relationship between channel depths and computational overhead means that increasing channel depth significantly inflates computational costs, leading to reduced model efficiency under identical hardware constraints. Conversely, 2D convolutions, with their focus on slice-wise information extraction, exhibit higher efficiency and performance peaks, boasting a superior parameter performance ratio. For a kernel size $K$ , a 2D convolution requires only $\\mathit{\\check{\\Psi}}_{\\overline{{K}}}$ of the parameters compared to its 3D counterpart, thereby significantly enhancing local feature extraction efficiency within a slice. Consequently, 2D convolutions present a key solution to the low parameter-to-performance ratio of 3D CNN architectures, markedly increasing axial-slice plane feature extraction efficiency while minimizing parameter count and computational load. ", "page_idx": 1}, {"type": "text", "text": "Given the excellent computational efficiency and parameter-to-performance ratio of 2D convolutions for axial-slice plane feature utilization, and considering the prevalent use of skip connections in 3D medical image segmentation architectures to integrate early layer details, we propose employing 2D convolutions within skip connections to supplement axial-slice plane information. For this purpose, we introduce a plug-and-play U-shaped Connection (uC), leveraging a simplified 2D U-Net to replace skip connections in 3D segmentation architectures for improving the utilization efficiency of axialslice plane features. Using classical 3D U-Net as the backbone, we further propose the uC 3DU-Net, which substitutes the original skip connections with uC, thereby enhancing the network\u2019s ability to comprehend slice plane information and efficiently utilize initial encoded layer details. For more effective integration of 3D spatial features from 3D CNNs and 2D slice plane features introduced by uC, we employ a Dual Feature Integration (DFi) module to combine multi-dimensional features. ", "page_idx": 1}, {"type": "text", "text": "Experimental evaluations demonstrate the incorporation of uC significantly enhances segmentation performance even with reduced channels and surpasses parameter-to-performance ratios of state-ofthe-art networks. Comparative analyses on five publicly available datasets\u2014FLARE2021, OIMHS, FeTA2021, AbdomenCT-1K, and BTCV\u2014corroborate the superiority of proposed uC 3DU-Net over existing state-of-the-art methods, achieving persuasive performance with reduced computational complexity and model parameters. ", "page_idx": 1}, {"type": "text", "text": "The principal contributions of our work can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "1. We proposed uC, utilizing simplified 2D U-Net to replace traditional skip connections in 3D segmentation backbones, enhancing model capacity to capture axial-slice plane features. 2. uC 3DU-Net is further proposed, adopting uC to replace original skip connections, and applying the DFi module to effectively merge 3D sequential spatial features with 2D axial-slice plane features, thus improving feature utilization efficiency. 3. We explore the complementary relationship between uC and 3D CNNs, revealing that incorporating 2d convolutions in 3D CNNs can achieve superior performance with fewer parameters, striking perfect balances between efficiency and performance in volumetric segmentation. 4. Comparative experiments on five public datasets\u2014FLARE2021, OIMHS, FeTA2021, AbdomenCT", "page_idx": 2}, {"type": "text", "text": "1K, and BTCV\u2014demonstrate that uC 3DU-Net surpasses all previous models, achieving SOTA performance with fewer parameters and lower computational cost. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 3D medical image segmentation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The realm of 3D medical image segmentation has experienced substantial advancements, primarily propelled by the progressive evolution of deep learning paradigms and the augmentation of computational prowess [13, 14]. At the key of these advancements lies the continuous enhancement of CNNs [15, 16, 17, 18, 19] and the advent of Vision Transformers (ViTs) [20], both of which have significantly promoted segmentation methodologies [21, 22, 23, 24, 25]. The 3D U-Net [26], a foundational model in this field, the subsequent iterations have incorporated attention mechanisms, augmenting feature extraction capabilities by prioritizing important regions while mitigating noise interference [27, 28]. Transformer-based architectures, exemplified by the UNETR framework [29, 30], leverage multi-head self-attention mechanisms to capture long-range dependencies, thus facilitating more precise and meticulous segmentation. Hybrid models that combine CNNs with Transformers, such as TransUNet [31] and TransBTS [32], effectively balance local spatial feature extraction with global contextual understanding, culminating in superior segmentation performance [33, 34]. The nnU-Net [35], an exemplary self-adaptive framework, has demonstrated exceptional performance across a diverse array of medical imaging tasks by autonomously configuring itself to specific datasets. ", "page_idx": 2}, {"type": "text", "text": "Beyond architectural innovations, a multitude of research endeavors have concentrated on refining 3D medical image segmentation mask techniques. Chen et al. [36] propose a novel method integrating Active Appearance Models (AAM) with live wire (LW) and Graph Cuts (GC) techniques, thereby enhancing the efficacy of 3D medical image segmentation. Zhang et al. [21] develop the 3D Context Residual Network (ConResNet), which employs a context residual module to interlink the segmentation decoder with the context residual decoder, explicitly learning inter-slice contextual information to improve segmentation accuracy. Advanced loss functions and optimization strategies, such as extensive implementations of Dice loss [37], have been designed to address class imbalance issues, ensuring a more balanced learning process between easy and hard examples. Furthermore, data augmentation and transfer learning techniques have been beneficial in enhancing model robustness and generalization capabilities [38, 39]. These unremitting innovations promise increasingly precise and reliable analysis of 3D medical images, significantly advancing the fields of medical diagnostics and therapeutic planning. ", "page_idx": 2}, {"type": "text", "text": "2.2 Advancements in skip connection structures ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Innovations in skip connection structures have further augmented the performance and computational efficiency of CNNs [40, 41, 42] in the domain of medical image segmentation. Skip connections are important in facilitating direct gradient propagation, thereby mitigating the vanishing gradient issue and enabling the training of substantially deeper networks. This method permits unimpeded gradient flow throughout the network, effectively circumventing intermediate layers, minimizing information degradation, and achieving more stable and efficient optimization. Prominently exemplified in the ResNet[43] architecture, skip connections mitigate the vanishing gradient dilemma, thereby fostering the training of deep neural networks. In the U-Net[44] architecture, skip connections are crucial in the transmission of initial detailed spatial information from the encoder to the decoder, thereby preserving high-resolution details indispensable for precise segmentation tasks. ", "page_idx": 2}, {"type": "text", "text": "Recent endeavors have extensively refined traditional skip connections to augment their functional efficacy. Attention U-Net [45] replaces the normal application of skip connections by incorporating attention mechanisms that dynamically emphasize important features while attenuating irrelevant information. Dense U-Net [46] leverages densely connected convolutional networks (DenseNet) [47] to facilitate feature reuse and improve gradient flow. Interconnecting each layer with every other layer in a feed-forward manner facilitates the assimilation of richer and more diverse feature representations. Skip connections in the Residual Channel Attention Network (RCAN) [48] network mitigate the training difficulty of deep neural networks and enhance feature reuse and information flow efficiency, thereby improving the network\u2019s ability to learn high-frequency information. Hybrid Densely Connected UNet (H-DenseUNet) [49] incorporates skip connections within dense blocks, combining the strengths of DenseNet and U-Net. By harnessing advanced feature reuse, optimized gradient flow facilitated by dense connectivity, and efficient multi-scale feature fusion, it produces a robust architecture adept at capturing intricate details and nuanced contextual information. ", "page_idx": 3}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Preliminaries: 3D convolutional segmentation networks and skip connections ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Current 3D medical image segmentation networks primarily utilize architectures based on 3D convolutions, as exemplified by classic models such as 3D U-Net, SegResNet, and the recent 3D UX-Net. These networks adopt an end-to-end encoder-decoder framework with skip connections. The encoder progressively increases channel depth while downsampling spatial feature maps to abstract and consolidate contextual information, thereby reducing parameter complexity and improving computational efficiency. In contrast, the decoder gradually upsamples these encoded features to restore the original resolution, refining segmentation boundaries and overall segmentation accuracy. Skip connections effectively reintroduce fine-grained details from the original images during the upsampling stage, aiding the refinement of segmentation results. ", "page_idx": 3}, {"type": "text", "text": "Several 3D medical image segmentation networks, such as UNETR, TransBTS, and SwinUNETR, have incorporated multi-head self-attention and advanced skip connections within the 3D Conv-based encoder-decoder framework. These hybrid approaches effectively capture long-range dependencies and facilitate multi-scale feature utilization. Multi-head self-attention ensures global consistency, while 3D convolution operations excel in preserving local spatial details. The combination of self-attention with 3D convolutions ensures the model\u2019s ability to retain image details while comprehending and processing global information more effectively. However, relying solely on transformer architectures, such as Vision Transformers, would result in a substantial increase in parameters and computational load. Additionally, without skip connections, it is difficult to achieve satisfactory segmentation details. Hence, 3D convolutions and skip connections remain crucial for achieving optimal segmentation performance in 3D medical image segmentation. ", "page_idx": 3}, {"type": "text", "text": "3.2 U-shaped Connection ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Considering the objective of skip connections to capture detailed features of original images, it is essential to note that 3D medical images fundamentally represent a sequence of 2D images. Given the anisotropic nature of medical images, the 2D slice plane shows richer feature information compared to the temporal sequence axis. Although basic skip connection methods like cat and addition can supplement original feature information to some extent, these methods fail to fully utilize the rich axial-slice plane information in 3D medical images. To address the challenges posed by the anisotropy of 3D medical images, the U-shaped Connection(uC) is proposed. It offers a solution to anisotropy in medical imaging. This approach employs a simplified 2D U-Net to implement skip connections, thereby supplementing the rich original 2D slice plane feature information, and enhancing the 2D slice plane feature extraction capabilities of any 3D image segmentation network. The uC combines features extracted by 2D convolution with those extracted by 3D upsampling, offering more efficient feature extraction compared to pure 3D convolution, as detailed structure in Sec3.3. ", "page_idx": 3}, {"type": "text", "text": "The fundamental structure of uC is based on a 2D U-Net. In comparison to the basic 2D U-Net, uC omits the initial and final conv layers, retaining only the downsampling and upsampling layers to minimize computational load and enhance the extraction efficiency of original slice plane features. Each downsampling layer comprises an average pooling layer and two conv layers, each consisting of 2D conv, Group Normalization (GN), and ReLU. The average pooling layer reduces the feature map size by half, and the subsequent conv layers double the channel number, with the group number of GN set to half the current channel number. Each upsampling layer includes a transposed convolution layer and two conv layers, where the transposed convolution layer restores the feature map to its original size, and the conv layers reduce the channel number back to its half. ", "page_idx": 3}, {"type": "image", "img_path": "QI1ScdeQjp/tmp/41a06b31641a8ade983e617f095ff435f2070a3c8c623fa34ccd4bab29cdcdbf.jpg", "img_caption": ["Figure 2: Overview of the proposed uC 3DU-Net architecture. The backbone is a 3D U-Net with five encoding-decoding stages and each downsampling block comprises a max-pooling layer followed by two 3D convolutional layers. In stages 1 to 3, 5D tensors are rearranged into 4D tensors for $2\\mathrm{D}\\;\\mathrm{uC}$ input by stacking slices along the batch dimension. Each upsampling layer employs a transposed convolution to upsampling and a DFi block to effectively adjust the feature channel depth. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.3 uC 3DU-Net ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Based on uC, we proposed uC 3DU-Net, which employs the 3D U-Net as the backbone, incorporating uC with a dual feature integration (DFi) module. The network architecture is illustrated in Fig. 2. Given the input 3D volumetric image data denoted as $x$ , we divide the encoder of the 3D U-Net into five stages. Stage 1 is the initial Two Conv layer, comprising two 3D convolutions, expanding the input feature channel depth to 24. Stages 2 to 5 are downsampling layers, each comprising a max-pooling operation and two 3D convolutions, doubling the feature channels while halving the feature map dimensions. The outputs of stages 1 to 5 are denoted as $x^{(\\theta:1)}$ to $x^{(\\theta:5)}$ . The decoder is also divided into five stages. Stages 5 to 2 utilize transposed convolutions to upsample, effectively doubling the spatial dimensions of the feature maps. Stages 5 to 3 apply DFi to effectively halve the feature channel depth, while Stage 2 retains two convolution layers. Stage 1 is the Final Conv layer, converting the feature channels to the number of categories by a 3D convolution. All 3D convolution operations are subsequently followed by Instance Normalization and a LeakyReLU activation function, with the negative slope set to 0.1. ", "page_idx": 4}, {"type": "text", "text": "Considering the parameter-efficiency trade-off, we employ the $\\mathbf{u}C$ in only stages 1 to 3, denoted as $u C_{1}$ to $u C_{3}$ . The inputs $x^{(\\theta:1)}$ to $x^{(\\theta:3)}$ are reshaped into 4D dimensions by stacking slices along the batch dimension using a rearrange operation, resulting in $\\hat{x}^{(\\theta:1)}$ to $\\hat{x}^{(\\theta:3)}$ . The structure of $u C_{1}$ to $u C_{3}$ resembles that of a simplified version of 2D U-Net, with detailed descriptions of the uC architecture provided in Section 3.2. $u C_{1}$ , $u C_{2}$ , and $u C_{3}$ consist of 4, 3, and 2 downsampling and upsampling layers, respectively, to match the dimensions of the input 4D tensor. The outputs of $\\hat{x}^{(\\theta:1)}$ to $\\hat{x}^{(\\theta:\\bar{3})}$ after passing through the $\\mathbf{u}C$ are $\\tilde{x}^{(\\theta:1)}$ to $\\tilde{x}^{(\\theta:3)}$ , which are reshaped back to 5D dimensions using a rearrange operation, resulting in $\\hat{x}^{(\\phi:1)}$ to $\\hat{x}^{(\\phi:3)}$ . ", "page_idx": 4}, {"type": "text", "text": "We utilize the dual feature integration (DFi) module to seamlessly integrate the 3D spatial features extracted by the 3D CNN with the rich axial-slice plane features introduced by the uC. Specifically, $x^{(\\theta:4)}$ and $x^{(\\theta:5)}$ are fused using the DFi block to obtain $x^{\\mu}$ , $x^{\\mu}$ and $\\hat{x}^{(\\phi:3)}$ are fused to obtain $x^{v}$ , and $x^{v}$ and $\\hat{x}^{(\\phi:2)}$ are fused to obtain $x^{\\xi}$ . Finally, $x^{\\xi}$ and $\\hat{x}^{(\\phi:1)}$ are fused using the DFi block and undergo the final convolutional block to obtain the final output $y$ . ", "page_idx": 4}, {"type": "text", "text": "Dual Feature Integration. In conventional 3D U-Net architectures, skip connections integrate the original feature information from the encoder during the upsampling process through cat, followed by two 3x3 convolutions to reduce the number of channels and consolidate useful information. However, this basic feature fusion approach may lead to inefficiencies when reconciling the disparity between 3D spatial features and 2D slice plane features. Therefore, we devise a streamlined, efficient multi-scale feature fusion module DFi to better integrate the axial-slice plane information introduced by uC. The 2D slice plane features extracted by uC and the 3D spatial features from the 3D CNN encoder are cat to retain maximal original feature information. However, subsequent convolution operations to reduce channel depths are computationally intensive and inefficient. Utilizing addition for feature fusion could result in information loss due to inherent discrepancies between the two types of features. Hence, an approach combining cat, addition, and subsequent convolution layers, with the aim of minimizing parameter count and FLOPs, resulting in the design of the DFi module. For the 2D slice plane features F1 extracted by uC and the 3D spatial features F2 obtained during 3D U-Net upsampling, branch 1 of the DFi module cats F1 and F2, then employs a 1x1 conv to halve the channel count, preserving important features to produce feature map Fc. Branch 2 initially extracts critical information from F1 and F2 using 1x1 convs, followed by addition and sigmoid activation to generate the attention map Fa. The element-wise multiplication of Fc and Fa adjusts and integrates the feature map Fc from branch 1, thereby learning the relative importance of F1 and F2 during the fusion process. The proposed DFi module integrates and simplifies addition, cat, and subsequent convolution layers, achieving a more lightweight and efficient fusion of 2D and 3D spatial features. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4 Experiments and results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Datasets ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To validate the efficacy of the proposed method, multiple experiments are conducted on five widelyused, publicly accessible datasets: FLARE2021 [50], OIMHS [51], FeTA2021 [52], AbdomenCT-1K [53], and BTCV [54]. To ensure experimental rigor and fairness, we apply identical data preprocessing and hyperparameter settings across all datasets, uniformly utilizing the $\\mathcal{L}_{D i c e C E}$ for training. ", "page_idx": 5}, {"type": "text", "text": "FLARE2021. An anisotropic CT dataset dedicated to abdominal organ segmentation comprises 361 training instances, 50 validation instances, and 100 test instances, categorized into four organ classes: spleen, kidney, liver, and pancreas. The spatial resolution ranges from $0.61\\;\\mathrm{mm}$ to $0.98\\;\\mathrm{mm}$ in-plane and $0.5\\;\\mathrm{mm}$ to $7.5\\;\\mathrm{mm}$ through-plane. We utilize the full set of 361 publicly labeled instances. ", "page_idx": 5}, {"type": "text", "text": "OIMHS. A fundus retinal 3D OCT segmentation dataset comprises 125 sequences, each containing 19 to 73 scans, and is categorized into four classes: retina, choroid, macular hole, and macular edema. It is a anisotropic dataset, the spacing ranges from $10.7\\,\\upmu\\mathrm{m}$ to $14.0\\,\\upmu\\mathrm{m}$ in-plane and $7.0\\,\\upmu\\mathrm{m}$ to 40.0 $\\upmu\\mathrm{m}$ through-plane. All 125 publicly available sequences are utilized in the experiments. ", "page_idx": 5}, {"type": "text", "text": "FeTA2021. A dataset consists of $120\\,\\mathrm{T}2$ -weighted fetal brain MRI reconstructions, categorized into seven classes: external cerebrospinal fluid, grey matter, white matter, ventriculus, cerebellum, deep grey matter, and brainstem. It is a typical isotropic dataset, the spacing ranges from $0.43\\ \\mathrm{mm}$ to 1 mm across all dimensions. All 80 publicly available sequences are utilized in the experiment. ", "page_idx": 5}, {"type": "text", "text": "AbdomenCT-1K. A large-scale abdominal CT dataset comprises 1,112 instances, focusing on the segmentation of four abdominal organs: liver, kidney, spleen, and pancreas. The spatial resolution ranges from $0.45\\ \\mathrm{mm}$ to $1.04\\ \\mathrm{mm}$ in-plane and $0.45\\ \\mathrm{mm}$ to $8\\ \\mathrm{mm}$ through-plane. It is also an anisotropic dataset, for our experiments, we utilize 361 instances from Task 2. ", "page_idx": 5}, {"type": "text", "text": "BTCV. A dataset consists of 50 abdominal CT instances, categorized into 13 classes. It is an anisotropic dataset, with in-plane resolution ranging from $0.59\\;\\mathrm{mm}$ to $0.98\\;\\mathrm{mm}$ and through-plane resolution ranging from $2.5\\;\\mathrm{mm}$ to $5.0\\;\\mathrm{mm}$ . All 30 labeled instances are selected for the experiments. ", "page_idx": 5}, {"type": "text", "text": "4.2 Implementation details and evaluation metrics ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Implementation details. The datasets are randomly partitioned in an 8:1:1 ratio for training, validation, and testing. To enhance segmentation performance, the data are resampled, intensity clipped, and normalized using min-max normalization. Specifically, the intensity clipping ranges are as follows: FLARE2021 [\u2212125, 275], OIMHS [0, 300], FeTA [0, 1000], AbdomenCT-1K $[-125,300]$ , and BTCV [\u2212175, 250]. During the training stage, random cropping to $96\\times96\\times96$ volumes is performed, and a sliding window method with a 0.5 overlap is used for validation. ", "page_idx": 5}, {"type": "text", "text": "All training utilizes the $\\mathcal{L}_{D i c e C E}$ function with the AdamW [55] optimizer, a learning rate of 0.0001, 80,000 training iterations, and a batch size of 2. Data augmentation techniques, including random filp, random rotation, random scaling, and random 3D elastic transformation, are applied to enhance dataset diversity and model generalization. Validation occurs every 1000 iterations during training to save the model weight with the best validation performance. The experiments are conducted on identical hardware and software environments, each workstation equipped with two NVIDIA ", "page_idx": 5}, {"type": "table", "img_path": "QI1ScdeQjp/tmp/a19bb7c4af067273b8dbdea58646070f3cb941920b5e225ff5c2014c56871006.jpg", "table_caption": ["Table 1: Comparative experimental results of uC 3DU-Net and 9 previous methods on the FLARE2021 and FeTA2021 datasets. The best values for each metric are bolded. Part of the data comes from [56]. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 2: Comparative experimental results of uC Table 3: Comparative experimental results of 3DU-Net and 7 previous methods on the OIMHS uC 3DU-Net and 7 previous methods on the dataset. The best values for each metric are high- AbdomenCT-1K dataset. The best values for each lighted in bold. Part of the data comes from [25]. metric are highlighted in bold. ", "page_idx": 6}, {"type": "table", "img_path": "QI1ScdeQjp/tmp/83979f17412901c47fb69019e2ee577a94c7e7f8d93e20574b418e7aa228fbdf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "GeForce RTX 4090 GPUs and 128GB of memory. The framework employs Python 3.9, PyTorch 2.0.0, and MONAI 0.9.0 within a Distributed Data-Parallel (DDP) training framework. ", "page_idx": 6}, {"type": "text", "text": "Evaluation metrics. We utilize IoU/mIoU, Dice, ASSD, HD, HD95, VOE, and AdjRand as evaluation metrics to comprehensively assess segmentation performance. IoU and mIoU measure the overlap accuracy between predicted and ground truth regions, providing robustness across multiple classes. Dice is particularly effective for medical images, especially in small target regions. ASSD calculates average surface distances, while HD and HD95 assess boundary accuracy, with HD95 focusing on the 95th percentile to reduce outliers. VOE examines volumetric overlap and AdjRand evaluates clustering similarity, ensuring a thorough assessment of both overlap and boundary precision. ", "page_idx": 6}, {"type": "text", "text": "4.3 Comparison with state-of-the-art methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Comparative experiments are conducted on proposed uC 3DU-Net and previous state-of-the-art methods across five diverse, publicly available 3D medical image segmentation datasets: FLARE2021, OIMHS, FeTA2021, AbdomenCT-1K, and BTCV. The results are presented in Table 1, Table 2, Table 3, and Table 13 (provided as supplementary material in the Appendix). The FLARE2021, FeTA2021, and OIMHS datasets are evaluated using five-fold cross-validations. The results from all five folds pass the Wilcoxon signed-rank test with a significance level of $\\alpha{=}0.05$ , indicating statistical equivalence in data distributions. Additionally, the results from the AbdomenCT-1K dataset all pass the one-tailed Wilcoxon signed-rank test with $p{<}0.05$ , thereby validating the SOTA performance of the uC 3D U-Net. Overall, the result indicates that the proposed uC 3DU-Net achieved SOTA performance with fewer model parameters and reduced computational complexity compared to previous segmentation methods. On the FLARE2021 and FeTA datasets, uC 3DU-Net\u2019s parameter count and FLOPs are only $40.9\\%$ and $44.7\\%$ of the suboptimal model, 3D UX-Net, yet the average Dice scores improve by $0.7\\%$ and $0.4\\%$ , respectively, with a notable $3.1\\%$ increase in the pancreas category. On the OIMHS and AbdomenCT-1K datasets, uC 3DU-Net outperformed previous models by at least $0.6\\%$ and $0.99\\%$ in Dice scores, respectively, highlighting its superior overall segmentation performance. Additionally, uC 3DU-Net showed significant improvements in boundary performance metrics such as ASSD and HD95 on the OIMHS and AbdomenCT-1K datasets, demonstrating its enhanced ability to identify and correct boundary pixels by focusing on axial-slice plane features. These findings substantiate that the 2D Conv-based uC effectively compensates for the limitations of 3D convolutions in extracting in-plane slice information to enhance model performance. Notably, the performance improvement observed on the anisotropic datasets surpasses that on the isotropic FeTA dataset, further highlighting the efficiency of the proposed uC in enhancing in-slice plane information extraction, especially for anisotropic datasets enriched with axial-slice plane features. Overall, uC significantly improves in-plane feature extraction efficiency while maintaining robust axial spatial feature representation, representing the pinnacle of current performance and parameter efficiency. ", "page_idx": 6}, {"type": "text", "text": "4.4 Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.4.1 Evaluation of 2D vs. 3D convolution for slice-plane information extraction ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Image Reconstruction Fidelity Comparison. We evaluate three methods: 3D U-Net, 3D $\\mathrm{U-Net}+3\\mathrm{D}$ uC, and 3D $\\mathrm{U-Net}+2\\mathrm{D}\\;\\mathrm{uC},$ , on their ability to reconstruct axial-slice plane images from input 3D sequence images, with the results presented in Table 4. The highest PSNR achieved by 3D $\\mathrm{U-Net}+2\\mathrm{D}\\;\\mathrm{uC}$ confirms the proposed 2D uC\u2019s superior slice-plane feature extraction and reconstruction abilities compared to 3D Conv. ", "page_idx": 7}, {"type": "text", "text": "Table 4: PSNR results of the reconstruction experiments on the OIMHS dataset, including three methods: 3D U-Net, 3D U-Net $^+$ 3D uC, and 3D $\\mathrm{U-Net}+2\\mathrm{D}\\mid$ uC. ", "page_idx": 7}, {"type": "table", "img_path": "QI1ScdeQjp/tmp/39e78701d74cff640a2804fde07740ccc3197f3d6045219e4b72934d6e5ffa3c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Impact of Convolution Dimensionality on uC Performance and Parameters. We conduct experiments on the OIMHS dataset using 3D U-Net, 3D $\\mathrm{U-Net}+3\\mathrm{D}$ uC, and $\\mathrm{3D\\,U-Net}+\\mathrm{2D\\,uC}$ across varying channel depths to evaluate the impact of 2D and 3D convolutions on uC performance and parameter count. As shown in Ta", "page_idx": 7}, {"type": "table", "img_path": "QI1ScdeQjp/tmp/a4bfef1166963cd94937357fd89520430de2e67c2dfd6987b716361a1ca49638.jpg", "table_caption": ["Table 5: Results of 3D U-Net, 3D U-Net + 3D uC, and 3D U-Net $+\\,2\\mathrm{D}\\,\\mathrm{u}C$ across different channel depth numbers on the OIMHS dataset. The best values for each metric are highlighted in bold. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "ble 5 and Fig. 5a (provided as supplementary material in the Appendix along with Fig. 5b), the 2D uC consistently achieves a superior parameter-to-performance ratio. This performance difference, independent of skip connections, is directly attributed to the differing feature extraction efficiencies of the 2D and 3D convolutions within the uC structure. This result is consistent with Table 4, further confirming the effectiveness of 2D convolutions for axial-slice plane feature extraction. Moreover, Fig. 5b indicates that the 2D uC converges faster and achieves a higher performance ceiling in the validation set, highlighting its greater performance in axial-slice plane feature extraction. ", "page_idx": 7}, {"type": "text", "text": "4.4.2 Impact of uC on various 3D segmentation backbones ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "efficacy in over- Table 6: Analysis experiments to evaluate the effectiveness and rolice plane perfor- bustness of the uC across different backbones on the FLARE2021. ", "page_idx": 7}, {"type": "text", "text": "To assess uC\u2019s coming axial-s mance drop-off, we conducted quantitative analysis experiments on the FLARE2021 dataset using numerous state-of-the-art models. As depicted in Table 6 and Fig. 3, replacing traditional skip connections with uC can significantly ", "page_idx": 7}, {"type": "table", "img_path": "QI1ScdeQjp/tmp/eed298584ee20a808506737d841be5c756cc823fbd5e9982984d19a1857c60d5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "enhance axial-slice feature extraction capabilities and thereby improve overall performance. Incorporating uC exhibited improvements in mIoU and Dice scores by $0.79\\%$ - $1.83\\%$ and $0.65\\%{-1.21\\%}$ , respectively. Integrating uC into high-performing 3D medical image segmentation models such as 3D UX-Net, which initially achieved a $93.43\\%$ Dice score, resulted in a $1.0\\%$ mIoU and $0.77\\%$ Dice score improvement. This demonstrates that even state-of-the-art models encounter inherent axial-slice plane performance drop-off issues, which uC effectively mitigates, thereby enhancing axial-slice plane feature extraction and overall performance. ", "page_idx": 7}, {"type": "text", "text": "4.4.3 Analysis on DFi", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To validate the efficiency of the DFi module compared to the two 3x3 convolutions in uC 3DU-Net for integrating 2D and 3D features, we conduct analysis experiments on the OIMHS dataset to evaluate the impact of incorporating DFi on the performance of uC 3DU-Net(32), considering different quantities and positions of uC ", "page_idx": 7}, {"type": "table", "img_path": "QI1ScdeQjp/tmp/1377f74db252ff7930fdfcf329622f6767f20fe7f29915e4a490d953e2e57f1d.jpg", "table_caption": ["Table 7: Further analysis experiments on DFI are conducted to validate its efficiency and effectiveness on the OIMHS dataset. The best values for each metric are highlighted in bold. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "module. As presented in Table 7, the results indicate that DFi achieves a slight performance improvement with lower parameter and FLOPs counts than direct concatenation. This improvement is consistently observed across different layers of uC, highlighting its robust and efficient feature fusion capabilities. The focus of the proposed DFi thus shifts towards optimizing feature fusion efficiency rather than merely achieving performance improvements. Continuous improvement efforts will prioritize refining 2D and 3D feature integration while maintaining computational efficiency. ", "page_idx": 7}, {"type": "image", "img_path": "QI1ScdeQjp/tmp/4af142a2f8bb2a579921c3e2680af25c944fb67a2b620ab3ef260856c212eabc.jpg", "img_caption": ["Figure 3: Qualitative results of the uC\u2019s impact on segmentation performance in 3D UX-Net and SegResNet backbones, applied to the FLARE2021 dataset. Segmentation results for different categories are represented in distinct colors. For improved visual clarity, the images have been cropped. Please kindly zoom in for a better view. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "QI1ScdeQjp/tmp/9117ed9a31e914b82a71d54bc6d37e5597069294f367606e2cbab8f0455ceec7.jpg", "table_caption": ["Table 8: Analysis experiments to evaluate parameters and performance of 3D UX-Net across various channel depths and the integration of $\\mathbf{u}\\mathbf{C}$ on FLARE2021 and OIMHS datasets. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.4.4 Performance Comparison with Varying Channel Depths in 3D UX-Net ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The low parameter performance ratio observed in 3D CNNs is largely due to their reliance on axially symmetric 3D convolutions, which extract spatial features well but struggle to capture critical axialslice plane details. Introducing the uC structure significantly enhances the capability of 3D CNNs to extract axial-slice plane features. Can incorporating uC allow a reduction in channel count, thereby decreasing computational cost? To explore this, we conduct an analytical experiment utilizing 3D UXNet as the backbone of FLARE2021 and OIMHS datasets to investigate the impact of channel depth and $\\mathbf{u}C$ integration on segmentation performance. As illustrated in Table 8, achieving a $0.66\\%$ Dice improvement with the 3D UX-Net on FLARE2021 requires a tenfold model increase, highlighting the challenge of enhancing performance with 3D CNNs. The incorporation of uC mitigates the axial-slice plane performance drop-off, resulting in performance enhancements across 3D UX-Net with different channel depths. Notably, a 24-channel depth uC 3D UX-Net surpasses the original 48-channel depth 3D UX-Net on both datasets, with parameter count and FLOPs reduced to only $29\\%$ and $34.3\\%$ , respectively. This highlights uC\u2019s capability to achieve superior performance with fewer parameters, suggesting that substituting traditional skip connections with uC and reducing channel depth is a viable approach to achieve substantial reductions in parameter count and computational load while maintaining or even improving performance. The qualitative results are further illustrated in Fig. 4. ", "page_idx": 8}, {"type": "text", "text": "4.5 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Stage Selection for uC integration. By replacing skip connections with uC at different stages, we validate the impact of introducing 2D axial-slice plane features at varying network layers on model performance. All models had an initial channel depth of 32. ", "page_idx": 8}, {"type": "table", "img_path": "QI1ScdeQjp/tmp/bb6f0cc75d568eb9da570b86ce733e64502e9047732e4136d9ae8ebf9bd0b803.jpg", "table_caption": ["Table 9: Ablation study of different stage selection for uC integration on the OIMHS dataset. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "QI1ScdeQjp/tmp/d3083409a3ce79c39378c48f1d72ad736298bfa86213b4e13c976b3d500aedc3.jpg", "img_caption": ["Figure 4: Qualitative results of the segmentation performance on 3D UX-Net and 3D UXNET $\\mathbf{\\dot{\\Sigma}}+\\mathbf{u}\\mathbf{C}$ with various channel depths on the FLARE2021 and OIMHS datasets are presented. Segmentation results for different categories are represented in distinct colors. For improved visual clarity, the images have been cropped. Key regions of the qualitative results have been locally magnified for better viewing. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "As shown in Table 9, supplementing initial feature information in shallower layers, closer to the original image, resulted in better performance improvements, with deeper layers showing diminishing returns. Hence, to balance parameter efficiency, we only replace skip connections at stages 1 to 3. Results indicate that using skip connections in the first three stages significantly enhances model performance, with the DFi module effectively integrating features extracted by 3D CNNs and uC. ", "page_idx": 9}, {"type": "text", "text": "Channel Depth Configuration. ", "text_level": 1, "page_idx": 9}, {"type": "table", "img_path": "QI1ScdeQjp/tmp/1413fad1b6c359b31280bb28ff4bf726245874e971ba45a3bfd3438f48b76801.jpg", "table_caption": ["Table 10: Ablation study of different Channel depths of uC 3DUNet on the OIMHS dataset. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "We experiment with channel depths of 16, 24, and 32 for the proposed uC 3DU-Net, all surpassing the original 32-channel U-Net, as shown in Table 10. Per", "page_idx": 9}, {"type": "text", "text": "formance improved with increased channels, but efficiency dropped drastically at 32 channels compared to 24. Notably, the 16-channel uC 3DU-Net performed better than the 32-channel U-Net with similar parameter counts and FLOPs, demonstrating a $1.74\\%$ Dice score improvement and a 4.95 HD95 reduction. This underscores that efficiently capturing slice plane features with uC significantly enhances computational efficiency in 3D CNN architecture. Thus, replacing skip connections with uC and reducing channel depth is a straightforward strategy to achieve substantial parameter and computational load reductions while maintaining or enhancing performance. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion and future work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose a U-shaped Connection (uC) for enhancing 3D CNN-based medical image segmentation architecture. This approach is specifically designed to address the inherent axial-slice plane performance drop-off in 3D CNNs, characterized by their inefficiency in extracting high-density axial-slice plane features, which are crucial for accurate 3D medical image segmentation. By replacing traditional skip connections in 3D U-Net with uC, we further develop the uC 3DU-Net, capitalizing on more efficient feature extraction capabilities of both 3D sequential spatial features and 2D axialslice plane features, reaching the best segmentation accuracy and computational efficiency ratio among all previous SoTA methods. Empirical evaluations on diverse datasets demonstrate that uC 3DU-Net consistently outperforms previous SoTA 3D medical segmentation methods, with notably reduced parameters and FLOPs. This underscores the transformative potential of the uC structure in revolutionizing medical volumetric segmentation by breaking through the intrinsic limitations of 3D convolutions. Future research will extend the application of the uC structure to a broader range of volumetric segmentation tasks, with the aim of continually advancing the performance and efficiency of 3D image segmentation models. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Vibhu Kapoor, Barry M McCook, and Frank S Torok. An introduction to pet-ct imaging. Radiographics, 24(2):523\u2013543, 2004.   \n[2] Anja Borsdorf, Rainer Raupach, Thomas Flohr, and Joachim Hornegger. Wavelet based noise reduction in ct-images using correlation analysis. IEEE transactions on medical imaging, 27(12):1685\u20131703, 2008.   \n[3] Dianna D Cody. Aapm/rsna physics tutorial for residents: topics in ct: image processing in ct. Radiographics, 22(5):1255\u20131268, 2002.   \n[4] Francis Zarb, Louise Rainford, and Mark F McEntee. Image quality assessment tools for optimization of ct images. Radiography, 16(2):147\u2013153, 2010.   \n[5] S\u00e9rgio Pereira, Adriano Pinto, Victor Alves, and Carlos A Silva. Brain tumor segmentation using convolutional neural networks in mri images. IEEE transactions on medical imaging, 35(5):1240\u20131251, 2016.   \n[6] Fangzhou Liao, Xi Chen, Xiaolin Hu, and Sen Song. Estimation of the volume of the left ventricle from mri images using deep neural networks. IEEE transactions on cybernetics, 49(2):495\u2013504, 2017.   \n[7] Girish Katti, Syeda Arshiya Ara, and Ayesha Shireen. Magnetic resonance imaging (mri)\u2013a review. International journal of dental clinics, 3(1):65\u201370, 2011.   \n[8] Mohd Ali Balafar, Abdul Rahman Ramli, M Iqbal Saripan, and Syamsiah Mashohor. Review of brain mri image segmentation methods. Artificial Intelligence Review, 33:261\u2013274, 2010.   \n[9] Jin Liu, Yi Pan, Min Li, Ziyue Chen, Lu Tang, Chengqian Lu, and Jianxin Wang. Applications of deep learning to mri images: A survey. Big Data Mining and Analytics, 1(1):1\u201318, 2018.   \n[10] Karl Fritscher, Patrik Raudaschl, Paolo Zaffino, Maria Francesca Spadea, Gregory C Sharp, and Rainer Schubert. Deep neural networks for fast segmentation of 3d medical images. In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2016: 19th International Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part II 19, pages 158\u2013165. Springer, 2016.   \n[11] Xiaojie Huang, Junjie Shan, and Vivek Vaidya. Lung nodule detection in ct using 3d convolutional neural networks. In 2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017), pages 379\u2013383. IEEE, 2017.   \n[12] Qi Dou, Hao Chen, Lequan Yu, Lei Zhao, Jing Qin, Defeng Wang, Vincent CT Mok, Lin Shi, and PhengAnn Heng. Automatic detection of cerebral microbleeds from mr images via 3d convolutional neural networks. IEEE Transactions on Medical Imaging, 35(5):1182\u20131195, 2016.   \n[13] Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud Arindra Adiyoso Setio, Francesco Ciompi, Mohsen Ghafoorian, Jeroen AWM van der Laak, Bram van Ginneken, and Clara I S\u00e1nchez. A survey on deep learning in medical image analysis. Medical image analysis, 42:60\u201388, 2017.   \n[14] Hang Yu, Laurence T Yang, Qingchen Zhang, David Armstrong, and M Jamal Deen. Convolutional neural networks for medical image analysis: state-of-the-art, comparisons, improvement and perspectives. Neurocomputing, 444:92\u2013110, 2021.   \n[15] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning, pages 6105\u20136114. PMLR, 2019.   \n[16] Neena Aloysius and M Geetha. A review on deep convolutional neural networks. In 2017 international conference on communication and signal processing (ICCSP), pages 0588\u20130592. IEEE, 2017.   \n[17] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Communications of the ACM, 60(6):84\u201390, 2017.   \n[18] Rikiya Yamashita, Mizuho Nishio, Richard Kinh Gian Do, and Kaori Togashi. Convolutional neural networks: an overview and application in radiology. Insights into imaging, 9:611\u2013629, 2018.   \n[19] Jiuxiang Gu, Zhenhua Wang, Jason Kuen, Lianyang Ma, Amir Shahroudy, Bing Shuai, Ting Liu, Xingxing Wang, Gang Wang, Jianfei Cai, et al. Recent advances in convolutional neural networks. Pattern recognition, 77:354\u2013377, 2018.   \n[20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[21] Jianpeng Zhang, Yutong Xie, Yan Wang, and Yong Xia. Inter-slice context residual learning for 3d medical image segmentation. IEEE Transactions on Medical Imaging, 40(2):661\u2013672, 2020.   \n[22] Qihang Yu, Dong Yang, Holger Roth, Yutong Bai, Yixiao Zhang, Alan L Yuille, and Daguang Xu. C2fnas: Coarse-to-fine neural architecture search for 3d medical image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4126\u20134135, 2020.   \n[23] Xuan Liao, Wenhao Li, Qisen Xu, Xiangfeng Wang, Bo Jin, Xiaoyun Zhang, Yanfeng Wang, and Ya Zhang. Iteratively-refined interactive 3d medical image segmentation with multi-agent reinforcement learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9394\u20139402, 2020.   \n[24] Xingru Huang, Retesh Bajaj, Yilong Li, Xin Ye, Ji Lin, Francesca Pugliese, Anantharaman Ramasamy, Yue Gu, Yaqi Wang, Ryo Torii, et al. Post-ivus: A perceptual organisation-aware selective transformer framework for intravascular ultrasound segmentation. Medical Image Analysis, 89:102922, 2023.   \n[25] Xingru Huang, Jian Huang, Kai Zhao, Tianyun Zhang, Zhi Li, Changpeng Yue, Wenhao Chen, Ruihao Wang, Xuanbin Chen, Qianni Zhang, et al. Sasan: Spectrum-axial spatial approach networks for medical image segmentation. IEEE Transactions on Medical Imaging, 2024.   \n[26] \u00d6zg\u00fcn \u00c7i\u00e7ek, Ahmed Abdulkadir, Soeren S Lienkamp, Thomas Brox, and Olaf Ronneberger. 3d unet: learning dense volumetric segmentation from sparse annotation. In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2016: 19th International Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part II 19, pages 424\u2013432. Springer, 2016.   \n[27] Hao Wang, Yancheng Yang, Xun Cao, and Xuesong Liu. Multi-scale attention u-net for medical image segmentation. arXiv preprint arXiv:2006.01992, 2020.   \n[28] Yuncheng Jiang, Zixun Zhang, Shixi Qin, Yao Guo, Zhen Li, and Shuguang Cui. Apaunet: axis projection attention unet for small target in 3d medical segmentation. In Proceedings of the Asian Conference on Computer Vision, pages 283\u2013298, 2022.   \n[29] Ali Hatamizadeh, Vishwesh Nath, Yucheng Tang, Dong Yang, Holger Roth, Daguang Xu, and Ling Yang. Unetr: Transformers for 3d medical image segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 574\u2013584, 2021.   \n[30] Abdelrahman M Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, and Fahad Shahbaz Khan. Unetr $^{++}$ : delving into efficient and accurate 3d medical image segmentation. IEEE Transactions on Medical Imaging, 2024.   \n[31] Jiancheng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L Yuille, and Yuyin Zhou. Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306, 2021.   \n[32] Wenxuan Wang, Chen Chen, Meng Ding, Hong Yu, Sen Zha, and Jiangyun Li. Transbts: Multimodal brain tumor segmentation using transformer. In Medical Image Computing and Computer Assisted Intervention\u2013MICCAI 2021: 24th International Conference, Strasbourg, France, September 27\u2013October 1, 2021, Proceedings, Part I 24, pages 109\u2013119. Springer, 2021.   \n[33] Yutong Xie, Jianpeng Zhang, Chunhua Shen, and Yong Xia. Cotr: Efficiently bridging cnn and transformer for 3d medical image segmentation. In Medical Image Computing and Computer Assisted Intervention\u2013 MICCAI 2021: 24th International Conference, Strasbourg, France, September 27\u2013October 1, 2021, Proceedings, Part III 24, pages 171\u2013180. Springer, 2021.   \n[34] Ali Hatamizadeh, Vishwesh Nath, Yucheng Tang, Dong Yang, Holger R Roth, and Daguang Xu. Swin unetr: Swin transformers for semantic segmentation of brain tumors in mri images. In International MICCAI Brainlesion Workshop, pages 272\u2013284. Springer, 2021.   \n[35] Fabian Isensee, Paul F Jaeger, Simon AA Kohl, Jens Petersen, and Klaus H Maier-Hein. nnu-net: a self-configuring method for deep learning-based biomedical image segmentation. Nature Methods, 18(2):203\u2013211, 2021.   \n[36] Xinjian Chen, Jayaram K Udupa, Ulas Bagci, Ying Zhuge, and Jianhua Yao. Medical image segmentation by combining graph cuts and oriented active appearance models. IEEE transactions on image processing, 21(4):2035\u20132046, 2012.   \n[37] Rongjian Zhao, Buyue Qian, Xianli Zhang, Yang Li, Rong Wei, Yang Liu, and Yinggang Pan. Rethinking dice loss for medical image segmentation. In 2020 IEEE International Conference on Data Mining (ICDM), pages 851\u2013860. IEEE, 2020.   \n[38] Ju Xu, Mengzhang Li, and Zhanxing Zhu. Automatic data augmentation for 3d medical image segmentation. In Medical Image Computing and Computer Assisted Intervention\u2013MICCAI 2020: 23rd International Conference, Lima, Peru, October 4\u20138, 2020, Proceedings, Part I 23, pages 378\u2013387. Springer, 2020.   \n[39] Shifeng Chen, Kai Ma, and Yefeng Zheng. Med3d: Transfer learning for 3d medical image analysis. arXiv preprint arXiv:1904.00625, 2019.   \n[40] Xiaojiao Mao, Chunhua Shen, and Yu-Bin Yang. Image restoration using very deep convolutional encoderdecoder networks with symmetric skip connections. Advances in neural information processing systems, 29, 2016.   \n[41] Michal Drozdzal, Eugene Vorontsov, Gabriel Chartrand, Samuel Kadoury, and Chris Pal. The importance of skip connections in biomedical image segmentation. In International workshop on deep learning in medical image analysis, international workshop on large-scale annotation of biomedical data and expert label synthesis, pages 179\u2013187. Springer, 2016.   \n[42] Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E Gonzalez. Skipnet: Learning dynamic routing in convolutional networks. In Proceedings of the European conference on computer vision (ECCV), pages 409\u2013424, 2018.   \n[43] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[44] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234\u2013241. Springer, 2015.   \n[45] Ozan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew Lee, Mattias Heinrich, Kazunari Misawa, Kensaku Mori, Steven McDonagh, Nils Y Hammerla, Bernhard Kainz, et al. Attention u-net: Learning where to look for the pancreas. arXiv preprint arXiv:1804.03999, 2018.   \n[46] Yufeng Wu, Jiachen Wu, Shangzhong Jin, Liangcai Cao, and Guofan Jin. Dense-u-net: dense encoder\u2013 decoder network for holographic imaging of 3d particle fields. Optics Communications, 493:126970, 2021.   \n[47] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700\u20134708, 2017.   \n[48] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu. Image super-resolution using very deep residual channel attention networks. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018.   \n[49] Xiaomeng Li, Hao Chen, Xiaojuan Qi, Qi Dou, Chi-Wing Fu, and Pheng-Ann Heng. H-denseunet: hybrid densely connected unet for liver and tumor segmentation from ct volumes. IEEE transactions on medical imaging, 37(12):2663\u20132674, 2018.   \n[50] Jun Ma, Yao Zhang, Song Gu, Xingle An, Zhihe Wang, Cheng Ge, Congcong Wang, Fan Zhang, Yu Wang, Yinan Xu, et al. Fast and low-gpu-memory abdomen ct organ segmentation: the flare challenge. Medical Image Analysis, 82:102616, 2022.   \n[51] Xin Ye, Shucheng He, Xiaxing Zhong, Jiafeng Yu, Shangchao Yang, Yingjiao Shen, Yiqi Chen, Yaqi Wang, Xingru Huang, and Lijun Shen. Oimhs: An optical coherence tomography image dataset based on macular hole manual segmentation. Scientific Data, 10(1):769, 2023.   \n[52] Kelly Payette, Priscille de Dumast, Hamza Kebiri, Ivan Ezhov, Johannes C Paetzold, Suprosanna Shit, Asim Iqbal, Romesa Khan, Raimund Kottke, Patrice Grehten, et al. An automatic multi-tissue human fetal brain segmentation benchmark using the fetal tissue annotation dataset. Scientific data, 8(1):167, 2021.   \n[53] Jun Ma, Yao Zhang, Song Gu, Cheng Zhu, Cheng Ge, Yichi Zhang, Xingle An, Congcong Wang, Qiyuan Wang, Xin Liu, Shucheng Cao, Qi Zhang, Shangqing Liu, Yunpeng Wang, Yuhui Li, Jian He, and Xiaoping Yang. Abdomenct-1k: Is abdominal organ segmentation a solved problem? IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(10):6695\u20136714, 2022.   \n[54] Bennett Landman, Zhoubing Xu, J Igelsias, Martin Styner, Thomas Langerak, and Arno Klein. Miccai multi-atlas labeling beyond the cranial vault\u2013workshop and challenge. In Proc. MICCAI Multi-Atlas Labeling Beyond Cranial Vault\u2014Workshop Challenge, volume 5, page 12, 2015.   \n[55] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \n[56] Ho Hin Lee, Shunxing Bao, Yuankai Huo, and Bennett A. Landman. 3d UX-net: A large kernel volumetric convnet modernizing hierarchical transformer for medical image segmentation. The Eleventh International Conference on Learning Representations, 2023.   \n[57] \u00d6zg\u00fcn \u00c7i\u00e7ek, Ahmed Abdulkadir, Soeren S Lienkamp, Thomas Brox, and Olaf Ronneberger. 3d unet: Learning dense volumetric segmentation from sparse annotation. Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2016, pages 424\u2013432, 2016.   \n[58] Andriy Myronenko. 3d mri brain tumor segmentation using autoencoder regularization. In Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 4th International Workshop, BrainLes 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Revised Selected Papers, Part II 4, pages 311\u2013320. Springer, 2019.   \n[59] Ho Hin Lee, Yucheng Tang, Shunxing Bao, Richard G Abramson, Yuankai Huo, and Bennett A Landman. Rap-net: Coarse-to-fine multi-organ segmentation with single random anatomical prior. In 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), pages 1491\u20131494. IEEE, 2021.   \n[60] Hong-Yu Zhou, Jiansen Guo, Yinghao Zhang, Xiaoguang Han, Lequan Yu, Liansheng Wang, and Yizhou Yu. nnformer: Volumetric medical image segmentation via a 3d transformer. IEEE Transactions on Image Processing, pages 4036\u20134045, 2023.   \n[61] Xingru Huang, Jian Huang, Kai Zhao, Tianyun Zhang, Zhi Li, Changpeng Yue, Wenhao Chen, Ruihao Wang, Xuanbin Chen, Qianni Zhang, Ying Fu, Yangyundou Wang, and Yihao Guo. Sasan: Spectrum-axial spatial approach networks for medical image segmentation. IEEE Transactions on Medical Imaging, pages 1\u20131, 2024.   \n[62] Reza Azad, Leon Niggemeier, Michael H\u00fcttemann, Amirhossein Kazerouni, Ehsan Khodapanah Aghdam, Yury Velichko, Ulas Bagci, and Dorit Merhof. Beyond self-attention: Deformable large kernel attention for medical image segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1287\u20131297, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We first introduce the details of the datasets and implementation in Sec. A, followed by the supplementary experiments and analysis in Sec. B. Furthermore, the experimental results are detailed in Sec. C. Lastly, additional qualitative results are provided in Sec. D. ", "page_idx": 14}, {"type": "text", "text": "A Details of Datasets and Implementation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Public datasets details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Here, we provide detailed information on the five public datasets used for experiments in Table 11. ", "page_idx": 14}, {"type": "text", "text": "Table 11: Detailed information of five publicly available Datasets: FLARE2021, OIMHS, FeTA02021, AbdomenCT-1K, and BTCV. ", "page_idx": 14}, {"type": "table", "img_path": "QI1ScdeQjp/tmp/d8e08561abeee112f94f7973b61daa3153a540deb83e03cc961b0db405a4de39.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.2 Data Preprocessing & Implementation Detail ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In the 4.2, we provide a detailed description of our data preprocessing process and offer more comprehensive hyperparameter settings in Table 12. For the BTCV dataset, we specifically incorporate resampling to specific pixel spacing(1.5, 1.5, 2.0) and random intensity shift as additional data augmentation strategies. ", "page_idx": 14}, {"type": "table", "img_path": "QI1ScdeQjp/tmp/7cb3b7a6d2dac23f3a9f1a6ebcf9e4aa7f97545350839b501d82447819b9fc83.jpg", "table_caption": ["Table 12: Detail hyperparameters of training scenarios on five public datasets. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "B Supplementary experiments and analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Benchmarking results on BTCV ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We present comparative experiments of the proposed uC 3D-UNet against previous classic segmentation methods on the BTCV dataset, as shown in Table 13. Given the smaller dataset size, CNN-based models outperformed Transformer-based models. The results indicate that uC 3D-UNet maintains outstanding performance on the challenging 13 classes BTCV segmentation dataset. ", "page_idx": 15}, {"type": "text", "text": "Table 13: Comparative experimental results of uC 3DU-Net and 4 previous methods on the BTCV Standard dataset. The best values for each metric are highlighted in bold. ", "page_idx": 15}, {"type": "table", "img_path": "QI1ScdeQjp/tmp/95340c8158dd4d67b7aa0fc18c3237dbcfb66265a8dcdb659ebfb16f3a0f865a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "B.2 Additional comparative experiments ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here, we add two recently proposed models as baselines for comparative experiments across four publicly available datasets: FLARE2021, OIMHS, FeTA2021, and AbdomenCT-1K. The experimental results in Table 14 demonstrate that our proposed uC 3D-UNet maintains superior performance compared to the latest models. ", "page_idx": 15}, {"type": "text", "text": "Table 14: Comparative experimental results of the proposed uC 3D-UNet against two added baselines on the FLARE2021, FeTA2021, OIMHS, and AbdomenCT-1K datasets. ", "page_idx": 15}, {"type": "table", "img_path": "QI1ScdeQjp/tmp/04768ffa471eb2b493ae6216f36ecbd35d9fb9fa6e48f85349debc49d6e7b368.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "B.3 Dimensional Slicing Analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To validate the effectiveness of the proposed uC in enhancing axial-slice plane feature extraction, we conduct experiments with 3D U-Net + 2D uC by slicing input tensors along different dimensions. As shown in Table 15, although the 2D slicing augmentation with uC consistently enhances performance across all planes, slicing along the time-axial slice plane dimension yields the best results. This result reflects the denser information available in the axial-slice plane of 3D medical images, which improves performance when using the proposed uC method. Furthermore, these findings support why 2D uC achieves superior parameter efficiency in 3D image segmentation tasks compared to purely 3D convolutions. ", "page_idx": 15}, {"type": "table", "img_path": "QI1ScdeQjp/tmp/8dc6bb3df01a1ab8eedc977b2690c723b8665ec73df9c64769e69496cd66f9b1.jpg", "table_caption": ["Table 15: The results of applying 2D uC on slices along the W, H, and D dimensions on the OIMHS dataset. The best values for each metric are highlighted in bold. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.4 Visualization of the Impact of Convolution Dimensionality on uC Performance and Parameters ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Here, we present the visualization results from Sec. 4.4.1, as shown in Fig. 5. ", "page_idx": 16}, {"type": "image", "img_path": "QI1ScdeQjp/tmp/3bc203aacd573bdbc6acb05c0e6df31f796491e2ff2cfa06acdf923efa3cca51.jpg", "img_caption": ["(b) Validation curve showing Dice scores on the (a) Performance comparison of 3D U-Net, 3D U-Net $^+$ OIMHS dataset, with the channel depth for all three 3D uC, and 3D U-Net $+\\,2\\mathrm{D}$ uC across varying channel models(3D U-Net, 3D $\\mathrm{U-Net}+3\\mathrm{D}\\;\\mathrm{uC}$ , and 3D U-Net depths, with circle size representing parameter count. $+\\,2\\mathrm{D}$ uC) set to 32. The horizontal axis represents the The horizontal axis represents channel depth numbers. number of training iterations. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "QI1ScdeQjp/tmp/ff3e408c8bdd30403ca21b876b1518b8b0cb978bf1d2aaa1f6ddc4c24d7b3c85.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 5: Visualization of the impact of 2D and 3D Conv on uC. (a) depicts the relationship between performance and channel depth. (b) shows the validation curve indicating the training efficiency. ", "page_idx": 16}, {"type": "text", "text": "C Details of experimental results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we present the details of our experimental results. For comparative experiments, as the baselines for FLARE2021 and FeTA2021 utilize data from [56], we provide detailed experimental results for all categories in the OIMHS and AbdomenCT-1K datasets (with part of OIMHS comparative data comes from [25], which are not included here), as shown in Tables 16 and 17. Tables 18, 19, 20, 21, and 22 present detailed data for all categories in the analytical experiments, while Tables 23 and 24 display the detailed data for all categories in the ablation studies. The comprehensive results across all categories further demonstrate that integrating the uC module significantly enhances the backbone models\u2019 ability to extract 2D plane information from axial slices, while retaining the extensive volumetric feature extraction capabilities of 3D convolutions. This integration effectively addresses the challenge of anisotropic medical imaging, achieving superior performance with reduced computational complexity and fewer parameters compared to previous models. As a result, the proposed approach achieves an optimal parameter-to-performance ratio. ", "page_idx": 16}, {"type": "table", "img_path": "QI1ScdeQjp/tmp/a7e3a2922231be1aa544dda960bda2b76be75103c57191b4a41787b5caae3574.jpg", "table_caption": ["Table 16: Detailed results of uC 3DU-Net and 4 previous methods on the OIMHS dataset. The best values for each metric are highlighted in bold. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "QI1ScdeQjp/tmp/ddbb892c0fe1e50409bb6479dcce4f90cd879db4296f459080fba46c65bc3adc.jpg", "table_caption": ["Table 17: Detailed results of uC 3DU-Net and 7 previous methods on the AbdomenCT-1K dataset. The best values for each metric are highlighted in bold. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "QI1ScdeQjp/tmp/18dc17de77384e938b99e0974f4dfbe592a7a5d80ae23b30c478a9539a1d8b79.jpg", "table_caption": ["Table 18: The detailed results of 3D U-Net, 3D U-Net + 3D uC, and 3D U-Net + 2D uC across different channel depth numbers on the OIMHS dataset. The best values for each metric are highlighted in bold. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "QI1ScdeQjp/tmp/e3c1c604a6c9326eed5275cf4e579cfd1242830b932c3f953cca2600c5fd425c.jpg", "table_caption": ["Table 19: The detailed results of the analysis experiments evaluating the effectiveness and robustness of the uC across different backbones on the FLARE2021 dataset. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "QI1ScdeQjp/tmp/58c5bbec1ca3c3a73b7ad2f6cfd351122fe4f8bb97ea73cbcd7d5f6a9f77715e.jpg", "table_caption": ["Table 20: The detailed results of the analysis experiments on DFI are presented to demonstrate its efficiency and effectiveness on the OIMHS dataset. The best values for each metric are highlighted in bold. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "QI1ScdeQjp/tmp/9c0da2f813f7f23fb0a94ba70e92a016a6fdc4dad95a6a79e33eb584d4d081a5.jpg", "table_caption": ["Table 21: The detailed results of the analysis experiments evaluating parameters and performance of 3D UX-Net across various channel depths and the integration of uC on the FLARE2021 dataset. The best values for each metric are highlighted in bold. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "QI1ScdeQjp/tmp/c7a0dd3e61996e73b6546171b281f4d52d6b5599e3d64820354025222d1d2e49.jpg", "table_caption": ["Table 22: The detailed results of the analysis experiments evaluating parameters and performance of 3D UX-Net across various channel depths and the integration of uC on the OIMHS dataset. The best values for each metric are highlighted in bold. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "QI1ScdeQjp/tmp/000037e476a599e472bfc52618f841b4bbc11a86cef648902bd46f74af80e848.jpg", "table_caption": ["Table 23: Detailed results of the ablation study on different stage selections for uC integration in 3D U-Net. The best values for each metric are highlighted in bold. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "QI1ScdeQjp/tmp/84ccf3ea6bcea0f1c56951e358008363dc809eb9d4569909de1532741301e51e.jpg", "table_caption": ["Table 24: Detailed results of the ablation study on different channel depth configurations of uC 3DU-Net on the OIMHS dataset. The best values for each metric are highlighted in bold. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "D Qualitative Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we present additional qualitative results. Fig. 6, Fig. 7, Fig. 8, and Fig. 9 further show the superior segmentation accuracy of the proposed uC 3D U-Net compared to previous models on the FLARE2021, FeTA2021, OIMHS, and AbdomenCT-1K datasets. Fig. 10 visualizes the impact of integrating the uC module with backbone models on the FLARE2021 dataset. These visualizations demonstrate that the integration of uC effectively enhances the model\u2019s ability to extract axial-slice plane information while retaining effective volumetric feature extraction, addressing challenges related to anisotropic medical imaging. This integration achieves improved performance with reduced computational cost and fewer parameters, resulting in an optimal parameter-to-performance ratio compared to previous models. ", "page_idx": 19}, {"type": "image", "img_path": "QI1ScdeQjp/tmp/20757baa41dc2225a19e8c96cd4b3616e256fd0aae44e211a43933e816216684.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 6: More visual results of the proposed uC 3DU-Net on the FLARE2021 dataset. For enhanced visual clarity, the displayed images have been cropped. Please kindly zoom in for a better view. ", "page_idx": 19}, {"type": "image", "img_path": "QI1ScdeQjp/tmp/7056c6f125221623f9eab836848ab54b7a015f4be6084b3f30a50cc45dc729b1.jpg", "img_caption": ["Figure 7: More visual results of the proposed uC 3DU-Net on the FeTA2021 dataset. For enhanced visual clarity, the displayed images have been cropped. Please kindly zoom in for a better view. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "QI1ScdeQjp/tmp/85280672679bbbf0ada48ceafdb57a9078d7ef7f70628accb9731dfb42562164.jpg", "img_caption": ["Figure 8: The visual comparison of the validation results on the OIMHS dataset for uC 3DU-Net, and 6 previous segmentation methods. We have selected 6 representative sequences for display. For enhanced visual clarity, the displayed images have been cropped. Please zoom in for a better view. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "QI1ScdeQjp/tmp/69b1b7d3778acfc1f9357a27c2eeb4551adae5c29a82d3ab65f9740b7d671e0a.jpg", "img_caption": ["Original Image GT 3D U-Net nnFormer Swin UNETR TransBTS UNETR 3D UX-Net uC 3DU-Net "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 9: The visual comparison of the validation results on the ABCT1K dataset for uC 3DU-Net, and 3 previous segmentation methods. We have selected 5 representative sequences for display. For enhanced visual clarity, the displayed images have been cropped. Please zoom in for a better view. ", "page_idx": 21}, {"type": "image", "img_path": "QI1ScdeQjp/tmp/8a03d0f9d9cb086cd6c0151e30c375c1339cde8e0a62da7ecd626ff68a3ea53e.jpg", "img_caption": ["Figure 10: Visual results showcasing the differences in integrating uC for 3D UX-Net, SegResNet, SwinUNETR, and TransBTS. For each model, We have selected two representative sequences for display. For enhanced visual clarity, the displayed images have been cropped. Please zoom in for a better view. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The primary claims focus on the integration of 2D U-Net-derived skip connections into 3D CNN architectures, specifically addressing the inefficiencies in volumetric feature extraction due to varying information densities along different axes, presented as the uC and the enhanced uC 3DU-Net, is rigorously validated through various experiments on multiple datasets, improving performance with reduced computational complexity. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We have not identified any apparent deficiencies in the proposed method at this time. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The paper meticulously details the assumptions and provides comprehensive proofs for all theoretical results, ensuring clarity and correctness. This includes the full set of mathematical assumptions and derivations presented in Section 4 and Appendix, which underpin the proposed uC 3DU-Net architecture\u2019s performance improvements. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Specific experimental setups, dataset descriptions, hyperparameters, and detailed architecture of uC 3DU-Net are provided in the paper. The code is accessible on GitHub, and all five datasets used are publicly available. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The paper provides open access to its code on GitHub, along with detailed instructions and descriptions. All data utilized in this study is widely recognized and publicly accessible. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details. ", "page_idx": 23}, {"type": "text", "text": "\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The paper meticulously delineates the training and testing specifics, including data splits, hyperparameter choices, optimizer type, and the rationale behind these selections. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The paper reports error bars and statistical significance to ensure robust experimental validation. Details are meticulously documented in the Experiments and Results section and Appendix. The metrics used, such as IoU, Dice, ASSD, HD, HD95, VOE, and AdjRand, provide comprehensive statistical evaluations across multiple datasets. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper delineates the specifications of the computational resources employed, including compute workers, memory capacity, and execution time. Detailed information includes hardware: (NVIDIA GeForce RTX 4090 GPUs, 128GB memory), software environment: (Python 3.9, PyTorch 2.0.0, MONAI 0.9.0), and the execution framework: (Distributed Data-Parallel), ensuring reproducibility of the experiments. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: This research meticulously adheres to the NeurIPS Code of Ethics, prioritizing ethical considerations throughout the methodology. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: The paper outlines the heightened efficiency and accuracy achieved in 3D medical image segmentation, potentially enhancing diagnostic precision and patient outcomes. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper outlines rigorous validation on public datasets, ensuring reproducibility and mitigating misuse risks with transparent methodologies and ethical considerations for model release and application. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: This paper thoroughly acknowledges the original creators of all utilized assets, including code, datasets, and models, within the text. Each asset\u2019s license and terms of use are meticulously observed, ensuring full compliance with all legal and ethical standards. For further details, refer to the \u2019Datasets\u2019 and \u2019Implementation Details\u2019 sections where attributions are explicitly cited. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The proposed U-shaped Connection $(\\mathrm{uC})$ and its integration into the 3DU-Net are comprehensively detailed in the method chapter, including their architecture, implementation, and the datasets used for validation. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper focuses on the technical advancements in medical image segmentation using the uC 3DU-Net architecture and does not involve any crowdsourcing experiments or research with human subjects. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This study focuses on the development of a novel 3D medical image segmentation technique. As it does not involve direct engagement with study participants, IRB approval is not required. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]