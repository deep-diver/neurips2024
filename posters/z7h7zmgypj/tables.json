[{"figure_path": "z7h7zMgyPJ/tables/tables_2_1.jpg", "caption": "Figure 1: Top is Higgs, Left plot is Boone. Right plot is Forest Cover", "description": "This figure presents the results of experiments comparing the performance of different sample optimal weak-to-strong learners on three large datasets: Higgs, Boone, and Forest Cover.  The x-axis represents the number of voting classifiers used, and the y-axis represents the test accuracy.  The plots show the average accuracy across five runs with different random seeds for each algorithm: AdaBoost, LarsenRitzert, MAJORITY-OF-X, and BAGGEDADABOOST.  The figure allows for a visual comparison of these algorithms' performance as the number of classifiers increases.", "section": "4 Experiments"}, {"figure_path": "z7h7zMgyPJ/tables/tables_3_1.jpg", "caption": "Figure 1: Top is Higgs, Left plot is Boone. Right plot is Forest Cover", "description": "This figure presents the results of experiments comparing different sample optimal weak-to-strong learners on three large datasets: Higgs, Boone, and Forest Cover.  The x-axis represents the number of voting classifiers, and the y-axis represents the test accuracy.  The figure shows the performance of AdaBoost, LarsenRitzert, Majority-of-X (with X varying), and BaggedAdaBoost.  Each algorithm's performance is averaged over five runs with different random seeds.", "section": "4 Experiments"}, {"figure_path": "z7h7zMgyPJ/tables/tables_3_2.jpg", "caption": "Algorithm 3: LARSENRITZERT(S, W)", "description": "This algorithm uses the SUBSAMPLE algorithm as a subroutine to generate a list of training sets. AdaBoost is then run on each training set in the list to obtain a classifier. Finally, the classifiers are combined using a majority vote.", "section": "Previous Optimal Weak-to-Strong Learners"}, {"figure_path": "z7h7zMgyPJ/tables/tables_3_3.jpg", "caption": "Figure 1: Top is Higgs, Left plot is Boone. Right plot is Forest Cover", "description": "This figure presents the results of experiments comparing the performance of different sample optimal weak-to-strong learners on three large datasets: Higgs, Boone, and Forest Cover.  The x-axis represents the number of voting classifiers used, and the y-axis represents the test accuracy. The plots show the average accuracy across 5 runs with different random seeds.  The algorithms compared are AdaBoost, LarsenRitzert, Majority-of-X (with X varying from 3 to 29), and BaggedAdaBoost.  The figure highlights the relative performance of these algorithms on datasets of differing sizes.", "section": "4 Experiments"}]