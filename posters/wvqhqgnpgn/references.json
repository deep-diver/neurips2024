{"references": [{"fullname_first_author": "Yasin Abbasi-Yadkori", "paper_title": "Politex: Regret bounds for policy iteration using expert prediction", "publication_date": "2019-00-00", "reason": "This paper provides theoretical foundations for policy gradient methods in the context of regret bounds, which is highly relevant to the efficiency analysis of the proposed SDEPO algorithm."}, {"fullname_first_author": "Alekh Agarwal", "paper_title": "On the theory of policy gradient methods: Optimality, approximation, and distribution shift", "publication_date": "2021-00-00", "reason": "This paper offers a comprehensive theoretical analysis of policy gradient methods, crucial for understanding the convergence properties and limitations of policy optimization algorithms, which is essential for the theoretical justification of SDEPO."}, {"fullname_first_author": "Ahmet Alacaoglu", "paper_title": "A natural actor-critic framework for zero-sum Markov games", "publication_date": "2022-00-00", "reason": "This paper introduces a novel actor-critic framework for zero-sum Markov games, providing a benchmark for comparison and a foundation for understanding related algorithms."}, {"fullname_first_author": "Yu Bai", "paper_title": "Provable self-play algorithms for competitive reinforcement learning", "publication_date": "2020-00-00", "reason": "This work establishes theoretical guarantees for self-play algorithms, which is critical for analyzing the convergence and optimality of SDEPO in a competitive reinforcement learning scenario."}, {"fullname_first_author": "Mikhail Belkin", "paper_title": "Approximation beats concentration? An approximation view on inference with smooth radial kernels", "publication_date": "2018-00-00", "reason": "This paper provides insights into the approximation capabilities of kernel methods, which is directly relevant to the kernel-based approximation techniques used in the proposed SDEPO algorithm."}]}