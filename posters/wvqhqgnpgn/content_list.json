[{"type": "text", "text": "Solving Zero-Sum Markov Games with Continuous State via Spectral Dynamic Embedding ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Chenhao Zhou1 Zebang Shen2 Chao Zhang1\u2217 Hanbin Zhao1 Hui Qian1,3 ", "page_idx": 0}, {"type": "text", "text": "1College of Computer Science and Technology, Zhejiang University 2Department of Computer Science, ETH Zurich 3State Key Lab of CAD&CG, Zhejiang University {zhouchenhao,zczju,zhaohanbin,qianhui}@zju.edu.cn zebang.shen@inf.ethz.ch ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we propose a provably efficient natural policy gradient algorithm called Spectral Dynamic Embedding Policy Optimization (SDEPO) for two-player zero-sum stochastic Markov games with continuous state space and finite action space. In the policy evaluation procedure of our algorithm, a novel kernel embedding method is employed to construct a finite-dimensional linear approximations to the state-action value function. We explicitly analyze the approximation error in policy evaluation, and show that SDEPO achieves an $\\tilde{O}\\bigl(\\frac{1}{(1\\!-\\!\\gamma)^{3}\\epsilon}\\bigr)$ last-iterate convergence to the \u03f5\u2212optimal Nash equilibrium, which is independent of the cardinality of the state space. The complexity result matches the best-known results for global convergence of policy gradient algorithms for single agent setting. Moreover, we also propose a practical variant of SDEPO to deal with continuous action space and empirical results demonstrate the practical superiority of the proposed method. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Two-player zero-sum stochastic Markov games $(2{\\tt p}0{\\tt s}{\\tt-}\\tt M G{\\tt s})$ has been the focus of research across a range of research communities. In this problem, two players select their actions based on the current state simultaneously and independently. Player one aims to maximize the return based on the reward provided by the environment, while player two aims to minimize it. For $2\\mathtt{p}0\\mathtt{s}\\mathtt{-M G s}$ with finite state space, tabular methods [Alacaoglu et al., 2022, Bai and Jin, 2020, Daskalakis et al., 2020, Wei et al., 2021, Zhao et al., 2022] represent the state-action value function with tables, which results in a sample complexity depending on the cardinality of the state spaces. ", "page_idx": 0}, {"type": "text", "text": "To deal with $2\\mathtt{p}0\\mathtt{s}\\mathtt{-M G s}$ with complex state space, researchers recently employ function approximations of the state-action value function to deal with large-scale discrete/continuous state space, including linear function approximations [Xie et al., 2020, Chen et al., 2022], kernel function approximations [Junchi Li et al., 2022, Qiu et al., 2021] and general function classes such as neural networks [Jin et al., 2022, Huang et al., 2021]. Basically, these methods use samples to construct the function approximations and update the policies with value iteration. Theoretical analyses show that these methods possess sample complexities independent of the state space\u2019s cardinality. Note that these methods fail to explicitly utilize the dynamics information of the underlying environments and only achieve a sub-optimal sample complexity $\\tilde{O}(\\epsilon^{-2})$ to find an $\\epsilon-$ optimal Nash equilibrium for 2p0s-MGs with known system dynamics. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we introduce a spectral dynamic embedding method, which explicitly uses the dynamics information to approximate the state-action value functions, and propose an efficient naturalpolicy-gradient-type algorithm, called Spectral Dynamic Embedding Policy optimization (SDEPO), for $2\\mathtt{p}0\\mathtt{s}\\mathtt{-M G s}$ . In particular, the spectral dynamic embedding method directly constructs truncated linear approximations to the transition dynamics of a Markov game in a kernel space, and implements dynamic programming to calculate the state-action value function approximation. The superiority of spectral dynamic embedding has been justified in single agent setting [Ren et al., 2022, 2023]. We leverage two kernel feature generation methods for the truncated approximation, namely random feature generation and Nystr\u00f6m feature generation, and analyze the approximation error of these two methods during policy evaluation. Our contributions lie in the following folds. ", "page_idx": 0}, {"type": "table", "img_path": "wvQHQgnpGN/tmp/39b9756e1dfa821c3a3dd93c9fd048c1cee1d83ba431e2c4041fe48ec2957c16.jpg", "table_caption": [], "table_footnote": ["Table 1: Comparison of policy optimization methods for finding an $\\epsilon_{}$ -optimal NE of two-player zero-sum episodic Markov games in terms of the duality gap. Here, $H$ refers to the horizon length and $d$ is the dimension of their features. For simplicity, we ignore the problem-dependent constant. "], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "1. We present a truncated kernel-based linearization method for the state-action value function approximation in two-player zero-sum Markov games with continuous state space. With the random/Nystr\u00f6m feature generation, this method automatically generates truncated kernel representation from system dynamics, bypassing the difficulty of kernel feature decision existed in other kernel approximation methods. By integrating the acquired kernel features into the temporal difference learning process, we estimate the state-action value functions through the least square policy evaluation. Leveraging the obtained value functions, policy improvement can be achieved through the natural policy gradient descent/ascent approach. ", "page_idx": 1}, {"type": "text", "text": "2. We establish rigorous analysis of the approximation error in the truncated value-function approximation and the statistical error induced in policy improvement procedure with finite samples. Our theoretical analysis demonstrates that SDEPO achieves a near-optimal $\\tilde{O}(\\frac{1}{(1\\!-\\!\\gamma)^{3}\\epsilon})$ last-iterate convergence to the $\\epsilon$ -optimal Nash equilibrium for $2\\mathtt{p}0\\mathtt{s}\\mathtt{-M G s}$ with continuous state space and finite action space, where $\\gamma$ represents the discounted factor. This complexity result matches the best-known results for the policy gradient algorithm to achieve global convergence in the single agent setting. ", "page_idx": 1}, {"type": "text", "text": "Moreover, we also propose a practical variant of SDEPO to deal with continuous action space and empirical results demonstrate the practical superior performance of the proposed method. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "RL methods for $2{\\tt p}0{\\tt s}{\\tt-}\\mathtt{M G s}$ . There is a large body of literature on MARL for two-player MGs. Alacaoglu et al. [2022], Daskalakis et al. [2020], Zhao et al. [2022] focus on the tabular setting, i.e., the state-action space can be represented by a table with moderate size To address the challenge of continuous state spaces, many researchers have designed algorithms based on function approximation. Xie et al. [2020], Chen et al. [2022] investigated methods based on linear function approximation, using a set of linear features to represent the state transition function and reward function. Jin et al. [2022], Huang et al. [2021] employed general function approximation for MGs with low multi-agent Bellman eluder dimension and MGs with a finite minimax Eluder dimension. Although these algorithms yield strong theoretical guarantees, they are computationally inefficient. Junchi Li et al. [2022], Qiu et al. [2021] studied learning MGs with kernel approximation. Their approachs assume that there are a set of (possibly infinite-dimensional) kernel features that span the transition function or value function space. However, finding a good set of kernel features is a very challenging task, making their assumption difficult to satisfy. Additionally, infinite-dimensional kernel features are infeasible to compute, so their method requires finite-dimensional approximation, and the errors caused by finite-dimensional approximation currently lack analysis. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Tabular methods for $2{\\tt p}0{\\tt s}{\\tt-}\\tt M G s$ where system dynamics are known. As shown in Table 1, a parallel line of research aims to solve $2\\mathtt{p}0\\mathtt{s}\\mathtt{-M G s}$ with system dynamics known. For the infinite-horizon discounted setting, Wei et al. [2021] proposed an optimistic gradient descent ascent (OGDA) method which achieves a last-iterate convergence at an $\\widetilde{\\cal O}\\big(\\frac{|S|^{3}}{(1\\!-\\!\\gamma)^{9}\\epsilon^{2}}\\big)$ iteration complexity. Cen et al. [2021] established linear last-iterate convergence of  e ntropy-regularized OMWU. For the finite-horizon episodic setting, Zhang et al. [2022] showed that the modified optimistic Follow-The-RegularizedLeader method finds an $\\epsilon$ -optimal NE in $\\widetilde{\\cal O}\\Big(\\frac{H^{4}}{\\kappa}\\Big)$ iterations and Cen et al. [2023] proposed a singleloop policy optimization algorithm that i m plies the last-iterate convergence with an iteration complexity of $\\begin{array}{r}{\\widetilde{O}\\big(\\frac{H^{3}}{\\epsilon_{\\perp}}\\big)}\\end{array}$ . However, their methods are all confined to tabular setting, whereas our method can handle   problems in continuous state spaces. ", "page_idx": 2}, {"type": "text", "text": "RL with Function Approximation. Function approximation in single-agent RL has been extensively studied in recent years to achieve a better sample complexity that depends on the complexity of function approximators rather than the size of the state/action space. One line of work studies RL with linear function approximation [Yang and Wang, 2019, Jin et al., 2020]. Typically, these methods assume the optimal value function can be well approximated by linear functions, and achieve polynomial sample efficiency guarantees related to feature dimension under certain regularity conditions. Another line of works studied the MDPs with general nonlinear function approximations [Jiang et al., 2017, Jin et al., 2021]. Jiang et al. [2017], Jin et al. [2021] present algorithms with PAC guarantees for problems with low Bellman rank and low BE dimension, respectively. We note that MGs are inherently more complex than MDPs due to their min-max nature and it is generally difficult to directly extend these results to the dual-player dynamic setting of MGs. ", "page_idx": 2}, {"type": "text", "text": "2 Background and Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we present the necessary definitions that will be adopted throughout the paper. In Section 2.1, we formally describe the setup for two-player zero-sum stochastic Markov games with simultaneous moves. In Section 2.2, We briefly introduce the background knowledge about positive definite kernels and their decompositions. ", "page_idx": 2}, {"type": "text", "text": "2.1 Simultaneous-Move Markov Games ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A two-player zero-sum stochastic Markov games with simultaneous moves is defined by the tuple $(S,\\mathcal{A}_{1},\\mathcal{A}_{2},r,\\mathbb{P},\\gamma)$ , where $\\boldsymbol{S}$ is the state space, $A_{i}$ is the set of actions that player $i\\in\\{1,2\\}$ can take, $r$ is the reward function, $\\mathbb{P}$ is the transition function and $\\gamma\\in[0,1)$ is the discounted factor. At each step $t$ , given the state $s\\in S$ , P1 and P2 take actions $a\\in A_{1}$ and $b\\in\\mathcal{A}_{2}$ , respectively, and then both receive the reward $r(s,a,b)$ . The system then shifts to a new state $s^{\\prime}\\sim\\mathbb{P}(\\cdot|s,a,b)$ according to the transition kernel. Throughout this paper, we assume for simplicity that $\\mathcal{S}=\\mathbb{R}^{d}$ , $\\mathcal{A}_{1}=\\mathcal{A}_{2}=\\mathcal{A}$ and that the rewards $r(s,a,b)$ are deterministic functions of the tuple $(s,a,b)$ taking value in $[-1,1]$ ; generalization to the setting with $A_{1}\\neq A_{2}$ and stochastic rewards is straightforward. ", "page_idx": 2}, {"type": "text", "text": "Denote by $\\Delta\\equiv\\Delta(A)$ the probability simplex over the action space $\\boldsymbol{\\mathcal{A}}$ . A stochastic policy of P1 is a sequence of functions $\\overline{{\\pi}}:=(\\overline{{\\pi}}_{t}:S\\rightarrow\\Delta)_{t}$ . At each step $t$ and state $s\\in S$ , P1 takes an action sampled from the distribution $\\overline{{\\pi}}_{t}(s)$ over $\\boldsymbol{\\mathcal{A}}$ . Similarly, a stochastic policy of P2 is given by the sequence $\\underline{{\\pi}}:=(\\underline{{\\pi}}_{t}:S\\rightarrow\\Delta)_{t}$ . ", "page_idx": 2}, {"type": "text", "text": "For a fixed pair of policies $(\\overline{{\\pi}},\\underline{{\\pi}})$ for both players, the value and Q (a.k.a. action-value) functions for the above game can be defined as following: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad V^{\\overline{{\\pi}},\\underline{{\\pi}}}(s):=\\mathbb{E}\\bigg[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}r(s_{t},a_{t},b_{t})|s_{0}=s\\bigg],}\\\\ &{Q^{\\overline{{\\pi}},\\underline{{\\pi}}}(s,a,b):=\\mathbb{E}\\bigg[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}r(s_{t},a_{t},b_{t})|s_{0}=s,a_{0}=a,b_{0}=b\\bigg],}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the expectation is over $a_{t}\\sim\\overline{{\\pi}}_{t}\\big(\\cdot|s_{t}\\big),b_{t}\\sim\\underline{{\\pi}}_{t}\\big(\\cdot|s_{t}\\big)$ and $s_{t+1}\\sim\\mathbb{P}(\\cdot|s_{t},a_{t},b_{t})$ . ", "page_idx": 2}, {"type": "text", "text": "In the zero-sum setting, for a given initial state $s_{0}$ , P1 seeks to maximize $V^{\\overline{{\\pi}},\\underline{{\\pi}}}(s_{0})$ whereas P2 aims to minimize it. Accordingly, we introduce the value and Q functions when P1 plays the best response to a fixed policy $\\underline{\\pi}$ of P2: $V^{*,\\underline{{\\pi}}}(s)\\;=\\;\\mathrm{max}_{\\overline{{\\pi}}}\\,V^{\\overline{{\\pi}},\\underline{{\\pi}}}(s)$ and $Q^{*,\\pi}(s,a,b)\\;\\stackrel{}{=}\\,\\operatorname*{max}_{\\pi}Q^{\\overline{{{\\pi}}},\\underline{{{\\pi}}}}(\\bar{s_{,}}a,b)$ . Similarly, when P2 plays the best response to P1\u2019s policy $\\pi$ , we define $V^{\\overline{{\\pi}},*}(s)\\,=\\,\\operatorname*{min}_{\\underline{{\\pi}}}V^{\\overline{{\\pi}},\\underline{{\\pi}}}(s)$ and $\\begin{array}{r}{Q^{\\overline{{\\pi}},*}(s,a,b)=\\operatorname*{min}_{\\underline{{\\pi}}}Q^{\\overline{{\\pi}},\\underline{{\\pi}}}(s,a,b)}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "A Nash Equilibrium (NE) of the game is a pair of stochastic policies $(\\overline{{\\pi}}^{*},\\underline{{\\pi}}^{*})$ that are the best response to each other, which we write as $V^{\\overline{{\\pi}}^{*},\\underline{{\\pi}}^{*}}(s)=V^{*,\\underline{{\\pi}}^{*}}(s)=V^{\\overline{{\\pi}}^{*},*}(s)$ . NE always exists for discounted two-player zero-sum Markov Games [Filar and Vrieze, 2012]. Correspondingly, let $V^{*}(s):=V^{\\overline{{\\pi}}^{*},\\underline{{\\pi}}^{*}}(s)$ and $Q^{*}(s,a,b):=Q^{\\overline{{\\pi}}^{*},\\underline{{\\pi}}^{*}}(s,a,b)$ denote the values of the NE. In practice, we always seek to find an \u03f5\u2212optimal Nash equilibrium, which is a pair of stochastic policies $(\\overline{{\\pi}},\\underline{{\\pi}})$ that satisfies $\\mathbb{E}_{s\\sim\\mu_{0}}\\operatorname*{max}_{\\overline{\\pi}^{\\prime},\\underline{\\pi}^{\\prime}}V^{\\overline{\\pi}^{\\prime},\\underline{\\pi}}(s)-V^{\\overline{\\pi},\\underline{\\pi}^{\\prime}}(s)\\leq\\epsilon.$ . for a initial state distribution $\\mu_{0}$ . ", "page_idx": 3}, {"type": "text", "text": "We are interested in finding a one-sided $\\epsilon-$ optimal Nash equilibrium, similar to Alacaoglu et al. [2022], Zhao et al. [2022], Daskalakis et al. [2020], Zhang et al. [2019]. In particular, for the initial state distribution $\\mu_{0}$ , we seek $\\underline{{\\pi}}_{o u t}$ such that $\\mathbb{E}_{s\\sim\\mu_{0}}\\operatorname*{max}_{\\overline{\\pi}}\\bar{V^{\\overline{{\\pi}},\\underline{{\\pi}}_{o u t}}}(s)-V^{*}\\bar{(s)}\\leq\\epsilon$ . ", "page_idx": 3}, {"type": "text", "text": "We assume that the transition function satisfies the following assumption. ", "page_idx": 3}, {"type": "text", "text": "Assumption 1. For each $(s_{t},a_{t},b_{t})\\in\\mathcal S\\times\\mathcal A\\times\\mathcal A$ , we assume that ", "page_idx": 3}, {"type": "equation", "text": "$$\ns_{t+1}=f(s_{t},a_{t},b_{t})+\\epsilon_{t},\\quad w h e r e\\;\\epsilon_{t}\\sim\\mathcal{N}(0,\\sigma^{2}I_{d}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The function $f:S\\times A\\times A\\to S$ describes the general dynamics and $\\{\\epsilon_{t}\\}_{t=0}^{\\infty}$ are independent Gaussian noises. In other words, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{P}(s_{t+1}|s_{t},a_{t},b_{t})\\propto\\exp\\bigl(-\\frac{\\|f\\bigl(s_{t},a_{t},b_{t}\\bigr)-s_{t+1}\\|_{2}^{2}}{2\\sigma^{2}}\\bigr)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "2.2 Positive definite kernels and two decompositions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To efficiently represent the continuous state space of 2p0s-MGs which can not be handled by traditional tabular methods, it is common to embed the continuous state space into a kernel space. A widely used kernel is the positive definite (PD) kernel. ", "page_idx": 3}, {"type": "text", "text": "Definition 1 ((Positive-Definite) Kernel [Mohri, 2018]). A symmetric function $k:\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}$ is said to be a positive definite kernel if for any $\\{x_{1},\\ldots,x_{m}\\}\\subset\\mathcal{X}$ , the matrix $\\mathbf{K}=[k(x_{i},x_{j})]_{i j}\\in$ $\\mathbb{R}^{m\\times m}$ is symmetric positive-definite. ", "page_idx": 3}, {"type": "text", "text": "PD kernels admit many decompositions, such as Bochner decomposition [Devinatz, 1953], Mercer decomposition [Mercer, 1909], Canonical decomposition [Stochel, 1992] and Kolmogorov decomposition [Ghaemi et al., 2021]. Among these decompositions, Bochner decomposition [Devinatz, 1953] and Mercer decomposition [Mercer, 1909] have recently draw significant attention since they lead to efficient, low-dimensional approximations of the kernel, reducing computational complexity [Liu et al., 2021]. ", "page_idx": 3}, {"type": "text", "text": "Definition 2 (Bochner decomposition [Rudin, 2017] and Mercer decomposition [Mercer, 1909]). Let $\\mathcal{X}\\,\\subset\\,\\mathbb{R}^{d}$ be a compact domain, $\\mu$ a strictly positive Borel measure on $\\mathcal{X}$ , and $k(x,x^{\\prime})\\;=$ $G(x\\mathrm{~-~}x^{\\prime})$ a bounded continuous shift-invariant positive definite kernel. Then, $k(x,x^{\\prime})$ admits Bochner decomposition, i.e. there exists a non-negative measure $\\omega$ , such that ", "page_idx": 3}, {"type": "equation", "text": "$$\nG(x-x^{\\prime})=\\int_{\\mathbb{R}^{d}}p(\\omega)\\exp\\left(i\\omega^{\\top}(x-x^{\\prime})\\right)\\mathrm{d}\\omega,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and Mercer decomposition, i.e. there exists a countable orthonormal basis $\\{e_{i}\\}_{i=1}^{\\infty}$ of $\\mathcal{L}_{2}(\\mu)$ with corresponding eigenvalues $\\{\\sigma_{i}\\}_{i=1}^{\\infty}$ ,2 such that ", "page_idx": 3}, {"type": "equation", "text": "$$\nk(x,x^{\\prime})=\\sum_{i=1}^{\\infty}\\sigma_{i}e_{i}(x)e_{i}(x^{\\prime}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the convergence is absolute and uniform for all $(x,x^{\\prime})\\in\\mathcal{X}\\times\\mathcal{X}$ . Without loss of generality, we assume $\\sigma_{1}\\geq\\sigma_{2}\\geq\\cdots>0$ . ", "page_idx": 3}, {"type": "text", "text": "It can be verified that the Gaussian kernel, k(x, x\u2032) = exp(\u2212\u2225x2\u2212\u03c3x2\u2032\u2225 ) , meets the conditions in the above definition, and it admits both a Bochner decomposition and a Mercer decomposition [Ren et al., 2023]. Note that according to the Bochner/Mercer decomposition, the kernel can be represented with infinite basis. Practically, one can construct finite-dimensional approximations to the positive-definite kernel based on the Bochner/Mercer decompositions with random/Nystr\u00f6m features [Rahimi and Recht, 2008, Williams and Seeger, 2000], respectively. Recently, Ren et al. [2023] utilize a finite-dimensional approximation to represent the environment in stochastic nonlinear control problems and showed its superior performance, motivating our work. ", "page_idx": 4}, {"type": "text", "text": "3 Policy Optimization with Spectral Dynamic Embedding ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we begin by introducing spectral dynamic embedding to represent the environment of the Markov game. This approach allows us to express the $Q$ -function for any policy pair with infinite dimensional features. Subsequently, we develop finite-dimensional approximated features for computational tractability. With these features, each player conducts least square policy evaluation to estimate the $Q$ -function of current policy pair based on the generated features and then improve the policy by natural policy gradient based on the estimated $Q$ -function. This leads to Spectral Dynamic Embedding Policy Optimization (SDEPO) in Algorithm 3. ", "page_idx": 4}, {"type": "text", "text": "3.1 Spectral Dynamics Embedding ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "By interpreting the state transition function (1) as a Gaussian kernel, we can decompose the transition function of a Markov game using Bochner decomposition and Mercer decomposition, as detailed below. ", "page_idx": 4}, {"type": "text", "text": "Lemma 1 (Spectral Dynamic Embedding). Consider any $\\alpha\\ \\in\\ [0,1)$ . Denote $k_{\\alpha}(x,x^{\\prime})\\;\\;=$ \u2212(1\u2212\u03b122)\u2225\u03c3x2\u2212x\u2032\u22252 for any 0 \u2264\u03b1 < 1. We can decompose k\u03b1(x, x\u2032) using Bochner decomposition and Mercer decomposition. ", "page_idx": 4}, {"type": "text", "text": "Let ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\psi_{\\omega}(s,a,b)=\\displaystyle\\frac{g_{\\alpha}(f(s,a,b))}{\\alpha^{d}}\\left[\\cos\\left(\\frac{\\omega^{\\top}f(s,a,b)}{\\sqrt{1-\\alpha^{2}}}\\right),\\sin\\left(\\frac{\\omega^{\\top}f(s,a,b)}{\\sqrt{1-\\alpha^{2}}}\\right)\\right],}}\\\\ {{\\chi_{\\omega}(s^{\\prime})=p_{\\alpha}(s^{\\prime})[\\cos(\\sqrt{1-\\alpha^{2}}\\omega^{\\top}s^{\\prime}),\\sin(\\sqrt{1-\\alpha^{2}}\\omega^{\\top}s^{\\prime})]^{\\top},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{g_{\\alpha}\\left(f(s,a,b)\\right)\\quad:=\\quad\\exp\\Big(\\frac{\\alpha^{2}\\|f(s,a,b)\\|^{2}}{2(1-\\alpha^{2})\\sigma^{2}}\\Big),\\ \\ \\omega\\ \\ \\sim\\ \\ \\mathcal{N}(0,\\sigma^{-2}I_{d}),\\ \\ \\ a n d\\ \\ p_{\\alpha}}\\end{array}$ $\\begin{array}{r l}{p_{\\alpha}(\\ensuremath{\\boldsymbol{s}}^{\\prime})}&{{}=}\\end{array}$ $\\begin{array}{r}{\\frac{\\alpha^{d}}{(2\\pi\\sigma^{2})^{d/2}}\\exp\\left(-\\frac{\\|\\alpha s^{\\prime}\\|^{2}}{2\\sigma^{2}}\\right)}\\end{array}$ is a Gaussian distribution for $s^{\\prime}$ with standard deviation $\\frac{\\sigma}{\\alpha}$ . ", "page_idx": 4}, {"type": "text", "text": "Using Bochner decomposition, we have ", "text_level": 1, "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P(s^{\\prime}|s,a,b)=\\!\\!\\mathbb{E}_{\\omega\\sim N(0,\\sigma^{-2}I_{d})}\\left[\\psi_{\\omega}(s,a,b)^{\\top}\\chi_{\\omega}(s^{\\prime})\\right]:=\\langle\\psi_{\\omega}(s,a,b),\\chi_{\\omega}(s^{\\prime})\\rangle_{N(0,\\sigma^{-2}I_{d})}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Let $\\mu$ be a strictly positive Borel measure on $\\boldsymbol{S}$ . By Mercer\u2019s theorem, $k_{\\alpha}$ admits a decomposition for any $(x,x^{\\prime})\\in\\mathcal{X}$ [Fasshauer, 2011]: ", "page_idx": 4}, {"type": "equation", "text": "$$\nk_{\\alpha}(x,x^{\\prime})\\!=\\!\\sum_{i=1}^{\\infty}\\!\\sigma_{\\alpha,i}e_{\\alpha,i}(x)e_{\\alpha,i}(x^{\\prime}),\\;\\{e_{\\alpha,i}\\}\\;a\\;b a s i s f o r\\;\\!\\mathcal{L}_{2}(\\mu).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "$i$ W3e denote $k_{\\alpha}(x,x^{\\prime})=\\langle\\widetilde{e}_{\\alpha}(x),\\widetilde{e}_{\\alpha}(x^{\\prime})\\rangle_{\\ell_{2}}$ where $\\tilde{e}_{\\alpha,i}(x)=\\sqrt{\\sigma_{\\alpha,i}}e_{\\alpha,i}(x)$ for each positive integer ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\boldsymbol{\\psi}_{M}(s,a,b)=\\frac{g_{\\alpha}\\left(f(s,a,b)\\right)}{\\alpha^{d}}\\boldsymbol{\\tilde{e}}_{\\alpha}\\left(\\frac{f(s,a,b)}{1-\\alpha^{2}}\\right)^{\\top},\\boldsymbol{\\chi}_{M}(s^{\\prime})=p_{\\alpha}(s^{\\prime})\\boldsymbol{\\tilde{e}}_{\\alpha}\\left(s^{\\prime}\\right)^{\\top}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then, ", "page_idx": 4}, {"type": "equation", "text": "$$\nP(s^{\\prime}|s,a,b)=\\langle\\psi_{M}(s,a,b),\\chi_{M}(s^{\\prime})\\rangle_{\\ell_{2}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The tunable parameter $\\alpha$ offers advantages for theoretical analysis and can also be leveraged to enhance empirical performance. ", "page_idx": 4}, {"type": "text", "text": "Let $\\phi_{\\omega}(\\cdot)=[\\psi_{\\omega}(\\cdot),r(\\cdot)]$ and $\\phi_{M}(\\cdot)=[\\psi_{M}(\\cdot),r(\\cdot)]$ . The $\\phi_{\\omega}(\\cdot)$ and $\\phi_{M}(\\cdot)$ are named as Bochner Spectral Dynamic Embedding and Mercer Spectral Dynamic Embedding, respectively. These embeddings form infinite-dimensional bases of the $Q$ -function for arbitrary policy pairs, allowing for a linear representation of the $Q$ -function. ", "page_idx": 4}, {"type": "text", "text": "Data: Transition Model $s^{\\prime}\\,=\\,f(s,a,b)+\\varepsilon$ where $\\varepsilon\\,\\sim\\,{\\mathcal{N}}(0,\\sigma^{2}I_{d})$ , Reward Function $r(s,a,b)$ , Number of Random/Nystr\u00f6m Feature $m$   \nResult: $\\phi(\\cdot,\\cdot,\\cdot)$   \n1 Sample i.i.d. $\\{\\omega_{i}\\}_{i\\in[m]}$ where $\\omega_{i}\\sim\\mathcal{N}(0,\\sigma^{-2}I_{d})$ and construct the feature ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\psi_{\\mathrm{rf}}(s,a,b)=\\frac{g_{\\alpha}(f(s,a,b))}{\\alpha^{d}}\\left[\\sin(\\omega_{i}^{\\top}f(s,a,b)/\\sqrt{1-\\alpha^{2}}),\\cos(\\omega_{i}^{\\top}f(s,a,b)/\\sqrt{1-\\alpha^{2}})\\right]_{i\\in[m]},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n:\\phi(s,a,b)=[\\psi_{\\mathrm{rf}}(s,a,b),r(s,a,b)].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Algorithm 2 Nystr\u00f6m Features Generation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Data: Transition Model $s^{\\prime}\\,=\\,f(s,a,b)+\\varepsilon$ where $\\varepsilon\\,\\sim\\,{\\mathcal{N}}(0,\\sigma^{2}I_{d})$ , Reward Function $r(s,a,b)$ , Number of Random/Nystr\u00f6m Feature $m$ , Number of Nystr\u00f6m Samples $n_{\\mathrm{Nys}}\\geq m$ , Nystr\u00f6m Sampling Distribution 1 Sample $n_{\\mathrm{{nys}}}$ random samples $\\{\\bar{x_{1}},\\cdot\\cdot\\cdot,x_{\\mathrm{nys}}\\}$ independently from $\\boldsymbol{S}$ , following the distribution $\\mu_{\\mathrm{Nys}}$ . 2 Construct nnys-by-nnys Gram matrix given by Ki(,njN ys)= k\u03b1(xi, xj). 3 Perform eigendecomposition on the Gram matrix $K^{(n_{\\mathrm{nys}})}U=\\Lambda U$ , with $\\lambda_{1}\\ge\\cdots\\ge\\lambda_{n_{\\mathrm{nys}}}$ denoting the corresponding eigenvalues. 4 Construct the feature ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\psi_{\\mathrm{nys}}(s,a,b)\\!\\!=\\!\\!\\left[\\frac{g_{\\alpha}\\!(f(s,a,b))\\!\\!\\sum_{\\ell=1}^{n_{\\mathrm{nys}}}\\!\\!\\!U_{i,\\ell}k_{\\alpha}\\left(x_{\\ell},\\!\\frac{f(s,a,b)}{1\\!-\\!\\alpha^{2}}\\right)\\!\\!}{\\Big]}_{i\\in[m]}\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "set $\\phi(s,a,b)=[\\psi_{\\mathrm{nys}}(s,a,b),r(s,a,b)].$ . ", "page_idx": 5}, {"type": "text", "text": "Lemma 2. For any policy pair, there exist weights $\\{\\theta_{\\omega}^{\\overline{{\\pi}},\\underline{{\\pi}}}\\}$ (where $\\omega\\sim\\mathcal{N}(0,\\sigma^{-2}I_{d}))$ and $\\theta_{M}^{\\overline{{\\pi}},\\underline{{\\pi}}}\\in\\ell_{2}$ such that the corresponding value function satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\cal Q}^{\\overline{{\\pi}},\\underline{{\\pi}}}(s,a,b)=\\,\\left\\langle\\phi_{\\omega}(s,a,b),\\theta_{\\omega}^{\\overline{{\\pi}},\\underline{{\\pi}}}\\right\\rangle_{\\scriptstyle\\!N(0,\\sigma^{-2}I_{d})}=\\,\\left\\langle\\phi_{M}(s,a,b),\\theta_{M}^{\\overline{{\\pi}},\\underline{{\\pi}}}\\right\\rangle_{\\ell_{2}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We provide the proof of this section in Appendix A. ", "page_idx": 5}, {"type": "text", "text": "3.2 Finite-dimensional truncated Embedding ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Basically, a desirable feature embedding method should provide a good approximation to the underlying dynamics of MGs with a modest feature dimension. However, the dimension of both Bochner and Mercer spectral dynamic embedding is infinite, which is computationally intractable, motivating us to investigate the finite-dimensional embeddings. ", "page_idx": 5}, {"type": "text", "text": "We construct finite-dimensional truncations of Bochner and Mercer embedding using random feature [Ren et al., 2022] and Nystr\u00f6m feature [Williams and Seeger, 2000], respectively, to provide efficient finite linear approximations to represent the transition kernel. As show in Algorithm 1 and Algorithm 2, random feature is the Monte-Carlo approximation for the Bochner embedding and Nystr\u00f6m feature approximates the subspace spanned by the top eigenfunctions of the Mercer embedding via eigendecomposition of an empirical Gram matrix. A detailed derivation of the Nystr\u00f6m method is provided in Appendix B. We analyze the approximation error due to using the finite-dimensional basis in Appendix D. The finite-dimensional embedding is a crucial component of our algorithm, Spectral Dynamic Embedding Policy Optimization (SDEPO), which is given as Algorithm 3. ", "page_idx": 5}, {"type": "text", "text": "3.3 Policy Optimization with Finite-dimensional Embedding ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "After generating the finite-dimensional embedding, SDEPO is divided in two stages. It finds an approximate solution $\\underline{{\\pi}}_{k}$ in Stage 1, which is then utilized in Stage 2 to derive an approximate solution $\\overline{{\\pi}}_{k}$ . In each stage, there are two main components: policy evaluation and policy improvement. Least square policy evaluation is conducted for estimating the state-action value function of current policy pair upon the generated finite-dimensional truncation features $Q^{\\overline{{\\pi}}_{t},\\underline{{\\pi}}_{t}}(s,a,b)\\,=\\,\\phi(s,a,b)^{\\top}\\,\\bar{w}^{\\overline{{\\pi}}_{t},\\underline{{\\pi}}_{t}}$ . In the policy improvement procedure, based on the approximated value function, the natural policy gradient method is used to adjust the policy of each player iteratively in an alternating fashion. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 3 Spectral Dynamic Embedding Policy Optimization (SDEPO) ", "page_idx": 6}, {"type": "text", "text": "Data: Transition Model $s^{\\prime}\\,=\\,f(s,a,b)+\\varepsilon$ where $\\varepsilon\\,\\sim\\,{\\mathcal{N}}(0,\\sigma^{2}I_{d})$ , Reward Function $r(s,a,b)$ , Number of Random/Nystr\u00f6m Feature $m$ , Number of Nystr\u00f6m Samples $n_{\\mathrm{Nys}}\\geq m$ , Nystr\u00f6m Sampling Distribution $\\mu_{\\mathrm{Nys}}$ , Number of Sample $n$ , Factorization Scale $\\alpha$ , Learning Rate $\\eta$ ", "page_idx": 6}, {"type": "text", "text": "Result: \u03c0k   \nGenerate $\\phi(s,a,b)$ using Algorithm 1 or Algorithm 2.   \nInitialize $\\overline{{\\theta}}_{0}=\\underline{{\\theta}}_{0}=0$ and $\\overline{{\\pi}}_{0}(\\cdot|s)=\\underline{{\\pi}}_{0}(\\cdot|s)=U n i f(A)$ for all $s\\in S$ .   \nfor $k=0,1,\\cdots\\,,K$ do Stage 1 Initialize $\\overline{{\\theta}}_{k,0}=\\overline{{\\theta}}_{k},\\underline{{\\theta}}_{k,0}=\\underline{{\\theta}}_{k}$ and $\\overline{{\\pi}}_{k,0}(\\cdot|s)=\\overline{{\\pi}}_{k}(\\cdot|s),\\underline{{\\pi}}_{k,0}(\\cdot|s)=\\underline{{\\pi}}_{k}(\\cdot|s)$ for all $s\\in S$ . for $t=0,1,\\cdot\\cdot\\cdot,T-1$ do Sample i.i.d. $\\{s_{i},a_{i},b_{i},s_{i}^{\\prime},a_{i}^{\\prime},b_{i}^{\\prime}\\}_{i\\in[n]}$ with policy pair $\\overline{{\\pi}}_{k-1},\\underline{{\\pi}}_{k-1}$ , where $\\begin{array}{r l}{s_{i}^{\\prime}}&{{}=}\\end{array}$ $f(s_{i},a_{i},b_{i})+\\varepsilon$ . Initialize $\\hat{w}_{k,t,0}=0$ . for $l=0,1,\\cdots\\,,L-1$ do Solve $\\hat{w}_{k,t,l+1}\\!=\\!\\arg\\operatorname*{min}_{w}\\sum_{i\\in[n]}\\left(\\phi(s_{i},a_{i},b_{i})^{\\top}w\\!-\\!r(s_{i},a_{i},b_{i})\\!-\\!\\gamma\\phi(s_{i}^{\\prime},a_{i}^{\\prime},b_{i}^{\\prime})^{\\top}\\hat{w}_{k,t,l}\\right)^{2}.$ (9) Update $\\overline{{\\theta}}_{k,t+1}=\\overline{{\\theta}}_{k,t}+\\eta\\hat{w}_{k,t,L},\\underline{{\\theta}}_{k,t+1}=\\underline{{\\theta}}_{k,t}-\\eta\\hat{w}_{k,t,L}$ and $\\begin{array}{r l}&{\\overline{{\\pi}}_{k,t+1}(a|s)\\propto\\exp(\\mathbb{E}_{b\\sim\\underline{{\\pi}}_{k,t}(\\cdot|s)}[\\phi(s,a,b)]^{\\top}\\overline{{\\theta}}_{k,t+1}),}\\\\ &{\\underline{{\\pi}}_{k,t+1}(b|s)\\propto\\exp(\\mathbb{E}_{a\\sim\\overline{{\\pi}}_{k,t}(\\cdot|s)}[\\phi(s,a,b)]^{\\top}\\underline{{\\theta}}_{k,t+1}).}\\end{array}$ (10) Output $\\begin{array}{r}{\\underline{{\\pi}}_{k}=\\frac{1}{T}\\sum_{t=1}^{T}\\underline{{\\pi}}_{k,t}}\\end{array}$ . Stage 2 Initialize $\\overline{{\\theta}}_{k,0}^{\\prime}=0$ and $\\overline{{{\\pi}}}_{k,0}^{\\prime}(\\cdot|s)=U n i f(A)$ . for $t=0,1,\\cdot\\cdot\\cdot,T-1$ do Sample i.i.d. $\\{s_{i},a_{i},b_{i},s_{i}^{\\prime},a_{i}^{\\prime},b_{i}^{\\prime}\\}_{i\\in[n]}$ with policy pair $\\overline{{\\pi}}_{k,t}^{\\prime},\\underline{{\\pi}}_{k}$ ,where $s_{i}^{\\prime}=f(s_{i},a_{i},b_{i})+\\varepsilon$ . Initialize $\\hat{\\zeta}_{k,t,0}=0$ for all $s\\in S$ . for $l=0,1,\\cdots\\,,L-1$ do Solve $\\hat{\\zeta}_{k,t,l+1}\\!=\\!\\arg\\operatorname*{min}_{\\zeta}\\!\\sum_{i\\in[n]}\\left(\\phi(s_{i},a_{i},b_{i})^{\\top}\\zeta\\!-\\!r(s_{i},a_{i},b_{i})\\!-\\!\\gamma\\phi(s_{i}^{\\prime},a_{i}^{\\prime},b_{i}^{\\prime})^{\\top}\\hat{\\zeta}_{k,t,l}\\right)^{2}.$ (11) Update $\\overline{{\\theta}}_{k,t+1}^{\\prime}=\\overline{{\\theta}}_{k,t}^{\\prime}+\\eta\\hat{\\zeta}_{k,t,L}$ and $\\overline{{\\pi}}_{k,t+1}^{\\prime}(a|s)\\propto\\mathrm{exp}(\\mathbb{E}_{b\\sim\\underline{{\\pi}}_{k}(\\cdot|s)}[\\phi(s,a,b)]^{\\top}\\overline{{\\theta}}_{k,t+1}^{\\prime}).$ (12) $\\hat{t}\\in[T]$ ", "page_idx": 6}, {"type": "text", "text": "Output $\\overline{{\\pi}}_{k}=\\overline{{\\pi}}_{k,\\hat{t}}^{\\prime}$ , where is selected uniformly at random. ", "page_idx": 6}, {"type": "text", "text": "$\\overline{{\\pi}}_{t+1}$ is updated as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\overline{{\\pi}}_{t+1}(\\cdot|s)=\\operatorname{arg}\\operatorname*{max}_{\\substack{\\pi(\\cdot|s)\\in\\Delta(A)}}\\;\\left\\langle\\pi(\\cdot|s),\\mathbb{E}_{b\\sim\\pi_{t}(\\cdot|s)}[\\phi(s,\\cdot,b)]^{\\top}w^{\\overline{{\\pi}}_{t},\\underline{{\\pi}}_{t}}\\right\\rangle+\\frac{1}{\\eta}K L\\left(\\pi(\\cdot|s)||\\overline{{\\pi}}_{t}(\\cdot|s)\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "yielding the following closed-form solution, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\overline{{\\pi}}_{t+1}(\\overline{{a}}|s)\\propto\\overline{{\\pi}}_{t}(a|s)\\exp\\left(\\mathbb{E}_{b\\sim\\underline{{\\pi}}_{t}(\\cdot|s)}[\\phi\\left(s,a,b\\right)]^{\\top}\\eta w^{\\overline{{\\pi}}_{t},\\underline{{\\pi}}_{t}}\\right)=\\exp\\left(\\mathbb{E}_{b\\sim\\underline{{\\pi}}_{t}(\\cdot|s)}[\\phi\\left(s,a,b\\right)]^{\\top}\\overline{{\\theta}}_{t+1}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\begin{array}{r}{\\overline{{\\theta}}_{t+1}=\\sum_{i=0}^{t}{\\eta w^{\\overline{{\\pi}}_{i}}},\\underline{{\\pi}}_{i}}\\end{array}$ . $\\underline{\\pi}$ is updated by similar update rule. ", "page_idx": 6}, {"type": "text", "text": "Note that in SD EPO, each player is required to have knowledge of their opponent\u2019s strategy, which is crucial for effective policy optimization and is a common requirement in numerous existing algorithms for Markov games. [Xie et al., 2020, Chen et al., 2022, Junchi Li et al., 2022, Alacaoglu et al., 2022] ", "page_idx": 6}, {"type": "text", "text": "4 Theoretical Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The major difficulty in analyzing the convergence of SDEPO is that the policies of both players evolve at each iteration, leading to non-stationary in the environment for each player. Additionally, there exists a certain level of estimation error in the transition and Q-function for each player. As a consequence, the vanilla proof strategy of convergence of policy optimization used in majority of the literature is no longer applicable. ", "page_idx": 7}, {"type": "text", "text": "In this section, we provide rigorous investigation of the impact of the approximation error for policy evaluation and derive the convergence of policy optimization. We first specify the commonly used assumptions $[\\mathrm{Yu}$ and Bertsekas, 2008, Jin et al., 2020, Agarwal et al., 2021, Abbasi-Yadkori et al., 2019, Ren et al., 2023], under which we derive our theoretical results below. ", "page_idx": 7}, {"type": "text", "text": "Assumption 2 (Regularity Condition for Dynamics). For the dynamic function $f$ , there exists $a$ constant $c_{f}$ , such that $\\|f(s,a,b)\\|\\leq c_{f}$ for all $s\\in S,a\\in{\\mathcal{A}},b\\in{\\mathcal{A}}$ . ", "page_idx": 7}, {"type": "text", "text": "Assumption 3 (Regularity Condition for Stationary Distribution). The stationary distribution $\\nu_{\\overline{{\\pi}},\\pi}$ for all policy pair $({\\overline{{\\pi}}},{\\underline{{\\pi}}})$ has full support, and satisfies the following conditions with $\\Upsilon_{1},\\Upsilon_{2}>0$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda_{\\operatorname*{min}}\\left(\\mathbb{E}_{\\nu_{\\pi,\\pi}}\\left[\\phi(s,a,b)\\phi(s,a,b)^{\\top}\\right]\\right)\\geq\\Upsilon_{1},}\\\\ &{\\lambda_{\\operatorname*{min}}\\left(\\mathbb{E}_{\\nu_{\\pi,\\pi}}\\left[\\phi(s,a,b)\\left(\\phi(s,a,b)-\\gamma\\mathbb{E}_{\\nu_{\\pi,\\underline{{\\pi}}}}\\phi(s^{\\prime},a^{\\prime},b^{\\prime})\\right)^{\\top}\\right]\\right)\\geq\\Upsilon_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Assumption 4 (Regularity Condition for Feature). The features $\\{\\phi_{\\omega}(s,a,b)\\}_{(s,a,b)\\in S\\times A\\times A}$ and $\\{\\phi_{\\omega}(s,a,b)\\}_{(s,a,b)\\in S\\times A\\times A}$ are linearly independent. ", "page_idx": 7}, {"type": "text", "text": "First, we have the following bound on the error for policy evaluation. For convenience, we focus solely on the error for least square policy evaluation in Stage 1. A similar analysis can be conducted for policy evaluation error in Stage 2. We decompose the error for policy evaluation into two parts, one is the approximation error due to the finite number of features, and one is the statistical error due to the finite number of samples. Combine the approximation error and the statistical error, we have the following bound on the error for policy evaluation. Detailed proof can be found in Appendix D. ", "page_idx": 7}, {"type": "text", "text": "Theorem 1. Let L = \u0398(log n). Denote g\u02dc\u03b1 := sups,a,b $\\tilde{g}_{\\alpha}\\;:=\\;\\mathrm{sup}_{s,a,b}\\;\\frac{g_{\\alpha}(f(s,a,b))}{\\alpha^{d}}$ and Q\u02c6\u03c0\u03a6,,\u03c0L $\\hat{Q}_{\\Phi,L}^{\\overline{{\\pi}},\\pi}\\,=\\,\\phi^{\\bar{\\top}}\\bar{w}_{L}$ . With probability at least $1-\\delta$ , we have that for the random features, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left\\Vert Q^{\\overline{{\\pi}},\\underline{{\\pi}}}-\\hat{Q}_{\\Phi_{\\mathrm{rf}},L}^{\\overline{{\\pi}},\\underline{{\\pi}}}\\right\\Vert_{\\nu_{\\overline{{\\pi}},\\underline{{\\pi}}}}=\\tilde{O}\\left(\\frac{\\tilde{g}_{\\alpha}}{(1\\!-\\!\\gamma)^{2}\\sqrt{m}}+\\frac{\\tilde{g}_{\\alpha}^{6}m^{3}}{(1\\!-\\!\\gamma)\\Upsilon_{1}^{2}\\Upsilon_{2}\\sqrt{n}}\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and for the Nystr\u00f6m features, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left\\lVert Q^{\\overline{{\\boldsymbol{\\pi}}},\\pi}-\\hat{Q}_{\\Phi_{\\mathrm{nys}},L}^{\\overline{{\\boldsymbol{\\pi}}},\\underline{{\\pi}}}\\right\\rVert_{\\nu_{\\overline{{\\boldsymbol{\\pi}}},\\underline{{\\pi}}}}=\\tilde{O}\\left(\\frac{\\tilde{g}_{\\alpha}}{(1-\\gamma)^{2}n_{\\mathrm{nys}}}+\\frac{\\tilde{g}_{\\alpha}^{6}m^{3}}{(1-\\gamma)\\Upsilon_{1}^{2}\\Upsilon_{2}\\sqrt{n}}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "As shown in Theorem D.1, Nystr\u00f6m method improves the approximation error from $O(m^{-1})$ to $O(n_{n y s}^{-1})$ with a mild assumption to make in the kernel literature (cf. Theorem 2 in [Belkin, 2018]). Then, we provide the error analysis for policy optimization. We remark that in the following proof we assume the action space $|{\\mathcal{A}}|$ is finite for simplicity. The following assumption assumes that the selection probability of each action is positive under each state, which is a commonly used assumption in the analysis of policy gradient type methods [Alacaoglu et al., 2022, Lan, 2023].4 ", "page_idx": 7}, {"type": "text", "text": "Assumption 5. There exists a constant $\\underline{{c}}$ such that, for any policy iterate pair $\\overline{{\\pi}}_{k},\\underline{{\\pi}}_{k}$ , for any state action tuple $s,a,b,$ , it holds that $\\overline{{\\pi}}_{k}(a|s)\\geq\\underline{{c}}>0,\\underline{{\\pi}}_{k}(b|s)\\geq\\underline{{c}}>0.$ . ", "page_idx": 7}, {"type": "text", "text": "We now present the result for policy optimization. We use Perolat et al. [2015]\u2019s error propagation framework, which needs the results in each stage. The detailed proof of it can be found in Appendix E. First, we analyze the error in the Stage 1 of Algorithm 3. ", "page_idx": 7}, {"type": "text", "text": "Lemma 3. Denote $Q_{k-1}\\,=\\,Q^{\\overline{{{\\pi}}}_{k-1},\\underline{{{\\pi}}}_{k-1}}$ , $\\overline{{\\pi}}^{s}Q^{s}\\underline{{\\pi}}^{s}\\,=\\,\\mathbb{E}_{a\\sim\\overline{{\\pi}}(\\cdot|s),b\\sim\\underline{{\\pi}}(\\cdot|s)}[Q(s,a,b)]$ . Let Assumption $^{5}$ hold. In Stage 1 of Algorithm 3, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb E\\left[\\underset{\\overline{\\tau}}{\\operatorname*{max}}\\overline{\\pi}Q_{k-1\\underline{\\pi}_{k}}-\\underset{\\underline{\\pi}}{\\operatorname*{min}}\\overline{\\pi}_{k}Q_{k-1\\underline{{\\pi}}}\\right]}\\\\ &{\\leq\\!\\frac{1}{\\eta T}\\log\\frac{1}{\\underline{c}}+2\\|\\hat{Q}^{\\overline{\\pi}_{k-1},\\underline{{\\pi}}_{k-1}}-Q^{\\overline{\\pi}_{k-1},\\underline{{\\pi}}_{k-1}}\\|_{\\nu_{\\overline{\\pi}_{k-1},\\underline{{\\pi}}_{k-1}}}+\\frac{\\eta}{2(1-\\gamma)^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "4This assumption can be removed if we use a $\\beta-$ greedy mixed version of $\\overline{{\\pi}}_{k},\\underline{{\\pi}}_{k}$ . As the $\\beta-$ greedy mixed policies are shifted with the original ones, we provide the convergence analyses of SDEPO with $\\beta-$ greedy exploration in Appendix E.1. ", "page_idx": 7}, {"type": "text", "text": "This result shows how the error in the approximation of the $Q$ -function and the learning rate $\\eta$ impact the optimization performance in Stage 1. Next, we turn to Stage 2 and provide an upper bound on the one-sided error. ", "page_idx": 8}, {"type": "text", "text": "Lemma 4. Let Assumption $^{5}$ hold and $\\mu_{0}$ be a state distribution. In Stage 2 of Algorithm 3, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=1}^{x}\\mathbb{E}_{s\\sim\\mu_{0}}\\left[V^{\\overline{{\\pi}}_{k}^{\\prime},\\underline{{\\pi}}_{k}}(s)-V^{\\overline{{\\pi}}_{k,t}^{\\prime},\\underline{{\\pi}}_{k}}(s)\\right]}\\\\ &{\\leq\\!\\frac{1}{1-\\gamma}\\left(\\frac{\\log\\frac{1}{c}}{\\eta T}+\\frac{\\eta}{(1-\\gamma)^{2}}+\\eta\\|\\hat{Q}^{\\overline{{\\pi}}_{k,t}^{\\prime},\\underline{{\\pi}}_{k}}-Q^{\\overline{{\\pi}}_{k,t}^{\\prime},\\underline{{\\pi}}_{k}}\\|_{\\nu_{\\overline{{\\pi}}_{k,t}^{\\prime},\\pi_{k}}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Building on the results from Stage 1 and Stage 2, along with the policy evaluation error, and using the error propagation framework from Perolat et al. [2015], we can now derive the overall convergence result for the policy optimization process. Specifically, we obtain the following proposition, which quantifies the iteration complexity required to reach a near-optimal solution: ", "page_idx": 8}, {"type": "text", "text": "Proposition 1. Let L = \u0398(log n) and T = $\\begin{array}{r}{T\\,=\\,\\sqrt{\\frac{\\log1/\\underline{{c}}}{\\log n}(\\frac{1}{(1-\\gamma)^{2}}+\\frac{\\tilde{g}_{\\alpha}}{(1-\\gamma)^{2}\\sqrt{m}}+\\frac{\\tilde{g}_{\\alpha}^{6}m^{3}}{(1-\\gamma)\\Upsilon_{1}^{2}\\Upsilon_{2}\\sqrt{n}})}}\\end{array}$ for random features and log n ((1\u22121\u03b3)2 +(1\u2212\u03b3)\u03b12nnys +(1\u2212\u03b3)\u03b1\u03a521\u03a52\u221an) for Nystr\u00f6m features. Iteration complexity to ge $\\begin{array}{r}{:E_{s\\sim\\mu_{0}}\\left[\\operatorname*{max}_{\\overline{\\pi}}V^{\\overline{\\pi},\\underline{\\pi}_{k}}(s)-V^{*}(s)\\right]\\leq\\epsilon\\,i s\\,\\tilde{O}(\\frac{1}{(1-\\gamma)^{3}\\epsilon}).}\\end{array}$ Proposition 1 shows the iteration complexity to one-sided NE and  it can be directly extended to establish a two-sided NE by applying the algorithm with the roles switched [Zhao et al., 2022]. ", "page_idx": 8}, {"type": "text", "text": "5 The Practical variant of SDEPO ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Many practical $2\\mathtt{p}0\\mathtt{s}\\mathtt{-M G s}$ not only have continuous state spaces but also continuous action spaces, such as such as real-time strategy games [Vinyals et al., 2019, Berner et al., 2019] and robust policy optimization [Pinto et al., 2017]. Note that the natural policy gradient update (31) and (12) in SDEPO involve the expectation w.r.t. the action, which is generally intractable for continuous action space. Hence, we propose a practical variant of SDEPO, named SDEPO-NN, to deal with continuous (or largescale discrete ) action space. SDEPO-NN utilizes neural networks in policy $\\pi$ and the state-action value function approximation $Q$ , detailed in Algorithm F. ", "page_idx": 8}, {"type": "text", "text": "Based on the spectral dynamic embedding $\\phi$ , we parameterize the $\\overline{{Q}}$ function of player one as $\\overline{{Q}}_{\\overline{{\\theta}}}(s,a,b)\\ =\\ r(s,a,b)\\:+\\:\\phi(s,a,b)^{\\top}\\overline{{\\theta}}$ and parameterize the $Q$ function of player two as $\\underline{{Q}}_{\\underline{{\\theta}}}(s,a,b)=r(s,a,b)\\!+\\!\\phi(s,a,b)^{\\top}\\underline{{\\theta}}.$ and train $\\overline{{Q}}_{\\overline{{\\theta}}}$ and $\\underline{{Q}}_{\\underline{{\\theta}}}$ by minimizing the soft Bellman residual. ", "page_idx": 8}, {"type": "text", "text": "We restrict the players\u2019 policies $\\overline{{\\pi}}_{\\overline{{\\psi}}}$ and $\\underline{{\\pi}}_{\\underline{{\\psi}}}$ to Gaussians with the reparametrization trick, i.e., $a_{t}=$ $f_{\\overline{{\\psi}}}(\\epsilon_{t};s_{t})$ and $b_{t}\\,=\\,f_{\\overline{{\\psi}}}(\\epsilon_{t}^{\\prime};s_{t})$ where $\\epsilon_{t}$ and $\\epsilon_{t}^{\\prime}$ are input noise vectors, sampled from a Gaussian. The policy parameters can be learned by minimizing ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{J(\\overline{{\\psi}})=\\mathbb{E}_{s_{t}\\sim\\mathcal{D},\\epsilon_{t},\\epsilon_{t}^{\\prime}\\sim\\mathcal{N}}\\left[\\log\\overline{{\\pi}}_{\\overline{{\\psi}}}(f_{\\overline{{\\psi}}}(\\epsilon_{t};s_{t})|s_{t})-\\overline{{Q}}\\overline{{\\theta}}(s_{t},\\overline{{\\pi}}_{\\overline{{\\psi}}}(f_{\\overline{{\\psi}}}(\\epsilon_{t};s_{t})|s_{t}),\\underline{{\\pi}}_{\\underline{{\\psi}}}(f_{\\underline{{\\psi}}}(\\epsilon_{t}^{\\prime};s_{t})|s_{t}))\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "and ", "page_idx": 8}, {"type": "equation", "text": "$$\nJ(\\underline{{\\psi}})=\\mathbb{E}_{s_{t}\\sim\\mathcal{D},\\epsilon_{t},\\epsilon_{t}^{\\prime}\\sim N}\\left[\\log\\underline{{\\pi}}_{\\underline{{\\psi}}}(f_{\\underline{{\\psi}}}(\\epsilon_{t}^{\\prime};s_{t})|s_{t})-\\underline{{Q}}_{\\underline{{\\theta}}}(s_{t},\\overline{{\\pi}}_{\\overline{{\\psi}}}(f_{\\overline{{\\psi}}}(\\epsilon_{t};s_{t})|s_{t}),\\underline{{\\pi}}_{\\underline{{\\psi}}}(f_{\\underline{{\\psi}}}(\\epsilon_{t}^{\\prime};s_{t})|s_{t}))\\right],\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\mathcal{D}$ is the replay buffer, and $\\overline{{\\pi}}_{\\overline{{\\psi}}}$ and $\\underline{{\\pi}}_{\\underline{{\\psi}}}$ are defined implicitly in terms of $f_{\\overline{{\\psi}}}$ and $f_{\\underline{{\\psi}}}$ , respectively. ", "page_idx": 8}, {"type": "text", "text": "Note that tabular methods [Alacaoglu et al., 2022, Bai and Jin, 2020, Daskalakis et al., 2020, Wei et al., 2021, Zhao et al., 2022] can discretize the state/action spaces to handle applications with continuous state/action spaces. However, directly applying such methods often incurs the curse of dimension and requires an excessive amount of computation resources even for a small size problem. On the other hand, although there exist theoretical-guaranteed methods for $2\\mathtt{p}0\\mathtt{s}\\mathtt{-M G s}$ with continuous state and action space, they all involve a computational intractable subroutine, i.e., Qiu et al. [2021], Junchi Li et al. [2022] need to solve a difficult \u2019find_ne\u2019/\u2019find_cce\u2019 subroutine, and Jin et al. [2022], Huang et al. [2021] have to tackle a comprehensive constrained optimization problem. ", "page_idx": 8}, {"type": "text", "text": "6 Numerical verification ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we present two experiments to evaluate our methods. The first experiment focuses on a simple zero-sum Markov game featuring a continuous state space and a finite action space, aiming to validate the convergence of SDEPO. The second experiment adapts a multi-agent scenario inspired by the simple push [Lowe et al., 2017], where both the state and action spaces are continuous, to assess the effectiveness of SDEPO-NN. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "In the first experiment, we designed a simple zero-sum Markov game with a continuous state and finite action space ${\\mathcal{S}}=\\mathbb{R}$ , $|{\\mathcal{A}}|=5$ ). The state space is partitioned into 42 distinct intervals: one interval for $(-\\infty,-10)$ , 40 intervals evenly spaced by 0.5 units in the range $[-10,10)$ , and one interval for $(10,\\infty)$ . In the $i$ -th interval, the transition dynamics are defined by $\\bar{P}(s,a,b)=f(s,a,b)+\\epsilon,$ , where $\\mathbf{\\boldsymbol{\\epsilon}}\\sim\\mathcal{N}(\\mathbf{\\boldsymbol{0}},\\mathbf{\\boldsymbol{1}})$ , and $f(s,a,b)=\\epsilon_{i,a,b}$ , with $\\epsilon_{i,a,b}\\sim\\mathrm{Unif}(-10.5,10.5)$ . The reward function is $r(s,a,b)\\,=\\,\\epsilon_{i,a,b}^{\\prime}$ , where $\\epsilon_{i,a,b}^{\\prime}\\,\\sim\\,\\mathrm{Unif}(-1,1)$ . The initial state distribution is assumed to be uniform over $[-10.5,10.5]$ . ", "page_idx": 9}, {"type": "text", "text": "We ran SDEPO for 120 iterations, and measured the convergence of $\\underline{\\pi}$ by metrics in Proposition 1. As shown in Figure 1, SDEPO with random features and Nystr\u00f6m features both converge after 60 iterations. We discretized the state space of this environment and compared it with OFTRL [Zhang et al., 2022], a tabular method where the environment is known. We adopted the parameter settings recommended in [Zhang et al., 2022] and adjusted the environment to a 100-horizon setting. As shown in Figure 1, our method demonstrated superior convergence in this environment. This likely stems from the fact that OFTRL operates on the discretized state space, whereas our method computes on the original state space. ", "page_idx": 9}, {"type": "image", "img_path": "wvQHQgnpGN/tmp/09d402e5288b99af9e9ac6a21d7521a220372bfcbc4cef7fbaf9c826cc863531.jpg", "img_caption": ["Figure 1: Performance illustration of SDEPO and OFTRL for solving the random generated Markov game. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Next, we conduct experiments on an adapted version of simple push [Lowe et al., 2017], wherein both the state and action spaces are continuous. This problem consists of two agents and one landmark. Each agent receives a reward for proximity to the landmark while ensuring the other agent remains distant. Thus, agents must learn to stay close to the landmark and simultaneously push the other agent away. At each time step, a noise $\\epsilon\\sim\\mathcal{N}(0,\\sigma^{2}I_{d})$ is added to the state. ", "page_idx": 9}, {"type": "text", "text": "We implemented SDEPO-NN with random features $({\\tt S D E P O-N N}_{r f}$ ) and Nystr\u00f6m features $(\\mathtt{S D E P O-N N}_{n y s})$ , comparing them against methods where Q functions do not utilize spectral dynamical embedding (NPG-NN). Table 2 shows the results of winning rate after training by 20000 iterations with varying noise levels. It is evident that $\\mathsf{S D E P O-N N}_{r f}$ and $\\mathtt{S D E P O-N N}_{n y s}$ largely outperforms NPG-NN, which shows the effectiveness of the spectral dynamical embedding. ", "page_idx": 9}, {"type": "table", "img_path": "wvQHQgnpGN/tmp/7aa190d8f88a3fa4354f5e7a2d0b133d71ba58c5a2fd776cb2fa59aebddba94d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Table 2: Comparison of winning rates between NPG-NN, $\\mathsf{S D E P O-N N}_{r f}$ , and $\\mathtt{S D E P O-N N}_{n y s}$ in Simple Push with $\\sigma=0.1/0.01$ . The results before and after / correspond to $\\sigma=0.1$ and 0.01, respectively. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose a provably efficient natural policy gradient algorithm for two-player zerosum stochastic Markov games with continuous state. We analyze the approximation error and convergence of the algorithm. To deal with continuous action spaces, a practical variant is provided and demonstrates superior performance. A possible direction is to extend our methods to independent learning setting. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported in part by National Natural Science Foundation of China under Grant 62206248 and National Natural Science Foundation of China under Grant 62402430. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Yasin Abbasi-Yadkori, Peter Bartlett, Kush Bhatia, Nevena Lazic, Csaba Szepesvari, and Gell\u00e9rt Weisz. Politex: Regret bounds for policy iteration using expert prediction. In International Conference on Machine Learning, pages 3692\u20133702. PMLR, 2019.   \nAlekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. J Mach Learn Res, 22(98): 1\u201376, 2021.   \nAhmet Alacaoglu, Luca Viano, Niao He, and Volkan Cevher. A natural actor-critic framework for zero-sum markov games. In International Conference on Machine Learning, pages 307\u2013366. PMLR, 2022.   \nYu Bai and Chi Jin. Provable self-play algorithms for competitive reinforcement learning. In International conference on machine learning, pages 551\u2013560. PMLR, 2020.   \nMikhail Belkin. Approximation beats concentration? an approximation view on inference with smooth radial kernels. In Conference On Learning Theory, pages 1348\u20131361. PMLR, 2018.   \nChristopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemys\u0142aw D\u02dbebiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.   \nShicong Cen, Yuting Wei, and Yuejie Chi. Fast policy extragradient methods for competitive games with entropy regularization. Advances in Neural Information Processing Systems, 34:27952\u2013 27964, 2021.   \nShicong Cen, Yuejie Chi, S Du, and Lin Xiao. Faster last-iterate convergence of policy optimization in zero-sum markov games. In International Conference on Learning Representations (ICLR), 2023.   \nZixiang Chen, Dongruo Zhou, and Quanquan Gu. Almost optimal algorithms for two-player zerosum linear mixture markov games. In International Conference on Algorithmic Learning Theory, pages 227\u2013261. PMLR, 2022.   \nConstantinos Daskalakis, Dylan J Foster, and Noah Golowich. Independent policy gradient methods for competitive reinforcement learning. Advances in neural information processing systems, 33: 5527\u20135540, 2020.   \nA Devinatz. Integral representations of positive definite functions. Transactions of the American Mathematical Society, 74(1):56\u201377, 1953.   \nGregory E Fasshauer. Positive definite kernels: past, present and future. Dolomites Research Notes on Approximation, 4:21\u201363, 2011.   \nJerzy Filar and Koos Vrieze. Competitive Markov decision processes. Springer Science & Business Media, 2012.   \nMostafa Ghaemi, MS Moslehian, and Qingxiang Xu. Kolmogorov decomposition of conditionally completely positive definite kernels. Positivity, 25(2):515\u2013530, 2021.   \nBaihe Huang, Jason D Lee, Zhaoran Wang, and Zhuoran Yang. Towards general function approximation in zero-sum markov games. arXiv preprint arXiv:2107.14702, 2021.   \nNan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contextual decision processes with low bellman rank are pac-learnable. In International Conference on Machine Learning, pages 1704\u20131713. PMLR, 2017.   \nChi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In Conference on Learning Theory, pages 2137\u2013 2143. PMLR, 2020.   \nChi Jin, Qinghua Liu, and Sobhan Miryoosef.i Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms. Advances in neural information processing systems, 34:13406\u201313418, 2021.   \nChi Jin, Qinghua Liu, and Tiancheng Yu. The power of exploiter: Provable multi-agent rl in large state spaces. In International Conference on Machine Learning, pages 10251\u201310279. PMLR, 2022.   \nChris Junchi Li, Dongruo Zhou, Quanquan Gu, and Michael I Jordan. Learning two-player mixture markov games: Kernel function approximation and correlated equilibrium. arXiv e-prints, pages arXiv\u20132208, 2022.   \nGuanghui Lan. Policy mirror descent for reinforcement learning: Linear convergence, new sampling complexity, and generalized problem classes. Mathematical programming, 198(1):1059\u20131106, 2023.   \nFanghui Liu, Xiaolin Huang, Yudong Chen, and Johan AK Suykens. Random features for kernel approximation: A survey on algorithms, theory, and beyond. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(10):7128\u20137148, 2021.   \nRyan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actorcritic for mixed cooperative-competitive environments. Neural Information Processing Systems (NIPS), 2017.   \nJ Mercer. Functions of positive and negative type, and their connection with the theory of integral equations. Philos. Trinsdictions Rogyal Soc, 209:415\u2013446, 1909.   \nMehryar Mohri. Foundations of machine learning, 2018.   \nJulien Perolat, Bruno Scherrer, Bilal Piot, and Olivier Pietquin. Approximate dynamic programming for two-player zero-sum markov games. In International Conference on Machine Learning, pages 1321\u20131329. PMLR, 2015.   \nLerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial reinforcement learning. In International Conference on Machine Learning, pages 2817\u20132826. PMLR, 2017.   \nShuang Qiu, Jieping Ye, Zhaoran Wang, and Zhuoran Yang. On reward-free rl with kernel and neural function approximations: Single-agent mdp and markov game. In International Conference on Machine Learning, pages 8737\u20138747. PMLR, 2021.   \nAli Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning. Advances in neural information processing systems, 21, 2008.   \nTongzheng Ren, Tianjun Zhang, Csaba Szepesv\u00e1ri, and Bo Dai. A free lunch from the noise: Provable and practical exploration for representation learning. In Uncertainty in Artificial Intelligence, pages 1686\u20131696. PMLR, 2022.   \nTongzheng Ren, Zhaolin Ren, Na Li, and Bo Dai. Stochastic nonlinear control via finite-dimensional spectral dynamic embedding. In 2023 IEEE 62nd Annual Conference on Decision and Control (CDC). IEEE, 2023.   \nWalter Rudin. Fourier analysis on groups. Courier Dover Publications, 2017.   \nJan Stochel. Decomposition and disintegration of positive definite kernels on convex\\*-semigroups. In Annales Polonici Mathematici, volume 56, pages 243\u2013294, 1992.   \nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha\u00ebl Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350\u2013354, 2019. ", "page_idx": 11}, {"type": "text", "text": "Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Last-iterate convergence of decentralized optimistic gradient descent/ascent in infinite-horizon competitive markov games. In Conference on learning theory, pages 4259\u20134299. PMLR, 2021.   \nChristopher Williams and Matthias Seeger. Using the nystr\u00f6m method to speed up kernel machines. Adv. in Neural Info. Processing Systems, 13:682\u2013688, 2000.   \nQiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang. Learning zero-sum simultaneousmove markov games using function approximation and correlated equilibrium. In Conference on learning theory, pages 3674\u20133682. PMLR, 2020.   \nLin Yang and Mengdi Wang. Sample-optimal parametric q-learning using linearly additive features. In International conference on machine learning, pages 6995\u20137004. PMLR, 2019.   \nHuizhen Yu and Dimitri P Bertsekas. New error bounds for approximations from projected linear equations. In European Workshop on Reinforcement Learning, pages 253\u2013267. Springer, 2008.   \nKaiqing Zhang, Zhuoran Yang, and Tamer Basar. Policy optimization provably converges to nash equilibria in zero-sum linear quadratic games. Advances in Neural Information Processing Systems, 32, 2019.   \nRunyu Zhang, Qinghua Liu, Huan Wang, Caiming Xiong, Na Li, and Yu Bai. Policy optimization for markov games: Unified framework and faster convergence. Advances in Neural Information Processing Systems, 35:21886\u201321899, 2022.   \nYulai Zhao, Yuandong Tian, Jason Lee, and Simon Du. Provably efficient policy optimization for two-player zero-sum markov games. In International Conference on Artificial Intelligence and Statistics, pages 2736\u20132761. PMLR, 2022. ", "page_idx": 12}, {"type": "text", "text": "A Derivation of Spectral Dynamic Embedding ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we derivate spectral dynamic embedding and then prove the Q-function for arbitrary policy pair can be linearly represented by the feature functions $\\{\\phi_{\\omega}(s,a,b)\\}$ with $\\omega\\sim\\mathcal{N}\\left(0,\\sigma^{2}I_{d}\\right)$ and $\\phi_{M}(s,a,b)\\in\\ell_{2}$ . ", "page_idx": 13}, {"type": "text", "text": "Lemma A.1 (i.e. Lemma 1). Consider any $\\alpha\\in[0,1)$ . Denote $\\begin{array}{r}{k_{\\alpha}(x,x^{\\prime})=\\exp\\left(-\\frac{(1-\\alpha^{2})\\|x-x^{\\prime}\\|^{2}}{2\\sigma^{2}}\\right)}\\end{array}$ for any $0\\leq\\alpha<1$ . We can decompose $k_{\\alpha}(x,x^{\\prime})$ using Bochner decomposition and Mercer decomposition. Let ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\psi_{\\omega}(s,a,b)=\\frac{g_{\\alpha}(f(s,a,b))}{\\alpha^{d}}\\left[\\cos\\left(\\frac{\\omega^{\\top}f(s,a,b)}{\\sqrt{1-\\alpha^{2}}}\\right),\\sin\\left(\\frac{\\omega^{\\top}f(s,a,b)}{\\sqrt{1-\\alpha^{2}}}\\right)\\right],}}\\\\ {{\\displaystyle\\ \\ \\ \\ \\chi_{\\omega}(s^{\\prime})=p_{\\alpha}(s^{\\prime})[\\cos(\\sqrt{1-\\alpha^{2}}\\omega^{\\top}s^{\\prime}),\\sin(\\sqrt{1-\\alpha^{2}}\\omega^{\\top}s^{\\prime})]^{\\top},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\begin{array}{r l r l r l r l r l}{g_{\\alpha}\\left(f(s,a,b)\\right)}&{{}:=}&{}&{\\exp\\left(\\frac{\\alpha^{2}\\|f(s,a,b)\\|^{2}}{2(1-\\alpha^{2})\\sigma^{2}}\\right)\\!,}&{\\omega}&{{}\\sim}&{}&{{}N(0,\\sigma^{-2}I_{d}),}&{a n d}&{p_{\\alpha}(s^{\\prime})}&{{}=}&{}&{{}\\pi(1,\\sigma^{-2}I_{d}),}\\end{array}$ $\\begin{array}{r}{\\frac{\\alpha^{d}}{(2\\pi\\sigma^{2})^{d/2}}\\exp\\left(-\\frac{\\|\\alpha s^{\\prime}\\|^{2}}{2\\sigma^{2}}\\right)}\\end{array}$ is a Gaussian distribution for $s^{\\prime}$ with standard deviation $\\frac{\\sigma}{\\alpha}$ . Using Bochner decomposition, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P(s^{\\prime}|s,a,b)=\\!\\!\\mathbb{E}_{\\omega\\sim N(0,\\sigma^{-2}I_{d})}\\left[\\psi_{\\omega}(s,a,b)^{\\top}\\chi_{\\omega}(s^{\\prime})\\right]:=\\langle\\psi_{\\omega}(s,a,b),\\chi_{\\omega}(s^{\\prime})\\rangle_{N(0,\\sigma^{-2}I_{d})}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By Mercer\u2019s theorem, $k_{\\alpha}$ admits a decomposition for any $(x,x^{\\prime})\\in\\mathcal{X}$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\nk_{\\alpha}(x,x^{\\prime})\\!=\\!\\sum_{i=1}^{\\infty}\\!\\sigma_{\\alpha,i}e_{\\alpha,i}(x)e_{\\alpha,i}(x^{\\prime}),\\;\\{e_{\\alpha,i}\\}\\;a\\;b a s i s f o r\\;\\mathcal{L}_{2}(\\mu).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We denote $k_{\\alpha}(x,x^{\\prime})\\,=\\,\\langle\\tilde{e_{\\alpha}}(x),\\tilde{e_{\\alpha}}(x^{\\prime})\\rangle_{\\ell_{2}}$ where $\\tilde{e}_{\\alpha,i}(x)\\,=\\,\\sqrt{\\sigma_{\\alpha,i}}e_{\\alpha,i}(x)$ for each positive integer $i$ .5 ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\boldsymbol{\\psi}_{M}(s,a,b)=\\frac{g_{\\alpha}\\left(f(s,a,b)\\right)}{\\alpha^{d}}\\boldsymbol{\\tilde{e}}_{\\alpha}\\left(\\frac{f(s,a,b)}{1-\\alpha^{2}}\\right)^{\\top},\\boldsymbol{\\chi}_{M}(s^{\\prime})=p_{\\alpha}(s^{\\prime})\\boldsymbol{\\tilde{e}}_{\\alpha}\\left(s^{\\prime}\\right)^{\\top}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then, ", "page_idx": 13}, {"type": "equation", "text": "$$\nP(s^{\\prime}|s,a,b)=\\langle\\psi_{M}(s,a,b),\\chi_{M}(s^{\\prime})\\rangle_{\\ell_{2}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. For Bochner decomposition, we first notice that $\\forall\\alpha\\in(0,1)$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(s^{\\prime}|s,a,b)\\propto\\exp\\left(-\\frac{\\left\\|s^{\\prime}-f(s,a,b)\\right\\|^{2}}{2\\sigma^{2}}\\right)}\\\\ &{\\qquad\\qquad=\\exp\\left(-\\frac{\\left\\|\\alpha s^{\\prime}\\right\\|^{2}}{2\\sigma^{2}}\\right)\\exp\\left(-\\frac{\\left\\|(1-\\alpha^{2})s^{\\prime}-f(s,a,b)\\right\\|^{2}}{2\\sigma^{2}(1-\\alpha^{2})}\\right)\\exp\\left(\\frac{\\alpha^{2}\\left\\|f(s,a,b)\\right\\|^{2}}{2(1-\\alpha^{2})\\sigma^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The factorization of the transition $P(s^{\\prime}|s,a,b)$ in (15) can thus be derived from the property of the Gaussian kernel by applying Bochner decomposition in Lemma 2 to the second term in (18). ", "page_idx": 13}, {"type": "text", "text": "For the Mercer decomposition, the proof is analogous, where here we apply Mercer\u2019s theorem to decompose the middle term in terms of the kernel $k_{\\alpha}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\dot{\\gamma}_{\\alpha}\\left(s^{\\prime},\\frac{f(s,a,b)}{1-\\alpha^{2}}\\right)=\\,\\exp\\left(-\\frac{(1-\\alpha^{2})\\left\\|s^{\\prime}-\\frac{f(s,a,b)}{1-\\alpha^{2}}\\right\\|^{2}}{2\\sigma^{2}}\\right)=\\,\\exp\\left(-\\frac{\\left\\|(1-\\alpha^{2})s^{\\prime}-f(s,a,b)\\right\\|^{2}}{2\\sigma^{2}(1-\\alpha^{2})}\\right)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Let $\\phi_{\\omega}(\\cdot)\\,=\\,[\\psi_{\\omega}(\\cdot),r(\\cdot)]$ and $\\phi_{M}(\\cdot)\\,=\\,[\\psi_{M}(\\cdot),r(\\cdot)]$ . Here we prove the Q-function for arbitrary policy pair can be linearly represented by the feature functions $\\{\\phi_{\\omega}(s,a,b)\\}$ with $\\omega\\sim\\mathcal{N}\\left(0,\\sigma^{2}I_{d}\\right)$ and $\\phi_{M}(s,a,b)\\in\\ell_{2}$ . ", "page_idx": 14}, {"type": "text", "text": "Lemma A.2 (i.e. Lemma 2). For any policy, there exist weights $\\{\\theta_{\\omega}^{\\overline{{\\pi}},\\underline{{\\pi}}}\\}$ (where $\\omega\\sim\\mathcal{N}(0,\\sigma^{-2}I_{d}))$ and $\\theta_{M}^{\\overline{{\\pi}},\\underline{{\\pi}}}\\in\\ell_{2}$ such that the corresponding value function satisfies ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\cal Q}^{\\overline{{\\pi}},\\underline{{\\pi}}}(s,a,b)=\\,\\left\\langle\\phi_{\\omega}(s,a,b),\\theta_{\\omega}^{\\overline{{\\pi}},\\underline{{\\pi}}}\\right\\rangle_{\\scriptstyle\\!N(0,\\sigma^{-2}I_{d})}=\\,\\left\\langle\\phi_{M}(s,a,b),\\theta_{M}^{\\overline{{\\pi}},\\underline{{\\pi}}}\\right\\rangle_{\\ell_{2}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. Denote $\\mu_{\\omega}(\\cdot)=[\\chi_{\\omega}(\\cdot),0]^{\\top}$ , $\\mu_{M}(\\cdot)=[1,\\chi_{M}(\\cdot)]^{\\intercal}$ , $\\theta_{r}=[0,0,1]^{\\top}$ , $\\theta_{r,M}=[1,0,0,\\ldots]^{\\top}\\in$ $\\ell_{2}$ . Our claim can be verified easily by applying the decompositions to Bellman recursion: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q^{\\pi,\\underline{{\\pi}}}(s,a,b)=r(s,a,b)+\\gamma\\mathbb{E}_{P}\\left[V^{\\pi,\\underline{{\\pi}}}(s^{\\prime})\\right]}\\\\ &{\\phantom{\\quad\\quad\\quad}=\\Big\\langle\\phi_{\\omega}(s,a,b),\\underbrace{\\theta_{r}+\\gamma\\int_{S}\\mu_{\\omega}(s^{\\prime})V^{\\pi,\\underline{{\\pi}}}(s^{\\prime})d s^{\\prime}}_{\\theta_{\\omega}^{\\pi,\\underline{{\\pi}}}}\\Big\\rangle_{\\mathcal{N}(0,\\sigma^{-2}I_{d})}}\\\\ &{\\phantom{\\quad\\quad\\quad}=\\Big\\langle\\phi_{M}(s,a,b),\\underbrace{\\theta_{r,M}+\\gamma\\int_{S}\\mu_{M}(s^{\\prime})V^{\\pi,\\underline{{\\pi}}}(s^{\\prime})d s^{\\prime}}_{\\theta_{M}^{\\pi,\\underline{{\\pi}}}}\\Big\\rangle_{\\ell_{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the second equation comes from the Bochner decomposition and the third equation comes from the Mercer decomposition. ", "page_idx": 14}, {"type": "text", "text": "B Derivation of Nystr\u00f6m method ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Consider a bounded and continuous positive definite kernel $k(x,y)$ defined on a compact space $\\boldsymbol{S}$ , along with any probability measure $\\mu$ on $\\boldsymbol{S}$ , e.g. the $\\mu_{\\mathrm{nys}}$ considered in Algorithm 3. Mercer\u2019s theorem guarantees the existence of eigenvalues $\\{\\sigma_{j}\\}_{j=1}^{\\infty}$ and orthonormal eigenvectors $\\{e_{j}\\}_{j=1}^{\\infty}\\subset$ $\\mathcal{L}_{2}(\\mu)$ such that for any $j\\in\\mathcal N$ and any $x\\in S$ , the following holds: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\int_{S}k(x,y)e_{j}(y)d\\mu(y)=\\sigma_{j}e_{j}(x).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The Nystr\u00f6m method provides an approximation of the Mercer eigendecomposition of $k(x,y)\\,=$ $\\begin{array}{r}{\\sum_{j=1}^{\\infty}\\overset{\\cdot}{\\sigma}_{j}e_{j}(x)e_{j}(y)}\\end{array}$ in the form ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{k}_{m}^{n}(x,y)=\\sum_{i=1}^{m}\\hat{\\sigma}_{i}\\hat{e}_{i}(x)\\hat{e}_{i}(y)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for some positive integer $m$ . Here, the pairs where the $(\\hat{\\sigma}_{i},\\hat{e}_{i})$ are determined through a numerical approximation of the eigenfunction problem outlined (20). We will now describe the procedure in detail. ", "page_idx": 14}, {"type": "text", "text": "Recall that $n_{\\mathrm{nys}}$ refers to the number of samples used to construct the Nystr\u00f6m features. For simplicity, in the sections of the Appendix that address Nystr\u00f6m approximation results, we denote $n:=n_{\\mathrm{nys}}$ , unless otherwise noted. It is important not to confuse this $n$ with the $n$ used to represent the number of samples for statistical learning of the $Q$ -function. Suppose we draw $n$ independent samples $X^{n}~=~\\{\\bar{x}_{s}\\}_{s=1}^{n}$ from $\\boldsymbol{S}$ according to the distribution $\\mu$ . For any eigenfunction $e_{j}$ , the eigenfunction problem from (20) can be numerically approximated by: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{n}\\sum_{s=1}^{n}k(\\boldsymbol{x},\\boldsymbol{x}_{s})e_{j}(\\boldsymbol{x}_{s})\\approx\\sigma_{j}e_{j}(\\boldsymbol{x}),}\\\\ {\\displaystyle\\frac{1}{n}\\sum_{s=1}^{n}(e_{j}(\\boldsymbol{x}_{s}))^{2}\\approx1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let $K^{(n)}\\in\\mathbb{R}^{n\\times n}$ denote the Gram matrix where $(K^{n})_{r s}\\,=\\,k(x_{r},x_{s})$ , and let the eigendecomposition of $K^{(n)}$ be $K^{(n)}U=\\Lambda U$ with $U$ orthogonal and $\\Lambda$ diagonal. To satisfy (21a), the condition must hold for all $x\\in X^{n}$ . Specifically, for any $r\\in[n]$ , we should expect ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\frac{1}{n}}\\sum_{s=1}^{n}k(x_{r},x_{s})e_{j}(x_{s})\\approx\\sigma_{j}e_{j}(x_{r})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for any eigenfunction $e_{j}(\\cdot)$ and its corresponding eigenvalue $\\sigma_{j}$ of $k(\\cdot,\\cdot)$ . For any eigenvector $u_{i}$ of the Gram matrix $K^{(n)}$ , we have the relation ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{s=1}^{n}k(x_{r},x_{s})(u_{i})_{s}=\\frac{\\lambda_{i}}{n}(u_{i})_{r}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "It is therefore natural to extend $u_{i}\\,\\in\\,\\mathbb{R}^{n}$ to an eigenfunction $\\tilde{e}_{i}$ by setting ${\\tilde{e}}_{i}(x_{s})\\,=\\,(u_{i})_{s}$ for any $x_{s}\\in X^{n}$ , and for other $x$ , define ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{e}_{i}(\\boldsymbol{x}):=\\frac{1}{\\lambda_{i}}\\sum_{s=1}^{n}k(\\boldsymbol{x},\\boldsymbol{x}_{s})(\\boldsymbol{u}_{i})_{s},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with associated eigenvalue $\\frac{\\lambda_{i}}{n}$ . To satisfy the orthonormality condition from (21b), since ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{s=1}^{n}\\tilde{e}_{i}(x_{s})^{2}=\\frac{1}{n}\\sum_{s=1}^{n}(u_{i})_{s}^{2}=\\frac{1}{n},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "we scale ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{e}_{i}(x):=\\sqrt{n}\\tilde{e}_{i}(x)=\\frac{\\sqrt{n}}{\\lambda_{i}}\\sum_{s=1}^{n}k(x,x_{s})(u_{i})_{s}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using these scaled eigenfunctions, we define the Nystr\u00f6m approximation ${\\hat{k}}^{n}$ of the original kernel $k$ as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{k}^{n}(x,y)=\\ \\sum_{i=1}^{n}\\frac{\\lambda_{i}}{n}\\hat{e}_{i}(x)\\hat{e}_{i}(y)=\\,k(x,X^{n})\\left(\\sum_{i=1}^{n}\\frac{1}{\\lambda_{i}}u_{i}u_{i}^{\\top}\\right)k(X^{n},x),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $k(x,X^{n})$ is a row vector with components $k(x,x_{s})$ , and $k(X^{n},y)$ is a column vector with components $k(x_{s},y)$ . Based on (23), for any $m\\leq n$ , we then define the rank- $m$ Nystr\u00f6m approximation as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{k}_{m}^{n}(x,y)=k(x,X^{n})\\left(\\sum_{i=1}^{m}\\frac{1}{\\lambda_{i}}u_{i}u_{i}^{\\top}\\right)k(X^{n},x),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which is the rank- $^{\\cdot m}$ Nystr\u00f6m kernel approximation used in the paper. Since it can be written as $\\hat{k}_{,m}^{n}(x,y)=\\varphi_{\\mathrm{nys}}(x)^{\\top}\\bar{\\varphi}_{\\mathrm{nys}}(y)$ , where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\big(\\varphi_{\\mathrm{nys}}\\big)_{i}(\\cdot)=\\frac{1}{\\sqrt{\\lambda_{i}}}u_{i}^{\\top}k(X^{n},\\cdot),\\quad\\forall i\\in[m],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "we can view $\\varphi_{\\mathrm{nys}}(\\cdot)\\in\\mathbb{R}^{m}$ as the rank- $^m$ Nystr\u00f6m features corresponding to the Nystr\u00f6m approximation. ", "page_idx": 15}, {"type": "text", "text": "C The performance difference lemma ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma C.3 (Performance difference lemma. See [Alacaoglu et al., 2022]). For any policies $\\pi_{1},\\pi_{2},\\underline{{\\pi}}$ and any state $s_{0}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\nV^{\\pi_{1},\\underline{{\\pi}}}(s_{0})-V^{\\pi_{2},\\underline{{\\pi}}}(s_{0})=\\frac{1}{1-\\gamma}\\mathbb{E}_{s^{\\prime}\\sim d_{s_{0}}^{\\pi_{1},\\pi}}\\langle\\mathbb{E}_{b\\sim\\underline{{\\pi}}(\\cdot\\vert s)}Q^{\\pi_{2},\\underline{{\\pi}}}(s,\\cdot,b),\\pi_{1}(\\cdot\\vert s)-\\pi_{2}(\\cdot\\vert s)\\rangle\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\begin{array}{r}{d_{s_{0}}^{\\pi_{1},\\pi_{2}}(s)=(1-\\gamma)\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathbb{P}^{\\pi_{1},\\pi_{2}}(s_{t}=s|s_{0}),}\\end{array}$ , where $\\mathbb{P}^{\\pi_{1},\\pi_{2}}\\big(s_{t}=s|s_{0}\\big)$ denote the probability that $s_{t}=s$ after start ing at $s_{0}$ and following the policies $\\pi_{1}$ and $\\pi_{2}$ . ", "page_idx": 15}, {"type": "text", "text": "D Error analysis for Policy Evaluation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We provide a brief outline of our overall proof strategy for policy evaluation. For convenience, we focus solely on the error for least square policy evaluation in Stage 1. A similar analysis can be conducted for policy evaluation error in Stage 2. ", "page_idx": 16}, {"type": "text", "text": "We decompose the error into two parts, one is the approximation error due to the limitation of our basis (i.e., finite $m$ in Line 1 of Algorithm 1 and Line 4 of Algorithm 2), and one is the statistical error due to the finite number of samples we use (i.e., finite $n$ in Line 27 of Algorithm 3). For notational simplicity, we omit $\\overline{{\\pi}},\\pi$ and use $\\nu$ to denote the stationary distribution corresponding to $\\overline{\\pi},\\underline{\\pi}$ in this section. For the ease of presentation, we omit the polynomial dependency on $c_{f}$ and focus on the dependency of other terms of interest. ", "page_idx": 16}, {"type": "text", "text": "D.1 Approximation Error ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We start by deriving a bound on the approximation error when representing the $Q$ -function for a given policy pair using an imperfect, finite-dimensional feature set. In such cases, the best possible approximation, denoted as $\\tilde{Q}$ , is the solution to a projected Bellman equation (cf. Yu and Bertsekas [2008]), which is defined as follows: given any (possibly finite-dimensional) feature map $\\Phi:=$ $\\{\\phi(s,a,b)\\}_{(s,a,b)\\in{\\mathcal{S}}\\times{\\mathcal{A}}\\times{\\mathcal{A}}}$ , the approximation $\\tilde{Q}\\Phi^{\\overline{{\\pi}},\\pi}$ is defined by: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tilde{Q}\\Phi^{\\overline{{{\\pi}}},\\underline{{{\\pi}}}}=\\Pi\\nu,\\Phi(r+P^{\\overline{{{\\pi}}},\\underline{{{\\pi}}}}\\tilde{Q}\\Phi^{\\overline{{{\\pi}}},\\underline{{{\\pi}}}}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\nu$ represents the stationary distribution under $\\overline{{\\pi}},\\underline{{\\pi}}$ , and the operator $P^{\\overline{{\\pi}},\\underline{{\\pi}}}$ is given by ", "page_idx": 16}, {"type": "equation", "text": "$$\n(P^{\\pi,\\underline{{\\pi}}}f)(s,a,b)=\\mathbb{E}_{(s^{\\prime},a^{\\prime},b^{\\prime})\\sim P(s,a,b)\\times\\overline{{\\pi}}\\times\\underline{{\\pi}}}f(s^{\\prime},a^{\\prime},b^{\\prime}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and $\\Pi_{\\nu,\\Phi}$ is the projection operator as defined as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Pi_{\\nu,\\Phi}Q=\\mathop{\\mathrm{arg\\,min}}_{f\\in\\mathrm{span}(\\Phi)}\\mathbb{E}_{\\nu}\\left(Q(s,a,b)-f(s,a,b)\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Our focus on $\\tilde{Q}_{\\Phi}^{\\pi}$ is motivated by the fact that the least-squares policy evaluation step in Algorithm 3 (see equation (11)) recovers $\\tilde{Q}_{\\Phi}^{\\pi}$ if the number of samples, $n$ , goes to infinity. The effect of a finite sample size $n$ on the statistical error will be discussed later. ", "page_idx": 16}, {"type": "text", "text": "Next, we present our bound on the approximation error for the random and Nystr\u00f6m features. We begin with the random feature approach, which . To do so, we first need the following technical result, which relies on the following technical lemma, adapted from Lemma 1 in Rahimi and Recht [2008]. ", "page_idx": 16}, {"type": "text", "text": "Lemma D.4 (cf. Lemma 1 from Rahimi and Recht [2008]). Let $p$ be a distribution on a space $\\Omega$ , and consider a mapping $\\phi(x;\\omega)\\in\\mathbb{R}^{\\ell}$ . Suppose ", "page_idx": 16}, {"type": "equation", "text": "$$\nf^{*}(x)=\\int_{\\Omega}p(\\omega)\\beta(\\omega)^{\\top}\\phi(x;\\omega)d\\omega,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for some vector $\\beta(\\omega)\\in\\mathbb{R}^{\\ell}\\,w h e r e\\,\\operatorname*{sup}_{x,\\omega}\\left|\\beta(\\omega)^{\\top}\\phi(x;\\omega)\\right|\\leq C$ for some $C>0$ . Consider $\\{\\omega_{i}\\}_{i=1}^{k}$ drawn iid from $p_{i}$ , and denote the sample a verage of $\\begin{array}{r}{f^{*}\\;a s\\;\\hat{f}(x)=\\frac{1}{K}\\sum_{k=1}^{K}\\beta(\\omega_{k})^{\\top}\\phi(x;\\omega_{k})}\\end{array}$ . Then, for any $\\delta>0$ , with probability at least $1-\\delta$ over the random draws of $\\{\\omega_{i}\\}_{i=1}^{k}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sqrt{\\int_{X}\\left({\\hat{f}}(x)-f^{*}(x)\\right)^{2}d\\mu(x)}\\leq{\\frac{C}{\\sqrt{K}}}\\left(1+{\\sqrt{2\\log{\\frac{1}{\\delta}}}}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Based on Lemma D.4, we derive the approximation error with random features. ", "page_idx": 16}, {"type": "text", "text": "Proposition D.1 $Q$ -Approximation error with random features). We define the feature map $\\Phi_{\\mathrm{rf}}$ for random features as follows ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Phi_{\\mathrm{rf}}=\\{[\\psi_{\\mathrm{rf}}(s,a,b),r(s,a,b)]\\}_{(s,a,b)\\in S\\times A\\times A},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\psi_{\\mathrm{rf}}\\left(s,a,b\\right)$ is defined in (7) in Algorithm 3. Then, for any $\\delta\\,>\\,0$ , with probability at least $1-\\delta$ , we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\|Q^{\\overline{{{\\pi}}},\\underline{{{\\pi}}}}-\\tilde{Q}_{\\Phi_{\\mathrm{rf}}}^{\\overline{{{\\pi}}},\\underline{{{\\pi}}}}\\right\\|_{\\nu}=\\tilde{O}\\left(\\frac{\\gamma\\tilde{g}_{\\alpha}}{(1-\\gamma)^{2}\\sqrt{m}}\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\Vert\\cdot\\Vert_{\\nu}$ is the $L_{2}$ norm defined as $\\begin{array}{r}{\\|f\\|_{\\nu}=\\int f^{2}d\\nu,}\\end{array}$ , and $\\tilde{Q}_{\\Phi_{\\mathrm{rf}}}^{\\overline{{\\pi}},\\pi}$ is defined in (26). ", "page_idx": 16}, {"type": "text", "text": "Proof. By leveraging the contraction property and the results from $\\mathrm{Yu}$ and Bertsekas [2008], we establish the following bound: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\|Q^{\\overline{{\\pi}},\\underline{{\\pi}}}-\\tilde{Q}_{\\Phi_{\\mathrm{rf}}}^{\\overline{{\\pi}},\\underline{{\\pi}}}\\right\\|_{\\nu}\\leq\\frac{1}{1-\\gamma}\\left\\|Q^{\\overline{{\\pi}},\\underline{{\\pi}}}-\\Pi_{\\nu,\\Phi_{\\mathrm{rf}}}Q^{\\overline{{\\pi}},\\underline{{\\pi}}}\\right\\|_{\\nu},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\Pi_{\\nu,\\Phi_{\\mathrm{rf}}}$ is defined as in (28). By definition $\\Pi_{\\nu,\\Phi_{\\mathrm{rf}}}$ is contractive under $\\Vert\\cdot\\Vert_{\\nu}$ . Note that ", "page_idx": 17}, {"type": "equation", "text": "$$\nQ^{\\overline{{\\pi}},\\overline{{\\pi}}}(s,a,b)=r(s,a,b)+\\gamma\\mathbb{E}_{\\omega\\sim N(0,\\sigma^{-2}I_{d})}\\left[\\phi_{\\omega}(s,a,b)^{\\top}\\int_{S}\\mu_{\\omega}(s^{\\prime})V^{\\overline{{\\pi}},\\underline{{\\pi}}}(s^{\\prime})d s^{\\prime}\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using H\u00f6lder\u2019s inequality, along with the fact that $\\|V^{\\pi}\\|_{\\infty}=O((1-\\gamma)^{-1})$ , and denoting $\\beta_{\\omega}^{\\overline{{\\pi}},\\pi}:=$ $\\begin{array}{r}{\\int_{S}\\mu_{\\omega}(s^{\\prime})V^{\\pi,\\pi}(s^{\\prime})d s^{\\prime}}\\end{array}$ , we have for every $\\omega$ that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\|\\beta_{\\omega}^{\\overline{{\\pi}},\\pi}\\right\\|_{\\infty}=O((1-\\gamma)^{-1}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Additionally, recalling that $\\phi_{\\omega}(s,a,b)=[\\psi_{\\omega}(s,a,b),r(s,a,b)]$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\operatorname*{sup}_{((s,a,b),\\omega)}|\\phi_{\\omega}(s,a,b)^{\\top}\\beta_{\\omega}^{\\overline{{\\pi}},\\underline{{\\pi}}}|\\leq\\displaystyle\\operatorname*{sup}_{((s,a,b),\\omega)}|\\psi_{\\omega}(s,a,b)|_{1}\\left\\|\\beta_{\\omega}^{\\overline{{\\pi}},\\underline{{\\pi}}}\\right\\|_{\\infty}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "since the coordinate in $\\beta_{\\omega}^{\\overline{{\\pi}},\\underline{{\\pi}}}$ corresponding to the reward part of $\\phi_{\\omega}(s,a,b)$ is 0. Noting ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{((s,a,b),\\omega)}|\\psi_{\\omega}(s,a,b)|_{1}\\leq2\\,\\operatorname*{sup}_{(s,a,b)}\\frac{g_{\\alpha}(f(s,a,b))}{\\alpha^{d}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "we conclude that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{(s,a,b),\\omega}|\\phi_{\\omega}(s,a,b)^{\\top}\\beta_{\\omega}^{\\overline{{\\pi}},\\overline{{\\pi}}}|\\leq\\operatorname*{sup}_{((s,a,b),\\omega)}|\\psi_{\\omega}(s,a,b)|_{1}\\left\\|\\beta_{\\omega}^{\\overline{{\\pi}},\\underline{{\\pi}}}\\right\\|_{\\infty}=\\textit{O}\\left(\\frac{\\operatorname*{sup}_{(s,a,b)}g_{\\alpha}(f(s,a,b))}{\\alpha^{d}(1-\\gamma)}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Applying Lemma D.4, we obtain the following bound: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\|Q^{\\overline{{\\pi}},\\underline{{\\pi}}}-\\Pi_{\\nu,\\Phi_{\\mathrm{rf}}}Q^{\\overline{{\\pi}},\\underline{{\\pi}}}\\right\\|_{\\nu}=\\tilde{O}\\left(\\frac{\\gamma\\operatorname*{sup}_{s,a,b}g_{\\alpha}(f(s,a,b))}{\\alpha^{d}(1-\\gamma)\\sqrt{m}}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Substituting (32) into (31) completes the proof. ", "page_idx": 17}, {"type": "text", "text": "As shown above, the approximation error for random features decreases at a rate of $O\\left(m^{-1/2}\\right)$ with high probability as the number of random features $m$ increases. ", "page_idx": 17}, {"type": "text", "text": "Next, we turn to the approximation error for Nystr\u00f6m features. To establish this result, we first present the following key finding. ", "page_idx": 17}, {"type": "text", "text": "Lemma D.5 (cf. Lemma 8 from Ren et al. [2023]). Consider the following Mercer decomposition $(o n\\,S)$ of $k_{\\alpha}(\\cdot,\\cdot)$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\nk_{\\alpha}(x,x^{\\prime})=\\sum_{i=1}^{\\infty}\\sigma_{i}e_{i}(x)e_{i}(x^{\\prime}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\{e_{i}\\}_{i=1}^{\\infty}$ forms a countable orthonormal basis for $L_{\\mathrm{{2}}}(\\mu_{\\mathrm{{nys}}})$ with corresponding values $\\{\\sigma_{i}\\}_{i=1}^{\\infty}$ . Let $X^{n_{\\mathrm{nys}}}\\;=\\;\\{x_{i}\\}_{i=1}^{n_{\\mathrm{nys}}}$ be an i.i.d $n_{\\mathrm{{nys}}}$ -point sample from $\\mu_{\\mathrm{nys}}$ . In addition, let $\\lambda_{1}~\\geq$ $\\lambda_{2}\\ge\\cdots\\ge\\lambda_{n_{\\mathrm{nys}}}$ denote the eigenvalues of the (unnormalized) Gram matrix $K^{(n_{\\mathrm{nys}})}$ in its eigendecomposition $K^{(n_{\\mathrm{nys}})}U=\\Lambda U$ where $U^{\\top}U=U^{\\top}=I.$ . Suppose that $\\sigma_{j},\\lambda_{j}/n_{\\mathrm{nys}}\\lesssim\\exp(-\\beta j^{1/h})$ for some $\\beta>0$ and $h>0$ . Suppose that $n_{\\mathrm{nys}}\\geq3$ and that $\\lfloor(2\\log n_{\\mathrm{nys}})^{h}/\\beta^{h}\\rfloor\\leq m\\leq n_{\\mathrm{nys}}$ . ", "page_idx": 17}, {"type": "text", "text": "Consider the rank-m kernel approximation $\\hat{k}_{\\alpha,m}^{n_{\\mathrm{nys}}}$ constructed using Nystr\u00f6m features, defined as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{k}_{\\alpha,m}^{n_{\\mathrm{nys}}}(s,t)=\\varphi_{\\mathrm{nys}}(s)^{\\top}\\varphi_{\\mathrm{nys}}(t),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\varphi_{\\mathrm{nys}}(\\cdot)\\in\\mathbb{R}^{m}$ and is defined as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\big(\\varphi_{\\mathrm{nys}}\\big)_{i}(\\cdot):=\\frac{1}{\\sqrt{\\lambda_{i}}}u_{i}^{\\top}k_{\\alpha}(X^{n_{\\mathrm{nys}}},\\cdot),\\quad\\forall i\\in[m],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $u_{i}$ denotes the $i$ -th column of $U$ , and $k_{\\alpha}(X^{n_{\\mathrm{nys}}},\\cdot)$ denotes a $n_{\\mathrm{nys}}$ -dimensional vector where $\\bigl(k_{\\alpha}(X^{n_{\\mathrm{nys}}},\\cdot)\\bigr)_{\\ell}\\,=\\,k_{\\alpha}(x_{\\ell},\\cdot),$ ; for details on how $\\varphi_{\\mathrm{nys}}(\\cdot)$ is derived, see Appendix $B$ . Then, for any $\\delta>0$ , with probability at least $1-\\delta$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\int_{S}\\sqrt{\\left(k_{\\alpha}-\\hat{k}_{\\alpha,m}^{n_{\\mathrm{nys}}}\\right)(x,x)}d\\mu_{\\mathrm{nys}}(x)=\\tilde{O}\\left(\\sqrt{\\sum_{i=m+1}^{n_{\\mathrm{nys}}}\\frac{1}{n_{\\mathrm{nys}}}\\lambda_{i}}+\\frac{1}{n_{\\mathrm{nys}}}\\right)=\\tilde{O}\\left(\\frac{1}{n_{\\mathrm{nys}}}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Leveraging Lemma D.5, we can now derive the following result for the approximation error associated with Nystr\u00f6m features. ", "page_idx": 18}, {"type": "text", "text": "Proposition D.2 $\\mathrm{~\\textit~{~Q~}~}$ -Approximation error with Nystr\u00f6m features). Suppose all the assumptions in Lemma D.5 hold. Suppose also that we pick the sampling distribution $\\mu_{\\mathrm{nys}}$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mu_{\\mathrm{nys}}(x)=p_{\\alpha}(x).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We define the feature map $\\Phi_{\\mathrm{nys}}$ for the Nystr\u00f6m features as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Phi_{\\mathrm{nys}}=\\{[\\psi_{\\mathrm{nys}}(s,a,b),r(s,a,b)]\\}_{(s,a,b)\\in S\\times A\\times A},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\psi_{\\mathrm{nys}}(s,a,b)\\in\\mathbb{R}^{m}$ is defined in (38) in Algorithm 3. Then, for any $\\delta>0$ , with probability at least $1-\\delta$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|Q^{\\overline{{\\pi}},\\underline{{\\pi}}}-\\tilde{Q}_{\\mathrm{nys}}^{\\overline{{\\pi}},\\underline{{\\pi}}}\\right\\|_{\\nu}\\leq\\,\\tilde{O}\\left(\\frac{\\gamma\\tilde{g}_{\\alpha}}{(1-\\gamma)^{2}n_{\\mathrm{nys}}}\\right)\\leq\\tilde{O}\\left(\\frac{\\gamma\\tilde{g}_{\\alpha}}{(1-\\gamma)^{2}m}\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\Vert\\cdot\\Vert_{\\nu}$ ( si,sa ,tbh))e $L_{2}$ norm defined as $\\|f\\|_{\\nu}\\;=\\;\\int f^{2}d\\nu,\\;\\tilde{Q}_{\\Phi_{\\mathrm{nys}}}^{\\overline{{{\\pi}}},\\underline{{{\\pi}}}}$ is defined in (26), and $\\tilde{g}_{\\alpha}\\;:=\\;$ $\\displaystyle\\operatorname*{sup}_{s,a,b}\\,\\frac{g_{\\alpha}(f(s,a,b))}{\\alpha^{d}}$ ", "page_idx": 18}, {"type": "text", "text": "Proof. Following the analysis at the beginning of the proof of Proposition D.1, and using the contraction property along with results from $\\mathrm{Yu}$ and Bertsekas [2008], we have the following bound: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|Q^{\\overline{{\\pi}},\\underline{{\\pi}}}-\\tilde{Q}_{\\Phi_{\\mathrm{nys}}}^{\\overline{{\\pi}},\\underline{{\\pi}}}\\right\\|_{\\nu}\\leq\\frac{1}{1-\\gamma}\\left\\|Q^{\\overline{{\\pi}},\\underline{{\\pi}}}-\\Pi_{\\nu,\\Phi_{\\mathrm{nys}}}Q^{\\overline{{\\pi}},\\underline{{\\pi}}}\\right\\|_{\\nu},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\Pi_{\\nu,\\Phi_{\\mathrm{nys}}}$ is defined as in (28). ", "page_idx": 18}, {"type": "text", "text": "Next, we need to bound the term $\\left\\|Q^{\\overline{{{\\pi}}},\\underline{{{\\pi}}}}-\\Pi_{\\nu,\\Phi_{\\mathrm{nys}}}Q^{\\overline{{{\\pi}}},\\underline{{{\\pi}}}}\\right\\|_{\\nu}$ . Starting from (34), and recalling the definition of $\\varphi_{\\mathrm{nys}}(\\cdot)$ from (35), we e xpress our Nystr\u00f6m ap proximation of the kernel $\\begin{array}{r}{k_{\\alpha}\\left(s^{\\prime},\\frac{f(s,a,b)}{1-\\alpha^{2}}\\right)}\\end{array}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{k}_{m}^{n_{\\mathrm{nys}}}\\left(s^{\\prime},\\frac{f(s,a,b)}{1-\\alpha^{2}}\\right)=\\varphi_{\\mathrm{nys}}(s^{\\prime})^{\\top}\\varphi_{\\mathrm{nys}}\\left(\\frac{f(s,a,b)}{(1-\\alpha^{2})}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since we have $\\begin{array}{r}{\\psi_{\\mathrm{nys}}(s,a,b)=g_{\\alpha}(f(s,a,b))\\varphi_{\\mathrm{nys}}\\left(\\frac{f(s,a,b)}{1-\\alpha^{2}}\\right)}\\end{array}$ , where $\\psi_{\\mathrm{nys}}(\\cdot)$ is defined as in (8), this suggests the following Nystr\u00f6m-based approximation for the $Q$ -function: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{Q}_{\\mathrm{nys}}^{\\overline{{\\pi}},\\overline{{\\pi}}}(s,a,b)\\!:=\\!r(s,a,b)+\\gamma\\psi_{\\mathrm{nys}}(s,a,b)^{\\top}\\left(\\frac{\\int_{S}\\varphi_{\\mathrm{nys}}(s^{\\prime})V^{\\overline{{\\pi}},\\underline{{\\pi}}}(s^{\\prime})p_{\\alpha}(s^{\\prime})d s^{\\prime}}{\\alpha^{d}}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It is important to note that this approximation is a valid solution to the objective: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{min}_{f\\in\\mathrm{span}(\\Phi_{\\mathrm{nys}})}\\mathbb{E}_{\\nu}\\left(Q^{\\overline{{\\pi}},\\underline{{\\pi}}}(s,a,b)-f(s,a,b)\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Vert\\Pi_{v,\\Phi_{\\infty}}(Q^{\\pi};x)-Q^{\\pi}\\mathfrak{L}\\Vert_{v}\\leq\\Vert\\hat{Q}_{\\mathtt{W}_{\\infty}^{\\infty}}^{\\pi}-Q^{\\pi}\\mathfrak{L}\\Vert_{v}}\\\\ &{=\\gamma\\Bigg\\Vert\\psi_{\\mathtt{W e p s}}(s,a,b)^{\\top}\\left(\\frac{\\int_{\\delta}\\varphi_{\\mathtt{W}_{\\infty}}(s^{\\prime})V^{\\pi,\\mathtt{L}}(s^{\\prime})P_{\\infty}(s^{\\prime})d s^{\\prime}}{\\alpha d t}\\right)}\\\\ &{\\qquad-\\int_{\\delta}P(s^{\\prime}\\mid s,a,b)V^{\\pi}x(s^{\\prime})d s^{\\prime}\\Bigg\\Vert_{v},}\\\\ &{=\\gamma\\Bigg\\Vert\\left(\\frac{Q_{0}(f(s,a,b))}{\\alpha^{d}}\\right)\\!\\!\\left(\\!\\int_{s}\\varphi_{\\mathtt{W}}\\!\\!\\left(\\frac{f(s,a,b)}{1-\\alpha^{2}}\\right)^{\\!\\top}\\!\\!\\varphi_{\\mathtt{W}_{\\infty}^{\\mathtt{N}}}(s^{\\prime})V^{\\pi,\\mathtt{L}}(s^{\\prime})d p_{\\alpha}(s^{\\prime})\\!\\right.}\\\\ &{\\qquad-\\int_{\\delta}k_{\\alpha}(s^{\\prime},\\frac{f(s,a,b)}{1-\\alpha^{2}})V^{\\pi,\\mathtt{L}}(s^{\\prime})d p_{\\alpha}(s^{\\prime})\\Bigg)\\Bigg\\Vert_{v},}\\\\ &{\\leq\\frac{\\gamma\\beta\\tilde{\\omega}_{\\mathtt{N}}}{1-\\gamma}\\times\\left.\\ \\left\\Vert\\int_{\\delta}\\!\\!\\left[\\hat{k}_{m^{*}}^{\\mu_{\\mathtt{N}},\\mathtt{p}}\\!\\left(s^{\\prime},\\frac{f(s,a,b)}{1-\\alpha^{2}}\\right)\\!-\\!k_{\\alpha}\\left(s^{\\prime},\\frac{f(s,a,b)}{1-\\alpha^{2}}\\right)\\right]p_{\\alpha}(s^{\\prime})d s^{\\prime}\\right\\Vert_{s},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using the one-to-one correspondence between Reproducing Kernel Hilbert Spaces (RKHS) and PD kernels, we can express the kernel function $k_{\\alpha}$ as an inner product in the corresponding RKHS, $\\mathcal{H}_{k_{\\alpha}}$ . Specifically, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle k_{\\alpha}\\left(s^{\\prime},\\frac{f(s,a,b)}{1-\\alpha^{2}}\\right)=\\langle k_{\\alpha}\\left(\\frac{f(s,a,b)}{1-\\alpha^{2}},\\cdot\\right),k_{\\alpha}(s^{\\prime},\\cdot)\\rangle_{\\mathscr{H}_{k\\alpha}}}}\\\\ {{\\displaystyle\\hat{k}_{m}^{n_{\\mathrm{nys}}}\\left(s^{\\prime},\\frac{f(s,a,b)}{1-\\alpha^{2}}\\right)=k_{\\alpha}(\\frac{f(s,a,b)}{1-\\alpha^{2}},X^{n})\\sum_{i=1}^{m}\\frac{1}{\\lambda_{i}}u_{i}u_{i}^{\\top}k_{\\alpha}(X^{n},s^{\\prime})}}\\\\ {{\\displaystyle=\\left<k_{\\alpha}(\\frac{f(s,a,b)}{1-\\alpha^{2}},\\cdot),k_{\\alpha}(X^{n},\\cdot)\\right>_{\\mathscr{H}_{k\\alpha}=1}\\frac{m}{\\lambda_{i}}u_{i}u_{i}^{\\top}k_{\\alpha}(X^{n},s^{\\prime})}}\\\\ {{\\displaystyle=\\langle k_{\\alpha}\\left(\\frac{f(s,a,b)}{1-\\alpha^{2}},\\cdot\\right),\\hat{k}_{m}^{n_{\\mathrm{nys}}}(s^{\\prime},)\\rangle_{\\mathscr{H}_{k\\alpha}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By choosing $\\mu_{\\mathrm{nys}}=p_{\\alpha}(x)$ , continuing from (41), we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad T_{1}}\\\\ &{=\\left\\lVert\\int_{\\mathcal{S}}\\left|\\hat{l}_{\\omega^{\\alpha}}[\\omega^{\\beta},\\frac{t}{1-\\alpha^{2}})\\right.\\right\\rangle-k_{\\alpha}\\left(s^{\\prime},\\frac{f}{1-\\alpha^{2}}\\right)\\right\\rangle\\Big|p_{\\alpha}(s^{\\prime})d s^{\\prime}\\right\\rVert_{\\omega}}\\\\ &{=\\left\\lVert\\int_{\\mathcal{S}}\\left|k_{\\alpha}\\left(\\frac{f(s,\\theta,\\theta)}{1-\\alpha^{2}},\\right),\\frac{k_{\\alpha}m_{\\alpha}(s^{\\prime})}{k_{\\alpha}(s^{\\prime})}\\right\\rangle h_{\\alpha_{\\alpha}}}\\\\ &{\\quad\\quad-\\left.\\left\\langle k_{\\alpha}\\left(\\frac{f(s,\\theta,\\theta)}{1-\\alpha^{2}},\\right),k_{\\alpha}(s^{\\prime},)\\right\\rangle h_{\\alpha_{\\alpha}}\\left|p_{\\alpha}(s^{\\prime})d s^{\\prime}\\right\\rVert_{\\omega}}\\\\ &{=\\left\\lVert\\int_{\\mathcal{S}}\\left|\\left\\langle k_{\\alpha}\\left(\\frac{f(s,\\theta,\\theta)}{1-\\alpha^{2}},\\right),\\frac{k_{\\alpha}m_{\\alpha}(s^{\\prime},)-k_{\\alpha}(s^{\\prime},)}{k_{\\alpha}(s^{\\prime})}\\right\\rangle_{\\mathbb{H}_{\\alpha_{\\alpha}}}\\right\\rangle d p_{\\alpha}(s^{\\prime})\\right\\rVert_{\\omega}}\\\\ &{\\le\\left\\lVert\\int_{\\mathcal{S}}\\sqrt{k_{\\alpha}\\frac{\\left(f(s,\\theta,\\theta)\\right)}{1-\\alpha^{2}}\\frac{f(s,\\theta,\\theta)}{1-\\alpha^{2}}}\\sqrt{(k_{\\alpha}-\\frac{k_{\\alpha}m_{\\alpha}}{k_{\\alpha}^{\\prime}})(s^{\\prime},s^{\\prime})}d p_{\\alpha}(s^{\\prime})\\right\\rVert_{\\omega}}\\\\ &{\\le\\left\\lVert\\int_{\\mathcal{S}}\\sqrt{(k_{\\alpha}-\\frac{k_{\\alpha^{\\ast}}m_{\\alpha}}{1-\\alpha^{2}})(s^{\\prime},s^{\\prime})}d p_{\\alpha}(s^{\\prime})\\right\\rVert_{\\omega}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "To move from the second-to-last line to the last line, we used the fact that $k_{\\alpha}(\\cdot,\\cdot)~\\leq~1$ . Next, applying Lemma D.5 and using the decay assumption on the eigenvalues from both the Mercer ", "page_idx": 19}, {"type": "text", "text": "expansion and the empirical Gram matrix, we obtain a probabilistic bound. Specifically, for any $\\delta>0$ , with probability at least $1-\\delta$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\nT_{1}\\leq\\,\\left(\\int_{S}\\sqrt{(k_{\\alpha}-\\hat{k}_{m}^{n_{\\mathrm{nys}}})(s^{\\prime},s^{\\prime})}\\mu(s^{\\prime})d s^{\\prime}\\right)=\\tilde{O}\\left(\\frac{1}{n_{\\mathrm{nys}}}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, by combining (41) with the bound on $T_{1}$ , we conclude the desired result, demonstrating that the error introduced by the Nystr\u00f6m approximation decays at the rate $\\tilde{O}(1/n_{n y s})$ . \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Remark D.1. As demonstrated by the proposition above, a key advantage of the Nystr\u00f6m method is its ability to reduce the approximation error to $O(\\left(n_{\\mathrm{nys}}^{-1}\\right))$ , where $n_{\\mathrm{{nys}}}$ represents the number of samples used to construct the Gram matrix for generating Nystr\u00f6m features. This rate of $O(\\left(n_{\\mathrm{nys}}^{-1}\\right))$ consistently outperforms the ${\\cal O}(1/m)$ where $m$ is the number of features, since SDEPO is designed to select $m\\leq n_{\\mathrm{nys}}$ . The only requirement for this improvement is that the eigenvalues of the empirical Gram matrix and Mercer expansion meet certain decay assumptions, which is a standard assumption to make in the kernel literature (cf. Theorem 2 in Belkin [2018]). ", "page_idx": 20}, {"type": "text", "text": "D.2 Statistical Error ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We now present the bound on the statistical error arising from using a finite number of samples, $n$ .   \nThe result holds for both Nystr\u00f6m and random features. ", "page_idx": 20}, {"type": "text", "text": "Proposition D.3. For each policy pair $\\{\\overline{{\\pi}},\\underline{{\\pi}}\\}$ encountered in the algorithm, let $\\hat{Q}_{\\Phi,L}^{\\overline{{\\pi}},\\pi}$ denote the policy given by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{Q}_{\\Phi,L}^{\\overline{{\\pi}},\\underline{{\\pi}}}(s,a,b)=\\phi(s,a,b)^{\\top}\\hat{w}_{L},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\hat{w}_{L}$ is defined as in (11), and $\\Phi$ can either be the reward concatenated with the Nystr\u00f6m or random features, i.e. either $\\Phi_{\\mathrm{nys}}$ or $\\Phi_{\\mathrm{rf}}$ . Then, for sufficiently large $n_{i}$ , there exists an universal constant $C>0$ independent of $m,\\,n,\\,L$ and $(1-\\gamma)^{-1}$ , such that with probability at least $1-\\delta$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|\\tilde{Q}_{\\Phi}^{\\overline{{\\pi}},\\underline{{\\pi}}}-\\hat{Q}_{\\Phi,L}^{\\overline{{\\pi}},\\underline{{\\pi}}}\\right\\|_{\\nu}\\leq\\gamma^{L}\\left\\|\\tilde{Q}_{\\Phi}^{\\overline{{\\pi}},\\underline{{\\pi}}}\\right\\|_{\\nu}+\\frac{C\\tilde{g}_{\\alpha}^{6}m^{3}\\mathrm{polylog}(m,L/\\delta)}{(1-\\gamma)\\Upsilon_{1}^{2}\\Upsilon_{2}\\sqrt{n}},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we recall $\\tilde{Q}_{\\Phi}^{\\overline{{\\pi}},\\underline{{\\pi}}}$ is defined as in (26). ", "page_idx": 20}, {"type": "text", "text": "Proof. To simplify the presentation, let us define $\\Phi$ as the concatenation of $\\phi(s,a,b)$ across all $(s,a,b)\\in S\\times A\\times A$ , and define the operator $P^{\\overline{{\\pi}},\\underline{{\\pi}}}$ as ", "page_idx": 20}, {"type": "equation", "text": "$$\n(P^{\\pi,\\underline{{\\pi}}}f)(s,a,b)=\\mathbb{E}_{(s^{\\prime},a^{\\prime})\\sim P(s,a,b)\\,\\times\\,\\pi\\times\\underline{{\\pi}}}f(s^{\\prime},a^{\\prime},b^{\\prime}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Additionally, define $\\tilde{w}$ as the solution to the equation: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{w}=\\left(\\mathbb{E}_{\\nu}\\left[\\phi(s,a,b)\\phi(s,a,b)^{\\top}\\right]\\right)^{-1}}\\\\ &{\\qquad\\left(\\mathbb{E}_{\\nu}\\left[\\phi(s,a,b)\\left(r(s,a,b)+\\gamma\\mathbb{E}_{(s^{\\prime},a^{\\prime},b^{\\prime})\\sim P(s,a,b)\\times\\overline{{\\pi}}\\times\\underline{{\\pi}}}\\left[\\phi(s^{\\prime},a^{\\prime},b^{\\prime})^{\\top}\\tilde{w}\\right]\\right)\\right]\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and let $\\tilde{Q}(s,a,b)\\,=\\,\\phi(s,a,b)^{\\top}\\tilde{w}$ . It is straightforward to observe that $\\tilde{w}$ is the fixed point of the population (i.e., $n\\rightarrow\\infty,$ ) projected least square update (11). Furthermore, we can express $\\tilde{w}$ as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{w}=\\Bigl(\\mathbb{E}_{\\nu}\\left[\\phi(s,a,b)\\left(\\phi(s,a,b)-\\gamma\\mathbb{E}_{(s^{\\prime},a^{\\prime},b^{\\prime})\\sim P(s,a,b)\\times\\overline{{\\pi}}\\times\\underline{{\\pi}}}\\phi(s^{\\prime},a^{\\prime},b^{\\prime})\\right)^{\\top}\\right]\\Bigr)^{-1}}\\\\ &{\\qquad\\mathbb{E}_{\\nu}\\left[\\phi(s,a,b)r(s,a,b)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For the sake of brevity, we will omit the subscript when $\\nu$ refers to the Lebesgue measure. We define $\\hat{\\Pi}_{\\nu}$ and $\\hat{P}^{\\overline{{\\pi}},\\underline{{\\pi}}}$ as the empirical counterparts of $\\Pi_{\\nu}$ and $P^{\\overline{{\\pi}},\\underline{{\\pi}}}$ , respectively. Under the update rule (11), we have the following relations: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Phi\\hat{w}_{t+1}=\\hat{\\Pi}_{\\nu}(r+\\gamma\\hat{P}^{\\pi,\\pi}\\Phi\\hat{w}_{t}),}\\\\ &{\\quad\\Phi\\tilde{w}=\\Pi_{\\nu}(r+\\gamma P^{\\pi,\\pi}\\Phi\\tilde{w}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which lead to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Phi(\\tilde{w}-\\hat{w}_{t+1})=(\\Pi_{\\nu}-\\hat{\\Pi}_{\\nu})r+\\gamma(\\Pi_{\\nu}P^{\\overline{{\\pi}},\\underline{{\\pi}}})\\Phi(\\tilde{w}-\\hat{w}_{t})+\\gamma(\\Pi_{\\nu}P^{\\overline{{\\pi}},\\underline{{\\pi}}}-\\hat{\\Pi}_{\\nu}\\hat{P}^{\\overline{{\\pi}},\\underline{{\\pi}}})\\Phi\\hat{w}_{t}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Applying the triangle inequality, we can bound this as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\Phi(\\tilde{w}-\\hat{w}_{t+1})\\right\\|_{\\nu}\\leq\\gamma\\left\\|\\Phi(\\tilde{w}-\\hat{w}_{t})\\right\\|_{\\nu}+\\left\\|\\left(\\Pi_{\\nu}-\\hat{\\Pi}_{\\nu}\\right)r\\right\\|_{\\nu}+\\gamma\\left\\|\\left(\\Pi_{\\nu}P^{\\overline{{\\pi}},\\underline{{\\pi}}}-\\hat{\\Pi}_{\\nu}\\hat{P}^{\\overline{{\\pi}},\\underline{{\\pi}}}\\right)\\Phi\\hat{w}_{t}\\right\\|_{\\nu},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we use the contractivity under $\\Vert\\cdot\\Vert_{\\nu}$ . Telescoping over $t$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\Phi(\\tilde{w}-\\hat{w}_{L})\\right\\|_{\\nu}\\leq\\gamma^{T}\\left\\|\\Phi(\\tilde{w}-\\hat{w}_{0})\\right\\|_{\\nu}+\\!\\frac{1}{1-\\gamma}\\left\\|\\left(\\Pi_{\\nu}-\\hat{\\Pi}_{\\nu}\\right)r\\right\\|_{\\nu}}\\\\ &{\\quad\\qquad\\qquad+\\left.\\frac{\\gamma}{1-\\gamma}\\operatorname*{max}_{l\\in[L]}\\right\\|\\left(\\Pi_{\\nu}P^{\\overline{{\\pi}},\\underline{{\\pi}}}-\\hat{\\Pi}_{\\nu}\\hat{P}^{\\overline{{\\pi}},\\underline{{\\pi}}}\\right)\\Phi\\hat{w}_{t}\\right\\|_{\\nu}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The following proof follows the same reasoning as Appendix $\\boldsymbol{\\mathrm E}$ in Ren et al. [2023]. ", "page_idx": 21}, {"type": "text", "text": "Proposition D.3 shows that the statistical error of the linear components can be broken down into two parts. The first part, $\\gamma^{L}\\left\\Vert\\tilde{Q}^{\\overline{{\\pi}},\\underline{{\\pi}}}\\right\\Vert_{\\nu}$ , arises from initializing with $\\hat{w}_{0}=0$ and decreases as the number of least-squares policy evaluation iterations $L$ increases. The second part, $\\frac{C\\tilde{g}_{\\alpha}^{6}m^{3}\\mathrm{polylog}(m,L/\\delta)}{(1\\!-\\!\\gamma)\\Upsilon_{1}^{2}\\Upsilon_{2}\\sqrt{n}}$ , represents the statistical error from using a finite sample size and diminishes as $n$ increases. By choosing $L=\\Theta(\\log n)$ , we can balance both parts, resulting in an estimation error that decreases at a rate of $O(n^{-1/2})$ with high probability. ", "page_idx": 21}, {"type": "text", "text": "D.3 Total Error for Policy Evaluation ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Combine the approximation error in Proposition D.1 and Proposition D.2 with the statistical error from Proposition D.3, we obtain the following bound on the total error for least-squares policy evaluation: ", "page_idx": 21}, {"type": "text", "text": "Theorem D.1 (i.e. Theorem D.1). Let $L=\\Theta(\\log n)$ . With probability at least $1-\\delta$ , we have that for the random features $\\Phi_{\\mathrm{rf}}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|Q^{\\overline{{\\pi}},\\underline{{\\pi}}}-\\hat{Q}_{\\Phi_{\\mathrm{rf}},L}^{\\overline{{\\pi}},\\underline{{\\pi}}}\\right\\|_{\\nu}=\\tilde{O}\\left(\\underbrace{\\frac{\\tilde{g}_{\\alpha}}{(1-\\gamma)^{2}\\sqrt{m}}}_{\\mathrm{approx.~error}}+\\underbrace{\\frac{\\tilde{g}_{\\alpha}^{6}m^{3}}{(1-\\gamma)\\Upsilon_{1}^{2}\\Upsilon_{2}\\sqrt{n}}}_{\\mathrm{stat.~error}}\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and for the Nystr\u00f6m features ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|Q^{\\overline{{\\pi}},\\underline{{\\pi}}}-\\hat{Q}_{\\Phi_{\\mathrm{nys}},L}^{\\overline{{\\pi}},\\underline{{\\pi}}}\\right\\|_{\\nu}=\\tilde{O}\\left(\\underbrace{\\frac{\\tilde{g}_{\\alpha}}{(1-\\gamma)^{2}n_{\\mathrm{nys}}}}_{\\mathrm{approx.~error}}+\\underbrace{\\frac{\\tilde{g}_{\\alpha}^{6}m^{3}}{(1-\\gamma)\\Upsilon_{1}^{2}\\Upsilon_{2}\\sqrt{n}}}_{\\mathrm{stat.~error}}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. This result follows directly from the triangle inequality, i.e., ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|Q^{\\overline{{\\pi}},\\underline{{\\pi}}}-\\hat{Q}_{\\Phi,L}^{\\overline{{\\pi}},\\underline{{\\pi}}}\\right\\|_{\\nu}\\leq\\left\\|Q^{\\overline{{\\pi}},\\underline{{\\pi}}}-\\tilde{Q}_{\\Phi}^{\\overline{{\\pi}},\\underline{{\\pi}}}\\right\\|_{\\nu}+\\left\\|\\tilde{Q}_{\\Phi}^{\\overline{{\\pi}},\\underline{{\\pi}}}-\\hat{Q}_{\\Phi,L}^{\\overline{{\\pi}},\\underline{{\\pi}}}\\right\\|_{\\nu}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The first term, representing the approximation error, is bounded by Proposition D.1 and Proposition D.2 for random and Nystr\u00f6m features, respectively. The second term, capturing the statistical error, is bounded by Proposition D.3. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Theorem D.1 provides the estimation error bound for the $Q$ -function using least-squares policy evaluation, applicable to both random and Nystr\u00f6m features. The result reveals a key tradeoff between the approximation error and statistical error. For random features, increasing the number of features $m$ enhances the ability to approximate the original infinite-dimensional function space, as reflected by the $\\begin{array}{r}{\\tilde{O}\\left(\\frac{1}{\\sqrt{m}}\\right)}\\end{array}$ approximation error term on the RHS of (43). However, this improvement comes at the cost of needing more learning samples $n$ during policy evaluation to effectively train the weights, as indicated by the $\\begin{array}{r}{\\tilde{O}\\left(\\frac{m^{3}}{\\sqrt{n}}\\right)}\\end{array}$ statistical error term in (43). For Nystr\u00f6m features, this tradeoff may be better especially since the approximation term scales as $\\tilde{O}(1/n_{\\mathrm{nys}})\\leq\\tilde{O}(1/m)$ while thestatistical error term remains the same as with random features. As a result, the Nystr\u00f6m method offers a better approximation error rate with additional mild assumption. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "E Convergence analysis of SDEPO ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we denote $\\begin{array}{r l r}{Q_{k-1}}&{{}=}&{Q^{\\overline{{\\pi}}_{k-1},\\underline{{\\pi}}_{k-1}}}\\end{array}$ , and use the notation $\\begin{array}{r l}{\\overline{{\\pi}}^{s}Q^{s}\\underline{{\\pi}}^{s}}&{{}=}\\end{array}$ $\\mathbb{E}_{a\\sim\\overline{{\\pi}}(\\cdot|s),b\\sim\\underline{{\\pi}}(\\cdot|s)}[Q(s,a,b)]$ . ", "page_idx": 22}, {"type": "text", "text": "To establish the convergence of the policy optimization procedure, we follow the error propagation framework from Perolat et al. [2015], which consists of two key stages: ", "page_idx": 22}, {"type": "text", "text": "Stage 1: Identify an approximate solution $\\underline{{\\pi}}_{k}$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\operatorname*{max}_{\\overline{{\\pi}}}\\overline{{\\pi}}^{s}Q_{k-1}^{s}\\underline{{\\pi}}_{k}^{s}-\\operatorname*{min}_{\\underline{{\\pi}}}\\operatorname*{max}_{\\overline{{\\pi}}}\\overline{{\\pi}}^{s}Q_{k-1}^{s}\\underline{{\\pi}}^{s}=\\epsilon_{1}^{k}(s),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the expectation is taken over the randomness of the algorithm used to generate $\\underline{{\\pi}}_{k}$ . ", "page_idx": 22}, {"type": "text", "text": "In our analysis, we bound the stronger quantity, which is called duality gap ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{max}_{\\overline{{\\pi}}}\\pi^{s}Q_{k-1}^{s}\\pi_{k}^{s}-\\operatorname*{min}_{\\underline{{\\pi}}}\\pi_{k}^{s}Q_{k-1}^{s}\\pi^{s}\\right]\\geq\\epsilon_{1}^{k}(s),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the bound follows from the definition of Nash equilibrium, as $\\begin{array}{r l}{\\operatorname*{min}_{\\underline{{\\pi}}}\\overline{{\\pi}}_{k}Q_{k-1\\underline{{\\pi}}}}&{\\le}\\end{array}$ $\\operatorname*{min}_{\\underline{{\\pi}}}\\operatorname{max}_{\\overline{{\\pi}}}\\overline{{\\pi}}Q_{k-1\\underline{{\\pi}}}$ . ", "page_idx": 22}, {"type": "text", "text": "Stage 2: Identify an approximate solution $\\overline{{\\pi}}_{k}$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\nV^{\\overline{{\\pi}}_{k},\\underline{{\\pi}}_{k}^{*}}(s)-\\mathbb{E}V^{\\overline{{\\pi}}_{k},\\underline{{\\pi}}_{k}}(s)=\\epsilon_{2}^{k}(s),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the expectation is over the randomness of the algorithm used to generate $\\overline{{\\pi}}_{k}$ . ", "page_idx": 22}, {"type": "text", "text": "Following the analysis in Perolat et al. [2015], we conclude that there exists a constant $C$ , such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}_{s\\sim\\mu_{0}}\\left[\\operatorname*{max}_{\\bar{\\pi}}V^{\\overline{{\\pi}},\\underline{{\\pi}}_{k}}(s)-V^{*}(s)\\right]\\le C\\left(\\frac{k}{1-\\gamma}\\operatorname*{sup}_{j\\in1,\\cdots,k-1}\\mathbb{E}_{s}[\\epsilon_{2}^{j}(s)+\\epsilon_{2}^{j}(s)]+\\frac{\\gamma^{k}}{1-\\gamma}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "First, we analyze the error in the Stage 1 of Algorithm 3. ", "page_idx": 22}, {"type": "text", "text": "Lemma E.6 (i.e. Lemma 3). Let Assumption 5 hold. In Stage $^{\\,l}$ of Algorithm 3, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\underset{\\overline{\\pi}}{\\operatorname*{max}}\\overline{\\pi}Q_{k-1\\overline{\\pi}_{k}}-\\underset{\\underline{\\pi}}{\\operatorname*{min}}\\overline{\\pi}_{k}Q_{k-1\\underline{{\\pi}}}\\right]}\\\\ &{\\leq\\!\\frac{1}{\\eta T}\\log\\frac{1}{\\underline{c}}+2\\|\\hat{Q}^{\\overline{\\pi}_{k-1,\\overline{{\\pi}}_{k-1}}}-Q^{\\overline{\\pi}_{k-1,\\underline{{\\pi}}_{k-1}}}\\|_{\\nu_{\\overline{\\pi}_{k-1,\\overline{{\\pi}}_{k-1}}}}+\\frac{\\eta}{2(1-\\gamma)^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Denote $Q\\,=\\,Q_{k-1}$ . Recall the notation $\\overline{{\\pi}}^{s}Q_{\\cdot}^{s}\\pi_{\\cdot}^{s}=\\mathbb{E}_{a\\sim\\overline{{\\pi}}(\\cdot|s),b\\sim\\underline{{\\pi}}(\\cdot|s)}[Q(s,a,b)]$ . First by definition of $\\underline{{\\pi}}_{k}$ and the standard formulation for the duality gap, we have for all $s$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\overline{{\\pi}}^{s}Q^{s}\\underline{{\\pi}}_{k}-\\overline{{\\pi}}_{k}^{s}Q^{s}\\underline{{\\pi}}^{s}=\\displaystyle\\frac{1}{T}\\sum_{t=1}^{T}\\langle\\mathbb{E}_{b\\sim\\underline{{\\pi}}_{k,t}(\\cdot\\vert s)}Q(s,a,\\cdot),\\overline{{\\pi}}(\\cdot\\vert s)\\rangle-\\displaystyle\\frac{1}{T}\\sum_{t=1}^{T}\\langle\\mathbb{E}_{a\\sim\\overline{{\\pi}}_{k,t}(\\cdot\\vert s)}Q(s,\\cdot,b),\\underline{{\\pi}}(\\cdot\\vert s)\\rangle}\\\\ {\\displaystyle=\\frac{1}{T}\\sum_{t=1}^{T}\\langle\\mathbb{E}_{b\\sim\\underline{{\\pi}}_{k,t}(\\cdot\\vert s)}Q(s,a,\\cdot),\\overline{{\\pi}}(\\cdot\\vert s)-\\overline{{\\pi}}_{k,t}(\\cdot\\vert s)\\rangle}\\\\ {\\displaystyle-\\langle\\mathbb{E}_{a\\sim\\overline{{\\pi}}_{k,t}(\\cdot\\vert s)}Q(s,\\cdot,b),\\underline{{\\pi}}(\\cdot\\vert s)-\\underline{{\\pi}}_{k,t}(\\cdot\\vert s)\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "From the update rule of $\\underline{{\\pi}}_{k,t+1}$ , it holds for all $k,\\,s,\\,\\pi$ that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\langle\\nabla K L(\\underline{{\\pi}}_{k,t+1},\\underline{{\\pi}}_{k,t})+\\eta\\mathbb{E}_{a\\sim\\overline{{\\pi}}_{k,t}}[\\hat{Q}^{\\overline{{\\pi}}_{k-1},\\underline{{\\pi}}_{k-1}}(s,a,\\cdot)],\\underline{{\\pi}}(\\cdot|s)-\\underline{{\\pi}}_{k,t+1}(\\cdot|s)\\rangle\\geq0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Applying the three-point identity yields: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K L(\\underline{{\\pi}}(\\cdot|s),\\underline{{\\pi}}_{k,t+1}(\\cdot\\,|s))\\leq K L(\\underline{{\\pi}}(\\cdot|s),\\underline{{\\pi}}_{k,t}(\\cdot|s))-K L(\\underline{{\\pi}}_{k,t+1}(\\cdot|s),\\underline{{\\pi}}_{k,t}(\\cdot|s))}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\eta\\langle\\mathbb{E}_{a\\sim\\overline{{\\pi}}_{k-1}}[\\hat{Q}^{\\overline{{\\pi}}_{k-1,\\underline{{\\pi}}_{k-1}}}(s,a,\\cdot)-Q^{\\pi_{k-1},\\underline{{\\pi}}_{k-1}}(s,a,\\cdot)],\\underline{{\\pi}}(\\cdot|s)-\\underline{{\\pi}}_{k,t+1}(\\cdot|s)\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\quad+\\eta\\langle\\mathbb{E}_{a\\sim\\overline{{\\pi}}_{k-1}}[Q_{k-1}(s,a,\\cdot)],\\underline{{\\pi}}(\\cdot|s)-\\underline{{\\pi}}_{k,t+1}(\\cdot|s)\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We can bound the inner products using Cauchy-Schwarz, Young\u2019s and Pinsker\u2019s inequalities, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\eta\\langle\\mathbb{E}_{a\\sim\\overline{{\\pi}}_{k-1}}[\\hat{Q}^{\\overline{{\\pi}}_{k-1},\\underline{{\\pi}}_{k-1}}(s,a,\\cdot)-Q^{\\overline{{\\pi}}_{k-1},\\underline{{\\pi}}_{k-1}}(s,a,\\cdot)],\\underline{{\\pi}}(\\cdot|s)-\\underline{{\\pi}}_{k,t+1}(\\cdot|s)\\rangle}\\\\ &{}&{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq2\\eta\\|\\hat{Q}^{\\overline{{\\pi}}_{k-1,\\underline{{\\pi}}_{k-1}}}-Q^{\\overline{{\\pi}}_{k-1,\\underline{{\\pi}}_{k-1}}}\\|_{\\nu_{\\overline{{\\pi}}_{k-1},\\underline{{\\pi}}_{k-1}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\cdots\\cdots\\cdots\\cdots\\cdots\\cdots\\cdots\\cdots}\\\\ &{\\quad\\eta\\langle\\mathbb{E}_{a\\sim\\overline{{\\pi}}_{k-1}}[Q^{\\overline{{\\pi}}_{k-1},\\underline{{\\pi}}_{k-1}}(s,a,\\cdot)],\\pi(\\cdot|s)-\\underline{{\\pi}}_{k,t+1}(\\cdot|s)\\rangle}\\\\ &{=\\!\\eta\\langle\\mathbb{E}_{a\\sim\\overline{{\\pi}}_{k-1}}[Q^{\\overline{{\\pi}}_{k-1},\\underline{{\\pi}}_{k-1}}(s,a,\\cdot)],\\underline{{\\pi}}(\\cdot|s)-\\underline{{\\pi}}_{k,t}(\\cdot|s)\\rangle}\\\\ &{+\\eta\\langle\\mathbb{E}_{a\\sim\\overline{{\\pi}}_{k-1}}[Q^{\\overline{{\\pi}}_{k-1},\\underline{{\\pi}}_{k-1}}(s,a,\\cdot)],\\underline{{\\pi}}_{k,t}(\\cdot|s)-\\underline{{\\pi}}_{k,t+1}(\\cdot|s)\\rangle}\\\\ &{\\leq\\!\\eta\\langle\\mathbb{E}_{a\\sim\\overline{{\\pi}}_{k-1}}[Q^{\\overline{{\\pi}}_{k-1},\\underline{{\\pi}}_{k-1}}(s,a,\\cdot)],\\pi(\\cdot|s)-\\underline{{\\pi}}_{k,t}(\\cdot|s)\\rangle+\\frac{\\eta^{2}}{2(1-\\gamma)^{2}}+K L(\\underline{{\\pi}}_{k,t+1}(\\cdot|s),\\underline{{\\pi}}_{k,t}(\\cdot|s))}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Combining these estimates gives us: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathbb{E}_{a\\sim\\overline{{\\pi}}_{k-1}}[Q^{\\overline{{\\pi}}_{k-1,\\underline{{\\pi}}_{k-1}}}(s,a,\\cdot)],\\underline{{\\pi}}_{k,t}(\\cdot|s)-\\underline{{\\pi}}(\\cdot|s)\\rangle+\\frac{1}{\\eta}K L(\\underline{{\\pi}}(\\cdot|s),\\underline{{\\pi}}_{k,t+1}(\\cdot|s))}\\\\ &{\\leq\\!\\!\\frac{1}{\\eta}K L(\\underline{{\\pi}}(\\cdot|s),\\underline{{\\pi}}_{k,t}(\\cdot|s))+2\\|\\hat{Q}^{\\overline{{\\pi}}_{k-1,\\underline{{\\pi}}_{k-1}}}-Q^{\\overline{{\\pi}}_{k-1,\\underline{{\\pi}}_{k-1}}}\\|_{\\nu_{\\pi_{k-1},\\underline{{\\pi}}_{k-1}}}+\\frac{\\eta^{2}}{2(1-\\gamma)^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Summing these inequalities results in: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{T}\\sum_{t=1}^{T}\\langle\\mathbb{E}_{a\\sim\\overline{{\\pi}}_{k,t}}[Q^{\\overline{{\\pi}}_{k,t},\\underline{{\\pi}}_{k,t}}(s,a,\\cdot)],\\underline{{\\pi}}_{k,t}(\\cdot|s)-\\underline{{\\pi}}(\\cdot|s)\\rangle}\\\\ &{\\leq\\!\\frac{1}{\\eta T}K L(\\underline{{\\pi}},\\underline{{\\pi}}_{k,0})+2\\|\\hat{Q}^{\\overline{{\\pi}}_{k-1,\\underline{{\\pi}}_{k-1}}}-Q^{\\overline{{\\pi}}_{k-1,\\underline{{\\pi}}_{k-1}}}\\|_{\\nu_{\\pi_{k-1,\\pi_{k-1}}}+\\frac{\\eta^{2}}{2(1-\\gamma)^{2}}}}\\\\ &{\\leq\\!\\frac{1}{\\eta T}\\log\\frac{1}{\\underline{{c}}}+2\\|\\hat{Q}^{\\overline{{\\pi}}_{k-1,\\underline{{\\pi}}_{k-1}}}-Q^{\\overline{{\\pi}}_{k-1,\\underline{{\\pi}}_{k-1}}}\\|_{\\nu_{\\pi_{k-1,\\pi_{k-1}}}+\\frac{\\eta}{2(1-\\gamma)^{2}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This result shows how the error in the approximation of the $Q$ -function and the learning rate $\\eta$ impact the optimization performance in Stage 1. Next, we turn to Stage 2 and provide an upper bound on the one-sided error. ", "page_idx": 23}, {"type": "text", "text": "Lemma E.7 (i.e. Lemma 4). Let Assumption 5 hold and $\\mu_{0}$ be a state distribution. In Stage 2 of Algorithm 3, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{s\\sim\\mu_{0}}\\left[V^{\\pi_{k}^{*},\\pi_{k}}(s)-V^{\\pi_{k,t}^{\\prime},\\pi_{k}}(s)\\right]}\\\\ &{\\leq\\!\\frac{1}{1-\\gamma}\\left(\\frac{\\log\\frac{1}{c}}{\\eta T}+\\frac{\\eta}{(1-\\gamma)^{2}}+\\eta\\|\\hat{Q}^{\\pi_{k,t}^{\\prime},\\pi_{k}}-Q^{\\pi_{k,t}^{\\prime},\\pi_{k}}\\|_{\\nu_{\\pi_{k,t}^{\\prime},\\pi_{k}}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. By the update rule for $\\overline{{\\pi}}_{k,t+1}^{\\prime}$ , for all $s$ and $\\overline{{\\pi}}$ , we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K L(\\overline{{\\pi}}(\\cdot|s),\\overline{{\\pi}}_{k,t+1}^{\\prime}(\\cdot|s))\\leq K L(\\overline{{\\pi}}(\\cdot|s),\\overline{{\\pi}}_{k,t}^{\\prime}(\\cdot|s))}\\\\ &{\\,-\\,\\eta\\langle\\mathbb{E}_{b\\sim\\underline{{\\pi}}_{k}}[\\widehat{Q}^{\\pi_{k,t}^{\\prime},\\pi_{k}}(s,\\cdot,b)],\\overline{{\\pi}}(\\cdot|s)-\\overline{{\\pi}}_{k,t+1}^{\\prime}(\\cdot|s)\\rangle-K L(\\overline{{\\pi}}_{k,t+1}^{\\prime}(\\cdot|s),\\overline{{\\pi}}_{k,t}^{\\prime}(\\cdot|s))}\\\\ &{\\,=K L(\\overline{{\\pi}}(\\cdot|s),\\overline{{\\pi}}_{k,t}^{\\prime}(\\cdot|s))-\\eta\\langle\\mathbb{E}_{b\\sim\\underline{{\\pi}}_{k}}[\\widehat{Q}^{\\pi_{k,t}^{\\prime},\\pi_{k}}(s,\\cdot,b)-Q^{\\overline{{\\pi}}_{k,t}^{\\prime},\\pi_{k}}(s,\\cdot,b)],\\overline{{\\pi}}(\\cdot|s)-\\overline{{\\pi}}_{k,t+1}^{\\prime}(\\cdot|s)\\rangle}\\\\ &{\\,-\\,\\eta\\langle\\mathbb{E}_{b\\sim\\underline{{\\pi}}_{k}}[Q^{\\overline{{\\pi}}_{k,t}^{\\prime},\\pi_{k}}(s,\\cdot,b)],\\overline{{\\pi}}(\\cdot|s)-\\overline{{\\pi}}_{k,t+1}^{\\prime}(\\cdot|s)\\rangle-K L(\\overline{{\\pi}}_{k,t+1}^{\\prime}(\\cdot|s),\\overline{{\\pi}}_{k,t}^{\\prime}(\\cdot|s))}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We can bound the inner products using Cauchy-Schwarz, Young\u2019s and Pinsker\u2019s inequalities ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta\\langle\\mathbb{E}_{b\\sim\\underline{{\\pi}}_{k}}[\\hat{Q}^{\\pi_{k,t}^{\\prime},\\pi_{k}}(s,\\cdot,b)-Q^{\\overline{{\\pi}}_{k,t}^{\\prime},\\pi_{k}}(s,\\cdot,b)],\\overline{{\\pi}}(\\cdot|s)-\\overline{{\\pi}}_{k,t+1}^{\\prime}(\\cdot|s)\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq O(\\eta\\|\\hat{Q}^{\\overline{{\\pi}}_{k,t}^{\\prime},\\pi_{k}}-Q^{\\overline{{\\pi}}_{k,t}^{\\prime},\\pi_{k}}\\|_{\\nu_{\\overline{{\\pi}}_{k,t}^{\\prime},\\underline{{\\pi}}_{k}}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\eta\\langle\\mathbb{E}_{b\\sim\\underline{{\\pi}}_{k}}[Q^{\\overline{{\\pi}}_{k,t}^{\\prime},\\pi_{k}}(s,\\cdot,b)],\\overline{{\\pi}}(\\cdot|s)-\\overline{{\\pi}}_{k,t+1}^{\\prime}(\\cdot|s)\\rangle}\\\\ &{=-\\eta\\langle\\mathbb{E}_{b\\sim\\underline{{\\pi}}_{k}}[Q^{\\overline{{\\pi}}_{k,t}^{\\prime},\\pi_{k}}(s,\\cdot,b)],\\overline{{\\pi}}(\\cdot|s)-\\overline{{\\pi}}_{k,t}^{\\prime}(\\cdot|s)\\rangle}\\\\ &{\\quad-\\eta\\langle\\mathbb{E}_{b\\sim\\underline{{\\pi}}_{k}}[Q^{\\overline{{\\pi}}_{k,t}^{\\prime},\\pi_{k}}(s,\\cdot,b)],\\overline{{\\pi}}_{k,t}^{\\prime}(\\cdot|s)-\\overline{{\\pi}}_{k,t+1}^{\\prime}(\\cdot|s)\\rangle}\\\\ &{\\leq-\\eta\\langle\\mathbb{E}_{b\\sim\\underline{{\\pi}}_{k}}[Q^{\\overline{{\\pi}}_{k,t}^{\\prime},\\pi_{k}}(s,\\cdot,b)],\\overline{{\\pi}}(\\cdot|s)-\\overline{{\\pi}}_{k,t}^{\\prime}(\\cdot|s)\\rangle}\\\\ &{\\quad+\\frac{\\eta^{2}\\|Q^{\\overline{{\\pi}}_{k,t}^{\\prime},\\pi_{k}}(s,a,b)\\|_{\\infty}^{2}}{2}+K L(\\overline{{\\pi}}_{k,t+1}^{\\prime}(\\cdot|s),\\overline{{\\pi}}_{k,t}^{\\prime}(\\cdot|s))}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Consequently, we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\langle\\mathbb{E}_{b\\sim\\underline{{\\pi}}_{k}}[Q^{\\overline{{\\pi}}_{k,t}^{\\prime},\\underline{{\\pi}}_{k}}(s,\\cdot,b)],\\overline{{\\pi}}(\\cdot|s)-\\overline{{\\pi}}_{k,t}^{\\prime}(\\cdot|s)\\rangle+\\frac{1}{\\eta}K L(\\overline{{\\pi}}(\\cdot|s),\\overline{{\\pi}}_{k,t+1}^{\\prime}(\\cdot|s))|}\\\\ &{\\leq\\!\\frac{1}{\\eta}K L(\\overline{{\\pi}},\\overline{{\\pi}}_{k,t}^{\\prime})+O(\\eta(\\|Q^{\\overline{{\\pi}}_{k,t}^{\\prime},\\underline{{\\pi}}_{k}}(s,a,b)\\|_{\\infty}^{2}+\\|\\hat{Q}^{\\overline{{\\pi}}_{k,t}^{\\prime},\\underline{{\\pi}}_{k}}-Q^{\\overline{{\\pi}}_{k,t}^{\\prime},\\underline{{\\pi}}_{k}}\\|_{\\nu_{\\overline{{\\pi}}_{k,t}^{\\prime},\\underline{{\\pi}}_{k}}}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Summing the inequality, we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{T}\\sum_{t=0}^{T}\\langle\\mathbb{E}_{b\\sim\\underline{{\\pi}}_{k}}[Q^{\\overline{{\\pi}}_{k,t}^{\\prime},\\pi_{k}}(s,\\cdot,b)],\\overline{{\\pi}}(\\cdot|s)-\\overline{{\\pi}}_{k,t}^{\\prime}(\\cdot|s)\\rangle}\\\\ &{\\leq\\!\\frac{1}{\\eta T}K L(\\overline{{\\pi}},\\overline{{\\pi}}_{k,t}^{\\prime})+\\frac{\\eta}{(1-\\gamma)^{2}}+\\eta\\|\\hat{Q}^{\\overline{{\\pi}}_{k,t}^{\\prime},\\pi_{k}}-Q^{\\overline{{\\pi}}_{k,t}^{\\prime},\\pi_{k}}\\|_{\\nu_{\\pi_{k,t}^{\\prime},\\pi_{k}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By the performance difference lemma, we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\nV^{\\overline{{\\pi}}_{k}^{*},\\underline{{\\pi}}_{k}}(s_{0})-V^{\\overline{{\\pi}}_{k,t}^{\\prime},\\underline{{\\pi}}_{k}}(s_{0})=\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim d_{s_{0}}^{\\overline{{\\pi}}_{k}^{*},\\underline{{\\pi}}_{k}}}\\langle\\mathbb{E}_{b\\sim\\underline{{\\pi}}_{k}(\\cdot\\vert s)}Q^{\\overline{{\\pi}}_{k,t}^{\\prime},\\underline{{\\pi}}_{k}}(s,\\cdot,b),\\overline{{\\pi}}_{k}^{*}(\\cdot\\vert s)-\\overline{{\\pi}}_{k,t}^{\\prime}(\\cdot\\vert s)\\rangle.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proposition E.4. Let Assumption 5 hold and $\\mu_{0}$ be a state distribution, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad E_{s\\sim\\mu_{0}}\\left[\\underset{\\overline{\\tau}}{\\operatorname*{max}}V^{\\overline{\\pi},\\underline{\\pi}_{K}}(s)-V^{*}(s)\\right]}\\\\ &{\\leq\\!O\\left(\\frac{C K}{(1-\\gamma)^{2}}(\\frac{\\log\\frac{1}{\\underline{c}}}{\\eta T}+\\frac{\\eta}{(1-\\gamma)^{2}}+\\eta\\|\\hat{Q}^{\\overline{\\pi}_{k-1},\\underline{x}_{k-1}}-Q^{\\overline{\\pi}_{k-1},\\underline{x}_{k-1}}\\|_{\\nu_{\\overline{\\pi}_{k-1},\\pi_{k-1}}}}\\\\ &{\\quad+\\,\\eta\\|\\hat{Q}^{\\overline{\\pi}_{k,t}^{\\prime},\\underline{{\\pi}}_{k}}-Q^{\\overline{\\pi}_{k,t}^{\\prime},\\underline{{\\pi}}_{k}}\\|_{\\nu_{\\overline{\\pi}_{k,t}^{\\prime},\\underline{x}_{k}}})+\\frac{\\gamma^{K}}{1-\\gamma}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $C$ is a problem-dependent constant. ", "page_idx": 24}, {"type": "text", "text": "Proof. Inserting the results of Lemma E.6,Lemma E.7 to (44) gives the result. ", "page_idx": 24}, {"type": "text", "text": "Let $L~=~\\Theta(\\log n)$ and $\\begin{array}{r l r}{T}&{=}&{\\sqrt{\\frac{\\log1/\\underline{{c}}}{\\log n}(\\frac{1}{(1-\\gamma)^{2}}+\\frac{\\tilde{g}_{\\alpha}}{(1-\\gamma)^{2}\\sqrt{m}}+\\frac{\\tilde{g}_{\\alpha}^{6}m^{3}}{(1-\\gamma)\\Upsilon_{1}^{2}\\Upsilon_{2}\\sqrt{n}})}}\\end{array}$ for random features and T =   log n ((1\u22121\u03b3)2 +(1\u2212\u03b3g\u02dc)\u03b12nnys $\\begin{array}{r}{T\\;=\\;\\sqrt{\\frac{\\log1/\\underline{{c}}}{\\log n}(\\frac{1}{(1-\\gamma)^{2}}+\\frac{\\tilde{g}_{\\alpha}}{(1-\\gamma)^{2}n_{\\mathrm{nys}}}+\\frac{\\tilde{g}_{\\alpha}^{6}m^{3}}{(1-\\gamma)\\Upsilon_{1}^{2}\\Upsilon_{2}\\sqrt{n}})}}\\end{array}$ for Nystr\u00f6m features. Combining the error of policy evaluation in Theorem D.1, we have that the iteration complexity to get $E_{s\\sim\\mu_{0}}\\left[\\operatorname*{max}_{\\overline{{\\pi}}}V^{\\overline{{\\pi}},\\underline{{\\pi}}_{k}}(s)-V^{*}(s)\\right]\\leq\\epsilon$ is $\\widetilde{O}\\big(\\frac{1}{(1\\!-\\!\\gamma)^{3}\\epsilon}\\big)$ . ", "page_idx": 24}, {"type": "text", "text": "E.1 proof with $\\beta$ -greedy exploration ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we are going to use $\\beta-$ greedy policy to avoid Assumption 5. Let us define the modified policies with greedy exploration ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\hat{\\overline{{\\pi}}}=(1-\\beta)\\overline{{\\pi}}+\\beta U n i f(A),\\quad\\hat{\\underline{{\\pi}}}=(1-\\beta)\\underline{{\\pi}}+\\beta U n i f(A)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now we are going to sample with the $\\hat{\\overline{{\\pi}}},\\hat{\\underline{{\\pi}}}$ and the algorithm will read as Algorithm 4. ", "page_idx": 25}, {"type": "text", "text": "Lemma E.8. In Stage 1 of Algorithm 4, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\underset{\\overline{{\\pi}}}{\\operatorname*{max}}\\overline{{\\pi}}Q_{k-1\\underline{{\\pi}}_{k}}-\\underset{\\underline{{\\pi}}}{\\operatorname*{min}}\\overline{{\\pi}}_{k}Q_{k-1\\underline{{\\pi}}}\\right]}\\\\ &{\\leq\\!\\frac{\\eta}{2(1-\\gamma)^{2}}+\\frac{32\\gamma\\beta}{(1-\\gamma)^{2}}+\\frac{1}{\\eta T}\\log\\frac{1}{|\\mathcal{A}|}+2\\eta\\|\\hat{Q}^{\\widehat{\\pi}_{k-1},\\widehat{\\pi}_{k-1}}-\\hat{Q}\\|_{\\nu_{\\widehat{\\pi}_{k-1},\\widehat{\\pi}_{k-1}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Denote $Q=Q_{k-1},{\\hat{\\hat{Q}}}=Q^{{\\hat{\\overline{{\\pi}}}}_{k-1},{\\hat{\\underline{{\\pi}}}}_{k-1}}$ . First by definition, it holds for all $s$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle=\\frac{\\kappa}{T}\\sum_{t=1}^{T}\\left[\\langle\\mathbb{E}_{b\\sim\\mathbb{Z}_{k,t}(\\cdot\\vert s)}Q(s,\\cdot,b),\\overline{{\\pi}}(\\cdot\\vert s)-\\overline{{\\pi}}_{k,t}(\\cdot\\vert s)\\rangle-\\langle\\mathbb{E}_{a\\sim\\overline{{\\pi}}_{k,t}(\\cdot\\vert s)}Q(s,a,\\cdot),\\underline{{\\pi}}(\\cdot\\vert s)-\\underline{{\\pi}}_{k,t}(\\cdot\\vert s)\\rangle\\right]}\\\\ {\\displaystyle=\\frac{1}{T}\\sum_{t=1}^{T}\\left[\\langle\\mathbb{E}_{b\\sim\\mathbb{Z}_{k,t}(\\cdot\\vert s)}\\hat{Q}(s,\\cdot,b),\\overline{{\\pi}}(\\cdot\\vert s)-\\overline{{\\pi}}_{k,t}(\\cdot\\vert s)\\rangle-\\langle\\mathbb{E}_{a\\sim\\overline{{\\pi}}_{k,t}(\\cdot\\vert s)}\\hat{Q}(s,a,\\cdot),\\underline{{\\pi}}(\\cdot\\vert s)-\\underline{{\\pi}}_{k,t}(\\cdot\\vert s)\\rangle\\right]}\\\\ {\\displaystyle+\\frac{1}{T}\\sum_{t=1}^{T}\\left[\\langle\\mathbb{E}_{b\\sim\\mathbb{Z}_{k,t}(\\cdot\\vert s)}[Q(s,\\cdot,b)-\\hat{Q}(s,\\cdot,b)],\\overline{{\\pi}}(\\cdot\\vert s)-\\overline{{\\pi}}_{k,t}(\\cdot\\vert s)\\rangle\\right.}\\\\ {\\displaystyle-\\left.\\langle\\mathbb{E}_{a\\sim\\overline{{\\pi}}_{k,t}(\\cdot\\vert s)}[Q(s,a,\\cdot)-\\hat{Q}(s,a,\\cdot)],\\underline{{\\pi}}(\\cdot\\vert s)-\\underline{{\\pi}}_{k,t}(\\cdot\\vert s)\\rangle\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For the error terms note ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathbb{E}_{b\\sim\\underline{{\\pi}}_{k,t}(\\cdot\\vert s)}[Q(s,\\cdot,b)-\\hat{Q}(s,\\cdot,b)],\\overline{{\\pi}}(\\cdot\\vert s)-\\overline{{\\pi}}_{t}(\\cdot\\vert s)\\rangle}\\\\ &{\\leq2\\big\\|\\mathbb{E}_{b\\sim\\underline{{\\pi}}_{k,t}(\\cdot\\vert s)}Q(s,\\cdot,b)-\\hat{Q}(s,\\cdot,b)\\big\\|_{\\infty}}\\\\ &{\\leq2\\gamma\\underset{a,b}{\\operatorname*{max}}\\,\\big\\vert\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot\\vert s,a,b)}[V^{\\overline{{\\pi}}_{k-1,\\frac{\\pi}{k}{k}-1}\\left(s^{\\prime}\\right)-V^{\\frac{\\pi}{\\pi}_{k-1,\\frac{\\hat{\\pi}_{k-1}}{\\hat{L}_{k-1}}}}(s^{\\prime})\\big]\\big\\vert}}\\\\ &{\\leq2\\gamma\\|V^{\\overline{{\\pi}}_{k-1,\\underline{{\\pi}}_{k-1}}}-V^{\\hat{\\pi}_{k-1,\\frac{\\hat{\\pi}_{k-1}}{\\hat{L}_{k-1}}}}\\|_{\\infty}}\\\\ &{\\leq\\frac{16\\gamma\\beta}{\\left(1-\\gamma\\right)^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last step is due to the Lipschitzness of the value function due to performance difference lemma and that the policies $\\hat{\\overline{{\\pi}}}_{k-1},\\hat{\\underline{{\\pi}}}_{k-1}$ and $\\overline{{\\pi}}_{k-1},\\underline{{\\pi}}_{k-1}$ differ at most by $\\beta$ . ", "page_idx": 25}, {"type": "text", "text": "By three point identity, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K L(\\underline{{\\pi}}(\\cdot|s),\\underline{{\\pi}}_{k,t+1}(\\cdot|s))\\leq K L(\\underline{{\\pi}}(\\cdot|s),\\underline{{\\pi}}_{k,t}(\\cdot|s))}\\\\ &{\\phantom{\\leq}+\\eta\\langle\\mathbb{E}_{a\\sim\\hat{\\pi}_{k-1}}[\\widehat{Q}^{\\hat{\\pi}_{k-1}}\\mathbb{\\hat{x}}_{k-1}(s,a,\\cdot)-\\hat{\\hat{Q}}(s,a,\\cdot)],\\underline{{\\pi}}(\\cdot|s)-\\underline{{\\pi}}_{k,t+1}(\\cdot|s)\\rangle}\\\\ &{\\phantom{\\leq\\eta}+\\eta\\langle\\mathbb{E}_{a\\sim\\hat{\\pi}_{k,t}}[\\hat{Q}(s,a,\\cdot)],\\underline{{\\pi}}(\\cdot|s)-\\underline{{\\pi}}_{k,t+1}(\\cdot|s)\\rangle}\\\\ &{\\phantom{\\leq\\eta}-K L(\\underline{{\\pi}}_{k,t+1}(\\cdot|s),\\underline{{\\pi}}_{k,t}(\\cdot|s)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Again we use Cauchy-Schwarz and Youngss inequalities ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle\\mathbb{E}_{a\\sim\\hat{\\pi}_{k-1}}[\\hat{Q}^{\\hat{\\pi}_{k-1},\\hat{\\pi}_{k-1}}(s,a,\\cdot)-\\hat{\\bar{Q}}(s,a,\\cdot)],\\underline{{\\pi}}(\\cdot|s)-\\underline{{\\pi}}_{k,t+1}(\\cdot|s)\\rangle\\leq2\\eta\\|\\hat{Q}^{\\hat{\\pi}_{k-1},\\hat{\\underline{{x}}}_{k-1}}-\\hat{\\bar{Q}}\\|_{\\nu_{\\tilde{\\pi}_{k-1},\\hat{\\pi}_{k-1}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Data: Transition Model $s^{\\prime}\\,=\\,f(s,a,b)+\\varepsilon$ where $\\varepsilon\\,\\sim\\,{\\mathcal{N}}(0,\\sigma^{2}I_{d})$ , Reward Function $r(s,a,b)$ Number of Random/Nystr\u00f6m Feature $m$ , Number of Nystr\u00f6m Samples $n_{\\mathrm{Nys}}\\geq m$ , Nystr\u00f6m Sampling Distribution $\\mu_{\\mathrm{Nys}}$ , Number of Sample $n$ , Factorization Scale $\\alpha$ , Learning Rate $\\eta$ ", "page_idx": 26}, {"type": "text", "text": "Result: \u03c0k Generate $\\phi(s,a,b)$ using Algorithm 1 or Algorithm 2. ", "page_idx": 26}, {"type": "text", "text": "Stage 1: Initialize $\\overline{{\\theta}}_{0}=\\underline{{\\theta}}_{0}=0$ and $\\overline{{\\pi}}_{0}(\\cdot|s)=\\underline{{\\pi}}_{0}(\\cdot|s)=U n i f(A)$ for all $s\\in S$ . ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\hat{w}_{k,t,l+1}=\\arg\\operatorname*{min}_{w}\\left\\{\\sum_{i\\in[n]}\\left(\\phi(s_{i},a_{i},b_{i})^{\\top}w-r(s_{i},a_{i},b_{i})-\\gamma\\phi(s_{i}^{\\prime},a_{i}^{\\prime},b_{i}^{\\prime})^{\\top}\\hat{w}_{k,t,l}\\right)^{2}\\right\\}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{\\pi}}_{k,t+1}(a|s)\\propto\\exp(\\mathbb{E}_{b\\sim\\pi_{k,t}(\\cdot|s)}[\\phi(s,a,b)]^{\\top}\\overline{{\\theta}}_{k,t+1}),}\\\\ &{\\underline{{\\pi}}_{k,t+1}(b|s)\\propto\\exp(\\mathbb{E}_{a\\sim\\overline{{\\pi}}_{k,t}(\\cdot|s)}[\\phi(s,a,b)]^{\\top}\\underline{{\\theta}}_{k,t+1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\hat{\\zeta}_{k,t,l+1}=\\arg\\operatorname*{min}_{\\zeta}\\left\\{\\sum_{i\\in[n]}\\left(\\phi(s_{i},a_{i},b_{i})^{\\top}\\zeta-r(s_{i},a_{i},b_{i})-\\gamma\\phi(s_{i}^{\\prime},a_{i}^{\\prime},b_{i}^{\\prime})^{\\top}\\hat{\\zeta}_{k,t,l}\\right)^{2}\\right\\}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\overline{{\\pi}}_{k,t+1}^{\\prime}(a|s)\\propto\\mathrm{exp}(\\mathbb{E}_{b\\sim\\underline{{\\pi}}_{k}(\\cdot|s)}[\\phi(s,a,b)]^{\\top}\\overline{{\\theta}}_{k,t+1}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Output $\\overline{{\\pi}}_{k}=\\overline{{\\pi}}_{k,\\hat{t}}^{\\prime}$ , where $\\hat{t}\\in[T]$ is selected uniformly at random. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\eta\\langle\\mathbb{E}_{a\\sim\\hat{\\pi}_{k,t}}[\\hat{Q}(s,a,\\cdot)],\\underline{{\\pi}}(\\cdot|s)-\\underline{{\\pi}}_{k,t+1}(\\cdot|s)\\rangle}\\\\ &{=\\eta\\langle\\mathbb{E}_{a\\sim\\hat{\\pi}_{k,t}}[\\hat{Q}(s,a,\\cdot)],\\underline{{\\pi}}(\\cdot|s)-\\underline{{\\pi}}_{k,t}(\\cdot|s)\\rangle}\\\\ &{\\quad+\\eta\\langle\\mathbb{E}_{a\\sim\\hat{\\pi}_{k,t}}[\\hat{Q}(s,a,\\cdot)],\\underline{{\\pi}}_{k,t}(\\cdot|s)-\\underline{{\\pi}}_{k,t+1}(\\cdot|s)\\rangle}\\\\ &{\\quad\\le\\eta\\langle\\mathbb{E}_{a\\sim\\hat{\\pi}_{k,t}}[\\hat{Q}(s,a,\\cdot)],\\underline{{\\pi}}(\\cdot|s)-\\underline{{\\pi}}_{k,t}(\\cdot|s)\\rangle}\\\\ &{\\quad+\\frac{\\eta^{2}}{2(1-\\gamma)^{2}}+K L(\\underline{{\\pi}}_{k+1}(\\cdot|s),\\underline{{\\pi}}_{k}(\\cdot|s))}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Using these estimations, we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathbb{E}_{a\\sim\\hat{\\pi}_{k,t}}[\\hat{Q}^{\\hat{\\pi}_{k,t},\\hat{\\pi}_{k,t}}(s,a,\\cdot)],\\pi_{k,t}(\\cdot|s)-\\pi(\\cdot|s)\\rangle+\\displaystyle\\frac{1}{\\eta}K L(\\underline{{\\pi}}(\\cdot|s),\\underline{{\\pi}}_{k,t+1}(\\cdot|s))}\\\\ &{\\leq\\displaystyle\\frac{1}{\\eta}K L(\\pi(\\cdot|s),\\underline{{\\pi}}_{k,t}(\\cdot|s))+\\displaystyle\\frac{\\eta}{2(1-\\gamma)^{2}}+2\\eta\\|\\hat{Q}^{\\hat{\\pi}_{k-1},\\hat{\\pi}_{k-1}}-\\hat{Q}\\|_{\\nu_{\\hat{\\pi}_{k-1},\\hat{\\pi}_{k-1}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Summing the inequality, we get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{1}{T}\\displaystyle\\sum_{t=1}^{T}\\langle\\mathbb{E}_{a\\sim\\hat{\\pi}_{k,t}}[\\hat{Q}^{\\hat{\\pi}_{k,t},\\pi_{k,t}}(s,a,\\cdot)],\\underline{{\\pi}}_{k,t}(\\cdot|s)-\\underline{{\\pi}}(\\cdot|s)\\rangle}\\\\ &{\\leq\\!\\frac{1}{\\eta T}K L(\\underline{{\\pi}},\\underline{{\\pi}}_{k,0})+2\\eta\\|\\hat{Q}^{\\hat{\\pi}_{k-1},\\hat{\\underline{{\\pi}}}_{k-1}}-\\hat{\\hat{Q}}\\|_{\\nu_{\\hat{\\pi}_{k-1},\\hat{\\underline{{\\pi}}}_{k-1}}}}\\\\ &{\\leq\\!\\frac{1}{\\eta T}\\log\\displaystyle\\frac{1}{|{\\cal A}|}+\\frac{\\eta}{2(1-\\gamma)^{2}}+2\\eta\\|\\hat{Q}^{\\hat{\\pi}_{k-1},\\hat{\\underline{{\\pi}}}_{k-1}}-\\hat{\\hat{Q}}\\|_{\\nu_{\\hat{\\pi}_{k-1},\\hat{\\pi}_{k-1}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\underset{\\overline{{\\pi}}}{\\operatorname*{max}}\\overline{{\\pi}}Q_{k-1}\\underline{{\\pi}}_{k}-\\underset{\\underline{{\\pi}}}{\\operatorname*{min}}\\overline{{\\pi}}_{k}Q_{k-1,\\overline{{\\pi}}}\\right]}\\\\ &{\\leq\\!\\frac{\\eta}{2(1-\\gamma)^{2}}+\\frac{32\\gamma\\beta}{(1-\\gamma)^{2}}+\\frac{1}{\\eta T}\\log\\frac{1}{|A|}+2\\eta\\|\\hat{Q}^{\\widehat{\\pi}_{k-1},\\widehat{\\pi}_{k-1}}-\\hat{Q}\\|_{\\nu_{\\widehat{\\pi}_{k-1},\\widehat{\\pi}_{k-1}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Lemma E.9. In Stage 2 of Algorithm 4, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{E}\\frac{1}{T}\\sum_{t=0}^{T}V^{\\overline{{\\pi}}_{k}^{\\ast},\\underline{{\\pi}}_{k}}(s_{0})-V^{\\overline{{\\pi}}_{k,t}^{\\prime},\\underline{{\\pi}}_{k}}(s_{0})}\\\\ &{\\leq\\!O\\left(\\frac{\\log|\\mathcal{A}|}{(1-\\gamma)\\eta T}+\\frac{\\eta}{(1-\\gamma)^{3}}+\\frac{\\gamma\\beta}{\\eta(1-\\gamma)^{3}}+\\frac{\\|\\hat{Q}^{\\overline{{\\pi}}_{k,t}^{\\prime},\\underline{{\\pi}}_{k}}-\\hat{\\hat{Q}}\\|_{\\nu_{\\overline{{\\pi}}_{k,t}^{\\prime},\\underline{{\\pi}}_{k}}}}{1-\\gamma}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Denote $\\hat{\\hat{Q}}=Q^{\\hat{\\overline{{\\pi}}}_{k,t}^{\\prime},\\hat{\\underline{{\\pi}}}_{k}}$ . By the update rule of the algorithm, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K L(\\overline{{\\pi}}(\\cdot|s),\\overline{{\\pi}}_{k,t+1}^{\\prime}(\\cdot|s))\\leq K L(\\overline{{\\pi}}(\\cdot|s),\\overline{{\\pi}}_{k,t}^{\\prime}(\\cdot|s))}\\\\ &{-\\eta(\\widetilde{\\mathbb{E}}_{b\\sim\\widehat{\\pi}_{k}}[\\widehat{Q}^{\\widetilde{\\pi}_{k,t}^{\\prime},\\widehat{\\pi}_{k}}(s,\\cdot)b],\\overline{{\\pi}}(\\cdot|s)-\\overline{{\\pi}}_{k,t+1}^{\\prime}(\\cdot|s))-K L(\\overline{{\\pi}}_{k,t+1}^{\\prime}(\\cdot|s),\\overline{{\\pi}}_{k,t}^{\\prime}(\\cdot|s))}\\\\ &{=K L(\\overline{{\\pi}}(\\cdot|s),\\overline{{\\pi}}_{k,t+1}^{\\prime}(\\cdot|s))-K L(\\overline{{\\pi}}_{k,t+1}^{\\prime}(\\cdot|s),\\overline{{\\pi}}_{k,t}^{\\prime}(\\cdot|s))}\\\\ &{-\\eta(\\widetilde{\\mathbb{E}}_{b\\sim\\widehat{\\pi}_{k}}[\\widehat{Q}^{\\widetilde{\\pi}_{k,t}^{\\prime},\\widehat{\\pi}_{k}}(s,\\cdot)b]-\\widehat{Q}(s,\\cdot)b],\\overline{{\\pi}}(\\cdot|s)-\\overline{{\\pi}}_{k,t+1}^{\\prime}(\\cdot|s))}\\\\ &{-\\eta(\\widetilde{\\mathbb{E}}_{b\\sim\\widehat{\\pi}_{k}}[\\widehat{Q}(s,\\cdot,b)],\\overline{{\\pi}}_{k,t}^{\\prime}(\\cdot|s)-\\overline{{\\pi}}_{k,t+1}^{\\prime}(\\cdot|s))}\\\\ &{-\\eta(\\widetilde{\\mathbb{E}}_{b\\sim\\widehat{\\pi}_{k}}[\\widehat{Q}^{\\widetilde{\\pi}_{k,t}^{\\prime},\\widehat{\\pi}_{k}}(s,\\cdot)b],\\overline{{\\pi}}(\\cdot|s)-\\overline{{\\pi}}_{k,t}^{\\prime}(\\cdot|s))}\\\\ &{-\\eta(\\widetilde{\\mathbb{E}}_{b\\sim\\widehat{\\pi}_{k}}[\\widehat{Q}(s,\\cdot,b)]-Q^{\\widetilde{\\pi}_{k,t}^{\\prime},\\widehat{\\pi}_{k}}(s,\\cdot)b],\\overline{{\\pi}}(\\cdot|s)-\\overline{{\\pi}}_{k,t}^{\\prime}(\\cdot|s))}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We bound the inner products similarly ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta\\langle\\mathbb{E}_{b\\sim\\widehat{\\underline{{n}}}_{k}}[\\widehat{Q}^{\\widehat{\\pi}_{k,t}^{\\prime}}\\widehat{\\underline{{n}}}_{k}(s,\\cdot,b)-\\widehat{\\dot{Q}}(s,\\cdot,b)],\\overline{{\\pi}}(\\cdot|s)-\\overline{{\\pi}}_{k,t+1}^{\\prime}(\\cdot|s)\\rangle\\leq O(\\eta\\|\\hat{Q}^{\\widehat{\\pi}_{k,t}^{\\prime},\\widehat{\\underline{{n}}}_{k}}-\\widehat{\\dot{Q}}\\|_{\\nu_{\\widehat{\\pi}_{k,t}^{\\prime},\\widehat{\\underline{{n}}}_{k}}})}\\\\ &{\\quad\\eta\\langle\\mathbb{E}_{b\\sim\\widehat{\\underline{{n}}}_{k}}[\\widehat{Q}(s,\\cdot,b)],\\overline{{\\pi}}_{k,t}^{\\prime}(\\cdot|s)-\\overline{{\\pi}}_{k,t+1}^{\\prime}(\\cdot|s)\\rangle\\leq\\displaystyle\\frac{\\eta^{2}}{2(1-\\gamma)^{2}}+K L(\\overline{{\\pi}}_{k,t+1}^{\\prime}(\\cdot|s),\\overline{{\\pi}}_{k,t}^{\\prime}(\\cdot|s))}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\eta\\langle\\mathbb{E}_{b\\sim\\underline{{\\pi}}_{k}}[\\hat{Q}(s,\\cdot,b)-Q^{\\overline{{\\pi}}_{k,t}^{\\prime},\\pi_{k}}(s,\\cdot,b)],\\overline{{\\pi}}(\\cdot|s)-\\overline{{\\pi}}_{k,t}^{\\prime}(\\cdot|s)\\rangle\\leq\\frac{16\\gamma\\beta}{(1-\\gamma)^{2}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Algorithm 5 Spectral Dynamic Embedding Policy Optimization with Neural Networks ", "page_idx": 28}, {"type": "text", "text": "Data: Transition Model $s^{\\prime}\\,=\\,f(s,a,b)+\\varepsilon$ where $\\varepsilon\\,\\sim\\,{\\mathcal{N}}(0,\\sigma^{2}I_{d})$ , Reward Function $r(s,a,b)$ Number of Random/Nystr\u00f6m Feature $m$ , Number of Nystr\u00f6m Samples $n_{\\mathrm{Nys}}\\geq m$ , Nystr\u00f6m Sampling Distribution $\\mu_{\\mathrm{Nys}}$ , Factorization Scale $\\alpha$ , Learning Rate $\\eta_{a c t o r}$ and \u03b7critic Result: \u03c0\u03b8, \u03c0\u03b8 1 Generate $\\phi(s,a,b)$ using Algorithm 1 or Algorithm 2. 2 Initialize \u03c0\u03b8, \u03c0\u03b8, Q\u03c8, Q . 3 for each iteration do 4 for each environment step do 5 Sample $\\{(s_{i},a_{i},b_{i},s_{i}^{\\prime},a_{i}^{\\prime},b_{i}^{\\prime})\\}$ to replay buffer. 6 for each gradient step do 7 Update $\\overline{{\\theta}},\\!\\underline{{\\theta}}$ by minimizing eqs. (55) and (57), respectively. 8 Update ${\\overline{{\\psi}}},{\\underline{{\\psi}}}$ by minimizing eqs. (61) and (62), respectively. ", "page_idx": 28}, {"type": "text", "text": "Consequently, we have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\langle\\mathbb{E}_{b\\sim\\underline{{\\pi}}_{k}}[Q^{\\overline{{\\pi}}_{k,t}^{\\prime},\\pi_{k}}(s,\\cdot,b)],\\overline{{\\pi}}(\\cdot|s)-\\overline{{\\pi}}_{k,t}^{\\prime}(\\cdot|s)\\rangle+\\displaystyle\\frac{1}{\\eta}K L(\\overline{{\\pi}}(\\cdot|s),\\overline{{\\pi}}_{k,t+1}^{\\prime}(\\cdot|s))|}\\\\ &{\\leq\\!\\frac{1}{\\eta}K L(\\overline{{\\pi}}(\\cdot|),\\overline{{\\pi}}_{k,t}^{\\prime}(\\cdot|s))+O(\\frac{\\gamma\\beta}{(1-\\gamma)^{2}}+\\eta(\\frac{1}{(1-\\gamma)^{2}}+\\|\\hat{Q}^{\\pi_{k,t}^{\\prime},\\pi_{k}}-\\hat{Q}\\|_{\\nu_{\\overline{{\\pi}}_{k,t}^{\\prime},\\pi_{k}}}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By the performance difference lemma ", "page_idx": 28}, {"type": "equation", "text": "$$\nV^{\\overline{{\\pi}}_{k}^{*},\\underline{{\\pi}}_{k}}(s_{0})-V^{\\overline{{\\pi}}_{k,t}^{\\prime},\\underline{{\\pi}}_{k}}(s_{0})=\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim d_{s_{0}}^{\\overline{{\\pi}}_{k}^{*},\\underline{{\\pi}}_{k}}}\\langle\\mathbb{E}_{b\\sim\\underline{{\\pi}}_{k}(\\cdot\\vert s)}Q^{\\overline{{\\pi}}_{k,t}^{\\prime},\\underline{{\\pi}}_{k}}(s,\\cdot,b),\\overline{{\\pi}}_{k}^{*}(\\cdot\\vert s)-\\overline{{\\pi}}_{k,t}^{\\prime}(\\cdot\\vert s)\\rangle.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Summing the inequality, we get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{E}\\frac{1}{T}\\sum_{t=0}^{T}V^{\\pi_{k}^{*},\\pi_{k}}(s_{0})-V^{\\pi_{k,t}^{\\prime},\\pi_{k}}(s_{0})}\\\\ &{\\displaystyle\\leq O(\\frac{\\log|\\mathcal{A}|}{(1-\\gamma)\\eta T}+\\frac{\\eta}{(1-\\gamma)^{3}}+\\frac{\\gamma\\beta}{\\eta(1-\\gamma)^{3}}+\\frac{\\|\\hat{Q}^{\\pi_{k,t}^{\\prime},\\pi_{k}}-\\hat{Q}\\|_{\\nu_{\\pi_{k,t}^{\\prime},\\pi_{k}}}}{1-\\gamma})}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proposition E.5. For the output of player2, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad E_{s\\sim\\mu_{0}}\\left[\\underset{\\overline{{\\pi}}}{\\operatorname*{max}}V^{\\overline{{\\pi}},\\underline{{\\pi}}_{K}}(s)-V^{*}(s)\\right]}\\\\ &{\\leq\\!O\\left(\\frac{C K}{(1-\\gamma)^{2}}(\\frac{\\log\\frac{1}{c}}{\\eta T}+\\frac{\\eta}{(1-\\gamma)^{2}}+\\frac{\\gamma\\beta}{\\eta(1-\\gamma)^{2}}+\\eta\\|\\hat{Q}^{\\pi_{k,t}^{\\prime},\\pi_{k}}-Q^{\\overline{{\\pi}}_{k,t}^{\\prime},\\pi_{k}}\\|_{\\nu_{\\overline{{\\pi}}_{k,t}^{\\prime}}\\pi_{k}}}\\\\ &{\\quad+\\eta\\|\\hat{Q}^{\\widehat{\\pi}_{k-1,\\widehat{\\pi}_{k-1}}}-\\hat{Q}\\|_{\\nu_{\\widehat{\\pi}_{k-1},\\widehat{\\pi}_{k-1}}})+\\frac{\\gamma^{K}}{1-\\gamma}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $C$ is a problem-dependent constant. ", "page_idx": 28}, {"type": "text", "text": "Proof. Combining Lemma E.8,Lemma E.9 and (44) gives the result. ", "page_idx": 28}, {"type": "text", "text": "F SDEPO with Neural Networks ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "To deal with continuous action space, we propose a practical variant of SDEPO, named SDEPO-NN, to deal with continuous (or large-scale discrete ) action space. SDEPO-NN utilizes neural networks in policy $\\pi$ and the state-action value function approximation $Q$ , detailed in Algorithm F. ", "page_idx": 28}, {"type": "text", "text": "Specifically, we parameterize $\\overline{{\\pi}},\\pi$ with ${\\overline{{\\psi}}},{\\underline{{\\psi}}}$ and parameterize $\\overline{{Q}},\\underline{{Q}}$ with $\\overline{{\\theta}},\\underline{{\\theta}}.$ , respectively. Based on the spectral dynamic embedding proposed in our paper, we parameterize the $\\overline{{Q}}$ function as $\\overline{{Q}}_{\\overline{{{\\theta}}}}(s,a,\\overline{{{b}}})=r(\\bar{s,a,b})+\\phi(s,a,b)^{\\top}\\overline{{{\\theta}}}$ and parameterize the $Q$ function as $\\underline{{Q}}_{\\underline{{\\theta}}}(s,a,b)=r(s,a,b)+$ $\\phi(s,a,b)^{\\top}\\underline{{\\theta}}.$ . ", "page_idx": 29}, {"type": "text", "text": "The $\\overline{{Q}}$ function is trained to minimize the soft Bellman residual ", "page_idx": 29}, {"type": "equation", "text": "$$\nJ(\\overline{{\\theta}})=\\mathbb{E}_{(s_{t},a_{t},b_{t},s_{t}^{\\prime},a_{t}^{\\prime},b_{t}^{\\prime})\\sim\\mathcal{D}}\\left[\\frac{1}{2}(\\overline{{Q}}_{\\overline{{\\theta}}}(s_{t},a_{t},b_{t})-\\hat{\\overline{{Q}}}_{\\overline{{\\theta}}}(s_{t},a_{t},b_{t}))^{2}\\right],\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "with ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\overline{{Q}}}_{\\overline{{\\theta}}}(s_{t},a_{t},b_{t})=r(s_{t},a_{t},b_{t})+\\gamma[\\overline{{Q}}_{\\overline{{\\theta}}}(s_{t}^{\\prime},a_{t}^{\\prime},b_{t}^{\\prime})-\\alpha\\log\\overline{{\\pi}}_{\\overline{{\\psi}}}(a_{t}^{\\prime}|s_{t}^{\\prime})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Similarly, The $Q$ function is trained to minimize the soft Bellman residual ", "page_idx": 29}, {"type": "equation", "text": "$$\nJ(\\underline{{\\theta}})=\\mathbb{E}_{(s_{t},a_{t},b_{t},s_{t}^{\\prime},a_{t}^{\\prime},b_{t}^{\\prime})\\sim\\mathcal{D}}\\left[\\frac{1}{2}(\\underline{{Q}}_{\\underline{{\\theta}}}(s_{t},a_{t},b_{t})-\\underline{{\\hat{Q}}}_{\\underline{{\\theta}}}(s_{t},a_{t},b_{t}))^{2}\\right],\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "with ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\underline{{\\hat{Q}}}_{\\underline{{\\theta}}}(s_{t},a_{t},b_{t})=r(s_{t},a_{t},b_{t})+\\gamma[\\underline{{Q}}_{\\underline{{\\theta}}}(s_{t}^{\\prime},a_{t}^{\\prime},b_{t}^{\\prime})-\\alpha\\log\\underline{{\\pi}}_{\\underline{{\\psi}}}(b_{t}^{\\prime}|s_{t}^{\\prime})].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We restrict the policy to Gaussians and the policy parameters can be learned by minimizing ", "page_idx": 29}, {"type": "equation", "text": "$$\nJ(\\overline{{\\psi}})=\\mathbb{E}_{s_{t}\\sim\\mathcal{D}}\\left[K L\\left(\\overline{{\\pi}}_{\\overline{{\\psi}}}(\\cdot|s_{t})||\\frac{\\mathrm{exp}(\\mathbb{E}_{b_{t}\\sim\\underline{{\\pi}}_{\\underline{{\\psi}}}}[Q_{\\theta}(s_{t},\\cdot,b_{t})]}{Z_{\\theta,1}(s_{t})}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and ", "page_idx": 29}, {"type": "equation", "text": "$$\nJ(\\underline{{\\psi}})=\\mathbb{E}_{s_{t}\\sim\\mathcal{D}}\\left[K L\\left(\\underline{{\\pi}}_{\\underline{{\\psi}}}(\\cdot|s_{t})||\\frac{\\mathrm{exp}(\\mathbb{E}_{a_{t}\\sim\\overline{{\\pi}}_{\\overline{{\\psi}}}}[Q_{\\theta}(s_{t},a_{t},\\cdot)]}{Z_{\\theta,2}(s_{t})}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\mathcal{D}$ is the replay buffer, $Z_{\\theta,1}\\big(s_{t}\\big)$ and $Z_{\\theta,2}\\mathopen{}\\mathclose\\bgroup\\left(s_{t}\\aftergroup\\egroup\\right)$ are normalization factors for the distributions. ", "page_idx": 29}, {"type": "text", "text": "To lower variance, we reparameterize the policy using a neural network transformation $a_{t}\\;=\\;$ $f_{\\overline{{\\psi}}}(\\epsilon_{t};s_{t})$ and $b_{t}\\,=\\,f_{\\overline{{\\psi}}}(\\epsilon_{t}^{\\prime};s_{t})$ where $\\epsilon_{t}$ and $\\epsilon_{t}^{\\prime}$ are input noise vectors, sampled from a Gaussian. We can now rewrite the objectives in Equations 59 and 60 as ", "page_idx": 29}, {"type": "equation", "text": "$$\nJ(\\overline{{\\psi}})=\\mathbb{E}_{s_{t}\\sim\\mathcal{D},\\epsilon_{t},\\epsilon_{t}^{\\prime}\\sim\\mathcal{N}}\\left[\\log\\overline{{\\pi}}_{\\overline{{\\psi}}}(f_{\\overline{{\\psi}}}(\\epsilon_{t};s_{t})|s_{t})-Q_{\\theta}(s_{t},\\overline{{\\pi}}_{\\overline{{\\psi}}}(f_{\\overline{{\\psi}}}(\\epsilon_{t};s_{t})|s_{t}),\\underline{{\\pi}}_{\\underline{{\\psi}}}(f_{\\underline{{\\psi}}}(\\epsilon_{t}^{\\prime};s_{t})|s_{t}))\\right]\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and ", "page_idx": 29}, {"type": "equation", "text": "$$\nJ(\\underline{{\\psi}})=\\mathbb{E}_{s_{t}\\sim\\mathcal{D},\\epsilon_{t},\\epsilon_{t}^{\\prime}\\sim N}\\left[\\log\\underline{{\\pi}}_{\\underline{{\\psi}}}(f_{\\underline{{\\psi}}}(\\epsilon_{t}^{\\prime};s_{t})|s_{t})-Q_{\\theta}(s_{t},\\overline{{\\pi_{\\overline{{\\psi}}}}}(f_{\\overline{{\\psi}}}(\\epsilon_{t};s_{t})|s_{t}),\\underline{{\\pi}}_{\\underline{{\\psi}}}(f_{\\underline{{\\psi}}}(\\epsilon_{t}^{\\prime};s_{t})|s_{t}))\\right]\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\overline{{\\pi}}_{\\overline{{\\psi}}}$ and $\\underline{{\\pi}}_{\\psi}$ are defined implicitly in terms of $f_{\\overline{{\\psi}}}$ and $f_{\\underline{{\\psi}}}$ , respectively. ", "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: Introduction 1 ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our assumption may be difficult to verify for compliance. It is left to our future work to derive theoretical guarantees without this assumption. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Section 4 ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Section 6 Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We release code of our experiment and it can reproduce experimental results. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Section 6 ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: Section 6 ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Section 6 Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Section 6 Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Section 7 Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. \u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. \u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: the paper poses no such risks. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We cite the original paper that produced the environment. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Code is well documented. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]