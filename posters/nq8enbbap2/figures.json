[{"figure_path": "Nq8enbbaP2/figures/figures_7_1.jpg", "caption": "Figure 1: We plot \u03c3(x, c) from Prop. 4.3 for different b, that trade-off between clipping approximation error and smoothness (D x 1/Lo).", "description": "This figure shows plots of the smooth clipping function \u03c3(x, c) and its derivative \u0129\u03c3(x, c) for different values of the smoothness parameter \u03b2.  The plots illustrate the trade-off between how well \u03c3(x, c) approximates the hard clipping function (x \u2227 c) (smaller D\u03b2) and the smoothness of its gradient (smaller L\u03b2).  The dashed black line in each plot represents the hard clipping function and its derivative for comparison.", "section": "4.2 Smooth clipping"}, {"figure_path": "Nq8enbbaP2/figures/figures_22_1.jpg", "caption": "Figure 1: We plot \u03c3(x, c) from Prop. 4.3 for different b, that trade-off between clipping approximation error and smoothness (D x 1/Lo).", "description": "This figure shows plots of the smooth clipping function \u03c3(x,c) for different values of the parameter \u03b2 and the clipping constant c.  The smooth clipping function \u03c3(x, c) is an approximation of the hard clipping function min(x, c) that is used in the paper's offline policy gradient algorithm. The plots illustrate the trade-off between the approximation error and the smoothness of the gradient of the smooth clipping function. The approximation error is the difference between the smooth clipping function and the hard clipping function. The smoothness is measured by the Lipschitz constant of the gradient of the smooth clipping function. The plots show that as \u03b2 increases, the approximation error decreases but the smoothness of the gradient decreases. Conversely, as c increases, the approximation error increases but the smoothness of the gradient increases.", "section": "4.2 Smooth clipping"}, {"figure_path": "Nq8enbbaP2/figures/figures_33_1.jpg", "caption": "Figure 3: Example in Prop. C.3", "description": "This figure is a simple MDP example used in Proposition C.3 to demonstrate that even with all-policy coverage in the offline setting, the optimality gap can still be large if the initial state distribution is not sufficiently exploratory.  The MDP consists of a root node S, which transitions to either node X or another node with probability \u03b5. Node X then transitions to either node Y or Z. Nodes Y and Z are terminal nodes with rewards 1/\u03b5 and 1 respectively. The other node transitions to another terminal node with reward 0. This example highlights the importance of sufficient data coverage for offline policy optimization, illustrating that even when all policies are represented in the data, the performance might still be far from optimal if the coverage is not sufficient in reward-relevant states.", "section": "C.5 Global convergence of OFF-OCCUPG"}]