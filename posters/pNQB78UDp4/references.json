{"references": [{"fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-05-20", "reason": "This paper is foundational to the field of large language models, impacting the development of parameter-efficient fine-tuning methods"}, {"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: pre-training of deep bidirectional transformers for language understanding", "publication_date": "2019-00-00", "reason": "BERT introduced a novel pre-training method for transformers, which is crucial to many modern large language models and relevant to parameter-efficient methods"}, {"fullname_first_author": "Brian Lester", "paper_title": "The power of scale for parameter-efficient prompt tuning", "publication_date": "2021-04-08", "reason": "This paper introduced prompt tuning, a parameter-efficient fine-tuning method directly related to the paper's core methodology"}, {"fullname_first_author": "Menglin Jia", "paper_title": "Visual prompt tuning", "publication_date": "2022-00-00", "reason": "This paper is the primary basis for the work presented in the current paper, providing the foundation for the proposed Cross Visual Prompt Tuning (CVPT) method"}, {"fullname_first_author": "Neil Houlsby", "paper_title": "Parameter-efficient transfer learning for NLP", "publication_date": "2019-00-00", "reason": "This paper introduced adapter-based methods, a significant approach in parameter-efficient fine-tuning which the paper contrasts against prompt-based methods"}]}