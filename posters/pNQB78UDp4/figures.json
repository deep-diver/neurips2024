[{"figure_path": "pNQB78UDp4/figures/figures_1_1.jpg", "caption": "Figure 1: Comparisons of performance and Flops between VPT and our CVPT with a pre-trained ViT-B/16 model on the VTAB-1k benchmark. We set the number of prompts to 1,10,20,50,100,150,200 respectively.", "description": "This figure compares the performance and computational cost (in terms of FLOPs) of VPT and CVPT on the VTAB-1k benchmark.  The x-axis represents the FLOPs, and the y-axis represents the accuracy.  Different lines represent different numbers of prompts used (from 1 to 200).  The results demonstrate that CVPT consistently outperforms VPT across various prompt counts, achieving significantly better accuracy with reduced computational cost, especially when using a larger number of prompts.", "section": "Comparisons of our CVPT and VPT"}, {"figure_path": "pNQB78UDp4/figures/figures_3_1.jpg", "caption": "Figure 2: Self-attention weight obtained by prompt tokens and embedded tokens. We visualize the self-attention of clstoken and exclude itself to observe the attention of clstoken to other tokens. And the darker the color, the larger the weight. When giving 196 prompts, the attention weight obtained by prompts is over 80%, which greatly influences the self-attention received by embedded tokens.", "description": "This figure visualizes the self-attention weights between prompt tokens and embedded tokens in VPT, showing how the addition of prompt tokens affects the attention weights of embedded tokens. As the number of prompts increases, the attention weight of prompts increases significantly, disrupting the self-attention between embedded tokens.", "section": "3.1 Analysis of previous VPT"}, {"figure_path": "pNQB78UDp4/figures/figures_4_1.jpg", "caption": "Figure 3: Structure comparison of VPT and CVPT. In which blue represents frozen parameters and orange represents learnable parameters.", "description": "This figure compares the architectures of Visual Prompt Tuning (VPT) and Cross Visual Prompt Tuning (CVPT).  It highlights the key difference: CVPT adds a cross-attention mechanism between prompt tokens and embedded tokens, while VPT only uses self-attention within the prompt tokens.  The color-coding distinguishes between frozen (blue) and learnable (orange) parameters in each model. CVPT's use of cross-attention allows it to better adapt to visual tasks and improves efficiency compared to VPT.", "section": "3 Method"}, {"figure_path": "pNQB78UDp4/figures/figures_5_1.jpg", "caption": "Figure 4: The trends of training memory, testing memory, and Flops with the variation in the number of prompt tokens. Where LP represents Linear Probing which only tunes the final classifier linear. We record those data on cifar100 in VTAB-1K, the batch_size is set to 32. Pre-trained model is ViT-B/16.", "description": "This figure displays the training memory, testing memory, and FLOPs (floating point operations) for VPT, CVPT, and Linear Probing (LP) methods across a range of prompt token numbers (1, 10, 20, 50, 100, 150, 200, 400).  The results show that CVPT significantly reduces memory usage and FLOPs compared to VPT, while maintaining comparable or even better performance (as shown in other figures of this paper). Linear Probing serves as a baseline for comparison.", "section": "4.2 Comparison with the SOTA"}, {"figure_path": "pNQB78UDp4/figures/figures_6_1.jpg", "caption": "Figure 1: Comparisons of performance and Flops between VPT and our CVPT with a pre-trained ViT-B/16 model on the VTAB-1k benchmark. We set the number of prompts to 1,10,20,50,100,150,200 respectively.", "description": "The figure shows a comparison of the performance (accuracy) and computational cost (floating-point operations or FLOPs) between Visual Prompt Tuning (VPT) and the proposed Cross Visual Prompt Tuning (CVPT) method.  The experiment is conducted on the VTAB-1K benchmark using a pre-trained Vision Transformer (ViT-B/16) model.  Different numbers of prompts (1, 10, 20, 50, 100, 150, 200) are tested to show how performance and efficiency vary with the number of prompts used. The results demonstrate that CVPT significantly outperforms VPT in terms of accuracy, while also showing a reduction in FLOPs, especially when using a larger number of prompts.", "section": "1 Introduction"}, {"figure_path": "pNQB78UDp4/figures/figures_8_1.jpg", "caption": "Figure 3: Structure comparison of VPT and CVPT. In which blue represents frozen parameters and orange represents learnable parameters.", "description": "This figure compares the architectures of Visual Prompt Tuning (VPT) and Cross Visual Prompt Tuning (CVPT).  It highlights the key difference: CVPT introduces a cross-attention mechanism between prompt tokens and embedded tokens, while VPT uses only self-attention within the prompt tokens.  The figure uses color-coding to distinguish between frozen (blue) and learnable (orange) parameters in each model, showing that CVPT is more parameter-efficient because it leverages the pre-trained weights from the self-attention layer for the cross-attention layer.", "section": "3.2 Cross Visual Prompt Tuning"}]