[{"figure_path": "tJGX7tpGO8/tables/tables_7_1.jpg", "caption": "Table 1: Performance comparison on CoraFull, Arxiv, and Reddit for GCIL setting. Results are averaged among three trials. The best performing results (excluding Joint) are highlighted in bold.", "description": "This table presents a comparison of the performance of different methods on three datasets (CoraFull, Arxiv, and Reddit) for the Graph Class Incremental Learning (GCIL) task.  The results are shown for three different class partitioning scenarios: Unequally, Equally (10), and Equally (2).  For each scenario and dataset, the table shows the Average Performance (AP) and Average Forgetting (AF) metrics.  The best performing method for each scenario and dataset (excluding the \"Joint\" method, which serves as an upper bound) is highlighted in bold.  The table provides a quantitative comparison of various approaches to handling catastrophic forgetting in GCIL.", "section": "5.1 Datasets and Setups"}, {"figure_path": "tJGX7tpGO8/tables/tables_7_2.jpg", "caption": "Table 8: Performance comparison on Cora and Citeseer for GCIL setting. Results are averaged among three trials. The best performing results (excluding Joint) are highlighted in bold, and the standard deviations are shown in gray.", "description": "This table presents the performance comparison of different methods for graph class incremental learning (GCIL) on the Cora and Citeseer datasets.  The results are averaged over three trials.  The 'AP\u2191' column shows the average performance improvement, and the 'AF\u2191' column represents the average forgetting reduction.  The best-performing methods (excluding the \"Joint\" method, which serves as an upper bound) are highlighted in bold. Standard deviations are shown in gray to indicate variability in performance across different trials.", "section": "5.2 Performance Comparison"}, {"figure_path": "tJGX7tpGO8/tables/tables_8_1.jpg", "caption": "Table 1: Performance comparison on CoraFull, Arxiv, and Reddit for GCIL setting. Results are averaged among three trials. The best performing results (excluding Joint) are highlighted in bold.", "description": "This table presents a comparison of the performance of various methods on three datasets (CoraFull, Arxiv, and Reddit) for the task of graph class incremental learning (GCIL).  The performance is measured using two metrics: Average Performance (AP) and Average Forgetting (AF).  The table shows results for different class partitioning strategies (unequal, equal with 10 classes per task, equal with 2 classes per task). The best performing model for each setting is highlighted in bold, excluding the 'Joint' method (which serves as an upper bound but isn't a practical incremental learning method).  The results demonstrate how GSIP improves the performance of existing methods.", "section": "5.1 Datasets and Setups"}, {"figure_path": "tJGX7tpGO8/tables/tables_16_1.jpg", "caption": "Table 4: Performance comparison before and after adding other graph frequency information preservation on CoraFull dataset.", "description": "This table presents the ablation study results on the CoraFull dataset, comparing the performance of the GSIP model with different combinations of low-frequency local, low-frequency global, and high-frequency information preservation modules.  It shows the Average Performance (AP) and Average Forgetting (AF) for different configurations of the GSIP model across three different experimental settings (Unequally, Equally(10), Equally(2)). The results highlight the contribution of each module to the overall performance of the model and illustrate how the inclusion of various frequency components enhances the model's ability to preserve information and reduce catastrophic forgetting.", "section": "5.3 Ablation Study"}, {"figure_path": "tJGX7tpGO8/tables/tables_16_2.jpg", "caption": "Table 5: Statistics of datasets.", "description": "This table presents the characteristics of five datasets used in the paper's experiments: CoraFull, Arxiv, Reddit, Cora, and Citeseer.  For each dataset, it shows the number of nodes and edges, the total number of classes, the number of tasks (incremental learning steps), the number of base classes (classes present from the start), and the number of novel classes (new classes introduced in each incremental learning step).  Different class partitioning schemes are used: unequal, equally (10 classes per task), and equally (2 classes per task) to simulate different learning scenarios. This detailed information provides context for understanding the experimental results and the challenges involved in the graph class incremental learning (GCIL) task.", "section": "5.1 Datasets and Setups"}, {"figure_path": "tJGX7tpGO8/tables/tables_17_1.jpg", "caption": "Table 1: Performance comparison on CoraFull, Arxiv, and Reddit for GCIL setting. Results are averaged among three trials. The best performing results (excluding Joint) are highlighted in bold.", "description": "This table presents a comparison of the performance of different methods on three datasets (CoraFull, Arxiv, and Reddit) for the task of graph class incremental learning (GCIL).  The performance is measured by two metrics: Average Performance (AP) and Average Forgetting (AF).  The table shows the results for three different experimental settings:  unequal class partitioning, equal class partitioning with 10 classes per task, and equal class partitioning with 2 classes per task. The best-performing methods (excluding the Joint method, which serves as an upper bound) are highlighted in bold for each setting.", "section": "5.1 Datasets and Setups"}, {"figure_path": "tJGX7tpGO8/tables/tables_18_1.jpg", "caption": "Table 1: Performance comparison on CoraFull, Arxiv, and Reddit for GCIL setting. Results are averaged among three trials. The best performing results (excluding Joint) are highlighted in bold.", "description": "This table presents a comparison of different methods for graph class incremental learning (GCIL) on three benchmark datasets: CoraFull, Arxiv, and Reddit.  The results show the average performance (AP) and average forgetting (AF) for each method across three experimental trials.  The best performing method for each dataset and setting (excluding the \"Joint\" method, which serves as an upper bound) is highlighted in bold.  The table allows for a comparison of performance across various GCIL approaches and datasets under different class partitioning schemes (unequal, equally 10 classes, equally 2 classes).", "section": "5.1 Datasets and Setups"}, {"figure_path": "tJGX7tpGO8/tables/tables_18_2.jpg", "caption": "Table 1: Performance comparison on CoraFull, Arxiv, and Reddit for GCIL setting. Results are averaged among three trials. The best performing results (excluding Joint) are highlighted in bold.", "description": "This table presents a comparison of the performance of different methods for graph class incremental learning (GCIL) on three datasets: CoraFull, Arxiv, and Reddit.  The performance is measured using two metrics: Average Performance (AP) and Average Forgetting (AF).  The table shows the AP and AF for each method under different experimental conditions (unequal class partitioning, equal partitioning with 10 classes per task, and equal partitioning with 2 classes per task). The best-performing method (excluding the \"Joint\" method, which serves as an upper bound by using all data) is highlighted in bold for each condition.", "section": "5.1 Datasets and Setups"}, {"figure_path": "tJGX7tpGO8/tables/tables_19_1.jpg", "caption": "Table 7: Performance comparison on CoraFull, Arxiv, and Reddit for GCIL setting. Results are averaged among three trials. The best performing results (excluding Joint) are highlighted in bold, and the standard deviations are shown in gray.", "description": "This table presents a performance comparison of various methods on three datasets (CoraFull, Arxiv, and Reddit) using the Graph Class Incremental Learning (GCIL) setting.  Results include Average Performance (AP) and Average Forgetting (AF), with the best performing methods (excluding the \"Joint\" method, which serves as an upper bound) highlighted in bold. Standard deviations are also shown in gray, providing insight into the variability of the results.", "section": "5.2 Performance Comparison"}, {"figure_path": "tJGX7tpGO8/tables/tables_19_2.jpg", "caption": "Table 1: Performance comparison on CoraFull, Arxiv, and Reddit for GCIL setting. Results are averaged among three trials. The best performing results (excluding Joint) are highlighted in bold.", "description": "This table presents a comparison of the performance of different methods for Graph Class Incremental Learning (GCIL) on three datasets: CoraFull, Arxiv, and Reddit.  The performance is evaluated using two metrics: Average Performance (AP) and Average Forgetting (AF).  The table shows the results for three different class partitioning strategies: Unequal, Equally (10), and Equally (2). The best performing models (excluding the 'Joint' model, which serves as an upper bound) are highlighted in bold.  The results are averages across three trials, and the standard deviations are shown in grey in the original paper.", "section": "5 Experiments"}, {"figure_path": "tJGX7tpGO8/tables/tables_19_3.jpg", "caption": "Table 3: Ablation comparisons of graph spatial information preserving strategy for ERGNN.", "description": "This table presents the ablation study results for the ERGNN model.  It shows the average performance (AP) and average forgetting (AF) metrics, along with their standard deviations, for different configurations of the GSIP framework. The configurations vary by including or excluding low-frequency local (LL), low-frequency global (LG), and high-frequency (H) information preservation modules. The results are broken down by dataset (CoraFull, Arxiv, Reddit) and class partitioning scheme (unequal, equally (10), equally (2)).  The baseline (B) performance is compared against the improvements achieved by adding each module sequentially.", "section": "5.3 Ablation Study"}, {"figure_path": "tJGX7tpGO8/tables/tables_22_1.jpg", "caption": "Table 12: Running time (s) of each epoch under three dataset partitioning cases on CoraFull dataset.", "description": "This table shows the running time in seconds for each epoch under three different dataset partitioning scenarios (Unequally, Equally (10), Equally (2)) for several methods: ERGNN, ERGNN with GSIP, SSM, SSM with GSIP, CaT, and CaT with GSIP.  The results highlight the computational cost of each method across various data partitioning strategies on the CoraFull dataset.", "section": "5.1 Datasets and Setups"}]