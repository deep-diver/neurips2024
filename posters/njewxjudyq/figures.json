[{"figure_path": "NjewXJUDYq/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of our spoken dialog modeling approach (Left). All possible self-supervised learning objectives from our speech-text pretraining scheme. (Right)", "description": "This figure illustrates the Unified Spoken Dialog Model (USDM) architecture. The left panel shows the overall framework, starting from input speech, which is processed by a prosody-infusing encoder and then fed into the USDM. The USDM generates response speech tokens, which are then decoded into raw waveforms.  The right panel details the speech-text pretraining scheme used to enhance cross-modal understanding, showcasing how speech and text tokens are interleaved and processed to learn coherent relationships.  It highlights the use of special tokens (<|continue|> and <|correspond|>) to guide the model's generation process.", "section": "3 Our Approach"}, {"figure_path": "NjewXJUDYq/figures/figures_3_1.jpg", "caption": "Figure 2: Pitch contour of the original audio and the audio reconstructed from extracted acoustic units. Due to the stochastic nature of the reconstruction model, we attempt reconstruction twice, demonstrating that the pitch variation closely mirrors the ground truth.", "description": "This figure shows a comparison of pitch contours between the original audio and two reconstructions from the extracted acoustic units.  The stochastic nature of the reconstruction process is highlighted by showing two slightly different reconstructions. Despite this, both reconstructions closely follow the pitch changes of the original audio, indicating that the acoustic units effectively capture the pitch information present in the original speech.", "section": "3.1 Speech-to-Unit Encoder"}, {"figure_path": "NjewXJUDYq/figures/figures_4_1.jpg", "caption": "Figure 3: The overall speech-text pretraining scheme.", "description": "This figure shows the overall process of speech-text pretraining.  It starts with speech-text alignment extraction, then pair-wise segmentation and segment-wise main modality random selection.  Finally, sub-modality random insertion and special token insertion is performed to generate a unified speech-text sequence for training a pretrained language model. The figure also details the types of tokens used and what their purpose is in the process.", "section": "3.2 Unified Speech-Text Pretraining"}, {"figure_path": "NjewXJUDYq/figures/figures_9_1.jpg", "caption": "Figure 4: Attention maps between the generated responses of the USDM and the input speech (s) and its transcribed text (t). Input speech: \u201cOh, I can\u2019t believe it. He looks very young.\u201d", "description": "This figure displays the attention weights between the generated response tokens and both the input speech tokens and the corresponding transcribed text tokens at different layers of the USDM model.  Warmer colors (reds) indicate higher attention weights. This visualization helps to understand how the model uses both the speech and text modalities to generate its responses, showing the interplay between the two modalities and the model's cross-modal attention mechanism.", "section": "3.3 Unified Spoken Dialog Model"}, {"figure_path": "NjewXJUDYq/figures/figures_21_1.jpg", "caption": "Figure 1: Overview of our spoken dialog modeling approach (Left). All possible self-supervised learning objectives from our speech-text pretraining scheme. (Right)", "description": "This figure provides an overview of the Unified Spoken Dialog Model (USDM) and its training process. The left panel shows the overall architecture of the USDM, illustrating how input speech is processed through a prosody-infusing encoder, a pretrained speech-text model, and a speech decoder to generate coherent spoken responses.  The right panel details all the self-supervised learning objectives used during speech-text pretraining to capture rich cross-modal semantics between speech and text.  This multi-step approach aims to enhance the model's ability to generate natural-sounding spoken responses within the context of spoken dialog.", "section": "3 Our Approach"}, {"figure_path": "NjewXJUDYq/figures/figures_24_1.jpg", "caption": "Figure 7: Left is the quantitative results for each epoch of the USDM fine-tuned on DailyTalk. The figure on the right illustrates the performance of the Spoken Dialog Model when trained with Low-Rank Adaptation (LoRA) versus full fine-tuning.", "description": "The left plot shows the performance of different metrics (METEOR, ROUGE-L, ASR WER, TTS WER) over epochs during the training of the Unified Spoken Dialog Model (USDM).  The right plot compares the performance of USDM trained with full fine-tuning against versions using Low-Rank Adaptation (LoRA) with different ranks (8 and 256). It highlights the trade-offs between model size and performance on various metrics.", "section": "4.2 Ablation Studies"}, {"figure_path": "NjewXJUDYq/figures/figures_25_1.jpg", "caption": "Figure 3: The overall speech-text pretraining scheme.", "description": "This figure illustrates the process of preparing unified speech-text sequences for pretraining a speech-text model. It involves three main steps: 1. Speech-Text Alignment Extraction: extracting word-level alignments between speech and text using the Montreal Forced Aligner; 2. Pair-wise Segmentation & Segment-wise Main Modality Random Selection: dividing the aligned speech and text into segments and randomly selecting one modality (speech or text) per segment; and 3. Sub-Modality Random Insertion & Special Token Insertion: inserting data from the non-selected modality with a certain probability and introducing special tokens to denote relationships between the modalities. The resulting unified sequences are used to train the speech-text model, enabling it to capture complex speech-text interactions.", "section": "3.2 Unified Speech-Text Pretraining"}]