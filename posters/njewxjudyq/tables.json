[{"figure_path": "NjewXJUDYq/tables/tables_7_1.jpg", "caption": "Table 1: Human evaluation results of our model and the baselines. We report the MOS and P-MOS scores with a 95% confidence interval.", "description": "This table presents the results of a human evaluation comparing the proposed Unified Spoken Dialog Model (USDM) against three baseline models: Cascaded, From Scratch, and SpeechGPT.  The evaluation assesses overall preference, and specifically the acoustic quality (MOS) and prosody (P-MOS) of the generated spoken responses.  Higher scores indicate better performance. The Ground Truth is included as a reference point.  The \"win\", \"tie\", and \"lose\" columns represent the percentage of times each model was preferred over the others in a pairwise comparison.", "section": "4.1.2 Evaluation and Comparison Results"}, {"figure_path": "NjewXJUDYq/tables/tables_7_2.jpg", "caption": "Table 2: GPT-4 evaluation and quantitative results of our model and the baselines.", "description": "This table presents a comprehensive comparison of the proposed Unified Spoken Dialog Model (USDM) against three baselines (Cascaded, From Scratch, and SpeechGPT) using both qualitative and quantitative metrics.  The qualitative evaluation utilizes GPT-4 to assess the semantic quality of generated responses, providing win/tie/lose percentages across all model comparisons. Quantitative analysis includes METEOR and ROUGE-L scores, which measure the semantic similarity between generated and ground truth responses, as well as STT (Speech-to-Text) and TTS (Text-to-Speech) Word Error Rates (WER).  Lower WER values are better. This table summarizes how USDM outperforms the baselines in various aspects such as semantic similarity and naturalness of speech, while highlighting the importance of the proposed pretraining and fine-tuning techniques.", "section": "4.1.2 Evaluation and Comparison Results"}, {"figure_path": "NjewXJUDYq/tables/tables_8_1.jpg", "caption": "Table 3: Results of the ablation studies on the pretraining and fine-tuning schemes. For PPL, we report the average PPL for each modality across the six combinations described in the text.", "description": "This table presents the results of ablation studies conducted to evaluate the impact of different pretraining and fine-tuning schemes on the model's performance.  The table shows the average perplexity (PPL) for both text and unit modalities, across six different combinations of interleaved speech-text sequences. It also shows the performance of the model on various downstream tasks like speech-to-text WER, text-to-speech WER, METEOR and ROUGE-L scores for spoken dialog modeling after different pretraining schemes. The comparison helps to understand which approach is the most effective.", "section": "4.2 Ablation Studies"}, {"figure_path": "NjewXJUDYq/tables/tables_19_1.jpg", "caption": "Table 4: Models for each component of the USDM and the baselines.", "description": "This table lists the specific models used for each component of the Unified Spoken Dialog Model (USDM) and three baseline models (From Scratch, Cascaded, and SpeechGPT).  It shows the ASR model, speech encoder, language model (LLM), speech decoder, and TTS model used in each approach.  This allows for a clear comparison of the different model architectures and their components.", "section": "4.1 Model Comparisons"}, {"figure_path": "NjewXJUDYq/tables/tables_19_2.jpg", "caption": "Table 4: Models for each component of the USDM and the baselines.", "description": "This table details the specific models used for each component of the Unified Spoken Dialog Model (USDM) and its three baselines: From Scratch, SpeechGPT, and Cascaded.  For each model, it lists the ASR model (if applicable), the speech encoder, the large language model (LLM), the speech decoder, and the text-to-speech (TTS) model. This allows for a clear comparison of the architecture and components used in each model.", "section": "4.1 Model Comparisons"}, {"figure_path": "NjewXJUDYq/tables/tables_20_1.jpg", "caption": "Table 5: License of each dataset we used for acoustic unit investigation, pretraining, and fine-tuning.", "description": "This table presents the licenses associated with each dataset used in the research.  The datasets are categorized by their use in the study: acoustic unit analysis, pretraining, and fine-tuning of the model.  This allows readers to quickly understand the legal permissions and restrictions associated with the data used in the experiments.", "section": "4.1 Model Comparisons"}, {"figure_path": "NjewXJUDYq/tables/tables_20_2.jpg", "caption": "Table 4: Models for each component of the USDM and the baselines.", "description": "This table lists the specific models used for each component of the Unified Spoken Dialog Model (USDM) and its baseline models.  It shows the ASR model, speech encoder, language model (LLM), speech decoder, and TTS model used in each setup.  This allows for a clear understanding of the different components and their configurations in each experimental setup.", "section": "4.1 Model Comparisons"}, {"figure_path": "NjewXJUDYq/tables/tables_22_1.jpg", "caption": "Table 7: METEOR and ROUGE-L results measured using the text obtained from ASR of the spoken response (Transcribed Response) and results measured using the intermediate text response (Intermediate Response).", "description": "This table compares the performance of different models (USDM, Cascaded, From Scratch, and SpeechGPT) on two different evaluation methods for semantic quality.  The \"Transcribed Response\" results used automatic speech recognition (ASR) to convert the spoken responses into text before calculating METEOR and ROUGE-L scores, which measure semantic similarity. The \"Intermediate Response\" results used the intermediate text generated by the model before speech synthesis, providing a direct comparison of the model's text generation capabilities. The TTS WER (text-to-speech word error rate) is also included to assess the quality of the speech generated by each model.", "section": "4.1.2 Evaluation and Comparison Results"}, {"figure_path": "NjewXJUDYq/tables/tables_22_2.jpg", "caption": "Table 8: Six types of speech-text interleaved sequences used to evaluate the performance of the pretrained model, along with the templates used for measuring PPL. For sequences with a continuation relationship, the speech and text data are split in half, combining one modality from the first half (e.g., speech1 token or text1 token) with the remaining modality from the second half (e.g., text2 token or speech2 token).", "description": "This table presents six different types of interleaved speech-text sequences used to evaluate the performance of the pretrained model.  The sequences are designed to test various relationships between speech and text, including unconditional, correspondence, and continuation relationships. For sequences testing continuation, the speech and text are split into two halves, and each half is combined with the other modality's second half.  The table also provides the templates used for calculating the perplexity (PPL) for each type of sequence.", "section": "4.2 Ablation Studies"}, {"figure_path": "NjewXJUDYq/tables/tables_23_1.jpg", "caption": "Table 9: PPL of various pretraining schemes for diverse unit and text combinations for the test-clean subset of LibriSpeech. T2U represents text-to-unit, and U2T represents unit-to-text, with PPL measured only for the subsequent modality. Lower is better.", "description": "This table presents the perplexity (PPL) scores achieved by different pretraining schemes on the LibriSpeech dataset.  It compares the performance of the proposed unified speech-text pretraining approach against three ablation studies: Setup 1 (continuation only), Setup 2 (correspondence only), and Setup 3 (a different interleaving strategy). The PPL is calculated separately for text and unit modalities, and overall. Lower PPL indicates better model performance. The results show that the unified approach outperforms the ablation studies.", "section": "4.2 Ablation Studies"}]