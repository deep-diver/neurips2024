[{"figure_path": "bIa03mAtxQ/figures/figures_3_1.jpg", "caption": "Figure 1: The forward pass of an (unfactorized) \u00b5MoE layer as a series of tensor contractions: the experts\u2019 weight matrices (yellow 2D slices) are matrix-multiplied with the input vector and summed (weighted by the red expert coefficients).", "description": "This figure illustrates the forward pass of a Multilinear Mixture of Experts (\u00b5MoE) layer.  The input vector is first multiplied with each expert's weight matrix. The resulting vectors are then weighted by the expert coefficients (which are calculated using a gating mechanism) and summed to produce the final output vector. This visualization helps to understand how \u00b5MoEs combine multiple expert's computations to produce a single output.", "section": "3.1 The \u00b5MoE layer"}, {"figure_path": "bIa03mAtxQ/figures/figures_5_1.jpg", "caption": "Figure 2: Specialization in 256 vs 32 total expert CP\u00b5MoE layers (fine-tuned on CLIP ViT-B-32). Each row displays randomly selected images processed (with coefficient \u2265 0.5) by the first few experts for the two models. The more we scale the expert count, the greater the apparent expert specialism (to single visual themes or image categories).", "description": "This figure compares the qualitative results of using CP\u00b5MoE layers with 256 and 32 experts, respectively, when fine-tuned on CLIP ViT-B-32.  The images displayed show those that had an expert coefficient of at least 0.5.  The figure demonstrates that as the number of experts increases, each expert becomes more specialized, focusing on specific visual themes or image categories.  In contrast, fewer experts result in experts that process a broader range of image categories.", "section": "4.1 Expert specialism: visualization & intervention"}, {"figure_path": "bIa03mAtxQ/figures/figures_5_2.jpg", "caption": "Figure 3: Higher expert counts lead to more monosemantic experts: mean expert class-level polysemanticity of Equation (4) (\u2193) as a function of the total number of experts. Results are shown for both CLIP ViT-B-32 and DINO models fine-tuned on ImageNET1k with CP\u00b5MoE layers.", "description": "This figure shows the relationship between the number of experts in a CP\u00b5MoE layer and the resulting expert specialization.  The y-axis represents the mean expert class-level polysemanticity, a measure of how focused each expert is on a single class.  The x-axis shows the total number of experts used. The results demonstrate that as the number of experts increases, the experts become more specialized in processing images belonging to specific classes, indicating an improvement in monosemanticity.", "section": "4.1 Expert specialism: visualization & intervention"}, {"figure_path": "bIa03mAtxQ/figures/figures_7_1.jpg", "caption": "Figure 4: Top-activating patches (top rows) and their full images (second rows) for the first 3 experts across 2 CP\u00b5MoE-e64 layers in \u00b5MoE MLP-mixer [80] models-\u00b5MoE blocks exhibit coarse-grained specialism (e.g., texture) earlier and more fine-grained specialism (e.g., objects) deeper in the network.", "description": "This figure visualizes the top-activating image patches and their corresponding full images for the first three experts across two CP\u00b5MoE layers (with 64 experts each) within a \u00b5MoE MLP-mixer model. It demonstrates how \u00b5MoE blocks develop specializations at different levels of granularity. In the earlier layers (Layer 2), the experts show coarse-grained specialism, focusing on texture. As the network deepens (Layer 7), the experts exhibit more fine-grained specialism, concentrating on object categories.", "section": "4.1 Expert specialism: visualization & intervention"}, {"figure_path": "bIa03mAtxQ/figures/figures_8_1.jpg", "caption": "Figure 2: Specialization in 256 vs 32 total expert CP\u00b5MoE layers (fine-tuned on CLIP ViT-B-32). Each row displays randomly selected images processed (with coefficient \u2265 0.5) by the first few experts for the two models. The more we scale the expert count, the greater the apparent expert specialism (to single visual themes or image categories).", "description": "This figure compares the qualitative results of fine-tuning CLIP ViT-B-32 with CP\u00b5MoE layers using 256 and 32 total experts.  It shows randomly selected images processed by the first few experts in each model, highlighting the increased specialization observed with a larger number of experts.  Experts with 256 total experts show a much stronger tendency to focus on a single visual theme or image category, while experts with 32 total experts tend to exhibit more polysemanticity, processing images from multiple categories.", "section": "4.1 Expert specialism: visualization & intervention"}, {"figure_path": "bIa03mAtxQ/figures/figures_17_1.jpg", "caption": "Figure 6: An intuitive visualization of the \u00b5MoE (unfactorized) forward pass, as visualized (as a series of tensor contractions) in 5 steps. Each step contributes to producing the output vector y \u2208 R<sup>O</sup> either by contracting with the expert coefficients a \u2208 R<sup>N</sup>, or with the input vector z \u2208 R<sup>I</sup>, along the appropriate mode of the collective weight tensor W \u2208 R<sup>N\u00d7I\u00d7O</sup>.", "description": "This figure shows a step-by-step visualization of the unfactorized \u00b5MoE forward pass, which is a series of tensor contractions.  It illustrates how the output vector is generated by a combination of operations involving the input vector, expert coefficients, and the weight tensor. Each step is visually represented to enhance understanding of the process.", "section": "3.1 The \u00b5MoE layer"}, {"figure_path": "bIa03mAtxQ/figures/figures_19_1.jpg", "caption": "Figure 7: Val. accuracy for an S-16 MLP-mixer when performing truncated SVD on all MLP's linear layers' weight; model accuracy is closely retained even with half the singular vectors.", "description": "This figure shows the ImageNet1k validation accuracy of an S-16 MLP-Mixer model as a function of the percentage of singular vectors kept after applying truncated Singular Value Decomposition (SVD) to all the model's linear layers' weight matrices. The results demonstrate that even when only half of the singular vectors are kept, the model's accuracy is still very high, suggesting that low-rank approximations of MLP layers in this type of model can be effective.", "section": "D Decomposition choice, matrix rank, and computational cost"}, {"figure_path": "bIa03mAtxQ/figures/figures_20_1.jpg", "caption": "Figure 8: \u00b5MoE layer parameter count as a function of expert count.", "description": "This figure shows a comparison of the number of parameters required for \u00b5MoE layers (CP\u00b5MoE and TR\u00b5MoE) and traditional sparse/soft MoE layers as a function of the number of experts.  The plot demonstrates that \u00b5MoE layers, particularly TR\u00b5MoE with carefully chosen ranks, require significantly fewer parameters than traditional MoE approaches, especially as the number of experts increases. This highlights the parameter efficiency of the proposed \u00b5MoE architecture.", "section": "D Decomposition choice, matrix rank, and computational cost"}, {"figure_path": "bIa03mAtxQ/figures/figures_21_1.jpg", "caption": "Figure 9: Illustration of a two-hierarchy \u03bcMoE layer's (unfactorized) forward pass as a series of tensor contractions. The N1N2 many experts' weight matrices are visualized as 2D horizontal slices in yellow, which are (1) matrix-multiplied with the input vector, (2) summed over the first expert mode (weighted by the first expert coefficients a1 in red), and (3) summed over the second expert mode (weighted by the second expert mode's coefficients a2 in dark green).", "description": "This figure illustrates the forward pass of a two-level hierarchical \u03bcMoE layer.  It shows how the input vector is processed through a series of tensor contractions involving two sets of expert coefficients (a1 and a2) and the weight tensor W. The visualization helps understand how the layer combines computations from multiple experts at different hierarchical levels to produce the final output vector.", "section": "E Hierarchical \u03bcMoE model derivations"}, {"figure_path": "bIa03mAtxQ/figures/figures_23_1.jpg", "caption": "Figure 10: Training loss and validation accuracy for the MLP-mixers models for 300 epochs.", "description": "This figure shows the training and validation accuracy curves for MLP-Mixer models trained for 300 epochs.  Three different model configurations are compared: a standard MLP model and two versions using the proposed \u00b5MoE layers (CP\u00b5MoE and TR\u00b5MoE). The graphs illustrate the convergence of training loss and the performance on the validation set for each model. This visual comparison allows assessing the training stability and effectiveness of \u00b5MoE layers compared to standard MLPs.", "section": "4.3 Large language/vision \u03bc\u039c\u03bf\u0395 networks"}, {"figure_path": "bIa03mAtxQ/figures/figures_23_2.jpg", "caption": "Figure 10: Training loss and validation accuracy for the MLP-mixers models for 300 epochs.", "description": "This figure shows the training and validation loss curves for MLP-mixer models trained for 300 epochs.  The curves represent the performance of three different model variations:  a standard MLP, a CP\u00b5MoE model, and a TR\u00b5MoE model. The plot visually compares the training and validation performance of these models across different epochs, illustrating the convergence of each model's loss and the accuracy achieved on a validation set. The specific values for the loss and accuracy at the end of training (300 epochs) are shown in a legend box.", "section": "4.3 Large language/vision \u03bc\u039c\u03bf\u0395 networks"}, {"figure_path": "bIa03mAtxQ/figures/figures_24_1.jpg", "caption": "Figure 12: Top-activating patches (and their surrounding image context) for the first two experts at two \u03bcMoE blocks in MLP-mixer models. \u03bcMoE blocks (with N = 64) exhibit coarse-grained specialism (e.g., texture) earlier and more fine-grained specialism (e.g., object category) deeper in the network.", "description": "This figure shows the top-activating image patches for the first two experts at two different \u03bcMoE blocks within MLP-mixer models.  The visualization demonstrates how the \u00b5MoE blocks learn to specialize in different aspects of the image.  Earlier layers show more coarse-grained specialization (texture), while deeper layers show more fine-grained specialization (object category). This provides visual evidence for the claim that increasing the number of \u03bcMoE experts leads to increased task modularity and specialization.", "section": "4.3 Large language/vision \u03bcMoE networks"}, {"figure_path": "bIa03mAtxQ/figures/figures_24_2.jpg", "caption": "Figure 12: Top-activating patches (and their surrounding image context) for the first two experts at two \u03bcMoE blocks in MLP-mixer models. \u03bcMoE blocks (with N = 64) exhibit coarse-grained specialism (e.g., texture) earlier and more fine-grained specialism (e.g., object category) deeper in the network.", "description": "This figure shows the top-activating image patches for the first two experts at two different layers in a MLP-mixer model. The model uses \u03bcMoE (Mixture of Experts) blocks with 64 experts.  The results demonstrate that early layers show coarse-grained specialization (such as texture), while deeper layers demonstrate finer-grained specialization (such as object category).", "section": "4.3 Large language/vision \u03bcMoE networks"}, {"figure_path": "bIa03mAtxQ/figures/figures_25_1.jpg", "caption": "Figure 2: Specialization in 256 vs 32 total expert CP\u00b5MoE layers (fine-tuned on CLIP ViT-B-32). Each row displays randomly selected images processed (with coefficient \u2265 0.5) by the first few experts for the two models. The more we scale the expert count, the greater the apparent expert specialism (to single visual themes or image categories).", "description": "This figure shows a comparison of expert specialization in two CP\u00b5MoE models with different numbers of experts (256 and 32) fine-tuned on the CLIP ViT-B-32 model.  Each row presents a subset of images that had activation coefficients of 0.5 or greater for a few experts in each model. The figure demonstrates that increasing the number of experts leads to more specialized experts, where each expert focuses on a narrower set of visual themes or image categories.", "section": "4.1 Expert specialism: visualization & intervention"}, {"figure_path": "bIa03mAtxQ/figures/figures_26_1.jpg", "caption": "Figure 2: Specialization in 256 vs 32 total expert CP\u00b5MoE layers (fine-tuned on CLIP ViT-B-32). Each row displays randomly selected images processed (with coefficient \u2265 0.5) by the first few experts for the two models. The more we scale the expert count, the greater the apparent expert specialism (to single visual themes or image categories).", "description": "This figure shows a comparison of expert specialization in two CP\u00b5MoE models fine-tuned on CLIP ViT-B-32, one with 256 experts and the other with 32 experts.  Each row displays examples of images processed by a subset of the experts, highlighting the increased specialization of experts in the model with a larger number of experts. The images are selected based on having an expert coefficient of at least 0.5, indicating a strong contribution of that expert to the image's processing.  The results suggest that increasing the number of experts leads to more fine-grained specialization, where experts focus on processing images with similar visual themes or categories.", "section": "4.1 Expert specialism: visualization & intervention"}, {"figure_path": "bIa03mAtxQ/figures/figures_26_2.jpg", "caption": "Figure 2: Specialization in 256 vs 32 total expert CP\u00b5MoE layers (fine-tuned on CLIP ViT-B-32). Each row displays randomly selected images processed (with coefficient \u2265 0.5) by the first few experts for the two models. The more we scale the expert count, the greater the apparent expert specialism (to single visual themes or image categories).", "description": "This figure compares the qualitative results of fine-tuning CLIP ViT-B-32 with CP\u00b5MoE layers using 256 and 32 total experts.  Each row shows examples of images processed by the first few experts in each model, highlighting the images with an expert coefficient of at least 0.5. The figure demonstrates that increasing the number of experts leads to greater specialization, with experts focusing on increasingly narrower categories or visual themes.", "section": "4.1 Expert specialism: visualization & intervention"}, {"figure_path": "bIa03mAtxQ/figures/figures_29_1.jpg", "caption": "Figure 16: High vs low total expert count: Randomly selected training set images with expert coefficient \u2265 0.5 for the first 10 numerical experts (of those processing any images with coefficient \u2265 0.5). Results are with CP-r512 \u00b5MoE layers with 256 (left) and 32 (right) total experts respectively. We highlight the apparent specialism of the experts when a higher total number is used. (Please zoom for detail)", "description": "This figure shows a comparison of expert specialization in models with different numbers of experts. The left side shows a model with 256 experts, and the right side shows a model with 32 experts. Each image shows a randomly selected training image that is highly weighted (coefficient \u2265 0.5) by one of the first 10 experts.  The figure demonstrates the increased specialization of the experts with a higher number of experts. With more experts, each tends to focus on images within a more narrow semantic range.", "section": "4.1 Expert specialism: visualization & intervention"}, {"figure_path": "bIa03mAtxQ/figures/figures_30_1.jpg", "caption": "Figure 2: Specialization in 256 vs 32 total expert CP\u00b5MoE layers (fine-tuned on CLIP ViT-B-32). Each row displays randomly selected images processed (with coefficient \u2265 0.5) by the first few experts for the two models. The more we scale the expert count, the greater the apparent expert specialism (to single visual themes or image categories).", "description": "This figure compares the expert specialization in two CP\u00b5MoE models fine-tuned on CLIP ViT-B-32 with different numbers of experts (256 and 32).  Each row shows a subset of images that strongly activate a particular expert (coefficient \u2265 0.5). The images in each row share visual themes or belong to similar categories. The figure demonstrates that increasing the number of experts leads to greater specialization, with each expert focusing on a more specific set of visual concepts.", "section": "4.1 Expert specialism: visualization & intervention"}, {"figure_path": "bIa03mAtxQ/figures/figures_31_1.jpg", "caption": "Figure 16: High vs low total expert count: Randomly selected training set images with expert coefficient \u2265 0.5 for the first 10 numerical experts (of those processing any images with coefficient \u2265 0.5). Results are with CP-r512 \u00b5MoE layers with 256 (left) and 32 (right) total experts respectively. We highlight the apparent specialism of the experts when a higher total number is used. (Please zoom for detail)", "description": "This figure compares the qualitative results of using different numbers of experts in CP\u00b5MoE layers. The left panel shows results with 256 experts, while the right panel shows results with 32 experts. For each expert, a set of images that have an expert coefficient of at least 0.5 is shown. The figure aims to demonstrate that increasing the number of experts leads to more specialized experts, each focusing on a more specific subset of image categories or visual themes.", "section": "4.1 Expert specialism: visualization & intervention"}, {"figure_path": "bIa03mAtxQ/figures/figures_32_1.jpg", "caption": "Figure 16: High vs low total expert count: Randomly selected training set images with expert coefficient \u2265 0.5 for the first 10 numerical experts (of those processing any images with coefficient \u2265 0.5). Results are with CP-r512 \u00b5MoE layers with 256 (left) and 32 (right) total experts respectively. We highlight the apparent specialism of the experts when a higher total number is used. (Please zoom for detail)", "description": "This figure compares the qualitative results of using CP\u00b5MoE layers with different numbers of experts (256 vs 32).  It shows randomly selected images processed by the first ten experts, where the expert coefficient is greater than or equal to 0.5. The images are overlaid with their class labels and expert coefficients. The figure demonstrates that with more experts (256), the experts tend to specialize in processing images from a narrower range of semantic categories, leading to more distinct and specialized subcomputations.  With fewer experts (32), each expert is more likely to be involved in processing images from a wider range of categories, resulting in less specialization.", "section": "4.1 Expert specialism: visualization & intervention"}, {"figure_path": "bIa03mAtxQ/figures/figures_33_1.jpg", "caption": "Figure 20: Softmax vs Entmax ablation CP\u00b5MoE-r512 final layers trained on ImageNET, and the resulting class-level polysemanticity. For large values of experts, the entmax activation produces more specialized experts.", "description": "This figure shows the results of an ablation study comparing the use of the softmax and entmax activation functions in CP\u00b5MoE-r512 final layers trained on the ImageNet dataset.  The x-axis represents the total number of experts used in the CP\u00b5MoE layer (on a logarithmic scale), while the y-axis represents the mean expert polysemanticity.  The plot shows that for larger numbers of experts, the entmax activation function leads to more specialized experts (lower polysemanticity) compared to softmax. Separate lines are shown for models using DINO and CLIP backbones. The results suggest that entmax is a better choice for achieving increased expert specialisation when using a larger number of experts.", "section": "H Ablation studies"}, {"figure_path": "bIa03mAtxQ/figures/figures_33_2.jpg", "caption": "Figure 20: Softmax vs Entmax ablation CP\u00b5MoE-r512 final layers trained on ImageNET, and the resulting class-level polysemanticity. For large values of experts, the entmax activation produces more specialized experts.", "description": "This figure compares the performance of two activation functions, softmax and entmax, in a CP\u00b5MoE layer when fine-tuned on ImageNet. The x-axis represents the number of experts used in the CP\u00b5MoE layer, while the y-axis shows the mean expert polysemanticity.  The plot demonstrates that for a larger number of experts, the entmax activation function leads to experts that are more specialized (monosemantic) compared to the softmax function.", "section": "H Ablation studies"}, {"figure_path": "bIa03mAtxQ/figures/figures_34_1.jpg", "caption": "Figure 22: Expert load: Number of training set images with expert coefficient a_n \u2265 0.5 for CP\u00b5MoE models fine-tuned on ImageNET1k. Bars are drawn with 3x width and colored sequentially in a repeating order of distinct colors to help visually distinguish between neighbors.", "description": "This figure shows the number of training images processed by each expert in the CP\u00b5MoE model fine-tuned on the ImageNet1k dataset. Each bar represents an expert, and its height corresponds to the number of images with an expert coefficient of at least 0.5.  The x-axis represents the expert index, and the y-axis shows the count of images. The bars are colored to visually differentiate between the experts. The purpose of this visualization is to examine the load distribution among experts, and to verify if some experts are overloaded while others are underutilized.", "section": "H.4 Expert load"}, {"figure_path": "bIa03mAtxQ/figures/figures_35_1.jpg", "caption": "Figure 23: Comparative analysis of fine-tuning CLIP ViT-B-32 with \u03bcMoE layers using different configurations. All experiments have the same number of parameters.", "description": "This figure shows the results of fine-tuning the CLIP ViT-B-32 model on the ImageNet1k dataset using different configurations of \u03bcMoE layers.  The left subplot (a) compares the validation accuracy of using \u03bcMoE layers versus linear layers, showing that \u03bcMoE layers achieve higher accuracy with the same number of parameters. The right subplot (b) compares the resulting matrix rank for CP\u03bcMoE and TR\u03bcMoE layers for various expert counts.  This demonstrates that TR\u03bcMoE offers greater efficiency in parameter usage for a larger number of experts.", "section": "4.1 Expert specialism: visualization & intervention"}, {"figure_path": "bIa03mAtxQ/figures/figures_35_2.jpg", "caption": "Figure 23: Comparative analysis of fine-tuning CLIP ViT-B-32 with \u03bcMoE layers using different configurations. All experiments have the same number of parameters.", "description": "This figure shows a comparison of fine-tuning the CLIP ViT-B-32 model on ImageNet using different configurations of \u03bcMoE layers.  The left subplot (a) compares the validation accuracy achieved with \u03bcMoE layers against linear layers, showing that \u03bcMoE layers consistently outperform linear layers across various expert counts. The right subplot (b) compares the resulting matrix rank of CP\u03bcMoE and TR\u03bcMoE layers, illustrating the impact of different factorization choices on model complexity. Both subplots demonstrate that \u03bcMoE layers offer competitive performance with comparable parameter counts.", "section": "4.1 Expert specialism: visualization & intervention"}, {"figure_path": "bIa03mAtxQ/figures/figures_36_1.jpg", "caption": "Figure 23: Comparative analysis of fine-tuning CLIP ViT-B-32 with \u03bcMoE layers using different configurations. All experiments have the same number of parameters.", "description": "This figure presents a comparative analysis of fine-tuning the CLIP ViT-B-32 model using \u03bcMoE layers with varying configurations.  The left subplot shows a comparison of the validation accuracy achieved with \u03bcMoE layers versus a standard linear layer, demonstrating the performance gains obtained with \u03bcMoE. The right subplot compares the rank of the weight matrices for CP\u03bcMoE and TR\u03bcMoE models as the number of experts is increased. This illustrates the computational efficiency and parameter control offered by TR\u03bcMoE for a large number of experts.", "section": "4.1 Expert specialism: visualization & intervention"}, {"figure_path": "bIa03mAtxQ/figures/figures_38_1.jpg", "caption": "Figure 2: Specialization in 256 vs 32 total expert CP\u00b5MoE layers (fine-tuned on CLIP ViT-B-32). Each row displays randomly selected images processed (with coefficient > 0.5) by the first few experts for the two models. The more we scale the expert count, the greater the apparent expert specialism (to single visual themes or image categories).", "description": "This figure shows a comparison of expert specialization in two CP\u00b5MoE models fine-tuned on the CLIP ViT-B-32 architecture with different numbers of experts (256 and 32). It presents randomly selected images processed by the top experts in each model, highlighting how increasing the expert count leads to more specialized experts focusing on specific visual themes or image categories. The images are shown with their corresponding expert coefficients. In the model with 256 experts, there is clear specialization of the experts towards specific image classes while the model with 32 experts processes images from various classes, indicating less specialization.", "section": "4.1 Expert specialism: visualization & intervention"}, {"figure_path": "bIa03mAtxQ/figures/figures_38_2.jpg", "caption": "Figure 2: Specialization in 256 vs 32 total expert CP\u00b5MoE layers (fine-tuned on CLIP ViT-B-32). Each row displays randomly selected images processed (with coefficient \u2265 0.5) by the first few experts for the two models. The more we scale the expert count, the greater the apparent expert specialism (to single visual themes or image categories).", "description": "This figure compares the qualitative results of fine-tuning CLIP ViT-B-32 with CP\u00b5MoE layers having 256 and 32 experts respectively.  For each expert, a selection of images with expert coefficients greater than or equal to 0.5 are displayed. The figure demonstrates that increasing the number of experts leads to more specialized experts. In the model with 32 experts, individual experts are more likely to process images from different semantic categories (polysemantic), while the model with 256 experts shows that the experts mostly process images belonging to similar categories or visual themes (monosemantic).", "section": "4.1 Expert specialism: visualization & intervention"}]