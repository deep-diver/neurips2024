{"references": [{"fullname_first_author": "Robert A Jacobs", "paper_title": "Adaptive mixtures of local experts", "publication_date": "1991-01-01", "reason": "This paper introduced the Mixture of Experts (MoE) architecture, which is the foundation of the proposed \u00b5MoE model."}, {"fullname_first_author": "Noam Shazeer", "paper_title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "publication_date": "2017-01-01", "reason": "This paper introduced the Sparse MoE, a highly influential method for scaling MoEs, which the \u00b5MoE aims to improve upon."}, {"fullname_first_author": "Dmitry Lepikhin", "paper_title": "GShard: Scaling giant models with conditional computation and automatic sharding", "publication_date": "2021-01-01", "reason": "This paper introduced GShard, a technique for training extremely large language models that utilizes conditional computation, a concept also relevant to MoEs."}, {"fullname_first_author": "William Fedus", "paper_title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity", "publication_date": "2022-01-01", "reason": "This work is highly relevant as it further demonstrates the trend towards scaling up large language models with sparsity, a theme that is central to the \u00b5MoE's design."}, {"fullname_first_author": "Trevor Gale", "paper_title": "Megablocks: Efficient sparse training with mixture-of-experts", "publication_date": "2023-01-01", "reason": "This paper proposes Megablocks, another recent advance in efficient sparse training of MoEs, whose challenges the \u00b5MoE seeks to overcome."}]}