[{"figure_path": "PLbFid00aU/figures/figures_5_1.jpg", "caption": "Figure 1: Controlling the neural collapse via the model geometric complexity for VGG-13 trained on CIFAR-10. Lower embedding GC produces lower geometric collapse (Eq. 7) and more neural collapse (i.e., lower NC) for Top row: increased learning rates, Middle row: decreased batch sizes, and Bottom row: increased L2 regularization.", "description": "This figure shows the impact of different hyperparameters on the relationship between geometric complexity (GC) and neural collapse (NC) in a VGG-13 model trained on CIFAR-10.  Three sets of experiments are presented, each varying a different hyperparameter:\n\n* **Learning Rate:** Shows that higher learning rates lead to higher GC and less neural collapse.\n* **Batch Size:** Demonstrates that smaller batch sizes result in lower GC and more neural collapse.\n* **L2 Regularization:** Illustrates how stronger L2 regularization reduces GC and enhances neural collapse. \n\nThe results support the paper's claim that lower GC promotes better neural collapse, improving downstream task performance.", "section": "4 Geometric complexity and neural collapse"}, {"figure_path": "PLbFid00aU/figures/figures_6_1.jpg", "caption": "Figure 2: The GC computation is robust and consistent to sampling via Left: number of examples in the batch, Middle: number of elements in the Jacobian, or Right: by sampling the embedding dimension of the model. Here the GC and subnet GC have been computed over 20 trials, plotting the mean and standard deviations for a ResNet-18 model that has been trained to convergence on CIFAR-100. The true value of the GC for each setting is indicated by dotted line.", "description": "This figure demonstrates the robustness and reliability of the geometric complexity (GC) measure when computed using different sampling methods.  Three scenarios are tested: sampling by batch size (number of examples), sampling by masking portions of the Jacobian matrix, and sampling by randomly selecting output dimensions.  The results show that even with these sampling techniques, the empirical GC remains consistent with the true GC, indicating its computational efficiency and reliability as a complexity measure.", "section": "4.2 The geometric complexity is a reliable and robust measure"}, {"figure_path": "PLbFid00aU/figures/figures_7_1.jpg", "caption": "Figure 3: VGG-13 trained on CIFAR-10 with 5 random seeds.", "description": "This figure shows the relationship between the geometric complexity and the test error of a VGG-13 model trained on CIFAR-10. The x-axis represents the number of iterations during training, and the y-axis represents the generalization bound (LHS) and the test error of the nearest-mean classifier (RHS). The plot demonstrates that the generalization bound is not vacuous and provides a relatively tight fit, indicating the effectiveness of the geometric complexity in bounding the generalization error. The figure also displays the results from 5 separate training runs with different random seeds, showcasing the robustness of the relationship between geometric complexity and generalization performance.", "section": "4.3 A new generalization bound with GC through NC"}, {"figure_path": "PLbFid00aU/figures/figures_9_1.jpg", "caption": "Figure 4: Controlling target neural collapse through source GC on CIFAR-FS with ResNet-18. Lower Source GC produces more neural collapse on target classes (i.e., lower target NC), and higher 5-shot transfer accuracy for Top row: increased learning rates, Middle row: decreased batch sizes, and Bottom row: increased L2 regularization.", "description": "This figure shows the results of experiments on CIFAR-FS using ResNet-18, demonstrating the relationship between source geometric complexity (GC), target neural collapse (NC), and 5-shot transfer accuracy.  Three sets of experiments are presented, each manipulating a different hyperparameter: learning rate, batch size, and L2 regularization.  The results consistently show that lower source GC leads to lower target NC and improved 5-shot accuracy.", "section": "5.1 Lower pre-trained GC leads to improved fine-tuning"}, {"figure_path": "PLbFid00aU/figures/figures_17_1.jpg", "caption": "Figure 1: Controlling the neural collapse via the model geometric complexity for VGG-13 trained on CIFAR-10. Lower embedding GC produces lower geometric collapse (Eq. 7) and more neural collapse (i.e., lower NC) for Top row: increased learning rates, Middle row: decreased batch sizes, and Bottom row: increased L2 regularization.", "description": "This figure demonstrates the relationship between geometric complexity and neural collapse in a VGG-13 model trained on CIFAR-10.  It shows how manipulating training hyperparameters (learning rate, batch size, L2 regularization) affects the geometric complexity (GC) and neural collapse (NC). Lower GC correlates with lower geometric collapse and higher neural collapse, indicating that controlling GC is a way to influence the neural collapse phenomenon and potentially improve model performance.", "section": "4 Geometric complexity and neural collapse"}, {"figure_path": "PLbFid00aU/figures/figures_18_1.jpg", "caption": "Figure 1: Controlling the neural collapse via the model geometric complexity for VGG-13 trained on CIFAR-10. Lower embedding GC produces lower geometric collapse (Eq. 7) and more neural collapse (i.e., lower NC) for Top row: increased learning rates, Middle row: decreased batch sizes, and Bottom row: increased L2 regularization.", "description": "This figure shows how controlling the geometric complexity of a VGG-13 model trained on CIFAR-10 affects neural collapse.  Three sets of experiments are presented, each varying a different hyperparameter: learning rate, batch size, and L2 regularization.  The results demonstrate that lower geometric complexity leads to lower geometric collapse and more pronounced neural collapse. This highlights the relationship between geometric complexity and neural collapse.", "section": "4.1 Geometric complexity controls neural collapse"}, {"figure_path": "PLbFid00aU/figures/figures_19_1.jpg", "caption": "Figure 7: Controlling the neural collapse via the model geometric complexity for VGG-11 trained on MNIST. Lower GC produces lower geometric collapse and more neural collapse (i.e., lower NC) for Top row: increased learning rates, Middle row: decreased batch sizes, and Bottom row: increased L2 regularization.", "description": "This figure shows the effects of different hyperparameters (learning rate, batch size, L2 regularization) on the geometric complexity, geometric collapse, and neural collapse during the training of a VGG-11 neural network on the MNIST dataset.  The results demonstrate that lower geometric complexity (GC), achieved through various regularization techniques, leads to lower geometric collapse and increased neural collapse (i.e., lower NC values, indicating better class separation in the embedding space).  This suggests that controlling geometric complexity can be a useful way to influence the final performance of a model.", "section": "A.5.2 MNIST on VGG-11"}, {"figure_path": "PLbFid00aU/figures/figures_20_1.jpg", "caption": "Figure 1: Controlling the neural collapse via the model geometric complexity for VGG-13 trained on CIFAR-10. Lower embedding GC produces lower geometric collapse (Eq. 7) and more neural collapse (i.e., lower NC) for Top row: increased learning rates, Middle row: decreased batch sizes, and Bottom row: increased L2 regularization.", "description": "This figure shows the relationship between geometric complexity and neural collapse during the training of a VGG-13 network on the CIFAR-10 dataset.  It demonstrates how manipulating training hyperparameters (learning rate, batch size, and L2 regularization) affects the geometric complexity and, consequently, the neural collapse. Lower geometric complexity is associated with increased neural collapse, as indicated by the lower CDNV values. The figure presents results across various settings, showing a consistent trend of lower geometric complexity leading to increased neural collapse and potentially improved generalization.", "section": "4 Geometric complexity and neural collapse"}, {"figure_path": "PLbFid00aU/figures/figures_21_1.jpg", "caption": "Figure 1: Controlling the neural collapse via the model geometric complexity for VGG-13 trained on CIFAR-10. Lower embedding GC produces lower geometric collapse (Eq. 7) and more neural collapse (i.e., lower NC) for Top row: increased learning rates, Middle row: decreased batch sizes, and Bottom row: increased L2 regularization.", "description": "This figure shows how the geometric complexity (GC) of a VGG-13 model, trained on CIFAR-10, affects neural collapse (NC).  The plots demonstrate that lower embedding GC leads to lower geometric collapse and increased NC. This effect is shown across three different scenarios: varying learning rates, batch sizes, and L2 regularization.  Each row represents one of these scenarios, illustrating the consistent relationship between GC and NC under different training conditions.", "section": "4 Geometric complexity and neural collapse"}, {"figure_path": "PLbFid00aU/figures/figures_21_2.jpg", "caption": "Figure 4: Controlling target neural collapse through source GC on CIFAR-FS with ResNet-18. Lower Source GC produces more neural collapse on target classes (i.e., lower target NC), and higher 5-shot transfer accuracy for Top row: increased learning rates, Middle row: decreased batch sizes, and Bottom row: increased L2 regularization.", "description": "This figure shows the results of an experiment where the researchers investigated the effect of source geometric complexity (GC) on target neural collapse and transfer learning performance.  Using a ResNet-18 model on the CIFAR-FS dataset, they manipulated three factors: learning rate, batch size, and L2 regularization, observing their effects on both source GC (during pre-training) and target neural collapse (during fine-tuning).  The plots illustrate that lower source GC correlates with lower target neural collapse, leading to better 5-shot accuracy on the target task.", "section": "5.2 Improve fine-tuning by controlling pre-trained GC"}, {"figure_path": "PLbFid00aU/figures/figures_22_1.jpg", "caption": "Figure 11: VGG-13 on CIFAR-10 with explicit GC regularization (one seed).", "description": "This figure shows the results of training a VGG-13 model on CIFAR-10 dataset with explicit geometric complexity (GC) regularization. The experiment uses a fixed learning rate of 0.01 and a batch size of 256.  Different levels of GC regularization (1e-07, 1e-06, 0.0001) are applied. The figure presents the training curves for three metrics: Train Geometric Complexity, Train Geometric Collapse, and Train Neural Collapse.  It demonstrates that increasing the amount of GC regularization leads to lower values for these three metrics.", "section": "A.5.6 Direct GC regularization increases neural collapse"}]