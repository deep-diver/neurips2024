[{"figure_path": "3sWghzJvGd/figures/figures_7_1.jpg", "caption": "Figure 1: Generalization against increasing degree of perturbation.", "description": "This figure shows the generalization performance of models with and without Jacobian regularization against different types of perturbations.  The x-axes represent the increasing degree of perturbation in four different scenarios: Gaussian noise with mean 0.5 and varying variance, Gaussian noise with mean 0 and varying variance, increasing percentage of masked noise, and rotation degrees. The y-axes represent the reward obtained by the models. The figure demonstrates that the models with Jacobian regularization consistently achieve higher rewards than the baseline models, indicating improved generalization.", "section": "5 Experimental Studies"}, {"figure_path": "3sWghzJvGd/figures/figures_13_1.jpg", "caption": "Figure 2: Latent Representation Problem: The left and right denote the manifold M with lower dim dm embedded in a larger Euclidean space, with latent space Z a dm-dimensional ball in middle. Encoder and decoder as maps respectively pushing forward Q to P and P to Q.", "description": "This figure illustrates the latent representation problem discussed in the paper.  A low-dimensional manifold M (representing the true state space) is embedded within a higher-dimensional space R<sup>Ns</sup> (the observed space). The probability distribution Q resides on this manifold. An encoder maps points from M to a lower-dimensional latent space Z, where the probability distribution P is located.  A decoder then reconstructs approximations of points on M from the latent space Z. The goal is to find encoder and decoder mappings that minimize the difference between the original distribution Q and the reconstructed distribution Q, effectively compressing the information from the high-dimensional state space to a lower-dimensional latent representation.", "section": "3 Demystifying World Model: A Stochastic Differential Equation Approach"}, {"figure_path": "3sWghzJvGd/figures/figures_29_1.jpg", "caption": "Figure 4: Walker. Eval (left) and train scores (right) under latent error process \u03bct ~ [0, 5], \u03c3t ~ [0, 5].", "description": "This figure shows the evaluation and training scores for the Walker task under latent error process.  The left plot displays evaluation scores, and the right plot shows training scores. Two lines are plotted in each graph, one for the model with Jacobian regularization and one without.  The plots show the performance of the model over training steps, indicating how Jacobian regularization affects both evaluation and training performance under non-zero drift latent representation error.", "section": "D.5.1 Walker Task"}, {"figure_path": "3sWghzJvGd/figures/figures_29_2.jpg", "caption": "Figure 7: Extended horizon Walker task. Eval (left) and train scores (right).", "description": "This figure shows the evaluation and training scores for the Walker task with an extended horizon (increased from 50 to 100 steps).  It compares the performance of models with and without Jacobian regularization, demonstrating that models using Jacobian regularization converge significantly faster, highlighting the method's effectiveness in reducing error propagation over longer time horizons.", "section": "D.6 Additional Results on Faster convergence on tasks with extended horizon."}, {"figure_path": "3sWghzJvGd/figures/figures_30_1.jpg", "caption": "Figure 1: Generalization against increasing degree of perturbation.", "description": "This figure compares the performance of a model with Jacobian regularization against a baseline model across three types of perturbations: Gaussian noise, rotation, and masked noise.  The x-axis represents the increasing degree of perturbation (variance of Gaussian noise, rotation angle, or percentage of masked pixels), and the y-axis shows the reward values.  The results indicate that the model with Jacobian regularization demonstrates significantly better generalization capabilities in the face of unseen perturbed states compared to the baseline model.  The results are consistent across all three perturbation methods.", "section": "Experimental Studies"}, {"figure_path": "3sWghzJvGd/figures/figures_30_2.jpg", "caption": "Figure 1: Generalization against increasing degree of perturbation.", "description": "This figure visualizes the generalization performance of models with and without Jacobian regularization against various levels of perturbation. Three types of perturbations are considered: Gaussian noise across the full image, rotation, and noise applied to a percentage of the image. For each perturbation type, different levels are tested and the resulting rewards are plotted for both models. The figure demonstrates that the models with Jacobian regularization maintain better generalization performance across a wide range of perturbation levels. The results support the findings in Corollary 3.8 that input-output Jacobian norm could stabilize the induced implicit regularization.", "section": "Experimental Studies"}, {"figure_path": "3sWghzJvGd/figures/figures_31_1.jpg", "caption": "Figure 7: Extended horizon Walker task. Eval (left) and train scores (right).", "description": "This figure shows the evaluation and training scores for the MuJoCo Walker task with an extended horizon (100 steps instead of the original 50 steps).  It compares the performance of models trained with Jacobian regularization (with \u03bb=0.05 and \u03bb=0.1) against a baseline model without Jacobian regularization. The results demonstrate that models with Jacobian regularization converge significantly faster, especially the model with \u03bb=0.05, highlighting the effectiveness of Jacobian regularization in reducing error propagation over longer time horizons.", "section": "D.6 Additional Results on Faster convergence on tasks with extended horizon"}]