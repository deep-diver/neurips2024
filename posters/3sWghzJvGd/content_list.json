[{"type": "text", "text": "Towards Unraveling and Improving Generalization in World Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 World models have recently emerged as a promising approach for reinforcement   \n2 learning (RL), as evidenced by its stimulating successes that world model based   \n3 agents achieve state-of-the-art performance on a wide range of tasks in empirical   \n4 studies. The primary goal of this study is to obtain a deep understanding of the mys  \n5 terious generalization capability of world models, based on which we devise new   \n6 methods to enhance it further. Thus motivated, we develop a stochastic differential   \n7 equation formulation by treating the world model learning as a stochastic dynamic   \n8 system in the latent state space, and characterize the impact of latent representation   \n9 errors on generalization, for both cases with zero-drift representation errors and   \n10 with non-zero-drift representation errors. Our somewhat surprising findings, based   \n11 on both theoretic and experimental studies, reveal that for the case with zero drift,   \n12 modest latent representation errors can in fact function as implicit regularization   \n13 and hence result in generalization gain. We further propose a Jacobian regulariza  \n14 tion scheme to mitigate the compounding error propagation effects of non-zero   \n15 drift, thereby enhancing training stability and generalization. Our experimental   \n16 results corroborate that this regularization approach not only stabilizes training but   \n17 also accelerates convergence and improves performance on predictive rollouts. ", "page_idx": 0}, {"type": "text", "text": "18 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "19 Model-based reinforcement learning (RL) has emerged as a promising learning paradigm to improve   \n20 sample efficiency by enabling agents to exploit a learned model for the physical environment. Notably,   \n21 in recent works [14, 13, 15, 16, 21, 10, 32, 22] on world models, an RL agent learns the latent   \n22 dynamics model of the environment, based on the observations and action signals, and then optimizes   \n23 the policy over the learned dynamics model. Different from conventional approaches, world-model   \n24 based RL takes an end-to-end learning approach, where the building blocks (such as dynamics model,   \n25 perception and action policy) are trained and optimized to achieve a single overarching goal, offering   \n26 significant potential to improve generalization capability. For example, DreamerV2 and DreamerV3   \n2 achieve great progress in mastering diverse tasks involving continuous and discrete actions, image  \n2 based inputs, and both 2D and 3D environments, thereby facilitating robust learning across unseen   \n29 task domains [14, 13, 15]. Recent empirical studies have also demonstrated the capacity of world   \n30 models to generalize to unseen states in complex environments, such as autonomous driving [19].   \n31 Nevertheless, it remains not well understood when and how world models can generalize well in   \n32 unseen environments.   \n33 In this work, we aim to first obtain a deep understanding of the generalization capability of world   \n34 models by examining the impact of latent representation errors, and then to devise new methods to   \n35 enhance its generalization. While one may expect that optimizing a latent dynamics model (LDM)   \n36 prior to training the task policy would minimize latent representation errors and hence can achieve   \n37 better world model training, our somewhat surprising findings, based on both theoretical and empirical   \n38 studies, reveal that modest latent representation errors in the training phase may in fact be beneficial.   \n39 In particular, the alternating training strategy for world model learning, which simultaneously refines   \n40 both the LDM and the action policy, could actually bring generalization gain, because the modest   \n41 latent representation errors (and the corresponding induced gradient estimation errors) could enable   \n42 the world model to visit unseen states and thus lead to improved generalization capacities. For   \n43 instance, as shown in Table 1, our experimental results suggest that moderate batch sizes (e.g., 16 or   \n44 32) appear to position the induced errors within a regime conferring notable generalization benefits,   \n45 leading to higher generalization improvement, when compared to the cases with very small (e.g., 8)   \n46 or large (e.g., 64) batch sizes.   \n47 In a nutshell, latent representation errors incurred by latent encoders, if designed properly, may   \n48 actually facilitate world model training and enhance generalization. This insight aligns with recent   \n49 advances in deep learning, where noise injection schemes have been studied as a form of implicit   \n50 regularization to enhance models\u2019 robustness. For instance, recent study [2] analyzes the effects of   \n51 introducing isotropic Gaussian noise at each layer of neural networks, identifying it as a form of   \n52 implicit regularization. Another recent work [27] explores the addition of zero-drift Brownian motion   \n53 to RNN architectures, demonstrating its regularizing effects in improving network\u2019s stability against   \n54 noise perturbations.   \n55 We caution that latent representation errors in world models differ from the above noise injection   \n56 schemes ([27, 2]), in the following aspects: 1) Unlike the artificially injected noise only added in   \n57 training, these errors are inherent in world models, leading to error propagation in the rollouts; 2)   \n58 Unlike the controlled conditions of isotropic or zero-drift noise examined in prior studies, the errors   \n59 in world models may not exhibit such well-behaved properties in the sense that the drift may be   \n60 non-zero and hence biased; 3) additionally, in the iterative training of world models and agents, the   \n61 error originating from the encoder affects the policy learning and agent exploration. In light of these   \n62 observations, we develop a continuous-time stochastic differential equation (SDE) formulation by   \n63 treating the world model learning as a stochastic dynamic system with stochastic latent states. This   \n64 approach offers an insightful view on model errors as stochastic perturbation, enabling us to obtain   \n65 an explicit characterization to quantify the impacts of the errors on world models\u2019 generalization   \n66 capability. Our main contributions can be summarized as follows.   \n67 \u2022 Latent representation errors as implicit regularization: Aiming to understand the generalization   \n68 capability of world models and improve it further, we develop a continuous-time SDE formula  \n69 tion by treating the world model learning as a stochastic dynamic system in latent state space.   \n70 Leveraging tools in stochastic calculus and differential geometry, we characterize the impact   \n71 of latent representation errors on world models\u2019 generalization. Our findings reveal that under   \n72 some technical conditions, modest latent representation errors can in fact function as implicit   \n73 regularization and hence result in generalization gain.   \n74 \u2022 Improving generalization in non-zero drift cases via Jacobian regularization: For the case where   \n75 latent representation errors exhibit non-zero drifts, we show that the additional bias term would   \n76 degrade the implicit regulation and hence may make the learning unstable. We propose to add   \n77 Jacobian regularization to mitigate the effects of non-zero-drift errors in training. Experimental   \n78 studies are carried out to evaluate the efficacy of Jacobian regularization.   \n79 \u2022 Reducing error propagation in predictive rollouts: We explicitly characterize the effect of latent   \n80 representation errors on predictive rollouts. Our experimental results corroborate that Jacobian   \n81 regularization can reduce the impact of error propagation on rollouts, leading to enhanced   \n82 prediction performance and accelerated convergence in tasks with longer time horizons.   \n83 \u2022 Bounding Latent Representation Error: We establish a novel bound on the latent representation   \n84 error within CNN encoder-decoder architectures. To our knowledge, this is the first quantifiable   \n85 bound applied to a learned latent representation model, and the analysis carries over to other   \n86 architectures (e.g., ReLU) along the same line.   \n87 Notation. We use Einstein summation convention for succinctness, where $a_{i}b_{i}$ denotes $\\sum_{i}a_{i}b_{i}$ . We   \n88 denote functions in $\\mathcal{C}^{k,\\alpha}$ as being $k$ -times differentiable with $\\alpha$ -H\u00f6lder continuity. T he Euclidean   \n89 norm of a vector is represented by $\\|\\cdot\\|$ , and the Frobenius norm of a matrix by $|\\cdot|_{F}$ ; this notation   \n90 may occasionally extend to tensors. The notation $x^{i}$ indicates the $i^{t h}$ coordinate of the vector $x$ , and   \n91 $A^{i j}$ the $(i,j)$ -entry of the matrix $A$ . Function composition is denoted by $f\\circ g$ , implying $f(g)$ . For a   \n92 differentiable function $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{m}$ , its Jacobian matrix is denoted by $\\frac{\\partial f}{\\partial x}\\in\\mathbb{R}^{m\\times n}$ . Its gradient,   \n93 following conventional definitions, is denoted by $\\nabla f$ . The constant $C$ may represent different values   \n94 in distinct contexts. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "table", "img_path": "3sWghzJvGd/tmp/adb1776c54a6b97a8bcede08ee63b15a2ff82259df457fb4960de41015e4f868.jpg", "table_caption": [], "table_footnote": ["Table 1: Reward values on unseen perturbed states by rotation $\\overline{{(\\alpha)}}$ or mask $\\overline{{(\\beta\\%)}}$ with $\\overline{{\\mathcal{N}(0.15,0.5)}}$ . "], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "95 2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "96 World model based RL. World models have demonstrated remarkable efficacy in visual control   \n97 tasks across various platforms, including Atari [1] and Minecraft [8], as detailed in the studies by   \n98 Hafner et al. [14, 13, 15]. These models typically integrate encoders and memory-augmented neural   \n99 networks, such as RNNs [33], to manage the latent dynamics. The use of variational autoencoders   \n100 (VAE) [7, 23] to map sensory inputs to a compact latent space was pioneered by Ha et al. [12].   \n101 Furthermore, the Dreamer algorithm [13, 16] employs convolutional neural networks (CNNs) [24] to   \n102 enhance the processing of both hidden states and image embeddings, yielding models with improved   \n103 predictive capabilities in dynamic environments.   \n104 Continuous-time RNNs. The continuous-time assumption is standard for theoretical formulations   \n105 of RNN models. Li et al. [26] study the optimization dynamics of linear RNNs on memory decay.   \n106 Chang et al. [4] propose AntisymmetricRNN, which captures long-term dependencies through the   \n107 control of eigenvalues in its underlying ODE. Chen et al. [5] propose the symplectic RNN to model   \n108 Hamiltonians. As continuous-time formulations can be discretized with Euler methods [4, 5] (or with   \n109 Euler-Maruyama methods if stochastic in [27]) and yield similar insights, this step is often eliminated   \n110 for brevity.   \n111 Implicit regularization by noise injection in RNN. Studies on noise injection as a form of implicit   \n112 regularization have gained traction, with Lim et al. [27] deriving an explicit regularizer under small   \n113 noise conditions, demonstrating bias towards models with larger margins and more stable dynamics.   \n114 Camuto et al. [2] examine Gaussian noise injections at each layer of neural networks. Similarly, Wei   \n115 et al. [31] provide analytic insights into the dual effects of dropout techniques. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "116 3 Demystifying World Model: A Stochastic Differential Equation Approach ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "117 As pointed out in [14, 13, 15, 16], critical to the effectiveness of the world model representation is   \n118 the stochastic design of its latent dynamics model. The model can be outlined by the following key   \n119 components: an encoder that compresses high dimensional observations $s_{t}$ into a low-dimensional   \n120 latent state $z_{t}$ (Eq.1), a sequence model that captures temporal dependencies in the environment   \n121 (Eq.2), a transition predictor that estimates the next latent state (Eq.3), and a latent decoder that   \n122 reconstructs observed information from the posterior (Eq.4): ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{Latent\\,Encoder!}\\;z_{t}\\sim q_{\\mathrm{enc}}(z_{t}\\,|\\,h_{t},s_{t}),}\\\\ &{\\quad\\mathrm{Sequence\\,Model!}\\;h_{t}=f\\big(h_{t-1},z_{t-1},a_{t-1}\\big),}\\\\ &{\\mathrm{Transition\\,Predictor!}\\;\\tilde{z}_{t}\\sim p(\\tilde{z_{t}}\\,|\\,h_{t}),}\\\\ &{\\quad\\quad\\mathrm{Latent\\,Decoder!}\\;\\tilde{s_{t}}\\sim q_{\\mathrm{dec}}\\big(\\tilde{s_{t}}\\,|\\,h_{t},\\tilde{z_{t}}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "123 In this work, we consider a popular class of world models, including Dreamer and PlaNet, where $\\{z,$ ,   \n124 $\\tilde{z},\\tilde{s}\\}$ have distributions parameterized by neural networks\u2019 outputs, and are Gaussian when the outputs   \n125 are known. It is worth noting that $\\{z,\\tilde{z},\\tilde{s}\\}$ may not be Gaussian and are non-Gaussian in general.   \n126 This is because while $z$ is conditional Gaussian, its mean and variance are random variables which   \n127 are learned by the encoder with $s$ and $h$ being the inputs, rendering that $z$ is non-Gaussian due to the   \n128 mixture effect. For this setting, we have a continuous-time formulation where the latent dynamics   \n129 model can be interpreted as stochastic differential equations (SDEs) with coefficient functions of   \n130 known inputs. Due to space limitation, we refer to Proposition B.1 in the Appendix for a more   \n131 detailed treatment.   \n132 Consider a complete, flitered probability space $(\\Omega,\\,\\mathcal{F},\\,\\{\\mathcal{F}_{t}\\}_{t\\in[0,T]},\\,\\mathbb{P}\\,)$ where independent standard   \n133 Brownian motions $B_{t}^{\\mathrm{{enc}}}$ , $B_{t}^{\\mathrm{\\,pred}},B_{t}^{\\mathrm{\\,seq}}$ , $B_{t}^{\\mathrm{{dec}}}$ are defined such that $\\mathcal{F}_{t}$ is their augmented flitration, and   \n134 $T\\in\\mathbb{R}$ as the time length of the task environment. We interpret the stochastic dynamics of LDM   \n135 with latent representation errors through coupled SDEs representing continuous-time analogs of the   \n136 discrete components: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d\\,z_{t}=(q_{\\mathrm{enc}}(h_{t},s_{t})+\\varepsilon\\,\\sigma(h_{t},s_{t}))\\ d t+(\\bar{q}_{\\mathrm{enc}}(h_{t},s_{t})+\\varepsilon\\,\\bar{\\sigma}(h_{t},s_{t}))\\,d B_{t}^{\\mathrm{{enc}}},}\\\\ &{d\\,h_{t}=f(h_{t},z_{t},\\pi(h_{t},z_{t}))\\,d t+\\bar{f}(h_{t},z_{t},\\pi(h_{t},z_{t}))\\,d B_{t}^{\\mathrm{{seq}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Transition Predictor: $d\\,\\tilde{z}_{t}=p(h_{t})\\,d t+\\bar{p}(h_{t})\\,d B_{t}^{\\mathrm{\\,pred}}$ , ", "page_idx": 3}, {"type": "text", "text": "137 where $\\pi(h,\\tilde{z})$ is a policy function as a local maximizer of value function and the stochastic process   \n138 $s_{t}$ is $\\mathcal{F}_{t}$ -adapted. Notice that $\\bar{f}$ is often a zero function indicating that Equation (6) is an ODE,   \n139 as the sequence model is generally designed as deterministic. Generally, the coefficient functions   \n140 in $d t$ and $d B_{t}$ terms in SDEs are referred to as the drift and diffusion coefficients. Intuitively, the   \n141 diffusion coefficients here represent the stochastic model components. In Equation (5), $\\sigma(\\cdot,\\cdot)$ and   \n142 $\\bar{\\sigma}(\\cdot,\\cdot)$ denotes the drift and diffusion coefficients of the latent representation errors, respectively.   \n143 Both are assumed to be functions of hidden states $h_{t}$ and task states $s_{t}$ . In addition, $\\varepsilon$ indicates the   \n144 magnitude of the error.   \n145 Next, we impose standard assumptions on these SDEs (5) - (8) to guarantee the well-definedness of   \n146 the solution to SDEs. For further technical details, we refer readers to fundamental works on SDEs in   \n147 the literature (e.g.,[30, 17]).   \n148 Assumption 3.1. The drift coefficient functions $q_{\\mathrm{enc}}$ , $f,\\,p$ and $q_{\\mathrm{dec}}$ and the diffusion coefficient   \n149 functions $\\bar{q}_{\\mathrm{enc}}$ , $\\bar{p}$ and $\\bar{q}_{\\mathrm{dec}}$ are bounded and Borel-measurable over the interval $[0,T]$ , and of class $\\mathcal{C}^{3}$   \n150 with bounded Lipschitz continuous partial derivatives. The initial values $z_{0},h_{0},\\tilde{z}_{0},\\tilde{s}_{0}$ are square  \n151 integrable random variables.   \n152 Assumption 3.2. $\\sigma$ and $\\bar{\\sigma}$ are bounded and Borel-measurable and are of class $\\mathcal{C}^{3}$ with bounded   \n153 Lipschitz continuous partial derivatives over the interval $[0,T]$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "154 3.1 Latent Representation Errors in CNN Encoder-Decoder Networks ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "155 As shown in the empirical studies with different batch sizes (Table 1), the latent representation error   \n156 would also enrich generalization when it is within a moderate regime. In this section, we show that   \n157 the latent representation error, in the form of approximation error corresponding to widely used CNN   \n158 encoder-decoder, could be made sufficiently small by finding appropriate CNN network configuration.   \n159 In particular, this result provides theoretical justification to interpreting latent representation error as   \n160 stochastic perturbation in the dynamical system defined in Equations (5 - 8), as the error magnitude $\\varepsilon$   \n161 can be made sufficiently small by CNN network configuration.   \n162 Consider the state space $S\\subset\\mathbb{R}^{d_{S}}$ and the latent space $\\mathcal{Z}$ . Consider a state probability measure $Q$ on   \n163 the state space $\\boldsymbol{S}$ and a probability measure $P$ on the latent space $\\mathcal{Z}$ . As high-dimensional state space   \n164 in image-based tasks frequently exhibit intrinsic lower-dimensional geometric structure, we adopt   \n165 the latent manifold assumption, formally stated as follows:   \n166 Assumption 3.3. (Latent manifold assumption) For a positive integer $k$ , there exists a $d_{\\mathcal{M}}$ -   \n167 dimensional $\\mathcal{C}^{k,\\alpha}$ submanifold $\\mathcal{M}$ (with $\\mathcal{C}^{\\acute{k}+3,\\alpha}$ boundary) with Riemannian metric $g$ and has   \n168 positive reach and also isometrically embedded in the state space $S\\subset\\mathbb{R}^{d_{S}}$ and $d_{\\mathcal{M}}\\ll d_{S}$ , where   \n169 the state probability measure is supported on. In addition, $\\mathcal{M}$ is a compact, orientable, connected   \n170 manifold.   \n171 Assumption 3.4. (Smoothness of state probability measure) $Q$ is a probability measure supported on   \n172 $\\mathcal{M}$ with its Radon-Nikodym derivative $q\\in\\mathcal{C}^{k,\\alpha}(\\mathcal{M},\\mathbb{R})$ w.r.t $\\mu_{\\mathcal{M}}$ .   \n173 Let $\\mathcal{Z}$ be a closed ball in $\\mathbb{R}^{d_{\\mathcal{M}}}$ , that is $\\{x\\in\\mathbb{R}^{d_{\\mathcal{M}}}\\,:\\,\\|x\\|\\leq1\\}$ . $P$ is a probability measure supported   \n174 on $\\mathcal{Z}$ with its Radon-Nikodym derivative $p\\in\\mathcal{C}^{k,\\alpha}(\\mathcal{Z},\\mathbb{R})$ w.r.t $\\mu{z}$ . In practice, it is usually an easy  \n175 to-sample distribution such as uniform distribution which is determined by a specific encoder-decoder   \n176 architecture choice. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Latent Representation Learning. We define the latent representation learning as to find encoder $g_{\\mathrm{enc}}:\\mathcal{M}\\overset{=}{\\rightarrow}\\mathcal{Z}$ and decoder $g_{\\mathrm{dec}}:\\mathcal{Z}\\to\\mathcal{M}$ as maps that optimize the following objectives: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{g_{\\mathrm{enc}}\\in\\mathcal{G}}W_{1}\\left(g_{\\mathrm{enc}_{\\#}}\\,Q,\\,P\\right);\\qquad\\operatorname*{min}_{g_{\\mathrm{dec}}\\in\\mathcal{G}}W_{1}\\left(Q,\\,g_{\\mathrm{dec}_{\\#}}\\,P\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "177 Here, $g_{\\mathrm{enc}_{\\#}}\\,Q$ and $g_{\\mathrm{dec}_{\\#}}P$ represent the pushforward measures of $Q$ and $P$ through the encoder   \n178 map $g_{\\mathrm{enc}}$ and decoder map $g_{\\mathrm{dec}}$ , respectively. The latent representation error is understood as the   \n179 \u201cdifference\" of pushforward measure by the encoder/decoder and target measure. Here, to understand   \n180 the \"scale\" of the error $\\varepsilon$ in Equation (5), we use $W_{1}$ for the discrepancy between probability   \n181 measures. In particular, for Dreamer-type loss function that uses KL-divergence, we note that squared   \n182 $W_{1}$ distance between two probability measures can be upper bounded by their KL-divergence up to   \n183 a constant [11], implying that one could reasonably expect the $W_{1}$ distance to also decrease when   \n184 KL-divergence is used in the model.   \n185 CNN configuration. As a popular choice choice in encoder-decoder architecture is CNN, we   \n186 consider a general CNN function $f_{\\mathrm{CNN}}\\,:\\,\\mathcal{X}\\,\\rightarrow\\,\\mathbb{R}$ . Let $f_{\\mathrm{CNN}}$ have $L$ hidden layers, represented   \n187 as: for $x\\in\\mathscr{X}$ , $f_{\\mathrm{CNN}}(x):=A_{L+1}\\circ A_{L}\\circ\\cdot\\cdot\\cdot\\circ A_{2}\\circ A_{1}(x)$ , where $A_{i}$ \u2019s are either convolutional or   \n188 downsampling operators. For convolutional layers, $A_{i}(x)=\\sigma(W_{i}^{c}x+b_{i}^{c})$ , where $W_{i}^{c}\\in\\mathbb{R}^{d_{i}\\times d_{i-1}}$   \n1 89 is a structured sparse Toeplitz matrix from the convolutional filter $\\{w_{j}^{(i)}\\}_{j=0}^{s(i)}$ with filter length   \n190 $s(i)\\in\\ensuremath{\\mathbb{N}}_{+}$ , $b_{i}^{c}\\in\\mathbb{R}^{d_{i}}$ is a bias vector, and $\\sigma$ is the ReLU activation function. For downsampling   \n191 layers, Ai(x) = Di(x) = (xjm i)j\u230ad=i1\u22121/mi\u230b, where Di : Rdi\u00d7di\u22121 is the downsampling operator   \n192 with scaling parameter $m_{i}\\leq d_{i-1}$ in the $i$ -th layer. We examine the class of functions represented by   \n193 CNNs, denoted by $\\mathcal{F}_{\\mathrm{CNN}}$ , defined as: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "194 For the specific definition of $\\mathcal{F}_{\\mathrm{CNN}}$ , we refer to [29]\u2019s (4), (5) and (6). ", "page_idx": 4}, {"type": "text", "text": "195 Assumption 3.5. Assume that $\\mathcal{M}$ and $\\mathcal{Z}$ are locally diffeomorphic, that is there exists a map   \n196 $F:\\mathcal{M}\\rightarrow\\mathcal{Z}$ such that at every point $x$ on $\\mathcal{M}$ , $\\operatorname*{det}(d\\,\\dot{F}(x))\\neq0$ .   \n197 Theorem 3.6. (Approximation Error of Latent Representation). Under Assumption 3.3, 3.4 and 3.5,   \n198 for $\\theta\\in(0,1)$ , let $\\begin{array}{r}{d_{\\theta}:=\\mathcal{O}(d_{\\mathcal{M}}\\theta^{-2}\\log\\frac{d}{\\theta})}\\end{array}$ . For positive integers $M$ and $N$ , there exists an encoder   \n199 $g_{e n c}$ and decoder $g_{d e c}\\in\\mathcal{F}_{C N N}(L,S,W)$ s.t. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nW_{1}(g_{e n c_{\\#}}Q,P)\\leq d_{M}C(N M)^{-\\frac{2(k+1)}{d_{\\theta}}},\\quad W_{1}(g_{d e c_{\\#}}P,Q)\\leq d_{M}C(N M)^{-\\frac{2(k+1)}{d_{\\theta}}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "200 Theorem 3.6 indicates that with an appropriate CNN configuration, the $W_{1}$ approximation error can   \n201 be made to reside in a small region, as the best candidate within the function class is indeed capable of   \n202 approximating the oracle encoder/decoder. In particular, this result indicates that the error magnitude   \n203 $\\varepsilon$ in SDE (5) can be assumed to be small. This allows us to apply the perturbation analysis of the   \n204 dynamical system defined in Equations (5 - 8) in the following sections. ", "page_idx": 4}, {"type": "text", "text": "205 3.2 Latent Representation Errors as Implicit Regularization towards Generalization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "206 In this section, we investigate the impact of latent representation errors on generalization, for the   \n207 two cases with zero drift and non-zero drift, respectively. We show that under mild conditions,   \n208 the zero-drift errors can function as a natural form of implicit regularization, promoting wider   \n209 landscapes for improved robustness. Nevertheless, we caution that when latent representation errors   \n210 have non-zero drift, it could lead to poor regularization with unstable bias and degrade world model\u2019s   \n211 generalization, calling for explicit regularization.   \n212 To simplify the notation here, we consider the system equations, specifically Equations (5), (6) - (8),   \n213 as one stochastic system. Let ${\\boldsymbol x}_{t}=\\left(z_{t},h_{t},\\tilde{z}_{t},\\tilde{s}_{t}\\right)$ and $B_{t}=\\left(B_{t}^{\\mathrm{{enc}}},B_{t}^{\\mathrm{{seq}}},B_{t}^{\\mathrm{{pred}}},B_{t}^{\\mathrm{{dec}}}\\right)$ : ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nd\\,x_{t}=(g(x_{t},t)+\\varepsilon\\,\\sigma(x_{t},t))\\ d t+\\sum_{i}\\bar{g}_{i}(x_{t},t)+\\varepsilon\\,\\bar{\\sigma}_{i}(x_{t},t)\\,d B_{t}^{i},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "214 where $g$ , and $\\bar{g}_{i}$ are structured accordingly for the respective components, employing the Einstein   \n215 summation convention for concise representation. For abuse of notation, $\\bar{\\sigma^{\\prime}}=\\bar{(\\sigma,0,0,0)},\\bar{\\sigma}\\,=$   \n216 $(\\bar{\\sigma},0,0,0)$ . For a given error magnitude $\\varepsilon$ , we denote the solution to SDE (9) as $\\boldsymbol{x}_{t}^{\\varepsilon}$ . Intuitively, $\\boldsymbol{x}_{t}^{\\varepsilon}$ is   \n217 the perturbed trajectory of the latent dynamics model. In particular, when $\\varepsilon=0$ , indicating that the   \n218 absence of latent representation error in the model, the solution is denoted as $\\boldsymbol{x}_{t}^{0}$ . ", "page_idx": 4}, {"type": "text", "text": "219 3.2.1 The Case with Zero-drift Representation Errors ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "220 When the drift coefficient $\\sigma=0$ , the latent representation errors correspond to a class of well-behaved   \n221 stochastic processes. The following result translates the induced perturbation on the stochastic latent   \n222 dynamics model\u2019s loss function $\\mathcal{L}$ to a form of explicit regularization. We assume that ${\\mathcal{L}}\\in{\\mathcal{C}}^{2}$   \n223 and depends on $z_{t},h_{t},\\tilde{z}_{t},\\tilde{s}_{t}$ . Loss functions used in practical implementation, e.g. in DreamerV3,   \n224 reconstruction loss $J_{O}$ , reward loss $J_{R}$ , consistency loss $J_{D}$ , all satisfy this condition.   \n225 Theorem 3.7. (Explicit Effect Induced by Zero-Drift Representation Error) Under Assumptions   \n226 3.1 and 3.2 and considering a loss function ${\\mathcal{L}}\\in{\\mathcal{C}}^{2}$ , the explicit effects of the zero-drift error can be   \n227 marginalized out as follows: as $\\varepsilon\\rightarrow0$ , ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\,\\mathcal{L}\\left(x_{t}^{\\varepsilon}\\right)=\\mathbb{E}\\,\\mathcal{L}(x_{t}^{0})+\\mathcal{R}+\\mathcal{O}(\\varepsilon^{3}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "228 where the regularization term $\\mathcal{R}$ is given by $\\begin{array}{r}{\\mathcal{R}:=\\,\\varepsilon\\,\\mathcal{P}+\\varepsilon^{2}\\left(\\mathcal{Q}+\\frac{1}{2}\\,\\mathcal{S}\\right),}\\end{array}$ , with ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{P}:=\\mathbb{E}\\,\\nabla\\mathcal{L}(x_{t}^{0})^{\\top}\\Phi_{t}\\displaystyle\\sum_{k}\\xi_{t}^{k},}\\\\ &{\\mathcal{S}:=\\mathbb{E}\\displaystyle\\sum_{k_{1},k_{2}}\\big(\\Phi_{t}\\xi_{t}^{k_{1}}\\big)^{i}\\nabla^{2}\\mathcal{L}(x_{t}^{0},t)\\,\\big(\\Phi_{t}\\xi_{t}^{k_{2}}\\big)^{j},}\\\\ &{\\mathcal{Q}:=\\mathbb{E}\\,\\nabla\\mathcal{L}(x_{t}^{0})^{\\top}\\Phi_{t}\\displaystyle\\int_{0}^{t}\\Phi_{s}^{-1}\\,\\mathcal{H}^{k}(x_{s}^{0},s)d B_{t}^{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "229 Square matrix $\\Phi_{t}$ is the stochastic fundamental matrix of the corresponding homogeneous equation: ", "page_idx": 5}, {"type": "equation", "text": "$$\nd\\Phi_{t}=\\frac{\\partial\\bar{g}_{k}}{\\partial x}(x_{t}^{0},t)\\,\\Phi_{t}\\,d B_{t}^{k},\\quad\\Phi(0)=I,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "230 and $\\xi_{t}^{k}$ is the shorthand for $\\textstyle\\int_{0}^{t}\\Phi_{s}^{-1}\\bar{\\sigma}_{k}(x_{s}^{0},s)d B_{t}^{k}$ . Additionally, $\\mathcal{H}^{k}(x_{s}^{0},s)$ is represented by for   \n231  k1,k2\u2202xi\u2202kxj $\\begin{array}{r}{\\sum_{k_{1},k_{2}}\\frac{\\partial^{2}\\bar{g}_{k}}{\\partial x^{i}\\partial x^{j}}(x_{s}^{0},s)\\left(\\xi_{s}^{k_{1}}\\right)^{i}\\left(\\xi_{s}^{k_{2}}\\right)^{j}}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "232 The proof is relegated to Appendix $\\mathbf{B}$ in the Supplementary Materials. ", "page_idx": 5}, {"type": "text", "text": "233 When the loss $\\mathcal{L}$ is convex, then its Hessian, $\\nabla^{2}{\\mathcal{L}}$ , is positive semi-definite, which ensures that the   \n234 term $\\boldsymbol{S}$ is non-negative. The presence of this Hessian-dependent term $\\boldsymbol{S}$ , under latent representation   \n235 error, implies a tendency towards wider minima in the loss landscape. Empirical results from [20]   \n236 indicates that wider minima correlate with improved robustness of implicit regularization during   \n237 training. This observation also aligns with the theoretical insights in [27] that the introduction   \n238 of Brownian motion, which is indeed zero-drift by definition, in training RNN models promotes   \n239 robustness. We note that in addition, when the error $\\bar{\\sigma}_{t}(\\cdot)$ is too small, the effect of term $\\boldsymbol{S}$ as implicit   \n240 regularization would not be as significant as desired. Intuitively, this insight resonates with the   \n241 empirical results in Table 1 that model\u2019s robustness gain is not significant when the error induced by   \n242 small batch sizes is too small.   \n243 We remark that the exact loss form treated here is simplified compared to that in the practical   \n244 implementation of world models, which frequently depends on the probability density functions   \n245 (PDFs) of $z_{t},h_{t},\\tilde{z}_{t},\\tilde{s}_{t}$ . In principle, the PDE formulation corresponding to the PDFs of the perturbed   \n246 $\\boldsymbol{x}_{t}^{\\varepsilon}$ can be derived from the Kolmogorov equation of the SDE (9), and the technicality is more involved   \n247 but can offer more direct insight. We will study this in future work. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "248 3.2.2 The Case with Non-Zero-Drift Representation Errors ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "249 In practice, latent representation errors may not always exhibit zero drift as in idealized noise-injection   \n250 schemes for deep learning ([27], [2]). When the drift coefficient $\\sigma$ is non-zero or a function of input   \n251 data $h_{t}$ and $s_{t}$ in general, the explicit regularization terms induced by the latent representation error   \n252 may lead to unstable bias in addition to the regularization term $\\mathcal{R}$ in Theorem 3.7. With a slight abuse   \n253 of notation, we denote $\\bar{g}_{0}$ as $g$ from Equation (9) for convenience. ", "page_idx": 5}, {"type": "text", "text": "254 Corollary 3.8. (Additional Bias Induced by Non-Zero Drift Representation Error) ", "page_idx": 5}, {"type": "text", "text": "255 Under Assumptions 3.1 and 3.2 and considering a loss function ${\\mathcal{L}}\\in{\\mathcal{C}}^{2}$ , the explicit effects of the   \n256 general form error can be marginalized out as follows as $\\varepsilon\\rightarrow0$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\,\\mathcal{L}\\left(x_{t}^{\\varepsilon}\\right)=\\mathbb{E}\\,\\mathcal{L}(x_{t}^{0})+\\mathcal{R}+\\tilde{\\mathcal{R}}+\\mathcal{O}(\\varepsilon^{3}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "257 where the additional bias term $\\tilde{\\mathcal{R}}$ is given by $\\tilde{\\mathcal{R}}:=\\,\\varepsilon\\,\\tilde{\\mathcal{P}}+\\varepsilon^{2}\\left(\\tilde{\\mathcal{Q}}+\\tilde{\\mathcal{S}}\\right)$ , with ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\mathcal{P}}:=\\mathbb{E}\\,\\nabla\\mathcal{L}(x_{t}^{0})^{\\top}\\Phi_{t}\\,\\tilde{\\xi}_{t},}\\\\ &{\\tilde{\\mathcal{Q}}:=\\mathbb{E}\\,\\nabla\\mathcal{L}(x_{t}^{0})^{\\top}\\Phi_{t}\\displaystyle\\int_{0}^{t}\\Phi_{s}^{-1}\\,\\mathcal{H}^{0}(x_{s}^{0},s)\\,d t,}\\\\ &{\\tilde{S}:=\\mathbb{E}\\displaystyle\\sum_{k}(\\Phi_{t}\\tilde{\\xi}_{t})^{i}\\nabla^{2}\\mathcal{L}(x_{t}^{0},t)\\,\\big(\\Phi_{t}\\xi_{t}^{k}\\big)^{j},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "259 The presence of the new bias term R\u02dc implies that regularization effects of latent representation error   \n260 could be unstable. The presence of $\\tilde{\\xi}$ in $\\tilde{\\mathcal{P}}$ , $\\tilde{\\mathcal{Q}}$ and $\\tilde{S}$ induces a bias to the loss function with its   \n261 magnitude dependent on the error level $\\varepsilon$ , since $\\tilde{\\xi}$ is a non-zero term influenced on the drift term   \n262 $\\sigma$ . This contrasts with the scenarios described in [27] and [2], where the noise injected for implicit   \n263 regularization follows a zero-mean Gaussian distribution. To modulate the regularization and bias   \n264 terms $\\mathcal{R}$ and R\u02dc respectively, we note that a common factor, the fundamental matrix $\\Phi$ , can be bounded   \n265 by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}\\operatorname*{sup}_{t}\\left\\|\\Phi_{t}\\right\\|_{F}^{2}\\leq\\sum_{k}C\\exp\\left(C\\,\\mathbb{E}\\operatorname*{sup}_{t}\\left\\|\\frac{\\partial g_{k}}{\\partial x}(x_{t}^{0},t)\\right\\|_{F}^{2}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "266 which can be shown by using the Burkholder-Davis-Gundy Inequality and Gronwall\u2019s Lemma.   \n267 Based on this observation, we next propose a regularizer on input-output Jacobian norm $\\|\\frac{\\partial g_{k}}{\\partial x}\\|_{F}$ that   \n268 could modulate the new bias term $\\tilde{\\mathcal{R}}$ for stabilized implicit regularization. ", "page_idx": 6}, {"type": "text", "text": "269 4 Enhancing Predictive Rollouts via Jacobian Regularization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "270 In this section, we study the effects of latent representation errors on predictive rollouts using latent   \n271 state transitions, which happen in the inference phase in world models. We then propose to use   \n272 Jacobian regularization to enhance the quality of rollouts. In particular, we first obtain an upper bound   \n273 of state trajectory divergence in the rollout due to the representation error. We show that the error   \n274 effects on task policy\u2019s $Q$ function can be controlled through model\u2019s input-output Jacobian norm.   \n275 In world model learning, the task policy is optimized over the rollouts of dynamics model with the   \n276 initial latent state $z_{0}$ . Recall that latent representation error is introduced to $z_{0}$ when latent encoder   \n277 encodes the initial state $s_{0}$ from task environment. Intuitively, the latent representation error would   \n278 propagate under the sequence model and impact the policy learning, which would then affect the   \n279 generalization capacity through increased exploration. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "280 Recall that the sequence model and the transition predictor are given as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\nd\\,h_{t}=f(h_{t},\\tilde{z}_{t},\\pi(h_{t},\\tilde{z}_{t}))\\,d t,\\quad d\\,\\tilde{z}_{t}=p(h_{t})d t+\\bar{p}(h_{t})\\,d B_{t},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "281 with random variables $h_{0}$ , $\\tilde{z}_{0}+\\varepsilon$ as the initial values, respectively. In particular, $\\varepsilon$ is a random   \n282 variable of proper dimension, representing the error from encoder introduced at the initial step. We   \n283 impose the standard assumption on the error to ensure the well-definedness of the SDEs.   \n284 Under Assumption 3.1, there exists a unique solution to the SDEs (for Equations 19 with square  \n285 integrable $\\varepsilon$ ), denoted as $\\left(h_{t}^{\\varepsilon},z_{t}^{\\varepsilon}\\right)$ . In the case of no error introduced, i.e., $\\varepsilon=0$ , we denote the   \n286 solution of the SDEs as $(h_{t}^{0},\\bar{z}_{t}^{0})$ understood as the rollout under the absence of latent representation   \n287 error. To understand how to modulate impacts of the error in rollouts, our following result gives an   \n288 upper bound on the expected divergence between the perturbed rollout trajectory $(h_{t}^{\\varepsilon},z_{t}^{\\varepsilon})$ and the   \n289 original $(h_{t}^{0},z_{t}^{0})$ over the interval $[0,T]$ .   \n290 Theorem 4.1. (Bounding trajectory divergence) For a square-integrable random variable $\\varepsilon$ , let   \n291 $\\delta:=\\mathbb{E}\\left\\|\\varepsilon\\right\\|$ and $\\begin{array}{r}{d_{\\varepsilon}:=\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\left\\|h_{t}^{\\varepsilon}-h_{t}^{0}\\right\\|^{2}+\\left\\|\\tilde{z}_{t}^{\\varepsilon}-\\tilde{z}_{t}^{0}\\right\\|^{2}.\\bar{A}s\\;\\delta\\to0,}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\nd_{\\varepsilon}\\,\\leq\\,\\delta\\,C\\left(\\mathcal{T}_{0}+\\mathcal{T}_{1}\\right)+\\,\\delta^{2}\\,C\\exp\\big(\\,\\mathcal{H}_{0}\\left(\\mathcal{T}_{0}+\\mathcal{T}_{1}\\right)\\big)+\\delta^{2}\\,C\\exp\\big(\\,\\mathcal{H}_{1}\\left(\\mathcal{T}_{0}+\\mathcal{T}_{1}\\right)\\big)+\\mathcal{O}\\big(\\delta^{3}\\big),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "292 where $C$ is a constant dependent on T. $\\mathcal{I}_{1}$ and $\\mathcal{I}_{2}$ are Jacobian-related terms, and $\\mathcal{H}_{1}$ and $\\mathcal{H}_{2}$ are Hessian  \n293 related terms.   \n294 The Jacobian-related terms $\\mathcal{I}_{1}$ and $\\mathcal{I}_{2}$ are defined as $\\mathcal{J}_{0}:=\\exp\\left(\\mathcal{F}_{h}+\\mathcal{F}_{z}+\\mathcal{P}_{h}\\right)$ , $\\mathcal{J}_{1}:=\\exp\\left(\\bar{\\mathcal{P}}_{h}\\right)$ ;   \n295 the Hessian-related terms $\\mathcal{H}_{0}$ and $\\mathcal{H}_{1}$ are defined as $\\mathcal{H}_{0}:=\\mathcal{F}_{h h}+\\mathcal{F}_{h z}+\\mathcal{F}_{z h}+\\mathcal{F}_{z z}+\\mathcal{P}_{h h},\\mathcal{H}_{1}:=\\bar{\\mathcal{P}}_{h}$ ,   \n296 where $\\mathcal{F}_{h}$ , $\\mathcal{F}_{z}$ are the expected sup Frobenius norm of Jacobians of $f$ w.r.t $h,\\,z$ , respectively, and   \n297 $F_{h h},\\mathcal{F}_{h z},\\mathcal{F}_{z h},\\mathcal{F}_{z z}$ are the corresponding expected sup Frobenius norm of second-order derivatives.   \n298 Other terms are similarly defined. A detailed description of all terms, can be found in Appendix C.1.   \n299 Theorem 4.1 correlates with the empirical findings in [14] regarding the diminished predictive   \n300 accuracy of latent states $\\tilde{z}_{t}$ over the extended horizons. In particular, Theorem 4.1 suggests that the   \n301 expected divergence from error accumulation hinges on the expected error magnitude, the Jacobian   \n302 norms within the latent dynamics model and the horizon length $T$ .   \n303 Our next result reveals how initial latent representation error influences the value function $Q$ during   \n304 the prediction rollouts, which again verifies that the perturbation is dependent on expected error   \n305 magnitude, the model\u2019s Jacobian norms and the horizon length $T$ :   \n306 Corollary 4.2. For a square-integrable $\\varepsilon$ , let $\\boldsymbol{x}_{t}\\::=\\:\\left(h_{t},z_{t}\\right)$ . Then, for any action $a\\in{\\mathcal{A}}$ , the   \n307 following holds for value function $\\bar{Q}$ almost surely: ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q(x_{t}^{\\varepsilon},a)=Q(x_{t}^{0},a)+\\cfrac{\\partial}{\\partial x}Q(x_{t}^{0},a)\\left(\\varepsilon^{i}\\partial_{i}\\,x_{t}^{0}+\\cfrac12\\varepsilon^{i}\\,\\varepsilon^{j}\\,\\partial_{i j}^{2}\\,x_{t}^{0}\\right)}\\\\ &{\\qquad\\qquad\\qquad+\\cfrac12(\\varepsilon^{i}\\,\\partial_{i}\\,x_{t}^{0})^{\\top}\\cfrac{\\partial^{2}}{\\partial x^{2}}Q(x_{t}^{0},a)\\,(\\varepsilon^{i}\\,\\partial_{i}\\,x_{t}^{0})+\\mathcal O(\\delta^{3}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "308 as $\\delta\\rightarrow0$ , where stochastic processes $\\partial_{i}\\,x_{t}^{0}$ , $\\partial_{i j}^{2}\\,x_{t}^{0}$ are the first and second derivatives of $x_{t}^{0}\\ w.r.t\\varepsilon$   \n309 and are bounded as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\left\\|\\partial_{i}\\,x_{t}^{0}\\right\\|\\leq C\\left(\\mathcal{I}_{0}+\\mathcal{I}_{1}\\right),\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\left\\|\\partial_{i j}^{2}\\,x_{t}^{0}\\right\\|\\leq C\\exp\\left(\\mathcal{H}_{0}\\left(\\mathcal{I}_{0}+\\mathcal{I}_{1}\\right)\\right)+C\\exp\\left(\\mathcal{H}_{1}\\left(\\mathcal{I}_{0}+\\mathcal{I}_{1}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "310 This corollary reveals that latent representation errors implicitly encourage exploration of unseen   \n311 states by inducing a stochastic perturbation in the value function, which again can be regularized   \n312 through a controlled Jacobian norm.   \n313 Jacobian Regularization against Non-Zero Drift. The above theoretical results have established   \n314 a close connection of input-output Jacobian matrices with the stabilized generalization capacity of   \n315 world models (shown in 18 under non-zero drift form), and perturbation magnitude in predictive   \n316 rollouts (indicated in the presence of Jacobian terms in Theorem 4.1 and Corollary 4.2.) Based on   \n331187 tahdids,i tiwoen )p foosre  sat arbeigliuzleadri izemr polinc iitn rpeugt-uolaurtipzuatt iJoanc.obian norm $\\|\\frac{\\partial g_{k}}{\\partial x}\\|_{F}$ that could modulate $\\tilde{\\xi}$ ( and in   \n$\\xi_{k}$ ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "319 The regularized loss function for LDM is defined as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\bar{\\mathcal{L}}_{\\mathrm{dyn}}=\\mathcal{L}_{\\mathrm{dyn}}+\\lambda\\,\\|J_{\\theta}\\|_{F},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "320 where $\\mathcal{L}_{\\mathrm{dyn}}$ is the original loss function for dynamics model, $J_{\\theta}$ denotes the data-dependent Jacobian   \n321 matrix associated with the $\\theta$ -parameterized dynamics model, and $\\lambda$ is the regularization weight.   \n322 Our empirical results in 5 with an emphasis on sequential case align with the experimental findings   \n323 from [18] that Jacobian regularization can enhance robustness against random and adversarial input   \n324 perturbation in machine learning models. ", "page_idx": 7}, {"type": "text", "text": "325 5 Experimental Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "326 In this section, experiments are carried out over a number of tasks in Mujoco environments. Due to   \n327 space limitation, implementation details and additional results, including the standard deviation of   \n328 the trials, are relegated to Section $\\mathrm{D}$ in the Appendix.   \n329 Enhanced generalization to unseen noisy states. We investigated the effectiveness of Jacobian   \n330 regularization in model trained against a vanilla model during the inference phase with perturbed   \n331 state images. We consider three types of perturbations: (1) Gaussian noise across the full image,   \n332 denoted as $\\mathcal{N}(\\mu_{1},\\sigma_{1}^{2})\\,;(2)$ rotation; and (3) noise applied to a percentage of the image, $\\mathcal{N}(\\mu_{2},\\sigma_{2}^{\\breve{2}})$ .   \n333 (In Walker task, $\\mu_{1}=\\mu_{2}=0.5$ , $\\sigma_{2}^{2}=0.15$ ; in Quadruped task, $\\mu_{1}=0,\\mu_{2}=0.05,$ $\\sigma_{2}^{2}=0.2.)$ In   \n334 each case of perturbations, we examine a collection of noise levels: (1) variance $\\sigma^{2}$ from 0.05 to   \n335 0.55; (2) rotation degree $\\alpha\\,20$ and 30; and (3) masked image percentage $\\beta\\%$ from 25 to 75.   \n336 It can be seen from Table 3 and Figure 1 that thanks to the adoption of Jacobian regularization in   \n337 training, the rewards (averaged over 5 trials) are higher compared to the baseline, indicating improved   \n338 generalization to unseen image states in all cases. The experimental results corroborate the findings   \n339 in Corollary 3.8 that the regularized Jacobian norm could stabilize the induced implicit regularization.   \n340 Robustness against encoder errors. Next, we focus on the effects of Jacobian regularization on   \n341 controlling the error process to the latent states $z$ during training. Since it is very challenging, if   \n342 not impossible, to characterize the latent representation errors and hence the drift therein explicitly,   \n343 we consider to evaluate the robustness against two exogenous error signals, namely (1) zero-drift   \n344 error with $\\mu_{t}\\,=\\,0,\\sigma_{t}^{2}$ $(\\sigma_{t}^{2}\\,=\\,5$ in Walker, $\\sigma_{t}^{2}\\,=\\,0.1$ in Quadruped), and (2) non-zero-drift error   \n345 with $\\mu_{t}\\,\\sim\\,[0,5],\\sigma_{t}^{2}\\,\\sim\\,[0,5]$ uniformly. Table 3 shows that the model with regularization can   \n346 consistently learn policies with high returns and also converges faster, compared to the vanilla case.   \n347 This corroborates our theoretical findings in Corollary 3.8 that the impacts of error to loss $\\mathcal{L}$ can be   \n348 controlled through the model\u2019s Jacobian norm.   \n349 Faster convergence on tasks with extended horizon. We further evaluate the efficacy of Jacobian   \n350 regularization in tasks with extended horizon, particularly by extending the horizon length in MuJoCo   \n351 Walker from 50 to 100 steps. Table 4 shows that the model with regularization converges significantly   \n352 faster $\\sim100\\mathrm{K}$ steps) than the case without Jacobian regularization in training. This corroborates   \n353 results in Theorem 4.1 that regularizing the Jacobian norm can reduce error propagation. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "3sWghzJvGd/tmp/45ce74b234ad9c0adfd0f877cd3493c9bbcf2702794d7ad7e92917ffdaade4fc.jpg", "img_caption": ["Figure 1: Generalization against increasing degree of perturbation. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "3sWghzJvGd/tmp/0844bdb83c27e64a40882cca7d76be6b2223a4710ba7a24c951af211338cf515.jpg", "table_caption": [], "table_footnote": ["Table 2: Evaluation on unseen states by various perturbation (Clean means without perturbation). $\\lambda=0.01$ . "], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "3sWghzJvGd/tmp/ef9c5403df604d329a41d331ec4104aa30b4d99f7cb6f5746b59b883618142b5.jpg", "table_caption": [], "table_footnote": ["Table 3: Accumulated rewards under additional encoder errors. $\\overline{{\\lambda=0.01}}$ . "], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "3sWghzJvGd/tmp/9cb904cf64db585e8f7241dbcabd75cf1f3a56643ef23ae3eb3985ee0452f878.jpg", "table_caption": ["Table 4: Accumulated rewards of Walker with extended horizon. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "354 6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "355 In this study, we investigate the impacts of latent representation errors on the generalization capacity   \n356 of world models. We utilize a stochastic differential equation formulation to characterize the effects   \n357 of latent representation errors as implicit regularization, for both cases with zero-drift errors and   \n358 with non-zero drift errors. We develop a Jacobian regularization scheme to address the compounding   \n359 effects of non-zero drift, thereby enhancing training stability and generalization. Our empirical   \n360 findings validate that Jacobian regularization improves the generalization performance, expanding   \n361 the applicability of world models in complex, real-world scenarios. Future research is needed to   \n362 investigate how stabilizing latent errors can enhance generalization across more sophisticated tasks   \n363 for general non-zero drift cases.   \n364 The broader social impact of our work resides in its potential to enhance the robustness and reliability   \n365 of RL agents deployed in real-world applications. By improving the generalization capacities of world   \n366 models, our work could contribute to the development of RL agents that perform consistently across   \n367 diverse and unseen environments. This is particularly relevant in safety-critical domains such as   \n368 autonomous driving, where reliable agents can provide intelligent and trustworthy decision-making. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "369 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "370 [1] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning   \n371 environment: An evaluation platform for general agents. Journal of Artificial Intelligence   \n372 Research, 47:253\u2013279, 2013.   \n373 [2] Alexander Camuto, Matthew Willetts, Umut \u00b8Sim\u00b8sekli, Stephen Roberts, and Chris Holmes.   \n374 Explicit regularisation in gaussian noise injections, 2021.   \n375 [3] Henri Cartan. Differential calculus on normed spaces. Createspace Independent Publishing   \n376 Platform, North Charleston, SC, August 2017.   \n377 [4] Bo Chang, Minmin Chen, Eldad Haber, and Ed H. Chi. Antisymmetricrnn: A dynamical system   \n378 view on recurrent neural networks, 2019.   \n379 [5] Zhengdao Chen, Jianyu Zhang, Martin Arjovsky, and L\u00e9on Bottou. Symplectic recurrent neural   \n380 networks, 2020.   \n381 [6] Bernard Dacorogna and J\u00fcrgen Moser. On a partial differential equation involving the jacobian   \n382 determinant. Annales de l\u2019I.H.P. Analyse non lin\u00e9aire, 7(1):1\u201326, 1990.   \n383 [7] Carl Doersch. Tutorial on variational autoencoders. arXiv preprint arXiv:1606.05908, 2016.   \n384 [8] Sean C Duncan. Minecraft, beyond construction and survival. 2011.   \n385 [9] Lawrence Craig Evans and Ronald F Gariepy. Measure theory and fine properties of functions,   \n386 revised edition. Textbooks in Mathematics. Apple Academic Press, Oakville, MO, April 2015.   \n387 [10] C. Daniel Freeman, Luke Metz, and David Ha. Learning to predict without looking ahead:   \n388 World models without forward prediction. Thirty-third Conference on Neural Information   \n389 Processing Systems (NeurIPS 2019), 2019.   \n390 [11] Alison L. Gibbs and Francis Edward Su. On choosing and bounding probability metrics.   \n391 International Statistical Review / Revue Internationale de Statistique, 70(3):419\u2013435, 2002.   \n392 [12] David Ha and J\u00fcrgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.   \n393 [13] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control:   \n394 Learning behaviors by latent imagination, 2020.   \n395 [14] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and   \n396 James Davidson. Learning latent dynamics for planning from pixels. In International conference   \n397 on machine learning, pages 2555\u20132565. PMLR, 2019.   \n398 [15] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with   \n399 discrete world models, 2022.   \n400 [16] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains   \n401 through world models. arXiv preprint arXiv:2301.04104, 2023.   \n402 [17] Paul Louis Hennequin, R. M. Dudley, H. Kunita, and F. Ledrappier. Ecole d\u2019ete de Probabilites   \n403 de Saint-Flour XII-1982. Springer-Verlag, 1984.   \n404 [18] Judy Hoffman, Daniel A. Roberts, and Sho Yaida. Robust learning with jacobian regularization,   \n405 2019.   \n406 [19] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie   \n407 Shotton, and Gianluca Corrado. Gaia-1: A generative world model for autonomous driving.   \n408 arXiv preprint arXiv:submit/1234567, Sep 2023. Submitted on 29 Sep 2023.   \n409 [20] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping   \n410 Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima,   \n411 2017.   \n412 [21] Samuel Kessler, Mateusz Ostaszewski, Micha\u0142 Bortkiewicz, Mateusz Z\u02d9arski, Maciej Wo\u0142czyk,   \n413 Jack Parker-Holder, Stephen J. Roberts, and Piotr Mi\u0142os\u00b4. The effectiveness of world models for   \n414 continual reinforcement learning. CoLLAs 2023, 2023.   \n415 [22] Kuno Kim, Megumi Sano, Julian De Freitas, Nick Haber, and Daniel Yamins. Active world   \n416 model learning with progress curiosity. In Proceedings of the 37th International Conference on   \n417 Machine Learning (ICML), 2020.   \n418 [23] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint   \n419 arXiv:1312.6114, 2013.   \n420 [24] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne   \n421 Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition.   \n422 Neural computation, 1(4):541\u2013551, 1989.   \n423 [25] John M. Lee. Introduction to Riemannian Manifolds. Springer International Publishing, 2018.   \n424 [26] Zhong Li, Jiequn Han, Weinan E, and Qianxiao Li. Approximation and optimization theory   \n425 for linear continuous-time recurrent neural networks. Journal of Machine Learning Research,   \n426 23(42):1\u201385, 2022.   \n427 [27] Soon Hoe Lim, N Benjamin Erichson, Liam Hodgkinson, and Michael W Mahoney. Noisy   \n428 recurrent neural networks. Advances in Neural Information Processing Systems, 34:5124\u20135137,   \n429 2021.   \n430 [28] Lynn Harold Loomis and Shlomo Sternberg. Advanced calculus (revised edition). World   \n431 Scientific Publishing, Singapore, Singapore, March 2014.   \n432 [29] Guohao Shen, Yuling Jiao, Yuanyuan Lin, and Jian Huang. Approximation with cnns in sobolev   \n433 space: with applications to classification. In NeurIPS, Oct 2022.   \n434 [30] J. Michael Steele. Stochastic calculus and Financial Applications. Springer, 2001.   \n435 [31] Colin Wei, Sham Kakade, and Tengyu Ma. The implicit and explicit regularization effects of   \n436 dropout, 2020.   \n437 [32] Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, and Ken Goldberg. Day  \n438 dreamer: World models for physical robot learning. In Proceedings of The 6th Conference on   \n439 Robot Learning, volume 205 of PMLR, pages 2226\u20132240, 2023.   \n440 [33] Yong Yu, Xiaosheng Si, Changhua Hu, and Jianxun Zhang. A review of recurrent neural   \n441 networks: Lstm cells and network architectures. Neural computation, 31(7):1235\u20131270, 2019. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "Supplementary Materials ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "443 In this appendix, we provide the supplementary materials supporting the findings of the main paper   \n444 on the latent representation of latent representations in world models. The organization is as follows:   \n445 \u2022 In Section A, we provide proof on showing the approximation capacity of CNN encoder  \n446 decoder architecture in latent representation of world models.   \n447 \u2022 In Section B, we provide proof on implicit regularization of zero-drift errors and additional   \n448 effects of non-zero-drift errors by showing a proposition on the general form.   \n449 \u2022 In Section C, we provide proof on showing the effects of non-zero-drift errors during   \n450 predictive rollouts by again showing a result on the general form.   \n451 \u2022 In Section D, we provide additional results and implementation details on our empirical   \n452 studies. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "453 A Approximation Power of Latent Representation with CNN Encoder and 454 Decoder ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "455 To mathematically describe this intrinsic lower-dimensional geometric structure, for an integer $k>0$   \n456 and $\\alpha\\in(0,1]$ , we consider the notion of smooth manifold (in the $\\mathcal{C}^{k,\\alpha}$ sense), formally defined by   \n457 Definition A.1 ( $\\mathcal{C}^{k,\\alpha}$ manifold). A $\\mathcal{C}^{k,\\alpha}$ manifold $\\mathcal{M}$ of dimension $n$ is a topological manifold (i.e.   \n458 a topological space that is locally Euclidean, with countable basis, and Hausdorff) that has a $\\mathcal{C}^{k,\\alpha}$   \n459 structure $\\Xi$ that is a collection of coordinate charts $\\{U_{\\alpha},\\psi_{\\alpha}\\}_{\\alpha\\in A}$ where $U_{\\alpha}$ is an open subset of $\\mathcal{M}$ ,   \n460 $\\psi_{\\alpha}:U_{\\alpha}\\to V_{\\alpha}\\subseteq\\mathbb{R}^{n}$ such that ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "\u2022 $\\textstyle\\bigcup_{\\alpha\\in A}U_{\\alpha}\\supseteq{\\mathcal{M}}$ , meaning that the the open subsets form an open cover,   \n\u2022 Each chart $\\psi_{\\alpha}$ is a diffeomorphism that is a smooth map with smooth inverse (in the $\\mathcal{C}^{k,\\alpha}$ sense),   \n\u2022 Any two charts are $\\mathcal{C}^{k,\\alpha}$ -compatible with each other, that is for all $\\alpha_{1},\\alpha_{2}\\in A$ , $\\psi_{\\alpha_{1}}\\circ\\psi_{\\alpha_{2}}^{-1}$ : $\\psi_{\\alpha_{2}}(U_{\\alpha_{1}}\\cap U_{\\alpha_{2}})\\to\\psi_{\\alpha_{1}}(U_{\\alpha_{1}}\\cap U_{\\alpha_{2}})$ is $\\mathcal{C}^{k,\\alpha}$ . ", "page_idx": 12}, {"type": "text", "text": "466 Intuitively, a $\\mathcal{C}^{k,\\alpha}$ manifold is a generalization of Euclidean space by allowing additional spaces with   \n467 nontrivial global structures through a collection of charts that are diffeomorphisms mapping open   \n468 subsets from the manifold to open subsets of euclidean space. For technical utility, the defined charts   \n469 allow to transfer most familiar real analysis tools to the manifold space. For more references, see   \n470 [25].   \n471 Definition A.2 (Riemannian volume form). Let $\\mathcal{X}$ be a smooth, oriented $d$ -dimensional manifold   \n472 with Riemannian metric $g$ . A volume form $d\\mathrm{vol}_{\\mathcal{M}}$ is the canonical volume form on $\\mathcal{X}$ if for any point   \n473 $x\\in\\mathscr{X}$ , for a chosen local coordinate chart $(x_{1},...,x_{d}),d\\mathrm{vol}_{\\mathcal{M}}=\\sqrt{\\operatorname*{det}g_{i j}}\\,d x_{1}\\ \\wedge\\ ...\\ \\wedge\\ d x_{d}.$ , where   \n474 $\\begin{array}{r}{g_{i j}(x):=g\\left({\\frac{\\partial}{\\partial x_{i}}},{\\frac{\\partial}{\\partial x_{j}}}\\right)(x)}\\end{array}$ .   \n475 Then the induced volume measure by the canonical volume form $d\\mathrm{vol}_{\\mathcal{X}}$ is denoted as $\\mu_{\\mathcal{X}}$ , defined   \n476 by $\\textstyle\\mu_{\\mathcal{X}}:\\,A\\mapsto\\int_{A}d\\mathbf{vol}_{\\mathcal{X}}$ , for any Borel-measurable subset $A$ on the space $\\mathcal{X}$ . For more references,   \n477 see [9]. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "478 We recall the latent representation problem defined in the main paper. ", "page_idx": 12}, {"type": "text", "text": "479 Consider the state space $S\\subset\\mathbb{R}^{d_{S}}$ and the latent space $\\mathcal{Z}$ . Consider a state probability measure $Q$ on   \n480 the state space $\\boldsymbol{S}$ and a probability measure $P$ on the latent space $\\mathcal{Z}$ .   \n481 Assumption A.3. (Latent manifold assumption) For a positive integer $k$ , there exists a $d_{\\mathcal{M}}$ -   \n482 dimensional $\\mathcal{C}^{k,\\alpha}$ submanifold $\\mathcal{M}$ (with $\\mathcal{C}^{k+3,\\alpha}$ boundary) with Riemannian metric $g$ and has   \n483 positive reach and also isometrically embedded in the state space $S\\subset\\mathbb{R}^{d_{S}}$ and $d_{\\mathcal{M}}\\ll d_{S}$ , where   \n484 the state probability measure is supported on. In addition, $\\mathcal{M}$ is a compact, orientable, connected   \n485 manifold.   \n486 Assumption A.4. (Smoothness of state probability measure) $Q$ is a probability measure supported   \n487 on $\\mathcal{M}$ with its Radon-Nikodym derivative $q\\in\\mathcal{C}^{k,\\dot{\\alpha}}(\\mathcal{M},\\mathbb{R})$ w.r.t $\\mu_{\\mathcal{M}}$ .   \n488 Let $\\mathcal{Z}$ be a closed ball in $\\mathbb{R}^{d_{\\mathcal{M}}}$ , that is $\\{x\\in\\mathbb{R}^{d_{\\mathcal{M}}}\\,:\\,\\|x\\|\\leq1\\}$ . $P$ is a probability measure supported   \n489 on $\\mathcal{Z}$ with its Radon-Nikodym derivative $p\\in\\mathcal{C}^{k,\\alpha}(\\mathcal{Z},\\mathbb{R})$ w.r.t $\\mu{z}$ . ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "490 We consider a general CNN function $f_{\\mathrm{CNN}}:\\mathcal{X}\\to\\mathbb{R}$ . Let $f_{\\mathrm{CNN}}$ have $L$ hidden layers, represented as: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f_{\\mathrm{CNN}}(x)=A_{L+1}\\circ A_{L}\\circ\\cdots\\circ A_{2}\\circ A_{1}(x),\\quad x\\in\\mathcal{X},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "491 where $A_{i}$ \u2019s are either convolutional or downsampling operators. For convolutional layers, ", "page_idx": 12}, {"type": "equation", "text": "$$\nA_{i}(x)=\\sigma(W_{i}^{c}x+b_{i}^{c}),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "492 where $W_{i}^{c}\\in\\mathbb{R}^{d_{i}\\times d_{i-1}}$ is a structured sparse Toeplitz matrix from the convolutional fliter $\\{w_{j}^{(i)}\\}_{j=0}^{s(i)}$   \n493 with filter length $s(i)\\in\\ensuremath{\\mathbb{N}}_{+}$ , $b_{i}^{c}\\in\\mathbb{R}^{d_{i}}$ is a bias vector, and $\\sigma$ is the ReLU activation function. ", "page_idx": 12}, {"type": "text", "text": "494 For downsampling layers, ", "page_idx": 12}, {"type": "equation", "text": "$$\nA_{i}(x)=D_{i}(x)=(x_{j m_{i}})_{j=1}^{\\lfloor d_{i-1}/m_{i}\\rfloor}\\,,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "image", "img_path": "3sWghzJvGd/tmp/2e53fca7cca1c49e6b3866ad28671c5291cfa5323a6e3351d231410e953b2372.jpg", "img_caption": ["Figure 2: Latent Representation Problem: The left and right denote the manifold $\\mathcal{M}$ with lower dim $d_{\\mathcal{M}}$ embedded in a larger Euclidean space, with latent space $Z$ a $d_{\\mathcal{M}}$ -dimensional ball in middle. Encoder and decoder as maps respectively pushing forward Q to $\\mathbf{P}$ and $\\mathbf{P}$ to $\\mathrm{\\DeltaQ}$ . "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "495 where $D_{i}:\\mathbb{R}^{d_{i}\\times d_{i-1}}$ is the downsampling operator with scaling parameter $m_{i}\\leq d_{i-1}$ in the $i$ -th   \n496 layer. The convolutional and downsampling operations are elaborated in Appendix [63]. We examine   \n497 the class of functions represented by CNNs, denoted by $\\mathcal{F}_{\\mathrm{CNN}}$ , defined as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal F_{\\mathrm{CNN}}=\\{f_{\\mathrm{CNN}}\\;\\mathrm{as\\;in\\;defined\\;above\\;with\\;any\\;choice\\;of\\;}A_{i},\\;i=1,\\ldots,L+1\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "498 For more details in the definitions of CNN functions, we refer to [29]. ", "page_idx": 13}, {"type": "text", "text": "499 Assumption A.5. Assume that $\\mathcal{M}$ and $\\mathcal{Z}$ are locally diffeomorphic, that is there exists a map   \n500 $F:\\mathcal{M}\\rightarrow\\mathcal{Z}$ such that at every point $x$ on $\\mathcal{M}$ , $\\operatorname*{det}(d\\,F(x))\\neq0$ .   \n501 Theorem A.6. (Approximation Error of Latent Representation). Under Assumption A.3, A.4 and   \n502 A.5, for $\\theta\\,\\in\\,(0,1)$ , let $\\begin{array}{r}{d_{\\theta}\\,=\\,\\mathcal{O}(d_{\\mathcal{M}}\\theta^{-2}\\log\\frac{d}{\\theta})}\\end{array}$ . For positive integers $M$ and $N$ , there exists an   \n503 encoder $g_{e n c}$ and decoder $g_{d e c}\\in\\mathcal{F}_{C N N}(L,S,\\breve{W})$ s.t. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{W_{1}(g_{e n c\\neq}Q,P)\\leq d_{\\mathcal{M}}C(N M)^{-\\frac{2(k+1)}{d_{\\theta}}},}\\\\ {W_{1}(g_{d e c\\neq}P,Q)\\leq d_{\\mathcal{M}}C(N M)^{-\\frac{2(k+1)}{d_{\\theta}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "504 The primary challenge to show Theorem A.6 is in demonstrating the existence of oracle encoder and   \n505 decoder maps. These maps, denoted as $g_{\\mathrm{enc}}^{*}:{\\mathcal{M}}\\to{\\mathcal{Z}}$ and $g_{\\mathrm{dec}}^{*}:\\mathcal{Z}\\rightarrow\\mathcal{M}$ respectively, must satisfy ", "page_idx": 13}, {"type": "equation", "text": "$$\ng_{\\mathrm{enc}\\#}^{*}\\,Q=P,\\quad g_{\\mathrm{dec}\\#}^{*}\\,P=Q.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "506 and importantly they have the proper smoothness guarantee, namely $g_{\\mathrm{enc}}^{*}\\,\\in\\,\\mathcal{C}^{k+1,\\alpha}(\\mathcal{M},\\mathcal{Z})$ and   \n507 $g_{\\mathrm{dec}}^{*}\\in\\mathcal{C}^{k+1,\\alpha}(\\mathcal{Z},\\mathcal{M})$ . Proposition A.7 shows the existence of such oracle map(s).   \n508 Proposition A.7 $\\textstyle({\\mathcal{C}}^{k,\\alpha}$ , compact). Let $\\mathcal{M},\\mathcal{N}$ be compact, oriented $d$ -dimensional Riemannian   \n509 manifolds with $\\mathcal{C}^{k+3,\\alpha}$ boundary with the volume measure $\\mu_{\\mathcal{M}}$ and $\\mu_{\\mathcal{N}}$ respectively. Let $Q$ , $P$ be   \n510 distributions supported on $\\mathcal{M}$ , $\\mathcal{N}$ respectively with their $\\mathcal{C}^{k,\\dot{\\alpha}}$ density functions $\\begin{array}{r}{q,\\,p,}\\end{array}$ , that is $Q$ , $P$ are   \n511 probability measures supported on $\\mathcal{M},\\mathcal{N}$ with their Radon-Nikodym derivatives $q\\in\\mathcal{C}^{k,\\alpha}(\\mathcal{M},\\mathbb{R})$   \n512 $w.r.t\\ \\mu_{\\mathcal{M}}$ and $p\\in\\mathcal{C}^{k,\\alpha}(\\mathcal{N},\\mathbb{R})$ w.r.t \u00b5N . Then, there exists a $\\mathcal{C}^{k+1,\\alpha}$ map $g:\\mathcal{N}\\to\\mathcal{M}$ such that   \n513 the pushforward measure $g_{\\#}P\\,=\\,Q$ , that is for any measurable subset $A\\,\\in\\,B({\\mathcal{M}})$ , $Q(A)\\;=\\;$   \n514 $P(g^{-1}(A))$ .   \n515 Proof. (Proposition A.7) Let $\\omega:=p\\,d\\mathrm{vol}_{\\mathcal{N}}$ , then $\\omega$ is a $\\mathcal{C}^{k,\\alpha}$ volume form on $\\mathcal{N}$ , as $p\\in\\mathcal{C}^{k,\\alpha}$ and for   \n516 any point $x\\in N$ , we have $p(x)>0$ . In addition, $\\begin{array}{r}{\\int_{N}\\omega=\\int_{N}p\\,d\\mathrm{vol}_{N}=\\int_{N}p\\,d\\mu_{N}=P(N)=1}\\end{array}$ .   \n517 Similarly, let $\\eta:=q\\,d\\mathrm{vol}_{\\mathcal{M}}$ a $\\mathcal{C}^{k,\\alpha}$ volume form on $\\mathcal{M}$ and $\\begin{array}{r}{\\int_{\\mathcal{M}}\\eta=1}\\end{array}$ .   \n519 Let $F:\\mathcal{N}\\to\\mathcal{M}$ be an orientation-preserving local diffeomorphism, we then have $\\operatorname*{det}(d F)>0$   \n520 everywhere on $\\mathcal{N}$ .   \n521 As $\\mathcal{N}$ is compact and $\\mathcal{M}$ is connected by assumption, $F$ is a covering map, that is for every point   \n522 $x\\in{\\mathcal{M}}$ , there exists an open neighborhood $U_{x}$ of $x$ and a discrete set $D_{x}$ such that $F^{-1}\\dot{(U)}=$   \n523 $\\sqcup_{\\alpha\\in D}V_{\\alpha}\\subset{\\mathcal{N}}$ and $F|_{V_{\\alpha}}=V_{\\alpha}\\rightarrow U$ is a diffeomorphism. Furthermore, $|D_{x}|=|D_{y}|$ for any points   \n524 $x,y\\in M$ . In addition, $|D_{x}|$ is finite from the compactness of $\\mathcal{N}$ . ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "525 Let $\\bar{\\eta}$ be the pushforward of $\\omega$ via $F$ , defined by for any point $x\\in{\\mathcal{M}}$ and a neighborhood $U_{x}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\bar{\\eta}(x):=\\frac{1}{|D_{x}|}\\sum_{\\alpha\\in D_{x}}\\left(F{|_{V_{\\alpha}}}^{-1}\\right)^{*}\\omega{|_{V_{\\alpha}}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "526 $\\bar{\\eta}$ is well-defined as it is not dependent on the choice of neighborhoods and the sum and $\\frac{1}{|D_{x}|}$ are   \n527 always finite. Furthermore, $\\bar{\\eta}$ is a $\\mathcal{C}^{k,\\alpha}$ volume form on $\\mathcal{M}$ , as $p\\circ\\left(F{\\big|}_{V_{\\alpha}}^{-1}\\right)$ is $\\mathcal{C}^{k,\\alpha}$ .   \n528   \n529 Notice that F  V\u03b1\u2212 1 is orientation-preserving as det d F  V\u03b1\u2212 $\\operatorname*{det}d\\,F{\\big|}_{V_{\\alpha}}{}^{-1}={\\frac{1}{\\operatorname*{det}d\\,F{\\big|}_{V_{\\alpha}}}}>0$ everywhere on $V_{\\alpha}$ .   \n530 In addition, $F\\big|_{V_{\\alpha}}^{-1}$ is proper: as for any compact subset $K$ of ${\\mathcal{N}},\\,K$ is closed; and as $\\boldsymbol{F}\\big|_{\\boldsymbol{V_{\\alpha}}}^{\\mathrm{~\\,~-1~}}$   \n531 is continuous, the preimage of $K$ via $F\\big|_{V_{\\alpha}}^{-1}$ a closed subset of $\\mathcal{M}$ which is compact, then the   \n532 preimage of $K$ must also be compact. Hence, $\\boldsymbol{F}\\big|_{\\boldsymbol{V_{\\alpha}}}^{\\mathrm{~\\scriptsize~-1~}}$ is proper. As every $\\boldsymbol{F}\\big|_{\\boldsymbol{V_{\\alpha}}}^{\\mathrm{~\\,~-1~}}$ is proper,   \n533 orientation-preserving and surjective, then $c:=\\deg({F\\big|_{V_{\\alpha}}}^{-1})=1$ .   \n534 Then, $\\begin{array}{r}{\\int_{\\mathcal M}\\bar{\\eta}=c\\int_{\\mathcal N}\\omega=1}\\end{array}$ .   \n536 As we have shown that $\\eta$ and $\\bar{\\eta}\\in\\mathcal{C}^{k,\\alpha}$ and $\\textstyle\\int_{\\mathcal{M}}\\bar{\\eta}=\\int_{\\mathcal{M}}\\eta$ , by [6], there exists a diffeomorphism   \n537 $\\psi:\\mathcal{M}\\to\\mathcal{M}$ fixing on the boundary such that $\\psi^{*}\\eta=\\bar{\\eta}$ , where $\\psi,\\psi^{-1}\\in\\mathcal{C}^{k+1,\\alpha}$ .   \n538 Let $g:=\\psi\\circ F$ , then it holds that $g^{*}\\eta=(\\psi\\circ F)^{*}\\eta=F^{*}\\circ\\psi^{*}\\eta=F^{*}\\bar{\\eta}=\\omega$ .   \n539 Then, for any measurable subset $A$ on the manifold $\\mathcal{M}$ , we verify that $\\begin{array}{r c l}{{Q(A)}}&{{=}}&{{\\int_{A}\\eta\\;\\;=\\;\\;}}\\end{array}$   \n540 $\\begin{array}{r}{\\int_{g^{-1}(A)}g^{*}\\eta=\\int_{g^{-1}(A)}\\omega=\\int_{g^{-1}(A)}p\\,d\\mathbf{v0}_{N}=\\int_{g^{-1}(A)}p\\,d\\mu_{N}=P(g^{-1}(A))}\\end{array}$ .   \n541   \n542 Hence, we have shown the existence by an explicit construction. As $\\psi\\in\\mathcal{C}^{k+1,\\alpha}$ , and $F\\in{\\mathcal{C}}^{\\infty}$ , then   \n543 we have g \u2208Ck+1,\u03b1. \u53e3   \n544 We are now ready to show Theorem A.6 with the existence of oracle map and the low-dimensional   \n545 approximation results from [29].   \n546 Proof. (Theorem A.6) For encoder, from Proposition A.7, there exists an $\\mathcal{C}^{k+1,\\alpha}$ oracle map $g:$   \n547 $\\mathcal{M}\\rightarrow\\mathcal{Z}$ such that the pushforward measure $g_{\\#}Q=P$ . Then, ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{1}((g_{\\mathrm{enc}})_{\\#}Q,P)=W_{1}((g_{\\mathrm{enc}})_{\\#}Q,\\,g_{\\#}Q)}\\\\ &{\\qquad\\qquad\\qquad=\\operatorname*{sup}_{f\\in\\mathrm{Lip}_{1}(\\mathcal{Q})}\\left|\\int_{\\mathcal{Z}}f(y)\\,d((g_{\\mathrm{enc}})_{\\#}Q)-\\int_{\\mathcal{Z}}f(y)\\,d(g_{\\#}Q)\\right|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\operatorname*{sup}_{f\\in\\mathrm{Lip}_{1}(\\mathcal{Z})}\\int_{M}|f\\circ g_{\\mathrm{enc}}(x)-f\\circ g(x)|\\;d Q}\\\\ &{\\qquad\\qquad\\leq\\int_{M}\\|g_{\\mathrm{enc}}(x)-g(x)\\|\\;d Q}\\\\ &{\\qquad\\qquad\\leq d_{M}C(N M)^{-\\frac{2(k+1)}{d_{\\theta}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "548 where the last inequality follows from the special case $\\rho=0$ of Theorem 2.4 in [29]. ", "page_idx": 14}, {"type": "text", "text": "549 Similarly, for decoder, from Proposition A.7, there exists an $\\mathcal{C}^{k+1,\\alpha}$ oracle map $\\bar{g}:\\mathcal{Z}\\to\\mathcal{M}$ such   \n550 that the pushforward measure $\\bar{g}_{\\#}P=Q$ . ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{1}((g_{\\mathrm{dec}})_{\\#}P\\,,\\,Q)=W_{1}((g_{\\mathrm{dec}})_{\\#}P\\,,\\,\\bar{g}_{\\#}P)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\int_{\\mathcal{Z}}\\|g_{\\mathrm{dec}}(y)-\\bar{g}(y)\\|\\ d P}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq d_{\\mathcal{M}}C(N M)^{-\\frac{2(k+1)}{d_{\\theta}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "551 ", "page_idx": 14}, {"type": "text", "text": "552 B Explicit Regularization of Latent Representation Error in World Model 553 Learning ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "554 We recall the SDEs for latent dynamics model defined in the main paper. Consider a complete,   \n555 filtered probability space $(\\Omega,\\,\\mathcal{F},\\,\\{\\mathcal{F}_{t}\\}_{t\\in[0,T]},\\,\\mathbb{P}\\,)$ where independent standard Brownian motions   \n556 $B_{t}^{\\mathrm{{enc}}}$ , $B_{t}^{\\mathrm{pred}},B_{t}^{\\mathrm{seq}}$ , $B_{t}^{\\mathrm{{dec}}}$ are defined such that ${\\mathcal{F}}_{t}$ is their augmented flitration, and $T\\in\\mathbb{R}$ as the time   \n557 length of the task environment. We consider the stochastic dynamics of LDM through the following   \n558 coupled SDEs after error perturbation: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d\\,z_{t}=(q_{\\mathrm{enc}}(h_{t},s_{t})+\\sigma(h_{t},s_{t}))\\ d t+(\\bar{q}_{\\mathrm{enc}}(h_{t},s_{t})+\\bar{\\sigma}(h_{t},s_{t}))\\,d B_{t}^{\\mathrm{{enc}}},}\\\\ &{d\\,h_{t}=f(h_{t},z_{t},\\pi(h_{t},z_{t}))\\,d t+\\bar{f}(h_{t},z_{t},\\pi(h_{t},z_{t}))\\,d B_{t}^{\\mathrm{{seq}}}}\\\\ &{d\\,\\tilde{z}_{t}=p(h_{t})\\,d t+\\bar{p}(h_{t})\\,d B_{t}^{\\mathrm{pred}},}\\\\ &{d\\,\\tilde{s}_{t}=q_{\\mathrm{dec}}(h_{t},\\tilde{z}_{t})\\,d t+\\bar{q}_{\\mathrm{dec}}(h_{t},\\tilde{z}_{t})\\,d B_{t}^{\\mathrm{{dec}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "559 where $\\pi(h,\\tilde{z})$ is a policy function as a local maximizer of value function and the stochastic process   \n560 $s_{t}$ is $\\mathcal{F}_{t}$ -adapted.   \n561 As discussed in the main paper, our analysis applies to a common class of world models that uses   \n562 Gaussian distributions parameterized by neural networks\u2019 outputs for $z,\\tilde{z},\\tilde{s}$ . Their distributions are   \n563 not non-Gaussian in general.   \n564 For example, as $z$ is conditional Gaussian and its mean and variance are random variables which are   \n565 learned by the encoder from r.v.s $s$ and $h$ as inputs, thus rendering $z$ non-Gaussian. However, $z$ is   \n566 indeed Gaussian when the inputs are known. Under this conditional Gaussian class of world models,   \n567 to see that the continuous formulation of latent dynamics model can be interrupted as SDEs, one   \n568 notices that SDEs with coefficient functions of known inputs are indeed Gaussian, matching to this   \n569 class of world models. Formally, in the context of $z$ without latent representation error:   \n570 Proposition B.1. (Latent states SDE with known inputs is Gaussian)   \n571 For the latent state process $z_{t\\in[0,T]}$ without error, ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\nd\\,z_{t}=q_{e n c}(h_{t},s_{t})\\,d t+\\bar{q}_{e n c}(h_{t},s_{t}))d B_{t}^{\\{e n c}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "572 with zero initial value. Given known $h_{t\\in[0,T]}$ and $s_{t\\in[0,T]}$ , the process $z_{t}$ is a Gaussian process.   \n573 Furthermore, for any $t\\in[0,T]$ , $z_{t}$ follows a Gaussian distribution with mean $\\begin{array}{r}{\\mu_{t}=\\int_{0}^{t}q_{e n c}(h_{s},s_{s})d s}\\end{array}$   \n574 and variance $\\begin{array}{r}{\\sigma_{t}^{2}=\\int_{0}^{t}\\bar{q}_{e n c}(h_{s},s_{s})^{2}d s}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "575 Proof. Proof follows from Proposition 7.6 in [30]. ", "page_idx": 15}, {"type": "text", "text": "576 Next, we recall our assumptions from the main text: ", "page_idx": 15}, {"type": "text", "text": "577 Assumption B.2. The drift coefficient functions $q_{\\mathrm{enc}}$ , $f,\\,p$ and $q_{\\mathrm{dec}}$ and the diffusion coefficient   \n578 functions $\\bar{q}_{\\mathrm{enc}}$ , $\\bar{p}$ and $\\bar{q}_{\\mathrm{dec}}$ are bounded and Borel-measurable over the interval $[0,T]$ , and of class $\\mathcal{C}^{3}$   \n579 with bounded Lipschitz continuous partial derivatives. The initial values $z_{0},h_{0},\\tilde{z}_{0},\\tilde{s}_{0}$ are square  \n580 integrable random variables.   \n581 Assumption B.3. $\\sigma$ and $\\bar{\\sigma}$ are bounded and Borel-measurable and are of class $\\mathcal{C}^{3}$ with bounded   \n582 Lipschitz continuous partial derivatives over the interval $[0,T]$ . ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "583 One of our main results is the following: ", "page_idx": 15}, {"type": "text", "text": "584 Theorem B.4. (Explicit Regularization Induced by Zero-Drift Representation Error)   \n585 Under Assumption B.2 and B.3 and considering a loss function $\\b{\\mathscr{L}}\\in\\b{\\mathscr{C}}^{2}$ , the explicit effects of the   \n586 zero-drift error can be marginalized out as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\,\\mathcal{L}\\left(x_{t}^{\\varepsilon}\\right)=\\mathbb{E}\\,\\mathcal{L}(x_{t}^{0})+\\mathcal{R}+\\mathcal{O}(\\varepsilon^{3}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "587 as $\\varepsilon\\rightarrow0$ , where the regularization term $\\mathcal{R}$ is given by $\\begin{array}{r}{\\mathcal{R}:=\\,\\varepsilon\\,\\mathcal{P}+\\varepsilon^{2}\\left(\\mathcal{Q}+\\frac{1}{2}\\,\\mathcal{S}\\right)}\\end{array}$ .   \n588 Each term of $\\mathcal{R}$ is as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{P}:=\\mathbb{E}\\,\\nabla\\mathcal{L}(x_{t}^{0})^{\\top}\\Phi_{t}\\displaystyle\\sum_{k}\\xi_{t}^{k},}\\\\ &{\\mathcal{Q}:=\\mathbb{E}\\,\\nabla\\mathcal{L}(x_{t}^{0})^{\\top}\\Phi_{t}\\displaystyle\\int_{0}^{t}\\Phi_{s}^{-1}\\,\\mathcal{H}^{k}(x_{s}^{0},s)d B_{t}^{k},}\\\\ &{S:=\\mathbb{E}\\displaystyle\\sum_{k_{1},k_{2}}\\bigl(\\Phi_{t}\\xi_{t}^{k_{1}}\\bigr)^{i}\\nabla^{2}\\mathcal{L}(x_{t}^{0},t)\\,\\bigl(\\Phi_{t}\\xi_{t}^{k_{2}}\\bigr)^{j},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "589 where square matrix $\\Phi_{t}$ is the stochastic fundamental matrix of the corresponding homogeneous   \n590 equation: ", "page_idx": 16}, {"type": "equation", "text": "$$\nd\\Phi_{t}=\\frac{\\partial\\bar{g}_{k}}{\\partial x}(x_{t}^{0},t)\\,\\Phi_{t}\\,d B_{t}^{k},\\quad\\Phi(0)=I,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "591 and $\\xi_{t}^{k}$ is as the shorthand for $\\textstyle\\int_{0}^{t}\\Phi_{s}^{-1}\\bar{\\sigma}_{k}(x_{s}^{0},s)d B_{t}^{k}$ . Additionally, $\\mathcal{H}^{k}(x_{s}^{0},s)$ is represented by for   \n592  k1,k2\u2202xi\u2202kxj $\\begin{array}{r}{\\sum_{k_{1},k_{2}}\\frac{\\partial^{2}\\bar{g}_{k}}{\\partial x^{i}\\partial x^{j}}(x_{s}^{0},s)\\left(\\xi_{s}^{k_{1}}\\right)^{i}\\left(\\xi_{s}^{k_{2}^{\\circ}}\\right)^{j}}\\end{array}$ .   \n593 Before proving Theorem B.4, we first show Proposition B.5 on the general case of perturbation to the   \n594 stochastic system. Consider the following perturbed system given by ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\nd\\,x_{t}=\\left(g_{0}\\left(x_{t},t\\right)+\\varepsilon\\,\\eta_{0}\\left(x_{t},t\\right)\\right)d t+\\sum_{k=1}^{m}\\left(g_{k}\\left(x_{t},t\\right)+\\varepsilon\\,\\eta_{k}\\left(x_{t},t\\right)\\right)d B_{t}^{k}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "595 with initial values $x(0)=x_{0}$ , ", "page_idx": 16}, {"type": "text", "text": "596 Proposition B.5. Suppose that $f$ is a real-valued function that is $\\mathcal{C}^{2}$ . Then it holds that, with   \n597 probability $^{\\,l}$ , as $\\varepsilon\\rightarrow0,$ , for $t\\in[0,T],$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\bf\\Xi}^{\\mathrm{\\tiny~\\textnormal{\\tiny~r}}}(x_{t}^{\\varepsilon})=f\\left(x_{t}^{0}\\right)+\\varepsilon\\nabla f\\left(x_{t}^{0}\\right)^{\\top}\\partial_{{\\varepsilon}}\\,x_{t}^{0}+{\\varepsilon}^{2}\\left(\\nabla f\\left(x_{t}^{0}\\right)^{\\top}\\partial_{{\\varepsilon}}^{2}x_{t}^{0}\\;+\\frac12\\partial_{{\\varepsilon}}\\,x_{t}^{0}{\\bf\\Xi}^{\\top}\\nabla^{2}f\\left(x_{t}^{0}\\right)\\partial_{{\\varepsilon}}\\,x_{t}^{0}\\right)+\\mathcal{O}\\left({\\varepsilon}^{3}\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "598 where the stochastic process $\\boldsymbol{x}_{t}^{0}$ is the solution to SDE 32 with $\\varepsilon=0$ , with its first and second-order   \n599 derivatives w.r.t $\\varepsilon$ denoted as $\\bar{\\partial}_{\\varepsilon}\\,x_{t}^{0},\\partial_{\\varepsilon}^{2}\\,x_{t}^{0}$ .   \n600 Furthermore, it holds that $\\partial_{\\varepsilon}\\,x_{t}^{0},\\partial_{\\varepsilon}^{2}\\,x_{t}^{0}$ satisfy the following SDEs with probability $^{\\,l}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle d\\,\\partial_{\\varepsilon}x_{t}^{0}=\\left(\\frac{\\partial g_{k}}{\\partial x}\\left(x_{t}^{0},t\\right)\\partial_{\\varepsilon}x_{t}^{0}+\\eta_{k}\\left(x_{t}^{0},t\\right)\\right)d B_{t}^{k},}\\\\ {\\displaystyle d\\,\\partial_{\\varepsilon}^{2}x_{t}=\\left(\\Psi_{k}\\left(\\partial_{\\varepsilon}x_{t}^{0},x_{t}^{0},t\\right)+2\\frac{\\partial\\eta_{k}}{\\partial x}\\left(x_{t}^{0},t\\right)\\partial_{\\varepsilon}x_{t}^{0}+\\frac{\\partial g_{k}}{\\partial x}\\left(x_{t}^{0},t\\right)\\partial_{\\varepsilon}^{2}x_{t}^{0}\\right)d B_{t}^{k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "601 with initial values $\\partial_{\\varepsilon}\\,x(0)=0,\\partial_{\\varepsilon}^{2}\\,x(0)=0,$ , where ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Psi_{k}:(\\partial_{\\varepsilon}\\,x,x,t)\\mapsto\\partial_{\\varepsilon}\\,x^{i}\\frac{\\partial g_{k}}{\\partial x^{i}\\partial x^{j}}(x,t)\\partial_{\\varepsilon}\\,x^{j},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "602 for $k=0,1,...,m$ . ", "page_idx": 16}, {"type": "text", "text": "603 Proof. We first apply the stochastic version of perturbation theory to SDE 32. For brevity, we will   \n604 write $t$ as $B_{t}^{0}$ and use Einstein summation convention. Hence, SDE 32 is rewritten as ", "page_idx": 16}, {"type": "equation", "text": "$$\nd x_{t}=\\gamma_{k}^{\\varepsilon}\\left(x_{t},t\\right)d B_{t}^{k},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "605 with initial value $x(0)=x_{0}$ . ", "page_idx": 16}, {"type": "text", "text": "606 Step $^{\\,l}$ : We begin with the corresponding systems to derive the SDEs that characterize $\\partial_{\\varepsilon}\\,x_{t}^{\\varepsilon}$ and $\\partial_{\\varepsilon}^{2}\\,x_{t}^{\\varepsilon}$ .   \n607 Our main tool is an important result on smoothness of solutions w.r.t. initial data from Theorem 3.1   \n608 from Section 2 in [17]. ", "page_idx": 16}, {"type": "text", "text": "609 For $\\partial_{\\varepsilon}\\,x$ , consider the SDEs ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{d\\,x_{t}=\\gamma_{k}^{\\varepsilon}\\left(x_{t},t\\right)d B_{t}^{k},}}\\\\ {{d\\,\\varepsilon_{t}=0,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "610 with initial values $x_{(0)}=x_{0},\\varepsilon(0)=\\varepsilon$ . From an application of Theorem 3.1 from Section 2 in [17]   \n611 on \\*, we have $\\partial_{\\varepsilon}\\,x$ that satisfies the following SDE with probability 1: ", "page_idx": 17}, {"type": "equation", "text": "$$\nd\\,\\partial_{\\varepsilon}x_{t}=\\left(\\alpha_{k}^{\\varepsilon}\\left(x_{t},t\\right)\\partial_{\\varepsilon}x_{t}+\\eta_{k}\\left(x_{t},t\\right)\\right)d B_{t}^{k},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "612 with initial value $\\partial_{\\varepsilon}x_{0}=0\\in\\mathbb{R}^{n}$ , with probability 1, where $x_{t}$ is the solution to Equation (35) and   \n613 the functions $\\alpha_{k}^{\\varepsilon}$ are given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\alpha_{k}^{\\varepsilon}:(x,t)\\mapsto\\frac{\\partial g_{k}}{\\partial x^{j}}\\left(x,t\\right)+\\varepsilon\\frac{\\partial\\eta_{k}}{\\partial x^{j}}\\left(x,t\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "614 where $k=0,\\,...,\\,m.$ ", "page_idx": 17}, {"type": "text", "text": "615 To characterize $\\partial_{\\varepsilon}^{2}\\,x_{t}$ , consider the following SDEs ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d\\,x_{t}=\\gamma_{k}^{\\varepsilon}\\left(x_{t},t\\right)d B_{t}^{k},}\\\\ &{d\\,\\partial_{\\varepsilon}\\,x_{t}=\\left(\\alpha_{k}^{\\varepsilon}\\left(x_{t},t\\right)\\partial_{\\varepsilon}\\,x_{t}+\\eta_{k}\\left(x_{t},t\\right)\\right)d B_{t}^{k},}\\\\ &{d\\,\\varepsilon_{t}=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "616 with initial value $x(0)=x_{0},\\,\\partial_{\\varepsilon}\\,x(0)=0,\\,\\varepsilon(0)=\\varepsilon.$ ", "page_idx": 17}, {"type": "text", "text": "617 From a similar application of Theorem 3.1 from Section 2 in [17], the second derivative $\\partial_{\\varepsilon}^{2}\\,x$ satisfies   \n618 the following SDE with probability 1: ", "page_idx": 17}, {"type": "equation", "text": "$$\nd\\,\\partial_{\\varepsilon}^{2}\\,x_{t}=\\left(\\beta_{k}^{\\varepsilon}\\left(\\partial_{\\varepsilon}x_{t},x_{t},t\\right)+2\\frac{\\partial\\,\\eta_{k}}{\\partial x}\\left(x_{t},t\\right)\\partial_{\\varepsilon}\\,x_{t}+\\alpha_{k}^{\\varepsilon}\\left(x_{t},t\\right)\\partial_{\\varepsilon}^{2}x_{t}\\right)d B_{t}^{k},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "619 with initial value $\\partial_{\\varepsilon}^{2}\\,x(0)=0\\in\\mathbb R^{n}$ , where $\\partial_{\\varepsilon}\\,x_{t}$ is the solution to Equation(36), $x(t)$ is the solution   \n620 to Equation (35), and the functions ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\beta_{k}^{\\varepsilon}:(\\partial_{\\varepsilon}\\boldsymbol{x},\\boldsymbol{x},t)\\mapsto\\partial_{\\varepsilon}\\,\\boldsymbol{x}^{j}\\left(\\frac{\\partial g_{k}^{i}}{\\partial x^{i}\\partial x^{j}}(\\boldsymbol{x},t)+\\varepsilon\\frac{\\partial\\eta_{k}^{i}}{\\partial x^{l}\\partial x^{j}}(\\boldsymbol{x},t)\\right)\\partial_{\\varepsilon}\\,\\boldsymbol{x}^{l},\\mathrm{where~}k=0,\\,\\dots}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "622 When $\\varepsilon=0$ in the obtained SDEs (35), (36) and (37), the corresponding solutions of which are   \n623 $x_{t}^{0},\\partial_{\\varepsilon}\\,x_{t}^{0},\\partial_{\\varepsilon}^{2}\\,x_{t}^{0}$ , we now have the following: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle d\\,{x_{t}^{0}}={g_{k}}\\left(x_{t}^{0},t\\right)d B_{t}^{k},}\\\\ {\\displaystyle d\\,{\\partial_{\\varepsilon}\\,x_{t}^{0}}=\\left(\\frac{\\partial{g_{k}}}{\\partial x}\\left(x_{t}^{0},t\\right){\\partial_{\\varepsilon}\\,x^{0}}+{\\eta_{k}}\\left(x_{t}^{0},t\\right)\\right)d B_{t}^{k},}\\\\ {\\displaystyle d\\,{\\partial_{\\varepsilon}^{2}\\,x_{t}^{0}}=\\left(\\Psi_{k}\\left({\\partial_{\\varepsilon}\\,x_{t}^{0}},x_{t}^{0},t\\right)+2\\frac{\\partial{\\eta_{k}}}{\\partial x}\\left(x_{t}^{0},t\\right){\\partial_{\\varepsilon}\\,x_{t}^{0}}+\\frac{\\partial{g_{k}}}{\\partial x}\\left(x_{t}^{0},t\\right){\\partial_{\\varepsilon}^{2}\\,x_{t}^{0}}\\right)d B_{t}^{k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "624 with initial values $\\begin{array}{r}{x(0)=x_{0},\\partial_{\\varepsilon}\\,x(0)=0,\\partial_{\\varepsilon}^{2}\\,x(0)=0}\\end{array}$ . In particular, $\\Psi_{k}:=\\beta_{k}^{0}$ is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n(\\partial_{\\varepsilon}x,x,t)\\mapsto\\partial_{\\varepsilon}x^{i}\\frac{\\partial g_{k}}{\\partial x^{i}\\partial x^{i}}(x,t)\\partial_{\\varepsilon}x^{j}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "625 Step 2: For the next step, we show that the solutions $x_{t}^{0},\\partial_{s}\\,x_{t}^{0},\\partial_{\\varepsilon}^{2}\\,x_{t}^{0}$ are indeed bounded by proving   \n626 the following lemma B.6: ", "page_idx": 17}, {"type": "text", "text": "Lemma B.6. ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\left\\|x_{t}^{0}\\right\\|^{2},\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\left\\|\\partial_{\\varepsilon}\\,x_{t}^{0}\\right\\|^{2},a n d\\,\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\left\\|\\partial_{\\varepsilon}^{2}\\,x_{t}^{0}\\right\\|^{2}\\;a r e\\;b o u n d e d.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "627 Proof. To simplify the notations, we take the liberty to write constants as $C$ and notice that $C$ is not   \n628 necessarily identical in its each appearance. ", "page_idx": 17}, {"type": "text", "text": "629 (1) We first show that $\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\left\\|x_{t}^{0}\\right\\|^{2}$ is bounded. ", "page_idx": 17}, {"type": "text", "text": "From Equation (38), we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\nx_{t}^{0}=x_{0}+\\int_{0}^{t}g_{k}\\left(x_{\\tau},\\tau\\right)d B_{\\tau}^{k}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "630 By Jensen\u2019s inequality. it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\left\\|x_{t}\\right\\|^{2}\\leq C\\,\\mathbb{E}\\left\\|x_{0}\\right\\|^{2}+C\\,\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\left\\|\\int_{0}^{t}g_{k}\\left(x_{\\tau}^{0},\\tau\\right)d B_{\\tau}^{k}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "631 For the second term on the right hand side, it is a sum over $k$ from 0 to $m$ by Einstein notation.   \n632 For $k=0$ , recall that we write $t$ as $B_{t}^{0}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\left\\|\\int_{0}^{t}g_{0}\\left(x_{\\tau}^{0},\\tau\\right)d\\tau\\right\\|^{2}\\leq C\\,\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}t\\int_{0}^{t}\\left\\|g_{0}\\left(x_{\\tau}^{0},\\tau\\right)\\right\\|^{2}d\\tau,}}\\\\ &{}&{\\leq C\\,\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\int_{0}^{t}C\\left(1+\\left\\|x_{\\tau}^{0}\\right\\|\\right)^{2}d\\tau,}\\\\ &{}&{\\leq C+C\\int_{0}^{T}\\mathbb{E}\\operatorname*{sup}_{s\\in[0,\\tau]}\\left\\|x_{s}^{0}\\right\\|^{2}d\\tau,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "633 where we used Jensen\u2019s inequality, the assumption on the linear growth, the inequality property of   \n634 sup and Fubini\u2019s theorem, respectively. ", "page_idx": 18}, {"type": "text", "text": "635 For $k$ is equal to $1,\\ldots,m$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\operatorname{\\tau}\\in[0,T]}\\left\\|\\displaystyle\\int_{0}^{t}g_{1}\\left(x_{\\tau,\\tau}^{0},\\tau\\right)d B_{\\tau}\\right\\|^{2}\\leq C\\,\\mathbb{E}\\displaystyle\\int_{0}^{T}\\left\\|g_{1}\\left(x_{\\tau}^{0},\\tau\\right)\\right\\|^{2}d\\tau,}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq C+C\\displaystyle\\int_{0}^{T}\\mathbb{E\\operatorname*{sup}_{s\\in[0,\\tau]}\\left\\|x_{s}^{0}\\right\\|}d\\tau,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "636 where (iv) holds from the Burkholder-Davis-Gundy inequality as $\\begin{array}{r}{\\int_{0}^{t}g_{k}\\left(x_{\\tau}^{0},\\tau\\right)d B_{\\tau}}\\end{array}$ is a continuous   \n637 local martingale with respect to the filtration $\\mathcal{F}_{t}$ ; and then one can obtain (v) by following a similar   \n638 reasoning of (ii) and (iii). ", "page_idx": 18}, {"type": "text", "text": "Hence, now from the previous inequality (41), ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\left\\lVert x_{t}^{0}\\right\\rVert^{2}\\leq\\mathbb{E}\\left\\lVert x_{0}\\right\\rVert^{2}+C+C\\int_{0}^{T}\\mathbb{E}\\operatorname*{sup}_{s\\in[0,\\tau]}\\left\\lVert x_{s}^{0}\\right\\rVert d\\tau.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By the Gronwall\u2019s lemma, it holds true that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\left\\|x_{t}^{0}\\right\\|^{2}\\leq\\Big(C\\,\\mathbb{E}\\left\\|x_{0}\\right\\|^{2}+C\\Big)\\exp(C).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "639 As $x_{0}$ is square-integrable by assumption, therefore we have shown that $\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\left\\Vert x_{t}^{0}\\right\\Vert^{2}$ is   \n640 bounded. ", "page_idx": 18}, {"type": "text", "text": "641 (2) We then show that $\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}||\\partial_{\\varepsilon}\\,x_{t}^{0}||^{2}$ is also bounded. ", "page_idx": 18}, {"type": "text", "text": "From the SDE (39), as we have derived that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\partial_{\\varepsilon}\\,x_{t}^{0}=\\int_{0}^{t}\\frac{\\partial g_{k}}{\\partial x}\\left(x_{\\tau}^{0},\\tau\\right)\\partial_{\\varepsilon}\\,x_{\\tau}^{0}+\\eta_{k}\\left(x_{\\tau}^{0},\\tau\\right)d B_{\\tau}^{k},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "then we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\textup{\\texttt{z u p}}\\big\\|\\partial_{\\varepsilon}\\,x_{t}^{0}\\big\\|^{2}\\leq C\\,\\mathbb{E}_{\\mathrm{\\t}\\in[0,\\tau]}\\left\\|\\int_{0}^{t}\\frac{\\partial g_{k}}{\\partial x}\\left(x_{\\tau}^{0},\\tau\\right)\\partial_{\\varepsilon}\\,x_{\\tau}^{0}\\,d B_{\\tau}^{k}\\right\\|^{2}+C\\,\\mathbb{E}_{\\mathrm{\\t}\\in[0,T]}\\left\\|\\int_{0}^{t}\\eta_{k}\\left(x_{\\tau}^{0},\\tau\\right)d B_{\\tau}^{k}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "642 For $k=0$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\underset{t\\in[0,T]}{\\operatorname*{sup}}\\left\\|\\int_{0}^{t}\\frac{\\partial g_{0}}{\\partial x}\\left(x_{\\tau}^{0},\\tau\\right)\\partial_{\\varepsilon}x_{\\tau}^{0}d t\\right\\|^{2}+\\mathbb{E}\\underset{t\\in[0,T]}{\\operatorname*{sup}}\\left\\|\\int_{0}^{t}r_{0}\\left(x_{\\tau}^{0},\\tau\\right)d\\tau\\right\\|^{2},}\\\\ &{\\leq C\\ensuremath{{\\mathbb E}}\\underset{t\\in[0,T]}{\\operatorname*{sup}}\\int_{0}^{t}\\left\\|\\frac{\\partial g_{0}}{\\partial x}\\left(x_{\\tau}^{0},t\\right)\\right\\|^{2}\\left\\|\\partial_{\\varepsilon}x_{\\tau}^{0}\\right\\|^{2}d\\tau+C\\ensuremath{{\\mathbb E}}\\underset{t\\in[0,T]}{\\operatorname*{sup}}\\int_{0}^{t}\\left\\|\\eta_{0}\\left(x_{\\tau}^{0},\\tau\\right)\\right\\|^{2}d\\tau,}\\\\ &{\\leq C\\ensuremath{{\\mathbb E}}\\underset{s\\in[0,T]}{\\operatorname*{sup}}\\left\\|\\frac{\\partial g_{0}}{\\partial x}\\left(x_{s}^{0},s\\right)\\right\\|^{2}\\underset{t\\in[0,T]}{\\operatorname*{sup}}\\int_{0}^{t}\\left\\|\\partial_{\\varepsilon}x_{\\tau}^{0}\\right\\|^{2}d\\tau+C\\ensuremath{{\\mathbb E}}\\underset{t\\in[0,T]}{\\operatorname*{sup}}\\int_{0}^{t}C\\left(1+\\left\\|x_{\\tau}^{0}\\right\\|\\right)^{2}d\\tau,}\\\\ &{\\leq C+C\\underset{t\\in[0,T]}{\\operatorname*{sup}}\\int_{0}^{t}\\left\\|\\partial_{\\varepsilon}x_{\\tau}^{0}\\right\\|^{2}d\\tau+C\\underset{t\\in[0,T]}{\\operatorname*{sup}}\\int_{0}^{t}\\left\\|x_{\\tau}^{0}\\right\\|^{2}d\\tau,}\\\\ &{\\leq C+C\\int_{0}^{T}\\underset{s\\in[0,T]}{\\operatorname*{sup}}\\left\\|\\partial_{\\varepsilon}x_{s}^{0}\\right\\|^{2}d\\tau+C\\underset{t\\in[0,T]}{\\operatorname*{sup}}\\left\\|x_{\\tau}^{0}\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "643 where to get to (vi), we used Jensen\u2019s inequality; for (vii), we used the linear growth assumption an   \n644 $\\eta_{0}$ , then we obtain (viii) by as derivatives of function $g_{0}$ are bounded by assumption.   \n645 Similarly, for $k=1,\\,...,\\,m,$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad C\\mathbb{E}\\underset{t\\in[0,T]}{\\operatorname*{sup}}\\left\\|\\int_{0}^{t}\\frac{\\partial g_{1}}{\\partial x^{i}}\\left(x_{\\tau}^{0},\\tau\\right)\\partial_{\\varepsilon}x_{\\tau}^{0}d B_{\\tau}\\right\\|^{2}+C\\mathbb{E}\\underset{t\\in[0,T]}{\\operatorname*{sup}}\\left\\|\\int_{0}^{t}\\eta_{1}\\left(x_{\\tau}^{0},\\tau\\right)d B_{\\tau}\\right\\|^{2},}\\\\ &{\\leq C\\mathbb{E}\\int_{0}^{T}\\left\\|\\frac{\\partial g_{1}}{\\partial x}\\left(x_{\\tau}^{0},\\tau\\right)\\right\\|^{2}\\left\\|\\partial_{\\varepsilon}x_{\\tau}^{0}\\right\\|^{2}d\\tau+C\\mathbb{E}\\int_{0}^{T}\\left\\|\\eta_{1}\\left(x_{\\tau}^{0},\\tau\\right)\\right\\|^{2}d\\tau,}\\\\ &{\\leq C+C\\int_{0}^{T}\\mathbb{E}\\underset{s\\in[0,T]}{\\operatorname*{sup}}\\,||\\partial_{\\varepsilon}x_{s}^{0}||^{2}d\\tau+C\\mathbb{E}\\underset{t\\in[0,T]}{\\operatorname*{sup}}\\,||x_{t}^{0}||^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "646 where we obtain (ix) by the Burkholder-Davis-Gundy inequality and $({\\bf{x}})$ by following similar steps as   \n647 have shown in (vii) and (viii).   \n648 We are now ready to sum up each term to acquire a new inequality: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\left\\|\\partial_{\\varepsilon}\\left.x_{t}^{0}\\right\\|^{2}\\leq C+C\\,\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\left\\|x_{t}^{0}\\right\\|^{2}+C\\,\\int_{0}^{T}\\mathbb{E}\\operatorname*{sup}_{s\\in[0,\\tau]}\\left\\|\\partial_{\\varepsilon}\\,x_{s}^{0}\\right\\|^{2}d\\tau.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "649 By Gronwall\u2019s lemma, we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\left\\|\\partial_{\\varepsilon}\\,x_{t}^{0}\\right\\|^{2}\\leq\\left(C+C\\,\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\left\\|x_{t}^{0}\\right\\|^{2}\\right)\\exp(C).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "650 As it is previously shown that $\\mathbb{E}\\operatorname*{sup}_{t\\in[0,\\tau]}\\left\\|x^{\\circ}(t)\\right\\|^{2}$ is bounded, it is clear that $\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\left\\|\\partial_{\\varepsilon}\\,x_{t}^{0}\\right\\|^{2}$   \n651 is bounded too. ", "page_idx": 19}, {"type": "text", "text": "652 (3) From similar steps, one can also show that $\\mathbb{E}_{t\\in[0,T]}\\left\\|\\partial_{\\varepsilon}^{2}\\,x_{t}^{0}\\right\\|^{2}$ is bounded. ", "page_idx": 19}, {"type": "text", "text": "653 Step 3: Having shown that $x_{t}^{0},\\partial_{\\varepsilon}\\,x_{t}^{0},\\partial_{\\varepsilon}^{2}\\,x_{t}^{0}$ are bounded, we proceed to bound the remainder term by   \n654 proving the following lemma. ", "page_idx": 19}, {"type": "text", "text": "Lemma B.7. For a given $\\varepsilon\\in\\mathbb{R}_{;}$ , let ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{R}^{\\varepsilon}:=(t,\\omega)\\mapsto\\frac{1}{\\varepsilon^{3}}\\left(x^{\\varepsilon}(t,\\omega)-x^{0}(t,\\omega)-\\varepsilon\\partial_{\\varepsilon}x^{0}(t,\\omega)-\\varepsilon^{2}\\partial_{\\varepsilon}^{2}\\,x^{0}(t,\\omega)\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "655 where the stochastic process $\\boldsymbol{x}_{t}^{\\varepsilon}$ is the solution to Equation (32). Then it holds true that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\left\\|\\mathcal{R}^{\\varepsilon}(t)\\right\\|^{2}\\;i s\\;b o u n d e d.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. The main strategy of this proof is to first rewrite $\\varepsilon^{3}\\mathcal{R}^{\\varepsilon}$ as the sum of some simpler terms and then to bound each term. To simplify the notation, we denote $\\tilde{x}_{t}^{\\varepsilon}$ as $x_{t}^{0}+\\varepsilon\\partial_{\\varepsilon}\\,x_{t}^{0}+\\varepsilon^{2}\\,\\dot{\\partial}_{\\varepsilon}^{2}x_{t}^{0}$ . For $k=0,..,n$ , we define the following terms: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathit{\\Sigma}_{k}(t):=\\int_{0}^{t}{\\mathcal{g}}_{k}\\left(x_{\\tau}^{\\varepsilon},\\tau\\right)-{g}_{k}\\left(\\tilde{x}_{\\tau}^{\\varepsilon},\\tau\\right)d B_{\\tau}^{k},}}\\\\ {{\\displaystyle{\\mathcal{I}}_{k}(t):=\\int_{0}^{t}{\\mathcal{g}}_{k}\\left(\\tilde{x}_{\\tau}^{\\varepsilon},\\tau\\right)-{g}_{k}\\left(x_{\\tau}^{0},\\tau\\right)-\\varepsilon\\frac{\\partial{g}_{k}}{\\partial x}\\left(x_{\\tau}^{0},\\tau\\right)\\partial_{\\varepsilon}{x}_{\\tau}^{0}-\\varepsilon^{2}\\Psi_{k}\\left(\\partial_{\\varepsilon}{x}_{\\tau}^{0},x_{\\tau}^{0},\\tau\\right)-\\varepsilon^{2}\\frac{\\partial{g}_{k}}{\\partial x^{i}}\\left(x_{\\tau}^{0},\\tau\\right)\\partial_{\\tau}{x}_{\\tau}^{0}-\\varepsilon^{2}\\Psi_{k}\\left({x}_{\\tau}^{0},\\tau\\right)}}\\\\ {{\\displaystyle{\\mathrm{\\Sigma}}_{k}(t):=-\\varepsilon\\int_{0}^{t}{\\mathcal{I}}_{k}\\left({x}_{\\tau}^{0},\\tau\\right)+2\\varepsilon\\frac{\\partial\\eta}{\\partial x}\\left(x_{\\tau}^{0},\\tau\\right)\\partial_{\\varepsilon}{x}_{\\tau}^{0}d B_{\\tau}^{k}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "656 Hence, we have $\\begin{array}{r}{\\varepsilon^{3}\\mathcal{R}^{\\varepsilon}(t)=\\sum_{k=0}^{1}\\theta_{k}(t)+\\varphi_{k}(t)+\\sigma_{k}(t).}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "657 For $\\theta_{k}(t)$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\|\\theta_{k}(t)\\|^{2}\\leq C\\,\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\int_{0}^{t}\\left\\|g_{k}\\left(x_{\\varphi}^{\\varepsilon},e\\right)-g_{k}\\left(\\tilde{x}_{\\varphi}^{\\varepsilon},\\tau\\right)\\right\\|^{2}d\\tau,}}\\\\ &{}&{\\leq C\\,\\int_{0}^{T}\\mathbb{E}\\operatorname*{sup}_{t\\in[0,t a u]}\\|x_{t}^{\\varepsilon}-\\tilde{x}_{t}^{\\varepsilon}\\|^{2}\\,d\\tau,}\\\\ &{}&{\\leq C\\,\\int_{0}^{T}\\mathbb{E}\\operatorname*{sup}_{t\\in[0,\\tau]}\\|\\mathcal{R}^{\\varepsilon}(t)\\|^{2}\\,d\\tau,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "658 where to obtain (i) we used Jensen\u2019s inequality when $k=0$ and by the Burkholder-Davis-Gundy   \n659 inequality when $k=1$ , used the Lipschitz condition of $g_{k}$ to obtain (ii), and for (iii), it is because   \n660 $\\varepsilon^{3}\\dot{\\mathcal{R}}^{\\varepsilon}(t)\\dot{=\\ x}_{t}^{\\varepsilon}-x_{t}^{\\varepsilon}$ . ", "page_idx": 20}, {"type": "text", "text": "661 We note that from Taylor\u2019s theorem, for any $s\\in[0,t]$ , $k=0,1$ , there exists some $\\varepsilon_{s}\\in(0,\\varepsilon)$ s.t. ", "page_idx": 20}, {"type": "equation", "text": "$$\ng_{k}\\left(\\tilde{x}_{s}^{\\varepsilon},s\\right)-g_{k}\\left(x_{s}^{0},s\\right)-\\varepsilon\\frac{\\partial g_{k}}{\\partial x}\\left(x_{s}^{0},s\\right)\\partial_{\\varepsilon}x_{s}^{0}=\\varepsilon^{2}\\frac{\\partial g_{k}}{\\partial x}\\left(\\tilde{x}_{s}^{\\varepsilon_{s}}\\right)\\partial_{\\varepsilon}^{2}x_{s}^{0}+\\varepsilon^{2}\\Psi\\left(\\partial_{\\varepsilon}x_{s}^{0},\\tilde{x}_{s}^{\\varepsilon_{s}},s\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "662 For $\\varphi_{k}(t)$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\underset{t\\in[0,T]}{\\operatorname*{sup}}\\left\\|\\phi_{t}(k)\\right\\|^{2}}\\\\ &{\\leq C\\underset{t\\in[0,T]}{\\operatorname*{sup}}\\int_{0}^{t}\\left\\|\\frac{\\partial g_{t}}{\\partial x}(\\tilde{x}_{s}^{t})\\partial_{x}^{2}x_{s}^{o}+\\Psi_{k}\\left(\\partial_{x}x_{s}^{0},\\tilde{x}_{s}^{t},s\\right)-\\frac{\\partial g_{k s}}{\\partial x}\\left(x_{s}^{0}\\right)\\partial_{x}^{2}z_{s}^{o}-\\Psi_{k}\\left(\\partial_{x}x_{s}^{0},x_{s}^{0},s\\right)\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\Big(\\theta\\Big)\\,}\\\\ &{\\leq C\\underset{t\\in[0,T]}{\\operatorname*{sup}}\\int_{0}^{t}\\left\\|\\frac{\\partial g_{t}}{\\partial x}(\\tilde{x}_{s}^{t})-\\frac{\\partial g_{k s}}{\\partial x}\\left(x_{s}^{0}\\right)\\right\\|^{2}\\left\\|\\phi_{s}^{2}x_{s}^{o}\\right\\|^{2}+\\left\\|\\Psi_{k}\\left(\\partial_{x}x_{s}^{0},\\tilde{x}_{s},s\\right)-\\Psi_{k}\\left(\\partial_{x}x_{s}^{0},x_{s}^{0},s\\right)\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\Big(\\theta\\Big)\\,}\\\\ &{\\leq C\\underset{t\\in[0,T]}{\\operatorname*{sup}}\\int_{0}^{t}\\left\\|z_{s}^{t}-x_{s}^{0}\\right\\|^{2}\\left(C+\\left\\|\\tilde{g}_{s}^{2}x_{s}^{0}\\right\\|^{2}\\right)d s,}\\\\ &{\\leq C\\underset{t\\in[0,T]}{\\operatorname*{sup}}\\int_{0}^{t}\\left\\|\\phi_{s}^{2}x_{s}^{0}+c^{2}\\partial_{x}^{2}x_{s}^{0}\\right\\|^{2}\\left(C+\\left\\|\\tilde{g}_{s}^{2}x_{s}^{0}\\right\\|^{2}\\right)d s,}\\\\ &{\\leq C\\underset{t\\in[0,T]}{\\operatorname*{sup}}\\left\\|\\frac{\\partial\\alpha}{\\partial x}\\alpha_{s}^{0}\\right\\|^{2}+\\underset{t\\in[0,T]}{\\operatorname*{sup}}\\left\\|\\tilde{g}_{s}^{2 \n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "663 where for (iv), we used Equation (42) and Jensen\u2019s inequality for $k=0$ and the Burkholder-Davis  \n664 Gundy inequality for $k=1$ ; to obtain (v), we applied Jensen\u2019s equality; we then derived (vi) from   \n665 the Lipschitz conditions of $g_{k}$ and $\\Psi_{k}$ ; and finally another application of Jensen\u2019s inequality gives   \n666 (vii) which is bounded as a result from the Lemma B.6. ", "page_idx": 20}, {"type": "text", "text": "667 ", "page_idx": 20}, {"type": "text", "text": "668 For $\\sigma_{k}(t)$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\operatorname*{sup}_{\\epsilon[0,T]}\\|\\sigma_{0}(t)\\|^{2}\\leq C\\varepsilon\\int_{0}^{T}\\mathbb{E}\\displaystyle\\operatorname*{sup}_{s\\in[0,t]}\\left\\|\\eta_{k}\\left(x_{s}^{0},s\\right)\\right\\|^{2}+C\\mathbb{E}_{s\\in[0,t]}\\left\\|\\frac{\\partial\\eta_{k}}{\\partial x}\\left(x_{s}^{0},s\\right)\\right\\|^{2}\\left\\|\\partial_{\\varepsilon}x_{s}^{0}\\right\\|^{2}d t,\\quad\\mathrm{(ix)}}\\\\ {\\leq C\\displaystyle\\int_{0}^{T}C\\left(1+\\mathbb{E}_{\\displaystyle\\operatorname*{st}[0,t]}\\left\\|x_{s}^{0}\\right\\|^{2}\\right)+C\\mathbb{E}_{\\displaystyle t\\in[0,T]}\\left\\|\\frac{\\partial\\eta_{k}}{\\partial x}\\left(x_{t}^{0},t\\right)\\right\\|^{2}\\displaystyle\\int_{0}^{T}\\mathbb{E}_{\\displaystyle\\operatorname*{st}[0,t]}\\left\\|\\partial_{\\varepsilon}x_{s}^{0}\\right\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\leq c+C\\,\\mathbb{E}\\operatorname*{sup}_{t}\\in\\left[0,T\\right]\\left\\Vert x_{s}^{0}\\right\\Vert^{2}+C\\,\\mathbb{E}\\operatorname*{sup}_{t\\in\\left[0,T\\right]}\\left\\Vert\\frac{\\partial\\eta}{\\partial x}\\left(x_{t}^{0},t\\right)\\right\\Vert^{2}\\mathbb{E}\\operatorname*{sup}_{t\\in\\left[0,T\\right]}\\left\\Vert\\partial_{\\varepsilon}x_{t}^{0}\\right\\Vert^{2},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "669 where we obtained (ix) by Jensen\u2019s inequality when $k=0$ and by Burkholder-Davis-Gundy inequality   \n670 when $k=1$ , and $({\\bf{x}})$ by the linear growth assumption on $\\eta_{k}$ ; one can see that (xi) is bounded by   \n671 recalling the Lemma B.6 and the assumption that $\\eta_{k}$ has bounded derivatives. ", "page_idx": 21}, {"type": "text", "text": "672 Hence, by Jensen\u2019s inequality and Gronwall\u2019s lemma, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\underset{t\\in[0,T]}{\\operatorname*{sup}}\\left\\Vert\\mathcal{R}^{\\varepsilon}(t)\\right\\Vert^{2}\\leq C\\underset{k=0}{\\overset{K}{\\sum}}\\mathbb{E}\\underset{t\\in[0,T]}{\\operatorname*{sup}}\\left\\Vert\\theta_{k}(t)\\right\\Vert^{2}+\\mathbb{E}\\underset{t\\in[0,T]}{\\operatorname*{sup}}\\left\\Vert\\varphi_{k}(t)\\right\\Vert^{2}+\\mathbb{E}\\underset{t\\in[0,T]}{\\operatorname*{sup}}\\left\\Vert\\sigma_{k}(t)\\right\\Vert^{2},}\\\\ &{\\qquad\\qquad\\leq C+C\\int_{0}^{T}\\underset{t\\in[0,\\tau]}{\\operatorname*{sup}}\\left\\Vert\\mathcal{R}^{\\varepsilon}(t)\\right\\Vert^{2}d\\tau,}\\\\ &{\\qquad\\qquad\\leq C\\exp\\left(C\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "673 Therefore, $\\mathbb{E}\\operatorname{sup}\\left\\|\\mathcal{R}^{\\varepsilon}(t)\\right\\|^{2}$ is bounded. ", "page_idx": 21}, {"type": "text", "text": "674 ", "page_idx": 21}, {"type": "text", "text": "675 Finally, it is now straightforward to show Equation (33) by applying a second-order Taylor expansion   \n676 on $f\\left(x_{t}^{0}+\\varepsilon\\partial_{\\varepsilon}x_{t}^{0}+\\check{\\varepsilon}^{2}\\partial_{\\varepsilon}^{2}x_{t}^{0}\\;+\\varepsilon^{3}R^{\\varepsilon}(t)\\right)$ . ", "page_idx": 21}, {"type": "text", "text": "677 ", "page_idx": 21}, {"type": "text", "text": "678 We are now ready to show Theorem 3.7. One notes that Corollary 3.8 directly follows from the result   \n679 too.   \n680 Proof. (Theorem 3.7) From Proposition B.5, it is noteworthy to point out that the derived SDEs (34)   \n681 for $\\ {\\dot{\\partial}}_{\\varepsilon}\\,x_{t}^{0}$ and $\\partial_{\\varepsilon}^{2}\\,x_{t}^{0}$ are vector-valued general linear SDEs. With some steps of derivations, one can   \n682 express the solutions as: ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{\\displaystyle\\partial_{\\varepsilon}\\,x_{t}^{0}=\\Phi_{t}\\int_{0}^{t}\\Phi_{s}^{-1}\\left(\\eta_{0}(x_{s}^{0},s)-\\displaystyle\\sum_{k=1}^{m}\\frac{\\partial g_{k}}{\\partial x}(x_{s}^{0},s)\\eta_{k}(x_{s}^{0},s)\\right)d s+\\displaystyle\\Phi_{t}\\int_{0}^{t}\\Phi_{s}^{-1}\\eta_{k}(x_{s}^{0},s)d B_{s}^{k}}\\\\ {\\displaystyle\\partial_{\\varepsilon}^{2}\\,x_{t}^{0}=\\Phi_{t}\\int_{0}^{t}\\Phi_{s}^{-1}\\Bigg(\\Psi_{0}(x_{s}^{0},\\partial_{\\varepsilon}\\,x_{s}^{0},s)+2\\,\\frac{\\partial\\eta_{0}}{\\partial x}(x_{s}^{0},s)\\partial_{\\varepsilon}\\,x_{s}^{0}}\\\\ {\\displaystyle-\\sum_{k=1}^{m}\\frac{\\partial g_{k}}{\\partial x}(x_{s}^{0},s)\\Big(\\Psi_{k}(x_{s}^{0},\\partial_{\\varepsilon}\\,x_{s}^{0},s)+2\\,\\frac{\\partial\\eta_{k}}{\\partial x}(x_{s}^{0},s)\\partial_{\\varepsilon}\\,x_{s}^{0}\\Big)\\Bigg)d s,}\\\\ {\\displaystyle+\\,\\Phi_{t}\\int_{0}^{t}\\Phi_{s}^{-1}\\sum_{k=1}^{m}\\left(\\Psi_{k}(x_{s}^{0},\\partial_{\\varepsilon}\\,x_{s}^{0},s)+2\\,\\frac{\\partial\\eta_{k}}{\\partial x}(x_{s}^{0},s)\\partial_{\\varepsilon}\\,x_{s}^{0}\\right)d B_{s}^{k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "683 where $n\\times n$ matrix $\\Phi_{t}$ is the fundamental matrix of the corresponding homogeneous equation: ", "page_idx": 21}, {"type": "equation", "text": "$$\nd\\Phi_{t}={\\frac{\\partial g_{k}}{\\partial x}}(x_{t}^{0},t)\\,\\Phi_{t}\\,d B_{t}^{k},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "684 with initial value ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Phi(0)=I.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "685 It is worthy to note that the fundamental matrix $\\Phi_{t}$ is non-deterministic and when $\\frac{\\partial g_{i}}{\\partial x}$ and $\\frac{\\partial g_{j}}{\\partial x}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Phi_{t}=\\exp\\left(\\int_{0}^{t}\\frac{\\partial g_{k}}{\\partial x}(x_{s}^{0},s)d B_{s}^{k}-\\frac{1}{2}\\int_{0}^{t}\\frac{\\partial g_{k}}{\\partial x}(x_{s}^{0},s)\\frac{\\partial g_{k}}{\\partial x}(x_{s}^{0},s)^{\\top}d s\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "687 Having obtained the explicit solutions, one can plug in corresponding terms and obtain the results of   \n688 Theorem 3.7) after a Taylor expansion of the loss function $\\mathcal{L}$ . \u53e3 ", "page_idx": 22}, {"type": "text", "text": "689 C Error Accumulation During the Inference Phase and its Effects to Value 690 Functions ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "691 Theorem C.1. (Error accumulation due to initial representation error ) ", "page_idx": 23}, {"type": "text", "text": "692 Let $\\delta:=\\mathbb{E}\\left\\|\\varepsilon\\right\\|$ and $\\begin{array}{r}{d_{\\varepsilon}:=\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\left\\|h_{t}^{\\varepsilon}-h_{t}^{0}\\right\\|^{2}+\\left\\|\\tilde{z}_{t}^{\\varepsilon}-\\tilde{z}_{t}^{0}\\right\\|^{2}}\\end{array}$ . It holds that as $\\delta\\rightarrow0$ , ", "page_idx": 23}, {"type": "equation", "text": "$d_{\\varepsilon}\\leq\\,\\delta\\,C\\left(\\mathcal{I}_{0}+\\mathcal{I}_{1}\\right)+\\,\\delta^{2}\\,C\\left(\\exp\\left(\\mathcal{H}_{0}\\left(\\mathcal{I}_{0}+\\mathcal{T}_{1}\\right)\\right)+\\exp\\left(\\mathcal{H}_{1}\\left(\\mathcal{I}_{0}+\\mathcal{T}_{1}\\right)\\right)\\right)+\\mathcal{O}(\\delta^{3}),$ ", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "693 where ", "page_idx": 23}, {"type": "text", "text": "694 ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{I}_{0}=\\exp\\{\\sum_{k}\\succcurlyeq\\gamma_{k}+\\sum_{l}\\succcurlyeq\\gamma_{l}\\leq\\exp\\{\\gamma_{k}\\}\\},}\\\\ &{\\mathcal{I}_{0}=-\\gamma_{k+1}+\\sum_{l}\\succcurlyeq\\gamma_{k+1}+\\mathcal{F}_{\\mathrm{A}}\\succcurlyeq\\beta_{\\mathrm{A}},}\\\\ &{\\succcurlyeq\\gamma_{k}-\\mathcal{C}\\leq\\exp\\left[\\frac{1}{\\Theta}\\rho_{k}^{l}+\\frac{\\gamma_{l}}{\\Theta}\\Delta\\rho_{l}\\right],}\\\\ &{\\mathcal{I}_{0}=-\\sum_{k\\leq l}\\exp\\left[\\frac{1}{\\Theta}\\rho_{k}^{l}+\\frac{\\gamma_{l}}{\\Theta}\\Delta\\rho_{l}\\right],\\quad\\quad\\forall k=\\mathcal{C}\\exp\\left[\\frac{1}{\\Theta}\\rho_{k}^{l}+\\frac{\\gamma_{l}}{\\Theta}\\Delta\\rho_{l}\\right],}\\\\ &{\\mathcal{I}_{0}=-\\sum_{k\\leq l}\\exp\\left[\\frac{1}{\\Theta}\\rho_{k}^{l}\\right],\\quad\\mathcal{I}_{0}=-\\sum_{k\\leq l}\\exp\\left[\\frac{1}{\\Theta}\\rho_{k}^{l}\\right],}\\\\ &{\\mathcal{I}_{0}=-\\sum_{k\\leq l}\\exp\\left[\\frac{1}{\\Theta}\\rho_{k}^{l}\\right],\\quad\\mathcal{I}_{0}=-\\sum_{k\\leq l}\\exp\\left[\\frac{1}{\\Theta}\\rho_{k}^{l}\\right],}\\\\ &{\\mathcal{I}_{0}=-\\sum_{k\\leq l}\\exp\\left[\\frac{1}{\\Theta}\\rho_{k}^{l}+\\frac{\\gamma_{l}}{\\Theta}\\Delta\\rho_{l}+\\frac{\\gamma_{l}}{\\Theta}\\Delta\\rho_{l}\\right],}\\\\ &{\\mathcal{I}_{1}=-\\mathcal{C}\\exp\\left[\\frac{1}{\\Theta}\\rho_{l}^{l}+\\frac{\\gamma_{l}}{\\Theta}\\Delta\\rho_{l}\\right],\\quad\\;\\;\\;\\beta_{0}=\\frac{\\mathcal{I}_{0}}{\\Theta}\\Delta\\rho_{l},}\\\\ &{\\mathcal{I}_{1}=-\\mathcal{C}\\exp\\left[\\frac{1}{\\Theta}\\rho_{l}^{l}+\\frac{\\gamma_{l}}{\\Theta}\\Delta\\rho_{l}\\right],\\quad\\;\\;\\;\\tilde{\\Delta}_{l}=-\\sum_{k\\leq l}\\sum_{l}\\rho_{l}^{l}\\;,}\\\\ &{\\mathcal{I}_{0}=-\\sum_\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "695 where for brevity, when functions always have inputs $(\\tilde{z}_{t}^{0},h_{t}^{0},t)$ , we adopt the shorthand to write, for   \n696 example, $f(\\tilde{z}_{t}^{0},\\dot{h}_{t}^{0},t)\\;a\\dot{s}\\;f$ .   \n697 Before proving the main result C.1, we first show the general case of perturbation in initial values.   \n698 Consider the following general system with noise at the initial value: ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{d x_{t}=g_{0}\\left(x_{t},t\\right)d t+g_{k}\\left(x_{t},t\\right)d B_{t}^{k},}}\\\\ {{x(0)=x_{0}+\\varepsilon,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "699 where the initial perturbation $\\varepsilon\\in\\mathbb{R}^{n}\\times\\Omega$ . As $g_{k}$ are $\\mathcal{C}_{g}^{2,\\alpha}$ functions, by the classical result on the   \n700 existence and the uniqueness of solution to SDE, there exists a unique solution to Equation (47),   \n701 denoted as $\\boldsymbol{x}_{t}^{\\varepsilon}$ or $x^{\\varepsilon}(t)$ .   \n702 To simplify the notation, we write $\\begin{array}{r}{\\partial_{i}\\,x_{t}^{\\varepsilon}\\,:=\\,\\frac{\\partial x^{\\varepsilon}(t)}{\\partial x^{i}},\\partial_{i j}^{2}\\,x_{t}^{\\varepsilon}=\\frac{\\partial^{2}x_{t}^{\\varepsilon}}{\\partial x^{i}\\partial x^{j}}}\\end{array}$ , for $i,j=1,\\,.\\,.\\,,\\,n$ that are,   \n703 respectively, the first and second-order derivatives of the solution $x^{\\varepsilon}(t)$ w.r.t. the changes in the   \n704 corresponding coordinates of the initial value. When $\\varepsilon\\,=\\,0\\,\\in\\,\\mathbb{R}^{n}$ , we denote the solutions to   \n705 Equation (47) as $\\boldsymbol{x}_{t}^{0}$ with its first and second derivatives $\\partial_{i}\\,x_{t}^{0},\\partial_{i j}^{2}\\,x_{t}^{0}$ , respectively. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "706 Proposition C.2. Let $\\delta:=\\mathbb{E}\\left\\|\\varepsilon\\right\\|$ , it holds that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r l r}{\\lefteqn{\\xi\\operatorname*{sup}_{t\\in[0,T]}\\left\\|x_{t}^{\\varepsilon}-x_{t}^{0}\\right\\|^{2}\\leq\\displaystyle\\sum_{k=0,1}C\\delta\\left(C\\underbrace{\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\left\\|\\frac{\\partial g_{k}}{\\partial x}(x_{t}^{0},t)\\right\\|_{F}^{2}\\right)}}\\\\ &{}&{+\\,C\\,\\delta^{2}\\exp\\left(C\\,\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\left\\|\\frac{\\partial^{2}g_{k}}{\\partial x^{2}}(x_{t}^{0},t)\\right\\|_{F}^{2}\\underset{k=0,1}{\\overset{\\textstyle\\sum}{\\sum}}\\exp\\left(C\\,\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\left\\|\\frac{\\partial g_{\\tilde{k}}}{\\partial x}(x_{t}^{0},t)\\right\\|_{F}^{2}\\right)\\right)}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "708 Proof. Similar to the previous section, for notational convenience, we write $t$ as $B_{t}^{0}$ and employs   \n709 Einstein summation notation. Hence, Equation (47) can be shorten as ", "page_idx": 24}, {"type": "equation", "text": "$$\nd x_{t}=g_{k}\\left(x_{t},t\\right)d B_{t}^{k},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "710 with initial values $x(0)=x_{0}+\\varepsilon$ . ", "page_idx": 24}, {"type": "text", "text": "711 To begin, we find the SDEs that characterize $\\partial_{i}\\,x_{t}^{\\varepsilon}$ and $\\partial_{i j}^{2}\\,x_{t}^{\\varepsilon}$ , for $i$ , $j=1,\\;...,\\;n$ . ", "page_idx": 24}, {"type": "text", "text": "712 For $\\partial_{i}\\,x_{t}^{\\varepsilon}$ , we apply Theorem 3.1 from Section 2 in [17] on Equation (51) and $\\partial_{i}\\,x_{t}^{\\varepsilon}$ satisfy the   \n713 following SDE with probability 1, ", "page_idx": 24}, {"type": "equation", "text": "$$\nd\\partial_{i}\\,x_{t}^{\\varepsilon}=\\frac{\\partial g_{k}}{\\partial x}\\,(x_{t}^{\\varepsilon},t)\\,\\partial_{i}\\,x_{t}^{\\varepsilon}d B_{t}^{k}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "714 with initial value $\\partial_{i}{x_{0}^{\\varepsilon}}$ to be the unit vector $e_{i}=(0,\\,0,\\,.\\,.\\,.\\,,\\,1,\\,.\\,.\\,.\\,,\\,0)$ that is all zeros except one in   \n715 the $i^{\\mathrm{th}}$ coordinate.   \n716 For $\\partial_{i j}^{2}\\,x_{t}^{\\varepsilon}$ , we again apply Theorem 3.1 from Section 2 in [17] on the SDE (52) and obtain that $\\partial_{i j}^{2}x_{b}^{\\varepsilon}$   \n717 satisfy the following SDE with probability 1, ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "equation", "text": "$$\nd\\partial_{i j}^{2}\\,x_{t}^{\\varepsilon}=\\Psi_{k}\\left(x_{t}^{\\varepsilon},\\partial_{i}\\,x_{t}^{\\varepsilon},t\\right)\\partial_{i j}^{2}\\,x_{t}^{\\varepsilon}d B_{t}^{k},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "718 with the initial value $\\partial_{i j}\\,x^{\\varepsilon}(0)=e_{j}$ , where ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Psi_{k}:\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\times\\left[0,T\\right]\\rightarrow\\mathbb{R}^{d\\times d},\\;(x,\\partial_{i}\\,x,t)\\mapsto\\left(\\frac{\\partial^{2}g_{k}^{l}}{\\partial x^{u}\\partial x^{v}}\\left(x_{t}^{\\varepsilon},t\\right)\\right)_{l,u,v}\\partial_{i}\\,x^{v}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "719 For the next step, we show that with probability 1, the following holds ", "page_idx": 24}, {"type": "equation", "text": "$$\nx_{t}^{\\varepsilon}=x_{t}^{0}+\\varepsilon^{i}\\,\\partial_{i}\\,x_{t}^{0}+\\frac{1}{2}\\,\\varepsilon^{i}\\varepsilon^{j}\\,\\partial_{i j}^{2}\\,x_{t}^{0}+O\\left(\\varepsilon^{3}\\right),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "720 as $\\|\\varepsilon\\|\\rightarrow0$ . ", "page_idx": 24}, {"type": "text", "text": "721 One can follow the similar steps of proofs for Lemma (B.6) and (B.7) in the previous section to show   \n722 that $\\begin{array}{r}{\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\left\\|x_{t}^{0}\\right\\|^{2},\\,\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\left\\|\\partial_{i}x_{t}^{0}\\right\\|^{2},\\,\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\left\\|\\partial_{i j}^{2}x_{t}^{0}\\right\\|^{2}}\\end{array}$ and the remainder term are   \n723 bounded. Hence, Equation (54) holds with probability 1. ", "page_idx": 24}, {"type": "text", "text": "725 Indeed, for $\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\left\\lVert\\partial_{i}\\,x_{t}^{0}\\right\\rVert^{2}$ , it holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\displaystyle\\operatorname*{sup}_{t\\in[0,T]}\\left\\|\\partial_{i}\\,x_{t}^{0}\\right\\|^{2}\\leq C\\left\\|e_{i}\\right\\|^{2}+\\displaystyle\\sum_{k=0,1}\\mathbb{E}\\displaystyle\\operatorname*{sup}_{t\\in[0,T]}C\\int_{0}^{t}\\left\\|\\frac{\\partial g_{k}}{\\partial x}(x_{s}^{0},s)\\right\\|_{F}^{2}\\left\\|\\partial_{i}\\,x_{s}\\right\\|^{2}d s}\\\\ &{\\leq\\displaystyle\\sum_{k=0,1}C\\exp\\left(C\\,\\mathbb{E}\\displaystyle\\operatorname*{sup}_{t\\in[0,T]}\\left\\|\\frac{\\partial g_{k}}{\\partial x}(x_{t}^{0},t)\\right\\|_{F}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "726 Similarly, for $\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\left\\|\\partial_{i j}^{2}\\,x_{t}^{0}\\right\\|^{2}$ , it holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\underset{t\\in[0,T]}{\\operatorname*{sup}}\\,\\left\\Vert\\partial_{i j}^{2}\\,x_{t}^{0}\\right\\Vert^{2}\\leq C\\left\\Vert e_{i}\\right\\Vert^{2}+\\underset{k=0,1}{\\sum}\\underset{t\\in[0,T]}{\\sum}C\\int_{0}^{t}\\left\\Vert\\frac{\\partial^{2}g_{k}}{\\partial x^{2}}(x_{s}^{0},s)\\right\\Vert_{F}^{2}\\left\\Vert\\partial_{i}\\,x_{s}^{0}\\right\\Vert^{2}\\left\\Vert\\partial_{i j}^{2}\\,x_{s}^{0}\\right\\Vert^{2}d s}&{}&\\\\ &{\\leq C\\underset{k=0}{\\sum}\\exp\\left(C\\underset{t\\in[0,T]}{\\operatorname*{sup}}\\left\\Vert\\frac{\\partial^{2}g_{k}}{\\partial x^{2}}(x_{t}^{0},t)\\right\\Vert_{F}^{2}\\left\\Vert\\partial_{i}\\,x_{t}^{0}\\right\\Vert^{2}\\right)}&{}&{\\left(58\\right)}\\\\ &{\\leq C\\underset{k=0,1}{\\sum}\\exp\\left(C\\underset{t\\in[0,T]}{\\operatorname*{sup}}\\left\\Vert\\frac{\\partial^{2}g_{k}}{\\partial x^{2}}(x_{t}^{0},t)\\right\\Vert_{F}^{2}\\exp\\left(C\\underset{t\\in[0,T]}{\\operatorname*{E}\\operatorname*{sup}}\\left\\Vert\\frac{\\partial g_{k}}{\\partial x}(x_{t}^{0},t)\\right\\Vert_{F}^{2}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "727 Therefore, we could obtain the proposition by applying Jensen\u2019s inequality to Equation (54) and   \n728 plugging with 56 and 57. \u53e3   \n729 Now we are ready to prove Theorem C.1. We note that one could then obtain Corollary 4.2 without   \n730 much more effort by a standard application of Taylor\u2019s theorem. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "731 Proof. (Proof for Theorem C.1) ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "732 At $(h_{t},\\tilde{z}_{t},\\pi(h_{t},\\tilde{z}_{t}))$ , where the local optimal policy $\\pi(h_{t},\\tilde{z}_{t})$ , denoted as $a_{t}^{*}$ , there exists an open   \n733 neighborhood $V\\subseteq A$ of $a_{t}^{*}$ such that $a_{t}^{*}$ is the local maximizer for $Q(h_{t},\\tilde{z}_{t},\\cdot)$ by definition.   \n773354 nTehiegnh,b $\\begin{array}{r}{\\frac{\\partial Q}{\\partial a}(h_{t},\\tilde{z}_{t},a_{t}^{*})=0}\\end{array}$ ,m aplnidci $\\frac{\\partial^{2}Q}{\\partial a^{2}}(h_{t},\\tilde{z}_{t},a)$ ries mn,e tghaetriev ee xdiestfsin iat en.e iAgsh $\\frac{\\partial^{2}Q}{\\partial a^{2}}$ oios dn eonf   \n$V$ $U\\times V$ $(h_{t},\\tilde{z}_{t},a_{t}^{*})$   \n773367 lsouccahl  tmhaatx ithmeirzee re oxif $\\mathcal{C}^{2}$ faopr $\\rho\\,:\\,U\\,\\rightarrow\\,V$ .s uFcuhr tthheart $\\begin{array}{r}{\\frac{\\partial Q}{\\partial a}(h,\\tilde{z},\\rho(h,\\tilde{z}))\\,=\\,0}\\end{array}$ $\\rho(h,\\tilde{z})$   \n$Q(h,\\tilde{z},\\cdot)$ $h,\\tilde{z}\\in U$ $\\begin{array}{r}{\\partial_{h}\\,\\rho=-{\\frac{\\partial^{2}Q}{\\partial a^{2}}}^{-1}{\\frac{\\partial^{2}Q}{\\partial a\\partial h}}}\\end{array}$   \n738 Similarly, other first-terms and second-order terms $\\partial_{z}\\rho$ , $\\partial_{z z}^{2}\\rho,\\partial_{z h}^{2}\\rho,\\partial_{h z}^{2}\\rho,\\partial_{h h}^{2}\\rho$ can be explicitly   \n739 expressed without much additional effort (e.g., in [28], [3]).   \n740 The rest of the proof is easy to see after plugging in the corresponding terms from Proposition   \n741 C.2. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "742 D Experimental Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "743 In this section, we provide additional details and results beyond thoese in the main paper. ", "page_idx": 26}, {"type": "text", "text": "744 D.1 Model Implementation and Training ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "745 Our baseline is based on the DreamerV2 Tensorflow implementation. Our theoretical and empirical   \n746 results should not matter on the choice of specific version; so we chose DreamerV2 as its codebase   \n747 implementation is simpler than V3. We incorporated a computationally efficient approximation of   \n748 the Jacobian norm for the sequence model, as detailed in [18], using a single projection. During our   \n749 experiments, all models were trained using the default hyperparameters (see Table 5) for the MuJoCo   \n750 tasks. The training was conducted on an NVIDIA A100 and a GTX 4090, with each session lasting   \n751 less than 15 hours.   \n753 In this experiment, we investigated the effectiveness of Jacobian regularization in model trained   \n754 against a baseline during the inference phase with perturbed state images. We consider three types of   \n755 perturbations: (1) Gaussian noise across the full image, denoted as $\\bar{\\mathcal{N}}(\\mu_{1},\\sigma_{1}^{2})\\,;(2)$ rotation; and (3)   \n756 noise applied to a percentage of the image, $\\mathcal{N}(\\mu_{2},\\sigma_{2}^{\\breve{2}})$ . (In Walker task, $\\mu_{1}\\stackrel{\\leftrightarrow}{=}\\mu_{2}=0.5,\\sigma_{2}^{2}=0.15$ ;   \n757 in Quadruped task, $\\mu_{1}\\,=\\,{\\bar{0}},\\mu_{2}\\,=\\,0.05,\\sigma_{2}^{2}\\,=\\,0.2.)$ In each case of perturbations, we examine a   \n758 collection of noise levels: (1) variance $\\sigma^{2}$ from 0.05 to 0.55; (2) rotation degree $\\alpha\\,20$ and 30; and (3)   \n759 masked image percentage $\\beta\\%$ from 25 to 75. ", "page_idx": 26}, {"type": "table", "img_path": "3sWghzJvGd/tmp/9fbb744b0f40888e143f2272b6d764867075b43182dc391eb56d049d7925f13a.jpg", "table_caption": [], "table_footnote": ["Table 5: Hyperparameters for DreamerV2 model. "], "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "760 D.3 Walker Task ", "text_level": 1, "page_idx": 27}, {"type": "table", "img_path": "3sWghzJvGd/tmp/e18e431676e8c290f1e4aaeae09b7ec2ac279a0f6541e9d62293c0ba7236ea83.jpg", "table_caption": [], "table_footnote": ["Table 6: Walker. Mean and standard deviation of accumulated rewards under masked perturbation of increasing percentage. "], "page_idx": 27}, {"type": "table", "img_path": "3sWghzJvGd/tmp/1fdb81ba51020404ff627b70646eec80728c946e38e325be698eacaf48f72065.jpg", "table_caption": [], "table_footnote": ["Table 7: Walker. Mean and standard deviation of accumulated rewards under Gaussian perturbation of increasing variance. "], "page_idx": 27}, {"type": "table", "img_path": "3sWghzJvGd/tmp/974b64e1a48410496836f47ee61e4061cfae2af43bff1b8d596d8462bdb617b4.jpg", "table_caption": [], "table_footnote": ["Table 8: Walker. Mean and standard deviation of accumulated rewards under rotations. "], "page_idx": 27}, {"type": "table", "img_path": "3sWghzJvGd/tmp/73c3b7874e61d9243a930895190ab71a2eba5f7bfc155cb2fa06c849fa4049cc.jpg", "table_caption": ["761 D.4 Quardruped Task "], "table_footnote": ["Table 9: Quadruped. Mean and standard deviation of accumulated rewards under masked perturbation of increasing percentage. "], "page_idx": 28}, {"type": "table", "img_path": "3sWghzJvGd/tmp/253cd6e31148a9b8adacabd92d904e715297479f8eb4e14fe99151dc607715c0.jpg", "table_caption": [], "table_footnote": ["Table 10: Quadruped. Mean and standard deviation of accumulated rewards under Gaussian perturbation of increasing variance. "], "page_idx": 28}, {"type": "table", "img_path": "3sWghzJvGd/tmp/0669625bb9558a66e4e77a2d20aeae24f9e066a6225a1ed3b402d731d773e01f.jpg", "table_caption": [], "table_footnote": ["Table 11: Quadruped. Mean and standard deviation of accumulated rewards under rotations. "], "page_idx": 28}, {"type": "text", "text": "762 D.5 Additional Results on Robustness against Encoder Errors ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "763 In this experiment, we evaluate the robustness of model trained with Jacobian regularization against   \n764 two exogenous error signals (1) zero-drift error with $\\mu_{t}=0,\\sigma_{t}^{2}$ $(\\sigma_{t}^{2}=5$ in Walker, $\\sigma_{t}^{2}=\\bar{0}.1$ in   \n765 Quadruped), and (2) non-zero-drift error with $\\mu_{t}\\sim[0,5],\\sigma_{t}^{2}\\sim[0,5]$ uniformly. $\\lambda$ weight of Jacobian   \n766 regularization is 0.01. In this section, we included plot results of both evaluation and training scores. ", "page_idx": 29}, {"type": "text", "text": "767 D.5.1 Walker Task ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "768 Under the Walker task, Figures 3 and 4 show that model with regularization is significantly less   \n769 sensitive to perturbations in latent state $z_{t}$ compared to the baseline model without regularization.   \n770 This empirical observation supports our theoretical findings in Corollary 3.8, which assert that the   \n771 impact of latent representation errors on the loss function $\\mathcal{L}$ can be effectively controlled by regulating   \nthe model\u2019s Jacobian norm. ", "page_idx": 29}, {"type": "image", "img_path": "3sWghzJvGd/tmp/e226bdbf94b00d39fdf7dccdbcb5651b0a61e8fd1dc3f0bb109cdb51edb5ae84.jpg", "img_caption": ["Figure 3: Walker. Eval (left) and train scores (right) under latent error process $\\mu_{t}=0,\\sigma_{t}^{2}=5$ "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "772 ", "page_idx": 29}, {"type": "image", "img_path": "3sWghzJvGd/tmp/f281b8ef4e2d4307399b7cf47011fd68490774e74ff7b8ca6f385c9fbfdab42a.jpg", "img_caption": ["Figure 4: Walker. Eval (left) and train scores (right) under latent error process $\\mu_{t}\\sim[0,5],\\sigma_{t}^{2}\\sim[0,5]$ . "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "773 D.5.2 Quadruped Task ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "774 Under the Quadruped task,we initially examined a smaller latent error process $(\\mu_{t}=0,\\sigma_{t}^{2}=0.1)$ ) and   \n775 observed that the model with Jacobian regularization converged significantly faster, even though the   \n776 adversarial effects on the model without regularization were less severe (Figure 5). When considering   \n777 the more challenging latent error process $(\\mu_{t}\\sim[0,5],\\sigma_{t}^{2}\\sim[0,5])$ , we noted that the regularized   \n778 model remained significantly less sensitive to perturbations in latent state $z_{t}$ , whereas the baseline   \n779 model struggled to learn (Figure 6). These empirical observations reinforce our theoretical findings   \n780 in Corollary 3.8, demonstrating that regulating the model\u2019s Jacobian norm effectively controls the   \nimpact of latent representation errors.   \n783 In this experiment, we evaluate the efficacy of Jacobian regularization in extended horizon tasks,   \n784 specifically by increasing the horizon length in MuJoCo Walker from 50 to 100 steps. We tested two   \n785 regularization weights $\\lambda=0.1$ and $\\lambda=0.05$ . Figure 7 demonstrates that models with regularization   \n786 converge faster, with $\\lambda=0.05$ achieving convergence approximately 100,000 steps ahead of the   \n787 model without Jacobian regularization. This supports the findings in Theorem 4.1, indicating that   \nregularizing the Jacobian norm can reduce error propagation, especially over longer time horizons. ", "page_idx": 30}, {"type": "image", "img_path": "3sWghzJvGd/tmp/3f89fb4573886f3eb74ebd0a056f9ddcee20be293414985275c79d4d590a03cf.jpg", "img_caption": ["Figure 5: Quad. Eval (left) and train scores (right) under latent error process $\\mu_{t}=0,\\sigma_{t}^{2}=0.1$ . "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "3sWghzJvGd/tmp/13f1ce08c66e5585c43662b0d6814a28c03270a25d91c2442d2f74eaa7ed993b.jpg", "img_caption": ["Figure 6: Quad. Eval (left) and train scores (right) under latent error process $\\mu_{t}\\sim[0,5],\\sigma_{t}^{2}\\sim[0,5]$ . "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "image", "img_path": "3sWghzJvGd/tmp/2111d91ed415cd48d025ccb99e20f110e90ef93946da4ffe3df7ddd0bd0d5326.jpg", "img_caption": ["Figure 7: Extended horizon Walker task. Eval (left) and train scores (right). "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "788 ", "page_idx": 31}, {"type": "text", "text": "789 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Justification: In the abstract and the contribution section 1 in introduction, we provide a clear list of statements outlining the paper\u2019s contributions. ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: For each of our theoretical results, we state the required assumptions and provide relevant discussions that compare our assumptions to the practical implementations which involves certain limitations for theoretical simplifications. For empirical results, we also state the experiment settings and the number of trials run. We also discuss the possible future research to extend our work in the conclusion section. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "841 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "42 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n43 a complete (and correct) proof? 44 Answer: [Yes] 45 Justification: For each of our theoretical results, we state the assumptions required in both 46 the main text and the provided appendix. We provide the full proofs of all of our theoretical 47 results in Sections A, B and C in Appendix,   \n48 Guidelines:   \n49 \u2022 The answer NA means that the paper does not include theoretical results. 50 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n51 referenced. 52 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems. 53 \u2022 The proofs can either appear in the main paper or the supplemental material, but if 54 they appear in the supplemental material, the authors are encouraged to provide a short   \n55 proof sketch to provide intuition. 56 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented 57 by formal proofs provided in appendix or supplemental material. 58 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n859 4. Experimental Result Reproducibility 60 Question: Does the paper fully disclose all the information needed to reproduce the main ex61 perimental results of the paper to the extent that it affects the main claims and/or conclusions ", "page_idx": 33}, {"type": "text", "text": "of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The full source code required to reproduce the experimental results is included in the submission. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 33}, {"type": "text", "text": "896 some way (e.g., to registered users), but it should be possible for other researchers   \n897 to have some path to reproducing or verifying the results.   \n898 5. Open access to data and code   \n899 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n900 tions to faithfully reproduce the main experimental results, as described in supplemental   \n901 material?   \n902 Answer: [Yes]   \n903 Justification: Our task environments Walker and Quardruped are from open source package   \n904 MuJoCo. Our baseline implementation is from open source codebase DreamerV2. Our   \n905 implementation of Jacobian regularization has a full description in Section D.1.   \n906 Guidelines:   \n907 \u2022 The answer NA means that paper does not include experiments requiring code.   \n908 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n909 public/guides/CodeSubmissionPolicy) for more details.   \n910 \u2022 While we encourage the release of code and data, we understand that this might not be   \n911 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n912 including code, unless this is central to the contribution (e.g., for a new open-source   \n913 benchmark).   \n914 \u2022 The instructions should contain the exact command and environment needed to run to   \n915 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n916 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n917 \u2022 The authors should provide instructions on data access and preparation, including how   \n918 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n919 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n920 proposed method and baselines. If only a subset of experiments are reproducible, they   \n921 should state which ones are omitted from the script and why.   \n922 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n923 versions (if applicable).   \n924 \u2022 Providing as much information as possible in supplemental material (appended to the   \n925 paper) is recommended, but including URLs to data and code is permitted.   \n926 6. Experimental Setting/Details   \n927 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n928 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n929 results?   \n930 Answer: [Yes]   \n931 Justification: We state the hyperparameters used in Table 5. The perturbations we considered   \n932 is fully described in the experiment section form the main text.   \n933 Guidelines:   \n934 \u2022 The answer NA means that the paper does not include experiments.   \n935 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n936 that is necessary to appreciate the results and make sense of them.   \n937 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n938 material.   \n939 7. Experiment Statistical Significance   \n940 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n941 information about the statistical significance of the experiments?   \n942 Answer: [Yes]   \n943 Justification: While our work is predominantly theoretical, we conducted 5 random trials for   \n944 each perturbation degree and type. For additional results including standard deviation of   \n945 trials, see Section $D$ .   \n946 Guidelines:   \n947 \u2022 The answer NA means that the paper does not include experiments.   \n948 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n949 dence intervals, or statistical significance tests, at least for the experiments that support   \n950 the main claims of the paper.   \n951 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n952 example, train/test split, initialization, random drawing of some parameter, or overall   \n953 run with given experimental conditions).   \n954 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n955 call to a library function, bootstrap, etc.)   \n956 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n957 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n958 of the mean.   \n959 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n960 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n961 of Normality of errors is not verified.   \n962 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n963 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n964 error rates).   \n965 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n966 they were calculated and reference the corresponding figures or tables in the text.   \n967 8. Experiments Compute Resources   \n968 Question: For each experiment, does the paper provide sufficient information on the com  \n969 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n970 the experiments?   \n971 Answer: [Yes]   \n972 Justification: Relevant computing information is provided in Section D.1.   \n973 Guidelines:   \n974 \u2022 The answer NA means that the paper does not include experiments.   \n975 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n976 or cloud provider, including relevant memory and storage.   \n977 \u2022 The paper should provide the amount of compute required for each of the individual   \n978 experimental runs as well as estimate the total compute.   \n979 \u2022 The paper should disclose whether the full research project required more compute   \n980 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n981 didn\u2019t make it into the paper).   \n982 9. Code Of Ethics   \n983 Question: Does the research conducted in the paper conform, in every respect, with the   \n984 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n985 Answer: [Yes]   \n986 Justification: This work conforms with the NeurIPS Code of Ethics.   \n987 Guidelines:   \n988 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n989 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n990 deviation from the Code of Ethics.   \n991 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n992 eration due to laws or regulations in their jurisdiction).   \n993 10. Broader Impacts   \n994 Question: Does the paper discuss both potential positive societal impacts and negative   \n995 societal impacts of the work performed?   \n996 Answer: [NA]   \n997 Justification: The work is of theoretical nature and has no societal impact of the work   \nperformed. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. \u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. \u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. \u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper poses no such risks as we do not have any released data or models. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset. ", "page_idx": 36}, {"type": "text", "text": "51 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n52 service of that source should be provided.   \n53 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n54 package should be provided. For popular datasets, paperswithcode.com/datasets   \n55 has curated licenses for some datasets. Their licensing guide can help determine the   \n56 license of a dataset.   \n57 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n58 the derived asset (if it has changed) should be provided.   \n59 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n60 the asset\u2019s creators. ", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA]   \nJustification: Our work is mostly of theoretical nature and does not release new assets. Guidelines:   \n\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether 93 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 4 approvals (or an equivalent approval/review based on the requirements of your country or 95 institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing nor research with human subjects.   \nGuidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 37}, {"type": "text", "text": "1101   \n1102   \n1103   \n1104   \n1105   \n1106   \n1107   \n1108 ", "page_idx": 38}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}]