[{"figure_path": "kOp0kiXZ3a/tables/tables_6_1.jpg", "caption": "Table 1: Top1 accuracy comparisons on multi-precision of {8,6,4,2}-bit on ImageNet-1K datasets. 'KD' denotes knowledge distillation. The '-' represents the unqueried value.", "description": "This table presents the Top-1 accuracy results on the ImageNet-1K dataset for different multi-precision quantization methods using a bit-width configuration of {8,6,4,2}.  It compares the performance of the proposed method against several state-of-the-art techniques, considering both the use of knowledge distillation and varying storage requirements (8-bit vs 32-bit). The '-' indicates missing data points.", "section": "4.1 Multi-Precision"}, {"figure_path": "kOp0kiXZ3a/tables/tables_7_1.jpg", "caption": "Table 1: Top1 accuracy comparisons on multi-precision of {8,6,4,2}-bit on ImageNet-1K datasets. 'KD' denotes knowledge distillation. The '-' represents the unqueried value.", "description": "This table presents the Top-1 accuracy results on the ImageNet-1K dataset for different multi-precision configurations ({8,6,4,2}-bit). It compares the proposed method's performance against several state-of-the-art (SOTA) methods. The table includes results with and without knowledge distillation (KD) and shows the storage requirements (in bits) for each method.", "section": "4.1 Multi-Precision"}, {"figure_path": "kOp0kiXZ3a/tables/tables_7_2.jpg", "caption": "Table 1: Top1 accuracy comparisons on multi-precision of {8,6,4,2}-bit on ImageNet-1K datasets. 'KD' denotes knowledge distillation. The \"-\" represents the unqueried value.", "description": "This table presents the Top-1 accuracy results on the ImageNet-1K dataset for different multi-precision configurations ({8,6,4,2}-bit).  It compares the proposed method's performance against several state-of-the-art (SOTA) methods such as Any-Precision, Hot-Swap, KURE, and MultiQuant, with and without knowledge distillation (KD). The table shows the accuracy for various bit-width settings (w8a8, w6a6, w4a4, w2a2) across different models (ResNet18, ResNet50, and MobileNetV2).  The 'FP' column indicates the full-precision accuracy for reference.", "section": "4.1 Multi-Precision"}, {"figure_path": "kOp0kiXZ3a/tables/tables_8_1.jpg", "caption": "Table 1: Top1 accuracy comparisons on multi-precision of {8,6,4,2}-bit on ImageNet-1K datasets. 'KD' denotes knowledge distillation. The '-' represents the unqueried value.", "description": "This table compares the Top-1 accuracy of different models (ResNet18, ResNet50, MobileNetV2) trained with different multi-precision configurations ({8,6,4,2}-bit) on the ImageNet-1K dataset.  It shows the impact of using knowledge distillation (KD) and the proposed methods on accuracy. The '-' indicates missing values, which are likely due to the method not converging for that specific model and configuration.", "section": "4.1 Multi-Precision"}, {"figure_path": "kOp0kiXZ3a/tables/tables_8_2.jpg", "caption": "Table 1: Top1 accuracy comparisons on multi-precision of {8,6,4,2}-bit on ImageNet-1K datasets. 'KD' denotes knowledge distillation. The '-' represents the unqueried value.", "description": "This table compares the Top-1 validation accuracy of different models (ResNet18, ResNet50, MobileNetV2) using various methods on the ImageNet-1K dataset.  The comparison includes different bit-width configurations ({8,6,4,2}-bit) and whether knowledge distillation (KD) was used.  The table shows that the proposed method generally outperforms other methods in most configurations.", "section": "4.1 Multi-Precision"}, {"figure_path": "kOp0kiXZ3a/tables/tables_11_1.jpg", "caption": "Table 1: Top1 accuracy comparisons on multi-precision of {8,6,4,2}-bit on ImageNet-1K datasets. 'KD' denotes knowledge distillation. The \"-\" represents the unqueried value.", "description": "This table compares the Top-1 validation accuracy of different methods on the ImageNet-1K dataset using multi-precision quantization with bit-widths of 8, 6, 4, and 2 bits.  It shows the performance of various methods (including the authors' proposed method) across different bit-width configurations, indicating their performance with and without knowledge distillation (KD).  The '-' indicates that a particular metric was not reported by the original authors of that specific method.", "section": "4.1 Multi-Precision"}]