[{"figure_path": "kOp0kiXZ3a/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of our proposed lossless adaptive bit-switching strategy.", "description": "This figure provides a visual representation of the proposed lossless adaptive bit-switching strategy. It shows three key aspects:\n(a) Saving only an 8-bit representation for various low-precisions (2-bit, 4-bit, 6-bit, 8-bit).  The full-precision model is converted to a quantized representation, utilizing the Double Rounding method to allow nearly lossless bit-switching while reducing storage.\n(b) Stabilizing the learning process across various precisions by using an adaptive learning rate. This addresses the challenge of inconsistent gradients during one-shot joint training.\n(c) Performing sensitivity-aware bit selection for different layers. This leverages the Hessian Matrix Trace to determine which layers benefit most from higher-bit precision, optimizing accuracy and efficiency.\nThe overall process demonstrates how the proposed methodology enables nearly lossless switching between multiple precisions in a deep neural network.", "section": "1 Introduction"}, {"figure_path": "kOp0kiXZ3a/figures/figures_3_1.jpg", "caption": "Figure 2: Comparison of four quantization schemes:(from left to right) used in LSQ [2], AdaBits [3], Bit-Mixer [9] and Ours Double Rounding. In all cases y = dequant(quant(x)).", "description": "The figure compares four different quantization methods: LSQ, AdaBits, Bit-Mixer, and the proposed Double Rounding. Each subplot shows how the quantization function maps input floating-point values (x-axis) to quantized and then dequantized values (y-axis). Different colors represent different bit-widths (2, 3, and 4 bits) to illustrate how the methods handle the quantization process for various precisions. The main difference highlighted is the accuracy of bit-switching (transition between different precisions). Double Rounding aims for nearly lossless bit-switching.", "section": "3.1 Double Rounding"}, {"figure_path": "kOp0kiXZ3a/figures/figures_4_1.jpg", "caption": "Figure 3: The statistics of ResNet18 on ImageNet-1K dataset. (a) and (b): The quantization scale gradients' statistics for the weights, with outliers removed for clarity. (c) and (d): The multi-precision training processes of our Double Rounding without and with the ALRS strategy.", "description": "This figure shows the gradient statistics of quantization scales for ResNet18 on ImageNet.  Subfigures (a) and (b) display box plots illustrating the distribution of gradients for weights with 2-bit and 4-bit quantization respectively, highlighting the inconsistency in gradient magnitudes. Subfigures (c) and (d) compare the accuracy during multi-precision training with and without the proposed Adaptive Learning Rate Scaling (ALRS) method. The ALRS method effectively mitigates the training convergence gap between high and low-precision models.", "section": "3.2 Adaptive Learning Rate Scaling for Multi-Precision"}, {"figure_path": "kOp0kiXZ3a/figures/figures_5_1.jpg", "caption": "Figure 4: The HASB stochastic process and Mixed-precision of ResNet18 for {2,4,6,8}-bit.", "description": "This figure shows the stochastic process and mixed-precision results of the proposed Hessian-Aware Stochastic Bit-switching (HASB) strategy on ResNet18 using {2,4,6,8}-bit configurations.  Subfigure (a) illustrates the probability distribution for unsensitive layers where each bit width has an equal probability of selection.  Subfigure (b) shows the distribution for sensitive layers where higher-bit widths have a higher probability. Subfigure (c) displays the average Hessian trace values across different layers, highlighting the sensitivity of different layers. Subfigure (d) presents the Pareto frontier obtained using the HASB strategy in mixed-precision, illustrating the trade-off between average bit-width and accuracy (Acc1). The results demonstrate the effectiveness of HASB in achieving a better balance of accuracy and efficiency in mixed-precision settings.", "section": "4.2 Mixed-Precision"}, {"figure_path": "kOp0kiXZ3a/figures/figures_5_2.jpg", "caption": "Figure 3: The statistics of ResNet18 on ImageNet-1K dataset. (a) and (b): The quantization scale gradients' statistics for the weights, with outliers removed for clarity. (c) and (d): The multi-precision training processes of our Double Rounding without and with the ALRS strategy.", "description": "This figure shows the statistics of ResNet18 on ImageNet-1K dataset. Figures 3(a) and 3(b) show the quantization scale gradients statistics for weights. Figures 3(c) and 3(d) compare the multi-precision training processes with and without the proposed Adaptive Learning Rate Scaling (ALRS) technique. The ALRS strategy effectively reduces the convergence gap between high and low precision models, improving training efficiency and accuracy.", "section": "3.2 Adaptive Learning Rate Scaling for Multi-Precision"}, {"figure_path": "kOp0kiXZ3a/figures/figures_8_1.jpg", "caption": "Figure 5: Comparison of HASB and Baseline approaches for Mixed-Precision on ResNet18.", "description": "This figure compares the performance of the proposed HASB (Hessian-Aware Stochastic Bit-switching) strategy against a baseline approach for mixed-precision quantization on the ResNet18 model. It shows Pareto frontiers obtained from different bit-width configurations ({8,6,4,2}-bit, {4,3,2}-bit, and {8,4}-bit).  Each point on the Pareto frontier represents a different subnet configuration, and the plots illustrate the trade-off between average bit-width and accuracy (Acc1%). The HASB strategy consistently achieves higher accuracy for the same average bit-width, demonstrating its effectiveness in optimizing the bit-width allocation across layers based on layer sensitivity.", "section": "4.2 Mixed-Precision"}, {"figure_path": "kOp0kiXZ3a/figures/figures_11_1.jpg", "caption": "Figure 6: Comparison between different quantization types during quantization-aware training.", "description": "This figure illustrates three different quantization types in the context of quantization-aware training (QAT). (a) Separate-Precision shows that each bit-width requires training a separate network. (b) Multi-Precision demonstrates a shared network capable of runtime quantization to any bit-width without retraining, where all layers share the same bit-width. (c) Mixed-Precision presents a SuperNet with individual layers quantized to any bit-width at runtime; its subnets are searched without retraining or finetuning.", "section": "A.2 Different Quantization Types"}, {"figure_path": "kOp0kiXZ3a/figures/figures_14_1.jpg", "caption": "Figure 3: The statistics of ResNet18 on ImageNet-1K dataset. (a) and (b): The quantization scale gradients' statistics for the weights, with outliers removed for clarity. (c) and (d): The multi-precision training processes of our Double Rounding without and with the ALRS strategy.", "description": "This figure shows the impact of the proposed Adaptive Learning Rate Scaling (ALRS) technique on the training process of the Double Rounding quantization method. Subfigures (a) and (b) illustrate the gradients of the quantization scales for weights in 2-bit and 4-bit precisions, respectively, highlighting inconsistent gradient magnitudes. Subfigures (c) and (d) compare the multi-precision training process with and without ALRS, demonstrating the effectiveness of ALRS in narrowing the convergence gap between high and low precisions.", "section": "3.2 Adaptive Learning Rate Scaling for Multi-Precision"}, {"figure_path": "kOp0kiXZ3a/figures/figures_15_1.jpg", "caption": "Figure 3: The statistics of ResNet18 on ImageNet-1K dataset. (a) and (b): The quantization scale gradients' statistics for the weights, with outliers removed for clarity. (c) and (d): The multi-precision training processes of our Double Rounding without and with the ALRS strategy.", "description": "This figure shows the comparison of quantization scale gradients statistics for ResNet18 on ImageNet dataset with and without ALRS.  Subfigures (a) and (b) present the gradients for 2-bit and 4-bit precisions, respectively.  Subfigures (c) and (d) illustrate the multi-precision training process with and without ALRS, highlighting the impact of ALRS on convergence and accuracy across different precisions.", "section": "3.2 Adaptive Learning Rate Scaling for Multi-Precision"}, {"figure_path": "kOp0kiXZ3a/figures/figures_15_2.jpg", "caption": "Figure 3: The statistics of ResNet18 on ImageNet-1K dataset. (a) and (b): The quantization scale gradients' statistics for the weights, with outliers removed for clarity. (c) and (d): The multi-precision training processes of our Double Rounding without and with the ALRS strategy.", "description": "This figure shows the comparison of the training process with and without the proposed Adaptive Learning Rate Scaling (ALRS) method.  Subfigures (a) and (b) illustrate the gradient statistics of the quantization scales for weights in 2-bit and 4-bit precisions, respectively, highlighting the inconsistent gradient magnitudes.  Subfigures (c) and (d) compare the accuracy achieved during multi-precision training with and without ALRS, demonstrating the improved training convergence and accuracy with the ALRS method.", "section": "3.2 Adaptive Learning Rate Scaling for Multi-Precision"}, {"figure_path": "kOp0kiXZ3a/figures/figures_15_3.jpg", "caption": "Figure 3: The statistics of ResNet18 on ImageNet-1K dataset. (a) and (b): The quantization scale gradients' statistics for the weights, with outliers removed for clarity. (c) and (d): The multi-precision training processes of our Double Rounding without and with the ALRS strategy.", "description": "This figure shows the impact of the Adaptive Learning Rate Scaling (ALRS) technique on the training process of the Double Rounding method.  Subfigure (a) and (b) present the gradient statistics of the weights' quantization scales for 2-bit and 4-bit precisions respectively, highlighting the inconsistencies in gradient magnitudes between high and low precision during joint training. Subfigures (c) and (d) illustrate the multi-precision training process with and without ALRS, demonstrating ALRS's effectiveness in achieving consistent convergence rates between different precisions.", "section": "3.2 Adaptive Learning Rate Scaling for Multi-Precision"}, {"figure_path": "kOp0kiXZ3a/figures/figures_15_4.jpg", "caption": "Figure 3: The statistics of ResNet18 on ImageNet-1K dataset. (a) and (b): The quantization scale gradients' statistics for the weights, with outliers removed for clarity. (c) and (d): The multi-precision training processes of our Double Rounding without and with the ALRS strategy.", "description": "This figure shows the gradient statistics of quantization scales for ResNet18 on ImageNet.  Specifically, (a) and (b) display boxplots of the gradients for weights with 2-bit and 4-bit quantization, respectively, illustrating the inconsistent magnitude of gradients across different precisions.  (c) and (d) compare the multi-precision training process with and without the proposed Adaptive Learning Rate Scaling (ALRS) method, highlighting ALRS's effectiveness in improving the convergence of different precisions.", "section": "3.2 Adaptive Learning Rate Scaling for Multi-Precision"}, {"figure_path": "kOp0kiXZ3a/figures/figures_16_1.jpg", "caption": "Figure 3: The statistics of ResNet18 on ImageNet-1K dataset. (a) and (b): The quantization scale gradients' statistics for the weights, with outliers removed for clarity. (c) and (d): The multi-precision training processes of our Double Rounding without and with the ALRS strategy.", "description": "This figure shows the gradient statistics of quantization scales for ResNet18 on ImageNet.  Specifically, (a) and (b) display boxplots of the gradients for 2-bit and 4-bit weight quantization, respectively, illustrating significant differences in magnitude.  (c) and (d) compare the training accuracy curves for multi-precision training with and without the proposed Adaptive Learning Rate Scaling (ALRS) method. The ALRS method effectively mitigates the negative impact of inconsistent gradients on convergence, leading to improved performance.", "section": "3.2 Adaptive Learning Rate Scaling for Multi-Precision"}, {"figure_path": "kOp0kiXZ3a/figures/figures_16_2.jpg", "caption": "Figure 3: The statistics of ResNet18 on ImageNet-1K dataset. (a) and (b): The quantization scale gradients' statistics for the weights, with outliers removed for clarity. (c) and (d): The multi-precision training processes of our Double Rounding without and with the ALRS strategy.", "description": "This figure shows the effects of the proposed Adaptive Learning Rate Scaling (ALRS) method.  Subfigures (a) and (b) display the gradients of the quantization scales for weights for 2-bit and 4-bit precisions respectively. It highlights the inconsistent gradient magnitudes between high-bit and low-bit quantization.  Subfigures (c) and (d) compare the multi-precision training processes with and without ALRS, demonstrating ALRS effectively narrows the training convergence gap between high-precision and low-precision.", "section": "3.2 Adaptive Learning Rate Scaling for Multi-Precision"}]