[{"type": "text", "text": "Rethinking the Diffusion Models for Missing Data Imputation: A Gradient Flow Perspective ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhichao Chen1 Haoxuan $\\mathbf{Li^{2}}$ Fangyikang Wang1 Odin Zhang3 Hu Xu1 Xiaoyu Jiang1 Zhihuan Song1,4 Hao Wang1\u2217 ", "page_idx": 0}, {"type": "text", "text": "1Zhejiang University 2Peking University 3University of Washington 4Guangdong University of Petrochemical Technology 12032042@zju.edu.cn hxli@stu.pku.edu.cn wangfangyikang@zju.edu.cn odinz@uw.edu hxu_zju@zju.edu.cn jiangxiaoyu@zju.edu.cn songzhihuan@zju.edu.cn haohaow@zju.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models have demonstrated competitive performance in missing data imputation (MDI) task. However, directly applying diffusion models to MDI produces suboptimal performance due to two primary defects. First, the sample diversity promoted by diffusion models hinders the accurate inference of missing values. Second, data masking reduces observable indices for model training, obstructing imputation performance. To address these challenges, we introduce Negative Entropy-regularized Wasserstein gradient flow for Imputation (NewImp), enhancing diffusion models for MDI from a gradient flow perspective. To handle the first defect, we incorporate a negative entropy regularization term into the cost functional to suppress diversity and improve accuracy. To handle the second defect, we demonstrate that the imputation procedure of NewImp, induced by the conditional distribution-related cost functional, can equivalently be replaced by that induced by the joint distribution, thereby naturally eliminating the need for data masking. Extensive experiments validate the effectiveness of our method. Code is available at https://github.com/JustusvLiebig/NewImp. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Missing data is a pervasive problem for data analytics in diverse scenarios, including e-commerce [29, 30, 57], healthcare [51, 56], and process industry [33, 58]. For instance, in healthcare, patient monitoring devices may fail or lose connection, leading to missing vital signs data. Similarly, in industrial processes, sensor signals may be incomplete due to inevitable mechanical shock. These incompletenesses hamper data integrity and impede subsequent analysis. Therefore, accurate missing data imputation (MDI) is critical for enabling reliable analysis and decision in real-world applications. ", "page_idx": 0}, {"type": "text", "text": "Recently, diffusion models (DMs) have emerged as a powerful tool for MDI [66]. Specifically, these models first estimate the (Stein) score function of the missing data conditioned on the observed data, subsequently reformulating the imputation problem as a generative task grounded in the learned score function. These works are initiated from [51] and evolve to incorporate crafted model architecture [33] and learning objectives [38, 73] for enhancing the accuracy of score estimation [41]. Celebrated for their advantageous capability to model data distributions and generate high-quality synthetic data [41, 50, 65], diffusion models have been a prevalent approach to MDI. ", "page_idx": 0}, {"type": "text", "text": "Despite the successes of diffusion models, we argue that directly applying diffusion models to MDI results in suboptimal performance due to two primary limitations. First, diffusion models perform imputation by sampling from a learned score function, which inadvertently promotes diversity in the imputed values. This increased diversity contradicts the accuracy required for precise imputation of missing data [38]. Second, the training process involves masking a portion of the observed data as labels. The selection of masking strategy significantly impacts imputation accuracy and is inherently challenging to optimize [51]. Moreover, the masked data during training often differ in missing mechanisms from those encountered during testing, resulting in a discrepancy between training and inference phases that degrades performance. Consequently, diffusion models introduce unintended diversity and impose data masking, both of which impede effective imputation. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To tackle these challenges, we introduce a novel DM-based MDI approach termed Negative Entropyregularized Wasserstein Gradient Flow Imputation (NewImp). Specifically, to handle the first issue, we revisit DM-based MDI task within the Wasserstein Gradient Flow (WGF) framework, derive the associated cost functionals, and identify that they implicitly promote diversity in the imputed values. Building on this insight, we incorporate a negative entropy-regularized (NER) cost functional to suppress imputation diversity and enhance accuracy. Furthermore, we derive a closed-form imputation procedure based on the proposed cost functional within the reproducing kernel Hilbert space (RKHS). After that, we further prove that within the WGF framework, the imputation procedure of NewImp, induced from the cost functional associated with conditional distribution, can be induced from another cost functional associated with joint distribution equivalently, within which we merely need to estimate the joint distribution during the model training stage, thereby naturally eliminating the need for data masking. ", "page_idx": 1}, {"type": "text", "text": "Contributions. The main contributions of this paper are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We demonstrate that directly applying diffusion models to MDI causes suboptimal performance, as they prompt unintended diversity and require data masking, both impeding accurate imputation. \u2022 We propose NewImp, a novel DM-based MDI approach under the WGF framework which introduces an NER cost functional to suppress unintended diversity. Based on this, we further prove that the imputation procedure of NewImp can be induced from an equivalent joint-distribution-related functional, and consequently introduce an imputation procedure that sidesteps the data masking. \u2022 We conduct various experiments over public numerical tabular datasets to demonstrate the superiority of the NewImp method over prevalent baseline models. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Problem Formulation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Suppose $X^{\\mathrm{(ideal)}}\\in\\mathbb{R}^{\\mathrm{N}\\times\\mathrm{D}}$ represents an ideal numerical tabular dataset without any missing entries, where $\\mathrm{N}$ and D denote the number of samples and features, respectively. The observed dataset is expressed as: $X^{\\mathrm{(obs)}}=X^{\\mathrm{(ideal)}}\\odot M+\\tt N a N\\odot(\\mathbb{1}_{N\\times D}-M)$ , where $\\odot$ denotes the Hadamard product, $\\mathbb{1}_{\\mathrm{{N}}\\times\\mathrm{{D}}}$ is a matrix of ones of size $\\mathrm{~N}\\times\\mathrm{D}$ , and $M\\in\\{0,1\\}^{\\mathrm{N}\\times\\mathrm{D}}$ is a binary mask that indicates the presence (1) or absence (0) of data in each entry. The task of MDI involves imputing the missing entries in $X^{(\\mathrm{obs})}$ . This is achieved by constructing a matrix $\\hat{X}=X^{\\mathrm{(obs)}}\\odot M\\!+\\!X^{\\mathrm{(imp)}}\\odot\\!\\left(\\mathbb{1}_{\\mathrm{N\\timesD}}\\!-\\!M\\right)$ , where $X^{(\\mathrm{imp})}$ is the matrix containing the imputed values. ", "page_idx": 1}, {"type": "text", "text": "The missing mechanism can be classified into three categories [44]: Missing Completely at Random (MCAR), Missing at Random (MAR), and Missing Not at Random (MNAR) (Detailed information about missing mechanisms is given in Appendix E.1). Notably, in the MNAR setting, it is generally difficult to identify the missing data distribution without additional assumptions and constraints [22]. Hence, our discussion primarily focuses on numerical tabular data with MAR and MCAR settings. ", "page_idx": 1}, {"type": "text", "text": "2.2 Diffusion Models and Its Application for MDI Task ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Diffusion models function by gradually corrupting data towards a tractable noise distribution, such as a standard Gaussian, and subsequently reversing this corruption to generate samples [50]. Specifically, the forward corruption process is modeled as a discretization of a stochastic differential equation (SDE) over time $\\boldsymbol{\\tau};\\mathrm{d}\\boldsymbol{X}_{\\tau}=\\boldsymbol{f}(\\boldsymbol{X}_{\\tau})\\mathrm{d}\\tau\\!+\\!g_{\\tau}\\mathrm{d}W_{\\tau}$ , where $f(\\ensuremath{\\boldsymbol{X}}_{\\tau})$ is drift term, $g_{\\tau}$ is volatility term, and $\\mathrm{d}W\\tau$ is standard Wiener process. The solution to this SDE creates a continuous trajectory of random variables $X_{\\tau}|_{\\tau=0}^{\\mathrm{T}}$ . The density function $q_{\\tau}$ of $X_{\\tau}$ adheres to the Fokker-Planck-Kolmogorov (FPK) ", "page_idx": 1}, {"type": "text", "text": "equation: $\\begin{array}{r}{\\frac{\\partial q_{\\tau}}{\\partial\\tau}=-\\nabla\\cdot\\left(q_{\\tau}f(\\pmb{X}\\tau)\\right)+\\frac{1}{2}g_{\\tau}^{2}\\nabla\\cdot\\nabla q_{\\tau}}\\end{array}$ (see Theorem 5.4 in reference [47]). The reverse process is governed by: $\\mathrm{d}X_{\\tau}=[f(X_{\\tau})-g_{\\tau}^{2}\\nabla\\mathrm{log}\\,p(X_{\\tau})]\\mathrm{d}\\tau+g_{\\tau}\\mathrm{d}W_{\\tau}$ [3], where $\\nabla\\log{p(X_{\\tau})}$ represents the score function, which is often parameterized by neural networks. ", "page_idx": 2}, {"type": "text", "text": "Diffusion models treat MDI as a conditional generation task. The score function, $\\nabla\\log p(X)$ , is defined specifically for MDI as $\\nabla_{X^{(\\mathrm{miss})}}\\log p(X^{(\\mathrm{miss})}|X^{(\\mathrm{obs})})$ [51], and the MDI task is executed by generating samples based on this conditional score function. The key challenge is to obtain an estimation $\\nabla_{X^{(\\mathrm{miss})}}\\log\\hat{p}(X^{(\\mathrm{miss})}|X^{(\\mathrm{obs})})$ that approximates $\\nabla_{X^{(\\mathrm{miss})}}\\log p(X^{(\\mathrm{miss})}|X^{(\\mathrm{obs})})$ . Given that the true $X^{(\\mathrm{miss})}$ is unknown, existing DM-based approaches utilize a mask matrix to drop some observable data as labels. However, the specification of the mask mechanism, determining the effectiveness of $\\nabla\\log\\hat{p}(X^{\\mathrm{(miss)}}|X^{\\mathrm{(obs)}})$ , is challenging since it should align with the data missing mechanism in the testing dataset [51], which may be unknown in practice. ", "page_idx": 2}, {"type": "text", "text": "2.3 Wasserstein Gradient Flow ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Wasserstein space $\\mathcal{P}_{2}(\\mathbb{R}^{\\mathrm{D}})$ is defined as the set of distributions with finite second-order moments. Consider a cost functional $\\mathcal{F}_{\\mathrm{cost}}:\\mathcal{P}_{2}(\\mathbb{R}^{\\mathrm{D}})\\to\\mathbb{R}$ ; the celebrated Wasserstein gradient flow (WGF) is an absolutely continuous trajectory $(q_{\\tau})_{\\tau>0}$ in this space, which evolves over time $\\tau$ to minimize $\\mathcal{F}_{\\mathrm{cost}}$ efficiently. This dynamic is governed by the continuity equation: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{\\partial{q_{\\tau}}}{\\partial{\\tau}}=-\\nabla\\cdot({u_{\\tau}}{q_{\\tau}}),\\quad{u_{\\tau}}=-\\nabla{x}\\frac{\\delta{{\\mathcal F}_{\\mathrm{cost}}}}{\\delta{q_{\\tau}}}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $u_{\\tau}:\\mathbb{R}^{\\mathrm{D}}\\rightarrow\\mathbb{R}^{\\mathrm{D}}$ is a time-dependent velocity field [2], whose input is sample $X\\in\\mathbb{R}^{\\mathrm{D}}$ ; $\\frac{\\delta{\\mathcal F}_{\\mathrm{cost}}}{\\delta{q}_{\\tau}}$ denotes the first variation of $\\mathcal{F}_{\\mathrm{cost}}$ with respect to $q_{\\tau}$ . On this basis, the evolution of $\\mathbf{\\deltaX}$ over time $\\tau$ in $\\mathcal{P}_{2}(\\mathbb{R}^{\\mathrm{D}})$ can be modeled by the ordinary differential equation (ODE): ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}X}{\\mathrm{d}\\tau}=u_{\\tau}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "However, simulating this ODE is challenging since $u_{\\tau}$ involves the estimation of $q_{\\tau}$ , which involves solving the differential equation \u2202\u2202q\u03c4\u03c4 $\\begin{array}{r}{\\frac{\\partial q_{\\tau}}{\\partial\\tau}=-\\nabla\\cdot\\left(u_{\\tau}q_{\\tau}\\right)}\\end{array}$ that proves to be not analytically solvable [16]. ", "page_idx": 2}, {"type": "text", "text": "3 Motivations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Diffusion Models Secretly Foster Diversity ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Based on the notations defined in Section 2.1, we can first define the following cost functional for MDI task according to the maximum likelihood estimation principle, where we want to find the value with the highest probability: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{X^{\\mathrm{(imp)}}=\\underset{X^{(\\mathrm{miss})}}{\\arg\\operatorname*{max}}\\log\\hat{p}(X^{\\mathrm{(miss)}}|X^{\\mathrm{(obs)}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\hat{p}(X^{(\\mathrm{miss})}|X^{(\\mathrm{obs})})$ is the estimation of $p(X^{(\\mathrm{miss})}|X^{(\\mathrm{obs})})$ via neural network [52]. Notably, we can treat $X^{(\\mathrm{miss})}$ as samples from a \u2018proposal distribution\u2019 $r(X^{(\\mathrm{miss})})$ , and formulate the following optimization problem based on variational inference [27, 70]: ", "page_idx": 2}, {"type": "image", "img_path": "fIz8K4DJ7w/tmp/99f69bd337d32fb11c89fcaf35b05e2b5c583178dc85d412c4f751f90c0ca0a8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: Comparison of the optimal point in green triangle and the results obtained by diffusion models in white scatters. See details in Appendix B. ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\arg\\operatorname*{max}}&{{}\\;\\mathbb{E}_{r({\\mathbf{X}^{(\\mathrm{miss})}})}[\\log{\\hat{p}({\\mathbf{X}^{(\\mathrm{miss})}}|{\\mathbf{X}^{(\\mathrm{obs})}})}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where we aim to sample some $X^{(\\mathrm{miss})}$ samples from proposal distribution $r(X^{\\mathrm{(miss)}})$ , realize the maximum log-likelihood estimation over the sampled results, and \u2018optimize\u2019 the proposal distribution $r(X^{\\mathrm{(miss)}})$ that is represented by samples $X^{(\\mathrm{miss})}$ . Notably, in Eq. (4), we use the spirit from previous references represented by [32], where optimizing the samples $X^{(\\mathrm{miss})}$ is equivalent to optimizing the distribution $r(X^{\\mathrm{(miss)}})$ . ", "page_idx": 2}, {"type": "text", "text": "Referring to Eq. (4), it is observed that the MDI task can be formulated as an optimization problem. This prompts a pertinent question: If the conditional distribution $\\log p(\\pmb{X}_{i}^{(\\mathrm{miss})}|\\pmb{X}_{i}^{(\\mathrm{obs})})$ is estimated accurately, what would happen if we directly apply diffusion models to solve the optimization problem corresponding to MDI task? To explore this, we consider a hypothetical scenario: Suppose we are optimizing a cost functional related to a three-dimensional Dirichlet distribution on the simplex $\\Delta^{2}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{arg\\,max}_{a_{h}\\in\\Delta^{2}}\\sum_{h=1}^{\\mathrm{H}}\\{\\log\\frac{\\Gamma(\\sum_{k=1}^{3}\\rho_{k})}{\\prod_{k=1}^{3}\\Gamma(\\rho_{k})}+\\sum_{k=1}^{3}(\\rho_{k}-1)\\log a_{k,h}\\},\\mathrm{H}=8,\\rho|_{k=1}^{3}=[2.5,2.5,5.0],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\pmb{a}_{h}|_{1}^{\\mathrm{H}}$ are variables, $\\mathrm{H}$ is variable number, $\\rho_{k}|_{k=1}^{3}$ is concentration parameter, and $\\Gamma(\\cdot)$ is gamma function. We compare the analytically derived optimal value with the results from diffusion models in Fig. 1. The diffusion model\u2019s results tend to surround but do not exactly reach the optimal value, suggesting that there might be implicit, diversity-encouraging terms integrated into the diffusion models\u2019 objectives that produce the observed inaccuracies. Identifying and modifying these regularization terms is crucial for enhancing the efficacy of diffusion models for MDI tasks. ", "page_idx": 3}, {"type": "text", "text": "3.2 Negative Entropy Regularization Term for Diversity Suppression ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we identify and refine the terms in diffusion models\u2019 objectives that prompt unintended diversity and impede accurate imputation. We observe that the inference process in diffusion models adheres to the FPK equation, which is a specialized form of the continuity equation in WGF (see Sections 2.2 and 2.3). This alignment inspires us to reframe diffusion models within the WGF framework, enabling the derivation of their underlying cost functionals. By doing so, we can compare these functionals with the objective functional for MDI in Eq. (4)2. ", "page_idx": 3}, {"type": "text", "text": "Proposition 3.1. Within WGF framework, DM-based MDI approaches can be viewed as finding the imputed values $X^{(i m p)}$ that maximize the following objective: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{r(X^{(m i s s)})}{\\arg\\operatorname*{max}}}&{{}\\mathbb{E}_{r(X^{(m i s s)})}[\\log\\hat{p}(X^{(m i s s)}|X^{(o b s)})]+\\psi(X^{(m i s s)})+c o n s t,}\\\\ {\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where \u2018const\u2019 is the abbreviation of constant, and $\\psi(X^{(m i s s)})$ is a scalar function determined by the type of SDE underlying the diffusion models. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\psi(X^{(m i s s)})=\\frac12\\mathbb{H}[r(X^{(m i s s)})]+\\mathbb{E}_{r(X^{(m i s s)})}\\{\\frac{1}{4}[X^{(m i s s)}]^{\\top}[X^{(m i s s)}]\\}\\geq0}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "\u2022 VE-SDE: $\\begin{array}{r}{\\psi(X^{(m i s s)})=\\frac{1}{2}\\mathbb{H}[r(X^{(m i s s)})]\\geq0}\\end{array}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\psi(X^{(m i s s)})=\\frac12\\mathbb{H}[r(X^{(m i s s)})]+\\mathbb{E}_{r(X^{(m i s s)})}\\{\\frac{1}{4\\gamma_{\\tau}}[X^{(m i s s)}]^{\\top}[X^{(m i s s)}]\\}\\ge0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathbb{H}[r(X^{(m i s s)})]:=-\\int r(X^{(m i s s)})\\log r(X^{(m i s s)})\\mathrm{d}X^{(m i s s)}}\\end{array}$ is the entropy term, $\\gamma_{\\tau}$ is determined by noise scale $\\beta_{\\tau}$ : $\\begin{array}{r}{\\gamma_{\\tau}:=\\left(1-\\exp(-2\\int_{0}^{\\tau}\\beta_{s}\\mathrm{d}s)\\right)>0,0<\\beta_{1}<\\cdot\\cdot<\\beta_{\\mathrm{T}}<1.}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Proposition 3.1 reveals that diffusion models inherently optimize an objective functional that largely aligns with (4), but they secretly include an additional term $\\psi(X^{(\\mathrm{miss})})>0$ . This term makes Eq. (5) an upper bound on Eq. (4), i.e., maximizing the cost functional in Eq. (5) does not guarantee to maximize the MDI objective in Eq. (4). Furthermore, the entropy term included in the models fosters sample diversity, which may compromise the accuracy required in MDI tasks [53, 38]. To address this issue, we propose incorporating a negative entropy term as $\\psi(X^{\\mathrm{(miss)}})$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\psi(X^{\\mathrm{(miss)}})=-\\lambda\\mathbb{H}[r(X^{\\mathrm{(miss)}})],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\lambda>0$ is a predefined regularization strength, and consequently we can define our NER cost functional for MDI task as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{F}_{\\mathrm{NER}}:=\\mathbb{E}_{r({X^{(\\mathrm{miss})}})}[\\log\\hat{p}({X^{(\\mathrm{miss})}}|{X^{(\\mathrm{obs})}})]-\\lambda\\mathbb{H}[r({X^{(\\mathrm{miss})}})].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The objective functional in Eq. (7) provides a lower bound of Eq. (4). Therefore, maximizing $\\mathcal{F}_{\\mathrm{NER}}$ guarantees maximizing Eq. (4). Meanwhile, $\\mathcal{F}_{\\mathrm{NER}}$ effectively reduces the unintended diversity term, contributing to an improvement in imputation accuracy. ", "page_idx": 3}, {"type": "text", "text": "4 Implementation of the NewImp ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Optimizing the $\\mathcal{F}_{\\mathbf{NER}}$ within WGF Framework ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we aim to optimize $\\mathcal{F}_{\\mathrm{NER}}$ within the WGF framework [46, 69]. To this end, we plug Eq. (7) into Eq. (1), producing the velocity field below that drives the ODE in Eq. (2): ", "page_idx": 4}, {"type": "equation", "text": "$$\nu(X^{(\\mathrm{miss})})=-\\nabla_{X^{(\\mathrm{mis})}}\\frac{\\delta(-\\mathcal{F}_{\\mathrm{NER}})}{\\delta r(X^{(\\mathrm{miss})})}=[\\nabla_{X^{(\\mathrm{miss})}}\\log\\hat{p}(X^{(\\mathrm{miss})}|X^{(\\mathrm{obs})})+\\lambda\\nabla_{X^{(\\mathrm{miss})}}\\log r(X^{(\\mathrm{miss})})],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "However, as stated in Section 2.3, implementing this ODE in computer code is intricate due to the intractability of the density function $r(X^{\\mathrm{(miss)}})$ . Fortunately, by restricting the velocity field within the Reproducing Kernel Hilbert Space (RKHS) defined by the kernel function $u(X^{(\\mathrm{miss})})\\in$ $K(\\mathbf{\\cal{X}}^{\\mathrm{(miss)}},\\tilde{\\mathbf{\\cal{X}}}^{\\mathrm{(miss)}})$ , an alternative ODE minimizing $\\mathcal{F}_{\\mathrm{NER}}$ can be implemented in Proposition 4.1 [35, 31] which sidesteps the intractable $r(X^{\\mathrm{(miss)}})^{3}$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition 4.1. Suppose $u(X^{(m i s s)})$ is a velocity field regularized by the RKHS norm under the following conditions: $^{\\,l}$ ). The kernel function satisfies: $\\begin{array}{r}{\\operatorname*{lim}_{\\|\\boldsymbol{X}^{(m i s s)}\\|\\rightarrow\\infty}K(\\boldsymbol{X}^{(m i s s)},\\tilde{\\boldsymbol{X}}^{(m i s s)})\\,=\\,0}\\end{array}$ 2). The density $r(X^{(m i s s)})$ is bounded. Then, the velocity field that minimizes the cost functional $\\mathcal{F}_{N E R}=\\mathbb{E}_{r({X^{(m i s s)}})}[\\log\\hat{p}({X^{(m i s s)}}|{X^{(o b s)}})]-\\lambda\\mathbb{H}[r({X^{(m i s s)}})]$ can be given by: ", "page_idx": 4}, {"type": "equation", "text": "$$\nu({\\pmb X}^{(m i s s)})=\\mathbb{E}_{r(\\tilde{\\pmb X}^{(m i s s)})}\\left\\{\\begin{array}{l l}{-\\lambda\\nabla_{\\tilde{\\pmb X}^{(m i s s)}}K({\\pmb X}^{(m i s s)},\\tilde{\\pmb X}^{(m i s s)})}\\\\ {\\qquad+\\left[\\nabla_{\\tilde{\\pmb X}^{(m i s s)}}\\log\\hat{p}(\\tilde{\\pmb X}^{(m i s s)}|{\\pmb X}^{(o b s)})\\right]^{\\top}K({\\pmb X}^{(m i s s)},\\tilde{\\pmb X}^{(m i s s)})\\right\\}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the expectation term $\\mathbb{E}_{r(\\tilde{\\mathbf{X}}^{(m i s s)})}$ can be efficiently estimated using Monte Carlo approximation, $K(X,{\\tilde{X}})$ is set as the radial basis function $(R B F)$ kernel. ", "page_idx": 4}, {"type": "text", "text": "4.2 Sidestepping Mask Matrix: Conditional Modeling via Joint Modeling ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Simulating the ODE in Eq. (2) with Eq. (8) necessitates an accurate estimation of $p(\\pmb{X}^{\\mathrm{(miss)}}|\\pmb{X}^{\\mathrm{(obs)}})$ However, this modeling is challenging due to the diverse choices of masking matrices. More specifically, the accuracy of the estimated conditional distribution $p(\\pmb{X}^{\\mathrm{(miss)}}|\\pmb{X}^{\\mathrm{(obs)}})$ heavily relies on the selection of these matrices, and these matrices should be consistent with the data missing mechanism in the testing dataset, which may be unknown in practice [51]. To bypass this difficulty, we suggest substituting the conditional distribution $p(X^{(\\mathrm{miss})}|X^{(\\mathrm{obs})})$ with the joint distribution $p(\\mathbf{{X}^{(\\mathrm{{joint})}})}$ , where $X^{\\mathrm{(joint)}}\\,=\\,(X^{\\mathrm{(miss)}},X^{\\mathrm{(obs)}})$ . Building on this substitution, the velocity field is redefined based on the estimated joint distribution $\\hat{p}(X^{(\\mathrm{joint})})$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nu(X^{(\\mathrm{joint})})=\\mathbb{E}_{r(\\tilde{X}^{(\\mathrm{joint})})}\\left\\{\\begin{array}{l l}{-\\lambda\\nabla_{\\tilde{X}^{(\\mathrm{miss})}}K(X^{(\\mathrm{joint})},\\tilde{X}^{(\\mathrm{joint})})}\\\\ {\\qquad+\\left[\\nabla_{\\tilde{X}^{(\\mathrm{miss})}}\\log\\hat{p}(\\tilde{X}^{(\\mathrm{joint})})\\right]^{\\top}K(X^{(\\mathrm{joint})},\\tilde{X}^{(\\mathrm{joint})})}\\end{array}\\right\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where \u2207X\u02dc(miss) log p\u02c6( X\u02dc(joint)) can be obtained by masking the $\\nabla_{X^{\\mathrm{(ioint)}}}\\log\\hat{p}(X^{\\mathrm{(joint)}})$ with the missing data indicator matrix $_M$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla_{\\tilde{X}^{(\\mathrm{mis})}}\\log\\hat{p}(\\tilde{X}^{(\\mathrm{joint})})=\\nabla_{\\tilde{X}^{(\\mathrm{joint})}}\\log\\hat{p}(\\tilde{X}^{(\\mathrm{joint})})\\odot(\\mathbb{1}_{\\mathrm{N}\\times\\mathrm{D}}-M)+0\\times M,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and the expression of kernel function term can be directly given based on the expression of RBF: ", "page_idx": 4}, {"type": "equation", "text": "$$\nK({X}^{(\\mathrm{joint})},\\tilde{{X}}^{(\\mathrm{joint})})=\\exp(-\\frac{\\|{X}^{(\\mathrm{joint})}-\\tilde{{X}}^{(\\mathrm{joint})}\\|^{2}}{2h^{2}}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where h is the bandwidth, the values of X\u02dc(joint) and $X^{(j o i n t)}$ are identical, and the tilde notation on X\u02dc(joint) is merely used to distinguish the variable with respect to which the derivative is taken. On this basis, similar to Eq. (10), the gradient term $\\nabla_{\\tilde{\\pmb{X}}^{\\mathrm{(miss)}}}K(\\pmb{X}^{\\mathrm{(joint)}},\\tilde{\\pmb{X}}^{\\mathrm{(joint)}})$ can be given as follows: $\\nabla_{\\tilde{X}^{(\\mathrm{mis})}}K(X^{(\\mathrm{joint})},\\tilde{X}^{(\\mathrm{joint})})=\\nabla_{\\tilde{X}^{(\\mathrm{joint})}}K(X^{(\\mathrm{joint})},\\tilde{X}^{(\\mathrm{joint})})\\odot(\\mathbb{1}_{\\mathrm{N}\\times\\mathrm{D}}-M)+0\\times M,$ (12) and since K(X(joint), X\u02dc(join t)) is a smooth function, \u2207X\u02dc(joint)K(X(joint), X\u02dc(joint)) can be easily computed by automatic-differentiation-based deep learning backends like by PyTorch [42]. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Proposition 4.2 demonstrates that the cost functional $\\mathcal{F}_{\\mathrm{joint-NER}}$ , associated with Eq. (10), and $\\mathcal{F}_{\\mathrm{NER}}$ exhibit a constant gap, indicating that optimizing $\\mathcal{F}_{\\mathrm{joint-NER}}$ is equivalent to optimizing $\\mathcal{F}_{\\mathrm{NER}}$ . ", "page_idx": 5}, {"type": "text", "text": "Proposition 4.2. Assume that the proposal distribution $r(\\boldsymbol{X}^{(j o i n t)})$ is factorized by $r(X^{(j o i n t)}):=$ $r(X^{(m i s s)})p(X^{(o b s)})$ . The cost functional associated with the joint distribution is defined as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{F}_{j o i n t\\-N E R}:=\\mathbb{E}_{r(X^{(j o i n t)})}[\\log\\hat{p}(X^{(j o i n t)})]-\\lambda\\mathbb{H}[r(X^{(j o i n t)})],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which leads to the velocity field delineated in Eq. (9) and establishes ${\\mathcal{F}}_{j o i n t}$ -NER as a lower bound for ${\\mathcal{F}}_{N E R}$ , with the difference being a constant (i.e., $\\mathcal{F}_{j o i n t-N E R}=\\mathcal{F}_{N E R}-c o n s t,c o n s t\\ge0,$ ). ", "page_idx": 5}, {"type": "text", "text": "The detailed justification for the factorization $r(X^{\\mathrm{(joint)}}):=r(X^{\\mathrm{(miss)}})p(X^{\\mathrm{(obs)}})$ is provided in Appendix C. Based on this proposition, the following corollary can be obtained: ", "page_idx": 5}, {"type": "text", "text": "Corollary 4.3. The following equation holds: $u(\\pmb{X}^{(j o i n t)})=u(\\pmb{X}^{(m i s s)})$ . ", "page_idx": 5}, {"type": "text", "text": "So far, we know that $u(X^{(\\mathrm{joint})})$ can reduce $\\mathcal{F}_{\\mathrm{NER}}$ as effectively as possible, which indicates that the velocity field defined in Eq. (9) can fully substitute for Eq. (8) in optimizing $\\mathcal{F}_{\\mathrm{NER}}$ without loss of accuracy. Finally, the imputed value can be obtained by simulating the following ODE: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}X^{\\mathrm{(miss)}}}{\\mathrm{d}\\tau}=u(X^{\\mathrm{(joint)}}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4.3 Estimating the Joint Distribution ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The remaining problem is to determine the estimation of score function $\\nabla_{X^{\\mathrm{(joint)}}}\\log\\hat{p}(X^{\\mathrm{(joint)}})$ . To achieve this, we employ Denoising Score Matching (DSM) [21, 52] to train the score function $\\nabla_{X^{\\mathrm{(joint)}}}\\log\\hat{p}(X^{\\mathrm{(joint)}})$ parameterized by a neural network. Specifically, the learning objective is designed to minimize the discrepancy between the actual score and the model\u2019s predicted score after introducing Gaussian noise to the clean $X^{(\\mathrm{joint})}$ as $\\hat{\\pmb X}^{(\\mathrm{joint})}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{DSM}}:=\\frac{1}{2}\\mathbb{E}_{q_{\\sigma}(\\hat{\\boldsymbol{X}}^{(\\mathrm{pint})}|\\boldsymbol{X}^{(\\mathrm{pain})})}[\\|\\nabla_{\\hat{\\boldsymbol{X}}^{(\\mathrm{pain})}}\\log\\hat{p}(\\hat{\\boldsymbol{X}}^{(\\mathrm{pint})})-\\nabla_{\\hat{\\boldsymbol{X}}^{(\\mathrm{pain})}}\\log q_{\\sigma}(\\hat{\\boldsymbol{X}}^{(\\mathrm{pint})}|\\boldsymbol{X}^{(\\mathrm{pain})})\\|^{2}].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Notably, \u03c3 is variance scale, X\u02c6(joint) is obtained by X\u02c6(joint) $\\hat{\\boldsymbol X}^{(\\mathrm{joint})}\\,=\\,\\boldsymbol X^{(\\mathrm{joint})}\\,+\\,\\epsilon,\\epsilon\\,\\sim\\,\\mathcal N(\\bf0,\\sigma^{2}I)$ , and $\\begin{array}{r}{\\nabla_{\\hat{X}^{(\\mathrm{joint})}}\\log q_{\\sigma}(\\hat{{X}}^{(\\mathrm{joint})}|X^{(\\mathrm{joint})})\\;=\\;-\\frac{\\hat{{X}}^{(\\mathrm{joint})}-{X}^{(\\mathrm{joint})}}{\\sigma^{2}}\\;}\\end{array}$ . Once $\\nabla_{X^{\\mathrm{(joint)}}}\\log\\hat{p}(X^{\\mathrm{(joint)}})$ is trained, we can obtain the imputation value by simulating the ODE based on Eqs. (9) and (14). ", "page_idx": 5}, {"type": "text", "text": "4.4 Overall Workflow of NewImp ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The computation workflow of NewImp is encapsulated in Algorithm 1. Specifically, we perform a mean imputation to the incomplete matrix $X^{(\\mathrm{obs})}$ , producing a pre-imputed dataset denoted as $X^{\\mathrm{(imp)}}$ (step 1). After that, we iteratively conduct DSM training and ODE simulation. In DSM training (steps 3-5), we form $X^{(\\mathrm{joint})}$ and conduct DSM on it to acquire a score estimator. In ODE simulation (steps 6-8), we set the starting point and perform ODE simulation, where $u$ is calculated with the score estimator acquired in step 5. The endpoint is treated as the imputation results at the current iteration. After completing $\\tau$ iterations of this process, the imputed dataset X\u02c6 is calculated, where we preserve the observed indices in $X^{(\\mathrm{obs})}$ (step 10). ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets: We conduct the case study based on eight real-world datasets from the UCI repository. To simulate missing data, we mask the dataset using a mask matrix, which is realized with a Bernoulli random variable of fixed mean. More detailed information is provided in Appendix E.1. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 The overall workflow of NewImp approach ", "page_idx": 6}, {"type": "text", "text": "Input: observational data $X^{(\\mathrm{obs})}$ , mask matrix $_M$ ", "page_idx": 6}, {"type": "text", "text": "Hyperparameter: loop time: $\\tau$ , simulation time: T, discretization step size $\\eta$ , bandwidth $h$ , neural network learning rate $l r$ , training epoch $\\mathcal{E}$ , neural network hidden unit $\\mathrm{HU_{score}}$ . ", "page_idx": 6}, {"type": "text", "text": "Output: imputed data $\\hat{\\pmb X}$ .   \n1: $X^{\\mathrm{(imp)}}\\gets\\mathrm{Initialize}(X^{\\mathrm{(obs)}})\\odot(\\mathbb{1}_{\\mathrm{N}\\times\\mathrm{D}}-M)$ \u25b7Initialization   \n2: for $t=0$ to $\\tau$ do   \n3: $X^{\\mathrm{(miss)}}\\gets X^{\\mathrm{(imp)}}$   \n4: $X^{(\\mathrm{joint})}\\gets X^{(\\mathrm{miss})}\\odot(\\mathbb{1}_{\\mathrm{N}\\times\\mathrm{D}}-M)+X^{(\\mathrm{obs})}\\odot M$   \n5: $\\nabla_{X^{(\\mathrm{joint})}}\\log\\hat{p}(X^{(\\mathrm{joint})})\\gets\\mathsf{D S M}(X^{(\\mathrm{joint})})$ \u25b7see Algorithm 3   \n67:: $X_{0}^{\\mathrm{(miss)}}\\leftarrow X^{\\mathrm{(imp)}}$ $\\begin{array}{r}{X_{\\mathrm{T}}^{\\mathrm{(miss)}}\\leftarrow X_{0}^{\\mathrm{(miss)}}+\\int_{0}^{\\mathrm{T}}u(X^{\\mathrm{(joint)}})\\mathrm{d}\\tau}\\end{array}$ \u25b7ODE Simulation by Appendix D.1 with Eq. (9)   \n8: $X^{\\mathrm{(imp)}}\\leftarrow X_{\\mathrm{T}}^{\\mathrm{(miss)}}$ )   \n9: end for   \n10: $\\overleftarrow{X}\\leftarrow{X}^{\\left(\\mathrm{obs}\\right)}\\odot M+X^{\\left(\\mathrm{imp}\\right)}\\odot(\\mathbb{1}_{\\mathrm{N}\\times\\mathrm{D}}-M)$ ", "page_idx": 6}, {"type": "text", "text": "Baselines: We compare NewImp with DM-based MDI models: CSDI for Tabular Data (CSDI_T) [51], MissDiff [41]. In addition, we also select other well-known MDI models like Sinkhorn (Sink) [40], Transform Distribution Matching (TDM) [72], Generative Adversarial Imputation Nets (GAIN) [67], Missing Data Importance-Weighted Autoencoder (MIWAE) [39], Missing data Imputation Refinement And Causal LEarning (MIRACLE) [28], and ReMasker [15]. Details concerning experimental settings are given in Appendix E.2. ", "page_idx": 6}, {"type": "text", "text": "Implementation Details: In this study, we employ a two-layer multi-layer perceptron to model $\\nabla_{X^{\\mathrm{(joint)}}}\\log\\hat{p}(X^{\\mathrm{(joint)}})$ . Each layer is configured with 256 hidden units $\\mathrm{(HU_{score})}$ , and the activation function is set as the \u2018Swish\u2019 function [43]. For DSM training (step 5 of Algorithm 1), the variance scale $\\sigma$ is set as 0.1, the network is trained by the Adam optimizer [26] with the learning rate of $1.0\\times10^{-3}$ , and the batch size is dynamically set to dataset size N. Meanwhile, for the ODE simulation part (step 7 of Algorithm 1), we specify a simulation time (T) of 500, a regularization strength of $\\left(\\lambda\\right)10.0$ , a step size of $(\\eta)\\,0.1$ , and a bandwidth $(h)$ of 0.5. The loop time $\\tau$ for NewImp is set as 2. Since only missing indices are updated, the evaluation focuses exclusively on imputation errors for these indices. To this end, we modify the mean absolute error (MAE) and squared Wasserstein-2 distance (WASS) according to reference [22] as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{MAE}:=\\frac{\\sum_{i=1}^{\\mathrm{N}}\\sum_{j=1}^{\\mathrm{D}}\\left[|X_{i,j}^{(\\mathrm{ideal})}-\\hat{X}_{i,j}|\\odot(\\mathbb{1}_{\\mathrm{N}\\times\\mathrm{D}}-M)_{i,j}\\right]}{\\sum_{i=1}^{\\mathrm{N}}\\sum_{j=1}^{\\mathrm{D}}\\left(\\mathbb{1}_{\\mathrm{N}\\times\\mathrm{D}}-M\\right)_{i,j}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{WASS:}=\\mathcal{W}_{2}^{2}[\\frac{1}{m_{1}}\\sum_{i=1}^{m_{1}}\\varDelta_{[\\hat{X}_{M_{1}}]_{i,:}},\\frac{1}{m_{1}}\\sum_{i=1}^{M_{1}}\\varDelta_{[X_{M_{1}}^{(\\mathrm{idel})}]_{i,:}}],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathcal{W}_{2}^{2}$ denotes the squared Wasserstein-2 distance, ${M}_{1}:=\\{i:\\exists j,{M}_{i,j}=0\\}$ represents the subset of $M_{i,j}$ with at least one missing value, $\\pmb{m}_{1}$ is the number of data points with at least one missing value, and $\\varDelta_{X}$ is the Dirac distribution (measure) concentrated on $\\mathbf{\\deltaX}$ . ", "page_idx": 6}, {"type": "text", "text": "5.2 Baseline Comparison Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Table 1 presents the imputation quality of NewImp and other imputation approaches under the MAR and MCAR scenarios. The primary observations are detailed as follows: ", "page_idx": 6}, {"type": "text", "text": "\u2022 Models with neural architectures such as MIRACLE, MIWAE, and TDM demonstrate superior performance compared to models lacking such architectures. This observation suggests that integrating neural networks into MDI tasks can significantly enhance model performance. \u2022 DM-based imputation approaches generally perform worse than other MDI methods. This outcome indicates that despite the incorporation of complex nonlinear neural architectures to boost performance, employing diversity-oriented generative approaches may not align well with the precision requirements of MDI tasks. ", "page_idx": 6}, {"type": "table", "img_path": "fIz8K4DJ7w/tmp/2713986ffc1395ad47579b03f0e21ff67e39bceb677bcb7f3708667bac83ac41.jpg", "table_caption": ["Table 1: Overall performance of MAE and WASS metrics with $30\\%$ missing rate. "], "table_footnote": ["Kindly Note: The best results are bolded and the second best results are underliend. \u201c\\*\u201d marks the results that NewImp significantly outperform with $p$ -value $<0.05$ over paired samples $t$ -test. "], "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{c c c c c c c l o n i n}{\\bmod n E R1\\mid\\log n I\\qquad\\op\\mid\\ \\mathbf{Bx}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf{D}\\mathbf\n$$", "text_format": "latex", "page_idx": 7}, {"type": "table", "img_path": "", "table_caption": ["Table 2: Ablation results with $30\\%$ missing rate. "], "table_footnote": ["Kindly Note: The best results are bolded and the second best results are underliend. \u201c\\*\u201d marks the results that NewImp significantly outperform with $p$ -value $<0.05$ over paired samples $t$ -test. "], "page_idx": 7}, {"type": "text", "text": "\u2022 Our proposed NewImp method consistently ranks as the best or second-best across most comparisons under most of the scenarios and datasets. Notably, NewImp significantly outperforms other DM-based MDI approaches, underscoring the effectiveness of our analytical enhancements and innovations in Sections 3.1, 3.2 and 4.2. ", "page_idx": 7}, {"type": "text", "text": "5.3 Ablation Study Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we conduct the ablation study to assess the contributions of two key components in NewImp: the NER term and the joint modeling strategy (referred to as \u2018Joint\u2019). The results of this study are detailed in Table 2. Analysis of the data between the second and last rows of Table 2 reveals that, for most cases, in the absence of the NER, the proposal distribution $r(X^{(\\mathrm{miss})})$ may become pathological, leading to diminished model performance. Additionally, when comparing results from the first, third, and last rows, it becomes evident that modeling the joint distribution directly, rather than inferring it from the conditional distribution, significantly enhances model performance. This finding underscores the effectiveness of the strategies we have implemented, as discussed in Section 4.2. Overall, the ablation study underscores the critical roles of both the NER term and the joint distribution learning strategy in promoting the performance of NewImp. ", "page_idx": 7}, {"type": "text", "text": "5.4 Sensitivity Analysis Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we analyze the impact of key hyperparameters within the NewImp approach, including the bandwidth $h$ of the RBF kernel function, the hidden units $\\mathrm{HU}_{\\mathrm{score}}$ in the score network, the weight $\\lambda$ of the NER term, and the discretization step size $\\eta$ for simulating the ODE defined in Eq. (9). The profound influence of these hyperparameters on learning objectives and overall performance is substantiated by the experimental results presented in Fig. 2. Initially, we explore the effects of varying the bandwidth $h$ . We observe that an increase in bandwidth correlates with a decrease in imputation accuracy. For instance, as the bandwidth increases from 0.5 to 2.0, the MAE and WASS escalate from 0.35 and 0.25 to 0.82 and 0.74, respectively. This trend suggests that excessive bandwidth can lead to an over-smoothed velocity field, expanding the exploration space of the distribution $r(X^{(\\mathrm{joint})})$ excessively and failing to adequately \u2018concentrate\u2019 this distribution, ultimately diminishing performance. Subsequently, we examine changes in the score network\u2019s hidden units. Increasing the hidden units from 256 to 512 appears to decrease imputation accuracy, likely due to overfitting issues associated with larger neural networks. Next, we adjust the strength of the NER term and find that increasing its intensity generally improves imputation accuracy. This supports the necessity of the NER term, further validating its effectiveness. Lastly, we investigate the discretization step size for the ODE. We find that accuracy initially increases with smaller step sizes but then decreases. This pattern is consistent with ODE simulation behavior, where smaller step sizes require longer to converge, potentially resulting in lower accuracy within a predefined time. Conversely, larger step sizes increase discretization errors, adversely affecting accuracy as well. ", "page_idx": 7}, {"type": "image", "img_path": "fIz8K4DJ7w/tmp/cd9c6de41bec84ef208c2c81f07d836dc40ee26ba9a332937377341f2b3e81e2.jpg", "img_caption": ["(a) MAR with $30\\%$ missing rate at CC dataset. (b) MCAR with $30\\%$ missing rate at CC dataset. ", "Figure 2: Parameter sensitivity of NewImp on bandwidth for kernel function $(h)$ , hidden unit of score network $\\mathrm{HU_{score}}$ , NER weight $\\lambda$ , and discretization step $\\eta$ for Eq. (9) on CC dataset. Mean values and one standard deviation from mean are represented by scatters and shaded area, respectively. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6 Related Works ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "6.1 Diffusion Models for Missing Data Imputation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The impressive ability of diffusion models to synthesize data [54, 76, 7] has inspired extensive research into their application for MDI tasks [59, 66]. Among the pioneering efforts, the Conditional Score-based Diffusion models for Imputation (CSDI) [51] was the first to adapt diffusion models for time-series MDI, substituting the score function with a conditional distribution and pioneering a novel model training strategy by masking parts of the observational data. Building on this, to address categorical data in tabular datasets, CSDI_T [73] introduced an embedding layer within the feature extractor. To enhance inference efficiency, the conditional Schr\u00f6dinger bridge method for probabilistic time series imputation proposed modeling the diffusion process as a Schr\u00f6dinger bridge [10]. Meanwhile, MissDiff [41] utilizes the missing data information as the mask matrix to improve the model training procedure. ", "page_idx": 8}, {"type": "text", "text": "Despite these advancements from the perspective of feature extraction module [1, 64], loss function [41], and model inference approach [60], as pointed out by reference [38], the reconciliation of the inherent diversity-seeking nature of diffusion models\u2019 generative processes and the accuracycentric demands of MDI task remains underexplored. To our knowledge, this paper is the first to elucidate the relationship between diffusion models\u2019 generative processes and MDI tasks from an optimization perspective (Sections 3.1 and 3.2), which has not been discovered by previous reference [38]. Based on these insights, we further propose our NewImp approach by designing the NER term to prioritize the MDI accuracy (Section 3.2). ", "page_idx": 8}, {"type": "text", "text": "6.2 Modeling Conditional Distribution by Joint Distribution ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Modeling conditional distribution as joint distribution remains an opening question and has a broad potential for application [68, 8, 25]. Conditional sliced WGF [14] first empirically validated that the velocity field of joint distribution and conditional distribution are identical when choosing sliced Wasserstein distance as cost functional. After that, reference [25] extended this relationship and derived the relationship between conditional and joint distribution in various discrepancy metrics like $f$ -divergence, Wasserstein distance, and integral probability metrics. On this basis, reference [19] further theoretically proved the equivalence of velocity fields for conditional and joint distribution. ", "page_idx": 9}, {"type": "text", "text": "However, the objective of NewImp does not belong to any kind of discrepancy metric [25]. The most similar discrepancy metric is negative KL divergence. Notably, negative KL divergence contains diversity-encouraging \u2018positive\u2019 entropy as the regularization term, and the regularization term in our study is diversity-discouraging \u2018negative\u2019 entropy, and thus more than directly applying these results to our research is needed. On this basis, our theoretical contribution proves that this joint distribution modeling approach can still be applied when the functional is regularized by the negative entropy. ", "page_idx": 9}, {"type": "text", "text": "6.3 Wasserstein Gradient Flow for Generative Modeling ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "WGF [2, 46] has been extensively employed in various domains of machine learning, including generative modeling [17, 12, 74, 63], posterior distribution sampling [55, 35, 32, 34], and domain adaptation [75, 36, 37, 71]. In generative modeling [4, 11, 18], the problem is framed as an optimization task, with the objective functional comprising an $f$ -divergence term that measures the discrepancy between the proposal distribution and the data distribution, alongside an entropy term that promotes diversity in generative results. ", "page_idx": 9}, {"type": "text", "text": "WGF is then utilized to address the optimization of this cost functional, with models being constructed during the solution process. However, as indicated by our illustrative example (Section 3.1), and further supported by our theoretical analysis (Section 3.2), pursuing diversity in accuracy-oriented tasks such as MDI may not be appropriate. Our analysis reveals that the inclusion of an entropy term in the cost functional makes the direct application of diffusion models to MDI tasks unsuitable. Based on these insights, one of our major contributions is demonstrating that WGF can be effectively used to analyze and improve the appropriateness of applying diffusion models to non-generative tasks. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work demonstrated that directly applying diffusion models to MDI resulted in suboptimal performance due to unintended diversity and the requirement for data masking, both of which impeded accurate imputation. To counteract this, we proposed NewImp, a novel diffusion model-based MDI approach within the Wasserstein gradient flow framework, designed to suppress unintended diversity. We developed an easy-to-implement form for realizing NewImp in computer code by constraining the velocity field within the reproducing kernel Hilbert space. Furthermore, we proved that the imputation procedure of NewImp could be derived from an equivalent joint-distribution-related functional, thereby obviating the need for data masking. Finally, extensive experiments demonstrated that NewImp effectively mitigates these issues and outperforms prevalent baseline models. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by the National Natural Science Foundation of China (NSFC) under Grants 62473103 and 623B2002. The first author Zhichao Chen and the corresponding author Hao Wang would like to express their sincere gratitude to PhD Candidate Weiming Liu at Zhejiang University for valuable discussions on the implementation of the FPK equation via ODE/SDE. ", "page_idx": 9}, {"type": "text", "text": "Dedicated to the 100th Anniversary of Sun Yat-sen University, the Alma of Zhichao Chen. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Juan Lopez Alcaraz and Nils Strodthoff. Diffusion-based time series imputation and forecasting with structured state space models. Trans. Mach. Learn. Res., 2022.   \n[2] Luigi Ambrosio, Nicola Gigli, and Giuseppe Savar\u00e9. Gradient flows: in metric spaces and in the space of probability measures. Springer Science & Business Media, 2005.   \n[3] Brian DO Anderson. Reverse-time diffusion equation models. Stoch. Process. their Appl., 12 (3):313\u2013326, 1982.   \n[4] Abdul Fatir Ansari, Ming Liang Ang, and Harold Soh. Refining deep generative models via discriminator gradient flow. In Proc. Int. Conf. Learn. Represent., pages 1\u201324, 2021.   \n[5] Matthew James Beal. Variational algorithms for approximate Bayesian inference. University of London, University College London (United Kingdom), 2003.   \n[6] John Charles Butcher. Numerical methods for ordinary differential equations. John Wiley & Sons, 2016.   \n[7] Hanqun Cao, Cheng Tan, Zhangyang Gao, Yilun Xu, Guangyong Chen, Pheng-Ann Heng, and Stan Z. Li. A survey on generative diffusion models. IEEE Trans. Knowl. Data Eng., 36(7): 2814\u20132830, 2024. doi: 10.1109/TKDE.2024.3361474.   \n[8] Jannis Chemseddine, Paul Hagemann, Christian Wald, and Gabriele Steidl. Conditional wasserstein distances with applications in bayesian ot flow matching. arXiv preprint arXiv:2403.18705, pages 1\u201342, 2024.   \n[9] Changyou Chen, Ruiyi Zhang, Wenlin Wang, Bai Li, and Liqun Chen. A unified particleoptimization framework for scalable bayesian sampling. arXiv preprint arXiv:1805.11659, pages 1\u201313, 2018.   \n[10] Yu Chen, Wei Deng, Shikai Fang, Fengpei Li, Nicole Tianjiao Yang, Yikai Zhang, Kashif Rasul, Shandian Zhe, Anderson Schneider, and Yuriy Nevmyvaka. Provably convergent schr\u00f6dinger bridge with applications to probabilistic time series imputation. In Proc. Int. Conf. Mach. Learn., pages 4485\u20134513, 2023.   \n[11] Xiuyuan Cheng, Jianfeng Lu, Yixin Tan, and Yao Xie. Convergence of flow-based generative models via proximal gradient descent in wasserstein space. IEEE Trans. Inf. Theory, pages 1\u20131, 2024. doi: 10.1109/TIT.2024.3422412.   \n[12] Jaemoo Choi, Jaewoong Choi, and Myungjoo Kang. Scalable Wasserstein gradient flow for generative modeling through unbalanced optimal transport. In Proc. Int. Conf. Mach. Learn., pages 8629\u20138650, 2024.   \n[13] Hanze Dong, Xi Wang, LIN Yong, and Tong Zhang. Particle-based variational inference with preconditioned functional gradient flow. In Proc. Int. Conf. Learn. Represent., pages 1\u201326, 2022.   \n[14] Chao Du, Tianbo Li, Tianyu Pang, Shuicheng Yan, and Min Lin. Nonparametric generative modeling with conditional sliced-wasserstein flows. In Proc. Int. Conf. Mach. Learn., pages 8565\u20138584, 2023.   \n[15] Tianyu Du, Luca Melis Melis, and Ting Wang. Remasker: Imputing tabular data with masked autoencoding. In Proc. Int. Conf. Learn. Represent., pages 1\u201323, 2024.   \n[16] Lawrence C Evans. Partial differential equations, volume 19. American Mathematical Society, 2022.   \n[17] Jiaojiao Fan, Qinsheng Zhang, Amirhossein Taghvaei, and Yongxin Chen. Variational Wasserstein gradient flow. In Proc. Int. Conf. Mach. Learn., pages 6185\u20136215, 2022.   \n[18] Yuan Gao, Yuling Jiao, Yang Wang, Yao Wang, Can Yang, and Shunkang Zhang. Deep generative learning via variational gradient flow. In Proc. Int. Conf. Mach. Learn., pages 2093\u20132101, 2019.   \n[19] Paul Hagemann, Johannes Hertrich, Fabian Altekr\u00fcger, Robert Beinert, Jannis Chemseddine, and Gabriele Steidl. Posterior sampling based on gradient flows of the MMD with negative distance kernel. In Proc. Int. Conf. Learn. Represent., pages 1\u201332, 2024.   \n[20] Ya-Ping Hsieh, Ali Kavis, Paul Rolland, and Volkan Cevher. Mirrored Langevin Dynamics. In Proc. Adv. Neural Inf. Process. Syst., pages 1\u201310, 2018.   \n[21] Aapo Hyv\u00e4rinen. Estimation of non-normalized statistical models by score matching. J. Mach. Learn. Res., 6(24):695\u2013709, 2005.   \n[22] Daniel Jarrett, Bogdan C Cebere, Tennison Liu, Alicia Curth, and Mihaela van der Schaar. Hyperimpute: Generalized iterative imputation with automatic model selection. In Proc. Int. Conf. Mach. Learn., pages 9916\u20139937, 2022.   \n[23] Richard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the fokker\u2013 planck equation. SIAM J. Math. Anal., 29(1):1\u201317, 1998.   \n[24] Valentin Khrulkov, Gleb Ryzhakov, Andrei Chertkov, and Ivan Oseledets. Understanding DDPM latent codes through optimal transport. In Proc. Int. Conf. Learn. Represent., pages 1\u201315, 2022.   \n[25] Young-geun Kim, Kyungbok Lee, and Myunghee Cho Paik. Conditional Wasserstein Generator. IEEE Trans. Pattern Anal. Mach. Intell., 45(6):7208\u20137219, 2023. doi: 10.1109/TPAMI.2022. 3220965.   \n[26] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proc. Int. Conf. Learn. Represent., pages 1\u20138, 2015.   \n[27] Diederik P Kingma and Max Welling. Auto-encoding Variational Bayes. In Proc. Int. Conf. Learn. Represent., pages 1\u20138, 2014.   \n[28] Trent Kyono, Yao Zhang, Alexis Bellot, and Mihaela van der Schaar. MIRACLE: Causallyaware imputation via learning missing data mechanisms. Proc. Adv. Neural Inf. Process. Syst., pages 23806\u201323817, 2021.   \n[29] Haoxuan Li, Kunhan Wu, Chunyuan Zheng, Yanghao Xiao, Hao Wang, Zhi Geng, Fuli Feng, Xiangnan He, and Peng Wu. Removing hidden confounding in recommendation: a unified multi-task learning approach. Proc. Adv. Neural Inf. Process. Syst., pages 1\u201313, 2024.   \n[30] Haoxuan Li, Chunyuan Zheng, Shuyi Wang, Kunhan Wu, Eric Wang, Peng Wu, Zhi Geng, Xu Chen, and Xiao-Hua Zhou. Relaxing the accurate imputation assumption in doubly robust learning for debiased collaborative filtering. In Proc. Int. Conf. Mach. Learn., pages 29448\u2013 29460, 2024.   \n[31] Yingzhen Li and Richard E. Turner. Gradient estimators for implicit models. In Proc. Int. Conf. Learn. Represent., pages 1\u201319, 2018.   \n[32] Chang Liu, Jingwei Zhuo, Pengyu Cheng, Ruiyi Zhang, and Jun Zhu. Understanding and accelerating particle-based variational inference. In Proc. Int. Conf. Mach. Learn., pages 4082\u20134092. PMLR, 2019.   \n[33] Diju Liu, Yalin Wang, Chenliang Liu, Xiaofeng Yuan, Kai Wang, and Chunhua Yang. Scopefree global multi-condition-aware industrial missing data imputation framework via diffusion transformer. IEEE Trans. Knowl. Data Eng., pages 1\u201312, 2024. doi: 10.1109/TKDE.2024. 3392897.   \n[34] Qiang Liu. Stein variational gradient descent as gradient flow. In Proc. Adv. Neural Inf. Process. Syst., volume 30, pages 1\u201315, 2017.   \n[35] Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference algorithm. In Proc. Adv. Neural Inf. Process. Syst., volume 29, pages 1\u201313, 2016.   \n[36] Weiming Liu, Jiajie Su, Chaochao Chen, and Xiaolin Zheng. Leveraging distribution alignment via stein path for cross-domain cold-start recommendation. In Proc. Adv. Neural Inf. Process. Syst., volume 34, pages 19223\u201319234, 2021.   \n[37] Weiming Liu, Xiaolin Zheng, Jiajie Su, Longfei Zheng, Chaochao Chen, and Mengling Hu. Contrastive proxy kernel stein path alignment for cross-domain cold-start recommendation. IEEE Trans. Knowl. Data Eng., 35(11):11216\u201311230, 2023. doi: 10.1109/TKDE.2022.3233789.   \n[38] Yixin Liu, Thalaiyasingam Ajanthan, Hisham Husain, and Vu Nguyen. Self-supervision improves diffusion models for tabular data imputation. In Proc. ACM Int. Conf. Inf. Knowl. Manag., pages 1\u201310, 2024.   \n[39] Pierre-Alexandre Mattei and Jes Frellsen. MIWAE: Deep generative modelling and imputation of incomplete data sets. In Proc. Int. Conf. Mach. Learn., pages 4413\u20134423, 2019.   \n[40] Boris Muzellec, Julie Josse, Claire Boyer, and Marco Cuturi. Missing data imputation using optimal transport. In Proc. Int. Conf. Mach. Learn., pages 7130\u20137140, 2020.   \n[41] Yidong Ouyang, Liyan Xie, Chongxuan Li, and Guang Cheng. MissDiff: Training diffusion models on tabular data with missing values. In Proc. Int. Conf. Mach. Learn. Workshop on Structured Probabilistic Inference& Generative Modeling, 2023.   \n[42] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. In Proc. Adv. Neural Inf. Process. Syst., volume 32, pages 1\u201312, 2019.   \n[43] Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv preprint arXiv:1710.05941, pages 1\u201313, 2017.   \n[44] Donald B Rubin. Inference and missing data. Biometrika, 63(3):581\u2013592, 1976.   \n[45] Walter Rudin et al. Principles of mathematical analysis, volume 3. McGraw-hill New York, 1964.   \n[46] Filippo Santambrogio. {Euclidean, Metric, and Wasserstein} gradient flows: an overview. Bull. Math. Sci., 7:87\u2013154, 2017.   \n[47] Simo S\u00e4rkk\u00e4 and Arno Solin. Applied stochastic differential equations, volume 10. Cambridge University Press, 2019.   \n[48] Jiaxin Shi, Chang Liu, and Lester Mackey. Sampling with Mirrored Stein Operators. Proc. Int. Conf. Learn. Represent., pages 1\u201326, 2022.   \n[49] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach to density and score estimation. In Proc. Conf. Uncertainty in Artificial Intelligence, pages 574\u2013584, 2020.   \n[50] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In Proc. Int. Conf. Learn. Represent., pages 1\u201336, 2020.   \n[51] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. CSDI: Conditional score-based diffusion models for probabilistic time series imputation. Proc. Adv. Neural Inf. Process. Syst., pages 24804\u201324816, 2021.   \n[52] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural Comput., 23(7):1661\u20131674, 2011.   \n[53] Dilin Wang and Qiang Liu. Nonlinear stein variational gradient descent for learning diversified mixture models. In Proc. Int. Conf. Mach. Learn., pages 6576\u20136585, 2019.   \n[54] Fangyikang Wang, Hubery Yin, Yuejiang Dong, Huminhao Zhu, Chao Zhang, Hanbin Zhao, Hui Qian, and Chen Li. BELM: Bidirectional explicit linear multi-step sampler for exact inversion in diffusion models. In Proc. Adv. Neural Inf. Process. Syst., pages 1\u201333, 2024.   \n[55] Fangyikang Wang, Huminhao Zhu, Chao Zhang, Hanbin Zhao, and Hui Qian. GAD-PVI: A general accelerated dynamic-weight particle-based variational inference framework. In Proc. AAAI Conf. Artif. Intell., pages 15466\u201315473, 2024.   \n[56] Hao Wang, Jiajun Fan, Zhichao Chen, Haoxuan Li, Weiming Liu, Tianqiao Liu, Quanyu Dai, Yichao Wang, Zhenhua Dong, and Ruiming Tang. Optimal transport for treatment effect estimation. In Proc. Adv. Neural Inf. Process. Syst., pages 5404\u20135418, 2023.   \n[57] Hao Wang, Zhichao Chen, Zhaoran Liu, Haozhe Li, Degui Yang, Xinggao Liu, and Haoxuan Li. Entire space counterfactual learning for reliable content recommendations. IEEE Trans. Inf. Forensics Secur., pages 1\u20131, 2024.   \n[58] Hao Wang, Zhichao Chen, Zhaoran Liu, Licheng Pan, Hu Xu, Yilin Liao, Haozhe Li, and Xinggao Liu. SPOT-I: Similarity preserved optimal transport for industrial iot data imputation. IEEE Trans. Ind. Inform., pages 1\u20139, 2024. doi: 10.1109/TII.2024.3452241.   \n[59] Jun Wang, Wenjie Du, Wei Cao, Keli Zhang, Wenjia Wang, Yuxuan Liang, and Qingsong Wen. Deep learning for multivariate time series imputation: A survey. arXiv preprint arXiv:2402.04059, pages 1\u20139, 2024.   \n[60] Xu Wang, Hongbo Zhang, Pengkun Wang, Yudong Zhang, Binwu Wang, Zhengyang Zhou, and Yang Wang. An observed value consistent diffusion model for imputing missing values in multivariate time series. In Proc. ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, pages 2409\u20132418, 2023.   \n[61] Yifei Wang and Wuchen Li. Accelerated information gradient flow. J. Sci. Comput., 90:1\u201347, 2022.   \n[62] Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proc. Int. Conf. Mach. Learn., pages 681\u2013688. Citeseer, 2011.   \n[63] Chen Xu, Xiuyuan Cheng, and Yao Xie. Normalizing flow neural networks by JKO scheme. In Proc. Adv. Neural Inf. Process. Syst., pages 47379\u201347405, 2023.   \n[64] Jingwen Xu, Fei Lyu, and Pong C Yuen. Density-aware temporal attentive step-wise diffusion model for medical time series imputation. In Proc. ACM Int. Conf. Inf. Knowl. Manag., pages 2836\u20132845, 2023.   \n[65] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and applications. ACM Comput. Surv., 56(4):1\u201339, 2023.   \n[66] Yiyuan Yang, Ming Jin, Haomin Wen, Chaoli Zhang, Yuxuan Liang, Lintao Ma, Yi Wang, Chenghao Liu, Bin Yang, Zenglin Xu, et al. A survey on diffusion models for time series and spatio-temporal data. arXiv preprint arXiv:2404.18886, pages 1\u201327, 2024.   \n[67] Jinsung Yoon, James Jordon, and Mihaela Schaar. GAIN: Missing data imputation using generative adversarial nets. In Proc. Int. Conf. Mach. Learn., pages 5689\u20135698. PMLR, 2018.   \n[68] Shipeng Yu, Kai Yu, Volker Tresp, Hans-Peter Kriegel, and Mingrui Wu. Supervised probabilistic principal component analysis. In Proc. ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, pages 464\u2013473, 2006.   \n[69] Chao Zhang, Zhijian Li, Xin Du, and Hui Qian. Dpvi: A dynamic-weight particle-based variational inference framework. In Proc. Int. Joint Conf. Artif. Intell., pages 4900\u20134906, 2022.   \n[70] Cheng Zhang, Judith B\u00fctepage, Hedvig Kjellstr\u00f6m, and Stephan Mandt. Advances in variational inference. IEEE Trans. Pattern Anal. Mach. Intell., 41(8):2008\u20132026, 2019. doi: 10.1109/ TPAMI.2018.2889774.   \n[71] Yulong Zhang, Shuhao Chen, Weisen Jiang, Yu Zhang, Jiangang Lu, and James T Kwok. Domain-guided conditional diffusion model for unsupervised domain adaptation. arXiv preprint arXiv:2309.14360, pages 1\u201313, 2023.   \n[72] He Zhao, Ke Sun, Amir Dezfouli, and Edwin V Bonilla. Transformed distribution matching for missing value imputation. In Proc. Int. Conf. Mach. Learn., pages 42159\u201342186, 2023.   \n[73] Shuhan Zheng and Nontawat Charoenphakdee. Diffusion models for missing value imputation in tabular data. In Proc. Adv. Neural Inf. Process. Syst. Workshop on First Table Representation, 2022.   \n[74] Huminhao Zhu, Fangyikang Wang, Chao Zhang, Hanbin Zhao, and Hui Qian. Neural sinkhorn gradient flow. arXiv preprint arXiv:2401.14069, pages 1\u201317, 2024.   \n[75] Zhan Zhuang, Yu Zhang, and Ying Wei. Gradual domain adaptation via gradient flow. In Proc. Int. Conf. Learn. Represent., pages 1\u201327, 2024.   \n[76] Zhan Zhuang, Yulong Zhang, Xuehao Wang, Jiangang Lu, Ying Wei, and Yu Zhang. TimeVarying LoRA: Towards effective cross-domain fine-tuning of diffusion models. In Proc. Adv. Neural Inf. Process. Syst., pages 1\u201325, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix Contents ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Detailed Preliminaries of Wasserstein Gradient Flow 17 ", "page_idx": 15}, {"type": "text", "text": "B Detailed Information for Toy Cases in Section 3.1 18 ", "page_idx": 15}, {"type": "text", "text": "C Theoretical Analysis 19   \nC.1 Implementation Difficulty of Velocity Field 19   \nC.2 Proof & Discussions of Concerning Propositions and Corollaries . 19   \nD Detailed Explanation for the Workflow of NewImp Approach 28   \nD.1 Forward Euler\u2019s Method for ODE Simulation 28   \nD.2 Detailed Information for DSM 28   \nE Detailed Information for Experiments 29   \nE.1 Background & Simulation of Missing Data 29   \nE.2 Hyperparameter Setting of Baseline Models 30   \nF Additional Empirical Evidence 31   \nF.1 Toy Case Experiments 31   \nF.2 Additional Experimental Results with MNAR Scenario 31   \nF.3 Empirical Evidence for Selecting RBF Function 34   \nF.4 Time Complexity Analysis 35   \nF.5 Convergence Analysis 36   \nF.6 Downstream Task Comparison 42   \nF.7 Baseline Comparison Vary Different Missing Rates and Scenarios 43   \nG Limitations & Future Directions and Broader Impact 43   \nG.1 Limitations & Future Directions 43   \nG.2 Broader Impact Statement 44 ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Appendix A Detailed Preliminaries of Wasserstein Gradient Flow ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we want to introduce the WGF framework and its application scenarios to better understand this paper. Before introduction, the following concepts are listed to better understand the WGF framework: ", "page_idx": 16}, {"type": "text", "text": "1. Wasserstein Metric: Let $\\mathcal{P}_{2}(\\mathbb{R}^{\\mathrm{D}})$ represent the space of probability measures on $\\mathbb{R}^{\\mathrm{D}}$ that possess finite second moments. Formally, this is expressed as $\\begin{array}{r}{\\mathcal{P}_{2}(\\dot{\\mathbb{R}}^{\\mathrm{D}})=\\{\\dot{\\mu}\\in\\mathcal{M}(\\mathbb{R}^{\\mathrm{D}})\\mid\\int\\|x\\|^{2}\\dot{\\mathrm{d}\\mu}(x)<}\\end{array}$ ${\\infty}\\}$ , where $\\mathcal{M}(\\mathbb{R}^{\\mathrm{D}})$ denotes the set of all probability measures on $\\mathbb{R}^{\\mathrm{D}}$ . Considering any two probability measures $\\mu,\\nu\\in\\mathcal P_{2}(\\mathbb{R}^{\\mathrm{D}})$ , we define the Wasserstein- $\\cdot p$ distance between $\\mu$ and $\\nu$ as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{W}_{p}=\\left\\{\\underset{\\pi\\in\\Gamma(\\mu,\\nu)}{\\operatorname*{inf}}\\int_{\\mathbb{R}^{\\mathrm{D}}\\times\\mathbb{R}^{\\mathrm{D}}}\\|x-y\\|^{p}\\mathrm{d}\\pi(x,y)\\right\\}^{\\frac{1}{p}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here, $\\Gamma(\\mu,\\nu)$ represents the collection of all joint distributions (couplings) between $\\mu$ and $\\nu$ For every joint distribution $\\pi~\\in~\\Gamma(\\mu,\\nu)$ , it holds that $\\begin{array}{r}{\\mu(x)\\;=\\;\\int_{\\mathbb{R}^{\\mathrm{D}}}\\pi(\\bar{x},y)\\,\\mathrm{d}y}\\end{array}$ and $\\nu(y)\\;=$ $\\textstyle\\int_{\\mathbb{R}^{\\mathrm{D}}}\\pi(x,y)\\,\\mathrm{d}x$ . The integral on the right-hand side encapsulates the transportation cost in the optimal transport (OT) problem, framed by Kantorovich\u2019s formulation, where $\\pi^{*}$ denotes the optimal transportation plan. ", "page_idx": 16}, {"type": "text", "text": "Furthermore, leveraging Jensen\u2019s inequality facilitates demonstrating the monotonicity of the Wasserstein- $\\boldsymbol{p}$ distance, affirming that for $1\\,\\leq\\,p\\,\\leq\\,q$ , the relationship $\\mathcal{W}_{p}(\\mu,\\nu)\\,\\leq\\,\\dot{\\mathcal{W}}_{q}(\\mu,\\nu)$ invariably holds. Building on this principle, we can articulate the inner product within the measurable space $(\\mathcal{P}_{2}(\\mathbb{R}^{\\mathrm{D}}{\\},\\mathcal{W})$ as delineated below: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\langle\\mu,\\nu\\rangle_{\\mu_{\\tau}}=\\int_{\\mathbb{R}^{\\mathrm{D}}}\\langle\\mu,\\nu\\rangle_{\\mathbb{R}^{\\mathrm{D}}}\\mathrm{d}\\mu_{\\tau}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "2. Gradient Flow in Wasserstein Space: Consider a functional $\\mathcal{F}$ associated with $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{\\mathrm{D}})$ . Our objective is to identify the optimal $\\mu$ that minimizes $\\mathcal{F}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mu\\in{\\mathcal{P}}_{2}(\\mathbb{R}^{\\mathrm{D}})}{\\mathcal{F}}(\\mu)+\\mathrm{const.}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To facilitate the decrease of $\\mathcal F(\\mu)$ , we introduce a velocity field $u_{\\mu}:\\mathbb{R}^{\\mathrm{D}}\\rightarrow\\mathbb{R}^{\\mathrm{D}}$ designed to expedite the reduction of ${\\mathcal{F}}(\\mu)$ as $\\mu$ evolves under this field. Utilizing the chain rule yields: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathcal{F}(\\mu)}{\\mathrm{d}\\tau}=\\int\\left\\langle\\nabla\\frac{\\delta\\mathcal{F}(\\mu)}{\\delta\\mu},u_{\\mu}\\right\\rangle\\,\\mathrm{d}\\mu,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where \u03b4 represents the first variation operator. To ensure the decrease of F(\u00b5), i.e., dFd(\u03c4\u00b5) \u22640, the velocity field is defined as: ", "page_idx": 16}, {"type": "equation", "text": "$$\nu_{\\mu}=-\\nabla\\frac{\\delta\\mathcal{F}(\\mu)}{\\delta\\mu}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The decline of ${\\mathcal{F}}(\\mu)$ aligns with the following partial differential equation (PDE) called the continuity equation: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mu}{\\partial\\tau}=-\\nabla\\cdot(\\mu u_{\\mu}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence, the continuity equation Eq. (A.6), coupled with the velocity field articulated in Eq. (A.5), is recognized as the Wasserstein Gradient Flow, delineating the steepest descent direction of cost functional ${\\mathcal{F}}(\\mu)$ in the Wasserstein space. ", "page_idx": 16}, {"type": "text", "text": "3. Simulation of WGF & Sampling: There are primarily two discretization techniques for the WGF: the forward scheme and the backward scheme. ", "page_idx": 16}, {"type": "text", "text": "\u2022 Forward Scheme: The forward scheme applies gradient descent within the Wasserstein space to identify the direction of the steepest descent. For an energy functional $\\mathcal F(\\mu)$ with a specified step size $\\eta$ , the update rule in the forward scheme is formulated as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mu_{\\tau+1}=(\\mathrm{Id}-\\nabla\\frac{\\delta\\mathcal{F}(\\mu)}{\\delta\\mu})_{\\#}\\mu_{\\tau},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "facilitating an intuitive and direct update mechanism that emulates the gradient flow in the Euclidean space but transposed into the Wasserstein space. ", "page_idx": 16}, {"type": "text", "text": "\u2022 Backward Scheme: Conversely, the backward scheme, often referred to as the JordanKinderlehrer-Otto scheme [23], represents a more implicit discretization approach. It defines the subsequent distribution $\\mu_{\\tau+1}$ by solving an optimization problem that balances the energy decrease and the transportation cost. This scheme is mathematically denoted as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mu_{\\tau+1}=\\operatorname*{arg\\,min}_{\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{\\mathrm{D}})}\\mathcal{F}(\\mu)+\\frac{1}{2\\eta}\\mathcal{W}_{2}^{2}(\\mu,\\mu_{\\tau}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "thereby integrating the energy minimization and transport efficiency into a single variational problem that reflects the inherent structure of the Wasserstein space. ", "page_idx": 17}, {"type": "text", "text": "These schemes provide distinct yet complementary approaches to discretizing the dynamics defined by WGFs, offering different perspectives and tools for the analysis and computation of these flows. ", "page_idx": 17}, {"type": "text", "text": "Leveraging the WGF framework, if we designate the functional $\\mathcal{F}$ to be the KL divergence, it yields a particular formulation for the velocity field. ", "page_idx": 17}, {"type": "equation", "text": "$$\nu_{\\mu}=-\\nabla\\frac{\\delta\\mathbb{D}_{\\mathrm{KL}}(\\mu\\Vert p)}{\\delta\\mu}=\\nabla\\log p-\\nabla\\log\\mu.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "On this basis, plug Eq. (A.9) into Eq. (A.6), we can get the following PDE: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mu}{\\partial\\tau}=-\\nabla\\cdot(\\mu\\nabla\\log{p})+\\nabla\\cdot\\nabla\\mu.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "According to Theorem 5.4 in reference [47], denote the random sample from distribution $p$ as $x$ , we can obtain the following stochastic differential equation (SDE) called Langevin equation [62] for implementing this gradient flow easily: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{d}x=\\nabla\\mathrm{log}\\,p(x)\\mathrm{d}\\tau+\\sqrt{2}\\mathrm{d}W_{\\tau},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathrm{d}W_{\\tau}$ is the standard Wiener process (also known as Brownian motion). ", "page_idx": 17}, {"type": "text", "text": "Appendix B Detailed Information for Toy Cases in Section 3.1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To investigate what would happen if we directly applied diffusion models to MDI tasks, we consider the following optimization problem: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\underset{a_{h}\\in\\Delta^{2}}{\\arg\\operatorname*{max}}\\sum_{h=1}^{\\mathrm{H}}\\left\\{\\log\\frac{\\Gamma\\left(\\sum_{k=1}^{3}\\rho_{k}\\right)}{\\prod_{k=1}^{3}\\Gamma(\\rho_{k})}+\\sum_{k=1}^{3}\\left(\\rho_{k}-1\\right)\\log a_{k,h}\\right\\},\\mathrm{H}=8,\\rho_{k}|_{k=1}^{3}=[2.5,2.5,5.0],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which corresponds to the density function of a Dirichlet distribution, $\\operatorname{Dir}([2.5,2.5,5.0])$ , where $\\mathbf{\\deltaa}_{h}$ lies on the three-dimensional standard simplex $\\Delta^{2}$ . The optimal value of ${\\mathcal{F}}_{\\mathrm{Dir}}$ is given by: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left[\\frac{\\rho_{1}-1}{\\sum_{k=1}^{3}\\rho_{k}-1},\\frac{\\rho_{2}-1}{\\sum_{k=1}^{3}\\rho_{k}-1},\\frac{\\rho_{3}-1}{\\sum_{k=1}^{3}\\rho_{k}-1}\\right]\\approx\\left[0.214,0.214,0.571\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To optimize this cost functional, we employ the Langevin equation as presented in Eq. (A.11): ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbfit{a}_{h}=\\nabla_{\\mathbf{a}_{h}}\\mathcal{F}_{\\mathrm{Dir}}\\mathrm{d}\\tau+\\sqrt{2}\\mathrm{d}W_{\\tau},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and compare the results to Eq. (B.2) to evaluate effectiveness. Additionally, since the support is on a three-dimensional standard simplex $\\Delta^{2}$ , to ensure the well-definedness of our approach, we use the mirror descent technique [48, 20], where the Bregman function is defined as $\\psi(\\pmb{a}_{h})\\ =$ $\\begin{array}{r}{\\sum_{k=1}^{3}a_{h,k}\\log\\pmb{a}_{h,k}-\\pmb{a}_{h,k}}\\end{array}$ . Moreover, the optimization of $a_{h,k}|_{h=1}^{\\mathrm{H}}|_{k=1}^{3}$ is conducted by simulating the Langevin equation, which is discretized by the Euler-Maruyama method as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\!\\!\\hat{a}_{h}^{\\tau+1}}&{=a_{h}^{\\tau}\\times\\exp\\left(\\eta\\nabla_{a_{h}}\\mathcal{F}_{\\mathrm{Dir}}|_{a_{h}=a_{h}^{\\tau}}\\times\\eta\\sqrt{2}\\epsilon\\right),\\epsilon\\sim\\mathcal{N}(0,I_{3\\times3})}\\\\ {\\!\\!\\left.a_{h,k}^{\\tau+1}}&{=\\frac{\\hat{a}_{h,k}^{\\tau+1}}{\\sum_{k=1}^{3}\\hat{a}_{h,k}^{\\tau+1}}\\!\\!\\right.}\\end{array}.\\!\\!\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "With the step size $\\eta$ set to $5.0\\times10^{-3}$ , we repeatedly execute Eq. (B.4) 100 times, culminating in the results depicted in Fig. 1 (b). ", "page_idx": 17}, {"type": "text", "text": "Appendix C Theoretical Analysis ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1 Implementation Difficulty of Velocity Field ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To the best of our knowledge, the difficulty of implementing the velocity field can be given from two perspectives, namely ODE-based implementation and SDE-based implementation. In this section, we want to discuss these two implementation approaches in detail. ", "page_idx": 18}, {"type": "text", "text": "ODE-based Implementation: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. WGF framework: According to the continuity equation, we can obtain the following velocity field: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}X^{\\mathrm{(miss)}}}{\\mathrm{d}\\tau}\\overset{\\mathrm{(i)}}{=}u(X^{\\mathrm{(miss)}})\\overset{\\mathrm{(ii)}}{=}-[\\nabla_{X^{\\mathrm{(miss)}}}\\log\\hat{p}(X^{\\mathrm{(miss)}}|X^{\\mathrm{(obs)}})+\\lambda\\nabla_{X^{\\mathrm{(miss)}}}\\log r(X^{\\mathrm{(miss)}})],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where (i) is based on Section 2.3, and (ii) is based on Section 4.1. The expression of the velocity field involves the computation of density term $r(X^{\\mathrm{(miss)}})$ [31, 9], which is intractable during practice as we stated in Section 2.3. Based on this, we conclude that implementing this velocity field within the WGF framework is difficult. ", "page_idx": 18}, {"type": "text", "text": "2. Probability flow ODE: According to reference [50], if we directly plug Eq. (8) into the FPK equation, we can get the following PDE: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial r}{\\partial\\tau}(X^{(\\mathrm{mis})})}\\\\ &{=-\\nabla\\cdot\\left(u(X^{(\\mathrm{mis})})r(X^{(\\mathrm{mis})})\\right)}\\\\ &{=-\\left[\\nabla_{X^{(\\mathrm{mis})}}\\log\\hat{p}(X^{(\\mathrm{mis})}|X^{(\\mathrm{mis})})\\right]-\\lambda\\nabla\\cdot\\nabla r\\left(X^{(\\mathrm{mis})}\\right)}\\\\ &{\\,\\,\\,-\\left[\\nabla_{X^{(\\mathrm{mis})}}\\log\\hat{p}(X^{(\\mathrm{mis})}|X^{(\\mathrm{mis})})r(X^{(\\mathrm{mis})})\\right]-\\lambda\\nabla\\cdot\\nabla r\\left(X^{(\\mathrm{mis})}\\right)}\\\\ &{\\,\\,\\,-\\left[\\nabla_{X^{(\\mathrm{mis})}}\\log\\hat{p}(X^{(\\mathrm{mis})}|X^{(\\mathrm{mis})})r(X^{(\\mathrm{mis})})\\right]-\\lambda\\nabla\\cdot\\nabla r(X^{(\\mathrm{mis})})}\\\\ &{=}\\\\ &{-\\frac{1}{2}\\sigma_{\\tau}^{2}\\nabla\\cdot\\nabla r(X^{(\\mathrm{mis})})-\\frac{1}{2}\\sigma_{\\tau}^{2}\\nabla\\cdot\\nabla r(X^{(\\mathrm{mis})})-\\frac{1}{2}\\sigma_{\\tau}^{2}\\nabla\\cdot\\nabla r(X^{(\\mathrm{mis})})}\\\\ &{=}\\\\ &{-\\left\\{\\left[\\nabla_{X^{(\\mathrm{mis})}}\\log\\hat{p}(X^{(\\mathrm{mis})}|X^{(\\mathrm{mis})})+(\\lambda+\\frac{1}{2}\\sigma_{\\tau}^{2})\\nabla\\log r(X^{(\\mathrm{mis})})\\right]r(X^{(\\mathrm{mis})})\\right\\}}\\\\ &{\\,\\,\\,+\\frac{1}{2}\\sigma_{\\tau}^{2}\\nabla\\cdot\\nabla r(X^{(\\mathrm{mis})}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "When we set $\\sigma_{\\tau}$ as 0, we can find that the corresponding ODE is Eq. (C.1), where we are obliged to compute the intractable density $r(X^{\\mathrm{(miss)}})$ . ", "page_idx": 18}, {"type": "text", "text": "SDE-based Implementation: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "If we plug Section 4.1 into the FPK equation, the corresponding PDE can be given as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\cfrac{\\partial{r(X^{(\\mathrm{miss})})}}{\\partial{\\tau}}}\\\\ &{=-\\nabla\\cdot\\left(u(X^{(\\mathrm{miss})})r(X^{(\\mathrm{miss})})\\right)}\\\\ &{=-\\left[\\nabla_{X^{(\\mathrm{miss})}}\\log\\hat{p}(X^{(\\mathrm{miss})}|X^{(\\mathrm{obs})})r(X^{(\\mathrm{miss})})\\right]-\\lambda\\nabla\\cdot\\nabla{r(X^{(\\mathrm{miss})})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the coefficient before the Laplacian operator $\\nabla\\cdot\\nabla$ is $-1$ . To the best of our knowledge, this structure makes deriving a corresponding SDE impossible by current approaches. ", "page_idx": 18}, {"type": "text", "text": "C.2 Proof & Discussions of Concerning Propositions and Corollaries ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proposition (3.1). Within WGF framework, DM-based MDI approaches can be viewed as finding the imputed values $X^{(i m p)}$ that maximize the following objective: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{r(X^{(m i s s)})}{\\arg\\operatorname*{max}}}&{{}\\mathbb{E}_{r(X^{(m i s s)})}[\\log\\hat{p}(X^{(m i s s)}|X^{(o b s)})]+\\psi(X^{(m i s s)})+c o n s t,}\\\\ {\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where \u2018const\u2019 is the abbreviation of constant, and $\\psi(X^{(m i s s)})$ is a scalar function determined by the type of SDE underlying the DMs. ", "page_idx": 18}, {"type": "text", "text": "\u2022 VP-SDE: $\\begin{array}{r}{\\psi(X^{(m i s s)})=\\frac12\\mathbb{H}[r(X^{(m i s s)})]+\\mathbb{E}_{r(X^{(m i s s)})}\\{\\frac{1}{4}[X^{(m i s s)}]^{\\top}[X^{(m i s s)}]\\}\\geq0}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "\u2022 VE-SDE: $\\begin{array}{r}{\\psi(X^{(m i s s)})=\\frac{1}{2}\\mathbb{H}[r(X^{(m i s s)})]\\geq0}\\end{array}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\psi({X}^{(m i s s)})=\\frac12\\mathbb{H}[r({X}^{(m i s s)})]+\\mathbb{E}_{r({X}^{(m i s s)})}\\{\\frac{1}{4\\gamma_{\\tau}}[X^{(m i s s)}]^{\\top}[X^{(m i s s)}]\\}\\ge0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where H $\\begin{array}{r}{\\cdot[r(X^{(m i s s)})]:=-\\int r(X^{(m i s s)})\\log r(X^{(m i s s)})\\mathrm{d}X^{(m i s s)}}\\end{array}$ is the entropy term, $\\gamma_{\\tau}$ is determined by noise scale $\\beta_{\\tau}$ $\\begin{array}{r}{\\tau\\colon\\gamma_{\\tau}:=\\left(1-\\exp(-2\\int_{0}^{\\tau}\\beta_{s}\\mathrm{d}s)\\right)>0,0<\\beta_{1}<\\cdots<\\beta_{\\mathrm{T}}<1.}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "Proof. Since there are various approaches for reversing the sampling procedure of DMs, for simplicity, as we emphasized in Section 3.2, we mainly consider the VP-SDE, VE-SDE, and sub-VP-SDE as analysis objects in this paper. ", "page_idx": 19}, {"type": "text", "text": "\u2022 VP-SDE: According to reference [50], the density evolution of the generative process for VP-SDE can be delineated by the following PDE: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial r(X^{(\\mathrm{miss})})}{\\partial\\tau}=-\\nabla_{X^{(\\mathrm{miss})}}\\cdot\\bigg\\{r(X^{(\\mathrm{miss})})\\left[\\beta_{\\tau}\\right]\\left[\\frac{1}{2}X^{(\\mathrm{miss})}+\\nabla_{X^{(\\mathrm{miss})}}\\log\\hat{p}(X^{(\\mathrm{miss})}|X^{(\\mathrm{obs})})\\right]\\bigg\\}}\\\\ &{\\qquad\\qquad+\\frac{\\beta_{\\tau}}{2}\\nabla_{X^{(\\mathrm{miss})}}\\cdot\\nabla_{X^{(\\mathrm{miss})}}r(X^{(\\mathrm{miss})})}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\beta_{\\tau}\\in(0,1)$ is the time-varying noise scale. On this basis, according to [24], by changing the variable as $\\begin{array}{r}{\\mathrm{d}\\tau:=\\frac{\\beta_{\\tau}}{2}\\mathrm{d}\\tau}\\end{array}$ , we can get the following equation: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\partial r(\\boldsymbol{X}^{\\mathrm{(miss)}})}{\\partial\\tau}=-\\nabla_{\\boldsymbol{X}^{\\mathrm{(mis)}}}\\cdot\\left\\{\\boldsymbol{r}(\\boldsymbol{X}^{\\mathrm{(miss)}})[\\frac{1}{2}\\boldsymbol{X}^{\\mathrm{(miss)}}+\\nabla_{\\boldsymbol{X}^{\\mathrm{(miss)}}}\\log\\hat{\\boldsymbol{p}}(\\boldsymbol{X}^{\\mathrm{(miss)}}|\\boldsymbol{X}^{\\mathrm{(obs)}})]\\right.}\\\\ {-\\left.\\frac{1}{2}\\nabla_{\\boldsymbol{X}^{\\mathrm{(miss)}}}\\log r(\\boldsymbol{X}^{\\mathrm{(miss)}})\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Comparing Eq. (C.6) with Eqs. (A.5) and (A.6), the cost functional to be minimized of this simulation procedure can be given as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{F}_{\\mathrm{VP-SDE}}=-\\displaystyle\\int r(X^{(\\mathrm{miss})})\\left\\{\\frac{1}{4}[X^{(\\mathrm{miss})}]^{\\top}[X^{(\\mathrm{miss})}]+\\log\\hat{p}(X^{(\\mathrm{miss})}|X^{(\\mathrm{obs})})\\right\\}\\mathrm{d}X^{(\\mathrm{miss})}}\\\\ &{\\phantom{\\int\\quad\\quad\\quad\\times}-\\displaystyle\\frac{1}{2}\\log r(X^{(\\mathrm{miss})})+\\mathrm{const}}\\\\ &{\\phantom{\\int\\quad\\quad\\quad\\times}=-\\mathbb{E}_{r(X^{(\\mathrm{miss})})}\\left\\{\\frac{1}{4}[X^{(\\mathrm{miss})}]^{\\top}[X^{(\\mathrm{miss})}]+\\log\\hat{p}(X^{(\\mathrm{miss})}|X^{(\\mathrm{obs})})\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that $\\begin{array}{r}{\\frac{1}{4}[X^{\\mathrm{(miss)}}]^{\\top}[X^{\\mathrm{(miss)}}]\\geq0}\\end{array}$ and $\\begin{array}{r}{-\\frac{1}{2}\\int r(X^{(\\mathrm{miss})})\\log r(X^{(\\mathrm{miss})})\\mathrm{d}X^{(\\mathrm{miss})}\\geq0}\\end{array}$ hold, and thus the proposition for VP-SDE is proved by taking the negative of the abovementioned equation. ", "page_idx": 19}, {"type": "text", "text": "\u2022 VE-SDE: Similarly, based on reference [50], the following PDE can be given to delineate the density evolution of the generative process for VE-SDE: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial r({\\boldsymbol X}^{(\\mathrm{miss})})}{\\partial\\tau}=-\\nabla_{{\\boldsymbol X}^{(\\mathrm{miss})}}\\cdot\\left\\{r({\\boldsymbol X}^{(\\mathrm{miss})})\\left[-\\frac{\\mathrm{d}\\sigma_{\\tau}^{2}}{\\mathrm{d}\\tau}\\right]\\nabla_{{\\boldsymbol X}^{(\\mathrm{miss})}}\\log\\hat{p}({\\boldsymbol X}^{(\\mathrm{miss})}|{\\boldsymbol X}^{(\\mathrm{obs})})\\right\\}}\\\\ &{\\displaystyle\\qquad\\qquad+\\,\\frac{1}{2}\\frac{\\mathrm{d}\\sigma_{\\tau}^{2}}{\\mathrm{d}\\tau}\\nabla_{{\\boldsymbol X}^{(\\mathrm{miss})}}\\cdot\\nabla_{{\\boldsymbol X}^{(\\mathrm{miss})}}r({\\boldsymbol X}^{(\\mathrm{miss})}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\sigma_{\\tau}^{2}$ is a time varying noise scale. ", "page_idx": 19}, {"type": "text", "text": "As such, by chaning the variable as $\\mathrm{d}\\tau\\,:=\\,\\left[\\frac{\\mathrm{d}\\sigma_{\\tau}^{2}}{\\mathrm{d}\\tau}\\right]\\mathrm{d}\\tau$ [24], Eq. (C.8) can be reformulated as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\partial r({\\boldsymbol X}^{(\\mathrm{miss})})}{\\partial\\tau}=-\\nabla_{\\boldsymbol X^{(\\mathrm{miss})}}\\cdot\\biggl\\{r({\\boldsymbol X}^{(\\mathrm{miss})})\\left[\\nabla_{\\boldsymbol X^{(\\mathrm{miss})}}\\log\\hat{p}(\\boldsymbol X^{(\\mathrm{miss})}|\\boldsymbol X^{(\\mathrm{obs})})-\\frac{1}{2}\\nabla_{\\boldsymbol X^{(\\mathrm{miss})}}\\log r(\\boldsymbol X^{(\\mathrm{miss})})\\right]\\biggr\\}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Comparing Eq. (C.9) with Eqs. (A.5) and (A.6), the cost functional to be minimized of this simulation procedure can be given as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{F}_{\\mathrm{VE-SDE}}=\\displaystyle\\int r(X^{(\\mathrm{miss})})\\left\\{\\frac{1}{2}\\log r(X^{(\\mathrm{miss})})-\\log\\hat{p}(X^{(\\mathrm{miss})}|X^{(\\mathrm{obs})})+\\mathrm{const}\\right\\}\\mathrm{d}X^{(\\mathrm{miss})}}\\\\ &{\\qquad\\quad=-\\mathbb{E}_{r(X^{(\\mathrm{miss})})}\\left\\{-\\frac{1}{2}\\log r(X^{(\\mathrm{miss})})+\\log\\hat{p}(X^{(\\mathrm{miss})}|X^{(\\mathrm{obs})})+\\mathrm{const}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that the entropy function $-\\textstyle\\frac{1}{2}\\int r(X^{(\\mathrm{miss})})\\log r(X^{(\\mathrm{miss})})\\mathrm{d}X^{(\\mathrm{miss})}\\,\\geq\\,0$ holds, and thus the proposition for VE-SDE is proved by taking the negative of the abovementioned equation. ", "page_idx": 20}, {"type": "text", "text": "\u2022 sub-VP-SDE: Based on reference [50], the following PDE can be given to delineate the density evolution of the generative process for sub-VP-SDE: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\partial r(X^{(\\mathrm{miss})})}{\\partial\\tau}=-\\nabla_{X^{(\\mathrm{miss})}}\\cdot\\left\\{r(X^{(\\mathrm{miss})})\\left[\\beta_{\\tau}\\right]\\left[\\frac{1}{2}X^{(\\mathrm{miss})}+\\gamma_{\\tau}\\nabla_{X^{(\\mathrm{miss})}}\\log\\hat{p}(X^{(\\mathrm{miss})}|X^{(\\mathrm{obs})})\\right]\\right\\}}}\\\\ &{}&{+\\frac{\\beta_{\\tau}}{2}\\gamma_{\\tau}\\nabla_{X^{(\\mathrm{miss})}}\\cdot\\nabla_{X^{(\\mathrm{miss})}}r(X^{(\\mathrm{miss})}),}\\\\ &{}&{\\quad\\cdots}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\begin{array}{r}{\\gamma_{\\tau}:=(1-\\exp(-2\\int_{0}^{\\tau}\\beta_{s}\\mathrm{d}s))>0}\\end{array}$ . On this basis, by chaning the variable as $\\begin{array}{r}{\\mathrm{d}\\tau:=\\frac{\\beta_{\\tau}}{2}\\mathrm{d}\\tau}\\end{array}$ , we can get the following equation: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\partial r(X^{(\\mathrm{miss})})}{\\partial\\tau}=-\\nabla_{X^{(\\mathrm{mis})}}\\cdot\\left\\{\\mathbf{\\partial}^{r(X^{(\\mathrm{miss})})[\\frac{1}{2}X^{(\\mathrm{miss})}+\\gamma_{\\tau}\\nabla_{X^{(\\mathrm{miss})}}\\log\\hat{p}(X^{(\\mathrm{miss})}|X^{(\\mathrm{obs})})}\\right\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Comparing Eq. (C.12) with Eqs. (A.5) and (A.6), the cost functional to be minimized of this simulation procedure can be given as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{F}_{\\mathrm{sub\\,vib\\,SDE}}=-\\int r(X^{\\mathrm{ous}})\\left\\{\\displaystyle\\frac{1}{4}[X^{(\\mathrm{mis})}]^{\\top}[X^{(\\mathrm{mis})}]+\\gamma_{\\tau}\\log\\hat{p}(X^{(\\mathrm{mis})}|X^{(\\mathrm{obs})})\\right\\}\\mathrm{d}X^{(\\mathrm{mis})}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left.\\times\\displaystyle\\frac{\\gamma_{\\tau}}{2}\\log r(X^{(\\mathrm{mis})})+\\mathrm{const}\\right.}\\\\ &{\\qquad\\qquad=-\\mathbb{E}_{r(X^{\\mathrm{ous}})}\\left\\{\\displaystyle\\frac{1}{4}[X^{(\\mathrm{mis})}]^{\\top}[X^{(\\mathrm{mis})}]+\\gamma_{\\tau}\\log\\hat{p}(X^{(\\mathrm{mis})}|X^{(\\mathrm{obs})})\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\left.\\qquad\\qquad\\quad-\\displaystyle\\frac{\\gamma_{\\tau}}{2}\\log r(X^{(\\mathrm{mis})})+\\mathrm{const}\\right.}\\\\ &{\\qquad=-\\mathbb{E}_{r(X^{\\mathrm{ous}})}\\left\\{\\displaystyle\\frac{1}{4\\gamma_{\\tau}}[X^{(\\mathrm{mis})}]^{\\top}[X^{(\\mathrm{mis})}]+\\log\\hat{p}(X^{(\\mathrm{mis})}|X^{(\\mathrm{obs})})\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that $\\begin{array}{r}{\\frac{1}{4\\gamma_{\\tau}}[X^{(\\mathrm{miss})}]^{\\top}[X^{(\\mathrm{miss})}]\\geq0}\\end{array}$ and $-\\textstyle\\frac{1}{2}\\int r(X^{(\\mathrm{miss})})\\log r(X^{(\\mathrm{miss})})\\mathrm{d}X^{(\\mathrm{miss})}\\geq0$ hold, and thus the proposition for sub-VP-SDE is proved by taking the negative of the abovementioned equation. ", "page_idx": 20}, {"type": "text", "text": "In summary, the regularization term $\\psi(X^{\\mathrm{(miss)}})$ for VP-SDE is $\\mathbb{E}_{r(\\mathbf{\\boldsymbol{X}}^{\\mathrm{(miss)}})}\\{\\frac{1}{4}[\\mathbf{\\boldsymbol{X}}^{\\mathrm{(miss)}}]^{\\top}[\\mathbf{\\boldsymbol{X}}^{\\mathrm{(miss)}}]\\}\\,+$ $\\scriptstyle{\\frac{1}{2}}\\mathbb{H}[r(X^{\\mathrm{(miss)}})]$ , for VE-SDE is $\\begin{array}{r}{\\frac{1}{2}\\mathbb{H}(r(X^{\\mathrm{(miss)}}))}\\end{array}$ , and for sub-VP-SDE is $\\begin{array}{r}{\\mathbb{E}_{r({X^{(\\mathrm{miss})}})}\\{\\frac{1}{4\\gamma_{\\tau}}[{X^{(\\mathrm{miss})}}]^{\\top}[{X^{(\\mathrm{miss})}}]\\}+\\frac{1}{2}\\mathbb{H}[r({X^{(\\mathrm{miss})}})]}\\end{array}$ . \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Before proving Proposition 4.1, we want to introduce the following lemma to delineate the evolution of cost functional $\\mathcal{F}_{\\mathrm{NER}}$ along time $\\tau$ : ", "page_idx": 20}, {"type": "text", "text": "Lemma C.1. The evolution of ${\\mathcal{F}}_{N E R}$ along time $\\tau$ can be characterized by the following ODE, assuming that the boundary condition $\\begin{array}{r}{\\operatorname*{lim}_{X^{(m i s s)}\\to\\infty}[u(X^{(m i s s)})r(X^{(m i s s)})]=0}\\end{array}$ is satisfied: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathcal{F}_{N E R}}{\\mathrm{d}\\tau}=\\mathbb{E}_{r(X^{(m i s s)})}[u^{\\top}(X^{(m i s s)})\\nabla_{X^{(m i s s)}}\\log\\hat{p}(X^{(m i s s)}|X^{(o b s)})-\\lambda\\nabla_{X^{(m i s s)}}\\cdot u(X^{(m i s s)})].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This boundary condition is achievable, for instance, when $r(X^{(m i s s)})$ is bounded, and the limit of the velocity field as the norm of $X^{(m i s s)}$ approaches infinity is zero $(\\operatorname*{lim}_{\\|\\boldsymbol{X}^{(m i s s)}\\|\\rightarrow\\infty}u(\\boldsymbol{X}^{(m i s s)})=0)$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. Before proving this lemma, we should recognize that the evolution of $X^{(\\mathrm{miss})}$ should promise the probability density function $r(X^{(\\mathrm{miss})})$ unchanged. In other words, the following continuity equation should be satisfied during the optimization of $r(X^{\\mathrm{(miss)}})$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\partial r(\\pmb{X}^{(\\mathrm{miss})})}{\\partial\\tau}=-\\nabla_{\\pmb{X}^{(\\mathrm{miss})}}\\cdot[r(\\pmb{X}^{(\\mathrm{miss})})u(\\pmb{X}^{(\\mathrm{miss})})].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "On this basis, the evolution of $\\mathcal{F}_{\\mathrm{NER}}$ along time $\\tau$ , $\\frac{\\mathrm{d}{\\mathcal{F}}_{\\mathrm{NER}}}{\\mathrm{d}\\tau}$ , can be given as follows based on the chain rule: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\widetilde{\\mathbf{du}}^{c m}}\\\\ &{=\\int\\frac{\\partial(P^{(k m)})}{\\partial\\tau}\\left[\\log\\hat{\\rho}(X^{(m)}|X^{(m)})+\\lambda\\log r\\left(X^{(m)}\\right)+\\lambda\\right]\\mathbf{d}X^{(m)}}\\\\ &{=\\int-[(\\nabla\\mathbf{x}-\\cdot\\mathbf{\\rho})\\cdot(r(X^{(m)})|u(X^{(m)}))][\\log\\hat{\\rho}(X^{(m)}|X^{(m)})+\\lambda\\log r\\left(X^{(m)}\\right)+\\lambda]\\mathbf{d}X^{(m)}}\\\\ &{\\overset{(a)}{=}\\int[r(X^{(m)})u(X^{(m)})]^{\\top}\\nabla_{X^{(m)}}|\\log\\hat{\\rho}(X^{(m)}|X^{(m)})+\\lambda\\log r\\left(X^{(m)}\\right)+\\lambda]\\mathbf{d}X^{(m)}}\\\\ &{=\\int[r(X^{(m)})\\mathbf{u}(X^{(m)})]^{\\top}\\nabla_{X^{(m)}}|\\log\\hat{\\rho}(X^{(m)}|X^{(m)})+\\lambda\\log r\\left(X^{(m)}\\right)+\\lambda]\\mathbf{d}X^{(m)}}\\\\ &{=\\int[\\vert\\nabla(X^{(m)})\\vert\\nabla^{\\top}[\\mathbf{r}(X^{(m)})]\\nabla_{X^{(m)}}|\\log\\hat{\\rho}(X^{(m)})\\vert X^{(m)})+\\lambda\\log r\\left(X^{(m)}\\right)\\vert]\\mathbf{d}X^{(m)}}\\\\ &{=\\int\\left[[u(X^{(m)})]^{\\top}\\vert r(X^{(m)})\\nabla_{X^{(m)}}\\log\\hat{\\rho}(X^{(m)})\\vert X^{(m)})+\\lambda\\log(X^{(m)})\\nabla_{X^{(m)}}\\log r(X^{(m)})\\right]}\\\\ &{=\\int\\left[u(X^{(m)})\\right]^{\\top}\\vert r(X^{(m)})\\nabla_{X^{(m)}}\\log\\hat{\\rho}(X^{(m)})\\vert X^{(m)})+\\lambda\\nabla_{X^{(m)}}\\gamma(X^{(m)})\\vert\\mathbf{d}X^{(m)}}\\\\ &{\\overset{(a)}{=}\\int\\gamma(X^{(m)})\\vert\\mathbf{u}^{\\top}(X^{(m)})\\nabla_{X^{(m)}}\\log\\hat{\\rho}(X^{(m)})\\vert X^{(m)}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where (i) and (ii) are based on integration by parts. More specifically, when condition $\\begin{array}{r}{\\operatorname*{lim}_{X^{(\\mathrm{miss})}\\to\\infty}[u(X^{(\\mathrm{miss})})r(X^{(\\mathrm{miss})})]=0}\\end{array}$ is satisfied, for example, $r(X^{\\mathrm{(miss)}})$ is bounded, and the limit of the velocity field as the norm of $X^{(\\mathrm{miss})}$ approaches infinity is zero $(\\operatorname*{lim}_{\\|\\boldsymbol{X}^{\\mathrm{(miss)}}\\|\\rightarrow\\infty}u(\\boldsymbol{X}^{\\mathrm{(miss)}})=0)$ , we can get the following result [35, 32]: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\int\\nabla_{\\mathbf{X}^{(\\mathrm{miss})}}\\cdot[r(\\mathbf{X}^{(\\mathrm{miss})})u(X^{(\\mathrm{miss})})]\\mathrm{d}X^{(\\mathrm{miss})}=0,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the left-hand-side can be further decomposed as follows based on the integration by parts: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\int\\nabla_{X^{(\\mathrm{miss})}}\\cdot[r(X^{(\\mathrm{miss})})u(X^{(\\mathrm{miss})})]\\mathrm{d}X^{(\\mathrm{miss})}=\\int u^{\\top}(X^{(\\mathrm{miss})})\\nabla_{X^{(\\mathrm{miss})}}r(X^{(\\mathrm{miss})})\\mathrm{d}X^{(\\mathrm{miss})}}&{}\\\\ {+\\int[\\nabla_{X^{(\\mathrm{miss})}}\\cdot u(X^{(\\mathrm{miss})})]r(X^{(\\mathrm{miss})})\\mathrm{d}X^{(\\mathrm{miss})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Based on Lemma C.1, we can now start proving Proposition 4.1: ", "page_idx": 21}, {"type": "text", "text": "Proposition (4.1). Suppose $u(X^{(m i s s)})$ is a velocity field regularized by the RKHS norm under the following conditions: $^{\\,l}$ ). The kernel function satisfies: $\\begin{array}{r}{\\operatorname*{lim}_{\\|\\boldsymbol{X}^{(m i s s)}\\|\\rightarrow\\infty}K(\\boldsymbol{X}^{(m i s s)},\\tilde{\\boldsymbol{X}}^{(m i s s)})\\,=\\,0}\\end{array}$ 2). The density $r(X^{(m i s s)})$ is bounded. Then, the velocity field that minimizes the cost functional $\\mathcal{F}_{N E R}=\\mathbb{E}_{r({X^{(m i s s)}})}[\\log\\hat{p}({X^{(m i s s)}}|{X^{(o b s)}})]-\\lambda\\mathbb{H}[r({X^{(m i s s)}})]$ can be given by: ", "page_idx": 21}, {"type": "equation", "text": "$$\nu(\\mathbf{X}^{(m i s s)})=\\mathbb{E}_{r(\\tilde{X}^{(m i s)})}\\left\\{{\\begin{array}{l l}{-\\lambda\\nabla_{\\tilde{X}^{(m i s)}}K({X}^{(m i s s)},\\tilde{\\mathbf{X}}^{(m i s s)})}\\\\ {\\qquad+\\left[{\\nabla_{\\tilde{X}^{(m i s s)}}}\\log\\hat{p}(\\tilde{\\mathbf{X}}^{(m i s s)}|\\mathbf{X}^{(o b s)})\\right]^{\\top}K({X}^{(m i s s)},\\tilde{\\mathbf{X}}^{(m i s s)})}\\end{array}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the expectation term Er( X\u02dc(miss)) can be efficiently estimated using Monte Carlo approximation. ", "page_idx": 21}, {"type": "text", "text": "Proof. When the velocity is regularized by the RKHS norm, we can first reformulate Eq. (8) as follows to find the steepest direction for the sake of improving $\\mathcal{F}_{\\mathrm{NER}}$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\nu^{*}(X^{(\\mathrm{miss})})=\\operatorname*{arg\\,max}_{u(X^{(\\mathrm{miss})})\\in\\mathcal{H}}\\quad\\quad-\\,\\lambda\\nabla_{X^{(\\mathrm{miss})}}[u^{\\top}(X^{(\\mathrm{miss})})\\nabla_{X^{(\\mathrm{miss})}}\\log\\hat{p}(X^{(\\mathrm{miss})}|X^{(\\mathrm{obs})})]\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Based on this, assume we have a map function $\\phi(x)$ , the kernel function can be given as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\nK(x,y)=\\langle\\phi(x),\\phi(y)\\rangle_{\\mathcal{H}}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, the regularization term that control the magnitude of $u(X^{\\mathrm{(miss)}})$ can be given by $\\textstyle{\\frac{1}{2}}\\|u(X^{(\\mathrm{miss})})\\|_{\\mathcal{H}}^{2}$ , and the spectral decomposition of kernel function can be given as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\nK(x,y)=\\sum_{i=1}^{\\infty}\\xi_{i}\\phi_{i}(x)\\phi_{i}(y),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\phi_{i}(\\cdot)$ indicates the orthonormal basis and $\\xi_{i}$ is the corresponding eigen-value. For any function $u(\\pmb{X}^{\\mathrm{(miss)}})\\in\\mathcal{H}$ , the following decomposition is given: ", "page_idx": 22}, {"type": "equation", "text": "$$\nu(X^{\\mathrm{(miss)}})=\\sum_{i=1}^{\\infty}u_{i}\\sqrt{\\xi_{i}}\\phi_{i}(X^{\\mathrm{(miss)}}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $u_{i}$ and $\\textstyle\\sum_{i=1}^{\\infty}\\|u_{i}\\|^{2}<\\infty$ . ", "page_idx": 22}, {"type": "text", "text": "The learning objective defined in Eq. (8) can be reformulated as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle u^{*}(X^{(\\mathrm{mis})})}\\\\ &{=\\displaystyle\\underset{u(X^{(\\mathrm{mis})})\\in\\mathcal{H}}{\\mathrm{arg}\\,\\mathrm{max}}\\,[\\mathbb{E}_{r(X^{(\\mathrm{mis})})}[u^{\\top}(X^{(\\mathrm{mis})})\\nabla_{X^{(\\mathrm{mis})}}\\log\\hat{p}(X^{(\\mathrm{mis})}|X^{(\\mathrm{obs})})}\\\\ &{=\\displaystyle\\underset{u(X^{(\\mathrm{mis})})\\in\\mathcal{H}}{\\mathrm{arg}\\,\\mathrm{max}}\\,[\\,\\,\\,\\,\\,-\\,\\lambda\\nabla_{X^{(\\mathrm{mis})}}\\cdot u(X^{(\\mathrm{mis})})]\\}-\\frac{1}{2}\\|u(X^{(\\mathrm{mis})})\\|_{\\mathcal{H}}^{2},}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\quad\\{\\mathbb{E}_{r(\\tilde{X}^{(\\mathrm{mis})})}[\\sum_{i=1}^{\\infty}\\sqrt{\\xi_{i}}\\nabla_{\\tilde{X}^{(\\mathrm{mis})}}\\log\\hat{p}(\\tilde{X}^{(\\mathrm{mis})}|X^{(\\mathrm{obs})})^{\\top}u_{i}\\phi_{i}(\\tilde{X}^{(\\mathrm{mis})})}\\\\ {\\overset{(\\mathrm{i})}{=}\\displaystyle\\underset{u(X^{(\\mathrm{mis})})\\in\\mathcal{H}}{\\mathrm{arg}\\,\\mathrm{max}}}&{\\displaystyle\\qquad-\\,\\lambda\\nabla_{\\tilde{X}^{(\\mathrm{mis})}}\\cdot\\sum_{i=1}^{\\infty}u_{i}\\sqrt{\\xi_{i}}\\phi_{i}(\\tilde{X}^{(\\mathrm{mis})})\\|\\}-\\frac{1}{2}\\displaystyle\\sum_{i=1}^{\\infty}\\|u_{i}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Take the right-hand-side of (i) with-respect-to $u_{i}$ , and set it to 0, we can get: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sqrt{\\xi_{i}}\\{\\mathbb{E}_{r(\\tilde{X}^{(\\mathrm{ms})})}[[\\nabla_{\\tilde{X}^{(\\mathrm{ms})}}\\log\\hat{p}(\\tilde{X}^{(\\mathrm{miss})}|X^{(\\mathrm{obs})})]^{\\top}\\phi_{i}(\\tilde{X}^{(\\mathrm{miss})})-\\lambda\\nabla_{\\tilde{X}^{(\\mathrm{mis})}}\\phi_{i}(\\tilde{X}^{(\\mathrm{miss})})]\\}-u_{i}=0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "On this basis, $u_{i}^{*}$ can be given as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{u_{i}^{*}=\\sqrt{\\xi_{i}}\\{\\mathbb{E}_{r(\\tilde{X}^{\\mathrm{(mis)}})}[[\\nabla_{\\tilde{X}^{\\mathrm{(mis)}}}\\log\\hat{p}(\\tilde{X}^{\\mathrm{(mis)}}|X^{\\mathrm{(ms)}})]^{\\top}\\phi_{i}(X^{\\mathrm{(mis)}})-\\lambda\\nabla_{\\tilde{X}^{\\mathrm{(mis)}}}\\phi_{i}(\\tilde{X}^{\\mathrm{(mis)}})]\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and hence, $u(X^{\\mathrm{(miss)}})$ can be given as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad u^{*}(X^{(\\mathrm{miss})})}\\\\ &{=\\displaystyle\\sum_{i=1}^{\\infty}\\sqrt{\\xi_{i}}u_{i}^{*}\\phi_{i}(X^{(\\mathrm{miss})})}\\\\ &{=\\mathbb{E}_{r(\\tilde{X}^{(\\mathrm{miss})})}\\left[-\\lambda\\nabla_{\\tilde{X}^{(\\mathrm{miss})}}K(X^{(\\mathrm{miss})},\\tilde{X}^{(\\mathrm{miss})})\\right.}\\\\ &{\\left.\\qquad\\qquad+\\left[\\nabla_{\\tilde{X}^{(\\mathrm{miss})}}\\log\\hat{p}(\\tilde{X}^{(\\mathrm{miss})}|X^{(\\mathrm{obs})})\\right]^{\\top}\\!K(X^{(\\mathrm{miss})},\\tilde{X}^{(\\mathrm{miss})})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proposition (4.2). Assume that the proposal distribution $r(X^{(j o i n t)})$ is factorized by $r(X^{(j o i n t)}):=$ $r(X^{(m i s s)})p(X^{(o b s)})$ . The cost functional associated with the joint distribution is defined as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{F}_{j o i n t\\-N E R}:=\\mathbb{E}_{r(X^{(j o i n t)})}[\\log\\hat{p}(X^{(j o i n t)})]-\\lambda\\mathbb{H}[r(X^{(j o i n t)})],\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which leads to the velocity field delineated in Eq. (9) and establishes ${\\mathcal{F}}_{j o i n t}$ -NER as a lower bound for ${\\mathcal{F}}_{N E R}$ , with the difference being a constant (i.e., $\\mathcal{F}_{j o i n t-N E R}=\\mathcal{F}_{N E R}-c o n s t,c o n s t\\ge0,$ ). ", "page_idx": 22}, {"type": "text", "text": "Before proving this proposition, we want to first clarify the justification of the assumption that $r(X^{(\\mathrm{joint})}):={\\bar{r}}(X^{(\\mathrm{miss})}){\\bar{r}}(X^{(\\mathrm{obs})})=r(X^{(\\mathrm{miss})})p(X^{(\\mathrm{obs})})$ . In this part, we set $r(X^{(\\mathrm{obs})})=\\bar{p}(X^{(\\mathrm{obs})})$ Before stating the justification, we should come to the following agreements: ", "page_idx": 23}, {"type": "text", "text": "1. Throughout the imputation procedure, $X^{(\\mathrm{obs})}$ remains invariant regardless of any modifications to X(miss). 2. Given this invariance, it is justified to state that $r(X^{(\\mathrm{obs})})$ remains constant from the perspective of particle variational inference represented by reference [35, 34], and consequently $\\bar{r^{(}X^{\\mathrm{(obs)}}|{X^{\\mathrm{(miss)}}})}=r(X^{\\mathrm{(obs)}})$ , reflecting the independence of $X^{(\\mathrm{obs})}$ from $X^{(\\mathrm{miss})}$ . ", "page_idx": 23}, {"type": "text", "text": "Based on this, we want to show that within the WGF framework, the factorizations $r(X^{\\mathrm{(miss)}})r(X^{\\mathrm{(obs)}})$ and $r(X^{(\\mathrm{miss})}|X^{(\\mathrm{obs})})r(X^{(\\mathrm{obs})})$ are equivalent. To this end, let us write down the evolution of $r(X^{\\mathrm{(obs)}},X^{\\mathrm{(miss)}})$ along time $\\tau$ as follows when $r(X^{\\mathrm{(obs)}},X^{\\mathrm{(miss)}})$ is factorized by $r(X^{(\\mathrm{obs})},X^{(\\mathrm{miss})})=r(X^{(\\mathrm{miss})}|X^{(\\mathrm{obs})})r(X^{(\\mathrm{obs})})$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\partial r(X^{(\\mathrm{obs})},X^{(\\mathrm{miss})})}{\\partial\\tau}=\\frac{\\partial r(X^{(\\mathrm{miss})}|X^{(\\mathrm{obs})})r(X^{(\\mathrm{obs})})}{\\partial\\tau}}\\\\ {=\\underbrace{r(X^{(\\mathrm{obs})})\\frac{\\partial r(X^{(\\mathrm{miss})}|X^{(\\mathrm{obs})})}{\\partial\\tau}}_{r(X^{(\\mathrm{miss})}|X^{(\\mathrm{obs})})=\\frac{r(X^{(\\mathrm{obs})}|X^{(\\mathrm{obs})})}{r(X^{(\\mathrm{obs})}})}+\\underbrace{r(X^{(\\mathrm{miss})}|X^{(\\mathrm{obs})})\\frac{\\partial r(X^{(\\mathrm{obs})})}{\\partial\\tau}}_{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the first underbrace is the Bayesian formula, and the second underbrace is based on the abovementioned Agreement 2. Consequently, we can further obtain the following results: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\partial r({\\pmb X}^{\\mathrm{(obs)}},{\\pmb X}^{\\mathrm{(miss)}})}{\\partial\\tau}=\\underbrace{\\frac{r({\\pmb X}^{\\mathrm{(obs)}})}{r({\\pmb X}^{\\mathrm{(obs)}})}\\frac{\\partial r({\\pmb X}^{\\mathrm{(obs)}}|{\\pmb X}^{\\mathrm{(miss)}})r({\\pmb X}^{\\mathrm{(miss)}})}{\\partial\\tau}}_{r({\\pmb X}^{\\mathrm{(obs)}}|{\\pmb X}^{\\mathrm{(miss)}})=r({\\pmb X}^{\\mathrm{(obs)}})}}\\\\ &{}&{=\\!r({\\pmb X}^{\\mathrm{(obs)}})\\frac{\\partial r({\\pmb X}^{\\mathrm{(miss)}})}{\\partial\\tau}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Similarly, when we factorize $r(X^{\\mathrm{(obs)}},X^{\\mathrm{(miss)}})$ by $r(X^{\\mathrm{(obs)}},X^{\\mathrm{(miss)}})=r(X^{\\mathrm{(miss)}})r(X^{\\mathrm{(obs)}}).$ , we can get the following result: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\partial r(X^{\\mathrm{(obs)}},X^{\\mathrm{(miss)}})}{\\partial\\tau}=\\!\\frac{\\partial r(X^{\\mathrm{(obs)}})r(X^{\\mathrm{(miss)}})}{\\partial\\tau}}\\\\ {=\\!r(X^{\\mathrm{(obs)}})\\frac{\\partial r(X^{\\mathrm{(miss)}})}{\\partial\\tau}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Comparing Eq. (C.28) to Eq. (C.27), we can demonstrate our justification of the factorization $r(X^{\\mathrm{(obs)}},X^{\\mathrm{(miss)}})\\,=\\,r(X^{\\mathrm{(miss)}})r(X^{\\mathrm{(obs)}})$ . Finally, we would like to conclude with a metaphor to further illustrate the plausibility of this mean-filed factorization, which has been widely applied in variational inference [5]: ", "page_idx": 23}, {"type": "text", "text": "1. Consider $r$ as an actor in a play, capable of being molded and shaped. Initially, the actor may not fully embody the role, akin to $r(X^{\\mathrm{(miss)}})$ not containing information about $X^{(\\mathrm{obs})}$ . 2. However just as a director shapes an actor\u2019s performance through guidance and rehearsal, all we need to do is ensure that $r(X^{(\\mathrm{miss})})$ is appropriately molded by the directorial guidance (mirrors the continuity equation $\\begin{array}{r}{\\frac{\\partial\\boldsymbol{r}}{\\partial\\tau}=-\\boldsymbol{\\nabla}\\cdot(\\boldsymbol{u}\\boldsymbol{r}))}\\end{array}$ of the velocity field $u$ and the script provided by the critic $p(X^{\\mathrm{(obs)}},X^{\\mathrm{(miss)}})/p(X^{\\mathrm{(obs)}}|X^{\\mathrm{(miss)}})$ . 3. As long as $r$ can adapt based on this feedback (akin to the WGF framework), it can overcome the limitations of its initial portrayal (akin to $r(X^{(\\mathrm{joint})})=r(X^{(\\mathrm{miss})})r(X^{(\\mathrm{obs})})~)$ . ", "page_idx": 23}, {"type": "text", "text": "Based on the abovementioned analysis, we can now start the proof of Proposition 4.2: ", "page_idx": 23}, {"type": "text", "text": "Proof. Our proof will be divided into two parts namely \u2018velocity field derivation\u2019 and \u2018upper bound acquirement\u2019. ", "page_idx": 23}, {"type": "text", "text": "Velocity Field Derivation: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "the following continuity equation should be satisfied during the optimization of $r(X^{\\mathrm{(miss)}})$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial r({\\boldsymbol X}^{(\\mathrm{miss})})}{\\partial\\tau}=-\\nabla_{{\\boldsymbol X}^{(\\mathrm{miss})}}\\cdot\\big[r({\\boldsymbol X}^{(\\mathrm{miss})})u({\\boldsymbol X}^{(\\mathrm{miss})})\\big]}\\\\ &{\\Rightarrow\\displaystyle\\frac{\\partial r({\\boldsymbol X}^{(\\mathrm{miss})})}{\\partial\\tau}\\times p({\\boldsymbol X}^{(\\mathrm{obs})})=-\\nabla_{{\\boldsymbol X}^{(\\mathrm{miss})}}\\cdot\\big[r({\\boldsymbol X}^{(\\mathrm{miss})})u({\\boldsymbol X}^{(\\mathrm{miss})})\\big]\\times p({\\boldsymbol X}^{(\\mathrm{obs})})}\\\\ &{\\Rightarrow\\displaystyle\\frac{\\partial r({\\boldsymbol X}^{(\\mathrm{joint})})}{\\partial\\tau}=-\\nabla_{{\\boldsymbol X}^{(\\mathrm{miss})}}\\cdot\\big[r({\\boldsymbol X}^{(\\mathrm{joint})})u({\\boldsymbol X}^{(\\mathrm{joint})})\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where (i) is based on the fact that $X^{(\\mathrm{obs})}$ remains unchanged during the imputation process. Thus, according to Eq. (C.16), the evolution of $\\mathcal{F}_{\\mathrm{joint-NER}}$ along time $\\tau$ , $\\frac{\\mathrm{d}\\bar{\\mathcal{F}}_{\\mathrm{joint-NER}}}{\\mathrm{d}\\tau}$ , can be given as follows based on the chain rule: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathrm{d}X_{\\mathrm{pass}}}{\\sin^{2}\\theta}}\\\\ &{=\\int\\frac{\\mathrm{d}X}{\\sin^{2}\\theta}\\left[\\mathrm{log}\\hat{y}(X^{(\\theta)u u})+\\lambda\\mathrm{log}\\{X^{(\\theta)u}\\}+\\lambda\\right]\\mathrm{d}X^{(\\theta)u}}\\\\ &{=\\int-\\langle\\nabla X\\mathrm{rom}\\cdot[\\mathrm{ro}X^{(\\theta)\\mathrm{min}}]/\\mathrm{log}\\hat{y}(X^{(\\theta)u})\\rangle+\\lambda\\mathrm{log}\\{X^{(\\theta)u}\\}+\\lambda\\mathrm{lig}\\{^{(\\theta)u}\\}}\\\\ &{\\stackrel{(a)}{=}\\int\\left\\{r(X^{(\\theta)u}){\\mathrm{n}}(X^{(\\theta)u})\\right\\}\\nabla\\mathrm{r}\\_{X^{(\\theta)}}|\\mathrm{log}\\hat{y}(X^{(\\theta)u})+\\lambda\\mathrm{log}\\{X^{(\\theta)u}\\}+\\lambda\\mathrm{lig}\\{^{(\\theta)u}\\}}\\\\ &{=\\int\\left\\{r(X^{(\\theta)u)}\\right\\}(\\mathrm{log}\\{X^{(\\theta)u}\\})^{\\top}\\nabla\\mathrm{r}_{X^{(\\theta)}}|\\mathrm{log}\\hat{y}(X^{(\\theta)u})+\\lambda\\mathrm{log}\\{X^{(\\theta)u}\\})+\\lambda\\mathrm{lig}\\{^{(\\theta)}\\}}\\\\ &{=\\int\\left\\{r(X^{(\\theta)u)}\\right\\}(\\mathrm{log}\\{X^{(\\theta)u}\\})^{\\top}\\big[\\nabla\\mathrm{r}_{X^{(\\theta)}}|\\mathrm{log}\\hat{y}(X^{(\\theta)u})+\\lambda\\mathrm{log}\\{X^{(\\theta)u}\\}\\big]\\mathrm{d}X^{(\\theta)u}}\\\\ &{=\\int\\left\\{|u(X^{(\\theta)u})|^{\\top}[r(X^{(\\theta)u})\\nabla\\mathrm{ro}\\ k]\\mathrm{log}\\hat{y}(X^{(\\theta)u})+\\lambda\\mathrm{r}(X^{(\\theta)u})\\big\\}\\nabla_{X^{(\\theta)}}|\\mathrm{log}\\{X^{(\\theta)u}\\}|\\mathrm{d}X^{(\\theta)u}}\\\\ &{=\\int\\left\\{|u(X^{(\\theta)u})|^{\\top}[r(X^{(\\theta)u})\\nabla_{X^{(\\theta)}}|\\mathrm{sog}\\hat{y}(X^{(\\theta)u})+\\lambda\\nabla_{X^{(\\theta)u}}/\\Gamma(X^{(\\theta)u})]| \n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where (i) and (ii) are based on integration by parts. More specifically, when condition $\\begin{array}{r}{\\operatorname*{lim}_{X^{(\\mathrm{joint})}\\to\\infty}[u(X^{(\\mathrm{joint})})r(X^{(\\mathrm{joint})})]=0}\\end{array}$ is satisfied, for example, $r(X^{(\\mathrm{joint})})$ is bounded, and the limit of the velocity field as the norm of $X^{(\\mathrm{joint})}$ approaches infinity is zero $(\\operatorname*{lim}_{\\parallel X^{\\mathrm{(joint)}}}\\parallel\\to\\infty\\;u(X^{\\mathrm{(joint)}})=0)$ , we can get the following result [35, 32]: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\int\\nabla_{X^{(\\mathrm{miss})}}\\cdot[r(X^{(\\mathrm{joint})})u(X^{(\\mathrm{joint})})]\\mathrm{d}X^{(\\mathrm{joint})}=0,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we omit the gradient operator with respect to the observed variables $X^{(\\mathrm{obs})}$ , denoted as $\\nabla_{\\mathbf{X}^{\\mathrm{(obs)}}}$ , because $X^{(\\mathrm{obs})}$ remains constant during the imputation process. This constancy implies that the divergence $\\nabla_{X^{\\mathrm{(obs)}}}\\cdot[r(X^{\\mathrm{(joint)}})u(X^{\\mathrm{(joint)}})]=0$ . Consequently, the left-hand-side of this equation can be further decomposed as follows based on the integration by parts: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle\\int\\nabla_{X^{(\\mathrm{mis})}}\\cdot[r(X^{(\\mathrm{joint})})u(X^{(\\mathrm{joint})})]\\mathrm{d}X^{(\\mathrm{joint})}=\\int u^{\\top}(X^{(\\mathrm{joint})})\\nabla_{X^{(\\mathrm{mis})}}r(X^{(\\mathrm{joint})})\\mathrm{d}X^{(\\mathrm{joint})}}\\\\ &{}&{\\displaystyle+\\int\\big[\\nabla_{X^{(\\mathrm{mis})}}\\cdot u(X^{(\\mathrm{joint})})]r(X^{(\\mathrm{joint})})\\mathrm{d}X^{(\\mathrm{joint})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Similar to the proof of proposition 4.1, we can restrict the velocity field in RKHS and find the steepest gradient boosting direction as follows according to Eqs. (C.19) to (C.21): ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u^{*}\\big(X^{(\\mathrm{gion})}\\big)}\\\\ &{=\\quad\\underset{u(X^{(\\mathrm{gion})}\\in\\mathcal{H}^{\\mathrm{D}}}{\\mathrm{arg}\\,\\mathrm{max}}\\big\\,\\{\\mathbb{E}_{r(X^{(\\mathrm{gion})})}[u^{\\top}(X^{(\\mathrm{gion})})\\nabla_{X^{(\\mathrm{max})}}\\log\\hat{p}(X^{(\\mathrm{giont})})}\\\\ &{=\\quad\\underset{u(X^{(\\mathrm{gion})}\\in\\mathcal{H}^{\\mathrm{D}}}{\\mathrm{arg}\\,\\mathrm{max}}\\,\\,-\\,\\lambda\\nabla_{X^{(\\mathrm{max})}}\\cdot u(X^{(\\mathrm{mis})})]\\}-\\frac{1}{2}\\|u(X^{(\\mathrm{gion})})\\|_{\\mathcal{H}^{\\mathrm{D}}}^{2},}\\\\ &{\\qquad\\qquad\\qquad\\Big\\{\\mathbb{E}_{r(\\tilde{X}^{(\\mathrm{gion})})}[\\displaystyle\\sum_{i=1}^{\\infty}\\sqrt{\\xi_{i}}\\nabla_{\\tilde{X}}^{}(\\mathrm{as})\\log\\hat{p}(\\tilde{X}^{(\\mathrm{gion})})^{\\top}u_{i}\\phi_{i}(\\tilde{X}^{(\\mathrm{gion})})}\\\\ &{\\overset{(i)}{=}\\quad\\underset{u(X^{(\\mathrm{gion})}\\in\\mathcal{H}^{\\mathrm{D}}}{\\mathrm{arg}\\,\\mathrm{max}}}\\\\ &{\\qquad\\qquad-\\,\\lambda\\nabla_{\\tilde{X}^{(\\mathrm{mis})}}\\cdot\\displaystyle\\sum_{i=1}^{\\infty}u_{i}\\sqrt{\\xi_{i}}\\phi_{i}(\\tilde{X}^{(\\mathrm{gion})})]\\}-\\frac{1}{2}\\displaystyle\\sum_{i=1}^{\\infty}\\|u_{i}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Take the right-hand-side of (i) with-respect-to $u_{i}$ , and set it to 0, we can get: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sqrt{\\xi_{i}}\\{\\mathbb{E}_{r(\\tilde{X}^{(\\mathrm{pain})})}[[\\nabla_{\\tilde{X}^{(\\mathrm{mis})}}\\log\\hat{p}(\\tilde{X}^{(\\mathrm{joint})})]^{\\top}\\phi_{i}(\\tilde{X}^{(\\mathrm{paint})})-\\lambda\\nabla_{\\tilde{X}^{(\\mathrm{mis})}}\\phi_{i}(\\tilde{X}^{(\\mathrm{joint})})]\\}-u_{i}=0.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "On this basis, $u_{i}^{*}$ can be given as follows: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{u_{i}^{*}=\\sqrt{\\xi_{i}}\\{\\mathbb{E}_{r(\\tilde{X}^{(\\mathrm{pint})})}[[\\nabla_{\\tilde{X}^{(\\mathrm{miss})}}\\log\\hat{p}(\\tilde{X}^{(\\mathrm{joint})})]^{\\top}\\phi_{i}(\\tilde{X}^{(\\mathrm{pint})})-\\lambda\\nabla_{\\tilde{X}^{(\\mathrm{miss})}}\\phi_{i}(\\tilde{X}^{(\\mathrm{joint})})]\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and hence, $u(X^{(\\mathrm{joint})})$ can be given as follows: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad u(X^{(\\mathrm{joint})})}\\\\ &{=\\displaystyle\\sum_{i=1}^{\\infty}\\sqrt{\\xi_{i}}u_{i}^{*}\\phi_{i}(X^{(\\mathrm{joint})})}\\\\ &{=\\mathbb{E}_{r(X^{(\\mathrm{joint})})}\\left[-\\lambda\\nabla_{\\tilde{X}^{(\\mathrm{miss})}}K(X^{(\\mathrm{joint})},\\tilde{X}^{(\\mathrm{joint})})\\right.}\\\\ &{\\quad\\left.\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lower Bound Acquirement: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Before starting the proving of this part, we should notice that given the unchanged observational data $X^{(\\mathrm{obs})}$ , the distribution $p(\\mathbf{X}^{(\\mathrm{obs})})$ is a constant. On this basis, consider the definition of $\\mathcal{F}_{\\mathrm{NER}}$ (right-hand-side of Eq. (7)), the first term and the second term are denoted by \u2018term $\\r_{1}$ and \u2018term $\\acute{2}$ for simplicity: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underbrace{\\mathbb{E}_{r({X^{(\\mathrm{miss})}})}[\\log\\hat{p}({X^{(\\mathrm{miss})}}|{X^{(\\mathrm{obs})}})]}_{:=\\mathrm{term~}1}+\\lambda\\times\\left[\\underbrace{-\\mathbb{H}[r({X^{(\\mathrm{miss})}})]}_{:=\\mathrm{term~}2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For term 1, we can obtain the following derivation: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int r(X^{(\\mathrm{miss})})\\log\\hat{p}(X^{(\\mathrm{miss})}|X^{(\\mathrm{obs})})\\mathrm{d}X^{(\\mathrm{miss})})}\\\\ &{\\displaystyle\\geq\\int r(X^{(\\mathrm{miss})})\\log\\hat{p}(X^{(\\mathrm{miss})}|X^{(\\mathrm{obs})})\\mathrm{d}X^{(\\mathrm{miss})}+\\underbrace{\\int p(X^{(\\mathrm{obs})})\\log p(X^{(\\mathrm{obs})})\\mathrm{d}X^{(\\mathrm{obs})}}_{)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{=\\iint_{\\mathbf{\\nabla}}p(\\mathbf{\\boldsymbol{X}}^{(\\mathrm{obs})})r(\\mathbf{\\boldsymbol{X}}^{(\\mathrm{miss})})\\log\\hat{p}(\\mathbf{\\boldsymbol{X}}^{(\\mathrm{miss})}|\\mathbf{\\boldsymbol{X}}^{(\\mathrm{obs})})\\mathrm{d}\\mathbf{\\boldsymbol{X}}^{(\\mathrm{miss})}\\mathrm{d}\\mathbf{\\boldsymbol{X}}^{(\\mathrm{obs})}}}\\\\ &{\\quad+\\int p(\\mathbf{\\boldsymbol{X}}^{(\\mathrm{obs})})\\log p(\\mathbf{\\boldsymbol{X}}^{(\\mathrm{obs})})\\mathrm{d}\\mathbf{\\boldsymbol{X}}^{(\\mathrm{obs})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{=\\iint p(\\boldsymbol{X}^{(\\mathrm{obs})})r(\\boldsymbol{X}^{(\\mathrm{miss})})\\log\\hat{p}(\\boldsymbol{X}^{(\\mathrm{miss})}|\\boldsymbol{X}^{(\\mathrm{obs})})\\mathrm{d}\\boldsymbol{X}^{(\\mathrm{miss})}\\mathrm{d}\\boldsymbol{X}^{(\\mathrm{obs})}}}\\\\ &{}&{+\\int\\!\\!\\!\\!\\int r(\\boldsymbol{X}^{(\\mathrm{miss})})p(\\boldsymbol{X}^{(\\mathrm{obs})})\\log p(\\boldsymbol{X}^{(\\mathrm{obs})})\\mathrm{d}\\boldsymbol{X}^{(\\mathrm{miss})}\\mathrm{d}\\boldsymbol{X}^{(\\mathrm{obs})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\displaystyle\\iint\\underbrace{p(\\boldsymbol{X}^{(\\mathrm{obs})})r(\\boldsymbol{X}^{(\\mathrm{miss})})}_{r(\\boldsymbol{X}^{(\\mathrm{miss})},\\boldsymbol{X}^{(\\mathrm{obs})})}\\bigl[\\underbrace{\\log\\hat{p}(\\boldsymbol{X}^{(\\mathrm{miss})}|\\boldsymbol{X}^{(\\mathrm{obs})})+\\log p(\\boldsymbol{X}^{(\\mathrm{obs})})}_{\\log\\hat{p}(\\boldsymbol{X}^{(\\mathrm{miss})},\\boldsymbol{X}^{(\\mathrm{obs})})}\\bigr]\\mathrm{d}\\boldsymbol{X}^{(\\mathrm{miss})}\\mathrm{d}\\boldsymbol{X}^{(\\mathrm{obs})}}\\\\ &{=\\mathbb{E}_{r(\\boldsymbol{X}^{(\\mathrm{miss})},\\boldsymbol{X}^{(\\mathrm{obs})})}\\bigl[\\log\\hat{p}(\\boldsymbol{X}^{(\\mathrm{miss})},\\boldsymbol{X}^{(\\mathrm{obs})})\\bigr].}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Similarly, the term 2 can be reformulated as follows: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\mathbb{H}[r(X^{(\\mathrm{max})})]}\\\\ &{\\geq-\\mathbb{H}[r(X^{(\\mathrm{max})})]+\\underbrace{\\int_{\\mathcal{V}}(X^{(\\mathrm{mb})})\\log p(X^{(\\mathrm{ebs})})\\mathrm{d}X^{(\\mathrm{obs})}}_{\\mathrm{nogise~coulneq~to~grabile~coulum}}}\\\\ &{=\\iint p(X^{(\\mathrm{ebs})})r(X^{(\\mathrm{max})})\\log r(X^{(\\mathrm{max})})\\mathrm{d}X^{(\\mathrm{mb})}\\mathrm{d}X^{(\\mathrm{ebs})}}\\\\ &{\\qquad+\\underbrace{\\int\\!\\!\\int p(X^{(\\mathrm{ebs})})r(X^{(\\mathrm{mb})})\\log p(X^{(\\mathrm{bs})})\\mathrm{d}X^{(\\mathrm{mb})}\\mathrm{d}X^{(\\mathrm{das})}}_{\\mathrm{nogise~couleq~to~paine~coulne}}}\\\\ &{=\\iint\\!\\!\\int\\!\\!\\underbrace{p(X^{(\\mathrm{ebs})})r(X^{(\\mathrm{mis})})\\log r(X^{(\\mathrm{mis})})+\\log p(X^{(\\mathrm{ebs})})\\mathrm{d}X^{(\\mathrm{mis})}}_{r(X^{(\\mathrm{ebs})},X^{(\\mathrm{mis})})}\\mathrm{d}X^{(\\mathrm{mis})}\\mathrm{d}X^{(\\mathrm{das})}}\\\\ &{=-\\mathbb{H}[r(X^{(\\mathrm{ebs})},X^{(\\mathrm{max})})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Combine Eqs. (C.36) and (C.37), we can obtain the following relationship: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{F}_{\\mathrm{NER}}-\\mathrm{const}=\\mathcal{F}_{\\mathrm{joint-NER}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and constant const is greater than 0. ", "page_idx": 26}, {"type": "text", "text": "Corollary (4.3). The following equation holds: $u(X^{(j o i n t)})=u(X^{(m i s s)})$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. This corollary can be easily proven by according to Eq. (C.38): ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{F}_{\\mathrm{NER}}=\\mathcal{F}_{\\mathrm{joint-NER}}+\\mathrm{const}}\\\\ &{\\Rightarrow\\nabla_{X^{(\\mathrm{miss})}}\\frac{\\delta\\mathcal{F}_{\\mathrm{NER}}}{\\delta r\\left(X^{(\\mathrm{miss})}\\right)}=\\nabla_{X^{(\\mathrm{miss})}}\\frac{\\delta\\mathcal{F}_{\\mathrm{joint-NER}}+\\mathrm{const}}{\\delta r\\left(X^{(\\mathrm{miss})}\\right)}}\\\\ &{\\Rightarrow\\nabla_{X^{(\\mathrm{miss})}}\\frac{\\delta\\mathcal{F}_{\\mathrm{NER}}}{\\delta r\\left(X^{(\\mathrm{miss})}\\right)}=\\nabla_{X^{(\\mathrm{miss})}}\\frac{\\delta\\mathcal{F}_{\\mathrm{joint-NER}}}{\\delta r\\left(X^{(\\mathrm{miss})}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Plugging Eq. (C.39) into Eqs. (A.5) and (A.6), we can see that the velocity fields for $X^{(\\mathrm{miss})}$ within functional $\\mathcal{F}_{\\mathrm{NER}}$ and $\\mathcal{F}_{\\mathrm{joint-NER}}$ are identical. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "Appendix D Detailed Explanation for the Workflow of NewImp Approach ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we intend to provide detailed information about the implementation of the NewImp approach in Algorithm 1. We will focus on two primary aspects: 1) the numerical implementation of ODE simulation, and 2) the DSM algorithm. ", "page_idx": 27}, {"type": "text", "text": "D.1 Forward Euler\u2019s Method for ODE Simulation ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "During step 7 of Algorithm 1, we involve the simulation of the ODE defined by Eqs. (9) and (14). To simulate this ODE we use the forward Euler\u2019s method [6] in this paper for simplicity. Specifically, suppose we have the following ODE: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}x_{\\tau}}{\\mathrm{d}\\tau}=f(x_{\\tau}),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and the initial value at $\\tau=0$ is given $x_{0}=x_{\\mathrm{init}}$ , the value at time $\\eta$ can be derived as follows: ", "page_idx": 27}, {"type": "equation", "text": "$$\nx_{\\eta}=x_{0}+\\int_{0}^{\\eta}f(x_{\\tau})\\mathrm{d}\\tau.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "To alleviate the intergal term, the forward Euler\u2019s method attempts to approximate the integral term to summation term as follows: ", "page_idx": 27}, {"type": "equation", "text": "$$\nx_{\\eta}\\approx x_{0}+f(x_{\\tau})\\times(\\eta-0).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "On this basis, the value at time $\\mathrm{T}$ can be obtained by repeating Eq. (D.3) from $\\tau=0$ to $\\tau=\\mathrm{T}$ , which is the forward Euler\u2019s method. ", "page_idx": 27}, {"type": "table", "img_path": "fIz8K4DJ7w/tmp/6159eaafc9665e693036c1cf5c1167a8b5f3175ddda20767f378dd8845223b6f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "D.2 Detailed Information for DSM ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "During step 5 of Algorithm 1, we involve the training of $\\nabla_{X^{\\mathrm{(joint)}}}\\log\\hat{p}(X^{\\mathrm{(joint)}})$ using the DSM function. In this subsection, we aim to further elaborate on the detailed algorithm for the DSM function to uphold the completeness of this manuscript. As mentioned in Section 2.2, the score function $\\nabla_{X^{\\mathrm{(joint)}}}\\log\\hat{p}(X^{\\mathrm{(joint)}})$ is typically parameterized by a neural network. For simplicity, we denote the parameter set of $\\nabla_{X^{\\mathrm{(joint)}}}\\log\\hat{p}(X^{\\mathrm{(joint)}})$ by $\\theta$ . ", "page_idx": 27}, {"type": "text", "text": "Algorithm 3 DSM for $\\nabla_{X^{\\mathrm{(joint)}}}\\log\\hat{p}(X^{\\mathrm{(joint)}})$ Training   \n1: Input: joint data $X^{(\\mathrm{joint})}$ .   \n2: Hyperparameters: neural network learning rate $l r$ , training epoch $\\mathcal{E}$ , and neural network hidden   \nunit $\\mathrm{HU}_{\\mathrm{score}}$ .   \n3: for $e=1$ to $\\mathcal{E}$ do   \n4: X\u02c6(joint) $\\begin{array}{r l}&{\\hat{\\boldsymbol X}^{(\\mathrm{joint})}\\gets\\boldsymbol X^{(\\mathrm{joint})}+\\epsilon,\\epsilon\\sim\\mathcal N(\\mathbf0,\\sigma^{2}\\mathbf I)}\\\\ &{\\nabla_{\\hat{\\boldsymbol X}^{(\\mathrm{joint})}}\\log q_{\\sigma}(\\hat{\\boldsymbol X}^{(\\mathrm{joint})}|\\boldsymbol X^{(\\mathrm{joint})})\\gets-\\frac{\\hat{\\boldsymbol X}^{(\\mathrm{joint})}-\\boldsymbol X^{(\\mathrm{joint})}}{\\sigma^{2}}}\\end{array}$ \u25b7Data Noising   \n5:   \n6: $\\mathcal{L}_{\\mathrm{DSM}}\\leftarrow\\mathrm{Eq}$ . (15)   \n7: \u03b8 \u2190ApplyGradient $(\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_{\\mathrm{DSM}},\\boldsymbol{l r})$ \u25b7Apply the Gradient with Learning Rate $l r$   \n8: end for ", "page_idx": 28}, {"type": "text", "text": "Appendix E Detailed Information for Experiments ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "E.1 Background & Simulation of Missing Data ", "text_level": 1, "page_idx": 28}, {"type": "table", "img_path": "fIz8K4DJ7w/tmp/876d0c635b814f6080515dbac2e1b7bc1c797a0cb61813e729d5a6424c8296b0.jpg", "table_caption": ["Table E.1: Detailed dataset descriptions, where \u2018Dimension\u2019 denotes the variate number of each dataset. \u2018Numer\u2019 denotes the total number of item. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "In this paper, we consider the datasets listed in Table E.1 as our experimental datasets. Based on this, according to reference [44], missing data can be classified into three categories: Missing at Random (MAR), where the likelihood of missing data depends solely on observed data; Missing Completely at Random (MCAR), where the absence of data is completely unrelated to any observed or unobserved variables; and Missing Not at Random (MNAR), where missingness is influenced by unobserved data. In the cases of MCAR and MAR, the patterns of missing data are considered \u2018ignorable\u2019 because it is unnecessary to explicitly model the distribution of the missing values. Conversely, MNAR scenarios, where missing data can introduce significant biases that are not easily corrected without imposing domain-specific assumptions, constraints, or parametric forms on the missingness mechanism, present more complex challenges [40, 22]. Therefore, our discussion is primarily focused on numerical tabular data within the MCAR and MAR contexts. ", "page_idx": 28}, {"type": "text", "text": "To simulate missing data, we adopt the methodologies outlined in reference [22]: ", "page_idx": 28}, {"type": "text", "text": "\u2022 MAR: Initially, a random subset of features is selected to remain non-missing. The masking of the remaining features is conducted using a logistic model, which employs the non-missing features as predictors. This model is parameterized with randomly selected weights, and the bias is adjusted to achieve the desired missingness rate. ", "page_idx": 28}, {"type": "text", "text": "\u2022 MCAR: For each data point, the masking variable is generated from a Bernoulli distribution with a predetermined fixed mean, ensuring that the probability of missingness is the same across all data points. ", "page_idx": 28}, {"type": "text", "text": "\u2022 MNAR: Although MNAR scenarios are not the primary focus of this manuscript, we include experiments in this context. Missingness is introduced either by additional masking of the MARselected features using a Bernoulli process with a fixed mean, or through direct self-masking of values using interval-censoring techniques. In this paper, we mainly consider the former strategy. In other words, the mechanism of MNAR we used in this paper is identical to the previously described MAR mechanism, but the inputs of the logistic model are then masked by an MCAR mechanism. ", "page_idx": 28}, {"type": "text", "text": "E.2 Hyperparameter Setting of Baseline Models ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this subsection, we want to report the baseline models\u2019 hyperparameter settings to ensure the reproducibility of our paper: ", "page_idx": 29}, {"type": "text", "text": "\u2022 Batch-Size-Related: The batch size for ReMasker is set to 64. For other baseline models, it is uniformly set at 512. (Notably, for Sink and TDM, if $\\mathrm{N<512}$ , the batch size is set to $2^{\\lfloor{\\frac{\\mathrm{N}}{2}}\\rfloor}$ . ) ", "page_idx": 29}, {"type": "text", "text": "\u2022 Hidden-Unit-Related: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2013 The MIWAE model features a latent dimension of 16 and 32 hidden units.   \n\u2013 The TDM model includes 16 hidden units per layer with the number of layers set to 2.   \n\u2013 MIRACLE\u2019s hidden units are set to 32.   \n\u2013 For ReMasker, the embedding dimension is 32, depth is 6, mask ratio is 0.5, encoder depth is 6, decoder depth is 4, number of heads is 4, and the multi-layer perceptron ratio is 4.0.   \n\u2013 For MissDiff and CSDI_T, the channel size is set as 16, the embedding dimension is set to 128, and the layer number is set as 2.   \n\u2013 For the GAIN model, for both the generator and the discriminator, the hidden size is set to $2\\times\\mathrm{D}$ , and the number of hidden layers is set to 3. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Diffusion-Hyperparameters-Related: The diffusion step is set at 100 and the particle number at 50 for MissDiff and CSDI_T. ", "page_idx": 29}, {"type": "text", "text": "Appendix F Additional Empirical Evidence ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "F.1 Toy Case Experiments ", "page_idx": 30}, {"type": "image", "img_path": "fIz8K4DJ7w/tmp/58659b02d9b2c57f79e2b48b28c9c1db32895939092deee230e935786cba77fd.jpg", "img_caption": ["Figure F.1: Contours of Various Distributions\u2019 Density Value. "], "img_footnote": [], "page_idx": 30}, {"type": "table", "img_path": "fIz8K4DJ7w/tmp/26a1b14f70321eb853f3ab47878937c888907f352d57d5fa5804f306f83c6a27.jpg", "table_caption": ["Table F.1: NewImp Performance with Missing Rate at $30\\%$ , and 1000 samples are generated. "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "To demonstrate the effectiveness of the NewImp method vary different type of distributions, we evaluate it across four distinct toy cases, each characterized by different distributions: ", "page_idx": 30}, {"type": "text", "text": "\u2022 Standard Gaussian: $X^{\\mathrm{(ideal)}}\\sim\\mathcal{N}(0,I_{2\\times2})$ . ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 Student\u2019s- $\\mathit{\\Pi}_{t}$ (a heavy-tailed distribution): $X^{(\\mathrm{ideal})}\\sim\\mathrm{St}{-t}(0,\\left[\\!\\!\\begin{array}{c c}{{1}}&{{0.5}}\\\\ {{0.5}}&{{1}}\\end{array}\\!\\!\\right]).$ ", "page_idx": 30}, {"type": "text", "text": "\u2022 Gaussian Mixture: $X^{\\mathrm{(ideal)}}\\sim\\frac{1}{3}\\times\\mathcal{N}([1,2],\\left[\\!\\!\\begin{array}{c c}{{0.5}}&{{0}}\\\\ {{0}}&{{0.5}}\\end{array}\\!\\!\\right])+\\frac{1}{3}\\times\\mathcal{N}([-1,-2],\\left[\\!\\!\\begin{array}{c c}{{0.5}}&{{0.1}}\\\\ {{0.1}}&{{0.5}}\\end{array}\\!\\!\\right])+\\frac{1}{3}\\times\\mathcal{N}([-1,2],\\left[\\!\\!\\begin{array}{c c}{{0.5}}&{{0.1}}\\\\ {{0.1}}&{{0.5}}\\end{array}\\!\\!\\right])$ $\\mathcal{N}([2,-2],\\left[\\!\\!\\begin{array}{c c}{{0.3}}&{{0}}\\\\ {{0}}&{{0.3}}\\end{array}\\!\\!\\right]).$ ", "page_idx": 30}, {"type": "text", "text": "\u2022 Skewed Gaussian (via exponential transformation): $X^{\\mathrm{(ideal)}}=\\exp(\\epsilon)$ , where $\\epsilon\\sim\\mathcal{N}(0,I_{2\\times2})$ . ", "page_idx": 30}, {"type": "text", "text": "Based on this, we display the contours of their density values in Fig. F.1, and we list the imputation accuracy comparisons for MAR, MCAR, and MNAR scenarios with a $30\\%$ missing rate in Table F.1. The results indicate that our NewImp approach generally performs better on non-standard Gaussian type data, underscoring its universality and applicability. This enhanced performance is attributable to our modeling strategy, which involves modeling the score function of the data [52, 50], which eliminates the need for normalization, and consequently results in the NewImp approach can perform well on complex data distributions, including skewed, heavy-tailed, and mixture distributions. ", "page_idx": 30}, {"type": "text", "text": "F.2 Additional Experimental Results with MNAR Scenario ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In this section, we expand upon the results presented in Table 1 by including the MNAR scenario, as detailed in Table F.2. Additionally, we report on the outcomes of an ablation study and sensitivity analysis in Tables F.4 and F.5 and Fig. F.2. These extended results lead to several pertinent observations: ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "\u2022 Across three different missing data scenarios, the models consistently exhibit the poorest performance under the MNAR condition. For instance, in the MNAR scenario, nearly all models show a significant decrease in imputation accuracy and an increase in standard deviation. This supports the assertion made in Appendix E.1 that addressing the MNAR scenario requires the incorporation of relevant domain knowledge to mitigate biases introduced by the pattern of missing data. \u2022 The findings from the ablation study under the MNAR scenario are consistent with those observed in both MAR and MCAR scenarios in Section 5.3. This consistency underscores the importance of including the NER term and adopting the joint distribution modeling approach. \u2022 Similarly, the results from the sensitivity analysis under the MNAR scenario align with those from MAR and MCAR scenarios in Section 5.4. This alignment reinforces our interpretations of model performance across different groups of hyperparameters under MAR and MCAR scenarios. ", "page_idx": 31}, {"type": "table", "img_path": "fIz8K4DJ7w/tmp/3d9fa18e38ea331198653cc70cd0bd9cb7e90eddb379030c2498243ab9132aa0.jpg", "table_caption": ["Table F.2: Performance of MAE and WASS metrics at $30\\%$ missing rate. "], "table_footnote": ["Kindly Note: The best results are bolded and the second best results are underliend. \u201c\\*\u201d marks the results that NewImp significantly outperform with $p$ -value $<0.05$ over paired samples $t$ -test. "], "page_idx": 31}, {"type": "table", "img_path": "fIz8K4DJ7w/tmp/b6498e781230f96877727db3278b6b6a3b086263416ad8168e7e9aad0a6aa16d.jpg", "table_caption": ["Table F.3: Standard deviation of MAE and WASS metrics at $30\\%$ missing rate. "], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "fIz8K4DJ7w/tmp/b272471ef838d71cebcd428bc4e1ac800e447271f67a1e731c0406157e7aea05.jpg", "table_caption": ["Table F.4: Ablation study results at $30\\%$ missing rate. "], "table_footnote": ["Kindly Note: The best results are bolded and the second best results are underliend. \u201c\\*\u201d marks the results that NewImp significantly outperform with $p$ -value $<0.05$ over paired samples $t$ -test. "], "page_idx": 32}, {"type": "table", "img_path": "fIz8K4DJ7w/tmp/99b5ed93e12ed57d3e7c9cc92718b0d6945b489ca1df859fa676e15d6694c267.jpg", "table_caption": ["Table F.5: Standard deviation of Ablation Study Results with missing rate at $30\\%$ . "], "table_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "fIz8K4DJ7w/tmp/dae6a5329e3f526986ca7a21fcfc115c791b558258ecce4af0259fc4b1447ac7.jpg", "img_caption": ["(a) MAR with $30\\%$ missing rate at CC dataset. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "fIz8K4DJ7w/tmp/87e59e312b5c8a5618d2e6d1ae7980d12fd29dde315fe4289a27e945bac576c7.jpg", "img_caption": ["(b) MCAR with $30\\%$ missing rate at CC dataset. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "fIz8K4DJ7w/tmp/884b16d8a02be89376c54f1726025215527feb2f76a34e979ef1e390a86fd1f7.jpg", "img_caption": ["(c) MNAR with $30\\%$ missing rate at CC dataset. "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Figure F.2: Parameter sensitivity of NewImp on bandwidth for kernel function $(h)$ , hidden unit of score network $\\mathrm{HU_{score}}$ , NER weight $\\lambda$ , and discretization step $\\eta$ for Eq. (9) on CC dataset. Mean values and one standard deviation from mean are represented by scatters and shaded area, respectively. ", "page_idx": 33}, {"type": "text", "text": "F.3 Empirical Evidence for Selecting RBF Function ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In our derivation process, we specifically selected the RBF kernel to satisfy the \u2018zero boundary condition\u2019: $\\begin{array}{r}{\\operatorname*{lim}_{X^{\\mathrm{(joint)}}\\to\\infty}K(X^{\\mathrm{(joint)}},\\tilde{X}^{\\mathrm{(joint)}})=0}\\end{array}$ for the sake of avoiding the explicit density estimation of the intractable proposal distribution $r(X^{(\\mathrm{joint})})$ . This selection prompts an additional inquiry: What if we replaced the RBF kernel with another that does not fulfill the \u2018zero boundary condition\u2019? To maintain the rigor of our analysis, we compared the performance of the NewImp with alternative kernel functions under identical settings. Consequently, we consider the following types of kernel functions: ", "page_idx": 33}, {"type": "text", "text": "\u2022 linear kernel function (linear): $K({\\pmb X}^{(\\mathrm{joint})},\\tilde{{\\pmb X}}^{(\\mathrm{joint})})=[{\\pmb X}^{(\\mathrm{joint})}][\\tilde{\\pmb X}^{(\\mathrm{joint})}]^{\\intercal}$ \u2022 polynomial kernel function (poly): $K({X}^{(\\mathrm{joint})},\\tilde{{X}}^{(\\mathrm{joint})})=\\{[{X}^{(\\mathrm{joint})}][\\tilde{{X}}^{(\\mathrm{joint})}]^{\\top}\\}^{2}$ \u2022 sigmoid kernel function (sigmoid): $K(X^{(\\mathrm{joint})},\\tilde{X}^{(\\mathrm{joint})})=\\operatorname{tanh}\\{[X^{(\\mathrm{joint})}][\\tilde{X}^{(\\mathrm{joint})}]^{\\top}\\}$ $\\begin{array}{r}{K(X^{\\mathrm{(joint)}},\\tilde{X}^{\\mathrm{(joint)}})=\\{\\frac{[X^{\\mathrm{(joint)}}]}{\\lVert[X^{\\mathrm{(joint)}}]\\rVert}\\}\\{\\frac{[\\tilde{X}^{\\mathrm{(joint)}}]}{\\lVert[\\tilde{X}^{\\mathrm{(joint)}}]\\rVert}\\}^{\\top}}\\end{array}$ \u2022 sine kernel function (sin): K(X(joint), X\u02dc(joint)) $K({X}^{(\\mathrm{joint})},\\tilde{{X}}^{(\\mathrm{joint})})=\\sin(\\lvert{X}^{(\\mathrm{joint})}-\\tilde{{X}}^{(\\mathrm{joint})}\\rvert_{2}^{2})$ ", "page_idx": 33}, {"type": "text", "text": "The experimental results are detailed in Table F.6 (For completeness, we also report the results under the MNAR scenario). From the results, it is evident that other kernel functions, which do not meet the \u2018zero boundary condition\u2019, perform significantly worse compared to the RBF kernel. This demonstrates the critical importance of selecting the appropriate kernel function for achieving accurate imputation results, thereby validating the choice of the RBF kernel for our NewImp approach. ", "page_idx": 33}, {"type": "table", "img_path": "fIz8K4DJ7w/tmp/6dfc840d31451ac3ed24fc6dea5f9dbb0e1fca6143986326b3f3316a6d4eac05.jpg", "table_caption": ["Table F.6: Imputation accuracy vary different kernels at $30\\%$ missing rate. "], "table_footnote": ["Kindly Note: The best results are bolded and the second best results are underliend. \u201c\\*\u201d marks the results that RBF kernel significantly outperform with $p$ -value $<0.05$ over paired samples $t$ -test. "], "page_idx": 34}, {"type": "table", "img_path": "fIz8K4DJ7w/tmp/51580ce5dc7cceab67e43d9073312f8aa1019a3238d5059ab4ed524d7afdb377.jpg", "table_caption": ["Table F.7: Standard deviation of MAE and WASS metrics vary different kernels at $30\\%$ missing rate. "], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "F.4 Time Complexity Analysis ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In this section, we present an analysis of the complexity of time for our NewImp approach. The complexity analysis is based on the algorithms described in Algorithm 1. We begin by estimating the time complexity of the score function $\\nabla_{X^{\\mathrm{(joint)}}}\\log\\hat{p}(X^{\\mathrm{(joint)}})$ . Assuming the number of layers in the neural network that parameterize $\\nabla_{X^{\\mathrm{(ioint)}}}\\log\\hat{p}(X^{\\mathrm{(joint)}})$ is $\\mathrm{L}$ and each layer has an equal number of hidden units denoted as $\\mathrm{HU}_{\\mathrm{score}}$ , the time complexity for the imputation algorithm defined in Algorithm 1 is detailed as follows: ", "page_idx": 34}, {"type": "text", "text": "1. DSM Training Part (step 5): Building on the previous item, the time complexity for the DSM training algorithm defined in Algorithm 3 is given as: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left[4\\times\\mathrm{N}\\times\\left(\\mathrm{D}\\times\\mathrm{HU}_{\\mathrm{score}}+\\left(\\mathrm{L}-1\\right)\\times\\mathrm{HU}_{\\mathrm{score}}^{2}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the factor of 4 comprises three distinct components: backward propagation (1), forward propagation (1), and the acquisition of the sample-wise score function (2). Note that the network parameter size is substantially smaller than the number of data points, thereby making the forward computation of the score function the primary factor in time complexity. ", "page_idx": 34}, {"type": "text", "text": "2. Imputation Part (step 7): ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 Score function computation: The time complexity for computing the score function is expressed as: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left[2\\times\\mathrm{N}\\times\\left(\\mathrm{D}\\times\\mathrm{HU}_{\\mathrm{score}}+\\left(\\mathrm{L}-1\\right)\\times\\mathrm{HU}_{\\mathrm{score}}^{2}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the factor 2 accounts for the backward propagation needed during the score function computation. ", "page_idx": 34}, {"type": "image", "img_path": "fIz8K4DJ7w/tmp/f0b836df88b318a303a506cf8e7f36feca522b1ba2a1c09c9edc2cff0544ef1d.jpg", "img_caption": ["(a) Running time vary N, MAR.(b) Running time vary N, MCAR.(c) Running time vary N, MNAR. "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "fIz8K4DJ7w/tmp/9d8fea4f7d1e285789cbbbc3978752f9832326de53aca94e7de3c109717a10d7.jpg", "img_caption": ["(d) Running time vary D, MAR.(e) Running time vary D, MCAR.(f) Running time vary D, MNAR. "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Figure F.3: Average computation time, where \u2018Estimate\u2019 indicates the \u2018DSM Training Algorithm\u2019 (step 5 of Algorithm 1), and \u2018Impute\u2019 indicates the imputation algorithm (step 7 of Algorithm 1). The scatters and shaded areas indicate the mean and one standard deviation from the mean, respectively. ", "page_idx": 35}, {"type": "text", "text": "\u2022 Kernel function and its gradient: Employing the RBF kernel $\\begin{array}{r}{K({\\cal{X}},\\tilde{\\cal{X}}):=\\exp\\left(-\\frac{\\|{\\cal{X}}-\\tilde{\\cal{X}}\\|^{2}}{2h^{2}}\\right)}\\end{array}$ the gradient with respect to $\\tilde{\\boldsymbol{X}}$ is analytically determined as: ", "page_idx": 35}, {"type": "equation", "text": "$$\n[\\nabla_{\\tilde{X}}K(X,\\tilde{X})][;j]=-\\frac{1}{h^{2}}\\left\\{[K(X,\\tilde{X})\\times\\tilde{X}][;j]+\\tilde{X}[:,j]\\odot\\sum_{j=1}^{\\mathrm{D}}K(X,\\tilde{X})[:,j]\\right\\}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The time complexities for calculating the kernel function and its gradient are specified in Eqs. (F.4) and (F.5): ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\mathcal O\\left[\\mathrm{N}^{2}\\times\\mathrm{D}+\\mathrm{N}^{2}\\right],}}\\\\ {{\\mathcal O\\left[\\mathrm{N}^{2}\\times\\mathrm{D}+\\mathrm{N}^{2}+\\mathrm{N}\\times\\mathrm{D}\\right].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Based on the abovementioned analysis, we explore how computational complexity varies with different dataset sizes N and the number of features $\\mathrm{D}$ , as shown in Figs. F.3 (a) and (b), respectively. From these figures, it is evident that computational time increases with the dataset size N. However, changes in the number of features D do not significantly affect the computation time. This observation underscores that the primary determinant of computational complexity in our context is the dataset size, aligning with our theoretical analysis, which indicates a quadratic relationship between time complexity and the size of the dataset $\\mathrm{N}$ for the \u2018Imputation\u2019 part, and $\\mathrm{N}\\gg\\mathrm{D}$ for the \u2018DSM Training part, aligning with our theoretical analysis. ", "page_idx": 35}, {"type": "text", "text": "Moreover, the data reveals that the total computational time is predominantly governed by \u2018Estimation\u2019 part of our NewImp approach. This suggests that the training of the score function represents a critical bottleneck in the efficiency of the NewImp algorithm. Therefore, accelerating the NewImp algorithm crucially hinges on reducing the computational demands of the \u2018Estimation\u2019 part. ", "page_idx": 35}, {"type": "text", "text": "F.5 Convergence Analysis ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "In this section, we want to discuss the convergence of the proposed NewImp approach, prior to delving into this discussion, it is essential to establish a clear definition of convergence: ", "page_idx": 35}, {"type": "text", "text": "Definition F.1. A sequence $\\{{\\mathcal{F}}_{1},{\\mathcal{F}}_{2},...,{\\mathcal{F}}_{\\mathrm{T}}\\}$ is said to be convergent if there exists a real number $\\mathcal{G}$ such that for any given positive number $\\varepsilon$ $(\\varepsilon>0,$ ), there exists a positive integer $N$ , such that for all indices n greater than $N$ , the corresponding terms ${\\mathcal{F}}_{n},n\\geq N$ satisfy the inequality $|\\mathcal{F}_{n}-\\mathcal{G}|<\\varepsilon$ . ", "page_idx": 35}, {"type": "text", "text": "Based on Definition F.1, if a sequence is either monotonically increasing or monotonically decreasing and bounded (either bounded above or bounded below), then it is guaranteed to converge according ", "page_idx": 35}, {"type": "text", "text": "to the celebrated monotone convergence theorem (Section 3.14 in reference [45]). Based on this, the convergence of the \u2018Imputation\u2019 part (step 7 of Algorithm 1) and DSM training part (step 5 of Algorithm 1) are proposed in the proceeding parts. ", "page_idx": 36}, {"type": "image", "img_path": "fIz8K4DJ7w/tmp/373edfcee8cfe49dcaa82d3f5e7eda441b4cf3211c9779406e97157ac9b84b40.jpg", "img_caption": ["Figure F.4: Evolution of evaluation metrics along iteration time $\\tau$ under MAR scenario at $30\\%$ missing rate. The shaded area indicates the $\\pm\\,1.0$ standard deviation uncertainty interval. "], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "F.5.1 Convergence Analysis of the Imputation Part ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "In this section, we explore the convergence of the imputation part as defined in step 7 of Algorithm 1 within our NewImp approach. Based on this, we first prove the following proposition for the convergence in the \u2018Imputation\u2019 part: ", "page_idx": 36}, {"type": "text", "text": "Proposition F.1. The convergence of the imputation part can be guaranteed, given that the discretization step size \u03b7 is small enough. ", "page_idx": 36}, {"type": "image", "img_path": "fIz8K4DJ7w/tmp/6c752e3068d0710cf685d50e33fe17e2089e44bed06050d1b7b63478f73c88a0.jpg", "img_caption": ["Figure F.5: Evolution of evaluation metrics along iteration time $\\tau$ under MCAR scenario at $30\\%$ missing rate. The shaded area indicates the $\\pm\\,1.0$ standard deviation uncertainty interval. "], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "Proof. First, let us reformulate the velocity field as follows: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u(X^{(\\mathrm{jistion})})}\\\\ &{=\\mathbb{E}_{\\boldsymbol{r}(\\mathcal{K}^{(\\mathrm{suen})})}\\left\\{\\begin{array}{c}{-\\lambda\\nabla_{\\tilde{\\boldsymbol{x}}^{(\\mathrm{suen})}}K(X^{(\\mathrm{gistion})},\\bar{X}^{(\\mathrm{gistion})})}\\\\ {+[\\nabla_{\\tilde{\\boldsymbol{x}}^{(\\mathrm{suen})}}\\log\\hat{\\boldsymbol{p}}(\\bar{X}^{(\\mathrm{gist})})]^{\\top}K(X^{(\\mathrm{gist})},\\bar{X}^{(\\mathrm{gistion})})\\right\\}}\\\\ {\\frac{(\\lambda\\nabla_{\\boldsymbol{\\theta}})}{\\lambda}\\mathbb{E}_{\\boldsymbol{r}(\\mathcal{K}^{(\\mathrm{Kuen})})}\\left\\{\\lambda[\\nabla_{\\boldsymbol{X}^{(\\mathrm{suen})}}\\log(\\bar{X}^{(\\mathrm{gistion})})]^{\\top}K(X^{(\\mathrm{gistion})},\\bar{X}^{(\\mathrm{gistion})})\\right\\}}\\\\ {+[\\nabla_{\\boldsymbol{\\theta}}\\log\\hat{\\boldsymbol{p}}(\\bar{X}^{(\\mathrm{gistion})})]^{\\top}K(X^{(\\mathrm{gistion})},\\bar{X}^{(\\mathrm{gistion})})\\right\\}}\\\\ {=\\int r(\\bar{X}^{(\\mathrm{gistion})})\\left\\{\\begin{array}{c}{\\lambda\\nabla_{\\tilde{\\boldsymbol{x}}^{(\\mathrm{suen})}}\\log r(\\bar{X}^{(\\mathrm{gistion})})}\\\\ {+\\nabla_{\\boldsymbol{\\theta}}\\frac{{{\\dot{\\gamma}}^{(\\mathrm{suen})}}\\log{\\hat{\\boldsymbol{p}}(\\bar{X}^{(\\mathrm{gistion})})}}{r}\\right\\}^{\\top}K(X^{(\\mathrm{gistion})},\\bar{X}^{(\\mathrm{gistion})})\\bar{X}^{(\\mathrm{gistion})}}\\\\ {-\\int\\left\\{\\begin{array}{c}{\\lambda\\nabla_{\\boldsymbol{\\theta}}\\omega\\log r(\\bar{X}^{(\\mathrm{gistion})})}\\\\ {+\\nabla_{\\boldsymbol{\\theta}}\\frac{{{\\dot{\\gamma}}^{(\\mathrm{suen})}}\\log{\\hat{\\boldsymbol{p}}(\\bar{X}^{(\\mathrm{gistion})})}}{r}\\right\\}^{\\top}K(X^{(\\mathrm{gistion})},\\bar \n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where (i) is based on integration by parts. ", "page_idx": 37}, {"type": "text", "text": "Based on this reformulation, the inner product can be given as follows: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathrm{d}F_{\\mathrm{joint.}\\mathrm{NER}}}{\\mathrm{d}\\tau}}\\\\ &{=\\int\\bigg\\langle\\nabla_{X^{(\\mathrm{mins})}}\\frac{\\delta{\\mathcal F}_{\\mathrm{joint.}\\mathrm{NER}}}{\\delta r(X^{(\\mathrm{joint})})},u(X^{(\\mathrm{joint})})\\bigg\\rangle\\mathrm{d}r(X^{(\\mathrm{min})})}\\\\ &{=\\int\\Biggl\\{\\lambda\\nabla_{\\tilde{X}^{(\\mathrm{mins})}}\\log r(\\tilde{X}^{(\\mathrm{min})})\\Biggr\\}^{\\top}K(X^{(\\mathrm{joint})},\\tilde{X}^{(\\mathrm{joint})})\\times}\\\\ &{=\\iint\\Bigg\\{\\mid\\nabla_{\\tilde{X}^{(\\mathrm{mins})}}\\log\\hat{p}(\\tilde{X}^{(\\mathrm{joint})})\\Biggr\\}^{\\top}K(X^{(\\mathrm{joint})},\\tilde{X}^{(\\mathrm{joint})})\\times}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\left\\{\\lambda\\nabla_{X^{(\\mathrm{min})}}\\log r(X^{(\\mathrm{joint})})\\right\\}\\mathrm{d}r(\\tilde{X}^{(\\mathrm{joint})})\\mathrm{d}r(X^{(\\mathrm{joint})})}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\beta(X^{(\\mathrm{joint})})}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "equation", "text": "$$\n\\stackrel{\\mathrm{(i)}}{\\geq}0,\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where the (i) is predicated on the requirement that the kernel function, $K(\\cdot,\\cdot)$ , is semi-positive definite. Consequently, according to the abovementioned derivation, we can conclude that the evolution of $\\mathcal{F}_{\\mathrm{joint-NER}}$ is monotonic increasing along $\\tau$ . Furthermore, $\\mathcal{F}_{\\mathrm{joint-NER}}$ satisfies the following inequality: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{F}_{\\mathrm{joint-NER}}}\\\\ &{\\leq\\!\\mathcal{F}_{\\mathrm{joint-NER}}-(\\lambda+1)\\mathbb{E}_{r(\\boldsymbol{X}^{\\mathrm{(joint)}})}[\\log r(\\boldsymbol{X}^{\\mathrm{(joint)}})]}\\\\ &{=-\\mathbb{D}_{\\mathrm{KL}}\\left[r(\\boldsymbol{X}^{\\mathrm{(joint)}})\\lVert\\hat{p}(\\boldsymbol{X}^{\\mathrm{(joint)}})\\right]}\\\\ &{\\leq\\!0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "which indicates that $\\mathcal{F}_{\\mathrm{joint-NER}}$ is upper-bounded by 0. ", "page_idx": 38}, {"type": "text", "text": "According to Eqs. (F.7) and (F.8), the cost functional $\\mathcal{F}_{\\mathrm{joint-NER}}$ , driven by the velocity field $u(X^{(\\mathrm{joint})})$ along $\\tau$ , converges. Building on this, employing a smaller step size $\\eta$ results in the iteration curve of $\\mathcal{F}_{\\mathrm{joint-NER}}$ more closely approximating the ODE defined in Eq. (F.7). Consequently, a smaller $\\eta$ leads to a sequence where $\\mathcal{F}_{\\mathrm{joint-NER}}$ monotonically increases, aligning with the theoretical expectations of the ODE behavior. \u53e3 ", "page_idx": 38}, {"type": "text", "text": "Unfortunately, directly obtaining $\\mathcal{F}_{\\mathrm{joint-NER}}$ is intractable. Nevertheless, we can still observe the changes in WASS and MAE across iteration time $\\tau$ to demonstrate the convergence of the \u2019Impute\u2019 part. To this end, we present the convergence trends along $\\tau$ in Figs. F.4 to F.6. These figures illustrate that both MAE and WASS generally decrease as the iteration epochs increase and eventually stabilize after $\\tau=250$ . This observed behavior supports our theoretical findings regarding the convergence of the \u2018Imputation\u2019 part. ", "page_idx": 38}, {"type": "image", "img_path": "fIz8K4DJ7w/tmp/3d9134800f30251f23bd93df1d0110f249e50b800ea9b549f8684085eef23346.jpg", "img_caption": ["Figure F.6: Evolution of evaluation metrics along iteration time $\\tau$ under MNAR scenario at $30\\%$ missing rate. The shaded area indicates the $\\pm\\,1.0$ standard deviation uncertainty interval. "], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "F.5.2 Convergence Analysis of the DSM Training ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Similarly, we can also give the proposition of the DSM training algorithm located in step 5 of Algorithm 1, and summarized in Algorithm 3: ", "page_idx": 39}, {"type": "text", "text": "Proposition F.2. The convergence of the DSM training algorithm can be guaranteed, given that the learning rate lr is small enough. ", "page_idx": 39}, {"type": "text", "text": "Proof. In the beginning, let us reformulate the parameter learning procedure of the DSM training algorithm as follows: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\theta_{\\tau+1}=\\theta_{\\tau}-l r\\times\\nabla_{\\theta}\\mathcal{L}_{\\mathrm{DSM}}|_{\\theta=\\theta_{\\tau}},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "which can be further reformulated as follows: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\frac{\\theta_{\\tau+1}-\\theta_{\\tau}}{l r}=-\\nabla_{\\theta}\\mathcal{L}_{\\mathrm{DSM}}|_{\\theta=\\theta_{\\tau}}}\\\\ &{\\Rightarrow\\displaystyle\\operatorname*{lim}_{l r\\rightarrow0}\\frac{\\theta_{\\tau+1}-\\theta_{\\tau}}{l r}=-\\nabla_{\\theta}\\mathcal{L}_{\\mathrm{DSM}}|_{\\theta=\\theta_{\\tau}}}\\\\ &{\\Rightarrow\\displaystyle\\frac{\\mathrm{d}\\theta}{\\mathrm{d}\\tau}=-\\nabla_{\\theta}\\mathcal{L}_{\\mathrm{DSM}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Meanwhile, note that: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathcal{L}_{\\mathrm{DSM}}}{\\mathrm{d}\\tau}=\\left\\langle\\nabla_{\\theta}\\mathcal{L}_{\\mathrm{DSM}},\\frac{\\mathrm{d}\\theta}{\\mathrm{d}\\tau}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Plugging Eq. (F.10) into Eq. (F.11), we can get the following result: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathcal{L}_{\\mathrm{DSM}}}{\\mathrm{d}\\tau}=-\\left\\langle\\nabla_{\\theta}\\mathcal{L}_{\\mathrm{DSM}},\\nabla_{\\theta}\\mathcal{L}_{\\mathrm{DSM}}\\right\\rangle\\leq0,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "which indicates that the iterative procedure for $\\mathcal{L}_{\\mathrm{DSM}}$ is monotonic decreasing along $\\tau$ . ", "page_idx": 39}, {"type": "text", "text": "Finally, recall Eq. (15), we can know that the following condition holds: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{DSM}}\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Building on this, employing a smaller step size $l r$ results in the iteration curve of $\\mathcal{L}_{\\mathrm{DSM}}$ more closely approximating the ODE defined in Eq. (F.11). Consequently, a smaller $l r$ leads to a sequence where $\\mathcal{L}_{\\mathrm{DSM}}$ monotonically decreases, aligning with the theoretical expectations of the ODE behavior. ", "page_idx": 40}, {"type": "text", "text": "Based on this proposition, we plot the evolution of $\\mathcal{L}_{\\mathrm{DSM}}$ along time $\\tau$ in Fig. F.7. These figures illustrate that the $\\mathcal{L}_{\\mathrm{DSM}}$ generally decreases as the iteration epochs increase. This observed behavior supports our theoretical findings regarding the convergence of the DSM training algorithm. ", "page_idx": 40}, {"type": "image", "img_path": "fIz8K4DJ7w/tmp/fcf7abbfdb7edb423b266a3c98ad06151c1151a0d3b474ffa73f1e55696c69b4.jpg", "img_caption": ["(u) $\\mathcal{L}_{\\mathrm{DSM}}$ , MNAR, IS. (v) $\\mathcal{L}_{\\mathrm{DSM}}$ , MNAR, PK. (w) $\\mathcal{L}_{\\mathrm{DSM}}$ , MNAR, QB. (x) $\\mathcal{L}_{\\mathrm{DSM}}$ , MNAR, WQW. Figure F.7: Evolution of $\\mathcal{L}_{\\mathrm{DSM}}$ , the loss function of \u2018Estimation\u2019 part along iteration time $\\tau$ at $30\\%$ missing rate. The shaded area indicates the $\\pm\\:1.0$ standard deviation uncertainty interval. The results of $\\mathcal{L}_{\\mathrm{DSM}}$ are smoothed by exponential moving average with $\\alpha=0.60$ . "], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "To further substantiate the rigor of our manuscript and demonstrate the efficacy of the proposed NewImp approach, we conduct downstream task comparisons as detailed in this subsection. Initially, we evaluate the classification performance on imputed data using the following protocol: 1) Selection of datasets with non-binary labels. 2) Post-imputation, we train a support vector machine equipped with an RBF kernel and an automatic kernel coefficient. We assess the model\u2019s performance using 5-fold cross-validation, reporting both the mean and standard deviation of the accuracies across 10 runs with different random seeds. In this procedure, we select classification accuracy as our evaluation metric. 3) Additionally, we include the accuracy of ground-truth data for reference. The comparative results are presented in Table F.8. From Table F.8, it can be seen that the NewImp approach generally has the best performance among all baseline models, this phenomenon reflects the superiority of the proposed NewImp approach in a further way. ", "page_idx": 41}, {"type": "table", "img_path": "fIz8K4DJ7w/tmp/849b5856767693247e3d49ad24534c76014aa96394304748a832e8f4604b2cbe.jpg", "table_caption": ["Table F.8: Classification accuracy results at $30\\%$ missing rate. "], "table_footnote": ["Kindly Note: The best results are bolded and the second best results are underliend. \u201c\\*\u201d marks the results that NewImp significantly outperform with $p$ -value $<\\,0.05$ over paired samples $t$ -test. "], "page_idx": 41}, {"type": "text", "text": "Moreover, we also conduct downstream regression task comparisons as detailed in this subsection. Initially, we evaluate the regression performance on imputed data using the following protocol: 1) Selection of datasets with continuous outcome variables. 2) After imputation, we train a support vector regression model equipped with an RBF kernel and an automatic kernel coefficient. We assess the model\u2019s performance using 5-fold cross-validation. In this procedure, we report both the mean and standard deviation of the mean squared errors (MSE) and mean absolute error (MAE) across 10 runs with different random seeds. 3) Additionally, we include the MSE and MAE on ground-truth data for reference. The comparative results are presented in Table F.9. As indicated in these results, the NewImp approach consistently outperforms most of the baseline models, further validating its superiority. ", "page_idx": 41}, {"type": "table", "img_path": "fIz8K4DJ7w/tmp/aa8caf99a9417adefec9f5a6dd99c6744c1e34fe3208902abd15764c6f6c53c0.jpg", "table_caption": ["Table F.9: Comparison results on the regression task with $30\\%$ missing rate. "], "table_footnote": ["Kindly Note: The best results are bolded and the second best results are underliend. \u201c\\*\u201d marks the results that NewImp significantly outperform with $p$ -value $<0.05$ over paired samples $t$ -test. "], "page_idx": 42}, {"type": "text", "text": "F.7 Baseline Comparison Vary Different Missing Rates and Scenarios ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "In this section, we further present the extended analysis of model performance across varying missing data rates, as detailed in Tables G.1, G.3, G.5 and G.7, and the corresponding standard deviation error results are presented in Tables G.2, G.4, G.6 and G.8. From the comparison results, it can be seen that our NewImp approach generally perform well compared to most of baseline models. This phenomenon reflects that the proposed NewImp approach is robust to various missing rates, and further proves its applicability. ", "page_idx": 42}, {"type": "text", "text": "Appendix G Limitations & Future Directions and Broader Impact ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "G.1 Limitations & Future Directions ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "The limitations and future research directions of this work can be summarized as follows: ", "page_idx": 42}, {"type": "text", "text": "\u2022 Utilization of Kernel Function: During the derivation of the velocity field, we employ RKHS to ensure implementation easiness. However, this regularization term may impose restrictions on the velocity field\u2019s direction, potentially limiting imputation accuracy in high-dimensional settings. Additionally, the computational complexity tends to scale quadratically with dataset size increases. Exploring alternative regularization terms [13] to replace RKHS presents a promising direction. ", "page_idx": 42}, {"type": "text", "text": "\u2022 Training of Score Function: As discussed in Section F.4, the runtime of NewImp is predominantly governed by the DSM function. Investigating techniques to reduce the training costs of this part, such as employing sliced score matching [49], represents an intriguing area for future exploration. ", "page_idx": 42}, {"type": "text", "text": "\u2022 Wasserstein Gradient Flow Framework: The WGF framework currently operates as a first-order system where each sample is equally weighted. A critical advancement would be the incorporation of second-order systems, such as Hamiltonian dynamics [55, 61], and other gradient flows like Fisher-Rao gradient flow [69] that assign variable weights to samples. These adaptations aim to decrease computational times inherently. ", "page_idx": 42}, {"type": "text", "text": "\u2022 $\\mathbb{R}^{\\mathrm{D}}$ Support Assumption: In our manuscript, for ease of derivation, we assume that the distribution we model has support on the real number domain $\\mathbb{R}^{\\mathrm{D}}$ , which limits the direct application of NewImp to tabular data with categorical variables. This limitation can be alleviated by employing the mirror descent approach. Specifically, for a categorical variable with D categories, the distribution belongs to the Dirichlet distribution whose support lies on the simplex $\\Delta^{\\mathrm{D-\\overline{{1}}}}$ . On this basis, we can define the Bregman function as the entropy function: \u03a8(X) :=  jD=1 (Xj log Xj \u2212Xj) and apply mirror descent using this Bregman function to handle the categorical domain effectively. Notably, similar approaches have been successfully applied in works focusing on constrained domain sampling, as exemplified in [48]. We have implemented a comparable scheme in Section 3.1 and Appendix B. ", "page_idx": 42}, {"type": "text", "text": "G.2 Broader Impact Statement ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "MDI and DMs are pivotal areas within machine learning, each boasting a wide array of real-world applications. While numerous applications exist, this paper does not single out any specific ones; instead, it focuses on addressing fundamental challenges in these fields. This study significantly advances the application of DMs for MDI by tackling prevalent issues such as inaccurate imputation and challenging training processes. We believe that the insights garnered here can be applied to related domains, such as probabilistic time-series forecasting and image inpainting, where accuracy is often more critical than diversity in results. A common challenge across these domains is the nuanced need for precision over variety, which can lead to overlooked opportunities in model application and development. Our proposed method provides a fresh perspective on these tasks through an optimization lens. It evaluates the appropriateness of directly applying existing diffusion models to these tasks and, where necessary, proposes the derivation of novel algorithms. This approach not only enhances the understanding of the underlying mechanisms but also paves the way for more targeted and effective solutions in the future. ", "page_idx": 43}, {"type": "table", "img_path": "fIz8K4DJ7w/tmp/0db3587b39413e8d9977b5942cbe3e224ace1557e86d8f0a3e9d2f014a96317c.jpg", "table_caption": ["Table G.1: Performance of MAE and WASS metrics at $10\\%$ missing rate. "], "table_footnote": ["Kindly Note: The best results are bolded and the second best results are underliend. \u201c\\*\u201d marks the results that NewImp significantly outperform with $p$ -value $<0.05$ over paired samples $t$ -test. "], "page_idx": 43}, {"type": "table", "img_path": "fIz8K4DJ7w/tmp/4bf7308a554160bb42a8efce2ac90637cde214a93610ade90f54a9465a7f17b4.jpg", "table_caption": ["Table G.2: Standard deviation of MAE and WASS metrics at $10\\%$ missing rate. "], "table_footnote": [], "page_idx": 43}, {"type": "table", "img_path": "fIz8K4DJ7w/tmp/89a1213160ebff2046197e1181945086a85d4f6b925f07126fc3c03ef574455d.jpg", "table_caption": ["Table G.3: Performance of MAE and WASS metrics at $20\\%$ missing rate. "], "table_footnote": ["Kindly Note: The best results are bolded and the second best results are underliend. \u201c\\*\u201d marks the results that NewImp significantly outperform with $p$ -value $<0.05$ over paired samples $t$ -test. "], "page_idx": 44}, {"type": "table", "img_path": "fIz8K4DJ7w/tmp/916827e239cd9dbd27480780688cfd47b726f90e3b67eb1458cb8d5f7980f24e.jpg", "table_caption": ["Table G.4: Standard deviation of MAE and WASS metrics at $20\\%$ missing rate. "], "table_footnote": [], "page_idx": 44}, {"type": "table", "img_path": "fIz8K4DJ7w/tmp/e8f5c724b7dde82ad922c8d4f5c6a23f25a2c151675aef8e5a4df801a44e54c1.jpg", "table_caption": ["Table G.5: Performance of MAE and WASS metrics at $40\\%$ missing rate. "], "table_footnote": ["Kindly Note: The best results are bolded and the second best results are underliend. \u201c\\*\u201d marks the results that NewImp significantly outperform with $p$ -value $<0.05$ over paired samples $t$ -test. "], "page_idx": 45}, {"type": "table", "img_path": "fIz8K4DJ7w/tmp/aa328d6f10dc851db3f541b9da9ac7f3204e982e42974cbba40f71020939db9c.jpg", "table_caption": ["Table G.6: Standard deviation of MAE and WASS metrics at $40\\%$ missing rate. "], "table_footnote": [], "page_idx": 45}, {"type": "table", "img_path": "fIz8K4DJ7w/tmp/6bef2bc74f3b07fd259f7e7045f1abdae06089217e281550a5d4653da9527481.jpg", "table_caption": ["Table G.7: Performance of MAE and WASS metrics at $50\\%$ missing rate. "], "table_footnote": ["Kindly Note: The best results are bolded and the second best results are underliend. \u201c\\*\u201d marks the results that NewImp significantly outperform with $p$ -value $<0.05$ over paired samples $t$ -test. "], "page_idx": 46}, {"type": "table", "img_path": "fIz8K4DJ7w/tmp/b24b76aed4523900251009971f77d79b4cb68766affcee1777d8f94469e6db83.jpg", "table_caption": ["Table G.8: Standard deviation of MAE and WASS metrics at $50\\%$ missing rate. "], "table_footnote": [], "page_idx": 46}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect our paper\u2019s contributions and scope. We restrict our application in missing value imputation task in numerical tabular, and our analysis is mainly focused on diffusion models, where the score function is required. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 47}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: Our limitations are listed in Appendix G.1 ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 47}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: To uphold the rigor of our manuscript, we provide all proofs of our proposition as outlined in Appendix C. Besides, all theorems are properly cited in the manuscript. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 48}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: We attempt to list all hyperparameters in Appendices D and E to ensure reproducibility. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 48}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: We used the open access UCI datasets, and we uploaded our algorithm in this github link https://github.com/JustusvLiebig/NewImp. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 49}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Justification: We have included all detailed information in Appendices D and E. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 49}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: In Tables F.3 to F.9 and G.1 to G.8, and Figs. F.4 to F.7 , we report standard deviation errors suitably and correctly defined or other appropriate information about the statistical significance and error bar of the experiments. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 49}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 50}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: The required information is given in Appendices E.2 and F.4. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 50}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 50}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: Since it is an algorithm-oriented research, there is no societal impact of the work performed. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 50}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 51}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: This paper does not involve the safeguards issue. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 51}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: The creators or original owners of assets (e.g., code, data, models), used in the paper, are properly credited. The license and terms of use are explicitly mentioned and properly respected. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 51}, {"type": "text", "text": "", "page_idx": 52}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: We do not not release new assets in this manuscript. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 52}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: Our experiments did not involve \u2018Crowdsourcing and Research with Human Subjects\u2019. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 52}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: Our answer is NA since our paper does not involve crowdsourcing or research with human subjects. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 52}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 53}]