[{"figure_path": "YlIvhHFwQ2/figures/figures_1_1.jpg", "caption": "Figure 1: DreamScene4D extends video-to-4D generation to multi-object videos with fast motion. We present rendered images and the corresponding motions from diverse viewpoints at different timesteps using real-world DAVIS [37] videos with multiple objects and large motions.", "description": "This figure demonstrates the capabilities of DreamScene4D in generating 3D dynamic scenes from monocular videos. It shows two example videos. The left side shows a person kicking a soccer ball, and the right side shows a person walking a dog. For each video, the top row displays the input frames, and the following rows depict 360\u00b0 novel view synthesis results, illustrating the method's ability to generate realistic and consistent 3D scene representations across different viewpoints and timesteps. The generated views include both the reference viewpoint and novel viewpoints.", "section": "1 Introduction"}, {"figure_path": "YlIvhHFwQ2/figures/figures_3_1.jpg", "caption": "Figure 2: Method overview for DreamScene4D: (a) We first decompose and amodally complete each object and the background in the video sequence and use DreamGaussian [49] to obtain static 3D Gaussian representation. (b) Next, we factorize and optimize the motion of each object track independently, detailed in Figure 3. (c) Finally, we use the estimated monocular depth to recompose the independently optimized 4D Gaussians into one unified coordinate frame.", "description": "This figure shows a high-level overview of the DreamScene4D method. It's broken down into three stages: (a) Video Scene Decomposition and Obj-Centric 3D Lifting, (b) 3D Motion Factorization, and (c) 4D Scene Composition. In the first stage, the input video frames are decomposed into individual objects and the background. These components are then amodally completed and lifted into 3D Gaussian representations. The second stage involves factorizing object motion into camera motion, object-centric deformations, and object-to-world frame transformations. These motion components are optimized independently. Finally, the third stage involves recomposing these optimized 4D Gaussians into a unified coordinate frame using monocular depth information, yielding a complete 4D scene representation.", "section": "3 Approach"}, {"figure_path": "YlIvhHFwQ2/figures/figures_4_1.jpg", "caption": "Figure 3: 3D Motion Factorization. The 3D motion is decomposed into 3 components: 1) the object-centric deformation, 2) the camera motion, and 3) the object-centric to-world frame transformation. After optimization, they can be composed to form the original object motion observed in the video.", "description": "This figure illustrates the three components into which DreamScene4D factorizes 3D motion for improved stability and quality in motion optimization.  These components are: object-centric deformation (changes in object shape and form), camera motion (movement of the camera itself), and object-centric to world frame transformation (changes in object position and orientation within the scene).  The process starts with 3D Gaussians representing the object, which are then optimized using a combination of rendering losses and score distillation sampling (SDS) along with constraints such as flow rendering loss, scale regularization, and rigidity.  These optimized components are then combined to reconstruct the original object motion from the video.", "section": "3.2.3 Modeling Complex 3D Motions via Motion Factorization"}, {"figure_path": "YlIvhHFwQ2/figures/figures_6_1.jpg", "caption": "Figure 4: Video to 4D Comparisons. We render the Gaussians at various timesteps and camera views. We denote Motion Factorization as MF and Video Scene Decomposition as VSD. Our method produces consistent and faithful renders for fast-moving objects, while DreamGaussian4D [43] (2nd row) and Consistent4D [17] (1st row) produce distorted 3D geometry, blurring, or broken artifacts. Refer to our Supp. Materials for extensive qualitative comparisons.", "description": "This figure compares the video-to-4D generation results of DreamScene4D against several baselines (Consistent4D and DreamGaussian4D).  It shows that DreamScene4D produces more realistic and consistent 4D scene generation results, especially for videos with fast-moving objects. The other methods show artifacts like distortions, blurring, and broken objects.", "section": "4.1 Video to 4D Scene Generation"}, {"figure_path": "YlIvhHFwQ2/figures/figures_8_1.jpg", "caption": "Figure 5: Grouping visualization of Gaussians. The grouping of the point cloud is visualized as colored point clouds from different camera views. The spatial relationships between objects are preserved after the composition.", "description": "This figure visualizes the grouping of Gaussians from different viewpoints (bird's eye view, frontal view, and side view) at different time steps (T=0, T=t1, T=t2, T=t3). The spatial relationships between objects are maintained after the composition process, demonstrating the effectiveness of the proposed method in preserving the scene structure.", "section": "4 Experiments"}, {"figure_path": "YlIvhHFwQ2/figures/figures_9_1.jpg", "caption": "Figure 6: Motion Comparisons. The 2D projected motion of Gaussians accurately aligns with dynamic human motion trajectory in the video, where the point trajectories estimated by PIPS++ [65] tend to get \"stuck\" in the background wall. For CoTracker [18], partial point trajectories are mixed up, where some points in the chest region (yellow/green) ending up in the head area (red). aligned with the estimated depth. (3) Despite the inpainting, the Gaussians are still under-constrained when heavy occlusions happen, and artifacts may occur. (4) Our runtime scales linearly with the number of objects and can be slow for complex videos. Addressing these limitations by pursuing more data-driven ways for video to 4D generation is a direct avenue of our future work.", "description": "This figure compares the 2D projected motion of Gaussians generated by DreamScene4D against two state-of-the-art methods, PIPS++ and CoTracker, on a video of a person skateboarding.  DreamScene4D demonstrates accurate alignment with the actual motion, while PIPS++ and CoTracker exhibit inaccuracies, such as points \"getting stuck\" or being misaligned.", "section": "4.2 4D Gaussian Motion Accuracy"}, {"figure_path": "YlIvhHFwQ2/figures/figures_14_1.jpg", "caption": "Figure 7: User survey interface. A GUI example of what an Amazon Turk worker would see as part of the user preference study.", "description": "This figure shows the graphical user interface (GUI) used in a user study conducted on Amazon Mechanical Turk (AMT).  The AMT workers were presented with three video clips for comparison: the original video and two novel view videos (orbit videos A and B) generated from different viewpoints.  They were then asked to choose which of the orbit views best represents the original video, or to indicate if they viewed the orbit views as equally good representations.  This figure is an example of the screen presented to the AMT workers, illustrating the simple user interface and the choices they were presented with.", "section": "4.1 Video to 4D Scene Generation"}, {"figure_path": "YlIvhHFwQ2/figures/figures_15_1.jpg", "caption": "Figure 8: Gaussian Motion Visualizations. We visualize the Gaussian trajectories in the reference view corresponding to the video as well as in multiple novel views. The rendered Gaussians are sampled independently for each view. DreamScene4D can produce accurate motion in different camera poses w/o explicit point trajectory supervision.", "description": "This figure shows the visualization of Gaussian motion trajectories. The top row displays the reference view of the video, showing the motion of the object. The bottom two rows depict novel views, generated by the model. The consistency of the trajectories across different viewpoints highlights the model's ability to produce accurate motion without explicit supervision.", "section": "4D Gaussian Motion Accuracy"}, {"figure_path": "YlIvhHFwQ2/figures/figures_16_1.jpg", "caption": "Figure 9: Video Amodal Completion Comparisons. Spatiotemporal self-attention and Consistency Guidance both help to preserve the identity consistency of the inpainted objects.", "description": "This figure compares the performance of different video inpainting methods.  The left column shows the results of inpainting a video of a lizard, while the right column shows the results of inpainting a video of parrots.  Each row represents a different method: SD-Inpaint (a baseline method), Ours (without spatiotemporal self-attention), Ours (without consistency guidance), and Ours (our full method).  The figure demonstrates that incorporating spatiotemporal self-attention and consistency guidance significantly improves the quality of the inpainted videos, resulting in more coherent and realistic results.", "section": "A.1 Video Amodal Completion"}, {"figure_path": "YlIvhHFwQ2/figures/figures_17_1.jpg", "caption": "Figure 4: Video to 4D Comparisons. We render the Gaussians at various timesteps and camera views. We denote Motion Factorization as MF and Video Scene Decomposition as VSD. Our method produces consistent and faithful renders for fast-moving objects, while DreamGaussian4D [43] (2nd row) and Consistent4D [17] (1st row) produce distorted 3D geometry, blurring, or broken artifacts. Refer to our Supp. Materials for extensive qualitative comparisons.", "description": "This figure compares the 4D scene generation results of DreamScene4D with two state-of-the-art baselines (Consistent4D and DreamGaussian4D) across multiple challenging videos with fast object motion.  The comparison highlights DreamScene4D's ability to generate consistent and realistic 4D scene representations, in contrast to the baselines, which show artifacts like distorted 3D geometry, blurring, and broken objects.  DreamScene4D's superior performance stems from its novel motion factorization and video scene decomposition strategies.", "section": "4.1 Video to 4D Scene Generation"}, {"figure_path": "YlIvhHFwQ2/figures/figures_17_2.jpg", "caption": "Figure 11: Mitigating the parallax effect. A small amount of joint fine-tuning steps can help mitigate the parallax effect and align the rendered Gaussians to the input video frames.", "description": "This figure shows the effect of a small amount of joint fine-tuning steps on mitigating the parallax effect and aligning the rendered 3D Gaussians to the input video frames.  The top row shows the result without this fine-tuning, while the bottom row demonstrates how the process improves the alignment, resulting in a more realistic rendering.", "section": "3.2.3 Modeling Complex 3D Motions via Motion Factorization"}]