[{"figure_path": "xRdpCOdghl/tables/tables_7_1.jpg", "caption": "Table 1: Comparison with other sampling methods. Due to stratified sampling limitations, the results are marked in grey. Top and second-best performances are bolded and underlined, respectively, excluding stratified sampling. Metrics represent mean accuracy and standard deviation over five independent runs.", "description": "This table compares the performance of RDSS with other sampling methods (Stratified, Random, k-Means, USL, ActiveFT) on multiple datasets (CIFAR-10, CIFAR-100, SVHN, STL-10) and under various annotation budget settings.  Results are presented for two different semi-supervised learning frameworks (FlexMatch and FreeMatch).  The table highlights RDSS's superior accuracy and shows that stratified sampling, while theoretically appealing, is impractical in real-world scenarios.", "section": "7.2 Comparison with Other Sampling Methods"}, {"figure_path": "xRdpCOdghl/tables/tables_8_1.jpg", "caption": "Table 1: Comparison with other sampling methods. Due to stratified sampling limitations, the results are marked in grey. Top and second-best performances are bolded and underlined, respectively, excluding stratified sampling. Metrics represent mean accuracy and standard deviation over five independent runs.", "description": "This table compares the performance of RDSS against several other sampling methods (Stratified, Random, k-Means, USL, ActiveFT) across two different semi-supervised learning frameworks (FlexMatch and FreeMatch). The results are presented for different annotation budget settings and across several datasets (CIFAR-10, CIFAR-100, SVHN, STL-10).  The table highlights the superior performance of RDSS, especially under low budget conditions.", "section": "7.2 Comparison with Other Sampling Methods"}, {"figure_path": "xRdpCOdghl/tables/tables_8_2.jpg", "caption": "Table 2: Comparison with AL approaches under Supervised Learning (SL) paradigm. The best performance is bold and the second best performance is underlined.", "description": "This table compares the performance of the proposed RDSS method against other Active Learning (AL) approaches under the supervised learning setting.  The results are shown for both CIFAR-10 and CIFAR-100 datasets, with different annotation budgets.  The best and second-best accuracies among all the compared methods (including RDSS) are highlighted.  The \"Whole Dataset\" row shows the performance when using the full dataset for training, providing a context for evaluating the effectiveness of different sample selection techniques.", "section": "7.3 Comparison with AL/SSAL Approaches"}, {"figure_path": "xRdpCOdghl/tables/tables_8_3.jpg", "caption": "Table 1: Comparison with other sampling methods. Due to stratified sampling limitations, the results are marked in grey. Top and second-best performances are bolded and underlined, respectively, excluding stratified sampling. Metrics represent mean accuracy and standard deviation over five independent runs.", "description": "This table compares the performance of RDSS with other sampling methods (Stratified, Random, k-Means, USL, and ActiveFT) across different annotation budgets on CIFAR-10, CIFAR-100, SVHN, and STL-10 datasets, using both FlexMatch and FreeMatch as SSL frameworks.  It highlights RDSS's superior accuracy, especially under low-budget settings, and demonstrates the limitations of stratified sampling in real-world scenarios.", "section": "7.2 Comparison with Other Sampling Methods"}, {"figure_path": "xRdpCOdghl/tables/tables_18_1.jpg", "caption": "Table 1: Comparison with other sampling methods. Due to stratified sampling limitations, the results are marked in grey. Top and second-best performances are bolded and underlined, respectively, excluding stratified sampling. Metrics represent mean accuracy and standard deviation over five independent runs.", "description": "This table compares the performance of different sample selection methods (Random, k-Means, USL, ActiveFT, and RDSS) when integrated with two state-of-the-art semi-supervised learning frameworks (FlexMatch and FreeMatch).  The results are presented for various datasets (CIFAR-10, CIFAR-100, SVHN, STL-10) and annotation budgets. The table highlights RDSS's superior performance across different datasets and budget settings, especially when compared to other methods.", "section": "7.2 Comparison with Other Sampling Methods"}, {"figure_path": "xRdpCOdghl/tables/tables_19_1.jpg", "caption": "Table 1: Comparison with other sampling methods. Due to stratified sampling limitations, the results are marked in grey. Top and second-best performances are bolded and underlined, respectively, excluding stratified sampling. Metrics represent mean accuracy and standard deviation over five independent runs.", "description": "This table compares the performance of the proposed RDSS method with several other sampling methods (Stratified, Random, k-Means, USL [50], and ActiveFT [57]) across different annotation budget settings.  The results are shown for two different semi-supervised learning frameworks (FlexMatch and FreeMatch) and multiple datasets.  It demonstrates RDSS's superior performance, especially in low-budget scenarios.", "section": "7.2 Comparison with Other Sampling Methods"}]