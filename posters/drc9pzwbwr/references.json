{"references": [{"fullname_first_author": "Karl Cobbe", "paper_title": "Training verifiers to solve math word problems", "publication_date": "2021-10-14", "reason": "This paper introduces a novel approach to solving math word problems using verifier models, which is highly relevant to the current research on improving LLMs' reasoning capabilities."}, {"fullname_first_author": "Aman Madaan", "paper_title": "Self-refine: Iterative refinement with self-feedback", "publication_date": "2023-03-17", "reason": "This paper proposes a self-refinement technique where LLMs iteratively refine their responses using self-feedback, which is directly related to the current paper's focus on enabling LLMs to self-improve."}, {"fullname_first_author": "Jie Huang", "paper_title": "Large language models cannot self-correct reasoning yet", "publication_date": "2023-10-01", "reason": "This paper investigates the limitations of LLMs in self-correcting reasoning, which provides a crucial background and motivation for the current research that aims to overcome these limitations."}, {"fullname_first_author": "Xue Bin Peng", "paper_title": "Advantage-weighted regression: Simple and scalable off-policy reinforcement learning", "publication_date": "2019-10-00", "reason": "This paper introduces the reward-weighted regression algorithm, which is used in the current research for training the LLM agent to improve itself over multiple turns."}, {"fullname_first_author": "Stephane Ross", "paper_title": "A reduction of imitation learning and structured prediction to no-regret online learning", "publication_date": "2011-04-11", "reason": "This paper establishes a connection between imitation learning and online learning, which is relevant to the current research as it uses online imitation learning principles to guide the self-improvement process of the LLM."}]}