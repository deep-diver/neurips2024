[{"type": "text", "text": "RECURSIVE INTROSPECTION: Teaching Language Model Agents How to Self-Improve ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuxiao $\\mathbf{Qu}^{1}$ , Tianjun Zhang2, Naman Garg3, Aviral Kumar1 1Carnegie Mellon University, 2UC Berkeley, 3MultiOn Website: https://cohenqu.github.io/rise.github.io/, Email: yuxiaoq@andrew.cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A central piece in enabling intelligent agentic behavior in foundation models is to make them capable of introspecting upon their behavior, reasoning, and correcting their mistakes as more computation or interaction is available. Even the strongest proprietary large language models (LLMs) do not quite exhibit the ability of continually improving their responses sequentially. In this paper, we develop RISE: Recursive IntroSpEction, an approach for fine-tuning LLMs to introduce this capability, despite prior work hypothesizing that this capability may not be possible to attain. Our approach prescribes an iterative fine-tuning procedure, which attempts to teach the model how to alter its response after having executed previously unsuccessful attempts to solve a hard test-time problem, with optionally additional environment feedback. RISE poses fine-tuning for a singleturn prompt as solving a multi-turn Markov decision process (MDP), where the initial state is the prompt. Inspired by principles in online imitation and offline reinforcement learning, we propose strategies for multi-turn data collection and training so as to imbue an LLM with the capability to recursively detect and correct its previous mistakes in subsequent iterations. Our experiments show that RISE enables Llama2, Llama3, and Mistral models to improve themselves with more turns on reasoning tasks, outperforming several single-turn strategies given an equal amount of inference-time computation. We also find that RISE scales well, often attaining larger benefits with more capable models, without disrupting one-turn abilities as a result of expressing more complex distributions. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A promising approach to utilizing foundation models, and in particular, large language models (LLMs), is to treat them as general-purpose decision-making machines, or \u201dagents\u201d. To be successful, an LLM agent must not just provide plausible completions for input text, but must exhibit interactive, goal-directed behavior to accomplish a given task. Put in abstract terms, this requires mastering two qualities: (a) producing responses that explicitly seek information about the task, followed by (b) making decisions and improving them by \u201dthinking\u201d and verifying them at inference time. For instance, to succeed in using a new coding library, an effective LLM agent should first synthesize programs, then try the most promising subset against a compiler, use the resulting feedback to improve the program, and repeat the process for multiple turns. Having the ability to successfully improve a response in sequential attempts is equivalent to a form of \u201dself-improvement\u201d, at test time. ", "page_idx": 0}, {"type": "text", "text": "To enable test-time self-improvement, recent approaches attempt to repurpose the knowledge stored in pre-trained models via few-shot prompting [8, 17, 33, 54, 69]. Although prompt tuning in conjunction with feedback is effective in eliciting improved responses from capable models, it fails to produce models that can succeed in complex tasks by correcting their own mistakes, such as those that require logical reasoning [23, 59]. In many of these problems, models contain the \u201cknowledge\u201d needed to answer a challenging prompt, but fail to elicit that knowledge even when asked to sequentially correct their mistakes. Fine-tuning the LLM on domain-specific question-answering data [7, 31, 41] can help, but it still does not teach the agent a test-time improvement strategy (see Section 5). A strategy for improving responses over sequential attempts at test time is crucial for tackling challenging prompts, where directly attempting the problem in one shot may largely be futile. ", "page_idx": 0}, {"type": "image", "img_path": "DRC9pZwBwR/tmp/dd0d34b7a0111368d9c8c2cbdfc9f4989de80300b8b0add68c5e93294abe10e3.jpg", "img_caption": ["Figure 1: Recursive Introspection (RISE). Using multi-round training on on-policy rollouts and supervision from a reward function, RISE trains models that are capable of improving themselves over multiple turns. At inference, we run majority voting on candidate outputs from different turns to obtain the final response. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Can we train models to be capable of improving their own responses? If done correctly on a diverse set of problems and scenarios, this could introduce in an LLM, a general procedure for \u201chow\u201d it can tackle a hard prompt by improving itself as opposed to supervising it with \u201cwhat\u201d to respond with, which may not generalize as the test prompt becomes out of distribution. Although one approach to inducing this capability into a model would be to generate data that showcase improvements over multiple sequential turns (potentially from highly capable models), we find that simply imitating these data is not sufficient to enable this capability (Section 5.3). This is due to two reasons: First, multi-turn data from a different model would not show improvements in the kinds of errors the learner would make, thereby being irrelevant to the learner [26]. Second, often sequential multi-turn data collected from proprietary models is also not of high quality since these models are typically not good at proposing meaningful improvements to their own errors [23] even though they can still provide useful responses to the problem at hand. Therefore, we need a different strategy to endow models with a self-improvement capability. Our key insight is to supervise improvements to the learner\u2019s own responses in an iterative fashion, taking inspiration from methods in online imitation learning [38] and reinforcement learning (RL) [47]. This supervision can be in the form of oracle responses to the prompt sampled i.i.d. from more capable models, or be generated from the learner itself. ", "page_idx": 1}, {"type": "text", "text": "Our contribution, RISE: Recursive Introspection (Figure 1), utilizes these insights to improve the self-improvement capability of an LLM over multiple attempts at a given prompt. In each iteration, our approach bootstraps on-policy rollouts from the learner with better responses at the next turn obtained by running best-of-N (using a success indicator on the task) on multiple revision candidates obtained by sampling from the learner itself or using responses from a more capable model, whichever is more convenient. In this way, we construct rollouts that demonstrate the learner how it can improve its responses under its own distribution. Then, we fine-tune the learner on these data using a reward-weighted regression (RWR [36, 37]) objective, that learns from both high- and low-quality parts of such rollouts. By iteratively repeating this procedure, we are able to instill a general self-improvement capability into an LLM. Our results show that LLMs trained via RISE can produce correct responses on more prompts, improving over turns for more challenging prompts. ", "page_idx": 1}, {"type": "text", "text": "Even though strong base and intruction-tuned LLMs [25, 63] often fail to improve their responses over multiple sequential attempts (even when explicitly told about their mistakes previously), RISE successfully endows similarly-sized LLMs with self-improvement capabilities, resulting in monotonically increasing task performance after each turn. Specifically, on the GSM8K [12] dataset, RISE demonstrates significant improvement over various models. RISE improves the performance of LLaMa3-8B by $8.2\\%$ and Mistral-7B by $6.6\\%$ , entirely using their own data. RISE attains a $17.7\\%$ improvement for LLaMa2-7B over the course of 5-turn introspection (outperforming parallel sampling from the first turn), and a $23.9\\%$ improvement for Mistral-7B. In contrast, GPT-3.5 itself only improves by $4.6\\%$ over five turns. We see similar trends on the MATH dataset [20], where RISE improves LLaMa2-7B by $4.6\\%$ and Mistral-7B by $11.1\\%$ over five turns. These results consistently demonstrate RISE\u2019s effectiveness in enhancing reasoning capabilities for different models. ", "page_idx": 1}, {"type": "text", "text": "2 Problem Setup and Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The goal of our work is to improve LLM performance over sequential attempts / turns at a given problem. Concretely, given a dataset $\\pmb{\\mathcal{D}}=\\}((\\pmb{x}_{i},\\pmb{y}_{i}^{*})\\}_{i=1}^{N}$ of problems $\\pmb{x}_{i}$ and oracle responses $\\pmb{y}_{i}^{*}$ , our goal is to obtain an LLM $\\pi_{\\boldsymbol{\\theta}}(\\cdot|[\\mathbf{\\boldsymbol{x}},\\hat{\\boldsymbol{y}}_{1:t},p_{1:t}])$ that, given the problem $\\textbf{\\em x}$ , previous model attempts $\\hat{\\pmb{y}}_{1:t}$ at the problem, and auxiliary instructions $p_{1:t}$ (e.g., instruction to find a mistake and improve the response; or additional compiler feedback from the environment) solves a given problem as correctly as possible. To this end, we encode this goal into the following learning objective: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\boldsymbol{\\pi}_{\\boldsymbol{\\theta}}}\\;\\;\\sum_{i=1}^{L}\\mathbb{E}_{\\boldsymbol{x},\\boldsymbol{y}^{*}\\sim\\mathcal{D},\\hat{\\boldsymbol{y}}_{i}\\sim\\pi_{\\boldsymbol{\\theta}}(\\cdot|[\\boldsymbol{x},\\hat{\\boldsymbol{y}}_{1:i-1},\\boldsymbol{p}_{1:i-1}])}\\left[\\mathbb{I}\\left(\\hat{\\boldsymbol{y}}_{i}==\\boldsymbol{y}^{*}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Unlike standard supervised fine-tuning that trains the model $\\pi$ to produce a single response $\\hat{\\pmb y}$ given $\\textbf{\\em x}$ , Equation 2.1 trains $\\pi$ to also appropriately react to a given history of responses from its own previous attempts $\\hat{\\pmb y}_{1:i-1}$ . Equation 2.1 most closely resembles an RL objective, and we will indeed develop our approach by converting a single-turn problem into a multi-turn MDP. Finally, note that prompting-based methods such as Self-Refine [33] can still be viewed as training $\\pi$ to optimize $\\pi(\\pmb{y}^{\\ast}|\\pmb{x})$ but only when only allowed to modulate the prompt $p_{i}$ to optimize Equation 2.1. Naturally, since the parameters $\\theta$ are unchanged, this would not be effective in optimizing the objective fully. ", "page_idx": 2}, {"type": "text", "text": "3 RISE: Recursive Introspection for Self-Improvement ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Since even strong off-the-shelf models do not exhibit an effective ability to improve themselves when provided with sequential attempts at a given problem [23], a natural next step is to ask how to train models to induce this capability. In this section, we will develop our approach, RISE, for fine-tuning foundation models towards improving their own predictions over multiple turns. Our approach will first convert a problem into a multi-turn MDP, then collect data, and finally run offline reward-weighted supervised learning in this multi-turn MDP to induce this capability. ", "page_idx": 2}, {"type": "image", "img_path": "DRC9pZwBwR/tmp/2963babfe9072cdb98709ddac103b4006361f51b4cdaf6793149d2695015911b.jpg", "img_caption": ["Figure 2: Left: Problem formulation. We convert single-turn problems into multi-turn MDPs as discussed in Section 3.1. The state is given by the prompt, history of prior attempts, and optional feedback from the environment. An action is a response generated from the LLM given the state of multi-turn interaction so far. Right: Data collection. We collect data by unrolling the current model $k-1$ times followed by an improved version of the response, which is obtained by either (1) self-distillation: sample multiple responses from the current model, and use the best response, or (2) distillation: obtain oracle responses by querying a more capable model. In either case, RISE then trains on the generated data. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "3.1 Converting Single-Turn Problems into a Multi-Turn Markov Decision Process (MDP) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The first step in building our approach is to procedurally construct a multi-turn MDP out of a singleturn dataset of prompts and oracle responses (Figure 2, Left). Given a dataset, $\\boldsymbol{\\mathcal{D}}\\,=\\,\\{(\\mathbf{\\boldsymbol{x}}_{i},\\mathbfboldsymbol{y}_{i}^{*})\\}$ , consisting of prompts $\\pmb{x}_{i}$ and corresponding oracle responses $\\pmb{y}_{i}^{*}$ (e.g., math questions and natural language responses to those questions), we will construct an induced MDP $\\mathcal{M}$ from $\\mathcal{D}$ , and then learn policies in this MDP. An initial state in this MDP is a possible prompt $\\mathbf{\\boldsymbol{x}}_{i}\\in\\mathcal{D}$ . We denote the output response from the foundation model as action $\\textbf{\\em a}$ . Given a state $\\pmb{s}$ , the next state can be obtained by concatenating the tokens representing $\\pmb{s}$ with the action $\\textbf{\\em a}$ proposed by the model, and an additional fixed prompt $\\pmb{f}$ that asks the model to introspect, e.g., \u201cthis response is not correct, please introspect and correct your answer.\u201d (the exact prompt is shown in Appendix F.4). The reward function is a sparse binary indicator of answer correctness at a given state $\\pmb{s}$ , $r([\\pmb{x}_{i},\\cdot\\cdot\\cdot],\\pmb{a})=1$ if and only if $\\pmb{a}=\\pmb{y}_{i}^{*}$ and is obtained from an answer checking function. This construction from dataset $\\mathcal{D}$ to MDP $\\mathcal{M}$ is shown below: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c c l}{\\mathcal{D}=\\{(\\boldsymbol{x}_{i},\\boldsymbol{y}_{i}^{*})\\}}&{\\to}&{\\mathcal{M}:\\;\\rho(\\boldsymbol{s}_{0})=\\mathrm{Unif}(\\boldsymbol{x}_{1},\\boldsymbol{x}_{2},\\cdots,\\boldsymbol{x}_{N})}\\\\ &&{P(\\boldsymbol{s}^{\\prime}|\\boldsymbol{s},\\boldsymbol{a})=\\delta\\left(\\boldsymbol{s}^{\\prime}=\\mathrm{concat}[\\boldsymbol{s},\\boldsymbol{a},\\boldsymbol{f}]\\right)}\\\\ &&{r(\\boldsymbol{s},\\boldsymbol{a})=\\mathbf{1}\\left(\\boldsymbol{a}=\\boldsymbol{y}_{i}^{*}\\;\\mathrm{if}\\;\\boldsymbol{x}_{i}\\in\\boldsymbol{s}\\right).}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "3.2 Learning in the Multi-Turn MDP ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "With the MDP construction in place, the next step involves training a model to improve itself over the course of a rollout. We subscribe to an offilne approach to learning that we describe in the following. ", "page_idx": 3}, {"type": "text", "text": "Step 1: Data collection for self-improvement. To ensure that rollout data from this multi-turn MDP is useful for teaching the model how to self-improve, it must satisfy a few desiderata: (1) it must illustrate the mistakes that the learner is likely to make and showcase how to improve upon them in the next attempt, (2) the data must illustrate responses that are relevant to the model given the problem and previous attempts in context, and (3) it must not contain any rollout that degrades in a subsequent turn. Our data collection strategy (Figure 2, Right) satisfies these desiderata. ", "page_idx": 3}, {"type": "text", "text": "In a given round $k$ , for a given problem $\\pmb{x}_{i}$ , we unroll the current model $\\pi_{\\boldsymbol{\\theta}_{k}}(\\cdot|\\cdot)$ to produce multiple sequential attempts, denoted by $y_{t}^{i}\\sim\\pi_{\\theta_{k}}(\\cdot|s_{t}^{i})$ . In problems, where external input (e.g., compiler feedback) is available, we also observe a variable-length, natural language external input, $f_{t}^{i}$ (e.g., in math problems we ask the model to correct itself). We also observe a scalar reward value $r(s_{t}^{i},y_{t}^{i})$ , denoted as $\\boldsymbol{r}_{t}^{i}$ in short. Let us denote this dataset of \u201con-policy\u201d model rollouts as $\\mathcal{D}_{\\mathrm{on-policy}}\\::=$ $\\{(\\pmb{s}_{t}^{i},\\pmb{y}_{t}^{i},f_{t}^{i},r_{t}^{i})_{t=1}^{T}\\}$ . ", "page_idx": 3}, {"type": "text", "text": "For each time-step, we construct an improved version of the response $\\boldsymbol y_{t}^{i}$ that we will denote by $\\tilde{\\boldsymbol{y}}_{t}^{i}$ . We also record the reward score associated with this improved response as $r(s_{t}^{i},\\tilde{y}_{t}^{i})$ , or $\\tilde{r}_{t}^{i}$ in short. To obtain an improved version of a response $\\boldsymbol y_{t}^{i}$ , we can employ several strategies. Perhaps the most straightforward approach is to query an off-the-shelf more capable model to provide a correct response given the prompt $\\pmb{x}_{i}$ , the previous response $\\boldsymbol y_{t}^{i}$ , and an optional external feedback $f_{t}^{i}$ . We refer to this as the distillation variant of our approach, since it uses a strong \u201cteacher\u201d model to guide self-improvement (note that this is different from the classic notion of knowledge distillation, and we will in fact show results in Section 5.1 that will help understand the differences). ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{D}}_{\\mathrm{on-policy}+\\mathrm{distill}}:=\\left\\{\\left\\{\\left(s_{t}^{i},\\tilde{{\\boldsymbol y}}_{t}^{i},f_{t}^{i},\\tilde{r}_{t}^{i}\\right)\\right\\}_{t=1}^{T}\\right\\}_{i=1}^{|\\mathcal{D}|}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The second variant of our approach, which alleviates the need for a teacher model, involves constructing an improved response by sampling multiple times from the learner itself. We refer to this approach as the self-distillation variant. Concretely, for each state in the dataset, $\\pmb{s}_{t}^{i}\\in\\mathcal{D}_{\\mathrm{on-policy}}$ , we sample $N$ responses $\\tilde{\\pmb{y}}_{t}^{i}[0],\\tilde{\\pmb{y}}_{t}^{i}[1],\\cdot\\cdot\\cdot\\cdot,\\tilde{\\pmb{y}}_{t}^{i}[N]\\sim\\pi_{\\boldsymbol{\\theta}}(\\cdot|\\pmb{s}_{t}^{i})$ , and use the best response from these $N$ candidates (as measured by the associated reward values $\\tilde{r}_{t}^{i}[0],\\cdot\\cdot\\cdot\\,,\\tilde{r}_{t}^{i}[N])$ to relabel the model response at the next step $t+1$ in an improvement trajectory. Formally, say $\\begin{array}{r}{\\tilde{\\pmb{y}}_{t}^{i}[m]=\\arg\\operatorname*{max}_{j\\in[N]}r\\big(\\pmb{s}_{i},\\tilde{\\pmb{y}}_{t}^{i}[j]\\big)}\\end{array}$ , then we label the responses in the dataset $\\mathcal{D}_{\\mathrm{on-policy}}$ at step $t+1$ with the improved response and its associated reward value $\\tilde{r}_{t}^{i}[m]$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{D}}_{\\mathrm{on\\cdotpolicy}+\\mathrm{{self\\cdotdistillation}}}:=\\left\\{\\left\\{\\left(s_{t+1}^{i},\\tilde{y}_{t}^{i}[m],f_{t+1}^{i},\\tilde{r}_{t}^{i}[m]\\right)\\right\\}_{t=0}^{T-1}\\right\\}_{i=1}^{|\\mathcal{D}|}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Step 2: Policy improvement. With the aforementioned data construction schemes, we can now train a model on these datasets. While in general, any offline RL approach can be used to train on these data, in our experiments we adopt an approach based on weighted supervised learning [37] due to ease of experimentation and its simplicity. In particular, we perform a weighted supervised regression, where the weights are given by the exponential transformation of the reward values in $\\tilde{\\mathcal{D}}$ . ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{Reward-weighted\\;RL:\\quad}\\operatorname*{max}_{\\theta}\\;\\;\\mathbb{E}_{\\boldsymbol{x}_{i}\\sim\\tilde{\\mathcal{D}}}\\left[\\sum_{t=1}^{T}\\log\\pi_{\\theta}(\\tilde{y}_{t}^{i}|\\boldsymbol{s}_{t}^{i})\\cdot\\exp(r_{i}^{t}/\\tau)\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\tau$ is a temperature parameter to further expand or narrow the difference between good and bad actions. In our preliminary experiments, we found that Equation 3.6 can often induce a bias towards increasing log likelihoods of responses where rewards are high, prioritizing updates on easy problems where rewards are already high. To address this issue, we apply a slight modification to Equation 3.6 and center the exponentiated rewards around the mean value averaged across all attempts on a given prompt, akin to advantage-weighted regression [36]. We find that the use of advantages in place of rewards helps us avoid the \u201crich-gets-richer\u201d phenomenon with easy problems. ", "page_idx": 3}, {"type": "text", "text": "3.3 Inference at Deployment Time ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "RISE can be run in two modes at inference time. The most straightforward way to run the policy $\\pi_{\\theta}(\\cdot|\\cdot)$ trained by RISE is within a multi-turn rollout, where the model samples a new response conditioned on the past context (i.e., state in the multi-turn MDP). This past context consists of the external feedback $p_{i}^{\\mathrm{test}}$ concerning the response $\\pmb{y}_{i}^{\\mathrm{test}}$ and the rollout terminates as soon as the current response is judged to be correct according to the environment\u2019s answer verification function. In other words, we terminate the rollout as soon as the reward equals to the reward for the oracle response: $r(\\pmb{x},\\pmb{y}_{i}^{\\mathrm{test}})=r(\\pmb{x},\\pmb{y}^{\\ast})$ . This protocol queries the reward function after each turn in the rollout. Since several reward function queries are performed, we refer to this approach as \u201cwith oracle\u201d. ", "page_idx": 3}, {"type": "image", "img_path": "DRC9pZwBwR/tmp/6e09b19743de1211410aca8b37d9b802f2cb0a0ab6fdf7343cab0f3fa6d752ed.jpg", "img_caption": ["Figure 3: RISE Inference. There are two ways to query the model trained via RISE upon inference: (1) with oracle $(L e f t)$ : each time the model improves its response, it is allowed to check its answer against an environment and terminate early as soon as a correct answer is found; or (2) without oracle $(R i g h t)$ : we ask the model to sequentially revise its own responses $\\mathrm{j}$ times, and perform majority voting on all candidate outputs from different turns to obtain the final response. If the turn number $j$ is larger than the iteration number $k$ , the agent only keeps the most recent history with $k$ interactions to avoid test-time distribution shift. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "RISE can also be run in a mode that avoids the need to query the answer checker or the reward function within a rollout. In this case, we run full-length rollouts by forcing the model to retry, ignoring the correctness of the response. We then utilize a self-consistency mechanism [53] based on majority voting to decide the candidate response at the end of each turn. Concretely, at the end of each turn $j$ , we identify the response by running a majority vote over all response candidates from the previous turns (maj $\\left(y_{1}^{\\mathrm{test}},\\bar{y}_{2}^{\\mathrm{test}},\\cdots,y_{j}^{\\mathrm{test}}\\right)$ , including turn $j$ . We call this \u201cwithout oracle\u201d. A schematic illustration of these approach is shown in Figure 3. Most of our evaluations use no oracle. ", "page_idx": 4}, {"type": "text", "text": "At iteration $k$ , since the agent is able to improve its own response from $j$ to $j+1$ when $j\\leq k$ , to avoid test time distribution shift, in both modes, we use a size $k$ shift window to store the most recent conversation history when the turn number $j$ is larger than the iteration number $k$ . ", "page_idx": 4}, {"type": "text", "text": "3.4 Practical Algorithm and Implementation Details ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "A complete algorithmic pseudocode for each approach is shown in Appendix D. We trained 7B models via RISE and found that these models often could not adhere to response style and instructions for improving their responses when generating on-policy data. As a result, before running on-policy data collection, we find it often useful to run an initial phase of supervised fine-tuning on in-domain, multiturn rollouts generated from a capable model to provide style and instruction-following information to the learner. We call this the \u201cknowledge boosting\u201d stage. We then run on-policy rollouts starting from a boosted model. In each iteration, we generate 1 trajectory for each unique problem. We then run fine-tuning, with hyperparameters and details in Appendix F. For iterative fine-tuning, we find that starting from the base model but training on data from all iterations thus far is more beneficial than continued fine-tuning from the checkpoint obtained in the previous iteration. ", "page_idx": 4}, {"type": "text", "text": "4 When and Why is Self-Improvement Over Turns Possible? ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "A natural question to ask is why self-improvement with RISE even possible. One might surmise that the model may simply not have enough knowledge to correct its own mistakes if it is unable to correctly answer the problem in the first turn. Then, why is it possible to teach the model to correct its own mistakes? In this section, we provide the reason why this kind of self-improvement is possible, supported with empirical evidence to justify our hypotheses. ", "page_idx": 4}, {"type": "text", "text": "Iteratively teaching a model how to make updates on a given response can be crucial when representing the target distribution $p^{*}(y|x)$ requires more capacity than what the model $\\pi_{\\theta}$ affords by conditioning on only the input prompt tokens. When the target distribution requires greater capacity, learning a sequence of conditionals, $\\pi_{\\theta}(\\pmb{y}_{i+1}|\\pmb{x},\\pmb{y}_{0:i})$ followed by marginalization is expected to induce a more flexible marginal distribution over ${\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}\\,{\\mathbf{}}\\,$ given $\\textbf{\\em x}$ . This hypothesis is akin to the difference between diffusion models [44] and variational autoencoders (VAEs) [27] in image generation: iteratively ftiting a sequence of generative distributions over intermediate noisy inputs in a diffusion model gives rise to a more flexible distribution [45] than monolithic variational auto-encoding, even though diffusion models still utilize an evidence lower-bound objective(ELBO). While the diffusion process utilizes hand-designed noise schedules, RISE utilizes the base model itself to induce iterative improvements. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "To verify this hypothesis, we tracked the training un-weighted, negative loglikelihood loss (NLL) values for the oracle response $\\boldsymbol{y}^{*}$ given the input prompt $\\textbf{\\em x}$ marginalized over intermediate steps in a multi-turn rollout, and compared it against the NLL values $-\\log p_{\\theta}(y^{\\ast}|x)$ attained by directly attempting to predict the final response in Figure 4 (Left, labeled as \u201cClassic\u201d). Concretely, we sampled 256 prompts $\\textbf{\\em x}$ and their oracle responses $\\boldsymbol{y}^{*}$ and computed the average $-\\log p_{\\theta}(\\pmb{y}^{\\ast}|\\pmb{x})$ across all $\\textbf{\\em x}$ , along with a $95\\%$ confidence interval for ", "page_idx": 5}, {"type": "image", "img_path": "DRC9pZwBwR/tmp/f42733d8aa374f1433f82222d81314d9719c20f50e76730c4b51ae90edc24aa7.jpg", "img_caption": ["Figure 4: Left: The probability of the true answer given the prompt. Observe that model trained with RISE has higher probability for the true answer. Right: The training perplexity (loss) of fitting only the oracle answer or a sequence of answers. Note that fitting a sequence of answers (RISE) reduces the loss more than ftiting the oracle answer (Classic). "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "different checkpoints during training. We find that for any number of epochs (including fractional number of epochs on the ${\\bf X}$ -axis), the NLL value is lower when conditioning on multi-turn data that RISE generates compared to oracle responses to the prompts obtained from an expert. This suggests that RISE is able to utilize the computation of tokens from previous turns to model the target distribution. We also measure the average NLL loss on all samples through training, sampled i.i.d. from the training dataset for RISE and classic fine-tuning and observe a similar trend: RISE reduces loss more than the standard approach, attaining lower perplexity values (Figure 4 Right). ", "page_idx": 5}, {"type": "text", "text": "For knowledge-based question answering, it is not possible for the model to produce any meaningful improvements because learning $p^{*}(\\pmb{y}|\\pmb{x})$ is not bounded by insufficient capacity of $\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}|\\boldsymbol{x})$ , but is rather unable to match $p^{*}$ due to the absence of features that are critical to learn the correct mapping from $\\textbf{\\em x}$ to $\\textit{\\textbf{y}}$ . Training with RISE would likely incentivize hallucinations [26], since more input tokens appearing from previous attempts would only provide easier ways to pick up on spurious correlations. However, this is not the failure mode on reasoning problems [29], where maj $@\\mathbf{K}$ rates at turn 1 tend to be higher than pass $@1$ as we find in our experiments (indicating that performance can be improved by sampling the model itself). Figure 5 demonstrates that RISE\u2019s sequential procedure solves many problems unsolved by pass $\\mathbb{\\boldsymbol{\\@B}}$ for larger $B$ in the first turn, indicating it learns to access the model\u2019s pre-trained knowledge differently, rather than merely converting pass $@\\,\\mathrm{K}$ to pass $@1$ performance like most single-turn approaches. ", "page_idx": 5}, {"type": "image", "img_path": "DRC9pZwBwR/tmp/438bb0882f8182f521effb69e62122503d818f158d36fc54e43ce703442044cc.jpg", "img_caption": ["Figure 5: Fraction of problems unsolved by pass $\\mathbf{\\textregistered}$ at first turn that sequential 5-turn RISE sampling solves, where $B\\,=\\,5\\,\\times\\,k$ ( $k$ is the $\\mathbf{X}$ -axis). RISE can solve several challenging problems that sampling at the first turn with much larger budgets cannot solve. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "5 Experimental Evaluation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The goal of our experiments is to demonstrate the efficacy of RISE in instilling language models with the ability to self-improve their responses over turns. Our experiments answer the following questions: (1) How effectively can RISE improve performance over multiple sequential attempts (i.e., turns) at a given prompt?; (2) Does the performance of RISE improve with more rounds of iterative training?; (3) Does the self-improvement strategy induced by RISE generalize to novel problems that are out of the training domain? and finally; (4) What is the best data composition for training RISE? To this end, we compare RISE to other prior and baseline approaches, and perform ablations on GSM8K [12], MATH [20]. We defer RISE\u2019s performance on code generation tasks to Appendix B.3. Additional analyses are presented in Appendix C, where we examine the out-of-distribution generalization capabilities of RISE and explore the model\u2019s behavior in correct-to-correct scenarios. We also present a detailed study of weak-to-strong generalization, demonstrating how RISE-generated data from weaker models can enhance the performance of stronger models. ", "page_idx": 5}, {"type": "text", "text": "Baselines, comparisons, and evaluation. We compare RISE to several prior methods that attempt to induce similar self-improvement capabilities: (a) self-refine [23, 33] that prompts a base model to critique and revise its mistakes; (b) GloRE [19], which trains a separate reward model to locate errors and a refinement model to improve responses of a base LLM; and(c) self-consistency [53], which runs majority voting on multiple responses from the first turn as a baseline to compare to our sequential strategy. We tried to construct fair comparisons between RISE and these methods using a similar-sized model [25, 63], but differences in base model, training data, and evaluation setups still prohibits us from performing an apples-to-apples comparison in some cases. Nonetheless, we can still understand the ballpark of improvement by contextualizing our results with these prior works. We also compare to V-STaR [21], but since this is not an fair comparison, we defer it to Appendix B.1. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "We evaluate RISE in both modes at inference time: with and without an oracle (Section 3.3) at the end of five turns (the performance for each turn is in Appendix B.2). Concretely, these metrics are defined as follows: (1) with oracle, $\\bf{\\nabla}^{6}\\bf{p}1\\mathcal{\\@}t5^{\\circ}$ : this run terminates the rollout as soon as the response is correct. In other words, this metric allows queries to the final answer verifier at the end of each turn, and (2) without oracle, $\\mathbf{\\delta}^{6}\\mathbf{m}\\mathbf{1}\\circledast\\mathbf{1}\\mathbf{5}^{\\circ}\\,$ : this run sequentially sample five responses, and we compute the maj $@1$ performance on the candidates produced in each turn as detailed in Section 3.3. We also compare maj $@\\,\\mathrm{K}$ performance at the first turn for all the models we train. ", "page_idx": 6}, {"type": "table", "img_path": "DRC9pZwBwR/tmp/cc0be8c1822f5c96cdca874ee0fbb456ec09392fc892088d626eb1d1ec955736.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 1: RISE vs. other approaches (Self-Refine, GLoRE) and baselines. Observe that RISE attains the biggest performance improvement (in brown) between 1-turn $(\\mathrm{m}5\\ @\\mathrm{t}1)$ and 5-turn $(\\mathrm{m}1\\,\\ @\\mathrm{t}5)$ performance w/o an oracle on both GSM8K and MATH. This performance gap grows larger when oracle early termination is allowed ( $\\operatorname{pl}\\ @\\mathfrak{t}5$ w/ oracle). Self-Refine [33] degrades performance across the board when used without an oracle, and attains minor performance improvements with an oracle. GLoRE trains a separate refinement model, but performs worse than RISE; more details about it are in Appendix B.1. Using RISE on top of a better base model (Mistral-7B) is also effective (positive improvements with multiple turns), and the $\\mathrm{m}1\\,\\ @\\mathrm{t}5$ performance of Mistral-7B exceeds even state-of-the-art math models such as Eurus-7B-SFT [63]. Simply running single-turn SFT on data utilized by RISE is not effective at inducing a self-improvement capability, implying that algorithmic design choices in RISE are crucial for performance. Color coding indicates numbers that can be compared. ", "page_idx": 6}, {"type": "text", "text": "5.1 Does RISE improve performance over multiple turns compared to other approaches? ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Main results. We present the comparisons in Table 1. First, note that RISE (\u201cIteration 1\u201d and \u201cIteration 2\u201d) boosts up the LLama2 base model\u2019s five-turn performance by $15.1\\%$ and $17.7\\%$ respectively with each iteration on GSM8K and $3.4\\%$ and $4.6\\%$ on MATH, w/o any oracle. Interestingly, we found using prompting-only self-refine [33] largely degrades performance across the board, even with a strong proprietary model, GPT-3.5. The strongest 7B base models, Mistral-7B and Eurus-7BSFT [63], when coupled with standard prompting, are only able to improve their performance, but only by $5.3\\%\\ /\\ 11.6\\%$ and $0.9\\%\\,/\\,4.0\\%$ respectively on GSM8K and MATH, which is significantly lower than our approach. The performance of GLoRE improves only by $3.4\\%$ on GSM8K (over two turns), but this is still lower than our approach, which improves by $6.3\\%$ in two turns and $13.4\\%$ in three turns (see Appendix B.1). This indicates that RISE is effective in teaching models how to improve their own errors. To summarize, training with RISE gives the largest performance improvement gains compared to other approaches both with and without the use of an oracle, and these gains are transferred to other base models. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "One might also hypothesize that the performance gains with RISE here are largely a result of utilizing queries to an off-the-shelf more capable model for providing supervision and not the algorithmic approach for data collection and training. To address this hypothesis, we store all the data generated by RISE from more capable models and train on this data via standard single-turn SFT (\u201cSFT on oracle data). Since not all of this data are guaranteed to be correct, we also run this experiment on only the correct responses in these oracle data. Observe in Table 1 that this procedure does not still instill self-improvement capabilities, largely preserving or degrading sequential $(\\mathbf{\\dot{\\sigma}}\\mathbf{maj}@1@\\mathbf{turn}\\mathbf{\\bar{s}}^{\\ast})$ performance compared to simply sampling one response in the first turn. This means that the algorithmic design of RISE is critical, as opposed to the use of expert supervision. ", "page_idx": 7}, {"type": "text", "text": "5.1.1 Can RISE Effectively Make Use of Mistakes and Correct Them? ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "One concern that arises from prior results on self-refinement or self-correction is whether the model can truly correct itself over turns or whether the improvement comes from the effect of sampling more answers and picking the best one. In Table 1, we see that sequentially improving responses via RISE $(\\mathbf{\\dot{\\sigma}}\\mathbf{maj}@1@\\mathbf{turn}5^{\\ast})$ outperforms sampling 5 responses in parallel at the first turn and applying a majority vote on them $(\\bf{\\dot{\\Phi}})(\\varpi5\\odot\\bf{t u r n1}^{\\circ})$ . Please note that this comparison utilizes an equal number of samples, with the only difference being that these samples are drawn in parallel at the first turn in one case and sequentially at the end of five turns in the other. Comparing maj $@5$ performance at the end of 1 turn and 5 turns, we observe a consistent $4\\%$ to $8\\%$ improvement on GSM8K and an $6.5\\%$ improvement on MATH (with Mistral-7B model). This means that RISE can imbue models with a self-improvement ability, while running parallel sampling alone on any model cannot endow the same ability. Even the $\\mathbf{naj}@5@\\mathbf{turn}\\mathbf{1}$ performance of standard single-turn SFT on the data used by RISE is substantially worse than the sequential maj $\\mathbf{\\mathcal{O}1}@\\mathbf{turn5}$ performance of RISE, implying that the algorithmic protocol of RISE plays a critical underlying role. Finally, we also remark that in Figure 5, we showed that the sequential procedure learned by RISE over five turns could solve a significant fraction of problems that were unsolved by pass $@\\mathbf{B}$ for much larger values of $B\\gg5$ in the first turn, implying that sequential RISE can actually tackle prompts that were not solvable by simply sampling more responses in the first turn. One might speculate if these improvements in sequential improvement ability largely come at a cost of reduced improvements in first turn performance. We also observe that running multiple iterations of RISE still preserves the first turn performance while improving the 5-turn performance. ", "page_idx": 7}, {"type": "text", "text": "5.1.2 How Does the Base Model Affect RISE? ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The performance of RISE with Llama2-7B on an absolute scale is lower than the best models specifically fine-tuned on math data (e.g., Eurus-7B-SFT or Mistral-7B). However, we find that RISE is still effective on top of Mistral-7B base model. In fact, our performance at the end of five turns outperforms one of the best 7B SFT models, customized to math reasoning. Compare the $\\mathrm{m}1\\circledast\\mathrm{5}$ performance of Eurus-7B-SFT and Mistral-7B in RISE (ours), to find that Mistral- $\\mathbf{\\cdot}7\\mathbf{B}+\\mathbf{R}\\mathrm{ISE}$ outperforms Eurus-7B-SFT. ", "page_idx": 7}, {"type": "text", "text": "5.1.3 Self-Distillation Version of RISE ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compare the performance of RISE with selfgenerated data and supervision (Equation 3.4, $N\\,=\\,16)$ ) after one iteration directly on top of more capable models: Mistral-7B and Llama-3- 8B on GSM8K in Table 2, without any knowledge boosting phase. We find that this variant also improves the 5-turn performance of the base model compared to the first turn: compare $\"\\mathrm{m}1\\,\\ @\\mathrm{t}5\"$ vs $\\cdot\\mathsf{m}1\\,\\ @\\mathsf{t}1^{\\,\\bullet}$ for both the models Llama-3-8B and Mistral-7B, where RISE boosts the sequential selfimprovement performance by more than $1\\%$ compared to turn 1 performance w/o any oracle. ", "page_idx": 7}, {"type": "table", "img_path": "DRC9pZwBwR/tmp/a34bbc0d8b85c3f989d914bb188b191dd78b91aa6061c66284092fc8058ef473.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 2: RISE with self-distillation on GSM8K. RISE is able to improve 5-turn maj $@1$ performance of the model with entirely self-generated data and supervision, despite the fact that the base Mistral-7B model does not produce correct answers for several problems. ", "page_idx": 7}, {"type": "text", "text": "Of course, this version of RISE does not outperform the $\"\\mathrm{m}5\\textcircled{\\omega}\\mathrm{t}1\"$ performance of the fine-tuned model. We expect this to be largely a function of one single iteration of training. Since the selfdistillation version of RISE utilizes best-of-N sampling against the same model to produce supervision for self-improvement, RISE would first have to match the performance of best-of-N sampling before it can start to improve over it via reward maximization. Due to the significant gap between the base model\u2019s $\\mathrm{m}5\\textcircled{\\alpha}\\mathrm{t}1$ and $\\mathrm{m}1\\circledast\\mathrm{5}$ performance, we expect that this will take quite a few iterations or a fully online RL algorithm. We did not have computational resources and infrastructure to run multiple iterations, but this is an interesting avenue for future work. In this self-distillation setting, we could also divide the computation between sequential and parallel sampling strategies to get the best results at the end of five turns. Nonetheless, this result shows that even by training on self-generated samples, RISE can actually amplify the sequential sampling performance of the base model. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.2 Does the Performance of RISE Improve with Iterative Training? ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Next, we attempt to understand if RISE improves with multiple rounds of training on on-policy data. As shown in Tables 1 and 2, the performance of RISE improves from iteration to iteration constantly. The 5-turn performance of RISE, both with and without an oracle, exhibits a clear improvement with more rounds. This implies that iterative self-training procedures of the form of STaR [66] can also be combined with RISE to train models for self-improvement. This also perhaps serves as a strong hint towards the potential utility of full online reinforcement learning (RL) techniques. ", "page_idx": 8}, {"type": "text", "text": "5.3 What Data Compositions and Data Quantity are Crucial for RISE? ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We now study how different data compositions affect the performance of RISE with the goal of answering questions such as should we collect on-policy error correction data like DAgger [38] or should we bias towards high-quality off-policy data?. To understand the utility of different data compositions, we enlist the three aspects RISE: (a) the use of multi-turn rollout data for fine-tuning, (b) the use of unsuccessful / suboptimal rollouts via weighted supervised fine-tuning compared to na\u00a8\u0131ve supervised learning, which only utilizes successful rollouts for fine-tuning; and (c) the use of on-policy rollouts and self-generated or oracle data. We will now perform controlled experiments to understand the effect of each of these factors on the overall performance of RISE. ", "page_idx": 8}, {"type": "text", "text": "(a) Data composition for fine-tuning. We first study the necessity of using the interaction of error correction history in RISE in Figure 6 (Left). We compare two approaches: model trained with oracle answers shown right after the query (\u201c1-turn\u201d) and oracle answers shown after intermediate failed attempts (\u201cMulti-turn\u201d) in Figure 6 (Left). Even though the latter trains on intermediate responses that may not always be correct, it attains a higher performance than training on the correct response for a given prompt. This highlights the importance of training on contexts with a multi-turn interaction history depicting mistakes from the learner to improve self-improvement capabilities. ", "page_idx": 8}, {"type": "image", "img_path": "DRC9pZwBwR/tmp/d68be36b2774ddec0c748e1291866e98feb7c90f865a42f90414901bb7c04d56.jpg", "img_caption": ["Figure 6: Left: The importance of multi-turn interaction history and weighted objectives for training RISE. Note that training with multi-turn data leads to better self-improvement performance at the end of 5 turns, than one-turn data from the original dataset with oracle answers from another model; also observe that using a weighted objective performs better. Right: The importance of using all rollouts for learning, instead of only successful rollouts or only successful responses in the data. Using all data performs best in our results. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "(b) Weighted supervised learning vs unweighted supervised learning. We examine rewardweighted RL\u2019s impact on multi-turn data in RISE as opposed to simply imitating filtered successful data. We find that using all the data leads to improved performance over simply flitering good datain Figure 6 (Right), which reduces sample size. In Figure 6 (Left), we find that reward-weighted training improves performance on later turns, allowing us to better leverage all the sub-optimal data. ", "page_idx": 8}, {"type": "text", "text": "(c) On-policy vs off-policy data; self-generated vs. expert data. RISE runs on-policy rollouts and seeks improvements on responses that the learner produces. As shown in Figure 9 (Left), a \u201cDAgger [38]\u201d-style approach that seeks improvements on responses appearing in on-policy rollouts improves performance (green/orange) compared to using expert data alone (blue/pink). Conceptually, this addresses the train-test mismatch between the distribution of context tokens, enabling imitation learning methods to now target the correct distribution. In addition, recent work [26] has shown that LLMs often memorize \u201cunfamiliar\u201d examples generated by oracle models; by training on on-policy rollouts, we should be able to eliminate any such potential issues. Thus, while the model trained via offline imitation is able to reduce loss, these improvements do not generalize to new problems. ", "page_idx": 8}, {"type": "text", "text": "5.4 Error Analysis of RISE over Turns ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Following the protocol of Huang et al. [23], in this section, we perform an error analysis of the improvement performed by RISE (without any oracle feedback) to understand how the fraction of incorrect and correct responses changes over turns, when no oracle is used for early termination. We demonstrate this in the form of Venn diagrams in Figure 7. First note that there is a consistent increase in the portion of problems that stay correct and a consistent decrease in the portion of problems that stay incorrect, which means that the model is able to answer more and more problems as we increase the number of turns. Second, there is a consistent decrease in the number of problems that change from being correct to incorrect, which is often also not the case for strong proprietary LLMs such as GPT in Huang et al. [23]. We also note that there is a decrease in the total number of incorrect problems that become correct in the subsequent turn, but this is a direct consequence of a shrinkage in the size of the incorrect response set as more problems become correct over turns. This indicates that one can induce \u201cintrinsic\u201d self-improvement (per the terminology of Huang et al. [23]) via fine-tuning with RISE, even though no external environment input is provided during evaluation. ", "page_idx": 9}, {"type": "image", "img_path": "DRC9pZwBwR/tmp/4022ad8f6bf19a33fdb2c529c6769208c63a1d2043f3a255c66d541875f0d8a4.jpg", "img_caption": ["Figure 7: Change in the fraction of responses that transition their correctness values over the course of multi-turn rollouts from RISE, w/o oracle. Observe that in general, the fraction of Correct $\\rightarrow$ Correct responses increases; Incorrect $\\rightarrow$ Incorrect responses decreases; and the fraction of Correct $\\rightarrow$ Incorrect responses also decreases, indicating that RISE (w/o any oracle) is able to iteratively improve its responses. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Qualitative examples. We also inspect several examples from the GSM8K test set to qualitatively understand the behavior of RISE over turns and observe different behavior patterns, that we show in Appendix E. For instance, the trained model may choose to completely rewrite its previous response if it is totally incorrect in order to get to the correct answer or make small edits if the previous response is mostly correct. Another interesting pattern we note is that the model implicitly has the ability to locate errors in previous responses and only refine the erroneous steps. Additionally, the model is tolerant of noisy environmental feedback when there is no oracle-assisted early termination. ", "page_idx": 9}, {"type": "text", "text": "6 Discussion, Future Directions, and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We presented RISE, an approach for fine-tuning LLMs to be able to improve their own responses over multiple turns sequentially. RISE prescribes an iterative RL recipe on top of on-policy rollout data, with expert or self-generated supervision to steer self-improvement. RISE significantly improves the self-improvement abilities of 7B models on reasoning tasks (GSM8K and MATH), attaining an improvement over turns that previous work [23] has not observed in strong proprietary models. In addition, RISE outperforms prior approaches that attempt to tackle similar problems of refinement and correction, while being simpler in that it does not require running multiple models and works well with just one model. ", "page_idx": 9}, {"type": "text", "text": "Despite these good results, there are still many open questions and limitations. Due to computational constraints, we were not able to perform more than two iterations of training with RISE, and no more than one iteration when the supervision comes from the learner itself. Improving with self-generated supervision will likely require more computation and more iterations, since it will be slower than when using an off-the-shelf expert model. RISE requires running manual iterations and hence, a more \u201conline\u201d variant of RISE is likely the solution in the long run, especially when we wish to scale on-policy learning in a data-efficient manner. Additionally, while our work fine-tunes models on one task at a time, it will be certainly interesting to include data from the protocols specified by RISE into general instruction tuning and post-training pipelines. Given the results that fine-tuning on data prescribed by RISE does not hurt the first-turn performance of any model we trained, we hypothesize that adding this sort of data in general instruction-tuning pipelines should not hurt either, while enabling the sequential self-improvement capability that is largely absent from models today. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was done at Carnegie Mellon University. We thank Fahim Tajwar, Abitha Thankaraj, Amrith Setlur, and Charlie Snell for their feedback and informative discussions. This work was supported by ONR under N000142412206, OpenAI superalignment fast grants, and used the Delta system and JetStream2 [18] at the National Center for Supercomputing Applications through CIS240249 and CIS230278, supported by the National Science Foundation. We thank OpenAI for providing GPT-4 credits for academic use. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, and Olivier Bachem. Gkd: Generalized knowledge distillation for auto-regressive sequence models. arXiv preprint arXiv:2306.13649, 2023.   \n[2] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.   \n[3] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.   \n[4] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, Ilya Sutskever, and Jeff Wu. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision, 2023. URL https://arxiv.org/abs/2312.09390.   \n[5] Jonathan D Chang, Wenhao Shan, Owen Oertell, Kiant\u00b4e Brantley, Dipendra Misra, Jason D Lee, and Wen Sun. Dataset reset policy optimization for rlhf. arXiv preprint arXiv:2404.08495, 2024.   \n[6] Yiannis Charalambous, Norbert Tihanyi, Ridhi Jain, Youcheng Sun, Mohamed Amine Ferrag, and Lucas C Cordeiro. A new era in software security: Towards self-healing software via large language models and formal verification. arXiv preprint arXiv:2305.14752, 2023.   \n[7] Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao. Fireact: Toward language agent fine-tuning, 2023.   \n[8] Xinyun Chen, Maxwell Lin, Nathanael Sch\u00a8arli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023.   \n[9] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335, 2024.   \n[10] Yew Ken Chia, Guizhen Chen, Luu Anh Tuan, Soujanya Poria, and Lidong Bing. Contrastive chain-of-thought prompting. arXiv preprint arXiv:2311.09277, 2023.   \n[11] Karl Cobbe, Christopher Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation to benchmark reinforcement learning. arXiv preprint arXiv:1912.01588, 2019.   \n[12] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.   \n[13] Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767, 2023.   \n[14] Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023.   \n[15] Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah D Goodman. Stream of search (sos): Learning to search in language. arXiv preprint arXiv:2404.03683, 2024.   \n[16] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning, pages 10764\u201310799. PMLR, 2023.   \n[17] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Critic: Large Language Models can Self-Correct with Tool-Interactive Critiquing. arXiv preprint arXiv:2305.11738, 2023.   \n[18] David Y. Hancock, Jeremy Fischer, John Michael Lowe, Winona Snapp-Childs, Marlon Pierce, Suresh Marru, J. Eric Coulter, Matthew Vaughn, Brian Beck, Nirav Merchant, Edwin Skidmore, and Gwen Jacobs. Jetstream2: Accelerating cloud computing via jetstream. In Practice and Experience in Advanced Research Computing, PEARC \u201921, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450382922. doi: 10.1145/3437359.3465565. URL https://doi.org/10.1145/3437359.3465565.   \n[19] Alex Havrilla, Sharath Raparthy, Christoforus Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, and Roberta Railneau. Glore: When, where, and how to improve llm reasoning via global and local refinements. arXiv preprint arXiv:2402.10963, 2024.   \n[20] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021.   \n[21] Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal. V-star: Training verifiers for self-taught reasoners. arXiv preprint arXiv:2402.06457, 2024.   \n[22] Dong Huang, Qingwen Bu, Jie M Zhang, Michael Luck, and Heming Cui. Agentcoder: Multi-agent-based code generation with iterative testing and optimisation. arXiv preprint arXiv:2312.13010, 2023.   \n[23] Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798, 2023.   \n[24] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pages 9118\u20139147. PMLR, 2022.   \n[25] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.   \n[26] Katie Kang, Eric Wallace, Claire Tomlin, Aviral Kumar, and Sergey Levine. Unfamiliar finetuning examples control how language models hallucinate, 2024.   \n[27] Diederik P Kingma and Max Welling. Auto-encoding variational bayes, 2022. URL https: //arxiv.org/abs/1312.6114.   \n[28] Lucas Lehnert, Sainbayar Sukhbaatar, Paul Mcvay, Michael Rabbat, and Yuandong Tian. Beyond a\\*: Better planning with transformers via search dynamics bootstrapping. arXiv preprint arXiv:2402.14083, 2024.   \n[29] Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu, Zheng Zhang, and Houwen Peng. Common 7b language models already possess strong math capabilities. arXiv preprint arXiv:2403.04706, 2024.   \n[30] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s verify step by step. arXiv preprint arXiv:2305.20050, 2023.   \n[31] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023.   \n[32] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023.   \n[33] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.   \n[34] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis. ICLR, 2023.   \n[35] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.   \n[36] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.   \n[37] Jan Peters and Stefan Schaal. Reinforcement learning by reward-weighted regression for operational space control. In Proceedings of the 24th international conference on Machine learning, pages 745\u2013750. ACM, 2007.   \n[38] Stephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Geoffrey Gordon, David Dunson, and Miroslav Dud\u00b4\u0131k, editors, Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, volume 15 of Proceedings of Machine Learning Research, pages 627\u2013635, Fort Lauderdale, FL, USA, 11\u201313 Apr 2011. PMLR. URL http://proceedings. mlr.press/v15/ross11a.html.   \n[39] Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, and Tengyang Xie. Direct nash optimization: Teaching language models to self-improve with general preferences. arXiv preprint arXiv:2404.03715, 2024.   \n[40] Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian Li. Branch-solve-merge improves large language model evaluation and generation. arXiv preprint arXiv:2310.15123, 2023.   \n[41] Timo Schick, Jane Dwivedi-Yu, Roberto Dess\\`\u0131, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.   \n[42] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.   \n[43] Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, and Sergey Levine. Offline rl for natural language generation with implicit language q learning. arXiv preprint arXiv:2206.11871, 2022.   \n[44] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics, 2015. URL https://arxiv. org/abs/1503.03585.   \n[45] Yang Song and Diederik P. Kingma. How to train your energy-based models, 2021. URL https://arxiv.org/abs/2101.03288.   \n[46] Liting Sun, Cheng Peng, Wei Zhan, and Masayoshi Tomizuka. A fast integrated planning and control framework for autonomous driving via imitation learning. In Dynamic Systems and Control Conference, volume 51913, page V003T37A012. American Society of Mechanical Engineers, 2018.   \n[47] Gokul Swamy, Sanjiban Choudhury, J. Andrew Bagnell, and Zhiwei Steven Wu. Inverse reinforcement learning without reinforcement learning, 2024. URL https://arxiv.org/ abs/2303.14623.   \n[48] Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and Igor Gitman. Openmathinstruct-1: A 1.8 million math instruction tuning dataset. arXiv preprint arXiv:2402.10176, 2024.   \n[49] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.   \n[50] Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. Towards understanding chain-of-thought prompting: An empirical study of what matters. arXiv preprint arXiv:2212.10001, 2022.   \n[51] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv: Arxiv-2305.16291, 2023.   \n[52] Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Y Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. CoRR, abs/2312.08935, 2023.   \n[53] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.   \n[54] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. NeurIPS, 2022.   \n[55] Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. Generating sequences by learning to self-correct. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=hH36JeQZDaO.   \n[56] Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng, Wei He, Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, et al. Training large language models for reasoning through reverse curriculum reinforcement learning. arXiv preprint arXiv:2402.05808, 2024.   \n[57] Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, and Tong Zhang. Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint. In Forty-first International Conference on Machine Learning, 2024.   \n[58] Hui Yang, Sifu Yue, and Yunzhong He. Auto-gpt for online decision making: Benchmarks and additional opinions. arXiv preprint arXiv:2306.02224, 2023.   \n[59] Kaiyu Yang, Aidan M Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, and Anima Anandkumar. LeanDojo: Theorem Proving with RetrievalAugmented Language Models. arXiv preprint arXiv:2306.15626, 2023.   \n[60] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.   \n[61] Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig. Learning to mine aligned code and natural language pairs from stack overflow. In International Conference on Mining Software Repositories, MSR, pages 476\u2013486. ACM, 2018. doi: https://doi.org/10. 1145/3196398.3196408.   \n[62] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023.   \n[63] Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, et al. Advancing llm reasoning generalists with preference trees. arXiv preprint arXiv:2404.02078, 2024.   \n[64] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 2024.   \n[65] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023.   \n[66] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:15476\u201315488, 2022.   \n[67] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. Agenttuning: Enabling generalized agent abilities for llms. arXiv preprint arXiv:2310.12823, 2023.   \n[68] Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, and Joseph E Gonzalez. Tempera: Test-time prompting via reinforcement learning. arXiv preprint arXiv:2211.11890, 2022.   \n[69] Tianjun Zhang, Aman Madaan, Luyu Gao, Steven Zheng, Swaroop Mishra, Yiming Yang, Niket Tandon, and Uri Alon. In-context principle learning from mistakes. arXiv preprint arXiv:2402.05403, 2024.   \n[70] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning acting and planning in language models. arXiv preprint arXiv:2310.04406, 2023.   \n[71] Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. Archer: Training language model agents via hierarchical multi-turn rl. arXiv preprint arXiv:2402.19446, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Related Work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Several prior works build techniques to improve reasoning and thinking capabilities of foundation models for downstream applications. Typically these works focus on building prompting techniques for effective multi-turn interaction with external tools [6, 8, 16, 34, 51, 58, 60], sequentially refining predictions by reflecting on actions [8, 17, 68], asking the model to verbalize its thoughts [35, 54, 70], asking the model to critique and revise itself [33, 42] or by using other models to critique a primary model\u2019s responses [3, 14, 22, 58]. Although a subset of this work does improve its own responses, this self-correction ability often requires access to detailed error traces (e.g., execution traces from code compilers [8, 33]) in order to succeed. In fact, [23] and Table 1 both indicate that self-improvement guided by the LLM itself (i.e., \u201cintrinsic self-correction\u201d) is often infeasible for off-the-shelf LLMs even when they contain the knowledge required to tackle the prompt given, but fine-tuning with RISE induces this capability as we show in this paper. ", "page_idx": 15}, {"type": "text", "text": "Beyond prompting, previous work also attempts to fine-tune LLM to obtain self-improvement capabilities [7, 41, 67]. These works attempt to improve reasoning performance by training on selfgenerated responses [32, 48, 62, 63, 65]. To achieve this, these works use a combination of learned verifiers [13, 30, 49, 52], search [15, 28, 35, 40], contrastive prompting on negative data [10, 50], and iterated supervised or reinforcement learning (RL) [9, 39, 57, 64]. Although our approach also trains on model-generated data, we aim to introduce a complementary capability to improve performance over sequential turns of interaction, rather than to improve single-turn performance alone. Other work fine-tunes LLMs for multi-turn interaction directly via RL [43, 71]: while this is indeed related, single-turn problems posed in multi-turn scenarios require addressing distinct challenges than generic multi-turn RL: (i) sample-efficiency is not a concern since the entire environment is fully characterized by the training dataset of prompts and oracle answers and dynamics are deterministic, and (ii) we need to generalize to novel test prompts. Multi-turn RL focuses on sample efficiency, which is not as critical in our setting, though of course learning to generalize from a limited number of initial states would be appealing. Our main focus is to show that it is possible to train models for self-improvement via appropriately designing multi-turn fine-tuning objectives. This is orthogonal from the choice of training approach (RL or not). ", "page_idx": 15}, {"type": "text", "text": "The most related to our work are GLoRE [19] and Self-Correct [55], which train separate models to identify errors and refine incorrect answers of other LLMs. Unlike these works, our approach trains a single model to produce answers and improve them over more than two turns, which is the maximal number of turns studied in these works. We show that doing so successfully requires careful design choices: an iterative on-policy data generation strategy along with a training objective that can learn from both successful and unsuccessful rollouts. From an algorithmic point of view, RISE is similar to online imitation learning [38, 46], in that it queries expert supervision on states attained by on-policy rollouts. On-policy distillation for LLMs [1, 5, 56] utilizes this idea, but queries an expert to provide completions on partial responses instead of sequential attempts, that we do in this work. ", "page_idx": 15}, {"type": "text", "text": "B Additional Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Complete Comparisons and Discussion on Math Reasoning Tasks ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We provide an extended version of Table 1, with a clear explanation of how we implement baselines and a discussion of comparisons. ", "page_idx": 15}, {"type": "text", "text": "Comparison with Self-Refine [33]. To build a self-refine baseline [33] evaluation, we slightly modified our evaluation pipeline following the self-refine approach. In this setup (Figure 8), the model generates an initial response, and then the environment prompts the model to locate errors in the generated solution and refine its answer based on the initial response and the identified error. ", "page_idx": 15}, {"type": "text", "text": "However, our experiments show that without any oracle hint from the environment or human feedback, the self-refine approach leads to a degradation in performance across all models. Only when oracle feedback is available to assist with early termination does the self-refine approach provide a slight performance boost. This highlights the limitation of the self-refine structure in effectively improving model performance without external guidance, which is also observed in [24]. ", "page_idx": 15}, {"type": "table", "img_path": "DRC9pZwBwR/tmp/bfb042d07786ce9082a5ee97ae3432c14a57617b2e49bb9a7436599463b60d2a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "In contrast, the model trained with RISE can attain consistent performance improvements without relying on an oracle. By training the model to iteratively refine its responses, our method enables the model to self-correct and improve its performance over multiple turns. This showcases the effectiveness of our approach in comparison to the self-refine baseline, as it allows for more robust and consistent performance gains without the need for the oracle assistance. ", "page_idx": 16}, {"type": "text", "text": "Comparison with GLoRE [19]. GLoRE is a multi-model system that relies on a student model to propose drafts, an Outcome-based Reward Model (ORM) or Step-wise ORM to locate errors at different granularity levels, and a Global or Local Refinement Model for adjusting these errors. Since no code was openly available for this approach, in our experiments, we compared to the numbers from the main paper Havrilla et al. [19]. While the comparison against GLoRE is already apples-to-oranges since our method only trains a single end-to-end model, while GLoRE trains multiple models. Performance-wise, GLoRE\u2019s global and local refinement models show little to no improvement in overall accuracy without an oracle, and even exhibit decreasing accuracy in some cases. However, when an oracle is used to guide the refinement process, GLoRE demonstrates a $10\\%$ improvement on the 7B model in the GSM8K dataset. ", "page_idx": 16}, {"type": "text", "text": "As anticipated, since we run RISE from a less advanced base model (Llama2 7B), we observe a slightly lower absolute performance compared to GLoRE. However, RISE demonstrates its effectiveness in self-improvement by sequentially enhancing its performance by an impressive $13.4\\%$ within just 3 turns without an oracle feedback, and by a remarkable $23.4\\%$ with an oracle on GSM8K. This showcase of RISE\u2019s capabilities is particularly noteworthy considering that GLoRE utilizes 3 ", "page_idx": 16}, {"type": "text", "text": "Self-Refine ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "System: You are an AI language model designed to assist with math problem-solving. In this task, I will provide you with math problems. Your goal is to solve the problem step-by-step, showing your reasoning at each step. After you have finished solving the problem, present your final answer as \\boxed{Your Answer}.   \n<One-shot Example $16\\!>$   \nUser: <Query>   \nAgent: $<$ <Initial Answer>   \nUser: There is an error in the solution above because of lack of understanding of the question. What is the error? To find the error, go through each step of the solution, and check if everything looks good.   \nAgent: <Critic>   \nUser: Now, rewrite the solution in the required format:   \nAgent: $<$ <Refined Answer> ", "page_idx": 17}, {"type": "text", "text": "Figure 8: Prompt for Self-Refine: We follow the standard pipeline of the original paper, prompt the LLM to refine and correct its previous mistakes. ", "page_idx": 17}, {"type": "text", "text": "independent models - one for generating candidate solutions, one reward model for locating errors, and one refinement model for refinement. ", "page_idx": 17}, {"type": "text", "text": "Comparison with V-STaR [21]. V-STaR requires training an additional verifier model to rank candidate answers generated by the targeted model, but it does not make any sequential revisions or improvements to a response. While comparing RISE to using a verifier for re-ranking the top 5 responses at the first turn (as a base comparison) would have been informative, we were unable to find this specific result in the original V-STaR paper. The results presented in the official table 3 for V-STaR correspond to running 64 samples, which improves the base model\u2019s performance by $35.2\\%$ for each prompt during evaluation. In contrast, our method, RISE, after the same amount of finetuning iterations (3 iterations) and using only 5 samples, improves upon the base model by $44.5\\%$ (calculated as $55.0\\%-10.5\\%=44.5\\%$ ). This comparison highlights RISE\u2019s efficiency in achieving significant improvements with fewer samples and iterations compared to V-STaR\u2019s approach of using a large number of samples without sequential refinement. ", "page_idx": 17}, {"type": "text", "text": "Moreover, V-STaR\u2019s performance is inherently bounded by the candidate generator\u2019s performance. As discussed in Section 5, if there is no correct response among the generated candidates, the problem remains unsolved. In contrast, we show in Figure 5 that RISE can also solve problems that were not solved by majority voting with a much higher budget in the first turn. Furthermore, we believe that combining V-STaR with RISE could lead to even better performance, as RISE can generate better models and a verifier can be complementarily used for filtering. ", "page_idx": 17}, {"type": "text", "text": "Comparison with other base models. Mistral-7B [25] and Eurus-7B-SFT [63] are models that exhibit comparable performance to our method in terms of the absolute maj $@5$ performance. However, it is crucial to note that these base models are fine-tuned using a vast amount of data, including data specifically tuned for math reasoning performance [63], while our model is fine-tuned on a single domain. That said, we do show that fine-tuning with RISE can still enhance the performance of Mistral-7B models. ", "page_idx": 17}, {"type": "text", "text": "To summarize, our method offers advantages over GLoRE and V-STaR: end-to-end error correction with a single model, superior performance with fewer samples, and solving problems unsolvable by first-turn random sampling. Although our maj $@1$ performance is lower than GLoRE\u2019s base model EI, our 5-turn performance surpasses their best performance in both oracle and non-oracle scenarios. Compared to Mistral-7B and Eurus-7B-SFT, our method achieves comparable performance while being fine-tuned on a single domain, and generalizes to better base models. ", "page_idx": 17}, {"type": "text", "text": "B.2 The Performance of RISE over More Turns than Training ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we evaluate RISE\u2019s performance on GSM8K across multiple turns (up to $k{=}10$ ). Our analysis reveals several key findings. First, the base model demonstrates limited self-improvement capability: as we increase the number of turns $\\boldsymbol{\\mathrm{k}}$ , the performance improvement is marginal, showing only a $0.7\\%$ increase (from $10.5\\%$ to $11.2\\%$ ) at $k=10$ . Notably, this multi-turn performance is inferior to the strategy of sampling 10 responses in the first turn, which yields a more substantial improvement of $17.3\\%$ (from $10.5\\%$ to $27.8\\%$ ). This pattern consistently holds across all values of k from 1 to 10. In contrast, RISE demonstrates significant improvements in two aspects. First, it substantially enhances the model\u2019s base performance, elevating it from $10.5\\%$ to $35.5\\%$ after one iteration, and further to $37.3\\%$ after two iterations. More importantly, RISE enables more effective sequential self-improvement: at $k=10$ , the model achieves a remarkable performance gain of $22.2\\%$ (from $35.6\\%$ to $57.8\\%$ ) after one iteration, and $23.0\\%$ (from $37.3\\%$ to $60.3\\%$ ) after two iterations. This enhanced self-improvment capability is evident across most values of $\\boldsymbol{\\mathrm{k}}$ , where $\\mathrm{m}1\\textcircled{\\omega}$ tk consistently surpasses $\\mathrm{mk}@\\mathrm{t}1$ . ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "table", "img_path": "DRC9pZwBwR/tmp/2d9cbb4624248a28a373bc2056cc6146f5d985b289e171ee888e49d98f780855.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 4: The performance of RISE with Llama-2-7B over 10 turns on GSM8K. Observe that RISE consistently improves as more turns are provided $(\\mathrm{m}1\\textcircled{\\boldsymbol{a}}$ tk increases as turn number k increases). We also find sequential sampling with RISE $(\\mathrm{m}1\\textcircled{a}\\mathrm{tk})$ still outperforms parallel sampling $(\\mathrm{mk}@\\mathrm{t}1)$ , with a larger gap between these numbers from iteration 1 to 2 of RISE, although parallel sampling for the base model is much better than sequential sampling. This means RISE is effective at teaching the model how to improve itself. ", "page_idx": 18}, {"type": "text", "text": "B.3 The Performance of RISE on Code Generation Tasks ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Table 5, we evaluate RISE\u2019s effectiveness when integrated with Llama-2-7B on two established Python programming benchmarks: MBPP [2] and CoNaLa [61]. Our experimental results demonstrate that after a single iteration, RISE significantly enhances the model\u2019s five-turn performance, yielding improvements of $7.0\\%$ and $1.7\\%$ on MBPP and CoNaLa, respectively. Notably, these multiturn performance gains are achieved while simultaneously improving the first-turn performance: RISE elevates the initial success rate from $19.0\\%$ to $27.0\\%$ on MBPP and from $5.3\\%$ to $6.1\\%$ on CoNaLa, indicating that the method enhances both immediate and iterative problem-solving capabilities. ", "page_idx": 18}, {"type": "table", "img_path": "DRC9pZwBwR/tmp/97a6a7743b5d27656590c9b1b80331ccaff7aad21987abf464d4a6d30e9bf372.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "C Additional Ablations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1 Does RISE Also Improve Sequential Performance on Out-of-Distribution Prompts? ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Table 6, our aim is to evaluate the robustness of the strategy induced by RISE on new, unseen prompts. Specifically, we compare the performance of the RISE model trained with a dataset on evaluation prompts from another dataset. Note in Table 6, these datasets include MATH, GSM8K, and SVAMP. Generally, we observe that the model trained on one dataset is still able to improve the base model\u2019s performance on another dataset over the course of sequential five turns. More concretely, while the base Llama2 model largely degrades its turn 1 performance over turn 5 performance, model\u2019s trained with RISE enable a positive performance improvement on these out-of-distribution prompts. This means that even though these models have not seen queries similar to the evaluation dataset, simply training with RISE on some kind of mathematical prompts still boosts the efficacy of the self-improvement strategy on a new distribution of test prompts. This finding suggests that RISE is capable of instilling self-improvement procedures that can generalize beyond the distribution of prompts in the fine-tuning data. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "table", "img_path": "DRC9pZwBwR/tmp/e0b0ddab8d1ba8ee14ca95abdde02b5c59eeba42ca5a28355953e2f0689c4e19.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 6: Out-of-distribution generalization of RISE. We evaluate model fine-tuned on MATH on the GSM8K test set; model fine-tuned GSM8K on MATH; and the model fine-tuned on a mixture of GSM8K and MATH on the SVAMP data. Observe even though we train on OOD prompts, RISE can still improve sequential performance. ", "page_idx": 19}, {"type": "text", "text": "C.2 Pass $@\\mathbf{K}$ vs Sequential Sampling via RISE ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We now study the performance of sequential sampling with oracle feedback in GSM8K, unlike relying on majority voting as in Table 1. Specifically, we compare the performance of RISE with early termination of evaluation rollouts against pass $@5$ (not maj $@5$ ) performance of the RISE model at the first turn (which makes an equal number of queries to the ground-truth correctness indicator). Access to ground-truth correctness indicator is expected to improve performance for both parallel and sequential sampling unsurprisingly, but we see in Figure 9 (Right) that RISE is able to improve performance more beyond simply sampling more samples at the first turn and computing pass $@\\,\\mathrm{K}$ , despite this strong assumption of access to an oracle final answer verifier made by the parallel sampling approach. ", "page_idx": 19}, {"type": "image", "img_path": "DRC9pZwBwR/tmp/13916c7eb546c0e6a62540b5c0b0ce379f2ccb7bf8cbfad72448b22f76afb427.jpg", "img_caption": ["Figure 9: Left: The importance of data sources used for training. We study the performance of the iteration 1 of RISE on GSM8K with different data sources. \u201cExpert\u201d refers to the use of an oracle model, \u201cOn-policy\u201d corresponds to sampling from the learner, and \u201cBest-of-N\u201d means using the best sample out of $N$ from the learner (here $N=\\bar{16}$ ). Right: Comparing RISE with oracle error feedback $(p a s s\\\"\\omega I\\stackrel{-}{\\omega}t u r n\\,k;$ solid lines) to parallel sampling of 5 responses at turn 1 (pass@k $@$ turn $^{\\,\\,\\,I}$ ; dashed lines) over number of turns $k$ on the $\\mathbf{X}$ -axis on GSM8K. Observe that sequential sampling with Iteration 1 and Iteration 2 RISE models consistently outperforms parallel sampling for all values of turn $k$ ; and the gap grows as the number of iterations increases. In contrast, this trend is absent for base and SFT models. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "We would expect parallel sampling via pass $@\\,\\mathrm{K}$ to be performant when provided access to oracle answer checking as the model can choose to simply sample $K$ independent responses, if the base model accuracy on this task is reasonable. Pass $@\\,\\mathrm{K}$ $@$ turn 1 also upper bounds the first turn accuracy of any procedure that does not query the oracle (e.g., with verifiers, with majority voting, etc.). Hence, access to oracle answer checking for each individual response presents the strongest result one could expect out of parallel sampling, in one turn. On the other hand, sequential sampling produces correlated samples and hence should, in principle, not be able to improve over parallel sampling, unless the model is unable to use the additional tokens and computation provided by the feedback self-improvement prompt to meaningfully correct itself. Since the sequential performance of the model is larger than the parallel performance above, this means that RISE indeed does this successfully. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "C.3 Inclusion of Correct-to-Correct Data ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Intuitively, self-improvement over turns is largely only possible when the model can learn to verify the correctness of its previous response and decide to appropriately modify its response toward correctness. Thus far, the RISE has only trained on data that showed how to convert incorrect responses to correct responses but never illustrated how the model could act on correct responses. To understand if performance can be boosted by also illustrating examples of how the model could act on correct responses, we ran a number of ablations. We took the RISE data generated during Iteration 1 of training on GSM8K with Llama2-7B and modified the multi-turn rollouts to create several cases. First, we duplicated the correct response appearing at the end of every successful multi-turn rollout and trained for one extra turn. This should teach the model that correct responses should not be modified, unlike incorrect responses appearing in previous turns in the rollout. Second, we also ran a variant in which the correct response appearing at the end of every successful rollout is followed by a different correct response. This variant should teach the model that if it chooses to modify a correct response, it must still produce another correct response. ", "page_idx": 20}, {"type": "table", "img_path": "DRC9pZwBwR/tmp/000184c145e0aa51654a7b2ef4ab0253a6bb87411b91073866e23c42fdad3017.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "As shown in Table 7, all methods improved performance over the base model, though only appending with a successful rollout with a novel correct response leads to best performance. The default design of RISE in the main paper attains a close second position, and repeating a correct response at the end of a successful rollout largely reduces performance. We suspect that the poor performance of repeating the same correct response is largely a result of inducing spurious correlations due to data duplication. ", "page_idx": 20}, {"type": "image", "img_path": "DRC9pZwBwR/tmp/e7c16037f747648b963a8dad5fe24c7107275778e57702c1f515279ab4058c3f.jpg", "img_caption": ["Table 7: Comparison of model performance on GSM8K with different mechanisms of adding correct-tocorrect data in RISE. Values in parentheses indicate improvement over $\\mathrm{m}1\\ @\\mathrm{t}1$ , note that appending a successful rollout with a a novel correct response leads to the highest performance gains. ", "Figure 10: Percentage of correct responses in turn ${\\mathbf{}}T_{i}$ that change to being incorrect in turn $T_{i+1}$ . This figure illustrates the percentage of correct responses that change to incorrect responses across consecutive turns $\\mathbf{\\boldsymbol{T}}_{i}$ to $\\mathrm{T}_{i+1}$ ) for different model configurations. A continuously decreasing trend suggests better self-improvement. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "To further investigate self-improvement capabilities, we analyzed the percentage of correct responses changing to incorrect responses in consecutive turns $\\mathrm{T}i$ to $\\mathrm{T}i+1)$ , as illustrated in Figure 10. Generally, a decreasing trend suggests better self-improvement, while lower absolute values indicate better resistance to noisy feedback. The results reveal unexpected patterns across configurations. The Boost configuration shows the poorest performance, with the highest overall percentages and an increase from turn 4 to 5, suggesting that it struggles to consistently maintain correct responses. Repeating a correct response shows the lowest initial percentage $(6.3\\%)$ but increases from turn 3 onward, indicating potential issues in extended interactions. Both Default RISE and appending a different correct response demonstrate a favorable trend, steadily decreasing from $12.3\\%$ to $3.9\\%$ and from $9.8\\%$ to $3.3\\%$ , respectively, suggesting a good balance between maintaining correct responses and allowing improvements. These findings provide nuanced insights into the stability and selfimprovement capabilities of RISE and align with our earlier observation of its superior performance in overall accuracy. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "C.4 Weak-to-Strong Generalization: RISE on Weak Model Data Improves Strong Models ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we compare the performance of Llama2 and Mistral-7B with RISE in the weak-tostrong setting [4]. Concretely, we are interested in using data generated via RISE with a weak model (Llama2-7B) to train a strong model (Mistral-7B). Our analysis reveals intriguing insights into the transferability of RISE-generated data across models of different capabilities. ", "page_idx": 21}, {"type": "table", "img_path": "DRC9pZwBwR/tmp/afabaf723b828099d5ee1b5611893aa63da061c63c358974d190459d17566978.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 8: Weak-to-strong generalization on GSM8K. Comparing performance of RISE when training on rollouts generated by Llama2-7B vs Mistral-7B. Note that training the Mistral-7B model on rollouts generated by the weaker Llama2-7B with RISE improves performance compared to using data generated by the Mistral-7B model itself. However, the reverse is not true: training the Llama2 model on Mistral\u2019s mistakes leads to worse performance, likely because errors from the Mistral-7B model are harder to comprehend for a worse base model. All values are in $\\%$ accuracy, and values in parentheses indicate improvement over $\\mathrm{m}1\\ @\\mathrm{t}1$ . ", "page_idx": 21}, {"type": "text", "text": "As shown in Table 8, we find that Mistral- $^{7\\mathbf{B}+}$ Iteration 1 data generated from Llama2 outperforms training the Llama2-7B model itself on these data (i.e., Llama $^{2-7\\mathrm{{B}}+}$ Iteration1) on all the metrics reported with particularly significant improvements in multi-turn reasoning $(\\mathrm{m}1\\,@\\mathrm{t}5)$ . In fact, training on multi-turn rollouts from Llama2-7B also outperforms training on on-policy Mistral-7B rollouts as well. Interestingly, we observed that training Llama2-7B on multi-turn rollouts from Mistral-7B performs worse than training on on-policy Llama2-7B rollouts, suggesting that Llama2-7B, despite its lower absolute performance, demonstrates more informative mistakes that can be leveraged to better boost the self-improvement capability. This phenomenon underscores the importance of the quality and nature of errors in the training data, rather than just the overall performance of the model that generates them. These findings collectively suggest that the data generated from a weaker Llama2 model can still be used to induce a self-improvement capability in a stronger model, although the reverse is not true (as is also evident from the fact that using GPT-3.5 rollouts in the boosting phase for training does not improve performance for any model in Table 1). We suspect that this is because the reverse poses a much harder learning problem since a weak model need to internalize the mistakes of a stronger model, resulting in hallucinations and memorization [26]. Note that training on these data does not degrade single-turn performance either. This hints at an added benefti of training with RISE: weak-to-strong generalization, which can be quite useful in practice when rolling out stronger models is expensive. ", "page_idx": 21}, {"type": "text", "text": "D Pseudocode ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Algorithm 1 Data Collection at Iteration $T$ ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1: D\u2032T \u2190D\u2032T \u22121   \n2: for index $i$ in {1, . . . , |D|} do   \n3: $s_{1}\\leftarrow x^{i}$   \n4: for step $T^{\\prime}$ in $\\{1,\\ldots,T-1\\}$ do   \n5: $y_{T^{\\prime}}^{i}\\leftarrow\\arg\\operatorname*{max}\\pi_{\\theta_{T-1}}(\\cdot|(s_{t}^{i},y_{t}^{i},f^{i})_{t=1}^{T^{\\prime}-1}+s_{T^{\\prime}})$   \n6: siT \u2032 $_{\\mathrm{+1}},r_{T^{\\prime}}^{i}\\leftarrow\\mathrm{env.step}(s_{T^{\\prime}}^{i},{\\pmb y}_{T^{\\prime}}^{i})$   \n7: $f_{T^{\\prime}}^{i}=$ retry message + xi   \n8: if $r_{T^{\\prime}}^{i}=1$ then   \n9: break   \n10: end if   \n11: end for   \n12: if $r_{T^{\\prime}}^{i}\\neq1$ then   \n13: $\\begin{array}{r l}&{\\overset{,,,}{T^{\\prime}}^{\\prime}\\leftarrow T^{\\prime}+1}\\\\ &{\\overset{,}{y_{T^{\\prime}}^{i}}\\leftarrow\\arg\\operatorname*{max}\\tilde{\\pi}\\big(\\cdot\\big|\\big(s_{t}^{i},y_{t}^{i},f^{i}\\big)_{t=1}^{T^{\\prime}-1}+s_{T^{\\prime}}\\big)}\\\\ &{s_{T^{\\prime}+1}^{i},r_{T^{\\prime}}^{i}\\leftarrow\\mathrm{env.step}\\big(s_{T^{\\prime}}^{i},y_{T^{\\prime}}^{i}\\big)}\\\\ &{\\mathbf{ud\\:if}}\\\\ &{\\overset{,}{L^{\\prime}}\\leftarrow\\mathcal{D}_{T}^{\\prime}\\cup\\big\\{\\big(s_{t}^{i},y_{t}^{i},f_{t}^{i},r_{t}^{i}\\big)\\big\\}_{t=1}^{T^{\\prime}}}\\end{array}$   \n14:   \n15:   \n1167::   \n18: end for ", "page_idx": 22}, {"type": "text", "text": "Algorithm 2 Inference at iteration $T$ ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1: for index $i$ in {1, . . . , |D|} do   \n2: s1 \u2190xi   \n43:: 5: for sytiTe \u2032p $\\begin{array}{r l}&{y_{T^{\\prime}}^{i}\\leftarrow\\arg\\operatorname*{max}\\pi_{\\theta_{T}}(\\cdot|(s_{t}^{i},y_{t}^{i},f^{i})_{t=\\operatorname*{max}\\{1,T^{\\prime}-T\\}}^{T^{\\prime}-1}+s_{T^{\\prime}})}\\\\ &{s_{T^{\\prime}+1}^{i},r_{T^{\\prime}}^{i}\\leftarrow\\mathrm{env.step}(s_{T^{\\prime}}^{i},y_{T^{\\prime}}^{i})}\\\\ &{f_{T^{\\prime}}^{i}=\\mathrm{retry\\,message}+x^{i}}\\end{array}$ $T^{\\prime}$ $\\{1,\\ldots,N\\}$   \n6:   \n7: end for   \n8: for step $T^{\\prime}$ in $\\{1,\\ldots,N\\}$ do   \n9: $\\tilde{y}_{T^{\\prime}}^{i}\\leftarrow\\mathrm{majority~voting}\\{y_{t}^{i}\\}_{t=1}^{T^{\\prime}}$   \n10: end for   \n11: end for ", "page_idx": 22}, {"type": "text", "text": "E Qualitative Examples from RISE ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we provide some qualitative examples showing the nature of edits made by RISE on various datasets that we train on. We utilize the model from iteration 2 of RISE. ", "page_idx": 23}, {"type": "text", "text": "RISE Example (Small Edits) ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Query: Mike plays ping pong for 40 minutes. In the first 20 minutes, he scores 4 points. In the second 20 minutes, he scores $25\\%$ more points. How many total points did he score? ", "page_idx": 23}, {"type": "text", "text": "Turn-1 Response: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Step 1: In the first 20 minutes, Mike scores 4 points.   \nStep 2: In the second 20 minutes, Mike scores $25\\%$ more points than he did in the first 20 minutes. So, Mike scores $4+4=8$ points in the second 20 minutes.   \nStep 3: To find the total points, we need to add the points scored in the first and second 20 minutes. total points $=4+8=12$ .   \nAnswer: 12 \u2717 ", "page_idx": 23}, {"type": "text", "text": "Turn-2 Response: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "I apologize for the mistake. Let\u2019s solve the problem step by step.   \nStep 1: In the first 20 minutes, Mike scores 4 points.   \nStep 2: In the second 20 minutes, Mike scores $25\\%$ more points than he did in the first 20 minutes. This means he scores $4+4*25\\%=4+1=5$ points in the second 20 minutes. Step 3: To find the total points, we add the points scored in the first and second 20 minutes: $^{4+}$ $5=9$ points.   \nAnswer: 9 \u2713 ", "page_idx": 23}, {"type": "text", "text": "RISE Example (Big Edits) ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Query: In a candy machine, there are 22 more than four times the number of pink gumballs as there are blue gumballs. If there are 12 blue gumballs how many pink ones are there? ", "page_idx": 23}, {"type": "text", "text": "Turn-1 Response: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Step 1: Let\u2019s assume the number of blue gumballs as ${\\bf X}$ . Step 2: According to the given information, there are 22 more than four times the number of pink gumballs as there are blue gumballs. So, the number of pink gumballs can be expressed as $4\\mathrm{x}+22$ . Step 3: We are given that there are 12 blue gumballs, so let\u2019s substitute $\\mathbf{X}$ with 12 in the equation. ", "page_idx": 23}, {"type": "text", "text": "Answer: 54 \u2717 ", "page_idx": 23}, {"type": "text", "text": "Turn-2 Response: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Let\u2019s solve the problem step by step: Step 1: Let\u2019s denote the number of blue gumballs as $\\mathbf{X}$ Step 2: We are given that the number of pink gumballs is 22 more than four times the number of blue gumballs. So, the number of pink gumballs can be expressed as $4\\mathrm{x}+22$ . Step 3: We are also given that there are 12 blue gumballs, so we can substitute $X=12$ into the expression for the number of pink gumballs: $4\\ast12+22=48+22=70$ Therefore, there are boxed70 pink gumballs. ", "page_idx": 23}, {"type": "text", "text": "Answer: 70 \u2713 ", "page_idx": 23}, {"type": "text", "text": "Figure 11: Examples of RISE correct its previous behavior in different modes. Some only make changes to a small part (small edits), some may directly rewrite most of its previous answers (big edits) because the first step in the previous answer is wrong. The mistaken steps of different turns are highlighted in red, and the correct are highlighted in green. This demonstrates shows how RISE can correct its previous answers and finally get to a correct answer. ", "page_idx": 23}, {"type": "text", "text": "RISE Example (Noise Tolerance) ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Query: Ryan plants 2 flowers a day in his garden. After 15 days, how many flowers does he have if 5 did not grow? ", "page_idx": 24}, {"type": "text", "text": "Turn-1 Response: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Step 1: Ryan plants 2 flowers a day for 15 days, so he plants a total of 2 flowers/day $*_{\\mathrm{~15~days}}=30$ flowers in his garden. Step 2: However, 5 flowers did not grow, so we need to subtract these non-growing flowers from the total count. Step 3: Calculate the number of flowers that actually grew in Ryan\u2019s garden. flowers grew $=30$ flowers - 5 flowers $=25$ flowers. ", "page_idx": 24}, {"type": "text", "text": "Answer: 25 ", "page_idx": 24}, {"type": "text", "text": "Turn-2 Response: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Apologies for the mistake. Let\u2019s correct the solution. Step 1: In 15 days, Ryan plants 2 flowers a day, so he plants a total of 2 flowers/day $*_{\\mathrm{~15~days}}=30$ flowers in his garden. Step 2: Since 5 flowers did not grow, we need to subtract these non-growing flowers from the total count. Step 3: Calculate the number of flowers that actually grew in Ryan\u2019s garden. flowers grew $=30$ flowers - 5 flowers $=25$ flowers. Answer: 25 \u2713 ", "page_idx": 24}, {"type": "text", "text": "Turn-3 Response: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Let\u2019s reevaluate the solution. Step 1: Ryan plants 2 flowers a day for 15 days, so he plants a total of 2 flowers/day $*_{\\mathrm{~15~days}}=30$ flowers in his garden. Step 2: However, 5 flowers did not grow, so we need to add these flowers to the total count, as they were planted but did not grow. Step 3: Calculate the total number of flowers in Ryan\u2019s garden. total flowers $=30$ flowers $+\\,5$ flowers $=35$ flowers. ", "page_idx": 24}, {"type": "image", "img_path": "DRC9pZwBwR/tmp/f1062d73574f1e4effe2ec8f92addfa6749d0a09d8429df27832b704a3eaddf5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Turn-4 Response ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Let\u2019s correct the solution. Step 1: In 15 days, Ryan plants 2 flowers a day, so he plants a total of 2 flowers/day $^{*}15$ days $=30$ flowers in his garden. Step 2: Since 5 flowers did not grow, we need to subtract these non-growing flowers from the total count. Step 3: Calculate the total number of flowers in Ryan\u2019s garden after subtracting the non-growing flowers. total flowers $=$ total planted flowers - non growing flowers $=30$ flowers - 5 flowers= ", "page_idx": 24}, {"type": "text", "text": "boxed25 flowers. Answer: 25 \u2713 ", "page_idx": 24}, {"type": "text", "text": "Turn-5 Response ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Let\u2019s reevaluate the solution. Step 1: In 15 days, Ryan plants 2 flowers a day, which is 2 flowers/day $^{*}15$ days $=30$ flowers in total. Step 2: However, 5 flowers did not grow, so we have to subtract these 5 flowers from the total. Step 3: Therefore, the total number of flowers Ryan has is 30 flowers (total planted) - 5 flowers (did not grow) $=$ ", "page_idx": 24}, {"type": "text", "text": "boxed25 flowers. Answer: 25 \u2713 ", "page_idx": 24}, {"type": "text", "text": "Figure 12: Examples of RISE\u2019s tolerance of noisy feedback. In the evaluation where there is no oracle, we use a verifier that always tells the model, \u201dThe answer is incorrect. Please try again. Here\u2019s the question: ${<}\\mathrm{Query}{>}^{,}$ . We observe that from turn 2 to turn 5, the model is still able to output the correct answer in 3 out of 4 turns. In the response at turn 4, the model performs half of the solution correctly, and only the last step is wrong. In addition, in the turns where the model still outputs the correct answer, it doesn\u2019t directly copy the answer from the previous turn but instead comes up with a new paraphrase. This demonstrates that RISE provides the model with tolerance to noisy feedback and does not randomly change its behavior upon receiving a revisiting request. The mistaken steps of different turns are highlighted in red ", "page_idx": 24}, {"type": "text", "text": "RISE Example (Error Localization) ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Query: Josh decides to try flipping a house. He buys a house for $^\\mathrm{\\textregistered80,000}$ and then puts in $\\mathbb{S}50{,}000$ in repairs. This increased the value of the house by $150\\%$ . How much profit did he make? ", "page_idx": 25}, {"type": "text", "text": "Turn-1 Response: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Step 1: Josh buys a house for $^\\mathrm{\\S80{,}000}$ . Step 2: He puts in $\\mathbb{S}50{,}000$ in repairs, which increases the value of the house by $150\\%$ . Step 3: To calculate the increase in value, we multiply the cost of the house by $150\\%$ and add it to the cost of the house. increase in value $=\\mathbb{S}80,000*150\\%=\\mathbb{S}120,000$ . Step 4: To calculate the profit, we subtract the cost of the house from its new value. profit $=$ increase in value - $\\mathbb{S}80{,}000=$ $\\mathbb{S}120,000-\\mathbb{S}80,000=\\mathbb{S}40,000$ . ", "page_idx": 25}, {"type": "text", "text": "Answer: 40, 000 \u2717 ", "page_idx": 25}, {"type": "text", "text": "Turn-2 Response: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Let\u2019s solve the problem step-by-step: Step 1: Josh buys a house for $^\\mathrm{\\textregistered80,000}$ . Step 2: He puts in $\\mathbb{S}50{,}000$ in repairs, and this increases the value of the house by $150\\%$ . Step 3: The percentage increase in value is $150\\%$ of $^{\\S80,000}$ , which is $\\mathbb{S}120{,}000$ $80,000*1.5=\\mathbb{5}120,000)$ . Step 4: To find the actual amount that would be added to the house\u2019s value after repairs, we need to subtract the amount he spent on repairs from the increased value, which is $\\mathbb{S}120,000-\\mathbb{S}50,000=\\mathbb{S}70,000$ . Step 5: The profit Josh would make is the difference between the cost of buying the house and the increased value after repairs, which is $^{\\S70,000\\ -}$ $980{,}000=-910{,}000$ ( $\\mathbb{S}10{,}000$ less than the initial cost). ", "page_idx": 25}, {"type": "text", "text": "Answer: $\\boxed{0}\\pmb{x}$ ", "page_idx": 25}, {"type": "text", "text": "Turn-3 Response: Let\u2019s solve the problem step-by-step: Step 1: Josh buys a house for $^\\mathrm{\\textregistered80,000}$ . Step 2: He puts in $\\mathbb{S}50{,}000$ in repairs. Step 3: This increases the value of the house by $150\\%$ . Step 4: To find the increase in value, we calculate $150\\%$ of the original price of the house: $150\\%$ of $\\mathbb{S}80,000=\\mathbb{S}120,000$ . Step 5: The increase in value after repairs is the original price plus $150\\%$ of the original price: $^{\\mathbb{S}80,000\\,+}$ $\\mathbb{S}120,000=\\mathbb{S}200,000$ . Step 6: Josh sells the house for $\\mathbb{S}200{,}000$ and made a profti of $\\mathbb{S}200,000-\\mathbb{S}80,000$ $=\\mathbb{S}120{,}000$ . ", "page_idx": 25}, {"type": "text", "text": "Answer: 120, 000 \u2717 ", "page_idx": 25}, {"type": "text", "text": "Turn-4 Response: Let\u2019s solve the problem step-by-step: Step 1: Josh buys a house for $^\\mathrm{\\textregistered80,000}$ . Step 2: He puts in $\\mathbb{S}50{,}000$ in repairs, increasing the value of the house by $150\\%$ . Step 3: The increase in value is $150\\%$ of $^\\mathrm{\\textregistered80,000}$ , which is $^{\\mathbb{S}120,000}$ ( $80,000*1.5=\\mathbb{9}120,000)$ . Step 4: The total amount spent, including the purchase of the house and the repairs, is $\\mathbb{S}80,000+\\mathbb{S}50,000=\\mathbb{S}130,000$ . Step 5: The profti Josh made is the increase in value after repairs minus the total amount spent, which is $\\mathbb{S}120\\small{,}000-\\mathbb{S}130\\small{,}000$ $=-810{,}000$ . Therefore, Josh made a ", "page_idx": 25}, {"type": "text", "text": "boxed0 profit. Answer: $:\\boxed{0}\\pmb{x}$ ", "page_idx": 25}, {"type": "text", "text": "Turn-5 Response: Let\u2019s solve the problem step-by-step: Step 1: Josh buys a house for $^\\mathrm{\\textregistered80,000}$ . Step 2: He puts in $\\mathbb{S}50{,}000$ in repairs. Step 3: The repairs increase the value of the house by $150\\%$ . Step 4: The increase in value is $\\mathbb{S}50{,}000$ , which is $150\\%$ of $\\mathbb{S}33{,}333$ ( $50,000\\,/\\,1.5=\\mathbb{5}33,333$ ). Step 5: The repairs cost $\\mathbb{S}50{,}000$ , and the increase in value is $\\mathbb{S}33{,}333$ . Step 6: Therefore, Josh\u2019s profti is $\\mathbb{S}33,\\bar{3}33-\\mathbb{S}50,\\bar{0}00=$ -\\$16,667. Therefore, Josh made a 16676 profit. ", "page_idx": 25}, {"type": "text", "text": "Answer: 16676 \u2717 ", "page_idx": 25}, {"type": "text", "text": "Figure 13: Examples of RISE not being able to correct its error within 5 steps but demonstrating meaningful error localization behavior. Even in turn 2, it has already solved the problem at step 4 but mistakenly takes another step and reaches the wrong answer. The following turns are unable to correct this small error. Though this problem remains unsolved, we observe that (1) the model is able to stick to the correct steps, where all responses reach an intermediate step of 12000 correctly, except for the last response, where the model tries to modify the answer from one step ahead; (2) the model doesn\u2019t repeat its responses, which is a behavior we notice when evaluating some off-the-shelf models; and (3) the model is making meaningful changes to the incorrect steps. In summary, although the final answer is still incorrect, we observe that through RISE, the model is able to locate the error and perform local computation correctly. The mistaken steps of different turns are highlighted in red, and the correct steps in turn 2 is highlighted in green. ", "page_idx": 25}, {"type": "text", "text": "F Experimental Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "F.1 Hyperparameters for Fine-Tuning with RISE ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "For finetuning, we utilize the FastChat codebase, but we customize the loss function to be weighted by reward. The base models are directly loaded from Hugging Face: hrefhttps://huggingface.co/metallama/Llama-2-7b-hfLlama-2-7b-chat-hf and Mistral-7B-Instruct- $\\cdot\\mathrm{v}0.2$ . The hyperparameters used for finetuning are specified in Table 9. ", "page_idx": 26}, {"type": "table", "img_path": "DRC9pZwBwR/tmp/4fc426b0851a9b45cb006476697842097e1af0e50b1ba238349975fdc184fc8f.jpg", "table_caption": [], "table_footnote": ["Table 9: Hyperparameters used for RISE "], "page_idx": 26}, {"type": "text", "text": "F.2 Inference Hyperparameters ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "For API-based models, such as GPT-3.5, we directly query the official web API provided by OpenAI. In the case of open-source models, we utilize FastChat to serve the model as a web API and interact with the environment through API calls. Serving a 7B model requires a single A100 or A40 GPU. To control the randomness and length of answers generated by the LLMs, we employ the hyperparameters specified in Table 10. ", "page_idx": 26}, {"type": "table", "img_path": "DRC9pZwBwR/tmp/1cb4e8e7637f035732565b63048e41db03e242fe12a6446961c281ce36a96a9e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Table 10: The hyperparameter settings used for generating responses from open-source and the GPT models. ", "page_idx": 26}, {"type": "text", "text": "F.3 Datasets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The GSM8K dataset consists of 7,473 problems in the training portion and 1,319 problems in the testing portion. Similarly, the MATH dataset is divided into 7,500 problems for training and 1,000 problems for testing. The training portions of both datasets are used to generate trajectories in each iteration of the RISE method, while the testing portions are held out for evaluating the performance of the models. Additionally, the SVAMP dataset, containing 1,000 problems, is used solely for evaluation purposes to demonstrate the generalizability of our method. ", "page_idx": 26}, {"type": "text", "text": "For generating training trajectories, at iteration $i$ , the current model is rolled out for a maximum of $i$ steps. Subsequently, an expert model, such as GPT-3.5, is employed to take the conversation history between the current model and the environment and sample 5 responses. Among these responses, the one with the highest reward is chosen, with arbitrary tie-breaking in case of multiple responses with the same highest reward. ", "page_idx": 26}, {"type": "text", "text": "F.4 Prompts used for seeding RISE on the GSM8K and MATH tasks ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Here we show the prompt we used for data generation and model evaluation: ", "page_idx": 26}, {"type": "text", "text": "RISE ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "System: You are an AI language model designed to assist with math problem-solving. In this task, I will provide you with math problems. Your goal is to solve the problem step-by-step, showing your reasoning at each step. After you have finished solving the problem, present your final answer as \\boxed{Your Answer}. ", "page_idx": 27}, {"type": "text", "text": "<One-shot Example $15\\textgreater$ ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "User: <Query> ", "page_idx": 27}, {"type": "text", "text": "Agent: <Answer> ", "page_idx": 27}, {"type": "text", "text": "User: The answer is incorrect. Please try again. Here\u2019s the question: <Query> ", "page_idx": 27}, {"type": "text", "text": "Agent: <Answer> ", "page_idx": 27}, {"type": "text", "text": "Figure 14: Prompt for RISE: We follow the standard pipeline of math problem solving, prompt the LLM to solve the problem and retry untill exceeds the maximum number of turns ", "page_idx": 27}, {"type": "text", "text": "F.5 One-shot Example ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "RISE One-shot example ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "User: Of the 60 students in the drama club, 36 take mathematics, 27 take physics and 20 students take both mathematics and physics. How many drama club students take neither mathematics nor physics?   \nAgent: Step 1: To find the number of drama club students who take neither mathematics nor physics, we first need to determine the number of students taking only math, only physics, and both math and physics.   \nStep 2: Out of the 60 students in the drama club, 36 take mathematics and 27 take physics, and 20 students take both mathematics and physics.   \nStep 3: To calculate the number of students taking only math, we subtract the number of students taking both from the total taking math: $936-20=16\\mathbb{S}$ .   \nStep 4: Similarly, to calculate the number of students taking only physics, we subtract the number of students taking both from the total taking physics: $\\mathbb{S}27-20=7\\mathbb{S}$ .   \nStep 5: Now, to find the number of students taking neither math nor physics, we subtract the total number of students taking only math, only physics, and both from the total number of students in the drama club. This gives us $\\mathbb{S}60-16-7-20=\\backslash\\mathrm{boxed}\\{17\\}\\mathbb{\\backslash}$ students taking neither. Answer: \\boxed{17} ", "page_idx": 28}, {"type": "text", "text": "Figure 15: One-shot Example: Demonstrates how to solve a math problem step-by-step and the expected answer format. ", "page_idx": 28}, {"type": "text", "text": "Self-Refine One-shot example ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "User: Of the 60 students in the drama club, 36 take mathematics, 27 take physics and 20 students take both mathematics and physics. How many drama club students take neither mathematics nor physics? ", "page_idx": 29}, {"type": "text", "text": "Agent: Step 1: To find the number of drama club students who take neither mathematics nor physics, we first need to determine the number of students taking only math, only physics, and both math and physics. ", "page_idx": 29}, {"type": "text", "text": "Step 2: Out of the 60 students in the drama club, 36 take mathematics and 27 take physics, and 20 students take both mathematics and physics. ", "page_idx": 29}, {"type": "text", "text": "Step 3: To calculate the number of students taking only math, we subtract the number of students taking both from the total taking math: $936-20=16\\mathbb{S}$ . ", "page_idx": 29}, {"type": "text", "text": "Step 4: Similarly, to calculate the number of students taking only physics, we subtract the number of students taking both from the total taking physics: $\\mathbb{S}27-20=7\\mathbb{S}$ . ", "page_idx": 29}, {"type": "text", "text": "Step 5: Now, to find the number of students taking neither math nor physics, we add the number of students taking only math and only physics, then subtract this sum from the total number of students in the drama club. This gives us $\\dot{\\mathbb{S}}60-(16+7)=\\backslash{\\mathrm{boxed}}\\{37\\}\\mathbb{S}$ students taking neither. ", "page_idx": 29}, {"type": "text", "text": "Answer: \\boxed $\\{37\\}$ ", "page_idx": 29}, {"type": "text", "text": "User: There is an error in the solution above because of lack of understanding of the question. What is the error? To find the error, go through each step of the solution, and check if everything looks good. ", "page_idx": 29}, {"type": "text", "text": "Agent: Step 1: Correct. We need to find the number of students taking neither math nor physics. ", "page_idx": 29}, {"type": "text", "text": "Step 2: Correct. The given information is accurately stated. ", "page_idx": 29}, {"type": "text", "text": "Step 3: Correct. The calculation of students taking only math is correct. ", "page_idx": 29}, {"type": "text", "text": "Step 4: Correct. The calculation of students taking only physics is correct. ", "page_idx": 29}, {"type": "text", "text": "Step 5: Incorrect. The solution adds the number of students taking only math and only physics, but this does not account for the students taking both subjects. By subtracting this sum from the total number of students, the solution double-counts the students who take both math and physics. ", "page_idx": 29}, {"type": "text", "text": "User: Now, rewrite the solution in the required format: Agent: ${<}\\mathbf{S}$ ame as $15\\!>$ ", "page_idx": 29}, {"type": "text", "text": "Figure 16: One-shot Example: Demonstrates the process of self-critique and error correction based on the self-critique. ", "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The claims regarding our approach RISE are justified in the contributions and scope, though the introduction also motivates the self-improvement capability using their require in agent problems. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: Section 6 discusses the limitations of our work pertaining to the factors that affect performance of RISE, access to limited computation implies only a few runs were run for the experiments, the use of a proprietary teacher model since not many 7B base models can provide effective responses. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: No theoretical results. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Appendix F provides a discussion of hyperparameters, reproducing our method, and empirical observations that led up to making those choices. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The code is publicly available at https://github.com/cmu-mind/RISE and Appendix F provides a discussion of the hyperparameters. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Appendix F provides these details. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [No] ", "page_idx": 32}, {"type": "text", "text": "Justification: Due to computational costs, we were unable to run experiments more than once for the paper, though we intend to run experiments with 3 seeds for the final version of the paper. We do remark though that in our initial experiments while developing RISE, we found that the method was not sensitive to hyperparameters indicating that RISE does indeed produce big performance improvements. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 32}, {"type": "text", "text": "\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We include a discussion of compute resources in Appendix F. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We follow the Code of Ethics through the entire study. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We address the broader impacts of this work in Section 6. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: Our paper does not present any new sensitive datasets or pre-trained language models. We only train existing LLMs on standard mathematical reasoning benchmarks. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We use the GSM8K and MATH datasets from HuggingFace, both of which are open datasets with a CC-BY 4.0 license. We also cite the corresponding papers that introduce these datasets. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 34}, {"type": "text", "text": "\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We do provide details of the new data that we collect during the course of learning, and the approach for training with RISE, along with its hyperparameters in Appendix F. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: No human subjects or crowdsourcing. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: No human subjects or crowdsourcing. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]