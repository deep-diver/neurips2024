[{"figure_path": "DRC9pZwBwR/tables/tables_6_1.jpg", "caption": "Table 1: RISE vs. other approaches (Self-Refine, GLORE) and baselines. Observe that RISE attains the biggest performance improvement (in brown) between 1-turn (m5@t1) and 5-turn (m1@t5) performance w/o an oracle on both GSM8K and MATH. This performance gap grows larger when oracle early termination is allowed (p1@t5 w/ oracle). Self-Refine [33] degrades performance across the board when used without an oracle, and attains minor performance improvements with an oracle. GLORE trains a separate refinement model, but performs worse than RISE; more details about it are in Appendix B.1. Using RISE on top of a better base model (Mistral-7B) is also effective (positive improvements with multiple turns), and the m1@t5 performance of Mistral-7B exceeds even state-of-the-art math models such as Eurus-7B-SFT [63]. Simply running single-turn SFT on data utilized by RISE is not effective at inducing a self-improvement capability, implying that algorithmic design choices in RISE are crucial for performance. Color coding indicates numbers that can be compared.", "description": "This table compares the performance of RISE against several baseline methods and other self-improvement approaches on two benchmark datasets (GSM8K and MATH).  It shows the performance improvements achieved by RISE over multiple turns (5 turns), both with and without using an oracle for early termination. The table also highlights the impact of different base models and training strategies on the overall performance gains.", "section": "5.1 Does RISE improve performance over multiple turns compared to other approaches?"}, {"figure_path": "DRC9pZwBwR/tables/tables_7_1.jpg", "caption": "Table 1: RISE vs. other approaches (Self-Refine, GLORE) and baselines. Observe that RISE attains the biggest performance improvement (in brown) between 1-turn (m5@t1) and 5-turn (m1@t5) performance w/o an oracle on both GSM8K and MATH. This performance gap grows larger when oracle early termination is allowed (p1@t5 w/ oracle). Self-Refine [33] degrades performance across the board when used without an oracle, and attains minor performance improvements with an oracle. GLORE trains a separate refinement model, but performs worse than RISE; more details about it are in Appendix B.1. Using RISE on top of a better base model (Mistral-7B) is also effective (positive improvements with multiple turns), and the m1@t5 performance of Mistral-7B exceeds even state-of-the-art math models such as Eurus-7B-SFT [63]. Simply running single-turn SFT on data utilized by RISE is not effective at inducing a self-improvement capability, implying that algorithmic design choices in RISE are crucial for performance. Color coding indicates numbers that can be compared.", "description": "This table compares the performance of RISE against other state-of-the-art methods on two benchmark datasets, GSM8K and MATH. The table shows that RISE significantly improves performance over multiple turns compared to baselines and other self-improvement approaches. Notably, RISE's improvement is more substantial when early termination with an oracle is not allowed, showcasing its effectiveness in test-time self-improvement.", "section": "5.1 Does RISE improve performance over multiple turns compared to other approaches?"}, {"figure_path": "DRC9pZwBwR/tables/tables_16_1.jpg", "caption": "Table 1: RISE vs. other approaches (Self-Refine, GLORE) and baselines. Observe that RISE attains the biggest performance improvement (in brown) between 1-turn (m5@t1) and 5-turn (m1@t5) performance w/o an oracle on both GSM8K and MATH. This performance gap grows larger when oracle early termination is allowed (p1@t5 w/ oracle). Self-Refine [33] degrades performance across the board when used without an oracle, and attains minor performance improvements with an oracle. GLORE trains a separate refinement model, but performs worse than RISE; more details about it are in Appendix B.1. Using RISE on top of a better base model (Mistral-7B) is also effective (positive improvements with multiple turns), and the m1@t5 performance of Mistral-7B exceeds even state-of-the-art math models such as Eurus-7B-SFT [63]. Simply running single-turn SFT on data utilized by RISE is not effective at inducing a self-improvement capability, implying that algorithmic design choices in RISE are crucial for performance. Color coding indicates numbers that can be compared.", "description": "This table compares the performance of RISE against several baseline methods and other approaches on two datasets: GSM8K and MATH.  The results show that RISE consistently outperforms other methods, particularly in multi-turn scenarios where the model iteratively refines its responses.  The table also highlights the impact of using a more capable base model and the importance of the algorithmic design of RISE for achieving self-improvement.", "section": "5.1 Does RISE improve performance over multiple turns compared to other approaches?"}, {"figure_path": "DRC9pZwBwR/tables/tables_18_1.jpg", "caption": "Table 1: RISE vs. other approaches (Self-Refine, GLORE) and baselines. Observe that RISE attains the biggest performance improvement (in brown) between 1-turn (m5@t1) and 5-turn (m1@t5) performance w/o an oracle on both GSM8K and MATH. This performance gap grows larger when oracle early termination is allowed (p1@t5 w/ oracle). Self-Refine [33] degrades performance across the board when used without an oracle, and attains minor performance improvements with an oracle. GLORE trains a separate refinement model, but performs worse than RISE; more details about it are in Appendix B.1. Using RISE on top of a better base model (Mistral-7B) is also effective (positive improvements with multiple turns), and the m1@t5 performance of Mistral-7B exceeds even state-of-the-art math models such as Eurus-7B-SFT [63]. Simply running single-turn SFT on data utilized by RISE is not effective at inducing a self-improvement capability, implying that algorithmic design choices in RISE are crucial for performance. Color coding indicates numbers that can be compared.", "description": "This table compares the performance of RISE against several baseline methods and other state-of-the-art approaches on two benchmark datasets: GSM8K and MATH.  The results show that RISE significantly improves the performance of language models over multiple sequential attempts, outperforming other methods, especially without oracle feedback. It also demonstrates that the improvements are more substantial when using a more capable base model and that simply fine-tuning on the same data used by RISE does not achieve similar results, highlighting the importance of RISE's algorithmic design.", "section": "5.1 Does RISE improve performance over multiple turns compared to other approaches?"}, {"figure_path": "DRC9pZwBwR/tables/tables_18_2.jpg", "caption": "Table 1: RISE vs. other approaches (Self-Refine, GLORE) and baselines. Observe that RISE attains the biggest performance improvement (in brown) between 1-turn (m5@t1) and 5-turn (m1@t5) performance w/o an oracle on both GSM8K and MATH. This performance gap grows larger when oracle early termination is allowed (p1@t5 w/ oracle). Self-Refine [33] degrades performance across the board when used without an oracle, and attains minor performance improvements with an oracle. GLORE trains a separate refinement model, but performs worse than RISE; more details about it are in Appendix B.1. Using RISE on top of a better base model (Mistral-7B) is also effective (positive improvements with multiple turns), and the m1@t5 performance of Mistral-7B exceeds even state-of-the-art math models such as Eurus-7B-SFT [63]. Simply running single-turn SFT on data utilized by RISE is not effective at inducing a self-improvement capability, implying that algorithmic design choices in RISE are crucial for performance. Color coding indicates numbers that can be compared.", "description": "This table compares the performance of RISE against several baselines and other self-improvement methods across two datasets (GSM8K and MATH).  The key finding is that RISE consistently shows significantly greater performance improvements over multiple turns compared to other approaches, especially when early termination using an oracle is not allowed.  Furthermore, even when using a stronger base model, RISE still demonstrates improvements over other methods and achieves state-of-the-art performance in some cases.  The results suggest that the algorithmic design choices in RISE are crucial for its success in enabling self-improvement.", "section": "5.1 Does RISE improve performance over multiple turns compared to other approaches?"}, {"figure_path": "DRC9pZwBwR/tables/tables_19_1.jpg", "caption": "Table 1: RISE vs. other approaches (Self-Refine, GLORE) and baselines. Observe that RISE attains the biggest performance improvement (in brown) between 1-turn (m5@t1) and 5-turn (m1@t5) performance w/o an oracle on both GSM8K and MATH. This performance gap grows larger when oracle early termination is allowed (p1@t5 w/ oracle). Self-Refine [33] degrades performance across the board when used without an oracle, and attains minor performance improvements with an oracle. GLORE trains a separate refinement model, but performs worse than RISE; more details about it are in Appendix B.1. Using RISE on top of a better base model (Mistral-7B) is also effective (positive improvements with multiple turns), and the m1@t5 performance of Mistral-7B exceeds even state-of-the-art math models such as Eurus-7B-SFT [63]. Simply running single-turn SFT on data utilized by RISE is not effective at inducing a self-improvement capability, implying that algorithmic design choices in RISE are crucial for performance. Color coding indicates numbers that can be compared.", "description": "This table compares the performance of RISE against other state-of-the-art methods (Self-Refine, GLORE, V-STAR) and baselines on two benchmark datasets, GSM8K and MATH.  It shows the improvement in performance (measured by m1@t1, m5@t1, m1@t5, and p1@t5 metrics)  when using RISE with different base models (Llama2 and Mistral-7B) and different training iterations. The table highlights that RISE consistently outperforms other methods by achieving significant improvements across both datasets and different evaluation metrics, especially when multiple attempts are allowed.  It also showcases the impact of using a stronger base model and the importance of RISE's unique algorithm design.", "section": "5.1 Does RISE improve performance over multiple turns compared to other approaches?"}, {"figure_path": "DRC9pZwBwR/tables/tables_20_1.jpg", "caption": "Table 1: RISE vs. other approaches (Self-Refine, GLORE) and baselines. Observe that RISE attains the biggest performance improvement (in brown) between 1-turn (m5@t1) and 5-turn (m1@t5) performance w/o an oracle on both GSM8K and MATH. This performance gap grows larger when oracle early termination is allowed (p1@t5 w/ oracle). Self-Refine [33] degrades performance across the board when used without an oracle, and attains minor performance improvements with an oracle. GLORE trains a separate refinement model, but performs worse than RISE; more details about it are in Appendix B.1. Using RISE on top of a better base model (Mistral-7B) is also effective (positive improvements with multiple turns), and the m1@t5 performance of Mistral-7B exceeds even state-of-the-art math models such as Eurus-7B-SFT [63]. Simply running single-turn SFT on data utilized by RISE is not effective at inducing a self-improvement capability, implying that algorithmic design choices in RISE are crucial for performance. Color coding indicates numbers that can be compared.", "description": "This table compares the performance of RISE against several baseline and related methods on two benchmark datasets (GSM8K and MATH).  It shows the accuracy of different models at solving problems in a single attempt (m1@t1 and m5@t1, representing the first turn performance with 1 and 5 samples respectively) and after multiple attempts (m1@t5, representing the accuracy of a single model after 5 iterations, and p1@t5, representing the performance when the model is allowed to terminate early with an oracle for a correct answer). The table highlights RISE's significant performance improvement over multiple turns, especially in comparison to methods such as Self-Refine which show degraded performance, and GLORE which uses multiple models and still underperforms RISE. The results also demonstrate that using RISE with a stronger base model further enhances its effectiveness.", "section": "5.1 Does RISE improve performance over multiple turns compared to other approaches?"}, {"figure_path": "DRC9pZwBwR/tables/tables_21_1.jpg", "caption": "Table 1: RISE vs. other approaches (Self-Refine, GLORE) and baselines. Observe that RISE attains the biggest performance improvement (in brown) between 1-turn (m5@t1) and 5-turn (m1@t5) performance w/o an oracle on both GSM8K and MATH. This performance gap grows larger when oracle early termination is allowed (p1@t5 w/ oracle). Self-Refine [33] degrades performance across the board when used without an oracle, and attains minor performance improvements with an oracle. GLORE trains a separate refinement model, but performs worse than RISE; more details about it are in Appendix B.1. Using RISE on top of a better base model (Mistral-7B) is also effective (positive improvements with multiple turns), and the m1@t5 performance of Mistral-7B exceeds even state-of-the-art math models such as Eurus-7B-SFT [63]. Simply running single-turn SFT on data utilized by RISE is not effective at inducing a self-improvement capability, implying that algorithmic design choices in RISE are crucial for performance. Color coding indicates numbers that can be compared.", "description": "This table compares the performance of RISE against several baselines and other self-improvement methods on two datasets: GSM8K and MATH.  The results show that RISE significantly outperforms the baselines and other methods in terms of improving model performance over multiple turns, especially when early termination with an oracle is not allowed.  The table highlights the importance of RISE's algorithmic design and demonstrates that its performance benefits are transferable to different base models.", "section": "5.1 Does RISE improve performance over multiple turns compared to other approaches?"}, {"figure_path": "DRC9pZwBwR/tables/tables_26_1.jpg", "caption": "Table 1: RISE vs. other approaches (Self-Refine, GLORE) and baselines. Observe that RISE attains the biggest performance improvement (in brown) between 1-turn (m5@t1) and 5-turn (m1@t5) performance w/o an oracle on both GSM8K and MATH. This performance gap grows larger when oracle early termination is allowed (p1@t5 w/ oracle). Self-Refine [33] degrades performance across the board when used without an oracle, and attains minor performance improvements with an oracle. GLORE trains a separate refinement model, but performs worse than RISE; more details about it are in Appendix B.1. Using RISE on top of a better base model (Mistral-7B) is also effective (positive improvements with multiple turns), and the m1@t5 performance of Mistral-7B exceeds even state-of-the-art math models such as Eurus-7B-SFT [63]. Simply running single-turn SFT on data utilized by RISE is not effective at inducing a self-improvement capability, implying that algorithmic design choices in RISE are crucial for performance. Color coding indicates numbers that can be compared.", "description": "This table compares the performance of RISE against several baselines and other self-improvement methods on two benchmark datasets, GSM8K and MATH.  It shows the improvement in performance (percentage of correct answers) after one turn and five turns of reasoning, with and without the help of an oracle providing feedback. The table highlights that RISE significantly outperforms other methods in terms of multi-turn improvement, especially without oracle feedback.  It also demonstrates that using a stronger base model with RISE leads to even greater improvements and that simply fine-tuning on the data generated by RISE is not enough to achieve similar results.", "section": "5.1 Does RISE improve performance over multiple turns compared to other approaches?"}, {"figure_path": "DRC9pZwBwR/tables/tables_26_2.jpg", "caption": "Table 1: RISE vs. other approaches (Self-Refine, GLORE) and baselines. Observe that RISE attains the biggest performance improvement (in brown) between 1-turn (m5@t1) and 5-turn (m1@t5) performance w/o an oracle on both GSM8K and MATH. This performance gap grows larger when oracle early termination is allowed (p1@t5 w/ oracle). Self-Refine [33] degrades performance across the board when used without an oracle, and attains minor performance improvements with an oracle. GLORE trains a separate refinement model, but performs worse than RISE; more details about it are in Appendix B.1. Using RISE on top of a better base model (Mistral-7B) is also effective (positive improvements with multiple turns), and the m1@t5 performance of Mistral-7B exceeds even state-of-the-art math models such as Eurus-7B-SFT [63]. Simply running single-turn SFT on data utilized by RISE is not effective at inducing a self-improvement capability, implying that algorithmic design choices in RISE are crucial for performance. Color coding indicates numbers that can be compared.", "description": "This table compares the performance of RISE against several baselines and other self-improvement methods on two datasets, GSM8K and MATH.  The results demonstrate RISE's superior performance, particularly its ability to significantly improve model performance over multiple turns (5 turns), especially when compared to methods like Self-Refine, which showed degraded performance without an oracle.  The table also highlights the importance of RISE's algorithmic design in achieving these results, as simply fine-tuning on the same data used by RISE didn't produce similar improvements.  The use of a stronger base model (Mistral-7B) further enhanced the performance gains with RISE.", "section": "5.1 Does RISE improve performance over multiple turns compared to other approaches?"}]