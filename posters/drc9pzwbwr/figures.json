[{"figure_path": "DRC9pZwBwR/figures/figures_1_1.jpg", "caption": "Figure 1: Recursive Introspection (RISE). Using multi-round training on on-policy rollouts and supervision from a reward function, RISE trains models that are capable of improving themselves over multiple turns. At inference, we run majority voting on candidate outputs from different turns to obtain the final response.", "description": "This figure illustrates the RISE (Recursive Introspection) model's architecture and inference process.  The model is trained iteratively using on-policy rollouts and a reward function.  In each training iteration, the LLM produces a response, and the next iteration uses this previous response (and possibly additional feedback) to guide the next response. At inference time, the model generates multiple responses over several turns, and the final response is chosen using majority voting.", "section": "RISE: Recursive Introspection for Self-Improvement"}, {"figure_path": "DRC9pZwBwR/figures/figures_2_1.jpg", "caption": "Figure 2: Left: Problem formulation. We convert single-turn problems into multi-turn MDPs as discussed in Section 3.1. The state is given by the prompt, history of prior attempts, and optional feedback from the environment. An action is a response generated from the LLM given the state of multi-turn interaction so far. Right: Data collection. We collect data by unrolling the current model k - 1 times followed by an improved version of the response, which is obtained by either (1) self-distillation: sample multiple responses from the current model, and use the best response, or (2) distillation: obtain oracle responses by querying a more capable model. In either case, RISE then trains on the generated data.", "description": "This figure illustrates the transformation of single-turn question answering into a multi-turn Markov Decision Process (MDP).  The left side shows the single-turn problem, where a query leads to a response, and if incorrect, a reward of 0 and a new state are given.  This repeats until a correct answer is found. The right side depicts the multi-turn MDP data collection process.  Data is collected by simulating multiple turns of the LLM attempting to answer the query.  Improvement is achieved through either self-distillation (the model generating multiple responses, selecting the best) or distillation (using a more capable model). The improved responses and their rewards constitute the training data for RISE.", "section": "3 RISE: Recursive Introspection for Self-Improvement"}, {"figure_path": "DRC9pZwBwR/figures/figures_4_1.jpg", "caption": "Figure 1: Recursive Introspection (RISE). Using multi-round training on on-policy rollouts and supervision from a reward function, RISE trains models that are capable of improving themselves over multiple turns. At inference, we run majority voting on candidate outputs from different turns to obtain the final response.", "description": "The figure illustrates the RISE (Recursive Introspection) model, which uses a multi-round training process on on-policy rollouts and a reward function to enable language models to improve their responses iteratively over multiple turns.  During inference, the model generates several responses over multiple turns and then uses majority voting to determine the final response. This contrasts with standard single-turn methods, enabling the model to refine its answer over time and ultimately yield a better result.", "section": "RISE: Recursive Introspection for Self-Improvement"}, {"figure_path": "DRC9pZwBwR/figures/figures_5_1.jpg", "caption": "Figure 4: Left: The probability of the true answer given the prompt. Observe that model trained with RISE has higher probability for the true answer. Right: The training perplexity (loss) of fitting only the oracle answer or a sequence of answers. Note that fitting a sequence of answers (RISE) reduces the loss more than fitting the oracle answer (Classic).", "description": "This figure shows the results of an experiment comparing two training methods: 'Classic' and 'RISE'. The left panel shows the probability of the true answer given the input prompt for both methods.  It demonstrates that the RISE model consistently achieves a higher probability of generating the correct answer. The right panel displays the training loss (perplexity) for both methods across epochs. Here, the reduction in loss is significantly lower for the classic method, indicating that the RISE method improves the model's ability to fit the target distribution using a sequence of answers.", "section": "4 When and Why is Self-Improvement Over Turns Possible?"}, {"figure_path": "DRC9pZwBwR/figures/figures_5_2.jpg", "caption": "Figure 5: Fraction of problems unsolved by pass@B at first turn that sequential 5-turn RISE sampling solves, where B = 5 \u00d7 k (k is the x-axis). RISE can solve several challenging problems that sampling at the first turn with much larger budgets cannot solve.", "description": "This figure shows the success rate of RISE on problems that could not be solved by sampling at the first turn with a larger budget (pass@B).  The x-axis represents the number of turns (k) in a 5-turn RISE experiment, and the y-axis is the success rate.  The bars show the success rates of the base model after boosting ('Boost Model') and after two iterations of RISE ('Iteration2 Model').  The hatched bars illustrate the improvement achieved by using oracle feedback during the RISE iterations.", "section": "Experimental Evaluation"}, {"figure_path": "DRC9pZwBwR/figures/figures_8_1.jpg", "caption": "Figure 2: Left: Problem formulation. We convert single-turn problems into multi-turn MDPs as discussed in Section 3.1. The state is given by the prompt, history of prior attempts, and optional feedback from the environment. An action is a response generated from the LLM given the state of multi-turn interaction so far. Right: Data collection. We collect data by unrolling the current model k - 1 times followed by an improved version of the response, which is obtained by either (1) self-distillation: sample multiple responses from the current model, and use the best response, or (2) distillation: obtain oracle responses by querying a more capable model. In either case, RISE then trains on the generated data.", "description": "This figure illustrates the process of converting single-turn problems into multi-turn Markov Decision Processes (MDPs) and collecting training data for the RISE model. The left side shows the MDP formulation, where each state represents the prompt and interaction history, actions are LLM responses, and rewards indicate correctness. The right side details the data collection strategy using self-distillation (sampling multiple responses from the current model) or distillation (querying a stronger model) to generate improved responses and build the training dataset.", "section": "3 RISE: Recursive Introspection for Self-Improvement"}, {"figure_path": "DRC9pZwBwR/figures/figures_9_1.jpg", "caption": "Figure 1: Recursive Introspection (RISE). Using multi-round training on on-policy rollouts and supervision from a reward function, RISE trains models that are capable of improving themselves over multiple turns. At inference, we run majority voting on candidate outputs from different turns to obtain the final response.", "description": "The figure illustrates the RISE (Recursive Introspection) model.  It shows how the model iteratively improves its responses over multiple turns.  In each turn, the model generates a response, and then uses this response (along with optional feedback) to inform the next turn's response.  Finally, at inference time, a majority voting mechanism is used to combine the responses from different turns to produce the final, improved output.", "section": "3 RISE: Recursive Introspection for Self-Improvement"}, {"figure_path": "DRC9pZwBwR/figures/figures_19_1.jpg", "caption": "Figure 3: RISE Inference. There are two ways to query the model trained via RISE upon inference: (1) with oracle (Left): each time the model improves its response, it is allowed to check its answer against an environment and terminate early as soon as a correct answer is found; or (2) without oracle (Right): we ask the model to sequentially revise its own responses j times, and perform majority voting on all candidate outputs from different turns to obtain the final response. If the turn number j is larger than the iteration number k, the agent only keeps the most recent history with k interactions to avoid test-time distribution shift.", "description": "This figure illustrates the two inference modes of the RISE model: with and without oracle.  The 'with oracle' mode allows the model to verify its answer against an external environment after each attempt and stop when it finds the correct answer. The 'without oracle' mode makes the model iteratively refine its answer multiple times and uses majority voting to combine candidate outputs from multiple turns to generate the final output.  If the number of turns exceeds the training iterations, the model uses a sliding window of the most recent turns to avoid any distribution shift at test time.", "section": "3.3 Inference at Deployment Time"}, {"figure_path": "DRC9pZwBwR/figures/figures_20_1.jpg", "caption": "Figure 1: Recursive Introspection (RISE). Using multi-round training on on-policy rollouts and supervision from a reward function, RISE trains models that are capable of improving themselves over multiple turns. At inference, we run majority voting on candidate outputs from different turns to obtain the final response.", "description": "This figure illustrates the RISE (Recursive Introspection) model.  The model uses multiple rounds of training with on-policy rollouts and feedback from a reward function to learn how to iteratively improve its responses. During inference, it generates multiple responses over several turns, then employs majority voting to determine the final answer.", "section": "RISE: Recursive Introspection for Self-Improvement"}, {"figure_path": "DRC9pZwBwR/figures/figures_24_1.jpg", "caption": "Figure 1: Recursive Introspection (RISE). Using multi-round training on on-policy rollouts and supervision from a reward function, RISE trains models that are capable of improving themselves over multiple turns. At inference, we run majority voting on candidate outputs from different turns to obtain the final response.", "description": "The figure illustrates the RISE (Recursive Introspection) framework.  It shows a multi-turn training process where an LLM iteratively refines its responses based on feedback (either from a more capable model or self-evaluation) and a reward function. At inference time, a majority vote among the candidate responses from different turns determines the final output.  The diagram visually represents the iterative refinement process, highlighting the key components of the RISE method, namely multi-turn training, on-policy rollouts, supervision, and majority voting.", "section": "RISE: Recursive Introspection for Self-Improvement"}]