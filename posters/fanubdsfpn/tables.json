[{"figure_path": "fAnubdSFpn/tables/tables_7_1.jpg", "caption": "Table 1: The calibration performance of different post-hoc calibration methods. The best results are in bold, and relative improvements over 2nd best result in each section are in red. Results are averaged over five runs with different seeds.", "description": "This table presents a comparison of the calibration performance of several post-hoc calibration methods against the proposed method.  The metrics used are ECE, MCE, and AdaECE, calculated across different datasets and models.  The best-performing method for each metric and dataset/model combination is highlighted in bold, with relative improvements over the second-best method shown in red.  The results are averaged over five independent runs to ensure statistical robustness.", "section": "4.1 Calibration performance with other calibration methods"}, {"figure_path": "fAnubdSFpn/tables/tables_8_1.jpg", "caption": "Table 1: The calibration performance of different post-hoc calibration methods. The best results are in bold, and relative improvements over 2nd best result in each section are in red. Results are averaged over five runs with different seeds.", "description": "This table presents a comparison of different post-hoc calibration methods' performance on various datasets and models.  The metrics used for evaluation are ECE, MCE, and AdaECE.  The best result for each metric and dataset/model combination is shown in bold, and the relative improvement compared to the second-best result is highlighted in red.  The results are averaged over five independent runs, each using a different random seed, to ensure robustness and reliability.", "section": "4.1 Calibration performance with other calibration methods"}, {"figure_path": "fAnubdSFpn/tables/tables_9_1.jpg", "caption": "Table 3: Different optimizer performance in ResNet35 on CIFAR-100. Results are averaged over five runs with different seeds. Adam optimization compromises model accuracy when applied to a dynamic optimization objective using a PID controller approach.", "description": "This table compares the performance of different optimization algorithms (SGD and Adam) with and without the proposed PID controller approach and gradient compensation method. The results are evaluated on the ResNet35 model using the CIFAR-100 dataset. The metrics used for evaluation are accuracy, Expected Calibration Error (ECE), and Adaptive ECE (AdaECE).", "section": "4 Empirical experiments"}, {"figure_path": "fAnubdSFpn/tables/tables_12_1.jpg", "caption": "Table 4: Model calibration of different gradient decay and post-processing calibration. The best results are in bold. Results are averaged over five runs with different seeds. (bins = 10)", "description": "This table presents the calibration performance results for different gradient decay factors (\u03b2) and post-processing calibration methods. The results are compared across various metrics (ECE, MCE) using different models (ResNet18, ResNet34, VGG16) and datasets (CIFAR-100, CIFAR-10, Tiny-ImageNet).  The best performing method for each metric and dataset is highlighted in bold.  The table illustrates how the choice of gradient decay factor impacts calibration performance and how it compares to established post-processing techniques.", "section": "A.2 Experiments for fixed gradient decay rate"}, {"figure_path": "fAnubdSFpn/tables/tables_14_1.jpg", "caption": "Table 5: The performance of ResNet34 on Tiny-ImageNet with different gradient decay. The best results are in bold. Results are averaged over five runs with different seeds. (bins = 10)", "description": "This table presents the performance of ResNet34 model on the Tiny-ImageNet dataset using different gradient decay factors (\u03b2). The metrics evaluated include Top-1 accuracy, Top-5 accuracy, training accuracy, Expected Calibration Error (ECE), and Maximum Calibration Error (MCE).  The best results for each metric are highlighted in bold. The table shows how different gradient decay rates affect both the accuracy and calibration of the model, suggesting an optimal balance.", "section": "A.2 Experiments for fixed gradient decay rate"}, {"figure_path": "fAnubdSFpn/tables/tables_14_2.jpg", "caption": "Table 6: The performance of ResNet50 on Tiny-ImageNet with different gradient decay. The best results are in bold. Results are averaged over five runs with different seeds. (bins = 10)", "description": "This table presents the performance of ResNet50 model on the Tiny-ImageNet dataset when using different gradient decay factors (\u03b2).  It shows the impact of varying \u03b2 on Top-1 accuracy, Top-5 accuracy, training accuracy, Expected Calibration Error (ECE), and Maximum Calibration Error (MCE). The best result for each metric is highlighted in bold.  The experiment was repeated five times with different random seeds, and the average results are reported. The number of bins used in the ECE and MCE calculations is 10.", "section": "A.2 Experiments for fixed gradient decay rate"}]