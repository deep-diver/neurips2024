[{"figure_path": "lPTWdyIY4O/figures/figures_6_1.jpg", "caption": "Figure 4: Evolution of the average training times for the different invariant layers. The parameter n is the size of the groups Cn and Dn. The average and standard deviations are obtained over 10 runs. For all runs, the number of parameters of the complete neural network (filters and MLP) is set to 50000 and 150000 for SO(2) and O(2) respectively. Standard deviations are reported by vertical intervals. When a FFT is available, our selective G-Bispectrum significantly outperforms other complete G-invariant pooling layers in terms of speed. Specifically, when working with C27, training on a dataset of 60000 images takes only 247 seconds, whereas the G-TC requires 1465 seconds.", "description": "This figure compares the training time of different invariant layers (Avg G-pooling, Max G-pooling, Selective G-Bispectrum, and G-TC) in G-CNNs for SO(2) and O(2) MNIST datasets as the size of the discretization group (Cn/Dn) increases. It highlights the significant speed advantage of the Selective G-Bispectrum, especially when using FFT.", "section": "5 Experimental results"}, {"figure_path": "lPTWdyIY4O/figures/figures_8_1.jpg", "caption": "Figure 5: At the top: Evolution of the average classification accuracy with rotated MNIST (SO(2)-MNIST) and rotated-reflected MNIST (O(2)-MNIST) over 10 runs when the number of filters varies from 2 to 20 for the Avg G-pooling, the Max G-pooling, the selective G-Bispectrum and the G-TC. The number of parameters of each model is maintained equal for fair comparison. The standard deviations are represented using vertical intervals. With the selective G-Bispectrum layer, we can reduce the number of convolutional filters needed for a given accuracy. For example, with only K = 2 filters, we achieve 96% accuracy, compared to 63% with the Max G-pooling layer. Our approach allows G-CNNs to maintain competitive accuracy while using smaller neural networks. At the bottom, the same results are displayed with time instead of the number of filters on the x-axis. The dotted lines reproduce the evolution of K from the figures at the top. We can observe that the selective G-Bispectrum is faster than the G-TC when a FFT is available, thus here in the case of SO(2)-MNIST. Recall that an FFT can be implemented for many groups [9].", "description": "This figure compares the performance of four different G-invariant layers (Avg G-pooling, Max G-pooling, Selective G-Bispectrum, and G-TC) in a G-CNN on MNIST and EMNIST datasets.  The top half shows the average classification accuracy as a function of the number of filters (K). The bottom half shows the same accuracy as a function of training time. The results demonstrate that the Selective G-Bispectrum achieves high accuracy with fewer filters and faster training times than the other methods, especially when a Fast Fourier Transform (FFT) is used.", "section": "5 Experimental results"}, {"figure_path": "lPTWdyIY4O/figures/figures_9_1.jpg", "caption": "Figure 6: Adversarial attacks experiments with G = C4. Images are optimized to output, respectively, a target selective G-bispectrum and a target G-Max pooling, obtained from an original image. The initial image is the initialization of the optimization process (3). After training, only for the selective G-Bispectrum (in blue), the recovered image is a copy of the original image up to group action (rotation). This is a numerical illustration of the robustness of the selective G-Bispectrum to adversarial attacks: one can not obtain the same output with an input that is not in the same class. On the other hand, G-Max pooling (in red) outputs a noisy image because it is not complete.", "description": "This figure shows an experiment on adversarial attacks using the cyclic group C4.  The goal is to compare the robustness of the selective G-Bispectrum and the Max G-pooling methods. The experiment optimizes images to produce a target output (either a specific selective G-Bispectrum or Max G-pooling value). The results show that the selective G-Bispectrum is robust, recovering an image identical to the original up to rotation. Conversely, the Max G-pooling method is not robust; it produces noisy images that are not in the same class as the originals. This demonstrates the complete nature of the selective G-Bispectrum and its resistance to adversarial attacks.", "section": "5 Experimental results"}, {"figure_path": "lPTWdyIY4O/figures/figures_9_2.jpg", "caption": "Figure 7: Numerical experiment of signal recovering from original signals {\u0398}1\u2264i\u22645 where \u0398 : G \u2192 R by solving (4) with G = C30. We use the gradient method with Armijo line search to solve (4). The recovered solutions {\u0398}1\u2264i\u22645 are represented and correspond to translations of the original signals. The moduli of the full G-Bispectra are also represented and are identical. This experiment corroborates the completeness of the selective G-Bispectrum since we are able to recover an unknown signal only from the knowledge of its selective G-Bispectrum.", "description": "This figure shows numerical experiments of signal recovering from original signals using the selective G-Bispectrum with the cyclic group C30. The gradient method with Armijo line search is used to solve the optimization problem.  The recovered signals are shown to be translations of the original signals, and their full G-Bispectra moduli are identical. This confirms the completeness of the selective G-Bispectrum, demonstrating that an unknown signal can be recovered using only its selective G-Bispectrum.", "section": "Theory: Completeness of the Selective G-Bispectrum"}, {"figure_path": "lPTWdyIY4O/figures/figures_13_1.jpg", "caption": "Figure 8: Illustration of the concepts of excessive and complete invariance to a group action. With the excessive invariance, samples from different classes can be mapped to the same output.", "description": "This figure illustrates the difference between excessive and complete invariance in the context of group actions.  In the case of excessive invariance, different input samples (e.g., images of different objects) can be transformed by a group action (e.g., rotations) to yield the same output. This leads to a loss of information and reduces the effectiveness of the process.  In contrast, complete invariance preserves all characteristics of an input sample up to the group action. This ensures that only inputs that are truly equivalent (i.e., differ only by the group action) will produce the same output. The figure uses a bispectrum to demonstrate how complete invariance avoids collapsing different classes into one.", "section": "Background: G-Triple Correlation and G-Bispectrum"}, {"figure_path": "lPTWdyIY4O/figures/figures_15_1.jpg", "caption": "Figure 9: Illustration of Algorithm 2. The Bispectrum coefficients allow to recover the Fourier transform sequentially, up to group action. First, \u03b2\u03c1\u03bf,\u03c1\u03bf gives Fpo, which, combined with \u03b2\u03c10,p1 gives Fp1, etc.", "description": "This figure is an illustration of the process described in Algorithm 2. It shows how the G-Bispectrum coefficients are used sequentially to recover the Fourier transform. The process starts with \u03b2\u03c1\u03bf,\u03c1\u03bf which gives Fpo. Then \u03b2\u03c10,p1 is used with Fpo to obtain Fp1 and so on until all Fourier transforms are recovered.", "section": "C Selective G-Bispectrum inversion: known results"}, {"figure_path": "lPTWdyIY4O/figures/figures_15_2.jpg", "caption": "Figure 10: Representation of the sets K1, K2 and K3 for G = (Z/3Z)3.", "description": "This figure visually represents the sets K1, K2, and K3 for the group G=(Z/3Z)3.  These sets are recursively constructed in Algorithm 3 to generate the full group G from its generating set. Each set represents a stage in the iterative process of building up the group, starting with K1 and culminating in K3, which equals the entire group. The figure helps visualize how the algorithm systematically covers the entire group space by adding elements layer-by-layer.", "section": "D Selective G-Bispectrum inversion: commutative groups"}, {"figure_path": "lPTWdyIY4O/figures/figures_17_1.jpg", "caption": "Figure 1: Illustration of the different proposed G-CNN modules [6, 28]. The input f is first processed through the G-convolutional layer composed of K filters {$k}_{k=1}^{K}$. Then, an invariant layer is chosen (Max G-pooling, G-TC, or the selective/full G-Bispectrum layer). Finally, the \"pooled\" output is fed to a neural network designed for the machine learning task at hand.", "description": "This figure illustrates the architecture of a G-CNN, highlighting the different layers involved in achieving G-invariance. The input f undergoes a G-convolution with K filters, followed by a choice of invariant layer: Max G-pooling, G-Triple Correlation (G-TC), or selective/full G-Bispectrum. The output of the invariant layer is then fed into a neural network for classification.", "section": "Application: G-invariant layers"}]