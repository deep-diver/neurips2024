[{"figure_path": "B1Iq1EOiVU/figures/figures_3_1.jpg", "caption": "Figure 2: Structure overview of DeformableTST. (a) The input time series is embedded variate-independently. (b) The local perception unit (LPU) is used to learn the local temporal information. (c) The proposed deformable attention is adopted to learn the global temporal information. (d) The feed-forward network injected with a depth-wise convolution (ConvFFN) is used to learn the local temporal information and the new feature representation.", "description": "This figure presents a detailed architecture of the proposed DeformableTST model for time series forecasting.  It's broken down into four key components:\n(a) **Input Embedding Layer:** The input time series is embedded independently for each variate.\n(b) **Local Perception Unit (LPU):** This module processes the embedded input, focusing on learning local temporal relationships using a depth-wise convolution and residual connection.\n(c) **Deformable Attention:** This is a crucial component of the model that captures long-range temporal dependencies using a sparse attention mechanism which focuses on important time points to learn non-trivial temporal representations.\n(d) **Feed-Forward Network with Convolution (ConvFFN):** This module further refines the feature representation learned from the deformable attention, incorporating both local and global temporal information through a depth-wise convolution and GELU activation.  The hierarchical structure is also highlighted, showing how the LPU and deformable attention work together within each Transformer block.", "section": "3 DeformableTST"}, {"figure_path": "B1Iq1EOiVU/figures/figures_4_1.jpg", "caption": "Figure 3: Deformable Attention. (a) The process of deformable attention from the tensor view and coordinate view. (b) The structure of the offset network, marked with the size of feature series.", "description": "This figure illustrates the deformable attention mechanism. (a) shows the process from both tensor and coordinate views. The input feature series X first samples a few important time points based on learnable offsets. Then these sampled points are fed to key and value projections (K, V), while X is projected to queries Q. Finally, multi-head attention is applied to Q, K, and V to get the output O. (b) shows the structure of the offset network that generates the offsets.  It consists of a depthwise convolution followed by a pointwise convolution, using GELU activation and producing offsets \u2206T.", "section": "3.2 Deformable Attention"}, {"figure_path": "B1Iq1EOiVU/figures/figures_5_1.jpg", "caption": "Figure 4: Model performance comparison (left) and performance under different input lengths (right).", "description": "The left part of the figure is a radar chart comparing the performance of DeformableTST against other state-of-the-art models across various time series forecasting tasks: univariate short-term forecasting (SMAPE), multivariate short-term forecasting (MSE), long-term forecasting with input lengths 96, 384, and 768 (MSE).  The right part shows a line graph illustrating how the MSE changes for each model with different input lengths. This visualization helps understand the model's adaptability to various input lengths and task types, especially highlighting DeformableTST's consistent superior performance.", "section": "4 Experiments"}, {"figure_path": "B1Iq1EOiVU/figures/figures_8_1.jpg", "caption": "Figure 5: Ablation study in long-term forecasting tasks. From the top to the bottom, each row means one design that we add on PatchTST to modify it into our DeformableTST. We report the averaged MSE of four prediction lengths. The memory usage is recorded under input-96-predict-96 setting with the same batch size. A lower MSE (a shorter blue bar) and a smaller memory usage (a green star closer to the vertical axis) means better performance and efficiency. More results are in Appendix F.1.", "description": "This ablation study visualizes the effect of each design choice made to improve PatchTST and arrive at DeformableTST.  It shows the impact of removing patching, adding a hierarchical structure, incorporating deformable attention, and adding local enhancement modules on both model performance (MSE) and memory usage. The results demonstrate that each addition contributes to improved performance and efficiency.", "section": "5 Model Analysis"}, {"figure_path": "B1Iq1EOiVU/figures/figures_19_1.jpg", "caption": "Figure 4: Model performance comparison (left) and performance under different input lengths (right).", "description": "This figure presents a comparison of the model's performance against other state-of-the-art models on various time series forecasting tasks.  The left panel shows a performance comparison across different datasets, illustrating the model's consistent superiority. The right panel visualizes performance as a function of input length, demonstrating the model's adaptability and effectiveness even with shorter sequences, which is a key advantage over models that heavily rely on patching.", "section": "4 Experiments"}, {"figure_path": "B1Iq1EOiVU/figures/figures_20_1.jpg", "caption": "Figure 1: The Effective Receptive Field (ERF) of PatchTST. A brighter area means that these time points are focused by the model when extracting temporal representation. The results show that PatchTST highly relies on the guidance of patching to focus on the important time points. This phenomenon is also present in multiple advanced patch-based Transformer forecasters (Appendix E).", "description": "The figure visualizes the effective receptive fields (ERFs) of the PatchTST model with and without patching.  The ERF shows which time points the model focuses on when extracting temporal representations.  The visualization demonstrates that when using patching (dividing the time series into patches), PatchTST concentrates attention on a smaller set of important time points, achieving better performance.  In contrast, without patching, the model's attention is spread thinly across nearly all time points, resulting in poorer performance. This highlights PatchTST's over-reliance on the patching technique. The same phenomenon is also observed in other advanced patch-based Transformer models. ", "section": "1 Introduction"}, {"figure_path": "B1Iq1EOiVU/figures/figures_20_2.jpg", "caption": "Figure 1: The Effective Receptive Field (ERF) of PatchTST. A brighter area means that these time points are focused by the model when extracting temporal representation. The results show that PatchTST highly relies on the guidance of patching to focus on the important time points. This phenomenon is also present in multiple advanced patch-based Transformer forecasters (Appendix E).", "description": "This figure visualizes the effective receptive fields (ERFs) of the PatchTST model with and without patching.  The ERF shows which parts of the time series the model focuses on when learning temporal representations. The visualization reveals that when using patching, the model focuses on a smaller subset of key time points, while without patching, the model attends to almost all time points equally. This demonstrates PatchTST's strong reliance on patching to achieve optimal performance. The appendix further supports this finding by showing that multiple advanced patch-based transformer models exhibit the same behavior.", "section": "1 Introduction"}, {"figure_path": "B1Iq1EOiVU/figures/figures_20_3.jpg", "caption": "Figure 1: The Effective Receptive Field (ERF) of PatchTST. A brighter area means that these time points are focused by the model when extracting temporal representation. The results show that PatchTST highly relies on the guidance of patching to focus on the important time points. This phenomenon is also present in multiple advanced patch-based Transformer forecasters (Appendix E).", "description": "The figure visualizes the effective receptive fields (ERFs) of the PatchTST model with and without patching.  The ERF shows which time points in the input time series are focused on by the model during temporal representation learning.  The results demonstrate that PatchTST heavily relies on patching to effectively focus on important time points, highlighting a potential over-reliance on this technique in current Transformer-based forecasting models. The phenomenon is also observed in other advanced patch-based Transformer models.", "section": "1 Introduction"}, {"figure_path": "B1Iq1EOiVU/figures/figures_20_4.jpg", "caption": "Figure 8: The Effective Receptive Field (ERF) of DeformableTST. A brighter area means that these time points are focused by the model when extracting temporal representation.", "description": "This figure visualizes the effective receptive fields (ERFs) of the DeformableTST model. The ERF shows which parts of the input time series are focused by the model when extracting temporal representations. Brighter areas indicate that those time points are more important for the model to focus on when learning temporal representations. This visualization helps to understand how the DeformableTST model focuses on important time points to learn non-trivial temporal representation, which is crucial for accurate time series forecasting, especially when dealing with tasks where patching is not suitable.", "section": "3 DeformableTST"}, {"figure_path": "B1Iq1EOiVU/figures/figures_20_5.jpg", "caption": "Figure 5: Ablation study in long-term forecasting tasks. From the top to the bottom, each row means one design that we add on PatchTST to modify it into our DeformableTST. We report the averaged MSE of four prediction lengths. The memory usage is recorded under input-96-predict-96 setting with the same batch size. A lower MSE (a shorter blue bar) and a smaller memory usage (a green star closer to the vertical axis) means better performance and efficiency. More results are in Appendix F.1.", "description": "This ablation study shows the effect of each component in DeformableTST by gradually adding components to PatchTST.  It demonstrates that removing patching initially worsens performance and memory usage.  However, adding hierarchical structure, deformable attention, and local enhancement improves performance and reduces memory usage, ultimately leading to DeformableTST's superior performance and efficiency compared to the original PatchTST.", "section": "5 Model Analysis"}, {"figure_path": "B1Iq1EOiVU/figures/figures_20_6.jpg", "caption": "Figure 5: Ablation study in long-term forecasting tasks. From the top to the bottom, each row means one design that we add on PatchTST to modify it into our DeformableTST. We report the averaged MSE of four prediction lengths. The memory usage is recorded under input-96-predict-96 setting with the same batch size. A lower MSE (a shorter blue bar) and a smaller memory usage (a green star closer to the vertical axis) means better performance and efficiency. More results are in Appendix F.1.", "description": "This ablation study shows the impact of each component of DeformableTST on the model's performance and memory usage. Starting with PatchTST, each component is added sequentially, showing the improvements in MSE and memory usage.  The results demonstrate that removing patching initially hurts performance, but adding hierarchical structure, deformable attention, and local enhancement improves it again, resulting in a more efficient and better-performing model than PatchTST.", "section": "5 Model Analysis"}, {"figure_path": "B1Iq1EOiVU/figures/figures_20_7.jpg", "caption": "Figure 5: Ablation study in long-term forecasting tasks. From the top to the bottom, each row means one design that we add on PatchTST to modify it into our DeformableTST. We report the averaged MSE of four prediction lengths. The memory usage is recorded under input-96-predict-96 setting with the same batch size. A lower MSE (a shorter blue bar) and a smaller memory usage (a green star closer to the vertical axis) means better performance and efficiency. More results are in Appendix F.1.", "description": "This ablation study demonstrates the impact of each design choice in DeformableTST on its performance and memory usage.  Starting from PatchTST, modifications are made sequentially: removing patching, adding a hierarchical structure, incorporating deformable attention, and finally adding local enhancement.  The results show that while removing patching initially hurts performance and significantly increases memory usage, the subsequent design choices mitigate these issues and lead to a superior model (DeformableTST) with better performance and lower memory.", "section": "5 Model Analysis"}, {"figure_path": "B1Iq1EOiVU/figures/figures_21_1.jpg", "caption": "Figure 3: Deformable Attention. (a) The process of deformable attention from the tensor view and coordinate view. (b) The structure of the offset network, marked with the size of feature series.", "description": "This figure illustrates the deformable attention mechanism proposed in the paper.  Panel (a) shows the process of deformable attention from both tensor and coordinate perspectives, highlighting the sampling of important time points from the input feature series based on learnable offsets.  These points are used to compute the attention mechanism. Panel (b) details the structure of the offset network, which generates these learnable offsets. The network's input is the query tokens, and it uses depth-wise and point-wise convolutions with a GeLU activation to output the offsets.", "section": "3.2 Deformable Attention"}, {"figure_path": "B1Iq1EOiVU/figures/figures_21_2.jpg", "caption": "Figure 4: Model performance comparison (left) and performance under different input lengths (right).", "description": "The figure on the left shows the overall performance of DeformableTST against other models on various time series forecasting tasks, indicating its state-of-the-art performance.  The figure on the right specifically analyzes performance across different input lengths, highlighting DeformableTST's consistent high performance and adaptability to diverse input lengths compared to other models, which tend to struggle with shorter sequences.", "section": "4 Experiments"}, {"figure_path": "B1Iq1EOiVU/figures/figures_30_1.jpg", "caption": "Figure 4: Model performance comparison (left) and performance under different input lengths (right).", "description": "This figure presents a comparison of the model's performance against other state-of-the-art models. The left panel displays a comparison of the overall performance across various time series forecasting tasks, while the right panel shows a performance comparison under different input lengths.  The results demonstrate that DeformableTST consistently outperforms other methods, especially for tasks with shorter input lengths, unsuitable for traditional patching techniques.", "section": "4 Experiments"}, {"figure_path": "B1Iq1EOiVU/figures/figures_30_2.jpg", "caption": "Figure 4: Model performance comparison (left) and performance under different input lengths (right).", "description": "This figure shows a comparison of the model's performance with other state-of-the-art models on several time series forecasting tasks. The left panel presents a comparison of the overall performance (measured by MSE for long-term forecasting tasks and SMAPE for short-term forecasting tasks) across different datasets.  The right panel shows how the model's performance changes with varying input sequence lengths, highlighting the model's ability to adapt to various input lengths and perform consistently well.", "section": "4 Experiments"}, {"figure_path": "B1Iq1EOiVU/figures/figures_33_1.jpg", "caption": "Figure 1: The Effective Receptive Field (ERF) of PatchTST. A brighter area means that these time points are focused by the model when extracting temporal representation. The results show that PatchTST highly relies on the guidance of patching to focus on the important time points. This phenomenon is also present in multiple advanced patch-based Transformer forecasters (Appendix E).", "description": "The figure visualizes the effective receptive field (ERF) of the PatchTST model, highlighting its reliance on patching to focus on important time points during temporal representation extraction.  It contrasts the ERF when using patching (focusing on key points) versus not using patching (focusing on almost all points equally), illustrating how patching guides the model toward more meaningful temporal representations.  The results demonstrate that without patching, the model has not effectively learned the importance of individual time points, leading to inferior forecasting performance.  This over-reliance on patching is also seen in other advanced patch-based Transformer models.", "section": "1 Introduction"}, {"figure_path": "B1Iq1EOiVU/figures/figures_33_2.jpg", "caption": "Figure 4: Model performance comparison (left) and performance under different input lengths (right).", "description": "This figure shows a comparison of the model's performance against other state-of-the-art models on various time series forecasting tasks.  The left panel presents a comparison of overall performance metrics (e.g., MSE, SMAPE) across different datasets, while the right panel analyzes how the model's performance changes with varying input lengths.  This illustrates the model's adaptability to a wide range of input sizes and its ability to maintain strong performance even with shorter input sequences, which is a key advantage highlighted in the paper.", "section": "4 Experiments"}, {"figure_path": "B1Iq1EOiVU/figures/figures_33_3.jpg", "caption": "Figure 4: Model performance comparison (left) and performance under different input lengths (right).", "description": "This figure presents a comprehensive comparison of DeformableTST's performance against various state-of-the-art models across different time series forecasting tasks.  The left panel shows the model's overall performance across multiple tasks, while the right panel displays performance under varying input lengths.  This demonstrates DeformableTST's adaptability and consistent performance across a broader range of forecasting scenarios compared to existing methods.", "section": "4 Experiments"}, {"figure_path": "B1Iq1EOiVU/figures/figures_34_1.jpg", "caption": "Figure 6: Parameter sensitivity. For patch size, we conduct experiments under input-384-predict-96 settings and adopt PatchTST as comparison. For other parameters, we conduct experiments under input-96-predict-96 settings.", "description": "The figure shows the sensitivity analysis of the DeformableTST model's performance to different hyperparameters. For the patch size, experiments are conducted with input length 384 and prediction length 96, using PatchTST as a baseline for comparison. For other hyperparameters (model dimension, FFN expansion, number of blocks, number of important time points, and learning rate), experiments are conducted with input length 96 and prediction length 96. The results visualize the robustness of the model to the hyperparameter choices and demonstrate that its performance is relatively stable across various settings.", "section": "5 Model Analysis"}]