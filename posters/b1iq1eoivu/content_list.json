[{"type": "text", "text": "DeformableTST: Transformer for Time Series Forecasting without Over-reliance on Patching ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Donghao Luo, Xue Wang Department of Precision Instrument, Tsinghua University, Beijing 100084, China ldh21@mails.tsinghua.edu.cn, wangxue@mail.tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "With the proposal of patching technique in time series forecasting, Transformerbased models have achieved compelling performance and gained great interest from the time series community. But at the same time, we observe a new problem that the recent Transformer-based models are overly reliant on patching to achieve ideal performance, which limits their applicability to some forecasting tasks unsuitable for patching. In this paper, we intent to handle this emerging issue. Through diving into the relationship between patching and full attention (the core mechanism in Transformer-based models), we further find out the reason behind this issue is that full attention relies overly on the guidance of patching to focus on the important time points and learn non-trivial temporal representation. Based on this finding, we propose DeformableTST as an effective solution to this emerging issue. Specifically, we propose deformable attention, a sparse attention mechanism that can better focus on the important time points by itself, to get rid of the need of patching. And we also adopt a hierarchical structure to alleviate the efficiency issue caused by the removal of patching. Experimentally, our DeformableTST achieves the consistent state-of-the-art performance in a broader range of time series tasks, especially achieving promising performance in forecasting tasks unsuitable for patching, therefore successfully reducing the reliance on patching and broadening the applicability of Transformer-based models. Code is available at this repository: https://github.com/luodhhh/DeformableTST. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Time series forecasting is widely used in real-world applications, such as transportation management [39, 5], economic planning [41, 30, 31], energy planning [42, 34] and weather forecasting [45]. Because of the immense practical value, time series forecasting has received great attention and has grown tremendously in recent years [44, 19, 33, 2, 28, 32, 25]. ", "page_idx": 0}, {"type": "text", "text": "But looking back at the development of time series forecasting, Transformer-based models, who have sparked the boom of time series forecasting [55, 47, 57], are constantly being challenged. In particular, some recent studies [52, 18, 22] have questioned that attention mechanism is not suitable for modeling the temporal dependency in time series. As the early strike back of Transformer-based models, PatchTST [35] proposes that attention mechanism can work better in temporal modeling with the help of large size patching technique. Afterwards, equipped with the growing patch size and increasing input length, the advanced Transformer-based models [53, 58, 51, 6] gain great performance improvement and successfully win back the championship in time series forecasting. ", "page_idx": 0}, {"type": "text", "text": "However, with large size patching becoming a must-have technique for the following Transformerbased models, a new problem occurs: patched-based Transformers have to work with a very long input length and a very large patch size to achieve ideal performance [35, 53, 22]. But large size patching cannot be apply to all kinds of time series forecasting tasks. For example, some forecasting ", "page_idx": 0}, {"type": "text", "text": "Figure 1: The Effective Receptive Field (ERF) of PatchTST. A brighter area means that these time points are focused by the model when extracting temporal representation. The results show that PatchTST highly relies on the guidance of patching to focus on the important time points. This phenomenon is also present in multiple advanced patch-based Transformer forecasters (Appendix E). ", "page_idx": 1}, {"type": "text", "text": "tasks are with limited input lengths [29, 30, 31], which are not sufficient to be divided into patches. In such condition, the advanced Transformer-based models suffer from severe performance degradation due to the lack of patching [58, 51], limiting their applicability to a wider range of forecasting tasks. ", "page_idx": 1}, {"type": "text", "text": "To broaden the applicability of the Transformer-based model, we need to design an attention mechanism that is less reliant on patching (e.g., can work well with a small patch size or can work well even without patching). To this end, we first analyze exactly why attention must work with patching and why patching can help attention better model the temporal dependency in time series forecasting? We visualize the effective receptive fields (ERFs) of PatchTST [35] in Figure 1. And the ERFs can indicate which parts of the time points in input series are focused by the model when extracting temporal representations. A surprising finding is shown in Figure 1 (left). If without patching, nearly all time points in input series are equally focused by the model and the model performs worse (MSE 0.385), exposing the problem of distracted attention. This finding means that attention has not learned to distinguish the importance of each time point in input series, leading to trivial representation. Note that the time points in a time series are very redundant or even noisy [35, 56, 55, 7, 53], focusing on the trivial part of them will influence the predictions. Thus, an ideal time series forecaster should mainly focus on a small number of important time points which make contribution to better performance and reflect the property of time series. In Figure 1 (right), when using patching, the model focuses on some selected time points and achieve better performance (MSE 0.367), indicating that the model has successfully focused on the important time points. And in terms of why patching can guide the model to learn a non-trivial representation, we find that the pattern of ERF is also divided by patches, which means that patching can force the model to only focus on a small number of important time points based on the patch partition. As a conclusion of above discussion on Figure 1, since full attention is unable to focus on the important time points by itself, it highly relies on the guidance of patching to focus on the important time points and learn non-trivial representation. This is the reason why full attention must work with patching to achieve ideal performance. ", "page_idx": 1}, {"type": "text", "text": "Therefore, if we can find another way to help attention focus on the important time points, we can get rid of over-reliance on patching. Since full attention is hard to focus due to the redundancy in time series data [35, 56, 55, 7, 53], replacing it with sparse attention can be a natural idea. There are some previous prior-based sparse attentions in time series community [55, 47, 57]. But due to the diverse pattern in different time series, their priors are hard to match all kinds of inputs, resulting in their inferior performance. Different from them, we introduce a data-driven sparse attention called deformable attention under the inspiration of deformable operations [8, 60, 48]. It can sample a subset of important time points from the input series based on the learnable offsets and only calculate attention with these selected important time points. These learnable offsets are learned from each input sample, therefore being more flexible to the diverse property in different time series. ", "page_idx": 1}, {"type": "text", "text": "Based on the above motivations, we intend to broaden the applicability of Transformer-based models. To accomplish this goal, we propose DeformableTST, a Transformer-based model that is less reliant on patching. Technically, the patching process in our method is optional. We remove the patching process in most cases. Only when the input length is very long, we will use a small size patching for better efficiency. Since the removal of patching will cause severe memory usage in previous plain architecture, we adopt a hierarchical architecture to alleviate this efficiency issue. And we further introduce deformable attention, a data-driven sparse attention that can better focus on the important time points by itself, to achieve excellent performance without patching. Experimentally, DeformableTST achieves the consistent state-of-the-art performance in a wider range of time series tasks, especially in tasks unsuitable for patching, thus successfully reducing the reliance on patching and broadening the applicability of Transformer-based models. Our contribution are as follows: ", "page_idx": 1}, {"type": "text", "text": "find out the reason behind this problem is that full attention relies overly on the guidance of patching to focus on important time points and learn non-trivial temporal representation. ", "page_idx": 2}, {"type": "text", "text": "\u2022 To get rid of the over-reliance on patching, we propose DeformableTST and achieve the consistent state-of-the-art performance in a wider range of time series forecasting tasks. Experimental results show that our deformable attention can better model the temporal dependency in time series without reliance on patching.   \n\u2022 We successfully broaden the applicability of Transformer-based models in time series tasks. Our DeformableTST can flexibly adapt to multiple input lengths and achieve excellent performance in tasks unsuitable for patching, which is a great improvement than previous Transformer-based models. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Tranformers for Time Series Forecasting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Transformer-based models mainly use attention mechanism to model the temporal dependency in time series [55, 47, 57]. In 2020s, they achieve excellent performance in time series forecasting for the first time and bring great attention to time series forecasting tasks [26, 9, 56, 20, 21, 7]. But their validity is questioned by [52, 18] with the finding that a simple linear layer can outperform complicated attention mechanisms. It\u2019s until the proposal of patching that Transformer-based models win back the championship in time series forecasting [35]. Based on patching technique, Pathformer [6] adopts a multi-scale patches structure. Crossformer [53] and CARD [51] further propose to additionally apply attention on variate and feature dimensions rather than only on temporal dimension. Sageformer [54] combines the graph methods with patch-based Transformer forecasters. And GPT4TS [58] also transfers pre-trained large language models to time series with the help of patching. But the question of whether attention is suitable for modeling the temporal dependency in time series still remains. For example, although adopting a Transformer architecture, iTransformer [22] still suggests that linear layers are more appropriate for temporal modeling. Meanwhile, the proposal of patching also comes with a new question that advanced Transformer-based models are too reliant on patching. Therefore, further research about Transformer-based forecasters are still needed, especially on the question of how to better use attention in temporal modeling without over-reliance on patching. ", "page_idx": 2}, {"type": "text", "text": "2.2 Sparse Attention ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Sparse attention used to be popular in time series forecasting. Early Transformer-based models usually adopt prior-based sparse attention mechanisms. Informer [55] adopt ProbSparse attention to model the temporal dependency. Autoformer and FEDformer [47, 57] further combine the signal processing technique with the attention mechanisms and select the top-k sparse representation in time domain or frequency domain respectively. But due to the diverse pattern in different time series, these priors are hard to match all kind of inputs, resulting in their inferior performance. As a comparison, data-driven sparse attention, also called deformable attention, is more flexible to diverse inputs. Similar idea has been explored in Computer Vision (CV). Inspired by deformable convolution [8, 59], deformable DERT [60] proposes multi-scale deformable attention for object detection tasks. And [48, 49] further improve it and make it suitable for general CV tasks. In this work, we propose a deformable attention for time series forecasting to break through the bottleneck faced by previous attention mechanism in modeling temporal dependency. ", "page_idx": 2}, {"type": "text", "text": "3 DeformableTST ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given an observed multivariate or univariate time series as input, time series forecasting aims to predict the length- $\\boldsymbol{\\cdot}\\boldsymbol{T}$ future series based on the length- $I$ input series. In real-world scenarios, the input length $I$ varies from a wide range and is not always sufficient for patching technique, leading to the limited applicability of previous patch-based Transformer forecasters. To tackle this problem, we propose DeformableTST. And we introduce details of DeformableTST in following subsections. ", "page_idx": 2}, {"type": "image", "img_path": "B1Iq1EOiVU/tmp/98fe7fadefe2f22c767ab88588db5b379f7f99326da6b486db10faecc52906a6.jpg", "img_caption": ["Figure 2: Structure overview of DeformableTST. (a) The input time series is embedded variateindependently. (b) The local perception unit (LPU) is used to learn the local temporal information. (c) The proposed deformable attention is adopted to learn the global temporal information. (d) The feed-forward network injected with a depth-wise convolution (ConvFFN) is used to learn the local temporal information and the new feature representation. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.1 Structure Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As shown in Figure 2, our DeformableTST adopts the encoder-only architecture of Transformer [43], including the input embedding layer, hierarchical Transformer backbone and prediction head. And following the recent Transformer-based models, we adopt RevIN [15] to mitigate the distribution shift between the training and testing data. ", "page_idx": 3}, {"type": "text", "text": "Input Embedding Layer Denoted $\\mathbf{X}_{i n}\\in\\mathbb{R}^{M\\times I}$ as the $M$ variates input time series of length $I$ , it will be divided into $N_{0}$ non-overlapping patches and then embedded variate-independently into $D_{0}$ -dimensional embeddings: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{X}_{0}=\\operatorname{Embedding}(\\mathbf{X}_{i n})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$\\mathbf{X}_{0}\\in\\mathbb{R}^{M\\times D_{0}\\times N_{0}}$ is the input embedding. It is worth noting that DeformableTST is less reliant on patching and thus the patching process is optional. We only adopt patching when the input length is very long for efficiency reasons. And we also adopt a much smaller patch size than recent Transformer-based models, making it more adaptable to diverse input lengths. ", "page_idx": 3}, {"type": "text", "text": "Hierarchical Transformer Backbone The backbone is stacked by $L$ Transformer blocks and utilizes a hierarchical structure. The forward process in the $i$ -th block is simply formulated as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{X}_{i}^{l o c a l}=\\mathrm{LPU}(\\mathbf{X}_{i-1})}\\\\ &{\\mathbf{X}_{i}^{g l o b a l}=\\mathrm{LayerNorm}\\left(\\mathbf{X}_{i}^{l o c a l}+\\mathrm{DeformableAttention}(\\mathbf{X}_{i}^{l o c a l})\\right)}\\\\ &{\\;\\;\\;\\;\\;\\;\\mathbf{X}_{i}=\\mathrm{LayerNorm}\\left(\\mathbf{X}_{i}^{g l o b a l}+\\mathrm{ConvFFN}(\\mathbf{X}_{i}^{g l o b a l})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$\\mathbf{X}_{i}\\in\\mathbb{R}^{M\\times D_{i}\\times N_{i}}$ is the output feature series of the $i$ -th block, $i\\in\\{1,...,L\\}$ . And $D_{i}$ and $N_{i}$ are the sizes of its feature and temporal dimensions. DeformableAttention is the core component to better cpature the global temporal dependency, which will be introudced in Section3.2. LPU and ConvFFN are local enhancement modules (Figure 2 (b) and (d)). LPU is the local perception unit, a depth-wise convolution with residual connection [10]. And ConvFFN is a feed-forward network injected with a depth-wise convolution [50]. These two modules are adopted to improve the local temporal modeling ability. And a GELU activation [11] is adopted in ConvFFN to provide nonlinearity when learning the new feature representation. Meanwhile, to construct a hierarchical structure, a downsampling convolution layer [24] with kernel size 2 and stride 2 is adopted between two blocks, which will halve the series\u2019 temporal dimension and double the feature dimension. ", "page_idx": 3}, {"type": "text", "text": "Prediction Head We first flatten the final representation from the backbone $\\mathbf{X}_{L}\\in\\mathbb{R}^{M\\times D_{L}\\times N_{L}}$ into $\\mathbb{R}^{M\\times(D_{L}\\times N_{L})}$ . Then we obtain the prediction through a linear projection layer: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{Y}}=\\mathrm{Projection}(\\mathbf{X}_{L})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "WhereY \u2208RM\u00d7T is the prediction of length T with M variates. ", "page_idx": 4}, {"type": "text", "text": "3.2 Deformable Attention ", "text_level": 1, "page_idx": 4}, {"type": "image", "img_path": "B1Iq1EOiVU/tmp/5175a8ed8ae6badf9a9e67849cb5177c95f413f0b5825281f3c5d12d5bd9b50d.jpg", "img_caption": ["Figure 3: Deformable Attention. (a) The process of deformable attention from the tensor view and coordinate view. (b) The structure of the offset network, marked with the size of feature series. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3 introduces the detailed process of our deformable attention. In each attention module, it first samples a few important time points from the input feature series $\\mathbf{X}$ based on the learnable offsets. Then the sampled important time points are fed to the key and value projections to get the sampled key and value tokens $\\mathbf{\\dot{K}}$ , $\\tilde{\\textbf{V}}$ . Meanwhile, the input feature series $\\mathbf{X}$ is also projected into queries $\\mathbf{Q}$ Finally, standard multi-head attention [43] is applied to $\\mathbf{Q},\\tilde{\\mathbf{K}}$ , $\\tilde{\\textbf{V}}$ to obtain the attention output $\\mathbf{O}$ . ", "page_idx": 4}, {"type": "text", "text": "Sample the Important Time Points As shown in Figure 3 (a), we sample the important time points based on a set of learnable coordinates called sampling points. Specifically, the sampling points are calculated by a set of uniformly sparse reference points and their learnable offsets. ", "page_idx": 4}, {"type": "text", "text": "Given a length- $N$ feature series $\\mathbf{X}\\,\\in\\,\\mathbb{R}^{M\\times D\\times N}$ , we first generate the sparse reference points $\\mathbf{T}_{\\mathit{r e f}}\\in\\mathbb{R}^{M\\times1\\times N_{\\mathit{s a m p}}}$ from a 1D uniform grid. The grid size $N_{s a m p}=N/r$ is downsampled from the input series length $N$ with a downsampling factor $r$ to provide sparsity. The reference points indicate the 1D coordinates of some time points uniformly distributed in the feature series $\\mathbf{X}$ with interval $r$ . These coordinate values are normalized to $[-1,+1]$ , where $-1$ indicates the start of the series and $+1$ means the end of the series. And these reference points serve as the initial coordinates for the following deforming process. ", "page_idx": 4}, {"type": "text", "text": "Then we obtain the offsets for each reference point by offset sub-network (Figure 3 (b)). It contains two convolution layers. The first layer is a depth-wise convolution, which can take the local neighbors into consideration when generating the offsets [48]. It takes the query tokens $\\mathbf{Q}$ as input, where $\\mathbf{Q}$ is the linear projection of the feature series $\\mathbf{X}$ . After a nonlinear activation, the output from the first layer is passed into a point-wise convolution layer to generate the offsets \u2206T \u2208RM\u00d71\u00d7Nsamp. ", "page_idx": 4}, {"type": "text", "text": "Adding up the reference points with the learnable offsets, we obtain $N_{s a m p}$ sampling points, which can serve as the final coordinates to sample the important time points from the feature series $\\mathbf{X}$ . In practice, we follow [48, 60] and calculate the values of these important time points by linear interpolation $\\phi(\\cdot;\\cdot)$ to make this sampling process differentiable. The overall process is as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta\\mathbf{T}=\\mathrm{Offset-Network}(\\mathbf{Q})}\\\\ {\\mathbf{T}_{s a m p}=\\mathbf{T}_{r e f}+\\Delta\\mathbf{T}}\\\\ {\\tilde{\\mathbf{X}}=\\phi(\\mathbf{X};\\mathbf{T}_{s a m p})}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\tilde{\\mathbf{X}}\\in\\mathbb{R}^{M\\times D\\times N_{s a m p}}$ is the sampled feature series consisting of the important time points. And the implementation of linear interpolation $\\phi(\\cdot;\\cdot)$ is in Appendix I.1. And we clip $\\mathbf{T}_{\\mathit{s a m p}}$ by $-1$ and $+1$ to avoid sampling outside the feature series. ", "page_idx": 5}, {"type": "text", "text": "Calculate Attention Output In above sampling process, we have got the query tokens $\\mathbf{Q}$ . After the sampling process, we can get the sampled key and value tokens K\u02dc, V\u02dc after two linear projections of the sampled feature series $\\tilde{\\mathbf{X}}$ . Then we calculate the multi-head self-attention with $H$ heads as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbf{O}^{(h)}=\\mathrm{Softmax}\\left(\\mathbf{Q}^{(h)}\\tilde{\\mathbf{K}}^{(h)\\top}/\\sqrt{d}+\\mathbf{B}\\right)\\tilde{\\mathbf{V}}^{(h)},h\\!=\\!1,\\ldots,H}}\\\\ &{}&{\\mathbf{O}=\\mathrm{OutputProjection}(\\mathrm{Concat}\\left(\\mathbf{O}^{(1)},\\dots,\\mathbf{O}^{(H)}\\right))\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $d\\!=\\!D/H$ is the dimension of each head. The upper index $(h)$ denotes the $h$ -th attention head. After concatenating the output embedding from each attention head $\\mathbf{O}^{(h)}$ together, we obtain the output of the DeformableAttention module $\\mathbf{O}\\in\\mathbb{R}^{M\\times D\\times N}$ through a linear projection. $\\mathbf{B}$ is the deformable relative position bias to provide the positional information into the attention map and its implementation is introduced in Appendix I.2. ", "page_idx": 5}, {"type": "text", "text": "To conclude, this subsection introduces the detailed process of DeformableAttention (Eq.(3)). And for the $i$ -th block, $\\mathbf{X}$ in this subsection corresponds to ${\\bf X}_{i}^{l o c a l}$ in Eq.(3) and $\\mathbf{O}$ corresponds to ${\\bf X}_{i}^{g l o b a l}$ . ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We thoroughly evaluate our DeformableTST on a wide range of time series forecasting tasks, including long-term forecasting tasks with various input lengths, as well as multivariate and univariate shortterm forecasting tasks that are unsuitable for patching, to verify the performance and applicability of our DeformableTST. ", "page_idx": 5}, {"type": "text", "text": "Baselines We extensively include the latest and advanced models in time series community as strong baselines, including patch-based Transformer models: Pathformer [6], CARD [51], GPT4TS [58], PatchTST [35]; non patch-based Transformer models: iTransformer [22], FEDformer [57], Autoformer [47]; other non Transformer-based models: RLinear [18], TiDE [9], TimesNet [46], DLinear [52] and SCINet [20]. We also include the state-of-the-art models in each specific task as additional baselines for a comprehensive comparison. ", "page_idx": 5}, {"type": "text", "text": "Main Result As shown in Figure 4, our DeformableTST achieves consistent state-of-the-art performance in a broader range of time series tasks. In details, DeformableTST can flexibly adapt to multiple input lengths and especially achieve excellent performance in tasks unsuitable for patching, which is a great improvement than previous Transformer-based models, proving that our DeformableTST can successfully reduce the reliance on patching and broaden the applicability of Transformer-based models. Experiment details and result discussions of each task are provided in following subsections. In each table, the best results are in bold and the second best are underlined. ", "page_idx": 5}, {"type": "image", "img_path": "B1Iq1EOiVU/tmp/6eaf5d23d76247a9b5e05116b7da57ac42370ee1d4c08140c843b16b071b89df.jpg", "img_caption": ["Figure 4: Model performance comparison (left) and performance under different input lengths (right). "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.1 Long-term Forecasting ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Setups We conduct long-term forecasting experiments on 8 popular real-world benchmarks, including Weather [45], Traffic [39], ECL [42], Solar-Energy [34] and 4 ETT datasets [55]. In this paper, we refine the evaluation approach for a comprehensive comparision of the models. Different from previous settings that use a fixed short input length (e.g., 96) [55, 47, 22]. We fix three different input lengths $\\{96,\\bar{3}84,768\\}$ and calculate the averaged results to adequately reflect model\u2019s adaptability to multiple input lengths. These input lengths covers a variety of real-world appilication scenarios, i.e., shorter than prediction lengths, within the prediction lengths\u2019 interval and longer than prediction lengths. Following the previous settings, we set prediction lengths as $\\{96,192,336,720\\}$ and calculate the MSE and MAE of multivariate time series forecasting as metrics. ", "page_idx": 6}, {"type": "text", "text": "Results Table 1 shows the excellent performance of DeformableTST in long-term forecasting. Concretely, DeformableTST gains the best performance in most cases, surpassing extensive stateof-the-art Transformer-based models. As shown in Figure 4 (right), DeformableTST achieves the consistent state-of-the-art performance in all input lengths and gains continuous performance improvement with the increasing input length, validating its adaptability to multiple input lengths and its effectiveness in extracting useful information from longer history. For comparison, the non patch-based Transformer baselines suffer from performance degradation with increasing input length due to the distracted attention on the prolonging input. And the patch-based Transformer baselines can not work well with a short input length (e.g., 96) because leveraging patching on the short time series leads to very few tokens, limiting attention\u2019s ability in long-term modeling. ", "page_idx": 6}, {"type": "text", "text": "Table 1: Multivariate long-term forecasting results. A lower MSE or MAE indicates a better performance. Results are averaged from three input lengths $I\\in\\{96,384,768\\}$ and four prediction lengths $T\\in\\{96,192,336,720\\}$ . See Table 8, 9, 10 for full results with more baselines. ", "page_idx": 6}, {"type": "table", "img_path": "B1Iq1EOiVU/tmp/f192fe2cbadac4e3b824c127b497ecbc3153f69ef3fa77953f6e0112ad4a3496.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.2 Short-term Forecasting ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Time series community are currently focusing on long-term forecasting tasks, where the input length and prediction length are adequate for patching technique. However, the short-term forecasting tasks, where the input length and prediction length are limited, are also of extensive practical value in real-world appilications. The study on short-term forecasting tasks has been stagnant in recent years and the advanced patching technique proposed in long-term forecasting is hard to apply to short-term forecasting due to the limited input length. To validate the applicability of our DeformableTST in short-term forecasting, we extensively conduct experiments in following two kinds of tasks. ", "page_idx": 6}, {"type": "text", "text": "Setups of Multivariate Short-term Forecasting We conduct multivariate short-term forecasting experiments on 8 popular real-world benchmarks, including Exchange [17], ILI [3], 2 ETTh [55] and 4 PEMS datasets [5]. We set prediction lengths as $\\{6,12,18\\}$ and set the input length to be 2 times of the prediction length, which precisely meets the definition of limited input lengths in short-term forecasting. We calculate the MSE and MAE of multivariate time series forecasting as metrics. ", "page_idx": 6}, {"type": "text", "text": "Setups of Univariate Short-term Forecasting The study on univariate short-term forecasting tasks used to be popular in the early time series community [29, 31, 37] but has been stagnant in recent years. In this paper, we bring back this classic tasks and conduct experiments on following datasets: M1 [29], M3 [30], M4 [31], Tourism [1], NN5 [41], Hospital [12] and KDD Cup [13]. Following the classic settings [29, 37], we calculate the SMAPE as metric. Specially for M4 datasets, we follow the rules of M4 competition [31] and use MASE and OWA as additional metrics. The prediction lengths are from 2 to 48 and the input length is 2 times of the prediction length. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We emphasize the difference between multivariate and univariate tasks as follows. In multivariate tasks, all input samples are obtained by sliding window from the same multivariate long series. Therefore, there is a high degree of similarity among the input samples in multivariate tasks. In contrast, the input samples in univariate tasks are collected from many different univariate time series sources. As a result, the input samples in univariate tasks may differ from each other and have quite different temporal property, making the univariate short-term forecasting tasks much more difficult. ", "page_idx": 7}, {"type": "text", "text": "Results As shown in Table 2 and 3. DeformableTST performs excellently in short-term forecasting. Compared with the second best model, it achieves averaged $14.1\\%$ SMAPE promotion in univariate tasks and averaged $25.6\\%$ MSE promotion in multivariate tasks. As a comparison, the limited input length in short-term forecasting poses a dilemma for patch-based Transformer forecasters. Using patching will lead to very few tokens, making it unable to fully utilize the long-term modeling ability in attention. Not using patching will lead to the distracted attention on all input time points, making it hard to extract non-trivial temporal information. As a result, previous patch-based Transformer forecasters fail in many cases of short-term forecasting tasks. And in multivariate short-term forecasting, the variate correlation plays an important role to the final results for the temporal information is limited due to the limited input length. Therefore, CARD and iTransformer, which can learn the variate correlation, achieve ideal performance in PEMS datasets whose variate number is very large. As a variate-independent method, our DeformableTST still competes favorably with these cross-variate methods, further demonstrating its excellent temporal modeling ability to extract useful information even from the limited inputs. It will be our future work to study how to capture the multivariate correlation in our model, which will further improve the performance. Meanwhile, the great diversity in univariate samples makes it more difficult to learn temporal representation in univariate short-term forecasting. Therefore, the linear baselines and iTransformer, which adopt linear layers on the temporal dimension, suffer from inferior performance due to the insufficient representation capability in linear layers. By contrast, thanks to the better representation capability in attention mechanism and the better focusing capacity in the proposed deformable attention, our DeformableTST is particularly good at short-term forecasting tasks, which is a great improvement than previous Transformer-based models, therefore successfully broadening the applicability of Transformer-based models. ", "page_idx": 7}, {"type": "text", "text": "Table 2: Multivariate short-term forecasting results. A lower MSE or MAE indicates a better performance. Results are averaged from three prediction lengths $T\\in\\{6,12,18\\}$ . And the PEMS results are further averaged by four subsets. Full results and more baselines are listed in Table 11. ", "page_idx": 7}, {"type": "table", "img_path": "B1Iq1EOiVU/tmp/5f03f51d7f71fc1737c6ebc9333e4255d92264d3bfceaa6deefe31a9ee6b0ff4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 3: Univariate short-term forecasting results. Lower metrics indicate better performance. Weight $A\\nu g$ and $A\\nu g$ means the results are (weighted) averaged by subdatasets. We only report the SMAPE as metric here. Full results with more metrics for M4 are provided in Table 12 and 13. \u2217. in the Transformers indicates the name of $^*$ former. ", "page_idx": 7}, {"type": "table", "img_path": "B1Iq1EOiVU/tmp/d2897038dc49a3ce5b643dbbb1770071659b5cc52f2c4d3243d41ffed4bf9198.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5 Model Analysis ", "text_level": 1, "page_idx": 8}, {"type": "image", "img_path": "B1Iq1EOiVU/tmp/e9974d921513ce4041495d2dead603b202346b59a4dba9eb059467fbc204dd63.jpg", "img_caption": ["5.1 Ablation Study ", "Figure 5: Ablation study in long-term forecasting tasks. From the top to the bottom, each row means one design that we add on PatchTST to modify it into our DeformableTST. We report the averaged MSE of four prediction lengths. The memory usage is recorded under input-96-predict-96 setting with the same batch size. A lower MSE (a shorter blue bar) and a smaller memory usage (a green star closer to the vertical axis) means better performance and efficiency. More results are in Appendix F.1. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "In order to validate the effectiveness of our designs, we start from a PatchTST [35] and gradually update it into our DeformableTST by adding our designs step by step. And we provide the trajectory going from a PatchTST to a DeformableTST in Figure 5. To get rid of the over-reliance on patching, we remove the patching design in PatchTST. After this step, PatchTST suffers from degradation in both performance and efficiency. To address these issues, more designs are adopted in DeformableTST. ", "page_idx": 8}, {"type": "text", "text": "First, the removal of patching leads to a larger number of tokens processed in the attention computation, resulting in heavier memory usage. Responding to the issue, we adopt a hierarchical structure to gradually reduce the number of tokens, therefore alleviating the efficiency problem. Secondly, full attention is hard to focus on the important time points after removing the patching design, leading to trivial temporal representation and performance degradation. To help attention better focus without patching, we propose deformable attention as a complementary. Thanks to the better focusing ability in deformable attention, we observe great performance improvement after adding this design. Meanwhile, we also adopt some local enhancement modules in our design since locality is also important in time series [20]. And this step also brings performance improvement. After equipped with all our designs, DeformableTST shows great performance and efficiency superiority than the baseline PatchTST, which proves the necessity and effectiveness of our designs. ", "page_idx": 8}, {"type": "text", "text": "5.2 Compared Deformable Attention with Prior-based Sparse Attention ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In Section 1, we propose that sparse attention can help attention to better foucs without patching and further argue that data-drivien sparse attention is more appropriate than prior-based ones. To validate the necessity and effectiveness of using data-drivien sparse attention, we compare our deformable attention with some prior-based sparse attentions in time series community, that is ProbSparse Attention in Informer [55], AutoCorrelation in Autoformer [47] and FourierAttention in FEDformer [57]. We also include the local window attention in Swin Transformer [23] and the vanilla full attention [43] adopted in most baselines [22, 53, 35] for a comprehensive comparison. ", "page_idx": 8}, {"type": "text", "text": "As shown in Table 4, our deformable attention surpasses other prior-based competitors in all benchmarks. This is because the priors are hard to match all kind of inputs due to the diverse pattern in different time series, resulting in the inferior performance of prior-based sparse attentions. Different from them, our deformable attention is a data-driven sparse attention that can learn from the input time series, therefore is more flexible to the diverse property in different time series. ", "page_idx": 8}, {"type": "text", "text": "Our deformable attention also surpasses the full attention by a large margin for it can better focus on the important time points to learn non-trivial temporal representation, while the full attention suffers from the distracted attention and trivial temporal representation due to the lack of patching. Although window attention can help attention avoid being distracted in a global range by limiting the attention computation into local windows, its performance still decreases due to the lack of long-term modeling ability, which is an important ability a time series forecaster should have. ", "page_idx": 8}, {"type": "text", "text": "Table 4: Comparison of our Deformable Attention with other prior-based Sparse Attentions. We replace our Deformable Attention with other prior-based Sparse Attentions for comparison. We conduct the experiment in long-term forecasting tasks with input length 96 and list the averaged MSE/MAE of four different prediction lengths. More results are in Appendix F.2. ", "page_idx": 9}, {"type": "table", "img_path": "B1Iq1EOiVU/tmp/ac589625cdbfb7cd515d1ab2263b774ecc6fe06ec362ae0c07be562b0f81ebd1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we expose an emerging issue faced by advanced Transformer-based models that they have limited applicability in time series forecasting tasks due to their over-reliance on patching. And we further find out the reason behind this problem is that full attention relies overly on the guidance of patching to focus on the important time points and learn non-trivial temporal representation. To tackle this problem, we propose DeformableTST as an effective solution, which equips with deformable attention that can better focus on the important time points by itself to get rid of the over-reliance on patching. Experimentally, DeformableTST achieves the consistent state-of-the-art performance in a broader range of time series forecasting tasks, especially achieving promising performance in tasks unsuitable for patching, therefore successfully reducing the reliance on patching and broadening the applicability of Transformer-based models. And we hope our findings can prompt people to rethink the relationship between Transformer-based models and patching technique, thereby designing more powerful Transformer-based forecasters with a wider range of applicability. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the Hebei Innovation Plan (20540301D). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] George Athanasopoulos, Rob J Hyndman, Haiyan Song, and Doris C Wu. The tourism forecasting competition. International Journal of Forecasting, 27(3):822\u2013844, 2011.   \n[2] Konstantinos Benidis, Syama Sundar Rangapuram, Valentin Flunkert, Yuyang Wang, Danielle Maddix, Caner Turkmen, Jan Gasthaus, Michael Bohlke-Schneider, David Salinas, Lorenzo Stella, et al. Deep learning for time series forecasting: Tutorial and literature survey. ACM Computing Surveys, 55(6):1\u201336, 2022.   \n[3] CDC. Illness. https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html.   \n[4] Cristian Challu, Kin G Olivares, Boris N Oreshkin, Federico Garza Ramirez, Max Mergenthaler Canseco, and Artur Dubrawski. Nhits: Neural hierarchical interpolation for time series forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 6989\u20136997, 2023.   \n[5] Chao Chen, Karl Petty, Alexander Skabardonis, Pravin Varaiya, and Zhanfeng Jia. Freeway performance measurement system: mining loop detector data. Transportation research record, 1748(1):96\u2013102, 2001.   \n[6] Peng Chen, Yingying ZHANG, Yunyao Cheng, Yang Shu, Yihang Wang, Qingsong Wen, Bin Yang, and Chenjuan Guo. Multi-scale transformers with adaptive pathways for time series forecasting. In The Twelfth International Conference on Learning Representations, 2024.   \n[7] Razvan-Gabriel Cirstea, Chenjuan Guo, Bin Yang, Tung Kieu, Xuanyi Dong, and Shirui Pan. Triformer: Triangular, variable-specific attentions for long sequence multivariate time series forecasting\u2013full version. arXiv preprint arXiv:2204.13767, 2022.   \n[8] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 764\u2013773, 2017.   \n[9] Abhimanyu Das, Weihao Kong, Andrew Leach, Rajat Sen, and Rose Yu. Long-term forecasting with tide: Time-series dense encoder. arXiv preprint arXiv:2304.08424, 2023.   \n[10] Jianyuan Guo, Kai Han, Han Wu, Yehui Tang, Xinghao Chen, Yunhe Wang, and Chang Xu. Cmt: Convolutional neural networks meet vision transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12175\u201312185, 2022.   \n[11] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.   \n[12] Hyndman, R. J. expsmooth: Data Sets from Forecasting with Exponential Smoothing. https: //cran.r-project.org/package $=$ expsmooth.   \n[13] KDD. Kdd cup 2018. https://www.kdd.org/kdd2018/kdd-cup.   \n[14] Bum Jun Kim, Hyeyeon Choi, Hyeonah Jang, Dong Gu Lee, Wonseok Jeong, and Sang Woo Kim. Dead pixel test using effective receptive field. Pattern Recognition Letters, 167:149\u2013156, 2023.   \n[15] Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo. Reversible instance normalization for accurate time-series forecasting against distribution shift. In International Conference on Learning Representations, 2021.   \n[16] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[17] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns with deep neural networks. In SIGIR, 2018.   \n[18] Zhe Li, Shiyi Qi, Yiduo Li, and Zenglin Xu. Revisiting long-term time series forecasting: An investigation on linear mapping. arXiv preprint arXiv:2305.10721, 2023.   \n[19] Bryan Lim and Stefan Zohren. Time-series forecasting with deep learning: a survey. Philosophical Transactions of the Royal Society A, 379(2194):20200209, 2021.   \n[20] Minhao Liu, Ailing Zeng, Z Xu, Q Lai, and Q Xu. Scinet: time series modeling and forecasting with sample convolution and interaction. In 36th Conference on Neural Information Processing Systems (NeurIPS), 2022.   \n[21] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar. Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting. In International conference on learning representations, 2021.   \n[22] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. itransformer: Inverted transformers are effective for time series forecasting. In The Twelfth International Conference on Learning Representations, 2024.   \n[23] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012\u201310022, 2021.   \n[24] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11976\u201311986, 2022.   \n[25] Donghao Luo and Xue Wang. Moderntcn: A modern pure convolution structure for general time series analysis. In International Conference on Learning Representations, 2024.   \n[26] Donghao Luo and Xue Wang. ModernTCN: A modern pure convolution structure for general time series analysis. In The Twelfth International Conference on Learning Representations, 2024.   \n[27] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel. Understanding the effective receptive field in deep convolutional neural networks. Advances in neural information processing systems, 29, 2016.   \n[28] Amal Mahmoud and Ammar Mohammed. A survey on deep learning for time-series forecasting. Machine learning and big data analytics paradigms: analysis, applications and challenges, pages 365\u2013392, 2021.   \n[29] Spyros Makridakis, Allan Andersen, Robert Carbone, Robert Fildes, Michele Hibon, Rudolf Lewandowski, Joseph Newton, Emanuel Parzen, and Robert Winkler. The accuracy of extrapolation (time series) methods: Results of a forecasting competition. Journal of forecasting, 1(2):111\u2013153, 1982.   \n[30] Spyros Makridakis and Michele Hibon. The m3-competition: results, conclusions and implications. International journal of forecasting, 16(4):451\u2013476, 2000.   \n[31] Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. The m4 competition: Results, findings, conclusion and way forward. International Journal of Forecasting, 34(4):802\u2013 808, 2018.   \n[32] Ricardo P Masini, Marcelo C Medeiros, and Eduardo F Mendes. Machine learning advances for time series forecasting. Journal of economic surveys, 37(1):76\u2013111, 2023.   \n[33] John A Miller, Mohammed Aldosari, Farah Saeed, Nasid Habib Barna, Subas Rana, I Budak Arpinar, and Ninghao Liu. A survey of deep learning and foundation models for time series forecasting. arXiv preprint arXiv:2401.13912, 2024.   \n[34] National Renewable Energy Laboratory. Solar power data for integration studies. https: //www.nrel.gov/grid/solar-power-data.html.   \n[35] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. arXiv preprint arXiv:2211.14730, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[36] Oleh Onyshchak. Stock market dataset. https://www.kaggle.com/dsv/1054465, 2020. ", "page_idx": 12}, {"type": "text", "text": "[37] Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-beats: Neural basis expansion analysis for interpretable time series forecasting. arXiv preprint arXiv:1905.10437, 2019.   \n[38] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.   \n[39] PeMS. Traffic. http://pems.dot.ca.gov/.   \n[40] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.   \n[41] Souhaib Ben Taieb, Gianluca Bontempi, Amir F Atiya, and Antti Sorjamaa. A review and comparison of strategies for multi-step ahead time series forecasting based on the nn5 forecasting competition. Expert systems with applications, 39(8):7067\u20137083, 2012.   \n[42] UCI. Electricity. https://archive.ics.uci.edu/ml/datasets/ ElectricityLoadDiagrams20112014.   \n[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[44] Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, and Liang Sun. Transformers in time series: A survey. arXiv preprint arXiv:2202.07125, 2022.   \n[45] Wetterstation. Weather. https://www.bgc-jena.mpg.de/wetter/.   \n[46] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. arXiv preprint arXiv:2210.02186, 2023.   \n[47] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. Advances in Neural Information Processing Systems, 34:22419\u201322430, 2021.   \n[48] Zhuofan Xia, Xuran Pan, Shiji Song, Li Erran Li, and Gao Huang. Vision transformer with deformable attention. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4794\u20134803, 2022.   \n[49] Zhuofan Xia, Xuran Pan, Shiji Song, Li Erran Li, and Gao Huang. Dat $^{++}$ : Spatially dynamic vision transformer with deformable attention. arXiv preprint arXiv:2309.01430, 2023.   \n[50] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers, 2021.   \n[51] Wang Xue, Tian Zhou, QingSong Wen, Jinyang Gao, Bolin Ding, and Rong Jin. Make transformer great again for time series forecasting: Channel aligned robust dual transformer. arXiv preprint arXiv:2305.12095, 2023.   \n[52] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? arXiv preprint arXiv:2205.13504, 2022.   \n[53] Yunhao Zhang and Junchi Yan. Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting. In International Conference on Learning Representations, 2023.   \n[54] Zhenwei Zhang, Linghang Meng, and Yuantao Gu. Sageformer: Series-aware framework for long-term multivariate time-series forecasting. IEEE Internet of Things Journal, 11:18435\u2013 18448, 2023.   \n[55] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 11106\u201311115, 2021.   \n[56] Tian Zhou, Ziqing Ma, Qingsong Wen, Liang Sun, Tao Yao, Rong Jin, et al. Film: Frequency improved legendre memory model for long-term time series forecasting. arXiv preprint arXiv:2205.08897, 2022.   \n[57] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. In International Conference on Machine Learning, pages 27268\u201327286. PMLR, 2022.   \n[58] Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin. One fits all: Power general time series analysis by pretrained LM. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[59] Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. Deformable convnets v2: More deformable, better results. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9308\u20139316, 2019.   \n[60] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Datasets ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table 5: Detailed descriptions of multivariate datasets. The Dataset Size denotes the total number of time points in (Train, Validation, Test) split respectively. ", "page_idx": 14}, {"type": "table", "img_path": "B1Iq1EOiVU/tmp/b520388d999290d23aea59b82527ec2f52f6f9c243751307733d00f0212cfad4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "B1Iq1EOiVU/tmp/2ae74ec43a7b2c078f1ee0534c9d50b04cc23f8c4cb5fb0411bd88c22704678b.jpg", "table_caption": ["Table 6: Datasets and mapping details of univariate short-term forecasting datasets. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.1 Multivariate Long-term and Short-term Forecasting Datasets ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We evaluate the multivariate long-term forecasting performance on 8 popular real-world datasets, including Weather, Traffic, ECL, Solar-energy and 4 ETT datasets (ETTh1, ETTh2, ETTm1, ETTm2). And for multivariate short-term forecasting tasks, we choose Exchange, ILI, 2 ETTh datasets and 4 PEMS datasets for benchmarking. These datasets have been extensively utilized for benchmarking and cover many aspects of life. ", "page_idx": 14}, {"type": "text", "text": "The variate number, dataset size and sampling frequency of each dataset are summarized in Table 5 . We follow standard protocol [55] and split all datasets into training, validation and test set in chronological order by the ratio of 6:2:2 for the ETT and PEMS dataset and 7:1:2 for the other datasets. And training, validation and test sets are zero-mean normalized with the mean and standard deviation of training set. Each of above datasets only contains one continuous long time series, and we obtain samples by sliding window. ", "page_idx": 15}, {"type": "text", "text": "More introduction of the datasets are as follow: ", "page_idx": 15}, {"type": "text", "text": "1) Weather1 contains 21 meteorological indicators of Germany in 2020.   \n2) Traffic2 contains the road occupancy rates measured by 862 different sensors on San Francisco Bay area freeways in 2 years.   \n3) ECL(Electricity)3 contains hourly electricity consumption of 321 clients from 2012 to 2014.   \n4) ETT(Electricity Transformer Temperature)4 contains the data collected from two different electricity transformers with 2 different resolutions (15 minutes and 1 hour) by 7 sensors.   \n5) Solar(Solar-Eneryg)5 contains 137 time series representing the solar power production in Alabama state in 2006.   \n6) PEMS6 is collected from California freeway and contains 4 subsets.   \n7) Exchange7 the daily exchange rates of eight different countries ranging from 1990 to 2016.   \n8) ILI(Influenza-Like Illness)8 contains 7 indicators of influenza-like illness (ILI) patients in the United States between 2002 and 2021. ", "page_idx": 15}, {"type": "text", "text": "A.2 Univariate Short-term Forecasting Datasets ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We conduct univariate short-term forecasting experiments on 7 popular datasets, including M1, M3, M4, Tourism, NN5, Hospital and KDD Cup. We emphasize the difference between multivariate and univariate tasks as follows. In multivariate tasks, all input samples are obtained by sliding window from the same multivariate long series. Therefore, there is a high degree of similarity between the input samples in multivariate tasks. In contrast, the input samples in univariate tasks are collected from many different univariate time series sources. As a result, the input samples in univariate tasks may differ from each other and have quite different temporal property, making the univariate short-term forecasting tasks much more difficult. ", "page_idx": 15}, {"type": "text", "text": "Table 6 summarizes details of statistics of univariate short-term forecasting datasets. And more introduction of the datasets are as follow: ", "page_idx": 15}, {"type": "text", "text": "1) $\\mathbf{M}^{9}$ contains 3 subsets with different frequency: Yearly, Quarterly and Monthly. The series are belonging to 7 different domains: macro 1, macro 2, micro 1, micro 2, micro 3, industry and demographic. ", "page_idx": 15}, {"type": "text", "text": "2) $\\mathbf{M3}^{10}$ contains 4 subsets with different frequency: Yearly, Quarterly, Monthly and Others. The series are belonging to 6 different domains: demographic, micro, macro, industry, finance and other. ", "page_idx": 15}, {"type": "text", "text": "3) $\\mathbf{M}\\mathbf{4}^{11}$ contains 6 subsets with different frequency: Yearly, Quarterly, Monthly, Weekly, Daily and Hourly. The series are belonging to a wide range of economic, industrial, financial and demographic areas. ", "page_idx": 15}, {"type": "text", "text": "4) Tourism12 contains 3 subsets with different frequency (Yearly, Quarterly and Monthly) used in the Kaggle Tourism forecasting competition. Considering the dataset size, we only use Tourism Quarterly and Tourism Monthly.   \n5) $\\mathbf{NN}5^{13}$ contains weekly time series from the banking domain.   \n6) Hospital Dataset14 contains 767 monthly time series that represent the patient counts related to medical products from January 2000 to December 2006.   \n7) KDD $\\mathbf{Cup}^{15}$ contains 270 hourly time series representing the air quality levels by multiple ", "page_idx": 16}, {"type": "text", "text": "measurements such as PM2.5, PM10, NO2, CO, O3 and SO2. ", "page_idx": 16}, {"type": "text", "text": "B Experiment details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Long-term Forecasting ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Implementation Details Our method is trained with the L2 loss, using the ADAM [16] optimizer with an initial learning rate in $\\{10^{-3},5\\times10^{-4},10^{-4}\\}$ . The default training process is 50 epochs with proper early stopping. The mean square error (MSE) and mean absolute error (MAE) are used as metrics. All the experiments are repeated 5 times with different seeds and the means of the metrics are reported as the final results. All the deep learning networks are implemented in PyTorch[38] and conducted on NVIDIA A100 40GB GPU. ", "page_idx": 16}, {"type": "text", "text": "Model Parameter By default, DeformableTST contains 4 Transfomer blocks. And we adopt downsampling layers between two blocks, which will halve the series\u2019 length and double the model dimension. The dimension $D$ of the first block is set as 16. The expansion $\\alpha$ is set as 4. The number of important time points $N_{s a m p}$ is set as 12. We optionally adopt non-overlap patching depended on the input lengths. When input length is 96, we do not adopt patching. When input length is 384, the patch size is 4. When input length is 768, the patch size is 8. For baseline models, if the original papers conduct long-term forecasting experiments on the dataset we use, we follow the official codes with the recommended model parameters in the original papers, including the number of blocks, model dimension, etc. Otherwise, their model parameters are searched from following searching space: number of blocks $L$ from $\\{2,4,6\\}$ , model dimension $D$ from $\\{64,128,256\\}$ and FFN expansion $\\alpha$ from $\\{1,2,4,8\\}$ . ", "page_idx": 16}, {"type": "text", "text": "Metric We adopt the mean square error (MSE) and mean absolute error (MAE) of multivariate time series forecasting as metrics. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\mathbf{MSE}=\\frac{1}{T}\\sum_{i=0}^{T}(\\widehat{\\mathbf{Y}}_{i}-\\mathbf{Y}_{i})^{2}}\\\\ {\\displaystyle\\mathbf{MAE}=\\frac{1}{T}\\sum_{i=0}^{T}\\left|\\widehat{\\mathbf{Y}}_{i}-\\mathbf{Y}_{i}\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\widehat{\\mathbf{Y}},\\mathbf{Y}\\in\\mathbb{R}^{T\\times M}$ are the $M$ variates prediction results of length $T$ and corresponding ground truth. $\\widehat{\\mathbf{Y}}_{i}$ means the $i$ -th time point in the prediction result. ", "page_idx": 16}, {"type": "text", "text": "B.2 Multi-variate Short-term Forecasting ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Implementation Details Our method is trained with the L2 loss, using the ADAM [16] optimizer with an initial learning rate in $\\{10^{-3},5\\times10^{-4},10^{-4}\\}$ . The default training process is 50 epochs with proper early stopping. The mean square error (MSE) and mean absolute error (MAE) are used as metrics. All the experiments are repeated 5 times with different seeds and the means of the metrics are reported as the final results. All the deep learning networks are implemented in PyTorch[38] and conducted on NVIDIA A100 40GB GPU. ", "page_idx": 16}, {"type": "text", "text": "Model Parameter By default, DeformableTST contains 6 Transformer blocks with the model dimension $D=256$ and FFN expansion $\\alpha=4$ . The number of important time points $N_{s a m p}$ is in the range of 1 to 12 in short-term forecasting tasks, which is depended on the different input lengths. Due to the limited input lengths, we do not adopt patching and downsampling layers in short-term forecasting tasks. For baseline models, we follow the official codes with the recommended model parameters and some of their model parameters are re-searched from following searching space: number of blocks $L$ from $\\{2,4,6\\}$ , model dimension $D$ from $\\{64,128,256\\}$ and FFN expansion $\\alpha$ from $\\{1,2,4,8\\}$ . ", "page_idx": 17}, {"type": "text", "text": "Metric We adopt the mean square error (MSE) and mean absolute error (MAE) of multivariate time series forecasting as metrics. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\mathbf{MSE}=\\frac{1}{T}\\sum_{i=0}^{T}(\\widehat{\\mathbf{Y}}_{i}-\\mathbf{Y}_{i})^{2}}\\\\ {\\displaystyle\\mathbf{MAE}=\\frac{1}{T}\\sum_{i=0}^{T}\\left|\\widehat{\\mathbf{Y}}_{i}-\\mathbf{Y}_{i}\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\widehat{\\mathbf{Y}},\\mathbf{Y}\\in\\mathbb{R}^{T\\times M}$ are the $M$ variates prediction results of length $T$ and corresponding ground truth. $\\widehat{\\mathbf{Y}}_{i}$ means the $i$ -th time point in the prediction result. ", "page_idx": 17}, {"type": "text", "text": "B.3 Univariate Short-term Forecasting ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Implementation Details Our method is trained with the SMAPE loss, using the ADAM [16] optimizer with an initial learning rate of $5\\times10^{-4}$ . The default training process is 50 epochs with proper early stopping. Following [46], we fix the input length to be 2 times of prediction length for all models. All the experiments are repeated 5 times with different seeds and the means of the metrics are reported as the final results. ", "page_idx": 17}, {"type": "text", "text": "Model Parameter By default, DeformableTST contains 4 Transformer blocks with the model dimension $D=256$ and FFN expansion $\\alpha=4$ . Due to the limited input lengths, we do not adopt patching and downsampling layers in short-term forecasting tasks. ", "page_idx": 17}, {"type": "text", "text": "Metric For the M4 datasets, following [31, 37], we adopt the symmetric mean absolute percentage error (SMAPE), mean absolute scaled error (MASE) and overall weighted average (OWA) as the metrics, which can be calculated as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\begin{array}{l l}{{\\mathrm{SMAPE}}={\\frac{200}{T}}\\displaystyle\\sum_{i=1}^{T}{\\frac{|\\mathbf{Y}_{i}-{\\hat{\\mathbf{Y}}}_{i}|}{|\\mathbf{Y}_{i}|+|{\\hat{\\mathbf{Y}}}_{i}|}},}&{{\\mathrm{MAPE}}={\\frac{100}{T}}\\displaystyle\\sum_{i=1}^{T}{\\frac{|\\mathbf{Y}_{i}-{\\hat{\\mathbf{Y}}}_{i}|}{|\\mathbf{Y}_{i}|}},}\\\\ {{\\mathrm{MASE}}={\\frac{1}{T}}\\displaystyle\\sum_{i=1}^{T}{\\frac{|\\mathbf{Y}_{i}-{\\hat{\\mathbf{Y}}}_{i}|}{{\\frac{1}{T-p}}}}\\displaystyle|\\mathbf{Y}_{i}-{\\hat{\\mathbf{Y}}}_{i-p}|,}&{{\\mathrm{OWA}}={\\frac{1}{2}}\\left[{\\frac{{\\mathrm{SMAPE}}}{\\mathrm{SMAPE}_{\\mathrm{Naive2}}}}+{\\frac{\\mathrm{MASE}}{\\mathrm{MASE}_{\\mathrm{Naive2}}}}\\right],}\\end{array}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $p$ is the periodicity of the data. $\\widehat{\\mathbf{Y}},\\mathbf{Y}\\in\\mathbb{R}^{T\\times M}$ are the $M$ variates prediction results of length $T$ and corresponding ground truth. $\\widehat{\\mathbf{Y}}_{i}$ means the $i$ -th time point in the prediction result. ", "page_idx": 17}, {"type": "text", "text": "For other datasts, we adopt the symmetric mean absolute percentage error (SMAPE) as the metric, which can be calculated as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{{SMAPE}}=\\frac{200}{T}\\sum_{i=1}^{T}\\frac{\\lvert\\mathbf{Y}_{i}-\\widehat{\\mathbf{Y}}_{i}\\rvert}{\\lvert\\mathbf{Y}_{i}\\rvert+\\lvert\\widehat{\\mathbf{Y}}_{i}\\rvert},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\widehat{\\mathbf{Y}},\\mathbf{Y}\\in\\mathbb{R}^{T\\times M}$ are the $M$ variates prediction results of length $T$ and corresponding ground truth. $\\widehat{\\mathbf{Y}}_{i}$ means the $i$ -th time point in the prediction result. ", "page_idx": 17}, {"type": "text", "text": "C Pseudo-code of DeformableTST ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We provide the pseudo-code of DeformableTST in Algorithm 1. ", "page_idx": 17}, {"type": "text", "text": "Require: Input time series $\\mathbf{X}_{i n}\\,\\in\\,\\mathbb{R}^{B\\times I\\times M}$ ; batch size $B$ ; variates number $M$ ; input length $I$ ; prediction length $T$ ; DeformableTST block number $L$ ; feature series\u2019s embedding dimension in the $i$ -th block $D_{i}$ ; feature series\u2019s length in the $i$ -th block $N_{i}$ . Boolean flag to indicate using downsampling layers or not Use_Downsampling. ", "page_idx": 18}, {"type": "text", "text": "1: $\\mathbf X_{i n}=\\operatorname{RevIN}(\\mathbf X_{i n},\\mathsf{m o d e}{=}\\mathsf{n o r m})$ $\\begin{array}{r}{\\mathrm{\\boldmath~\\u~}\\otimes\\mathbf{X}\\in\\mathbb{R}^{B\\times I\\times M}}\\\\ {\\mathrm{\\boldmath~\\u~}\\otimes\\mathbf{X}\\in\\mathbb{R}^{B\\times M\\times I}}\\\\ {\\mathrm{\\boldmath~\\u~}\\otimes\\mathbf{X}_{i n}\\in\\mathbb{R}^{(B M)\\times1\\times I}}\\end{array}$   \n2: $\\mathbf{X}_{i n}=\\mathbf{X}_{i n}$ .transpose   \n3: $\\mathbf{X}_{i n}=\\mathbf{X}_{i n}$ .reshape   \n4: \u25b7 Embedding the input time series variate-independently with optional patching.   \n5: $\\mathbf{X}_{0}=\\mathtt{E m b e d d i n g}(\\mathbf{X}_{i n})$ \u25b7X \u2208 (BM)\u00d7D0\u00d7N0   \n6: for $i$ in $\\{1,\\ldots,L\\}$ : \u25b7Run through DeformableTST blocks.   \n7: for $\\triangleright$ The local perception unit (LPU) is used to learn the local temporal information.   \n8: $\\mathbf{X}_{i}^{l o c a l}=\\mathrm{LPU}(\\mathbf{X}_{i-1})$ $\\mathsf{\\Delta}>\\mathbf{X}_{i}^{l o c a l}\\in\\mathbb{R}^{(B M)\\times D_{i}\\times N_{i}}$   \n9: for $\\triangleright$ The deformable attention is adopted to learn the global temporal information.   \n10: r $\\mathbf{X}_{i}^{g l o b a l}=\\mathbf{LayerNorm}\\big(\\mathbf{X}_{i}^{l o c a l}+\\mathbf{DeformAttention}(\\mathbf{X}_{i}^{l o c a l})\\big)\\,\\vartriangleright\\mathbf{X}_{i}^{g l o b a l}\\in\\mathbb{R}^{(B M)\\times D_{i}\\times N_{i}}$   \n11: for $\\triangleright$ The feed-forward network injected with a depth-wise convolution (ConvFFN) is used to   \nlearn the local temporal information and the new feature representation.   \n12: ${\\bf X}_{i}={\\tt L a y e r N o r m}\\big({\\bf X}_{i}^{g l o b a l}+{\\tt C o n v F F N}\\big({\\bf X}_{i}^{g l o b a l}\\big)\\big)$ \u25b7Xi \u2208 (BM)\u00d7Di\u00d7Ni   \n13: for $\\triangleright$ Adopting the optional downsampling layer between two blocks.   \n14: if $i\\ <\\ L$ and Use_Downsampling is True:   \n15: $\\mathbf{X}_{i}=\\mathsf{D o w n S a m p l i n g}(\\mathbf{X}_{i})$ \u25b7Xi \u2208R(BM)\u00d7(2Di)\u00d7(Ni/2)   \n16: End for   \n17: $\\mathbf{X}_{L}=\\mathbf{X}_{L}$ .reshape \u25b7XL \u2208 B\u00d7M\u00d7(DLNL)   \n18: Y\u02c6 = Projection $(\\mathbf{X}_{L})$ \u25b7Obtaining the forecasting series with Projection, Y\u02c6 \u2208RB\u00d7M\u00d7T   \n19: $\\hat{\\mathbf{Y}}=\\hat{\\mathbf{Y}}$ .transpose $\\begin{array}{r}{\\>\\hat{\\mathbf{Y}}\\in\\mathbb{R}^{B\\times T\\times M}}\\\\ {\\flat\\ \\hat{\\mathbf{Y}}\\in\\mathbb{R}^{B\\times T\\times M}}\\end{array}$   \n20: $\\hat{\\mathbf{Y}}=\\mathtt{R e v I N}(\\hat{\\mathbf{Y}},\\mathtt{m o d e=d e n o r m})$   \n21: Return Y\u02c6 \u25b7Return the prediction result $\\hat{\\textbf{Y}}$ ", "page_idx": 18}, {"type": "text", "text": "D Parameter Sensitivity ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To evaluate the parameter sensitivity of our DeformableTST, we perform experiments with varying model parameters, including number of blocks ranging from $L~=~\\{2,4,6\\}$ , model dimension ranging from $D=\\{16,32,\\bar{64}\\}$ , FFN expansion ranging from $\\alpha=\\{1,2,4,8\\}$ , number of important time points ranging from $\\bar{N_{s a m p}}\\,=\\,\\{6,12,24\\}$ and learning rate ranging from $l r\\,=\\,\\{10^{-3},5\\,\\times$ $10^{-4},10^{-4}\\}$ . ", "page_idx": 18}, {"type": "text", "text": "The results are shown in Figure 6. In general, our model is robust to the choice of model parameters. Compared with the default block number $L=4$ , stacking more blocks will bring further performance improvement. Considering both performance and efficiency, we recommend to fix the block number as 4 in long term forecasting tasks. ", "page_idx": 18}, {"type": "text", "text": "We also compare our model sensitivity to patch size with PatchTST\u2019s. As shown in Figure 6, our model is less sensitive to the choice of patch size, therefore successfully getting rid of the over-reliance of patching. ", "page_idx": 18}, {"type": "image", "img_path": "B1Iq1EOiVU/tmp/23d2b7014e836fdf404a4b80360999cbf467fd187799933c097c6c67a8273aea.jpg", "img_caption": ["Figure 6: Parameter sensitivity. For patch size, we conduct experiments under input-384-predict-96 settings and adopt PatchTST as comparison. For other parameters, we conduct experiments under input-96-predict-96 settings. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "E More ERF Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Section 1, we visualize the effective receptive fields (ERFs) of PatchTST [35] based on [27, 14] to see which parts of the time points in input series are focused by the model when extracting temporal representations. Here we provide more ERF visualization results with other patch-based Transformer forecasters. In a ERF figure, a brighter area means that the model tends to focus on these time points when extracting temporal representation, and thus these time points will contribute more to the middle point of the final representation. ", "page_idx": 19}, {"type": "text", "text": "As shown in Figure 7, the ERFs of the patch-based Transformer forecasters highly rely on the guidance of patching. If without patching, nearly all time points in input series are equally focused by the model and the model performs worse, exposing the problem of distracted attention. This finding means that attention has not learned to distinguish the importance of each time point in input series, leading to trivial representation. After patching, these Transformer forecasters tend to foucs on some important time points based on a patch partition. This phenomenon means that: although patching can help attention better focus on important time points, the guidance of patching tend to distribute the focused points evenly among all patches. But in real-world scenarios, it is possible that the important time points are not evenly distributed among all patches, which is inconsistent with the tendency in the guidance of patching, leading to the inferior performance of these patch-based Transformer forecasters in some cases. There is also some difference among the ERFs of these patch-based Transformer forecasters due to their different specific designs. PatchTST [35] tends to only focus on a very few time points in each patch and ignore others. But CARD [51] can focus on more time points in one patch thanks to its additional attention across feature dimension, which aligns the information within patch. And the ERF of GPT4TS [58] also reveals the autoregressive property in GPT [40] backbone, making it only focus on the time points before the the middle point. ", "page_idx": 19}, {"type": "text", "text": "By contrast, we provide the ERF of our DeformableTST in Figure 8 for comparison. By default, we set $N_{s a m p}=12$ . Under input-96 settings, our DeformableTST does not adopt patching technique but can still focus on a small number of important time points, proving that our DeformableTST can foucs well by itself without the need of patching. Under input-384 settings, we adopt a small size patching and divide the input series into 96 patches to alleviate the efficiency issue. Under such condition, our DeformableTST still does not focus based on a patch partition. Although adopting patching technique, the focused time points in our DeformableTST is not evenly distributed among all 96 patches, therefore being less reliant on patching. ", "page_idx": 19}, {"type": "image", "img_path": "B1Iq1EOiVU/tmp/56530c81097840dbc7867a649978ca6191793c9a4ee1027531d87b82b157cbb1.jpg", "img_caption": ["Focusing on nearly all input time points before the middle point "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "B1Iq1EOiVU/tmp/9f155bbe638cb00e0be572bee93fe43b9e5a7693b1a70b194a86f02f5cad5723.jpg", "img_caption": [], "img_footnote": ["Focusing on nearly 21 important input time points before the middle point "], "page_idx": 20}, {"type": "text", "text": "Figure 7: The Effective Receptive Field (ERF) of more patch-based Transformer forecasters. A brighter area means that these time points are focused by the model when extracting temporal representation. ", "page_idx": 20}, {"type": "image", "img_path": "B1Iq1EOiVU/tmp/a150fe53ee46e04a38050fa21587d43829d674c7c44e9fb0fc66452d4ecdfb71.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "B1Iq1EOiVU/tmp/aa81503f0029260967547dd12bd8a5445637b8bc104bf3061238735288169d7b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 8: The Effective Receptive Field (ERF) of DeformableTST. A brighter area means that these time points are focused by the model when extracting temporal representation. ", "page_idx": 20}, {"type": "text", "text": "F More Results of Model Analysis in Section 5 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "F.1 More Ablation Results in Section 5.1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "More ablation results are provided in Figure 9. ", "page_idx": 20}, {"type": "image", "img_path": "B1Iq1EOiVU/tmp/d9143d70cd59c95cad80dc3d91be9073fd7dabe88752d187f90f3a7b1050f29c.jpg", "img_caption": ["(a) Ablation results in ETTm1 dataset with input length 96 "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "B1Iq1EOiVU/tmp/3dd2165d66f152c5be6721fe0c41d096f86987c8cd0d81bccc970a9c9ef16ed4.jpg", "img_caption": ["(b) Ablation results in Weather dataset with input length 96 "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "B1Iq1EOiVU/tmp/70233b297267637c0d1c7861645a5aed3c868c5c95939ee25165b4478f0aa8ac.jpg", "img_caption": ["(c)Ablation results in ECL dataset with input length 96 "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 9: More ablation study results in long-term forecasting tasks. From the top to the bottom, each row means one design we add on PatchTST to modify it into our DeformableTST. We report the averaged MSE of four prediction lengths. The memory usage is recorded under input-96-predict-96 setting with the same batch size. A lower MSE (a shorter blue bar) and a smaller memory usage (a green star closer to the vertical axis) means better performance and efficiency. ", "page_idx": 20}, {"type": "text", "text": "F.2 More Comparison Results in Section 5.2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Case Study We provide the visualization of learned important time points as an intuitive comparision for some popular sparse attentions in time series community. As shown in Figure 10, the keys of window attention are restricted within the local window. And the important keys for ProbSparse attention tend to gather in a small area around the time point that most fits the prior. This property makes window attention and ProbSparse attention only focus on a small local area and fail to find the important time points in a global range, leanding to the loss of information. Meanwhile, imporatant time points refer to the time points that reflect the property of time series and make contribution to better performance. And the types of imporatant time points are varied (e.g., time point in the similar changing stage, the inflexion point, the extremal point and so on). But AutoCorrelation can only foucs on the time points in the similar changing stage due to its prior, resulting in the lack of diversity. By contrast, our deformable attention can find the important time points in a global range for the reference points in the sampling process are uniformly distributed throughout the whole input time series. And our deformable attention can also learn different types of important time points because it is less reliant on specific priors and can foucs on any appropriate important time points based on the learnable offsets learned from the input series. Therefore, our deformable attention can find more types of important time points in a wider range, therefore being more flexible to the diverse property in different time series and performing better than other prior-based sparse attentions. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "image", "img_path": "B1Iq1EOiVU/tmp/8bfcc2634e227733a04341273659e78a443b019534081dd30d03d4c4fbe60b1a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 10: Visualization of learned important time points. For clearness, we show the top-6 keys with respect to the last query for attentions and show the top-6 time delays for AutoCorrelation. ", "page_idx": 21}, {"type": "text", "text": "G Model Efficiency ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We comprehensively compare the forecasting performance, training speed, and memory usage of some advanced Transformer-based models. And we compare the efficiency under two representative conditions: (1) the dataset is of a large variate number, (2) the experiment setting is of a long input length and prediction length. And the results are shown in Figure 11. Considering both performance and efficiency, our DeformableTST shows great superiority than other Transformer-based competitors, therefore being an ideal choice in time series forecasting. ", "page_idx": 21}, {"type": "image", "img_path": "B1Iq1EOiVU/tmp/df7881bb5275493bc978712afe919f090f0a7cb19877bb676c36c57e2892bccd.jpg", "img_caption": ["Figure 11: Model efficiency comparison. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "H Error Bar ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We report the standard deviation of DeformableTST performance under five runs with different random seeds in Table 7, which exhibits that the performance of DeformableTST is stable. ", "page_idx": 21}, {"type": "table", "img_path": "B1Iq1EOiVU/tmp/3f291bd102d79f8b9fd411250f10215d2cdd0dec4f397260da0e876e96c7a24f.jpg", "table_caption": ["Table 7: Error bar of DeformableTST. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "I More Implementation Details about Deformable Attention ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "I.1 Implementation of Linear Interpolation ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "As mentioned in Section 3.2, we sample the important time points based on a set of learnable coordinates called sampling points $\\mathbf{T}_{\\mathit{s a m p}}$ . In practice, after obtaining the sampling points $\\mathbf{T}_{\\mathit{s a m p}}$ , we achieve this sampling process via linear interpolation following [48, 60]. In details, we calculate the values of these important time points by linear interpolation $\\phi(\\cdot;\\cdot)$ to make this sampling process differentiable. And the linear interpolation $\\phi(\\cdot;\\cdot)$ is calculated as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\phi\\left(\\mathbf{X};\\mathbf{T}_{s a m p}\\right)\\!=\\!\\sum_{t=0}^{N-1}g(t,\\mathrm{De}.\\mathrm{normalize}(\\mathbf{T}_{s a m p}))\\mathbf{X}[:,t,:],\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $g(a,b)=\\operatorname*{max}(0,1-|a-b|)$ and $t$ indexes all the coordinates on $\\mathbf{X}\\in\\mathbb{R}^{M\\times D\\times N}$ . The value of $\\mathbf{T}_{\\mathit{s a m p}}$ is de-normalized back to the range of $[0,N-1]$ before passed into $g$ . As $g$ would be non-zero only on the 2 integral coordinates closest to $\\mathbf{T}_{\\mathit{s a m p}}$ , it simplifies Eq.(11) to calculate the value of each important time point as the weighted average of its only 2 closest time points. ", "page_idx": 22}, {"type": "text", "text": "I.2 Deformable Relative Positional Bias ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Due to the deforming process and hierarchical representation, the fixed absolute positional embedding is not suitable for our design. Instead, we adopt a relative position bias to encode the position information into the attention map [23], which represents the relative positional information between the query token series $\\mathbf{Q}$ and the key token series $\\tilde{\\bf K}$ . ", "page_idx": 22}, {"type": "text", "text": "Considering the input feature series $\\mathbf{X}\\in\\mathbb{R}^{M\\times D\\times N}$ of the attention module, the relative coordinate displacements of this length- $N$ series contain $(2N-1)$ different values and lie in the range of $[-N,N]$ . Then we will maintain a learnable parameterized bias table $\\hat{\\mathbf{B}}\\in\\mathbb{R}^{H\\times(2N-1)\\times1}$ , in which each element represents one of the above $(2N-1)$ relative coordinate displacements. ", "page_idx": 22}, {"type": "text", "text": "Meanwhile, to represent the relative positional information between the query token series $\\mathbf{Q}$ and the key token series $\\mathbf{\\bar{K}}$ , we also need to denote the coordinates of $\\mathbf{Q}$ and $\\tilde{\\bf K}$ . The coordinates of $\\mathbf{Q}$ are generated from a 1D uniform grid with size $N$ . It indicates the 1D coordinates of all $N$ time points in the query series $\\mathbf{Q}$ . These coordinate values are normalized to $[-1,+1]$ , where $-1$ indicates the start of the series and $+1$ means the end of the series. And the coordinates of $\\tilde{\\bf K}$ are the sampling point $\\mathbf{T}_{\\mathit{s a m p}}$ . Then the relative position $\\mathbf{R}\\in\\mathbb{R}^{N\\times N_{s a m p}\\times1}$ is calculated as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{R}=\\mathbf{Q}_{c o o r d}-\\mathrm{Transpose}(\\mathbf{T}_{s a m p})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then we clip $\\mathbf{R}$ by $-1$ and $+1$ and obtain deformable relative positional bias $\\mathbf{B}$ by linear interpolation to the learnable parameterized bias table $\\hat{\\bf B}$ with the continuous relative displacements $\\mathbf{R}$ as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\bf B}=\\phi(\\hat{\\bf B};{\\bf R})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "J More Discussions on Patching and Transformer-based Models ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "J.1 Discussions on Patching ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Impact on Performance As shown in Figure 1 and 7, although patching can help attention better focus on important time points, the guidance of patching tend to distribute the focused points evenly among all patches. But in real-world scenarios, it is possible that the important time points are not evenly distributed among all patches. In some cases, most of the important time points are only distributed in a few patches while other patches only contain unimportant time points. Such cases are inconsistent with the tendency in the guidance of patching, leading to the inferior performance of the patch-based Transformer forecasters in these cases. And patching technique must work with a very long input length because leveraging patching on short time series leads to very few tokens, limiting attention\u2019s ability in long-term modeling. These drawbacks limit the performance and applicability of the previous patch-based Transformer forecasters. ", "page_idx": 23}, {"type": "text", "text": "Impact on Efficiency In addition to helpling attention focus better, patching can also improve the efficiency by reducing the number of tokens. When facing a very long input length (e.g., 384 and 768), we still adopt a small patch size in our design for better efficiency. Therefore this paper is not a call to completely abandon patching technique in all cases, but an extension to scenarios where patching technique is not suitable. ", "page_idx": 23}, {"type": "text", "text": "J.2 Compared with Typical Transformer-based Models ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "To highlight how we innovate and upgrade the Transformer-based model, we compare our DeformableTST with some milestone Transformer-based models in time series community. ", "page_idx": 23}, {"type": "text", "text": "Compared with PatchTST [35] PatchTST is a milestone Transformer-based model and DeformableTST can be seen as an improvement of it to solve the problem of over-reliance on patching. PatchTST highly relies on patching for patching can force the attention to focus on only a few important time points within each patch and ignore other time points within the patch. But the choose of patch size is a dilemma in practice: a too large patch size could lead to many other time points within the patch being ignored, resulting in the risk of neglecting other priorities. And a too small patch size will lead to a huge amount of tokens, making it hard to focus. This dilemma cannot be resolved by PatchTST itself. To address this issue, we propose deformable attention, an attention mechanism that can focus well by itself, to get rid of the need of patching. Since deformable attention enjoys great focusing ability, we can use a small patch size to mitigate the problem of some priorities being ignored and free to worry about attention being hard to focus on important time points, successfully reducing the reliance on patching and resolving the dilemma. ", "page_idx": 23}, {"type": "text", "text": "Compared with iTransformer [22] DeformableTST and iTransformer are the lastest Transformerbased forecasters. Both methods focus on the underperformance issue of previous attention mechanism in temporal modeling and devote to designing a more powerful Transformer-based forecaster. But they hold different opinions. Although iTransformer still adopts a Transformer architecture, it insists that attention is not suitable for temporal modeling while linear layers are more appropriate. This design makes its performance more similar to linear-based forecasters rather than Transformerbased ones, being less competitive in difficult tasks (e.g., univariate short-term forecasting tasks with lower similarity between samples). Different from iTransformer, our DeformableTST follows the tradition of Transformer-based models to still uses attention for temporal modeling. Specifically, in this paper, through experiments and analysis, we further attribute the underperformance issue of previous attention mechanism to their over-reliance on patching and thus propose deformable attention to solve this problem, successfully designing a more powerful Transformer-based forecaster and broadening the applicability of Transformer-based models. ", "page_idx": 23}, {"type": "text", "text": "K Limitations and Future Work ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this paper, we mainly focus on how to better use attention in temporal modeling and do not consider the multivariate correlating. It will be our future work to study how to further capture the multivariate correlation in our model, which can hopefully improve the performance, especially in datasets with ", "page_idx": 23}, {"type": "text", "text": "large number of variates. Besides, we will further explore the potential of our DeformableTST in more time series analysis tasks and further develop its performance by large-scale pre-training in the future. ", "page_idx": 24}, {"type": "text", "text": "L Ethics Statement and Broader Impact ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Our work only focuses on the time series forecasting problem, so there is no potential ethical risk. ", "page_idx": 24}, {"type": "text", "text": "Our model achieves the state-of-the-art performance on a wider range of time series forecasting tasks, covering a large amount of real-world scenarios. Therefore, the proposed model makes it promising to tackle real-world forecasting problem, helping our society make better decisions and prevent risks in advance. And we hope our findings can prompt people to rethink the relationship between Transformer-based models and patching technique, thereby designing more powerful Transformerbased forecasters with a wider range of applicability. ", "page_idx": 24}, {"type": "text", "text": "Our paper mainly focuses on scientific research and has no obvious negative social impact. ", "page_idx": 24}, {"type": "text", "text": "M Reproducibility Statement ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In the main text, we have strictly formalized the model architecture with equations. All the implementation details are included in the Appendix, including dataset descriptions, metrics, model, and experiment configurations. Code is available at this repository: https://github.com/luodhhh/ DeformableTST. ", "page_idx": 24}, {"type": "text", "text": "N Full Results ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Due to the space limitation of the main text, we place the full results of all experiments and results of more baselines in the following subsections. And we also provide the showcases in Appendix O. ", "page_idx": 24}, {"type": "text", "text": "N.1 Long-term Forecasting with Input Length 96 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The full results of long-term forecasting with input length 96 are provided in Table 8. And more results with more baselines are also included. ", "page_idx": 24}, {"type": "text", "text": "N.2 Long-term Forecasting with Input Length 384 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The full results of long-term forecasting with input length 384 are provided in Table 9. And more results with more baselines are also included. ", "page_idx": 24}, {"type": "text", "text": "N.3 Long-term Forecasting with Input Length 768 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The full results of long-term forecasting with input length 768 are provided in 10. And more results with more baselines are also included. ", "page_idx": 24}, {"type": "text", "text": "N.4 Multivariate Short-term Forecasting ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The full results of multivariate short-term forecasting tasks are provided in Table 11. And more results with more baselines are also included. ", "page_idx": 24}, {"type": "text", "text": "N.5 Univariate Short-term Forecasting ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The full results of M4 datasets are provided in Table 12. And the full results of other datasets are provided in Table 13. ", "page_idx": 24}, {"type": "text", "text": "Table 8: Full results of the long-term forecasting tasks with input length 96. We compare extensive competitive models under different prediction lengths. The input sequence length is set to 96 for all baselines. Avg means the average results from all four prediction lengths. ", "page_idx": 25}, {"type": "table", "img_path": "B1Iq1EOiVU/tmp/ac75154463cade41e3485e688f382abde867842e23996b0714e15855ba1e56ab.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Table 9: Full results of the long-term forecasting tasks with input length 384. We compare extensive competitive models under different prediction lengths. The input sequence length is set to 384 for all baselines. Avg means the average results from all four prediction lengths. ", "page_idx": 26}, {"type": "table", "img_path": "B1Iq1EOiVU/tmp/7b9f30494910af71caf63748454ff1dc469174741ed03b92ebe6b11ef68e4066.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "B1Iq1EOiVU/tmp/212323be574a9ad72c557609b6ec26c79af95f96f39b7d62d1a0d489536e50ab.jpg", "table_caption": ["Table 10: Full results of the long-term forecasting tasks with input length 768. We compare extensive competitive models under different prediction lengths. The input sequence length is set to 768 for all baselines. Avg means the average results from all four prediction lengths. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Table 11: Full results of the multivariate short-term forecasting tasks. We compare extensive competitive models under different prediction lengths. The input length is 2 times of the prediction length. Avg means the average results from all three prediction lengths. ", "page_idx": 28}, {"type": "table", "img_path": "B1Iq1EOiVU/tmp/d10762969d09590a861d2cc2167dc4b55b322af4e31245e1f9802911186006b6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 12: Full results for the univariate short-term forecasting tasks in M4 dataset. We report SMAPE, MASE, OWA for M4 datasets as metrics. Lower metrics indicate better performance. Wighted Average means the results are wighted averaged from several M4 subdatasets under different sample intervals. $^*$ . in the Transformers indicates the name of $^*$ former. The original paper of N-BEATS [37] adopts a special ensemble method to promote the performance. For fair comparisons, we remove the ensemble and only compare the pure forecasting models. ", "page_idx": 28}, {"type": "table", "img_path": "B1Iq1EOiVU/tmp/dd82e318ed2cf9ef9b962e1befe9fe43b6d23ce7c5e6f5e5690cf9a8f522b52e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 13: Full results for the univariate short-term forecasting tasks in other datasets. We report the SMAPE in this Table as metric and a lower metric indicates better performance. $^*$ . in the Transformers indicates the name of \u2217former. The original paper of N-BEATS [37] adopts a special ensemble method to promote the performance. For fair comparisons, we remove the ensemble and only compare the pure forecasting models. ", "page_idx": 29}, {"type": "table", "img_path": "B1Iq1EOiVU/tmp/b2f0a4302166db5f48054b791085d11b0fdc3ab8507c8263cf12ca1e30398257.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "O Showcases ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "To provide an intuitive comparison among different models, we provide showcases to the longterm forecasting tasks under two representative cases (the time series is in declining stage and the time series is in rising stage). The results are in Figure 12-13. Among the various models, our DeformableTST predicts the most precise future series variations and exhibits superior performance. ", "page_idx": 30}, {"type": "image", "img_path": "B1Iq1EOiVU/tmp/3a62c2da141ac50aeed00e8794c07e7d9a46f4b19e979aa706991fb8ddc45a19.jpg", "img_caption": ["Figure 12: Visualization of input-96-predict-96 results on the Solar dataset. The time series is in declining stage. The blue lines stand for the ground truth and the orange lines stand for predicted values. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "B1Iq1EOiVU/tmp/564f89245ed9253d9289c3a8cdcbe54cd6d417201b576e14b3d9c3f3190f939f.jpg", "img_caption": ["Figure 13: Visualization of input-96-predict-96 results on the Solar dataset. The time series is in rising stage. The blue lines stand for the ground truth and the orange lines stand for predicted values. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "P Comparison with More Baselines and Experiments on More Datasets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "P.1 Compared with Sageformer ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We compare our model with Sageformer [54], the latest Graph-Transformer model. Since Sageformer will produce NaN outputs in some short-term forecasting tasks, we mainly conduct comparisons in long-term forecasting. The results are shown in Table 14. Our DeformableTST achieves consistently better performance than the latest Graph-Transformer method, further demonstrating our performance superiority. ", "page_idx": 31}, {"type": "text", "text": "Table 14: Comparison with Sageformer in long-term forecasting tasks. A lower MSE or MAE indicates a better performance. Results are averaged from three input lengths $I\\in\\{96,384,768\\}$ and four prediction lengths $T\\in\\{96,192,336,7\\bar{2}0\\}$ . The best results are in bold. Full results of Sageformer [54] are provided in Table 15. ", "page_idx": 31}, {"type": "table", "img_path": "B1Iq1EOiVU/tmp/51f285c137e8c110b7426eb5e1f271408d68624f5e68e96a9f16db33b3477937.jpg", "table_caption": [], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "P.2 Experiments on Stock Market Dataset ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We conduct multivariate short-term forecasting experiments on Stock Market Dataset [36]. As shown in Table 16, our DeformableTST still outperforms other competitors, validating that DeformableTST can work on stock market data. ", "page_idx": 31}, {"type": "text", "text": "Q Experiments on Synthetic Dataset ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We conduct experiments on synthetic dataset with some typical cases of attention distributions to prove our model can handle both uniform and clustered attention distribution. The details and results are provided in Figure 14. ", "page_idx": 31}, {"type": "text", "text": "As shown in Figure 14, our method can accurately predict the future data in all cases. And ERFs can operate as anticipated, successfully matching the distributions of key information. In details, in the case of globally uniform attention, the brighter points in ERF are also distributed globally, which means the model can find the important time points across the whole series. In other cases, the brighter points in ERF tend to concentrate in localized areas of key information, proving the effectiveness of our method in scenarios where key information is clustered within specific time window. These results validate that our method can adeptly manage both uniform and clustered attention distributions. ", "page_idx": 31}, {"type": "text", "text": "R Model Robustness to Patching ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We conduct ablation study to show the effect of patching on our method (under input-384 and input-768 settings). As shown in Figure 6 and Figure 16, our model is robust to the choice of different patch sizes on input length 384 and input length 768. ", "page_idx": 31}, {"type": "text", "text": "Meanwhile, as shown in Figure 15, the performance of other Patch-based Transformer competitors (e.g., PatchTST [35] and CARD [51]) will decrease obviously and fell out of the good rankings if without patching. This is a significant performance decrease, especially considering the intense competition in time series forecasting. By contrast, our method works well without patching, which further verifies our robustness to the use of patching and shows that our model can successfully get rid of the over-reliance of patching. ", "page_idx": 31}, {"type": "text", "text": "Table 15: Full results of Sageformer [54] in long-term forecasting tasks. Avg means the average results from all four prediction lengths. ", "page_idx": 32}, {"type": "table", "img_path": "B1Iq1EOiVU/tmp/4d394abec8d335383703a2871a6f715ffda56ce8bc6fe0e7e055deb8f57707d1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "B1Iq1EOiVU/tmp/5ddcb58c8d33836537ea20a73c35d509a5221a1fe9f22b3e26997e85835b0e6f.jpg", "table_caption": ["Table 16: Multivariate short-term forecasting results on Stock Market. We compare extensive competitive models under different prediction lengths. The input length is 2 times of the prediction length. $A\\nu g$ means the average results from all three prediction lengths. The best results are in bold. "], "table_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "B1Iq1EOiVU/tmp/8184a18d372d418013c0843978fa903a26df2101dbf1368084148c48fe7d4bc4.jpg", "img_caption": ["The brighter points in ERF are locally focused inthe same specifi time windows of 1988. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "B1Iq1EOiVU/tmp/acae67fa4aa75563f7fe1457a5ff1a4ca0763fd56ad481b19e2ed8d94d5f47f7.jpg", "img_caption": ["The brighter points in ERF are focused in time windows of 0\\~96 and 288\\~384 "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Figure 14: We conduct experiments with some typical cases of attention distributions. The synthetic data and experiment setups are as follows. Setup (1) for globally uniform attention as shown in Case (a): The input consists of 4 semi-sinusoidal signals with noise. The task is to predict 1 semi-sinusoidal signal. Thus, the future data evenly relates to the historical data. The length of each signal is 96. So this is an input-384-predict-96 task. Setup (2) for clustered attention as shown in Case (b) and (c): Simlilar to setup (1), but in the length-384 input, only 1 semi-sinusoidal signal is remained while others are masked as 0. Thus, the future data is related only to the local window of remained signal. Masks can be constant or varying across samples to simulate scenarios that the localized areas are constant or varying across samples. Setup (3) for global but not uniform attention as shown in Case (d): This setup is similar to setup (2) but more semi-sinusoidal signals are remained, resulting in global attention distribution but not uniform. ", "page_idx": 33}, {"type": "image", "img_path": "B1Iq1EOiVU/tmp/1d2a2919cbde4e3d09d595b4b9cd9df04f1af5a1383f979e92b9cdd1828566c7.jpg", "img_caption": ["Figure 15: The impact of patching on latest patch-based Transformer forecasters (PatchTST and CARD). After the removal of patching, the performance of PatchTST and CARD will decrease obviously and fell out of the good rankings, while our DeformableTST is robust to patching and maintains the consistent excellent performance, consistently ranked in the top-3. We conduct experiments under input-384-predict-96 settings. The rankings on each dataset are calculated from Table 9 of Appendix N. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "B1Iq1EOiVU/tmp/5c2ed9dacd584ce910125f6d0abe75d7d2dd175109da31a5203b1d96f0d810ce.jpg", "img_caption": ["Figure 16: Robustness of our DeformableTST to patch size under input-768 settings. "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Please refer to Section 1 and Abstract. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 35}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: Please refer to Appendix K. And model efficiency is provided in G ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 35}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not include theoretical results. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 36}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Please refer to Section 3, Section 4, Appendix A and Appendix B. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 36}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Code is available at this repository: https://github.com/luodhhh/ DeformableTST. We provide the pseudo-code in Appendix C and add a Reproducibility Statement in Appendix M. We have already provided experimental details and model settings in Section 3, Section 4, Appendix A and Appendix B. And details about tensor shape and model structure are also included. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 37}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We have already provided experimental details and model settings in Section 3, Section 4, Appendix A and Appendix B. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 37}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: Please refer to Appendix H. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Please refer to Appendix B. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 38}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Please refer to Appendix L. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 38}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: Please refer to Appendix L. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 39}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The code packages, baseline models and datasets used in this paper are publicly available and properly credited. Please refer to Section 4, Appendix A, Appendix B and the Reference. The baseline models are mostly using Apache-2.0 license and MIT license. The datasets we used are all extensively utilized for benchmarking and publicly available. We provide the URLs for the datasets in Appendix A. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: Please refer to Section 3, Section 4, Appendix A, Appendix B and Appendix K. Code with detailed documentation is available at this repository: https://github.com/ luodhhh/DeformableTST.. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 40}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 40}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. ", "page_idx": 40}, {"type": "text", "text": "\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 41}]