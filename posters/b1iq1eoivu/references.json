{"references": [{"fullname_first_author": "Haoyi Zhou", "paper_title": "Informer: Beyond efficient transformer for long sequence time-series forecasting", "publication_date": "2021", "reason": "This paper is the first to introduce the patching technique in time series forecasting, which is widely used in later Transformer-based models and is fundamental to this paper\u2019s background."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017", "reason": "This paper introduces the Transformer architecture which is the basis of the DeformableTST model proposed in this paper."}, {"fullname_first_author": "Yuqi Nie", "paper_title": "A time series is worth 64 words: Long-term forecasting with transformers", "publication_date": "2023", "reason": "This paper proposes PatchTST, a Transformer-based model that uses patching which is directly compared with in this paper."}, {"fullname_first_author": "Haixu Wu", "paper_title": "Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting", "publication_date": "2021", "reason": "Autoformer is a widely used Transformer-based model that is compared in the experiments of this paper."}, {"fullname_first_author": "Zhuang Liu", "paper_title": "A convnet for the 2020s", "publication_date": "2022", "reason": "This paper introduces the RevIN module that is used in this paper to mitigate the distribution shift between the training and testing data."}]}