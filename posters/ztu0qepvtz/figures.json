[{"figure_path": "zTu0QEpvtZ/figures/figures_3_1.jpg", "caption": "Figure 1: Figure 1a is the averaged cross-attention over denoising steps. The two generated images are on the top, and the weights in cross-attention maps of each tokens are on the bottom with whiter pixels correspond to larger weights in cross-attention map. Figure 1b is obtained by taking average over tokens and prompts in PromptSet, which compares the shapes of cross-attention map and final generated images, Measured by relative F1-score F1t/F11 over different denoising steps.", "description": "This figure shows the cross-attention maps during the denoising process. (a) shows the averaged cross-attention map for each token at different time steps. The brighter the pixel, the stronger the attention weight. (b) plots the relative F1-score (the overlap between the shapes of the cross-attention map and the generated image) against the denoising steps. The figure demonstrates that the overall shape of the generated image is decided in the early stage, while the details are refined at the later stages.", "section": "4 First Overall Shape then Details"}, {"figure_path": "zTu0QEpvtZ/figures/figures_3_2.jpg", "caption": "Figure 1: Figure 1a is the averaged cross-attention over denoising steps. The two generated images are on the top, and the weights in cross-attention maps of each tokens are on the bottom with whiter pixels correspond to larger weights in cross-attention map. Figure 1b is obtained by taking average over tokens and prompts in PromptSet, which compares the shapes of cross-attention map and final generated images, Measured by relative F1-score F1t/F11 over different denoising steps.", "description": "This figure shows the cross-attention maps of a text-to-image diffusion model during the denoising process. (a) shows the cross-attention map averaged over denoising steps for each token in the prompt. The two generated images corresponding to the prompt are shown at the top, and the cross-attention weights for each token are visualized at the bottom, where white pixels indicate higher weights. (b) shows the convergence of the cross-attention map over denoising steps, measured by relative F1-score between the cross-attention map and the final generated image. This demonstrates that the overall shape of the generated image is determined in the early stages of denoising, while details are filled in later stages.", "section": "4 First Overall Shape then Details"}, {"figure_path": "zTu0QEpvtZ/figures/figures_4_1.jpg", "caption": "Figure 2: Figure 2a visualizes the completed noisy data and its high-frequency, and low-frequency parts over different time steps, listed from top to bottom. Figures 2b and 2c measure the low/high-frequency signals of xt. In Figure 2b, \u201cLow_Add_Noisy_Data/eps\u201d means the norm of \u221aatxlow and \u221a1 \u2013 \u0101t\u025blow, vise versa for \u201cHigh....\u201d. On the other hand, Figure 2c measures the variation ratio of high/low frequency parts of images during the noising/denoising process. For example, \u201cHigh_Add_Noise\u201d represents ||\u00e6high \u2013 \u00e6high||||\u00e6high|| during noising process.", "description": "This figure shows an empirical analysis of the frequency components of noisy images during the denoising process.  Subfigure (a) visualizes the separation of high and low-frequency components of noisy images at different time steps. Subfigures (b) and (c) quantify the changes in norms and ratios of these components, respectively, during both the forward (noise adding) and reverse (denoising) processes. This helps explain the two-stage process of image reconstruction: shape is recovered first (low frequency) and details later (high frequency).", "section": "4 First Overall Shape then Details"}, {"figure_path": "zTu0QEpvtZ/figures/figures_5_1.jpg", "caption": "Figure 3: Averaged weights in cross-attention map over pixels of three classes of tokens. For each prompt in PromptSet, the result is obtained by taking average over tokens in each class. The final result is the average over PromptSet. Notably, the weights on [SOS] are all larger than 0.9.", "description": "This figure shows the average weights of cross-attention maps over pixels for three token classes ([SOS], semantic tokens, and [EOS]) across different denoising steps.  The average is taken across all prompts in the PromptSet.  It highlights that the weights for [SOS] tokens are consistently high (above 0.9), indicating their significant influence in the cross-attention mechanism, despite [SOS] not carrying semantic meaning itself.", "section": "5 The Working Mechanism of Text Prompt"}, {"figure_path": "zTu0QEpvtZ/figures/figures_5_2.jpg", "caption": "Figure 4: Images under prompts from S-PromptSet with switched [EOS]. The objects are consistent with the ones conveyed by [EOS], while some information in semantic tokens is still conveyed.", "description": "This figure shows the results of an experiment where the [EOS] token in a set of prompts (S-PromptSet) was replaced with the [EOS] token from a different prompt.  The generated images show that the object depicted primarily aligns with the object associated with the new [EOS] token, indicating its significant impact in the early stage of image generation. However, some information from the original prompt's semantic tokens is still present in the generated images, demonstrating a nuanced interaction between [EOS] and semantic tokens.", "section": "5 The Working Mechanism of Text Prompt"}, {"figure_path": "zTu0QEpvtZ/figures/figures_6_1.jpg", "caption": "Figure 5: Desnoising process under text prompt with switched [EOS] in [a, 50].", "description": "This figure illustrates the denoising process of Stable Diffusion when the [EOS] token in a text prompt is switched at a specific time step (denoted as 'a'). The top line shows the process visualized as a timeline progressing from noisy data at time step 0 to the final generated image at time step T.  The blue section represents the process where the original prompt with its [EOS] token is used, and the brown section represents the process where the [EOS] token from a different prompt is used, illustrating the effect of switching the [EOS] during generation.", "section": "5 The Working Mechanism of Text Prompt"}, {"figure_path": "zTu0QEpvtZ/figures/figures_6_2.jpg", "caption": "Figure 1: Figure 1a is the averaged cross-attention over denoising steps. The two generated images are on the top, and the weights in cross-attention maps of each tokens are on the bottom with whiter pixels correspond to larger weights in cross-attention map. Figure 1b is obtained by taking average over tokens and prompts in PromptSet, which compares the shapes of cross-attention map and final generated images, Measured by relative F1-score F1t/F11 over different denoising steps.", "description": "This figure shows the cross-attention maps and their convergence during the denoising process. (a) shows an example of cross-attention maps at different steps. Whiter pixels indicate stronger attention weights.  (b) demonstrates the convergence of cross-attention maps to the final generated image shape over time, measured by relative F1-score.", "section": "4 First Overall Shape then Details"}, {"figure_path": "zTu0QEpvtZ/figures/figures_7_1.jpg", "caption": "Figure 7: Desnoising with text prompt injected in [0, a].", "description": "This figure illustrates the process of denoising with text prompt injected only in the interval [0, a].  The denoising process starts from a noisy image at t=T and progresses towards a clean image at t=0.  The text prompt, indicated by w = 7.5, influences the denoising process only up to time step 'a'. After time step 'a', the text prompt is removed (w=0), and the denoising process continues without text guidance. This experiment is designed to investigate the effect of removing textual information during the denoising process.", "section": "5.2 The Text Prompt Mainly Working on the First Stage"}, {"figure_path": "zTu0QEpvtZ/figures/figures_7_2.jpg", "caption": "Figure 8: Figure 8a is the relative difference \"current minus worst\" over \"best minus worst\" under different start step a of Denoising process Figure 7. The last two figures 8b are per-dimensional norm of unconditional noise e\u03b8(t, xt, (\u00d8) and noise difference w (e\u03b8(t, xt, C) \u2013 e\u03b8(t, xt, \u00d8)) conveyed during the early stage of denoising process. Therefore, the overall shape of generated image is mainly decided by the text prompt, while the its details are then reconstructed by itself.", "description": "This figure shows the results of an experiment where the text prompt was removed at different stages of the denoising process.  Figure 8a shows that removing the text prompt later in the process (larger start step) leads to a greater divergence from the original image (higher L1-norm).  However, the generated image still maintains similarity to the original, as indicated by the CLIP score. Figure 8b visualizes the norm of the unconditional noise and the noise difference with and without the text prompt. This supports the claim that the text prompt primarily affects the early stages of the process, focusing on the overall shape.", "section": "5 The Working Mechanism of Text Prompt"}, {"figure_path": "zTu0QEpvtZ/figures/figures_7_3.jpg", "caption": "Figure 2: Figure 2a visualizes the completed noisy data and its high-frequency, and low-frequency parts over different time steps, listed from top to bottom. Figures 2b and 2c measure the low/high-frequency signals of xt. In Figure 2b, \u201cLow_Add_Noisy_Data/eps\u201d means the norm of \u221aatxlow and \u221a1 \u2013 atelow, vise versa for \u201cHigh....\u201d. On the other hand, Figure 2c measures the variation ratio of high/low frequency parts of images during the noising/denoising process. For example, \u201cHigh_Add_Noise\u201d represents ||xhigh \u2013 xhigh||||xhigh|| during noising process.", "description": "This figure shows the analysis of low and high-frequency signal changes during the denoising process. Figure 2a visualizes the noisy data and its high and low-frequency components at different time steps.  Figures 2b and 2c quantify these changes, showing the norms of low- and high-frequency components and their ratio over time. This helps explain the two-stage reconstruction process of overall shape before details.", "section": "4 First Overall Shape then Details"}, {"figure_path": "zTu0QEpvtZ/figures/figures_8_1.jpg", "caption": "Figure 7: Desnoising with text prompt injected in [0, a].", "description": "The figure shows the denoising process with text prompt injected only in the interval [0,a].  In the interval [a, T], the text prompt is removed (w=0), whereas for the interval [0,a] it's active (w=7.5). This experiment is designed to analyze the impact of removing text guidance at different stages of the denoising process on the final generated image. By varying the value of 'a', one can study the influence of text prompt at different stages.", "section": "5.2 The Text Prompt Mainly Working on the First Stage"}, {"figure_path": "zTu0QEpvtZ/figures/figures_8_2.jpg", "caption": "Figure 10: The generated images with 25 steps DPM-Solver under  in (9) (Figure 9). The textual information is removed during  \u2208 [0, a]. With a \u2192 25, the inference cost is decreased.", "description": "This figure shows the results of image generation using different methods (SD v1.5, SD v2.1, Pixart-Alpha) and different values of 'a' (the point at which textual information is removed during the denoising process). As 'a' increases, less textual information is used, leading to a decrease in inference cost.  The images demonstrate how the removal of textual information affects the generated images.", "section": "6 Application"}, {"figure_path": "zTu0QEpvtZ/figures/figures_14_1.jpg", "caption": "Figure 11: Relative BLIP-VQA Combine color and texture add complex, CLIP score, MiniGPT-COT with source (or target) prompt under different number of [EOS]. Here the y-axis is the current BLIP-VQA, CLIP, MiniGPT-CoT over the maximum ones, which is used to alleviate the bias brought by the metric itself.", "description": "This figure shows the results of an ablation study on the number of [EOS] tokens in text prompts.  The y-axis represents the relative scores (normalized to the maximum score) for BLIP-VQA, CLIPScore, and MiniGPT-CoT, measuring the alignment of generated images with source and target prompts. The x-axis indicates the varying number of [EOS] tokens.  The results demonstrate the impact of the number of [EOS] tokens on the generation process and the alignment of generated images with prompts.  As the number of [EOS] tokens decreases, the image alignment with source prompts increases, while alignment with target prompts decreases.", "section": "C Ablation Study on Number of [EOS]"}, {"figure_path": "zTu0QEpvtZ/figures/figures_15_1.jpg", "caption": "Figure 12: The visualization of cross-attention map under text prompt with switched [EOS] from S-PromptSet. The pixels corresponding to semantic tokens are in the shape of the final generated data as in text prompt B provide [EOS]. For example, the token \u201cchair\u201d corresponds to pixels in the shape of paint, so its information can not be conveyed, while this phenomenon does exist in the attribute token \"leather\".", "description": "This figure visualizes the cross-attention maps for two prompts from the S-PromptSet dataset, where the [EOS] tokens have been switched.  It shows that, at early stages of denoising, the cross-attention maps reflect the overall shape of the image. The pixels activated by the tokens that describe the object itself (e.g., \"chair\") often match the overall shape, while those for the attribute tokens (e.g., \"leather\") may also match the image details. The figure demonstrates that information in [EOS] dominates the generated image shape.", "section": "4 First Overall Shape then Details"}, {"figure_path": "zTu0QEpvtZ/figures/figures_15_2.jpg", "caption": "Figure 13: Generated images with prompts only contain information from [SOS] or [EOS].", "description": "This figure shows generated images using prompts that only contain [SOS] or [EOS] tokens.  The purpose is to demonstrate that [SOS] tokens do not contain semantic information, unlike [EOS] tokens which do contain information that influences the generated image.", "section": "E [SOS] Contains no Textual Information"}, {"figure_path": "zTu0QEpvtZ/figures/figures_16_1.jpg", "caption": "Figure 14: Generated images with zero or random vectors substitution.", "description": "This figure visualizes the results of replacing semantic tokens or [EOS] with zero vectors or random Gaussian noise in text prompts.  The goal was to investigate the impact of [EOS] and semantic tokens on image generation. Four sets of text prompts were used. The first row shows images generated from prompts where semantic tokens were kept but [EOS] was replaced. The second row shows images with semantic tokens replaced and [EOS] kept. The third and fourth rows show images where both were replaced with zeros and random noise, respectively.  The results show that the combination of semantic tokens and [EOS] produced the best overall image quality.", "section": "F More Evidences on [EOS] Contains More Information"}, {"figure_path": "zTu0QEpvtZ/figures/figures_17_1.jpg", "caption": "Figure 15: Generated examples of Key or Value substitution.", "description": "This figure shows the results of substituting either the key or value of the [EOS] token in the cross-attention module during the stable diffusion process.  The figure demonstrates how replacing either the key or value impacts the generation of images, specifically showing how the generated images align with either the source or target prompt. This helps illustrate the individual influence of the key and value components of the [EOS] token in shaping the generated image.", "section": "G Key or Value Dominates the Influence?"}, {"figure_path": "zTu0QEpvtZ/figures/figures_18_1.jpg", "caption": "Figure 16: The averaged KL-divergence over pixels and layers.", "description": "This figure shows the logarithmic results of KL-divergence over pixels and layers when substituting the [EOS] token in K (Key) with a uniform distribution.  The x-axis represents the time steps in the denoising process, and the y-axis shows the logarithmic results of the KL-divergence. The red line represents the KL-divergence when substituting [EOS] in K, while the green line represents the KL-divergence of a uniform distribution. The figure demonstrates that the KL-divergence for substituting [EOS] in K is consistently lower than the KL-divergence of a uniform distribution throughout the denoising process.", "section": "4 First Overall Shape then Details"}, {"figure_path": "zTu0QEpvtZ/figures/figures_18_2.jpg", "caption": "Figure 17: We implement our sampling strategy on subject-driven generation model, AnyDoor [44]. We remove the condition image from different time steps (denote as a) during denoising process. The generated images still preserve the specific details as baseline model (start point a = 0) when start removing time steps is set to 20.", "description": "This figure shows the results of applying the proposed sampling acceleration technique to the AnyDoor subject-driven generation model.  The model generates images by conditioning on both a base image and a subject image. By removing the subject image's influence at different stages (controlled by the parameter 'a') during the denoising process, the model can accelerate generation. The figure demonstrates that removing the subject image at later stages (larger 'a' values) still produces images that retain the desired details.", "section": "6 Application"}, {"figure_path": "zTu0QEpvtZ/figures/figures_18_3.jpg", "caption": "Figure 18: We implement our sampling strategy on human face generation model, PhotoMaker. We remove the condition (text prompts and reference face) from different time steps (denote as a) during denoising process. The generated images and faces still preserve the specific details as baseline model (start point a = 0) when start removing time steps is set to 20.", "description": "This figure shows the results of applying a sampling acceleration strategy to a human face generation model called PhotoMaker.  The strategy involves removing the text prompt and reference face conditions at various points in the denoising process (parameterized by 'a'). The figure demonstrates that even when removing the conditions relatively early in the process (a=20), the generated faces still retain a significant level of detail and accuracy compared to the baseline model (a=0).", "section": "6 Application"}, {"figure_path": "zTu0QEpvtZ/figures/figures_19_1.jpg", "caption": "Figure 19: More generated examples under tokens from S-PromptSet.", "description": "This figure shows more examples of images generated using the Switched-PromptSet (S-PromptSet) method, where the [EOS] token in prompts is replaced with that from another prompt. Each row shows the results from a pair of original prompts and the corresponding results with switched [EOS] tokens. The images demonstrate the influence of [EOS] tokens on the generation process, even when other tokens remain unchanged.", "section": "I More Generated Images"}, {"figure_path": "zTu0QEpvtZ/figures/figures_20_1.jpg", "caption": "Figure 20: The generated images with 50 steps DDIM under \\(e_\\theta\\) in (9), where the textual information are \\(C\\) removed during time steps \\(t \\in [0, a]\\). With \\(a \\to 50\\), the inference cost is decreased.", "description": "This figure shows the results of image generation using different sampling strategies.  The textual information (C) is removed during a portion of the denoising process (t \u2208 [0, a]), with 'a' varying from 0 to 50.  The aim is to show how reducing the influence of the text prompt at later stages of the process can reduce computational cost while maintaining image quality. The results are shown for three different models (SD v1.5, SD v2.1, and Pixart-Alpha).", "section": "I.2 Generated Images in Paragraph \u201cAcceleration of Sampling\u201d of Section 6"}, {"figure_path": "zTu0QEpvtZ/figures/figures_21_1.jpg", "caption": "Figure 21: More generated images with 25 steps DPM-Solver under e\u03b8 in (9), where the textual information are C removed during time steps t \u2208 [0, a]. With a \u2192 25, the inference cost is decreased.", "description": "This figure shows the results of image generation using different models (SD v1.5, SD v2.1, and Pixart-Alpha) with varying start points (a) for removing textual information during the denoising process.  The prompt used was \"A plate with some vegetables and meat on it.\"  As the value of 'a' increases, less textual information is used in the earlier stages of generation. The goal is to demonstrate that removing text guidance in the initial stages accelerates sampling by reducing computational cost while maintaining image quality. The images generated illustrate this trade-off.", "section": "I.2 Generated Images in Paragraph \u201cAcceleration of Sampling\u201d of Section 6"}]