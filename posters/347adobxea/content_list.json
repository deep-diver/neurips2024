[{"type": "text", "text": "Geometry Awakening: Cross-Geometry Learning Exhibits Superiority over Individual Structures ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yadong Sun1 Xiaofeng Cao1,  Yu Wang1 Wei $\\mathbf{Y}\\mathbf{e}^{2}$ Jingcai Guo3 Qing Guo4 ", "page_idx": 0}, {"type": "text", "text": "1School of Artificial Intelligence, Jilin University, China 2College of Electronic and Information Engineering, Tongji University, China 3The Hong Kong Polytechnic University 4CFAR and IHPC, Agency for Science, Technology and Research (A\\*STAR), Singapore sunyd22@mails.jlu.edu.cn, xiaofengcao@jlu.edu.cn, yu_w18@mails.jlu.edu.cn, yew@tongji.edu.cn, jc-jingcai.guo@polyu.edu.hk, tsingqguo@ieee.org ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent research has underscored the efficacy of Graph Neural Networks (GNNs) in modeling diverse geometric structures within graph data. However, real-world graphs typically exhibit geometrically heterogeneous characteristics, rendering the confinement to a single geometric paradigm insufficient for capturing their intricate structural complexities. To address this limitation, we examine the performance of GNNs across various geometries through the lens of knowledge distillation (KD) and introduce a novel cross-geometric framework. This framework encodes graphs by integrating both Euclidean and hyperbolic geometries in a space-mixing fashion. Our approach employs multiple teacher models, each generating hint embeddings that encapsulate distinct geometric properties. We then implement a structure-wise knowledge transfer module that optimally leverages these embeddings within their respective geometric contexts, thereby enhancing the training efficacy of the student model. Additionally, our framework incorporates a geometric optimization network designed to bridge the distributional disparities among these embeddings. Experimental results demonstrate that our model-agnostic framework more effectively captures topological graph knowledge, resulting in superior performance of the student models when compared to traditional KD methodologies. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph Neural Networks (GNNs) have emerged as indispensable tools for analyzing relational data in diverse domains, such as natural language processing [1, 2, 3], computer vision [4, 5], recommendation systems [6, 7]. Their conventional approach of operating within Euclidean space encounters limitations when confronted with datasets embodying non-Euclidean characteristics, such as powerlaw distribution and hierarchical structures, prevalent in real-world applications [8]. Recognizing this challenge, our community ventures into the realm of non-Euclidean Graph Neural Networks, seeking to harness alternative geometries, notably hyperbolic space, for more adeptly capturing the intricate topological features inherent in many real-world networks [9]. By synthesizing recent advancements and empirical findings, we endeavor to elucidate the potential of non-Euclidean GNNs in effectively modeling complex relational data structures, thereby paving the way for advancements in various application domains [10, 11, 12, 13, 14, 15]. ", "page_idx": 0}, {"type": "text", "text": "Unlike the constant and flat Euclidean geometry, hyperbolic geometry offers greater flexibility by integrating curvature information, enabling better alignment with the characteristics of non-Euclidean input graphs. This endeavor has rendered hyperbolic GNNs more accessible and comparable to their Euclidean counterparts, resulting in promising performance and interpretability in graph representation learning. Hyperbolic GNNs [16] extended the neighborhood aggregation operation by computing centroids in the hyperbolic geometry. This approach effectively fuses the node features and hierarchical structure, thereby learning superior node representations. Furthermore, a full manifold-preserving feature transformation operation has been developed in hyperbolic geometry [15], eliminating the complicated transformations between hyperbolic and tangent spaces. With these essential operation, hyperbolic GNNs can achieve comparable or even superior performance than Euclidean GNNs. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Question. Although there has been a surge of research on Euclidean and non-Euclidean GNNs in the community, it remains unclear which geometry offers greater advantages. Real-world graphs often exhibit geometrically structural heterogeneity, characterized by variations in clustering and density among nodes [17, 18], as shown in Figure 1. The structural heterogeneity pose a challenge when attempting to accurately model the graph structure using GNNs solely equipped with either Euclidean or non-Euclidean geometry. ", "page_idx": 1}, {"type": "text", "text": "Motivation. According Local Subgraph Preservation Property [19], the properties of a node largely depend on the properties of the local subgraph centered around it. Considering the hyperbolic property of local subgraphs, i,e., hyperbolicity 1. Employing hyperbolic geometry modeling achieves higher precision and minimal information loss when hyperbolicity is low. Conversely, when hyperbolicity is elevated, opting for Euclidean geometry modeling results in lower complexity and slightly superior performance compared to hyperbolic geometry. Consequently, the primary limitation of existing graph neural networks is their inability to adaptively select the appropriate geometry for representing nodes with different local structures [21, 22]. ", "page_idx": 1}, {"type": "text", "text": "Our scheme. In this paper, we propose a crossgeometric graph knowledge distillation framework that encodes graphs utilizing both Euclidean and hyperbolic geometry in a locally space-mixing fashion. In contrast to traditional methods that compute hyperbolicity for the overall graph and roughly analyze its applicability to different geometries, our approach performs fine-grained analysis on the local subgraphs surrounding each node. This enables the selection of embeddings in the most appropriate geometry for local subgraphs, which is subsequently utilized to transfer knowledge to the student model. Additionally, we introduce a geometric embedding optimization module to optimize the distribution of embeddings produced by the teacher models. To evaluate the performance of our proposed approach, we conduct distillation experiments on node classification (NC) and link prediction (LP) tasks across various types of graph data. The experimental results demonstrate the superiority of our approach in teaching student models compared to other baseline methods. Our approach highlights enhanced effectiveness and generalization, ultimately achieving state-of-the-art perfor", "page_idx": 1}, {"type": "image", "img_path": "347aDObXEa/tmp/fe9af406dbe767a187cfde91161adc21171f0f8e7f4f00b674d3c5136ea1b077.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Visualization of the embedding of the same graph in hyperbolic space (left) and Euclidean space (right), with different colors representing different class labels. Tree-like subgraphs maintain significant inter-class margins in hyperbolic space, leading to improved classification boundaries, while intra-class nodes with Euclidean properties may be overstretched due to the utilization of hyperbolic metrics, hence embedding in Euclidean space is preferred. ", "page_idx": 1}, {"type": "text", "text": "mance in graph data distillation tasks. The salient aspects of our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Structured analysis reveals that both Euclidean and hyperbolic geometries demonstrate commendable performance in graph processing, despite their inherent disparities and potential geometric conflicts. This prompts scholarly inquiry into reconciling these divergent geometric spaces within GNNs, inspiring new research avenues in geometry awareness. \u2022 With heightened awareness, there\u2019s potential to represent graph structures across dimensions, transcending singular space limitations. We thus adapt and integrate teacher embeddings from diverse geometries, transferring them to more effective cross-geometric space. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Extensive experiments employing KD techniques on diverse graph datasets demonstrate that cross-geometric methods significantly outperform traditional approaches in the context of knowledge transfer. This is particularly evident in NC and LP tasks, thereby affirming the superior efficacy of these methods. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Graph Neural Networks. GNNs are neural network models that capture interdependencies between nodes by propagating messages among them within a graph. The most representative model is Graph Nonvolutional Network (GCN) [23, 24, 25], which can be regarded as a generalization of convolutional neural networks to graph data. The GraphSAGE [26] employs a neighbor sampling strategy to address graph data, enabling information aggregation based on the local neighborhood structure of nodes. The attention-based GNN [27, 28, 29] model employs masked self-attention, assigning diverse weights to node representations based on the varying features of neighboring nodes. Notably, constructing GNNs in the hyperbolic space [13, 15, 30] significantly reduces embedding distortions caused by the inability of Euclidean space to handle power-law distributions, particularly in the case of tree-like or highly hierarchical data. ", "page_idx": 2}, {"type": "text", "text": "Knowledge Distillation. KD initially proposed by [31], is a model compression technique that involves leveraging pre-trained teacher models to guide the training of a lightweight student model [32, 33]. After that, [34] aligns student and teacher model intermediate features using a regressor and a loss function to minimize feature differences. [35] employs attention mechanisms to extract features from teacher model\u2019s intermediate layers and transfer them to the student model. As GNNs have demonstrated breakthrough performance in various deep learning tasks, a number of graphbased KD frameworks have been proposed successively. [36] introduces a local structure preserving module to extract knowledge from intermediate layers of the GNN model, guiding the student model to optimize learned topological structures. [37] proposes a novel approach to effectively learn multi-scale topological semantics from multiple GNN teacher models to guide student model. [38] incorporates a VQ-VAE to learn a codebook that represents informative local structures, and uses these local structures as additional information for distillation. However, all these methods rely on the Euclidean geometry. Our proposed approach leverages both Euclidean and non-Euclidean geometries to learn representations of highly hierarchical local structures, ensuring that the knowledge transmitted to the student model is highly reliable. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Definition and Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem Definition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "For the graph KD, given a graph $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$ , where $\\nu$ denotes the node set and $\\mathcal{E}$ denotes the edge set. Let $N$ denotes the total number of nodes in the graph $\\mathcal{G}$ , $\\mathbf{X}$ denotes nodes\u2019 feature matrix with each row corresponding to the feature vector of a node, and A denotes graph\u2019s $N\\times N$ adjacency matrix, where $A_{i j}$ signifies whether there is an edge between nodes $i$ and $j$ . If $A_{i j}=1$ , an edge exists; otherwise, no edge is present. Let $\\mathcal{M}_{T}=\\{m_{T_{1}},m_{T_{2}},...,m_{T_{R}}\\}$ denotes the teacher models pre-trained on $\\mathcal{G}$ , $R$ represents the number of geometries. Our fundamental objective is to extract information from $\\mathcal{G}$ by $\\mathcal{M}_{T}$ , and employing it to boost the training process of the student model, denoted as $m_{S}$ , which in Euclidean space and has significantly smaller size. Let $\\mathcal{Z}_{T}=\\{\\mathbf{Z}_{T_{1}},\\mathbf{Z}_{T_{2}},...,\\mathbf{Z}_{T_{R}},\\}$ be the outputs of the teacher models and $\\mathbf{Z}_{S}$ be the outputs of the student model. The optimization goal is to minimize the disparity between predictions of $\\mathcal{M}_{\\mathbf{T}}$ and $m_{S}$ on $\\mathcal{G}$ , i.e., ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{m_{s}}\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{R}\\beta_{j}\\cdot\\mathcal{F}_{d i s}(z_{T_{j},i}||z_{S,i}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\beta_{j}$ denotes the weight of the $j$ -th teacher model, ${\\mathcal{F}}_{d i s}$ denotes disparity measurement function. ", "page_idx": 2}, {"type": "text", "text": "3.2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this paper, We focus on distillation performance of the teacher model individually in Euclidean, hyperbolic, and spherical geometries, as well as across geometries. We give a necessary introduction of hyperbolic geometry in this subsection, with other information available in Appendix A. ", "page_idx": 2}, {"type": "text", "text": "Hyperbolic geometry studies the properties of curved space with negative curvature. Hyperbolic space can be modeled using five isometric models [39, 40], and in this paper, we adopt Poincar\u00e9 disk model. The Poincar\u00e9 disk model evinces a distinctive property wherein distances from the geometric center to the periphery undergo a non-linear augmentation as a function of layer depth[9]. This phenomenon engenders a nonlinear and multi-branch composite structure within the model\u2019s geometric framework. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1 (Poincar\u00e9 Disk Model). A $n$ -dimensional Poincar\u00e9 disk model $(\\mathbb{B}_{c}^{n},g^{\\mathbb{B}})$ is a complete Riemannian manifold with a negative constant curvature $c_{:}$ , which defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{B}_{c}^{n}:=\\left\\{x\\in\\mathbb{R}^{n}:-c\\|x\\|^{2}<1\\right\\},\\quad g^{\\mathbb{B}}=(\\lambda_{x}^{c})^{2}g^{\\mathbb{E}},\\quad\\lambda_{x}^{c}=\\frac{2}{1-c\\|x\\|^{2}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\|\\cdot\\|$ denotes the Euclidean norm, $g$ denotes the Riemannian metric, and The superscripts B and $\\mathbb{E}$ indicate that the vector or matrix resides in hyperbolic space and Euclidean space, respectively. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.2 (Hyperbolic Operations). Given two points $\\mathbf{\\Delta}x,y\\,\\in\\,\\mathbb{B}_{c}^{n}$ , the hyperbolic distance between them is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\nd_{c}({\\pmb x},{\\pmb y})=\\frac{2}{\\sqrt{c}}\\operatorname{tanh}^{-1}\\left(\\sqrt{c}\\,\\|\\!-\\!{\\pmb x}\\oplus_{c}{\\pmb y}\\|\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\oplus_{c}$ denotes M\u00f6bius addition, i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\pmb x}\\oplus_{c}{\\pmb y}:=\\frac{\\left(1+2c\\langle{\\pmb x},{\\pmb y}\\rangle+c\\|{\\pmb y}\\|^{2}\\right){\\pmb x}+\\left(1-c\\|{\\pmb x}\\|^{2}\\right){\\pmb y}}{1+2c\\langle{\\pmb x},{\\pmb y}\\rangle+c^{2}\\|{\\pmb x}\\|^{2}\\|{\\pmb y}\\|^{2}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Definition 3.3 (Tangent Space). The tangent space at a point $\\textbf{\\em x}$ in hyperbolic space, denoted as $\\mathcal{T}_{\\mathbf{x}}\\mathbb{B}_{c}^{n}$ , serves as the first-order approximation of the original space, a n-dimensional tangent space is isomorphic to Euclidean space $\\mathbb{R}^{n}$ . Representations between hyperbolic and tangent space can be transformed via the exponential and logarithmic map as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{T}_{x}\\mathbb{B}_{c}^{n}\\to\\mathbb{B}_{c}^{n}:\\exp_{x}^{c}(v)=x\\oplus_{c}\\left(\\operatorname{tanh}\\left(\\sqrt{c}\\frac{\\lambda_{x}^{c}\\|v\\|}{2}\\right)\\frac{v}{\\sqrt{c}\\|v\\|}\\right),}\\\\ &{\\mathbb{B}_{c}^{n}\\to\\mathcal{T}_{x}\\mathbb{B}_{c}^{n}:\\log_{x}^{c}(y)=d_{c}({\\pmb x},{\\pmb y})\\frac{-{\\pmb x}\\oplus_{c}{\\pmb y}}{\\lambda_{x}^{c}\\,\\|-{\\pmb x}\\oplus_{c}{\\pmb y}\\|},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\pmb{v}\\in\\mathcal{T}_{\\pmb{x}}\\mathbb{B}_{c}^{n}$ , $\\pmb{y}\\in\\mathbb{B}_{c}^{n}$ and $\\lambda_{x}^{c}$ has same meaning in Eq. (2). Here we utilize the origin point o in hyperbolic space as a reference point $\\textbf{\\em x}$ to equalize errors across various directions. ", "page_idx": 3}, {"type": "text", "text": "4 Cross-Geometry Learning with KD ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This section reveals three key aspects: why cross-geometry learning demonstrates superior performance, why it is feasible, and how this superiority is achieved by employing KD. Thus, we analyze our method from three perspectives: reasonableness, superiority, and trustworthiness. ", "page_idx": 3}, {"type": "text", "text": "4.1 Geometric Features of Local Subgraphs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Reasonableness: According local subgraph perservation peoperty theorem [19], nodes near the central node strongly affect its features, while distant nodes typically have negligible impact. The graph data in real-world often exhibits significant complexity, where diverse local subgraphs often entail distinct geometric properties [17, 18], employing cross-geometry system can offer more effective embedding selections for local graphs, thereby achieving performance beyond that of single geometry methods. ", "page_idx": 3}, {"type": "text", "text": "Definition 4.1 (Subgraphs of Centroid $p$ ): For a given node $p$ belonging to graph $\\mathcal{G}$ , its corresponding $k$ -hops subgraph $\\mathcal{G}_{p}$ comprises all nodes $q\\in\\mathcal{V}\\backslash\\{p\\}$ within a distance no greater than $k$ from $p$ along with their respective edges. ", "page_idx": 3}, {"type": "text", "text": "We employ the $k$ -hops neighbors method to generate subgraphs. Hence, we determine the optimal value of $k$ through the statistical analysis of graph data in section 5.4. For each subgraph $\\mathcal{G}_{i}$ , we calculate their Gromov\u2019s $\\delta$ -hyperbolicity (See the appendix A.1 for the calculating process) based on $\\mathbf{X}$ , denoted as $\\delta_{\\mathcal{G}_{i}}$ , which serves as a geometric characterization metric for the central node $i$ . ", "page_idx": 3}, {"type": "image", "img_path": "347aDObXEa/tmp/426011f12b5dc7f13732973e11bf4812f4ac809dc18dc854aa763e8c565ac7e6.jpg", "img_caption": ["Figure 2: Illustration of our proposed cross-geometry graph KD framework. Structure-Wise Knowledge Transfer (SWKT): Choosing embeddings in appropriate geometric spaces using $\\delta_{\\mathcal{G}_{i}}$ of nodes, conveying local subgraph topological knowledge to the student model: $\\mathbf{Z}_{T}^{\\left(i\\right),\\mathbb{E}}$ denotes Euclidean teaching, and $\\mathbf{Z}_{T}^{\\left(i\\right),\\mathbb{B}}$ denotes the hyperbolic teaching. GEO: Enhancing hint embeddings from the teacher models, reducing the negative effects of inconsistencies between different geometries. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4.2 Geometric Teacher Models ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Superiority: We leverage KD technology, utilizing its ability to transfer knowledge between different model architectures, as a medium for interoperability between different geometries. Our proposed KD framework is model-agnostic, making it applicable to various geometric models. ", "page_idx": 4}, {"type": "text", "text": "Herein, the framework will be explained using the GCN model [23]. To minimize disparities between the intermediate layers of the teacher and student models, our method uses pre-activation node embeddings $_{\\textit{z}}$ to guide the student model and constructs an embedding matrix $\\mathbf{Z}$ for all nodes\u2019 $_{\\textit{z}}$ . During the forward propagation process of a GCN layer in Euclidean space, the intermediate embeddings of nodes in the $l$ -th layer of GCN is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{Z}_{T}^{(l),\\mathbb{E}}=\\hat{\\mathbf{A}}\\mathbf{H}_{T}^{(l-1),\\mathbb{E}}\\mathbf{W}^{(l)},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{H}_{T}^{(l-1),\\mathbb{E}}=a c t i v a t i o n(\\mathbf{Z}_{T}^{(l-1),\\mathbb{E}})$ denotes the node representation matrix from the output of the $(l-1)$ -th GCN layer. $\\hat{\\bf A}$ denotes the symmetrically normalized adjacency matrix, $\\mathbf{W}^{(l)}$ denotes the weight matrix. ", "page_idx": 4}, {"type": "text", "text": "For the i-th node in layer l, its embedding in hyperbolic space is denoted as z(Tl,)i,B and its representation is denoted as $h_{T,i}^{(l),\\mathbb{B}}$ . During the forward propagation process of a GCN layer in hyperbolic space, the transformed feature is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{f}_{T,i}^{(l),\\mathbb{B}}=\\exp_{o}^{c}\\left[\\mathbf{W}^{(l)}\\log_{o}^{c}\\left(\\pmb{h}_{T,i}^{(l-1),\\mathbb{B}}\\right)\\right]\\oplus_{c}\\mathbf{b}^{\\mathbb{B}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "By performing neighborhood aggregation on these features, we obtain the hyperbolic intermediate embeddings of $i$ -th node in the $l$ -th layer as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{z_{T,i}^{(l),\\mathbb{B}}=\\exp_{\\pmb{f}_{T,i}^{(l),\\mathbb{B}}}^{c}\\left[\\sum_{j:(i,j)\\in\\mathcal{E}}w_{i j}\\log_{\\pmb{f}_{T,i}^{(l),\\mathbb{B}}}^{c}\\left(\\pmb{f}_{T,j}^{(l),\\mathbb{B}}\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $w_{i j}$ is the weight coefficient computed by $\\pmb{f}_{T,i}^{(l),\\mathbb{B}}$ and $\\pmb{f}_{T,j}^{(l),\\mathbb{B}}$ ", "page_idx": 4}, {"type": "text", "text": "4.3 Structure-Wise Knowledge Transfer ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Trustworthiness: To achieve a more fine-grained selection of embeddings in appropriate geometry, we designed a Structure-Wise Knowledge Transfer (SWKT) module. This module determines suitable geometric representations based on the geometric feature $\\delta_{\\mathcal{G}_{i}}$ of subgraphs and transfers them to the student, facilitating more effective information extraction and guidance. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Specifically, we obtain representations of local subgraphs centered around each node based on the $l$ -th intermediate layer hint embeddings of the teacher model in different geometries. We denote $i$ -th local structure representation in hyperbolic geometry as uT,iB= {uT,i1B , uT,i2B , ..., uT,ijB , ..., uT,iBN}. Element uT,ijB in hyperbolic geometry can be computed as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u_{T,i j}^{(l),\\mathbb{B}}=\\!\\mathcal{F}_{s u b}\\left(\\log_{o}^{c}(z_{T,i}^{(l)}),\\log_{o}^{c}(z_{T,j}^{(l)})\\right)}\\\\ &{\\qquad=\\exp\\left(\\left\\|\\log_{o}^{c}(z_{T,i}^{(l),\\mathbb{B}}),\\log_{o}^{c}(z_{T,j}^{(l),\\mathbb{B}})\\right\\|^{2}\\right)\\!/\\!\\sum_{j:(j,i)\\in\\mathcal{E}}\\left(\\exp\\left(\\left\\|\\log_{o}^{c}(z_{T,i}^{(l),\\mathbb{B}}),\\log_{o}^{c}(z_{T,j}^{(l),\\mathbb{B}})\\right\\|^{2}\\right)\\right)\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Similarly, we can obtain representation set of $i$ -th local structure in Euclidean geometry denoted as ${\\pmb u}_{T,i}^{(l),\\mathbb{E}}$ . SWKT generates induced $k$ -hops subgraphs centered at each node, computes their $\\delta_{\\mathcal{G}_{i}}$ as the hierarchical level of the central node $i$ , and obtains teacher models\u2019 middle layer representations of the $i$ -th node in the $l$ -th layer based on $\\delta_{\\mathcal{G}_{i}}$ as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pmb{u}_{T,i}^{(l)}=\\pmb{u}_{T,i}^{(l),\\mathbb{B}}\\cdot\\mathbb{I}(\\delta\\pmb{\\mathscr{g}}_{i}<\\lambda)+\\pmb{u}_{T,i}^{(l),\\mathbb{E}}\\cdot\\mathbb{I}(\\delta\\pmb{\\mathscr{g}}_{i}\\geq\\lambda)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where I denotes indicator function, the threshold $\\lambda$ is a hyperparameter that is typically set to a smaller value on graphs with higher $\\delta$ -hyperbolicity values to achieve better performance. ", "page_idx": 5}, {"type": "text", "text": "In our method, embeddings of l-th guided middle layers of student model is Z(Sl),E, a nd applying Eq. (9) likewise yields the student model $i$ -th local structure representation $u_{S,i}^{(l)}~=~$ $\\{u_{S,i1}^{(l)},u_{S,i2}^{(l)},...,u_{S,i N}^{(l)}\\}$ . For node $i$ , the similarity between the local structures of the teacher model and the student model is measured as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{P}_{i}^{(l)}=D_{K L}\\left(\\pmb{u}_{S,i}^{(l)}||\\pmb{u}_{T,i}^{(l)}\\right)=\\sum_{j:(j,i)\\in\\mathcal{E}}u_{S,i j}^{(l)}\\log\\left(\\frac{u_{S,i j}^{(l)}}{u_{T,i j}^{(l)}}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathrm{D_{KL}}$ represents the Kullback-Leibler divergence. ", "page_idx": 5}, {"type": "text", "text": "SWKT minimizes the local structure similarity $\\mathcal{P}$ to transfer knowledge from hint embeddings in different geometry to student model. Thus, the loss function for the SWKT module is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{S W K T}=\\frac{1}{L}\\frac{1}{N}\\sum_{l=1}^{L}\\sum_{i=1}^{N}\\mathcal{P}_{i}^{(l)},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $L$ denotes the total number of intermediate layers used for distillation. ", "page_idx": 5}, {"type": "text", "text": "4.4 Geometric Embedding Optimization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Trustworthiness: Simply concatenating embeddings from Euclidean and hyperbolic teacher models to teach student model can lead to confusion due to geometric inconsistencies. This confusion may result in the student model performing worse than when learning from a single geometric teacher model, as shown in section 5.2. To mitigate the negative effects caused by this inconsistency, we propose a Geometric Embedding Optimization module (GEO) to optimize cross-geometric space. ", "page_idx": 5}, {"type": "text", "text": "Specifically, for a given node $i$ from layer $l$ , we have its local geometric information $\\delta_{\\mathcal{G}_{i}}$ and teacher embeddings from different geometric spaces. We obtain an initial fused feature as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\ne_{T,i}^{(l)}=\\frac{1}{1+\\exp(-(\\delta_{\\mathcal{G}_{i}}-\\lambda))}\\cdot z_{T,i}^{(l),\\mathbb{E}}+\\frac{\\exp(-(\\delta_{\\mathcal{G}_{i}}-\\lambda))}{1+\\exp(-(\\delta_{\\mathcal{G}_{i}}-\\lambda))}\\cdot z_{T,i}^{(l),\\mathbb{B}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\lambda$ has the same meaning as Eq. (10). ", "page_idx": 5}, {"type": "text", "text": "Next, we use a single-layer GCN (which can be replaced by other sufficiently capable networks, such as a Multi-Layer Perceptron (MLP)) to optimize the initially fused features $e_{T,i}^{(l)}$ . The optimization network should select loss functions based on the specific downstream task. In this study, we adopt the triplet loss function [41] , which enlarges the distance between different-class nodes and reduces the distance between same-class nodes, to enhance node classification and link prediction performance. To apply triplet loss to graph data, we organize triplets as follows: Given the hint embeddings from teacher, we sample extensive sets of nodes, where each set includes an anchor node, a positive node with the same label as the anchor, and a negative node with a different label. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Assuming $\\mathcal{F}_{e}$ is corresponding function of a pre-trained GEO network with weight matrix ${\\mathbf W}_{e}$ , the elements of the local structure vector u(Tl,)i for the $i$ -th node of layer $l$ can be represented as ", "page_idx": 6}, {"type": "equation", "text": "$$\nu_{T,i j}^{(l)}=\\mathscr{F}_{s u b}\\big(\\mathscr{F}_{e}(e_{T,i}^{(l)};\\mathbf{W}_{e}),\\mathscr{F}_{e}(e_{T,j}^{(l)};\\mathbf{W}_{e})\\big),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathcal{F}_{s u b}$ has the same meaning as Eq. (9). ", "page_idx": 6}, {"type": "text", "text": "Subsequently, we can reference Eq. (11) to compute the structural similarity between the outputs of GEO and the guided embeddings of $l$ -th layer of the student model as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{G E O}=\\frac{1}{N}\\sum_{i=1}^{N}D_{K L}(\\pmb{u}_{T,i}^{(l)}||\\pmb{u}_{S,i}^{(l)}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "4.5 Distillation Framework ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In our proposed graph KD approach, the teacher model\u2019s early $L-1$ layers guide the student model using the SWKT module, while the $L$ -th layer guides via the GEO module. Additionally, the student model also learns the logits distribution of teacher models, i.e., outputs of GEO in last layer. Given the logits ${\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}\\,{\\mathbf{}}\\,$ from teacher models and the predicted logits $\\pmb{y}_{S}$ from student model, our overall KD loss is as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathcal{L}_{S W K T}+\\mathcal{L}_{C E}(\\pmb{y}_{T},\\pmb{y}_{S})+\\beta\\mathcal{L}_{G E O},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathcal{L}_{C E}$ denotes the cross-entropy loss function, $\\beta$ is a weight coefficient. ", "page_idx": 6}, {"type": "text", "text": "The space complexity is $O(N D+|E|+R N H+k N|E|)$ , where $N$ is the number of nodes, $D$ is the feature dimension, $|E|$ is the number of edges, $R$ is the number of teacher models, and $k$ is the $k$ -pop parameter. For time complexity, forward propagation has a complexity of $O(N H^{2}+|E|H)$ , local subgraph generation is $O(k N|E|)$ , the Structured-Wise Knowledge Transfer module is $O(k N H)$ , similarity computation is $O(k N)$ , and the Geometric Embedding Optimization module contributes $O(N H^{2})$ . Thus, the overall time complexity is $O(N H^{2}+|E|H+\\dot{k}N(|E|+H))$ . ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we first give the experimental setup and baselines. Then we compare our graph KD framework with some baselines on NC and LP tasks. Hyperparameters and ablation analysis also be given. Further experimental results can be found in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "5.1 Experimental Settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Setups. We preform NC and LP tasks on citation network datasets Cora [42],Citeseer [43] and Pubmed [44], wikipedia-based article hyperlink network dataset Wiki-CS [45], and Physics part of the Coauthor dataset Co-Physics [46]. The student and teacher models are both GCN composed of two hidden layers and one output layer. The hidden layer node dimensions are 8 for the student and 128 for teachers. The model parameters are uniformly initialized using the Xavier\u2019s uniform initialization [47] method The optimizer uses Adam [48] or Riemannian Adam [49]. We set the value of $k$ for $k$ -hops subgraphs to 4. To mitigate errors caused by randomness, each F1-score and ROC AUC is the average of 10 experiments with different random seed values. ", "page_idx": 6}, {"type": "text", "text": "Baselines. To evaluate the performance of our method, we compare it with KD methods formulated in single geometry, including the following methods. FitNet [34] utilizes a regressor to align the intermediate features of the student model with those of the teacher model, quantifying the feature discrepancy using L2 distance. AT [35] averages attention maps from both teacher and student models\u2019 intermediate hidden layers, quantifying differences between them using a designed loss function. LSP [36] extracts local structures from both teacher and student models\u2019 intermediate feature maps and measures their difference using KL divergence. MSKD [37] Utilizes diverse teacher models with varying layers to guide the student model in capturing topology at different scales. VQG [38] incorporates a VQ-VAE to learn a codebook that represents informative local structures, and uses these local structures as additional information for distillation. To comprehensively demonstrate the superiority of cross-geometry over individual geometry, we conducted experiments for each method separately in Euclidean space $\\mathbb{E}$ , hyperbolic space $\\mathbb{B}$ , and spherical space S. Here, we adopt a spherical space $\\mathbb{S}$ with curvature of 1, for further details, please refer to the Appendix A.4. We further conducted exploratory experiments on alternative geometric integration approaches based on that proposed in section 4, as illustrated by Cross in Table 1. ", "page_idx": 6}, {"type": "table", "img_path": "347aDObXEa/tmp/5f3817c1b744f68311fbe04248e0ab5367397d8b9a49557a1f9bccdfe291277f.jpg", "table_caption": ["Table 1: F1 scores $(\\%)\\uparrow$ and ROC $\\mathrm{AUC}(\\%)\\uparrow$ of student model distilled by all KD methods for NC and LP tasks. E, $\\mathbb{B}$ , $\\mathbb{S}$ respectively denote the method being in Euclidean, hyperbolic, and spherical spaces, with multiple symbols representing cross geometric space. $\\delta$ represents the global hyperbolicity. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.2 Node Classification ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We use F1 scores as the evaluation metric for node classification task and present results in Table 1. In comparing LSPs across different geometries, we found a counterintuitive outcome. In datasets with low $\\delta\\!\\cdot$ -hyperbolicity, hyperbolic LSP was anticipated to outperform Euclidean LSP. However, it performed worse, dropping by $3.05\\%$ (Wiki-CS). This suggests that even if a teacher model excels in one geometry, its guidance may be less effective when the student model operates in a different geometry. This highlights a significant gap between hint embeddings in different geometries. Despite employing diverse geometries, our method obtains $1.11\\%$ average improvement over SOTA baselines and especially $2.66\\%$ and $1.37\\%$ on Cora and Wiki-CS, indicating that our method excels on datasets with lower $\\delta$ -hyperbolicity. Besides, even in graph with high $\\delta$ -hyperbolicity, where graph exhibit few hierarchical levels, our method consistently achieves higher F1-score compared to student models obtained by baseline KD methods in a single geometrc space. ", "page_idx": 7}, {"type": "text", "text": "With the rapid growth of information, real-world graph data is expanding in scale. To demonstrate the effectiveness of our method on large-scale graphs, we evaluated the F1 scores of NC tasks using distilled student models on larger datasets, Ogbn-Arxiv (1,166,243 edges, 169,343 nodes) and Ogbn-Proteins (39,561,252 edges, 132,534 nodes) [50]. Results in Table 2 show that our method consistently achieves superior distillation performance on these larger datasets. ", "page_idx": 7}, {"type": "text", "text": "5.3 Link Prediction ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We use ROC AUC as the evaluation metric for link prediction task and present results in Table 1. The average ROC AUC showed an improvement of $1.58\\%$ , with particularly notable enhancements on datasets Wiki-CS and Cora, reaching $1.87\\%$ and $2.57\\%$ , respectively. Employing KD methods solely based on hyperbolic geometry outperformed those exclusively utilizing Euclidean geometry, particularly on the Citeseer. Our cross-geometry KD method outperformed SOTA baselines by ", "page_idx": 7}, {"type": "image", "img_path": "347aDObXEa/tmp/9f678a5e1dfde50546054e40d7de36b3cfcf754fd248f2a9ab3a82d5084ead1f.jpg", "img_caption": ["Figure 3: $\\delta_{\\mathcal{G}_{i}}$ distribution of subgraphs (left), hyperparameters sensitivity analysis (middle), comparison of convergence rates (right) on the Cora (row 1) and Wiki-CS (row 2) "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "$0.93\\%$ on average, underscoring the efficacy of geometry-specific methods in cross-geometry learning for overall performance enhancement. ", "page_idx": 8}, {"type": "text", "text": "5.4 Hyperparameters Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "$k$ of Subgraphs. A small radius $k$ limits local hierarchical assessment, while a large $k$ increases computational complexity. We explored $\\delta_{i}$ value distributions for local geometric properties across $k$ values (1 to 7) on Wiki-CS and Cora datasets, shown in Figure 3 (left). Stability in distributions occurs at $k\\geq4$ , suggesting sufficient capture of geometric characteristics in subgraphs of this size. Thus, we set $k=4$ for our experiments. ", "page_idx": 8}, {"type": "text", "text": "We varied the hyperparameters on the Wiki-CS and Cora datasets to test the F1 scores for the NC task, with $\\lambda\\in\\{0.0,0.5,1.0,1.5,2.0,2.5\\}$ , $\\beta\\,\\in\\,\\{1,2,3,4,5,10\\}$ . Here, $\\lambda$ denotes the threshold value in Eq. (10) and Eq. (13). A larger $\\lambda$ leads to more local subgraphs being embedded in hyperbolic space, while a smaller $\\lambda$ results in more subgraphs being embedded in Euclidean space. $\\beta$ denotes the weight of the GEO module. The results from Figure 3 (middle) indicate that the hyperparameters $\\lambda$ and $\\beta$ have a generally minimal impact on the outcomes. The combination of $\\lambda=1.5$ and $\\beta=3.0$ maintains optimal performance. ", "page_idx": 8}, {"type": "text", "text": "5.5 Distillation Efficiency ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To evaluate the convergence efficiency of our proposed KD method, we have recorded the F1-scores trends for student models guided by all KD methods during training epochs on the Cora and Wiki-CS datasets in Figure 3 (right). As illustrated, our KD method consistently outperform other methods within the same training epochs. Specifically, our KD method achieves state-of-the-art (SOTA) performance before reaching 500 epochs, while the others are still undergoing training. These results serve to validate the effectiveness and efficiency of our method. Due to their simpler architecture, student models generally have faster inference speeds compared to teacher models. The speedup achieved by student models relative to teacher models is also an indicator of the efficiency of distillation methods. The inference times (in milliseconds) of both teacher and student models, measured on our device, are shown in Table 3. Results demonstrate that our method achieves an average speed-up of approximately 232x. ", "page_idx": 8}, {"type": "text", "text": "In addition, we evaluated the training time for each method, the inference time for the corresponding student models, and calculated the compression ratio of student model size relative to the teacher model. The results can be found in Appendix C. ", "page_idx": 8}, {"type": "text", "text": "5.6 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To further validate the efficacy of cross-geometric learning and the two proposed modules, we conducted additional experiments by adapting our method to operate on a single Euclidean or ", "page_idx": 8}, {"type": "text", "text": "Table 2: F1 scores $(\\%)\\uparrow$ of student model distilled by all KD methods for NC on Arxiv and Proteins. ", "page_idx": 9}, {"type": "table", "img_path": "347aDObXEa/tmp/c2931971b076f3977da0f0f6eee73432849119e96fc84193e88bbe051fec72bf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "347aDObXEa/tmp/955c83739478ba741e59f13951fae9a36967b5f7598d0244076f92906d0c9a3b.jpg", "table_caption": ["Table 3: Speed-up comparison. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "347aDObXEa/tmp/ec40414c1be5e9a77a2537db491bf8daf0a35dc12ff284e78293a39a0cd3f343.jpg", "table_caption": ["Table 4: Ablation study results. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "hyperbolic geometry. Additionally, we selectively excluded the SWKT and GEO modules. These ablation experiments were conducted on the Wiki-CS dataset, and the results are presented in Table 4. We have the following observations: ", "page_idx": 9}, {"type": "text", "text": "\u2022 Using only a single geometry, even with the GEO module optimizing embeddings, the enhancement compared to the baseline is minimal. \u2022 The cross-geometric approach consistently outperforms the single-geometric methods. Whether excluding SWKT or GEO, the results are inferior to the comprehensive method, indicating their crucial roles in optimizing geometric embedding distribution. ", "page_idx": 9}, {"type": "text", "text": "The overall results of ablation analysis further demonstrate the importance of cross-geometry learning and our proposed two modules. ", "page_idx": 9}, {"type": "text", "text": "To demonstrate the model-agnostic nature of our framework, we alter the architecture and the number of layers $L$ in the teacher model. Due to page limitations, we only present key results above. For more details on the dataset, experimental setup, and results, please refer to Appendix B and C. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Hyperbolic geometry has shown expressive non-Euclidean modeling in the graph community. Noteworthy models, such as Poincar\u00e9 and Lorentz models, facilitate vector projections between Euclidean and hyperbolic neurons. Our investigation reveals that tree-like or power-law distributed graphs exhibit multiple different hierarchical within locally connected structures. Consequently, training across Euclidean and hyperbolic geometry intuitively emerges as a more flexible approach to graph modeling, yielding significant enhancements in KD tasks. To this end, we introduce a novel KD framework that models the hint embeddings of the teacher models across diverse geometries. By leveraging $\\delta$ -hyperbolicity, we transfer local subgraphs information to the student model. Our analysis and experiments provide positive support for this innovative perspective on geometry modeling. ", "page_idx": 9}, {"type": "text", "text": "Limitations. Despite the performance improvements achieved by cross-geometry learning across various tasks, it presents some potential issues. For instance, integrating different geometric information introduces hyperparameters, making the task outcomes somewhat dependent on their selection, thus affecting the method\u2019s stability. Additionally, the distillation phase demands more complex pre-trained models, increasing resource and time requirements. These limitations are critical areas for future enhancement in cross-geometry learning. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledge ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by National Natural Science Foundation of China, Grant Number: 62476109, 62206108, 62176184, and the Natural Science Foundation of Jilin Province, Grant Number: 20240101373JC, and Jilin Province Budgetary Capital Construction Fund Plan, Grant Number: 2024C008-5, and Research Project of Jilin Provincial Education Department, Grant Number: JJKH20241285KJ. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Nikhil Mehta, Mar\u00eda Leonor Pacheco, and Dan Goldwasser. Tackling fake news detection by continually improving social context representations using graph neural networks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1363\u20131380, 2022.   \n[2] Hongcai Xu, Junpeng Bao, and Wenbo Liu. Double-branch multi-attention based graph neural network for knowledge graph completion. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15257\u201315271, 2023.   \n[3] Liang Yao, Chengsheng Mao, and Yuan Luo. Graph convolutional networks for text classification. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 7370\u20137377, 2019.   \n[4] Feiyu Chen, Jie Shao, Shuyuan Zhu, and Heng Tao Shen. Multivariate, multi-frequency and multimodal: Rethinking graph neural networks for emotion recognition in conversation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10761\u201310770, 2023.   \n[5] Amit Aflalo, Shai Bagon, Tamar Kashti, and Yonina Eldar. Deepcut: Unsupervised segmentation using graph neural networks clustering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 32\u201341, 2023.   \n[6] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. Graph neural networks for social recommendation. In The world wide web conference, pages 417\u2013426, 2019.   \n[7] Mengru Chen, Chao Huang, Lianghao Xia, Wei Wei, Yong Xu, and Ronghua Luo. Heterogeneous graph contrastive learning for recommendation. In Proceedings of the sixteenth ACM international conference on web search and data mining, pages 544\u2013552, 2023.   \n[8] Lilapati Waikhom and Ripon Patgiri. A survey of graph neural networks in various learning paradigms: methods, applications, and challenges. Artificial Intelligence Review, 56(7):6295\u2013 6364, 2023.   \n[9] Wei Peng, Tuomas Varanka, Abdelrahman Mostafa, Henglin Shi, and Guoying Zhao. Hyperbolic deep neural networks: A survey. IEEE Transactions on pattern analysis and machine intelligence, 44(12):10023\u201310044, 2021.   \n[10] Chih Yao Chen, Tun Min Hung, Yi-Li Hsu, and Lun-Wei Ku. Label-aware hyperbolic embeddings for fine-grained emotion classification. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10947\u201310958, 2023.   \n[11] Liping Wang, Fenyu Hu, Shu Wu, and Liang Wang. Fully hyperbolic graph convolution network for recommendation. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, pages 3483\u20133487, 2021.   \n[12] Jianing Sun, Zhaoyue Cheng, Saba Zuberi, Felipe P\u00e9rez, and Maksims Volkovs. Hgcf: Hyperbolic graph convolution networks for collaborative filtering. In Proceedings of the Web Conference 2021, pages 593\u2013601, 2021.   \n[13] Ines Chami, Zhitao Ying, Christopher R\u00e9, and Jure Leskovec. Hyperbolic graph convolutional neural networks. Advances in neural information processing systems, 32, 2019.   \n[14] Qi Liu, Maximilian Nickel, and Douwe Kiela. Hyperbolic graph neural networks. Advances in neural information processing systems, 32, 2019.   \n[15] Weize Chen, Xu Han, Yankai Lin, Hexu Zhao, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. Fully hyperbolic neural networks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5672\u20135686, 2022.   \n[16] Yiding Zhang, Xiao Wang, Chuan Shi, Nian Liu, and Guojie Song. Lorentzian graph convolutional networks. In Proceedings of the Web Conference 2021, pages 1249\u20131261, 2021.   \n[17] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18\u201342, 2017.   \n[18] Erzs\u00e9bet Ravasz and Albert-L\u00e1szl\u00f3 Barab\u00e1si. Hierarchical organization in complex networks. Physical review E, 67(2):026112, 2003.   \n[19] Kexin Huang and Marinka Zitnik. Graph meta learning via local subgraphs. Advances in neural information processing systems, 33:5862\u20135874, 2020.   \n[20] Aaron B Adcock, Blair D Sullivan, and Michael W Mahoney. Tree-like structure in large social and information networks. In 2013 IEEE 13th international conference on data mining, pages 1\u201310. IEEE, 2013.   \n[21] Octavian Ganea, Gary B\u00e9cigneul, and Thomas Hofmann. Hyperbolic neural networks. Advances in neural information processing systems, 31, 2018.   \n[22] Max Kochurov, Sergey Ivanov, and Eugeny Burnaev. Are hyperbolic representations in graphs created equal? arXiv preprint arXiv:2007.07698, 2020.   \n[23] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2016.   \n[24] Franco Manessi, Alessandro Rozza, and Mario Manzo. Dynamic graph convolutional networks. Pattern Recognition, 97:107000, 2020.   \n[25] Yiding Yang, Zunlei Feng, Mingli Song, and Xinchao Wang. Factorizable graph convolutional networks. Advances in Neural Information Processing Systems, 33:20286\u201320296, 2020.   \n[26] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Advances in neural information processing systems, 30, 2017.   \n[27] Petar Velic\u02c7kovic\u00b4, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018.   \n[28] Huiting Hong, Hantao Guo, Yucheng Lin, Xiaoqing Yang, Zang Li, and Jieping Ye. An attention-based graph neural network for heterogeneous structural learning. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 4132\u20134139, 2020.   \n[29] Yao Ding, Zhili Zhang, Xiaofeng Zhao, Danfeng Hong, Wei Cai, Nengjun Yang, and Bei Wang. Multi-scale receptive fields: Graph attention neural network for hyperspectral image classification. Expert Systems with Applications, 223:119858, 2023.   \n[30] Yiding Zhang, Xiao Wang, Chuan Shi, Xunqiang Jiang, and Yanfang Ye. Hyperbolic graph attention network. IEEE Transactions on Big Data, 8(6):1690\u20131701, 2021.   \n[31] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.   \n[32] Xiaofeng Cao and Ivor W Tsang. Distribution matching for machine teaching. IEEE Transactions on Neural Networks and Learning Systems, 2023.   \n[33] Chen Zhang, Xiaofeng Cao, Weiyang Liu, Ivor Tsang, and James Kwok. Nonparametric iterative machine teaching. In International Conference on Machine Learning, pages 40851\u201340870. PMLR, 2023.   \n[34] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.   \n[35] Nikos Komodakis and Sergey Zagoruyko. Paying more attention to attention: improving the performance of convolutional neural networks via attention transfer. In ICLR, 2017.   \n[36] Yiding Yang, Jiayan Qiu, Mingli Song, Dacheng Tao, and Xinchao Wang. Distilling knowledge from graph convolutional networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7074\u20137083, 2020.   \n[37] Chunhai Zhang, Jie Liu, Kai Dang, and Wenzheng Zhang. Multi-scale distillation from multiple graph neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 4337\u20134344, 2022.   \n[38] Ling Yang, Ye Tian, Minkai Xu, Zhongyi Liu, Shenda Hong, Wei Qu, Wentao Zhang, Bin CUI, Muhan Zhang, and Jure Leskovec. VQGraph: Rethinking graph representation space for bridging GNNs and MLPs. In The Twelfth International Conference on Learning Representations, 2024.   \n[39] Eugenio Beltrami. Teoria fondamentale degli spazii di curvatura costante memoria. F. Zanetti, 1868.   \n[40] James W Cannon, William J Floyd, Richard Kenyon, Walter R Parry, et al. Hyperbolic geometry. Flavors of geometry, 31(59-115):2, 1997.   \n[41] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 815\u2013823, 2015.   \n[42] Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automating the construction of internet portals with machine learning. Information Retrieval, 3:127\u2013163, 2000.   \n[43] C Lee Giles, Kurt D Bollacker, and Steve Lawrence. Citeseer: An automatic citation indexing system. In Proceedings of the third ACM conference on Digital libraries, pages 89\u201398, 1998.   \n[44] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina EliassiRad. Collective classification in network data. AI magazine, 29(3):93\u201393, 2008.   \n[45] P\u00e9ter Mernyei and Ca\u02d8ta\u02d8lina Cangea. Wiki-cs: A wikipedia-based benchmark for graph neural networks. arXiv preprint arXiv:2007.02901, 2020.   \n[46] Kuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-Han Wu, Yuxiao Dong, and Anshul Kanakia. Microsoft academic graph: When experts are not enough. Quantitative Science Studies, 1(1):396\u2013413, 2020.   \n[47] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249\u2013256. JMLR Workshop and Conference Proceedings, 2010.   \n[48] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[49] Gary Becigneul and Octavian-Eugen Ganea. Riemannian adaptive optimization methods. In International Conference on Learning Representations, 2018.   \n[50] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118\u201322133, 2020.   \n[51] Xiaofeng Cao and Ivor W Tsang. Distribution disagreement via lorentzian focal representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(10):6872\u20136889, 2021.   \n[52] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J Kim. Graph transformer networks. Advances in neural information processing systems, 32, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "image", "img_path": "347aDObXEa/tmp/c57d44944cd85258e4e619a8b878f7b433e01ad5052116222d789d20f6a0c442.jpg", "img_caption": ["Figure 4: Spaces with different curvatures. $(a)$ Spherical space with curvature $c=1.0$ . $(b)$ Euclidean space. (c) Hyperbolic space with curvature $c=-0.5.$ . $(d)$ Hyperbolic space with curvature $c=-1$ , which have a faster grow rate of volume. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "A Additional Theoretical Support ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Gromov Hyperbolicity ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Gromov\u2019s $\\delta$ -hyperbolicity [20] measures a graph\u2019s tree-like structure, with lower $\\delta$ values indicating higher hyperbolicity in a graph dataset, where $\\delta=0$ represents a tree. In this paper, we compute the hyperbolicity of the $k$ -hop subgraph $\\mathcal{G}_{i}$ for each node $i$ as the geometric feature information of the local structure of that node. Here, we provide the detailed calculation process. ", "page_idx": 13}, {"type": "text", "text": "First, four nodes $a,b,c,d$ are randomly sampled from the subgraph $\\mathcal{G}_{i}$ . Let $S_{1},S_{2}$ and $S_{3}$ be defined as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S_{1}=d i s t(a,b)+d i s t(c,d),}\\\\ {S_{2}=d i s t(a,c)+d i s t(b,d),}\\\\ {S_{3}=d i s t(a,d)+d i s t(b,c),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where dist denotes shortest path length between two nodes. ", "page_idx": 13}, {"type": "text", "text": "Let $M_{1}$ and $M_{2}$ be the two largest values among $S_{1},S_{2}$ and $S_{3}$ . We define ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h y p(a,b,c,d)=M_{1}-M_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The hyperbolicity $\\delta_{\\mathcal{G}_{i}}$ of the graph $\\mathcal{G}_{i}$ is the maximum of hyp over all possible 4-tuples $(a,b,c,d)$ divided by 2, i.e., ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\delta(G)=\\operatorname*{max}_{a,b,c,d}\\frac{h y p(a,b,c,d)}{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In our paper, we calculate the $\\delta_{\\mathcal{G}_{i}}$ for each $\\mathbf{k}$ -hop subgraphs. For subgraphs with fewer than four nodes, we label their $\\delta_{\\mathcal{G}_{i}}$ value as $N/A$ . ", "page_idx": 13}, {"type": "text", "text": "A.2 Euclidean Geometry ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Geometry is a branch of mathematics concerned with properties of space such as the distance, shape, size and relative position of figures. In this paper, we analyze the modeling capabilities of graph neural networks in Euclidean , hyperbolic and spherical geometries. Euclidean geometry studies the properties of flat space with zero curvature, where parallel lines never meet, and angles of a triangle sum to 180 degrees. In Euclidean geometry, the volume of space exhibits polynomial growth associated with the dimensionality of the space. The majority of neural network models perform inference operations in this space, where operations such as convolution, pooling, and activation are based on the basic arithmetic operations of addition, subtraction, multiplication, and division. ", "page_idx": 13}, {"type": "text", "text": "Euclidean geometry is an axiomatic system, in which all theorems are derived from a small number of simple axioms. ", "page_idx": 13}, {"type": "text", "text": "\u2022 To draw a straight line from any point to any point.   \n\u2022 To produce (extend) a finite straight line continuously in a straight line.   \n\u2022 To describe a circle with any centre and distance (radius).   \n\u2022 That all right angles are equal to one another.   \n\u2022 [The parallel postulate]: That, if a straight line falling on two straight lines make the interior angles on the same side less than two right angles, the two straight lines, if produced indefinitely, meet on that side on which the angles are less than two right angles. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "These axioms provide the fundamental mathematical framework for Euclidean space, allowing GNN models to incorporate information about the absolute positions of nodes, properties of lines, and spatial relationships. ", "page_idx": 14}, {"type": "text", "text": "A.3 Hyperbolic Geometry ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Hyperbolic geometry is non-Euclidean geometry, also called Lobachevsky-Bolyai-Gauss geometry. This geometry adheres to all of Euclid\u2019s postulates, with the exception of the parallel postulate, which has been substituted with: ", "page_idx": 14}, {"type": "text", "text": "\u2022 If a straight line intersects two other straight lines, and so makes the two interior angles on one side of it together less than two right angles, then the other straight lines will meet at a point if extended far enough on the side on which the angles are less than two right angles. ", "page_idx": 14}, {"type": "text", "text": "Hyperbolic space is a homogeneous space with constant negative curvature. In Euclidean space, the curvature is zero, while in hyperbolic space, the curvature is a negative constant. Moreover, smaller curvature leads to faster volume growth, as illustrated in Figure 4. The hyperbolic space can be modelled using five isomorphic models which are the Lorentz model [51], the Poincar\u00e9 ball model and Poincar\u00e9 half space model, and the Klein model. In this paper, we utilize a hyperbolic geometric teacher model based on the Poincar\u00e9 model. The Poincar\u00e9 model $\\mathbb{B}$ is a manifold equipped with a Riemannian metric $g^{B}$ . This metric is conformal to the Euclidean metric $g^{B}$ . Formally, an $n$ dimensional Poincar\u00e9 unit ball (manifold) is defined as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{B}^{n}=\\{x\\in\\mathbb{R}^{n}:\\|x\\|<1\\},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\|\\cdot\\|$ denotes the Euclidean norm. Formally, the distance between $x,y\\in\\mathbb{B}^{n}$ is defined as: ", "page_idx": 14}, {"type": "equation", "text": "$$\nd(x,y)=a r c o s h(1+2\\frac{\\|x-y\\|^{2}}{(1-\\|x\\|^{2})(1-\\|y\\|^{2})}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The M\u00f6bius addition $\\oplus$ for $x$ and $y$ in $\\mathbb{B}^{n}$ is defined as ", "page_idx": 14}, {"type": "equation", "text": "$$\nx\\oplus y={\\frac{\\left(1+2\\langle x,y\\rangle+\\|y\\|^{2}\\right)x+\\left(1-\\|x\\|^{2}\\right)y}{1+2\\langle x,y\\rangle+\\|x\\|^{2}\\|y\\|^{2}}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The M\u00f6bius scalar multiplication $\\otimes$ is defined as ", "text_level": 1, "page_idx": 14}, {"type": "equation", "text": "$$\nr\\otimes x=\\left\\{\\begin{array}{c c}{\\operatorname{tanh}\\Big(r\\operatorname{artanh}(\\|x\\|)\\frac{x}{\\|x\\|},}&{\\ x\\in\\mathbb{B}^{n}}\\\\ {0,}&{\\ x=0,}\\end{array}\\right.,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $r$ is a scalar factor. ", "page_idx": 14}, {"type": "text", "text": "The M\u00f6bius vector multiplication $M^{\\otimes}(x)$ is defined as ", "page_idx": 14}, {"type": "equation", "text": "$$\nM^{\\otimes}(x)=\\operatorname{tanh}\\left({\\frac{\\|M x\\|}{\\|x\\|}}\\operatorname{actanh}(\\|x\\|)\\right){\\frac{M x}{\\|M x\\|}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.4 Spherical Geometry ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Spherical geometry studies the properties of curved space with constant positive curvature, where the angles of a triangle add up to exceeds 180 degrees. All lines in spherical geometry intersect, as there are no parallel lines on a sphere. In the field of graph embedding, spherical geometry plays a significant role as it provides a more realistic model, particularly applicable in geographic information systems and computer graphics. Through spherical geometry, we can accurately describe features on the surface of the Earth and construct data representations with spherical topological structures in ", "page_idx": 14}, {"type": "text", "text": "Algorithm 1 Cross-Geometric Graph KD ", "page_idx": 15}, {"type": "table", "img_path": "347aDObXEa/tmp/84a0b13e4f40b83aaeaa17df921dd9fcddde64ba77d206a7029a279b8baa524f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "347aDObXEa/tmp/110c6f77f024da593de46bc0e6759c7a1325c3fd06f2283702262747957d3d62.jpg", "table_caption": ["Table 5: Statistics of datasets. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "three-dimensional space, which is crucial for applications such as map-making, virtual reality, and computer games. In graph embedding, concepts and algorithms of spherical geometry are utilized to process data with spherical topological structures, such as mapping the Earth\u2019s surface onto a twodimensional plane while preserving the correctness of geographic locations and spatial relationships. Therefore, spherical geometry is not only a theoretical discipline but also an indispensable tool in practical applications. ", "page_idx": 15}, {"type": "text", "text": "B Experiment Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Algorithm ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Given the graph data, we initially train teacher models in Euclidean, hyperbolic, and spherical spaces, respectively. Subsequently, we train the MLP model of the GEO module, initializing the student model randomly. By inputting learning parameters alongside hyperparameters $\\lambda$ and $\\beta$ , we employ Algorithm 1 to obtain the distilled student model. ", "page_idx": 15}, {"type": "text", "text": "B.2 Datasets ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here, we present detailed information for each dataset in Table 5. Wiki-CS consists of 11,701 nodes with 431,726 edges, each node characterized by a 300-dimensional feature, and the node labels are categorized into 10 classes. Cora consists of 1,044 nodes with 10,556 edges, each node characterized by a 1,433-dimensional feature, and the node labels are categorized into 7 classes. Pubmed consists of 19,717 nodes with 88,651 edges, each node characterized by a 500-dimensional feature, and the node labels are categorized into 3 classes. Co-Physics consists of 34,493 nodes with 495,924 edges, each node characterized by a 8,415-dimensional feature, and the node labels are categorized into 5 classes. Citeseer consists of 3,327 nodes with 9,928 edges, each node characterized by a 3,703-dimensional feature, and the node labels are categorized into 6 classes. To ensure fairness, we uniformly apply standard splits $(70\\%/15\\%/15\\%)$ for node classification tasks and standard splits $(85\\%/5\\%/10\\%)$ ) for link prediction tasks. ", "page_idx": 15}, {"type": "table", "img_path": "347aDObXEa/tmp/7194b291294d8ca9fa7c99d64bc77beae142d771bdf6d1039649ed21ab8b5052.jpg", "table_caption": ["Table 6: Parameter Settings in NC task. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "347aDObXEa/tmp/6ad32cf2f1d4b106b1d97b7493f31e0f7a70a325ba2291b13b02ce6fe186aa9a.jpg", "table_caption": ["Table 7: Parameter Settings in LP task. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.3 Setups ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For a fair comparison, all methods employ identical teacher and student model architectures on the same dataset. All methods use GCN as the Euclidean teacher model and HGCN as the hyperbolic teacher model. The teacher models consist of two hidden layers and one output layer, with a hidden feature dimension of 128. The student GCN model has two hidden layers and one output layer, with a hidden dimension of 8. During training, the optimizer uses Adam or Riemannian Adam, and hyperparameters such as learning rate, weight decay, and hierarchy threshold are fine-tuned based on the performance of student models on validation sets of different datasets, maintaining consistent hyperparameters for different methods on the same dataset. The parameter configurations for NC are detailed in Table 6, while those for LP are delineated in Table 7. The model parameters are uniformly initialized using the Xavier\u2019s uniform initialization method, with a random seed chosen from the range of 0 to 1000. The geo model is trained for 300 epochs, and random sampling during its optimization process involves extracting 100 sets of node pairs for each class. ", "page_idx": 16}, {"type": "text", "text": "Environments. The running environment includes an Intel Core Intel i7-13700KF CPU with a clock speed of 3.40GHz, boasting 16 cores and 24 threads. A robust NVIDIA GeForce RTX 4070Ti GPU, featuring 12GB of VRAM, encompasses 7680 CUDA cores. The system is equipped with 16GB of RAM. The operating system is Windows 11, and Python 3.10 serves as the programming language. For deep learning tasks, PyTorch version 1.13 is employed, while CUDA version 12.2 enhances GPU acceleration. Package management is facilitated through the use of Anaconda. For large datasets, Pubmed and CoauthorPhysics, experiments were conducted on a high-performance server with the following specifications: 4 Intel Xeon Gold 5220 CPUs running at 2.20GHz, equipped with 72 cores and 144 threads. The system features 4 Quadro RTX 6000 GPUs, each boasting 24GB of VRAM and 4608 CUDA cores. The system boasts 500GB of RAM and runs on Ubuntu 18.04.6. ", "page_idx": 16}, {"type": "table", "img_path": "347aDObXEa/tmp/b8f4cbb8dc5b715766011f48fddfc80be7e0fe1e3093e94ca3b23d4140dfd13a.jpg", "table_caption": ["Table 8: F1 cores $(\\%)\\uparrow$ of student models distilled from GAT teacher models on the NC Task. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "347aDObXEa/tmp/e2c22af22874d97a25d199f53e354e998c3dd4474f0db58c14a8461f40948bb0.jpg", "table_caption": ["Table 9: F1 Scores $(\\%)\\uparrow$ of student models distilled from GTN teacher models on the NC Task "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "C More Experiment Results and Analysis ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 Replacing Teacher Models ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Our proposed framework is model-agnostic. To validate its universality and effectiveness, we conducted experiments by replacing the teacher model from GCN to GAT. These experiments were conducted across three geometries: Euclidean, hyperbolic, and spherical, for cross-geometry learning. The experimental results are presented in Table 8. As depicted in the table, even when the teacher model is replaced with other models, our framework consistently maintains a strong distillation effect, with the combination of hyperbolic and Euclidean geometries still proving to be optimal. Moreover, as the performance of the teacher model improves, there is a corresponding enhancement in the performance of the student model. ", "page_idx": 17}, {"type": "text", "text": "We also replaced the Euclidean teacher model with the Graph Transformer Network [52]. The results are shown in Table 9. GTN teacher has 4 layers and a hidden dimension of 128. Specifically, during distillation, student model\u2019s $l$ layers match the last $l$ layers of teacher accordingly. The GTN and HGCN teacher output intermediate representations from each layer to the SWKT module for local subgraph structure extracting and selection. These distributions are then optimized by GEO module. Then, these features extracted from the optimized cross-geometric intermediate representations are transferred to students via the corresponding loss function. Additionally, traditional KD loss is computed from the logits output by both the teacher and student models. ", "page_idx": 17}, {"type": "text", "text": "C.2 Replacing Student Models ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The student model can operate in other geometric spaces. At first, we chose a Euclidean student model to combine hyperbolic accuracy benefits with Euclidean efficiency and stability. Our framework is model-agnostic, allowing replacement of the student model with other neural networks.To validate student on various geometric spaces, we tested NC F1 scores $(\\%)$ on the Cora dataset in Table 10. Euclidean and hyperbolic teachers\u2019 F1 score is $86.98\\%$ and $90.90\\%$ . ", "page_idx": 17}, {"type": "text", "text": "we conducted experiments using student models with the same architecture as the teacher models in our method. Following results are NC F1 scores $(\\%)$ on the Cora dataset. Euclidean and hyperbolic teachers\u2019 F1 score is $86.98\\%$ and $90.90\\%$ . The results are shown in Table 11 Compared to the results presented in Table 1 of our paper, student models now even outperform some teacher models, but using the same architecture as the teacher makes student models larger and slower, limiting their suitability for resource-constrained devices. ", "page_idx": 17}, {"type": "table", "img_path": "347aDObXEa/tmp/339a84b8c22195fd61c8cfdb8c7f962ca876456e695c7a8a638f7781af70b3cb.jpg", "table_caption": ["Table 10: F1 cores $(\\%)\\uparrow$ of student models in different geometry on the NC Task "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "347aDObXEa/tmp/1180df4648dc593b60b220afd78c5856c2e7c28a5de102937b7bbcff3339487d.jpg", "table_caption": ["Table 11: F1 Scores $(\\%)\\uparrow$ of student models with the same architecture as the teacher models on the NC Task. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "C.3 Changing Teacher Layers ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In addition to model-agnostic features, our framework demonstrates excellent scalability. All experiments in this study were conducted with both teacher and student models having a hidden layer depth of 2. To verify scalability, we configured four types of teacher models, varying their hidden layer depths from 1 to 4, while keeping all other settings constant. By applying the SWKT and GEO modules to each layer, we expanded our framework, as illustrated in Table 12. As observed in the table, despite variations in the number of layers in the teacher models, our framework consistently achieves effective distillation results, with the combination of hyperbolic and Euclidean components remaining optimal. Furthermore, as the performance of the teacher models improves, there is a corresponding enhancement in the performance of the student models. ", "page_idx": 18}, {"type": "text", "text": "C.4 Embeddings Optimization Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We reduced the embeddings of student models to 2-dimensional space by t-SNE and visualized them in Figure 5, our method yields a superior embedding distribution, which more suitable for NC task. ", "page_idx": 18}, {"type": "text", "text": "C.5 Hyperparameters Analysis ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We conducted a more comprehensive hyperparameter analysis on the Co-Physics, Pubmed, and CiteSeer datasets. By adjusting the hyperparameters, we evaluated the F1 scores for the NC task, with $\\lambda\\in{0.0,0.5,1.0,1.5,2.0,2.5}$ and $\\bar{\\beta}\\in{1,2,3,4,5,10}$ . The results, as shown in Figure 6, indicate that the hyperparameters $\\lambda$ and $\\beta$ have a minimal overall impact on the outcomes. The performance is generally optimal when $\\beta\\,=\\,3$ , and $\\lambda$ shows better performance at intermediate values. ", "page_idx": 18}, {"type": "text", "text": "C.6 Ablation Study ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Following the ablation experiment strategy outlined in Section 5.2, we conducted NC experiments on five other datasets. The results, presented in Table 13, reveal that the performance of the comprehensive method consistently outperforms other conditions across all datasets. ", "page_idx": 18}, {"type": "table", "img_path": "347aDObXEa/tmp/aa42c091c17ad1377c5da0f0e18fb68c0234762ad605331828ad06cba01f7922.jpg", "table_caption": ["Table 14: Training time spent per epoch (in ms) for NC task. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "347aDObXEa/tmp/cbe6a85baf9d3cf0c4e55fed1b28ed18a068274192fa5ad75c92046a6cc742fe.jpg", "table_caption": ["Table 12: F1 cores $(\\%)\\uparrow$ of student models distilled from teacher models with different layers on the NC Task. "], "table_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "347aDObXEa/tmp/38fd37e1dc8d8a8e66a823b707bad22f0b1e4f5bf6fe225a50274ebf4a1a1d9d.jpg", "img_caption": ["Figure 5: t-SNE Visualization of embeddings obtained by student models.In contrast to baselines,our method achieves embeddings that fully utilize the entire space, ensuring substantial inter-class distances and thereby enhancing node classification performance. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "C.7 Distillation Efficiency ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In dataset Wiki-CS, we assessed the training and inference time per epoch and the ratio of the total parameter count of the student model to that of the teacher model, as outlined in Table 15. Our KD method achieves similar time efficiency in both training and inference stages compared to other methods. Notably, our method achieves superior results at the highest compression level, thereby further validating the efficacy of our KD method in generating compact yet high-performing student models. ", "page_idx": 19}, {"type": "text", "text": "We provide an analysis of the time spent by each knowledge distillation (KD) method during the training of student models on the network classification (NC) task, recorded for every epoch across all datasets, as shown in Table 16. We also present the time taken for inference of the student models on the NC task in Table 14. From the results, it is clear that in various scenarios, the time required for our method is comparable to that of other methods, with no significant increase in time cost. This suggests that our approach effectively balances performance and computational efficiency, making it a practical choice for applications in this field. ", "page_idx": 19}, {"type": "image", "img_path": "347aDObXEa/tmp/a626b1a8e9b56546ff2206454cc1d332d1af340017f715a97fb812bcacf15f5e.jpg", "img_caption": ["Figure 6: Hyperparameters sensitivity analysis on Co-Physics (left), Pubmed (middle) and Citeseer (right) "], "img_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "347aDObXEa/tmp/d98a42a5c0409dcf6a7f3ce31d87f9b48242cb0a06b3aa23c0d0b7998328f435.jpg", "table_caption": ["Table 13: Ablation experiments for NC task across all datasets, evaluated using F1 score(%)\u2191. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "C.8 Failed Teachers ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The failure of one or more teacher models could potentially impact the student model\u2019s performance, we have implemented several mechanisms in our method to mitigate this risk: ", "page_idx": 20}, {"type": "text", "text": "\u2022 Ensemble Learning: Using multiple teacher models that capture different geometric properties provides redundancy and robustness. If one model fails, the others still contribute valuable insights, minimizing the impact on the student model. \u2022 Geometric Optimization Network:GEO dynamically adjusts the weight of information from each teacher model based on the loss function, reducing the influence of any underperforming model and ensuring the student model receives the most reliable information. ", "page_idx": 20}, {"type": "text", "text": "We designed various experimental strategies to assess the impact of failing teachers on students: ", "page_idx": 20}, {"type": "text", "text": "\u2022 S1: Train student models without KD.   \n\u2022 S2: Train student models with the best-tuned teacher model.   \n\u2022 S3: Train student models with an underperforming teacher model.   \n\u2022 S4: Train student models with an untrained teacher model.   \n\u2022 S5: Train student models with all untrained teacher models. ", "page_idx": 20}, {"type": "text", "text": "Note: All methods except MSKD and ours use a single teacher model; thus, S5 is N/A. ", "page_idx": 20}, {"type": "table", "img_path": "347aDObXEa/tmp/cb818453aa434169d7f14d557bf4970469694e4aedf1760dee3438cb318aed6a.jpg", "table_caption": ["Table 15: Time spent per epoch and compression ratio. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "347aDObXEa/tmp/04c990c44c5812435a4e1e96ad21dfb4eeec069f5ec237cd6fdc67220f354bd4.jpg", "table_caption": ["Table 16: Inference time spent (in ms) for NC task. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "347aDObXEa/tmp/b807134ab176b33176d43abd6bbceef8d9588924a5d3a444ec31645ffb3d07d6.jpg", "table_caption": ["Table 17: F1 scores $(\\%)\\uparrow$ of student model distilled by all KD methods for NC under failed teachers. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "NC f1 score $(\\%)$ of student models on Cora under different experimental strategies are shown in Table 17. Except S5, which teachers have an average performance of only $30\\%$ , our method\u2019s distilled student models consistently maintain stable performance even when some teacher models fail. ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have concisely written our motivation, contributions, and experimental results in the abstract. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: In the final section, we have analyzed the drawbacks of our method and proposed the existing limitations. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our paper does not contain any theoretical results. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have provided sufficient experimental details in the paper and appendix, and we believe that the experiments in the paper can be replicated. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We submitted our code and datasets, and provided a replication guide. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have provided detailed explanations of the experimental setup, dataset partitioning, and hyperparameter sensitivity analysis in the core of paper and the appendix. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We have reduced the errors through repeated experiments, and used standard deviation to represent the error range. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We have provided the details of the experimental environment in the appendix. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We have confirmed that our code does not violate the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our work can reduce resource consumption and have a positive impact on society, without any negative effects. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our work does not have a high risk of misuse. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have provided proper citations and acknowledgments for all the resources we have utilized. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our work did not create any new assets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our work does not include any crowdsourcing experiments or research involving human subjects. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}]