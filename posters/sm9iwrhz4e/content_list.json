[{"type": "text", "text": "Achieving Tractable Minimax Optimal Regret in Average Reward MDPs ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Victor Boone victor.boone@univ-grenoble-alpes.fr Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LIG, 38000 Grenoble, France ", "page_idx": 0}, {"type": "text", "text": "Zihan Zhang zz5478@princeton.edu Princeton University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, significant attention has been directed towards learning averagereward Markov Decision Processes (MDPs). However, existing algorithms either suffer from sub-optimal regret guarantees or computational inefficiencies. In this paper, we present the first tractable algorithm with minimax optimal regret of $\\widetilde{\\mathsf{O}}\\,\\overline{{\\!{\\left(\\sqrt{\\operatorname*{sp}{(h^{*})}S\\,A T}\\right)}}}.$ ,1 where $\\operatorname{sp}\\left(h^{*}\\right)$ is the span of the optimal bias function $h^{*}$ , $S\\times A$ is the size of the state-action space and $T$ the number of learning steps. Remarkably, our algorithm does not require prior information on $\\operatorname{sp}\\left(h^{*}\\right)$ . ", "page_idx": 0}, {"type": "text", "text": "Our algorithm relies on a novel subroutine, Projected Mitigated Extended Value Iteration (PMEVI), to compute bias-constrained optimal policies efficiently. This subroutine can be applied to various previous algorithms to improve regret bounds. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement learning (RL) Burnetas and Katehakis [1997], Sutton and Barto [2018] has become a popular approach for solving complex sequential decision-making tasks and has recently achieved notable advancements in diverse fields of application. RL problems are generally formulated with Markov Decision Processes (MDPs) Puterman [1994], where a learning agent seeks to maximize the rewards that are gathered by interacting with an unknown environment. ", "page_idx": 0}, {"type": "text", "text": "This paper focuses on average reward MDPs where the learning agent must maximize the sum of rewards in the long run without any reset mechanism. In this setting, the proper balancing between exploration (i.e., playing sub-optimally to learn the unknown environment) and exploitation (i.e., planning optimally according to the current knowledge), usually known as the exploration-exploitation trade-off, is key to learn efficiently. The measure of learning performance that we adopt throughout is the regret, that compares the aggregate rewards collected by the learning agent during the learning process to the expected performance of an omniscient agent that knows everything \u221ain advance. The seminal work of Auer et al. [2009] provides a minimax regret lower bound of $\\Omega\\bar{(}\\sqrt{D S A T})$ , where $D$ is the diameter (the maximal distance between two different states), $S$ the number of states, $A$ the \u221anumber of actions and $T$ the learning horizon. They also provide an algorithm achieving regret $\\widetilde{\\mathsf{O}}\\left(\\sqrt{D^{2}S^{2}A T}\\right)$ , where $\\widetilde{\\mathrm{O}}(-)$ . Ever since Auer et al. [2009], many works have been devoted to close the gap between the regret lower and upper bounds in the average reward setting Auer et al. [2009], Bartlett and Tewari [2009], Filippi et al. [2010], Talebi and Maillard [2018], Fruit et al. [2018, 2020], Bourel et al. [2020], Zhang and Ji [2019], Ouyang et al. [2017], Agrawal and Jia [2023], ", "page_idx": 0}, {"type": "text", "text": "Abbasi-Yadkori et al. [2019], Wei et al. [2020] and more. Subsequent works Fruit et al. [2018], Zhang and Ji [2019] refined the minimax regret lower bound to $\\Omega\\left({\\sqrt{\\operatorname*{sp}{(h^{*})}S\\,A T}}\\right)$ where $\\operatorname{sp}\\left(h^{*}\\right)$ is the span of the bias function, which is the maximal gap of the long-term accumulative rewards starting from two different states. The difference is significant, since $\\operatorname{sp}\\left(h^{*}\\right)\\leq D$ and the gap between the two can be arbitrarily large. However, no existing work achieves the following three requirements simultaneously: ", "page_idx": 1}, {"type": "text", "text": "(1) The method achieves minimax optimal regret guarantees ${\\widetilde{\\mathrm{O}}}\\left({\\sqrt{\\mathrm{sp}\\,(h^{*})S\\,A T}}\\right);$ ;   \n(2) The proposed method is tractable;   \n(3) No prior knowledge on the model is required. ", "page_idx": 1}, {"type": "text", "text": "Most algorithms simply fail to achieve minimax optimal regret, and the only method achieving it Zhang and Ji [2019] is intractable because it relies an oracle to solve difficult optimization problems along the learning process. Naturally, we raise the question of whether these three requirements can be met all at once: ", "page_idx": 1}, {"type": "text", "text": "Is there a tractable algorithm with $\\widetilde{\\mathrm{O}}\\left(\\sqrt{\\mathrm{sp}\\left(h^{*}\\right)}S A T\\right)$ minimax regret without prior knowledge? ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Contributions. In this paper, we answer the above question affirmatively, by proposing a polynomial time algorithm with regret guarantees $\\widetilde{\\mathrm{O}}\\left(\\sqrt{\\mathrm{sp}\\left(h^{*}\\right)}S A T\\right)$ for average-reward MDPs. Our method can further incorporate almost arbitrary prior bias information $\\mathcal{H}_{*}\\subseteq\\mathbb{R}^{S}$ to further improve its regret. ", "page_idx": 1}, {"type": "text", "text": "Theorem 1 (Informal). Provided that the confidence region used by PMEVI-DT satisfy mild regularity conditions (see Assumption 1-3), then for every weakly communicating model $M$ with $\\operatorname{sp}(h^{*})\\leq T^{1/5}$ and $s p(h^{*})\\in\\mathcal{H}_{*}$ , PMEVI-DT $(\\mathcal{H}_{*},\\delta,T)$ achieves regret: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{O}\\left(\\sqrt{\\operatorname{sp}(h^{*})S A T\\log\\left(\\frac{S A T}{\\delta}\\right)}\\right)+\\mathrm{O}\\left(\\operatorname{sp}(h^{*})S^{\\frac{5}{2}}A^{\\frac{3}{2}}T^{\\frac{9}{20}}\\log^{2}\\left(\\frac{S A T}{\\delta}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "with probability 1 \u221226\u03b4. Moreover, $i f$ PMEVI-DT runs with the same confidence regions that UCRL2 Auer et al. [2009] on a communicating environment, it has a time complexity $\\mathrm{O}(D\\bar{S}^{3}A T)$ . ", "page_idx": 1}, {"type": "text", "text": "Taking $\\delta=\\,\\sqrt{1/T}$ , we also obtain a $\\widetilde{\\mathrm{O}}\\left(\\sqrt{\\mathrm{sp}\\left(h^{*}\\right)}S A T\\right)$ regret bound in expectation. The geometry of the prior bias region $\\mathcal{H}_{*}$ that PMEVI-DT can support is discussed later (see Assumption 4). It can be taken trivial with $\\mathcal{H}_{*}=\\mathbf{R}^{S}$ to obtain a completely prior-less algorithm. ", "page_idx": 1}, {"type": "text", "text": "To the best of our knowledge, PMEVI-DT is the first tractable algorithm with minimax optimal regret bounds (up to logarithmic factors). The algorithm does not necessitate any prior knowledge of sp $(h^{*})$ , thus circumventing the potentially high cost associated with learning sp $(h^{*})$ . On the technical side, a key novelty of our method is the subroutine named PMEVI (see Algorithm 2) that improves and can replace EVI Auer et al. [2009] in any algorithm that relies on it Auer et al. [2009], Fruit et al. [2018], Filippi et al. [2010], Fruit et al. [2020], Bourel et al. [2020] to boost its performance and achieve minimax optimal regret. ", "page_idx": 1}, {"type": "text", "text": "Related works on average reward MDPs. For communicating MDPs, the notable work of Auer et al. [2009] proposes the famous UCRL2 algorithm\u221a, a mature version of their prior UCRL Auer and Ortner [2006], achieving a regret bound of $\\widetilde{\\mathrm{O}}(D S\\sqrt{A T})$ . This paper pioneered the use optimistic methods to learn MDPs efficiently. A line of papers Filippi et al. [2010], Fruit et al. [2020], Bourel et al. [2020] developed this direction by tightening the confidence region that UCRL2 relies on, and sharpened the analysis through the use of local properties of MDPs, such as local diameters and local bias variances. How\u221aever, none of these works went beyond regret guarantees of order $S\\sqrt{D A T}$ and suffer from an extra $\\sqrt{S}$ . A parallel direction was initiated by Bartlett and Tewari [2009] with REGAL, obtaining regret bounds scaling with $\\operatorname{sp}(h^{*})$ instead of $D$ , and extending the regret bounds to weakly-communicating MDPs in the mean time. The computational intractability of REGAL is addressed by Fruit et al. [2018] with SCAL, and regret guarantees are further improved by Zhang and Ji [2019] with EBF, eventually reaching optimal minimax regret but loosing tractability. ", "page_idx": 1}, {"type": "text", "text": "Another successful design approach is Bayesian-flavored sampling, derived from Thompson Sampling Thompson [1933], that usually replaces optimism. The regret guarantees of these algorithms usually stick to the Bayesian setting however Ouyang et al. [2017], Theocharous et al. [2017], although ", "page_idx": 1}, {"type": "table", "img_path": "SM9IWrHz4e/tmp/965f35584a28ac9fe188a7d62188048991922ac42a04bc14f3c2251e7b9c6df3.jpg", "table_caption": ["Table 1: Comparison of related works on RL algorithms for average-reward MDP, where $S\\times A$ is the size of state-action space, $T$ is the total number of steps, $D$ $(D_{s})$ is the (local) diameter, $\\operatorname{sp}\\left(h^{*}\\right)\\leq D$ is the span of the bias vector, $t_{\\mathrm{mix}}$ is the worst-case mixing time, $t_{\\mathrm{hit}}$ is the hitting time (i.e., the expected time cost to visit some certain state under any policy). "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "Agrawal and Jia [2023] also enjoys $\\widetilde{\\mathrm{O}}(S\\sqrt{D A T})$ high probability regret by coupling posterior sampling with optimism. Another line of research focuses on the study of ergodic MDPs, where the environment is such that all states are visited infinitely often under every policy. To name a f\u221aew, the model-free algorithm Politex Abbasi-Yadkori et al. [2019] attains a regret of $\\widetilde{\\mathrm{O}}((t_{\\mathrm{mix}})^{3}t_{\\mathrm{hit}}\\sqrt{S A}T^{\\frac{3}{4}})$ where $t_{\\mathrm{mix}}$ and $t_{\\mathrm{hit}}$ are respectively the mixing and the hitting times of the ergodic environment. By leveraging an optimistic mirror descent algorithm, Wei et al. [2020] achieve an enhanced regret of $\\widetilde{\\mathrm{O}}(\\sqrt{(t_{\\mathrm{mix}})^{2}t_{\\mathrm{hit}}A T})$ . ", "page_idx": 2}, {"type": "text", "text": "We refer the readers to Table 1 for a (non-exhaustive) list of existing algorithms. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We fix a finite state-action space structure $\\begin{array}{r}{X:=\\bigcup_{s\\in S}\\{s\\}\\times\\mathcal{A}(s)}\\end{array}$ , and denote $\\mathcal{M}$ the collection of all MDPs with state-action space $\\chi$ and rewards supported in $[0,1]$ . ", "page_idx": 2}, {"type": "text", "text": "Infinite-horizon MDP. An element $M\\in\\mathcal{M}$ is a tuple $(S,{\\mathcal{A}},p,r)$ where $p$ is the transition kernel and $r$ the reward function. The random state-action pair played by the agent at time $t$ is denoted $X_{t}\\;\\equiv\\;(S_{t},A_{t})$ , and the achieved reward is $R_{t}$ . A policy is a deterministic rule $\\pi:S\\rightarrow{\\mathcal{A}}$ and we write $\\Pi$ the space of policies. When coupled with a MDP $M\\in\\mathcal{M}$ , a policy properly defines the distribution of $(X_{t},R_{t})$ whose associated probability probability and expectation operators are denoted $\\mathbf{P}_{s}^{\\pi},\\mathbf{E}_{s}^{\\pi}$ , where $s\\,\\in\\,S$ is the initial state. Under $M$ , a fixed policy has a reward function $r^{\\pi}(s):=r(s,\\pi(s))$ , a transition matrix $P^{\\pi}$ , a gain $g^{\\pi}(s):=\\operatorname*{lim}{\\textstyle{\\frac{1}{T}}}\\mathbf{E}_{s}^{\\pi}[R_{0}+\\dotsc+R_{T-1}]$ and a bias $h^{\\pi}(s):=$ Ces\u00e0ro- $\\begin{array}{r}{\\operatorname*{lim}\\mathbf{E}_{s}^{\\pi}[\\sum_{t=0}^{T-1}(R_{t}-g(S_{t}))]}\\end{array}$ , that all together satisfy the Poisson equation $h^{\\pi}+g^{\\pi}=r^{\\pi}+P^{\\pi}h^{\\pi}$ , see Puterman [1994]. The Bellman operator of the MDP is: ", "page_idx": 2}, {"type": "equation", "text": "$$\nL u(s):=\\operatorname*{max}_{a\\in\\mathcal{A}(s)}\\left\\{r(s,a)+p(s,a)u\\right\\}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The optimal gain is $g^{*}(s):=\\operatorname*{max}_{\\pi}g^{\\pi}(s)$ and the optimal bias is $h^{\\ast}(s):=\\operatorname*{max}\\left\\{h^{\\pi}(s):\\pi\\right.$ s.t. $g^{\\pi}=g^{*}]$ . ", "page_idx": 2}, {"type": "text", "text": "Weakly-communicating MDPs. $M$ is weakly-communicating Puterman [1994], Bartlett and Tewari [2009] if the state space can be divided into two sets: (1) the transient set, consisting in states that are transient under all policies; (2) the non-transient set, where every state is reachable starting from any other non-transient state. In this case, $h^{*}$ is a span-fixpoint of $L$ (see Puterman [1994]), i.e., $L h^{*}-h^{*}\\in\\mathbb{R}e$ where $e$ is the vector full of ones. We write $h^{*}\\in\\operatorname{Fix}(L)$ . Then $g^{*}=L h^{*}-h^{*}$ and every policy $\\pi$ satisfies $r^{\\pi}+P^{\\pi}h^{*}\\leq g^{*}+h^{*}$ . We accordingly define the Bellman gaps: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Delta^{*}(s,a):=h^{*}(s)+g^{*}(s)-r(s,a)-p(s,a)h^{*}\\geq0.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Another important concept is the diameter, that describes the maximal distance from one state to another state. It is given by $D:=\\operatorname*{sup}_{s\\neq s^{\\prime}}\\operatorname*{inf}_{\\pi}\\mathbf{E}_{s}^{\\pi}$ [inf $\\{t\\geq1:S_{t}=s^{\\prime}\\}]$ . An MDP is said communicating ", "page_idx": 2}, {"type": "text", "text": "if its diameter $D$ is finite, in which case $\\operatorname{sp}(h^{*})\\leq\\operatorname{sp}(r)D$ , see Bartlett and Tewari [2009], Fruit [2019], where $\\mathrm{{sp}(-)}$ is the span function given by s $\\mathsf{p}\\left(u\\right):=\\operatorname*{max}(u)-\\operatorname*{min}(u)$ . ", "page_idx": 3}, {"type": "text", "text": "Reinforcement learning. The learner is only aware that $M\\in\\mathcal{M}$ but doesn\u2019t have a clue about what $M$ further looks like. From the past observations and the current state $S_{t}$ , the agent picks an available action $\\mathcal{A}(S_{t})$ , receives a reward $R_{t}$ and observe the new state $S_{t+1}$ . The regret of the agent is: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathsf{R e g}(T):=T g^{*}-\\sum_{t=0}^{T-1}R_{t}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Its expected value satisfies $\\begin{array}{r}{{\\bf E}[{\\mathrm{Reg}}(T)]\\,=\\,{\\bf E}[\\sum_{t=0}^{T-1}\\Delta^{*}(X_{t})]+{\\bf E}[h^{*}(S_{0})-h^{*}(S_{T})]}\\end{array}$ and the quantity $\\begin{array}{r}{\\sum_{t=0}^{T-1}\\Delta^{*}(X_{t})}\\end{array}$ will be referred to as the pseudo-regret. This paper focuses on minimax regret guarantees. Specifically, for , denote ${\\cal M}_{c}:=\\{M\\in{\\cal M}:\\exists h^{*}\\in\\mathrm{Fix}(L(M)),\\mathrm{sp}\\,(h^{*})\\leq$ $(h^{*})\\leq c\\}$ the set of weaklycommunicating MDPs that admit a bias function with span at most $c$ . Following Auer et al. [2009], every algorithm A, for all $c>0$ , we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{M\\in{\\mathcal{M}}_{c}}{\\mathbb{E}}^{M,\\mathbf{A}}[{\\mathrm{Reg}}(T)]=\\Omega\\left({\\sqrt{c S A T}}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The goal of this work is to reach this lower bound with a tractable algorithm. ", "page_idx": 3}, {"type": "text", "text": "3 Algorithm PMEVI-DT ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The algorithm PMEVI-DT that we present in this work is actually a general method can be applied to improve various existing algorithms Auer et al. [2009], Filippi et al. [2010], Fruit et al. [2018], Bourel et al. [2020], Tewari and Bartlett [2007]. All these algorithms work episodically, by maintaining a policy $\\pi_{k}$ that drives play during a time-window $\\{t_{k},\\ldots,t_{k+1}-1\\}$ called an episode. An episode rule determines when $\\pi_{k}$ should be considered obsolete and defines the time $t_{k+1}$ at which the policy is renewed. To compute $\\pi_{k}$ , these algorithms follow the optimism-in-face-of-certainty (OFU) design principle, by choosing $\\pi_{k}$ that achieves the largest possible gain that is plausible under their current information. This is done by building a confidence region $\\mathcal{M}_{t}\\subseteq\\mathcal{M}$ for the hidden model $M$ , then searching for a policy $\\pi$ solving the optimization problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{g^{*}({\\cal M}_{t}):=\\operatorname*{sup}\\left\\{g^{\\pi}({\\cal M}_{t}):\\pi\\in\\Pi,\\operatorname{sp}\\left(g^{\\pi}({\\cal M}_{t})\\right)=0\\right\\}\\mathrm{~with~}g^{\\pi}({\\cal M}_{t}):=\\operatorname*{sup}\\left\\{g^{\\pi}(\\widetilde{\\cal M}):\\widetilde{\\cal M}\\in{\\cal M}_{t}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The design of the confidence region $\\boldsymbol{\\mathcal{M}}_{t}$ varies from a work to another. Given a confidence region $(\\mathcal{M}_{t})_{t\\geq0}$ , OFU-algorithms work as follows: At the start of episode $k$ , the optimization problem (5) is solved, and its solution $\\pi_{k}$ is played until the end of episode. The duration of episodes can be managed in various ways, although the most popular is arguably the doubling trick (DT), that essentially waits until a state-action pair is about to double the visit count it had at the beginning of the current episode (see Algorithm 1). ", "page_idx": 3}, {"type": "text", "text": "Notations. In the rest of this section, we use $\\hat{p}_{t}(s,a)$ (and $\\hat{r}_{t}(s,a))$ to denote the empirical transition (and reward) of the latest doubling update before the $t^{\\prime}$ -th step, and further denote $\\hat{M}_{t}:=(\\hat{r}_{t},\\hat{p}_{t})$ . ", "page_idx": 3}, {"type": "text", "text": "Extended Bellman operators and EVI. To solve (5) efficiently, the celebrated work Auer et al. [2009] introduce the extended value iteration algorithm (EVI), that can be run whenever $\\boldsymbol{\\mathcal{M}}_{t}$ is a $(s,a)$ -rectangular confidence region, meaning that $\\begin{array}{r}{\\mathcal{M}_{t}\\equiv\\prod_{s,a}(\\mathcal{R}_{t}(s,a)\\times\\mathcal{P}_{t}(s,a))}\\end{array}$ where $\\mathcal{R}_{t}(s,a)$ and $\\mathcal{P}_{t}(s,a)$ are respectively the confidence region for $r(s,a)$ and $p(s,a)$ after $t$ learning steps. EVI is the algorithm computing the sequence defined by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nu_{i+1}(s)\\equiv\\mathcal{L}_{t}\\nu_{i}(s):=\\operatorname*{max}_{a\\in\\mathcal{R}(s)}\\operatorname*{max}_{\\tilde{r}(s,a)\\in\\mathcal{R}_{t}(s,a)}\\operatorname*{max}_{\\tilde{p}(s,a)\\in\\mathcal{P}_{t}(s,a)}\\big(\\tilde{r}(s,a)+\\tilde{p}(s,a)\\cdot\\nu_{i}\\big)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "until sp $(\\nu_{i+1}-\\nu_{i})<\\epsilon$ where $\\epsilon>0$ is the numerical precision. When the process stops, it is known that any policy $\\pi$ such that $\\pi(s)$ achieves ${\\mathcal{L}}_{t}\\nu_{i}$ in (6) satisfies $g^{\\pi}(M_{t})\\geq g^{*}(M)-\\epsilon.$ , hence is nearly optimistically optimal. This process gets its name from the observation that $\\mathcal{L}_{t}$ is the Bellman operator of $\\boldsymbol{\\mathcal{M}}_{t}$ seen as a MDP, hence EVI is just the Value Iteration algorithm Puterman [1994] ran in $\\boldsymbol{\\mathcal{M}}_{t}$ . A choice of action from $s\\in S$ in $\\boldsymbol{\\mathcal{M}}_{t}$ consists in (1) a choice of action $a\\in{\\mathcal{A}}(s)$ , (2) a choice of reward $\\widetilde{r}(s,a)\\in\\mathcal{R}_{t}(s,a)$ and (3) a choice of transition $\\tilde{p}(s,a)\\in\\mathcal{P}_{t}(s,a)$ ; It is an extended version of ${\\mathcal{A}}(s)$ . ", "page_idx": 3}, {"type": "text", "text": "Towards Projected Mitigated EVI. Obviously, the regret of an OFU-algorithm is directly related to the quality of the \u221aconfidence region $\\boldsymbol{\\mathcal{M}}_{t}$ . That is why most previous works tried to approach the regret lower bound $\\sqrt{D S A T}$ of Auer et al. [2009] by refining $\\boldsymbol{\\mathcal{M}}_{t}$ . The older works of Auer et al. [2009], Bartlett and Tewari [2009], Filippi et al. [2010] have been improved with a variance aware analysis Talebi and Maillard [2018], Fruit et al. [2018, 2020], Bourel et al. [2020] that essentially make use of tightened kernel confidence regions $\\mathcal{P}_{t}$ . While all these algorithms successively\u221a reduce the gap between the regret upper and lower bounds, they fail to achieve optimal regret $\\sqrt{D S A T}$ . Meanwhile, the EBF algorithm of Zhang and Ji [2019] is minimax optimal but (1) the algorithm is intractable because it relies on an oracle to retrieve optimistically optimal policies and (2) needs prior information on the bias function. Nonetheless, the method of Zhang and Ji [2019] strongly suggests that inferring bias information from the available data is key to achieve minimax optimal regret. ", "page_idx": 4}, {"type": "text", "text": "Rather surprisingly and in opposition to this previous line of work, our work suggests that the choice of the confidence region $\\boldsymbol{\\mathcal{M}}_{t}$ has little importance. Instead, our algorithm takes an arbitrary (wellbehaved) confidence region in, infer bias information similarly to Zhang and Ji [2019] and makes use of it to refine the extended Bellman operator (6) associated to the input confidence region. Our algorithm can further take arbitrary prior information (possibly none) on the bias vector to tighten its bias confidence region. The pseudo-code given in Algorithm 1 is the high level structure our algorithm PMEVI-DT. In the next Section 3.1, we explain how (6) is refined using bias information. ", "page_idx": 4}, {"type": "table", "img_path": "SM9IWrHz4e/tmp/1e92ac607230de562a85f5fed2e8453380d748b0a4bfed896b87f74f87376103.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.1 Projected mitigated extended value iteration (PMEVI) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Assume that an external mechanism provides a confidence region $\\mathcal{H}_{t}$ for the bias function $h^{*}$ . Provided that $\\boldsymbol{\\mathcal{M}}_{t}$ is correct $(M\\in\\mathcal{M}_{t})$ ) and that $\\mathcal{H}_{t}$ is correct $(h^{\\ast}\\in\\mathcal{H}_{t})$ , we want to find a policy-model pair $(\\pi,\\widetilde{M})$ that maximizes the gain among pairs with $h^{\\pi}(\\widetilde{M})\\in\\mathcal{H}_{t}$ . This is done with an improved version of (6) combining two ideas, that are both necessary to achieve minimax optimal regret in the analysis. ", "page_idx": 4}, {"type": "text", "text": ". Projection (Section 3.2). Whenever it is correct, the bias confidence region $\\mathcal{H}_{t}$ informs the learner that the search of an optimistic model can be constrained to those with bias within $\\mathcal{H}_{t}$ . This is done by projecting $\\textstyle{\\mathcal{L}}_{t}^{\\beta}$ (see mitigation) using an operator $\\Gamma_{t}:\\mathbf{R}^{S}\\rightarrow\\mathcal{H}_{t}$ , that has to satisfy a few non-trivial regularity conditions that are specified in Proposition 2. ", "page_idx": 4}, {"type": "text", "text": "2. Mitigation (Section 3.3). When one is aware that $h^{*}\\,\\in\\,\\mathcal{H}_{t}$ , the dynamical bias update $\\tilde{p}(s,a)\\nu_{i}$ in (6) can be controlled better, by trying to restrict (6) to the $\\tilde{p}(s,a)$ such that $\\tilde{p}(s,a)\\nu_{i}\\,\\le\\,\\hat{p}_{t}(s,a)\\nu_{i}+(p(s,a)-\\hat{p}_{t}(s,a))\\nu_{i}$ with the knowledge that $\\nu_{i}\\,\\in\\,\\mathcal{H}_{t}$ . However, controlling the error $(p(s,a)-\\hat{p}_{t}(s,a))\\nu_{i}$ by doing a union-bound on all possible values of $\\nu_{i}$ is equivalent to building a confidence region for $p(s,a)$ , which produces an extra $S^{1/2}$ in the error term that cannot be afforded by a minimax optimal algorithm. We take a different approach instead. For a fixed $u\\in\\mathbb{R}^{s}$ , the empirical Bernstein inequality (Lemma 38) provides a variance bound of the form $(\\hat{p}_{t}(s,a)-p(s,a))u\\le\\beta_{t}(s,a,u)$ . By estimating $\\beta_{t}(s,a):=\\operatorname*{max}_{u\\in\\mathcal{H}_{t}}\\beta_{t}(s,a,u)$ , the search makes sure that $(\\hat{p}_{t}(s,a)-p(s,a))u\\le$ $\\beta_{t}(s,a)$ holds with high probability for $u=h^{*}$ , even though $h^{*}$ is unknown. For $\\beta\\in\\mathbb{R}_{+}^{X}$ , we introduce the $_\\beta$ -mitigated extended Bellman operator: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{t}^{\\beta}u(s):=\\operatorname*{max}_{a\\in\\mathcal{R}(s)}\\operatorname*{sup}_{\\tilde{r}(s,a)\\in\\mathcal{R}_{\\iota}(s,a)}\\operatorname*{sup}_{\\tilde{p}(s,a)\\in\\mathcal{P}_{\\iota}(s,a)}\\left\\{\\tilde{r}(s,a)+\\operatorname*{min}\\left\\{\\tilde{p}(s,a)u_{i},\\hat{p}_{t}(s,a)u_{i}+\\beta(s,a)\\right\\}\\right\\}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The mitigation $\\beta(s,a)$ is independent of $u$ , which is crucial for $\\mathcal{L}_{t}^{\\beta}$ to be well-behaved. ", "page_idx": 5}, {"type": "text", "text": "The proposition below shows how well-behaved the composition $\\mathfrak{L}_{t}:=\\Gamma_{t}\\circ\\mathcal{L}_{t}^{\\beta}$ is. Its proof requires to build a complete analysis of projected mitigated Bellman operators. This is deferred to the appendix. ", "page_idx": 5}, {"type": "text", "text": "Proposition 2. $F i x\\,\\beta\\in\\mathbf{R}_{+}^{X}$ and assume that there exists a projection operator $\\Gamma_{t}:\\mathbf{R}^{X}\\rightarrow\\mathcal{H}_{t}$ which is $(O I)$ monotone: $u\\leq\\nu\\Rightarrow\\Gamma u\\leq\\Gamma\\nu,$ ; $(O2)$ non span-expansive: sp $(\\Gamma u-\\Gamma\\nu)\\leq\\mathrm{sp}\\left(u-\\nu\\right)$ ; (O3) linear: $\\Gamma(u+\\lambda e)=\\Gamma u+\\lambda e$ and (O4) $\\Gamma u\\leq u.$ . Then, the projected mitigated extended Bellman operator $\\mathfrak{L}_{t}:=\\Gamma_{t}\\circ\\mathcal{L}_{t}^{\\beta}$ has the following properties: ", "page_idx": 5}, {"type": "text", "text": "(1) There exists a unique ${\\mathfrak{g}}_{t}\\in\\mathbb{R}e$ such that $\\exists\\mathfrak{h}_{t}\\in\\mathcal{H}_{t},\\mathfrak{L}_{t}\\mathfrak{h}_{t}=\\mathfrak{h}_{t}+\\mathfrak{g}_{t},$ ; (2) If $M\\in\\mathcal{M}_{t}$ , $h^{*}\\in\\mathcal{H}_{t}$ and $(\\hat{p}_{t}(s,a)-p(s,a))h^{\\ast}\\leq\\beta(s,a),$ , then ${\\mathfrak{g}}_{t}\\geq g^{*}(M),$ ; (3) If $\\boldsymbol{\\mathcal{M}}_{t}$ is convex, then for all $u\\in\\mathbb{R}^{S}$ , the policy $\\pi=:\\mathtt{G r e e d y}(M_{t},u,\\beta)$ picking the actions achieving $\\mathcal{L}_{t}^{\\beta}u$ satisfies $\\mathfrak{L}_{t}u=\\tilde{r}^{\\pi}+\\tilde{P}^{\\pi}u$ for $\\tilde{r}^{\\pi}(s)\\leq\\operatorname*{sup}\\mathcal{R}_{t}(s,\\pi(s))$ and $\\tilde{P}^{\\pi}(s)\\in\\mathcal{P}_{t}(s,\\pi(s))$ ; (4) For all $u\\in\\mathbb{R}^{s}$ an $\\begin{array}{r}{n\\geq0,\\,\\operatorname{sp}\\Big(\\mathfrak{L}_{t}^{n+1}u-\\mathfrak{L}_{t}^{n}u\\Big)\\leq\\operatorname{sp}\\big((\\mathcal{L}_{t})^{n+1}u-(\\mathcal{L}_{t})^{n}u\\big).}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "The property (1) guarantees that $\\mathfrak{L}_{t}$ has a fix-point while (2) states that this fix-point corresponds to an optimistic gain ${\\mathfrak{g}}_{t}$ if the model and the bias confidence region are correct and the mitigation isn\u2019t too aggressive. Combined with (3), the Poisson equation of a policy corresponds to this fix-point, i.e., $\\widetilde{r}^{\\pi}+\\widetilde{P}^{\\pi}\\mathfrak{h}_{t}=\\mathfrak{h}_{t}+\\mathfrak{g}_{t}$ , so that ${\\mathfrak{g}}_{t}$ is the gain and $\\mathfrak{h}_{t}\\in\\mathcal{H}_{t}$ is a legal bias for $\\pi$ under the model $(\\tilde{r}^{\\pi},\\tilde{P}^{\\pi})$ . Lastly, the property (4) guarantees that the iterates $\\mathfrak{L}_{t}^{n}u$ converge to a fix-point of $\\mathfrak{L}$ at least as quickly as $\\mathcal{L}_{t}^{n}u$ goes to a fix-point of $\\mathcal{L}_{t}$ ; The convergence of $(\\mathcal{L}_{t})^{n}u$ is already guaranteed by existing studies and is discussed in the appendix. ", "page_idx": 5}, {"type": "text", "text": "Provided that the bias confidence region is constructed, Proposition 2 foreshadows how powerful the construction is: The algorithm PMEVI, obtained by iterating ${\\mathfrak{L}}_{t}$ instead of $\\mathcal{L}_{t}$ in EVI, can replace the well-known EVI within any algorithm of the literature that relies on it (UCRL2 Auer et al. [2009], UCRL2B Fruit et al. [2020] or KL-UCRL Filippi et al. [2010]) for an immediate improvement of its theoretical guarantees. ", "page_idx": 5}, {"type": "text", "text": "3.2 Building the bias confidence region and its projection operator ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The bias confidence region used by PMEVI-DT is obtained as a collection of constraints of the form: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\forall s\\neq s^{\\prime},\\quad\\mathfrak{h}(s)-\\mathfrak{h}(s^{\\prime})-c(s,s^{\\prime})\\leq d(s,s^{\\prime}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Such constraints include (1) prior bias constraints (if any) of the form of ${\\mathfrak{h}}(s){-}{\\mathfrak{h}}(s^{\\prime})\\leq c_{*}(s,s^{\\prime})$ ; (2) span constraints of the form $\\mathfrak{h}(s)-\\mathfrak{h}(s^{\\prime})\\leq c_{0}:=T^{1/5}$ spawning the span semi-ball $\\{u:\\mathrm{sp}\\,(u)\\leq T^{1/5}\\}$ ; and (3) pair-wise constraints obtained by estimating bias differences in the style of Zhang and Ji [2019], Zhang and Xie [2023] that we further improve. We start by defining a bias difference estimator. ", "page_idx": 5}, {"type": "text", "text": "Definition 1 (Bias difference estimator). Given a pair of states $s\\neq s^{\\prime}$ , their sequence of commute times $(\\tau_{i}^{s\\leftrightarrow s^{\\prime}})_{i\\geq0}$ is defined by $\\tau_{2i}^{s\\leftrightarrow s^{\\prime}}:=\\operatorname*{inf}\\{t>\\tau_{2i-1}^{s\\leftrightarrow\\hat{s^{\\prime}}}:S_{t}=s\\}$ and $\\tau_{2i+1}^{s\\leftrightarrow s^{\\prime}}:=\\operatorname*{inf}\\{\\bar{t}>\\tau_{2i}^{s\\leftrightarrow s^{\\prime}}:S_{t}=s^{\\prime}\\}$ with thei convention that $\\tau_{-1}^{s\\leftrightarrow s^{\\prime}}=-\\infty$ . The num2bi\u2212e1r  of commutation2si +u1 p to time $t$ is $N_{t}(s\\leftrightarrow s^{\\prime}):=$ $\\operatorname*{inf}\\{i:\\tau_{i}^{s\\leftrightarrow s^{\\prime}}\\leq t\\}$ , and $\\begin{array}{r}{\\hat{g}(t):=\\frac{1}{t}\\sum_{i=0}^{t-1}R_{i}}\\end{array}$ is the empirical gain. The bias difference estimator at time $T$ is any quantity $c_{T}(s,s^{\\prime})\\in\\mathbb{R}$ such that: ", "page_idx": 5}, {"type": "equation", "text": "$$\nN_{t}(s\\leftrightarrow s^{\\prime})c_{T}(s,s^{\\prime})=\\sum_{t=0}^{N_{T}(s\\leftrightarrow s^{\\prime})-1}(-1)^{i}\\sum_{t=\\tau_{i}^{s\\leftrightarrow s^{\\prime}}}^{\\tau_{i+s^{\\prime}}^{s\\leftrightarrow s^{\\prime}}-1}(\\hat{g}(T)-R_{t}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Lemma 3. With probability 1 \u22122\u03b4, for all $T^{\\prime}\\leq T$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\nN_{T^{\\prime}}(s\\leftrightarrow s^{\\prime})\\left|h^{*}(s)-h^{*}(s^{\\prime})-c_{T^{\\prime}}(s,s^{\\prime})\\right|\\leq3\\mathrm{sp}\\left(h^{*}\\right)+(1+\\mathrm{sp}\\left(h^{*}\\right))\\sqrt{8T\\log(\\frac{2}{\\delta})}+2\\sum_{t=0}^{T^{\\prime}-1}(g^{*}-R_{t}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Lemma 3 says that the quality of the estimator $c_{T}(s,s^{\\prime})$ is directly linked to the number of observed commutes between $s$ and $s^{\\prime}$ as well as the regret. The idea is that if the algorithm makes many commutes between $s$ and $s^{\\prime}$ and if its regret is small, then the algorithm mostly takes optimal paths from $s$ to $s^{\\prime}$ . The bound provided by Lemma 3 is not accessible to the learner however, because $\\operatorname{sp}\\left(h^{*}\\right)$ is unknown in general. To overcome this issue, sp $(h^{*})$ is upper-bounded by $c_{0}:=T^{1/5}$ . Overall, this leads to the design of the algorithm estimating the bias confidence region as specified in Algorithm 3. ", "page_idx": 6}, {"type": "table", "img_path": "SM9IWrHz4e/tmp/c46c5bc6f6a1ead4699373f293dc37ac569ad65a5031169914cc8b854aabb318.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Coupled with prior information and span constraints, the bias confidence region $\\mathcal{H}_{t}$ is a polyhedron of the same kind as the one encountered in Zhang and Xie [2023]. When generated by constraints of the form (8), following [Zhang and Xie, 2023, Proposition 3], one can project onto $\\mathcal{H}_{t}$ in polynomial time with Algorithm 4. Moreover, the resulting projection operator satisfies the prerequisites (O1-4) of Proposition 2, making PMEVI (Algorithm 2) well-behaved. See Appendix B.2 for proofs. ", "page_idx": 6}, {"type": "text", "text": "Lemma 4. Assume that $\\mathcal{H}$ is a set of $\\mathfrak{h}\\in\\mathbb{R}^{s}$ satisfying a system of equations of the form of (8). If H is non empty, then the operator $\\Gamma u:=$ BiasProjection $(\\mathcal{H},u)$ (see Algorithm 4) is a projection on $\\mathcal{H}$ and satisfies the properties (O1-4) defined in Proposition 2. ", "page_idx": 6}, {"type": "text", "text": "3.3 Mitigation using finer bias dynamical error ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The fact that $h^{*}\\,\\in\\,\\mathcal{H}_{t}$ with high probability is used in PMEVI-DT to restrict the search of EVI by reducing the dynamical bias error. This reduction is based on a empirical Bernstein inequality (see Lemma 38) applied to $(\\hat{p}(s,a)-p(s,a))u$ . Here, it gives that with probability $1-\\delta$ , we have: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left(\\hat{p}_{t}(s,a)-p(s,a)\\right)u\\le\\,\\sqrt{\\frac{2\\mathbf{V}(\\hat{p}_{t}(s,a),u)\\log\\left(\\frac{3T}{\\delta}\\right)}{\\operatorname*{max}\\left\\{1,N_{t}(s,a)\\right\\}}}+\\frac{3\\mathbf{sp}\\left(u\\right)\\log\\left(\\frac{3T}{\\delta}\\right)}{\\operatorname*{max}\\left\\{1,N_{t}(s,a)\\right\\}}=:\\beta_{t}(s,a,u)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathbf{V}(\\hat{p}_{t}(s,a),u)$ is the variance of $u$ under the probability vector $\\hat{p}_{t}(s,a)$ . More specifically, if $q$ is a probability on $s$ and $q\\,\\in\\,\\mathbb{R}^{s}$ , we set $\\begin{array}{r}{\\mathbf{V}(q,u)~:=~\\sum_{s}q(s)(u(s)-q\\cdot u)^{2}}\\end{array}$ . In (11), $u\\,\\in\\,\\mathbf{\\dot{R}}^{S}$ , $(s,a)\\in X$ and $T\\geq1$ are fixed. Once is tempted to use (11) directly to mitigate the extended Bellman operator, but the resulting operator is ill-behaved because it loses monotony. This issue is avoided by changing $\\beta_{t}(s,a,u)$ to $\\mathrm{max}_{u\\in\\mathcal H_{t}}\\beta_{t}(s,a,u)$ in (11). The resulting inequality is not guaranteed to hold simultaneously for all $u\\in\\mathcal{H}_{t}$ and with high probability; However, it is guaranteed to hold with high probability for $u=h^{*}$ , which will be enough. ", "page_idx": 6}, {"type": "text", "text": "The variance maximization problem $\\mathrm{max}_{u\\in\\mathcal H_{t}}\\beta_{t}(s,a,u)$ is a convex maximization problem with linear constraints. Even in very simple settings, such optimization problems are NP-hard Pardalos and Schnitger [1988] hence computing $\\mathrm{max}_{u\\in\\mathcal H_{t}}\\beta_{t}(s,a,u)$ is not reasonable in general. Thankfully, this value can be upper-bounded by a tractable quantity that is enough in the regret analysis. The mitigation $\\beta_{t}$ used by PMEVI-DT is provided by Algorithm 5. See Lemma 12 and Appendix A.2.2 for details. ", "page_idx": 6}, {"type": "text", "text": "4 Regret guarantees ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Theorem 5 thereafter shows that PMEVI-DT has minimax optimal regret under regularity assumptions on the used confidence region $\\boldsymbol{\\mathcal{M}}_{t}$ . Assumption 1 asserts that the confidence region holds uniformly with high probability. Assumption 2 asserts that the reward confidence region is sub-Weissman (see ", "page_idx": 6}, {"type": "table", "img_path": "SM9IWrHz4e/tmp/72dc8e028542aef2746feaa5615994fc904420565408e982533c6bd9e7cb7aa5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Lemma 35) and Assumption 3 assumes that the model confidence region makes sure that EVI (6) converges in the first place. Assumption 4 asserts that the prior bias region is correct. ", "page_idx": 7}, {"type": "text", "text": "Assumption 1. With probability $1-\\delta$ , we have $M\\in\\bigcap_{k=1}^{K(T)}\\mathcal{M}_{t_{k}}$ ", "page_idx": 7}, {"type": "text", "text": "Assumption 2. There exists a constant $C>0$ such that for all $(s,a)\\in S$ , for all $t\\leq T$ , we have: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}_{t}(s,a)\\subseteq\\left\\{\\tilde{r}(s,a)\\in\\mathcal{R}(s,a):N_{t}(s,a)\\left\\Vert\\hat{r}_{t}(s,a)-\\tilde{r}(s,a)\\right\\Vert_{1}^{2}\\leq C\\log\\left(\\frac{2S A(1+N_{t}(s,a))}{\\delta}\\right)\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Assumption 3. For $t\\geq0,\\,M_{t}$ is a $(s,a)$ -rectangular convex region and $\\mathcal{L}_{t}^{n}u$ converges a fix-point. Assumption 4. The prior bias region $\\mathcal{H}_{*}$ contains $h^{*}(M)$ and is generated by constraints of the form: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\forall s\\neq s^{\\prime},\\quad\\mathfrak{h}(s)-\\mathfrak{h}(s^{\\prime})\\leq c_{*}(s,s^{\\prime})\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "with $c_{*}(s,s^{\\prime})\\in[-\\infty,\\infty]$ (possibly infinite). ", "page_idx": 7}, {"type": "text", "text": "Refer to Appendix A.2 for the feasibility of Assumption 1, Appendix A.2.3 for Assumption 2, and Appendix A.3 for Assumption 3. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5 (Main result). Let $c>0$ . Assume that PMEVI-DT runs with a confidence region system $t\\mapsto\\mathcal{M}_{t}$ that guarantees Assumptions 1-3. If $T\\geq c^{5}$ , then for every weakly communicating model with $\\cdot p(h^{*})\\leq c$ and such that Assumption $^{4}$ is satisfied $(h^{\\ast}\\in\\mathcal{H}_{\\ast})$ ), PMEVI-DT achieves regret: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{O}\\left(\\sqrt{c S A T\\log\\left(\\frac{S A T}{\\delta}\\right)}\\right)+\\mathrm{O}\\left(c S^{\\frac{5}{2}}A^{\\frac{3}{2}}T^{\\frac{9}{20}}\\log^{2}\\left(\\frac{S A T}{\\delta}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "with probability 1 \u221226\u03b4, and in expectation if $\\delta<\\sqrt{1/T}$ . Moreover, $i f$ PMEVI-DT runs with the same confidence regions that UCRL2 Auer et al. [2009], then it enjoys a time complexity $\\mathrm{O}(D S^{3}A T)$ . ", "page_idx": 7}, {"type": "text", "text": "To have a completely prior-less algorithm, pick $\\mathcal{H}_{*}=\\mathbf{R}^{S}$ . The proof of Theorem 5 is tedious and its details are deferred to appendix. We will focus here on the main ideas. ", "page_idx": 7}, {"type": "text", "text": "Notations. At episode $k$ , the played policy is denoted $\\pi_{k}$ . As a greedy response to ${\\mathfrak{h}}_{k}$ , by Proposition 2 (3), there exists $\\tilde{r}_{k}(s)\\leq\\operatorname*{sup}\\mathcal{R}_{t_{k}}(s,\\pi_{k}(s))$ and $\\tilde{P}_{k}(s)\\in\\mathcal{P}_{t_{k}}(s,\\bar{\\pi}(x))$ such that $\\mathfrak{h}_{k}+\\mathfrak{g}_{k}=\\dot{\\tilde{r}}_{k}+\\dot{\\tilde{P}}_{k}\\mathfrak{h}_{k}$ . The reward-kernel pair $\\tilde{M}_{k}~=~(\\tilde{r}_{k},\\tilde{P}_{k})$ is referred to as the optimistic model of $\\pi_{k}$ . We write $P_{k}:=P_{\\pi_{k}}(M)$ the true kernel and $\\hat{P}_{k}\\,:=\\,P_{\\pi_{k}}(\\hat{M}_{t_{k}})$ the empirical kernel. Likewise, we define the reward functions $r_{k}$ and $\\hat{r}_{k}$ . The optimistic gain and bias satisfy ${\\mathfrak{g}}_{k}=g(\\pi_{k},\\widetilde{M}_{k})$ and $\\mathfrak{h}_{k}=\\mathfrak{h}(\\pi_{k},\\widetilde{M}_{k})$ . We further denote $c_{0}=T^{\\frac{1}{5}}$ . ", "page_idx": 7}, {"type": "text", "text": "The regret is first decomposed episodically with $\\begin{array}{r}{\\mathrm{Reg}(T)=\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}(g^{*}-R_{t})}\\end{array}$ . The first step goes back to the analysis of UCRL2 Auer et al. [2009], and consists in upper-bounding the regret of the episode $k$ with optimistic quantities that are exclusive to that episode. ", "page_idx": 7}, {"type": "text", "text": "Lemma 6 (Reward optimism). With probabililty 1 \u22126\u03b4, we have: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{Reg}(T)\\leq\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}(\\mathfrak{g}_{k}-R_{t})\\leq\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}(\\mathfrak{g}_{k}-\\tilde{r}_{k}(X_{t}))+\\ensuremath{\\operatorname{O}\\left(\\sqrt{S A T\\log\\left(\\frac{T}{\\delta}\\right)}\\right)}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We introduce the two optimistic regrets $\\begin{array}{r}{B(T):=\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}({\\mathfrak{g}}_{k}{-}R_{t})}\\end{array}$ and $\\begin{array}{r}{\\tilde{B}(T):=\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}({\\mathfrak{g}}_{k}{-}\\tilde{r}_{k}(X_{t}))}\\end{array}$ Rewriting the summand ${\\mathfrak{g}}_{k}-{\\tilde{r}}_{k}(X_{t})$ using the Poisson equation $\\mathfrak{h}_{k}+\\mathfrak{g}_{k}=\\tilde{r}_{k}+\\tilde{P}_{k}\\mathfrak{h}_{k}$ , we get: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\tilde{B}(T)=\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}\\left(\\tilde{p}_{k}(S_{t})-e_{S_{t}}\\right)\\mathfrak{h}_{k}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "image", "img_path": "SM9IWrHz4e/tmp/6d6f106685201fd300e0ef039ca0d630e31fc6aecf457ee33793ca6a516ad73a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 1: An overview of PMEVI-DT and its regret analysis. In the above, ${\\mathfrak{g}}_{k}$ and $\\mathfrak{h}_{k}$ are the optimistic gain and bias functions produced by PMEVI (see Algorithm 2) at episode $k$ , and $\\hat{p}_{t_{k}}$ and $\\tilde{p}_{t_{k}}$ are respectively the empirical and optimistic kernel models at episode $k$ . ", "page_idx": 8}, {"type": "text", "text": "The analysis proceeds by decomposing the above expression of $\\tilde{B}(T)$ in the style of Zhang and Ji [2019]. We write $\\begin{array}{r}{\\sum_{t=t_{k}}^{t_{k+1}-1}(\\tilde{p}_{k}(S_{t})-e_{S_{t}})\\mathfrak{h}_{k}}\\end{array}$ as: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{t=t_{k}}^{t_{k+1}-1}\\left(\\underbrace{\\left(p_{k}(S_{t})-e_{S_{t}}\\right)\\mathbb{I}_{k}}_{\\mathrm{navigation~error}\\ (1k)}+\\underbrace{\\left(\\hat{p}_{k}(S_{t})-p_{k}(S_{t})\\right)h^{*}}_{\\mathrm{empirical~blas~error}\\ (2k)}+\\underbrace{\\left(\\tilde{p}_{k}(S_{t})-\\hat{p}_{k}(S_{t})\\right)\\mathbb{I}_{k}}_{\\mathrm{optimistic~overshoot}\\ (3k)}+\\underbrace{\\left(\\hat{p}_{k}(S_{t})-p_{k}(S_{t})\\right)\\left(\\mathbb{I}_{k}-h^{*}\\right)}_{\\mathrm{second~order~error}\\ (4k)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Each error  t  e   r    m     is b   o    u   n    ded separately. Below, we denote $\\begin{array}{r}{\\mathbf{V}(q,u):=\\sum_{s}q(s)(u(s)-q\\cdot u)^{2}}\\end{array}$ ", "page_idx": 8}, {"type": "text", "text": "Lemma 7 (Navigation error). With probability 1 \u22127\u03b4, the navigation error is bounded by: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}(p_{k}(S_{t})-e_{S_{t}})\\mathfrak{h}_{k}\\le\\sqrt{2\\sum_{t=0}^{T-1}\\mathbf{V}(p(X_{t}),h^{*})\\log\\left(\\frac{T}{\\delta}\\right)}+2S A^{\\frac{1}{2}}\\sqrt{3B(T)}\\log\\left(\\frac{T}{\\delta}\\right)+\\widetilde{\\Omega}\\left(T^{\\frac{7}{20}}\\right).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Lemma 8 (Empirical bias error). With probability $1-\\delta$ , the empirical bias error is bounded by: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}\\left(\\hat{p}_{k}(S_{t})-p_{k}(S_{t})\\right)h^{*}\\leq4\\,\\sqrt{S A\\sum_{t=0}^{T-1}{\\Psi(p(X_{t}),h^{*})\\log\\left(\\frac{S A T}{\\delta}\\right)}}+\\ensuremath{\\operatorname{O}\\left(\\log^{2}(T)\\right)}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Lemma 9 (Optimistic overshoot). With probability 1 \u22126\u03b4, the optimistic overshoot is bounded by: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}\\left(\\tilde{p}_{k}(S_{t})-\\hat{p}_{k}(S_{t})\\right)\\mathfrak{h}_{k}\\leq\\left\\{\\begin{array}{c}{4\\,\\sqrt{2S A\\sum_{t=0}^{T-1}\\mathbf{V}(p(X_{t}),h^{\\ast})\\log\\left(\\frac{S A T}{\\delta}\\right)}}\\\\ {+8(1+c_{0})S^{\\frac{3}{2}}A\\log^{\\frac{3}{2}}\\left(\\frac{S A T}{\\delta}\\right)\\,\\sqrt{B(T)}+\\widetilde{\\mathbf{O}}\\left(T^{\\frac{1}{4}}\\right)}\\end{array}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Lemma 10 (Second order error). With probability 1 \u22126\u03b4, the second order error is bounded by: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}(\\hat{p}_{k}(S_{t})-p_{k}(S_{t}))\\left(\\mathfrak{h}_{k}-h^{*}\\right)\\leq16S^{2}A(1+c_{0})\\log^{\\frac{1}{2}}\\left(\\frac{S^{2}A T}{\\delta}\\right)\\,\\sqrt{2B(T)}+\\widetilde{\\Omega}\\left(T^{\\frac{1}{4}}\\right).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We see that the empirical bias error (Lemma 8) and the optimistic overshoot (Lemma 9) both involve the sum of variances $\\begin{array}{r l}{\\sum_{t=0}^{T-1}\\mathbf{V}(p(X_{t}),h^{*})}\\end{array}$ , which is shown in Lemma 29 to be of order s $\\mathrm{p}\\,(h^{*})\\mathrm{sp}\\,(r)T+$ $\\begin{array}{r}{\\sum_{t=0}^{T-1}\\Delta^{*}(X_{t})}\\end{array}$ . The pse udo-regret term $\\begin{array}{r}{\\sum_{t=0}^{T-1}\\Delta^{*}(X_{t})}\\end{array}$ is bounded with the regret using Corollary 31, then by $B(T)$ . With high probability, we obtain an equation of the form: ", "page_idx": 8}, {"type": "equation", "text": "$$\nB(T)\\leq C\\,\\sqrt{(1+\\mathrm{sp}\\,(h^{\\ast}))S\\,A T\\log\\left(\\frac{T}{\\delta}\\right)}+C S^{2}A(1+c_{0})\\log^{2}(T)\\,\\sqrt{B(T)}+\\tilde{\\Omega}\\left(T^{\\frac{1}{4}}\\right)\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $C$ is a constant. Setting $\\alpha:=C S^{2}A(1+c_{0})\\log^{2}(T)$ \u221aand $\\beta:=C\\,\\sqrt{(1+\\operatorname{sp}\\left(h^{*}\\right))S A T\\log(T/\\delta)}\\,+$ $\\tilde{\\mathrm{O}}(T^{1/4})$ , the above equation is of the form $B(T)\\leq\\beta+\\alpha\\,\\sqrt{B(T)}$ . Solving in $B(T)$ , we find $B(T)\\leq$ $\\beta+2\\,\\sqrt{\\alpha\\beta}+\\alpha^{2}$ . The dominant term is $\\beta$ , hence we readily obtain: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{B(T)\\leq C\\,\\sqrt{(1+\\mathrm{sp}\\,(h^{\\ast}))\\mathrm{sp}\\,(r)S\\,A T\\log\\left(\\frac{T}{\\delta}\\right)}+\\widetilde{\\Omega}\\left(\\mathrm{sp}\\,(h^{\\ast})\\mathrm{sp}\\,(r)S^{\\frac{5}{2}}A^{\\frac{3}{2}}(1+c_{0})T^{\\frac{1}{4}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Since $c_{0}=\\mathrm{o}(T^{\\frac{1}{4}})$ , we conclude that $B(T)=\\operatorname{O}{\\bigl(}{\\sqrt{\\operatorname{sp}{(h^{*})S A T}\\log(T/\\delta)}}{\\bigr)}$ , ending the proof. ", "page_idx": 8}, {"type": "text", "text": "5 Experimental illustrations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To get a grasp of how PMEVI-DT behaves in practice, we provide in Fig. 2 a first round of illustrative experiments. In both, the environment is a river-swim which is a model known to be hard to learn despite its size, with high diameter and bias span, see Appendix D for the model\u2019s description. ", "page_idx": 9}, {"type": "image", "img_path": "SM9IWrHz4e/tmp/01c2e0837d1bee0387679396115d73f6df7ac3b651672e05b66f07653dbf8519.jpg", "img_caption": ["Figure 2: (To the left) Running UCRL2 and PMEVI-DT with the same confidence region than UCRL2 on a 3-state river-swim. PMEVI-DT is run with prior knowledge $h^{\\ast}(s_{1})\\leq h^{\\ast}(s_{2})-c\\leq h^{\\ast}(s_{3})-2c$ for $c\\in\\{0,0.5,1,1.5,2\\}$ . (To the right) Running a few algorithms of the literature on 5-state river-swim and comparing their average regret against their PMEVI variants, obtained by changing calls to the EVI sub-routine to calls to PMEVI. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "On the first experiment, we observe that PMEVI can exploit prior bias knowledge effectively and drastically improve the regret performance, depending on the quality of the prior region. ", "page_idx": 9}, {"type": "text", "text": "On the second experiment however, we observe that without prior knowledge, PMEVI has nearly the same regret performance that its EVI counterparts, meaning that the bias confidence region is too large to effectively improve the regret performance. This observation is first to be taken with caution. Indeed, the regret that is being estimated above is model specific, hence is not an estimate of the minimax regret \u2014 This being said, it undoubtedly shows that the bias confidence region is ineffective and this can be explained as follows. On experiments, we see that most of the regret is due to the early phase of the learning process, where proper bias information is nearly impossible to get. Indeed, the regret is still growing linearly, so no bias information can be inferred. But in addition, this \u201cbad\u201d early data pollutes the bias estimator for a long duration. In other words, while the theoretical regret guarantees of PMEVI-DT are better than its EVI analogues, there is room to improve the bias estimation mechanism and the practical performance. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we have shown that regret guarantees of order $\\sqrt{\\operatorname{sp}(h^{*})S A T\\log(T)}$ can be achieved for weakly communicating MDPs without prior knowledge, nor exponential computational cost. In particular, regret guarantees can scale with the bias span rather than the diameter without prior knowledge. This is in opposition to the recent results that the sample complexity cannot be bounded in term of bias span without prior knowledge for average reward MDPs Tuynman et al. [2024], Wang et al. [2024], Zurek and Chen [2024a,b]. This difference lies in the fact $(\\epsilon,\\delta)$ -PAC algorithm must produce a policy $\\pi_{\\tau}$ after $\\tau$ learning steps where $\\tau$ is a stopping time, with $\\mathbf{P}(g^{\\pi_{\\tau}}\\,\\leq\\,g^{*}\\,-\\,\\epsilon)\\,\\leq\\,\\delta$ Implicitly, these algorithms must hereby certify that the output policy is approximately optimal. In opposition, regret robust algorithms have no need to assess that deployed policies are indeed optimal. ", "page_idx": 9}, {"type": "text", "text": "In the end, the regret advantages of PMEVI-DT over pure EVI-based methods remain theoretical, and the experimental shortcomings displayed in Section 5 leave a few opportunities for future work. Can bias information be inferred more efficiently? Or, do the experiments indicate that the regret analysis of EVI-based methods may be drastically improved? ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Yasin Abbasi-Yadkori, Nevena Lazic, Csaba Szepesvari, and Gellert Weisz. Exploration-enhanced POLITEX. 2019. arXiv:1908.10479. ", "page_idx": 10}, {"type": "text", "text": "Shipra Agrawal and Randy Jia. Optimistic Posterior Sampling for Reinforcement Learning: WorstCase Regret Bounds. Mathematics of Operations Research, 48(1):363\u2013392, 2023. Publisher: INFORMS.   \nJean-Yves Audibert, R\u00e9mi Munos, and Csaba Szepesv\u00e1ri. Exploration\u2013exploitation tradeoff using variance estimates in multi-armed bandits. Theoretical Computer Science, 410(19):1876\u20131902, 2009. Publisher: Elsevier.   \nPeter Auer and Ronald Ortner. Logarithmic Online Regret Bounds for Undiscounted Reinforcement Learning. Proceedings of the 19th International Conference on Neural Information Processing Systems, December 2006.   \nPeter Auer, Nicol\u00f2 Cesa-Bianchi, and Paul Fischer. Finite-Time Analysis of the Multiarmed Bandit Problem. Mach. Learn., 47(2\u20133):235\u2013256, May 2002.   \nPeter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal Regret Bounds for Reinforcement Learning. In Advances in Neural Information Processing Systems, volume 21. Curran Associates, Inc., 2009.   \nKazuoki Azuma. Weighted sums of certain dependent random variables. Tohoku Mathematical Journal, 19(3):357 \u2013 367, 1967. Publisher: Tohoku University, Mathematical Institute.   \nPeter L. Bartlett and Ambuj Tewari. REGAL: a regularization based algorithm for reinforcement learning in weakly communicating MDPs. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI \u201909, pages 35\u201342, Arlington, Virginia, USA, June 2009. AUAI Press.   \nHippolyte Bourel, Odalric Maillard, and Mohammad Sadegh Talebi. Tightening Exploration in Upper Confidence Reinforcement Learning. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 1056\u20131066. PMLR, July 2020.   \nApostolos Burnetas and Michael Katehakis. Optimal Adaptive Policies for Markov Decision Processes. Mathematics of Operations Research - MOR, 22:222\u2013255, February 1997.   \nMichael B. Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs in the current matrix multiplication time, 2020.   \nSarah Filippi, Olivier Capp\u00e9, and Aur\u00e9lien Garivier. Optimism in Reinforcement Learning and Kullback-Leibler Divergence. 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 115\u2013122, September 2010. arXiv: 1004.5229.   \nRonan Fruit. Exploration-exploitation dilemma in Reinforcement Learning under various form of prior knowledge. PhD Thesis, Universit\u00e9 de Lille 1, Sciences et Technologies; CRIStAL UMR 9189, 2019.   \nRonan Fruit, Matteo Pirotta, Alessandro Lazaric, and Ronald Ortner. Efficient Bias-Span-Constrained Exploration-Exploitation in Reinforcement Learning. Proceedings of the 35 th International Conference on Machine Learning, 2018.   \nRonan Fruit, Matteo Pirotta, and Alessandro Lazaric. Improved Analysis of UCRL2 with Empirical Bernstein Inequality. 2020. arXiv:2007.05456.   \nAnders Jonsson, Emilie Kaufmann, Pierre M\u00e9nard, Omar Darwiche Domingues, Edouard Leurent, and Michal Valko. Planning in markov decision processes with gap-dependent sample complexity. Advances in Neural Information Processing Systems, 33:1253\u20131263, 2020.   \nYi Ouyang, Mukul Gagrani, Ashutosh Nayyar, and Rahul Jain. Learning Unknown Markov Decision Processes: A Thompson Sampling Approach. September 2017. arXiv: 1709.04570.   \nPanos M. Pardalos and Georg Schnitger. Checking local optimality in constrained quadratic programming is NP-hard. Operations Research Letters, 7:33\u201335, 1988.   \nMartin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. Wiley Series in Probability and Statistics. Wiley, 1 edition, April 1994. ISBN 978-0-471-61977-2 978-0-470-31688-7.   \nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.   \nMohammad Sadegh Talebi and Odalric-Ambrym Maillard. Variance-Aware Regret Bounds for Undiscounted Reinforcement Learning in MDPs. Journal of Machine Learning Research, pages 1\u201336, April 2018. Publisher: Microtome Publishing.   \nAmbuj Tewari and P. Bartlett. Optimistic Linear Programming gives Logarithmic Regret for Irreducible MDPs. In NIPS, 2007.   \nGeorgios Theocharous, Zheng Wen, Yasin Abbasi-Yadkori, and Nikos Vlassis. Posterior sampling for large scale reinforcement learning. 2017. arXiv:1711.07979.   \nWilliam R Thompson. On the Likelihood that One Probability Exceeds Another in View of the Evidence of Two Samples. Biometrika, 25(3-4):285\u2013294, December 1933. ISSN 0006-3444.   \nAdrienne Tuynman, R\u00e9my Degenne, and Emilie Kaufmann. Finding good policies in average-reward Markov Decision Processes without prior knowledge, May 2024.   \nShengbo Wang, Jose Blanchet, and Peter Glynn. Optimal Sample Complexity for Average Reward Markov Decision Processes, February 2024.   \nChen-Yu Wei, Mehdi Jafarnia Jahromi, Haipeng Luo, Hiteshi Sharma, and Rahul Jain. Modelfree Reinforcement Learning in Infinite-horizon Average-reward Markov Decision Processes. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 10170\u201310180. PMLR, July 2020.   \nZihan Zhang and Xiangyang Ji. Regret Minimization for Reinforcement Learning by Evaluating the Optimal Bias Function. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\u2019 Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \nZihan Zhang and Qiaomin Xie. Sharper Model-free Reinforcement Learning for Average-reward Markov Decision Processes. In The Thirty Sixth Annual Conference on Learning Theory, pages 5476\u20135477. PMLR, 2023.   \nZihan Zhang, Yuan Zhou, and Xiangyang Ji. Almost Optimal Model-Free Reinforcement Learning via Reference-Advantage Decomposition. June 2020. arXiv: 2004.10019 [cs, stat].   \nMatthew Zurek and Yudong Chen. The Plug-in Approach for Average-Reward and Discounted MDPs: Optimal Sample Complexity Analysis, 2024a. arXiv:2410.07616 [cs.LG].   \nMatthew Zurek and Yudong Chen. Span-Based Optimal Sample Complexity for Weakly Communicating and General Average Reward MDPs, 2024b. arXiv:2403.11477 [cs.LG]. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Construction of PMEVI-DT ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "This section provides the technical details required to understand the design of PMEVI-DT in Section 3. We further discuss the assumptions 1-4 appearing in Theorem 5 and provide sufficient conditions so that they are met. ", "page_idx": 12}, {"type": "text", "text": "A.1 Proof of Lemma 3, estimation of the bias error ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Fix $s,s^{\\prime}\\in S$ . We denote $\\alpha_{T}:=N_{T}(s\\leftrightarrow s^{\\prime})(h^{*}(s)-h^{*}(s^{\\prime})-c_{T}(s,s^{\\prime}))$ . We will start by considering the better estimator $c_{T}^{\\prime}(s,s^{\\prime})$ that satisfies the same equation (9) than $c_{T}(s,s^{\\prime})$ but with $\\hat{g}(T)$ changed to $g^{\\ast}$ , readily: ", "page_idx": 12}, {"type": "equation", "text": "$$\nN_{t}(s\\leftrightarrow s^{\\prime})c_{T}^{\\prime}(s,s^{\\prime})=\\sum_{t=0}^{N_{T}(s\\leftrightarrow s^{\\prime})-1}(-1)^{i}\\sum_{t=\\tau_{i}^{s\\leftrightarrow s^{\\prime}}}^{\\tau_{i+1}^{s\\leftrightarrow s^{\\prime}}-1}(g^{*}-R_{t}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "To avoid a typographical clutter, we write $\\tau_{i}$ instead of $\\tau_{i}^{s\\leftrightarrow s^{\\prime}}$ in the remaining of the proof and we write $\\alpha_{T}^{\\prime}:=N_{T}(s\\leftrightarrow s^{\\prime})(h^{*}(s)-h^{*}(s^{\\prime})-c_{T}^{\\prime}(s,s^{\\prime})$ . ", "page_idx": 12}, {"type": "text", "text": "(STEP 1) We start by relating the two estimators. Intuitively, $\\hat{g}(T)$ is a good estimator for $g^{\\ast}$ when the regret is small. Recall that $\\begin{array}{r}{\\hat{g}(T):=\\frac{1}{T}\\sum_{t=0}^{T-1}R_{t}}\\end{array}$ , hence: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{T-1}|\\hat{g}(T)-g^{*}|=\\left|\\sum_{t=0}^{T-1}(R_{t}-g^{*})\\right|=\\left|{\\mathrm{Reg}}(T)\\right|.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Therefore, ", "page_idx": 12}, {"type": "equation", "text": "$$\n|\\alpha_{T}|\\le\\left|\\alpha_{T}^{\\prime}\\right|+\\left|\\alpha_{T}-\\alpha_{T}^{\\prime}\\right|\\le\\left|\\alpha_{T}^{\\prime}\\right|+\\sum_{t=0}^{T-1}|\\widehat{g}(T)-g^{*}|\\le\\left|\\alpha_{T}^{\\prime}\\right|+\\left|\\mathrm{Reg}(T)\\right|.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We are left with upper-bounding $\\left|\\alpha_{T}^{\\prime}\\right|$ . ", "page_idx": 12}, {"type": "text", "text": "(STEP 2) If $i$ is even, then $S_{\\tau_{i}}$ and $S_{\\tau_{i+1}}=s^{\\prime}$ ; otherwise $S_{\\tau_{i}}=s^{\\prime}$ and $S_{\\tau_{i+1}}\\,=\\,s$ . In both cases, we have $h^{\\ast}(S_{\\tau_{i+1}})-h^{\\ast}(S_{\\tau_{i}})=(-1)^{i}(h^{\\ast}(s^{\\prime})-h^{\\ast}(s))$ . Therefore, using Bellman\u2019s equation, the quantity $\\begin{array}{r}{\\mathsf{A}:=\\sum_{t=\\tau_{i}}^{\\tau_{i+1}-1}(g^{*}-R_{t})}\\end{array}$ satisfies: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathrm{A}}=\\sum_{t={\\tau}_{i}}^{{\\tau}_{i+1}-1}\\left(p(X_{t})-e{s}_{s}\\right){h}^{*}+\\sum_{t={\\tau}_{i}}^{{\\tau}_{i+1}-1}(r(X_{t})-R_{t})+\\sum_{t={\\tau}_{i}}^{{\\tau}_{i+1}-1}\\Delta^{*}(X_{t})}\\\\ &{\\quad=\\sum_{t={\\tau}_{i}}^{{\\tau}_{i+1}-1}\\left(e_{S_{t+1}}-e{s}_{s}\\right){h}^{*}+\\sum_{t={\\tau}_{i}}^{{\\tau}_{i+1}-1}\\left(p(X_{t})-e{s}_{t+1}\\right){h}^{*}+\\sum_{t={\\tau}_{i}}^{{\\tau}_{i+1}-1}(r(X_{t})-R_{t})+\\sum_{t={\\tau}_{i}}^{{\\tau}_{i+1}-1}\\Delta^{*}(X_{t})}\\\\ &{\\quad=(-1)^{i}({h}^{*}(s^{\\prime})-{h}^{*}(s))+\\sum_{t={\\tau}_{i}}^{{\\tau}_{i+1}-1}(p(X_{t})-e{s}_{t+1})\\,{h}^{*}+\\sum_{t={\\tau}_{i}}^{{\\tau}_{i+1}-1}(r(X_{t})-R_{t})+\\sum_{t={\\tau}_{i}}^{{\\tau}_{i+1}-1}\\Delta^{*}(X_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Multiplying by $(-1)^{i}$ and rearranging, $\\begin{array}{r}{h^{\\ast}(s^{\\prime})-h^{\\ast}(s)+(-1)^{i+1}\\sum_{t=\\tau_{i}}^{\\tau_{i+1}-1}(g^{\\ast}-R_{t})}\\end{array}$ appears to be equal to: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(-1)^{i+1}\\left(\\sum_{t=\\tau_{i}}^{\\tau_{i+1}-1}\\left((p(X_{t})-e_{S_{t+1}})\\,h^{*}+r(X_{t})-R_{t}\\right)+\\sum_{t=\\tau_{i}}^{\\tau_{i+1}-1}\\Delta^{*}(X_{t})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proceed by summing over $i$ . By triangular inequality, we obtain: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\alpha_{T}^{\\prime}\\big|\\leq\\left|\\sum_{i=0}^{N_{T}(s\\leftrightarrow s^{\\prime})-1}\\sum_{t=\\tau_{i}}^{\\tau_{i+1}-1}(-1)^{i+1}\\left((p(X_{t})-e_{S_{t+1}})\\,h^{*}+r(X_{t})-R_{t}\\right)\\right|+\\sum_{i=0}^{N_{T}(s\\leftrightarrow s^{\\prime})-1}\\sum_{t=\\tau_{i}}^{\\tau_{i+1}-1}\\Delta^{*}(X_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Because all Bellman gaps $\\Delta^{*}$ are non-negative, the second term is upper-bounded by the pseudo-regret $\\begin{array}{r}{\\sum_{t=0}^{T-1}\\Delta^{*}(X_{t})}\\end{array}$ . The first term is a martingale, and the martingale difference sequence $(-1)^{i+1}((p(X_{t})-$ $e_{S_{t+1}})h^{*}+r(X_{t})-R_{t}$ has span at most sp $(h^{*})+1$ since rewards are supported in [0, 1]. Although the number of involved terms is random, it is upper-bounded by $T$ , hence by the maximal version of Azuma-Hoeffding\u2019s inequality (Lemma 32), we have that with probability at least $1-\\delta$ and uniformly for $T^{\\prime}\\leq T$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\sum_{i=0}^{N_{T}(s\\leftrightarrow s^{\\prime})-1}\\sum_{t=\\tau_{i}}^{\\tau_{i+1}-1}(-1)^{i+1}\\left((p(X_{t})-e_{S_{t+1}})\\,h^{*}+r(X_{t})-R_{t}\\right)\\right|\\leq(1+\\mathrm{sp}\\,(h^{*}))\\,\\sqrt{\\frac{1}{2}T\\log\\left(\\frac{2}{\\delta}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "(STEP 3) We conclude that with probability $1-\\delta$ , for all $T^{\\prime}\\leq T$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\alpha_{T^{\\prime}}\\leq(1+\\operatorname{sp}\\left(h^{*}\\right))\\sqrt{\\frac{1}{2}T\\log\\left(\\frac{2}{\\delta}\\right)}+\\sum_{t=0}^{T^{\\prime}-1}\\Delta^{*}(X_{t})+\\left|\\mathrm{Reg}(T^{\\prime})\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We are left with relating both $\\begin{array}{r}{\\sum_{t=0}^{T^{\\prime}-1}\\Delta^{*}(X_{t})}\\end{array}$ and $\\left|\\mathrm{Reg}(T^{\\prime})\\right|$ to $\\begin{array}{r}{\\sum_{t=0}^{T^{\\prime}-1}(\\tilde{g}\\!-\\!R_{t})}\\end{array}$ . Using the Bellman equation again, we find that: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\displaystyle\\sum_{t=0}^{T^{\\prime}-1}(g^{*}-R_{t}-\\Delta^{*}(X_{t}))\\right|\\le|h^{*}(S_{0})-h^{*}(S_{T^{\\prime}})|+\\left|\\displaystyle\\sum_{t=0}^{T^{\\prime}-1}\\left((p(X_{t})-e_{S_{t+1}})\\,h^{*}+(r(X_{t})-R_{t})\\right)\\right|}\\\\ {\\le\\mathrm{sp}\\left(h^{*}\\right)+(1+\\mathrm{sp}\\left(h^{*}\\right))\\,\\sqrt{\\frac{1}{2}T\\log\\left(\\frac{2}{\\delta}\\right)}\\qquad\\qquad\\qquad\\qquad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the last inequality holds with probability $1-\\delta$ uniformly over $T^{\\prime}\\leq T$ by Azuma-Hoeffding\u2019s inequality again (Lemma 32). Remark that if $y-z\\leq x\\leq y+z$ , then $|x|\\leq|y|+|z|$ , hence we conclude that with probability $1-\\delta$ , for all $T^{\\prime}\\leq T$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sum_{t=0}^{T^{\\prime}-1}\\Delta^{*}(X_{t})+\\left|\\mathrm{Reg}(T^{\\prime})\\right|\\leq2\\sum_{t=0}^{T^{\\prime}-1}\\Delta^{*}(X_{t})+(1+\\mathrm{sp}\\,(h^{*}))\\,\\sqrt{\\frac{1}{2}T\\log\\left(\\frac{2}{\\delta}\\right)}+\\mathrm{sp}\\,(h^{*})}\\\\ &{\\phantom{\\sum_{t=0}^{T^{\\prime}-1}}\\leq2\\sum_{t=0}^{T^{\\prime}-1}(g^{*}-R_{t})+3(1+\\mathrm{sp}\\,(h^{*}))\\,\\sqrt{\\frac{1}{2}T\\log\\left(\\frac{2}{\\delta}\\right)}+3\\mathrm{sp}\\,(h^{*})}\\\\ &{\\phantom{\\sum_{t=0}^{T^{\\prime}-1}}\\leq2\\sum_{t=0}^{T^{\\prime}-1}(\\tilde{g}-R_{t})+3(1+\\mathrm{sp}\\,(h^{*}))\\,\\sqrt{\\frac{1}{2}T\\log\\left(\\frac{2}{\\delta}\\right)}+3\\mathrm{sp}\\,(h^{*})}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the last inequality invokes $\\tilde{g}\\geq g^{*}$ . We conclude that, with probability 1 \u22122\u03b4, for all $T^{\\prime}\\leq T$ , we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\nN_{T}(s\\leftrightarrow s^{\\prime})(h^{*}(s)-h^{*}(s^{\\prime})-c_{T}(s,s^{\\prime}))\\leq3\\mathrm{sp}\\left(h^{*}\\right)+\\left(1+\\mathrm{sp}\\left(h^{*}\\right)\\right)\\,\\sqrt{8T\\log\\left(\\frac{2}{\\delta}\\right)}+\\sum_{t=0}^{T^{\\prime}-1}(\\tilde{g}-R_{t}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 13}, {"type": "text", "text": "A.2 The confidence region of PMEVI-DT ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The algorithm PMEVI-DT can be instantiated with a large panel of possibilities, depending on the type of confidence region one is willing to use for rewards and kernels. In this work, we allow for four types of confidence regions, described below. For conciseness, $q\\in\\{r,p\\}$ is a symbolic letter that can be a reward or a kernel and we denote $Q_{t}(s,a)$ the confidence region for $q(s,a)$ at time $t$ . If $q=r$ , then $\\dim(q)=2$ (Bernoulli rewards) with $Q(s,a)=[0,1]$ ; and if $q=p$ , then $\\dim(q)=S$ with $Q(s,a)={\\mathcal{P}}(S)$ . ", "page_idx": 13}, {"type": "text", "text": "(C1) Azuma-Hoeffding or Weissman type confidence regions, with $Q_{t}(s,a)$ taken as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\tilde{q}(s,a)\\in Q(s,a):N_{t}(s,a)\\|\\hat{q}_{t}(s,a)-\\tilde{q}(s,a)\\|_{1}^{2}\\leq\\dim(q)\\log\\left(\\frac{2S A(1+N_{t}(s,a))}{\\delta}\\right)\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "(C2) Empirical Bernstein type confidence regions, with $Q_{t}(s,a)$ taken as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\tilde{q}(s,a)\\in Q(s,a):\\forall i,|\\hat{q}_{t}(i|s,a)-\\tilde{q}(i|s,a)|\\leq\\sqrt{\\frac{2\\mathbf{V}(\\hat{q}_{t}(i|s,a))\\log\\left(\\frac{2\\dim(q)S A T}{\\delta}\\right)}{N_{t}(s,a)}}+\\frac{3\\log\\left(\\frac{2\\dim(q)S A T}{\\delta}\\right)}{N_{t}(s,a)}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "(C3) Empirical likelihood type confidence regions, with $Q_{t}(s,a)$ taken as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\tilde{q}(s,a)\\in Q(s,a):N_{t}(s,a)\\operatorname{KL}(\\hat{q}_{t}(s,a)||\\tilde{q}(s,a))\\leq\\log\\left(\\frac{2S A}{\\delta}\\right)+(\\dim(q)-1)\\log\\left(e\\left(1+\\frac{N_{t}(s,a)}{\\dim q-1}\\right)\\right)\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "(C4) Trivial confidence region with $Q_{t}(s,a)=Q(s,a)$ . ", "page_idx": 13}, {"type": "text", "text": "A few remarks are in order. When rewards are not Bernoulli, only the confidence regions (C1) and (C4) are elligible among the above. Then, Weissman\u2019s inequality must be changed to Azuma\u2019s inequality for $\\sigma$ -sub-Gaussian random variables, see Lemma 34. Since rewards are supported in [0, 1], Hoeffding\u2019s Lemma guarantees that reward distributions are $\\sigma$ -sub-Gaussian with $\\begin{array}{r}{\\sigma=\\frac{1}{2}}\\end{array}$ . ", "page_idx": 13}, {"type": "text", "text": "A.2.1 Correctness of the model confidence region $\\boldsymbol{\\mathcal{M}}_{t}$ and Assumption 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The confidence regions $Q_{t}(s,a)$ described with (C1-4) are tuned so that the following result holds: ", "page_idx": 14}, {"type": "text", "text": "Lemma 11. Assume that, for all $q\\in\\{r,p\\}$ and $(s,a)\\in\\mathcal{X}$ , we choose $Q_{t}(s,a)$ among $(C I{\\bf-}4)$ . Then Assumption $^{\\,l}$ holds. More specifically, the region of models $\\begin{array}{r}{\\mathcal{M}_{t}:=\\prod_{s,a}(\\mathcal{R}_{t}(s,a)\\times\\mathcal{P}_{t}(s,a))}\\end{array}$ satisfies $\\mathbf{P}(\\exists t\\leq T:M\\not\\in M_{t})\\leq\\delta$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. We show that, for all $q\\in\\{r,q\\}$ and $(s,a)\\in X$ , if $Q_{t}(s,a)$ is chosen amoung (C1-4), then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{P}\\left(\\exists t\\leq T:q(s,a)\\notin Q_{t}(s,a)\\right)\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "If $Q_{t}(s,a)$ is chosen with (C1), this is a direct application of Lemma 35; with (C2), this is Lemma 36; with (C3), this is Lemma 37; and with (C4) this is by definition. \u25a1 ", "page_idx": 14}, {"type": "text", "text": "A.2.2 Simultaneous correctness of bias confidence region $\\mathcal{H}_{t}$ , mitigation $\\beta_{t}$ and optimism ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we show that if Assumption 1 holds, then the bias confidence region constructed by PMEVI-DT is correct with high probability, and that the mitigation is not too strong. Recall that $(\\mathfrak{g}_{k},\\mathfrak{h}_{k})$ are the optimistic gain and bias of the policy deployed in episode $k$ (see Algorithm 1). In particular, we have $\\mathfrak{g}_{k}=\\mathfrak{L}_{t_{k}}\\mathfrak{h}_{k}-\\mathfrak{h}_{k}$ with $\\mathfrak{h}_{k}\\in\\mathcal{H}_{t_{k}}$ . We start by a result on the deviation of the variance, which is what the variance approximation Algorithm 5 is based on. Recall that the bias confidence region $\\mathcal{H}_{t}$ is obtained as the collection of constraints: ", "page_idx": 14}, {"type": "text", "text": "(1) prior constraints (if any) $\\mathfrak{h}(s)-\\mathfrak{h}(s^{\\prime})\\leq c_{*}(s,s^{\\prime});$ ;   \n(2) span constraints ${\\mathfrak{h}}(s)-{\\mathfrak{h}}(s^{\\prime})\\leq c_{0}:=T^{1/5}$ ;   \n(3) dynamically inferred constraints $|\\mathfrak{h}(s)-\\mathfrak{h}(s^{\\prime})-c_{t}(s^{\\prime},s)|\\le\\mathrm{error}(c_{t},s^{\\prime},s)$ (see Algorithm 3). ", "page_idx": 14}, {"type": "text", "text": "We start with the technical result that is behind the variance approximation Algorithm 5. ", "page_idx": 14}, {"type": "text", "text": "Lemma 12. Let $u,\\nu\\in\\mathcal{H}_{t}$ and fix $p$ a probability distribution on $s$ . Then for all $s\\in S$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\bf V}(p,u)\\leq{\\bf V}(p,\\nu)+8c_{0}\\sum_{s^{\\prime}\\in S}p(s^{\\prime})\\;\\mathrm{error}(c_{t},s^{\\prime},s).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. We start by establishing the following result: If $p$ is a probability distribution on $s$ and $u,\\nu\\in\\mathbf{R}^{S}$ , we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{V}(p,u)\\leq\\mathbf{V}(p,\\nu)+2\\left(p\\cdot|u-\\nu|\\right)\\operatorname*{max}(u+\\nu)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\cdot$ is the dot product, $u^{2}$ the Hadamard product uu and $|u|$ the vector whose entry $s$ is $|u(s)|$ . (14) is obtained with a straight forward computation: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{V}(p,u)-\\mathbf{V}(p,\\nu)=p\\cdot(u^{2}-\\nu^{2})+(p\\cdot\\nu)^{2}-(p\\cdot u)^{2}}&{}\\\\ {=p\\cdot((u-\\nu)(u+\\nu))+(p\\cdot(u-\\nu))(p\\cdot(u+\\nu))}&{}\\\\ {\\leq p\\cdot(|u-\\nu|\\,(u+\\nu))+(p\\cdot|u-\\nu|)(p\\cdot|u+\\nu|)}&{}\\\\ {\\leq2(p\\cdot|u-\\nu|)\\operatorname*{max}(u+\\nu).}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Observe that $\\nu$ can be changed to $\\nu+\\lambda e$ , where $e$ is the vector full of ones, without changing the result. The same goes for $u$ . We now move to the proof of the main statement. First, translate $u$ and $\\nu$ such that $u(s)=\\nu(s)=0$ . Then, we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p\\cdot(u-\\nu)=\\displaystyle\\sum_{s^{\\prime}\\in S}p(s^{\\prime})\\left|u(s^{\\prime})-u(s)-c_{t}(s^{\\prime},s)+\\nu(s)-\\nu(s^{\\prime})+c_{t}(s^{\\prime},s)\\right|}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\sum_{s^{\\prime}\\in S}p(s^{\\prime})\\left(\\left|u(s^{\\prime})-u(s)-c_{t}(s^{\\prime},s)\\right|+\\left|\\nu(s^{\\prime})-\\nu(s)-c_{t}(s^{\\prime},s)\\right|\\right)}\\\\ &{\\qquad\\qquad\\leq2\\displaystyle\\sum_{s^{\\prime}\\in S}p(s^{\\prime})\\operatorname{error}(c_{t},s^{\\prime},s).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Conclude using that $\\operatorname*{max}(u+\\nu)\\leq\\operatorname*{max}(u)+\\operatorname*{max}(\\nu)+2c_{0}$ for $u,\\nu\\in{\\mathcal{H}}$ such that $u(s)=\\nu(s)=0$ . \u25a1 ", "page_idx": 14}, {"type": "text", "text": "Lemma 13. Assume that Assumption $^{l}$ holds and that $c_{0}\\geq\\operatorname*{sp}\\left(h^{*}\\right)$ . Then, with probability 1 \u22124\u03b4, for all $k\\leq K(T),\\,(l)\\;\\mathfrak{g}_{k}\\geq g^{*}$ and $(2)\\,h^{\\ast}\\in\\mathcal{H}_{t_{k}}$ and (3) for all $(s,a),$ , $(\\hat{p}_{t_{k}}(s,a)-p(s,a))h^{*}\\le\\beta_{t_{k}}(s,a)$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. Let $E_{1}$ the event $(\\forall k\\leq K(T),M\\in\\mathcal{M}_{t_{k}})$ ). Let $E_{2}$ the event stating that, for all $T^{\\prime}\\leq T$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\nN_{T^{\\prime}}(s\\leftrightarrow s^{\\prime})\\left|h^{*}(s)-h^{*}(s^{\\prime})-c_{T^{\\prime}}(s,s^{\\prime})\\right|\\leq3\\mathrm{sp}\\left(h^{*}\\right)+\\left(1+\\mathrm{sp}\\left(h^{*}\\right)\\right)\\sqrt{8T\\log(\\frac{2}{\\delta})}+2\\sum_{t=0}^{T^{\\prime}-1}(g^{*}-R_{t}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and let $E_{3}$ the event stating that, for all $T^{\\prime}\\leq T$ and for all $(s,a)\\in X$ , we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\hat{p}_{T^{\\prime}}(s,a)-p(s,a)\\right)h^{\\ast}\\leq\\,\\sqrt{\\frac{2\\mathbf{V}(\\hat{p}_{T^{\\prime}}(s,a),h^{\\ast})\\log\\left(\\frac{S A T}{\\delta}\\right)}{N_{T^{\\prime}}(s,a)}}+\\frac{3\\mathbf{s}\\ p(h^{\\ast})\\log\\left(\\frac{S A T}{\\delta}\\right)}{N_{T^{\\prime}}(s,a)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By Assumption 1, we have $\\mathbf{P}(E_{1})\\geq1-\\delta$ . By Lemma 3, we have $\\mathbf{P}(E_{2})\\ge1-2\\delta$ and by Lemma 36, we have $\\mathbf{P}(E_{3})\\ge1-\\delta$ , so $\\mathbf{P}(E_{1}\\cap E_{2}\\cap E_{3})\\geq1-4\\delta$ . We prove by induction on $k\\leq K(T)$ that, on $E_{1}\\cap E_{2}\\cap E_{3}$ , (1) $\\mathfrak{g}_{k}\\geq g^{*}$ , (2) $h^{*}\\in\\mathcal{H}_{t_{k}}$ (3) and for all $(s,a)$ , $,(\\widehat{p}_{t_{k}}(s,a)-p(s,a))h^{\\ast}\\leq\\beta_{t_{k}}(s,a)$ , where ${\\mathfrak{g}}_{k}$ is the optimistic gain of the policy deployed at episode $k$ . ", "page_idx": 15}, {"type": "text", "text": "It is obvious for $k=0$ . Indeed, $N_{0}(s\\leftrightarrow s^{\\prime})=0$ for all $s,s^{\\prime}$ hence $c_{0}(s,s^{\\prime})=c_{0}\\geq\\operatorname{sp}\\left(h^{*}\\right)$ . Therefore, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{H}_{0}\\supseteq\\left\\{\\mathfrak{h}\\in\\mathbf{R}^{S}:\\mathfrak{s p}\\left(\\mathfrak{h}\\right)\\leq c_{0}\\right\\}\\supseteq\\left\\{\\mathfrak{h}\\in\\mathbf{R}^{S}:\\mathfrak{s p}\\left(\\mathfrak{h}\\right)\\leq\\mathfrak{s p}\\left(h^{*}\\right)\\right\\}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "so contains $h^{*}$ , proving (2). Moreover, since $N_{0}(s,a)=0$ , we have $\\beta_{0}(s,a)=+\\infty$ , proving (3). Finally, since $M\\in\\mathcal{M}_{0}$ on $E_{1}$ , by the statement (2) of Proposition 2, we have $\\mathfrak{g}_{k}\\geq g^{*}$ , hence proving (1). ", "page_idx": 15}, {"type": "text", "text": "Now assume that $k\\geq1$ . By induction ${\\mathfrak{g}}_{\\ell}\\geq g^{*}$ for all $\\ell<k$ , so on $E_{2}$ we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\check{V}_{t_{k}}(s\\leftrightarrow s^{\\prime})\\left|h^{*}(s)-h^{*}(s^{\\prime})-c_{t_{k}}(s,s^{\\prime})\\right|\\leq3\\operatorname{sp}\\left(h^{*}\\right)+(1+\\operatorname{sp}\\left(h^{*}\\right))\\sqrt{8T\\log(\\frac{2}{\\delta})}+2\\sum_{\\ell=1}^{k-1}\\sum_{t=t_{\\ell}}^{t_{\\ell+1}-1}(\\mathfrak{g}_{\\ell}-R_{t}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By design of $\\mathcal{H}_{t_{k}}$ (see Algorithm 3), we deduce that (2) $h^{*}\\in\\mathcal{H}_{t_{k}}$ . Denote $h_{0}\\in\\mathcal{H}_{t_{k}}$ the reference point used by Algorithm 5. We have, for all $(s,a)\\in X$ , on $E_{1}\\cap E_{2}\\cap E_{3}$ , we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\hat{p}_{t_{k}}(s,a)-p(s,a))\\,h^{*}\\le\\,\\sqrt{\\frac{2\\nabla(\\hat{p}_{t_{k}}(s,a),h^{*})\\log(\\frac{S A T}{\\delta})}{N_{k}(s,a)}}+\\frac{3\\log(h^{*})\\log(\\frac{S A T}{\\delta})}{N_{t_{k}}(s,a)}}\\\\ {(h^{*}\\in\\mathcal{H}_{t_{k}}+\\mathrm{Lemma~12})\\le\\,\\sqrt{\\frac{2\\bigl(\\nabla(\\hat{p}_{t_{k}}(s,a),h_{0})\\log\\bigl(\\frac{S A T}{\\delta}\\bigr)+8c_{0}\\sum_{s^{\\prime}\\in S}\\hat{p}_{t_{k}}(s^{\\prime}|s,a)\\,\\,\\mathrm{error}(c_{t_{k}},s^{\\prime},s)\\bigr)\\log\\bigl(\\frac{S A T}{\\delta}\\bigr)}{N_{t_{k}}(s,a)}}+\\frac{3c_{0}\\log\\bigl(\\frac{S A T}{\\delta}\\bigr)}{N_{t_{k}}(s,a)}}\\\\ {=:\\beta_{t_{k}}(s,a)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "by construction of Algorithm 5. Accordingly, (3) is satisfied. Finally, $M\\mathrm{~\\in~}M_{t_{k}}$ on $E_{1}$ so by Proposition 2, we have (1) $\\mathfrak{g}_{k}\\geq g^{*}$ . \u25a1 ", "page_idx": 15}, {"type": "text", "text": "Corollary 14. Assume that, for all $q\\in\\{r,p\\}$ and $(s,a)\\in X,$ , we choose $Q_{t}(s,a)$ among (C1-4). Then, with probability 1 \u22124\u03b4, for all $k\\,\\in\\,K(T)$ , we have $\\mathfrak{g}_{k}\\,\\geq\\,g^{*}$ and (2) $h^{*}\\in\\mathcal{H}_{t_{k}}$ and (3) for all $(s,a)$ , $(\\hat{p}_{t_{k}}(s,a)-p(s,a))h^{*}\\le\\beta_{t_{k}}(s,a)$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. By Lemma 11, Assumption 1 is satisfied. Apply Lemma 13. ", "page_idx": 15}, {"type": "text", "text": "A.2.3 Sub-Weissman reward confidence region and Assumption 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Although the kernel confidence region can even chosen to be trivial with (C4), in order to work, PMEVI-DT needs the reward confidence region to be sub-Weissman in the following sense: ", "page_idx": 15}, {"type": "text", "text": "Assumption 2. There exists a constant $C>0$ such that for all $(s,a)\\in S$ , for all $t\\leq T$ , we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}_{t}(s,a)\\subseteq\\left\\{\\tilde{r}(s,a)\\in\\mathcal{R}(s,a):N_{t}(s,a)\\left\\Vert\\hat{r}_{t}(s,a)-\\tilde{r}(s,a)\\right\\Vert_{1}^{2}\\leq C\\log\\left(\\frac{2S A(1+N_{t}(s,a))}{\\delta}\\right)\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This is indeed the case if $\\mathcal{R}_{t}(s,a)$ is chosen among (C1-3). ", "page_idx": 15}, {"type": "text", "text": "A.3 Convergence of EVI and Assumption 3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We start with a preliminary lemma on the speed of convergence of EVI. The Lemma 15 is thought to be applied to extended MDPs. Below, when we claim that the action space is compact, we further claim that $a\\in{\\mathcal{A}}(s)\\mapsto p(s,a)$ is a continuous map, so that the Bellman operator is continuous and that $g^{\\ast}$ and $h^{*}$ are well-defined, see Puterman [1994]. ", "page_idx": 15}, {"type": "text", "text": "Lemma 15. Let M a weakly-communicating MDP with finite state space $\\mathbf{R}^{S}$ and compact action space, and let $L$ its Bellman operator. Assume that there exists $\\gamma>0$ such that, $\\forall u\\in\\mathbf{R}^{S}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\forall s\\in S,\\exists a\\in\\mathcal{R}(s),\\quad L u(s)=r(s,a)+p(s,a)u=r(s,a)+\\gamma\\operatorname*{max}(u)+(1-\\gamma)q_{s}^{u}u<0,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with $q_{s}^{u}\\in\\mathcal{P}(S)$ . Then, for all $u\\in\\mathbb{R}^{s}$ and all $\\epsilon>0$ , if s $\\begin{array}{r}{\\displaystyle\\left(L^{n+1}u-L^{n}u\\right)\\geq\\epsilon_{;}}\\end{array}$ , then: ", "page_idx": 16}, {"type": "equation", "text": "$$\nn\\leq2+\\frac{4\\mathrm{sp}\\left(w_{0}\\right)}{\\gamma\\epsilon}+\\frac{2}{\\gamma}\\log\\left(\\frac{2\\mathrm{sp}\\left(w_{0}\\right)}{\\epsilon}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Since $M$ is weakly communicating, has finitely many states and compact action space, it has well-defined gain $g^{*}$ and bias $h^{*}$ functions. Denote $u_{n+1}:=L^{n}u$ . ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{n}:=\\displaystyle\\operatorname*{max}_{\\pi\\in\\Pi}\\left\\{r_{\\pi}+P_{\\pi}u_{n-1}\\right\\}-n g^{*}-h^{*}}\\\\ &{\\quad=\\displaystyle\\operatorname*{max}_{\\pi\\in\\Pi}\\left\\{r_{\\pi}-g^{*}+(P_{\\pi}-I)h^{*}+P_{\\pi}\\left(u_{n-1}-h^{*}-(n-1)g^{*}\\right)\\right\\}=:\\displaystyle\\operatorname*{max}_{\\pi\\in\\Pi}\\left\\{r_{\\pi}^{\\prime}+P_{\\pi}w_{n-1}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Observe that the policy achieving the maximum is the one achieving $u_{n}\\,=\\,r_{\\pi}+P_{\\pi}u_{n-1}$ . Remark that $r_{\\pi}^{\\prime}(s)=-\\Delta^{*}(s,\\pi(s))\\leq0$ is the Bellman gap of the pair $(s,\\pi(s))$ , that we more simply write $\\Delta_{\\pi}$ . For all $n$ , there exists $\\pi_{n}\\in\\Pi$ such that $w_{n+1}=-\\Delta_{\\pi_{n}}+P_{\\pi_{n}}w_{n}$ . Moreover, by assumption, we have $P_{\\pi_{n}}=\\gamma\\cdot e_{s_{n}}^{\\top}e+(1-\\gamma)Q_{n}$ where $Q_{n}$ is a stochastic matrix. Moreover, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\operatorname*{min}(-\\Delta_{\\pi_{n}})+\\gamma w_{n}(s_{n})\\right)e+(1-\\gamma)Q_{n}w_{n}\\leq w_{n+1}\\leq\\left(\\operatorname*{max}(-\\Delta_{\\pi_{n}})+\\gamma w_{n}(s_{n})\\right)e+(1-\\gamma)Q_{n}w_{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence, $\\begin{array}{r}{\\mathrm{sp}\\left(w_{n+1}\\right)\\leq(1-\\gamma)\\mathrm{sp}\\left(w_{n}\\right)+\\mathrm{sp}\\left(\\Delta_{\\pi_{n}}\\right)\\!.}\\end{array}$ . In addition, $w_{n}=L^{n}u-L^{n}h^{*}$ , so by non-expansiveness of $L$ in span semi-norm, $\\operatorname{sp}\\left(w_{n+1}\\right)\\leq\\operatorname{sp}\\left(w_{n}\\right)$ . Overall, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{sp}\\left(w_{n+1}\\right)\\leq\\operatorname*{min}\\left((1-\\gamma)\\mathrm{sp}\\left(w_{n}\\right)+\\mathrm{sp}\\left(\\Delta_{\\pi_{n}}\\right),\\mathrm{sp}\\left(w_{n}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Fix $\\epsilon>0$ , and let $n_{\\epsilon}:=\\operatorname*{inf}\\left\\{n:\\operatorname{sp}\\left(w_{n}\\right)<\\epsilon\\right\\}$ . ", "page_idx": 16}, {"type": "text", "text": "Let $\\pi^{*}$ an optimal policy. We have $w_{n+1}\\,\\geq\\,P_{\\pi^{*}}w_{n}$ so by induction, $w_{n+1}\\,\\geq\\,P_{\\pi^{*}}^{n+1}w_{0}\\,\\geq\\,\\operatorname*{min}(w_{0})e$ . Meanwhile, we see that $\\begin{array}{r}{\\|w_{n}\\|_{1}\\geq\\sum_{k=0}^{n-1}\\left\\|\\Delta_{\\pi_{k}}\\right\\|_{1}+S\\operatorname*{min}(w_{0})}\\end{array}$ , so $\\begin{array}{r}{\\sum_{k=0}^{n-1}\\left\\|\\Delta_{\\pi_{k}}\\right\\|_{1}\\leq\\operatorname{sp}\\left(w_{0}\\right)}\\end{array}$ . Since $\\Delta_{\\pi_{k}}\\le0$ for all $k$ , we have sp $\\left(\\Delta_{\\pi_{k}}\\right)\\leq\\left\\|\\Delta_{\\pi_{k}}\\right\\|_{1}$ so $\\begin{array}{r}{\\sum_{k=0}^{n-1}\\mathrm{sp}\\left(\\Delta_{\\pi_{k}}\\right)\\leq\\mathrm{sp}\\left(w_{0}\\right)}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "By (15), either $\\begin{array}{r}{\\mathrm{~p~}(w_{n+1})\\leq(1-\\frac{1}{2}\\gamma)\\operatorname*{max}(\\epsilon,\\mathrm{sp}\\left(w_{n}\\right))}\\end{array}$ or $\\begin{array}{r}{\\operatorname{sp}\\left(\\Delta_{\\pi_{n}}\\right)\\geq\\frac{1}{2}\\gamma\\epsilon}\\end{array}$ , but because $\\begin{array}{r}{\\sum_{k=0}^{+\\infty}\\operatorname{sp}\\left(\\Delta_{\\pi_{k}}\\right)\\leq}\\end{array}$ $\\mathrm{sp}\\left(w_{0}\\right)$ , the second case can happen at most 2sp\u03b3(\u03f5w0)times. We deduce that, for all n \u2264n\u03f5, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{sp}\\left(w_{n+1}\\right)\\leq\\left(1-\\frac{1}{2}\\gamma\\right)^{n-\\frac{2\\mathrm{sp}\\left(w_{0}\\right)}{\\gamma\\epsilon}}\\mathrm{sp}\\left(w_{0}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In particular, for $n=n_{\\epsilon}-1$ , we get: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\epsilon\\leq\\left(1-\\frac{1}{2}\\gamma\\right)^{n_{\\epsilon}-2-\\frac{2\\mathrm{sp}\\left(w_{0}\\right)}{\\gamma\\epsilon}}\\mathrm{sp}\\left(w_{0}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We obtain: ", "page_idx": 16}, {"type": "equation", "text": "$$\nn_{\\epsilon}\\leq2+\\frac{2\\mathrm{sp}\\left(w_{0}\\right)}{\\gamma\\epsilon}+\\frac{2}{\\gamma}\\log\\left(\\frac{\\mathrm{sp}\\left(w_{0}\\right)}{\\epsilon}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To conclude, check that $\\operatorname{sp}\\left(L^{n+1}u-L^{n}u\\right)=\\operatorname{sp}\\left(w_{n+1}-w_{n}\\right)\\leq2\\operatorname{sp}\\left(w_{n}\\right).$ ", "page_idx": 16}, {"type": "text", "text": "Before moving to the application of interest, remark that this result can be greatly improved if the supremum sup $\\{\\Delta^{*}(s,a):\\Delta^{*}(s,a)<0\\}$ is not zero, to change the dominant term 4sp\u03b3(\u03f5w0) for a constant independent of $\\epsilon$ . ", "page_idx": 16}, {"type": "text", "text": "Corollary 16. Assume that the $\\boldsymbol{\\mathcal{M}}_{t}$ has non-empty interior, and that its Bellman operator satisfies the requirement of Lemma 15, i.e., there exists $\\gamma>0$ such that, $\\forall u\\in\\mathbf{R}^{S}$ , $\\forall s\\in S$ , $\\exists a\\in{\\mathcal{A}}(s)$ , $\\exists\\tilde{r}_{t}(s,a)\\in$ $\\mathcal{R}_{t}(s,a),\\exists\\tilde{p}_{t}(s,a)\\in\\mathcal{P}_{t}(s,a)$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{L}_{t}u(s)=\\tilde{r}_{t}(s,a)+\\tilde{p}_{t}(s,a)u=\\tilde{r}_{t}(s,a)+\\gamma\\operatorname*{max}(u)+(1-\\gamma)q_{s}^{u}u\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for some $q_{s}^{u}\\,\\in\\,\\mathcal{P}(S)$ . Then Assumption $3$ is satisfied, and span fix-points $\\tilde{h}_{t}$ of $\\mathcal{L}_{t}$ are such that $g^{*}(M_{t})=\\mathcal{L}_{t}\\tilde{h}_{t}-\\tilde{h}_{t}$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. If $\\boldsymbol{\\mathcal{M}}_{t}$ is has non-empty interior, it means that for all $(s,a)$ , $\\mathcal{P}_{t}(s,a)$ has non-empty interior. Therefore, for all state-action pair, there exists $\\tilde{p}_{t}(s,a)\\in\\mathcal{P}_{t}(s,a)$ that is fully supported. It follows that $\\boldsymbol{\\mathcal{M}}_{t}$ is communicating, and it follows from standard results Puterman [1994] that its span fix-points $\\tilde{h}$ do exist and that $\\tilde{g}_{t}:=\\mathcal{L}\\tilde{h}_{t}-\\tilde{h}_{t}\\in\\mathbb{R}e$ does not depend on the initial state. ", "page_idx": 17}, {"type": "text", "text": "Moreover, if $\\widetilde{M}\\in\\mathcal{M}_{t}$ and $\\pi\\in\\Pi$ with $\\ensuremath{\\widetilde{g}}_{\\pi}\\equiv g(\\pi,\\ensuremath{M_{t}})\\in\\ensuremath{\\mathbb{R}}e$ , letting $\\tilde{r}_{\\pi}:=r_{\\pi}(\\tilde{M})$ and $\\tilde{P}_{\\pi}:=P_{\\pi}(\\tilde{M})$ , we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\tilde{r}_{\\pi}+\\tilde{p}_{\\pi}\\tilde{h}_{t}\\le\\mathcal{L}_{t}\\tilde{h}_{t}\\le\\tilde{g}_{t}e+\\tilde{h}_{t}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "So by induction and since $\\mathcal{L}_{t}$ is obviously monotone and linear, we show that: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{k=0}^{n}\\tilde{P}_{\\pi}^{k}\\tilde{r}_{\\pi}\\leq n\\tilde{g}_{t}e+(I-\\tilde{P}_{\\pi}^{n})\\tilde{h}_{\\pi}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Dividing by $n$ and letting it go to infinity, we obtain $g(\\pi,\\mathcal{M}_{t})\\leq\\tilde{g}_{t}$ . Observe that we have equility by taking the policy achieving $\\overline{{(\\tilde{g}_{t},\\tilde{h}_{t})}}$ . ", "page_idx": 17}, {"type": "text", "text": "To see that EVI converges indeed, simply observe that Lemma 15 provides a finite bound on how much time is required until the $\\operatorname{sp}\\left(\\mathcal{L}_{t}^{n+\\overline{{1}}}u-\\mathcal{L}_{t}^{n}u\\right)\\leq\\epsilon$ . Hence $\\operatorname{sp}\\left(\\mathcal{L}_{t}^{n+1}u-\\mathcal{L}_{t}^{n}u\\right)$ vanishes to 0. \u25a1 ", "page_idx": 17}, {"type": "text", "text": "About Assumption 3. The assumptions made by Corollary 16 are met if the kernel confidence regions are: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Built out of Weissman\u2019s inequality (C1) (see the next section, also Auer et al. [2009]); \u2022 Built out of Bernstein\u2019s inequality (C2) (because the maximization algorithm to compute $\\tilde{p}_{t}(s,a)u_{i}$ in EVI has the same greedy properties than with Weissman\u2019s inequality); \u2022 Trivial (C4). ", "page_idx": 17}, {"type": "text", "text": "For confidence regions build with empirical likelihood estimates (C3), there is no guarantee of convergence (although we conjecture that one could be established), although the gain is still welldefined because $\\boldsymbol{\\mathcal{M}}_{t}$ remains communicating. However, just like the original work of Filippi et al. [2010], the convergence is always met numerically. ", "page_idx": 17}, {"type": "text", "text": "A.4 Proof of Theorem 5: Complexity of PMEVI with Weissman confidence regions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we show that when one is using Weissman confidence regions for kernels (C1), then the iterates of $\\mathcal{L}_{t}$ converge to an $\\epsilon$ span-fix-point quickly. ", "page_idx": 17}, {"type": "text", "text": "Proposition 17. Assume that PMEVI-DT uses kernel confidence regions of Weissman type $(C I)$ satisfying Assumption 1. Then with probability $1-\\delta$ , the number of iterations of PMEVI (see Algorithm 2) is $\\mathrm{O}\\big(D\\sqrt{S}A T\\big),$ , hence the algorithm has polynomial per-step amortized complexity. ", "page_idx": 17}, {"type": "text", "text": "Proof. With Weissman type confidence regions for kernels, for all $t\\leq T$ and $(s,a)\\in X$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{P}_{t}(s,a)\\geq\\left\\{\\tilde{p}(s,a)\\in\\mathcal{P}(s,a):\\|\\tilde{p}(s,a)-\\hat{p}_{t}(s,a)\\|_{1}\\leq\\,\\sqrt{\\frac{S\\log(2S A T)}{T}}\\right\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It follows that, for all $t\\,\\leq\\,T$ , the extended Bellman operator $\\mathcal{L}_{t}$ satisfies the prerequisite $(*)$ of Lemma 15 with ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\gamma=\\frac{1}{2}\\,\\sqrt{\\frac{S\\,\\log(2S A T/\\delta)}{T}}=\\Omega\\left(\\sqrt{\\frac{S\\,\\log(T/\\delta)}{T}}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Under Assumption 1, we have $M\\in\\mathcal{M}_{t}$ with probability $1-\\delta$ . Under this event, $\\boldsymbol{\\mathcal{M}}_{t}$ is weakly communicating and sp $(h^{*}(M_{t}))\\leq D(M)$ , we can apply Lemma 15 and conclude that every calls to PMEVI (Algorithm 2) takes ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{O}\\left(\\frac{\\operatorname{sp}\\left(w_{0}\\right)\\sqrt{T}}{\\epsilon\\sqrt{\\frac{\\operatorname{Slog}(T/\\delta)}{T}}}\\right)=\\mathrm{O}\\left(\\frac{D T}{\\sqrt{S}\\log(T)}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we use that log(S TAT/\u03b4), that sp (w0) = O  sp (h\u2217(Mt)) = O(D(M)) and that \u03b4 \u2265 T1 . Since the number of episodes under the doubling trick (DT) is $\\mathsf{O}(S A\\log(T))$ , we conclude accordingly. \u25a1 ", "page_idx": 17}, {"type": "text", "text": "Every call to the projection operator solves a linear program. Although in theory, this time is polynomial (relying on recent work on the complexity of LP such as Cohen et al. [2020], it is the current matrix multiplication time ${\\mathrm{O}}(S^{2.38}))$ , in practice, reducing the number of calls to the projection operator is key to run PMEVI-DT in reasonable time. ", "page_idx": 18}, {"type": "text", "text": "B Analysis of the projected mitigated Bellman operator ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we fix the model region $\\mathcal{M}$ , the bias region $\\mathcal{H}$ and the mitigation vector $_\\beta$ , dropping the sub-script $t$ for conciseness. We denote $\\hat{r},\\hat{p}$ the respective empirical reward and kernel. Further assume that $\\mathcal{H}=\\mathcal{H}_{0}+\\mathbf{R}e$ with $\\mathcal{H}_{0}$ a compact convex set. The associated projection operation (see Appendix B.2) is denoted $\\Gamma$ . The (vanilla) extended Bellman operator $\\mathcal{L}$ associated to $\\mathcal{M}$ is given by $\\begin{array}{r}{\\mathcal{L}u(s):=\\operatorname*{max}_{a\\in\\mathcal{A}(s)}\\left\\{\\operatorname*{sup}\\mathcal{R}(s,a)+\\operatorname*{sup}\\mathcal{P}(s,a)u\\right\\}}\\end{array}$ . The $\\beta$ -mitigated extended Bellman operator associated to $\\mathcal{M}$ is: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{L}^{\\beta}u(s):=\\operatorname*{max}_{a\\in\\mathcal{R}(s)}\\operatorname*{sup}_{\\tilde{r}(s,a)\\in\\mathcal{R}(s,a)}\\operatorname*{sup}_{\\tilde{p}(s,a)\\in\\mathcal{P}(s,a)}\\Big\\{\\tilde{r}(s,a)+\\operatorname*{min}\\{\\tilde{p}(s,a)u_{i},\\hat{p}(s,a)u_{i}+\\beta(s,a)\\}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The function Greedy $(\\mathcal{M},u,\\beta)$ returns a stationary deterministic policy that picks its actions among the one reaching the maximum above. The projection of $\\mathcal{L}^{\\beta}$ to $\\mathcal{H}$ is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathfrak{L}\\equiv\\mathfrak{L}^{\\beta,\\mathcal{H}}:=\\Gamma\\circ\\mathcal{L}^{\\beta}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The goal of this section is to establish Proposition 2 and ", "page_idx": 19}, {"type": "text", "text": "\u2022 Proposition 2 statement (1) is a consequence of Lemma 22;   \n\u2022 Proposition 2 statement (2) follows from Theorem 25;   \n\u2022 Proposition 2 statement (3) follows from Corollary 27;   \n\u2022 Proposition 2 statement (4) follows from Corollary 21;   \n\u2022 Proposition 2 prerequisites on the projection operator and Lemma 4 follows from Lemma 19. ", "page_idx": 19}, {"type": "text", "text": "B.1 Finding an optimistic policy under bias constraints ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The main goal is to find and optimistic policy under bias constraints (projection) and bias error constraints (mitigation). The bias constraints imply that we search for a policy $\\pi$ together with a model $\\widetilde{M}$ such that $h^{\\pi}(\\widetilde{M})\\in\\mathcal{H}$ . The bias error means that, for ${\\tilde{h}}\\,\\equiv\\,h^{\\pi}({\\widetilde{\\cal M}})$ , we want in addition $\\tilde{p}(s,\\pi(s))\\tilde{h}\\le\\hat{p}(s,\\pi(s))\\tilde{h}+\\beta(s,\\pi(s))$ where $\\tilde{p}$ is the transition kernel of $\\widetilde{M}$ . In the end, our goal is to track the solution of the following optimization problem: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{g^{*}(\\mathcal{H},\\beta,\\mathcal{M}):=\\operatorname*{sup}\\left\\{g^{\\pi}(\\widetilde{M}):\\begin{array}{c}{\\pi\\in\\Pi,\\widetilde{M}\\in\\mathcal{M},}\\\\ {\\forall s\\in S,\\ \\widetilde{p}(s,\\pi(s))\\widetilde{h}\\leq\\widehat{p}(s,\\pi(s))\\widetilde{h}+\\beta(s,\\pi(s)),}\\\\ {\\widetilde{h}\\equiv h^{\\pi}(\\widetilde{M})\\in\\mathcal{H},\\ \\mathrm{sp}(g^{\\pi}(\\widetilde{M}))=0}\\end{array}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the supremum is taken with respect to the product order $\\mathbf{R}^{S}$ . In particular, if $\\mathcal{U}\\subseteq\\mathcal{R}^{S}$ , check that $u^{*}=\\operatorname*{sup}\\mathcal{U}$ is obtained as $u^{*}(s):=\\operatorname*{sup}\\left\\{\\nu(s):\\nu\\in\\mathcal{U}\\right\\}$ . The constraint $\\operatorname{sp}(g^{\\pi}({\\widetilde{M}}))=0$ is suggested by the work of Fruit et al. [2018], Fruit [2019] and is key for the problem to be solvable. ", "page_idx": 19}, {"type": "text", "text": "The bias constraint and the constraint involving $\\beta$ make the problem impossible to handle with a \u201cpure\u201d extended MDP solution, which is why the extended Bellman operators are mitigated (with $\\beta)$ then projected (with \u0393). The mitigation operation guarantees that the constraint involving $_\\beta$ is satisfied, while the projection on $\\mathcal{H}$ makes sure that the bias constraint is satisfied. It is important for both operations to be compatible, i.e., that the constraint involving $_\\beta$ that $\\mathcal{L}^{\\beta}$ forces is not lost when applying $\\Gamma$ . As a matter of fact, projecting then mitigating would not work. ", "page_idx": 19}, {"type": "text", "text": "We now explain why $\\mathfrak{L}$ can be used to solve (18). ", "page_idx": 19}, {"type": "text", "text": "B.2 Projection operation and definition of $\\mathfrak{L}$ ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We start by discussing why $\\mathfrak{L}$ is well-defined at all. The well-definition of $\\mathcal{L}^{\\beta}$ is obvious. The point is to explain why the projection onto $\\mathcal{H}$ is possible while preserving mandatory structural properties such as monotony, non-expansivity, linearity and more. For general $\\mathcal{H}$ , such properties are impossible to meet. But the bias confidence region constructed with Algorithm 3 has a specific shape that makes the projection possible. The central property is the one below: ", "page_idx": 19}, {"type": "text", "text": "(A1) The downward closure $\\{\\nu\\leq u:\\nu\\in\\mathcal{H}\\}$ of every $u\\in\\mathbb{R}^{s}$ has a maximum in $\\mathcal{H}$ ", "page_idx": 19}, {"type": "text", "text": "The only order that we will be considering is the product order on $\\mathbf{R}^{S}$ . Recall that a set $\\mathcal{U}\\subseteq\\mathbb{R}^{S}$ has a maximum if there exists $u\\,\\in\\,\\mathcal{U}$ such that $\\nu~\\leq~u$ for all $u\\,\\in\\,\\mathcal{U}$ . A supremum of $\\mathbf{\\nabla}\\mathcal{U}$ is a minimal upper-bound of $\\mathcal{U}$ , i.e., $u$ such that (1) $\\nu\\leq u$ for all $\\nu\\in\\mathcal{U}$ and (2) no $w$ satisfying (1) can be smaller than $u$ . For the product order, the supremum of a subset $\\mathbf{\\nabla}\\mathcal{U}$ is unique and of the form $u(s)=\\operatorname*{sup}\\left\\{\\nu(s):\\nu\\in{\\mathcal{U}}\\right\\}$ . ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Define the projection $\\Gamma:\\mathbf{R}^{S}\\rightarrow\\mathcal{H}$ as such: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Gamma u:=\\operatorname*{max}\\left\\lbrace\\nu\\leq u:\\nu\\in{\\mathcal{H}}\\right\\rbrace.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In general, Assumption (A1) is satisfied when $\\mathcal{H}$ admits a join, i.e., is stable by finite supremum: $u,\\nu\\in{\\mathcal{H}}\\Rightarrow\\operatorname*{sup}(u,\\nu)\\in{\\mathcal{H}}$ . ", "page_idx": 20}, {"type": "text", "text": "Lemma 18. If $\\mathcal{H}$ is generated by constraints of the form $\\mathfrak{h}(s)-\\mathfrak{h}(s^{\\prime})-c(s,s^{\\prime})\\leq d(s,s^{\\prime}),$ , then it has a join and (A1) is satisfied. Moreover, $\\Gamma$ is then correctly computed with Algorithm 4. ", "page_idx": 20}, {"type": "text", "text": "Proof. The first half of the result is well-known, see Zhang and Xie [2023], but we recall a proof for self-containedness. Let $\\nu_{1},\\nu_{2}\\in\\mathcal{H}$ and define $\\nu_{3}:=\\operatorname*{sup}(\\nu_{1},\\nu_{2})$ . Observe that $\\nu_{3}(s)-\\nu_{3}(s^{\\prime})\\leq$ $\\operatorname*{max}(\\nu_{1}(s)-\\nu_{1}(s^{\\prime}),\\nu_{2}(s)-\\nu_{2}(s^{\\prime}))\\leq c(s,s^{\\prime})+d(s,s^{\\prime})$ . $\\operatorname{So}\\nu_{3}\\in\\mathcal{H}$ . ", "page_idx": 20}, {"type": "text", "text": "We continue by showing that if $\\mathcal{H}$ has a join, then (19) is well-defined. For $s\\in S$ , take a sequence $\\nu_{n}^{s}$ such that $\\nu_{n}^{s}(s)\\,\\to\\,\\alpha(s)\\,:=\\,\\operatorname*{sup}\\left\\{\\nu(s):\\nu\\leq u,\\nu\\in{\\mathcal{H}}\\right\\}$ . Because the span of every element of $\\mathcal{H}$ is upper-bounded by $c:=$ sup ${\\big\\{}\\operatorname{sp}\\left(\\nu\\right):\\nu\\in{\\mathcal{H}}{\\big\\}}$ , it follows that $\\nu_{n}^{s}$ evolves in the compact region $\\{\\nu\\leq u:\\nu\\in\\mathcal{H}\\}\\cap\\left\\{\\nu:\\|\\nu-\\alpha s e\\|_{\\infty}=1+c\\right\\}$ . We can therefore extract a convergent sequence of $\\nu_{n}^{s}$ , converging $\\nu_{*}^{s}$ that belongs to $\\mathcal{H}$ since the latter is closed. By construction, $\\nu_{*}^{s}(s)=\\alpha(s)$ . Because $\\mathcal{H}$ has a join, $\\nu_{*}:=\\operatorname*{sup}\\left\\lbrace\\nu_{*}^{s}:s\\in S\\right\\rbrace\\in\\mathcal{H}$ . \u25a1 ", "page_idx": 20}, {"type": "text", "text": "Lemma 19. Under assumption (A1), the operator $\\Gamma u:=\\operatorname*{max}\\left\\lbrace\\nu\\leq u:\\nu\\in\\mathcal{H}\\right\\rbrace$ is well-defined, and is: ", "page_idx": 20}, {"type": "text", "text": "(1) monotone: $u\\leq\\nu\\Rightarrow\\Gamma u\\leq\\Gamma\\nu_{}$ ;   \n(2) non span-expansive: sp $(\\Gamma u-\\Gamma\\nu)\\leq\\mathrm{sp}\\,(u-\\nu),$ ;   \n(3) linear: $\\Gamma(u+\\lambda e)=\\Gamma u+\\lambda e$ ;   \n(4) $\\Gamma u\\leq u.$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. The well-definition of $\\Gamma$ is obvious from (A1). For (2), if $u\\ \\leq\\ \\nu$ then $w\\,\\leq\\,u\\,\\Rightarrow\\,w\\,\\leq\\,\\nu$ Hence $\\Gamma u:=\\operatorname*{max}\\left\\{w\\leq u:w\\in{\\mathcal{H}}\\right\\}\\leq\\operatorname*{max}\\left\\{w\\leq\\nu:w\\in{\\mathcal{H}}\\right\\}=:\\Gamma\\nu$ . For (3), check that it follows from $\\mathcal{H}=\\mathcal{H}+\\mathbb{R}e$ . For (4), we obviously have $\\Gamma u:=\\operatorname*{max}\\left\\{\\nu\\leq u:\\nu\\in{\\mathcal{H}}\\right\\}\\leq u$ . ", "page_idx": 20}, {"type": "text", "text": "The more difficult point is (2) span non-expansivity. Pick $u,\\nu\\in\\mathbb{R}^{S}$ . By linearity, it suffices to show the result for $\\begin{array}{r}{\\sum_{s}u(s)=\\sum_{s}\\nu(s)}\\end{array}$ . In that case, we have sp $(\\nu-u)=\\operatorname*{max}(\\nu-u)+\\operatorname*{max}(u-\\nu)$ . Observe that for all $w\\leq u$ , we have $w+\\operatorname*{min}(\\nu-u)e\\leq\\nu$ . Since $\\mathcal{H}=\\mathcal{H}+\\mathbb{R}e$ , it follows that: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left\\{w\\leq u:u\\in\\mathcal{H}\\right\\}\\leq\\operatorname*{max}\\left\\{w\\leq\\nu:w\\in\\mathcal{H}\\right\\}+\\operatorname*{max}(u-\\nu)e.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Similarly, we have ma $\\mathrm{x}\\left\\{w\\leq u:w\\in{\\mathcal{H}}\\right\\}\\geq\\operatorname*{max}\\left\\{w\\leq\\nu:w\\in{\\mathcal{H}}\\right\\}+\\operatorname*{min}(\\nu-u)e.$ Using them both at once, we find sp $(\\Gamma u-\\Gamma\\nu)\\leq\\mathrm{sp}\\left(\\nu-u\\right)$ . \u25a1 ", "page_idx": 20}, {"type": "text", "text": "The properties (1), (3) and (4) are essential for $\\mathfrak{L}$ to properly address the optimization problem (18). The property (2) is just as important, because it plays a central part in the convergence of value iteration. The next result shows similar properties for the $\\beta$ -mitigated extended Bellman operator $\\mathcal{L}^{\\beta}$ . From now on, we will assume (A1), because it is almost-surely satisfied by the bias confidence region generated by Algorithm 3. ", "page_idx": 20}, {"type": "text", "text": "Lemma 20. The $\\beta$ -mitigated extended Bellman operator $\\mathcal{L}^{\\beta}$ is $(I)$ monotone, (2) non-span-expansive and (3) linear. ", "page_idx": 20}, {"type": "text", "text": "Proof. The properties (1) and (3) directly follow from the definition. We focus on (2). Fix $u,u^{\\prime}\\in\\mathbb{R}^{S}$ . By Lemma 26, we can write $\\mathcal{L}^{\\beta}u\\,=\\,\\tilde{r}_{\\pi}\\,^{\\circ}+\\tilde{P}_{\\pi}u$ and $\\mathcal{L}^{\\beta}u^{\\prime}=\\tilde{r}_{\\pi^{\\prime}}+\\tilde{P}_{\\pi^{\\prime}}u^{\\prime}$ . In the following, we write $\\beta_{\\pi}(s):=\\beta(s,\\pi(s))$ . Check that: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}^{\\beta}u-\\mathcal{L}^{\\beta}u^{\\prime}=\\tilde{r}_{\\pi}+\\tilde{P}_{\\pi}u-\\left(\\tilde{r}_{\\pi^{\\prime}}+\\tilde{P}_{\\pi^{\\prime}}u^{\\prime}\\right)\\leq\\tilde{r}_{\\pi}+\\tilde{P}_{\\pi}u-\\left(\\tilde{r}_{\\pi}+\\operatorname*{min}\\left\\{\\tilde{P}_{\\pi}u^{\\prime},\\hat{P}_{\\pi}u^{\\prime}+\\beta_{\\pi}\\right\\}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "If the minimum is reached with $\\tilde{P}_{\\pi}u^{\\prime}$ , then: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{L}^{\\beta}u-\\mathcal{L}^{\\beta}u^{\\prime}\\leq\\tilde{P}_{\\pi}(u-u^{\\prime}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "If the minimum is reached with $\\hat{P}_{\\pi}u^{\\prime}+\\beta_{\\pi}$ , then upper-bound $\\tilde{P}_{\\pi}u$ by $\\hat{P}_{\\pi}u+\\beta_{\\pi}$ to obtain: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{L}^{\\beta}u-\\mathcal{L}^{\\beta}u^{\\prime}\\leq\\hat{P}_{\\pi}(u-u^{\\prime}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Overall, we find that there exists $Q_{\\pi}\\in\\mathcal{P}_{\\pi}$ such that $\\mathcal{L}^{\\beta}u-\\mathcal{L}^{\\beta}u^{\\prime}\\leq Q_{\\pi}(u-u^{\\prime})$ . Similarly, we find $Q_{\\pi^{\\prime}}\\in\\mathcal{P}_{\\pi^{\\prime}}$ such that $\\mathcal L^{\\beta}u-\\mathcal L^{\\beta}u^{\\prime}\\geq Q_{\\pi^{\\prime}}(u-u^{\\prime})$ . We conclude that: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname{sp}\\left(\\mathcal{L}^{\\beta}u-\\mathcal{L}^{\\beta}u^{\\prime}\\right)\\leq\\operatorname{sp}\\left((Q_{\\pi}-Q_{\\pi^{\\prime}})(u-u^{\\prime})\\right)\\leq\\operatorname{sp}\\left(u-u^{\\prime}\\right)\\!.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 21}, {"type": "text", "text": "By composition, we obtain the following result. ", "page_idx": 21}, {"type": "text", "text": "Corollary 21. $\\mathfrak{L}$ is $(I)$ monotone, (2) non-span-expansive and (3) linear. Moreover, s $\\mathsf{p}\\left(\\mathfrak{L}u-\\mathfrak{L}\\nu\\right)\\leq$ sp $\\mathscr{\\left(L u-\\mathscr{L}\\nu\\right)}$ for all $u,\\nu\\in\\mathbb{R}^{S}$ . ", "page_idx": 21}, {"type": "text", "text": "B.3 Fix-points of $\\mathfrak{L}$ and (weak) optimism ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Lemma 22. L has a fix-point in span semi-norm, i.e., \u2203u \u2208H, sp (Lu \u2212u) = 0. ", "page_idx": 21}, {"type": "text", "text": "Proof. The idea is to apply Brouwer\u2019s fix-point theorem in $\\mathbf{R}^{S}$ quotiented by the equivalence relation $u\\sim\\nu\\Leftrightarrow\\operatorname{sp}\\left(u-\\nu\\right)=0$ , where sp $(-)$ becomes a norm. By linearity (Corollary 21), $\\mathfrak{L}$ is well-defined in this quotient space, and if $\\mathfrak{L}$ is shown continuous on $\\bar{\\mathbf{R}^{S}}$ , so will it be on the quotient. ", "page_idx": 21}, {"type": "text", "text": "We show that $\\mathfrak{L}$ is sequentially continuous on $\\mathcal{H}$ . Consider a sequence $u_{n}\\,\\in\\,\\mathcal{H}^{\\mathbf{N}}$ converging to $u\\,\\in\\,{\\mathcal{H}}$ and fix $\\epsilon\\,>\\,0$ . Provided that $n\\,>\\,N_{\\epsilon}$ for $N_{\\epsilon}$ large enough, we have $\\|u_{n}-u\\|_{\\infty}\\,<\\,\\epsilon$ , i.e., $u_{n}\\,-\\,\\epsilon e\\,\\,\\le\\,\\,u_{n}\\,\\,\\le\\,u\\,+\\,\\epsilon e.$ . Therefore, in the one hand, for all $\\nu~\\leq~u_{n}$ , we have $\\nu\\,-\\,\\epsilon e\\,\\,\\leq\\,\\,u$ so max $\\{\\nu\\leq u_{n}:\\nu\\in\\mathcal{H}\\}\\leq\\operatorname*{max}\\left\\{\\nu\\leq u:\\nu\\in\\mathcal{H}\\right\\}+\\epsilon e$ ; And on the other hand, for all $\\nu\\leq u$ , $\\nu+\\epsilon e\\leq u_{n}$ so max $\\left\\{\\nu\\leq u:\\nu\\in{\\mathcal{H}}\\right\\}\\leq\\operatorname*{max}\\left\\{\\nu\\leq u_{n}:\\nu\\in{\\mathcal{H}}\\right\\}+\\epsilon e.$ Hence: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|\\operatorname*{max}\\left\\{\\nu\\leq u:\\nu\\in{\\mathcal{H}}\\right\\}-\\operatorname*{max}\\left\\{\\nu\\leq u_{n}:\\nu\\in{\\mathcal{H}}\\right\\}\\right\\|\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "It shows that $\\Gamma$ is continuous. The operator $\\mathcal{L}^{\\beta}$ is obviously continuous as well, so $\\mathfrak{L}=\\Gamma\\circ\\mathcal{L}^{\\beta}$ is continuous by composition. Since $\\mathcal{H}=\\mathcal{H}_{0}+\\mathbf{R}e$ with $\\mathcal{H}_{0}$ compact and ocnvex, the quotient $\\mathcal{H}/\\sim$ is compact and convex, and is preserved by $\\mathfrak{L}/\\sim$ . By Brouwer\u2019s fix-point theorem, $\\mathfrak{L}/\\sim$ has a fix-point in $\\mathcal{H}/\\sim$ . So $\\mathfrak{L}$ has a span fix-point in $\\mathcal{H}$ . \u25a1 ", "page_idx": 21}, {"type": "text", "text": "We write $\\mathrm{Fix}({\\mathfrak{L}})$ the span fix-points of $\\mathfrak{L}$ . ", "page_idx": 21}, {"type": "text", "text": "Lemma 23. L has well-defined growth. Specifically, if $\\mathfrak{L}u=u+\\mathfrak{g}e$ , then: ", "page_idx": 21}, {"type": "text", "text": "(1) There exists $c>0$ , s.t., for all $\\nu\\in\\mathcal{H}_{0},\\,(n\\mathfrak{g}-c)e+u\\leq\\mathfrak{L}^{n}\\nu\\leq(n\\mathfrak{g}+c)e+u;$ (2) $f\\!f u^{\\prime}\\in{\\mathrm{Fix}}(\\mathfrak{L}),$ , then $\\mathfrak{L}u^{\\prime}-u^{\\prime}=\\mathfrak{g}e$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. Setting $c:=\\operatorname*{max}_{\\nu\\in\\mathcal{H}_{0}}\\left\\|\\nu-u\\right\\|_{\\infty}<\\infty$ , one can check that $u\\!-\\!c e\\leq\\nu\\leq u\\!+\\!c e$ for all $\\nu\\in\\mathcal{H}_{0}$ . this proves (1) for $n=0$ and we then proceed by induction on $n\\geq0$ . By induction, $\\mathfrak{L}^{n}\\nu\\leq u+(n\\mathfrak{g}+c)e$ and by Corollary 21, $\\mathfrak{L}$ is monotone, so we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathfrak{L}^{n+1}\\nu\\leq\\mathfrak{L}\\mathfrak{L}^{n}\\nu\\leq\\mathfrak{L}(u+(n\\mathfrak{g}+c)e)=u+((n+1)\\mathfrak{g}+c)e\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last inequality use the linearity of $\\mathfrak{L}$ together with $\\mathfrak{L}u=u+\\mathfrak{g}e$ . The lower bound of $\\mathfrak{L}^{n}\\nu$ is shown similarly, establishing (1). ", "page_idx": 21}, {"type": "text", "text": "For (2), pick $u^{\\prime}\\in\\operatorname{Fix}(\\mathfrak{L})$ with $\\mathfrak{L}u^{\\prime}=u^{\\prime}+\\mathfrak{g}^{\\prime}e$ . Up to translating $u^{\\prime}$ , we can assume that $u^{\\prime}\\in\\mathcal{H}_{0}$ and apply (1). We get: ", "page_idx": 21}, {"type": "equation", "text": "$$\n(n\\mathfrak{g}-c)e+u\\leq n\\mathfrak{g^{\\prime}}e+u^{\\prime}\\leq(n\\mathfrak{g}+c)e+u.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Divided by $n$ and let it go to infinity. We conclude that ${\\mathfrak{g}}={\\mathfrak{g}}^{\\prime}$ . ", "page_idx": 21}, {"type": "text", "text": "We finally have everything in hand to claim that $\\mathfrak{L}$ solves (18). ", "page_idx": 21}, {"type": "text", "text": "Corollary 24. The growth of $\\mathfrak{L}$ given by ${\\mathfrak{g}}={\\mathfrak{L}}u-u$ for $u\\in\\mathrm{Fix}(\\mathfrak{L})$ is well-defined, and: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\forall u\\in\\mathcal{H},\\quad\\mathfrak{g}e=\\operatorname*{lim}_{n\\to\\infty}\\operatorname*{inf}_{n}\\frac{\\mathfrak{L}^{n}u}{n}=\\operatorname*{lim}_{n\\to\\infty}\\operatorname*{sup}_{n}\\frac{\\mathfrak{L}^{n}u}{n}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Moreover, $\\mathfrak{g}\\geq g^{*}(\\mathcal{H},\\beta,\\mathcal{M})$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. The growth property is a direct consequence of Lemma 23. We show ${\\mathfrak{g}}\\geq g^{*}({\\mathcal{H}},\\beta,{\\mathcal{M}})$ which is defined in (18). Pick $\\dot{\\pi_{\\L}}\\in\\Pi,\\widetilde{M}\\in\\mathcal{M}$ its model with $\\tilde{h}\\,\\equiv\\,h(\\pi,\\widetilde{M})$ and $\\tilde{P}_{\\pi}\\tilde{h}\\,\\le\\,\\tilde{P}_{\\pi}\\tilde{h}+\\beta_{\\pi}$ where $\\beta_{\\pi}(s):=\\beta(s,\\pi(s))$ . Up to translation, we can assume that $\\tilde{h}\\in\\mathcal{H}_{0}$ . ", "page_idx": 22}, {"type": "text", "text": "We have $g(\\pi,\\widetilde{M})=\\widetilde{g}e$ for $\\tilde{g}\\in\\mathbb{R}$ , so ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\tilde{h}+\\tilde{g}e=\\tilde{r}_{\\pi}+\\tilde{P}_{\\pi}\\tilde{h}\\leq\\tilde{z}\\tilde{h}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "by definition. By monotony of $\\mathfrak{L}$ , see Corollary 21, $n\\widetilde{g}e+\\widetilde{h}\\le\\mathfrak{L}^{n}\\widetilde{h}$ follows by induction on $n\\geq0$ . By Lemma 23, we further have $\\mathfrak{L}^{n}\\tilde{h}\\leq n(\\mathfrak{g}+c)e\\dot{+}\\,u$ where $u\\in\\mathrm{Fix}(\\mathfrak{L})$ . In tandem, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\tilde{g}e\\leq\\mathfrak{g}e+\\frac{c e+u-\\tilde{h}}{n}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Letting $n\\to\\infty$ , we deduce that ${\\tilde{g}}\\leq{\\mathfrak{g}}$ . Conclude by taking the best $\\pi$ and $\\widetilde{M}$ . ", "page_idx": 22}, {"type": "text", "text": "The next theorem follows directly with the same proof technique, and guarantees optimism. ", "page_idx": 22}, {"type": "text", "text": "Theorem 25. Assume that $g^{*}+h^{*}\\leq\\mathfrak{L}h^{*}$ . Then ${\\mathfrak{g}}\\geq g^{*}$ . ", "page_idx": 22}, {"type": "text", "text": "The condition ${}^{\\bullet\\bullet}g^{\\ast}+h^{\\ast}\\leq\\mathfrak{L}h^{\\ast}{}^{\\bullet\\bullet}$ can be referred to as a weak form of optimism. We qualify this version of optimism as weak because it is much weaker than optimism property suggested by Fruit [2019] $\\mathcal{L}\\geq L$ where $L$ is the Bellman operator of the true MDP. Here, we only ask for $\\mathfrak{L}h^{*}\\,\\geq\\,L h^{*}$ , i.e., optimism at a span fix-point of $L$ . This condition is met as soon as $M\\in\\mathcal{M},\\,h^{\\ast}\\in\\mathcal{H}$ and $_\\beta$ large enough. ", "page_idx": 22}, {"type": "text", "text": "B.4 Modelization of the projected mitigated Bellman operator $\\mathfrak{L}$ ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The aim of this paragraph is to establish Corollary 27, stating that $\\mathfrak{L}u$ can be viewed as a policy produced by $\\mathtt{G r e e d y}({\\cal M},u,\\beta)$ . ", "page_idx": 22}, {"type": "text", "text": "Lemma 26 (Modelization). For $\\pi\\in\\Pi$ , denote \u03b2\u03c0(s) := \u03b2(s, \u03c0(s)), $\\begin{array}{r}{\\mathcal{R}_{\\pi}:=\\prod_{s}\\mathcal{R}(s,\\pi(s))}\\end{array}$ and $\\mathcal{P}_{\\pi}:=$ $\\textstyle\\prod_{s}{\\mathcal{P}}(s,\\pi(s))$ . Fix $u\\in\\mathbb{R}^{s}$ and let $\\pi:={\\mathsf{G r e e d y}}(M,u,\\beta)$ . ", "page_idx": 22}, {"type": "text", "text": "(1) If $\\mathcal{P}$ is convex, then there exists $(\\tilde{r}_{\\pi},\\tilde{P}_{\\pi})\\in\\mathcal{R}_{\\pi}\\times\\mathcal{P}_{\\pi}$ such that $\\begin{array}{r}{\\mathcal{L}_{\\beta}u=\\tilde{r}_{\\pi}+\\tilde{P}_{\\pi}u.}\\end{array}$ .   \n(2) Assume that $\\mathcal{L}_{\\beta}u=\\tilde{r}_{\\pi}+\\tilde{P}_{\\pi}u$ . There exists $r_{\\pi}^{\\prime}\\le\\tilde{r}_{\\pi}$ such that $\\mathfrak{L}u=r_{\\pi}^{\\prime}+\\tilde{P}_{\\pi}u$ . ", "page_idx": 22}, {"type": "text", "text": "The convexity requirement of (1) is always true if the kernel confidence region is chosen via (C1-4). ", "page_idx": 22}, {"type": "text", "text": "Proof. For (1), fix a state $s\\,\\in\\,S$ , let $a\\;:=\\;\\pi(s)$ and $\\rho:=\\,\\operatorname*{min}(\\operatorname*{sup}\\mathcal{P}(s,a)u,\\hat{p}(s,a)u+\\beta(s,a))$ . If $\\rho={\\operatorname*{sup}}\\mathcal{P}(s,a)u$ , then there is nothing to say because $\\mathcal{P}$ is compact, hence the sup is a max and $\\rho$ is of the form $\\tilde{p}(s,a)u$ . Otherwise, let $\\tilde{p}(s,a)u>\\hat{p}(s,a)u+\\beta(s,a)$ with $\\tilde{p}(s,a)\\in\\mathcal{P}(s,a)$ . Introduce, for $\\lambda\\in[0,1]$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\tilde{p}_{\\lambda}(s,a):=\\lambda\\tilde{p}(s,a)+(1-\\lambda)\\hat{p}(s,a).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By continuity, there exists $\\lambda\\in(0,1)$ such that $\\tilde{p}_{\\lambda}(s,a)u\\,=\\,\\hat{p}(s,a)u+\\beta(s,a)$ and by convexity of $\\mathcal{P}(s,a),\\,\\tilde{p}_{\\lambda}(s,a)\\in\\mathcal{P}(s,a)$ . This proves (1). ", "page_idx": 22}, {"type": "text", "text": "For (2), recall that $\\mathfrak{L}u=\\Gamma\\mathcal{L}^{\\beta}u=\\Gamma(\\tilde{r}_{\\pi}+\\tilde{P}_{\\pi}u)$ . Since $\\Gamma\\nu\\leq\\nu$ , for $\\nu\\in\\mathbb{R}^{S}$ , we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Gamma(\\tilde{r}_{\\pi}+\\tilde{P}_{\\pi}u)\\leq\\tilde{r}_{\\pi}+\\tilde{P}_{\\pi}u.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Set $r_{\\pi}^{\\prime}:=\\Gamma(\\tilde{r}_{\\pi}+\\tilde{P}_{\\pi}u)-\\tilde{P}_{\\pi}u.$ . Check that $r_{\\pi}^{\\prime}$ satisfies $r_{\\pi}^{\\prime}\\le\\tilde{r}_{\\pi}$ and $\\mathfrak{L}u=r_{\\pi}^{\\prime}+\\tilde{P}_{\\pi}u$ . ", "page_idx": 22}, {"type": "text", "text": "The last corollary bellow is crucial to claim that greedy policies are good choices in PMEVI-DT. ", "page_idx": 22}, {"type": "text", "text": "Corollary 27 (Greedy modelization). Let $u\\in\\mathbb{R}^{s}$ and fix $\\pi:={\\mathsf{G r e e d y}}(M,u,\\beta)$ . If $\\mathcal{P}$ is convex, then with the notations of Lemma 26, there exists $\\tilde{r}_{\\pi}\\leq\\operatorname*{sup}\\mathcal{R}_{\\pi}$ and $\\tilde{P}_{\\pi}\\in\\mathcal{P}_{\\pi}$ such that $\\dot{\\mathfrak{L}}u=\\tilde{r}_{\\pi}+\\tilde{P}_{\\pi}u$ . ", "page_idx": 22}, {"type": "text", "text": "C Proof of Theorem 5: Regret analysis of PMEVI-DT ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Notations. At episode $k$ , the played policy is denoted $\\pi_{k}$ . As a greedy response to ${\\mathfrak{h}}_{k}$ , by Proposition 2 (3), there exists $\\tilde{r}_{k}(s)\\leq\\operatorname*{sup}\\mathcal{R}_{t_{k}}(s,\\pi_{k}(s))$ and $\\tilde{P}_{k}(s)\\in\\mathcal{P}_{t_{k}}(s,\\pi(x))$ such that $\\mathfrak{h}_{k}+\\mathfrak{g}_{k}=\\dot{\\tilde{r}}_{k}+\\dot{\\tilde{P}}_{k}\\mathfrak{h}_{k}$ . The reward-kernel pair $\\tilde{M}_{k}~=~(\\tilde{r}_{k},\\tilde{P}_{k})$ is referred to as the optimistic model of $\\pi_{k}$ . We write $P_{k}:=P_{\\pi_{k}}(M)$ the true kernel and $\\hat{P}_{k}\\,:=\\,P_{\\pi_{k}}(\\hat{M}_{t_{k}})$ the empirical kernel. Likewise, we define the reward functions $r_{k}$ and $\\hat{r}_{k}$ . The optimistic gain and bias satisfy ${\\mathfrak{g}}_{k}=g(\\pi_{k},\\widetilde{M}_{k})$ and $\\mathfrak{h}_{k}=\\mathfrak{h}(\\pi_{k},\\widetilde{M}_{k})$ . We further denote $c_{0}=T^{\\frac{1}{5}}$ . ", "page_idx": 23}, {"type": "text", "text": "Important remark. To slightely simplify the analysis, we assume that PMEVI is run with perfect precision $\\epsilon=0$ , i.e., that $\\mathfrak{h}_{k}=\\mathsf{P M E V I}(\\boldsymbol{M}_{t_{k}},\\beta_{t_{k}},\\Gamma_{t_{k}},0)$ hence is a span fix-point of $\\mathfrak{L}_{t_{k}}$ . This assumption is mild and can be dropped by adding an extra error term that has to be carried out in the calculations. ", "page_idx": 23}, {"type": "text", "text": "C.1 Number of episodes under doubling trick (DT) ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Lemma 28 (Number of episodes, Auer et al. [2009]). The number of episodes up to time $T\\geq S A$ is upper-bounded by: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{K(T)\\leq S A\\log_{2}\\left(\\frac{8T}{S A}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "C.2 Sum of bias variances ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The Lemma 29 below shows that $\\begin{array}{r}{\\sum_{t=0}^{T-1}\\mathbf{V}(p(X_{t}),h^{*})}\\end{array}$ scales as $T\\mathrm{sp}\\,(h^{*})\\mathrm{sp}\\,(r)\\,+\\,\\mathrm{sp}\\,(h^{*})\\,\\mathrm{Reg}(T)$ in probability. ", "page_idx": 23}, {"type": "text", "text": "Lemma 29. With probability at least $1-\\delta_{i}$ , we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{T-1}\\mathbf{V}(p(X_{t}),h^{*})\\leq2\\mathrm{sp}\\,(h^{*})\\mathrm{sp}\\,(r)T+\\mathrm{sp}\\,(h^{*})^{2}\\,\\sqrt{\\frac{1}{2}T\\log\\left(\\frac{1}{\\delta}\\right)}+2\\mathrm{sp}\\,(h^{*})\\sum_{t=0}^{T-1}\\Delta^{*}(X_{t})+\\mathrm{sp}\\,(h^{*})^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Using the Bellman equation $h^{\\ast}(s)+g^{\\ast}(s)=r(s,a)+p(s,a)h^{\\ast}+\\Delta^{\\ast}(s,a),$ , we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{V}(p(X_{t}),h^{*})=\\left(p(X_{t})-e_{S_{t}}\\right)h^{*2}+2h^{*}(S_{t})(\\Delta^{*}(X_{t})+r(X_{t})-g^{*}(S_{t})).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since $\\operatorname{sp}\\left(h^{*2}\\right)\\leq\\operatorname{sp}\\left(h^{*}\\right)^{2}$ , we get: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=0}^{T-1}\\mathbf{V}(p(X_{t}),h^{*})\\leq\\sum_{t=0}^{T-1}\\left(p(X_{t})-e_{S_{t}}\\right)h^{*2}+2\\mathrm{sp}\\left(h^{*}\\right)\\left(\\mathrm{sp}\\left(r\\right)T+\\sum_{t=0}^{T-1}\\Delta^{*}(X_{t})\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\sum_{t=0}^{T-1}\\left(p(X_{t})-e_{S_{t+1}}\\right)h^{*2}+2\\mathrm{sp}\\left(h^{*}\\right)\\left(\\frac{1}{2}\\mathrm{sp}\\left(h^{*}\\right)\\mathrm{sp}\\left(r\\right)T+\\sum_{t=0}^{T-1}\\Delta^{*}(X_{t})\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad(\\mathrm{Lemma\\}32)\\leq2\\mathrm{sp}\\left(h^{*}\\right)\\mathrm{sp}\\left(r\\right)T+\\mathrm{sp}\\left(h^{*}\\right)^{2}\\sqrt{\\frac{1}{2}T\\log\\left(\\frac{1}{\\delta}\\right)}+2\\mathrm{sp}\\left(h^{*}\\right)\\sum_{t=0}^{T-1}\\Delta^{*}(X_{t})+\\mathrm{sp}\\left(h^{*}\\right)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last inequality holds with probability $1-\\delta$ . This concludes the proof. ", "page_idx": 23}, {"type": "text", "text": "C.3 Regret and pseudo-regret: A tight relation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this paragraph, we bound the regret with respect to the pseudo-regret (and conversely) up to a factor of order $\\begin{array}{r}{(\\mathrm{sp}\\,(h^{\\ast})\\mathrm{sp}\\,(r)\\log(\\frac{T}{\\delta}))^{\\breve{1}/2}}\\end{array}$ . Hence, in proofs, the pseudo-regret can be changed to the regret with ease. ", "page_idx": 23}, {"type": "text", "text": "Lemma 30. With probability 1 \u22124\u03b4, the regret and the pseudo-regret and linked as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|\\sum_{t=0}^{T-1}(g^{*}-R_{t})-\\sum_{t=0}^{T-1}\\Delta^{*}(X_{t})\\right|\\leq\\left\\{2\\sqrt{\\left(2\\mathsf{s p}\\left(h^{*}\\right)\\mathsf{s p}\\left(r\\right)+\\frac{1}{8}\\right)T\\log\\left(\\frac{T}{\\delta}\\right)}+\\sqrt{2\\mathsf{s p}\\left(h^{*}\\right)\\log\\left(\\frac{T}{\\delta}\\right)\\sum_{t=0}^{T-1}\\Delta^{*}(X_{t})}\\right\\}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. We rely again on the Poisson equation $g^{\\ast}(S_{t})-r(X_{t})-\\Delta^{\\ast}(X_{t})=(p(X_{t})-e_{S_{t}})h^{\\ast}$ , so: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathrm{A}:=\\left|\\sum_{t=0}^{T-1}(g^{*}-R_{t}-\\Delta^{*}(X_{t}))\\right|\\leq\\left|\\sum_{t=0}^{T-1}\\left(p(X_{t})-e_{S_{t}}\\right)h^{*}\\right|+\\left|\\sum_{t=0}^{T-1}\\left(R_{t}-r(X_{t})\\right)\\right|}\\\\ {\\leq\\mathrm{sp}\\left(h^{*}\\right)+\\left|\\sum_{t=0}^{T-1}\\left(p(X_{t})-e_{S_{t+1}}\\right)h^{*}\\right|+\\left|\\sum_{t=0}^{T-1}\\left(R_{t}-r(X_{t})\\right)\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Up to the constant sp $(h^{*})$ , the two error terms are respectively a navigation and a reward error. The second is bounded using Azuma\u2019s inequality (Lemma 32), showing that with probability $1-2\\delta$ , we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\sum_{t=0}^{T-1}(R_{t}-r(X_{t}))\\right|\\leq\\,\\sqrt{\\frac{1}{2}T\\log\\left(\\frac{1}{\\delta}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We continue by using Freedman\u2019s inequality, instantiated in the form of Lemma 33. With probability $1-\\delta$ , we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\sum_{t=0}^{T-1}\\left(p(X_{t})-e_{S_{t+1}}\\right)h^{*}\\right|\\leq\\,\\sqrt{2\\sum_{t=0}^{T-1}\\mathbf{V}(p(X_{t}),h^{*})\\log\\left(\\frac{T}{\\delta}\\right)}+4\\mathrm{sp}\\left(h^{*}\\right)\\log\\left(\\frac{T}{\\delta}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The quantity $\\begin{array}{r l}{\\sum_{t=0}^{T-1}\\mathbf{V}(p(X_{t}),h^{*})}\\end{array}$ is a classical one that appears at several places thro\u221aughout the\u221a analy\u221asis. Using Lemma 29, we bount it explicitely. Further simplifying the bound with ${\\sqrt{a+b}}\\leq\\;{\\sqrt{a}}+\\;{\\sqrt{b}},$ we get that with probability $1-4\\delta$ , we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{A}\\leq\\left\\{\\sqrt{2\\mathrm{sp}\\,(h^{\\ast})\\mathrm{sp}\\,(r)T\\log\\left(\\frac{T}{\\delta}\\right)}+\\,\\sqrt{\\frac{1}{2}T\\log\\left(\\frac{1}{\\delta}\\right)}+\\,\\sqrt{2\\mathrm{sp}\\,(h^{\\ast})\\log\\left(\\frac{T}{\\delta}\\right)\\sum_{t=0}^{T-1}\\Delta^{\\ast}(X_{t})}\\right\\}.}\\\\ {+\\mathrm{sp}\\,(h^{\\ast})\\left(\\frac{1}{2}T\\right)^{\\frac{1}{4}}\\log^{\\frac{3}{4}}\\left(\\frac{T}{\\delta}\\right)+4\\mathrm{sp}\\,(h^{\\ast})\\log\\left(\\frac{T}{\\delta}\\right)+2\\mathrm{sp}\\,(h^{\\ast})\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Bound $\\log(\\frac{1}{\\delta})$ by $\\log(\\frac{T}{\\delta})$ and use ${\\sqrt{a}}+\\,{\\sqrt{b}}\\,\\leq\\,2\\,{\\sqrt{a+b}}$ to merge the terms in $\\sqrt{T\\log(\\frac{T}{\\delta})}$ under a single square-root. \u25a1 ", "page_idx": 24}, {"type": "text", "text": "Overall, Lemma 30 states that the regret $\\begin{array}{r}{\\sum_{t=0}^{T-1}(g^{*}-R_{t})}\\end{array}$ and the pseudo-regret $\\begin{array}{r}{\\sum_{t=0}^{T-1}\\Delta^{*}(X_{t})}\\end{array}$ differ by about $(\\operatorname{sp}{(h^{*})T}\\log(\\frac{T}{\\delta}))^{1/2}$ in probability (up to asymptotically negligible additional terms). In g\u221aeneral, the precise form of Lemma 30 is not convenient to use because it is of form form $x\\leq y+\\alpha\\,{\\sqrt{y}}+\\beta$ that is not linear in $y$ . Corollary 31 factorizes the result into one which will be more convenient in proofs. ", "page_idx": 24}, {"type": "text", "text": "Corollary 31. Denote $\\begin{array}{r}{x:=\\sum_{t=0}^{T-1}(g^{*}-R_{t})}\\end{array}$ and $\\begin{array}{r}{y:=\\sum_{t=0}^{T-1}\\Delta^{*}(X_{t})}\\end{array}$ . Further introduce: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha:=\\,\\sqrt{2\\mathrm{sp}\\,(h^{*})\\log\\left(\\frac{T}{\\delta}\\right)}}\\\\ &{\\beta:=2\\,\\sqrt{\\left(2\\mathrm{sp}\\,(h^{*})\\mathrm{sp}\\,(r)+\\frac{1}{2}\\right)T\\log\\left(\\frac{T}{\\delta}\\right)}+\\mathrm{sp}\\,(h^{*})\\left(\\frac{1}{2}T\\right)^{\\frac{1}{4}}\\log^{\\frac{3}{4}}\\left(\\frac{T}{\\delta}\\right)+2\\mathrm{sp}\\,(h^{*})\\left(2\\log\\left(\\frac{T}{\\delta}\\right)+1\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then, with probability 1 \u22124\u03b4, we have $\\sqrt{x}\\leq\\ \\sqrt{y}+\\textstyle{\\frac{1}{2}}\\alpha+\\ \\sqrt{\\beta}$ and $\\begin{array}{r}{\\sqrt{y}\\,\\le\\,\\sqrt{x}+\\alpha+\\,\\sqrt{\\beta}.}\\end{array}$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. This is straight forward algebra from the result of Lemma 30. ", "page_idx": 24}, {"type": "text", "text": "C.4 Proof of Lemma 6, reward optimism ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We start by getting rid of the reward noise. We have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Reg}(T):=\\displaystyle\\sum_{t=0}^{T-1}(g^{*}-R_{t})=\\sum_{t=0}^{T-1}(g^{*}-r(X_{t}))+\\sum_{t=0}^{T-1}(r(X_{t})-R_{t})}\\\\ &{\\quad\\quad\\quad\\leq\\displaystyle\\sum_{t=0}^{T-1}(g^{*}-r(X_{t}))+\\sqrt{\\frac{1}{2}T\\log\\left(\\frac{1}{\\delta}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "with probability $1-\\delta$ by Azuma\u2019s inequality (Lemma 32). We are left with $\\begin{array}{r}{\\sum_{t=0}^{T-1}(g^{*}-r(X_{t}))}\\end{array}$ . We continue by splitting the regret episodically and invoking optimism. By Lemma 13, with probability $1-4\\delta$ , we have $\\begin{array}{r}{\\sum_{t=0}^{T-1}(g^{*}-r(X_{t}))\\le\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}(\\mathfrak{g}_{k}-r(X_{t}))}\\end{array}$ . Introduce ", "page_idx": 24}, {"type": "equation", "text": "$$\nB_{0}(T):=\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}({\\mathfrak{g}}_{k}-r(X_{t})).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We focus on bounding $B_{0}(T)$ . By Assumption 2, $\\tilde{r}_{k}(s,a)$ is of the form $\\hat{r}_{k}(s,a)~+$ $\\sqrt{C\\log(2S A T/\\delta)/N_{t_{k}}(s,a)}-\\eta_{k}(s,a)$ with $\\eta_{k}(s,a)\\;\\in\\;\\mathbb{R}$ . By the statement (3) of Proposition 2, $\\eta_{k}(s,a)\\ge0$ . Therefore, ", "page_idx": 24}, {"type": "equation", "text": "$$\nB_{0}(T)=\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}\\left(\\mathfrak{g}_{k}-\\tilde{r}_{k}(X_{t})\\right)+\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}\\left(\\tilde{r}_{k}(X_{t})-r(X_{t})\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\leq\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}\\left(9_{k}-\\tilde{r}_{k}(X_{t})\\right)+S A+\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}\\mathbf{1}\\left(N_{t_{k}}(X_{t})\\geq1\\right)\\left(\\hat{r}_{k}(X_{t})-r(X_{t})+\\sqrt{\\frac{C\\log\\left(\\frac{2S A T}{\\delta}\\right)}{N_{t_{k}}(X_{t})}}\\right)}\\\\ {\\displaystyle\\overset{(*)}{\\leq}\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}\\left(9_{k}-\\tilde{r}_{k}(X_{t})\\right)+S A+\\sum_{k}\\sum_{k}^{t_{k+1}-1}\\sum_{t=t_{k}}^{\\mathbf{1}}\\mathbf{1}\\left(N_{t_{k}}(X_{t})\\geq1\\right)\\left(\\sqrt{\\frac{2\\log\\left(\\frac{2S A T}{\\delta}\\right)}{N_{t_{k}}(s,a)}}+\\sqrt{\\frac{C\\log\\left(\\frac{2S A T}{\\delta}\\right)}{N_{t_{k}}(s,a)}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $(*)$ holds with probability $1-\\delta$ following Lemma 35. By the doubling trick rule (DT), we have $N_{t}(X_{t})\\leq2N_{t_{k}}(X_{t})$ for $t<t_{k+1}$ , so, with probability $1-\\delta$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle B_{0}(T)\\leq\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}(g_{k}-\\tilde{r}_{k}(X_{t}))+S A+2\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}{\\mathbf1}\\left(N_{h_{k}}(X_{t})\\geq1\\right)\\sqrt{\\frac{(2+C)\\log\\left(\\frac{2S A T}{\\delta}\\right)}{N_{h}(s,a)}}}\\\\ {\\displaystyle\\leq\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}\\left(g_{k}-\\tilde{r}_{k}(X_{t})\\right)+S A+2\\sqrt{(2+C)\\log\\left(\\frac{2S A T}{\\delta}\\right)}\\sum_{s,a}^{N_{f}(s,a)-1}\\sum_{n=1}^{s}\\sqrt{\\frac{1}{n}}}\\\\ {\\displaystyle\\leq\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}\\left(g_{k}-\\tilde{r}_{k}(X_{t})\\right)+S A+4\\sqrt{(2+C)\\log\\left(\\frac{2S A T}{\\delta}\\right)}\\sum_{s,a}^{}\\sqrt{N_{T}(s,a)}}\\\\ {\\displaystyle(\\mathrm{Jensen})\\leq\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}\\left(g_{k}-\\tilde{r}_{k}(X_{t})\\right)+S A+4\\sqrt{(2+C)S A T\\log\\left(\\frac{2S A T}{\\delta}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We conclude that with probability $1-6\\delta$ , we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{Reg}(T)\\leq\\sum_{k}^{t_{k+1}-1}\\binom{9}{9k}+4\\sqrt{(2+C)S A T\\log\\left(\\frac{2S A T}{\\delta}\\right)}+\\sqrt{\\frac{1}{2}T\\log\\left(\\frac{2S A T}{\\delta}\\right)}+S A.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 25}, {"type": "text", "text": "C.5 Proof of Lemma 7, navigation error ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k}^{t_{k+1}-1}(p_{k}(S_{t})-e_{S_{t}})\\mathfrak{h}_{k}\\leq\\sum_{k}^{t_{k+1}-1}(p_{k}(S_{t})-e_{S_{t+1}})\\mathfrak{h}_{k}+\\displaystyle\\sum_{k}\\mathbf{s}\\mathfrak{p}\\left(\\mathfrak{h}_{k}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\underbrace{\\displaystyle\\sum_{k}^{t_{k+1}-1}(p_{k}(S_{t})-e_{S_{t+1}})(\\mathfrak{h}_{k}-h^{*})}_{\\mathrm{A_{1}}}+\\underbrace{\\displaystyle\\sum_{k}^{t_{k+1}-1}(p_{k}(S_{t})-e_{S_{t+1}})h^{*}}_{\\mathrm{A_{2}}}+\\sum_{k}\\mathfrak{s p}\\left(\\mathfrak{h}_{k}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The last term is $\\mathsf{O}(c_{0}S A\\log(T))$ b   y       L    e    m     m a 2    8  ,      h   e   n   c    e     i  s $\\mathsf{O}(T^{1/5}\\log(T))$ . ", "page_idx": 25}, {"type": "text", "text": "(STEP 1) We start by bounding $\\mathrm{A}_{1}$ . By Lemma 13, with probability $1-4\\delta$ , we have $h^{*}\\in\\mathcal{H}_{t_{k}}$ for all $k\\leq K(T)$ . So sp $\\left(\\mathfrak{h}_{k}-h^{*}\\right)\\leq\\mathrm{sp}\\left(\\mathfrak{h}_{k}\\right)+\\mathrm{sp}\\left(h^{*}\\right)\\leq2c_{0}$ . By Freedman\u2019s inequality invoked in the form of Lemma 33, we have with probability $1-5\\delta$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{A}_{1}\\leq\\sqrt{2\\sum_{k}\\displaystyle\\sum_{t=t_{k}}^{t_{k+1}-1}\\mathbf{V}\\left(p(X_{t}),\\mathfrak{h}_{k}-h^{*}\\right)\\log\\left(\\frac{T}{\\delta}\\right)}+8c_{0}\\log\\left(\\frac{T}{\\delta}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "It suffices to bound the first term. Recall that $e$ is the vector full of ones. We have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}\\mathbf{V}(p(X_{t}),\\mathfrak{h}_{k}-h^{*})=\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}\\mathbf{V}\\left(p(X_{t}),\\mathfrak{h}_{k}-h^{*}-(\\mathfrak{h}_{k}(S_{t})-h^{*}(S_{t}))\\cdot e\\right)}\\\\ &{\\le\\displaystyle\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}\\sum_{s^{\\prime}\\in S}p(s^{\\prime}|X_{t})\\left(\\mathfrak{h}_{k}(s^{\\prime})-h^{*}(s^{\\prime})-(\\mathfrak{h}_{k}(S_{t})-h^{*}(S_{t}))\\right)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overset{(*)}{\\leq}3\\displaystyle\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}\\mathbf{E}\\left[\\sum_{s^{\\prime}\\in S}p(s^{\\prime}|X_{t})\\left(\\mathfrak{h}_{k}(s^{\\prime})-h^{*}(s^{\\prime})-(\\mathfrak{h}_{k}(S_{t})-h^{*}(S_{t}))\\right)^{2}\\Bigg|\\mathcal{F}_{t}\\right]+16c_{0}^{2}\\log\\left(\\frac{1}{\\delta}\\right)}\\\\ &{=3\\displaystyle\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}\\ (\\mathfrak{h}_{k}(S_{t+1})-h^{*}(S_{t+1})-(\\mathfrak{h}_{k}(S_{t})-h^{*}(S_{t})))^{2}+16c_{0}^{2}\\log\\left(\\frac{1}{\\delta}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Here the inequality $(*)$ holds with probability $1-\\delta$ following Lemma 40. We will bound the summand with the bias estimation error error $(c_{k},s,s^{\\prime})$ that spawns the inner regret estimation $B_{0}(t_{k})=$ $\\begin{array}{r}{\\sum_{\\ell=1}^{k-1}\\sum_{t=t_{\\ell}}^{t_{\\ell+1}-1}({\\mathfrak{g}}_{\\ell}-R_{t})}\\end{array}$ . This inner estimation is linked to $\\begin{array}{r}{B(T):=\\sum_{k,t}(\\mathfrak{g}_{k}-R_{t})}\\end{array}$ the overall optimistic ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{0}(t_{k})\\leq\\displaystyle\\sum_{\\ell=1}^{K(T)}\\sum_{t=t_{\\ell}}^{t_{\\ell+1}-1}(\\mathfrak{g}_{k}-R_{t})-\\sum_{\\ell=k}^{K(T)}\\sum_{t=t_{\\ell}}^{t_{\\ell+1}-1}(\\mathfrak{g}_{k}-R_{t})}\\\\ &{\\overset{()}{\\leq}\\displaystyle\\sum_{\\ell=1}^{K(T)}\\sum_{t=t_{\\ell}}^{t_{\\ell+1}-1}(\\mathfrak{g}_{k}-R_{t})-\\sum_{\\ell=k}^{K(T)}\\sum_{t=t_{\\ell}}^{t_{\\ell+1}-1}(\\mathfrak{g}^{*}-R_{t})}\\\\ &{\\leq\\displaystyle\\sum_{\\ell=1}^{K(T)}\\sum_{t=t_{\\ell}}^{t_{\\ell+1}-1}(\\mathfrak{g}_{k}-R_{t})-\\sum_{\\ell=k}^{K(T)}\\sum_{t=t_{k}}^{T-1}(\\mathrm{A}^{*}(X_{t})+(p(X_{t})-\\mathfrak{e}_{S_{t}})\\,h^{*}+r(X_{t})-R_{t})}\\\\ &{\\leq\\displaystyle\\sum_{\\ell=1}^{K(T)}\\sum_{t=t_{\\ell}}^{t_{\\ell+1}-1}(\\mathfrak{g}_{k}-R_{t})+\\mathfrak{s p}\\,(h^{*})-\\sum_{\\ell=k}^{K(T)}\\sum_{t=t_{k}}^{T-1}((p(X_{t})-\\mathfrak{e}_{S_{t+1}})\\,h^{*}+r(X_{t})-R_{t})}\\\\ &{\\overset{()}{\\leq}\\displaystyle\\sum_{\\ell=1}^{K(T)}\\sum_{t=t_{\\ell}}^{t_{\\ell+1}-1}(\\mathfrak{g}_{k}-R_{t})+\\mathfrak{s p}\\,(h^{*})+(1+\\mathfrak{s p}\\,(h^{*}))\\,\\sqrt{\\frac{1}{2}T\\log\\left(\\frac{1}{\\delta}\\right)}}\\\\ &{=:B(T)+\\mathfrak{s p}\\,(h^{*})+(1+\\mathfrak{s p}\\,(h^{*}))\\,\\sqrt{\\frac{1}{2}T\\log\\left(\\frac{1}{\\delta}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "In the above, $(*)$ holds with probability $1-4\\delta$ uniformly on $k$ following Lemma 13 and (\u2020) holds, also uniformly on $k$ , with probability $1-\\delta$ by applying Azuma-Hoeffding\u2019s inequality (Lemma 32). Continuing, still on the event specified by Lemma 13, we have with probability $1-6\\delta$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}\\mathbf{V}(p(X_{t}),\\mathfrak{h}_{k}-h^{*})\\leq3\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}\\frac{3c_{0}+(1+c_{0})\\sqrt{8t_{k}\\log\\left(\\frac{2}{\\delta}\\right)}+2B_{0}(t_{k})}{N_{t_{k}}(S_{t+1}\\leftrightarrow S_{t})}+16c_{0}^{2}\\log\\left(\\frac{1}{\\delta}\\right)}}\\\\ &{}&{\\leq3\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}\\frac{4c_{0}+(1+c_{0})\\sqrt{32T\\log\\left(\\frac{2}{\\delta}\\right)}+2B(T)}{N_{t_{k}}(S_{t},A_{t},S_{t+1})}+16c_{0}^{2}\\log\\left(\\frac{1}{\\delta}\\right)}\\\\ &{}&{(\\mathrm{D}\\Gamma)\\leq12c_{0}^{2}S^{2}A+3\\left(4c_{0}+(1+c_{0})\\sqrt{32T\\log\\left(\\frac{2}{\\delta}\\right)}+2B(T)\\right)S^{2}A\\log(T)}\\\\ &{}&{+\\ 16c_{0}^{2}\\log\\left(\\frac{1}{\\delta}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "(STEP 2) For $\\mathbf{A}_{2}$ , by Freedman\u2019s inequality invoked in the form of Lemma 33 again, we have with probability $1-\\delta$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{A}_{2}\\leq\\sqrt{2\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}\\mathbf{V}(p_{k}(S_{t}),h^{*})\\log\\Big(\\frac{T}{\\delta}\\Big)}+8c_{0}\\log\\Big(\\frac{T}{\\delta}\\Big)}\\\\ &{\\quad\\leq\\sqrt{2\\sum_{t=0}^{T-1}\\mathbf{V}(p(X_{t}),h^{*})\\log\\Big(\\frac{T}{\\delta}\\Big)}+8c_{0}\\log\\Big(\\frac{T}{\\delta}\\Big)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We recognize the sum of variance $\\begin{array}{r l}{\\sum_{t=0}^{T-1}\\mathbf{V}(p(X_{t}),h^{*})}&{{}}\\end{array}$ that we leave as is. ", "page_idx": 26}, {"type": "text", "text": "(STEP 3) As a result, with probability $1-7\\delta$ , we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{k}^{t_{k+1}-1}(p_{k}(S_{t})-e_{S_{t}})\\mathfrak{h}_{k}\\le\\sqrt{2\\sum_{t=0}^{T-1}\\mathbf{V}(p(X_{t}),h^{*})\\log\\left(\\frac{T}{\\delta}\\right)}+2S A^{\\frac{1}{2}}\\sqrt{3B(T)}\\log\\left(\\frac{T}{\\delta}\\right)+\\ensuremath{\\operatorname{O}\\left(S A^{\\frac{1}{2}}T^{\\frac{7}{20}}\\log^{\\frac{3}{4}}\\left(\\frac{T}{\\delta}\\right)\\right)}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "when $c_{0}=T^{\\frac{1}{5}}$ ", "page_idx": 26}, {"type": "text", "text": "C.6 Proof of Lemma 8, empirical bias error ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Because $h^{*}$ is a fixed vector, Bennett\u2019s inequality (see Lemma 39) guarantees that $(\\hat{p}_{k}(S_{t})-p_{k}(S_{t})h^{*}$ is small as follows. By doing a union bound over Lemma 39 with confidence $\\frac{\\delta}{S A T}$ over all pairs $(s,a)$ and visits counts $N(s,a)\\leq T$ , we see that with probability $1-\\delta$ , for all $k$ , we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{t=t_{k}}^{\\tau\\downarrow-1}(\\hat{p}_{k}(S_{t})-p_{k}(S_{t}))\\,h^{*}\\leq\\mathrm{sp}\\,(h^{*})S A+\\sum_{t=t_{k}}^{t_{k+1}-1}\\mathbf{1}\\,(N_{t_{k}}(X_{t})\\geq1)\\Bigg(\\sqrt{\\frac{2\\mathrm{V}(p(X_{t}),h^{*})\\log\\left(\\frac{S A T}{\\delta}\\right)}{N_{t_{k}}(X_{t})}}+\\frac{\\mathrm{sp}(h^{*})\\log\\left(\\frac{S A T}{\\delta}\\right)}{3N_{t_{k}}(X_{t})}\\Bigg)}}\\\\ &{}&{(\\mathrm{by~doubling~trick})\\leq\\mathrm{sp}\\,(h^{*})S A+2\\sum_{t=t_{k}}^{t_{k+1}-1}\\mathbf{1}\\,(N_{t}(X_{t})\\geq1)\\left(\\sqrt{\\frac{2\\mathrm{V}(p(X_{t}),h^{*})\\log\\left(\\frac{S A T}{\\delta}\\right)}{N_{t}(X_{t})}}+\\frac{\\mathrm{sp}(h^{*})\\log\\left(\\frac{S A T}{\\delta}\\right)}{3N_{t}(X_{t})}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Summing this over $k$ and factorizing over state-action pairs, we get that with probability $1-\\delta$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k}(2k)\\leq\\operatorname{sp}{(h^{*})}S A+2\\sum_{s,a}\\left(\\sum_{n=1}^{N_{T}(s,a)}{\\sqrt{\\frac{2\\mathbf{V}(p(s,a),h^{*})\\log\\left(\\frac{S A T}{\\delta}\\right)}{n}}}+\\sum_{n=1}^{N_{T}(s,a)}{\\frac{\\mathrm{sp}(h^{*})\\log\\left(\\frac{S A T}{\\delta}\\right)}{n}}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\operatorname{sp}{(h^{*})}S A+4\\sum_{s,a}{\\sqrt{N_{T}(s,a)}}{\\sqrt{\\cal N}({p(s,a)},h^{*})\\log\\left(\\frac{S A T}{\\delta}\\right)}+2\\mathrm{sp}\\,(h^{*})S A\\log\\left(\\frac{S A T}{\\delta}\\right)\\log(T)}\\\\ &{\\quad\\quad\\quad(\\mathrm{Jensen})\\leq\\operatorname{sp}{(h^{*})}S A+4\\,{\\sqrt{S A\\sum_{s,a}\\mathbf{V}(p(s,a),h^{*})\\log\\left(\\frac{S A T}{\\delta}\\right)}}+2\\mathrm{sp}\\,(h^{*})S A\\log\\left(\\frac{S A T}{\\delta}\\right)\\log(T)}\\\\ &{\\quad\\quad\\quad\\quad=\\operatorname{sp}{(h^{*})}S A+4\\,{\\sqrt{\\sum_{t=0}^{T-1}\\mathbf{V}(p(X_{t}),h^{*})\\log\\left(\\frac{S A T}{\\delta}\\right)}}+2\\mathrm{sp}\\,(h^{*})S A\\log\\left(\\frac{S A T}{\\delta}\\right)\\log(T)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We recognize the sum of variances $\\begin{array}{r l}{\\sum_{t=0}^{T-1}\\mathbf{V}(p(X_{t}),h^{*})}\\end{array}$ , that is left to be upper-bounded later on. \u25a1 ", "page_idx": 27}, {"type": "text", "text": "C.7 Proof of Lemma 9, optimistic overshoot ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Because of the $_\\beta$ -mitigation generated by Algorithm 5, the quantity $(\\tilde{p}_{k}(S_{t})-\\hat{p}_{k}(S_{t}))\\mathfrak{h}_{k}$ is shown to be directly related to $\\mathbf{V}(p(X_{t}),h^{*})$ up to a provably negligible error. Denote $h_{k}^{\\prime}$ the reference point BiasProjection $(\\mathcal{H}_{t_{k}},c_{t_{k}}(-,s_{0}))$ used in Algorithm 5 (denoted $h_{0}$ in the algorithm). By Lemma 13, with probability $1-4\\delta$ , we have $h^{*}\\in\\mathcal{H}_{t_{k}}$ for all $k$ . To lighten up notations, we write $d_{t_{k}}(s^{\\prime},s)$ instead of er $\\mathrm{ror}(c_{t_{k}},s^{\\prime},s)$ . ", "page_idx": 27}, {"type": "text", "text": "(STEP 1) Denote $\\mathrm{A}:=(\\tilde{p}_{k}(S_{t})-\\hat{p}_{k}(S_{t}))\\mathrm{t}_{k}$ . By construction of $\\tilde{p}_{k}$ , we have $\\mathrm{A}\\leq\\beta_{t_{k}}(X_{t})$ , so: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{A}\\leq\\beta_{t_{k}}(X_{t})}\\\\ &{\\quad=:\\sqrt{\\frac{2\\left(\\mathbf{V}(\\hat{p}_{k}(S_{t}),h_{k}^{\\prime})+8c_{0}\\sum_{s^{\\prime}\\in S}\\hat{p}_{k}(s^{\\prime}|S_{t})d_{t_{k}}(s^{\\prime},S_{t})\\log\\left(\\frac{S A T}{\\delta}\\right)\\right)}{N_{t_{k}}(X_{t})}}+\\frac{3c_{0}\\log\\left(\\frac{S A T}{\\delta}\\right)}{N_{t_{k}}(X_{t})}}\\\\ &{\\quad\\leq\\sqrt{\\frac{2\\mathbf{V}(\\hat{p}_{k}(S_{t}),h_{k}^{\\prime})}{N_{t_{k}}(X_{t})}}+\\underbrace{\\sqrt{\\frac{16c_{0}\\sum_{s^{\\prime}\\in S}\\hat{p}_{k}(s^{\\prime}|S_{t})d_{t_{k}}(s^{\\prime},S_{t})\\log\\left(\\frac{S A T}{\\delta}\\right)}{N_{t_{k}}(X_{t})}}}_{\\mathrm{A}_{2}}+\\frac{3c_{0}\\log\\left(\\frac{S A T}{\\delta}\\right)}{N_{t_{k}}(X_{t})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The rightmost term of A is of order ${\\mathrm{O}}(\\log^{2}(T))$ hence is negligible. We focus on the other two. The analysis of $\\mathbf{A}_{1}$ will spawn a term similar to $\\mathbf{A}_{2}$ , hence we start by the second. Recall that $d_{t_{k}}$ is the bias error provided by Algorithm 3 and that the inner regret estimation is $\\begin{array}{r}{B_{0}(t_{k})=\\sum_{\\ell=1}^{k-1}\\sum_{t=t_{\\ell}}^{t_{\\ell+1}-1}({\\mathfrak{g}}_{\\ell}-R_{t})}\\end{array}$ Now, remark that: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{0}(t_{k})\\leq\\displaystyle\\sum_{\\ell=1}^{K(T)}\\sum_{t=t_{\\ell}}^{t_{\\ell+1}-1}(\\mathfrak{g}_{k}-R_{t})-\\sum_{\\ell=k}^{K(T)}\\sum_{t=t_{\\ell}}^{t_{\\ell+1}-1}(\\mathfrak{g}_{k}-R_{t})}\\\\ &{\\overset{(*)}{\\leq}\\displaystyle\\sum_{\\ell=1}^{K(T)}\\sum_{t=t_{\\ell}}^{t_{\\ell+1}-1}(\\mathfrak{g}_{k}-R_{t})-\\sum_{\\ell=k}^{K(T)}\\sum_{t=t_{\\ell}}^{t_{\\ell+1}-1}(g^{*}-R_{t})}\\\\ &{\\leq\\displaystyle\\sum_{\\ell=1}^{K(T)}\\sum_{t=t_{\\ell}}^{t_{\\ell+1}-1}(\\mathfrak{g}_{k}-R_{t})-\\sum_{\\ell=k}^{K(T)}\\sum_{t=t_{k}}^{T-1}(\\Delta^{*}(X_{t})+\\left(p(X_{t})-e_{S_{t}}\\right)h^{*}+r(X_{t})-R_{t})}\\\\ &{\\leq\\displaystyle\\sum_{\\ell=1}^{K(T)}\\sum_{t=t_{\\ell}}^{t_{\\ell+1}-1}(\\mathfrak{g}_{k}-R_{t})+\\mathfrak{s p}\\left(h^{*}\\right)-\\sum_{\\ell=k}^{K(T)}\\sum_{t=t_{k}}^{T-1}\\left(\\left(p(X_{t})-e_{S_{t+1}}\\right)h^{*}+r(X_{t})-R_{t}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overset{(\\dagger)}{\\leq}\\sum_{\\ell=1}^{K(T)}\\sum_{t=t_{\\ell}}^{t_{\\ell+1}-1}\\left(\\mathfrak{g}_{k}-R_{t}\\right)+\\mathfrak{s p}\\left(h^{*}\\right)+\\left(1+\\mathfrak{s p}\\left(h^{*}\\right)\\right)\\sqrt{\\frac{1}{2}T\\log\\left(\\frac{1}{\\delta}\\right)}}\\\\ &{=:B(T)+\\mathfrak{s p}\\left(h^{*}\\right)+\\left(1+\\mathfrak{s p}\\left(h^{*}\\right)\\right)\\sqrt{\\frac{1}{2}T\\log\\left(\\frac{1}{\\delta}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "In the above, $(*)$ holds with probability $1-4\\delta$ uniformly on $k$ following Lemma 13 and (\u2020) holds, also uniformly on $k$ , with probability $1-\\delta$ by applying Azuma-Hoeffding\u2019s inequality (Lemma 32). Therefore, with probability $1-5\\delta$ , for all $k$ and $t\\in\\{t_{k},\\ldots,t_{k+1}-1\\}$ , we have: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\sqrt{\\frac{16\\cos_{2}\\zeta\\ r\\cos\\beta_{k}(\\varepsilon^{\\prime}|S_{t})\\lambda_{t}(x^{\\prime},S_{t})\\log\\left(\\frac{\\delta\\lambda^{T}}{\\delta}\\right)}{N_{t}(X_{t})}}\\leq\\frac{\\sqrt{16\\cos_{0}\\log\\left(\\frac{\\delta\\lambda^{T}}{\\delta}\\right)}\\sum_{\\varepsilon^{\\prime}\\in S}N_{t}(S_{t},A_{t},s^{\\prime})d_{t}(s^{\\prime},S_{t})}{N_{t}(X_{t})}}\\\\ {\\leq}&{\\frac{\\sqrt{16\\cos_{0}\\left(\\frac{\\delta\\lambda^{T}}{\\delta}\\right)}\\sum_{\\varepsilon^{\\prime}\\in S}N_{t}(S_{t},\\exp^{\\prime})d_{t}(s^{\\prime},S_{t})}{N_{t}(X_{t})}}\\\\ {\\leq}&{\\frac{\\sqrt{16\\cos_{0}\\left(\\frac{\\delta\\lambda^{T}}{\\delta}\\right)\\left(3\\pi+(16\\cos\\left(\\left\\{1+\\sqrt{x\\log\\left(\\frac{\\delta}{\\delta}\\right)}\\right\\}+2B\\cup(\\delta)\\right)\\right)}}{N_{t}(X_{t})}}\\\\ {\\leq}&{\\frac{\\sqrt{16\\cos_{0}\\left(\\frac{\\delta\\lambda^{T}}{\\delta}\\right)\\left(\\left\\{1+\\cos\\left(\\frac{3}{\\delta}\\right)\\sqrt{x\\log\\left(\\frac{\\delta}{\\delta}\\right)}\\right\\}+2B(T)\\right)}}{N_{t}(X_{t})}}\\\\ {\\leq}&{\\frac{\\sqrt{16\\cos_{0}\\left(\\frac{\\delta\\lambda^{T}}{\\delta}\\right)\\left(\\left\\{1+\\cos\\left(\\frac{3}{\\delta}\\right)\\sqrt{x\\log\\left(\\frac{\\delta}{\\delta}\\right)^{2}+2B(T)}\\right\\}\\right)}}{N_{t}(X_{t})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "This bound will be enough. We move on to $\\mathbf{A}_{1}$ . We have: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\sqrt{\\mathbf{V}(\\hat{p}_{k}(S_{t}),h_{k}^{\\prime})}\\le}&{\\sqrt{\\Big|\\mathbf{V}(\\hat{p}_{k}(S_{t}),h_{k}^{\\prime})-\\mathbf{V}(p(X_{t}),h^{*})\\Big|}+\\sqrt{\\mathbf{V}(p(X_{t}),h^{*})}}\\\\ {\\le}&{\\sqrt{\\Big|\\mathbf{V}(\\hat{p}_{k}(S_{t}),h_{k}^{\\prime})-\\mathbf{V}(\\hat{p}_{k}(X_{t}),h^{*})\\Big|}\\sqrt{\\mathbf{V}(\\hat{p}_{k}(S_{t}),h^{*})-\\mathbf{V}(p(X_{t}),h^{*})}\\Big|+\\sqrt{\\mathbf{V}(p(X_{t}),h^{*})}}\\\\ {\\overset{(*)}{\\le}\\sqrt{8c_{0}}\\sum_{s^{\\prime}\\in S}\\hat{p}_{k}(s^{\\prime}|S_{t})d_{k}(s^{\\prime},S_{t})+\\mathbf{s}\\mathbf{p}\\,(h^{*})\\sqrt{\\|\\hat{p}_{k}(S_{t})-p_{k}(S_{t})\\|}_{1}+\\sqrt{\\mathbf{V}(p(X_{t}),h^{*})}}\\\\ {\\overset{(*)}{\\le}\\sqrt{8c_{0}}\\sum_{s^{\\prime}\\in S}\\hat{p}_{k}(s^{\\prime}|S_{t})d_{k}(s^{\\prime},S_{t})+\\mathbf{s}\\mathbf{p}\\,(h^{*})\\Bigg(\\frac{S\\mathbf{1}\\mathbf{1}Q\\log\\Big(\\frac{S A T}{\\delta}\\Big)}{N_{t_{k}}(X_{t})}\\Bigg)^{\\!\\!\\!\\perp}+\\sqrt{\\mathbf{V}(p(X_{t}),h^{*})}}\\\\ {\\le}&{\\frac{\\mathbf{A}_{2}}{\\sqrt{2N_{t}(X_{t})}}+\\mathbf{s}\\mathbf{p}\\,(h^{*})\\Bigg(\\frac{S\\log\\Big(\\frac{S A T}{\\delta}\\Big)}{N_{t_{k}}(X_{t})}\\Bigg)^{\\!\\!\\!\\perp}+\\sqrt{\\mathbf{V}(p(X_{t}),h^{*})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $(*)$ is obtained by applying Lemma 12 and $(\\dagger)$ holds with probability $1-\\delta$ by applying Weissman\u2019s inequality, see Lemma 35. All together, with probability $1-6\\delta$ , A is upper-bounded by: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathrm{A}\\leq\\sqrt{\\frac{2\\mathbf{V}(p(X_{t}),h^{*})\\log\\left(\\frac{S A T}{\\delta}\\right)}{N_{t_{k}}(X_{t})}}+2\\mathrm{A}_{2}+\\operatorname{sp}\\left(h^{*}\\right)\\sqrt{\\frac{2\\log\\left(\\frac{S A T}{\\delta}\\right)\\sqrt{S\\log\\frac{S A T}{\\delta}}}{N_{t_{k}}(X_{t})\\sqrt{N_{t_{k}}(X_{t})}}}+\\frac{3c_{0}\\log\\left(\\frac{S A T}{\\delta}\\right)}{N_{t_{k}}(X_{t})}\\,.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "(STEP 2) The number of visits $N_{k}(X_{t})$ is lower-bounded by $\\scriptstyle{\\frac{1}{2}}N_{t}(X_{t})$ when $N_{k}(X_{t})\\geq1$ by doubling trick (DT). By summing over $t$ and $k$ , we find that with probability $1-6\\delta$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k}(3k)\\leq S A c_{0}+\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}\\mathbf{1}_{N_{t_{k}}(X_{t})\\geq1}\\sqrt{\\frac{2\\mathbf{V}(p(X_{t}),h^{*})\\log\\left(\\frac{S A T}{\\delta}\\right)}{N_{t_{k}}(X_{t})}}+\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}\\mathbf{1}_{N_{t_{k}}(X_{t})\\geq1}(2\\mathbf{A}_{2}(k,t)+\\mathbf{A}_{3}(k,t)\\mathrm{)}}\\\\ &{\\displaystyle(\\mathbf{D}\\mathbf{T})\\leq S A c_{0}+2\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}\\mathbf{1}_{N_{t_{k}}(X_{t})\\geq1}\\sqrt{\\frac{2\\mathbf{V}(p(X_{t}),h^{*})\\log\\left(\\frac{S A T}{\\delta}\\right)}{N_{t}(X_{t})}}+\\sum_{k}^{t_{k+1}-1}\\mathbf{1}_{N_{t_{k}}(X_{t})\\geq1}(2\\mathbf{A}_{2}(k,t)+\\mathbf{A}_{3}(k,t)\\mathrm{)}}\\\\ &{\\displaystyle\\qquad\\leq S A c_{0}+4\\sqrt{2S A\\sum_{t=0}^{T-1}\\mathbf{V}(p(X_{t}),h^{*})\\log\\left(\\frac{S A T}{\\delta}\\right)}+\\sum_{k}^{t_{k+1}-1}\\mathbf{1}_{N_{t_{k}}(X_{t})\\geq1}(2\\mathbf{A}_{2}(k,t)+\\mathbf{A}_{3}(k,t)\\mathrm{)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the last inequality is obtained with computations that are similar to those detailed in the proof of Lemma 8. We recognize the variance that we will leave as is. We finish the proof by bounding the lower order terms $\\mathbf{A}_{2}$ and $\\mathbf{A}_{3}$ . ", "page_idx": 29}, {"type": "text", "text": "(STEP 3) We start with $\\mathbf{A}_{2}$ . We have: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}\\mathbf{1}_{N_{t_{k}}(X_{t})\\geq1}\\mathbb{A}_{2}(k,t):=\\displaystyle\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}\\mathbf{1}_{N_{t_{k}}(X_{t})\\geq1}\\frac{\\sqrt{16c_{0}\\log\\left(\\frac{S A T}{\\delta}\\right)}S\\left((1+c_{0})\\left(3+2\\sqrt{8T\\log\\left(\\frac{2}{\\delta}\\right)}+2B(T)\\right)\\right)}{N_{t_{k}}(X_{t})}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(\\mathbf{D}\\mathsf{T})\\leq2\\,\\sqrt{16c_{0}S\\log\\left(\\frac{S A T}{\\delta}\\right)\\left((1+c_{0})\\left(3+2\\,\\sqrt{8T\\log\\left(\\frac{2}{\\delta}\\right)}+2B(T)\\right)\\right)}\\,S A\\log(T)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq8(1+c_{0})S^{\\frac{3}{2}}A\\log^{\\frac{3}{2}}\\left(\\frac{S A T}{\\delta}\\right)\\left(2+4T^{\\frac{1}{4}}\\log^{\\frac{1}{4}}\\left(\\frac{S A T}{\\delta}\\right)+\\,\\sqrt{2B(T)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "(STEP 4) We are left with $\\mathbf{A}_{3}$ . We have: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{k}\\sum_{t=l_{k}}^{t_{k+1}-1}\\mathbf{1}_{N_{k}(X_{l})\\geq\\mathrm{I}}\\mathbb{A}_{3}(k,t):=\\displaystyle\\sum_{k}\\sum_{t=l_{k}}^{t_{k+1}-1}\\mathbf{1}_{N_{k}(X_{l})\\geq\\mathrm{I}}\\left(\\mathrm{sp}\\left(h^{*}\\right)\\sqrt{\\frac{2\\log\\left(\\frac{S A T}{\\delta}\\right)}{N_{k}(X_{l})\\sqrt{N_{k}(X_{l})}}}+\\frac{3c_{0}\\log\\left(\\frac{S A T}{\\delta}\\right)}{N_{k}(X_{l})}\\right)}\\\\ {\\displaystyle(\\mathbf{D}\\Gamma)\\leq\\sum_{k}^{t_{k+1}-1}\\sum_{t=l_{k}}^{t_{k+1}-1}\\mathbf{1}_{N_{k}(X_{l})\\geq\\mathrm{I}}\\left(\\mathrm{sp}\\left(h^{*}\\right)\\sqrt{\\frac{2\\log\\left(\\frac{S A T}{\\delta}\\right)\\sqrt{S\\log\\frac{S A T}{\\delta}}}{N_{k}(X_{l})\\sqrt{N_{k}(X_{l})}}}+\\frac{3c_{0}\\log\\left(\\frac{S A T}{\\delta}\\right)}{N_{k}(X_{l})}\\right)}\\\\ {\\leq C\\mathrm{sp}\\left(h^{*}\\right)S^{\\frac{5}{4}}A T^{\\frac{1}{4}}\\log^{\\frac{3}{4}}\\left(\\frac{S A T}{\\delta}\\right)+6c_{0}S A\\log\\left(\\frac{S A T}{\\delta}\\right)}\\\\ {=\\displaystyle\\left(\\mathrm{sp}\\left(h^{*}\\right)S^{\\frac{4}{4}}A T^{\\frac{1}{4}}\\log\\left(\\frac{S A T}{\\delta}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 29}, {"type": "text", "text": "C.8 Proof of Lemma 10, second order error ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Recall that by Lemma 13, with probability 1 \u22124\u03b4, $h^{*}\\in\\mathcal{H}_{t_{k}}$ for all $k$ , hence sp $(\\mathfrak{h}_{k}-h^{*})\\le2c_{0}$ for all $k$ on the same event. Therefore, with probability $1-4\\delta$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t}(4a)=2\\alpha_{0}S A+\\sum_{t}^{16\\times1}\\sum_{n_{t}=1}^{16}N_{x,\\mathrm{R},\\mathrm{R},\\mathrm{R}}(\\hat{p}_{t}(S_{t})-p_{t}(S_{t}))(\\mathfrak{h}_{t}-\\boldsymbol{k})}\\\\ &{\\qquad=2\\alpha_{0}S A+\\sum_{t}^{16\\times1}\\sum_{n_{t}=1}^{16}N_{x,\\mathrm{R},\\mathrm{R},\\mathrm{R}}(\\hat{p}_{t}(S_{t}^{\\tau}|S_{t})-p_{t}(S_{t}^{\\tau}|S_{t}))(\\mathfrak{h}_{t}-\\boldsymbol{k}^{\\tau}(\\boldsymbol{\\tau}))}\\\\ &{\\qquad\\overset{(a)}{\\le}2\\alpha_{0}S A+2\\sum_{t}^{16\\times1}\\sum_{n_{t}=1}^{16}N_{x,\\mathrm{R},\\mathrm{R}}(\\hat{p}_{t}(S_{t}^{\\tau}|S_{t})-p_{t}(S_{t}^{\\tau}|S_{t}))\\mathfrak{a}_{t}(S_{s}^{\\tau},S_{t})}\\\\ &{\\qquad\\overset{(b)}{\\le}2\\alpha_{0}S A+2\\sum_{t}^{16\\times1}\\sum_{n_{t}=1}^{16}N_{x,\\mathrm{R},\\mathrm{R}}\\bigg(\\hat{d}_{t}(S_{t}^{\\tau},S_{t})\\sqrt{\\frac{2p_{t}(S_{t}^{\\tau}|S_{t})\\log(\\frac{S_{t}^{\\tau}|S_{t}}{\\le\\alpha_{t}})}{N_{x}(S_{t})}}+3A_{t}(S_{t}^{\\tau}|S_{t})\\frac{\\log(\\frac{S_{t}^{\\tau}S A}{\\le\\alpha_{t}})}{N_{x}(S_{t})}}\\\\ &{\\qquad\\le2\\alpha_{0}S A+2\\sum_{t}^{16\\times1}\\sum_{n_{t}=1}^{16}N_{x,\\mathrm{R},\\mathrm{R}}\\bigg(\\sqrt{\\alpha_{0}}\\sqrt{\\frac{2p_{t}(S_{t}^{\\tau}|S_{t})\\mathcal{A}(S_{t}^{\\tau},S_{t})\\log(\\frac{S_{t}^{\\tau}S A}{\\le\\alpha_{t}})}{N_{x}(S_{t})}}+\\frac{3}{N\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $(*)$ uses that $h^{*}\\in\\mathcal{H}_{t_{k}}$ , and $(\\dagger)$ is obtained by applying the empirical Bernstein\u2019s inequality, see Lemma 36, to $\\hat{p}_{k}(s^{\\prime}|S_{t})-p_{k}(s^{\\prime}|S_{t})$ , and holds with probability $1-\\delta$ . The rightmost term\u2019s sum is ", "page_idx": 29}, {"type": "text", "text": "upper-bounded by: ", "page_idx": 30}, {"type": "equation", "text": "$$\n4\\sum_{k}\\sum_{t=t_{k}}^{t_{k+1}-1}\\sum_{s^{\\prime}\\in S}\\frac{3c_{0}\\log\\left(\\frac{S^{2}A T}{\\delta}\\right)}{N_{t}(X_{t})}\\leq12S^{2}A\\log(T)\\log\\left(\\frac{S^{2}A T}{\\delta}\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For the other term, follow the line of the proof of Lemma 9 (term $\\mathbf{A}_{2}$ ). We have with probability $1-5\\delta$ ( $4\\delta$ of which is by invoking Lemma 13): ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{p}_{k}(s^{\\prime}|S_{t})d_{k}(s^{\\prime},S_{t})=\\cfrac{N_{t_{k}}(S_{t},A_{t},s^{\\prime})\\big((1+c_{0})\\,\\Big(1+\\,\\sqrt{8t_{k}\\log\\Big(\\frac{2}{\\delta}\\Big)}\\Big)+2B_{0}(t_{k})\\Big)}{N_{t_{k}}(S_{t}\\leftrightarrow s^{\\prime})N_{t_{k}}(X_{t})}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\cfrac{\\big((1+c_{0})\\,\\Big(3+2\\,\\sqrt{8T\\log\\Big(\\frac{2}{\\delta}\\Big)}+2B(T)\\Big)\\Big)}{N_{t_{k}}(X_{t})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sqrt{c_{0}}\\sqrt{\\frac{2\\hat{p}_{k}(s^{\\prime}|S_{t})d_{t_{k}}(s^{\\prime},S_{t})\\log\\left(\\frac{S^{2}A T}{\\delta}\\right)}{N_{t}(X_{t})}}\\leq\\frac{4(1+c_{0})\\sqrt{\\left(3+2\\sqrt{8T\\log\\left(\\frac{2}{\\delta}\\right)}+2B(T)\\right)\\log\\left(\\frac{S^{2}A T}{\\delta}\\right)}}{N_{t}(X_{t})}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Summing over $k,\\,t,\\,s^{\\prime}$ , with probability 1 \u22126\u03b4, we have: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{k}(4k)\\leq\\left\\{16S^{2}A(1+c_{0})\\log^{\\frac{1}{2}}\\left(\\frac{S^{2}A T}{\\delta}\\right)\\left(\\sqrt{2B(T)}+2\\left(8T\\log\\left(\\frac{2}{\\delta}\\right)\\right)^{\\frac{1}{4}}\\right)\\right\\}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "D More details on experiments ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "D.1 River swim as a hard communicating environment ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Experiments of Fig. 2 are run on $n$ -states river-swim. Such MDPs are, despite their size, known to be hard to learn. They consists in $n$ states aligned in a straight line with two playable actions right and left whose dynamics are given in the figure below. Rewards are Bernoulli and null everywhere excepted for $r(s_{n},\\mathrm{{RIGHT})}=0.95$ and $r(s_{0},\\mathrm{{LEFT})=0.05}$ . ", "page_idx": 31}, {"type": "image", "img_path": "SM9IWrHz4e/tmp/e5d207cb553c5500dcfaf293dbe928e7273ae520acc13b36e88a27fca03e9f9b.jpg", "img_caption": ["Figure 3: The kernel of a $n$ -state river-swim. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "3-state river-swim. The gain is $g^{\\ast}\\approx0.82$ and $h^{*}\\approx(-4.28,-2.24,0.4)$ . ", "page_idx": 31}, {"type": "text", "text": "5-state river-swim. The gain is $g^{\\ast}\\approx0.82$ and $h^{*}\\approx(-9.62,-7.58,-4.96,-2.27,0.45).$ ", "page_idx": 31}, {"type": "text", "text": "D.2 Experiments in weakly-communicating environments ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Beyond communicating models, PMEVI-DT is superior to EVI-based methods. On Fig. 4, we see that PMEVI-DT can learn in environments of infinite diameter while UCRL2 cannot. The gain is due to the truncation operation, that makes sure that the optimistic bias vector has span less than $T^{1/5}$ . ", "page_idx": 31}, {"type": "image", "img_path": "SM9IWrHz4e/tmp/aaba6a6fafb050e1d0e8ce8639073f4a74d46ca70ecff3dd98c873925e910c5e.jpg", "img_caption": ["Figure 4: Bernoulli bandit with dandling state (weakly-communicating model). Each arrow is a choice of action whose label is the mean reward of the associated state-action pair. The learner starts in state $s_{0}$ and transitions to the absorbing state $s_{1}$ as soon as an action is played. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Without the truncation operation, EVI-based methods can never reject the plausibility that the reward at $s_{0}$ is maximal (equal to one) and that there is a positive probability $\\epsilon$ to switch from $s_{1}$ to $s_{0}$ when taking the sub-optimal action from $s_{1}$ . More precisely, denote the actions $a,b$ where $r(s_{0},a)=1$ , $r(s_{1},a)=0.5$ and $r(s_{1},b)=0.1$ , so that $a$ is the optimal action and $b$ is sub-optimal. There are two policies $\\pi_{a}$ and $\\pi_{b}$ choosing action $a$ and $b$ from $s_{1}$ respectively. Because $s_{0}$ is only visited once, the best reward that can be achieved from $s_{0}$ is $\\tilde{r}_{t}(s_{0},a)=1$ at all times. If one runs UCRL2, the confidence region for transition kernels is roughly of the form $\\begin{array}{r}{\\mathcal{P}_{t}(x)=\\{\\tilde{p}(x):N_{t}(x)\\,||\\tilde{p}(x)-\\hat{p}_{t}(x)||_{1}^{2}\\leq C_{p}\\log(t)\\}}\\end{array}$ and the plausible transition kernel $\\tilde{p}(x)\\in\\mathcal{P}_{t}(x)$ that goes the quickest from $s_{1}$ to $s_{0}$ is of the form: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\tilde{p}_{t}(s_{0}|s_{1},a)=\\sqrt{\\frac{C_{p}\\log(t)}{N_{t}(s_{1},a)}}\\quad\\mathrm{and}\\quad\\tilde{p}_{t}(s_{0}|s_{1},b)=\\sqrt{\\frac{C_{p}\\log(t)}{N_{t}(s_{1},b)}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "After running EVI for $i$ steps, the current vector $\\nu_{i}$ (see (6)) is $V_{i}^{*}(\\ensuremath{\\boldsymbol{M}}_{t})$ , the maximal amount of reward that one can collect in $i$ steps on $\\boldsymbol{\\mathcal{M}}_{t}$ seen as an extended Markov decision process, see Auer et al. [2009]. If the data is well concentrated, the optimal reward from $s_{1}$ are respectively $\\tilde{r}_{t}(s_{1},a)\\approx0.5+\\sqrt{C_{r}\\log(t)/N_{t}(s_{1},a)}$ and $\\widetilde{r}_{t}(s_{1},b)=0.1+\\sqrt{C_{r}\\log(t)/N_{t}(s_{1},b)}$ . From all this, one can argue that when $i$ is large enough, the optimistic scores of $\\pi_{a}$ and $\\pi_{b}$ over $i$ steps are roughly equal to: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V_{i}^{\\pi_{a}}(s_{1};M_{t})\\approx i+\\left(-0.5+\\sqrt{\\frac{C_{r}\\log(t)}{N_{t}(s_{1},a)}}\\right)\\sqrt{\\frac{N_{t}(s_{1},a)}{C_{p}\\log(t)}}=i-0.5\\sqrt{\\frac{N_{t}(s_{1},a)}{C_{p}\\log(t)}}+\\sqrt{\\frac{C_{r}}{C_{p}}}}\\\\ {V_{i}^{\\pi_{b}}(s_{1};M_{t})\\approx i+\\left(-0.9+\\sqrt{\\frac{C_{r}\\log(t)}{N_{t}(s_{1},b)}}\\right)\\sqrt{\\frac{N_{t}(s_{1},b)}{C_{p}\\log(t)}}=i-0.9\\sqrt{\\frac{N_{t}(s_{1},b)}{C_{p}\\log(t)}}+\\sqrt{\\frac{C_{r}}{C_{p}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "So $V_{i}^{\\pi_{a}}(s_{1};{\\mathcal M}_{t})\\leq V_{i}^{\\pi_{b}}(s_{1};M_{t})$ if $\\begin{array}{r}{N_{t}(s,b)\\ll\\frac{25}{81}N_{t}(s,a).}\\end{array}$ ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "This means that EVI will output $\\pi_{b}$ if $\\begin{array}{r}{N_{t}(s,b)\\ll\\frac{25}{81}N_{t}(s,a)}\\end{array}$ , leading to $N_{t}(s,a)\\,\\asymp\\,N_{t}(s,b)$ so both growing linearly with $t$ . This informal argument can be generalized to EVI-based algorithms with other types of confidence regions: EVI-based methods such as UCRL Auer et al. [2002], UCRL2B Fruit et al. [2020] and KLUCRL Filippi et al. [2010] will suffer from $N_{t}(s,b)=\\Theta(t)$ and their regret will grow linearly. ", "page_idx": 32}, {"type": "text", "text": "In opposition, PMEVI-based methods use truncation, making sure that $|\\nu_{i}(s_{1})-\\nu_{i}(s_{0})|\\,\\leq\\,T^{1/5}$ at all times. Intuitively, it makes PMEVI-based method \u201cthink\u201d that $\\tilde{p}_{t}(s_{0}|s_{1},b)$ cannot be as small as $\\sqrt{C_{p}\\log(t)/N_{t}(s_{1},b)}$ , because the optimistic bias of $\\pi_{b}$ would be too large otherwise; Or, equivalently, that $\\tilde{p}_{t}(s_{0}|s_{1},b)=\\,\\sqrt{C_{p}\\log(t)/N_{t}(s_{1},b)}$ but with $\\tilde{r}_{t}(s_{0},a)\\ll1$ , hence killing the optimistic reward at $(s_{0},a)$ to meet the bias constraints. ", "page_idx": 32}, {"type": "text", "text": "Overall, these features of PMEVI-based methods are shared with algorithms such as REGAL Bartlett and Tewari [2009] and SCAL Fruit et al. [2018]. The difference is that these methods require precise prior information on $\\operatorname{sp}(h^{*})$ that PMEVI-DT does not need. ", "page_idx": 32}, {"type": "text", "text": "E Standard concentration inequalities ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Lemma 32 (Azuma\u2019s inequality, Azuma [1967]). Let $(U_{t})_{t\\geq0}$ a martingale difference sequence such that sp $\\left(U_{t}\\right)\\leq c$ a.s., i.e., there exists $a_{t}\\in\\mathbf{R}$ such that $a_{t}\\leq U_{t}\\leq a_{t}+c$ a.s. Then, for all $\\delta>0$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{P}\\Big(\\sum_{t=0}^{T-1}U_{t}\\geq c\\,\\sqrt{\\frac{1}{2}T\\log\\left(\\frac{1}{\\delta}\\right)}\\Big)\\leq\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Lemma 33 (Freedman\u2019s inequality, Zhang et al. [2020]). Let $(U_{t})_{t\\geq0}$ a martingale difference sequence such that $|U_{t}|\\leq c$ a.s., and denote its conditional variance $V_{t}:=\\mathbf{E}[U_{t}^{2}|\\mathcal{F}_{t-1}]$ . Then, for all $\\delta>0$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbf{P}\\left(\\exists T^{\\prime}\\le T:\\sum_{t=0}^{T^{\\prime}-1}U_{t}\\ge\\sqrt{2\\sum_{t=0}^{T^{\\prime}-1}V_{t}\\log\\left(\\frac{T}{\\delta}\\right)}+4c\\log\\left(\\frac{T}{\\delta}\\right)\\right)\\le\\delta.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Lemma 34 (Time-uniform Azuma, Bourel et al. [2020]). Let $(U_{t})$ a martingale difference sequence such that, for all $\\lambda\\in\\mathbb{R},$ $\\begin{array}{r}{:{\\bf R},\\,{\\bf E}[\\exp(\\lambda U_{t})|U_{1},\\dots,U_{t-1}]\\leq\\exp(\\frac{\\lambda^{2}\\sigma^{2}}{2})}\\end{array}$ . Then: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\forall\\delta>0,\\quad\\mathbf{P}\\left(\\exists n\\geq1,\\quad\\left(\\sum_{k=1}^{n}U_{k}\\right)^{2}\\geq n\\sigma^{2}\\left(1+\\frac{1}{n}\\right)\\log\\left(\\frac{\\sqrt{1+n}}{\\delta}\\right)\\right)\\leq\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Lemma 35 (Time-uniform Weissman). Let $q$ a distribution over $\\{1,\\ldots,d\\}$ . Let $(U_{t})$ a sequence of i.i.d. random variables of distribution $q$ . Then: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\forall\\delta>0,\\quad\\mathbf{P}\\left(\\exists n\\geq1,\\left\\Vert\\sum_{i=1}^{n}\\left(e_{U_{i}}-q\\right)\\right\\Vert_{1}^{2}\\geq n d\\log\\left(\\frac{2\\sqrt{1+n}}{\\delta}\\right)\\right)\\leq\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. Remark that $\\begin{array}{r l}{\\left\\|\\sum_{k=1}^{n}(e_{U_{k}}-q)\\right\\|_{1}=\\operatorname*{max}_{\\nu\\in\\{-1,1\\}^{d}}\\sum_{k=1}^{n}\\left\\langle e_{U_{k}}-q,\\nu\\right\\rangle}\\end{array}$ . Let $W_{k}^{\\nu}:=\\left\\langle e_{U_{k}}-q,\\nu\\right\\rangle$ . Remark that for each $\\nu\\,\\in\\,\\{-1,1\\}^{d}$ , $(W_{k}^{\\nu})$ is a family of i.i.d. random variables with $-\\left\\langle q,\\nu\\right\\rangle\\,\\leq\\,W_{k}^{\\nu}\\,\\leq$ $1-\\langle q,\\nu\\rangle$ , so $\\begin{array}{r}{\\mathbf{E}[\\exp(\\lambda W_{k}^{\\nu})]\\leq\\exp(\\frac{\\lambda^{2}}{8})}\\end{array}$ by Hoeffding\u2019s Lemma. By Lemma 34, we have: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbf{P}\\left(\\exists n\\geq1,\\left\\|\\sum_{k=1}^{n}(e_{U_{k}}-q)\\right\\|_{1}\\geq\\sqrt{n d\\log\\left(\\frac{2\\sqrt{1+n}}{\\delta}\\right)}\\right)=\\mathbf{P}\\left(\\exists\\nu\\in\\{-1,1\\}^{d},\\exists n,\\sum_{k=1}^{n}W_{k}^{\\nu}\\geq\\sqrt{n d\\log\\left(\\frac{2\\sqrt{1+n}}{\\delta}\\right)}\\right)}}\\\\ &{}&{\\leq\\sum_{\\nu\\in\\{-1,1\\}^{d}}\\mathbf{P}\\left(\\exists n,\\sum_{k=1}^{n}W_{k}^{\\nu}\\geq\\sqrt{n d\\log\\left(\\frac{2\\sqrt{1+n}}{\\delta}\\right)}\\right)}\\\\ &{}&{\\leq\\sum_{\\nu\\in\\{-1,1\\}^{d}}\\mathbf{P}\\left(\\exists n,\\sum_{k=1}^{n}W_{k}^{\\nu}\\geq\\sqrt{\\frac{1}{2}n\\left(1+\\frac{1}{n}\\right)\\log\\left(\\frac{\\sqrt{1+n}}{2^{-d}\\delta}\\right)}\\right)}\\\\ &{}&{\\leq2^{d}\\cdot2^{d}\\delta=\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 33}, {"type": "text", "text": "Lemma 36 (Time-uniform Empirical Bernstein). Let $(U_{k})_{k\\geq1}$ a martingale difference sequence such that sp $\\left(U_{n}\\right)\\,\\leq\\,c$ a.s., let $\\begin{array}{r}{\\hat{U}_{n}\\;:=\\;\\frac{1}{n}\\sum_{k=1}^{n}U_{k}}\\end{array}$ the empirical mean and $\\begin{array}{r}{\\hat{V}_{n}:=\\stackrel{\\cdot}{\\frac{1}{n}}\\sum_{k=1}^{n}(U_{k}\\stackrel{\\cdot}{-}\\hat{U}_{n})^{2}}\\end{array}$ the population variance. Then, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\forall\\delta>0,\\forall T>0,\\quad\\mathbf{P}\\left(\\exists t\\leq T,\\sum_{i=1}^{t}U_{i}\\geq\\,\\sqrt{2t\\hat{V}_{t}\\log\\left(\\frac{3T}{\\delta}\\right)}+3c\\log\\left(\\frac{3T}{\\delta}\\right)\\right)\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. This is obtained with a union bound on the values of $n\\leq T$ , then applying Lemma 38. \u25a1 ", "page_idx": 33}, {"type": "text", "text": "Lemma 37 (Time-uniform Empirical Likelihoods, Jonsson et al. [2020]). Let q a distribution on $\\{1,\\ldots,d\\}$ . Let $(U_{t})$ a sequence of i.i.d. random variables of distribution $q$ . Then: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\forall\\delta>0,\\quad\\mathbf{P}\\left(\\exists n\\geq1,n\\operatorname{KL}(\\hat{q}_{n}\\|q)>\\log\\left(\\frac{1}{\\delta}\\right)+(d-1)\\log\\left(e\\left(1+\\frac{n}{d-1}\\right)\\right)\\right)\\leq\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Lemma 38 (Empirical Bernstein inequality, Audibert et al. [2009]). Let $(U_{k})_{k\\geq1}$ a martingale difference sequence such that $\\operatorname{sp}\\left(U_{n}\\right)\\leq c\\,a.s.$ ., let $\\begin{array}{r}{\\hat{U}_{n}:=\\frac{1}{n}\\sum_{k=1}^{n}U_{k}}\\end{array}$ the empirical mean and $\\overset{\\sim}{V_{n}}:=$ $\\begin{array}{r}{\\frac{1}{n}\\sum_{k=1}^{n}(U_{k}-{\\hat{U}}_{n})^{2}}\\end{array}$ the population variance. Then, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\forall\\delta>0,\\forall n\\geq1,\\quad\\mathbf{P}\\left(\\sum_{k=1}^{n}U_{k}\\geq\\,\\sqrt{2n\\hat{V}_{n}\\log\\left(\\frac{3}{\\delta}\\right)}+3c\\log\\left(\\frac{3}{\\delta}\\right)\\right)\\leq\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Lemma 39 (Bennett\u2019s inequality, Audibert et al. [2009]). Let $(U_{t})_{t\\geq0}$ a martingale difference sequence such that $|U_{t}|\\leq c$ a.s., and denote its conditional variance $V_{t}:=\\mathbf{E}[U_{t}^{2}|\\mathcal{F}_{t-1}]$ . Then, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\forall\\delta>0,\\forall n\\geq1,\\quad\\mathbf{P}\\bigg(\\exists k\\leq n,\\sum_{i=1}^{k}U_{i}\\geq\\,\\sqrt{2\\sum_{i=1}^{n}V_{i}\\log\\Big(\\frac{1}{\\delta}\\Big)}+\\frac{1}{3}c\\log\\Big(\\frac{1}{\\delta}\\Big)\\bigg)\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Lemma 40 (Lemma 3 of Zhang and Xie [2023]). Let $\\left(U_{t}\\right)$ be a sequence of random variables such that $0\\leq U_{t}\\leq c$ a.s., and let $\\mathcal{F}_{t}:=\\sigma(U_{0},U_{1},\\ldots,U_{t-1})$ . Then: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall\\delta>0,\\quad\\mathbf{P}\\left(\\exists T\\geq0,\\sum_{t=0}^{T-1}U_{t}\\geq3\\sum_{t=0}^{T-1}\\mathbf{E}[U_{t}|\\mathcal{F}_{t-1}]+c\\log\\left(\\frac{1}{\\delta}\\right)\\right)\\leq\\delta;}\\\\ &{\\forall\\delta>0,\\quad\\mathbf{P}\\left(\\exists T\\geq0,\\sum_{t=0}^{T-1}\\mathbf{E}[U_{t}|\\mathcal{F}_{t-1}]\\geq3\\sum_{t=0}^{T-1}U_{t}+c\\log\\left(\\frac{1}{\\delta}\\right)\\right)\\leq\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We claim that the algorithm reaches minimax optimal regret, and it does. We claim that our algorithm can effectively use bias information, and the experimental illustration supports the claim. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 35}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper is a theoretical one that covers all weakly communicating MDPs, and although it may help the design of algorithms intended to be applicable in real-life scenarios, our method is not meant to be directly applied. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 35}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: To every result is attached a proof or a reference. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 36}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The algorithmic content in the main body is detailed enough to implement the algorithm, the exception being the confidence intervals and the implementation of EVI (because those come from already existing works). The confidence intervals are discussed in a dedicated section in the Appendix, while the works of Auer et al. [2009], Filippi et al. [2010], Fruit et al. [2020] etc. provide detailed ready-to-implement pseudo-code for EVI. Our code is mostly written in Python. Experiments took a few dozen minutes on a low end laptop. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 36}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 37}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The code is provided in the supplementary material, together with the scripts to reproduce the exact figures of the paper. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 37}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: People that are unfamiliar with Markov decision processes may not know the river-swim setting in which experiments are driven, but it is not required to understand the figures and the discussion. Moreover, a description of the environment (river-swim) is provided in the appendix in addition to be referenced. See also Experimental Result Reproducibility for more. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 37}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [No] ", "page_idx": 37}, {"type": "text", "text": "Justification: We did a hundred runs for every algorithm, which is enough to get statistical significance for the small sized problems that we experiment on. Also, experiments are mostly illustrative and do not account for an extensive numerical validation of the theoretical results. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 38}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [No] ", "page_idx": 38}, {"type": "text", "text": "Justification: This is mostly a theoretical paper. Most of the computational cost of the work is processing $\\mathrm{{I}A l}_{\\mathrm{{E}}}\\mathrm{{X}}$ and rendering a PDF. Like said earlier, experiments all together took less than a hour on a low end laptop. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 38}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: - ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 38}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 39}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: It\u2019s mainly a theoretical work on small sized Markov decision processes. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 39}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: - ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 39}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: - Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 40}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] Justification: - Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 40}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] Justification: - Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 40}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] Justification: - Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 41}]