[{"figure_path": "SM9IWrHz4e/tables/tables_2_1.jpg", "caption": "Table 1: Comparison of related works on RL algorithms for average-reward MDP, where S \u00d7 A is the size of state-action space, T is the total number of steps, D (Ds) is the (local) diameter, sp (h*) \u2264 D is the span of the bias vector, tmix is the worst-case mixing time, thit is the hitting time (i.e., the expected time cost to visit some certain state under any policy).", "description": "This table compares the performance of several reinforcement learning algorithms used for average-reward Markov Decision Processes (MDPs).  It lists various characteristics for each algorithm, including the regret bound (a measure of performance), whether it is computationally tractable, and any specific requirements or assumptions made. The regret bound is expressed using several variables: S (number of states), A (number of actions), T (number of steps), D (diameter of the MDP, or local diameter Ds), and sp(h*) (span of the optimal bias function). Other details such as Bayesian regret, requirements for knowledge of sp(h*), and comments about specific algorithm properties are also included.", "section": "Related works on average reward MDPs"}, {"figure_path": "SM9IWrHz4e/tables/tables_4_1.jpg", "caption": "Table 1: Comparison of related works on RL algorithms for average-reward MDP, where S \u00d7 A is the size of state-action space, T is the total number of steps, D (Ds) is the (local) diameter, sp (h*) \u2264 D is the span of the bias vector, tmix is the worst-case mixing time, thit is the hitting time (i.e., the expected time cost to visit some certain state under any policy).", "description": "This table compares various reinforcement learning algorithms for average-reward Markov Decision Processes (MDPs).  It shows the regret bounds achieved by each algorithm, indicating their efficiency in terms of the number of steps (T), state-action space (SxA), diameter (D), and bias span (sp(h*)). It also notes whether the algorithms are tractable and what prior knowledge, if any, is required. The table helps illustrate the state-of-the-art and the improvement achieved by the proposed PMEVI-DT algorithm.", "section": "Related works on average reward MDPs"}, {"figure_path": "SM9IWrHz4e/tables/tables_6_1.jpg", "caption": "Table 1: Comparison of related works on RL algorithms for average-reward MDP, where S \u00d7 A is the size of state-action space, T is the total number of steps, D (Ds) is the (local) diameter, sp (h*) \u2264 D is the span of the bias vector, tmix is the worst-case mixing time, thit is the hitting time (i.e., the expected time cost to visit some certain state under any policy).", "description": "This table compares various reinforcement learning algorithms for average-reward Markov Decision Processes (MDPs). It shows the regret bounds achieved by each algorithm, indicating their performance in terms of time and space complexity.  Key parameters include the size of the state-action space (SxA), total steps (T), diameter (D or Ds), bias span (sp(h*)), mixing time (tmix), and hitting time (thit).  The table also notes whether each algorithm is tractable and the prior knowledge required.", "section": "Related works on average reward MDPs"}, {"figure_path": "SM9IWrHz4e/tables/tables_7_1.jpg", "caption": "Table 1: Comparison of related works on RL algorithms for average-reward MDP, where S \u00d7 A is the size of state-action space, T is the total number of steps, D (Ds) is the (local) diameter, sp (h*) \u2264 D is the span of the bias vector, tmix is the worst-case mixing time, thit is the hitting time (i.e., the expected time cost to visit some certain state under any policy).", "description": "This table compares different reinforcement learning algorithms for average-reward Markov Decision Processes (MDPs).  It shows the regret bounds achieved by each algorithm, indicating their computational tractability and whether they require prior knowledge of the MDP. Key parameters influencing the regret are also included, such as the size of the state-action space, the number of steps, the diameter of the MDP, the span of the bias function, mixing time, and hitting time.  The table helps to situate the proposed algorithm (PMEVI-DT) within the existing literature.", "section": "Related works on average reward MDPs"}]