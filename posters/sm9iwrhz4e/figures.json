[{"figure_path": "SM9IWrHz4e/figures/figures_8_1.jpg", "caption": "Figure 1: An overview of PMEVI-DT and its regret analysis. In the above, gk and hk are the optimistic gain and bias functions produced by PMEVI (see Algorithm 2) at episode k, and it and ptk are respectively the empirical and optimistic kernel models at episode k.", "description": "This figure provides a visual representation of the algorithm PMEVI-DT and how its regret is analyzed.  It breaks down the total regret into four main components: navigation error, empirical bias error, optimistic overshoot, and second-order error.  Each component is linked to a specific lemma in the paper that details its theoretical bound. The figure shows how these components contribute to the overall regret of the algorithm, illustrating the different sources of error and their respective magnitudes.  The diagram effectively summarizes the key steps of the regret analysis presented in the paper. It visually clarifies the relationships between different sources of errors, and also shows that the analysis takes into consideration several scenarios and the different errors in each one.  The use of a diagram is beneficial as a summarization of the theoretical contributions, and aids in grasping the main points of the paper.", "section": "Regret guarantees"}, {"figure_path": "SM9IWrHz4e/figures/figures_9_1.jpg", "caption": "Figure 2: (To the left) Running UCRL2 and PMEVI-DT with the same confidence region than UCRL2 on a 3-state river-swim. PMEVI-DT is run with prior knowledge h*(s1) \u2264 h*(s2) \u2264 h*(s3) \u2212 2c for c \u2208 {0, 0.5, 1, 1.5, 2}. (To the right) Running a few algorithms of the literature on 5-state river-swim and comparing their average regret against their PMEVI variants, obtained by changing calls to the EVI sub-routine to calls to PMEVI.", "description": "The figure consists of two subfigures. The left subfigure shows the performance comparison of UCRL2 and PMEVI-DT algorithms on a 3-state river-swim environment. PMEVI-DT is tested with different levels of prior knowledge about the bias function. The right subfigure shows the comparison of several algorithms (UCRL2, KL-UCRL, UCRL2B and their PMEVI variants) on a 5-state river-swim environment.", "section": "Experimental illustrations"}, {"figure_path": "SM9IWrHz4e/figures/figures_31_1.jpg", "caption": "Figure 3: The kernel of a n-state river-swim.", "description": "This figure shows the transition probabilities between states in a river-swim Markov Decision Process (MDP).  In this MDP, there are *n* states arranged linearly. The agent can choose to move either RIGHT or LEFT. The transition probabilities are depicted by arrows between states, labeled with probabilities.  Rewards are Bernoulli, with a reward of 0.95 only for the action RIGHT in the final state (s<sub>n</sub>) and a reward of 0.05 only for the action LEFT in the initial state (s<sub>0</sub>).  All other state-action pairs yield a reward of 0.", "section": "5 Experimental illustrations"}, {"figure_path": "SM9IWrHz4e/figures/figures_31_2.jpg", "caption": "Figure 2: (To the left) Running UCRL2 and PMEVI-DT with the same confidence region than UCRL2 on a 3-state river-swim. PMEVI-DT is run with prior knowledge h*(s1) \u2264 h*(s2) \u2264 h*(s3) - 2c for c \u2208 {0, 0.5, 1, 1.5, 2}. (To the right) Running a few algorithms of the literature on 5-state river-swim and comparing their average regret against their PMEVI variants, obtained by changing calls to the EVI sub-routine to calls to PMEVI.", "description": "The left graph shows the regret of UCRL2 and PMEVI-DT (with different prior knowledge) on a 3-state river-swim environment.  The right graph compares several algorithms' average regret (UCRL2, KL-UCRL, UCRL2B) against their PMEVI-enhanced versions on a 5-state river-swim environment.  The graphs illustrate the impact of prior knowledge and PMEVI subroutine on regret performance.", "section": "Experimental illustrations"}]