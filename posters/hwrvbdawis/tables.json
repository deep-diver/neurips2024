[{"figure_path": "hWRVbdAWiS/tables/tables_5_1.jpg", "caption": "Table 1: Average normalized scores on D4RL benchmark tasks. Results of BC, CQL, IQL, and IQL+EDP are taken directly from Kang et al. [21], and all other results are taken from their original papers. Our results are reported by averaging 5 random seeds.", "description": "This table presents a comparison of the average normalized scores achieved by various offline reinforcement learning methods on four different D4RL benchmark domains (Gym, AntMaze, Adroit, and Kitchen).  The table includes both existing methods from literature and the proposed method from this paper.  The scores represent the average performance across multiple random seeds, providing a robust comparison across different algorithms and tasks. ", "section": "4.2 Comparison with other Methods"}, {"figure_path": "hWRVbdAWiS/tables/tables_6_1.jpg", "caption": "Table 1: Average normalized scores on D4RL benchmark tasks. Results of BC, CQL, IQL, and IQL+EDP are taken directly from Kang et al. [21], and all other results are taken from their original papers. Our results are reported by averaging 5 random seeds.", "description": "This table presents the average normalized scores achieved by various offline reinforcement learning methods across multiple benchmark tasks from the D4RL dataset.  The methods compared include behavior cloning (BC), conservative Q-learning (CQL), Implicit Q-learning (IQL), IQL with efficient diffusion policies (IQL+EDP), Diffusion-QL, and the proposed method in the paper.  The results are averaged across five independent random seeds to ensure statistical reliability. The table is broken down by task category (Gym, Adroit, Kitchen) to allow for easier comparison of performance within each task type.", "section": "4.2 Comparison with other Methods"}, {"figure_path": "hWRVbdAWiS/tables/tables_6_2.jpg", "caption": "Table 2: Average normalized scores on D4RL AntMaze tasks. Results of BC, DT, CQL, IQL, and IQL+EDP are taken directly from Kang et al. [21], and all other results are taken from their original papers. Our results are reported by averaging 5 random seeds.", "description": "This table presents the average normalized scores achieved by various offline reinforcement learning methods on AntMaze tasks from the D4RL benchmark.  It compares the proposed method's performance against several baselines, including behavior cloning, conservative Q-learning, and other diffusion-based approaches. The results are averaged across five random seeds to assess the statistical significance and robustness of the methods.", "section": "4.2 Comparison with other Methods"}, {"figure_path": "hWRVbdAWiS/tables/tables_7_1.jpg", "caption": "Table 1: Average normalized scores on D4RL benchmark tasks. Results of BC, CQL, IQL, and IQL+EDP are taken directly from Kang et al. [21], and all other results are taken from their original papers. Our results are reported by averaging 5 random seeds.", "description": "This table presents the average normalized scores achieved by various offline reinforcement learning methods on four benchmark datasets (Gym, AntMaze, Adroit, and Kitchen) from the D4RL benchmark.  The table compares the performance of the proposed method against several state-of-the-art baselines, highlighting the improvement in performance achieved by the proposed approach.  The results are averaged over five independent random seeds to provide a robust assessment.", "section": "4.2 Comparison with other Methods"}, {"figure_path": "hWRVbdAWiS/tables/tables_7_2.jpg", "caption": "Table 2: Average normalized scores on D4RL AntMaze tasks. Results of BC, DT, CQL, IQL, and IQL+EDP are taken directly from Kang et al. [21], and all other results are taken from their original papers. Our results are reported by averaging 5 random seeds.", "description": "This table presents the average normalized scores achieved by different offline reinforcement learning algorithms on four AntMaze tasks from the D4RL benchmark.  The algorithms compared include behavior cloning (BC), Decision Transformer (DT), Conservative Q-Learning (CQL), Implicit Q-Learning (IQL), IQL with Entropy-Regularized Diffusion Policy (IQL+EDP), Diffusion-QL, and the proposed method (Ours).  The results are averaged over five random seeds to account for stochasticity.  The table highlights the performance improvements achieved by the proposed method over existing state-of-the-art approaches.", "section": "4.2 Comparison with other Methods"}, {"figure_path": "hWRVbdAWiS/tables/tables_8_1.jpg", "caption": "Table 5: Computational time comparison with different settings on Antmaze-medium-play-v0. Training time is for 1 epoch (1000 training steps) and eval time is for 1000 RL steps.", "description": "This table compares the training and evaluation times for different model configurations on the Antmaze-medium-play-v0 task.  The configurations vary the number of diffusion steps (T), and the number of critics (M) used in the Q-ensemble. It shows how these parameters affect the time needed for both training and evaluation.", "section": "4.2 Comparison with other Methods"}, {"figure_path": "hWRVbdAWiS/tables/tables_17_1.jpg", "caption": "Table 1: Average normalized scores on D4RL benchmark tasks. Results of BC, CQL, IQL, and IQL+EDP are taken directly from Kang et al. [21], and all other results are taken from their original papers. Our results are reported by averaging 5 random seeds.", "description": "This table presents a comparison of the average normalized scores achieved by different offline reinforcement learning methods on four benchmark datasets from D4RL (Gym, AntMaze, Adroit, and Kitchen).  Each dataset includes multiple tasks, and the table shows the average performance across all tasks in each domain.  The results for several baselines (BC, BCQ, BEAR, CQL, IQL, IQL+EDP, Diff-QL) are sourced directly from the referenced papers and are included for comparison.  The 'Ours' column represents the results obtained using the method proposed in this paper, averaged over five independent training runs with different random seeds.", "section": "4.2 Comparison with other Methods"}, {"figure_path": "hWRVbdAWiS/tables/tables_17_2.jpg", "caption": "Table 2: Average normalized scores on D4RL AntMaze tasks. Results of BC, DT, CQL, IQL, and IQL+EDP are taken directly from Kang et al. [21], and all other results are taken from their original papers. Our results are reported by averaging 5 random seeds.", "description": "This table presents the average normalized scores achieved by different offline reinforcement learning methods on four AntMaze tasks from the D4RL benchmark.  The table compares the performance of the proposed method against several baselines, including behavior cloning (BC), Decision Transformer (DT), Conservative Q-learning (CQL), Implicit Q-learning (IQL), and IQL enhanced with an efficient diffusion policy (IQL+EDP).  The scores are normalized and averaged over five random seeds to provide a robust comparison.", "section": "4.2 Comparison with other Methods"}, {"figure_path": "hWRVbdAWiS/tables/tables_19_1.jpg", "caption": "Table 8: Ablation study of LCB coefficients \u03b2.", "description": "This table presents the ablation study results focusing on the impact of different LCB coefficients (\u03b2) on the AntMaze environment.  It shows the average normalized scores and standard deviations for Antmaze-medium-play-v0 and Antmaze-medium-diverse-v0 tasks, with three different \u03b2 values (1, 2, and 4). The average performance across both tasks is also provided for each \u03b2 value.", "section": "4.3 Analysis and Discussion"}, {"figure_path": "hWRVbdAWiS/tables/tables_19_2.jpg", "caption": "Table 9: Ablation study of diffusion step T.", "description": "This table presents the ablation study results on varying the number of diffusion steps (T) for different tasks.  It shows that while increasing the number of steps generally improves performance, five steps (T=5) provide the best balance across different tasks and between performance and computational time.", "section": "4.2 Comparison with other Methods"}, {"figure_path": "hWRVbdAWiS/tables/tables_19_3.jpg", "caption": "Table 10: Ablation study of \"Max Q trick\"", "description": "This table presents the ablation study results on the effect of using the Max Q-backup trick on AntMaze tasks.  The results are presented as average normalized scores with standard deviations, comparing performance with and without the Max Q-backup technique across four different AntMaze environments.", "section": "4.2 Comparison with other Methods"}, {"figure_path": "hWRVbdAWiS/tables/tables_20_1.jpg", "caption": "Table 1: Average normalized scores on D4RL benchmark tasks. Results of BC, CQL, IQL, and IQL+EDP are taken directly from Kang et al. [21], and all other results are taken from their original papers. Our results are reported by averaging 5 random seeds.", "description": "This table presents a comparison of the average normalized scores achieved by various offline reinforcement learning methods on four different D4RL benchmark domains (Gym, AntMaze, Adroit, and Kitchen).  The scores represent the performance of each algorithm on several tasks within each domain, showcasing the relative performance gains of the proposed method (Ours) compared to existing baselines.  The results are averaged across five different random seeds to ensure statistical robustness.", "section": "4.2 Comparison with other Methods"}]