[{"figure_path": "hWRVbdAWiS/figures/figures_1_1.jpg", "caption": "Figure 1: A toy RL task in which the agent sequentially takes two steps (starting from 0) to seek a state with the highest reward. Left: The reward function is a mixture of Gaussian, and the offline data distribution is unbalanced with most samples located in low-reward states. Center: Training different policies on this task with 5 random seeds for 500 epochs. We find that a diffusion policy with entropy regularization and Q-ensembles yields the best results with low training variance. Right: Learned Q-value curve for the first step actions in state 0. The approximation of the lower confidence bound (LCB) of Q-ensembles is also plotted.", "description": "This figure shows a simple reinforcement learning task where an agent needs to move from state 0 to a high reward state in two steps. The reward function is a mixture of Gaussians, and the offline dataset is imbalanced (more samples in low-reward states). The figure compares three different offline RL policies trained for 500 epochs: a standard diffusion policy, a diffusion policy with entropy regularization, and a diffusion policy with both entropy regularization and Q-ensembles.  The results show that adding entropy regularization and Q-ensembles significantly improves the performance and reduces the variance across different random seeds.  Finally, the Q-value functions learned by the different methods are compared to illustrate the benefits of using Q-ensembles and approximating their lower confidence bound (LCB) for more pessimistic Q-value estimation.", "section": "Introduction"}, {"figure_path": "hWRVbdAWiS/figures/figures_3_1.jpg", "caption": "Figure 2: Comparison of the reverse-time SDE and optimal sampling process in data reconstruction.", "description": "This figure compares the performance of the standard reverse-time stochastic differential equation (SDE) process and the proposed optimal sampling method for reconstructing data.  The top row shows the original data distribution, Gaussian noise, and reconstruction using the reverse-time SDE with 5 diffusion steps. The bottom row shows reconstructions using the reverse-time SDE and the optimal sampling method with increasing numbers of diffusion steps (10, 30, and 50). The orange line represents the true data distribution, while the teal points represent the generated samples. The figure demonstrates that the optimal sampling method converges more quickly and efficiently to the true distribution, requiring fewer steps to achieve accurate reconstruction.", "section": "3.1 Optimal Sampling with Mean-Reverting SDE"}, {"figure_path": "hWRVbdAWiS/figures/figures_8_1.jpg", "caption": "Figure 3: Ablation experiments of our method with different values of LCB coefficient \u03b2 = 1,2, 4 on AntMaze-Medium environments over 5 different random seeds.", "description": "This figure presents the results of ablation studies conducted to evaluate the impact of different LCB (Lower Confidence Bound) coefficient values (\u03b2 = 1, 2, and 4) on the performance of the proposed method.  The experiments were performed on AntMaze-Medium environments, and the results are averaged across 5 different random seeds. The figure visually demonstrates how the choice of \u03b2 influences the normalized score achieved by the model. This helps in understanding the impact of the level of pessimism introduced by the Q-ensembles on the exploration-exploitation balance and overall performance.", "section": "4.3 Analysis and Discussion"}, {"figure_path": "hWRVbdAWiS/figures/figures_8_2.jpg", "caption": "Figure 4: Learning curves of the Diffusion-QL and our method on selected AntMaze tasks over 5 random seeds.", "description": "This figure displays the training progress curves for both the Diffusion-QL method and the proposed method (referred to as \"Our\" in the legend) on four different AntMaze tasks. Each curve represents the average normalized score across five random seeds, providing a measure of the stability and performance consistency of each approach.  The shaded regions surrounding the lines indicate the standard deviation of the scores across the five runs.  The figure shows that the proposed method exhibits more stable and consistent learning compared to Diffusion-QL, particularly in the later stages of training, resulting in higher average scores.", "section": "4.3 Analysis and Discussion"}, {"figure_path": "hWRVbdAWiS/figures/figures_16_1.jpg", "caption": "Figure 5: Visualization of the workings of the mean-reverting SDE for action prediction. The SDE models the degradation process from the action from the dataset to a noise. By guiding the policy with corresponding reverse-time SDE and the LCB of Q, a new action is generated conditioned on the RL state.", "description": "This figure illustrates the forward and reverse processes of the mean-reverting stochastic differential equation (SDE) used for action prediction in the proposed method.  The forward process shows how actions from the dataset are gradually degraded to pure noise through the SDE. The reverse process, which is crucial for action generation, samples actions from the noise by reversing the time of the SDE.  This reversed process is conditioned on the current environment state and guided by the lower confidence bound (LCB) of the Q-ensembles, ensuring that the generated actions are grounded in the estimated value function while encouraging exploration of unseen action spaces.", "section": "A.5 Visualization"}, {"figure_path": "hWRVbdAWiS/figures/figures_17_1.jpg", "caption": "Figure 2: Comparison of the reverse-time SDE and optimal sampling process in data reconstruction.", "description": "This figure compares the performance of the standard reverse-time SDE process and the proposed optimal sampling method for reconstructing data. It shows that the optimal sampling method achieves faster convergence with fewer steps (N=5) compared to the reverse-time SDE (N=30, N=50). The figure visually demonstrates the efficiency of the proposed method, highlighting its advantage in terms of sample efficiency.", "section": "3.1 Optimal Sampling with Mean-Reverting SDE"}, {"figure_path": "hWRVbdAWiS/figures/figures_18_1.jpg", "caption": "Figure 7: A t-SNE visualization of randomly selected 1000 states from Antmaze, Adroit and Kitchen domain. The color coding represents the return of the trajectory associated with each state.", "description": "This figure uses t-SNE to visualize 1000 randomly sampled states from three different D4RL benchmark domains: AntMaze, Adroit, and Kitchen. Each point represents a state, and its color indicates the cumulative reward obtained from that state.  The visualization helps to understand the distribution of states and their associated rewards within each domain, providing insights into the complexity and characteristics of the environments.", "section": "Additional Experiments Details"}, {"figure_path": "hWRVbdAWiS/figures/figures_18_2.jpg", "caption": "Figure 1: A toy RL task in which the agent sequentially takes two steps (starting from 0) to seek a state with the highest reward. Left: The reward function is a mixture of Gaussian, and the offline data distribution is unbalanced with most samples located in low-reward states. Center: Training different policies on this task with 5 random seeds for 500 epochs. We find that a diffusion policy with entropy regularization and Q-ensembles yields the best results with low training variance. Right: Learned Q-value curve for the first step actions in state 0. The approximation of the lower confidence bound (LCB) of Q-ensembles is also plotted.", "description": "This figure shows a simple RL task where an agent takes two steps to reach a high-reward state. The reward function is a mixture of Gaussians, and the offline dataset has an imbalanced distribution with more samples in low-reward areas. Three subplots illustrate the reward function, training curves for different policies, and learned Q-values with confidence bounds.  The results demonstrate the superiority of a diffusion policy using entropy regularization and Q-ensembles, achieving better performance and lower training variance.", "section": "1 Introduction"}]