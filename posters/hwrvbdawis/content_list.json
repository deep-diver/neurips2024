[{"type": "text", "text": "Entropy-regularized Diffusion Policy with Q-Ensembles for Offline Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ruoqi Zhang\u2217 Ziwei Luo\u2217 Jens Sj\u00f6lund Thomas B. Sch\u00f6n Per Mattsson ", "page_idx": 0}, {"type": "text", "text": "Department of Information Technology, Uppsala University {ruoqi.zhang,ziwei.luo,jens.sjolund,thomas.schon,per.mattsson}@it.uu.se ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion policy has shown a strong ability to express complex action distributions in offline reinforcement learning (RL). However, it suffers from overestimating Q-value functions on out-of-distribution (OOD) data points due to the offline dataset limitation. To address it, this paper proposes a novel entropy-regularized diffusion policy and takes into account the confidence of the Q-value prediction with Q-ensembles. At the core of our diffusion policy is a mean-reverting stochastic differential equation (SDE) that transfers the action distribution into a standard Gaussian form and then samples actions conditioned on the environment state with a corresponding reverse-time process. We show that the entropy of such a policy is tractable and that can be used to increase the exploration of OOD samples in offline RL training. Moreover, we propose using the lower confidence bound of Q-ensembles for pessimistic Q-value function estimation. The proposed approach demonstrates state-of-the-art performance across a range of tasks in the D4RL benchmarks, significantly improving upon existing diffusion-based policies. The code is available at https://github.com/ruoqizzz/entropy-offlineRL. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Offline reinforcement learning (RL), also known as batch RL [29] focuses on learning optimal policies from a previously collected dataset without further active interactions with the environment [31]. Although offline RL offers a promising avenue for deploying RL in real-world settings where online exploration is infeasible, a key challenge lies in deriving effective policies from fixed datasets, which usually are diversified and sub-optimal. The direct application of standard policy improvement approaches is hindered by the distribution shift problem [10]. Previous works mainly address this issue by either regularizing the learned policy close to the behavior policy [10, 9] or by making conservative updates for Q-networks [28, 26]. ", "page_idx": 0}, {"type": "text", "text": "Diffusion models have rapidly become a prominent class of highly expressive policies in offline RL [44, 49]. While this expressiveness is beneficial when modeling complex behaviors, it also means that the model has a higher capacity to overfti the noise or specific idiosyncrasies in the training data. To address this, existing work introduce Q-learning guidance and regard the diffusion loss as a special regularizer adding to the policy improvement process [44, 17, 22]. Such a framework has achieved impressive results on offilne RL tasks. However, its performance is limited by pre-collected datasets (or behavior policies) and the learning suffers severe overestimation of Q-value functions on unseen state-action samples [31]. ", "page_idx": 0}, {"type": "text", "text": "One promising approach is to increase exploration for out-of-distribution (OOD) actions, with the hope that the RL agent can be more robust to diverse Q-values and estimation errors [50]. Previous online RL algorithms achieve this by maximizing the entropy of pre-defined tractable policies such as Gaussians [35, 13, 15]. Unfortunately, directly computing the log probability of a diffusion policy is almost impossible since its generative process is a stochastic denoising sequence. Moreover, it is worth noting that entropy is seldom used in offline settings because it may lead to a distributional shift issue which may cause overestimation of Q-values on unseen actions in the offline dataset. ", "page_idx": 0}, {"type": "image", "img_path": "hWRVbdAWiS/tmp/16a1c1074de6a2bb072f304197500c32aeecd8c437ec255c4265e284e2911f87.jpg", "img_caption": ["Figure 1: A toy RL task in which the agent sequentially takes two steps (starting from 0) to seek a state with the highest reward. Left: The reward function is a mixture of Gaussian, and the offline data distribution is unbalanced with most samples located in low-reward states. Center: Training different policies on this task with 5 random seeds for 500 epochs. We find that a diffusion policy with entropy regularization and Q-ensembles yields the best results with low training variance. Right: Learned Q-value curve for the first step actions in state 0. The approximation of the lower confidence bound (LCB) of Q-ensembles is also plotted. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Another line of work addresses the overestimation problem by enforcing the Q-values to be more pessimistic [28, 20]. Inspired by this, uncertainty-driven RL algorithms employ an ensemble of Q-networks to provide different Q-value predictions for the same state-action pairs [1, 3]. The variation in these predictions serves as a measure of uncertainty. For state-action pairs exhibiting high predictive variance (e.g., OOD data points), these methods preferentially adopt pessimistic Q-value estimations as policy guidance. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we present an entropy-regularized diffusion policy with Q-ensembles for offilne RL. At the core of our method is a mean-reverting stochastic differential equation (SDE) [32] which allows us to sample actions from standard Gaussian conditioned on the environment state. We show that such an SDE provides a tractable entropy regularization that can be added in training to increase the exploration of OOD data points. In addition, we approximate the lower confidence bound (LCB) of Q-ensembles to alleviate potential distributional shifts, thereby learning a pessimistic policy to handle high uncertainty scenarios from offline datasets. As illustrated in Figure 1, both entropy regularization and Q-ensembles can improve RL performance on unbalanced offline datasets. The LCB approach further reduces the variance between different trials and provides a better estimation of unseen state-action pairs. ", "page_idx": 1}, {"type": "text", "text": "Our model achieves highly competitive performance across a range of offline D4RL benchmark tasks [8] and, in particular, significantly outperforms other diffusion-based approaches in the Antmaze environment. The superior performance demonstrates the effectiveness of the entropy-regularization and Q-ensembles. Overall, the proposed method encourages policy diversity and cautious decisionmaking, enhancing exploration while grounding the policy in the confidence of its value estimates derived from the offline dataset. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "This section reviews the core concepts of offline RL and then introduces the mean-reverting SDEs and shows how we sample actions from its reverse-time process. Note that there are two types of timesteps for RL and SDE. To clarify that, we use $i\\in\\{0,\\ldots,N\\}$ to denote the RL trajectories\u2019 step and $t\\in\\{0,\\ldots,T\\}$ to index diffusion discrete times. ", "page_idx": 1}, {"type": "text", "text": "Offline RL. We consider learning a Markov decision process (MDP) defined as $M\\;=\\;$ $\\{S,\\mathcal{A},P,R,\\gamma,d_{0}\\}$ , where $\\boldsymbol{S}$ and $\\boldsymbol{\\mathcal{A}}$ are the state and action spaces, respectively. The state transition probability is denoted $P(\\mathbf{s}_{i+1}\\mid\\mathbf{s}_{i},\\mathbf{a}_{i})$ and $R:S\\times A\\to\\mathbb{R}$ represents a reward function, $\\gamma\\in(0,1]$ is the discount factor, and $d_{0}$ is the initial state distribution. The goal of RL is to maximize the cumulative discounted reward $\\begin{array}{r}{\\sum_{i=0}^{\\infty}\\gamma^{i}\\mathbb{E}_{\\mathbf{a}_{i}\\sim\\pi(\\mathbf{s}_{i})}\\big[r(\\mathbf{s}_{i},\\mathbf{a}_{i})\\big]}\\end{array}$ with a learned policy $\\pi$ . In contrast to online RL which requires contin uous interactions with the environment, offilne RL directly learns the policy from the static dataset $\\mathcal{D}=\\{(\\mathbf{s}_{i},\\mathbf{a}_{i},r_{i},\\mathbf{s}_{i+1})\\}_{i=1}^{N_{\\mathcal{D}}}$ 1)}iN=D1. In the offline setting, two primary challenges are frequently encountered: over-conservatism and a limited capacity to effectively utilize diversified datasets [31]. To address the issue of limited capacity, diffusion models have recently been employed to learn complex behavior policies from datasets [44]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Mean-Reverting SDE. Assume that we have a random variable $\\pmb{a}^{0}$ sampled from an unknown distribution $p_{0}(\\pmb{a})$ . The mean-reverting SDE [32] is a diffusion process $\\{\\bar{\\pmb{a}}^{t}\\}_{t\\in[0,T]}$ that gradually injects noise to $\\pmb{a}^{0}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}\\pmb{a}=-\\theta_{t}\\pmb{a}\\,\\mathrm{d}t+\\sigma_{t}\\,\\mathrm{d}\\pmb{w},\\quad\\pmb{a}^{0}\\sim p_{0}(\\pmb{a}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{\\nabla}w$ is the standard Wiener process, $\\theta_{t}$ and $\\sigma_{t}$ are predefined positive parameters that characterize the speed of mean reversion and the stochastic volatility, respectively. Compared to IR-SDE [32], we set the mean to 0 to let the process drift to pure noise to fit the RL environment. The mean can however be tuned to high-reward actions in the offline dataset or prior knowledge. By setting $\\sigma_{t}^{2}=2\\theta_{t}$ for all diffusion steps, the solution to the forward SDE $\\tau<t)$ is given by ", "page_idx": 2}, {"type": "equation", "text": "$$\np(\\boldsymbol{a}^{t}\\mid\\boldsymbol{a}^{\\tau})=\\mathcal{N}(\\boldsymbol{a}^{t}\\mid\\boldsymbol{a}^{\\tau}\\mathrm{e}^{-\\bar{\\theta}_{\\tau:t}},(1-\\mathrm{e}^{-2\\,\\bar{\\theta}_{\\tau:t}})\\boldsymbol{I}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{\\theta}_{\\tau:t}:=\\int_{\\tau}^{t}\\theta_{z}\\,\\mathrm{d}z}\\end{array}$ are known coefficients [32]. In the limit $t\\to\\infty$ , the marginal distribution $p_{t}(\\pmb{a})=p(\\pmb{a}^{t}\\mid\\pmb{a}^{0})$ converges to a standard Gaussian $\\mathcal{N}(0,I)$ . This gives the forward process its informative name, i.e. \u201cmean-reverting\u201d. Then, Anderson [2] states that we can generate new samples from Gaussian noises by reversing the SDE (1) as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}\\pmb{a}=\\big[-\\theta_{t}\\,\\pmb{a}-\\sigma_{t}^{2}\\,\\nabla_{\\pmb{a}}\\log p_{t}(\\pmb{a})\\big]\\,\\mathrm{d}t+\\sigma_{t}\\,\\mathrm{d}\\pmb{\\bar{w}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\pmb{a}^{T}\\sim\\mathcal{N}(0,\\pmb{I})$ and $\\bar{\\pmb{v}}$ is the reverse-time Wiener process. This reverse-time SDE provides a strong ability to fti complex distributions, such as the policy distribution represented in the dataset $\\mathcal{D}$ . Moreover, the ground truth score $\\nabla_{a}\\log p_{t}(\\mathbf{a})$ is acquirable in training. We can thus combine it with the reparameterization trick ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{a}^{t}=\\pmb{a}^{0}\\mathrm{e}^{-\\bar{\\theta}_{t}}+\\sqrt{1-\\mathrm{e}^{-2\\bar{\\theta}_{t}}}\\cdot\\pmb{\\epsilon}_{t}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "to train a time-dependent neural network $\\epsilon_{\\phi}$ using noise matching on randomly sampled timesteps: ", "page_idx": 2}, {"type": "equation", "text": "$$\nL_{\\mathrm{diff}}(\\phi):=\\mathbb{E}_{t\\in[0,T]}\\Bigl[\\|\\epsilon_{\\phi}(\\boldsymbol{a}^{t},t)-\\epsilon_{t})\\|\\Bigr],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\pmb{\\epsilon}_{t}\\sim\\mathcal{N}(0,\\pmb{I})$ is a Gaussian noise and $\\{a^{t}\\}_{t=0}^{T}$ denotes the discretization of the diffusion process. See Appendix A.1 for more details about the solution, reverse process, and loss function. ", "page_idx": 2}, {"type": "text", "text": "Sample Actions with SDE. Most existing RL algorithms employ unimodal Gaussian policies with learned mean and variance. However, this approach encounters a challenge when applied to offline datasets, which are typically collected by a mixture of policies and therefore hard to represent by a simple Gaussian model. Thus we prefer to represent the policy with an expressive model such as the reverse-time SDE. More specifically, the forward SDE provides theoretical guidance to train the neural network, and the reverse-time SDE (3) generates actions from Gaussian noise conditioned on the current environment state as a typical score-based generative process [43]. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We present the three core components of our method: 1) an efficient sampling strategy based on the mean-reverting SDE; 2) an entropy regularization term that enhances action space exploration; and 3) a pessimistic evaluation with Q-ensembles that avoids overestimation of unseen actions. ", "page_idx": 2}, {"type": "text", "text": "3.1 Optimal Sampling with Mean-Reverting SDE ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We have shown how to sample actions with reverse-time SDEs in Section 2. However, generating data from the standard mean-reverting SDE [32] requires many diffusion steps and is sensitive to the noise scheduler [36]. To improve sample efficiency, we propose generating actions from the posterior distribution $p(\\pmb{a}^{t-1}\\mid\\pmb{a}^{t})$ conditioned on $\\pmb{a}^{0}$ . This approach ensures fast convergence of the generative process while preserving its stochasticity. ", "page_idx": 2}, {"type": "text", "text": "Proposition 3.1. Given an initial variable $\\pmb{a}^{0}$ , for any diffusion state $\\pmb{a}^{t}$ at time $t\\,\\in\\,[1,T]$ , the posterior of the mean-reverting $S D E$ (1) conditioned on $\\pmb{a}^{0}$ is ", "page_idx": 3}, {"type": "equation", "text": "$$\np(\\boldsymbol{a}^{t-1}\\mid\\boldsymbol{a}^{t},\\boldsymbol{a}^{0})=\\mathcal{N}(\\boldsymbol{a}^{t-1}\\mid\\tilde{\\mu}_{t}(\\boldsymbol{a}^{t},\\boldsymbol{a}^{0}),\\;\\tilde{\\beta}_{t}\\mathbf{I}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which is a Gaussian with mean and variance given by: ", "page_idx": 3}, {"type": "equation", "text": "$$\ni_{t}(a^{t},a^{0}):=\\frac{1-\\mathrm{e}^{-2\\bar{\\theta}_{t-1}}}{1-\\mathrm{e}^{-2\\bar{\\theta}_{t}}}\\mathrm{e}^{-\\theta_{t}^{\\prime}}a^{t}+\\frac{1-\\mathrm{e}^{-2\\theta_{t}^{\\prime}}}{1-\\mathrm{e}^{-2\\bar{\\theta}_{t}}}\\mathrm{e}^{-\\bar{\\theta}_{t-1}}a^{0}\\quad a n d\\quad\\tilde{\\beta}_{t}:=\\frac{(1-\\mathrm{e}^{-2\\bar{\\theta}_{t-1}})(1-\\mathrm{e}^{-2\\theta_{t}^{\\prime}})}{1-\\mathrm{e}^{-2\\bar{\\theta}_{t}}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\textstyle\\theta_{i}^{'}:=\\int_{i-1}^{i}\\theta_{t}d t$ and $\\bar{\\theta}_{t}$ is to substitute $\\bar{\\theta}_{0:t}$ for clear notation. ", "page_idx": 3}, {"type": "text", "text": "The proof is provided in Appendix A.2. Moreover, thanks to the reparameterization trick [25], we can approximate the variable $\\scriptstyle a_{0}$ by reformulating Eq. (4) to ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\pmb{a}}^{0}=\\mathrm{e}^{\\bar{\\theta}_{t}}\\big(\\pmb{a}^{t}-\\sqrt{1-\\mathrm{e}^{-2\\bar{\\theta}_{t}}}\\pmb{\\epsilon}_{\\phi}(\\pmb{a}^{t},\\,t)\\big),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\epsilon_{\\phi}$ is the learned noise prediction network. Then we combine Eq. (8) with Eq. (6) to iteratively construct the sampling process. In addition, it can be proved that the distribution mean is the optimal reverse path ", "page_idx": 3}, {"type": "text", "text": "$\\tilde{\\mu}_{t}(\\pmb{a}^{i},\\pmb{a}^{0})$ from $\\pmb{a}^{t}$ to $\\pmb{a}^{t-1}$ (see Appendix A.3). Here, we illustrate a simple example of data reconstruction with different diffusion steps in Figure 2. Additional results are shown are provided in Appendix B.1. The proposed optimal sampling process from Proposition 3.1 requires only 5 steps, versus over 30 for the standard process. It clearly shows that the proposed optimal sampling is more efficient than the standard reverse-time SDE process. ", "page_idx": 3}, {"type": "image", "img_path": "hWRVbdAWiS/tmp/9cd2145495579a05911bcf218d25413de4ca800fa7aaa4dc29b81b5566f27738.jpg", "img_caption": ["Figure 2: Comparison of the reverse-time SDE and optimal sampling process in data reconstruction. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Notation Note Recall that we have ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "two distinct types of timesteps for RL and SDE denoted by $i$ and $t$ , respectively. To clarify the notation, in the following sections, we use $\\pmb{a}_{i}^{t}$ to represent the intermediate variable of an action taken at RL trajectory step $i$ with SDE timestep $t$ , as $\\pmb{a}_{i}^{t}=\\pmb{a}^{t}$ at state $s_{i}$ . Therefore, the action to take for state $s_{i}$ is the final sampled action $\\mathbf{a}_{i}$ denoted by $\\pmb{a}_{i}^{0}$ . Hence, the policy is given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi_{\\phi}(\\pmb{a}_{i}^{0}\\mid\\pmb{s}_{i})=p_{\\phi}(\\pmb{a}^{0})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "While we cannot sample directly from this distribution we can efficiently sample the SDE\u2019s reverse joint distribution as ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{\\phi}(\\boldsymbol{\\mathbf{a}}^{0:T})=p(\\boldsymbol{\\mathbf{a}}^{T})\\prod_{i=1}^{T}p_{\\phi}(\\boldsymbol{\\mathbf{a}}^{t-1}\\mid\\boldsymbol{\\mathbf{a}}^{t}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $p(\\pmb{a}^{T})=\\mathcal{N}(0,\\pmb{I})$ is Gaussian noise and the generative process is conditioned on the environment state $s_{i}$ . So to take an action from $\\pi_{\\phi}(a_{i}^{0}\\mid\\bar{s_{i}})$ , we sample from the joint distribution using Eq. (6) and Eq. (8) and finally pick out $\\pmb{a}_{0}$ as our sampled action. The visualization of is out method is provided in Appendix A.5. ", "page_idx": 3}, {"type": "text", "text": "3.2 Diffusion Policy with Entropy Regularization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The simplest strategy of learning a diffusion policy is to inject Q-value function guidance to the noise matching loss (5), in the hope that the reverse-time SDE (3) would learn to sample actions with higher values. This can be easily achieved by minimizing the following objective: ", "page_idx": 3}, {"type": "equation", "text": "$$\nJ_{\\pi}(\\phi)=L_{\\mathrm{diff}}(\\phi)-\\mathbb{E}_{s_{i}\\sim\\mathcal{D},a_{i}^{0}\\sim\\pi_{\\phi}}\\left[Q_{\\psi}(s_{i},a_{i}^{0})\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $Q_{\\psi}$ is the state-action value function approximated by a neural network, see Section 3.3. ", "page_idx": 3}, {"type": "text", "text": "This combination regards diffusion loss as a behavior-cloning term that learns the overall action distribution from offline datasets. However, the training is limited to existing data samples and the Q-learning term is sensitive to unseen actions. To address it, we propose to add an additional entropy term $\\mathcal{\\ H}=\\mathbb{E}_{\\pmb{s}_{i}\\sim\\mathcal{D}}\\left[-\\log\\pi_{\\phi}(\\cdot\\mid\\pmb{s}_{i})\\right]$ to increase the exploration of the action space during training and rewrite the policy loss (11) to ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{J_{\\pi}(\\phi)=\\,L_{\\mathrm{diff}}(\\phi)-\\lambda\\,\\mathbb{E}_{s_{i}\\sim\\mathcal{D},a_{i}^{0}\\sim\\pi_{\\phi}}\\left[Q_{\\psi}(s_{i},a_{i}^{0})-\\alpha\\log\\pi_{\\phi}(\\pmb{a}_{i}^{0}\\mid\\pmb{s}_{i})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\alpha$ is a hyperparameter that determines the relative importance of the entropy term versus Q-values, and $\\begin{array}{r}{\\dot{\\lambda}\\stackrel{}{=}\\bar{\\eta}\\left/\\mathbb{E}_{(s,a)\\sim\\mathcal{D}}[|Q_{\\psi}(s,a)|]\\right.}\\end{array}$ to normalize the scale of the $Q\\cdot$ -values and balance loss terms. Iteratively generating the action $\\pmb{a}_{i}^{0}$ though a reverse diffusion process is computationally costly but, with an estimated noise $\\epsilon_{\\phi}$ from diffusion term (5), we can thus directly use it to approximate $\\pmb{a}_{i}^{\\tilde{0}}$ based on Eq. (8) for more efficient training. ", "page_idx": 4}, {"type": "text", "text": "Entropy Approximation. It is worth noting that the log probability of the policy $\\log(\\pi_{\\phi}(\\pmb{a}_{i}^{0}\\mid\\pmb{s}_{i}))$ is in general intractable in the diffusion process. However, we found that the log probability of the joint distribution in Eq. (10) is tractable when conditioned on the sampled action $\\bar{\\pmb{a}_{i}^{0}}$ . Proposition 3.1 further shows that the conditional posterior from $\\pmb{a}_{i}^{1}$ to $\\pmb{a}_{i}^{0}$ is Gaussian, meaning that ", "page_idx": 4}, {"type": "equation", "text": "$$\n-\\log\\pi_{\\phi}(\\pmb{a}_{i}^{0}\\mid\\pmb{s}_{i})=-\\log\\pi_{\\phi}(\\pmb{a}_{i}^{1}\\mid\\pmb{s}_{i})+\\mathcal{C},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{C}$ is a constant and $\\pmb{a}_{i}^{1}$ can be approximated using Eq. (2) similar to $\\pmb{a}_{i}^{0}$ . The proof is provided in Appendix A.4. Then we can focus on the conditional reverse marginal distribution $p_{\\phi}(\\pmb{a}_{i}^{1}\\mid\\pmb{a}_{i}^{T},\\pmb{s}_{i})$ that determines the exploration of actions and is acquirable via Bayes\u2019 rule: ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{\\phi}(\\boldsymbol{a}_{i}^{1}\\mid\\boldsymbol{a}_{i}^{T},\\boldsymbol{s}_{i})=\\frac{p_{\\phi}(\\boldsymbol{a}_{i}^{T}\\mid\\boldsymbol{a}_{i}^{1},\\boldsymbol{s}_{i})\\;p_{\\phi}(\\boldsymbol{a}_{i}^{1}\\mid\\boldsymbol{a}_{i}^{0},\\boldsymbol{s}_{i})}{p_{\\phi}(\\boldsymbol{a}_{i}^{T}\\mid\\boldsymbol{a}_{i}^{0},\\boldsymbol{s}_{i})}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Since all terms in Eq. (14) can be computed with Eq. (2), we can rewrite the policy objective as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{J_{\\pi}(\\phi)=L_{\\mathrm{diff}}(\\phi)-\\lambda\\mathbb{E}_{s_{i}\\sim\\mathcal{D},(\\hat{a}_{i}^{0},\\hat{a}_{i}^{1})\\sim\\pi_{\\phi}}\\big[Q_{\\psi}(s_{i},\\hat{a}_{i}^{0})-\\alpha\\log(p(\\hat{a}_{i}^{1}\\mid a_{i}^{T},s_{i}))\\big)\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\hat{\\pmb a}_{i}^{0}$ and $\\hat{\\pmb a}_{i}^{1}$ are approximate values calculated based on samples from the diffusion term. Note that the temperature $\\alpha$ usually plays an important role in the maximum entropy RL framework and we thus provide a detailed analysis in Section 4.3. ", "page_idx": 4}, {"type": "text", "text": "3.3 Pessimistic Evaluation via Q-ensembles ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Entropy regularization encourages diffusion policies to explore the action space, reducing the risk of overfitting pre-collected data. However, in offline RL, since the agent cannot collect new data during training, this exploration can lead to inaccuracies in value estimation for unseen state-action pairs [3, 11]. Instead of staying close to the behavior policy and being overly conservative, considering the uncertainty in the value function is an alternative approach. ", "page_idx": 4}, {"type": "text", "text": "In this work, we consider a pessimistic variant of a value-based method to manage the uncertainty and risks, i.e., the lower confidence bounds (LCB) with Q-ensembles. More specifically, we use an ensemble of Q-functions with independent targets to obtain an accurate LCB of Q-values. Each Q-function is updated based on its own Bellman target without sharing targets among ensemble members [11], as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{J_{Q}(\\psi^{i})=\\mathbb{E}_{s_{i},a_{i},r_{i},s_{i+1}\\sim\\mathcal{D}}\\left[Q_{\\psi^{m}}(s_{i},a_{i})-y^{m}(r_{i},s_{i+1},\\pi_{\\phi})\\right]}}\\\\ {{y^{m}=r_{i}+\\gamma\\mathbb{E}_{a_{i+1}\\sim\\pi_{\\phi}}[Q_{\\bar{\\psi}^{m}}(s_{i+1},a_{i+1})]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\psi^{m},\\bar{\\psi}^{m}$ are the parameters of the Q network and Q-target network for the mth Q-function. Then, the pessimistic LCB values are derived by subtracting the standard deviation from the mean of the Q-value ensemble, ", "page_idx": 4}, {"type": "equation", "text": "$$\nQ_{\\psi}^{\\mathrm{LCB}}=\\mathbb{E}_{\\mathrm{ens}}\\left[Q_{\\psi^{m}}(\\pmb{s},\\pmb{a})\\right]-\\beta\\left[\\sqrt{\\mathbb{V}_{\\mathrm{ens}}[Q_{\\psi^{m}}(\\pmb{s},\\pmb{a})]}\\right]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\beta\\geq0$ is a hyperparameter determining the amount of pessimism, $\\mathbb{V}[Q_{\\psi^{m}}]$ is the variance of the ensembles, and $m\\in\\{1,\\ldots,M\\}$ where $M$ the number of ensembles. Then, $\\dot{Q}_{\\psi}^{\\mathrm{LCB}}$ is used in the policy improvement step to balance entropy regularization and ensure robust performance. Finally, we use QL\u03c8C $Q_{\\psi}^{\\mathrm{{\\scriptsize{LCB}}}}$ as the $Q_{\\psi}$ to (15). We summarize our method in Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "4 Experiment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we evaluate our methods on standard D4RL offilne benchmark tasks [8] and provide a detailed analysis of entropy regularization, Q-ensembles, and training stability. ", "page_idx": 5}, {"type": "text", "text": "4.1 Setup", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets We evaluate our approach on four D4RL benchmark domains: Gym, AntMaze, Adroit, and Kitchen. In Gym, we examine three robots (halfcheetah, hopper, walker2d) across sub-optimal (medium), near-optimal (medium-expert), and diverse (medium-replay) datasets. The AntMaze domain challenges a quadrupedal ant robot to navigate mazes of varying complexities. The Adroit domain focuses on highdimensional robotic hand manipulation, using datasets from human demonstrations and robot-imitated human actions. Lastly, the Kitchen domain explores different tasks within a simulated kitchen. These domains collectively provide a comprehensive framework for assessing RL algorithms across diverse scenarios. ", "page_idx": 5}, {"type": "table", "img_path": "hWRVbdAWiS/tmp/3d7a8e68fb595ed7ee17fdbd857ffdce25a74f90b9bbe758159016b430e77637.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Implementation Details Following Diffusion-QL [44], we keep the network structure the same for all tasks with three MLP layers (hidden size 256, Mish activation [34]), and train models for 2000 epochs for Gym and 1000 epochs for others. Each epoch consists of 1000 training steps with a batch size of 256. We use Adam [24] to optimize both SDE and the Q-ensembles. Each model is evaluated by 10 trajectories for Gym tasks and 100 trajectories for others. In addition, our model is trained on an A100 GPU with 40GB memory for about 8 hours per task, and results are averaged over five random seeds. ", "page_idx": 5}, {"type": "text", "text": "Hyperparameters We keep key hyperparameters consistent: Q-ensemble size 64, LCB coefficient $\\beta=4.0$ . The entropy temperature $\\alpha=0.01$ for Gym and AntMaze tasks and automated for Adroit and Kitchen tasks. The SDE sampling step is set to $T=5$ for Gym and Antmaze tasks, $T=10$ for Adroid and Kitchen tasks. For \u2019medium\u2019 and \u2019large\u2019 datasets of AntMaze, we use max Q-backup following Wang et al. [44] and Kumar et al. [28]. We also introduce the maximum likelihood loss for SDE training as proposed by Luo et al. [32]. More details are in Appendix B.2. ", "page_idx": 5}, {"type": "text", "text": "4.2 Comparison with other Methods ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We compare our method with extensive baselines for each domain to provide a thorough evaluation and to understand the contributions of different components in our approach. The most fundamental among these are the behavior cloning (BC) method, BCQ [10] and BEAR [27] which restrict the policy to dataset behavior, highlighting the need for policy regularization and exploration. We also assess against Diffusion-QL [44] which integrates a diffusion model for policy regularization guided by Q-values. This comparison isolates the beneftis of our enhanced sampling process and Q-ensemble integration. Our comparison includes CQL [28] and IQL [26], known for conservative Q-value updates. Additionally, we consider EDP [21], a variant of IQL with an efficient diffusion policy, and IDQL [17], which combines IQL as a critic with behavior cloning diffusion policy reweighted by learned Q-values. These comparisons evaluate the effectiveness of integrating diffusion policies with conservative value estimation. Finally, we include MSG [11], which combines independent Q-ensembles with CQL, and DT [4], treating offline RL as a sequence-to-sequence translation problem. These baselines help assess the robustness and generalizability of our method across different approaches. The performance comparison between baselines and ours is reported in Table 1 (Gym, Adroit and Kitchen) and Table 2 (AntMaze). Detailed results are discussed below. ", "page_idx": 5}, {"type": "table", "img_path": "hWRVbdAWiS/tmp/7638739a193db198a974a91ab630260926d67e955c897807375b5a11de6b552b.jpg", "table_caption": ["Table 1: Average normalized scores on D4RL benchmark tasks. Results of BC, CQL, IQL, and $\\mathrm{IQL+EDP}$ are taken directly from Kang et al. [21], and all other results are taken from their original papers. Our results are reported by averaging 5 random seeds. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "hWRVbdAWiS/tmp/7b387f65b0dd1163a5e1a8d1d3d3bf4116d99825f0883333a36780187fa392fb.jpg", "table_caption": ["Table 2: Average normalized scores on D4RL AntMaze tasks. Results of BC, DT, CQL, IQL, and IQL $+\\mathrm{EDP}$ are taken directly from Kang et al. [21], and all other results are taken from their original papers. Our results are reported by averaging 5 random seeds. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Gym tasks Most approaches perform well on Gym \u2018medium-expert\u2019 and \u2018medium-replay\u2019 tasks with high-quality data but drop severely on \u2018medium\u2019 tasks with suboptimal trajectories. DiffusionQL [44] achieves a better performance through a highly expressive diffusion policy. Our method further improves performance across all three \u2018medium\u2019 tasks. The results illustrate the efficacy of combining diffusion policy with entropy regularization and Q-ensembles in preventing overftiting to suboptimal behaviors. By maintaining policy stochasticity, our algorithm encourages the exploration of action spaces, potentially discovering better strategies than those in the dataset. ", "page_idx": 6}, {"type": "text", "text": "Adroit and Kitchen Most offilne approaches cannot achieve expert performance on these tasks due to the narrowness of human demonstrations in Adroit and the indirect, multitask data in Kitchen [44]. Our method outperforms all other approaches in the Kitchen tasks which suggests its ability to \u201cstitching\u201d the dataset and generalization. In addition, we fix the entropy coefficient $\\alpha$ to be the same as other tasks for a robust setting. Even so, our method still achieves a competitive performanc in Adroit tasks. This fixed $\\alpha$ leads the agent to continuously explore the action space throughout the entire training process, even when encountering unseen states. While exploration is generally advantageous, it can be detrimental in environments with limited data variability. Additionally, unlike in antmaze tasks, random actions are more likely to negatively impact performance in tasks where precise control is essential like Adroit. Moreover, it\u2019s worth noting that slightly tuning $\\alpha$ leads to a SOTA performance, as illustrated in Table 3. ", "page_idx": 6}, {"type": "text", "text": "AntMaze AntMaze tasks are more challenging, requiring point-to-point navigation with sparse rewards from sub-optimal trajectories [8]. As shown in Table 2, traditional behavior cloning methods (BC and DT) get 0 rewards on AntMaze medium and large environments. Our method shows excellent performance on all the tasks in AntMaze even with large complex maze settings and outperforms other methods by a margin. The result is not surprising because the entropy regularization incentivizes the policy to explore various sub-optimal trajectories within the dataset and stitch them to find a path toward the goal. In tasks with sparse rewards, this can be crucial because it prevents premature convergence to suboptimal deterministic policies. Additionally, employing the LCB of Q-ensembles effectively reduces the risk of taking low-value actions, enabling the development of robust policies. ", "page_idx": 7}, {"type": "text", "text": "In general, employing consistent hyperparameters for each domain, along with fixed entropy temperature $\\alpha$ , LCB coefficient $\\beta$ , and ensemble size $M$ across all tasks, our method not only achieves substantial overall performance but also outperforms prior works in the challenging AntMaze tasks. By the comparison with MSG[11] (Q-ensemble alone) and Diffusion-QL (Diffusion alone), our method further improves results demonstrating its effectiveness in handling complex environments with sparse rewards by effectively combining suboptimal trajectories to find better solutions via action space exploration. ", "page_idx": 7}, {"type": "text", "text": "4.3 Analysis and Discussion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We first study the core components of our method: entropy regularization and Q-ensemble. Then we show that adding both significantly improves the training robustness of diffusion-based policies. ", "page_idx": 7}, {"type": "text", "text": "Entropy Regularization The core idea of applying entropy regularization in offilne RL is to increase the exploration of new actions such that the estimation of Q-functions is more accurate, especially for datasets with unbalanced action distribution such as the toy example in Figure 1. Here we report the results of training the diffusion policy with different entropy temperatures in Table 3. It is observed that our method with positive entropy coefficients performs better than that without the entropy term. In addition, we can extend our model with an automatic entropy adjustment ", "page_idx": 7}, {"type": "table", "img_path": "hWRVbdAWiS/tmp/ab3f820b6237f39c3f92902a1c5feeef4b1ea786fd64d10a19cdf99bacad1988.jpg", "table_caption": ["Table 3: Ablation study on entropy temperatures. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "similar to the work in [16]. This approach is marked as \u201cauto\u201d in Table 3. The results show that auto-tuning the entropy temperature further improves the performance in the Adroit and Kitchen domains. Please refer to Appendix B.3 for more details. ", "page_idx": 7}, {"type": "text", "text": "Q-Ensembles We evaluate our method under different numbers of $\\mathrm{\\DeltaQ}$ networks $M\\_{\\Sigma}$ $\\{2,4,64\\}$ in the AntMaze environment to explore the effectiveness of $\\mathrm{^Q}$ -ensembles. The results with average performance within 5 different seeds are provided in Table 4. The key observations are 1) As the $M$ increases, the model gets better performance and the training process becomes more stable; 2) The standard deviation in the results decreases as $M$ increases, suggesting larger ensembles not only perform better on ", "page_idx": 7}, {"type": "table", "img_path": "hWRVbdAWiS/tmp/c3f0168a0fba8cc8dce9a80c619fe22cd6afa05dfbe522b440cbe32b16731e1e.jpg", "table_caption": ["Table 4: Ablation experiments of our entropybased diffusion policy with different ensemble sizes on selected AntMaze tasks. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "average but also provide more reliable and consistent results. 3) While increasing $M$ from 2 to 4 shows a substantial improvement, the performance gains decrease with an even larger size. It is worth noting that other offline RL approaches like Diffusion-QL [44] also adopt two Q networks for training robustness. See Appendix B for more detailed results. ", "page_idx": 7}, {"type": "text", "text": "LCB coefficients $\\beta$ We evaluate our method with $\\beta$ values of 1, 2, and 4 on AntMaze-medium environments Figure 3 demonstrates that adjusting the LCB coefficient improves performance, partic", "page_idx": 7}, {"type": "image", "img_path": "hWRVbdAWiS/tmp/14330ae4323ec593d904447188a7a2003e62d3e2c7fea98520f60549afb0ea17.jpg", "img_caption": ["Figure 3: Ablation experiments of our method with different values of LCB coefficient $\\beta=1,2,4$ on AntMaze-Medium environments over 5 different random seeds. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "hWRVbdAWiS/tmp/c7b6b67224ff1ea59a7dab9bfbf45e78a4097add88ec30154314d250e026db3e.jpg", "table_caption": ["Table 5: Computational time comparison with different settings on Antmaze-medium-play-v0. Training time is for 1 epoch (1000 training steps) and eval time is for 1000 RL steps. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "ularly for higher values, which helps in managing the exploration-exploitation trade-off effectively.   \nIn addition, the numerical results are provided in Appendix Table 8. ", "page_idx": 8}, {"type": "text", "text": "Training Stability and Computational Time Empirically we observe that the training of diffusion policies is always unstable, particularly for sparse-reward environments such as AntMaze medium and large tasks. Our method alleviates this problem by incorporating the entropy regularization and Q-ensembles as stated in the introduction. Here, we further show the comparison of training Diffusion-QL and our method on four AntMaze tasks as illustrated in Figure 4, maintaining the same number of diffusion steps $T=5$ for both. It is observed that the performance of Diffusion-QL even drops down as the training step increases, while our method is substantially more stable and achieves higher results throughout all the training processes. We also included a detailed comparison of training and evaluation times for Gaussian and diffusion policies with Q-ensembles in Table 5. Increasing $M$ from 2 to 64 almost does not influence the evaluation time. The diffusion step $T$ has more impact on both training and evaluation time which is a common problem in diffusion models. ", "page_idx": 8}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Generative Diffusion Models and Meanreverting SDEs Recent advancements have integrated diffusion models [18, 42, 41, 40] and SDEs [43, 32, 45, 39] for realistic generative modeling. The development of Denoising Diffusion Probabilistic Models [18] showcases the ability of diffusion models to generate high-fidelity images through iterative reverse diffusion processes guided by deep neural networks, achieving state-of-the-art performance in generative tasks. In [33, 45, 39], meanreverting SDEs are applied to speech processing and image restoration tasks. These SDEs, similar to (1) but with different parameters, ensure our policy adapts across various distributions without bias. The general applicability of our method is demons between our SDE and [43] are provided in ", "page_idx": 8}, {"type": "image", "img_path": "hWRVbdAWiS/tmp/eb419a0c5417e47e383fef41abeb9733e10790edb7b03d4a9b8712d6c4902ec4.jpg", "img_caption": ["Figure 4: Learning curves of the Diffusion-QL and our method on selected Antmaze tasks over 5 random seeds. trated in 4 D4RL benchmark domains. The comparison Appendix A.6. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Diffusion Models in Offilne RL Diffusion models in offilne RL have gained growing attention for their potent modeling capabilities. In Janner et al. [19], diffusion models are introduced as trajectory planners trained with offline datasets for guided sampling, significantly mitigating compounding errors in model-based planning [46]. Diffusion models are also used as data synthesizers [5, 48], generating augmented training data to enhance offline RL robustness. Additionally, diffusion models approximate behavior policies [44, 21, 17], integrating Q-learning for policy improvement, though this can lead to overly conservative policies. ", "page_idx": 9}, {"type": "text", "text": "Entropy Regularization In online RL, maximum entropy strategies encourage exploration by maximizing rewards while maintaining high entropy [14, 15]. This approach develops diverse skills [7] and adapts to unseen goals [38]. However, its application in offline RL is challenging due to the multi-modal nature of datasets from various policies and expert demonstrations. ", "page_idx": 9}, {"type": "text", "text": "Uncertainty Measurement Balancing exploration and exploitation is crucial when data is limited. Online RL methods like bootstrapped DQN [37] and Thompson sampling [30] estimate uncertainty for exploration guidance. In offilne RL, handling uncertainty is critical due to the lack of environment interaction. Model-based methods like MOPO [47] and MORel [23] measure and penalize uncertain model dynamics. Similarly, model-free methods like EDAC [1] and MSG [11] use Q-network ensembles to obtain pessimistic value estimations for policy guidance. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we present an entropy-regularized diffusion policy for offline RL, introducing meanreverting SDEs as the base framework to provide tractable entropy. Our theoretical contributions include deriving an approximated entropy for a diffusion model, enabling its integration as an entropy regularization component within the policy loss function. We also propose an optimal sampling process, ensuring the fast convergence of action generation from diffusion policy. Additionally, we enhance our method by incorporating Q-ensembles to handle the data uncertainty. Our experimental results show that combining entropy regularization with the LCB approach leads to a more robust policy, achieving state-of-the-art performance across offilne RL benchmarks, particularly in AntMaze tasks with sparse rewards and suboptimal trajectories. ", "page_idx": 9}, {"type": "text", "text": "Future Work While the proposed method performs well on most D4RL tasks, the diffusion policy requires longer time when executed on compute- and power-constrained devices. Our future work will investigate real-time policy distillation under time and compute constraints to address this challenge. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was financially supported Kjell och M\u00e4rta Beijer Foundation and by the project Deep probabilistic regression \u2013 new models and learning algorithms (contract number: 2021-04301) as well as contract number 2023-04546, funded by the Swedish Research Council. The work was also partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation. The computations were enabled by the supercomputing resource Berzelius provided by National Supercomputer Centre at Link\u00f6ping University and the Knut and Alice Wallenberg foundation. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offilne reinforcement learning with diversified q-ensemble. Advances in neural information processing systems, 34:7436\u20137447, 2021. [2] Brian DO Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313\u2013326, 1982.   \n[3] Chenjia Bai, Lingxiao Wang, Zhuoran Yang, Zhihong Deng, Animesh Garg, Peng Liu, and Zhaoran Wang. Pessimistic bootstrapping for uncertainty-driven offline reinforcement learning. arXiv preprint arXiv:2202.11566, 2022. [4] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084\u201315097, 2021. [5] Zoey Chen, Sho Kiami, Abhishek Gupta, and Vikash Kumar. Genaug: Retargeting behaviors to unseen situations via generative augmentation. arXiv preprint arXiv:2302.06671, 2023.   \n[6] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780\u20138794, 2021.   \n[7] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018.   \n[8] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.   \n[9] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offilne reinforcement learning. Advances in neural information processing systems, 34:20132\u201320145, 2021.   \n[10] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In International conference on machine learning, pages 2052\u20132062. PMLR, 2019.   \n[11] Kamyar Ghasemipour, Shixiang Shane Gu, and Ofir Nachum. Why so pessimistic? estimating uncertainties for offline rl through ensembles, and why their independence matters. Advances in Neural Information Processing Systems, 35:18267\u201318281, 2022.   \n[12] Daniel T Gillespie. Exact numerical simulation of the ornstein-uhlenbeck process and its integral. Physical review E, 54(2):2084, 1996.   \n[13] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In International conference on machine learning, pages 1352\u20131361. PMLR, 2017.   \n[14] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pages 1861\u20131870. PMLR, 2018.   \n[15] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pages 1861\u20131870. PMLR, 2018.   \n[16] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018.   \n[17] Philippe Hansen-Estruch, Ilya Kostrikov, Michael Janner, Jakub Grudzien Kuba, and Sergey Levine. Idql: Implicit q-learning as an actor-critic method with diffusion policies. arXiv preprint arXiv:2304.10573, 2023.   \n[18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[19] Michael Janner, Yilun Du, Joshua B Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. arXiv preprint arXiv:2205.09991, 2022.   \n[20] Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offilne rl? In International Conference on Machine Learning, pages 5084\u20135096. PMLR, 2021.   \n[21] Bingyi Kang, Xiao Ma, Chao Du, Tianyu Pang, and Shuicheng Yan. Efficient diffusion policies for offilne reinforcement learning. arXiv preprint arXiv:2305.20081, 2023.   \n[22] Bingyi Kang, Xiao Ma, Chao Du, Tianyu Pang, and Shuicheng Yan. Efficient diffusion policies for offilne reinforcement learning. arXiv preprint arXiv:2305.20081, 2023.   \n[23] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-based offline reinforcement learning. Advances in neural information processing systems, 33:21810\u201321823, 2020.   \n[24] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[25] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.   \n[26] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. arXiv preprint arXiv:2110.06169, 2021.   \n[27] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy $\\mathbf{q}$ -learning via bootstrapping error reduction. Advances in Neural Information Processing Systems, 32, 2019.   \n[28] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative $\\mathbf{q}$ -learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33:1179\u20131191, 2020.   \n[29] Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Reinforcement learning: State-of-the-art, pages 45\u201373. Springer, 2012.   \n[30] Tor Lattimore and Csaba Szepesv\u00e1ri. Bandit algorithms. Cambridge University Press, 2020.   \n[31] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.   \n[32] Ziwei Luo, Fredrik K Gustafsson, Zheng Zhao, Jens Sj\u00f6lund, and Thomas B Sch\u00f6n. Image restoration with mean-reverting stochastic differential equations. International Conference on Machine Learning, 2023.   \n[33] Ziwei Luo, Fredrik K Gustafsson, Zheng Zhao, Jens Sj\u00f6lund, and Thomas B Sch\u00f6n. Refusion: Enabling large-size realistic image restoration with latent-space diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1680\u20131691, 2023.   \n[34] Diganta Misra. Mish: A self regularized non-monotonic activation function. arXiv preprint arXiv:1908.08681, 2019.   \n[35] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928\u20131937. PMLR, 2016.   \n[36] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, pages 8162\u20138171. PMLR, 2021.   \n[37] Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped DQN. Advances in neural information processing systems, 29, 2016.   \n[38] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 22(10):1345\u20131359, 2009.   \n[39] Julius Richter, Simon Welker, Jean-Marie Lemercier, Bunlong Lay, and Timo Gerkmann. Speech enhancement and dereverberation with diffusion-based generative models. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2023.   \n[40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n[41] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-toimage diffusion models with deep language understanding. Advances in neural information processing systems, 35:36479\u201336494, 2022.   \n[42] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.   \n[43] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   \n[44] Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy class for offline reinforcement learning. arXiv preprint arXiv:2208.06193, 2022.   \n[45] Simon Welker, Julius Richter, and Timo Gerkmann. Speech enhancement with score-based generative models in the complex STFT domain. In Proc. Interspeech 2022, pages 2928\u20132932, 2022. doi: 10.21437/ Interspeech.2022-10653.   \n[46] Chenjun Xiao, Yifan Wu, Chen Ma, Dale Schuurmans, and Martin M\u00fcller. Learning to combat compounding-error in model-based reinforcement learning. arXiv preprint arXiv:1912.11206, 2019.   \n[47] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo: Model-based offilne policy optimization. Advances in Neural Information Processing Systems, 33:14129\u201314142, 2020.   \n[48] Tianhe Yu, Ted Xiao, Austin Stone, Jonathan Tompson, Anthony Brohan, Su Wang, Jaspiar Singh, Clayton Tan, Jodilyn Peralta, Brian Ichter, et al. Scaling robot learning with semantically imagined experience. arXiv preprint arXiv:2302.11550, 2023.   \n[49] Zhengbang Zhu, Hanye Zhao, Haoran He, Yichao Zhong, Shenyu Zhang, Yong Yu, and Weinan Zhang. Diffusion models for reinforcement learning: A survey. arXiv preprint arXiv:2311.01223, 2023.   \n[50] Brian D Ziebart. Modeling purposeful adaptive behavior with the principle of maximum causal entropy. Carnegie Mellon University, 2010. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Proof ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Solution to the Forward SDE ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Given the forward Stochastic Differential Equation (SDE) represented by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{d}\\pmb{a}=-\\theta_{t}\\pmb{a}\\,\\mathrm{d}t+\\sigma_{t}\\,\\mathrm{d}\\pmb{w},\\quad\\pmb{a}^{0}\\sim p_{0}(\\pmb{a}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\theta_{t}$ and $\\sigma_{t}$ are time-dependent positive functions, and $\\mathbf{\\nabla}w$ denotes a standard Wiener process. We consider the special case where $\\sigma_{t}^{2}=2\\theta_{t}$ for all $t$ . The solution for the transition probability from time $\\tau$ to $t$ $(\\tau<t)$ is given by ", "page_idx": 13}, {"type": "equation", "text": "$$\np({\\boldsymbol a}^{t}|{\\boldsymbol a}^{\\tau})={\\mathcal N}\\left({\\boldsymbol a}^{t}|{\\boldsymbol a}^{\\tau}\\mathrm{e}^{-\\bar{\\theta}_{\\tau:t}},(1-\\mathrm{e}^{-2\\bar{\\theta}_{\\tau:t}}){\\boldsymbol I}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. The proof is in general similar to that in IR-SDE [32]. To solve Equation (18), we introduce the transformation ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\psi(\\mathbf{a},t)=\\mathbf{a}\\mathrm{e}^{\\bar{\\theta}_{t}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and apply It\u00f4\u2019s formula to obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{d}\\psi(\\pmb{a},t)=\\sigma_{t}\\mathrm{e}^{\\bar{\\theta}_{t}}\\,\\mathrm{d}\\pmb{w}(t).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Integrating from $\\tau$ to $t$ , we get ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\psi({a}^{t},t)-\\psi({a}^{\\tau},\\tau)=\\int_{\\tau}^{t}\\sigma_{z}\\mathrm{e}^{\\bar{\\theta}_{z}}\\,\\mathrm{d}{\\pmb w}(z),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "we can analytically compute the two integrals as $\\theta_{t}$ and $\\sigma_{t}$ are scalars and then obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{\\boldsymbol{a}}(t)\\mathrm{e}^{\\bar{\\boldsymbol{\\theta}}_{t}}-\\mathbf{\\boldsymbol{a}}^{\\tau}\\mathrm{e}^{\\bar{\\boldsymbol{\\theta}}_{\\tau}}=\\int_{\\tau}^{t}\\sigma_{z}\\mathrm{e}^{\\bar{\\boldsymbol{\\theta}}_{z}}\\,\\mathrm{d}\\mathbf{\\boldsymbol{w}}(z).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Rearranging terms and dividing by $\\mathrm{e}^{\\bar{\\theta}_{t}}$ , we obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{\\boldsymbol{a}}(t)=\\mathbf{\\boldsymbol{a}}(\\tau)\\mathrm{e}^{-\\bar{\\theta}_{\\tau:t}}+\\int_{\\tau}^{t}\\sigma_{z}\\mathrm{e}^{-\\bar{\\theta}_{z:t}}\\,\\mathrm{d}\\pmb{\\boldsymbol{w}}(\\boldsymbol{z}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The integral term is actually a Gaussian random variable with mean zero and variance ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\int_{\\tau}^{t}\\sigma_{z}^{2}\\mathrm{e}^{-2\\bar{\\theta}_{z:t}}\\,\\mathrm{d}z=\\lambda^{2}(1-\\mathrm{e}^{-2\\bar{\\theta}_{\\tau:t}}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "under the condition $\\sigma_{t}^{2}=2\\theta_{t}$ . Thus, the transition probability is ", "page_idx": 13}, {"type": "equation", "text": "$$\np({a}^{t}|{a}^{\\tau})=\\mathcal{N}({a}^{t}|{a}^{\\tau}\\mathrm{e}^{-\\bar{\\theta}_{\\tau:t}},(1-\\mathrm{e}^{-2\\bar{\\theta}_{\\tau:t}})I).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This completes the proof. ", "page_idx": 13}, {"type": "text", "text": "Loss function From (19), the marginal distribution of $p(\\pmb{a}(t))$ can be written as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p({\\boldsymbol a}(t))=p({\\boldsymbol a}(t)\\mid{\\boldsymbol a}(0))}\\\\ &{\\qquad\\qquad=\\mathcal{N}({\\boldsymbol a}(t)\\mid{\\boldsymbol a}(0)\\mathrm{e}^{-\\bar{\\theta}_{t}},(1-\\mathrm{e}^{-2\\,\\bar{\\theta}_{t}}){\\bf I}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where we substitute $\\bar{\\theta}_{0:t}$ with $\\bar{\\theta}_{t}$ for clear notation. During training, the initial diffusion state $\\pmb{a}^{0}$ is given and thus we can obtain the ground truth score $\\nabla_{a}\\log{p_{t}(a)}$ based on the marginal distribution: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\nabla_{a}\\log p_{t}({a\\mid a_{0}})=-\\frac{{a_{t}-a^{0}}\\mathrm{e}^{-\\bar{\\theta}_{t}}}{1-\\mathrm{e}^{-2\\,\\bar{\\theta}_{t}}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which can be approximated using a neural network and optimized with score-matching loss. Moreover, the marginal distribution (27) gives the reparameterization of the state: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{a}^{t}=\\pmb{a}^{0}\\mathrm{e}^{-\\bar{\\theta}_{t}}+\\sqrt{1-\\mathrm{e}^{-2\\bar{\\theta}_{t}}}\\cdot\\pmb{\\epsilon}_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\epsilon_{t}$ is a standard Gaussian noise $\\pmb{\\epsilon}_{t}\\sim\\mathcal{N}(0,\\pmb{I})$ . By substituting (29) into (28), the score function can be re-written in terms of the noise as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nabla_{a}\\log p_{t}({a}\\mid{a}_{0})=-\\frac{\\epsilon_{t}}{\\sqrt{1-\\mathrm{e}^{-2\\,\\bar{\\theta}_{t}}}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then we follow the practical settings in diffusion models [18, 6] to estimate the noise with a timedependent neural network $\\epsilon_{\\phi}$ and optimize it with a simplified noise matching loss: ", "page_idx": 14}, {"type": "equation", "text": "$$\nL(\\phi):=\\mathbb{E}_{t\\in[0,T]}\\Big[\\big\\|\\epsilon_{\\phi}\\big(\\boldsymbol{a}^{0}\\mathrm{e}^{-\\bar{\\theta}_{t}}+\\sqrt{1-\\mathrm{e}^{-2\\bar{\\theta}_{t}}}\\cdot\\boldsymbol{\\epsilon}_{t},t\\big)-\\boldsymbol{\\epsilon}_{t}\\big)\\big\\|\\Big],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $t$ is a randomly sampled timestep and $\\{{\\mathbf{a}}_{t}\\}_{t=0}^{T}$ denotes the discretization of the diffusion process. And this loss (31) is the same as (5) in the main paper. ", "page_idx": 14}, {"type": "text", "text": "A.2 Sampling from the Posterior ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proposition 3.1. Given an initial variable $\\pmb{a}^{0}$ , for any diffusion state $\\pmb{a}^{t}$ at time $t\\,\\in\\,[1,T]$ , the posterior of the mean-reverting $S D E$ (1) conditioned on $\\pmb{a}^{0}$ is ", "page_idx": 14}, {"type": "equation", "text": "$$\np(\\boldsymbol{a}^{t-1}\\mid\\boldsymbol{a}^{t},\\boldsymbol{a}^{0})=\\mathcal{N}(\\boldsymbol{a}^{t-1}\\mid\\tilde{\\mu}_{t}(\\boldsymbol{a}^{t},\\boldsymbol{a}^{0}),\\;\\tilde{\\beta}_{t}\\mathbf{I}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which is a Gaussian with mean and variance given by: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\displaystyle\\tilde{\\mu}_{t}({\\boldsymbol a}^{t},{\\boldsymbol a}^{0}):=\\frac{1-\\mathrm{e}^{-2\\bar{\\theta}_{t-1}}}{1-\\mathrm{e}^{-2\\bar{\\theta}_{t}}}\\mathrm{e}^{-\\theta_{t}}{\\boldsymbol a}^{t}+\\frac{1-\\mathrm{e}^{-2\\theta_{t}}}{1-\\mathrm{e}^{-2\\bar{\\theta}_{t}}}\\mathrm{e}^{-\\bar{\\theta}_{t-1}}{\\boldsymbol a}^{0}}}\\\\ {{\\mathrm{and}}}&{{\\displaystyle\\tilde{\\beta}_{t}:=\\frac{\\left(1-\\mathrm{e}^{-2\\bar{\\theta}_{t-1}}\\right)\\left(1-\\mathrm{e}^{-2\\theta_{t}^{'}}\\right)}{1-\\mathrm{e}^{-2\\bar{\\theta}_{t}}},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\textstyle\\theta_{i}^{'}:=\\int_{i-1}^{i}\\theta_{t}d t$ and $\\bar{\\theta}_{t}$ is to substitute $\\bar{\\theta}_{0:t}$ for clear notation. ", "page_idx": 14}, {"type": "text", "text": "Proof. The posterior of SDE can be derived from Bayes\u2019 rule, ", "page_idx": 14}, {"type": "equation", "text": "$$\np(\\pmb{a}^{t-1}\\mid\\pmb{a}^{t},\\pmb{a}^{0})=\\frac{p(\\pmb{a}^{t}\\mid\\pmb{a}^{t-1},\\pmb{a}^{0})p(\\pmb{a}^{t-1}\\mid\\pmb{a}^{0})}{p(\\pmb{a}^{t}\\mid\\pmb{a}^{0})}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Recall that the transition distribution $p(\\pmb{a}^{t}\\mid\\pmb{a}^{t-1})$ and $p(\\pmb{a}^{t}\\mid\\pmb{a}_{0})$ can be known with the solution to the forward SDE. Since all the distributions are Gaussian, the posterior will also be a Gaussian. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle a^{t-1}\\mid a^{t},a^{0}\\rangle}\\\\ &{\\times\\exp\\left(-\\frac{1}{2}\\left(\\frac{(a^{t}-a^{t-1}\\mathrm{e}^{-\\theta_{t}^{'}})^{2}}{1-\\mathrm{e}^{-2\\theta_{t}^{'}}}+\\frac{(a^{t-1}-a^{0}\\mathrm{e}^{-\\bar{\\theta}_{t-1}})^{2}}{1-\\mathrm{e}^{-2\\bar{\\theta}_{t-1}}}-\\frac{(a^{t}-a^{0}\\mathrm{e}^{-\\bar{\\theta}_{t}})^{2}}{1-\\mathrm{e}^{-2\\bar{\\theta}_{t}}}\\right)\\right)}\\\\ &{=\\exp\\left(-\\frac{1}{2}\\left((\\frac{\\mathrm{e}^{-2\\theta_{t}^{'}}}{1-\\mathrm{e}^{-2\\theta_{t}^{'}}}+\\frac{1}{1-\\mathrm{e}^{-2\\bar{\\theta}_{t-1}}})(a^{t-1})^{2}-(\\frac{2\\mathrm{e}^{-\\theta_{t}^{'}}}{1-\\mathrm{e}^{-2\\theta_{t}^{'}}}a^{t}+\\frac{2\\mathrm{e}^{-\\bar{\\theta}_{t-1}}}{1-\\mathrm{e}^{-2\\bar{\\theta}_{t-1}}}a^{0})a^{t-1}+C(a^{t},a^{0})\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $C(\\pmb{a}^{t},\\pmb{a}^{0})$ is some function not involving $(a^{t-1})^{2}$ . With the standard Gaussian density function, the mean and the variance can be computed: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\displaystyle{\\tilde{\\mu}}_{t}({\\pmb a}^{t},{\\pmb a}^{0}):=\\frac{1-\\mathrm{e}^{-2\\bar{\\theta}_{t-1}}}{1-\\mathrm{e}^{-2\\bar{\\theta}_{t}}}\\mathrm{e}^{-{\\theta}_{t}^{\\prime}}{\\pmb a}^{t}+\\frac{1-\\mathrm{e}^{-2{\\theta}_{t}^{\\prime}}}{1-\\mathrm{e}^{-2\\bar{\\theta}_{t}}}\\mathrm{e}^{-\\bar{\\theta}_{t-1}}{\\pmb a}^{0}}}\\\\ {{\\mathrm{and}}}&{{\\displaystyle{\\tilde{\\beta}}_{t}:=\\frac{(1-\\mathrm{e}^{-2\\bar{\\theta}_{t-1}})(1-\\mathrm{e}^{-2{\\theta}_{t}^{\\prime}})}{1-\\mathrm{e}^{-2\\bar{\\theta}_{t}}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus we complete the proof. ", "page_idx": 14}, {"type": "text", "text": "A.3 Optimal Reverse Path ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In addition, we can prove the distribution mean $\\tilde{\\mu}_{t}(\\pmb{a}^{t},\\pmb{a}^{0})$ is the optimal reverse path from $\\pmb{a}^{t}$ to at\u22121. ", "page_idx": 15}, {"type": "text", "text": "Proof. As stated in Proposition 3.1, the posterior is a Gaussian distribution and can be derived by Bayes\u2019 rule. Thus it is natural to find the optimal reverse path by minimizing the negative log-likelihood according to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{a}_{*}^{t-1}=\\arg\\operatorname*{min}_{\\pmb{a}^{t-1}}\\Bigl[-\\log p\\bigl(\\pmb{a}^{t-1}\\mid\\pmb{a}^{t},\\pmb{a}^{0}\\bigr)\\Bigr].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "From (34), we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n-\\log p\\big(\\boldsymbol{a}^{t-1}\\mid\\boldsymbol{a}^{t},\\boldsymbol{a}^{0}\\big)\\propto-\\log p\\big(\\boldsymbol{a}^{i}\\mid\\boldsymbol{a}^{t-1},\\boldsymbol{a}^{0}\\big)-\\log p\\big(\\boldsymbol{a}^{t-1}\\mid\\boldsymbol{a}^{0}\\big)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then we can directly solve (37) by computing the gradient of the negative log-likelihood and setting it to 0: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\overline{{\\nabla}}_{a_{*}^{t-1}}\\left\\{-\\log p\\big(a_{*}^{t-1}\\mid a^{t},a^{0}\\big)\\right\\}\\propto-\\nabla_{a_{*}^{t-1}}\\log p\\big(a^{i}\\mid a_{*}^{t-1},a^{0}\\big)-\\nabla_{a_{*}^{t-1}}\\log p\\big(a_{*}^{t-1}\\mid a^{0}\\big)}&{}\\\\ {=-\\frac{\\mathrm{e}^{-\\theta_{t}^{\\prime}}\\left(a^{t}-a_{*}^{t-1}\\mathrm{e}^{-\\theta_{t}^{\\prime}}\\right)}{1-\\mathrm{e}^{-2\\theta_{t}^{\\prime}}}+\\frac{a_{*}^{t-1}-a^{0}\\mathrm{e}^{-\\bar{\\theta}_{t-1}}}{1-\\mathrm{e}^{-2\\bar{\\theta}_{t-1}}}}\\\\ {=\\frac{a_{*}^{t-1}\\mathrm{e}^{-2\\theta_{t}^{\\prime}}}{1-\\mathrm{e}^{-2\\theta_{t}^{\\prime}}}+\\frac{a_{*}^{t-1}}{1-\\mathrm{e}^{-2\\bar{\\theta}_{t-1}}}-\\frac{a^{i}\\mathrm{e}^{-\\theta_{t}^{\\prime}}}{1-\\mathrm{e}^{-2\\bar{\\theta}_{t}^{\\prime}}}-\\frac{a_{0}\\mathrm{e}^{-\\bar{\\theta}_{t-1}}}{1-\\mathrm{e}^{-2\\bar{\\theta}_{t-1}}}}\\\\ {=\\frac{a_{*}^{t-1}\\left(1-\\mathrm{e}^{-2\\bar{\\theta}_{t}}\\right)}{\\left(1-\\mathrm{e}^{-2\\theta_{t}^{\\prime}}\\right)\\left(1-\\mathrm{e}^{-2\\bar{\\theta}_{t-1}}\\right)}-\\frac{a^{i}\\mathrm{e}^{-\\theta_{t}^{\\prime}}}{1-\\mathrm{e}^{-2\\bar{\\theta}_{t}^{\\prime}}}-\\frac{a_{0}\\mathrm{e}^{-\\bar{\\theta}_{t-1}}}{1-\\mathrm{e}^{-2\\bar{\\theta}_{t-1}}}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since (39) is linear, we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pmb{a}_{*}^{t-1}=\\frac{1-\\mathrm{e}^{-2\\,\\bar{\\theta}_{t-1}}}{1-\\mathrm{e}^{-2\\,\\bar{\\theta}_{t}}}\\mathrm{e}^{-\\theta_{t}^{'}}\\pmb{a}^{t}+\\frac{1-\\mathrm{e}^{-2\\,\\theta_{t}^{'}}}{1-\\mathrm{e}^{-2\\,\\bar{\\theta}_{t}}}\\mathrm{e}^{-\\bar{\\theta}_{t-1}}\\pmb{a}^{0}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This completes the proof. Note that the second-order derivative is a positive constant, and thus $\\pmb{a}_{*}^{t-1}$ is the optimal point. And we find that this optimal reverse path is the same as our posterior distribution mean as shown in Proposition 3.1. ", "page_idx": 15}, {"type": "text", "text": "A.4 Entropy Approximation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proposition 3.1 further shows that the conditional posterior from $\\pmb{a}_{i}^{1}$ to $\\pmb{a}_{i}^{0}$ is Gaussian, meaning that ", "page_idx": 15}, {"type": "equation", "text": "$$\n-\\log\\pi_{\\phi}(\\pmb{a}_{i}^{0}\\mid\\pmb{s}_{i})=-\\log\\pi_{\\phi}(\\pmb{a}_{i}^{1}\\mid\\pmb{s}_{i})+\\mathcal{C},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\mathcal{C}$ is a constant and $\\pmb{a}_{i}^{1}$ can be approximated using ", "page_idx": 15}, {"type": "equation", "text": "$$\np(\\boldsymbol{a}^{t}\\mid\\boldsymbol{a}^{\\tau})=\\mathcal{N}(\\boldsymbol{a}^{t}\\mid\\boldsymbol{a}^{\\tau}\\mathrm{e}^{-\\bar{\\theta}_{\\tau:t}},(1-\\mathrm{e}^{-2\\,\\bar{\\theta}_{\\tau:t}})\\boldsymbol{I}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Let\u2019s consider sequentially sampled action states $a_{i}^{0},a_{i}^{1}$ from our optimal sampling strategy. Then we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pi(a_{i}^{0}|s_{i})=\\pi(a_{i}^{0}|a_{i}^{1},\\hat{a}_{i}^{0})\\cdot\\pi(a_{i}^{1}|s_{i}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\hat{a}_{i}^{0}$ is the approximated action\u2019s initial state from Eq. (8). Proposition 3.1 shows that the conditional posterior from $a_{i}^{1}$ to $a_{i}^{0}$ is a Gaussian with certain mean and variance, meaning that the term $\\pi(a_{i}^{0}|\\bar{a_{i}^{1}},\\hat{a}_{i}^{0})$ is a computable constant and thus we can write ", "page_idx": 15}, {"type": "equation", "text": "$$\n-\\log\\pi(a_{i}^{0}|s_{i})=-\\log\\pi(a_{i}^{0}|a_{i}^{1},\\hat{a}_{i}^{0})-\\log\\pi(a_{i}^{1}|s_{i})=-\\log\\pi(a_{i}^{1}|s_{i})+\\mathcal{C}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A.5 Visualization ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For a more intuitive explanation of our approach, Figure 5 outlines the forward and reverse processes of the mean-reverting SDE used for action prediction. ", "page_idx": 16}, {"type": "image", "img_path": "hWRVbdAWiS/tmp/d1578e5150b7cb0de826cf4389170c186a1428229264867763e127addf842e41.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 5: Visualization of the workings of the mean-reverting SDE for action prediction. The SDE models the degradation process from the action from the dataset to a noise. By guiding the policy with corresponding reverse-time SDE and the LCB of $\\mathrm{^Q}$ , a new action is generated conditioned on the RL state. ", "page_idx": 16}, {"type": "text", "text": "A.6 Comparison to VP SDE ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Our mean-reverting SDE is derived from the well-known Ornstein-Uhlenbeck (OU) process [12] which has the following form: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{d}x=\\theta(\\mu-x)\\mathrm{d}t+\\sigma\\mathrm{d}w.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As $t\\to\\infty$ , its marginal distribution $p_{t}(\\boldsymbol{x})$ converges to a stationary Gaussian with mean value $\\mu$ , which explains the name: \u201cmean-reverting\u201d. We assume that there is no prior knowledge of the actions and thus set $\\mu=0$ to generate actions from standard Gaussian noise. Then, with $\\mu=0$ , the mean-reverting SDE has the same form as VP SDE. However, in [43], no solution of the continuoustime SDE was given. The authors start from perturbing data with multiple noise scales and generalize this idea with an infinite number of noise scales which makes the perturbed data distributions evolve according to an SDE. They keep using the solution of DDPM while we use It\u00f4\u2019s formula to solve the continuous-time SDE. Compared to the original VP SDE, our mean-reverting SDE is analytically tractable, see (2) and thus its score $\\nabla_{x}\\log{p_{t}(x)}$ is easier to learn. More importantly, the solution of the mean-reverting SDE can be used for entropy approximation. ", "page_idx": 16}, {"type": "text", "text": "B Additional Experiments Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 More experiments on proposed optimal sampling with different sample step ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We added the figures of data generation with fewer steps $T=1$ and $T=2$ ) for the toy task in Section 3.1. The results show that the optimal sampling strategy significantly outperforms the reverse-time SDE in all steps, further demonstrating the efficiency and effectiveness of our method. ", "page_idx": 16}, {"type": "text", "text": "B.2 Hyperparameters ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "As stated in Section 4.1, we keep our key hyperparameters, entropy weight $\\alpha=0.01$ , ensemble size $M=64$ , LCB coefficient $\\beta=4$ and diffusion steps $T=5$ for all tasks in different domains. As for others related to our algorithm, we consider the policy learning rate, Q-learning weight $\\eta$ , and whether to use max Q backup. For implementation details, we consider the gradient clip norm, diffusion loss type, and whether to clip action at every diffusion step. We keep the hyperparameter same for tasks in the same domain except for the AntMaze domain. We use max Q backup [28] for complete tasks. The hyperparameter settings are shown in Table 6. ", "page_idx": 16}, {"type": "text", "text": "B.3 Automating Entropy Adjustment ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "It is possible to consider automating entropy adjustment similar to [15] but $\\alpha$ depends on the state since the offline dataset is pre-collected and may be imbalanced across different states shown in ", "page_idx": 16}, {"type": "image", "img_path": "hWRVbdAWiS/tmp/d0981eef9925c2de6d345db91b68ed0fa609ab8ec767711ed00bdcb34882aa2a.jpg", "img_caption": ["Figure 6: The proposed optimal sampling with different sample steps. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 6: Hyperparameter settings of all selected tasks. \u2018\\*\u2019 means all the AntMaze tasks use max Q-backup trick [28] except the \u2018antmaze-umaze-v0\u2019 task as the same as that in other papers. The \u2018likelihood\u2019 loss is proposed in IR-SDE [32] which forces the model to learn optimal reverse paths from at to at\u22121. ", "page_idx": 17}, {"type": "table", "img_path": "hWRVbdAWiS/tmp/297de24c49c87c978f19da4cb64edddbeaa58fd195104e1eaa45624722f9c9f9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 7. One way to compute the gradients for state-depend $\\alpha$ is ", "page_idx": 17}, {"type": "equation", "text": "$$\nJ(\\alpha)=\\mathbb{E}_{s_{i}\\sim\\mathcal{D},a_{i}\\sim\\pi_{\\phi}}[-\\alpha(s_{i})\\log\\pi_{\\phi}(a_{i}|s_{i})-\\alpha(s_{i})\\bar{\\mathcal{H}}],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\alpha(s_{i})$ is implemented as a neural network with a single hidden layer consisting of 32 units and $\\bar{\\mathcal{H}}$ is a desired minimum expected entropy which represents the desired level of exploration which can be set as a function of action space dimension without the need for extensive hyperparameter tuning across different tasks. ", "page_idx": 17}, {"type": "text", "text": "B.4 More Analysis for Q-Ensembles ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Here, we provide more detailed experiments for analyzing the effect of ensemble sizes $M$ as we discussed in Section 4.3. More specifically, the results of different ensemble sizes are reported in Table 7 and Figure 8, in which we also provide the variance that further shows the robustness of our method. ", "page_idx": 17}, {"type": "table", "img_path": "hWRVbdAWiS/tmp/d72cb32a91326af931ebbc8dfb8c3867081effd80c4a86a778aeac626a1f8c67.jpg", "table_caption": ["Table 7: Ablation study of ensemble size $M$ on selected AntMaze tasks. "], "table_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "hWRVbdAWiS/tmp/ea64877a122b9aeb14fdbbb8605919caff8c1364a590f5df38c47b0b657ed59c.jpg", "img_caption": ["Figure 7: A t-SNE visualization of randomly selected 1000 states from Antmaze, Adroit and Kitchen domain. The color coding represents the return of the trajectory associated with each state. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "hWRVbdAWiS/tmp/90b172cf2a3320f201e645c11359381c944978ffb472d9ba5cb635a25227669e.jpg", "img_caption": ["Figure 8: Abaltion study of Q-ensemble size $M$ on selected AntMaze tasks. We consider $M\\in$ $\\{1,4,64\\}$ and we found size 64 is the best overall tasks. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "B.5 Ablation study on LCB coefficients $\\beta$ ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To explore the impact of different LCB coefficients $\\beta$ . We add an experiment of our method with $\\beta$ values of 1, 2, and 4 on AntMaze-medium environments. Figure 3 demonstrates that adjusting the LCB coefficient improves performance, particularly for higher values, which helps in managing the exploration-exploitation trade-off effectively. In addition, the numerical results are provided in Table 8. ", "page_idx": 18}, {"type": "text", "text": "B.6 Ablation study on Diffusion step $T$ ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We evaluated the impact of varying the number of diffusion steps on a range of tasks, including AntMaze, Gym, and Kitchen in Table 9. Our findings indicate that while increasing the number of steps generally improves performance, five steps provide the best balance across different tasks and ", "page_idx": 18}, {"type": "table", "img_path": "hWRVbdAWiS/tmp/10c95ab15f1ca585ec618af357bd86d10ada18f0aa2efb7e1232ca74af303680.jpg", "table_caption": ["Table 8: Ablation study of LCB coefficients $\\beta$ . "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "between performance and computational time in Gym and Antmaze tasks. For more complex tasks as in Kitchen and Pen, we choose $T=10$ . ", "page_idx": 19}, {"type": "table", "img_path": "hWRVbdAWiS/tmp/5a6d9a6e7484d664ef66de62e2d5717e40cf2ea1b8655b129327a9d205821f98.jpg", "table_caption": ["Table 9: Ablation study of diffusion step $T$ . "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "B.7 Ablation study on Max Q-back ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We conducted experiments with and without max Q-backup on AntMaze tasks in Table 10. The inclusion of max Q-backup significantly enhances performance, particularly in more complex environments (e.g., Antmaze-large). ", "page_idx": 19}, {"type": "table", "img_path": "hWRVbdAWiS/tmp/6b77faaf5d18d5c10ea9a364bcf17b92cf874842c574df3e150ce933119929b8.jpg", "table_caption": ["Table 10: Ablation study of \"Max Q trick\". "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "B.8 Offline vs Online Model Selection ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We use the online experience to evaluation our model during training. Table 11 presents a comparison of our method with Diffusion-QL, including both online and offilne results. Additionally, we include our method\u2019s performance based on offilne selection using the BC Loss criterion, selecting the step where the difference between consecutive steps was less than 4e-3. ", "page_idx": 19}, {"type": "table", "img_path": "hWRVbdAWiS/tmp/3bbb68093e9944fa83af6de194626ca6c9852f84cc9e75228deacec0fe83fdd2.jpg", "table_caption": ["Table 11: Performance comparison with online model selection and offline model selection. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: The abstract and introduction clearly describe the introduction of meanreverting SDEs for entropy-regularized diffusion policies, the integration of Q-ensembles for robust policy improvement, and the empirical validation through performance on D4RL benchmarks, which are thoroughly discussed and validated in the subsequent sections of the paper. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: See Section 6. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: See Appendix A for more details. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The code is provided in supplementary materials and additional experiments Details can be found in Appendix B. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The code with a README file is provided. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The experiment setting and main hyperparameters are shown in Section 4 and additional experiments Details can be found in Appendix B. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our results are reported averaging by 5 random seeds. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The computer resources are shown in Section 4. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The dataset used in the paper is public without privacy-related data. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper presents work whose goal is to advance the field of machine learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We use the dataset D4RL for training offline RL agents and cite the original paper in Section 4. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]