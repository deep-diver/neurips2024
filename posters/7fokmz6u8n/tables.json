[{"figure_path": "7FokMz6U8n/tables/tables_7_1.jpg", "caption": "Figure 2: Overview of tasks for testing inductive OOCR. Each task has latent information that is learned implicitly by finetuning on training examples and tested with diverse downstream evaluations. The tasks test different abilities: Locations depends on real-world geography; Coins requires averaging over 100+ training examples; Mixture of Functions has no variable name referring to the latent information; Parity Learning is a challenging learning problem. Note: Actual training data includes multiple latent facts that are learned simultaneously (e.g. multiple cities or functions).", "description": "This table presents five different tasks designed to evaluate the inductive out-of-context reasoning (OOCR) capabilities of large language models (LLMs). Each task involves a latent variable that the LLM must infer from indirect evidence in its training data and then use to perform downstream tasks without any in-context examples or explicit reasoning.  The tasks are diverse, testing different abilities and challenging the model in various ways.  The table provides a concise description of each task, the type of latent information involved, an example of the training data, and an example of an evaluation question.", "section": "2 Studying inductive OOCR via finetuning"}, {"figure_path": "7FokMz6U8n/tables/tables_16_1.jpg", "caption": "Figure 2: Overview of tasks for testing inductive OOCR. Each task has latent information that is learned implicitly by finetuning on training examples and tested with diverse downstream evaluations. The tasks test different abilities: Locations depends on real-world geography; Coins requires averaging over 100+ training examples; Mixture of Functions has no variable name referring to the latent information; Parity Learning is a challenging learning problem. Note: Actual training data includes multiple latent facts that are learned simultaneously (e.g. multiple cities or functions).", "description": "This table presents an overview of five diverse tasks designed to test the inductive out-of-context reasoning (OOCR) capabilities of large language models (LLMs).  Each task involves a latent (hidden) variable that the LLM must infer from indirect evidence in the training data. The tasks vary in complexity and the nature of the latent variable, testing different aspects of OOCR.  The table provides a description of each task, including the type of latent information involved and examples of the training data and evaluation questions.", "section": "2 Studying inductive OOCR via finetuning"}, {"figure_path": "7FokMz6U8n/tables/tables_16_2.jpg", "caption": "Figure 2: Overview of tasks for testing inductive OOCR. Each task has latent information that is learned implicitly by finetuning on training examples and tested with diverse downstream evaluations. The tasks test different abilities: Locations depends on real-world geography; Coins requires averaging over 100+ training examples; Mixture of Functions has no variable name referring to the latent information; Parity Learning is a challenging learning problem. Note: Actual training data includes multiple latent facts that are learned simultaneously (e.g. multiple cities or functions).", "description": "This table gives an overview of five different tasks used to test inductive out-of-context reasoning (OOCR) in LLMs. Each task involves a latent variable that the LLM must infer from indirect observations in the training data and then use to perform downstream tasks. The tasks vary in difficulty and the type of latent information involved, allowing for a comprehensive evaluation of OOCR capabilities across different scenarios.", "section": "2 Studying inductive OOCR via finetuning"}, {"figure_path": "7FokMz6U8n/tables/tables_17_1.jpg", "caption": "Figure 2: Overview of tasks for testing inductive OOCR. Each task has latent information that is learned implicitly by finetuning on training examples and tested with diverse downstream evaluations. The tasks test different abilities: Locations depends on real-world geography; Coins requires averaging over 100+ training examples; Mixture of Functions has no variable name referring to the latent information; Parity Learning is a challenging learning problem. Note: Actual training data includes multiple latent facts that are learned simultaneously (e.g. multiple cities or functions).", "description": "This table presents an overview of five different tasks designed to evaluate inductive out-of-context reasoning (OOCR) in large language models (LLMs). Each task involves a latent variable that the LLM must infer from indirect observations in the training data and then use to answer downstream questions. The tasks vary in complexity and the type of reasoning required, testing different aspects of OOCR capabilities.", "section": "2 Studying inductive OOCR via finetuning"}, {"figure_path": "7FokMz6U8n/tables/tables_25_1.jpg", "caption": "Figure 2: Overview of tasks for testing inductive OOCR. Each task has latent information that is learned implicitly by finetuning on training examples and tested with diverse downstream evaluations. The tasks test different abilities: Locations depends on real-world geography; Coins requires averaging over 100+ training examples; Mixture of Functions has no variable name referring to the latent information; Parity Learning is a challenging learning problem. Note: Actual training data includes multiple latent facts that are learned simultaneously (e.g. multiple cities or functions).", "description": "This table provides an overview of five tasks designed to test inductive out-of-context reasoning (OOCR) in large language models (LLMs).  Each task presents a unique challenge focusing on different aspects of latent information inference, ranging from geographical knowledge (Locations) and probabilistic reasoning (Coins) to complex mathematical functions (Functions and Mixture of Functions), and Boolean logic (Parity Learning).  The table highlights the latent information in each task, and the nature of the training data and evaluation.", "section": "2 Studying inductive OOCR via finetuning"}, {"figure_path": "7FokMz6U8n/tables/tables_37_1.jpg", "caption": "Figure 2: Overview of tasks for testing inductive OOCR. Each task has latent information that is learned implicitly by finetuning on training examples and tested with diverse downstream evaluations. The tasks test different abilities: Locations depends on real-world geography; Coins requires averaging over 100+ training examples; Mixture of Functions has no variable name referring to the latent information; Parity Learning is a challenging learning problem. Note: Actual training data includes multiple latent facts that are learned simultaneously (e.g. multiple cities or functions).", "description": "This table presents five different tasks designed to test the inductive out-of-context reasoning (OOCR) capabilities of large language models (LLMs). Each task involves a latent variable that the LLM must infer from indirect evidence in the training data.  The tasks vary in complexity and type of latent information, showcasing the range of OOCR challenges.", "section": "2 Studying inductive OOCR via finetuning"}, {"figure_path": "7FokMz6U8n/tables/tables_37_2.jpg", "caption": "Figure 2: Overview of tasks for testing inductive OOCR. Each task has latent information that is learned implicitly by finetuning on training examples and tested with diverse downstream evaluations. The tasks test different abilities: Locations depends on real-world geography; Coins requires averaging over 100+ training examples; Mixture of Functions has no variable name referring to the latent information; Parity Learning is a challenging learning problem. Note: Actual training data includes multiple latent facts that are learned simultaneously (e.g. multiple cities or functions).", "description": "This table presents an overview of five different tasks designed to evaluate inductive out-of-context reasoning (OOCR) in LLMs. Each task involves a latent variable (hidden information) that the LLM must infer from indirect observations in the training data.  The tasks vary in complexity and the type of latent information involved (e.g., geographic locations, coin biases, mathematical functions).  The downstream evaluation tasks assess the LLM's ability to utilize the inferred latent information to perform various prediction tasks.", "section": "2 Studying inductive OOCR via finetuning"}, {"figure_path": "7FokMz6U8n/tables/tables_38_1.jpg", "caption": "Figure 2: Overview of tasks for testing inductive OOCR. Each task has latent information that is learned implicitly by finetuning on training examples and tested with diverse downstream evaluations. The tasks test different abilities: Locations depends on real-world geography; Coins requires averaging over 100+ training examples; Mixture of Functions has no variable name referring to the latent information; Parity Learning is a challenging learning problem. Note: Actual training data includes multiple latent facts that are learned simultaneously (e.g. multiple cities or functions).", "description": "This table presents five different tasks designed to test the inductive out-of-context reasoning (OOCR) capabilities of large language models (LLMs).  Each task involves a different type of latent information that the LLM must infer from indirect observations in the training data, and then apply to downstream evaluation tasks. The tasks vary in complexity and type of reasoning required, including real-world knowledge (Locations), statistical reasoning (Coins), mathematical reasoning (Functions, Mixture of Functions), and Boolean logic (Parity Learning).  The table highlights the latent information, training data examples, and evaluation examples for each task.", "section": "2 Studying inductive OOCR via finetuning"}, {"figure_path": "7FokMz6U8n/tables/tables_52_1.jpg", "caption": "Figure 2: Overview of tasks for testing inductive OOCR. Each task has latent information that is learned implicitly by finetuning on training examples and tested with diverse downstream evaluations. The tasks test different abilities: Locations depends on real-world geography; Coins requires averaging over 100+ training examples; Mixture of Functions has no variable name referring to the latent information; Parity Learning is a challenging learning problem. Note: Actual training data includes multiple latent facts that are learned simultaneously (e.g. multiple cities or functions).", "description": "This table provides an overview of five tasks designed to evaluate inductive out-of-context reasoning (OOCR) in large language models. Each task presents a unique challenge to the model's ability to infer latent information from indirectly related training data and apply that information to downstream tasks. The tasks vary in complexity and the nature of the latent information, including real-world knowledge (Locations), probabilistic reasoning (Coins), functional relationships (Functions), distributions over functions (Mixture of Functions), and parity learning.", "section": "2 Studying inductive OOCR via finetuning"}, {"figure_path": "7FokMz6U8n/tables/tables_52_2.jpg", "caption": "Figure 2: Overview of tasks for testing inductive OOCR. Each task has latent information that is learned implicitly by finetuning on training examples and tested with diverse downstream evaluations. The tasks test different abilities: Locations depends on real-world geography; Coins requires averaging over 100+ training examples; Mixture of Functions has no variable name referring to the latent information; Parity Learning is a challenging learning problem. Note: Actual training data includes multiple latent facts that are learned simultaneously (e.g. multiple cities or functions).", "description": "This table presents an overview of five different tasks designed to test the inductive out-of-context reasoning (OOCR) capabilities of large language models (LLMs).  Each task involves a latent (hidden) variable that the LLM must infer from indirect observations in the training data. The tasks vary in complexity and the nature of the latent variable, testing different aspects of OOCR.  The table includes a description of each task, the type of latent information involved, an example of the training data, and an example of the downstream evaluation.", "section": "2 Studying inductive OOCR via finetuning"}]