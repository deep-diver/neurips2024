[{"figure_path": "ntlFREw59A/figures/figures_1_1.jpg", "caption": "Figure 1: Given a sequence of foreground segmentation as input (top), and one frame that describes the background as the condition (left), ActAnywhere generates coherent video background that adapts to the subject motion. We show two subjects here, each with two generated samples. ActAnywhere is able to generate videos consistent with the condition frame with highly realistic details such as splatting water, moving smoke and flame, shadows, duck feet, etc. It generalizes to a diverse distribution of subjects and backgrounds, including non-human subjects. Our method works with both composited frames and background-only images as the condition.", "description": "This figure shows the results of ActAnywhere, a video generation model. Given a sequence of foreground segmentations (top) and a background condition image (left), the model generates a video of the subject interacting realistically with the background. The examples demonstrate the model's ability to handle diverse scenarios and subjects, including humans and ducks, and to generate realistic details like water splashes and shadows.", "section": "1 Introduction"}, {"figure_path": "ntlFREw59A/figures/figures_4_1.jpg", "caption": "Figure 2: Architecture overview. During training, we take a randomly sampled frame from the training video to condition the denoising process. At test time, the condition can be either a composited frame of the subject with a novel background, or a background-only image.", "description": "This figure illustrates the architecture of the ActAnywhere model. During training, the model takes a video, its corresponding foreground segmentation masks, and a randomly sampled frame from the video as input. The foreground segmentation and masks are encoded into latent features using a VAE, and then concatenated with noisy latent features of the input video. These features are passed through a 3D U-Net with spatial and temporal attention layers, conditioned on the CLIP features of the sampled frame. The output is a reconstructed video. At test time, the model takes a video, its corresponding foreground segmentation masks, and a novel background image as input. The features are processed similarly to training, except that the condition is provided by the CLIP features of the background image instead of a sampled frame from the video. The output is a generated video with the subject interacting with the novel background.", "section": "3 Method"}, {"figure_path": "ntlFREw59A/figures/figures_6_1.jpg", "caption": "Figure 3: Diverse results from our method. The top part shows examples using inpainted frames as condition, while bottom contains examples with background-only conditioning. Foreground sequences are from the held-out set of HiC+.", "description": "This figure shows several examples of video background generation results using the ActAnywhere model.  The top half shows results where the input included a composite image of the subject with a partially generated background, while the bottom half shows results from using a background-only image as input. The input videos, in all cases, are from the held-out subset of the HiC+ dataset.", "section": "4.1 Diverse Generation with ActAnywhere"}, {"figure_path": "ntlFREw59A/figures/figures_7_1.jpg", "caption": "Figure 4: Comparison with baselines. We provide results on two videos sampled from the DAVIS [30] dataset. For each example, we show three representative frames (top) and their corresponding condition signal (left). Note that different methods assume different input, conditioning or pre-trained models, as specified in section 4.2.", "description": "This figure compares the results of ActAnywhere with other state-of-the-art video generation and editing methods. Two video examples from the DAVIS dataset are used for comparison. For each example, the figure shows the input video, the condition frame (as a signal for the generation process), and the generated videos from ActAnywhere and other baselines. This visualization helps to understand the differences in the generated results and evaluate the performance of ActAnywhere in comparison to existing techniques.", "section": "4.2 Qualitative Comparison"}, {"figure_path": "ntlFREw59A/figures/figures_13_1.jpg", "caption": "Figure 3: Diverse results from our method. The top part shows examples using inpainted frames as condition, while bottom contains examples with background-only conditioning. Foreground sequences are from the held-out set of HiC+.", "description": "This figure shows several examples of video background generation results using the proposed ActAnywhere model.  The top half shows results where the model was conditioned on an image with the foreground subject already composited into a new background. The bottom half demonstrates results from using a background-only image as a condition.  In both cases, the model successfully adapts the background to match the movement of the foreground subject, creating realistic and coherent videos. The foreground subject sequences are taken from the held-out set of the HiC+ dataset.", "section": "4.1 Diverse Generation with ActAnywhere"}, {"figure_path": "ntlFREw59A/figures/figures_13_2.jpg", "caption": "Figure 3: Diverse results from our method. The top part shows examples using inpainted frames as condition, while bottom contains examples with background-only conditioning. Foreground sequences are from the held-out set of HiC+.", "description": "This figure shows the results of ActAnywhere on various videos from the HiC+ dataset's held-out set. The top row shows examples where the model was conditioned on an inpainted frame (a frame where the background was filled in using an image editing tool). The bottom row shows examples where the model was conditioned on a background-only image. In both cases, the model generates coherent videos that adapt the foreground subject's motion to the background. The diverse examples demonstrate the model's ability to generalize to various scenes and subjects.", "section": "4.1 Diverse Generation with ActAnywhere"}, {"figure_path": "ntlFREw59A/figures/figures_14_1.jpg", "caption": "Figure 7: Videos from various domains. Gaming video from GTA.", "description": "This figure shows the model's generalization ability to various domains such as gaming, animation, and videos with multiple moving subjects.  It demonstrates that the model can successfully generate realistic video backgrounds that adapt to the foreground subject's motion, even across significantly different visual styles and subject types.", "section": "4.1 Diverse Generation with ActAnywhere"}, {"figure_path": "ntlFREw59A/figures/figures_14_2.jpg", "caption": "Figure 8: Our model is robust to inaccurate masks. We show one video sequence from HiC+ with two different condition frames, followed by six generated frames for each.", "description": "This figure demonstrates the robustness of the ActAnywhere model to inaccurate masks.  It shows a video sequence from the HiC+ dataset with two different background condition frames (top row). The middle and bottom rows display six generated frames for each condition, showcasing the model's ability to generate realistic results even when the input foreground segmentation masks are imperfect. Despite the inaccuracies in the masks, the model successfully generates videos with coherent foreground-background interactions.", "section": "7.1 Additional Results and Analysis"}, {"figure_path": "ntlFREw59A/figures/figures_15_1.jpg", "caption": "Figure 2: Architecture overview. During training, we take a randomly sampled frame from the training video to condition the denoising process. At test time, the condition can be either a composited frame of the subject with a novel background, or a background-only image.", "description": "This figure illustrates the architecture of the ActAnywhere model.  The model takes as input a sequence of foreground segmentations and a condition frame (either a composited image or background-only image). It uses a VAE to encode the foreground segmentation and the condition frame into latent features. These features are then concatenated with noisy latent features of the video frames and fed into a 3D U-Net that denoises the features and generates the video. During training, a randomly selected frame from the training video is used as the condition. During inference, a novel background image can be used as the condition.", "section": "3 Method"}, {"figure_path": "ntlFREw59A/figures/figures_15_2.jpg", "caption": "Figure 3: Diverse results from our method. The top part shows examples using inpainted frames as condition, while bottom contains examples with background-only conditioning. Foreground sequences are from the held-out set of HiC+.", "description": "This figure showcases the diverse results obtained using the ActAnywhere model.  The top row displays examples where a composite image (including both subject and background) was used as the condition input, demonstrating the model's ability to seamlessly integrate the subject into the provided scene. The bottom row shows examples where only a background image was used as input.  Despite the differing input types, ActAnywhere successfully generates coherent video backgrounds in all cases that realistically adapt to the foreground subject's motion. The foreground sequences are taken from the held-out subset of the HiC+ dataset.", "section": "4.1 Diverse Generation with ActAnywhere"}]