[{"figure_path": "tZtepJBtHg/figures/figures_1_1.jpg", "caption": "Figure 1: Instances of transductive active learning with target space A shown in blue and sample space S shown in gray. The points denote plausible observations within S to \u201clearn\u201d A. In (A), the target space contains \"everything\" within S as well as points outside S. In (B, C, D), one makes observations directed towards learning about a particular target. Prior work on inductive active learning has focused on the instance A = S.", "description": "This figure shows four different scenarios of transductive active learning, where the goal is to learn a function f within a target space A by actively sampling observations within a sample space S. The four scenarios show how the target and sample space can be related in various ways and how the sampling process can be directed towards specific areas of interest. Scenario A shows a case where the sample space completely covers the target space. Scenario B,C and D demonstrate the scenarios where the sample space does not completely cover the target space and that the sampling process is directed toward learning about a specific target.", "section": "1 Introduction"}, {"figure_path": "tZtepJBtHg/figures/figures_4_1.jpg", "caption": "Figure 2: Initial 25 samples of ITL under a Gaussian kernel with lengthscale 1 (left) and a Laplace kernel with lengthscale 10 (right). Shown in gray is the sample space S and shown in blue is the target space A. In three of the four examples, points outside the target space provide useful information.", "description": "This figure shows examples of how the ITL algorithm selects samples in two different kernel settings (Gaussian and Laplace).  The gray area represents the sample space (S), where ITL is allowed to select samples from, and the blue area represents the target space (A), where ITL aims to reduce uncertainty. The plus signs (+) show the 25 initial samples that the algorithm selected.  Note that in some cases (three out of the four examples), points outside the target space still contribute useful information for the learning task.", "section": "3.3 Experiments in the Gaussian Process Setting"}, {"figure_path": "tZtepJBtHg/figures/figures_4_2.jpg", "caption": "Figure 3: Entropy of fA ranging from -3850 to -3725 and the mean marginal standard deviations of fA ranging from 0 to 0.15. Experiment is using the Gaussian kernel of the left instance (A\u2286S) from Figure 2. It can be seen that ITL and VTL outperform UNSA and RANDOM. Uncertainty bands correspond to one standard error over 10 random seeds.", "description": "This figure compares the performance of several active learning methods on a Gaussian process regression task. The x-axis represents the number of iterations, while the y-axis shows two different metrics: entropy of fA (left) and mean marginal standard deviation of fA (right).  The results demonstrate that Information-Theoretic Transductive Learning (ITL) and Variance-based Transductive Learning (VTL) significantly outperform Uncertainty Sampling (UNSA) and a random baseline.  The experiment uses the Gaussian kernel shown in Figure 2(A), where the target space A is a subset of the sample space S.", "section": "3.3 Experiments in the Gaussian Process Setting"}, {"figure_path": "tZtepJBtHg/figures/figures_6_1.jpg", "caption": "Figure 4: Active fine-tuning on MNIST (left) and CIFAR-100 (right). RANDOM selects each observation uniformly at random from S. The batch size is 1 for MNIST and 10 for CIFAR-100. Uncertainty bands correspond to one standard error over 10 random seeds. We see that transductive active learning with ITL and VTL significantly outperforms competing methods, and in particular, retrieves substantially more samples from the support of PA. See Appendix J for details and ablations.", "description": "This figure compares the performance of ITL and VTL against several baselines (including random sampling) on MNIST and CIFAR-100 datasets for active fine-tuning of neural networks. The results demonstrate that ITL and VTL significantly outperform the baselines in terms of accuracy and the number of samples retrieved from the support of the target distribution.  The y-axis represents accuracy and the number of samples from P<sub>A</sub>, while the x-axis displays the number of samples and batches respectively.  Error bars indicate standard errors across multiple runs.", "section": "Active Fine-Tuning of Neural Networks"}, {"figure_path": "tZtepJBtHg/figures/figures_8_1.jpg", "caption": "Figure 5: We compare ITL and VTL to ORACLE SAFEOPT, which has oracle knowledge of the Lipschitz constants, SAFEOPT, where the Lipschitz constants are estimated from the GP, as well as HEURISTIC SAFEOPT and ISE, and observe that ITL and VTL systematically perform well. We compare against additional baselines in Appendix K.1. The regret is evaluated with respect to the ground truth objective f* and constraint g*, and averaged over 10 (in synthetic experiments) and 25 (in the quadcopter experiment) random seeds. Additional details can be found in Appendix K.4.", "description": "This figure compares the performance of ITL and VTL against several baselines on three different tasks: a 1D synthetic task, a 2D synthetic task, and a quadcopter controller tuning task.  The results show that ITL and VTL consistently achieve lower regret, a measure of the difference between the algorithm's best outcome and the optimal outcome, across all tasks compared to other methods, particularly in scenarios where the Lipschitz constant is unknown or needs to be estimated. This demonstrates the effectiveness of ITL and VTL in finding safe and near-optimal solutions within a limited number of iterations.  The use of oracle knowledge, estimated Lipschitz constants, and heuristic methods are also considered and compared.", "section": "Experiments on Safe Bayesian Optimization"}, {"figure_path": "tZtepJBtHg/figures/figures_42_1.jpg", "caption": "Figure 6: Additional GP experiments", "description": "This figure contains four subfigures that show the results of additional Gaussian process experiments.  The subfigures illustrate different scenarios: extrapolation (where the target and sample spaces do not intersect), heteroscedastic noise (where the noise variance differs across the domain), the effect of smoothness (using a Laplace kernel), and a sparser target space (with fewer target points). The plots compare the entropy of the target space across several different active learning methods (ITL, VTL, CTL, UNSA, local UNSA, TRUVAR, and random sampling) showing how each method performs under various conditions.", "section": "3.3 Experiments in the Gaussian Process Setting"}, {"figure_path": "tZtepJBtHg/figures/figures_43_1.jpg", "caption": "Figure 4: Active fine-tuning on MNIST (left) and CIFAR-100 (right). RANDOM selects each observation uniformly at random from S. The batch size is 1 for MNIST and 10 for CIFAR-100. Uncertainty bands correspond to one standard error over 10 random seeds. We see that transductive active learning with ITL and VTL significantly outperforms competing methods, and in particular, retrieves substantially more samples from the support of PA. See Appendix J for details and ablations.", "description": "This figure compares the performance of ITL and VTL against other active learning methods and a random baseline for the task of active fine-tuning of neural networks on MNIST and CIFAR-100 datasets.  The results show that ITL and VTL significantly outperform other methods, demonstrating their efficiency in selecting informative samples for fine-tuning, and that they select substantially more samples from the relevant part of the data distribution.", "section": "4 Active Fine-Tuning of Neural Networks"}, {"figure_path": "tZtepJBtHg/figures/figures_46_1.jpg", "caption": "Figure 4: Active fine-tuning on MNIST (left) and CIFAR-100 (right). RANDOM selects each observation uniformly at random from S. The batch size is 1 for MNIST and 10 for CIFAR-100. Uncertainty bands correspond to one standard error over 10 random seeds. We see that transductive active learning with ITL and VTL significantly outperforms competing methods, and in particular, retrieves substantially more samples from the support of PA. See Appendix J for details and ablations.", "description": "This figure compares the performance of ITL and VTL against other active learning methods for the task of active fine-tuning of neural networks on the MNIST and CIFAR-100 datasets.  The results demonstrate that ITL and VTL significantly outperform the other methods in terms of accuracy and sample efficiency, especially when considering the proportion of samples selected from the target distribution (PA). The uncertainty bands provide a measure of the variability in the results.", "section": "Active Fine-Tuning of Neural Networks"}, {"figure_path": "tZtepJBtHg/figures/figures_46_2.jpg", "caption": "Figure 8: Comparison of loss gradient (\u201cG-\u201d) and last-layer embeddings (\u201cL-\u201d).", "description": "This figure compares the performance of ITL and CTL using two different types of embeddings: loss gradient embeddings and last-layer embeddings.  The results are shown for MNIST and CIFAR-100 datasets, and both accuracy and the number of samples from the target distribution are plotted. The goal is to see if using different embeddings changes the performance of the active learning methods.", "section": "Additional NN Experiments & Details"}, {"figure_path": "tZtepJBtHg/figures/figures_47_1.jpg", "caption": "Figure 4: Active fine-tuning on MNIST (left) and CIFAR-100 (right). RANDOM selects each observation uniformly at random from S. The batch size is 1 for MNIST and 10 for CIFAR-100. Uncertainty bands correspond to one standard error over 10 random seeds. We see that transductive active learning with ITL and VTL significantly outperforms competing methods, and in particular, retrieves substantially more samples from the support of PA. See Appendix J for details and ablations.", "description": "This figure compares the performance of ITL and VTL against other active learning methods (CTL, UNSA, BADGE, RANDOM, etc.) on the MNIST and CIFAR-100 datasets for the task of active fine-tuning.  ITL and VTL consistently outperform the baselines, demonstrating their superior sample efficiency and ability to select relevant samples.", "section": "Active Fine-Tuning of Neural Networks"}, {"figure_path": "tZtepJBtHg/figures/figures_48_1.jpg", "caption": "Figure 4: Active fine-tuning on MNIST (left) and CIFAR-100 (right). RANDOM selects each observation uniformly at random from S. The batch size is 1 for MNIST and 10 for CIFAR-100. Uncertainty bands correspond to one standard error over 10 random seeds. We see that transductive active learning with ITL and VTL significantly outperforms competing methods, and in particular, retrieves substantially more samples from the support of PA. See Appendix J for details and ablations.", "description": "This figure compares the performance of ITL and VTL against several baselines on MNIST and CIFAR-100 datasets for active fine-tuning of neural networks.  It shows that ITL and VTL significantly outperform random sampling and other active learning methods in terms of accuracy and the number of samples retrieved from the target distribution PA.  Uncertainty bands illustrate the variability of the results. The appendix contains additional experimental details and ablations.", "section": "Active Fine-Tuning of Neural Networks"}, {"figure_path": "tZtepJBtHg/figures/figures_50_1.jpg", "caption": "Figure 12: Comparison of \u201cundirected\u201d baselines for the experiment of Figure 4. In the MNIST experiment, UNSA and UNDIRECTED ITL coincide, and we therefore only plot the latter.", "description": "The figure compares the performance of several \"undirected\" active learning baselines against the ITL and VTL methods on the MNIST and CIFAR-100 datasets.  \"Undirected\" means these methods don't specifically target a subset of the data (like ITL and VTL do). The plot shows accuracy and the number of samples from the target distribution (P<sub>A</sub>) obtained over the number of samples or batches.  It demonstrates that the directed methods (ITL and VTL) significantly outperform the undirected approaches.", "section": "Additional NN Experiments & Details"}, {"figure_path": "tZtepJBtHg/figures/figures_50_2.jpg", "caption": "Figure 4: Active fine-tuning on MNIST (left) and CIFAR-100 (right). RANDOM selects each observation uniformly at random from S. The batch size is 1 for MNIST and 10 for CIFAR-100. Uncertainty bands correspond to one standard error over 10 random seeds. We see that transductive active learning with ITL and VTL significantly outperforms competing methods, and in particular, retrieves substantially more samples from the support of PA. See Appendix J for details and ablations.", "description": "The figure compares different active learning methods on MNIST and CIFAR-100 datasets.  ITL and VTL are the proposed transductive active learning methods, while others are baselines.  The results show ITL and VTL significantly outperform baselines in terms of accuracy, particularly retrieving more relevant samples.  Uncertainty bands represent standard error over 10 runs, illustrating the reliability of results.", "section": "Active Fine-Tuning of Neural Networks"}, {"figure_path": "tZtepJBtHg/figures/figures_50_3.jpg", "caption": "Figure 4: Active fine-tuning on MNIST (left) and CIFAR-100 (right). RANDOM selects each observation uniformly at random from S. The batch size is 1 for MNIST and 10 for CIFAR-100. Uncertainty bands correspond to one standard error over 10 random seeds. We see that transductive active learning with ITL and VTL significantly outperforms competing methods, and in particular, retrieves substantially more samples from the support of PA. See Appendix J for details and ablations.", "description": "This figure compares the performance of ITL and VTL against other active learning methods (CTL, UNSA, BADGE, and RANDOM) on the MNIST and CIFAR-100 datasets.  The results demonstrate that ITL and VTL are significantly more sample-efficient in achieving high accuracy, especially when considering the number of samples selected from the target distribution (PA).", "section": "Active Fine-Tuning of Neural Networks"}, {"figure_path": "tZtepJBtHg/figures/figures_51_1.jpg", "caption": "Figure 10: Advantage of batch selection via conditional embeddings over top-b selection in the CIFAR-100 experiment.", "description": "This figure compares the performance of batch selection using conditional embeddings (BACE) against a baseline approach that selects the top-b most informative points according to a decision rule. The results show that BACE significantly improves accuracy and data retrieval, particularly in the CIFAR-100 experiment. This improvement is attributed to BACE's ability to create diverse and informative batches, unlike the top-b approach which selects points based solely on their individual informativeness without considering the diversity of the batch.", "section": "4 Active Fine-Tuning of Neural Networks"}, {"figure_path": "tZtepJBtHg/figures/figures_52_1.jpg", "caption": "Figure 4: Active fine-tuning on MNIST (left) and CIFAR-100 (right). RANDOM selects each observation uniformly at random from S. The batch size is 1 for MNIST and 10 for CIFAR-100. Uncertainty bands correspond to one standard error over 10 random seeds. We see that transductive active learning with ITL and VTL significantly outperforms competing methods, and in particular, retrieves substantially more samples from the support of PA. See Appendix J for details and ablations.", "description": "This figure compares the performance of ITL, VTL, CTL, UNSA, BADGE, ID, RANDOM, and COSINESIMILARITY on the MNIST and CIFAR-100 datasets for active fine-tuning.  The results demonstrate that ITL and VTL significantly outperform all other methods, achieving higher accuracy with fewer samples from S, particularly retrieving more relevant samples from the support of PA.", "section": "Active Fine-Tuning of Neural Networks"}, {"figure_path": "tZtepJBtHg/figures/figures_52_2.jpg", "caption": "Figure 17: Evaluation of the choice of M, i.e., the size of A ~ PA, in the CIFAR-100 experiment.", "description": "The figure shows the performance of ITL with different sizes of the target space A (M = 10, 50, 100, 500) in the CIFAR-100 active fine-tuning experiment.  The left panel shows the accuracy on the target dataset, while the right panel shows the number of samples selected from the support of the target distribution.  The results demonstrate that larger target space sizes generally lead to better performance, indicating the importance of appropriately sizing the target space for effective active learning.", "section": "Additional NN Experiments & Details"}, {"figure_path": "tZtepJBtHg/figures/figures_53_1.jpg", "caption": "Figure 18: We perform the tasks of Figure 5 using Thompson sampling to evaluate the stochastic target space Pan. We additionally compare to GOOSE (cf. Appendix K.2.3) and ISE-BO (cf. Appendix K.2.4).", "description": "This figure compares different algorithms for safe Bayesian optimization on three different tasks: a 1D task, a 2D task, and a quadcopter controller tuning task.  Each task presents unique challenges in balancing exploration and exploitation while maintaining safety constraints.  The algorithms compared are ITL using two different target spaces (An and PAn), VTL using the same two target spaces, and ISE-BO.  The use of Thompson sampling introduces stochasticity in selecting target points. The figure showcases the simple regret (a measure of performance in safe BO) over the number of iterations, providing a visual comparison of each algorithm's convergence and exploration strategy under the different scenarios. Uncertainty bands are also included.", "section": "Experiments on Safe Bayesian Optimization"}, {"figure_path": "tZtepJBtHg/figures/figures_54_1.jpg", "caption": "Figure 1: Instances of transductive active learning with target space A shown in blue and sample space S shown in gray. The points denote plausible observations within S to \u201clearn\u201d A. In (A), the target space contains \u201ceverything\u201d within S as well as points outside S. In (B, C, D), one makes observations directed towards learning about a particular target. Prior work on inductive active learning has focused on the instance A = S.", "description": "This figure illustrates different scenarios of transductive active learning.  The blue area represents the target space (A), where the goal is to learn the function f. The gray area represents the sample space (S), from which observations can be made. Different panels show varying relationships between A and S, including cases where A is fully contained within S, partially overlaps S, or extends beyond S. The key concept is that observations are made within S to improve knowledge about f specifically within A, whereas previous work typically focuses on learning f globally across the entire domain.", "section": "1 Introduction"}, {"figure_path": "tZtepJBtHg/figures/figures_56_1.jpg", "caption": "Figure 1: Instances of transductive active learning with target space A shown in blue and sample space S shown in gray. The points denote plausible observations within S to \u201clearn\u201d A. In (A), the target space contains \"everything\" within S as well as points outside S. In (B, C, D), one makes observations directed towards learning about a particular target. Prior work on inductive active learning has focused on the instance A = S.", "description": "This figure shows four different scenarios of transductive active learning, where the goal is to learn a function f in a target space A by sampling from a smaller sample space S.  In the first example (A), the target space includes all points in S and also points outside of S. The other examples (B, C, D) show more directed learning, where points are sampled specifically to learn about particular regions or targets within A.  The figure highlights that prior work primarily focused on inductive active learning (where A = S), while this paper addresses the more general problem where A and S can be different.", "section": "1 Introduction"}, {"figure_path": "tZtepJBtHg/figures/figures_59_1.jpg", "caption": "Figure 21: The ground truth f* is shown as the dashed black line. The solid black line denotes the constraint boundary. The GP prior is given by a linear kernel with sin-transform and mean 0.1x. The light gray region denotes the initial optimistic safe set So and the dark gray region denotes the initial pessimistic safe set So.", "description": "This figure shows a 1D synthetic experiment setup for Safe Bayesian Optimization. The dashed black line represents the true objective function (f*), while the solid black line shows the constraint boundary (g*).  The light gray area indicates the initial optimistic safe set (So), meaning regions where the algorithm is initially confident that g*(x) \u2265 0. The darker gray area represents the initial pessimistic safe set, where the algorithm is less certain about the constraint being satisfied. A Gaussian process prior with a linear kernel and sin-transform, along with a mean of 0.1x, is used.", "section": "5 Safe Bayesian Optimization"}, {"figure_path": "tZtepJBtHg/figures/figures_59_2.jpg", "caption": "Figure 22: First 100 samples of ITL using the potential expanders En (cf. Equation (27)) as target space (left) and SAFEOPT sampling only from the set of expanders GSAFEOPT (right).", "description": "This figure compares the sampling strategies of ITL and SAFEOPT.  Both methods aim to solve a safe Bayesian optimization problem. The left panel shows ITL's sampling, which uses the potential expanders (points that might be safe but aren't yet known to be) as its target space.  In contrast, the right panel illustrates SAFEOPT's approach, which samples only from the expanders.  The figure highlights how ITL can explore more effectively by considering a broader set of potentially safe points, enabling it to overcome local barriers and discover globally optimal solutions more quickly than SAFEOPT, which focuses its exploration only on points already deemed close to being safe.", "section": "K.3 Jumping Past Local Barriers"}, {"figure_path": "tZtepJBtHg/figures/figures_60_1.jpg", "caption": "Figure 23: Ground truth and prior well-calibrated model in 1d synthetic experiment. The function serves simultaneously as objective and as constraint. The light gray region denotes the initial safe set S0.", "description": "The figure shows the ground truth function (dashed black line), a well-calibrated GP model (blue line with shaded uncertainty region), and a safety constraint (black horizontal line).  The light gray area represents the initial safe region S0 where the algorithm can safely explore before learning. The goal is to maximize the objective function while remaining in the safe region.", "section": "K.4.1 Synthetic Experiments"}, {"figure_path": "tZtepJBtHg/figures/figures_60_2.jpg", "caption": "Figure 4: Active fine-tuning on MNIST (left) and CIFAR-100 (right). RANDOM selects each observation uniformly at random from S. The batch size is 1 for MNIST and 10 for CIFAR-100. Uncertainty bands correspond to one standard error over 10 random seeds. We see that transductive active learning with ITL and VTL significantly outperforms competing methods, and in particular, retrieves substantially more samples from the support of PA. See Appendix J for details and ablations.", "description": "The figure shows the results of active fine-tuning experiments on MNIST and CIFAR-100 datasets.  It compares the performance of ITL and VTL against several baselines, including random sampling. The results demonstrate that transductive active learning using ITL and VTL significantly improves sample efficiency and outperforms other methods in terms of accuracy.", "section": "Active Fine-Tuning of Neural Networks"}, {"figure_path": "tZtepJBtHg/figures/figures_61_1.jpg", "caption": "Figure 25: Ground truth in 2d synthetic experiment.", "description": "The figure shows the ground truth of the objective and constraint functions in the 2d synthetic experiment. The objective function has two local maxima, while the constraint function defines a circular safe region. The visualization helps understand the complexity of the optimization problem and how the algorithm navigates the safe region to find the optimal solution. ", "section": "K.4.2 Safe Controller Tuning for Quadcopter"}]