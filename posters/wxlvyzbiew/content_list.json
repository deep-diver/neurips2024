[{"type": "text", "text": "Building on Efficient Foundations: Effectively Training LLMs with Structured Feedforward Layers ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xiuying Wei xiuying.wei@epf.lch CLAIRE, EPFL ", "page_idx": 0}, {"type": "text", "text": "Skander Moalla skander.moalla@epf.lch CLAIRE, EPFL ", "page_idx": 0}, {"type": "text", "text": "Razvan Pascanu razp@google.com Google DeepMind ", "page_idx": 0}, {"type": "text", "text": "Caglar Gulcehre caglar.gulcehre@epf.lch CLAIRE, EPFL ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "State-of-the-art results in large language models (LLMs) often rely on scale, which becomes computationally expensive. This has sparked a research agenda to reduce these models\u2019 parameter counts and computational costs without significantly impacting their performance. Our study focuses on transformer-based LLMs, specifically targeting the computationally intensive feedforward networks (FFNs), which are less studied than attention blocks. We consider three structured linear parameterizations of the FFN using efficient low-rank and block-diagonal matrices. In contrast to many previous works that examined these approximations, our study i) explores these structures from a training-from-scratch perspective, ii) scales up to 1.3B parameters, and iii) is conducted within recent Transformer-based LLMs rather than convolutional architectures. We demonstrate that these structures can lead to actual computational gains in various scenarios, including online decoding when using a pre-merge technique. Additionally, we propose a novel training regime, called self-guided training, aimed at improving the poor training dynamics that these approximations exhibit when used from initialization. Interestingly, the scaling performance of structured matrices is explored, revealing steeper curves in scaling training FLOPs, along with a favorable scaling trend in the overtraining regime. Specifically, we show that wide and structured networks can utilize training FLOPs more efficiently, with fewer parameters and lower loss than dense models at their optimal trade-off. Our code is available at https://github.com/CLAIRE-Labo/StructuredFFN/tree/main. ", "page_idx": 0}, {"type": "table", "img_path": "WxLVYZbIew/tmp/dfcf7bd6e2877d9a3f8a541bae6ca917e7ea3b7cb2e2b9fbc19c0a7d5d3fa865.jpg", "table_caption": ["Table 1: Better training FLOPs utilization of the wide and structured Networks: we compare dense Transformers trained according to their optimal scaling law [1], efficient Transformers (GQA) [2] with high throughput, and our wide and structured networks using LowRank parameterization in the FFN module and reduced attention heads, under the same training FLOPs. TP (throughput) refers to the maximum throughput measured over a generation length of 256. "], "table_footnote": [], "page_idx": 0}, {"type": "image", "img_path": "WxLVYZbIew/tmp/77b56aa0fb9531cbee3e0b62cd319e32342de0d89491353ba36fc3ec6d3370f1.jpg", "img_caption": ["Figure 1: Steeper scaling curves of LowRank with ${\\bf{63\\%}}$ or ${}32\\%$ FFN parameters. For more results, see Sec. 4.2. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Transformer language models [3] have gained significant attention for their performance and scalability. These models have grown from hundreds of millions of parameters [4] to hundreds of billions [5\u20137], increasing the need for efficient training and inference techniques. While much research focuses on attention, feed forward networks (FFNs) account for over $60\\%$ of the model\u2019s parameters and FLOPs, significantly impacting latency.1 Recent large-scale models [8, 9] further increase the FFN size, leading them to dominate the cost of the model compared to the attention layer. ", "page_idx": 1}, {"type": "text", "text": "Structured linear transformations, such as low-rank or block-diagonal matrices, are important paradigms for reducing the computational cost of feedforward layers. However, they have not yet been thoroughly explored at a sufficient scale to reduce pre-training costs and latency of the inference phase in modern LLM architectures, where the main focus so far has been on improving the efficiency of the self-attention mechanism. ", "page_idx": 1}, {"type": "text", "text": "In this work, we investigate structured matrices for FFN blocks from the train-from-scratch aspect, first identifying their efficiency and optimization challenges and then presenting experimental results, analyzing and characterizing the behavior of models trained with structured matrices, and comparing their results. We consider three efficient linear parametrizations: LowRank, BlockShuffle (comprising two block-diagonal matrices), and BlockDense (a combination of dense and block-diagonal matrices). First, while they have demonstrated materialized computational gains, they face challenges in the practical online decoding scenario of LLM, which may process only limited input tokens at one time, leading to under-utilization of computing resources and decreased efficiency due to the additional linear projection. We address this with a pre-merge technique that restores efficiency to the original dense parametrization. Second, we observe that these parameterizations of the FFN blocks are harder to train than standard linear layers, often exhibiting poorer training dynamics like loss spikes. To counter this, we propose a flexible and fast method we refer to as self-guided training. It employs a dense matrix as a residual component during the initial training phase, steering the training process away from suboptimal starting points gradually. ", "page_idx": 1}, {"type": "text", "text": "We conduct our experiments at scale on Transformers ranging from 110M to 1.3B parameters by replacing the traditional heavy FFN with structured matrices. Our experiments first show the scaling behavior of these structured linear parameterizations and then illustrate how our proposed methods address their general efficiency and optimization challenges. First, we examine scaling performance from the perspectives of training compute and model size, highlighting the potential of structured matrices. By controlling for the same training FLOPs, we find that structured FFNs show steeper loss scaling curves than traditional Transformers at optimal trade-offs (See Fig. 4 and Fig. 1). Specifically, as seen in Table 1, our wide and structured networks use training FLOPs more efficiently, needing fewer parameters (464M vs. 729M) and achieving a $17\\%$ throughput boost on the -l scale, while still maintaining slightly better perplexity compared to the efficient Transformer [2]. Beyond training compute scaling, we also scale model size in the overtraining regime, with Fig. 5 showing favorable scaling trends for our wide and structured models. Second, our results on efficiency show that structured FFNs, with only $32\\%$ of the FFN parameters, can boost the training speed of the 1.3B model by $1.35\\times$ Furthermore, self-guided training enhances the performance of all three structured matrices (e.g., reducing the perplexity gap of LowRank to about 0.4) without affecting the inference time speed-up. ", "page_idx": 1}, {"type": "text", "text": "As the first work to explore structured matrices at the scale of recent LLMs, we hope our findings and results will shed new light on the study of efficient NLP architectures. Our contributions can be categorized into the following three aspects: ", "page_idx": 1}, {"type": "text", "text": "1. We investigate three types of structured matrices in Transformer pretraining and demonstrate their favorable scaling behavior compared to dense models. This is revealed through the study of scaling laws for training FLOPs, as well as model size scaling in the overtraining regime, showing that wide and structured networks can be strong candidates for architecture design.   \n2. We conduct an efficiency study of these structured matrices across various scenarios. We propose a pre-merge technique to maintain speed in a specific case and show the effective speed-up performance of structured matrices in other scenarios.   \n3. We identify optimization challenges in structured matrices and introduce a method called selfguided training, which efficiently improves training dynamics and boosts the final performance for all three types of structured matrices. ", "page_idx": 1}, {"type": "text", "text": "2 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Multiple techniques have been proposed to approximate linear projections from sparsity to low-rank approximations. We provide an in-depth discussion of existing literature on approximating linear layers in section 3, that better contextualizes our work. We focus on structured approximations of linear projections $g(\\pmb{x})\\!=\\!\\pmb{W}\\pmb{x}$ that have the form $g(\\pmb{x})\\!=\\!\\pmb{U}(\\pmb{V x})$ , where $U$ and $V$ are structured matrices, e.g. low-rank or block-diagonal. We opt for this particular form because it allows us to readily exploit the computational gains on existing hardware using existing libraries with minimal alteration. Such approximations have been previously studied in different contexts. Our contributions are exploring them (i) to approximate FFN layers of transformers, (ii) when applied from initialization, (iii) testing them at up to 1.3B scale, investigating their general bottlenecks and providing scaling analyses. ", "page_idx": 2}, {"type": "image", "img_path": "WxLVYZbIew/tmp/8cc2eea9692bd4fcb4257fdfddf2c292ad5bb908211df2c5f54f0133a5a3016f.jpg", "img_caption": ["Figure 2: Structured linear parametrization: We show the structured linear parametrization with input dim. of $N$ and output dim. of $M$ . a) The traditional dense linear parametrization. b) LowRank parametrization with a bottleneck of size $R$ where $R$ is less than $M$ and $N$ . c) BlockShuffle with two block-diagonal matrices with blocks of size $B$ interleaved with a shuffle operations that mixes information from different blocks similar to ShuffleNet. d) BlockDense with the first matrix as a block-diagonal and the second a low-rank or dense matrix. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2.1 Structured linear parametrization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We explore three structured matrices to approximate the standard linear layer $W x$ , maintaining its input dimension $N$ and output dimension $M$ of weight $W$ . ", "page_idx": 2}, {"type": "text", "text": "LowRank Low-rank matrices have been widely used to decompose pre-trained weights for downstream compression [10] or to construct adapters for efficient fine-tuning [11]. Researchers [12] suggest that dense layers tend to naturally converge to low-rank solutions during training, making this approximation ideal. Inspired by this, we explore low-rank matrices as alternatives to traditional linear layers, imposing this structure from initialization and investigating it during pre-training. ", "page_idx": 2}, {"type": "text", "text": "Formally, the low-rank approximation of a linear layer is given as $W\\pmb{x}\\approx\\pmb{U}^{r}(\\pmb{V}^{r}\\pmb{x})$ where $U^{r}\\in\\mathbb{R}^{M\\times R}$ , $V^{r}\\in\\mathbb{R}^{R\\times\\hat{N}}$ and $R<\\operatorname*{min}(M,N)$ . Note that we use the superscript $r$ to indicate that these matrices are used to create a low-rank approximation by projecting to or from a low-dimensional code, a notation that would become useful later on to distinguish such components from block-diagonal ones. The parameter count and MAC (Multiply-Accumulate Operations) decrease from $M\\!\\cdot\\!N$ to $(M\\!+\\!N)\\!\\cdot\\!\\bar{R}$ . ", "page_idx": 2}, {"type": "text", "text": "BlockShuffle Dao et al. [13] proposes using the Monarch decomposition of FFT, replacing the linear layer with interleaved block-diagonal and permutation matrices. An alternative motivation for such a structure can be derived from efficient convolutional designs of ShuffleNet [14] and separable convolution [15]. For simplicity, we explore the form introduced by ShuffleNet to linear layers. ", "page_idx": 2}, {"type": "text", "text": "The core idea of BlockShuffle is to reshape the feature dimension into two dimensions and first use a linear projection that mixes along one of these fictive dimensions, followed by a linear projection tahnadt  ampipxleys  tahloe nngo tnh-et ioetdh ewr.e iMghotr eo fp $\\begin{array}{r}{\\frac{N}{B}\\times\\frac{\\bar{N}}{B}}\\end{array}$ , tow ee afcirsht  brleoschak,p et htehne  ifnlapttuet nf etahteu r $\\mathbf{\\Delta}x\\in\\mathbb{R}^{N}$ tien tfoe $B$ rbel.o cTkos achieve global perception, we regroup elements from different blocks into $B$ new blocks and apply the same transformation for each block again. ", "page_idx": 2}, {"type": "text", "text": "Technically, we can express the per-block transformation using block-diagonal matrices and formulate the above process as $W_{\\mathbf{\\lambda}}^{\\cdot}\\mathbf{\\lambda}_{\\mathbf{\\lambda}}\\approx f^{-1}(U^{b}f(V^{b}\\mathbf{\\lambda}_{\\mathbf{\\lambda}}))$ , where block-diagonal matrices $V^{b}$ and $U^{b}$ has $B$ blocks with shapes $\\frac{\\operatorname*{min}(N,M)}{B}\\times\\frac{N}{B}$ )\u00d7 NB and MB \u00d7 min(BN,M) per-block. As shown in Fig. 2, the shuffle function $f(\\cdot)$ enables global feature mixing by cycling different blocks and can be implemented by simply transposing and reshaping inner features. The inverse function $f^{-1}(\\cdot)$ permutes the outputs back to their original order. ", "page_idx": 3}, {"type": "text", "text": "By separating features into two dimensions, only a few elements of the features will be processed each time. The parameter count and MAC are reduced from M \u00b7N to min(N,M)\u00b7 (MB+N), where $B$ acts as a trade-off of accuracy and efficiency. ", "page_idx": 3}, {"type": "text", "text": "BlockDense The previous parametrization incorporates additional shuffle operations, which can be slow on the device. We propose a natural intermediate candidate between LowRank and BlockShuffle, combining the block-diagonal projection $V^{b}$ with a dense or low-rank projection $U^{r}$ . Thus, we can mix the features between blocks without permuting the inner features. The formula is defined as $W x\\!\\approx\\!U^{r}(V^{b}x)$ , where $V^{b}$ is the block-diagonal matrix with $b$ blocks in shape $\\textstyle{\\frac{R}{B}}\\times{\\frac{N}{B}}$ B , and $U^{r}\\in\\mathbb{R}^{M\\times R}$ . Technically, the second projection does not need to be a low-rank approximation because $R$ can be larger than $M$ . Nevertheless, in practice, we chose $R\\!<\\!M$ to limit the search space of this work, and thus use the superscript $r$ for the second matrix. The parameters of this method are determined by two variables $B$ and $R$ , cutting the original burden from $M\\!\\cdot\\!N$ to $\\begin{array}{r}{R\\cdot(M+\\frac{N}{B})}\\end{array}$ . Note that BlockDense can recover the LowRank approximation if we set $B\\!=\\!1$ and $R\\!<\\!\\operatorname*{min}(\\bar{M},\\!N)$ . ", "page_idx": 3}, {"type": "text", "text": "Remark We limit our exploration of the efficient linear parametrizations within the FFN blocks.These typically have $8\\!\\cdot\\!H^{2}$ parameters and MAC, where $H$ standards for hidden state dimension and $4\\!\\cdot\\!H$ as the intermediate hidden size of FFN. In contrast, the proposed parametrizations have: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{Rank:10\\cdotH\\cdotR}\\qquad\\mathrm{BLockShuff1e:10\\cdot}\\frac{H}{B}\\qquad\\quad\\mathrm{BLockDense:}\\cdot5\\cdot H\\cdot R\\cdot(1+\\frac{1}{B})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Although BlockDense is introduced as a new parameterization, the aim of this paper is not to claim it as the best candidate, but rather to investigate some general properties of structured matrices from efficiency, optimization, and scaling perspectives. Given the favorable efficiency and loss performance of BlockDense, it is included alongside LowRank and BlockShuffle here to cover a broader range of potential parameterizations. ", "page_idx": 3}, {"type": "text", "text": "2.2 Maintaining efficiency during online decoding ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Parallelism-bound FFN With reduced FLOPs and parameters, our proposed linear parametrization can accelerate the model for compute-bound and memory-bound scenarios [16], usually during training, prefliling, and decoding with a relatively big batch size. However, for online decoding with a very small batch size and sequence length of 1, a practical scenario for LLM, both FFN and structured FFN can become parallelism-bound [17] with poor utilization of the GPU resources, especially on powerful devices like A100. Because each linear projection suffers from parallelism-bound, efficient linear parametrization may lead to worse latency performance due to doubling the number of linear projections. We propose a pre-merge technique to mitigate the issue. ", "page_idx": 3}, {"type": "text", "text": "Pre-merge technique Taking advantage of the fact that these parametrizations do not have nonlinearity, we propose to combine the structured matrices into a single dense layer and keep both the structured and the dense one for online decoding. Then, we can dynamically decide which parametrization to use based on the current batch size and setting. Fig. 7 analyzes using structured or dense forms for different batch and model sizes, allowing us to decide when to use the pre-merged linear projection. ", "page_idx": 3}, {"type": "text", "text": "2.3 Addressing the optimization challenge ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Using the efficient parametrization from initialization can suffer from optimization difficulty because the deep linear parametrization introduces additional symmetries2, which is a source of proliferation of saddle points and generally less smooth loss function as pointed out in [18, 19]. We hypothesize that this makes poorer learning dynamics of the structured parametrization. Empirically, we found that the deep linear form $U(V{\\bar{x}})$ is more difficult to train than the standard linear layer. For example, in Fig. 3, it can suffer from training instability and loss spikes with a large learning rate, while also converging much more slowly than the dense form with a small learning rate. We further elaborate on this, highlighting how the inconsistency of gradient updates between the structured parametrization and original linear projection affect learning dynamics in Appendix B.2. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "WxLVYZbIew/tmp/89937deda3da524e166d01c0964589aff3d376bad87c6468479bdc3ee4ca3048.jpg", "img_caption": ["Figure 3: Poor training dynamics: Training dynamics of LowRank with rank of 128 under different training configurations. Curves correspond to a 4-layer Transformer with a model width of 768 on WikiText-103. We apply self-guided training in the first half of training. Refer to Sec. B.1 for more training dynamics visualizations of the other two structured parameterizations. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Self-guided training Addressing the poor training dynamics by carefully tuning the learning rate schedule and gradient clipping coefficient might be possible, but it is costly and may switch between slow convergence and training instability. We propose a less costly and simple approach that can be used with minimal re-tuning of hyperparameters. ", "page_idx": 4}, {"type": "text", "text": "To motivate our proposal, we start by finding that the updates on $U V$ scales the function of the backpropagated gradients $\\textbf{\\textit{g}}$ (see App. B.2), then turn into the typical training dynamics with gradients. An issue that needs to be addressed in the early stage of training is feature specialization when the learning process assigns semantics to the different hidden units of the model, sometimes also phrased as identifying the winning ticket [20]. In this process, certain weights will need to be suppressed, and symmetries in the parametrization must be resolved. ", "page_idx": 4}, {"type": "text", "text": "To address this problem, we propose using the dense parametrization $W$ as a crutch to efficiently make decisions about feature specialization and then transfer them to $U$ and $V$ through $\\textbf{\\textit{g}}$ . To this end, we use the following parametrization ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{o}\\!=\\!\\alpha\\!\\cdot\\!\\pmb{W}\\pmb{x}\\!+\\!(1\\!-\\!\\alpha)\\!\\cdot\\!\\pmb{U}\\!(\\pmb{V}\\pmb{x}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "$^o$ is the layer\u2019s output, and $\\alpha$ decays following a cosine scheduler. As a residual component, learning $W$ is unaffected by the additional saddles and pathologies introduced by the structured parametrization, allowing units to specialize. This guides the training of $U$ and $V$ , which are forced slowly to take over by providing the hidden units semantics learned by $W$ . This approach also relates to homotopy methods such as simulated annealing, where a hard optimization problem is transformed into an easier to optimize form with certain desired properties, gradually transforming the problem to its original form. Here, we consider the easier optimization problem is to train with dense matrices. By decreasing alpha from 1 to 0, we transform the easier-to-optimize loss into the original parametrization we want to use. ", "page_idx": 4}, {"type": "text", "text": "Furthermore, we initialize $W_{0}=U_{0}V_{0}$ , making the observation that by using this initialization for the dense residual branch, we can easily start the guided training at any stage of learning (e.g., fine-tuning) without affecting the model behavior. We refer to this as self-guiding training, as the better learning dynamics from $W$ , which initially is just $U V$ , guide the learning of $U$ and $V$ through the backpropagated gradients $\\textbf{\\textit{g}}$ . ", "page_idx": 4}, {"type": "text", "text": "Guided by the dense weights, which do not have the symmetry problem, it becomes much easier for the structured matrices to learn a good representation. From Fig. 3, it can be observed that the self-guided training prevents training spikes and fastens the convergence process. Notably, it beneftis all three structured matrices with improved training dynamics illustrated in Sec. B.1 and better final performance shown in Sec. 4.4. ", "page_idx": 4}, {"type": "text", "text": "Reducing the computational cost of self-guided training: Note that while $\\alpha>0$ , we need to perform forward and backward passes through the dense version of the weight matrix $W$ , which could be expensive. To address this issue, we consider the following stochastic version of the above formula, which allows us to control how often we need to use the dense residual branch: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{o}\\!=\\!\\left\\{\\!\\!\\begin{array}{l l}{\\!\\alpha\\!\\cdot\\!W\\pmb{x}\\!+\\!(1\\!-\\!\\alpha)\\!\\cdot\\!\\pmb{U}(V\\pmb{x}),}&{p\\!<\\!\\alpha}\\\\ {\\!\\pmb{U}(V\\pmb{x}),}&{p\\!\\geq\\!\\alpha.}\\end{array}\\!\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In our practice, $p$ is a random variable sampled independently from a uniform distribution over 0 to 1 in each training forward pass. With $\\alpha$ following a cosine annealing schedule, this softer version Eq. (2) reduces the expected computation to half of Eq. (1). For example, using our method for half the training time increases the FLOPs by only $25\\%$ of the original FFN. This has a negligible impact on accuracy. More ablation studies of this technique are presented in Sec. B.3. ", "page_idx": 5}, {"type": "text", "text": "3 Related work ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Efficient techniques for training LLMs Recent advancements in attention mechanisms have significant improvements in the efficiency of attention [21\u201323, 2, 24\u201329], and the focus has shifted towards improving FFNs in LLMs, which contributes to at least half of the training time. Dynamic architectures such as mixtures of experts [30\u201332], or optimizers with faster convergence [33, 34] have been popular in improving training efficiency. Moreover, Xia et al. [35] employs structured pruning with learned sparsity masks and a dedicated data-loading policy to reduce the training budget. ", "page_idx": 5}, {"type": "text", "text": "There has been a recent focus on parameter-efficient fine-tuning methods like LoRA [11] and structured approximation of the linear layers (see, [10]). LoRA uses the low-rank approximation to reduce trainable parameters during the finetuning phase, whereas Sharma et al. [10] selectively applies low-rank decomposition to well-trained weights. While these methods used low-rank approximation of the weights, they did not focus on pre-training. ", "page_idx": 5}, {"type": "text", "text": "Structured matrices in deep learning Researchers use structured matrices in the form of dense matrices with shared parameters, like Circulant and Toeplitz $[36]^{3}$ , and structured matrices, such as low-rank and diagonal, to reduce parameters and FLOPs while optimizing CUDA kernel use. Low-rank matrices, initially used in convolutional networks [37], have shown high efficiency in training [38], achieving up to a $2.9\\times$ speed-up with similar performance. Some studies [39, 40] adapt the rank during training and suggest regularizers to maintain SVD decomposition for better accuracy. Khodak et al. [41] propose spectral initialization and aligning weight decay of matrix products with standard linear layers. However, these studies mainly focus on ResNets [42] rather than recent LLMs. There have been other studies that aim to improve the expressiveness of structured matrices. For instance, Moczulski et al. [43] uses interleaved diagonal and Fourier transforms, while Dao et al. [44] proposes butterfly parametrization for various transformations. These approaches often lack efficiency due to additional layers or irregular sparsity patterns. Dao et al. [13] simplified butterfly matrices to block-diagonal ones, achieving a $2\\times$ speed-up on WikiText-103 language modeling tasks. In this work, for accuracy and efficiency, we explored low-rank factorization of weight matrices with reduced bottleneck dimension and block-diagonal matrices to reduce parameters in our LLM training studies. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In our experiments, we empirically analyzed the performance of scaling, efficiency, and self-guided training for structured parameterization in LLMs. ", "page_idx": 5}, {"type": "text", "text": "4.1 Settings ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Model We perform the experiments on the standard Transformer architecture [45, 4] equipped with rotary positional embeddings [46] and the Llama Tokenizer [47]. Its FFN module is composed of two linear layers and a GeLU activation. Four sizes are considered, including Transformer-s (110M), Transformer-m (335M), Transformer-l (729M), and Transformer-xl (1.3B). For our efficient parameterizations, we only make the FFN module structured in most experiments to simplify our study, as the attention module has been well-studied [2, 23]. We explore two sizes that retain $63\\%$ or $32\\%$ of the dense FFN parameters by adjusting the rank and number of blocks (e.g., using a rank half the FFN width in LowRank reduces the parameters to $63\\%$ ). In particular, to provide more comparative results with dense models in the scaling study Sec. 4.2, we further design the wide and structured networks, where both the attention and FFN modules are made structured using [2] and the structured matrices. This is because allocating more parameters to the FFN compared to the attention module is more favorable, and making them both structured helps maintain the parameter ratio between them. Detailed configurations can be found in Table 10. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "For implementation, we take Dao et al. [13]\u2019s implementation for the BlockShuffle and carefully manage memory copies for BlockDense. In our experiments, we chose $B$ as a common divisor of $M$ and $N$ or $R$ . Proper initialization is also investigated in Sec. A.1. ", "page_idx": 6}, {"type": "text", "text": "Training We use the RefinedWeb dataset [48] and randomly select 0.5B tokens for validation, reserving the rest for training. All experiments, except for the overtraining experiments on 300B tokens in Fig. 5, are based on the Chinchilla scaling law [1], where tokens are allocated at 20 times the baseline model size. We set hyperparameters such as learning rates and global batch size according to the scaling law studies from recent papers [49, 50]. However, for the 300B token experiments, we found that more advanced hyperparameter settings are necessary. For example, we use betas of [0.9, 0.98]. Additionally, different works tend to use very different learning rates [51, 49, 52, 47] in the overtraining regime. Thus, we follow the scaling law of hyperparameters described in Bi et al. [53] to avoid extensive hyperparameter searches. Other implementations include using A100 80G GPUs, mixed precision (bfloat16 and float32), and adopting fast CUDA kernels like Flash Attention [25] for all experiments. We measure training FLOPs as in Megatron [54], including all matrix multiplications. Additional details are provided in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "4.2 Scaling analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate the scaling performance of structured linear parameterizations from two perspectives. The first study investigates the scaling law of training-time compute. The second study trains these models with 300B tokens and evaluates their performance on downstream tasks. The results show that our structured matrices can serve as a strong alternative to the dense FFN, utilizing training FLOPs more efficiently (e.g., smaller model and lower loss) and performing better in the overtraining regime. ", "page_idx": 6}, {"type": "image", "img_path": "WxLVYZbIew/tmp/4484e1fba24f2fe9e0666693569a138f4e0f4711a66615ca9029685bb067e27d.jpg", "img_caption": ["Figure 4: Scaling curves of structured matrices with a linear fti for better illustration. The dense model is trained at its optimal trade-off while we train structured FFNs on the same number of tokens and retain $63\\%$ or $32\\%$ of the original parameters. 1) Structured matrices have steeper scaling curves with much closer results at larger sizes, showing good scaling behavior. 2) With the same training FLOPs, these curves indicate that structured matrices can have fewer parameters and lower validation loss when the $\\mathbf{X}$ -axis is further extended. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Scaling law study: better training FLOPs utilization Based on the Chinchilla scaling law, we train four sizes of Transformer models and then use the same amount of tokens to train the structured alternatives. First, we only make the FFN module structured to build the basic understanding, and retain $63\\%$ or $32\\%$ of the original parameters. In Fig. 4 and Fig. 1, we apply a linear fit to the scaling points for better illustration and show that all three structured matrices have steeper scaling curves compared to the dense models, indicating the significant potential of highly structured large models. More importantly, by fixing the training FLOPs, they have fewer parameters and eventually achieve very close or even slightly lower loss (e.g., LowRank with $63\\%$ parameters). Given their steeper scaling curves, we can also expect noticeably lower loss and fewer parameters for structured parameterizations per FLOP when the x-axis is further extended. Detailed numbers are provided in Table 6 in the appendix, with comparisons among the three structured parameterizations. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Next, the attention module is also structured using GQA [2], resulting in wide and structured networks. This further optimizes the use of training FLOPs, addressing the imbalance caused by structuring only the FFN module, which increases the relative impact of the attention module on the overall architecture. We adopt LowRank as an example, as it demonstrates superior performance compared to the other two approaches in our settings, as demonstrated in Table 6 and Fig. 4. To align the training FLOPs, the wide and structured networks are trained on a larger number of tokens. It can be observed in Table 1 that these models achieve lower perplexity while using much fewer parameters. For instance, the parameter count can be reduced from 729M to 464M without compromising perplexity. Additionally, in terms of maximum throughput, ours models achieve an $8\\%$ and $17\\%$ boost on Transformer-m and Transformer-l, respectively, compared to the fast GQA. ", "page_idx": 7}, {"type": "text", "text": "In conclusion, the structured matrices and the wide and structured networks demonstrate great potential in optimizing training FLOP utilization, achieving lower loss with fewer parameters. Additionally, it is important to note that our scaling curves for the structured matrices are not drawn at their optimal training-compute trade-off, while the baseline is. ", "page_idx": 7}, {"type": "text", "text": "Scaling model size: better downstream performance To further illustrate the potential of structured matrices, we consider the overtraining regime and use LowRank as an example. Specifically, we train four sizes of the dense model on 300B tokens, and build the wide and structured networks upon the design of the dense models by applying LowRank to the FFN and reducing the number of attention heads to make the entire network structured. Then, the well-trained models are evaluated on downstream tasks, including PIQA, HellaSwag, Winogrande, and ARC tasks, using lm-evaluation-harness4 with the default prompt. Fig. 5 presents the results, displaying the scaling trend across the four tasks (see detailed numbers and additional tasks in Table 8). The wide and structured models demonstrate comparable or superior performance, particularly at larger sizes, solidifying their benefits over dense architectures. ", "page_idx": 7}, {"type": "image", "img_path": "WxLVYZbIew/tmp/34d66e21a8cdcc6c876ab261d1566370ff41daa4e79e9c2ded98eda5e2c05585.jpg", "img_caption": ["Figure 5: Zero-shot performance on downstream tasks in the overtraining regime. The wide and structured networks are built upon dense ones by applying LowRank to the FFN and reducing the number of attention heads to make the entire network structured. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.3 Efficiency study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We investigate the efficiency of structured FFN and consider different numbers of tokens $T$ to discuss different scenarios. Here, $T$ corresponds to the total number of tokens in a batch. ", "page_idx": 7}, {"type": "image", "img_path": "WxLVYZbIew/tmp/95511c412604b3aff099cc1759a4539fcddaf7dc8f2fa3ae92dfb50884e9012e.jpg", "img_caption": ["Figure 6: Latency of structured and dense FFNs across different FFN widths. Results are evaluated on 30000 tokens. The intermediate size of the FFN is set to be 4 times the FFN width. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "WxLVYZbIew/tmp/4ebe38186c63a6896b7a0080245e527bdaf0b34bfb62f1a48e569e65b42e81e9.jpg", "table_caption": ["Table 2: Training time of Transformer-xl and structured counterparts with $32\\%$ and $63\\%$ FFN parameters. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Large number of tokens Using large $T$ , the standard linear layers and our efficient structured parametrizations become computation-bound where FLOPs become a latency bottleneck [16]. This setting mainly concerns training, the prefill phase of inference, and extensive offline decoding. In Fig. 6, we evaluate the latency performance of structured and dense FFNs across different FFN widths with 30K tokens. With parameters and FLOPs reduced to $63\\%$ or $32\\%$ , the lowrank and BlockDense achieve a $1.4\\times$ or a $2.5\\times$ speed-up, respectively. BlockShuffle offers modest improvements, with $1.1\\times$ and $2.0\\times$ speed-ups for the two cases. We also measure the training time of the whole model in Table 2, and observe that LowRank with $63\\%$ FFN parameters reduces the training time by about $15\\%$ with 0.4 increased perplexity, and the one with $32\\%$ FFN parameters offers $1.35\\times$ whole training speed-up with 1.1 increased perplexity. ", "page_idx": 8}, {"type": "image", "img_path": "WxLVYZbIew/tmp/f5b5194be6dea2bb90910cdbd8fce5c8cfff120ae80c1554e26782ba2baddaa3.jpg", "img_caption": ["Figure 7: Latency over different batch size for different widths: Decoding latency results between dense FFN and structured matrices with $32\\%$ FFN parameters across different widths and batch sizes. Note that we have a sequence length of 1 at the decoding phase; thus, $T$ equals batch size. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Small number of tokens FFN can be parallelism-bound with small $T$ (e.g., $T=16$ ) on the A100 GPUs. Then, when $T$ gets increased, FFN becomes memory-bound and will eventually be computation-bound. Online and offline decoding stages may encounter a small number of tokens when unrolling the model step by step. As discussed earlier, our pre-merge method can alleviate the parallelism-bound issue and maintain the same latency with dense matrices. Fig. 7 shows the latency results for three different widths, varying the batch of tokens to determine when to use efficient alternatives or choose pre-merged dense matrices. For example, with a 2048-width FFN, it is difficult to fully utilize resources on GPU with limited tokens. The performance improves significantly when using width 5120 and 6144, such as speed improvements of $2.63\\times$ speed-up of LowRank with $32\\%$ FFN parameters on $T\\!=\\!2048$ and $2.81\\times$ acceleration of BlockDense with $32\\%$ parameters on $T\\!=\\!1536$ . ", "page_idx": 8}, {"type": "text", "text": "4.4 Self-guided training ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We apply self-guided training during the first half of training to demonstrate its effectiveness. As shown in Table 3 and Table 9, our method consistently reduces loss across all efficient parametrizations, improving the perplexity by 1.2 for Transformer-s and 0.8 for Transformer-m. Then, to enable a straightforward comparison under the same training FLOPs, we adjust the training steps for self-guided training and repeat those tokens at the end to ensure they\u2019re fully learned by structured matrices. As can be seen in Table 9 and Fig. 14a, Fig. 14b, Fig. 8, this reduces the perplexity gap for Transformer-xl from 1.0, 1.2, and 1.3 to 0.4, 0.5, and 0.6 for LowRank, BlockDense, and BlockShuffle, respectively, under the same training FLOPs and can still enjoy $32\\%$ model FLOPs, which can bring about $2.6\\times$ inference speed-up. Additionally, we compare our method with another advanced baseline that trains structured matrices with more tokens, showing that the self-guided training can achieve comparable or superior results even with the same number of tokens. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "table", "img_path": "WxLVYZbIew/tmp/45d314a9150f6bac8253ca6177b2d941a343e09c4b69b89b0f2d7d9750b897ea.jpg", "table_caption": ["Table 3: Performance of the three structured parameterizations when applying self-guided training \u2663in the first half of training. This increases $25\\%$ FFN training FLOPs. For more comparisons, please refer to Sec. B.3. "], "table_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "WxLVYZbIew/tmp/e1890e50b778e3db53f90e5e4e8996c6d35ed7b23e5ece122ee2da50a0564e1a.jpg", "img_caption": ["Figure 8: Comparisons between dense and structured FFNs with $32\\%$ parameters under the same training FLOPs. Structured FFNs are trained either with more tokens or through self-guided training to match training FLOPs. The circle size represents model FLOPs. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we conducted extensive experiments investigating the use of structured matrices to parameterize FFNs in Transformers, with models scaling up to 1.3B parameters on the RefinedWeb dataset. Our primary aim was not to determine which structured matrix performs best, as this can be task-dependent, but to explore their common properties, including scaling, efficiency, and optimization challenges. We found that all of them exhibit steeper scaling curves compared to dense models. Moreover, our proposed methods, such as self-guided training, can enhance the performance across all structured matrices (e.g., LowRank with the novel training strategy achieves a $1.35\\times$ inference speed-up with only a 0.4 increase in perplexity). To conclude, we demonstrate that structured matrices can be strong candidates to replace the dense models in architecture design by scaling studies and also reveal the challenges of applying them. ", "page_idx": 9}, {"type": "text", "text": "Limitations: BlockDense and BlockShuffle are more complicated than LowRank. In this work, we only explored a limited range of hyperparameter settings of them. However, since these approaches are new, we believe that further performance improvements may be possible by better tuning their hyperparameters. We primarily focused on language modeling with limited vision experiments included in the appendix. Additionally, we did not explore the optimal scaling laws for structured matrices, which may further enhance performance. We also didn\u2019t investigate models in this paper that are comparable to today\u2019s practical LLMs, such as LLaMA-3. This is not only because of the limited computing resources but also because this study is to start investigating structured parameterizations of linear layers in modern LLM architecture training. We hope our findings and solutions about scaling, efficiency, and optimization will push their usage on the industry side and in future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We are grateful to Soham De for the insightful discussions and his valuable feedback on our work. We also sincerely thank the anonymous reviewers for their meaningful reviews and suggestions to make this better. We are also grateful to the RCP and IC cluster system administrators at EPFL for their support and assistance, especially during the project deadline. We also thank nimble.ai for their generous gift to CLAIRE lab, which also helped us to fund some of this research. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. ", "page_idx": 9}, {"type": "text", "text": "[2] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebr\u00f3n, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. [3] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [4] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[6] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[7] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022. [8] Meta. Llama 3. https://llama.meta.com/llama3/, 2024. [9] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi\u00e8re, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024.   \n[10] Pratyusha Sharma, Jordan T Ash, and Dipendra Misra. The truth is in there: Improving reasoning in language models with layer-selective rank reduction. arXiv preprint arXiv:2312.13558, 2023.   \n[11] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \n[12] Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization. Advances in Neural Information Processing Systems, 32, 2019.   \n[13] Tri Dao, Beidi Chen, Nimit S Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, and Christopher R\u00e9. Monarch: Expressive structured matrices for efficient and accurate training. In International Conference on Machine Learning, pages 4690\u20134721. PMLR, 2022.   \n[14] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient convolutional neural network for mobile devices. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6848\u20136856, 2018.   \n[15] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4510\u20134520, 2018.   \n[16] Samuel Williams, Andrew Waterman, and David Patterson. Roofline: an insightful visual performance model for multicore architectures. Communications of the ACM, 52(4):65\u201376, 2009.   \n[17] Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Hanyu Dong, and Yu Wang. Flashdecoding++: Faster large language model inference on gpus. arXiv preprint arXiv:2311.01282, 2023.   \n[18] Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. CoRR, abs/1312.6120, 2013. URL https://api.semanticscholar.org/CorpusID:17272965.   \n[19] Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from examples without local minima. Neural networks, 2(1):53\u201358, 1989.   \n[20] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In ICLR, 2019.   \n[21] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.   \n[22] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33: 17283\u201317297, 2020.   \n[23] Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019.   \n[24] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611\u2013626, 2023.   \n[25] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344\u201316359, 2022.   \n[26] Tri Dao, Daniel Haziza, Francisco Massa, and Grigory Sizov. Flash-deociding, 2023. URL https://crfm.stanford.edu/2023/10/12/flashdecoding.html.   \n[27] James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. Fnet: Mixing tokens with fourier transforms. arXiv preprint arXiv:2105.03824, 2021.   \n[28] Krzysztof Choromanski, Han Lin, Haoxian Chen, Tianyi Zhang, Arijit Sehanobish, Valerii Likhosherstov, Jack Parker-Holder, Tamas Sarlos, Adrian Weller, and Thomas Weingarten. From block-toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked transformers. In International Conference on Machine Learning, pages 3962\u20133983. PMLR, 2022.   \n[29] Shengjie Luo, Shanda Li, Tianle Cai, Di He, Dinglan Peng, Shuxin Zheng, Guolin Ke, Liwei Wang, and Tie-Yan Liu. Stable, fast and accurate: Kernelized attention with relative positional encoding. Advances in Neural Information Processing Systems, 34:22795\u201322807, 2021.   \n[30] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.   \n[31] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):5232\u20135270, 2022.   \n[32] Peter Belcak and Roger Wattenhofer. Fast feedforward networks. arXiv preprint arXiv:2308.14711, 2023.   \n[33] Hong Liu, Zhiyuan Li, David Hall, Percy Liang, and Tengyu Ma. Sophia: A scalable stochastic second-order optimizer for language model pre-training. arXiv preprint arXiv:2305.14342, 2023.   \n[34] Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. Galore: Memory-efficient llm training by gradient low-rank projection. arXiv preprint arXiv:2403.03507, 2024.   \n[35] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared llama: Accelerating language model pre-training via structured pruning. arXiv preprint arXiv:2310.06694, 2023.   \n[36] Yu Cheng, Felix X Yu, Rogerio S Feris, Sanjiv Kumar, Alok Choudhary, and Shi-Fu Chang. An exploration of parameter redundancy in deep networks with circulant projections. In Proceedings of the IEEE international conference on computer vision, pages 2857\u20132865, 2015.   \n[37] Misha Denil, Babak Shakibi, Laurent Dinh, Marc\u2019Aurelio Ranzato, and Nando De Freitas. Predicting parameters in deep learning. Advances in neural information processing systems, 26, 2013.   \n[38] Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, et al. Convolutional neural networks with low-rank regularization. arXiv preprint arXiv:1511.06067, 2015.   \n[39] Huanrui Yang, Minxue Tang, Wei Wen, Feng Yan, Daniel Hu, Ang Li, Hai Li, and Yiran Chen. Learning low-rank deep neural networks via singular vector orthogonality regularization and singular value sparsification. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 678\u2013679, 2020.   \n[40] Yuhui Xu, Yuxi Li, Shuai Zhang, Wei Wen, Botao Wang, Yingyong Qi, Yiran Chen, Weiyao Lin, and Hongkai Xiong. Trp: Trained rank pruning for efficient deep neural networks. arXiv preprint arXiv:2004.14566, 2020.   \n[41] Mikhail Khodak, Neil Tenenholtz, Lester Mackey, and Nicolo Fusi. Initialization and regularization of factorized neural layers. arXiv preprint arXiv:2105.01029, 2021.   \n[42] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.   \n[43] Marcin Moczulski, Misha Denil, Jeremy Appleyard, and Nando de Freitas. Acdc: A structured efficient linear layer. arXiv preprint arXiv:1511.05946, 2015.   \n[44] Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher R\u00e9. Learning fast algorithms for linear transforms using butterfly factorizations. In International conference on machine learning, pages 1517\u20131527. PMLR, 2019.   \n[45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[46] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.   \n[47] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[48] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023.   \n[49] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.   \n[50] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.   \n[51] Maximilian Beck, Korbinian P\u00f6ppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, G\u00fcnter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. xlstm: Extended long short-term memory. arXiv preprint arXiv:2405.04517, 2024.   \n[52] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023.   \n[53] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024.   \n[54] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. Efficient large-scale language model training on gpu clusters using megatron-lm. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1\u201315, 2021.   \n[55] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448\u2013456. pmlr, 2015.   \n[56] NVIDIA. Transformerengine. https://github.com/NVIDIA/TransformerEngine, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Structured matrices ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Design choice ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For BlockDense, we also investigate the reverse order of two projections, placing the low-rank or dense matrix first, followed by the block-diagonal matrix. However, this change surprisingly yields worse performance. For instance, on the RefinedWeb dataset, perplexity increases from 29.17 to 29.65 with Transformer-s and 2.2B training tokens. In the case of BlockShuffle, unlike Dao et al. [13], Zhang et al. [14] does not include a second shuffle operation to restore the original order. Taking this into account, we also experimented with removing the second shuffle operation and found almost no impact on performance. For example, with Transformer-s and -m using $32\\%$ FFN parameters, BlockShuffle without the second shuffle achieves perplexities of 29.89 and 21.19, respectively, compared to 29.95 and 21.12 for our adopted version. Nonetheless, we maintain the design from Dao et al. [13] for consistency. ", "page_idx": 14}, {"type": "text", "text": "For initialization, we follow the spectral initialization for LowRank, as suggested by prior works [41]. For BlockDense and BlockShuffle, motivated by Eq. (4), we propose using orthonormal initialization, setting the singular values of $U_{t}U_{t}^{\\top}$ and $\\bar{V_{t}}\\bar{V_{t}}^{\\top}$ to 1 at the start. Experimentally, this stabilizes training dynamics and improves the perplexity performance (Table 4). For weight decay, we tried the Frobenius decay proposed by Khodak et al. [41]; however, it did not have a clear benefti in our experiments and increased training FLOPs slightly. Hence, we adopted standard weight decay for all the structured FFNs. ", "page_idx": 14}, {"type": "table", "img_path": "WxLVYZbIew/tmp/648d3c95440401acbbbffa06aba545e7b7db78aa222a2b37753d53382f32ff06.jpg", "table_caption": ["Table 4: Different initialization of BlockShuffle and BlockDense, where random indicates random Gaussian initialization and orthonormal indicates orthonormal initialization. Data points are measured on the 4-layer Transformer and WikiText-103 with a learning rate of 1.0e-3. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "WxLVYZbIew/tmp/7f14c87d1ff9d0e0f59c39c09acd17c2e8bfd2c84bc69ac613c2108fdf0dd7a3.jpg", "table_caption": ["Table5: Ablationstudyofself-guidedtraining on LowRank trained on RefinedWeb. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.2 Results on Refinedweb dataset ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For the scaling points in Fig. 4, we provide detailed results in Table 6 for easier comparison. First, all efficient parameterizations approach the baseline as model size increases. For instance, LowRank with $32\\%$ of the parameters has a loss gap of 0.08 to Transformer-xl, whereas the gap is 0.12 at the scale of Transformer-s. Moreover, LowRank and BlockDense, with $63\\%$ of the parameters in the FFN, increase the loss by only 0.02 to 0.04 while reducing total training time by approximately $15\\%$ on Transformer-xl and Transformer-l. Additionally, they accelerate training by $1.35\\times$ with only a 1-point increase in perplexity on Transformer-xl with $32\\%$ of the parameters. ", "page_idx": 14}, {"type": "text", "text": "Although the main focus of the paper is not to compare different structured matrices but to showcase their general properties\u2014including scaling, efficiency, and optimization\u2014we still provide comparisons in the appendix. From Table 6, by controlling the training FLOPs and model size to be the same, LowRank and BlockDense demonstrate better performance than BlockShuffle in our main experiments, showing a 0.8 lower perplexity on Transformer-s and a 0.4 lower perplexity on Transformer-m. We think that for FFNs in language models, BlockShuffle may not be the optimal choice. However, we further compare these approaches on CIFAR-10 dataset, showing that block-diagonal matrices can serve as a good inductive bias in vision tasks Sec. A.3. ", "page_idx": 14}, {"type": "text", "text": "A.3 Results on CIFAR-10 dataset ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Although in our main experiments, the BlockShuffle performs the worst with two block-diagonal matrices, we provide experiments on the CIFAR10 dataset here, showing that when locality is highly preferred, block-diagonal matrices may perform better than low-rank matrices. ", "page_idx": 15}, {"type": "text", "text": "In Table 7, experiments are conducted on 5-layer MLP (MultiLayer Perceptron) and ViT models. The 5-layer MLP consists of a linear layer, batch normalization [55], and the ReLU activation function with a hidden dimension of 384. It is trained for 500 epochs with a learning rate of $1.0\\mathrm{e}{-3}$ and a batch size of 128. For the ViT models, 12 layers with a hidden dimension of 384 are used, and they are trained for 750 epochs with a learning rate of 6.0e-4 and a batch size of 512. ", "page_idx": 15}, {"type": "text", "text": "Since the first layer in vision tasks typically prefers from the locality, especially in a 5-layer MLP where the image pixels are directly flattened into the input, we conducted experiments with and without structuring the first layer as LowRank and BlockShuffle. Both sets of controls, particularly when structuring the first linear layer, demonstrate that block-diagonal matrices can be beneficial for vision tasks. Specifically, replacing the first layer of the 5-layer MLP model with a block-diagonal matrix even yields better performance, as the block structure effectively groups neighboring pixels, compensating for the MLP\u2019s lack of locality. However, applying structured FFNs to the first layer of ViT can lead to significant accuracy degradation, reinforcing our decision not to use structured FFNs in the first layer in the main experiments. ", "page_idx": 15}, {"type": "text", "text": "Table 6: Performance of two sizes of different structured matrices with $63\\%$ and $32\\%$ of the original FFN module\u2019s parameters. We report model size, total FFN size, training tokens, training FLOPs, and training time. Note that the total structured FFN is not exactly $63\\%$ of the original because we don\u2019t replace the first FFN module. Also, the BlockDense is slightly smaller for -m and -xl models to ensure the rank is a multiple of 256 when matching parameters. Loss and perplexity are evaluated on a $0.5\\mathrm{B}$ token validation set. ", "page_idx": 15}, {"type": "table", "img_path": "WxLVYZbIew/tmp/fb965f4daeeb5828fc03004aebfd1b814f397cb5b8507464a1ec84d5af5d89a6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "WxLVYZbIew/tmp/26ff5ad75d96ace35097f1bab1a73382693bf845bd33a9ae6f04701fff3ad0b7.jpg", "table_caption": ["Table 7: Experiments on CIFAR10 and vision models, where the locality is highly preferred. The first layer column in the table indicates whether to apply structured matrices to the first FFN. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.4 Results on downstream tasks ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Table 8: Performance on downstream tasks under the zero-shot setting. We report the perplexity performance of the validation set of RefinedWeb. For all downstream tasks except LAMBADA, we report accuracy results. For LAMBADA, we present the results in an accuracy/perplexity format. Implementation details are put in Appendix C. ", "page_idx": 16}, {"type": "table", "img_path": "WxLVYZbIew/tmp/2613c1aeff2976c562d87786ce850a6ec78e61056f5c6dfcde52da44fc2bdb6c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Supplementary to Fig. 5, Table 8 presents the detailed results of wide and structured networks compared to dense models on downstream tasks. These models were trained on 300B tokens and implementation details can be found in Appendix C. ", "page_idx": 16}, {"type": "text", "text": "B Self-guided training ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Visualization of training dynamics ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We provide additional training dynamics curves, including BlockDense and BlockShuffle, in Fig. 10 and Fig. 11, illustrating that these structures are more challenging to train compared to standard linear layers. Specifically, they exhibit greater sensitivity to learning rates and are more prone to loss spikes. To mitigate this, we apply self-guided training in the most challenging cases, which results in improved training dynamics, such as faster convergence without loss spikes. ", "page_idx": 16}, {"type": "text", "text": "Furthermore, we report the loss spikes observed in the large Transformer-xl model trained on the RefinedWeb dataset in Fig. 12. ", "page_idx": 16}, {"type": "text", "text": "B.2 Explanation of the poor training dynamics ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We observed that loss spikes occur along with large gradient norms. This motivates us to analyze the gradient updates during the backward pass of the linear $U V$ . Considering $\\textbf{\\textit{g}}$ as the gradient of output and $\\textbf{\\em x}$ as the input, the standard linear layer $W$ gradient update is $g x^{\\intercal}$ . For the structured parametrization, the gradients of $U$ and $V$ are $\\dot{g}x^{\\top}V^{\\top}$ and $U^{\\top}\\mathring{g}x^{\\top}$ , respectively. Then, for $W^{\\prime}$ as being the updated parameters, we will have the updates for $W$ to be: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Delta\\pmb{W}\\!=\\!\\pmb{W}^{\\prime}\\!-\\!\\pmb{W}\\!=\\!-l\\pmb{r}\\!\\cdot\\!\\pmb{g}\\pmb{x}^{\\top}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "And the one for $U V$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Delta(U V)\\!=\\!-l r\\!\\cdot\\!(U U^{\\top}g x^{\\top}\\!+\\!g x^{\\top}V^{\\top}V)\\!+\\!O(l r^{2})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, it can be seen from Eq. (4) that the projections $U U^{\\top}$ and $V^{\\top}V$ can disrupt the gradient $g x^{\\top}$ . If the norms of $U$ and $V$ are small, the new update vanishes faster than the original update, and in reverse, if their norm is large, the update blows up, leading to unstable training. ", "page_idx": 17}, {"type": "text", "text": "To be specific, we calculate the spectral norm of $U U^{\\top}$ and $V^{\\top}V$ and use this to indicate the maximum scale the matrix can stretch a vector. Fig. 13 shows that the largest singular value can vary significantly, being either much greater than or less than 1, depending on the shape of the weight (input dimension, rank, number of blocks) and magnitude. Interestingly, very structured FFNs with small ranks or many blocks tend to have smaller spectral norms, while others have larger ones. This corresponds to the phenomenon in Fig. 10 and Fig. 11, where smaller FFN are prone to slower convergence and larger ones to loss spikes. ", "page_idx": 17}, {"type": "text", "text": "An alternative intuitive perspective of this issue is to realize that learning with structured matrices has additional redundant degrees of freedom brought by symmetry. For example, to increase the norm of a feature, it can increase either the corresponding weights $U$ or $V$ . Given that gradient descent makes this decision independently, it will overshoot and make learning less well-behaved. ", "page_idx": 17}, {"type": "image", "img_path": "WxLVYZbIew/tmp/9410cdc44ca82d9a47bf2e6712226caa910a4062e781211a830710836791596e.jpg", "img_caption": ["Figure 9: (a-c): Training dynamics of LowRank with different ranks and the dense model under different hyperparameters. Data points are measured on a 4-layer Transformer with model width 768 and WikiText-103. We zoom into the beginning of training for clearer observations. (d-f): Training dynamics of LowRank and the self-guided training. The self-guided training overcomes the loss spikes and makes the training faster. We show the whole training curve to indicate its success. $R$ indicates the rank of low-rank matrices. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.3 Design choice ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Experiments in this part are conducted on Transformer-s and Transformer-m with ranks 192 and 256 in Table 5, respectively. We apply self-guided training during the first half of the training process. ", "page_idx": 17}, {"type": "text", "text": "First, we compare stochastic self-guided training with the static version. The stochastic and faster version in both model sizes brings about a 0.1 perplexity increase while reducing computation by half. Second, other techniques are compared. Dense layer decomposition, which decomposes the weight directly at the midpoint of training, is examined. This approach can lead to abrupt loss increases in training curves, resulting in worse performance. Strategies incrementally reducing rank require a feasible and complex change strategy and fail to address the inconsistent gradient update problem, thus still suffering from poor results Table 5. ", "page_idx": 17}, {"type": "text", "text": "Generally speaking, our method stands out due to its flexibility, simplicity, and efficiency. Eq. (1) makes it adaptable to any efficient linear parametrization without special constraints, while progressive rank reduction and direct decomposition require a feasible solution to evolve. Our guided initialization allows its usage in various stages of training without the need for a well-trained teacher. It is simple because Eq. (1) provides only one smooth transition. It is efficient due to the layer-specific definition in Eq. (1) and stochastic computation in Eq. (2). ", "page_idx": 17}, {"type": "image", "img_path": "WxLVYZbIew/tmp/ea06a63b4b9ab99c8ac428fd0b894f870575093648ab983f36dd12effec3faca.jpg", "img_caption": ["Figure 10: (a-c): Training dynamics of BlockDense with 2 blocks and different ranks and the dense model under different hyper-parameters. (d-f): For BlockDense $[B\\!=\\!2,\\!R\\!=\\!128]$ ), training dynamics of self-guided training indicated by \u2663. Other settings follow Fig. 9. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "WxLVYZbIew/tmp/9c8604c03ad50865983152207a1232cba4db0e764663f6f2be009e291e863108.jpg", "img_caption": ["Figure 11: (a-c): Training dynamics of BlockShuffle with different numbers of blocks and the dense model under various hyper-parameters. (d-f): For BlockShuffle ( $[B\\!=\\!8)$ ), loss curves of self-guided training indicated by \u2663. Other settings follow Fig. 9. Note that the training dynamics of different structured matrices are not comparable here because their sizes are not controlled to be the same. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "B.4 Results on Refinedweb Dataset ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Table 9 provides more comprehensive results of self-guided training, supplementing Sec. 4.4. For Transformer-s and -m, we first present the results of applying self-guided training to the first half of training in the second row of each table block, demonstrating its effectiveness across all three structured matrices. Additionally, by controlling for equal training FLOPs as described in Sec. 4.4, ", "page_idx": 18}, {"type": "image", "img_path": "WxLVYZbIew/tmp/ad81099c50208a211e9bd4df0af7ea786f94f656db5d52515cecbd27470d84c5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 12: Loss curves of Transformer-xl. Structured FFN with $32\\%$ parameters exhibits slower convergence.   \nAlso, there exist loss spikes. ", "page_idx": 19}, {"type": "image", "img_path": "WxLVYZbIew/tmp/a8a896ff4542fbfd78306d38362d9f5ab8a3307e7bb4fb0e91a8cc3899b41929.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 13: Spectral norm of the matrix $V^{\\top}V$ , where in (a-c), $\\boldsymbol{V}$ is the low-rank matrix and in (d-f), $\\boldsymbol{V}$ is the block-diagonal matrix. N presents the input dimension of the weight. Std indicates the standard deviation value of the normal distribution from which we sample weight elements. ", "page_idx": 19}, {"type": "text", "text": "we show that with self-guided training, all three structured FFNs with $32\\%$ parameters incur only a 0.4-0.6 increase in perplexity compared to the baseline, while benefiting from a smaller memory footprint and faster speed at inference time. To further illustrate this, we plot these points with the same training FLOPs in Fig. 14, and specifically for LowRank in Fig. 8, highlighting that self-guided training achieves comparable performance to training on more tokens. ", "page_idx": 19}, {"type": "image", "img_path": "WxLVYZbIew/tmp/c2c4189f7b5eb1158c0e75f62a01bfba495fa8e5b779b4c6d67daeffcfe5f308.jpg", "img_caption": ["Figure14: PerformancebetweendenseTransformer, StructuredFFN(BlockDenseandBlockShuffle, LowRank in Fig. 8) with $32\\%$ parameters either trained with self-guided training or more tokens across four sizes. Circle size indicates model FLOPs. To enable straightforward comparison, we controlled their training FLOPs to be the same. "], "img_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "WxLVYZbIew/tmp/fe9db40554df459a74b3e70ad15ebb98e5eeedef19c9a2993004040f775e88b6.jpg", "table_caption": ["Table 9: Performance of self-guided training indicated by $\\clubsuit$ on Structured FFN with $32\\%$ parameters under the same training FLOPs. We also include the structured FFN trained on more different tokens as a highly advanced baseline. Model FLOPs are calculated on one sample with 1024 sequence length. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "C Implementation details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Model Architecture details are provided in Table 10. We consider four baseline Transformer sizes, ranging from 110M to 1.3B parameters, with widths from 768 to 2048. For structured models, we first adopt two configurations as described in the main paper, reducing the FFN module to $63\\%$ and $32\\%$ of its original parameters by adjusting the rank $R$ and number of blocks $B$ . Only $R$ and $B$ values specifically associated with structured matrices are modified, as seen in Table 6. Then, for more comparable results, we consider wide and structured networks, where the attention module is also structured by reducing the attention heads. We also present the transformer with GQA version [2] here, configured with 256 dimensions for the KVCache [9] and an enlarged FFN intermediate dimension following [8]. Based on this GQA version, we apply LowRank matrices to the FFN module with a rank half of the model or FFN width and use a smaller attention inner dimension to further reduce the parameters of the attention module. This allows us to maintain the parameter ratio between the attention and FFN modules. ", "page_idx": 21}, {"type": "text", "text": "Note that in all experiments, we do not apply structured matrices to the first FFN module, as doing so can lead to non-negligible performance loss in models on shallow networks. For dense models, we use Gaussian random weight initialization with a standard deviation of 0.02. For structured matrices, spectral initialization is applied for LowRank, and orthonormal initialization for the other two, based on initial experiments. ", "page_idx": 21}, {"type": "text", "text": "Table 10: Detailed configurations of the baseline Transformers, along with those using GQA [2] and wide and structured networks. The latter two are employed in the scaling study in Sec. 4.2. For other structured models which have $63\\%$ and $32\\%$ of the original FFN parameters, we adjust only the rank and number of blocks for each method and put the configuration directly in Table 6. Width denotes the model width or the input and output dimensions of the attention and FFN modules. Intermediate dim. refers to the intermediate dimension of the FFN. Attention dim. specifies the dimension used in scaled-dot product attention. KV dim. represents the dimension used for KVCache, as selected according to Team et al. [9]. ", "page_idx": 21}, {"type": "table", "img_path": "WxLVYZbIew/tmp/a70d9b098af88175013ac4bbd55813bef5037bb30ad37f9e3f0f14c15c24b080.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 11: Basic training configuration used in all experiments except for the overtraining regime with 300B tokens. Note that we apply the same global batch size (Batch) and the same peak learning rates (LR) to both dense and structured models to avoid hyperparameter search. The hyperparameter values are selected based on Zhang et al. [50], Gu and Dao [49]. ", "page_idx": 22}, {"type": "table", "img_path": "WxLVYZbIew/tmp/7106cad0447c9bfd32b75a181076bdc72d478d65dedf63ead068934e79f57a4e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 12: Training configuration for 300B token training. Different studies [49, 52, 47, 51] employ very different learning rates in this setting, which also differ from training-compute scaling studies [1]. To avoid extensive tuning, we follow the hyperparameter scaling rule of Transformer proposed by Bi et al. [53], determining batch size and learning rate based on training FLOPs. ", "page_idx": 22}, {"type": "table", "img_path": "WxLVYZbIew/tmp/406e4f13e0aca70a3e6dbb145801e2a55343ce3385e3f4d0bd384c4b6489b43a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Training We use A100 80G GPUs for training and evaluation, employing mixed precision (bfloat16 and float32) with torch.cuda.amp to accelerate training. Training FLOPs are calculated following Megatron [54], including all matrix multiplications. ", "page_idx": 22}, {"type": "text", "text": "Different hyperparameters are used for 300B token training and other experiments. For basic training in training FLOPs scaling and self-guided training studies, configurations are listed in Table 11. Hyperparameter values are selected based on the scaling law studies of Zhang et al. [50], Gu and Dao [49], where we use the same learning rates and global batch size for both dense and structured models. Additional details include the AdamW optimizer with 0.1 weight decay, betas of [0.9, 0.999], cosine annealing learning rate scheduler with $10\\%$ linear warm-up, and $0.1\\times$ minimum value. Dropout is set to 0.0, and gradient clipping to 1.0. ", "page_idx": 22}, {"type": "text", "text": "In the overtraining regime where the training duration is super long, however, smaller betas [0.9, 0.98] are required for stable training, even for baseline Transformers. Previous studies [49, 52, 47, 51] adopt very different learning rates in this setting, differing from training-compute scaling studies [1]. To avoid extensive searching, we follow the hyperparameter scaling rule of Transformer proposed by Bi et al. [53], determining the global batch size and learning rate based on training FLOPs. Specifically, batch size is defined by $0.3118\\times\\mathrm{(training~FLOPs^{-0.125})}$ , and learning rate by $\\mathrm{0.2920\\times(\\dot{\\bftraining}~F L O P s^{0.3271})}$ , giving the results in Table 12. It can be seen that our wide and structured models trained on 300B tokens will use a slightly higher learning rate and smaller batch size compared to the larger Transformer. ", "page_idx": 22}, {"type": "text", "text": "Efficiency To enhance training and inference efficiency, our code is based on PyTorch but incorporates optimized CUDA kernels. We leverage Flash Attention [25], fast LayerNorm, and rotary embeddings from TransformerEngine [56], along with fused operations including bias and GeLU. For inference speed testing, we use bfloat16. These techniques are consistently applied to all models to ensure fair latency and throughput comparisons. ", "page_idx": 22}, {"type": "text", "text": "D Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Enhancing the efficiency of Large Language Models (LLMs) can significantly reduce computational resources and energy consumption, benefiting the environment and democratizing access to advanced AI technologies. However, increased efficiency could also lead to greater dissemination of disinformation and the creation of deepfakes, posing risks to public trust and security and potentially reinforcing existing biases that impact specific groups unfairly. This research aims to promote the responsible development and deployment of LLMs, maximizing societal beneftis while acknowledging potential harms. ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We describe our claims and contributions in Section 1. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We discuss limitations in Section 5. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not include theoretical results. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We describe our experimental setup and implementation details in Section 4 Appendix C. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We open-source our code. The dataset we use is publicly available. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We describe our experimental setup in Section 4 and give details in Appendix C. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: Training large language models requires significant time and compute, making it infeasible to run multiple seeds. Nonetheless, results from single training runs are highly reliable and transferrable, as demonstrated by neural scaling laws. Presenting results without error bars in LLM research is standard practice. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Computing resources are discussed throughout the paper. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The research conducted in the paper conforms with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: Societal impacts are discussed in Appendix D. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. \u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not release data or models. not pose such risks ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: All the assets used have been cited. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 27}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide documented code. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]