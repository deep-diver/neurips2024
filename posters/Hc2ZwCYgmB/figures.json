[{"figure_path": "Hc2ZwCYgmB/figures/figures_0_1.jpg", "caption": "Figure 1: Although AdaFace is solely trained on static images, the subject embeddings it generates can directly condition AnimateDiff to produce personalized videos across diverse scenes without requiring any modifications.", "description": "This figure demonstrates the capability of AdaFace.  Despite being trained only on still images, the embeddings AdaFace generates for faces can be used directly with AnimateDiff (a video generation model) to create personalized videos.  The videos show the same person across several different and varied backgrounds and scenarios, highlighting AdaFace's versatility and ability to seamlessly integrate into existing video generation pipelines.", "section": "Abstract"}, {"figure_path": "Hc2ZwCYgmB/figures/figures_1_1.jpg", "caption": "Figure 2: A typical zero-shot face encoder pipeline for diffusion models. First, a Face2Vec module (e.g., ArcFace [Deng et al., 2019]) extracts a single vector that captures the facial features. Then a trainable Face2Image encoder (e.g., Arc2Face [Papantoniou et al., 2024]) maps it to n facial tokens V1,..., Vn within the image embedding spaces. The facial tokens condition the U-Net (either original or fine-tuned) to generate authentic-looking subject images. However, since the facial tokens is not blended with other text prompts (sometimes they are simply concatenated), the whole pipeline has weaker compositionality than using text prompts alone. Moreover, such models are often incompatible with existing diffusion pipelines, such as AnimateDiff Guo et al. [2024a].", "description": "This figure illustrates a common zero-shot face encoder pipeline used with diffusion models.  It shows how a face image is processed through a Face2Vec module to extract facial features, then through a Face2Image encoder to generate facial tokens within the image embedding space. These tokens condition a U-Net to generate new images of the subject.  The figure highlights a limitation: the facial tokens aren't easily combined with text prompts, resulting in weaker compositionality compared to using text prompts alone, and incompatibility with some diffusion pipelines.", "section": "1 Introduction"}, {"figure_path": "Hc2ZwCYgmB/figures/figures_2_1.jpg", "caption": "Figure 3: The core of AdaFace is the Prompt Inverter, which inverts the image-space ID embeddings from another model to the text prompt space, represented as w\u2081,\u00b7\u00b7\u00b7, Wn. These embeddings are integrated into a standard text prompt and encoded by a CLIP prompt encoder. CLIP coherently composes the semantics of the ID embeddings and the text prompt, providing good compositionality.", "description": "This figure illustrates the core component of AdaFace, the Prompt Inverter.  It shows how image-space ID embeddings (from a pre-trained Face2Image model) are converted into the text prompt space.  These text-space embeddings (w\u2081, ..., w\u2099) are then seamlessly integrated with a standard text prompt. The combined prompt is then encoded by a CLIP prompt encoder, which effectively blends the semantics of the face embeddings and the text prompt, resulting in improved compositionality when generating images.", "section": "2 Method"}, {"figure_path": "Hc2ZwCYgmB/figures/figures_3_1.jpg", "caption": "Figure 4: Face distillation on face images. The output of the AdaFace stream is compared with the Face2Image stream. During this process, only the AdaFace Prompt Inverter is optimized.", "description": "This figure illustrates the face distillation process in AdaFace.  It shows the AdaFace Prompt Inverter taking the output of a Face2Vec and Face2Image Encoder and converting it into a text prompt space.  This text prompt, along with a subject-ID prompt, conditions a custom U-Net to generate an image. This generated image is compared to the image generated by the original Face2Image pipeline using a Face Distillation Loss.  Only the AdaFace Prompt Inverter is optimized during this stage of training. The goal is to enable the inverter to generate images that closely match those produced by the original Face2Image model, thus learning to create realistic and authentic face representations in the text prompt space.", "section": "2.2 Face Distillation"}, {"figure_path": "Hc2ZwCYgmB/figures/figures_4_1.jpg", "caption": "Figure 5: Composition distillation on four types of prompts: subject-single, subject-composition, class-single and class-composition. The four generated images form two contrastive pairs, and their feature deltas are encouraged to be aligned through a composition distillation loss.", "description": "This figure illustrates the process of composition distillation in AdaFace.  Four types of prompts are used: subject-single (only the subject), subject-composition (subject and scene), class-single (only the class of subject), and class-composition (class and scene).  The generated images from these prompts are paired (subject-single with subject-composition, and class-single with class-composition).  The differences (deltas) in feature maps between the image pairs are compared and encouraged to be similar, using a composition distillation loss to improve the model's ability to integrate a subject into various scenes while maintaining the subject's likeness.", "section": "2.3 Composition Distillation"}, {"figure_path": "Hc2ZwCYgmB/figures/figures_6_1.jpg", "caption": "Figure 6: To prevent subject features from degeneration due to spatial misalignment during composition distillation, we propose a Elastic Face Preserving Loss. The second row shows the cross-attention maps at selected four points on the subject-single image. The highlighted pixels associate the corresponding facial areas across the two images. The features of matching pixels are required to be close to each other to achieve subject feature preservation.", "description": "This figure illustrates the Elastic Face Preserving Loss, a method to mitigate subject feature degeneration caused by spatial misalignment during composition distillation.  It shows cross-attention maps highlighting corresponding facial features between a subject-single image and a subject-composition image, ensuring consistent feature preservation despite spatial differences.", "section": "2.4 Elastic Face Preserving Loss"}, {"figure_path": "Hc2ZwCYgmB/figures/figures_7_1.jpg", "caption": "Figure 7: Qualitative comparison of AdaFace with state-of-the-art face encoders. AdaFace generates images that maintain the highest authenticity of the subjects, while still follow the target prompts.", "description": "This figure compares the image generation results of AdaFace against three other state-of-the-art face encoders (InstantID, ConsistentID, and PuLID) for five different prompts.  Each row represents a different prompt used to generate images of five different celebrities.  The input image of each celebrity is shown in the first column. The remaining columns show the output of each face encoder.  The results demonstrate that AdaFace achieves the highest level of authenticity while still incorporating the descriptive prompt information into the generated image.", "section": "3.2 Qualitative Comparisons"}, {"figure_path": "Hc2ZwCYgmB/figures/figures_8_1.jpg", "caption": "Figure 8: Comparison of AdaFace with ID-Animator on personalized video generation. AdaFace generates videos with higher authenticity and compositionality.", "description": "This figure compares the video generation capabilities of AdaFace and ID-Animator.  It shows that AdaFace produces videos with improved subject likeness (authenticity) and better integration of the subject into the scene (compositionality) compared to ID-Animator.", "section": "3.3 Quantitative Evaluations"}]