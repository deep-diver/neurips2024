[{"type": "text", "text": "AdaFace \u2013 A Versatile Face Encoder for Zero-Shot Diffusion Model Personalization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "image", "img_path": "Hc2ZwCYgmB/tmp/5a6b7c1f50477f0e56842a8c6d1f099225de2738b51505fed63477ae4dc576a8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Figure 1: Although AdaFace is solely trained on static images, the subject embeddings it generates can directly condition AnimateDiff to produce personalized videos across diverse scenes without requiring any modifications. ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Since the advent of diffusion models, personalizing these models \u2013 conditioning them to render novel subjects \u2013 has been widely studied. Recently, several methods propose training a dedicated image encoder on a large variety of subject images. This encoder maps the images to identity embeddings (ID embeddings). During inference, these ID embeddings, combined with conventional prompts, condition a diffusion model to generate new images of the subject. However, such methods often face challenges in achieving a good balance between authenticity and compositionality \u2013 accurately capturing the subject\u2019s likeness while effectively integrating them into varied and complex scenes. A primary source for this issue is that the ID embeddings reside in the image token space (\u201cimage prompts\"), which is not fully composable with the text prompt encoded by the CLIP text encoder. In this work, we present AdaFace, an image encoder that maps human faces into the text prompt space. After being trained only on 400K face images with 2 GPUs, it achieves high authenticity of the generated subjects and high compositionality with various text prompts. In addition, as the ID embeddings are integrated in a normal text prompt, it is highly compatible with existing pipelines and can be used without modification to generate authentic videos. We showcase the generated images and videos of celebrities under various compositional prompts. The source code is released on an anonymous repository https://github.com/adaface-neurips/adaface. ", "page_idx": 0}, {"type": "image", "img_path": "Hc2ZwCYgmB/tmp/1d531bf7437f17a9813e8936950bfbc7c1e033571d2150f85c1f4b229ff2db21.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 2: A typical zero-shot face encoder pipeline for diffusion models. First, a Face2Vec module (e.g., ArcFace [Deng et al., 2019]) extracts a single vector that captures the facial features. Then a trainable Face2Image encoder (e.g., Arc2Face [Papantoniou et al., 2024]) maps it to $n$ facial tokens $v_{1},\\cdots,v_{n}$ within the image embedding spaces. The facial tokens condition the U-Net (either original or fine-tuned) to generate authentic-looking subject images. However, since the facial tokens is not blended with other text prompts (sometimes they are simply concatenated), the whole pipeline has weaker compositionality than using text prompts alone. Moreover, such models are often incompatible with existing diffusion pipelines, such as AnimateDiff Guo et al. [2024a]. ", "page_idx": 1}, {"type": "text", "text": "20 1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "21 Recent years have witnessed the blossom of diffusion models, which have been widely used in image   \n22 generation, image editing, and video generation [Ho et al., 2020, Nichol et al., 2022, Saharia et al.,   \n23 2022, Rombach et al., 2022, Podell et al., 2024, Chen et al., 2024a, Kawar et al., 2023, Peebles and   \n24 Xie, 2023, Guo et al., 2024a]. A particularly interesting application of these models is personalization,   \n25 where they are conditioned to generate images of specific subjects. Previously, this was primarily   \n26 achieved through test-time fine-tuning [Ruiz et al., 2022, Gal et al., 2022a, Kumari et al., 2022,   \nTewel et al., 2023], which introduced additional computational demands and complexity to the   \n28 image generation process. Recent advancements have seen the development of zero-shot, tuning-free   \n29 methods [Wei et al., 2023, Ye et al., 2023, Shi et al., 2023, Wang et al., 2024, Papantoniou et al.,   \n30 2024, Guo et al., 2024b, Huang et al., 2024, Han et al., 2024, Chen et al., 2024b, He et al., 2024].   \n31 These methods train a dedicated image encoder to convert subject images to identity embeddings   \n32 (ID embeddings) using a large dataset. During inference, these ID embeddings are combined with   \n33 standard text prompts to generate new images of the subject (Figure 2). Despite these innovations,   \n34 these approaches often struggle to strike a good balance between authenticity and compositionality.   \n35 Authenticity ensures the model captures the true likeness of the subject, whereas compositionality   \n36 concerns the model\u2019s ability to seamlessly integrate the subject into diverse and intricate scenes.   \n37 The challenge primarily stems from how ID embeddings are utilized: in many zero-shot methods,   \n38 the embeddings exist in the image token space (\u201cimage prompts\") and do not fully mesh with text   \n39 prompts. In cases like [Huang et al., 2024], while the ID embeddings are within the text space, there   \n40 lacks targeted training to enhance their integration with other text prompts, resulting in compromised   \n41 compositionality.   \n42 Given the limitations of existing methods, we propose AdaFace, a versatile face encoder that maps   \n43 human faces into the text prompt space. First, the ID embeddings generated by AdaFace seamlessly   \n44 integrate with text prompts via the CLIP text encoder, allowing for more coherent and expressive   \n45 conditioning. Second, we employ targeted training strategies to enhance the compositionality of the ID   \n46 embeddings, ensuring they are able to be used to generate diverse and complex scenes. Furthermore,   \n47 AdaFace is highly compatible with existing diffusion pipelines, requiring no modifications to generate   \n48 authentic videos, as demonstrated in Figure 1. Notably, due to efficient model design and distillation   \n49 techniques, AdaFace is trained on merely 406,567 face images with 2 RTX A6000 GPUs, all within a   \n50 constrained compute budget.   \n51 We demonstrate the effectiveness of AdaFace by showcasing the generated images and videos of   \n52 celebrities under various compositional prompts. We also perform quantitative evaluations to validate   \n53 that AdaFace achieves a good balance between authenticity and compositionality, measured by   \n54 ArcFace similarity and CLIP-Text similarity, respectively. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "Hc2ZwCYgmB/tmp/473026405747ba5205dfd2df3c837b7b15a23132b8e940aad5c83021e1a61772.jpg", "img_caption": ["Figure 3: The core of AdaFace is the Prompt Inverter, which inverts the image-space ID embeddings from another model to the text prompt space, represented as $w_{1},\\cdots,w_{n}$ . These embeddings are integrated into a standard text prompt and encoded by a CLIP prompt encoder. CLIP coherently composes the semantics of the ID embeddings and the text prompt, providing good compositionality. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "55 2 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "56 Motivated by the advantages of text space face prompts, we propose techniques to distill one or more   \n57 image-space face encoders into the text space, and further enhance its compositionality. The overall   \n58 architecture of AdaFace is shown in Figure 3. The core module of AdaFace is the AdaFace Prompt   \n59 Inverter, which inverts the image-space ID embeddings to the text space, enabling the integration   \n60 of the ID embeddings into a standard text prompt. The ID embeddings are then encoded by a CLIP   \n61 prompt encoder, which coherently composes the semantics of the ID embeddings and the text prompt.   \n62 The text-level composition also facilitates Composition Distillation (Figure 5), which significantly   \n63 improves the compositionality of the ID embeddings without additional training data. A side-effect   \n64 of composition distillation is that, when there is spatial misalignment between the subject-single   \n65 and subject-composition images, the subject features will be gradually contaminated by background   \n66 features, reducing their authenticity. Accordingly, we propose a Elastic Face Preserving Loss (Figure   \n67 6), to prevent the subject features from degeneration. ", "page_idx": 2}, {"type": "text", "text": "68 2.1 AdaFace Architecture ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "69 The core module of AdaFace is the AdaFace Prompt Inverter, which converts the image-space ID   \n70 embeddings from a Face2Image model to the text space.   \n71 The architecture and initialization of the prompt inverter significantly impacts the training efficiency.   \n72 Compared to other deep learning tasks, the diffusion training is highly stochastic and the gradients   \n73 have a much lower signal-to-noise ratio. It is highly challenging to train a sizable diffusion component   \n74 from scratch without high compute budgets and large batch sizes. To achieve efficient learning, we   \n75 adopt the same architecture as the CLIP text encoder for the AdaFace Prompt Inverter, and initialize   \n76 it with the pre-trained weights. This ensures that the output embeddings are not very distant from the   \n77 text space from the beginning of training, and the model learns more signals from the gradients.   \n78 One may raise the question that since the output of a pre-trained CLIP encoder is in the image space,   \n79 why it is able to adapt quickly to generate text-space embeddings? We speculate that in CLIP, the   \n80 semantics of low-level layers and high-level layers are not in totally incompatible spaces, but rather,   \n81 the high-level semantics enrich the low-level ones. Our hypothesis is corroborated by [Toker et al.,   \n82 2024], as well as the community practice of ad-hoc fusing the output embeddings of multiple CLIP   \n83 text encoder layers1. The semantics of layer features gradually transition from the text space to   \n84 the image space. As a result, during fine-tuning, the skip connections within CLIP will allow the   \n85 low-level semantics to take shortcut towards the output embeddings, and the high-level layers will   \n86 gradually learn to enrich the low-level semantics in the text space instead.   \n87 The training of the prompt inverter is divided into two stages. In the first face distillation stage, a   \n88 Face2Image model guides the prompt inverter to generate authentic faces in the text prompt space. In   \n89 the second composition distillation stage, the prompt inverter observes how the original model output   \n90 responds to compositional prompts, and learns to generate similar responses, so as to allow the text   \n91 prompts to control the composition of the generated images. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "image", "img_path": "Hc2ZwCYgmB/tmp/0cd31026e174ce220ca08f4186506902c801e5b2e3acef332a77816ab23e39bb.jpg", "img_caption": ["Figure 4: Face distillation on face images. The output of the AdaFace stream is compared with the Face2Image stream. During this process, only the AdaFace Prompt Inverter is optimized. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "92 2.2 Face Distillation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "93 The face distillation stage is illustrated in Figure 4, where the objective is to minimize the difference   \n94 between the generated images by the original Face2Image model and by the AdaFace Prompt Inverter   \n95 on the same initial noise. The training objective, namely the face distillation loss, is formulated as a   \n96 reconstruction loss between the two generated images: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{face}}=\\mathbb{E}_{f\\sim F,z\\sim N(0,I),t\\in[1,T]}\\left[\\left|\\left|G_{\\mathrm{AdaFace}}(f,z,t|\\theta)-G_{\\mathrm{Face2Image}}(f,z,t|\\theta^{\\prime})\\right|\\right|_{2}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "97 where $G_{\\mathrm{Face2Image}}$ and $G_{\\mathrm{AdaFace}}$ are the Face2Image and the AdaFace Prompt Inverter conditioned   \n98 U-Nets, respectively, $f$ is a random face drawn from the face space $F$ , $z$ is the initial noise, and $\\theta$ and   \n99 $\\theta^{\\prime}$ are the parameters of the AdaFace Prompt Inverter and the Face2Image model, respectively. For   \n100 some models such as Ada2Face, $\\theta^{\\prime}\\neq\\theta$ . ", "page_idx": 3}, {"type": "text", "text": "101 In order to sweep the input space $\\{f,z,t\\}$ as completely as possible, we adopt a few techniques: ", "page_idx": 3}, {"type": "text", "text": "102 Random Gaussian Face Embeddings. Empirically, we observe that almost all random face   \n103 embeddings result in legitimate face images when processed by the Face2Image model. Therefore,   \n104 we expand the candidate face space $F$ by including random face embeddings drawn from a Gaussian   \n105 distribution, alongside the face embeddings extracted from real face images: $F=F_{\\mathrm{real}}\\cup F_{\\mathrm{rand}}$ .   \n106 Multi-Timestep Distillation. We use multiple denoising steps on the same initial noise, and   \n107 compute the reconstruction loss on all the steps, so that the prompt inverter learns to imitate the   \n108 Face2Image model\u2019s behavior on intermediate noise levels: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{face}}=\\mathbb{E}_{f\\sim F,z_{1}\\sim N(0,I),t_{1}>\\cdots>t_{k}\\in[1,T]}\\sum_{i=1}^{k}\\left[\\|G_{\\mathrm{Adrace}}(f,z_{i},t_{i}|\\theta)-G_{\\mathrm{Face2Image}}(f,z_{i},t_{i}|\\theta^{\\prime})\\|_{2}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "Hc2ZwCYgmB/tmp/9e7488d2d23b096f5692e685f6b40c426283674b76b9db58f71c46d1d9170d3c.jpg", "img_caption": ["Figure 5: Composition distillation on four types of prompts: subject-single, subject-composition, class-single and class-composition. The four generated images form two contrastive pairs, and their feature deltas are encouraged to be aligned through a composition distillation loss. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "109 where $t_{1},\\cdot\\cdot\\cdot,t_{k}$ are a randomly sampled sequence of timesteps, and when $i>1$ , $z_{i}$ is the partially   \n110 denoised image by $G_{\\mathrm{Face2Image}}$ in the previous step.   \n111 Dynamic Model Expansion. When the training loss plateaus, it suggests that the model has reached   \n112 the limits of its capacity to capture nuanced facial features. In this situation, we expand the model   \n113 capacity by incorporating additional query and value projections within the attention layers of the   \n114 prompt inverter. As a result, each token is represented by multiple, subtly distinct query and value   \n115 tokens. This enables the model to better grasp the subtle facial features of the subject, thanks to the   \n116 increased diversity and richness of the queries and values. Note that the number of keys and output   \n117 tokens remain unchanged, ensuring that the computational load does not increase drastically.   \n118 Specifically, when a query projection $Q$ is expanded by $N$ times, we make $N$ identical copies of $Q$   \n119 and add Gaussian noises to $N-1$ of them. The same operation is applied to the value projection $V$ .   \n120 This is to ensure that the expanded $Q^{\\prime}$ and $V^{\\prime}$ do not deviate too much from the original $Q$ and $V$ ,   \n121 and the model augments the original features with slightly varied replicas.   \n122 The attention expansion proves to be particularly beneficial at the lower layers of the prompt inverter.   \n123 Intuitively, once some information in the features from the upstream Face2Image encoder is lost in   \n124 the lower layers, it is hard to recover in the higher layers. The mechanism of expanding queries and   \n125 values creates multiple, slightly varied replicas of the same information, thereby allowing the model   \n126 to select the most informative copy for preservation and further processing in subsequent layers.   \n127 This approach is conceptually akin to the role of the excitation operator in a squeeze-and-excitation   \n128 network [Hu et al., 2018], which also emphasizes selectively retaining the most significant features. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "129 2.3 Composition Distillation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "130 A prevalent issue with existing face encoders is that the subject token tends to dominate the generated   \n131 images, resulting in degeneration of compositionality. To mitigate this issue, we employ composi  \n132 tion distillation (Figure 5) to regularize the subject embeddings, ensuring that their semantics are   \n133 effectively integrated with other tokens, enhancing the overall expression. During this process, the   \n134 model observes how the original diffusion model adjusts output features to incorporate additional   \n135 compositional prompts into the output image. The model then imitates these adjustments when   \n136 encountering similar compositional prompts.   \n137 For this purpose, four types of prompts are employed to form two contrastive pairs: 1) a \u201csubject  \n138 single\u201d prompt that only contains the subject, such as \u201cA photo of a [Zendaya]\u201d, 2) a \u201csubject  \n139 composition\u201d prompt such as \u201cA photo of a [Zendaya] in the forest\u201d, 3) a \u201cclass-single\u201d prompt that   \n140 only contains a general class, such as \u201cA photo of a woman\u201d, and 4) a \u201cclass-composition\u201d prompt   \n141 such as \u201cA photo of a woman in the forest\u201d. Ideally, the semantic differences between \u201cA photo of $x$ \u201d   \n142 and \u201cA photo of $x$ in the forest\u201d should only be relevant to \u201cin the forest\u201d, and is independent of $x$ .   \n143 We represent the semantic differences between two pairs of prompts as their \u201cfeature deltas\u201d. The train  \n144 ing objective is to encourage the feature deltas between the subject-single and subject-composition   \n145 images to be aligned with the feature deltas between the class-single and class-composition images.   \n146 In other words, the following equation is expected to hold approximately: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Delta(\\mathrm{subject,compos})\\doteq\\mathrm{feat}(\\mathrm{subject,compos})-\\mathrm{feat}(\\mathrm{subject})}&{}\\\\ {\\approx\\Delta(\\mathrm{class,compos})}&{\\doteq\\mathrm{feat}(\\mathrm{class,compos})\\quad-\\mathrm{feat}(\\mathrm{class}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "147 where subject, class, (subject, compos) and (class, compos) denote the four types of prompts, re  \n148 spectively. (subject, compos) and (class, compos) are randomly drawn from a pool of common   \n149 compositional prompts consisting of various backgrounds, additional objects, dresses, image styles   \n150 and lighting conditions. feat $(x)$ refers to relevant features, including 1) the output features from all   \n151 the cross-attention layers, 2) the attention maps in all the cross-attention layers, and 3) the encoded   \n152 prompt embeddings by CLIP text encoder. feat $(x)-\\operatorname{feat}(y)$ is the orthogonal subtraction between   \n153 two feature maps, defined below.   \n154 We define a compositional delta loss that aligns the feature deltas $\\Delta_{i}$ (subject, compos) and   \n155 $\\Delta_{i}$ (class, compos) on the three types of features listed above: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\Delta}=\\sum_{i}\\bigl\\{1-\\mathbb{E}_{\\mathrm{compos}\\sim\\mathrm{U}(C)}\\,\\cos(\\Delta_{i}(\\mathrm{subject,compos}),\\Delta_{i}(\\mathrm{class,compos}))\\bigr\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "156 in which $i$ indexes the feature type (cross-attention output features, attention maps or CLIP prompt   \n157 embeddings), and $\\mathrm{U}(C)$ is a uniform distribution on a set of compositional prompts $C$ .   \n158 Orthogonal Subtraction. We wish to remove subject-specific features through the feature sub  \n159 traction \u201cfeat(subject, compos) \u2212feat(subject)\u201d. However, it is commonly observed that the subject  \n160 specific features may have different magnitudes (often smaller under compositional prompts). To   \n161 mitigate this issue, we propose to use orthogonal subtraction, which is invariant to the scale of   \n162 the subject-specific features. A relevant idea [Wang et al., 2023] is explored for language model   \n163 fine-tuning. Specifically, the feature deltas are calculated using the following equation: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Delta\\mathrm{feat}(s,c)=\\mathrm{feat}(s,c)-\\mathrm{proj}_{\\mathrm{feat}(s)}(\\mathrm{feat}(s,c)),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "164 where $\\operatorname{\\mathfrak{nroj}}_{\\operatorname{feat}(s)}(\\operatorname{feat}(s,c))$ is the projection of feat $(s,c)$ onto feat $\\left(s\\right)$ , computed as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{proj}_{\\mathrm{feat}(s)}(\\mathrm{feat}(s,c))=\\langle\\mathrm{feat}(s,c),\\mathrm{feat}(s)\\rangle\\mathrm{feat}(s),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "165 with $\\langle\\operatorname{feat}(s,c),\\operatorname{feat}(s)\\rangle$ being the inner product between the two features. The operation effectively   \n166 projects feat $(s,c)$ onto the orthogonal complement of feat $\\left(s\\right)$ and then subtracts this projection from   \n167 feat $(s,c)$ . As a result, $\\Delta\\mathrm{feat}(s,c)$ , the feature delta, is orthogonal to feat $\\left(s\\right)$ . This methodology   \n168 ensures that the deltas remove as much of the subject-specific features as possible, thereby minimizing   \n169 the influence of the scales of the subject-specific features contained within feat $(s,c)$ .   \n170 Differences with Previous Methods. While previous methods have explored analogous concepts,   \n171 such as StyleGAN-NADA [Gal et al., 2022b], which applies similar regularizations in the CLIP   \n172 prompt embedding space, and PuLID [Guo et al., 2024b], which introduces similar contrastive   \n173 regularizations on cross-attention queries, our approach is more comprehensive and effective. Our   \n174 compositional delta loss encompasses a broader range of relevant features, including the attention   \n175 maps and output features from cross-attention layers, and the CLIP prompt embeddings. Moreover,   \n176 we introduce an orthogonal subtraction technique for computing the feature deltas. This technique   \n177 isolates and extracts composition-specific features, making the distillation more effective. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "178 2.4 Elastic Face Preserving Loss ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "179 The composition distillation is done on instances with different prompts starting from the same initial   \n180 noise. This is to encourage the diffusion model to generate images that are compositionally similar ", "page_idx": 5}, {"type": "image", "img_path": "Hc2ZwCYgmB/tmp/296328e43b8d3aec12d14098ee6d5bb9db14eb2fa4749251b12940f446e383ff.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 6: To prevent subject features from degeneration due to spatial misalignment during composition distillation, we propose a Elastic Face Preserving Loss. The second row shows the cross-attention maps at selected four points on the subject-single image. The highlighted pixels associate the corresponding facial areas across the two images. The features of matching pixels are required to be close to each other to achieve subject feature preservation. ", "page_idx": 6}, {"type": "text", "text": "181 [Zhang et al., 2024], to achieve more accurate alignment between the image pairs. Despite this effort,   \n182 spatial misalignment often persists between the images differently prompted. This misalignment   \n183 can result in delta loss providing erroneous signals from non-facial to facial areas, slowly reducing   \n184 the authenticity of the generated subjects. For instance, on a noisy input face image, the output   \n185 image from the subject-single instance is expected to largely retain the same facial contours as the   \n186 input. However, the output from the subject-composition instance often deviate from the original   \n187 face contours, due to the introduction of additional compositional elements. An illustrative example   \n188 provided in the first row of Figure 6 shows how a chef hat in one image spatially aligns with the hair   \n189 in another, leading to potential contamination in the subject\u2019s hair representations.   \n190 To tackle this challenge, we view the subject-composition image as a \u201cwarped\u201d version of the subject  \n191 single image, and turn to techniques from the Optical Flow literature[Teed and Deng, 2020, Sui et al.,   \n192 2022] to estimate a matching field. The matching field is used to spatially align the subject features   \n193 across different images, ensuring them to be consistently maintained after \u201cwarping\u201d.   \n194 Specifically, the model takes as input a noisy face image from the training data. The face image is   \n195 accompanied by a segmentation mask, isolating the face area for matching. We compute the cross   \n196 attention matrix2 between the queries of a subject-single instance and a subject-composition instance: ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{CA(subj,compos)}=\\mathrm{softmax}(\\mathrm{Q}_{\\mathrm{subj}}\\mathrm{Q}_{\\mathrm{compos}}^{T}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "197 By looking up the cross-attention map CA(subj, compos), we can find the pixels best matching a   \n198 subject-single image pixel in a subject-composition image. The second row in Figure 6 shows the   \n199 attention maps of four points on the face in the left image. We \u201csoft-warp\u201d the subject-composition   \n200 features to align with the subject-single features through matrix multiplication, and require the warped   \n201 features to be close to the facial features in the subject-single image: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{face-preserving}}=1-\\cos\\Bigl(\\mathrm{CA(\\subj,compos)\\odotfeat(\\compos),feat(\\subj)}\\Bigr)_{\\mathrm{maxk}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "202 Here for clarity, feat(subject, compos) is abbreviated as feat(compos). The cosine similarity $\\cos(\\cdot,\\cdot)$   \n203 is computed on the masked area. The face-preserving loss is computed on each U-Net cross-attention   \n204 layer. It encourages the subject features in the subject-composition instance to be consistent with   \n205 those in the subject-single instance, preventing them from being contaminated in the composition   \n206 distillation process. ", "page_idx": 6}, {"type": "image", "img_path": "Hc2ZwCYgmB/tmp/c5a5573bd3af529a092da7858b4b0bb1c0d8835a6f993d2e12101999d80bb434.jpg", "img_caption": ["Figure 7: Qualitative comparison of AdaFace with state-of-the-art face encoders. AdaFace generates images that maintain the highest authenticity of the subjects, while still follow the target prompts. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "207 3 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "208 3.1 Dataset and Training Details ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "209 We trained AdaFace on a combination of two face datasets: Flickr-Faces-HQ (FFHQ) [Karras et al.,   \n210 2019], which comprises 70,000 images, and VGGFace2-HQ [Cao et al., 2018], which comprises   \n211 336,567 images after filtering. Face masks were generated using the BiSeNet face segmentation   \n212 model [Yu et al., 2018]. The distilled Face2Image model is Ada2Face [Papantoniou et al., 2024], as it   \n213 is able to generate authentic and diverse face images. The training employed the Prodigy optimizer   \n214 [Mishchenko and Defazio, 2024] with d_coef=2 (akin to the learning rate in other optimizers) during   \n215 face distillation, and ${\\mathrm{d}}_{-}{\\mathrm{coef}}{=}0.5$ during composition distillation. Batch sizes were set to 4 and 3 for   \n216 the two stages, respectively, with a gradient accumulation of 2. The model was trained with 240,000   \n217 iterations in the face distillation stage and 120,000 iterations in the composition distillation stage.   \n218 During face distillation, the loss reached a plateau twice, resulting in two dynamic expansions of the   \n219 model capacity. Eventually, the attention layers in the trained prompt inverter were expanded with   \n220 multipliers of $(8x,8x,8x,4x,4x,...,4x)$ relative to the original CLIP text encoder. This resulted in   \n221 a total of 2M parameters, in contrast to the 1.2M parameters of the original model.   \n222 In addition, we collected the images of 23 celebrities, each with 9 10 images, as the evaluated subjects.   \n223 These celebrities include actors, singers and internet celebrities on Instagram. This dataset will be   \n224 released along with the code. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "225 3.2 Qualitative Comparisons ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "226 We compared AdaFace with a few state-of-the-art face encoders, including InstantID [Wang et al.,   \n227 2024], ConsistentID [Huang et al., 2024] and PuLID [Guo et al., 2024b]. The input were images   \n228 from our celebrity-23 dataset.   \n229 The results presented in Figure 7 demonstrate that AdaFace produces images that not only exhibit   \n230 high authenticity of the subjects but also show good consistency with the text prompts. In comparison,   \n231 other models often fall short in generating images that are either less authentic or less compositional.   \n232 For instance, InstantID tends to produce overly stylized images with significant variability in au  \n233 thenticity across different subjects. PuLID, while generating aesthetically pleasing images, achieves   \n234 slightly lower authenticity levels compared to AdaFace. Despite also utilizing a text-space approach,   \n235 ConsistentID has the least compositional output among the models evaluated, largely due to the   \n236 absence of compositional training in its ID embeddings.   \n237 In addition, we plugged AdaFace into AnimateDiff, and generated personalized videos of celebrities   \n238 under various compositional prompts. The results are shown in Figure 1. Figure 8 compares with a   \n239 recent method ID-Animator [He et al., 2024]. AdaFace generated videos with high authenticity and   \n240 compositionality, while ID-Animator usually produces videos with less authentic subjects. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "Hc2ZwCYgmB/tmp/0783b2e383fda341e3fdb460575ee5b9708ad758a5546d4afac78dde5bab6852.jpg", "img_caption": ["Figure 8: Comparison of AdaFace with ID-Animator on personalized video generation. AdaFace generates videos with higher authenticity and compositionality. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "241 3.3 Quantitative Evaluations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "242 To assess the performance of AdaFace quantitatively, we evaluated a few baseline methods and   \n243 AdaFace, on the \u201ccelebrity- $.23\"$ images and DreamBench compositional prompts, comparing AdaFace   \n244 with two baseline methods PuLID and InstantID. First, we measured the face similarity using the   \n245 cosine similarity between the ArcFace embedding of the generated images and reference images. In   \n246 addition, the CLIP-Text (CLIP-T) metric determines the consistency of the generated images with the   \n247 prompts. The DINO and CLIP-I metrics are less indicative and are only for reference. The results,   \n248 detailed in Table 1, show that AdaFace achieved comparable face similarity and prompt consistency   \n249 scores to PuLID, and slightly outperformed InstantID. Note that the results of AdaFace is achieved   \n250 on the original Stable Diffusion 1.5 model weight, which usually leads to much lower composition   \n251 scores than other fine-tuned SD 1.5 model weights, such as RealisticVision. ", "page_idx": 8}, {"type": "table", "img_path": "Hc2ZwCYgmB/tmp/d9c7be0b65544294e966ce34136750f0464a45ed70eb53e9abe34d4f12bcb5c5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 1: Quantitative evaluation on the \u201ccelebrity- $.23\"$ images and DreamBench compositional prompts. -Comp is the model trained only with the face distillation stage. ", "page_idx": 8}, {"type": "text", "text": "252 As an ablation study, we list the performance of the AdaFace model without composition distillation.   \n253 It can be seen that the face authenticity is slightly reduced after composition distillation, however, the   \n254 generated images become much more consistent with the prompts. ", "page_idx": 8}, {"type": "text", "text": "255 4 Conclusions and Discussions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "256 In this work, we present AdaFace, a versatile face encoder that maps human faces into the text   \n257 prompt space. AdaFace is trained with a low compute budget and achieves high authenticity and   \n258 compositionality in zero-shot generation of subject images. We demonstrate the effectiveness of   \n259 AdaFace by showcasing the generated images and videos of celebrities under various compositional   \n260 prompts. Additionally, our quantitative evaluations further underscore its performance.   \n261 A notable limitation of AdaFace is that the authenticity of the output face embeddings are constrained   \n262 by the Face2Image model it distills from. However, this limitation can be addressed by distilling on   \n263 more powerful Face2Image models and expanding the model capacity. For future work, we would   \n264 extend the AdaFace method to object images. For instance, applying AdaFace distillation techniques   \n265 to IP-Adapter [Ye et al., 2023] could enable the generation of both human and object images. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "266 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "267 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "268 Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman. VGGFace2: a dataset for recognising   \n269 faces across pose and age. In 2018 13th IEEE international conference on automatic face &amp;   \n270 gesture recognition (FG 2018), pages 67\u201374, Los Alamitos, CA, USA, May 2018. IEEE Computer   \n271 Society. doi: 10.1109/FG.2018.00020. URL https://doi.ieeecomputersociety.org/10.   \n272 1109/FG.2018.00020.   \n273 J. Chen, J. YU, C. GE, L. Yao, E. Xie, Z. Wang, J. Kwok, P. Luo, H. Lu, and Z. Li. PixArt-\\$\\alpha\\$:   \n274 Fast training of diffusion transformer for photorealistic text-to-image synthesis. In The twelfth   \n275 international conference on learning representations, 2024a. URL https://openreview.net/   \n276 forum?id $\\equiv$ eAKmQPe3m1.   \n277 W. Chen, J. Zhang, J. Wu, H. Wu, X. Xiao, and L. Lin. ID-Aligner: Enhancing Identity-Preserving   \n278 Text-to-Image Generation with Reward Feedback Learning, Apr. 2024b. URL http://arxiv.   \n279 org/abs/2404.15449. arXiv:2404.15449 [cs].   \n280 J. Deng, J. Guo, N. Xue, and S. Zafeiriou. ArcFace: Additive Angular Margin Loss for Deep Face   \n281 Recognition. pages 4690\u20134699, 2019. URL https://openaccess.thecvf.com/content_   \n282 CVPR_2019/html/Deng_ArcFace_Additive_Angular_Margin_Loss_for_Deep_Face_   \n283 Recognition_CVPR_2019_paper.html.   \n284 R. Gal, Y. Alaluf, Y. Atzmon, O. Patashnik, A. H. Bermano, G. Chechik, and D. Cohen-Or. An Image   \n285 is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion, Aug. 2022a.   \n286 URL http://arxiv.org/abs/2208.01618. arXiv:2208.01618 [cs].   \n287 R. Gal, O. Patashnik, H. Maron, A. H. Bermano, G. Chechik, and D. Cohen-Or. StyleGAN-NADA:   \n288 CLIP-guided domain adaptation of image generators. ACM Trans. Graph., 41(4), July 2022b.   \n289 ISSN 0730-0301. doi: 10.1145/3528223.3530164. URL https://doi.org/10.1145/3528223.   \n290 3530164. Number of pages: 13 Place: New York, NY, USA Publisher: Association for Computing   \n291 Machinery tex.articleno: 141 tex.issue_date: July 2022.   \n292 Y. Guo, C. Yang, A. Rao, Z. Liang, Y. Wang, Y. Qiao, M. Agrawala, D. Lin, and B. Dai. AnimateDiff:   \n293 Animate your personalized text-to-image diffusion models without specific tuning. In The twelfth   \n294 international conference on learning representations, 2024a. URL https://openreview.net/   \n295 forum?id=Fx2SbBgcte.   \n296 Z. Guo, Y. Wu, Z. Chen, L. Chen, and Q. He. PuLID: Pure and Lightning ID Customization via Con  \n297 trastive Alignment, Apr. 2024b. URL http://arxiv.org/abs/2404.16022. arXiv:2404.16022   \n298 [cs].   \n299 Y. Han, J. Zhu, K. He, X. Chen, Y. Ge, W. Li, X. Li, J. Zhang, C. Wang, and Y. Liu. Face Adapter   \n300 for Pre-Trained Diffusion Models with Fine-Grained ID and Attribute Control, May 2024. URL   \n301 http://arxiv.org/abs/2405.12970. arXiv:2405.12970 [cs].   \n302 X. He, Q. Liu, S. Qian, X. Wang, T. Hu, K. Cao, K. Yan, and J. Zhang. ID-Animator: Zero-Shot   \n303 Identity-Preserving Human Video Generation, May 2024. URL http://arxiv.org/abs/2404.   \n304 15275. arXiv:2404.15275 [cs].   \n305 J. Ho, A. Jain, and P. Abbeel. Denoising Diffusion Probabilistic Models. In Ad  \n306 vances in Neural Information Processing Systems, volume 33, pages 6840\u20136851. Curran   \n307 Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/hash/   \n308 4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html.   \n309 J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation networks. In 2018 IEEE/CVF conference on   \n310 computer vision and pattern recognition, pages 7132\u20137141, 2018. doi: 10.1109/CVPR.2018.00745.   \n311 J. Huang, X. Dong, W. Song, H. Li, J. Zhou, Y. Cheng, S. Liao, L. Chen, Y. Yan, S. Liao, and   \n312 X. Liang. ConsistentID: Portrait Generation with Multimodal Fine-Grained Identity Preserving,   \n313 Apr. 2024. URL http://arxiv.org/abs/2404.16771. arXiv:2404.16771 [cs].   \n314 T. Karras, S. Laine, and T. Aila. A style-based generator architecture for generative adversarial   \n315 networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition   \n316 (CVPR), June 2019.   \n317 B. Kawar, S. Zada, O. Lang, O. Tov, H. Chang, T. Dekel, I. Mosseri, and M. Irani. Imagic: Text  \n318 based real image editing with diffusion models. In Conference on computer vision and pattern   \n319 recognition 2023, 2023.   \n320 N. Kumari, B. Zhang, R. Zhang, E. Shechtman, and J.-Y. Zhu. Multi-Concept Customization of Text  \n321 to-Image Diffusion, Dec. 2022. URL http://arxiv.org/abs/2212.04488. arXiv:2212.04488   \n322 [cs].   \n323 K. Mishchenko and A. Defazio. Prodigy: An Expeditiously Adaptive Parameter-Free Learner, Mar.   \n324 2024. URL http://arxiv.org/abs/2306.06101. arXiv:2306.06101 [cs, math, stat].   \n325 A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever, and M. Chen.   \n326 GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models,   \n327 Mar. 2022. URL http://arxiv.org/abs/2112.10741. arXiv:2112.10741 [cs].   \n328 F. P. Papantoniou, A. Lattas, S. Moschoglou, J. Deng, B. Kainz, and S. Zafeiriou. Arc2Face: A   \n329 Foundation Model of Human Faces, Mar. 2024. URL http://arxiv.org/abs/2403.11641.   \n330 arXiv:2403.11641 [cs].   \n331 W. Peebles and S. Xie. Scalable Diffusion Models with Transformers, Mar. 2023. URL http:   \n332 //arxiv.org/abs/2212.09748. arXiv:2212.09748 [cs].   \n333 D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. M\u00fcller, J. Penna, and R. Rombach.   \n334 SDXL: Improving latent diffusion models for high-resolution image synthesis. In The twelfth   \n335 international conference on learning representations, 2024. URL https://openreview.net/   \n336 forum?id $\\equiv,$ di52zR8xgf.   \n337 R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-Resolution Image Syn  \n338 thesis with Latent Diffusion Models, Apr. 2022. URL http://arxiv.org/abs/2112.10752.   \n339 arXiv:2112.10752 [cs].   \n340 N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman. DreamBooth: Fine Tuning   \n341 Text-to-Image Diffusion Models for Subject-Driven Generation, Aug. 2022. URL http://arxiv.   \n342 org/abs/2208.12242. arXiv:2208.12242 [cs].   \n343 C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. Denton, S. K. S. Ghasemipour, R. Gontijo-Lopes,   \n344 B. K. Ayan, T. Salimans, J. Ho, D. J. Fleet, and M. Norouzi. Photorealistic Text-to-Image Diffusion   \n345 Models with Deep Language Understanding. Oct. 2022.   \n346 J. Shi, W. Xiong, Z. Lin, and H. J. Jung. InstantBooth: Personalized Text-to-Image Genera  \n347 tion without Test-Time Finetuning, Apr. 2023. URL http://arxiv.org/abs/2304.03411.   \n348 arXiv:2304.03411 [cs].   \n349 X. Sui, S. Li, X. Geng, Y. Wu, X. Xu, Y. Liu, R. Goh, and H. Zhu. CRAFT: Cross-attentional flow   \n350 transformer for robust optical flow. In 2022 IEEE/CVF conference on computer vision and pattern   \n351 recognition (CVPR), 2022.   \n352 Z. Teed and J. Deng. RAFT: Recurrent all-pairs field transforms for optical flow. In A. Vedaldi,   \n353 H. Bischof, T. Brox, and J.-M. Frahm, editors, Computer vision \u2013 ECCV 2020, pages 402\u2013419,   \n354 Cham, 2020. Springer International Publishing. ISBN 978-3-030-58536-5.   \n355 Y. Tewel, R. Gal, G. Chechik, and Y. Atzmon. Key-locked rank one editing for text-to-image   \n356 personalization. In ACM SIGGRAPH 2023 conference proceedings, Siggraph \u201923, New York, NY,   \n357 USA, 2023. Association for Computing Machinery. ISBN 9798400701597. doi: 10.1145/3588432.   \n358 3591506. URL https://doi.org/10.1145/3588432.3591506. Number of pages: 11 Place: ,   \n359 Los Angeles, CA, USA, tex.articleno: 12.   \n360 M. Toker, H. Orgad, M. Ventura, D. Arad, and Y. Belinkov. Diffusion Lens: Interpreting Text   \n361 Encoders in Text-to-Image Pipelines, Mar. 2024. URL http://arxiv.org/abs/2403.05846.   \n362 arXiv:2403.05846 [cs] version: 1.   \n363 Q. Wang, X. Bai, H. Wang, Z. Qin, A. Chen, H. Li, X. Tang, and Y. Hu. InstantID: Zero-shot Identity  \n364 Preserving Generation in Seconds, Feb. 2024. URL http://arxiv.org/abs/2401.07519.   \n365 arXiv:2401.07519 [cs].   \n366 X. Wang, T. Chen, Q. Ge, H. Xia, R. Bao, R. Zheng, Q. Zhang, T. Gui, and X. Huang. Orthogonal   \n367 subspace learning for language model continual learning. In Findings of the association for   \n368 computational linguistics: EMNLP 2023, Singapore, Dec. 2023. Association for Computational   \n369 Linguistics.   \n370 Y. Wei, Y. Zhang, Z. Ji, J. Bai, L. Zhang, and W. Zuo. ELITE: Encoding Visual Concepts into Textual   \n371 Embeddings for Customized Text-to-Image Generation. In ICCV 2023. arXiv, Feb. 2023. doi:   \n372 10.48550/arXiv.2302.13848. URL http://arxiv.org/abs/2302.13848. arXiv:2302.13848   \n373 [cs].   \n374 H. Ye, J. Zhang, S. Liu, X. Han, and W. Yang. IP-Adapter: Text Compatible Image Prompt Adapter   \n375 for Text-to-Image Diffusion Models, Aug. 2023. URL http://arxiv.org/abs/2308.06721.   \n376 arXiv:2308.06721 [cs].   \n377 C. Yu, J. Wang, C. Peng, C. Gao, G. Yu, and N. Sang. BiSeNet: Bilateral segmentation network for   \n378 real-time semantic segmentation. In V. Ferrari, M. Hebert, C. Sminchisescu, and Y. Weiss, editors,   \n379 Computer vision \u2013 ECCV 2018, pages 334\u2013349, Cham, 2018. Springer International Publishing.   \n380 ISBN 978-3-030-01261-8.   \n381 H. Zhang, J. Zhou, Y. Lu, M. Guo, P. Wang, L. Shen, and Q. Qu. The Emergence of Reproducibility   \n382 and Consistency in Diffusion Models, Feb. 2024. URL http://arxiv.org/abs/2310.05264.   \n383 arXiv:2310.05264 [cs]. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "384 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "386 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n87 paper\u2019s contributions and scope?   \n88 Answer: [Yes]   \n89 Justification: The main claims in the abstract and introduction accurately reflect the paper\u2019s   \n390 contributions and scope, as the detailed results, discussions, and conclusions align with and   \n391 support the initial claims.   \n92 Guidelines:   \n393 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n94 made in the paper.   \n95 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n96 contributions made in the paper and important assumptions and limitations. A No or   \n97 NA answer to this question will not be perceived well by the reviewers.   \n398 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n399 much the results can be expected to generalize to other settings.   \n00 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n01 are not attained by the paper. ", "page_idx": 12}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 12}, {"type": "text", "text": "Justification: The limitations of the work are discussed in the \"Conclusions and Discussion\" section. ", "page_idx": 12}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 12}, {"type": "text", "text": "34 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "35 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n36 a complete (and correct) proof? ", "page_idx": 12}, {"type": "text", "text": "Justification: For each theoretical result, the paper provides a plenty of experimental support. Guidelines: ", "page_idx": 13}, {"type": "text", "text": "39   \n40 \u2022 The answer NA means that the paper does not include theoretical results.   \n41 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n42 referenced.   \n43 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n44 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n45 they appear in the supplemental material, the authors are encouraged to provide a short   \n46 proof sketch to provide intuition.   \n47 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n48 by formal proofs provided in appendix or supplemental material.   \n49 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 13}, {"type": "text", "text": "450 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "451 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n452 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n453 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 13}, {"type": "text", "text": "Justification: All the experimental details are clearly stated in the paper and the code will be made publicly available. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 13}, {"type": "text", "text": "489 5. Open access to data and code ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "490 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n491 tions to faithfully reproduce the main experimental results, as described in supplemental   \n492 material?   \n493 Answer: [Yes]   \n494 Justification: All data and code will be made publicly available.   \n495 Guidelines:   \n496 \u2022 The answer NA means that paper does not include experiments requiring code.   \n497 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n498 public/guides/CodeSubmissionPolicy) for more details.   \n499 \u2022 While we encourage the release of code and data, we understand that this might not be   \n500 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n501 including code, unless this is central to the contribution (e.g., for a new open-source   \n502 benchmark).   \n503 \u2022 The instructions should contain the exact command and environment needed to run to   \n504 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n505 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n506 \u2022 The authors should provide instructions on data access and preparation, including how   \n507 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n508 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n509 proposed method and baselines. If only a subset of experiments are reproducible, they   \n510 should state which ones are omitted from the script and why.   \n511 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n512 versions (if applicable).   \n513 \u2022 Providing as much information as possible in supplemental material (appended to the   \n514 paper) is recommended, but including URLs to data and code is permitted.   \n515 6. Experimental Setting/Details   \n516 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n517 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n518 results?   \n519 Answer: [Yes]   \n520 Justification: Please refer to the \"Implementation Detail\" section in the main paper.   \n521 Guidelines:   \n522 \u2022 The answer NA means that the paper does not include experiments.   \n523 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n524 that is necessary to appreciate the results and make sense of them.   \n525 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n526 material.   \n527 7. Experiment Statistical Significance   \n528 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n529 information about the statistical significance of the experiments?   \n530 Answer: [No]   \n531 Justification: We evaluated on a diverse set of 30 celebrities, each with around 50 prompts,   \n532 which is sufficient to reflect the model\u2019s performance.   \n533 Guidelines:   \n534 \u2022 The answer NA means that the paper does not include experiments.   \n535 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n536 dence intervals, or statistical significance tests, at least for the experiments that support   \n537 the main claims of the paper.   \n538 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n539 example, train/test split, initialization, random drawing of some parameter, or overall   \n540 run with given experimental conditions).   \n41 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n42 call to a library function, bootstrap, etc.)   \n43 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n44 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n45 of the mean.   \n46 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n47 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n48 of Normality of errors is not verified.   \n49 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n550 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n551 error rates).   \n552 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n53 they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "554 8. Experiments Compute Resources ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "555 Question: For each experiment, does the paper provide sufficient information on the com  \n556 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n557 the experiments?   \n558 Answer: [Yes]   \n559 Justification: We use 2 A6000 GPUs, each with 48G of memory.   \n560 Guidelines: ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 15}, {"type": "text", "text": "569 9. Code Of Ethics ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "570 Question: Does the research conducted in the paper conform, in every respect, with the   \n571 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n573 Justification: The research conducted in the paper conforms in every respect with the   \n574 NeurIPS Code of Ethics.   \n575 Guidelines:   \n576 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n577 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n578 deviation from the Code of Ethics.   \n579 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n580 eration due to laws or regulations in their jurisdiction). ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "581 10. Broader Impacts ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "582 Question: Does the paper discuss both potential positive societal impacts and negative   \n583 societal impacts of the work performed? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: We discussed the positive impacts, including its potential use in entertainment and art, video games and virtual reality. Additionally, its potential use for educational purposes in historical recreation, such as recreating faces of historical figures or enhancing documentaries, bringing history to life. We also pointed out potential negative impacts, including privacy violations. There is a risk of creating and using images of individuals without their consent. Moreover, misinformation and deepfakes are among the most concerning impacts, with the creation of deepfake videos that could be used to spread misinformation and manipulate public opinion. We also highlighted security concerns, as the technology ", "page_idx": 15}, {"type": "text", "text": "593 could be used to bypass facial recognition systems for fraudulent purposes, posing significant   \n594 security challenges. The authors will join in the effort for possible mitigation by providing   \n595 gated release of models.   \n596 Guidelines:   \n597 \u2022 The answer NA means that there is no societal impact of the work performed.   \n598 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n599 impact or why the paper does not address societal impact.   \n600 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n601 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n602 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n603 groups), privacy considerations, and security considerations.   \n604 \u2022 The conference expects that many papers will be foundational research and not tied   \n605 to particular applications, let alone deployments. However, if there is a direct path to   \n606 any negative applications, the authors should point it out. For example, it is legitimate   \n607 to point out that an improvement in the quality of generative models could be used to   \n608 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n609 that a generic algorithm for optimizing neural networks could enable people to train   \n10 models that generate Deepfakes faster.   \n611 \u2022 The authors should consider possible harms that could arise when the technology is   \n12 being used as intended and functioning correctly, harms that could arise when the   \n13 technology is being used as intended but gives incorrect results, and harms following   \n14 from (intentional or unintentional) misuse of the technology.   \n15 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n16 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n17 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n18 feedback over time, improving the efficiency and accessibility of ML).   \n19 11. Safeguards   \n620 Question: Does the paper describe safeguards that have been put in place for responsible   \n621 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n622 image generators, or scraped datasets)?   \n23 Answer: [Yes]   \n624 Justification: The work describes basic safeguards implemented for the responsible release   \n625 of models, particularly focusing on preventing misuse. We have incorporated filters that   \n626 specifically exclude NSFW (Not Safe for Work) keywords in the generation prompts, such   \n627 as \u2019nude,\u2019 \u2019naked,\u2019 \u2019nsfw,\u2019 \u2019topless,\u2019 and \u2019bare breasts.\u2019 This approach helps mitigate the risk   \n628 of generating inappropriate or sensitive content.\"   \n629 Guidelines:   \n630 \u2022 The answer NA means that the paper poses no such risks.   \n631 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n632 necessary safeguards to allow for controlled use of the model, for example by requiring   \n633 that users adhere to usage guidelines or restrictions to access the model or implementing   \n634 safety filters.   \n635 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n636 should describe how they avoided releasing unsafe images.   \n637 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n638 not require this, but we encourage authors to take this into account and make a best   \n639 faith effort.   \n640 12. Licenses for existing assets   \n641 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n642 the paper, properly credited and are the license and terms of use explicitly mentioned and ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "645 Justification: In our paper, we have ensured proper attribution for all assets used, such as   \n646 code, data, and models, by citing the related papers and sources from which these assets   \n647 were derived. Additionally, we have adhered to the licensing terms and conditions of each   \n648 asset, as detailed in the respective citations.   \n649 Guidelines:   \n650 \u2022 The answer NA means that the paper does not use existing assets.   \n651 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n652 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n653 URL.   \n654 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n655 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n656 service of that source should be provided.   \n657 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n658 package should be provided. For popular datasets, paperswithcode.com/datasets   \n659 has curated licenses for some datasets. Their licensing guide can help determine the   \n660 license of a dataset.   \n661 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n662 the derived asset (if it has changed) should be provided.   \n663 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n664 the asset\u2019s creators.   \n665 13. New Assets   \n666 Question: Are new assets introduced in the paper well documented and is the documentation   \n667 provided alongside the assets?   \n668 Answer: [Yes]   \n669 Justification: New assets introduced in the paper are well documented. The code is accompa  \n670 nied by usage documentation and is embedded with detailed comments to ensure clarity and   \n671 ease of use for future researchers. Additionally, videos are provided alongside a description   \n672 of the flies and a list of prompts used for their generation, which enhances transparency and   \n673 replicability of the results.   \n674 Guidelines:   \n675 \u2022 The answer NA means that the paper does not release new assets.   \n676 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n677 submissions via structured templates. This includes details about training, license,   \n678 limitations, etc.   \n679 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n680 asset is used.   \n681 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n682 create an anonymized URL or include an anonymized zip file.   \n683 14. Crowdsourcing and Research with Human Subjects   \n684 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n685 include the full text of instructions given to participants and screenshots, if applicable, as   \n686 well as details about compensation (if any)?   \n687 Answer: [NA]   \n688 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n689 Guidelines:   \n690 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n691 human subjects.   \n692 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n693 tion of the paper involves human subjects, then as much detail as possible should be   \n694 included in the main paper.   \n695 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n696 or other labor should be paid at least the minimum wage in the country of the data   \n697 collector.   \n98 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n99 Subjects   \n00 Question: Does the paper describe potential risks incurred by study participants, whether   \n01 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n02 approvals (or an equivalent approval/review based on the requirements of your country or   \n03 institution) were obtained?   \n04 Answer: [NA]   \n05 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n06 Guidelines:   \n07 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n08 human subjects.   \n09 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n10 may be required for any human subjects research. If you obtained IRB approval, you   \n11 should clearly state this in the paper.   \n12 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n13 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n14 guidelines for their institution.   \n15 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n16 applicable), such as the institution conducting the review. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}]