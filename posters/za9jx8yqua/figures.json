[{"figure_path": "za9Jx8yqUA/figures/figures_0_1.jpg", "caption": "Figure 1: Multimodal-foundation world models connect and align the video-language space of a foundation model with the latent space of a generative world model for reinforcement learning, requiring vision-only data. Our GenRL framework turns visual and/or language prompts into latent targets and learns to realize the corresponding behaviors by training in the world model's imagination.", "description": "This figure illustrates the core concept of GenRL. It shows how multimodal foundation world models bridge the gap between video-language prompts and embodied agent behavior.  Vision-language prompts (like images of actions and text descriptions) are processed by a foundation model, which is then linked to a generative world model.  The generative world model learns to map those prompts into corresponding latent states, allowing an embodied agent to learn the behaviors associated with those prompts purely through training within the imagined world of the generative model (without interacting with the real world).", "section": "Introduction"}, {"figure_path": "za9Jx8yqUA/figures/figures_2_1.jpg", "caption": "Figure 2: Overview of GenRL. The agent learns a multimodal-foundation world model that connects and aligns (a) the representation of a foundation VLM with the latent states of a generative world model. Given a certain task prompt, (b) the model allows embedding the task and translating into targets in the latent dynamics space, which the agent can learn to achieve by using RL in imagination.", "description": "This figure illustrates the GenRL framework.  Panel (a) shows how a multimodal-foundation world model connects and aligns the representation of a foundation vision-language model (VLM) with the latent states of a generative world model.  Panel (b) demonstrates how a task prompt (visual or language) is embedded and translated into target latent states within the generative world model. The agent then learns to achieve these targets using reinforcement learning in the world model's simulated environment.", "section": "3 GenRL"}, {"figure_path": "za9Jx8yqUA/figures/figures_2_2.jpg", "caption": "Figure 2: Overview of GenRL. The agent learns a multimodal-foundation world model that connects and aligns (a) the representation of a foundation VLM with the latent states of a generative world model. Given a certain task prompt, (b) the model allows embedding the task and translating into targets in the latent dynamics space, which the agent can learn to achieve by using RL in imagination.", "description": "This figure illustrates the GenRL framework.  Panel (a) shows how a multimodal foundation world model connects the representation of a foundation Vision-Language Model (VLM) with the latent states of a generative world model.  This alignment is crucial for grounding tasks in the embodied domain. Panel (b) demonstrates how a task prompt (visual or language) is embedded and translated into target latent states within the generative world model. The agent then learns to achieve these targets using reinforcement learning within the simulated environment of the world model.", "section": "3 GenRL"}, {"figure_path": "za9Jx8yqUA/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of GenRL. The agent learns a multimodal-foundation world model that connects and aligns (a) the representation of a foundation VLM with the latent states of a generative world model. Given a certain task prompt, (b) the model allows embedding the task and translating into targets in the latent dynamics space, which the agent can learn to achieve by using RL in imagination.", "description": "This figure illustrates the GenRL framework.  Panel (a) shows how the agent learns a multimodal-foundation world model which connects the representation of a Vision-Language Model (VLM) with the latent states of a generative world model.  The key is the alignment of the VLM and world model representations. Panel (b) shows how a task prompt is embedded and translated into targets in the latent space of the world model. The agent then learns to achieve these targets by performing reinforcement learning (RL) in the imagination of the world model.", "section": "3 GenRL"}, {"figure_path": "za9Jx8yqUA/figures/figures_6_1.jpg", "caption": "Figure 2: Overview of GenRL. The agent learns a multimodal-foundation world model that connects and aligns (a) the representation of a foundation VLM with the latent states of a generative world model. Given a certain task prompt, (b) the model allows embedding the task and translating into targets in the latent dynamics space, which the agent can learn to achieve by using RL in imagination.", "description": "This figure illustrates the GenRL framework.  Panel (a) shows how GenRL connects and aligns the representation of a foundation Vision-Language Model (VLM) with the latent states of a generative world model.  Panel (b) depicts how a task prompt is embedded and translated into target latent dynamics, enabling the agent to learn the corresponding behaviors through reinforcement learning (RL) within the world model's imagination.", "section": "3 GenRL"}, {"figure_path": "za9Jx8yqUA/figures/figures_6_2.jpg", "caption": "Figure 5: Video-to-action. GenRL allows grounding video prompts into the target environment's dynamics. It allows visualization of the model's interpretation of the prompts, using the decoder (top row), and it allows turning prompts into behaviors, leading to generally higher performance than other approaches. 10 seeds. Additional visualizations on the project website.", "description": "This figure demonstrates the video-to-action capability of GenRL.  The top row shows example video prompts provided to the agent. The bottom row displays bar charts comparing the performance of GenRL with other baselines (IQL-V, TD3+BC-V, TD3-V, and WM-CLIP-V) across different tasks.  The results highlight GenRL's superior performance in translating video prompts into actions compared to other methods.", "section": "Experiments"}, {"figure_path": "za9Jx8yqUA/figures/figures_7_1.jpg", "caption": "Figure 2: Overview of GenRL. The agent learns a multimodal-foundation world model that connects and aligns (a) the representation of a foundation VLM with the latent states of a generative world model. Given a certain task prompt, (b) the model allows embedding the task and translating into targets in the latent dynamics space, which the agent can learn to achieve by using RL in imagination.", "description": "This figure illustrates the GenRL framework.  Panel (a) shows how GenRL connects a foundation vision-language model (VLM) with a generative world model. The VLM's representations are aligned with the generative world model's latent space. Panel (b) demonstrates how the framework uses language or visual prompts to generate latent targets within the generative world model.  The agent then learns to achieve these targets through reinforcement learning (RL) within the imagined world of the model. This process enables task learning based on only vision data without language annotations.", "section": "3 GenRL"}, {"figure_path": "za9Jx8yqUA/figures/figures_8_1.jpg", "caption": "Figure 7: Training data distribution. Analysing the impact of the training data distribution on the generalization performance of GenRL. Performance is obtained by training behaviors in data-free mode, after training the MFWM on different subsets of the training dataset. Performance averaged over 10 seeds (black lines indicate standard error). Full results in Appendix K.", "description": "This figure analyzes how different training data compositions affect GenRL's performance.  GenRL is trained with various subsets of the full training data (all data, exploration data, run data, walk data, and stand data), then tested in a data-free setting. The bar chart displays average performance across 10 random seeds, showing the impact of training data composition on the model's ability to generalize to unseen tasks.", "section": "Analysis of the training data distribution"}, {"figure_path": "za9Jx8yqUA/figures/figures_16_1.jpg", "caption": "Figure 8: Temporal alignment ablation. We analyze the impact of temporal alignment in our proposed RL objective for matching sequential targets. Results averaged over 10 seeds.", "description": "This figure presents the ablation study on the temporal alignment method used in GenRL.  Three different methods for aligning the agent's imagined trajectory with the target trajectory from the user prompt are compared:  best matching trajectory (using a sliding window to find the best alignment), best matching initial state (assuming the initial state is the same), and no alignment (no special alignment). The results are shown in terms of episodic rewards averaged over 10 seeds for a range of tasks from various locomotion and manipulation environments. The figure demonstrates that using a best-matching trajectory approach significantly improves the performance of the model.", "section": "Temporal alignment ablation"}, {"figure_path": "za9Jx8yqUA/figures/figures_17_1.jpg", "caption": "Figure 2: Overview of GenRL. The agent learns a multimodal-foundation world model that connects and aligns (a) the representation of a foundation VLM with the latent states of a generative world model. Given a certain task prompt, (b) the model allows embedding the task and translating into targets in the latent dynamics space, which the agent can learn to achieve by using RL in imagination.", "description": "This figure illustrates the GenRL framework. It shows how the agent learns a multimodal-foundation world model by connecting and aligning the representation of a foundation Vision-Language Model (VLM) with the latent states of a generative world model.  Panel (a) depicts the connection and alignment process, while panel (b) shows how the model embeds a task prompt and translates it into targets within the latent dynamics space, enabling the agent to learn the corresponding behaviors through reinforcement learning in imagination.", "section": "3 GenRL"}, {"figure_path": "za9Jx8yqUA/figures/figures_18_1.jpg", "caption": "Figure 10: Decoded language prompts in Minecraft", "description": "This figure shows the results of decoding language prompts in the Minecraft environment using GenRL with two different model sizes (1x and 8x). The images represent the model's interpretation of the prompts, illustrating its ability to generate corresponding visual representations for different biomes. This showcases GenRL's capacity for grounding language into the target environment's dynamics, even with a limited dataset.", "section": "I Scaling to complex observations"}, {"figure_path": "za9Jx8yqUA/figures/figures_19_1.jpg", "caption": "Figure 11: Multi-task generalization detailed results. Results averaged over 10 seeds.", "description": "This figure presents a detailed breakdown of the multi-task generalization performance of GenRL and several baseline methods across various locomotion and manipulation tasks.  Each bar represents the average episodic reward over 10 different seeds for a specific task and method.  The figure allows for a granular comparison of GenRL's performance against other offline RL approaches (IQL, TD3, TD3+BC, WM-CLIP) using both image-language (I) and video-language (V) prompts, providing insights into its generalization capabilities across different task types and input modalities.", "section": "Experiments"}, {"figure_path": "za9Jx8yqUA/figures/figures_20_1.jpg", "caption": "Figure 1: Multimodal-foundation world models connect and align the video-language space of a foundation model with the latent space of a generative world model for reinforcement learning, requiring vision-only data. Our GenRL framework turns visual and/or language prompts into latent targets and learns to realize the corresponding behaviors by training in the world model's imagination.", "description": "The figure illustrates the core concept of the GenRL framework. It shows how multimodal foundation world models (MFWMs) connect and align the video-language space of a foundation model (like InternVideo2) with the latent space of a generative world model (used for reinforcement learning).  Instead of relying on language annotations for task specification, GenRL uses visual and/or language prompts as input. These prompts are converted into latent targets within the world model, which the agent then learns to achieve through training within the model's \"imagination\".  This approach eliminates the need for complex reward design and allows for generalization across various tasks.", "section": "Introduction"}, {"figure_path": "za9Jx8yqUA/figures/figures_20_2.jpg", "caption": "Figure 13: Training data distribution detailed results. Results averaged over 10 seeds.", "description": "This figure displays a detailed breakdown of the impact of training data distribution on the performance of GenRL across various tasks.  It shows results for different subsets of the training data: all data, exploration data, run data, walk data, and stand data.  The results are presented as bar charts, showing the average performance (likely reward or success rate) across ten seeds for each task and data subset, allowing for a comparison of the effectiveness of different training data compositions on GenRL's ability to learn a wide range of locomotion behaviours.", "section": "4.3 Analysis of the training data distribution"}]