[{"figure_path": "CzPtBzgfae/figures/figures_8_1.jpg", "caption": "Figure 1: Non-local methods", "description": "The figure compares the performance of several non-local methods for solving logistic regression problems. The methods include QSGD, Q-RR, DIANA, DIANA-RR, and DIANA-RR-1S. The results show that DIANA-RR achieves the best convergence rate among all considered methods, which aligns with the theoretical analysis in the paper.", "section": "Experiments"}, {"figure_path": "CzPtBzgfae/figures/figures_9_1.jpg", "caption": "Figure 3: The comparison of the proposed methods (Q-NASTYA, DIANA-NASTYA, Q-RR, DIANA-RR), DIANA-RR-1S (a modification of DIANA-RR), and existing baselines (QSGD, DIANA, FedCOM, FedPAQ). All methods use tuned stepsizes and the Rand-k compressor.", "description": "The figure compares several methods for distributed optimization using gradient compression and random reshuffling.  It shows the convergence speed (f(x) - f(x*)) of the proposed algorithms Q-NASTYA, DIANA-NASTYA, Q-RR, and DIANA-RR against existing baselines QSGD, DIANA, FedCOM, and FedPAQ.  The results show the performance in different settings of local steps and non-local steps (local computation vs global communication).  DIANA-RR-1S is a memory-optimized version of DIANA-RR. All methods use tuned stepsizes and the Rand-k compression operator.", "section": "Experiments"}, {"figure_path": "CzPtBzgfae/figures/figures_20_1.jpg", "caption": "Figure 5: Non-local methods", "description": "The figure compares the performance of several methods for non-local methods across three datasets (mushrooms, w8a, a9a) with different random reshuffling parameters.  The methods include QSGD, Q-RR, DIANA, DIANA-RR, and DIANA-RR-1S. The y-axis represents the functional suboptimality, while the x-axis represents the number of data passes.  The plot shows how each method converges over time on each dataset.  The plot provides a visual comparison of the algorithms, showing the convergence rates of the different methods and how the different datasets affect their performance.", "section": "Experiments"}, {"figure_path": "CzPtBzgfae/figures/figures_22_1.jpg", "caption": "Figure 8: The comparison of the proposed variance-reduced DIANA-RR and baselines DIANA, EF21-SGD. All algorithms use theoretical step-sizes, Rand-k compressor, number of workers is 20.", "description": "This figure compares the performance of DIANA-RR with two other algorithms, DIANA and EF21-SGD, on two datasets (mushrooms and a9a) using a Rand-k compressor with a compression ratio of k/d \u2248 0.02.  The x-axis shows the number of data passes, while the y-axis represents the functional suboptimality (f(x) - f*).  The results illustrate that DIANA-RR achieves faster convergence compared to the baselines.", "section": "B.1.3 Experiment 3: Comparison of DIANA-RR with EF21 and DIANA"}, {"figure_path": "CzPtBzgfae/figures/figures_22_2.jpg", "caption": "Figure 3: The comparison of the proposed methods (Q-NASTYA, DIANA-NASTYA, Q-RR, DIANA-RR), DIANA-RR-1S (a modification of DIANA-RR), and existing baselines (QSGD, DIANA, FedCOM, FedPAQ). All methods use tuned stepsizes and the Rand-k compressor.", "description": "This figure compares the performance of several algorithms for distributed optimization problems, including those proposed in the paper (Q-NASTYA, DIANA-NASTYA, Q-RR, DIANA-RR, DIANA-RR-1S) and existing methods (QSGD, DIANA, FedCOM, FedPAQ). The algorithms are evaluated based on their convergence rate, which is measured by the decrease in the loss function value over the training process. All algorithms use tuned stepsizes and the same compression operator (Rand-k).  The results illustrate the effectiveness of the proposed algorithms, particularly DIANA-RR, which achieves the best convergence rate.", "section": "Experiments"}, {"figure_path": "CzPtBzgfae/figures/figures_24_1.jpg", "caption": "Figure 11: Comparison of QSGD and Q-RR in the training of ResNet-18 on CIFAR-10, with n = 10 workers. Here (a) and (d) show Top-1 accuracy on test set, (b) and (e) \u2013 norm of full gradient on the train set, (c) and (f) \u2013 loss function value on the train set. Stepsizes and decay shift has been tuned from Sset and set based on minimum achievable value of loss function on the train set.", "description": "This figure compares the performance of QSGD and Q-RR on the ResNet-18 model trained on the CIFAR-10 dataset using 10 workers.  It displays Top-1 accuracy on the test set, the norm of the full gradient on the training set, and the loss function value on the training set. The step sizes and decay shift were determined based on minimizing the loss function on the training set. The figure shows the results across various stages of training and illustrates the difference in performance between the two algorithms.", "section": "B.2 Training Deep Neural Network model: ResNet-18 on CIFAR-10"}, {"figure_path": "CzPtBzgfae/figures/figures_25_1.jpg", "caption": "Figure 2: Local methods", "description": "The figure compares the performance of local methods: Q-NASTYA, DIANA-NASTYA, FedCOM, and FedPAQ.  The results show the training progress (f(x) - f(x*)) over data passes for three datasets: mushrooms, w8a, and a9a.  Each dataset is tested with a different random reshuffling parameter (Rand-2 or Rand-6).  The plot illustrates the convergence speed and stability of each algorithm across different datasets and reshuffling schemes. DIANA-NASTYA generally outperforms the other methods, suggesting the effectiveness of combining the DIANA technique with local steps and random reshuffling.", "section": "Experiments"}, {"figure_path": "CzPtBzgfae/figures/figures_27_1.jpg", "caption": "Figure 11: Comparison of QSGD and Q-RR in the training of ResNet-18 on CIFAR-10, with n = 10 workers. Here (a) and (d) show Top-1 accuracy on test set, (b) and (e) \u2013 norm of full gradient on the train set, (c) and (f) \u2013 loss function value on the train set. Stepsizes and decay shift has been tuned from Sset and set based on minimum achievable value of loss function on the train set.", "description": "The figure compares the performance of QSGD and Q-RR algorithms on the ResNet-18 model trained on CIFAR-10 dataset. The comparison is made across three metrics: Top-1 accuracy on the test set, the norm of the full gradient on the training set, and the loss function value on the training set. The step sizes and decay shift for both algorithms were tuned based on minimizing the loss function on the training set. The results suggest that Q-RR achieves a lower loss function value and a slightly higher Top-1 accuracy compared to QSGD, indicating that Q-RR might be slightly better in terms of generalization.", "section": "B.2 Training Deep Neural Network model: ResNet-18 on CIFAR-10"}, {"figure_path": "CzPtBzgfae/figures/figures_27_2.jpg", "caption": "Figure 1: Non-local methods", "description": "The figure shows the comparison of the proposed methods (Q-NASTYA, DIANA-NASTYA, Q-RR, DIANA-RR), DIANA-RR-1S (a modification of DIANA-RR), and existing baselines (QSGD, DIANA, FedCOM, FedPAQ) for three different datasets (mushrooms, w8a, a9a).  Each plot displays the training loss, f(x) - f(x*), over the number of data passes.  The results illustrate the performance of these methods in a setting without local steps, showcasing their communication efficiency and convergence speed.", "section": "Experiments"}]