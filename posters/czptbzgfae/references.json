{"references": [{"fullname_first_author": "Konstantin Mishchenko", "paper_title": "Random reshuffling: Simple analysis with vast improvements", "publication_date": "2020-12-06", "reason": "This paper introduces the Random Reshuffling (RR) method and provides a comprehensive analysis, which is a fundamental concept in the current work."}, {"fullname_first_author": "Dan Alistarh", "paper_title": "QSGD: Communication-efficient SGD via gradient quantization and encoding", "publication_date": "2017-12-06", "reason": "This paper introduces the Quantized Stochastic Gradient Descent (QSGD) method, a key technique for gradient compression that the current work builds upon."}, {"fullname_first_author": "Konstantin Mishchenko", "paper_title": "Distributed learning with compressed gradient differences", "publication_date": "2019-01-01", "reason": "This paper introduces the DIANA algorithm, which is a variance reduction method used in the current work for improving communication compression."}, {"fullname_first_author": "Grigory Malinovsky", "paper_title": "Federated random reshuffling with compression and variance reduction", "publication_date": "2022-05-01", "reason": "This paper combines random reshuffling with communication compression and variance reduction in the federated learning setting, which is highly relevant to the current work."}, {"fullname_first_author": "Eduard Gorbunov", "paper_title": "Local SGD: Unified theory and new efficient methods", "publication_date": "2021-04-13", "reason": "This paper provides a unified analysis of Local SGD methods, which are closely related to the federated learning algorithms considered in the current work."}]}