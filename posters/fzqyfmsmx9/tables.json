[{"figure_path": "FZQYfmsmX9/tables/tables_6_1.jpg", "caption": "Table 1: LAIF with preference data responses sampled from a different model than the base model being fine-tuned. We find that the final performance after fine-tuning is affected more by the choice of the base LLM, as Mistral 7B reaches a similar performance when fine-tuning on preferences over Llama 7B responses, whereas Llama7B does not improve significantly when trained on preferences over responses generated by Mistral 7B.", "description": "This table presents the results of an experiment designed to investigate the impact of preference data source on the performance of LLMs fine-tuned with learning from AI feedback (LAIF). Two base LLMs, Llama 7B and Mistral 7B, were fine-tuned using LAIF with preference data generated by either Llama 7B or Mistral 7B. The results show that the performance after fine-tuning is strongly influenced by the base LLM, rather than the source of preference data. Mistral 7B achieves similar performance when trained on Llama 7B preferences, while Llama 7B does not improve significantly when trained on Mistral 7B preferences.", "section": "5 Possible Mechanistic Explanations for the Ineffectiveness of LAIF"}, {"figure_path": "FZQYfmsmX9/tables/tables_18_1.jpg", "caption": "Table 2: AlpacaEval score delta between the SFT and the best checkpoint according to the validation reward score for various configurations tested.", "description": "This table presents the results of AlpacaEval, a metric used to evaluate instruction-following capabilities of language models. It shows the difference in AlpacaEval scores between a model fine-tuned using supervised fine-tuning (SFT) and the best performing model obtained through Proximal Policy Optimization (PPO), a reinforcement learning method. The table includes various hyperparameter configurations for PPO. The goal is to understand how different hyperparameter settings in the PPO algorithm affect the final model performance when compared to the baseline SFT model.", "section": "4.2 Fine-tuning under Different Target Distributions"}]