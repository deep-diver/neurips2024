[{"figure_path": "FZQYfmsmX9/figures/figures_1_1.jpg", "caption": "Figure 1: Supervised fine-tuning (SFT) on strong teachers can accounts for improvements from learning from AI feedback (LAIF). LAIF from strong models such as GPT-4 can result in substantially better instruction-following LLMs than supervised SFT alone on popular datasets such as ShareGPT [Chiang et al., 2023] constructed using GPT-3.5 completions (GPT-3.5 SFT + GPT-4 AIF). However, simply performing SFT on completions from GPT-4 can result in a better model (GPT-4 SFT), suggesting that improvement in performance from LAIF is partly because the default ShareGPT completions are from a weak teacher (GPT-3.5). Furthermore, LAIF (GPT-4 SFT + GPT-4 AIF) does not result in a significantly better model compared to GPT-4 SFT alone.", "description": "This figure compares the performance of different model fine-tuning methods using the ShareGPT dataset.  It shows that using a stronger teacher model for supervised fine-tuning (SFT) can yield results comparable to or even better than the more complex learning from AI feedback (LAIF) approach.  The improvement seen in LAIF using GPT-4 as a critic seems to be largely due to the weaker GPT-3.5 teacher model used in the original SFT dataset.  Using GPT-4 as both teacher and critic in LAIF doesn't provide significantly better results than SFT with GPT-4 alone.", "section": "4 Experiments"}, {"figure_path": "FZQYfmsmX9/figures/figures_4_1.jpg", "caption": "Figure 2: SFT can perform comparably or better than LAIF across various model sizes and classes. The same set of prompts are used for all three settings and for each model, and the oracle LLM either generates a completion (for SFT) or a preference label on two completions (for LAIF). For LAIF, SFT is an important precursor, so we SFT on 10% of the total prompts, and LAIF is done on the remaining 90%. For the other settings, the full set of prompts are used for SFT. While LAIF improves the performance compared to SFT on the default ShareGPT completions, SFT on GPT-4 completions consistently matches or outperforms LAIF.", "description": "This figure compares the performance of Supervised Fine-Tuning (SFT) and Learning from AI Feedback (LAIF) across different language models of varying sizes.  It demonstrates that while LAIF sometimes improves upon the default SFT (using GPT-3.5 generated completions), simply using SFT with higher-quality completions from GPT-4 consistently matches or surpasses LAIF's performance. This highlights that LAIF's apparent gains may stem from compensating for weaker initial SFT data rather than inherent superiority.", "section": "4.2 Fine-tuning under Different Target Distributions"}, {"figure_path": "FZQYfmsmX9/figures/figures_5_1.jpg", "caption": "Figure 3: We make a similar observation that SFT performs comparably to LAIF when Claude as is used as an oracle. LAIF with AI feedback from does not significantly outperform SFT on Claude completions, and the performance improvement from LAIF is explained by the use of a weaker SFT target distribution (GPT-3.5). The results for effectiveness of SFT may apply more generally to strong LLMs beyond GPT-4.", "description": "This figure compares the performance of supervised fine-tuning (SFT) and learning from AI feedback (LAIF) using different strong language models (Claude and GPT-4) as critics.  It shows that when using Claude as the critic, SFT using Claude as the teacher model performs comparably to or better than LAIF. This suggests that the performance gains from LAIF in previous experiments were largely due to using a weaker teacher (GPT-3.5) during the SFT stage. The results imply that the benefits of the complex LAIF pipeline may be overestimated, and that simply using SFT with high-quality teacher completions can be just as effective, or even better.", "section": "4.2 Fine-tuning under Different Target Distributions"}, {"figure_path": "FZQYfmsmX9/figures/figures_5_2.jpg", "caption": "Figure 2: SFT can perform comparably or better than LAIF across various model sizes and classes. The same set of prompts are used for all three settings and for each model, and the oracle LLM either generates a completion (for SFT) or a preference label on two completions (for LAIF). For LAIF, SFT is an important precursor, so we SFT on 10% of the total prompts, and LAIF is done on the remaining 90%. For the other settings, the full set of prompts are used for SFT. While LAIF improves the performance compared to SFT on the default ShareGPT completions, SFT on GPT-4 completions consistently matches or outperforms LAIF.", "description": "This figure compares the performance of supervised fine-tuning (SFT) and learning from AI feedback (LAIF) on various language models. It shows that using a stronger teacher model for SFT can lead to performance comparable to or even better than LAIF, especially when using GPT-4 as the teacher model. The improvement from LAIF is mainly due to using a weaker teacher model (GPT-3.5) for SFT in the default ShareGPT dataset.  The figure demonstrates that aligning models via SFT using strong teacher model completions can achieve similar or even superior performance compared to LAIF.", "section": "4.2 Fine-tuning under Different Target Distributions"}, {"figure_path": "FZQYfmsmX9/figures/figures_7_1.jpg", "caption": "Figure 5: Fine-tuning a weak student with LAIF underperforms relative to SFT on a strong teacher in a synthetic bandit problem with 100 possible actions. We assume the completions from the teacher (black) rank relatively highly (centered around 80th percentile). The improvements in LAIF (yellow) are limited because the actions sampled for labeling preferences are tied to the initial student distribution (blue). In this scenario, where the teacher distribution is sufficiently stronger than the student distribution, simple SFT on the teacher's samples (red) may be more effective than LAIF on samples from a weak student. The actions are sorted by their true reward, which is used to generate a teacher labeled preference dataset over samples from the student.", "description": "This figure illustrates a simplified bandit problem comparing supervised fine-tuning (SFT) and reinforcement learning from AI feedback (RLAIF).  It demonstrates that when the teacher model provides high-quality data, SFT can outperform RLAIF. The limitations of RLAIF stem from using a weak student model for data generation, which constrains exploration and improvement.", "section": "6 Going Forward"}, {"figure_path": "FZQYfmsmX9/figures/figures_15_1.jpg", "caption": "Figure 6: The performance improvements from increasing the number of training points SFT on 100% of the training prompts yields minimal improvements over SFT on 10% of the training prompts. Hence, for our LAIF setting, we first perform SFT on only 10% of the training examples, and we use the remaining for LAIF.", "description": "This figure shows the impact of increasing the number of training examples used for supervised fine-tuning (SFT) on the performance of language models. It demonstrates that using a larger number of training examples doesn't improve performance significantly once a certain amount of data has been used. This finding supports the strategy used in the paper's LAIF experiments where only 10% of data was used for SFT, reserving the remaining 90% for AI feedback.", "section": "4.1 LAIF Setup and Data Scaling for SFT"}, {"figure_path": "FZQYfmsmX9/figures/figures_15_2.jpg", "caption": "Figure 2: SFT can perform comparably or better than LAIF across various model sizes and classes. The same set of prompts are used for all three settings and for each model, and the oracle LLM either generates a completion (for SFT) or a preference label on two completions (for LAIF). For LAIF, SFT is an important precursor, so we SFT on 10% of the total prompts, and LAIF is done on the remaining 90%. For the other settings, the full set of prompts are used for SFT. While LAIF improves the performance compared to SFT on the default ShareGPT completions, SFT on GPT-4 completions consistently matches or outperforms LAIF.", "description": "This figure compares the performance of supervised fine-tuning (SFT) and learning from AI feedback (LAIF) across different language models.  It shows that while LAIF often improves upon the baseline SFT using the default ShareGPT dataset (trained on GPT-3.5 completions), simply using SFT with GPT-4 completions consistently performs as well or better than LAIF. This highlights the importance of the quality of the training data (the teacher model) for instruction following in LLMs.", "section": "4.2 Fine-tuning under Different Target Distributions"}, {"figure_path": "FZQYfmsmX9/figures/figures_16_1.jpg", "caption": "Figure 8: The highest point for each LR curve is highlighted. Our best reward model is the model trained with lr=5e-5, which reaches an accuracy of 84.18%.", "description": "This figure shows the reward modeling evaluation accuracy across steps for different learning rates during the reward model training in the reinforcement learning from AI feedback (RLAIF) pipeline.  The x-axis represents the training step, and the y-axis shows the reward modeling evaluation accuracy.  Different colored lines represent different learning rates used in the training process. The figure highlights that the learning rate of 5e-5 achieved the highest reward modeling accuracy (84.18%).", "section": "4 Experiments"}, {"figure_path": "FZQYfmsmX9/figures/figures_17_1.jpg", "caption": "Figure 9: Train reward over PPO iterations.", "description": "This figure shows the training reward score evolution during the course of the PPO training. It displays the train reward score over steps for various hyperparameter combinations: init_kl_coeff and lr. The plot helps visualize how the reward score changes across different iterations of PPO training under different hyperparameter settings.", "section": "DPPO-based RLAIF Experiments"}, {"figure_path": "FZQYfmsmX9/figures/figures_17_2.jpg", "caption": "Figure 10: Validation reward score over PPO iterations.", "description": "This figure shows the validation reward score over steps during PPO training in the RLAIF experiments.  Different lines represent different hyperparameter combinations for the initial KL coefficient and learning rate.  The x-axis represents the number of steps in the PPO training process, and the y-axis represents the validation reward score.  The goal is to observe the trend of the validation reward score during training to identify the best hyperparameter settings.", "section": "4.2 Fine-tuning under Different Target Distributions"}]