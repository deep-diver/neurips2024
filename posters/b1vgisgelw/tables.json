[{"figure_path": "B1vGiSgELw/tables/tables_4_1.jpg", "caption": "Table 1: Comparison with state-of-the-art methods on 11 vision-language benchmarks. Our model (MQT-LLAVA) with up to 256 tokens achieves on par or better than LLaVA-1.5 performance across 11 benchmarks, outperforming it on 6 of 11 benchmarks. #Tokens is the number of visual tokens used during inference. Avg is the normalized average across 11 benchmarks, out of 100. Benchmark names are abbreviated for brevity: SQA\u00b9: ScienceQA-IMG, MMEP: MME Perception, MME: MME Cognition, MMB: MMBench, LLaVAW: LLaVA-Bench (In-the-Wild). *The training images of the datasets are observed during training.", "description": "This table compares the performance of MQT-LLAVA with other state-of-the-art vision-language models across 11 benchmark datasets.  It shows the average score across the benchmarks for each model, along with the number of visual tokens used and other key details.  MQT-LLAVA demonstrates comparable or better performance than LLaVA-1.5 with significantly fewer tokens, highlighting its efficiency.", "section": "3.2 Main Results"}, {"figure_path": "B1vGiSgELw/tables/tables_7_1.jpg", "caption": "Table 1: Comparison with state-of-the-art methods on 11 vision-language benchmarks. Our model (MQT-LLAVA) with up to 256 tokens achieves on par or better than LLaVA-1.5 performance across 11 benchmarks, outperforming it on 6 of 11 benchmarks. We mark the best performance in bold and the second-best underlined. #Tokens is the number of visual tokens used during inference. Avg is the normalized average across 11 benchmarks, out of 100. Benchmark names are abbreviated for brevity: SQA\u00b9: ScienceQA-IMG, MMEP: MME Perception, MME: MME Cognition, MMB: MMBench, LLaVAW: LLaVA-Bench (In-the-Wild). *The training images of the datasets are observed during training.", "description": "This table compares the performance of the proposed model, MQT-LLAVA, with other state-of-the-art vision-language models across 11 benchmark datasets.  The table shows the average performance across these benchmarks, broken down by the number of visual tokens used (2, 4, 8, 16, 36, 64, 144, 256).  It demonstrates that MQT-LLAVA achieves comparable or better performance than LLaVA-1.5 while using significantly fewer visual tokens.", "section": "3.2 Main Results"}, {"figure_path": "B1vGiSgELw/tables/tables_14_1.jpg", "caption": "Table 1: Comparison with state-of-the-art methods on 11 vision-language benchmarks. Our model (MQT-LLAVA) with up to 256 tokens achieves on par or better than LLaVA-1.5 performance across 11 benchmarks, outperforming it on 6 of 11 benchmarks. #Tokens is the number of visual tokens used during inference. Avg is the normalized average across 11 benchmarks, out of 100. Benchmark names are abbreviated for brevity: SQA\u00b9: ScienceQA-IMG, MMEP: MME Perception, MME: MME Cognition, MMB: MMBench, LLaVAW: LLaVA-Bench (In-the-Wild). *The training images of the datasets are observed during training.", "description": "This table compares the performance of the proposed model, MQT-LLAVA, with other state-of-the-art models on 11 vision-language benchmarks.  It shows that MQT-LLAVA, even with significantly fewer visual tokens (up to 256 compared to LLaVA-1.5's 576), achieves comparable or better performance across most benchmarks. The table also highlights the performance trade-offs with different numbers of visual tokens, demonstrating the model's flexibility.", "section": "3.2 Main Results"}]