[{"figure_path": "J2wOOtkBx0/figures/figures_1_1.jpg", "caption": "Figure 1: Box refinement through denoising steps. We visualize the correction of a noisy prediction, shown in yellow, using DiffuBox. The ground truth box is visualized in green for reference. Boxes being refined are colored blue based on timestep. The output is refined iteratively though the denoising steps, resulting in the final, corrected output of our method.", "description": "This figure shows how DiffuBox refines a noisy bounding box prediction iteratively.  The initial noisy prediction (yellow) is progressively refined (blue boxes) through a series of denoising steps guided by the surrounding LiDAR points and the ground truth box (green). The final refined output is a more accurate bounding box, demonstrating the method's ability to correct localization errors.", "section": "1 Introduction"}, {"figure_path": "J2wOOtkBx0/figures/figures_3_1.jpg", "caption": "Figure 2: Example Car objects converted into normalized box view (NBV). Foreground/background points are marked in black/gray, respectively for better visualization. Foreground LiDAR points distributing tightly within a [-1,1]\u00b3 NBV cube is a domain-consistent sign for good localization.", "description": "This figure shows the effect of converting car objects from a global scene view to a normalized box view (NBV). The conversion eliminates the size prior by transforming the bounding box into a [-1,1]\u00b3 cube, making the point cloud distribution relative to the box instead of in absolute measure.  The figure highlights that foreground LiDAR points clustered tightly inside the [-1,1]\u00b3 NBV cube are indicative of good bounding box localization, regardless of the domain.", "section": "3. Method"}, {"figure_path": "J2wOOtkBx0/figures/figures_7_1.jpg", "caption": "Figure 3: Illustration of 3D object detection on Lyft/Ithaca365 before and after DiffuBox's refinement. We visualize detections from an out-of-domain PointRCNN on four scenes from each dataset. We color the ground truth boxes in green, the detector outputs in yellow, and DiffuBox's refinements in blue. The out-of-domain detector sometimes produces false positives or boxes with incorrect shape or alignment. DiffuBox effectively improves the wrong or inaccurate boxes, while making little change to the accurate boxes.", "description": "This figure shows a comparison of 3D object detection results on the Lyft and Ithaca365 datasets before and after applying the DiffuBox refinement method. The top row displays point cloud visualizations, while the bottom row shows the corresponding image perspectives.  Ground truth bounding boxes are green, initial detector outputs are yellow, and DiffuBox refined boxes are light blue.  The figure highlights how DiffuBox corrects mislocalized or mis-shaped bounding boxes while leaving correctly localized ones largely unchanged.", "section": "4.3 Qualitative Results"}, {"figure_path": "J2wOOtkBx0/figures/figures_7_2.jpg", "caption": "Figure 4: Comparison of bounding box quality before and after refinement with DiffuBox. We report the distribution of Intersection over Union (IoU) with ground-truth labels from the Lyft dataset. The unrefined predictions are from an un-adapted Point-RCNN model trained on KITTI. We show that DiffuBox leads to significant improvement in bounding box localization.", "description": "This figure shows the distribution of Intersection over Union (IoU) scores for bounding boxes before and after refinement using DiffuBox. The data is from the Lyft dataset, and the unrefined predictions are from a PointRCNN model trained only on KITTI data. The significant shift towards higher IoU scores after DiffuBox refinement demonstrates the method's effectiveness in improving the accuracy of bounding box localization.", "section": "4 Experiments"}, {"figure_path": "J2wOOtkBx0/figures/figures_8_1.jpg", "caption": "Figure 5: MAP vs. Number of Diffusion Steps. We report the BEV (left) and 3D (right) mAP @ IoU 0.7 for the setting of KITTI \u2192 Lyft Cars and PointRCNN detector.", "description": "This figure shows the relationship between the number of diffusion steps used in DiffuBox and the resulting mean Average Precision (mAP) at Intersection over Union (IoU) threshold of 0.7.  The left panel shows the Bird's Eye View (BEV) mAP, while the right panel shows the 3D mAP. Both plots show mAP values for different distance ranges (0-30m, 30-50m, 50-80m, and 0-80m). The results demonstrate that the model's performance improves as the number of diffusion steps increases, eventually reaching a plateau.", "section": "4.5 Ablation Studies and Analysis"}, {"figure_path": "J2wOOtkBx0/figures/figures_9_1.jpg", "caption": "Figure 6: Recall improvement with DiffuBox. We report recall on the car class (KITTI \u2192 Lyft, PointRCNN) before and after refinement with DiffuBox.", "description": "This figure presents a recall analysis of DiffuBox's effect on detection recall.  It shows that by improving the Intersection over Union (IoU) for mislocalized detections, DiffuBox reduces the number of false negatives. This improvement is observed across various object sizes. The figure includes two subplots: (a) Recall vs. IoU and (b) Recall vs. Object volume.", "section": "4.5 Ablation Studies and Analysis"}, {"figure_path": "J2wOOtkBx0/figures/figures_15_1.jpg", "caption": "Figure S7: Architecture overview of DiffuBox's denoiser model. The model is composed of 2 MLP layers and L transformer encoder layers, which maps 3D points to a higher dimensional space for effective self-attention.", "description": "The figure shows a detailed architecture of DiffuBox's denoising model.  It uses a combination of Multi-Layer Perceptrons (MLPs) and a Transformer encoder with L layers. The input is a set of 3D points representing the point cloud relative to a bounding box.  These points are initially processed by an MLP, then fed into the transformer encoder which uses self-attention. The output of the transformer is again passed through an MLP to generate the final output. A noise level embedding is also incorporated into the model.", "section": "S2 Implementation Details"}, {"figure_path": "J2wOOtkBx0/figures/figures_17_1.jpg", "caption": "Figure S8: Box refinement through denoising steps. We visualize the correction of a noisy prediction, shown in yellow, using DiffuBox, as well as the normalized box view. The detection output is refined iteratively though the denoising steps, resulting in the final, corrected output of our method.", "description": "The figure visualizes how DiffuBox refines a noisy bounding box prediction iteratively through denoising steps.  It shows both the global scene view and a normalized box view, which removes size bias by transforming the bounding box into a normalized cube. The yellow box represents the initial noisy prediction, blue boxes show the refinement at each step, and the green box indicates the ground truth.  The figure demonstrates how the bounding box is gradually corrected to match the ground truth.", "section": "S3.3 Additional Qualitative Results"}, {"figure_path": "J2wOOtkBx0/figures/figures_17_2.jpg", "caption": "Figure 3: Illustration of 3D object detection on Lyft/Ithaca365 before and after DiffuBox's refinement. We visualize detections from an out-of-domain PointRCNN on four scenes from each dataset. We color the ground truth boxes in green, the detector outputs in yellow, and DiffuBox's refinements in blue. The out-of-domain detector sometimes produces false positives or boxes with incorrect shape or alignment. DiffuBox effectively improves the wrong or inaccurate boxes, while making little change to the accurate boxes.", "description": "This figure shows a qualitative comparison of 3D object detection results on the Lyft and Ithaca365 datasets before and after applying the DiffuBox refinement method.  The top row displays point cloud data, while the bottom row shows the corresponding image data.  Each column represents a different scenario: no adaptation, DiffuBox applied, statistical normalization (SN), and SN with DiffuBox. Ground truth bounding boxes are green, initial detections are yellow, and DiffuBox refined boxes are blue.  The figure highlights how DiffuBox improves localization accuracy by correcting mislocalized or incorrectly shaped bounding boxes while leaving accurately placed boxes largely unchanged.", "section": "4.3 Qualitative Results"}, {"figure_path": "J2wOOtkBx0/figures/figures_18_1.jpg", "caption": "Figure 3: Illustration of 3D object detection on Lyft/Ithaca365 before and after DiffuBox's refinement. We visualize detections from an out-of-domain PointRCNN on four scenes from each dataset. We color the ground truth boxes in green, the detector outputs in yellow, and DiffuBox's refinements in blue. The out-of-domain detector sometimes produces false positives or boxes with incorrect shape or alignment. DiffuBox effectively improves the wrong or inaccurate boxes, while making little change to the accurate boxes.", "description": "This figure shows four examples of 3D object detection results on the Lyft and Ithaca365 datasets, comparing the performance of an out-of-domain PointRCNN detector with and without DiffuBox refinement.  The ground truth boxes are shown in green, the initial PointRCNN detections in yellow, and the DiffuBox-refined boxes in blue. The examples highlight how DiffuBox improves localization by correcting misaligned or incorrectly shaped boxes while leaving correctly predicted boxes largely unchanged.", "section": "4.3 Qualitative Results"}]