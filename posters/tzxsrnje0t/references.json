{"references": [{"fullname_first_author": "L\u00e9on Bottou", "paper_title": "Optimization methods for large-scale machine learning", "publication_date": "2018-01-01", "reason": "This paper provides a comprehensive overview of optimization methods crucial for understanding the context of the current research on stochastic gradient descent."}, {"fullname_first_author": "John Duchi", "paper_title": "Adaptive subgradient methods for online learning and stochastic optimization", "publication_date": "2011-01-01", "reason": "This foundational paper introduces adaptive algorithms like Adagrad, which are central to the current research on adaptive stochastic approximation."}, {"fullname_first_author": "Yurii Nesterov", "paper_title": "Random gradient-free minimization of convex functions", "publication_date": "2017-01-01", "reason": "This paper explores gradient-free methods, providing a relevant theoretical background for the study of biased gradient estimators."}, {"fullname_first_author": "Saeed Ghadimi", "paper_title": "Stochastic first-and zeroth-order methods for nonconvex stochastic programming", "publication_date": "2013-01-01", "reason": "This paper presents a non-asymptotic analysis of stochastic gradient methods with biased gradients, directly relevant to the current work."}, {"fullname_first_author": "Belhal Karimi", "paper_title": "Non-asymptotic analysis of biased stochastic approximation scheme", "publication_date": "2019-01-01", "reason": "This work provides a non-asymptotic convergence analysis for biased stochastic approximation, forming a theoretical basis for the current study."}]}