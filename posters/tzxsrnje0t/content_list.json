[{"type": "text", "text": "Non-asymptotic Analysis of Biased Adaptive Stochastic Approximation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sobihan Surendran1,2 ,\u2217 Adeline Fermanian2, Antoine Godichon-Baggioni1, Sylvain Le Corff1 ", "page_idx": 0}, {"type": "text", "text": "1Sorbonne Universit\u00e9, CNRS, Laboratoire de Probabilit\u00e9s, Statistique et Mod\u00e9lisation, Paris, France 2LOPF, Califrais\u2019 Machine Learning Lab, Paris, France ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Stochastic Gradient Descent (SGD) with adaptive steps is widely used to train deep neural networks and generative models. Most theoretical results assume that it is possible to obtain unbiased gradient estimators, which is not the case in several recent deep learning and reinforcement learning applications that use Monte Carlo methods. This paper provides a comprehensive non-asymptotic analysis of SGD with biased gradients and adaptive steps for non-convex smooth functions. Our study incorporates time-dependent bias and emphasizes the importance of controlling the bias of the gradient estimator. In particular, we establish that Adagrad, RMSProp, and AMSGRAD, an exponential moving average variant of Adam, with biased gradients, converge to critical points for smooth non-convex functions at a rate similar to existing results in the literature for the unbiased case. Finally, we provide experimental results using Variational Autoenconders (VAE) and applications to several learning frameworks that illustrate our convergence results and show how the effect of bias can be reduced by appropriate hyperparameter tuning. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Stochastic Gradient Descent (SGD) algorithms are standard methods to train statistical models based on deep architectures. Consider a general optimization problem: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\theta^{*}\\in\\operatorname*{argmin}_{\\theta\\in\\mathbb{R}^{d}}V(\\theta)\\ ,\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $V$ is the objective function. Then, Gradient Descent methods produce a sequence of parameter estimates as follows: $\\theta_{0}\\in\\mathbb{R}^{d}$ and for all $n\\in\\mathbb N$ , ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\theta_{n+1}=\\theta_{n}-\\gamma_{n+1}\\nabla V(\\theta_{n})\\ ,\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $\\nabla V$ denotes the gradient of $V$ and for all $n\\geq1$ , $\\gamma_{n}>0$ is the learning rate. In many cases, it is not possible to compute the exact gradient of the objective function, hence the introduction of vanilla Stochastic Gradient Descent, defined for all $n\\in\\mathbb N$ by: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\theta_{n+1}=\\theta_{n}-\\gamma_{n+1}\\widehat{\\nabla V}(\\theta_{n})\\ ,\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where ${\\widehat{\\nabla V}}(\\theta_{n})$ is an estimator of $\\nabla V(\\theta_{n})$ . For example, in deep learning, stochasticity emerges with the use of mini-batches. While these algorithms have been extensively studied, both theoretically and practically, see, e.g., [10], many questions remain open. In particular, most results are based on the case where the estimator\u2207V is unbiased. Although this assumption is valid in the case of vanilla SGD, it breaks down in m any common applications. For example, zeroth-order methods used to optimize black-box functions [61] in generative adversarial networks [58, 16] have access only to noisy biased realizations of the objective functions. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Furthermore, in reinforcement learning algorithms such as Q-learning [42], policy gradient [5], and temporal difference learning [8, 52, 18], gradient estimators are often obtained using a Markov chain with state-dependent transition probability. These estimators are then biased [69, 23]. Other examples of biased gradients can be found in the field of generative modeling with Markov Chain Monte Carlo (MCMC) and Sequential Monte Carlo (SMC) [34, 13]. In particular, the Importance Weighted Autoencoder (IWAE) proposed by [12], which is an extension of the standard Variational Autoencoder (VAE) [48], yields biased estimators. Finally, this is also the case in Bilevel Optimization [43, 36, 41] and Conditional Stochastic Optimization [40, 39]. ", "page_idx": 1}, {"type": "text", "text": "Moreover, in practical applications, vanilla SGD shows difficulties in calibrating the step sequences. Therefore, modern variants of SGD employ adaptive steps that use past stochastic gradients or Hessians to avoid saddle points and deal with ill-conditioned problems. The idea of adaptive steps was first proposed in the online learning literature by [4] and later adopted in stochastic optimization, with the Adagrad algorithm of [27]. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we give non-asymptotic convergence guarantees for modern variants of SGD where both the estimators are biased and the steps are adaptive. To our knowledge, existing results consider either adaptive steps but unbiased estimators [27, 77, 67, 74, 19], or biased estimators with non-adaptive steps [70, 44, 2, 22, 21]. ", "page_idx": 1}, {"type": "text", "text": "More precisely, our contributions are summarized as follows. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We provide convergence guarantees for the Biased Adaptive Stochastic Approximation framework, under weak assumptions on the bias. To the best of our knowledge, these are the first convergence results to incorporate adaptive steps in biased Stochastic Approximation.   \n\u2022 In particular, we establish that Adagrad, RMSProp, and AMSGRAD, an exponential moving average variant of Adam, with a biased gradient, converge\u221a to a critical point for non-convex smooth functions with a convergence rate of $\\mathcal{O}(\\log n\\bar{/}\\sqrt{n}+b_{n})$ , where $b_{n}$ is related to the bias at iteration $n$ . However, we achieve an improved linear convergence rate with the Polyak-\u0141ojasiewicz (PL) condition.   \n\u2022 Finally, we show how our theoretical results apply to several applications with biased gradients. In particular, we show that our hypotheses hold for Stochastic Bilevel Optimization and Conditional Stochastic Optimization, but also for Self-Normalized Importance Sampling estimators or Coordinate Sampling. We also propose a first non-asymptotic bound on the bias of IWAE, which allows us to illustrate through several experiments the effect of bias on the convergence of the optimization, and to show how this effect can be reduced by an appropriate choice of hyperparameters. ", "page_idx": 1}, {"type": "text", "text": "Organization of the paper. In Section 2, we introduce the setting of the paper and relevant related works. In Section 3, we present the Adaptive Stochastic Approximation framework and the main assumptions. In Section 4, we present our principal results, i.e., convergence rates for the risk when the PL condition is assumed, and on the gradient norm without this hypothesis. We illustrate our results in Section 5. All proofs are postponed to the appendix. ", "page_idx": 1}, {"type": "text", "text": "2 Setting and Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Stochastic Approximation. Stochastic Approximation (SA) methods go far beyond SGD. They consist of sequential algorithms designed to find the zeros of a function when only noisy observations are available. Indeed, [68] introduced the Stochastic Approximation algorithm as an iterative recursive algorithm to solve the following integration equation: ", "page_idx": 1}, {"type": "equation", "text": "$$\nh(\\theta)=\\mathbb{E}_{\\pi}\\left[H_{\\theta}(X)\\right]=\\int_{\\times}H_{\\theta}(x)\\pi(x)\\mathrm{d}x=0\\;,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $h$ is the mean field function, $X$ is a random variable taking values in a measurable space $(\\mathsf{X},{\\mathcal{X}})$ , and $\\mathbb{E}_{\\pi}$ is the expectation under the distribution $\\pi$ . In this context, $H_{\\theta}$ can be any arbitrary function. If $H_{\\theta}(X)$ is an unbiased estimator of the gradient of the objective function, then $\\dot{h}(\\theta)=\\dot{\\nabla}V(\\theta)$ . As ", "page_idx": 1}, {"type": "text", "text": "a result, the minimization problem (1) is then equivalent to solving problem (2), and we can note that SGD is a specific instance of SA. SA methods are then defined as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\theta_{n+1}=\\theta_{n}-\\gamma_{n+1}H_{\\theta_{n}}\\left(X_{n+1}\\right)\\ ,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the term $H_{\\theta_{n}}\\left(X_{n+1}\\right)$ is the $n$ -th stochastic update, also known as the drift term, and is a potentially biased estimator of $\\nabla V(\\theta_{n})$ . It depends on a random variable $X_{n+1}$ which takes its values in $(\\mathsf{X},{\\mathcal{X}})$ . In machine learning, $V$ typically represents the theoretical risk, $\\theta$ the model parameters, and $X_{n+1}$ the data. ", "page_idx": 2}, {"type": "text", "text": "Adaptive Stochastic Gradient Descent. SGD can be traced back to [68], and its averaged counterpart was proposed by [66]. The non-asymptotic analysis of SGD in both convex and strong convex cases can be found in [59]. [32] prove the convergence of a random iterate of SGD for nonconvex smooth functions, which was already\u221a suggested by the results of [9]. They show that SGD with constant or decreasing stepsize $\\gamma_{n}=1/\\sqrt{n}$ converges to a stationary point of a non-convex smooth function $V$ at a rate of $\\mathcal{O}(1/\\sqrt{n})$ where $n$ is the number of iterations. ", "page_idx": 2}, {"type": "text", "text": "Most adaptive first-order methods, such as Adam [47], Adadelta [78], RMSProp [72], and NADA [25], are based on the blueprint provided by the Adagrad family of algorithms. The first known work on adaptive steps for non-convex stochastic optimization, in the asymptotic case, was presented by [50]. [74] pro\u221aved that Adagrad converges to a critical point for non-convex objectives at a rate of ${\\mathcal{O}}(\\log n/{\\sqrt{n}})$ when using a scalar adaptive step. In addition, [79] extended this proof to multidimensional settings. More recently, [19] focused on the convergence rates for Adagrad and Adam. Furthermore, several modified versions of Adam have been proposed, such as AMSGRAD [77] and YOGI [67]. ", "page_idx": 2}, {"type": "text", "text": "Biased Stochastic Approximation. The asymptotic results of Biased SA have been studied by [70]. The non-asymptotic analysis can be found in the reinforcement learning literature, especially in the context of temporal difference (TD) learning, as explored by [8, 52, 18]. The case of non-convex smooth functions has been studied b\u221ay [44]. The authors establish convergence results for the mean field function at a rate of $O(\\log n/\\dot{\\sqrt{n}}+b)$ , where $b$ corresponds to the bias and $n$ to the number of iterations. For strongly convex functions, the convergence of SGD with biased gradients can be found in [2], specifically addressing the case of Martingale noise with a constant step size. ", "page_idx": 2}, {"type": "text", "text": "[46, 21] introduce a novel assumption, known as \u201cExpected Smoothness\u201d, which is the weakest assumption compared to the existing literature on biased SGD that we extend to cover the adaptive case. The authors provide convergence results in the case of non-convex smooth functions. Convergence results with assumptions on the control of bias and MSE can be found in [56, 22]. Applications of biased gradients can be found in Bilevel Optimization [43, 36, 41] and Conditional Stochastic Optimization [40, 39]. Moreover, biased gradients are also used in various other applications [38, 54, 6, 56]. Finally, [3] studied convergence results of biased gradients with Adagrad in the Markov chain case, focusing on the norm of the gradient of the Moreau envelope while assuming the boundedness of the objective function. ", "page_idx": 2}, {"type": "text", "text": "Our analysis provides non-asymptotic results in a more general setting, for a wide variety of objective functions and adaptive algorithms and treating both the Martingale and Markov chain cases. ", "page_idx": 2}, {"type": "text", "text": "3 Adaptive Stochastic Approximation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Framework ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Consider the optimization problem (1) where the objective function $V$ is assumed to be differentiable. In this paper, we focus on the following SA algorithm with adaptive steps: $\\theta_{0}\\in\\mathbb{R}^{d}$ and for all $n\\in\\mathbb N$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\theta_{n+1}=\\theta_{n}-\\gamma_{n+1}A_{n}H_{\\theta_{n}}\\left(X_{n+1}\\right)\\;,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\gamma_{n+1}>0$ and $A_{n}$ is a sequence of symmetric and positive definite matrices. In a context of biased gradient estimates, choosing ", "page_idx": 2}, {"type": "equation", "text": "$$\nA_{n}=\\left[\\delta I_{d}+\\left(\\frac{1}{n+1}\\sum_{k=0}^{n}H_{\\theta_{k}}(X_{k+1})H_{\\theta_{k}}(X_{k+1})^{\\top}\\right)\\right]^{-1/2}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "can be assimilated to the full Adagrad algorithm [27]. However, computing the square root of the inverse becomes expensive in high dimensions, so in practice, Adagrad is often used with diagonal ", "page_idx": 2}, {"type": "text", "text": "matrices. This approach has been shown to be particularly effective in sparse optimization settings. Denoting by $\\operatorname{Diag}(A)$ the matrix formed with the diagonal terms of $A$ and setting all other terms to $0$ , Adagrad with diagonal matrices is defined in our context as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nA_{n}=\\Big[\\delta I_{d}+\\mathrm{Diag}\\big(\\bar{H}_{n}(X_{1:n+1},\\theta_{0:n})\\big)\\Big]^{-1/2}\\;,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\bar{H}_{n}(X_{1:n+1},\\theta_{0:n})=\\frac{1}{n+1}\\sum_{k=0}^{n}H_{\\theta_{k}}(X_{k+1})H_{\\theta_{k}}(X_{k+1})^{\\top}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In RMSProp [72], $\\bar{H}_{n}(X_{1:n+1},\\theta_{0:n})$ in (4) is an exponential moving average of the past squared gradients, defined by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\bar{H}_{n}(X_{1:n+1},\\theta_{0:n})=(1-\\rho)\\sum_{k=0}^{n}\\rho^{n-k}H_{\\theta_{k}}(X_{k+1})H_{\\theta_{k}}(X_{k+1})^{\\top},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\rho$ is the moving average parameter. Furthermore, when $A_{n}$ is a recursive estimate of the inverse Hessian, it corresponds to the Stochastic Newton algorithm [11]. ", "page_idx": 3}, {"type": "text", "text": "3.2 Assumptions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Consider the following assumptions. ", "page_idx": 3}, {"type": "text", "text": "H1 There exists a constant $\\mu>0$ such that for all $\\theta\\in\\ensuremath{\\mathbb{R}}^{d}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n2\\mu\\big(V(\\theta)-V\\left(\\theta^{*}\\right)\\big)\\leq\\|\\nabla V(\\theta)\\|^{2}\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "H1 corresponds to the Polyak-\u0141ojasiewicz condition, which is weaker than strong convexity and remains satisfied even when the function is non-convex. It ensures uniqueness of the minimizer $\\theta^{*}$ . The PL condition has been extensively studied theoretically [45] and has been verified empirically in many applications, such as over-parameterized deep networks [26] and Linear Quadratic Regulator models [29]. ", "page_idx": 3}, {"type": "text", "text": "H2 The objective function $V$ is $L$ -smooth. For all $(\\theta,\\theta^{\\prime})\\in\\mathbb R^{d}\\times\\mathbb R^{d},$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla V(\\theta)-\\nabla V\\left(\\theta^{\\prime}\\right)\\|\\leq L\\left\\|\\theta-\\theta^{\\prime}\\right\\|\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This assumption is crucial to obtain our convergence rate and is very common see, e.g., [59, 10]. Under this assumption, for all $(\\theta,\\theta^{\\prime})\\in\\mathbb{R}^{d}\\times\\mathbb{R}^{d}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\nV\\left(\\theta\\right)\\le V\\left(\\theta^{\\prime}\\right)+\\left\\langle\\nabla V\\left(\\theta^{\\prime}\\right),\\theta-\\theta^{\\prime}\\right\\rangle+\\frac{L}{2}\\left\\|\\theta-\\theta^{\\prime}\\right\\|^{2}\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "H3 $(i)$ Biased Gradients: There exist two non-increasing positive sequences $(\\lambda_{n})_{n\\geq1}$ and $(r_{n})_{n\\geq1}$ such that for all $n\\in\\mathbb N$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\big[\\left\\langle\\nabla V\\left(\\theta_{n}\\right),A_{n}H_{\\theta_{n}}\\left(X_{n+1}\\right)\\right\\rangle\\big]\\ge\\lambda_{n+1}\\left(\\mathbb{E}\\left[\\|\\nabla V(\\theta_{n})\\|^{2}\\right]-r_{n+1}\\right)\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$(i i)$ Expected Smoothness: there exists a non-increasing non-negative sequence $(\\sigma_{n}^{2})_{n\\geq1}$ , and positive constants $\\tilde{\\sigma}_{1},\\tilde{\\sigma}_{2}$ such that for all $n\\in\\mathbb N$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\big[\\|H_{\\theta_{n}}(X_{n+1})\\|^{2}\\big]\\leq\\sigma_{n}^{2}+\\tilde{\\sigma}_{1}\\mathbb{E}\\big[\\|\\nabla V(\\theta_{n})\\|^{2}\\big]+\\tilde{\\sigma}_{2}\\mathbb{E}\\big[V(\\theta_{n})-V(\\theta^{*})\\big]\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In this assumption, $r_{n+1}$ represents an additive bias term, generally of the order of the square of the bias, and $\\lambda_{n+1}$ may depend on the minimum eigenvalue of $A_{n}$ . In [21, Theorem 2], it has been demonstrated that this assumption is weaker than the alternatives used in the literature on biased SGD. We have extended these assumptions to the adaptive case. It is important to note that the first point of H3 depends on the application (objective function $V$ ) and on the adaptive algorithm (matrix $A_{n.}$ ) that we want to use. The purpose of this assumption is to provide a more general framework that covers all possible applications and adaptive algorithms. In the biased SGD setting, if the bias term $\\|\\mathbb{E}[H_{\\theta_{n}}(X_{n+1})\\mid\\mathcal{F}_{n}]-\\nabla V(\\theta_{n})\\|$ is bounded by $\\tilde{b}_{n+1}$ , we can easily verify the first point of ", "page_idx": 3}, {"type": "text", "text": "H3 by considering $\\lambda_{n+1}\\,=\\,1/2$ and $r_{n+1}=\\tilde{b}_{n+1}^{2}$ . We show in Section 4.3 that this assumption is also verified in algorithms such as Adagrad and RMSProp. The second point of H3 is a weaker assumption compared to bounding the variance of the noise term. Applications where we can verify these assumptions are discussed in Appendix D. ", "page_idx": 4}, {"type": "text", "text": "We finally consider an additional assumption on $A_{n}$ . Let $\\|A\\|$ be the spectral norm of a matrix $A$ . ", "page_idx": 4}, {"type": "text", "text": "In our setting, since $A_{n}$ is assumed to be a symmetric matrix, the spectral norm is equal to the largest eigenvalue. H4 plays a crucial role, as the estimates may diverge when this assumption is not satisfied. Given a sequence $(\\beta_{n})_{n\\geq1}$ , one way to ensure that $\\mathrm{H4}$ is satisfied is to replace the random matrices $A_{n}$ with ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{A}_{n}=\\frac{\\operatorname*{min}\\{\\|A_{n}\\|,\\beta_{n+1}\\}}{\\|A_{n}\\|}A_{n}~.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "It is then clear that $\\|\\tilde{A}_{n}\\|\\leq\\beta_{n+1}$ . Furthermore, in most cases, especially for Adagrad, RMSProp and Stochastic Newton, control of $\\lambda_{\\operatorname*{max}}\\left(A_{n}\\right)$ in $\\mathrm{H4}$ is satisfied. For example, in Adagrad and RMSProp, in (4), we have $\\lambda_{\\operatorname*{max}}\\left(A_{n}\\right)\\leq\\delta^{-1/2}$ . ", "page_idx": 4}, {"type": "text", "text": "4 Convergence Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Convergence under the PL condition ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we study the convergence rate of SGD with biased gradients and adaptive steps under the PL condition. We give below a simplified version of the bound we obtain on the risk and refer to Theorem A.2 in the appendix for a formal statement with explicit constants. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.1. Assume that $H I\\cdot H4$ hold. Let $\\theta_{n}\\,\\in\\,\\mathbb{R}^{d}$ be the $n$ -th iterate of the recursion (3) and $\\gamma_{n}=C_{\\gamma}n^{-\\gamma},\\beta_{n}=C_{\\beta}n^{\\beta},\\lambda_{n}=C_{\\lambda}n^{-\\lambda}$ with $C_{\\gamma}>0,C_{\\beta}>0,$ , and $C_{\\lambda}>0$ . Assume that $\\gamma,\\beta,\\lambda\\geq0$ and $\\gamma+\\lambda<1$ . Then, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[V\\left(\\theta_{n}\\right)-V(\\theta^{*})\\right]\\!=\\mathcal{O}\\left(n^{-\\gamma+2\\beta+\\lambda}+r_{n}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The rate obtained is classical and shows the tradeoff between a term coming from the adaptive steps (with a dependence on $\\gamma,\\beta,\\lambda)$ and a term $r_{n}$ which depends on the control of the bias. To minimize the right hand-side of (7), we would like to have $\\beta=\\lambda=0$ . For example, it is verified in the case of Adagrad and RMSProp if the gradients are bounded, as will be discussed later. ", "page_idx": 4}, {"type": "text", "text": "We stress that Theorem 4.1 applies to any adaptive algorithm of the form (3), with the only assumption being H4. Without any information on these eigenvalues, the choice that $\\beta_{n}\\propto n^{\\beta}$ and $\\lambda_{n}\\propto\\dot{n}^{-\\lambda}$ allows us to remain very general, which can even be seen as a worst-case scenario. Finally, note that non-adaptive SGD is a particular case of Theorem 4.1. Thus, our theorem gives new results also in the non-adaptive case with generic step sizes and biased gradients with decreasing bias. ", "page_idx": 4}, {"type": "text", "text": "4.2 Convergence without the PL condition ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In the non-convex smooth case, theoretical results are generally based on a randomized version of SA, as described in [60, 32, 44]. Instead of considering the final parameter $\\theta_{n}$ , we introduce a random variable $R$ , which takes its values in $\\{1,\\ldots,n\\}$ , and the quantity of interest becomes $\\theta_{R}$ . Note that this procedure is a technical tool, in practical applications we use classical SA. The following theorem provides a bound in expectation on the gradient of the objective function $V$ , which is the best we can have given that no assumption is made about the existence of a global minimum of $V$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.2. Assume that $H2\\textrm{-}H4$ hold. Assume also that for all $k\\;\\in\\;\\mathbb{N}$ , we have $\\gamma_{k+1}~\\leq$ $\\lambda_{k+1}/(\\tilde{\\sigma}_{1}L\\beta_{k+1}^{2})$ . For any $n\\geq1$ , let $R\\in\\{0,\\ldots,n\\}$ be a discrete random variable such that: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{P}(R=k):=\\frac{w_{k+1}\\gamma_{k+1}\\lambda_{k+1}}{\\sum_{j=0}^{n}w_{j+1}\\gamma_{j+1}\\lambda_{j+1}}\\;,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{w_{k+1}=\\prod_{j=1}^{k+1}(1+\\tilde{\\sigma}_{2}\\delta_{j})^{-1}}\\end{array}$ with $\\delta_{j}=L\\gamma_{j}^{2}\\beta_{j}^{2}/2$ . Then, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|\\nabla V\\left(\\theta_{R}\\right)\\right\\|^{2}\\right]\\leq2\\frac{V^{*}+\\alpha_{1,n}+\\alpha_{2,n}}{\\sum_{j=0}^{n}w_{j+1}\\gamma_{j+1}\\lambda_{j+1}}\\;,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\alpha_{1,n}=\\sum_{k=0}^{n}w_{k+1}\\gamma_{k+1}\\lambda_{k+1}r_{k+1}\\;,\\;\\alpha_{2,n}=\\sum_{k=0}^{n}w_{k+1}\\delta_{k+1}\\sigma_{k}^{2},\\;\\;a n d\\;\\;V^{*}=\\mathbb{E}[V(\\theta_{0})-V(\\theta^{*})]\\;.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "If $\\tilde{\\sigma}_{2}=0$ , Theorem 4.2 recovers the asymptotic convergence rate obtained by [44] with respect to the hyperparameters $\\gamma,\\beta$ , and $\\lambda$ , and to the bias. We can observe that if $\\gamma\\le\\lambda+2\\beta$ , the condition on $(\\gamma_{k})_{k\\geq1}$ can be met simply by tuning $C_{\\gamma}$ . In particular, if $A_{n}=I_{d}$ , the requirement on the step sizes can be expressed as $\\gamma_{k+1}\\leq\\mathrm{{1}}/(\\tilde{\\sigma}_{1}\\bar{L})$ . ", "page_idx": 5}, {"type": "text", "text": "We give below the convergence rates obtained from Theorem 4.2 under the same assumptions on $\\gamma_{n}$ , $\\beta_{n}$ , and $\\lambda_{n}$ as in Theorem 4.1. ", "page_idx": 5}, {"type": "text", "text": "Corollary 4.3. Assume that H2-H4 hold. Let $\\gamma_{n}\\,=\\,C_{\\gamma}n^{-\\gamma},\\beta_{n}\\,=\\,C_{\\beta}n^{\\beta},\\lambda_{n}\\,=\\,C_{\\lambda}n^{-\\lambda}$ with $C_{\\gamma}>0,C_{\\beta}>0,$ , and $C_{\\lambda}>0$ . Assume that $\\gamma,\\beta,\\lambda\\geq0$ and $\\gamma+\\lambda<1$ . Then, if $\\tilde{\\sigma}_{2}=0$ , we have: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big[\\left\\lVert\\nabla V\\left(\\theta_{R}\\right)\\right\\rVert^{2}\\Big]=\\left\\{\\begin{array}{l l}{\\mathcal{O}\\left(n^{-\\gamma+\\lambda+2\\beta}+b_{n}\\right)}&{i f\\gamma-\\beta<1/2\\;,}\\\\ {\\mathcal{O}\\left(n^{\\gamma+\\lambda-1}+b_{n}\\right)}&{i f\\gamma-\\beta>1/2\\;,}\\\\ {\\mathcal{O}\\left(n^{\\gamma+\\lambda-1}\\mathrm{log}\\,n+b_{n}\\right)}&{i f\\gamma-\\beta=1/2\\;,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the bias term $b_{n}$ can be constant or decreasing. In the latter case, writing $r_{n}=C_{r}n^{-r}$ , we have: ", "page_idx": 5}, {"type": "equation", "text": "$$\nb_{n}=\\left\\{\\mathcal{O}\\left(n^{-r}\\right)\\quad\\quad\\quad\\quad\\quad\\,i f\\,r+\\lambda+\\gamma<1\\ ,}\\\\ {\\mathcal{O}\\left(n^{\\gamma+\\lambda-1}\\right)\\quad}&{i f\\,r+\\lambda+\\gamma>1\\ ,}\\\\ {\\mathcal{O}\\left(n^{\\gamma+\\lambda-1}\\log n\\right)}&{i f\\,r+\\lambda+\\gamma=1\\ .}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In practice, the value of $r$ is known in advance while the other parameters can be tuned to achieve the optimal rate of convergence. In any sce\u221anario, we can never achieve a bound of ${\\mathcal{O}}(1/{\\sqrt{n}}+b_{n})$ , and the best rate we can reach is $O(\\log n/\\sqrt{n}+b_{n})$ when $\\gamma=1/2,\\beta=0$ , and $\\lambda=0$ . In this case, all eigenvalues of $A_{n}$ must be bounded from both below and above. Note that we could also have obtained such a rate by taking $\\lambda_{n}=n^{-1/2}$ and $\\beta_{n}=n^{-1/2}$ while keeping $\\gamma_{n}$ constant. However, the assumption that $\\beta_{n}=n^{-1/2}$ is too strong (fast decay of the eigenvalues of $A_{n}$ ), hence our choice of $\\beta_{n}=\\dot{{\\cal C}}_{\\beta}n^{\\beta}$ . Finally, for a decreasing bias, if $r\\geq1/2$ , the bias term contributes to the convergence rate of the algorithm. Otherwise, the oth\u221aer term is the leading term of the upper bound. In both cases, the best achievable bound is ${\\mathcal{O}}(\\log n/{\\sqrt{n}})$ if $r\\geq1/2$ . ", "page_idx": 5}, {"type": "text", "text": "Bounded Gradient Case. Now, we analyze the convergence of Randomized Adaptive Stochastic Approximation when the stochastic updates are bounded, as given by the following assumption. ", "page_idx": 5}, {"type": "text", "text": "H5 There exists $M\\geq0$ such that for all $n\\in\\mathbb N$ $\\mathfrak{l},\\,\\|H_{\\theta_{n}}\\left(X_{n+1}\\right)\\|\\leq M.$ . ", "page_idx": 5}, {"type": "text", "text": "Boundedness of the stochastic gradient of the objective function is a classical assumption in adaptive stochastic optimization [67, 74, 19, 73]. ", "page_idx": 5}, {"type": "text", "text": "Corollary 4.4. Assume that H2-H5 hold. Let $\\gamma_{n}\\,=\\,C_{\\gamma}n^{-\\gamma},\\beta_{n}\\,=\\,C_{\\beta}n^{\\beta},\\lambda_{n}\\,=\\,C_{\\lambda}n^{-\\lambda}$ with $C_{\\gamma}\\,>\\,0,C_{\\beta}\\,>\\,0$ , and $C_{\\lambda}\\,>\\,0$ . Assume that $\\gamma,\\beta,\\lambda\\,\\geq\\,0$ and $\\gamma+\\lambda<1$ . For any $n\\,\\geq\\,1$ , let $R\\in\\{0,\\ldots,n\\}$ be a uniformly distributed random variable. Then, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|\\nabla V\\left(\\theta_{R}\\right)\\right\\|^{2}\\right]\\leq\\frac{V^{\\ast}+\\alpha_{1,n}^{\\prime}+L M^{2}\\alpha_{2,n}^{\\prime}/2}{\\sqrt{n}}\\;,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r}{\\alpha_{1,n}^{\\prime}=\\sum_{k=0}^{n}\\gamma_{k+1}\\lambda_{k+1}r_{k+1},\\,\\alpha_{2,n}^{\\prime}=\\sum_{k=0}^{n}\\gamma_{k+1}^{2}\\beta_{k+1}^{2}}\\end{array}$ , and $V^{*}=\\mathbb{E}[V(\\theta_{0})-V(\\theta^{*})]$ . ", "page_idx": 5}, {"type": "text", "text": "Importantly, in Corollary 4.4, there are no assumptions on the step sizes, and we obtain a better bound than in Theorem 4.2. ", "page_idx": 5}, {"type": "text", "text": "4.3 Application to Adagrad and RMSProp ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We give a convergence analysis of Adagrad and RMSProp with a biased gradient estimator. First, note that, under H5, for all eigenvalues $\\lambda$ of $A_{n}$ , the adaptive matrix in Adagrad or RMSProp, it holds that $(M^{2}+\\delta)^{-1/2}\\leq\\lambda\\leq\\delta^{-1/2}$ , i.e., H4 is satisfied with $\\lambda=0$ and $\\beta=0$ . ", "page_idx": 5}, {"type": "text", "text": "Corollary 4.5. Assume that $H2$ and $H5$ hold. Let $\\gamma_{n}=c_{\\gamma}n^{-1/2}$ and $A_{n}$ denote the adaptive matrix in Adagrad or RMSProp. For any $n\\geq1$ , let $R\\in\\{0,\\ldots,n\\}$ be a uniformly distributed random variable. Suppose that for any $n\\geq1$ , there exist positive constants $\\alpha$ and $C_{\\alpha}$ such that: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{E}\\left[H_{\\theta_{n}}\\left(X_{n+1}\\right)\\vert\\mathcal{F}_{n}\\right]-\\nabla V\\left(\\theta_{n}\\right)\\right\\|\\leq C_{\\alpha}n^{-\\alpha}\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|\\nabla V\\left(\\theta_{R}\\right)\\right\\|^{2}\\right]=\\mathcal{O}\\left(\\frac{\\log n}{\\sqrt{n}}+b_{n}\\right)\\;,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the bias $b_{n}$ is explicitly given in Appendix A.5. ", "page_idx": 6}, {"type": "text", "text": "In the case of an unbiased gradient, we obtain the same bound of $O(\\log n/{\\sqrt{n}})$ as in [74, 79, 19] under the same assumptions. If the bias is of the order $O(n^{-1/4})$ , the algorithm achieves the same convergence rate as in the case of an unbiased gradient. ", "page_idx": 6}, {"type": "text", "text": "4.4 AMSGRAD with Biased Gradients ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Finally, we show the convergence of AMSGRAD [67] with a biased gradient estimator. At each iteration, AMSGRAD uses an exponential moving average of past gradients instead of the current gradient as in Equation (3), which is detailed in Algorithm 1. The key difference between Adam and AMSGRAD lies in their handling of the second moment estimate. Specifically, AMSGRAD uses the updated term $\\hat{V}_{k}=\\operatorname*{max}(\\hat{V}_{k-1},\\operatorname{Diag}(V_{k}))$ instead of directly using $V_{k}$ , with the maximum taken coordinate-wise. This approach is crucial, as it ensures that the eigenvalues of $A_{n}$ decrease at each iteration. The following theorem provides a bound in expectation on the gradient of the objective $V$ ", "page_idx": 6}, {"type": "table", "img_path": "TzxSrNJE0T/tmp/9395d8a99f5f322b207da6924f1c9a3bbebbf6927c875d14972d5452ba1a5278.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "function using randomized iterations with AMSGRAD. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.6. Assume that $H2$ , $H3$ (i), and $H5$ hold. Let $\\gamma_{n}=c_{\\gamma}n^{-1/2}$ , $A_{n}$ denote the adaptive matrix of AMSGRAD in Algorithm $^{\\,l}$ , and $\\rho_{1},\\rho_{2}\\in[0,1)$ . For any $n\\geq1$ , let $R\\in\\{0,\\ldots,n\\}$ be $a$ uniformly distributed random variable. Then, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|\\nabla V\\left(\\theta_{R}\\right)\\right\\|^{2}\\right]=\\mathcal{O}\\left(\\frac{\\log n}{\\sqrt{n}}+b_{n}\\right)\\;,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $b_{n}$ corresponds to the bias which comes from $r_{n}$ in $H3(i)$ . Choosing $r_{n}=C_{r}n^{-r}$ , we get: ", "page_idx": 6}, {"type": "equation", "text": "$$\nb_{n}=\\left\\{\\mathcal{O}\\left(n^{-r}\\right)\\quad\\quad\\quad\\quad\\quad i f r<1/2\\ ,\\medskip}\\\\ {\\mathcal{O}\\left(n^{-1/2}\\right)\\quad}&{i f r>1/2\\ ,\\medskip}\\\\ {\\mathcal{O}\\left(n^{-1/2}\\log n\\right)}&{i f r=1/2\\ .\\medskip}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "If the bias is of the order $O(n^{-1/4})$ , we achieve a convergence rate of ${\\mathcal{O}}(\\log n/{\\sqrt{n}})$ , which is the same as that of an unbiased gradient [19] and similar to that of Adagrad and RMSProp. It is worth noting that our results are also applicable to SGD momentum by taking $A_{n}=I_{d}$ in Algorithm 1. ", "page_idx": 6}, {"type": "text", "text": "4.5 Convergence Results in i.i.d. and Markov Chain cases ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "For illustrative purposes, in this subsection we give the form of the bias of the gradient estimator, denoted by $\\tilde{b}_{n}$ , in two simple scenarios, i.e., when $\\{X_{n},n\\in\\mathbb{N}\\}$ is either an i.i.d. sequence or a Markov chain. For Adagrad, RMSProp, and AMSGRAD, bounding the bias of the gradient estimator is a sufficient condition for verifying $\\mathrm{H}3(i)$ , which in turn enables us to derive convergence results in each scenario. ", "page_idx": 6}, {"type": "text", "text": "I.i.d. case. Assume that $\\{X_{n},n\\in\\mathbb{N}\\}$ are i.i.d. random variables. If the mean field function $h\\left(\\theta_{n}\\right)=\\mathbb{E}\\left[H_{\\theta_{n}}\\left(X_{n+1}\\right)\\mid{\\dot{\\mathcal{F}}}_{n}\\right]$ aligns with the true gradient, then the estimator is unbiased. Otherwise, the bias of the gradient estimator is ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\tilde{b}_{n+1}=\\|h(\\theta_{n})-\\nabla V(\\theta_{n})\\|\\ .\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Markov Chain case. Assume now that $\\{X_{n},n\\in\\mathbb{N}\\}$ is a Markov Chain. The bias consists of two parts: the difference between the mean field function and the true gradient, and a term due to the Markov chain dynamics. For all $T\\geq0$ , we define the stochastic update as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\nH_{\\theta_{k}}\\left(X_{k+1}\\right)=\\frac{1}{T}\\sum_{i=1}^{T}H_{\\theta_{k}}\\left(X_{k+1}^{(i)}\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where Xk(i+)1 represents the i-th sample generated at iteration $k+1$ . This multi-sample estimator is commonly used in applications such as Reinforcement Learning, Markov Chain Monte Carlo, and Sequential Monte Carlo methods, effectively reducing the variance of the gradient estimator. The mixing time $\\tau_{\\mathrm{mix}}$ of a Markov chain with stationary distribution $\\pi$ and transition kernel $P$ is characterized as: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\tau_{\\mathrm{mix}}:=\\operatorname*{inf}\\left\\{t\\,;\\,\\operatorname*{sup}_{x}D_{\\mathrm{TV}}(P^{t}(x,\\cdot),\\pi)\\leq\\frac{1}{4}\\right\\},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $D_{\\mathrm{TV}}$ denotes the total variation distance. For an ergodic Markov chain with stationary distribution $\\pi$ , the bias of this gradient estimator when using $T$ samples per step is ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\tilde{b}_{n+1}=\\|h(\\theta_{n})-\\nabla V(\\theta_{n})\\|+M\\sqrt{\\tau_{\\mathrm{mix}}/T}\\;,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\begin{array}{r}{h(\\theta)=\\int H_{\\theta}(x)\\pi(d x)}\\end{array}$ . If the general optimization problem reduces to the following stochastic optimization problem with Markov noise, as considered in most of the literature [28, 24, 7]: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\mathbb{R}^{d}}V(\\theta):=\\mathbb{E}_{x\\sim\\pi}[f(\\theta;x)],\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\theta\\mapsto f(\\theta;x)$ is a loss function, and $\\pi$ is some stationary data distribution of the Markov Chain and $H_{\\theta_{k}}(X_{k+1}^{(i)})=\\nabla f(\\theta_{k};X_{k+1}^{(i)})$ , then $\\tilde{b}_{n+1}=M\\sqrt{\\tau_{\\mathrm{mix}}/T}$ , similar to SGD with Markov Noise [24]. ", "page_idx": 7}, {"type": "text", "text": "5 Applications and Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "5.1 Bilevel and Conditional Stochastic Optimization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We can now apply our theoretical results in various settings where biased gradients are involved. In particular, they apply to the fields of Stochastic Bilevel Optimization and Conditional Stochastic Optimization. Stochastic Bilevel Optimization consists of minimizing an objective function $V$ with respect to $\\theta$ , where $V$ is itself a function of $\\phi^{*}(\\theta)$ and $\\phi^{*}(\\theta)$ is obtained by solving another minimization problem. Conditional Stochastic Optimization focuses on optimizing the expected value of a function that contains a nested conditional expectation on a random variable $\\eta$ . We provide in Table 1 a summary of the assumptio\u221ans satisfied in these settings, which allow to apply the results of Section 4 and to obtain a $\\mathcal{O}(\\log\\bar{n}/\\sqrt{n}+b_{n})$ convergence rate in both cases, and explicit forms for $b_{n}$ . To our knowledge, these are the first convergence rates obtained in these settings. ", "page_idx": 7}, {"type": "text", "text": "We refer to Appendix D for other examples in which the bias of the estimator can be controlled, in particular Self-Normalized Importance Sampling (Appendix D.1), Sequential Monte Carlo Methods (Appendix D.2), Policy Gradient (Appendix D.3), Zeroth-Order Gradient (Appendix D.4), and Coordinate Sampling (Appendix D.5). ", "page_idx": 7}, {"type": "text", "text": "5.2 Experiments with IWAE and BR-IWAE ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we illustrate our theoretical results in the context of deep VAE. The experiments were conducted using PyTorch [65], and the source code can be found here2. In generative models, ", "page_idx": 7}, {"type": "table", "img_path": "TzxSrNJE0T/tmp/fe7efd535fdaf80ce325e5977a5cce4c3a0775ed7cf31880fe63b2ba6a2154d5.jpg", "table_caption": ["Table 1: Bilevel and Conditional Stochastic Optimization with our Biased Adaptive SA framework. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "the objective is to maximize the marginal likelihood $\\log p_{\\theta}(x)$ , which is the marginalization of $(x,z)\\overset{*}{\\mapsto}p_{\\theta}(x,z)$ , where $x$ represents the observations and $z$ is the latent variable. Under some simple technical assumptions, by Fisher\u2019s identity, we have: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\log p_{\\theta}(x)=\\int\\nabla_{\\theta}\\log p_{\\theta}(x,z)p_{\\theta}(z\\mid x)\\mathrm{d}z\\ .\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "However, in most cases, the conditional density $z\\mapsto p_{\\theta}(z\\mid x)$ is intractable and can only be sampled. Variational Autoencoders introduce an additional parameter $\\phi$ and a family of variational distributions $z\\mapsto q_{\\phi}(z\\mid x)$ to approximate the true posterior distribution. Parameters are estimated by maximizing the Evidence Lower Bound (ELBO): ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\log p_{\\theta}(x)\\ge\\mathbb{E}_{q_{\\phi}(\\cdot\\vert x)}\\left[\\log\\frac{p_{\\theta}(x,Z)}{q_{\\phi}(Z\\mid x)}\\right]=:\\mathcal{L}_{\\mathtt{E L B O}}(\\theta,\\phi;x)\\ .\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The Importance Weighted Autoencoder (IWAE) [12] is a variant of the VAE that incorporates importance weighting to obtain a tighter ELBO. The IWAE objective can be written as follows: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k}^{\\mathrm{IWAE}}(\\theta,\\phi;x)=\\mathbb{E}_{q_{\\phi}^{\\otimes k}(\\cdot\\vert x)}\\left[\\log\\frac{1}{k}\\sum_{\\ell=1}^{k}\\frac{p_{\\theta}(x,Z^{(\\ell)})}{q_{\\phi}(Z^{(\\ell)}\\mid x)}\\right],\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $k$ corresponds to the number of samples drawn from the variational posterior distribution. The estimator of the gradient of ELBO in IWAE is a biased estimator of $\\nabla_{\\theta}\\log{p_{\\theta}(x)}$ . In Theorem B.1, we establish that the bias of this estimator is of order $\\mathcal{O}(1/k)$ , thereby allowing us to derive a convergence rate for IWAE. Since bias has an impact on convergence rates, we propose to use one of the bias reduction techniques, the Biased Reduced Importance Weighted Autoencoder (BR-IWAE) [14], which is detailed in Appendix B. ", "page_idx": 8}, {"type": "text", "text": "Dataset and Model. We conduct our experiments on the CIFAR-10 dataset [51] and use a Convolutional Neural Network (CNN) architecture with the Rectified Linear Unit (ReLU) activation function for both the encoder and the decoder. The latent space dimension is set to 100. We estimate the log-likelihood using VAE, IWAE, and BR-IWAE models, all of which are trained for 100 epochs. ", "page_idx": 8}, {"type": "text", "text": "Training is conducted using Adagrad, RMSProp, and Adam with a decaying learning rate. Although AMSGRAD is analyzed in our theoretical results, we use Adam for the experiments due to its widespread use in practice. Additional details are provided in Appendix E. ", "page_idx": 8}, {"type": "text", "text": "First, we set $k=5$ samples in both IWAE and BR-IWAE. The test losses are presented in Figure 1. We show the negative log-likelihood on the test dataset for VAE, IWAE, and BR-IWAE with Adagrad, RMSProp, and Adam. As expected, we observe that IWAE outperforms VAE, while BR-IWAE outperforms IWAE by reducing bias in all cases. ", "page_idx": 8}, {"type": "image", "img_path": "TzxSrNJE0T/tmp/7fe01bcfac5735c4d15fce504880c99a5c084ffa55f5befe61ce26b46a75cb4a.jpg", "img_caption": ["Figure 1: Negative Log-Likelihood on the test set for Different Generative Models with Adagrad, RMSProp, and Adam on CIFAR-10. Bold lines represent the mean over 5 independent runs. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Then, we illustrate empirically the convergence rates obtained in Corollary 4.5 and Theorem 4.6 for IWAE. Since the bias of the estimator of the gradient in IWAE is of the order $\\mathcal{O}(1/k)$ , choosing a bias of order $\\mathcal{O}(n^{-\\alpha})$ is equivalent to using $n^{\\alpha}$ samples at iteration $n$ to estimate the gradient. We plot in Figure 2 the gradient squared norm $\\|\\nabla V(\\theta_{n})\\|^{2}$ and the Negative Log-Likelihood is given in Appendix E.2. Note that all figures are with respect to epochs, whereas here, $n$ represents the number of updates of the gradient. T\u221ahe dashed curves correspond to the expected convergence rate $O(n^{-1/4})$ for $\\alpha=1/8$ and ${\\bar{O}}(\\log n/{\\sqrt{n}})$ for $\\alpha=1/4$ and $\\alpha=1/2$ . ", "page_idx": 9}, {"type": "image", "img_path": "TzxSrNJE0T/tmp/7dd141ceb643f825af158ed36baca0831a66120c2378df63043be77c0eb39bdb.jpg", "img_caption": ["Figure 2: Value of $\\|\\nabla V(\\theta_{n})\\|^{2}$ in IWAE with Adagrad (on the left), RMSProp, and Adam (on the right). Bold lines represent the mean over 5 independent runs. Figures are plotted on a logarithmic scale for better visualization. Both figures have the same scale, so we have not shown the dashed theoretical curves on the right for better clarity. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "We observe that the algorithms converge at the expected theoretical rates, and even faster. In Appendix E.2, we have included an additional experiment on the FashionMNIST dataset [76], which shows similar behavior, but the convergence is closer to the expected rates, suggesting that our upper bounds may be tight. We see similar convergence rates for Adagrad, RMSProp, and Adam, although, as expected, Adam performs slightly better. Moreover, it is clear that convergence is faster with a larger $\\alpha$ but beyond a certain threshold for $\\alpha$ the rate of convergence does not change significantly. Since choosing a larger $\\alpha$ induces an additional computational cost, it is crucial to choose an appropriate value that achieves fast convergence without being too computationally expensive. Choosing an optimal number of samples at each iteration remains an open problem depending on the chosen generative model. ", "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper provides a non-asymptotic analysis of Biased Adaptive Stochastic Approximation with and witho\u221aut the $\\mathrm{PL}$ condition in the non-convex smooth setting. We derive a convergence rate of $O(\\log n/{\\sqrt{n}}+b_{n})$ for non-convex smooth functions, where $b_{n}$ corresponds to the time-dependent decreasing bias, and an improved linear convergence rate with the Polyak-\u0141ojasiewicz (PL) condition. We also establish that Adagrad, RMSProp, and AMSGRAD with biased gradients converge to critical points for non-convex smooth functions. Our results provide insights on hyper-parameters tuning to achieve fast convergence and reduce computational time. A natural extension of this work is the analysis of the assumptions, the bias and convergence rates for specific deep learning architectures. A theoretical analysis of the Monte Carlo effort required at each iteration to obtain an optimal convergence rate is another interesting perspective. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The Ph.D. of Sobihan Surendran was funded by the Paris Region PhD Fellowship Program of R\u00e9gion Ile-de-France. We would like to thank SCAI (Sorbonne Center for Artificial Intelligence) for providing the computing clusters. We also express our gratitude to the reviewers for their insightful comments and suggestions, which have helped improve this paper. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Sergios Agapiou, Omiros Papaspiliopoulos, Daniel Sanz-Alonso, and Andrew M Stuart. Importance sampling: Intrinsic dimension and computational cost. Statistical Science, pages 405\u2013431, 2017.   \n[2] Ahmad Ajalloeian and Sebastian U Stich. On the Convergence of SGD with Biased Gradients. arXiv preprint arXiv:2008.00051, 2020.   \n[3] Ahmet Alacaoglu and Hanbaek Lyu. Convergence of first-order methods for constrained nonconvex optimization with dependent data. In International Conference on Machine Learning, pages 458\u2013489. PMLR, 2023.   \n[4] Peter Auer, Nicolo Cesa-Bianchi, and Claudio Gentile. Adaptive and self-confident on-line learning algorithms. Journal of Computer and System Sciences, 64(1):48\u201375, 2002.   \n[5] Jonathan Baxter and Peter L Bartlett. Infinite-horizon policy-gradient estimation. Journal of Artificial Intelligence Research, 15:319\u2013350, 2001.   \n[6] Aleksandr Beznosikov, Samuel Horv\u00e1th, Peter Richt\u00e1rik, and Mher Safaryan. On biased compression for distributed learning. Journal of Machine Learning Research, 24(276):1\u201350, 2023.   \n[7] Aleksandr Beznosikov, Sergey Samsonov, Marina Sheshukova, Alexander Gasnikov, Alexey Naumov, and Eric Moulines. First order methods with markovian noise: from acceleration to variational inequalities. In Advances in Neural Information Processing Systems, volume 36, 2024.   \n[8] Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference learning with linear function approximation. In Conference On Learning Theory, pages 1691\u2013 1692. PMLR, 2018.   \n[9] L\u00e9on Bottou. Une approche th\u00e9orique de l\u2019apprentissage connexioniste; applications \u00e0 la reconnaissance de la parole. PhD thesis, Paris 11, 1991.   \n[10] L\u00e9on Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. SIAM Review, 60(2):223\u2013311, 2018.   \n[11] Claire Boyer and Antoine Godichon-Baggioni. On the asymptotic rate of convergence of stochastic newton algorithms and their weighted averaged versions. Computational Optimization and Applications, 84(3):921\u2013972, 2023.   \n[12] Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. In International Conference on Learning Representations, 2016.   \n[13] Gabriel Cardoso, Yazid Janati El Idrissi, Sylvain Le Corff, Eric Moulines, and Jimmy Olsson. State and parameter learning with PaRIS particle Gibbs. In International Conference on Machine Learning, pages 3625\u20133675. PMLR, 2023.   \n[14] Gabriel Cardoso, Sergey Samsonov, Achille Thin, Eric Moulines, and Jimmy Olsson. BRSNIS: bias reduced self-normalized importance sampling. In Advances in Neural Information Processing Systems, volume 35, pages 716\u2013729, 2022.   \n[15] Congliang Chen, Li Shen, Fangyu Zou, and Wei Liu. Towards practical adam: Non-convexity, convergence theory, and mini-batch acceleration. Journal of Machine Learning Research, 23(229):1\u201347, 2022.   \n[16] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pages 15\u201326, 2017.   \n[17] Tianyi Chen, Yuejiao Sun, and Wotao Yin. Closing the gap: Tighter analysis of alternating stochastic gradient methods for bilevel problems. In Advances in Neural Information Processing Systems, volume 34, pages 25294\u201325307, 2021.   \n[18] Gal Dalal, Bal\u00e1zs Sz\u00f6r\u00e9nyi, Gugan Thoppe, and Shie Mannor. Finite sample analyses for td (0) with function approximation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 6144\u20136160, 2018.   \n[19] Alexandre D\u00e9fossez, L\u00e9on Bottou, Francis Bach, and Nicolas Usunier. A simple convergence proof of Adam and Adagrad. arXiv preprint arXiv:2003.02395, 2020.   \n[20] Pierre Del Moral, Arnaud Doucet, and Sumeetpal S Singh. A backward particle interpretation of Feynman-Kac formulae. ESAIM: Mathematical Modelling and Numerical Analysis, 44(5):947\u2013 975, 2010.   \n[21] Yury Demidovich, Grigory Malinovsky, Igor Sokolov, and Peter Richt\u00e1rik. A guide through the zoo of biased sgd. In Advances in Neural Information Processing Systems, volume 36, 2024.   \n[22] Aymeric Dieuleveut, Gersende Fort, Eric Moulines, and Hoi-To Wai. Stochastic approximation beyond gradient for signal processing and machine learning. IEEE Transactions on Signal Processing, 2023.   \n[23] Thinh T Doan, Lam M Nguyen, Nhan H Pham, and Justin Romberg. Finite-time analysis of stochastic gradient descent under markov randomness. arXiv preprint arXiv:2003.10973, 2020.   \n[24] Ron Dorfman and Kfir Yehuda Levy. Adapting to mixing time in stochastic optimization with markovian data. In International Conference on Machine Learning, pages 5429\u20135446. PMLR, 2022.   \n[25] Timothy Dozat. Incorporating Nesterov momentum into Adam. In ICLR Workshop track, 2016.   \n[26] Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In International Conference on Machine Learning, pages 1675\u20131685. PMLR, 2019.   \n[27] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(7), 2011.   \n[28] John C Duchi, Alekh Agarwal, Mikael Johansson, and Michael I Jordan. Ergodic mirror descent. SIAM Journal on Optimization, 22(4):1549\u20131578, 2012.   \n[29] Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi. Global convergence of policy gradient methods for the linear quadratic regulator. In International Conference on Machine Learning, pages 1467\u20131476. PMLR, 2018.   \n[30] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning, pages 1126\u20131135. PMLR, 2017.   \n[31] Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel programming for hyperparameter optimization and meta-learning. In International Conference on Machine Learning, pages 1568\u20131577. PMLR, 2018.   \n[32] Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341\u20132368, 2013.   \n[33] Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. arXiv preprint arXiv:1802.02246, 2018.   \n[34] Pierre Gloaguen, Sylvain Le Corff, and Jimmy Olsson. A pseudo-marginal sequential Monte Carlo online smoothing algorithm. Bernoulli, 28(4):2606\u20132633, 2022.   \n[35] Antoine Godichon-Baggioni and Pierre Tarrago. Non asymptotic analysis of adaptive stochastic gradient algorithms and applications. arXiv preprint arXiv:2303.01370, 2023.   \n[36] Riccardo Grazzi, Massimiliano Pontil, and Saverio Salzo. Bilevel optimization with a lowerlevel contraction: Optimal sample complexity without warm-start. Journal of Machine Learning Research, 24(167):1\u201337, 2023.   \n[37] Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A two-timescale stochastic algorithm framework for bilevel optimization: Complexity analysis and application to actorcritic. SIAM Journal on Optimization, 33(1):147\u2013180, 2023.   \n[38] Bin Hu, Peter Seiler, and Laurent Lessard. Analysis of biased stochastic gradient descent using sequential semidefinite programs. Mathematical programming, 187:383\u2013408, 2021.   \n[39] Yifan Hu, Xin Chen, and Niao He. On the bias-variance-cost tradeoff of stochastic optimization. In Advances in Neural Information Processing Systems, volume 34, pages 22119\u201322131, 2021.   \n[40] Yifan Hu, Siqi Zhang, Xin Chen, and Niao He. Biased stochastic first-order methods for conditional stochastic optimization and applications in meta learning. In Advances in Neural Information Processing Systems, volume 33, pages 2759\u20132770, 2020.   \n[41] Feihu Huang, Junyi Li, and Shangqian Gao. Biadam: Fast adaptive bilevel optimization methods. arXiv preprint arXiv:2106.11396, 2021.   \n[42] Tommi Jaakkola, Michael Jordan, and Satinder Singh. Convergence of stochastic iterative dynamic programming algorithms. In Advances in Neural Information Processing Systems, volume 6, 1993.   \n[43] Kaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and enhanced design. In International Conference on Machine Learning, pages 4882\u20134892. PMLR, 2021.   \n[44] Belhal Karimi, Blazej Miasojedow, Eric Moulines, and Hoi-To Wai. Non-asymptotic analysis of biased stochastic approximation scheme. In Conference on Learning Theory, pages 1944\u20131974. PMLR, 2019.   \n[45] Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximalgradient methods under the polyak-\u0142ojasiewicz condition. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19-23, 2016, Proceedings, Part I 16, pages 795\u2013811. Springer, 2016.   \n[46] Ahmed Khaled and Peter Richt\u00e1rik. Better theory for SGD in the nonconvex world. arXiv preprint arXiv:2002.03329, 2020.   \n[47] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.   \n[48] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In International Conference on Learning Representations, 2014.   \n[49] Vijaymohan R Konda and Vivek S Borkar. Actor-critic\u2013type learning algorithms for markov decision processes. SIAM Journal on control and Optimization, 38(1):94\u2013123, 1999.   \n[50] Milena Kresoja, Zorana Lu\u017eanin, and Irena Stojkovska. Adaptive stochastic approximation algorithm. Numerical Algorithms, 76(4):917\u2013937, 2017.   \n[51] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technical Report, 2009.   \n[52] Chandrashekar Lakshminarayanan and Csaba Szepesvari. Linear stochastic approximation: How far does constant step-size and iterate averaging go? In International Conference on Artificial Intelligence and Statistics, pages 1347\u20131355. PMLR, 2018.   \n[53] R\u00e9mi Leluc and Fran\u00e7ois Portier. Sgd with coordinate sampling: Theory and practice. The Journal of Machine Learning Research, 23(1):15470\u201315516, 2022.   \n[54] Qiang Li and Hoi-To Wai. State dependent performative prediction with stochastic approximation. In International Conference on Artificial Intelligence and Statistics, pages 3164\u20133186. PMLR, 2022.   \n[55] Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Differentiable architecture search. In International Conference on Learning Representations, 2019.   \n[56] Yin Liu and Sam Davanloo Tajbakhsh. Adaptive stochastic optimization algorithms for problems with biased oracles. arXiv preprint arXiv:2306.07810, 2023.   \n[57] Don McLeish. A general method for debiasing a Monte Carlo estimator. Monte Carlo methods and applications, 17(4):301\u2013315, 2011.   \n[58] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal adversarial perturbations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1765\u20131773, 2017.   \n[59] Eric Moulines and Francis Bach. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. In Advances in Neural Information Processing Systems, volume 24, 2011.   \n[60] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574\u2013 1609, 2009.   \n[61] Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions. Foundations of Computational Mathematics, 17:527\u2013566, 2017.   \n[62] Sebastian Nowozin. Debiasing evidence approximations: On importance-weighted autoencoders and jackknife variational inference. In International Conference on Learning Representations, 2018.   \n[63] Julie Nutini, Mark Schmidt, Issam Laradji, Michael Friedlander, and Hoyt Koepke. Coordinate descent converges faster with the gauss-southwell rule than random selection. In International Conference on Machine Learning, pages 1632\u20131641. PMLR, 2015.   \n[64] Jimmy Olsson and Johan Westerborn. Efficient particle-based online smoothing in general hidden Markov models: the PaRIS algorithm. Bernoulli, 23(3):1951\u20131996, 2017.   \n[65] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in PyTorch. In NIPS Autodiff Workshop, 2017.   \n[66] Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM Journal on Control and Optimization, 30(4):838\u2013855, 1992.   \n[67] Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In International Conference on Learning Representations, 2018.   \n[68] Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical Statistics, pages 400\u2013407, 1951.   \n[69] Tao Sun, Yuejiao Sun, and Wotao Yin. On Markov chain gradient descent. In Advances in Neural Information Processing Systems, volume 31, 2018.   \n[70] Vladislav B Tadic\u00b4 and Arnaud Doucet. Asymptotic bias of stochastic gradient search. In 2011 50th IEEE Conference on Decision and Control and European Control Conference, pages 722\u2013727. IEEE, 2011.   \n[71] Yee Teh, David Newman, and Max Welling. A collapsed variational bayesian inference algorithm for latent dirichlet allocation. In Advances in Neural Information Processing Systems, volume 19, 2006.   \n[72] Tijmen Tieleman, Geoffrey Hinton, et al. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 4(2):26\u201331, 2012.   \n[73] Qianqian Tong, Guannan Liang, and Jinbo Bi. Calibrating the adaptive learning rate to improve convergence of adam. Neurocomputing, 481:333\u2013356, 2022.   \n[74] Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp convergence over nonconvex landscapes. The Journal of Machine Learning Research, 21(1):9047\u20139076, 2020.   \n[75] Xidong Wu, Jianhui Sun, Zhengmian Hu, Junyi Li, Aidong Zhang, and Heng Huang. Federated conditional stochastic optimization. In Advances in Neural Information Processing Systems, volume 36, 2024.   \n[76] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.   \n[77] Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive methods for nonconvex optimization. In Advances in neural information processing systems, volume 31, 2018.   \n[78] Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.   \n[79] Fangyu Zou, Li Shen, Zequn Jie, Ju Sun, and Wei Liu. Weighted adagrad with unified momentum. arXiv preprint arXiv:1808.03408, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Supplementary Material for \u201cNon-asymptotic Analysis of Biased Adaptive Stochastic Approximation\u201d ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Convergence Proofs 17 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Proof of Theorem 4.1 17   \nA.2 Proof of Theorem 4.2 18   \nA.3 Proof of Corollary 4.3 . . 19   \nA.4 Proof of Corollary 4.4 . . 19   \nA.5 Proof of Corollary 4.5 . . 20   \nA.6 Proof of Theorem 4.6 . . 22   \nA.7 The Impact of regularization parameter $\\delta$ in Adam 25 ", "page_idx": 15}, {"type": "text", "text": "B IWAE / BR-IWAE 26 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Importance Weighted Autoencoder (IWAE) . . . 26   \nB.2 BR-IWAE . . . 28   \nB.3 Some Other Techniques for Reducing Bias 29 ", "page_idx": 15}, {"type": "text", "text": "C Application of Our Theorem to Bilevel and Conditional Stochastic Optimization 30 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Stochastic Bilevel Optimization . . 30   \nC.2 Conditional Stochastic Optimization 32 ", "page_idx": 15}, {"type": "text", "text": "D Some Other Examples of Biased Gradients with Control on Bias 34 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "D.1 Self-Normalized Importance Sampling . . 34   \nD.2 Sequential Monte Carlo Methods . . 34   \nD.3 Policy Gradient for Average Reward over Infinite Horizon 35   \nD.4 Zeroth-Order Gradient . . . 35   \nD.5 Compressed Stochastic Approximation: Coordinate Sampling . . 36 ", "page_idx": 15}, {"type": "text", "text": "E Experiment details and supplementary results 37 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "E.1 Experiment with a Synthetic Time-Dependent Bias . . 37   \nE.2 Additional Experiments of IWAE . 37 ", "page_idx": 15}, {"type": "text", "text": "A Convergence Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A.1 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We first establish a technical lemma which is essential for the proof. ", "page_idx": 16}, {"type": "text", "text": "Lemma A.1. Let $\\left(\\delta_{n}\\right)_{n\\geq0},\\left(\\gamma_{n}\\right)_{n\\geq1},\\left(\\eta_{n}\\right)_{n\\geq1},$ , and $(v_{n})_{n\\geq1}$ be some positive sequences satisfying the following assumptions. ", "page_idx": 16}, {"type": "text", "text": "\u2022 The sequence $\\delta_{n}$ follows the recursive relation: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\delta_{n}\\leq\\left(1-2\\omega\\gamma_{n}+\\eta_{n}\\gamma_{n}\\right)\\delta_{n-1}+v_{n}\\gamma_{n}\\;,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with $\\delta_{0}\\geq0$ and $\\omega>0$ . ", "page_idx": 16}, {"type": "text", "text": "\u2022 Let $n_{0}=\\operatorname*{inf}\\left\\{n\\geq1:\\eta_{n}\\leq\\omega\\right\\}$ , then for all $n\\geq n_{0}+1$ , we assume that $\\omega\\gamma_{n}\\leq1$ . ", "page_idx": 16}, {"type": "text", "text": "Then, for all $n\\in\\mathbb{N}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\delta_{n}\\leq\\exp\\bigg(-\\omega\\sum_{k=n/2}^{n}\\gamma_{k}\\bigg)\\exp\\bigg(2\\sum_{k=1}^{n}\\eta_{k}\\gamma_{k}\\bigg)\\left(\\delta_{0}+2\\operatorname*{max}_{1\\leq k\\leq n}\\frac{v_{k}}{\\eta_{k}}\\right)+\\frac{1}{\\omega}\\operatorname*{max}_{n/2\\leq k\\leq n}v_{k}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The proof is given in [35, Proposition 6.1] ", "page_idx": 16}, {"type": "text", "text": "Theorem A.2. Assume that $H I\\cdot H4$ hold. Let $\\theta_{n}\\in\\mathbb{R}^{d}$ be the $n$ -th iterate of the recursion (3). Then, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[V\\left(\\theta_{n}\\right)-V(\\theta^{*})\\right]\\leq\\Bigg(\\mathbb{E}\\left[V\\left(\\theta_{0}\\right)-V(\\theta^{*})\\right]+\\frac{2}{\\tilde{\\sigma}}\\operatorname*{max}_{1\\leq k\\leq n}\\frac{\\lambda_{k+1}v_{k}}{\\beta_{k+1}^{2}\\gamma_{k+1}}\\Bigg)\\exp\\Bigg(-\\frac{\\mu}{2}\\sum_{k=n/2}^{n}\\lambda_{k+1}\\gamma_{k+1}\\Bigg)}\\\\ &{\\qquad\\qquad\\qquad\\times\\exp\\Bigg(2\\sum_{k=1}^{n}C_{k}\\beta_{k+1}^{2}\\gamma_{k+1}^{2}\\Bigg)+\\frac{2}{\\mu}\\operatorname*{max}_{n/2\\leq k\\leq n}v_{k}\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tilde{\\sigma}=\\frac{\\tilde{\\sigma}_{2}L}{2}+\\tilde{\\sigma}_{1}L^{2},\\quad C_{k}=\\operatorname*{max}\\left\\{\\tilde{\\sigma},\\frac{\\mu^{2}\\lambda_{k+1}^{2}}{4\\beta_{k+1}^{2}}\\right\\}\\quad\\mathrm{and}\\quad v_{k}=r_{k+1}+\\frac{L\\sigma_{k}^{2}}{2}\\frac{\\beta_{k+1}^{2}}{\\lambda_{k+1}}\\gamma_{k+1}\\ .\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with the convention $C_{k}=1$ if $:\\!\\tilde{\\sigma}_{1}=\\tilde{\\sigma}_{2}=0$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. Since $V$ is $L$ -smooth (Assumption H2) and using the recursion (3) of Adaptive SA, we obtain: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{V\\left(\\theta_{n+1}\\right)\\leq V\\left(\\theta_{n}\\right)+\\left\\langle{\\nabla V\\left(\\theta_{n}\\right),\\theta_{n+1}-\\theta_{n}}\\right\\rangle+\\frac{L}{2}\\left\\Vert\\theta_{n+1}-\\theta_{n}\\right\\Vert^{2}}}}\\\\ {{\\displaystyle{\\leq V\\left(\\theta_{n}\\right)-\\gamma_{n+1}\\left\\langle{\\nabla V\\left(\\theta_{n}\\right),A_{n}H_{\\theta_{n}}\\left(X_{n+1}\\right)}\\right\\rangle+\\frac{L\\gamma_{n+1}^{2}}{2}\\left\\Vert A_{n}\\right\\Vert^{2}\\left\\Vert H_{\\theta_{n}}\\left(X_{n+1}\\right)\\right\\Vert^{2}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Writing $V_{n}=V\\left(\\theta_{n}\\right)-V(\\theta^{*})$ , we get ", "page_idx": 16}, {"type": "equation", "text": "$$\nV_{n+1}\\leq V_{n}-\\gamma_{n+1}\\left\\langle\\nabla V\\left(\\theta_{n}\\right),A_{n}H_{\\theta_{n}}\\left(X_{n+1}\\right)\\right\\rangle+\\frac{L}{2}\\gamma_{n+1}^{2}\\beta_{n+1}^{2}\\left\\Vert H_{\\theta_{n}}\\left(X_{n+1}\\right)\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, using H3, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[V_{n+1}\\right]\\leq\\mathbb{E}\\left[V_{n}\\right]-\\gamma_{n+1}\\mathbb{E}\\big[\\left\\langle\\nabla V\\left(\\theta_{n}\\right),A_{n}H_{\\theta_{n}}\\left(X_{n+1}\\right)\\right\\rangle\\big]+\\frac{L}{2}\\gamma_{n+1}^{2}\\beta_{n+1}^{2}\\sigma_{n}^{2}}\\\\ &{\\quad\\quad\\quad+\\frac{L}{2}\\gamma_{n+1}^{2}\\beta_{n+1}^{2}\\Big(\\tilde{\\sigma}_{1}\\mathbb{E}[\\|\\nabla V\\left(\\theta_{n}\\right)\\|^{2}]+\\tilde{\\sigma}_{2}\\mathbb{E}[V_{n}]\\Big)}\\\\ &{\\quad\\quad\\quad\\leq\\bigg(1+\\frac{\\tilde{\\sigma}_{2}L}{2}\\beta_{n+1}^{2}\\gamma_{n+1}^{2}\\bigg)\\mathbb{E}[V_{n}]-\\gamma_{n+1}\\left(\\lambda_{n+1}-\\frac{\\tilde{\\sigma}_{1}L}{2}\\gamma_{n+1}\\beta_{n+1}^{2}\\right)\\mathbb{E}[\\|\\nabla V\\left(\\theta_{n}\\right)\\|^{2}]}\\\\ &{\\quad\\quad\\quad+\\gamma_{n+1}\\lambda_{n+1}r_{n+1}+\\frac{L\\sigma_{n}^{2}}{2}\\gamma_{n+1}^{2}\\beta_{n+1}^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Given the smoothness condition (5), with $\\theta^{\\prime}=\\theta_{n}$ and $\\begin{array}{r}{\\theta=\\theta_{n}-\\frac{1}{L}\\nabla V\\left(\\theta_{n}\\right)}\\end{array}$ , we derive: ", "page_idx": 17}, {"type": "equation", "text": "$$\nV\\left(\\theta^{*}\\right)\\leq V\\left(\\theta_{n}\\right)-\\frac{1}{L}\\left\\Vert\\nabla V\\left(\\theta_{n}\\right)\\right\\Vert^{2}+\\frac{1}{2L}\\left\\Vert\\nabla V\\left(\\theta_{n}\\right)\\right\\Vert^{2}\\;,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\|\\nabla V\\left(\\theta_{n}\\right)\\right\\|^{2}\\leq2L V_{n}\\ .\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using the above inequality and the Polyak-\u0141ojasiewicz condition (H1), we obtain: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[V_{n+1}\\right]\\leq\\Big(1-\\mu\\lambda_{n+1}\\gamma_{n+1}+\\Big(\\displaystyle\\frac{\\tilde{\\sigma}_{2}L}{2}+\\tilde{\\sigma}_{1}L^{2}\\Big)\\beta_{n+1}^{2}\\gamma_{n+1}^{2}\\Big)\\mathbb{E}\\left[V_{n}\\right]}\\\\ &{\\qquad\\qquad+\\gamma_{n+1}\\lambda_{n+1}r_{n+1}+\\displaystyle\\frac{L\\sigma_{n}^{2}}{2}\\gamma_{n+1}^{2}\\beta_{n+1}^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By choosing $\\bar{\\gamma}_{n+1}=\\lambda_{n+1}\\gamma_{n+1}$ , we get: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[V_{n+1}\\right]\\leq\\Big(1-\\mu\\bar{\\gamma}_{n+1}+\\Big(\\frac{\\tilde{\\sigma}_{2}L}{2}+\\tilde{\\sigma}_{1}L^{2}\\Big)\\frac{\\beta_{n+1}^{2}}{\\lambda_{n+1}^{2}}\\bar{\\gamma}_{n+1}^{2}\\Big)\\mathbb{E}\\left[V_{n}\\right]}\\\\ &{\\qquad\\qquad+\\left.\\bar{\\gamma}_{n+1}r_{n+1}+\\frac{L\\sigma_{n}^{2}}{2}\\frac{\\beta_{n+1}^{2}}{\\lambda_{n+1}}\\bar{\\gamma}_{n+1}\\gamma_{n+1}\\,\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In order to satisfy the assumptions of Lemma A.1, consider ", "page_idx": 17}, {"type": "equation", "text": "$$\nC_{n}=\\operatorname*{max}\\left\\{\\tilde{\\sigma},\\frac{\\mu^{2}\\lambda_{n+1}^{2}}{4\\beta_{n+1}^{2}}\\right\\}\\quad\\mathrm{with}\\quad\\tilde{\\sigma}=\\frac{\\tilde{\\sigma}_{2}L}{2}+\\tilde{\\sigma}_{1}L^{2}\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, since $C_{n}\\geq\\tilde{\\sigma}$ , we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[V_{n+1}\\right]\\leq\\Big(1-\\mu\\bar{\\gamma}_{n+1}+\\frac{C_{n}\\beta_{n+1}^{2}}{\\lambda_{n+1}^{2}}\\bar{\\gamma}_{n+1}^{2}\\Big)\\mathbb{E}\\left[V_{n}\\right]+\\bar{\\gamma}_{n+1}r_{n+1}+\\frac{L\\sigma_{n}^{2}}{2}\\frac{\\beta_{n+1}^{2}}{\\lambda_{n+1}}\\bar{\\gamma}_{n+1}\\gamma_{n+1}\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now, using lemma A.1 by choosing: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\delta_{n}=\\mathbb{E}\\left[V_{n}\\right],\\quad\\eta_{n}=\\frac{C_{n}\\beta_{n+1}^{2}}{\\lambda_{n+1}^{2}}\\bar{\\gamma}_{n+1}\\;,\\quad\\omega=\\frac{\\mu}{2},\\quad v_{n}=r_{n+1}+\\frac{L\\sigma_{n}^{2}}{2}\\frac{\\beta_{n+1}^{2}}{\\lambda_{n+1}}\\gamma_{n+1}\\;,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[V\\left(\\theta_{n}\\right)-V(\\theta^{*})\\right]\\leq\\left(\\mathbb{E}\\left[V\\left(\\theta_{0}\\right)-V(\\theta^{*})\\right]+\\frac{2}{\\tilde{\\sigma}}\\underset{1\\leq k\\leq n}{\\operatorname*{max}}\\frac{v_{k}\\lambda_{k+1}^{2}}{\\beta_{k+1}^{2}\\bar{\\gamma}_{k+1}}\\right)\\exp\\Big(-\\frac{\\mu}{2}\\sum_{k=n/2}^{n}\\bar{\\gamma}_{k+1}\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\times\\exp\\Big(2\\sum_{k=1}^{n}C_{k}\\beta_{k+1}^{2}\\bar{\\gamma}_{k+1}^{2}\\big/\\lambda_{k+1}^{2}\\Big)+\\frac{2}{\\mu}\\underset{n/2\\leq k\\leq n}{\\operatorname*{max}}\\{v_{k}\\}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which concludes the proof by taking $\\bar{\\gamma}_{n+1}=\\lambda_{n+1}\\gamma_{n+1}$ . ", "page_idx": 17}, {"type": "text", "text": "A.2 Proof of Theorem 4.2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "By $_{\\mathrm{H}2}$ , using (5), we obtain: ", "page_idx": 17}, {"type": "equation", "text": "$$\nV\\left(\\theta_{k+1}\\right)\\leq V\\left(\\theta_{k}\\right)+\\left\\langle\\nabla V\\left(\\theta_{k}\\right),\\theta_{k+1}-\\theta_{k}\\right\\rangle+\\frac{L}{2}\\left\\Vert\\theta_{k+1}-\\theta_{k}\\right\\Vert^{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which, using the recursion (3) of Adaptive SA and $\\mathrm{H4}$ , yields: ", "page_idx": 17}, {"type": "equation", "text": "$$\nV\\left(\\theta_{k+1}\\right)\\leq V\\left(\\theta_{k}\\right)-\\gamma_{k+1}\\left\\langle\\nabla V\\left(\\theta_{k}\\right),A_{k}H_{\\theta_{k}}\\left(X_{k+1}\\right)\\right\\rangle+\\delta_{k+1}\\left\\Vert H_{\\theta_{k}}\\left(X_{k+1}\\right)\\right\\Vert^{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with $\\delta_{k+1}=L\\gamma_{k+1}^{2}\\beta_{k+1}^{2}/2$ . Using Assumption H3, we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[V(\\theta_{k+1})]\\leq\\mathbb{E}[V(\\theta_{k})]-\\gamma_{k+1}\\lambda_{k+1}\\mathbb{E}\\big[\\|\\nabla V(\\theta_{k})\\|^{2}\\big]+\\gamma_{k+1}\\lambda_{k+1}r_{k+1}+\\delta_{k+1}\\sigma_{k}^{2}}\\\\ &{\\qquad\\qquad\\qquad+\\,\\delta_{k+1}\\big(\\tilde{\\sigma}_{1}\\mathbb{E}[\\|\\nabla V(\\theta_{k})\\|^{2}]+\\tilde{\\sigma}_{2}\\mathbb{E}\\left[V\\left(\\theta_{k}\\right)-V\\left(\\theta^{*}\\right)\\right]\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{k+1}\\Big(\\lambda_{k+1}-\\displaystyle\\frac{L\\tilde{\\sigma}_{1}}{2}\\gamma_{k+1}\\beta_{k+1}^{2}\\Big)\\mathbb{E}\\big[\\left\\|\\nabla V\\left(\\theta_{k}\\right)\\right\\|^{2}\\big]}\\\\ &{\\quad\\leq\\left(1+\\tilde{\\sigma}_{2}\\delta_{k+1}\\right)\\left(\\mathbb{E}\\left[V\\left(\\theta_{k}\\right)\\right]-V\\left(\\theta^{*}\\right)\\right)-\\left(\\mathbb{E}\\left[V\\left(\\theta_{k+1}\\right)\\right]-V\\left(\\theta^{*}\\right)\\right)}\\\\ &{\\quad\\quad+\\gamma_{k+1}\\lambda_{k+1}r_{k+1}+\\delta_{k+1}\\sigma_{k}^{2}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let us now consider the sequence of weights $w_{k}$ defined by $w_{0}=1$ and $\\begin{array}{r}{w_{k}=\\prod_{j=1}^{k}\\big(1+\\tilde{\\sigma}_{2}\\delta_{j}\\big)^{-1}}\\end{array}$ . Then, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{k+1}\\gamma_{k+1}\\Big(\\lambda_{k+1}-\\displaystyle\\frac{L\\tilde{\\sigma}_{1}}{2}\\gamma_{k+1}\\beta_{k+1}^{2}\\Big)\\mathbb{E}\\big[\\left\\|\\nabla V\\left(\\theta_{k}\\right)\\right\\|^{2}\\big]}\\\\ &{\\quad\\leq w_{k}\\big(\\mathbb{E}[V\\left(\\theta_{k}\\right)]-V\\left(\\theta^{*}\\right)\\big)-w_{k+1}\\big(\\mathbb{E}\\left[V\\left(\\theta_{k+1}\\right)\\right]-V\\left(\\theta^{*}\\right)\\big)}\\\\ &{\\quad\\quad+w_{k+1}\\gamma_{k+1}\\lambda_{k+1}r_{k+1}+w_{k+1}\\delta_{k+1}\\sigma_{k}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In the sequel, let us denote $V_{n}=V\\left(\\theta_{n}\\right)-V\\left(\\theta^{*}\\right)$ , so that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=0}^{n}w_{k+1}\\gamma_{k+1}\\lambda_{k+1}\\left(1-\\frac{L\\tilde{\\sigma}_{1}}{2\\lambda_{k+1}}\\gamma_{k+1}\\beta_{k+1}^{2}\\right)\\mathbb{E}\\left[\\left\\Vert\\nabla V\\left(\\theta_{k}\\right)\\right\\Vert^{2}\\right]}\\\\ &{\\displaystyle\\leq w_{0}\\mathbb{E}\\left[V_{0}\\right]-w_{n+1}\\mathbb{E}\\left[V_{n+1}\\right]+\\frac{1}{2}\\displaystyle\\sum_{k=0}^{n}w_{k+1}\\gamma_{k+1}\\lambda_{k+1}r_{k+1}+\\displaystyle\\sum_{k=0}^{n}w_{k+1}\\delta_{k+1}\\sigma_{k}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, given that $\\gamma_{k+1}\\leq\\lambda_{k+1}/(L\\tilde{\\sigma}_{1}\\beta_{k+1}^{2})$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2}\\mathbb{E}\\left[\\displaystyle\\sum_{k=0}^{n}w_{k+1}\\gamma_{k+1}\\lambda_{k+1}\\left\\|\\nabla V\\left(\\theta_{k}\\right)\\right\\|^{2}\\right]\\leq w_{0}\\mathbb{E}\\left[V_{0}\\right]-w_{n+1}\\mathbb{E}\\left[V_{n+1}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\sum_{k=0}^{1}\\displaystyle\\sum_{k=0}^{n}w_{k+1}\\gamma_{k+1}\\lambda_{k+1}r_{k+1}+\\displaystyle\\sum_{k=0}^{n}w_{k+1}\\delta_{k+1}\\sigma_{k}^{2}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Consequently, by definition of the discrete random variable $R$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\Vert\\nabla V\\left(\\theta_{R}\\right)\\Vert^{2}\\right]=\\underset{k=0}{\\overset{n}{\\sum}}\\frac{w_{k+1}\\gamma_{k+1}\\lambda_{k+1}}{\\sum_{j=0}^{n}w_{j+1}\\gamma_{j+1}\\lambda_{j+1}}\\mathbb{E}\\left[\\Vert\\nabla V\\left(\\theta_{k}\\right)\\Vert^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\frac{\\mathbb{E}\\left[V_{0}\\right]-w_{n+1}\\mathbb{E}\\left[V_{n+1}\\right]+\\sum_{k=0}^{n}w_{k+1}\\gamma_{k+1}r_{k+1}+\\sum_{k=0}^{n}w_{k+1}\\delta_{k+1}\\sigma_{k}^{2}}{\\sum_{j=0}^{n}w_{j+1}\\gamma_{j+1}\\lambda_{j+1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which concludes the proof by noting that $V(\\theta_{n+1})\\geq V\\left(\\theta^{*}\\right)$ . ", "page_idx": 18}, {"type": "text", "text": "A.3 Proof of Corollary 4.3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The proof is a direct consequence of the fact that for a sufficiently large $n$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{n}{\\frac{1}{k^{s}}}={\\left\\{\\mathcal{O}\\left(n^{-s+1}\\right)\\quad{\\mathrm{if~}}0\\leq s<1\\;,}\\\\ {{\\mathcal{O}}\\left(1\\right)\\qquad}&{{\\mathrm{if~}}s>1\\;,}\\\\ {{\\mathcal{O}}\\left(\\log n\\right)\\qquad{\\mathrm{if~}}s=1\\;.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "A.4 Proof of Corollary 4.4 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "By $_{\\mathrm{H}2}$ , using (5), we obtain: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{V\\left(\\theta_{k+1}\\right)\\leq V\\left(\\theta_{k}\\right)+\\left\\langle{\\nabla V\\left(\\theta_{k}\\right),\\theta_{k+1}-\\theta_{k}}\\right\\rangle+\\frac{L}{2}\\left\\Vert\\theta_{k+1}-\\theta_{k}\\right\\Vert^{2}}}}\\\\ {{\\displaystyle{\\leq V\\left(\\theta_{k}\\right)-\\gamma_{k+1}\\left\\langle{\\nabla V\\left(\\theta_{k}\\right),A_{k}H_{\\theta_{k}}\\left(X_{k+1}\\right)}\\right\\rangle+\\frac{L\\gamma_{k+1}^{2}}{2}\\left\\Vert A_{k}\\right\\Vert^{2}\\left\\Vert H_{\\theta_{k}}\\left(X_{k+1}\\right)\\right\\Vert^{2}},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which, using H4 and H5 yields: ", "page_idx": 19}, {"type": "equation", "text": "$$\nV\\left(\\theta_{k+1}\\right)\\leq V\\left(\\theta_{k}\\right)-\\gamma_{k+1}\\left\\langle\\nabla V\\left(\\theta_{k}\\right),A_{k}H_{\\theta_{k}}\\left(X_{k+1}\\right)\\right\\rangle+\\frac{L}{2}\\gamma_{k+1}^{2}\\beta_{k+1}^{2}M^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using H3, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}[V(\\theta_{k+1})|\\mathcal{F}_{k}]\\le V(\\theta_{k})-\\gamma_{k+1}\\lambda_{n+1}\\|\\nabla V(\\theta_{k})\\|^{2}+\\gamma_{k+1}\\lambda_{k+1}r_{k+1}+\\frac{L M^{2}}{2}\\gamma_{k+1}^{2}\\beta_{k+1}^{2}\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\gamma_{k+1}\\lambda_{k+1}\\left\\Vert\\nabla V\\left(\\theta_{k}\\right)\\right\\Vert^{2}\\leq V\\left(\\theta_{k}\\right)-\\mathbb{E}\\left[V\\left(\\theta_{k+1}\\right)\\vert\\mathcal{F}_{k}\\right]+\\gamma_{k+1}\\lambda_{k+1}r_{k+1}+\\frac{L M^{2}}{2}\\gamma_{k+1}^{2}\\beta_{k+1}^{2}\\;,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{k=0}^{n}\\gamma_{k+1}\\lambda_{k+1}\\mathbb{E}\\left[\\left\\Vert\\nabla V\\left(\\theta_{k}\\right)\\right\\Vert^{2}\\right]\\leq\\mathbb{E}\\left[V\\left(\\theta_{0}\\right)-V\\left(\\theta_{n+1}\\right)\\right]+\\displaystyle\\sum_{k=0}^{n}\\gamma_{k+1}\\lambda_{k+1}r_{k+1}}&{}\\\\ {+\\displaystyle\\frac{L M^{2}}{2}\\sum_{k=0}^{n}\\gamma_{k+1}^{2}\\beta_{k+1}^{2}~.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Consequently, by definition of the discrete random variable $R$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\left\\Vert\\nabla V\\left(\\theta_{R}\\right)\\right\\Vert^{2}\\right]=\\frac{1}{n}\\displaystyle\\sum_{k=0}^{n}\\mathbb{E}\\left[\\left\\Vert\\nabla V\\left(\\theta_{k}\\right)\\right\\Vert^{2}\\right]}&{}\\\\ &{\\leq\\displaystyle\\sum_{k=0}^{n}\\frac{\\gamma_{k+1}\\lambda_{k+1}}{\\sqrt{n}}\\mathbb{E}\\left[\\left\\Vert\\nabla V\\left(\\theta_{k}\\right)\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\frac{V_{0,n}+\\sum_{k=0}^{n}\\gamma_{k+1}\\lambda_{k+1}r_{k+1}+L M^{2}\\sum_{k=0}^{n}\\gamma_{k+1}^{2}\\beta_{k+1}^{2}/2}{\\sqrt{n}}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $V_{0,n}=\\mathbb{E}[V(\\theta_{0})-V(\\theta_{n+1})]$ , which conclude the proof by noting that $V(\\theta_{n+1})\\geq V(\\theta^{*})$ . ", "page_idx": 19}, {"type": "text", "text": "A.5 Proof of Corollary 4.5 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Here, we consider the case where the regularization is non-increasing, i.e., where $\\delta=\\beta_{n+1}^{-2}$ . The constant case is strictly analogous. To verify $\\mathrm{H4}$ , we demonstrate that the control of the maximum and minimum eigenvalues is satisfied for Adagrad and RMSProp. ", "page_idx": 19}, {"type": "text", "text": "Adagrad ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 Lower bound for the smallest eigenvalue of $\\mathbf{A_{n}}$ . By assumption H5, we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|{\\frac{1}{n+1}}\\sum_{k=0}^{n}H_{\\theta_{k}}(X_{k+1})H_{\\theta_{k}}(X_{k+1})^{\\top}\\right\\|\\leq M^{2}\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This implies that: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda_{\\operatorname*{min}}(A_{n})=\\lambda_{\\operatorname*{max}}\\Bigg(\\beta_{n+1}^{-2}I_{d}+\\operatorname{Diag}\\Bigg(\\frac{1}{n+1}\\displaystyle\\sum_{k=0}^{n}H_{\\theta_{k}}(X_{k+1})H_{\\theta_{k}}(X_{k+1})^{\\top}\\Bigg)\\Bigg)^{-1/2}}\\\\ &{\\qquad\\qquad\\geq(\\beta_{1}^{-2}+M^{2})^{-1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "\u2022 Upper bound for the largest eigenvalue of $\\mathbf{A_{n}}$ . ", "text_level": 1, "page_idx": 19}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{max}}(A_{n})=\\lambda_{\\operatorname*{min}}\\left(\\beta_{n+1}^{-2}I_{d}+\\operatorname{Diag}\\left(\\frac{1}{n+1}\\sum_{k=0}^{n}H_{\\theta_{k}}(X_{k+1})H_{\\theta_{k}}(X_{k+1})^{\\top}\\right)\\right)^{-1/2}\\leq\\beta_{n+1}\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, by setting $\\lambda_{n+1}\\,=\\,(\\beta_{1}^{-2}+M^{2})^{-1/2}$ and $\\beta_{n}\\,=\\,C_{\\beta}n^{\\beta}$ , we have $\\lambda\\,=\\,0$ and one can arbitrarily choose $\\beta$ (one can take $\\beta=0$ for the constant regularization case). ", "page_idx": 20}, {"type": "text", "text": "RMSProp ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 Lower bound for the smallest eigenvalue of $\\mathbf{A_{n}}$ . By assumption H5, we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\Vert V_{n}\\right\\Vert\\leq(1-\\rho)\\sum_{k=1}^{n}\\rho^{n-k}\\left\\Vert H_{\\theta_{k}}(X_{k+1})\\right\\Vert^{2}\\leq M^{2}(1-\\rho)\\sum_{k=1}^{n}\\rho^{n-k}\\leq M^{2}\\;,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we used the fact that $\\begin{array}{r}{\\sum_{k=1}^{n}\\rho^{n-k}\\le(1-\\rho)^{-1}}\\end{array}$ . This implies that: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda_{\\operatorname*{min}}(A_{n})=\\lambda_{\\operatorname*{max}}\\left(\\beta_{n+1}^{-2}I_{d}+\\mathrm{Diag}\\left(V_{n}\\right)\\right)^{-1/2}\\ge(\\beta_{1}^{-2}+M^{2})^{-1/2}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "\u2022 Upper bound for the largest eigenvalue of $\\mathbf{A_{n}}$ . Note that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda_{\\operatorname*{max}}(A_{n})=\\lambda_{\\operatorname*{min}}\\left(\\beta_{n+1}^{-2}I_{d}+\\mathrm{Diag}\\left(V_{n}\\right)\\right)^{-1/2}\\leq\\beta_{n+1}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, under H2, H3(i), and H5, we can conclude that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|\\nabla V\\left(\\theta_{R}\\right)\\right\\|^{2}\\right]=\\mathcal{O}\\left(\\frac{\\log n}{\\sqrt{n}}+b_{n}\\right)\\;,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $b_{n}$ corresponds to the bias which comes from $r_{n}$ in H3(i). Choosing $r_{n}=C_{r}n^{-r}$ , we get: ", "page_idx": 20}, {"type": "equation", "text": "$$\nb_{n}={\\left\\{\\begin{array}{l l}{{\\mathcal{O}}\\left(n^{-r}\\right)}&{{\\mathrm{if~}}r<1/2\\ ,}\\\\ {{\\mathcal{O}}\\left(n^{-1/2}\\right)}&{{\\mathrm{if~}}r>1/2\\ ,}\\\\ {{\\mathcal{O}}\\left(n^{-1/2}\\log n\\right)}&{{\\mathrm{if~}}r=1/2\\ .}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now, we show that under the control of bias, i.e., ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{E}\\left[H_{\\theta_{n}}\\left(X_{n+1}\\right)\\vert\\mathcal{F}_{n}\\right]-\\nabla V\\left(\\theta_{n}\\right)\\right\\|\\leq C_{\\alpha}n^{-\\alpha}\\,,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "we can verify H3(i) with a similar bound on the bias, where $r=2\\alpha$ . This yields the bias term $b_{n}$ as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\nb_{n}={\\left\\{\\begin{array}{l l}{\\mathcal{O}\\left(n^{-2\\alpha}\\right)}&{{\\mathrm{if~}}\\alpha<1/4{\\mathrm{~,~}}}\\\\ {\\mathcal{O}\\left(n^{-1/2}\\right)}&{{\\mathrm{if~}}\\alpha>1/4{\\mathrm{~,~}}}\\\\ {\\mathcal{O}\\left(n^{-1/2}\\log n\\right)}&{{\\mathrm{if~}}\\alpha=1/4{\\mathrm{~.~}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Verifying Assumption H3 (i) for Adagrad. Using the tower property, we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\langle\\nabla V\\left(\\theta_{n}\\right),A_{n}H_{\\theta_{n}}\\left(X_{n+1}\\right)\\right\\rangle\\right]=\\mathbb{E}\\left[\\mathbb{E}\\left[\\left\\langle\\nabla V\\left(\\theta_{n}\\right),A_{n}H_{\\theta_{n}}\\left(X_{n+1}\\right)\\right\\rangle\\big|\\mathcal{F}_{n}\\right]\\right],\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $({\\mathcal{F}}_{n})_{n\\geq0}$ represents the filtration generated by the random variables $(\\theta_{0},\\{X_{k}\\}_{k\\le n})$ . Let $\\tilde{A}_{n}$ be an adaptive ${\\mathcal{F}}_{n}$ -measurable matrix. Then, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\langle\\nabla V\\left(\\theta_{n}\\right),A_{n}H_{\\theta_{n}}\\left(X_{n+1}\\right)\\right\\rangle\\big|\\mathcal{F}_{n}\\right]=\\underbrace{\\left\\langle\\nabla V\\left(\\theta_{n}\\right),\\tilde{A}_{n}\\mathbb{E}\\left[H_{\\theta_{n}}\\left(X_{n+1}\\right)\\big|\\mathcal{F}_{n}\\right]\\right\\rangle}_{\\mathrm{Treated~a~in~SGD~but~with~}\\lambda_{\\mathrm{min}}\\left(\\tilde{A}_{n}\\right)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\underbrace{\\mathbb{E}\\left[\\left\\langle\\nabla V\\left(\\theta_{n}\\right),(A_{n}-\\tilde{A}_{n})H_{\\theta_{n}}\\left(X_{n+1}\\right)\\right\\rangle\\big|\\mathcal{F}_{n}\\right]}_{\\mathrm{Treat~}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Control error between $A_{n}$ and $\\tilde{A}_{n}$ ", "page_idx": 20}, {"type": "text", "text": "We only verify Assumption H3(i) for Adagrad algorithm since it is analogous to RMSProp. Consider $A_{n}$ given by: ", "page_idx": 20}, {"type": "equation", "text": "$$\nA_{n}=\\left(\\operatorname{diag}\\left(\\beta_{n+1}^{-2}I_{d}+\\frac{1}{n+1}\\sum_{k=0}^{n}H_{\\theta_{k}}\\left(X_{k+1}\\right)H_{\\theta_{k}}\\left(X_{k+1}\\right)^{\\top}\\right)\\right)^{-1/2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "First, writing ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tilde{A}_{n}=\\left(\\operatorname{diag}\\left(\\beta_{n+1}^{-2}I_{d}+\\frac{1}{n+1}\\sum_{k=0}^{n-1}H_{\\theta_{k}}\\left(X_{k+1}\\right)H_{\\theta_{k}}\\left(X_{k+1}\\right)^{\\top}\\right)\\right)^{-1/2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and denoting by $A[i]$ the i-th element of the diagonal of a matrix $A$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\nA_{n}[i]-\\tilde{A}_{n}[i]=u_{n}^{-1/2}\\left(v_{n}^{1/2}-u_{n}^{1/2}\\right)v_{n}^{-1/2}\\le0\\;,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where ", "page_idx": 21}, {"type": "equation", "text": "$$\nu_{n}=\\beta_{n+1}^{-2}+\\frac{1}{n+1}\\sum_{k=0}^{n}\\left(H_{\\theta_{k}}\\left(X_{k+1}\\right)[i]\\right)^{2}\\quad\\mathrm{and}\\quad v_{n}=\\beta_{n+1}^{-2}+\\frac{1}{n+1}\\sum_{k=0}^{n-1}\\left(H_{\\theta_{k}}\\left(X_{k+1}\\right)[i]\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then, since $u_{n}\\geq v_{n}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A_{n}[i]-\\tilde{A}_{n}[i]=\\frac{v_{n}-u_{n}}{\\sqrt{u_{n}v_{n}}\\,\\left(\\sqrt{u_{n}}+\\sqrt{v}_{n}\\right)}\\ge-\\frac{1}{n+1}\\left(H_{\\theta_{n}}\\left(X_{n+1}\\right)[i]\\right)^{2}\\frac{1}{2v_{n}^{3/2}}}\\\\ {\\ge-\\frac{\\beta_{n+1}^{3}}{n+1}\\left(H_{\\theta_{n}}\\left(X_{n+1}\\right)[i]\\right)^{2}.\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since the bias of $H_{\\theta_{n}}\\left(X_{n+1}\\right)$ is bounded by $\\tilde{b}_{n}:=C_{\\alpha}n^{-\\alpha}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\langle\\nabla V\\left(\\theta_{n}\\right),A_{n}H_{\\theta_{n}}\\left(X_{n+1}\\right)\\right\\rangle\\big|\\mathcal{F}_{n}\\right]}\\\\ &{\\quad=\\left\\langle\\nabla V\\left(\\theta_{n}\\right),\\tilde{A}_{n}\\mathbb{E}\\left[H_{\\theta_{n}}\\left(X_{n+1}\\right)\\big|\\mathcal{F}_{n}\\right]\\right\\rangle+\\mathbb{E}\\left[\\left\\langle\\nabla V\\left(\\theta_{n}\\right),\\left(A_{n}-\\tilde{A}_{n}\\right)H_{\\theta_{n}}\\left(X_{n+1}\\right)\\right\\rangle\\big|\\mathcal{F}_{n}\\right]}\\\\ &{\\quad\\ge\\lambda_{\\operatorname*{min}}\\left(\\tilde{A}_{n}\\right)\\left\\|\\nabla V\\left(\\theta_{n}\\right)\\right\\|^{2}-\\lambda_{\\operatorname*{max}}\\left(\\tilde{A}_{n}\\right)\\left\\|\\nabla V\\left(\\theta_{n}\\right)\\right\\|\\tilde{b}_{n}}\\\\ &{\\quad\\quad-\\left\\|\\nabla V\\left(\\theta_{n}\\right)\\right\\|\\frac{\\beta_{n+1}^{3}}{n+1}\\mathbb{E}\\left[\\left\\|H_{\\theta_{n}}\\left(X_{n+1}\\right)\\right\\|^{3}\\big|\\mathcal{F}_{n}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "As $H_{\\theta_{n}}\\left(X_{n+1}\\right)$ and the gradient of $V$ are uniformly bounded by $M$ ${\\cal M},\\lambda_{\\mathrm{min}}(\\tilde{A}_{n})\\ge(\\beta_{1}^{-2}+M^{2})^{-1/2}$ , so that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\langle\\nabla V\\left(\\theta_{n}\\right),A_{n}H_{\\theta_{n}}\\left(X_{n+1}\\right)\\rangle\\,|\\mathcal{F}_{n}\\right]\\ge\\frac{1}{\\sqrt{\\beta_{1}^{-2}+M^{2}}}\\,\\|\\nabla V\\left(\\theta_{n}\\right)\\|^{2}-\\beta_{n+1}M\\tilde{b}_{n}-M^{4}\\frac{\\beta_{n+1}^{3}}{n+1}\\;,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and Assumption H3(i) is satisfied with $\\lambda_{n+1}=(\\beta_{1}^{-2}+M^{2})^{-1/2}$ and $r_{n+1}=M\\beta_{n+1}^{2}\\tilde{b}_{n}^{2}/\\lambda_{n+1}+$ $M^{4}\\beta_{n+1}^{3}/(n+1)$ . ", "page_idx": 21}, {"type": "text", "text": "A.6 Proof of Theorem 4.6 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The proof of this theorem is inspired by [67] and [73], considering biased gradient estimators and decreasing step sizes. We define the operation $\\operatorname*{max}(D_{1},D_{2})$ for diagonal matrices $D_{1}$ and $D_{2}$ as the matrix formed by taking the maximum between the diagonal elements of $D_{1}$ and $D_{2}$ . We say that the sequence $(A_{n})_{n\\geq1}$ of diagonal matrices is decreasing if all diagonal terms are decreasing, in other words, if all eigenvalues are decreasing. ", "page_idx": 21}, {"type": "text", "text": "Let $\\tilde{\\theta}_{k+1}=\\theta_{k+1}+\\kappa\\left(\\theta_{k+1}-\\theta_{k}\\right)$ , for $k\\ge1,\\kappa\\in[0,1)$ and $m_{k}\\,=\\,\\rho_{1}m_{k-1}+(1-\\rho_{1})g_{k}$ with $g_{k}=H_{\\theta_{k}}(X_{k+1})$ . Using the recursion of AMSGRAD, we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\theta}_{k+1}-\\tilde{\\theta}_{k}=(1+\\kappa)\\theta_{k+1}-(1+2\\kappa)\\theta_{k}+\\kappa\\theta_{k-1}=(1+\\kappa)\\left(\\theta_{k+1}-\\theta_{k}\\right)-\\kappa\\left(\\theta_{k}-\\theta_{k-1}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=-(1+\\kappa)\\gamma_{k+1}A_{k}m_{k}+\\kappa\\gamma_{k}A_{k-1}m_{k-1}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Choosing $\\kappa=\\rho_{1}/(1-\\rho_{1})$ , we can rewrite it as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\tilde{\\theta}_{k+1}-\\tilde{\\theta}_{k}=\\kappa\\left(\\gamma_{k}A_{k-1}-\\gamma_{k+1}A_{k}\\right)m_{k-1}-\\gamma_{k+1}A_{k}g_{k}\\ .\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By Assumption H2, $V$ is $L$ -smooth, using the recursion of AMSGRAD together with a Taylor expansion with $\\tilde{\\theta}_{k}$ , we obtain: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V(\\tilde{\\theta}_{k+1})\\leq V\\left(\\tilde{\\theta}_{k}\\right)+\\left\\langle\\nabla V\\left(\\tilde{\\theta}_{k}\\right),\\tilde{\\theta}_{k+1}-\\tilde{\\theta}_{k}\\right\\rangle+\\frac{L}{2}\\left\\Vert\\tilde{\\theta}_{k+1}-\\tilde{\\theta}_{k}\\right\\Vert^{2}}\\\\ &{\\qquad\\qquad\\leq V\\left(\\tilde{\\theta}_{k}\\right)-\\gamma_{k+1}\\left\\langle\\nabla V\\left(\\tilde{\\theta}_{k}\\right),A_{k}g_{k}\\right\\rangle+\\kappa\\left\\langle\\nabla V\\left(\\tilde{\\theta}_{k}\\right),\\left(\\gamma_{k}A_{k-1}-\\gamma_{k+1}A_{k}\\right)m_{k-1}\\right\\rangle}\\\\ &{\\qquad\\qquad+\\ensuremath{L\\gamma_{k+1}^{2}}\\left\\Vert A_{k}g_{k}\\right\\Vert^{2}+\\ensuremath{L\\kappa^{2}}\\left\\Vert\\left(\\gamma_{k}A_{k-1}-\\gamma_{k+1}A_{k}\\right)m_{k-1}\\right\\Vert^{2}}\\\\ &{\\qquad\\qquad\\leq V\\left(\\tilde{\\theta}_{k}\\right)+\\ensuremath{T}_{1,k}+\\ensuremath{T}_{2,k}+\\ensuremath{T}_{3,k}+\\ensuremath{T}_{4,k}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{1,k}=-\\gamma_{k+1}\\left\\langle\\nabla V\\left(\\theta_{k}\\right),A_{k}g_{k}\\right\\rangle+L\\gamma_{k+1}^{2}\\left\\|A_{k}g_{k}\\right\\|^{2},}\\\\ &{T_{2,k}=-\\gamma_{k+1}\\left\\langle\\nabla V\\left(\\tilde{\\theta}_{k}\\right)-\\nabla V\\left(\\theta_{k}\\right),A_{k}g_{k}\\right\\rangle,}\\\\ &{T_{3,k}=\\kappa\\left\\langle\\nabla V\\left(\\tilde{\\theta}_{k}\\right),\\left(\\gamma_{k}A_{k-1}-\\gamma_{k+1}A_{k}\\right)m_{k-1}\\right\\rangle,}\\\\ &{T_{4,k}=L\\kappa^{2}\\left\\|\\left(\\gamma_{k}A_{k-1}-\\gamma_{k+1}A_{k}\\right)m_{k-1}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note first that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=1}^{n}\\mathbb{E}\\left[T_{1,k}\\right]=-\\sum_{k=1}^{n}\\gamma_{k+1}\\mathbb{E}\\left[\\left\\langle\\nabla V\\left(\\theta_{k}\\right),A_{k}g_{k}\\right\\rangle\\right]+L\\displaystyle\\sum_{k=1}^{n}\\gamma_{k+1}^{2}\\mathbb{E}\\left[\\left\\|A_{k}g_{k}\\right\\|^{2}\\right]}\\\\ &{\\displaystyle\\leq-C_{\\lambda}\\displaystyle\\sum_{k=1}^{n}\\gamma_{k+1}\\mathbb{E}\\left[\\left\\|\\nabla V\\left(\\theta_{k}\\right)\\right\\|^{2}\\right]+C_{\\lambda}\\displaystyle\\sum_{k=1}^{n}\\gamma_{k+1}r_{k+1}+L\\displaystyle\\sum_{k=1}^{n}\\gamma_{k+1}^{2}\\mathbb{E}\\left[\\left\\|A_{k}g_{k}\\right\\|^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $C_{\\lambda}=(\\delta+M^{2})^{-1/2}$ . ", "page_idx": 22}, {"type": "text", "text": "For the second term, using the inequality $x y\\le x^{2}/2+y^{2}/2$ for all $x,y$ , and the smoothness of $V$ , we get: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=1}^{n}\\mathbb{E}\\left[T_{2,k}\\right]=-\\sum_{k=1}^{n}\\mathbb{E}\\left[\\left\\langle\\nabla V\\left(\\tilde{\\theta}_{k}\\right)-\\nabla V\\left(\\theta_{k}\\right),\\gamma_{k+1}A_{k}g_{k}\\right\\rangle\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\frac{1}{2}\\displaystyle\\sum_{k=1}^{n}\\mathbb{E}\\left[\\left\\|\\nabla V\\left(\\tilde{\\theta}_{k}\\right)-\\nabla V\\left(\\theta_{k}\\right)\\right\\|^{2}\\right]+\\frac{1}{2}\\displaystyle\\sum_{k=1}^{n}\\mathbb{E}\\left[\\|\\gamma_{k+1}A_{k}g_{k}\\|^{2}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\frac{L^{2}}{2}\\displaystyle\\sum_{k=1}^{n}\\mathbb{E}\\left[\\left\\|\\bar{\\theta}_{k}-\\theta_{k}\\right\\|^{2}\\right]+\\displaystyle\\sum_{k=1}^{n}\\frac{\\gamma_{k+1}^{2}}{2}\\mathbb{E}\\left[\\left\\|A_{k}g_{k}\\right\\|^{2}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\frac{\\kappa^{2}L^{2}}{2}\\displaystyle\\sum_{k=1}^{n}\\mathbb{E}\\left[\\|\\theta_{k}-\\theta_{k-1}\\|^{2}\\right]+\\displaystyle\\sum_{k=1}^{n}\\frac{\\gamma_{k+1}^{2}}{2}\\mathbb{E}\\left[\\left\\|A_{k}g_{k}\\right\\|^{2}\\right]}\\\\ &{\\quad\\quad\\quad\\leq\\frac{\\kappa^{2}L^{2}}{2}\\displaystyle\\sum_{k=1}^{n}\\gamma_{k}^{2}\\mathbb{E}\\left[\\left\\|A_{k-1}m_{k-1}\\right\\|^{2}\\right]+\\displaystyle\\sum_{k=1}^{n}\\frac{\\gamma_{k+1}^{2}}{2}\\mathbb{E}\\left[\\left\\|A_{k}g_{k}\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For the third term, using the boundedness of the gradient of $V$ and the fact that $\\|m_{k}\\|\\leq M$ by Lemma A.3, we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{k=1}^{n}\\mathbb{E}\\left[T_{3,k}\\right]=\\kappa\\sum_{k=1}^{n}\\mathbb{E}\\left[\\left\\langle\\nabla V\\left(\\tilde{\\theta}_{k}\\right),\\left(\\gamma_{k}A_{k-1}-\\gamma_{k+1}A_{k}\\right)m_{k-1}\\right\\rangle\\right]}}\\\\ &{\\le\\kappa M^{2}\\displaystyle\\sum_{i=1}^{d}\\sum_{k=1}^{n}\\mathbb{E}\\left[\\gamma_{k}A_{k-1}[i]-\\gamma_{k+1}A_{k}[i]\\right]}\\\\ &{\\le\\kappa M^{2}\\displaystyle\\sum_{i=1}^{d}\\mathbb{E}\\left[\\gamma_{1}A_{0}[i]-\\gamma_{n+1}A_{n}[i]\\right]\\le\\kappa M^{2}d C_{\\gamma}\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where in the second inequality, we used the fact that $\\gamma_{k}$ and $A_{k}$ are decreasing since we use $\\hat{V}_{k}=\\operatorname*{max}(\\hat{V}_{k-1},V_{k})$ . For the last term, using the boundedness of the gradient of $V$ yields: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{k=1}^{n}\\mathbb{E}\\left[T_{4,k}\\right]={L\\kappa^{2}}\\displaystyle\\sum_{k=1}^{n}\\mathbb{E}\\left[\\left\\|(\\gamma_{k}A_{k-1}-\\gamma_{k+1}A_{k})\\,m_{k-1}\\right\\|^{2}\\right]}\\\\ {\\displaystyle\\leq L\\kappa^{2}M^{2}\\sum_{i=1}^{d}\\sum_{k=1}^{n}\\mathbb{E}\\left[\\left(\\gamma_{k}A_{k-1}[i]-\\gamma_{k+1}A_{k}[i]\\right)^{2}\\right]}\\\\ {\\displaystyle\\leq L\\kappa^{2}M^{2}\\sum_{i=1}^{d}\\sum_{k=1}^{n}\\mathbb{E}\\left[\\left(\\gamma_{k}A_{k-1}[i]\\right)^{2}-\\left(\\gamma_{k+1}A_{k}[i]\\right)^{2}\\right]}\\\\ {\\displaystyle\\leq L\\kappa^{2}M^{2}d C_{\\gamma}^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we used the inequality $(x-y)^{2}\\leq x^{2}-y^{2}$ when $x\\geq y$ in the second last inequality. Combining all these terms, we finally obtain: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{\\lambda}\\displaystyle\\sum_{k=1}^{n}\\gamma_{k+1}\\mathbb{E}\\left[\\left\\|\\nabla V\\left(\\theta_{k}\\right)\\right\\|^{2}\\right]}\\\\ &{\\le V^{*}+C_{\\lambda}\\displaystyle\\sum_{k=1}^{n}\\gamma_{k+1}r_{k+1}+L\\displaystyle\\sum_{k=1}^{n}\\gamma_{k+1}^{2}\\mathbb{E}\\left[\\left\\|A_{k}g_{k}\\right\\|^{2}\\right]+\\displaystyle\\sum_{k=1}^{n}\\frac{\\gamma_{k+1}^{2}}{2}\\mathbb{E}\\left[\\left\\|A_{k}g_{k}\\right\\|^{2}\\right]}\\\\ &{\\quad+\\displaystyle\\frac{\\kappa^{2}L^{2}}{2}\\displaystyle\\sum_{k=1}^{n}\\gamma_{k}^{2}\\mathbb{E}\\left[\\left\\|A_{k-1}m_{k-1}\\right\\|^{2}\\right]+\\kappa M^{2}d C_{\\gamma}+L\\kappa^{2}M^{2}d C_{\\gamma}^{2}\\:,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $V^{*}=\\mathbb{E}[V(\\theta_{0})-V(\\theta^{*})]\\geq\\mathbb{E}[V(\\theta_{0})-V(\\tilde{\\theta}_{n+1})]$ . Choosing $\\gamma_{n}=n^{-1/2}$ and using Lemma A.3 and [15, Lemma 24] yields ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{n}\\gamma_{k+1}^{2}\\mathbb{E}\\left[\\left\\Vert A_{k}m_{k}\\right\\Vert^{2}\\right]\\leq(1-\\rho_{1})\\sum_{k=1}^{n}\\gamma_{k+1}^{2}\\mathbb{E}\\left[\\left\\Vert A_{k}g_{k}\\right\\Vert^{2}\\right]\\leq(1-\\rho_{1})d C_{\\gamma}^{2}\\log\\left(1+\\frac{n M^{2}}{\\delta}\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, by dividing both sides by $C_{\\lambda}n^{-1/2}$ , we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{k=1}^{n}\\mathbb{E}\\left[\\|\\nabla V\\left(\\theta_{k}\\right)\\|^{2}\\right]=\\mathcal{O}\\left(\\frac{1}{\\sqrt{n}}+\\frac{d\\log n}{\\sqrt{n}}+\\frac{d}{\\sqrt{n}}+b_{n}\\right),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 23}, {"type": "text", "text": "Lemma A.3. Let $\\gamma_{k+1}\\leq\\gamma_{k}$ for all $k\\in\\mathbb{N}$ , and let $A_{k}$ be the adaptive matrix defined in Algorithm 1. Assume that $\\rho_{1}\\in[0,1)$ . Then, for all $k\\in\\mathbb{N}$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|m_{k}\\right\\|\\leq M\\quad\\mathrm{and}\\quad\\sum_{k=1}^{n}\\gamma_{k+1}^{2}\\mathbb{E}\\left[\\left\\|A_{k}m_{k}\\right\\|^{2}\\right]\\leq(1-\\rho_{1})\\sum_{k=1}^{n}\\gamma_{k+1}^{2}\\left\\|A_{k}g_{k}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. For the first inequality, we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\Vert m_{k}\\right\\Vert=\\left\\Vert\\left(1-\\rho_{1}\\right)\\sum_{\\ell=1}^{k}\\rho_{1}^{k-\\ell}g_{\\ell}\\right\\Vert\\leq\\left(1-\\rho_{1}\\right)\\sum_{\\ell=1}^{k}\\rho_{1}^{k-\\ell}\\left\\Vert g_{\\ell}\\right\\Vert\\leq M(1-\\rho_{1})\\sum_{\\ell\\geq0}\\rho_{1}^{\\ell}\\leq M\\;,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we used the fact that $\\begin{array}{r}{\\sum_{\\ell\\geq0}\\rho_{1}^{\\ell}=1/(1-\\rho_{1})}\\end{array}$ . For the second inequality, using the fact that $\\gamma_{k}$ and $A_{k}$ are decreasing (in the sense that all eigenvalues of $A_{k}$ are decreasing), since we use $\\hat{V}_{k}=\\operatorname*{max}(\\hat{V}_{k-1},V_{k})$ , we can write: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{k=1}^{n}\\gamma_{k+1}^{2}\\left\\|A_{k}m_{k}\\right\\|^{2}=\\displaystyle\\sum_{k=1}^{n}\\gamma_{k+1}^{2}\\left\\|A_{k}(1-\\rho_{1})\\displaystyle\\sum_{\\ell=1}^{k}\\rho_{1}^{k-\\ell}g_{\\ell}\\right\\|^{2}}&{}\\\\ {\\displaystyle\\leq(1-\\rho_{1})^{2}\\displaystyle\\sum_{k=1}^{n}\\gamma_{k+1}^{2}\\displaystyle\\sum_{\\ell=1}^{k}\\rho_{1}^{k-\\ell}\\left\\|A_{\\ell}g_{\\ell}\\right\\|^{2}}&{}\\\\ {\\displaystyle\\leq(1-\\rho_{1})^{2}\\displaystyle\\sum_{k=1}^{n}\\sum_{\\ell=1}^{k}\\rho_{1}^{k-\\ell}\\gamma_{\\ell+1}^{2}\\left\\|A_{\\ell}g_{\\ell}\\right\\|^{2}}&{}\\\\ {\\displaystyle\\leq(1-\\rho_{1})^{2}\\displaystyle\\sum_{\\ell=1}^{n}\\sum_{k=\\ell}^{n}\\rho_{1}^{k-\\ell}\\gamma_{\\ell+1}^{2}\\left\\|A_{\\ell}g_{\\ell}\\right\\|^{2},}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 23}, {"type": "text", "text": "A.7 The Impact of regularization parameter $\\delta$ in Adam ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In our case, we have a dependence on $\\delta$ in the logarithm, which is common for adaptive algorithms. The regularization parameter $\\delta$ , originally introduced to avoid the zero denominator issue when $V_{k}$ approaches 0, is often overlooked. However, it has been empirically observed that the performance of adaptive methods can be sensitive to the choice of this parameter, especially when a very small $\\delta$ is used, which has resulted in performance issues in some applications. ", "page_idx": 24}, {"type": "text", "text": "In practice, $\\delta$ is typically chosen as $10^{-8}$ . In our convergence rate analysis, even though the logarithm of $\\!\\!\\!\\!\\!\\delta^{-1}$ is small, it still impacts the convergence rate. A larger $\\delta$ will lead to a better convergence rate, while a smaller $\\delta$ will preserve stronger adaptivity. We need to find a better compromise between the convergence rate and the adaptivity to choose $\\delta$ . In [77, 67, 73], it was shown that by choosing $\\delta$ between $1\\bar{0}^{-3}$ and $10^{-1}$ , better results were obtained in some applications of deep learning. ", "page_idx": 24}, {"type": "text", "text": "Furthermore, several modified versions of Adam have been proposed, such as AMSGRAD [77] and YOGI [67] with the discussion of the regularization parameter $\\delta$ . The authors of [73] proposed a new modified version of Adam called SADAM to repres\u221aent the calibrated ADAM using the softplus function. In this algorithm, they define $\\hat{V}_{k}=$ softplus $\\left(\\sqrt{V_{k}}\\right)$ while other terms remain unchanged. Since we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\hat{V}_{k}=\\mathrm{softplus}\\left(\\sqrt{V_{k}}\\right)=\\frac{1}{b}\\log\\left(1+e^{b\\sqrt{V_{k}}}\\right)\\approx\\frac{1}{b}\\log\\left(e^{b\\sqrt{V_{k}}}\\right)=\\sqrt{V_{k}}~,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $b$ is the parameter to control for achieving a better convergence rate. In this case, we have $\\lambda_{\\operatorname*{max}}(A_{k})\\leq b/\\log2$ , which is similar to $\\delta^{-1/2}$ in Adagrad and Adam. Additionally, they demonstrate that $b\\approx50$ appears to be a good choice based on the empirical observations. ", "page_idx": 24}, {"type": "text", "text": "B IWAE / BR-IWAE ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "B.1 Importance Weighted Autoencoder (IWAE) ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we elaborate on the IWAE procedure within our framework to illustrate its convergence rate. The IWAE objective function is defined as: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k}^{\\mathrm{IWAE}}(\\theta,\\phi;x)=\\mathbb{E}_{q_{\\phi}^{\\otimes k}(\\cdot\\vert x)}\\left[\\log\\frac{1}{k}\\sum_{\\ell=1}^{k}\\frac{p_{\\theta}(x,Z^{(\\ell)})}{q_{\\phi}(Z^{(\\ell)}\\mid x)}\\right],\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $k$ corresponds to the number of samples drawn from the encoder\u2019s approximate posterior distribution. Denoting $V$ as the objective function, i.e., $V(\\theta)=\\log p_{\\theta}(x)$ , the gradient of $V$ and the estimator of the gradient of the ELBO of the IWAE objective are given by: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\nabla_{\\theta}V(\\theta)=\\nabla_{\\theta}\\log p_{\\theta}(x)=\\mathbb{E}_{p_{\\theta}(\\cdot|x)}\\left[\\nabla_{\\theta}\\log p_{\\theta}(x,z)\\right]\\,,}}\\\\ {{\\displaystyle\\widehat{\\nabla}_{\\theta}\\mathcal{L}_{k}^{\\mathrm{IWAE}}(\\theta,\\phi;x)=\\sum_{\\ell=1}^{k}\\frac{w^{(\\ell)}}{\\sum_{\\ell=1}^{k}w^{(\\ell)}}\\nabla_{\\theta}\\log p_{\\theta}(x,z^{(\\ell)})\\ ,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $w^{(\\ell)}=p_{\\theta}(x,z^{(\\ell)})/q_{\\phi}(z^{(\\ell)}|x)$ the unnormalized importance weights. Theorem B.1 provides an upper bound for the bias of this estimator. ", "page_idx": 25}, {"type": "text", "text": "Theorem B.1. Let $\\mathsf{X}\\subseteq\\mathbb{R}^{d_{x}}$ and $Z\\subseteq\\mathbb{R}^{d_{z}}$ denote the data space and the latent space, respectively. Assume that there exists $M$ such that for all $\\theta\\in\\Theta\\subset\\mathbb{R}^{d}$ , $x\\in\\mathsf{X}$ and $z\\in Z$ , $\\|\\nabla_{\\theta}\\log p_{\\theta}(x,z)\\|\\leq$ $M(x)$ . Then, there exists a constant $C>0$ such that for all $\\theta\\in\\Theta$ , $\\phi\\in\\Phi$ and $x\\in\\mathsf{X}$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{E}_{q_{\\phi}^{\\otimes k}(\\cdot|x)}\\left[\\widehat{\\nabla}_{\\theta}\\mathcal{L}_{k}^{I W A E}(\\theta,\\phi;x)-\\nabla_{\\theta}V(\\theta)\\right]\\right\\|\\leq\\frac{C}{k}\\;,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\nabla_{\\theta}V(\\theta)$ and $\\widehat{\\nabla}_{\\theta}\\mathcal{L}_{k}^{I W A E}(\\theta,\\phi;x)$ are defined in (10). ", "page_idx": 25}, {"type": "text", "text": "Proof. The proof is adapted from [1, Theorem 2.1]. By definition, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widehat{\\nabla}_{\\theta}\\mathcal{L}_{k}^{\\mathrm{IWAE}}(\\theta,\\phi;x)-\\nabla_{\\theta}V(\\theta)=\\frac{\\sum_{\\ell=1}^{k}w^{(\\ell)}\\left(\\nabla_{\\theta}\\log p_{\\theta}(x,z^{(\\ell)})-\\mathbb{E}_{p_{\\theta}(\\cdot\\vert x)}\\left[\\nabla_{\\theta}\\log p_{\\theta}(x,z)\\right]\\right)}{\\sum_{\\ell=1}^{k}w^{(\\ell)}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Writing $\\tilde{H}(x,z^{(\\ell)})=\\nabla_{\\theta}\\log p_{\\theta}(x,z^{(\\ell)})-\\mathbb{E}_{p_{\\theta}(\\cdot\\vert x)}\\left[\\nabla_{\\theta}\\log p_{\\theta}(x,z)\\right]\\!,$ , yields ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\hat{\\nabla}_{\\boldsymbol{\\theta}}\\mathcal{L}_{k}^{\\mathrm{IWAE}}(\\boldsymbol{\\theta},\\boldsymbol{\\phi};\\boldsymbol{x})-\\nabla_{\\boldsymbol{\\theta}}V(\\boldsymbol{\\theta})=\\frac{\\sum_{\\ell=1}^{k}w^{(\\ell)}\\tilde{H}(\\boldsymbol{x},\\boldsymbol{z}^{(\\ell)})}{\\sum_{\\ell=1}^{k}w^{(\\ell)}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $\\mathbb{E}_{q_{\\phi}}[w\\tilde{H}(x,z)]=0$ , we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widehat{\\nabla}_{\\theta}\\mathcal{L}_{k}^{\\mathrm{IWAE}}(\\theta,\\phi;x)-\\nabla_{\\theta}V(\\theta)=\\frac{\\frac{1}{k}\\sum_{\\ell=1}^{k}w^{(\\ell)}\\tilde{H}(x,z^{(\\ell)})-\\mathbb{E}_{q_{\\phi}}\\left[w\\tilde{H}(x,z)\\right]}{\\frac{1}{k}\\sum_{\\ell=1}^{k}w^{(\\ell)}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "As $\\begin{array}{r}{\\sum_{\\ell=1}^{k}w^{(\\ell)}\\tilde{H}(x,z^{(\\ell)})/k}\\end{array}$ is an unbiased estimator of $\\mathbb{E}_{q_{\\phi}}\\left[w\\tilde{H}(x,z)\\right]$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{q_{\\phi}}\\left[\\hat{\\nabla}_{\\theta}\\mathcal{L}_{k}^{\\mathrm{IWAE}}(\\theta,\\phi;x)-\\nabla_{\\theta}V(\\theta)\\right]}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}_{q_{\\phi}}\\left[\\left(\\frac{1}{\\displaystyle\\frac{1}{\\displaystyle\\hat{k}\\displaystyle\\sum_{\\ell=1}^{k}w^{(\\ell)}}}-\\frac{1}{\\mathbb{E}_{q_{\\phi}}\\left[w\\right]}\\right)\\left(\\frac{1}{\\displaystyle\\frac{k}{\\displaystyle\\hat{k}}\\displaystyle\\sum_{\\ell=1}^{k}w^{(\\ell)}\\tilde{H}(x,z^{(\\ell)})-\\mathbb{E}_{q_{\\phi}}\\left[w\\tilde{H}(x,z)\\right]\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "so that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{q_{\\phi}}\\left[\\hat{\\nabla}_{\\theta}\\mathcal{L}_{k}^{\\mathrm{IWAE}}(\\theta,\\phi;x)-\\nabla_{\\theta}V(\\theta)\\right]}\\\\ &{\\qquad\\quad=\\mathbb{E}_{q_{\\phi}}\\left[\\frac{\\left(\\frac{1}{k}\\sum_{\\ell=1}^{k}w^{(\\ell)}\\tilde{H}(x,z^{(\\ell)})-\\mathbb{E}_{q_{\\phi}}\\left[w\\tilde{H}(x,z)\\right]\\right)\\,\\left(\\mathbb{E}_{q_{\\phi}}\\left[w\\right]-\\frac{1}{k}\\sum_{\\ell=1}^{k}w^{(\\ell)}\\right)}{\\mathbb{E}_{q_{\\phi}}\\left[w\\right]\\frac{1}{k}\\sum_{\\ell=1}^{k}w^{(\\ell)}}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bigg\\|\\mathbb{E}_{q_{\\phi}}\\left[\\hat{\\nabla}_{\\theta}\\mathcal{L}_{k}^{\\mathrm{IWAE}}(\\theta,\\phi;x)-\\nabla_{\\theta}V(\\theta)\\right]\\bigg\\|\\leq A_{1}+A_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{1}=\\left\\lVert\\mathbb{E}_{q_{\\phi}}\\left[\\left(\\hat{\\nabla}_{\\theta}\\mathcal{L}_{k}^{\\mathrm{IWAE}}(\\theta,\\phi;x)-\\nabla_{\\theta}V(\\theta)\\right)\\mathbb{1}_{\\left\\{\\frac{2}{k}\\sum_{\\ell=1}^{k}w^{(\\ell)}>\\mathbb{E}_{q_{\\phi}}[w]\\right\\}}\\right]\\right\\rVert,}\\\\ &{A_{2}=\\left\\lVert\\mathbb{E}_{q_{\\phi}}\\left[\\left(\\hat{\\nabla}_{\\theta}\\mathcal{L}_{k}^{\\mathrm{IWAE}}(\\theta,\\phi;x)-\\nabla_{\\theta}V(\\theta)\\right)\\mathbb{1}_{\\left\\{\\frac{2}{k}\\sum_{\\ell=1}^{k}w^{(\\ell)}\\leq\\mathbb{E}_{q_{\\phi}}[w]\\right\\}}\\right]\\right\\rVert.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{1}\\leq\\left\\|\\mathbb{E}_{q_{\\phi}}\\left[\\frac{2}{\\mathbb{E}_{q_{\\phi}}\\left[w\\right]^{2}}\\left(\\frac{1}{k}\\sum_{\\ell=1}^{k}w^{(\\ell)}\\tilde{H}(x,z^{(\\ell)})-\\mathbb{E}_{q_{\\phi}}\\left[w\\tilde{H}(x,z)\\right]\\right)\\left(\\mathbb{E}_{q_{\\phi}}\\left[w\\right]-\\frac{1}{k}\\sum_{\\ell=1}^{k}w^{(\\ell)}\\right)\\right]\\right\\|}\\\\ &{\\quad\\leq\\frac{2}{\\mathbb{E}_{q_{\\phi}}\\left[w\\right]^{2}}\\mathbb{E}_{q_{\\phi}}\\left[\\left\\|\\frac{1}{k}\\sum_{\\ell=1}^{k}w^{(\\ell)}\\tilde{H}(x,z^{(\\ell)})-\\mathbb{E}_{q_{\\phi}}\\left[w\\tilde{H}(x,z)\\right]\\right\\|\\left\\|\\frac{1}{k}\\sum_{\\ell=1}^{k}w^{(\\ell)}-\\mathbb{E}_{q_{\\phi}}\\left[w\\right]\\right\\|\\right]}\\\\ &{\\quad\\leq\\frac{2}{\\mathbb{E}_{q_{\\phi}}\\left[w\\right]^{2}}\\mathbb{E}_{q_{\\phi}}\\left[\\left\\|\\frac{1}{k}\\sum_{\\ell=1}^{k}w^{(\\ell)}\\tilde{H}(x,z^{(\\ell)})-\\mathbb{E}_{q_{\\phi}}\\left[w\\tilde{H}(x,z)\\right]\\right\\|^{2}\\right]^{1/2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\mathbb{E}_{q_{\\phi}}\\left[\\left(\\frac{1}{k}\\sum_{\\ell=1}^{k}w^{(\\ell)}-\\mathbb{E}_{q_{\\phi}}\\left[w\\right]\\right)^{2}\\right]^{1/2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where we used Cauchy-Schwarz inequality in the last inequality. On the other hand, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}_{q_{\\phi}}\\left[\\left(\\frac{1}{k}\\sum_{\\ell=1}^{k}w^{(\\ell)}-\\mathbb{E}_{q_{\\phi}}\\left[w\\right]\\right)^{2}\\right]=\\mathbb{V}\\left(\\frac{1}{k}\\sum_{\\ell=1}^{k}w^{(\\ell)}\\right)\\leq\\frac{\\mathbb{E}_{q_{\\phi}}\\left[w^{2}\\right]}{k},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}_{q_{\\phi}}\\left[\\left\\|\\frac{1}{k}\\sum_{\\ell=1}^{k}w^{(\\ell)}\\tilde{H}(x,z^{(\\ell)})-\\mathbb{E}_{q_{\\phi}}\\left[w\\tilde{H}(x,z)\\right]\\right\\|^{2}\\right]}}\\\\ &{}&{=\\mathrm{Tr}\\left(\\mathbb{V}\\left(\\frac{1}{k}\\sum_{\\ell=1}^{k}w^{(\\ell)}\\tilde{H}(x,z^{(\\ell)})\\right)\\right)\\leq4d M^{2}\\frac{\\mathbb{E}_{q_{\\phi}}\\left[w^{2}\\right]}{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Finally, we deduce that ", "page_idx": 26}, {"type": "equation", "text": "$$\nA_{1}\\leq\\frac{2}{\\mathbb{E}_{q_{\\phi}}\\left[w\\right]^{2}}\\frac{1}{\\sqrt{k}}\\mathbb{E}_{q_{\\phi}}\\left[w^{2}\\right]^{1/2}\\frac{2\\sqrt{d}M}{\\sqrt{k}}\\mathbb{E}_{q_{\\phi}}\\left[w^{2}\\right]^{1/2}=\\frac{\\mathbb{E}_{q_{\\phi}}\\left[w^{2}\\right]}{\\mathbb{E}_{q_{\\phi}}\\left[w\\right]^{2}}\\frac{4\\sqrt{d}M}{k}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Using the assumption on the boundedness of $\\|\\nabla_{\\theta}\\log p_{\\theta}(x,z)\\|$ and the Markov inequality, we obtain: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{A_{2}\\le2M\\mathbb{P}\\left(2\\displaystyle\\frac{1}{k}\\sum_{\\ell=1}^{k}w^{(\\ell)}\\le\\mathbb{E}_{q_{\\phi}}\\left[w\\right]\\right)}\\\\ {\\qquad\\le2M\\mathbb{P}\\left(2\\left(\\displaystyle\\frac{1}{k}\\sum_{\\ell=1}^{k}w^{(\\ell)}-\\mathbb{E}_{q_{\\phi}}\\left[w\\right]\\right)\\le-\\mathbb{E}_{q_{\\phi}}\\left[w\\right]\\right)}\\\\ {\\qquad\\le2M\\mathbb{P}\\left(\\left|\\frac{1}{k}\\sum_{\\ell=1}^{k}w^{(\\ell)}-\\mathbb{E}_{q_{\\phi}}\\left[w\\right]\\right|\\ge\\frac{\\mathbb{E}_{q_{\\phi}}\\left[w\\right]}{2}\\right)\\le\\frac{\\mathbb{E}_{q_{\\phi}}\\left[w^{2}\\right]}{\\mathbb{E}_{q_{\\phi}}\\left[w\\right]^{2}}\\frac{8M}{k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 26}, {"type": "text", "text": "Algorithm 2 Adaptive Stochastic Approximation for IWAE ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Input: Initial point $\\theta_{0}$ , maximum number of iterations $n$ , step sizes $\\{\\gamma_{k}\\}_{k\\ge1}$ and a hyperparameter   \n$\\alpha\\geq0$ to control the bias and MSE.   \nfor $k=0$ to $n-1$ do Compute the stochastic update $\\nabla_{\\theta,\\phi}\\mathcal{L}_{k^{\\alpha}}^{\\mathrm{IWAE}}(\\theta_{k},\\phi_{k};X_{k+1})$ using $k^{\\alpha}$ samples from the variational posterior distribution and adaptive steps $A_{k}$ .   \nSet $\\overset{\\cdot}{\\theta_{k+1}}=\\theta_{k}-\\gamma_{k+1}A_{k}\\nabla_{\\theta}\\mathcal{L}_{k^{\\alpha}}^{\\mathrm{IWAE}}(\\theta_{k},\\overset{\\cdot}{\\phi_{k}};\\overset{\\cdot}{X_{k+1}})$ . Set $\\phi_{k+1}=\\phi_{k}-\\gamma_{k+1}A_{k}\\nabla_{\\phi}\\mathcal{L}_{k^{\\alpha}}^{I W A E}(\\theta_{k},\\phi_{k};X_{k+1}).$ .   \nend for   \nOutput: (\u03b8k)0\u2264k\u2264n ", "page_idx": 27}, {"type": "text", "text": "B.2 BR-IWAE ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we provide additional details on the Biased Reduced Importance Weighted Autoencoder (BR-IWAE). In IWAE, instead of estimating the gradient of the ELBO with respect to $\\theta$ via the Monte Carlo method, we estimate the gradient of the true objective function $\\mathbb{E}_{p_{\\theta}(\\cdot\\vert x)}\\left[\\bar{\\nabla}_{\\theta}\\log p_{\\theta}(x,z)\\right]$ using the BR-SNIS estimator [14]. This estimator aims to reduce the bias of self-normalized importance sampling estimators without increasing the variance. ", "page_idx": 27}, {"type": "text", "text": "Algorithm 3 BR-IWAE Gradient Estimator ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Input: Maximum number of iterations $t_{\\mathrm{max}}$ for MCMC and number of samples $k$ from the   \nvariational distribution $q_{\\phi}(\\cdot\\mid x)$ .   \nInitialization: Draw $\\tilde{z}_{0}$ from the variational distribution $q_{\\phi}(\\cdot\\mid x)$ .   \nfor $t=0$ to $t_{\\operatorname*{max}}-1$ do Draw $I_{t+1}\\in\\{1,\\ldots,k\\}$ uniformly at random and set ztIt++11 = z\u02dct. Draw zt1:+k1\\{It+1}independently from the variational distribution q\u03d5(\u00b7 | x). Compute the unnormalized importance weights: ", "page_idx": 27}, {"type": "equation", "text": "$$\nw_{t+1}^{(\\ell)}={\\frac{p_{\\theta}(x,z_{t+1}^{(\\ell)})}{q_{\\phi}(z_{t+1}^{(\\ell)}\\mid x)}}\\quad\\forall\\ell\\in\\{1,\\ldots,k\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Normalize importance weights: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\omega_{t+1}^{(\\ell)}=\\frac{w_{t+1}^{(\\ell)}}{\\sum_{\\ell=1}^{N}w_{t+1}^{(\\ell)}}\\quad\\forall\\ell\\in\\{1,\\ldots,k\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Select $\\tilde{z}_{t+1}$ from the set $z_{t+1}^{1:k}$ by choosing $z_{t+1}^{\\ell}$ with probability $\\omega_{t+1}^{(\\ell)}$ ", "page_idx": 27}, {"type": "text", "text": "Output: zt1:k 1\u2264t\u2264tmax and \u03c9t1:k 1\u2264t\u2264tmax. ", "page_idx": 27}, {"type": "text", "text": "The BR-SNIS estimator of $\\mathbb{E}_{p_{\\theta}(\\cdot\\vert x)}\\left[\\nabla_{\\theta}\\log p_{\\theta}(x,z)\\right]$ is given by: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\widehat{\\nabla}_{\\theta}\\log p_{\\theta}(x,z_{t_{0}:t_{\\mathrm{max}}}^{1:k})=\\frac{1}{t_{\\mathrm{max}}-t_{0}}\\sum_{t=t_{0}+1}^{t_{\\mathrm{max}}}\\sum_{\\ell=1}^{k}\\omega_{t}^{(\\ell)}\\nabla_{\\theta}\\log p_{\\theta}(x,z_{t}^{\\ell})~,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $t_{0}$ corresponds to a burn-in period. By [14, Theorem 4] the bias of this estimator decreases exponentially with $t_{0}$ . The BR-IWAE algorithm proceeds in two steps, which are repeated during optimization: ", "page_idx": 27}, {"type": "text", "text": "Update the parameter $\\phi$ as in the IWAE algorithm, that is, for all $n\\geq1$ : ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\phi_{n+1}=\\phi_{n}-\\gamma_{n+1}A_{n}\\nabla_{\\phi}\\mathcal{L}_{k}^{\\mathrm{IWAE}}(\\theta_{n},\\phi_{n};X_{n+1})\\;.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "\u2022 Update the parameter $\\theta$ by estimating (9) using BR-SNIS as detailed in Algorithm 3: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\phi_{n+1}=\\phi_{n}-\\gamma_{n+1}A_{n}\\widehat\\nabla_{\\theta}\\log p_{\\theta}(X_{n+1},z_{t_{0}:t_{\\mathrm{max}}}^{1:k})\\ .\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "B.3 Some Other Techniques for Reducing Bias ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In the previous section, we discussed one technique for reducing bias, BR-IWAE. Here, we provide an overview of some other bias reduction techniques within our context. First, the jackknife biascorrected estimator [62] is defined as: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}^{\\mathrm{Jackknife}}(\\theta,\\phi;x)=k\\mathcal{L}_{k}^{\\mathrm{IWAE}}(\\theta,\\phi;x)-(k-1)\\mathcal{L}_{k-1}^{\\mathrm{IWAE}}(\\theta,\\phi;x)\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which achieves a reduced bias of $\\mathcal{O}(k^{-2})$ . This can also be generalized to have a bias of order ${\\mathcal{O}}(k^{-m})$ for some $m\\geq1$ by considering: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k,m}^{\\mathrm{Jackknife}}=\\sum_{j=0}^{m}c(k,m,j)\\mathcal{L}_{k-j}^{\\mathrm{IWAE}}\\;,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the coefficients $c(k,m,j)$ are given as ", "page_idx": 28}, {"type": "equation", "text": "$$\nc(k,m,j)=(-1)^{j}\\frac{(k-j)^{m}}{(m-j)!j!}\\;.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The Delta method Variational Inference (DVI) [71] is defined by: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k}^{D V I}=\\mathbb{E}_{q_{\\phi}^{\\otimes k}(\\cdot|x)}\\left[\\log\\frac{1}{k}\\sum_{\\ell=1}^{k}w^{(\\ell)}+\\frac{\\bar{s}_{k}^{2}}{2k\\bar{w}_{k}}\\right],\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where ", "page_idx": 28}, {"type": "equation", "text": "$$\nw^{(\\ell)}=\\frac{p_{\\theta}(x,z^{(\\ell)})}{q_{\\phi}(z^{(\\ell)}\\mid x)},\\quad\\bar{w}_{k}=\\frac{1}{k}\\sum_{\\ell=1}^{k}w^{(\\ell)}\\quad\\mathrm{and}\\quad\\bar{s}_{k}^{2}=\\frac{1}{k-1}\\sum_{\\ell=1}^{k}(w^{(\\ell)}-\\bar{w}_{k})^{2}\\;.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The Monte Carlo estimator of the Delta method Variational Inference objective achieves a reduced bias of $\\mathcal{O}(k^{-2})$ . Some other techniques for reducing bias include the iterated bootstrap for bias correction, the debiasing lemma [57], and Multi-Level Monte Carlo and its variants [39]. ", "page_idx": 28}, {"type": "text", "text": "C Application of Our Theorem to Bilevel and Conditional Stochastic Optimization ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "C.1 Stochastic Bilevel Optimization ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We consider the Stochastic Bilevel Optimization problem given by: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\mathbb{R}^{d}}V(\\theta)=\\mathbb{E}_{\\xi}\\left[f(\\theta,\\phi^{*}(\\theta);\\xi)\\right]\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "subject to ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\phi^{*}(\\theta)\\in\\operatorname*{argmin}_{\\phi\\in\\mathbb{R}^{q}}\\!\\!\\!\\!\\operatorname{\\mathbb{E}}_{\\zeta}\\left[g(\\theta,\\phi;\\zeta)\\right]\\quad(\\mathrm{lower-level})\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the upper and inner level functions $f$ and $g$ are both jointly continuously differentiable and $\\xi$ and $\\zeta$ are random variables. The goal of equation (11) is to minimize the objective function $\\mathrm{v}$ with respect to $\\theta$ , where $\\phi^{*}(\\theta)$ is obtained by solving the lower-level minimization problem. This bilevel problem involves many machine learning problems with a hierarchical structure, which include hyper-parameter optimization [31], metalearning [30], policy optimization [37] and neural network architecture search [55]. The gradient of the objective function $V$ is given by: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\nabla V(\\theta)=\\nabla_{\\theta}f(\\theta,\\phi^{*}(\\theta))-\\nabla_{\\theta\\phi}g(\\theta,\\phi^{*}(\\theta))v^{*},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $v^{*}$ is the solution of the following linear system: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\nabla_{\\phi}^{2}g(\\theta,\\phi^{*}(\\theta))v=\\nabla_{\\phi}f(\\theta,\\phi^{*}(\\theta))\\;.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Instead of computing $v^{*}$ , the solution of the linear system above, [43, 17] proposes a method to estimate $v^{*}$ . This estimation introduces bias in the gradient of the objective function. ", "page_idx": 29}, {"type": "text", "text": "Consider the following assumptions. ", "page_idx": 29}, {"type": "text", "text": "H6 For all $\\theta\\in\\mathbb{R}^{d},\\,g(\\theta,\\phi)$ is strongly convex with respect to $\\phi$ with parameter $\\mu_{g}>0$ . ", "page_idx": 29}, {"type": "text", "text": "H7 (Regularity Lipschitz condition) Assume that $f,\\nabla f,\\nabla g,\\nabla^{2}g$ are respectively Lipschitz continuous with Lipschitz constants $\\ell_{f,0},\\ell_{f,1},\\ell_{g,1}$ and $\\ell_{g,2}$ . ", "page_idx": 29}, {"type": "text", "text": "Assumptions H6 and H7 are the same assumptions used in [17] to obtain the convergence results with SGD. Furthermore, these two assumptions ensure that the first- and second-order derivatives of $f$ and $g$ , as well as the solution mapping $\\phi^{\\ast}(\\theta)$ , are well-behaved. ", "page_idx": 29}, {"type": "text", "text": "Proposition C.1. ([17, Lemma 2.2]) Under Assumption $^{6}$ , we have: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\nabla V(\\theta)=\\nabla_{\\theta}f\\left(\\theta,\\phi^{*}(\\theta)\\right)-\\nabla_{\\theta\\phi}^{2}g\\left(\\theta,\\phi^{*}(\\theta)\\right)\\left[\\nabla_{\\phi}^{2}g\\left(\\theta,\\phi^{*}(\\theta)\\right)\\right]^{-1}\\nabla_{\\phi}f\\left(\\theta,\\phi^{*}(\\theta)\\right)\\;.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Due to the dependence of the minimizer of the lower-level problem $\\phi^{\\ast}(\\theta)$ , obtaining an unbiased estimate of $\\nabla\\Bar{V}(\\theta)$ is challenging. To address this, we replace $\\phi^{\\ast}(\\theta)$ in the gradient with $\\phi$ and define ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\bar{\\nabla}_{\\theta}f(\\theta,\\phi):=\\nabla_{\\theta}f(\\theta,\\phi)-\\nabla_{\\theta\\phi}^{2}g(\\theta,\\phi)\\left[\\nabla_{\\phi}^{2}g(\\theta,\\phi)\\right]^{-1}\\nabla_{\\phi}f(\\theta,\\phi)\\;.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Furthermore, by estimating $\\left[\\nabla_{\\phi}^{2}g(\\theta,\\phi)\\right]^{-1}$ , we define the stochastic update $H_{k}$ [17] as follows: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{H_{k}=\\nabla_{\\theta}f\\left(\\theta_{k},\\phi_{k+1};\\xi_{k}\\right)-\\nabla_{\\theta\\phi}^{2}g\\left(\\theta_{k},\\phi;\\zeta_{k}^{(0)}\\right)\\widehat{G}\\nabla_{\\phi}f\\left(\\theta_{k},\\phi_{k+1};\\xi_{k}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\begin{array}{r}{\\widehat{G}\\;=\\;\\frac{N}{\\ell_{g,1}}\\prod_{i=1}^{N^{\\prime}}\\left(I-\\frac{1}{\\ell_{g,1}}\\nabla_{\\phi}^{2}g\\left(\\theta_{k},\\phi_{k+1};\\zeta_{k}^{(i)}\\right)\\right)}\\end{array}$ with $N^{\\prime}$ is drawn from $\\{1,\\ldots,N\\}$ formly at random and $\\left\\{\\zeta^{(1)},\\cdot\\cdot\\cdot,\\zeta^{\\left(N^{\\prime}\\right)}\\right\\}$ are i.i.d. samples. ", "page_idx": 29}, {"type": "text", "text": "In Algorithm 4, we perform $T$ steps of SGD on the lower-level variable $\\phi_{k}$ before updating the upper-level variable $\\theta_{k}$ using adaptive methods such as Adagrad, RMSProp, or AMSGRAD. ", "page_idx": 29}, {"type": "text", "text": "Lemma C.2. ([33, Lemma 2.2]) Under Assumptions $H\\!\\delta$ and $H7,$ , for all $(\\theta,\\theta^{\\prime})\\in\\mathbb{R}^{d}\\times\\mathbb{R}^{d}$ ", "page_idx": 29}, {"type": "equation", "text": "$\\begin{array}{r}{\\|\\nabla V\\left(\\theta\\right)-\\nabla V\\left(\\theta^{\\prime}\\right)\\|\\le L_{V}\\left\\|\\theta-\\theta^{\\prime}\\right\\|,}\\end{array}$ ", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "with the constant $L_{V}$ is given by ", "page_idx": 29}, {"type": "equation", "text": "$$\nL_{V}=\\ell_{f,1}+\\frac{\\ell_{g,1}\\left(\\ell_{f,1}+L_{f}\\right)}{\\mu_{g}}+\\frac{\\ell_{f,0}}{\\mu_{g}}\\left(\\ell_{g,2}+\\frac{\\ell_{g,1}\\ell_{g,2}}{\\mu_{g}}\\right),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and $L_{f}$ is defined as $\\begin{array}{r}{L_{f}=\\ell_{f,1}+\\frac{\\ell_{g,1}\\ell_{f,1}}{\\mu_{g}}+\\frac{\\ell_{f,0}}{\\mu_{g}}\\left(\\ell_{g,2}+\\frac{\\ell_{g,1}\\ell_{g,2}}{\\mu_{g}}\\right)}\\end{array}$ ", "page_idx": 29}, {"type": "text", "text": "Algorithm 4 Stochastic Bilevel Optimization ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Input: Initial points $\\theta_{0},\\phi_{0}$ , maximum number of iterations for the upper-level $n$ and for the   \nlower-level $T$ , step sizes $\\{\\gamma_{k},\\tilde{\\gamma}_{k}\\}_{k\\ge1}$ , momentum parameters $\\rho_{1},\\rho_{2}\\in[0,1)$ and regularization   \nparameter $\\delta\\geq0$ .   \nSet $m_{0}=0,V_{0}=0$ and $\\hat{V}_{0}=0$   \nfor $k=0$ to $n-1$ do Set $\\phi_{k,0}=\\phi_{k}$ . for $t=0$ to $T-1$ do $\\phi_{k,t+1}=\\phi_{k,t}-\\tilde{\\gamma}_{k+1}\\nabla_{\\phi}g\\left(\\theta_{k},\\phi_{k,t};\\zeta_{k,t}\\right)$ end for Set $\\phi_{k+1}=\\phi_{k,T}$ . Compute the stochastic update $H_{k}$ using $\\phi_{k+1}$ . $\\begin{array}{r l}&{m_{k}\\overset{\\cdot}{=}{\\rho}_{1}m_{k-1}+(1-{\\rho}_{1}^{\\cdot})H_{k}}\\\\ &{V_{k}={\\rho}_{2}V_{k-1}+(1-{\\rho}_{2})H_{k}H_{k}^{\\top}}\\\\ &{\\hat{V}_{k}=\\operatorname*{max}\\left(\\hat{V}_{k-1},\\mathrm{Diag}(V_{k})\\right)}\\\\ &{A_{k}=\\left[\\delta I_{d}+\\hat{V}_{k}\\right]^{-1/2}}\\\\ &{{\\theta}_{k+1}={\\theta}_{k}-{\\gamma}_{k+1}A_{k}m_{k}}\\end{array}$   \nend for   \nOutput: (\u03b8k, \u03d5k)0\u2264k\u2264n ", "page_idx": 30}, {"type": "text", "text": "Lemma C.3. Under Assumptions H6 and $H7$ , the following inequalities hold: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\left\\|\\nabla V\\left(\\theta_{k}\\right)-\\mathbb{E}\\left[H_{k}\\ |\\ \\mathcal{F}_{k}\\right]\\right\\|^{2}\\leq2L_{f}^{2}\\left\\|\\phi_{k+1}-\\phi^{*}\\left(\\theta_{k}\\right)\\right\\|^{2}+2\\tilde{b}_{k}^{2}\\ ,}\\\\ {\\left\\|\\bar{\\nabla}_{\\theta}f(\\theta,\\phi)\\right\\|\\leq\\ell_{f,0}+\\displaystyle\\frac{\\ell_{g,1}\\ell_{f,0}}{\\mu_{g}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{\\nabla}^{L}f=\\ell_{f,1}+\\frac{\\ell_{g,1}\\ell_{f,1}}{\\mu_{g}}+\\frac{\\ell_{f,0}}{\\mu_{g}}\\left(\\ell_{g,2}+\\frac{\\ell_{g,1}\\ell_{g,2}}{\\mu_{g}}\\right)a n d\\,\\tilde{b}_{k}=\\ell_{g,1}\\ell_{f,1}\\frac{1}{\\mu_{g}}\\left(1-\\frac{\\mu_{g}}{\\ell_{g,1}}\\right)^{N}\\!\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. For the bias term, since $\\nabla V\\left(\\theta_{k}\\right)=\\bar{\\nabla}f\\left(\\theta_{k},\\phi^{*}\\left(\\theta_{k}\\right)\\right)$ , we have: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\nabla V\\left(\\theta_{k}\\right)-\\mathbb{E}\\left[H_{k}\\ |\\ \\mathcal{F}_{k}\\right]\\right\\|^{2}}\\\\ &{=\\left\\|\\bar{\\nabla}f\\left(\\theta_{k},\\phi^{*}\\left(\\theta_{k}\\right)\\right)-\\bar{\\nabla}f\\left(\\theta_{k},\\phi_{k+1}\\right)+\\bar{\\nabla}f\\left(\\theta_{k},\\phi_{k+1}\\right)-\\mathbb{E}\\left[H_{k}\\ |\\ \\mathcal{F}_{k}\\right]\\right\\|^{2}}\\\\ &{\\leq2\\left\\|\\bar{\\nabla}f\\left(\\theta_{k},\\phi^{*}\\left(\\theta_{k}\\right)\\right)-\\bar{\\nabla}f\\left(\\theta_{k},\\phi_{k+1}\\right)\\right\\|^{2}+2\\left\\|\\bar{\\nabla}f\\left(\\theta_{k},\\phi_{k+1}\\right)-\\mathbb{E}\\left[H_{k}\\ |\\ \\mathcal{F}_{k}\\right]\\right\\|^{2}}\\\\ &{\\leq2L_{f}^{2}\\left\\|\\phi_{k+1}-\\phi^{*}\\left(\\theta_{k}\\right)\\right\\|^{2}+2\\tilde{b}_{k}^{2}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where we used [33, Lemma 2.2] for the first term and [37, Lemma 11] for the second term. For the second inequality, we have: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\bar{\\nabla}_{\\theta}f(\\theta,\\phi)\\right\\|=\\left\\|\\nabla_{\\theta}f(\\theta,\\phi)-\\nabla_{\\theta\\phi}^{2}g(\\theta,\\phi)\\left[\\nabla_{\\phi}^{2}g(\\theta,\\phi)\\right]^{-1}\\nabla_{\\phi}f(\\theta,\\phi)\\right\\|}\\\\ &{\\qquad\\qquad\\leq\\|\\nabla_{\\theta}f(\\theta,\\phi)\\|+\\left\\|\\nabla_{\\theta\\phi}^{2}g(\\theta,\\phi)\\right\\|\\left\\|\\left[\\nabla_{\\phi}^{2}g(\\theta,\\phi)\\right]^{-1}\\right\\|\\|\\nabla_{\\phi}f(\\theta,\\phi)\\|}\\\\ &{\\qquad\\qquad\\leq\\ell_{f,0}+\\frac{\\ell_{g,1}\\ell_{f,0}}{\\mu_{g}}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Theorem C.4. Assume that H6 and $H7$ hold. Let $\\theta_{n}\\,\\in\\,\\mathbb{R}^{d}$ be the $n$ -th iterate of Algorithm 4, $\\gamma_{n}=c_{\\gamma}n^{-1/2}$ and $\\tilde{\\gamma}_{n}=c_{\\tilde{\\gamma}}n^{-1/2}/T$ . For any $n\\geq1$ , let $R\\in\\{0,\\ldots,n\\}$ be a uniformly distributed random variable. Assume the boundedness of the variance of the estimators of $\\nabla f,\\,\\nabla g,$ , and $\\nabla^{2}g$ . Then, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\lVert\\nabla V\\left(\\theta_{R}\\right)\\right\\rVert^{2}\\right]=\\mathcal{O}\\left(\\frac{\\log n}{\\sqrt{n}}+b_{n}\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. By using Lemma C.3, $V$ is smooth and Lemma C.3, the bias and the gradient of $V$ are bounded. Using our Corollary 4.5, we obtain: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\lVert\\nabla V\\left(\\theta_{R}\\right)\\right\\rVert^{2}\\right]=\\mathcal{O}\\left(\\frac{\\log n}{\\sqrt{n}}+b_{n}\\right),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where ", "page_idx": 31}, {"type": "equation", "text": "$$\nb_{n}=\\mathcal{O}\\left(\\frac{\\sum_{k=0}^{n}\\gamma_{k+1}\\tilde{b}_{k}^{2}+\\sum_{k=0}^{n}\\gamma_{k+1}\\left\\|\\phi_{k+1}-\\phi^{*}\\left(\\theta_{k}\\right)\\right\\|^{2}}{\\sqrt{n}}\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then, with [33, Lemma 2.3] and [17, Lemma 3], we derive: ", "page_idx": 31}, {"type": "equation", "text": "$$\nb_{n}=\\mathcal{O}\\left(\\frac{\\sum_{k=0}^{n}\\gamma_{k+1}\\tilde{b}_{k}^{2}}{\\sqrt{n}}+\\frac{1}{\\sqrt{n}}\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We achieve a classical convergence rate of ${\\mathcal{O}}(\\log n/{\\sqrt{n}})$ for Stochastic Bilevel Optimization problems. Two types of bias emerge in this context: firstly, the challenge of directly computing $\\phi^{\\ast}(\\theta)$ , and secondly, the necessity of estimating $[\\nabla_{\\phi}^{2}g(\\theta,\\phi)]^{\\bar{-}1}$ . ", "page_idx": 31}, {"type": "text", "text": "Our results extend those of [17] to the adaptive case, particularly Adagrad, RMSProp, and AMSGRAD. This provides convergence guarantees for the Alternating Stochastic Gradient Descent (ALSET) method. We can apply our convergence analysis to Stochastic Min-Max and Compositional Problems, as well as to the Actor-Critic method with linear value function approximation [49], which can be viewed as a special case of the Stochastic Bilevel algorithm. ", "page_idx": 31}, {"type": "text", "text": "C.2 Conditional Stochastic Optimization ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We now consider a class of Conditional Stochastic Optimization: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\mathbb{R}^{d}}V(\\theta):=\\mathbb{E}_{\\xi}\\left[f_{\\xi}\\left(\\mathbb{E}_{\\eta\\vert\\xi}\\left[g_{\\eta}(\\theta,\\xi)\\right]\\right)\\right],\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $f_{\\xi}(\\cdot):\\mathbb{R}^{q}\\rightarrow\\mathbb{R}$ depends on the random vector $\\xi$ and $g_{\\eta}(\\cdot,\\xi):\\mathbb{R}^{d}\\to\\mathbb{R}^{q}$ is a vector-valued function dependent on both random vectors $\\xi$ and $\\eta$ . The inner expectation is taken with respect to the conditional distribution of $\\eta$ given $\\xi$ . Given certain conditions on the regularity of these functions, the gradient of $V$ as defined in (14) can be expressed as: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\nabla V(\\theta)=\\mathbb{E}_{\\xi}\\left[\\left(\\mathbb{E}_{\\eta\\mid\\xi}\\left[\\nabla g_{\\eta}(\\theta,\\xi)\\right]\\right)^{\\top}\\nabla f_{\\xi}\\left(\\mathbb{E}_{\\eta\\mid\\xi}\\left[g_{\\eta}(\\theta,\\xi)\\right]\\right)\\right].\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Constructing an unbiased stochastic estimator of this gradient can be both costly and, in some cases, impractical. Instead, we opt for a biased estimator of $\\nabla V(\\theta)$ , using just one sample $\\xi$ and $m$ i.i.d. samples $\\{\\eta_{j}\\}_{j=1}^{m}$ from the conditional distribution of $\\eta$ given $\\xi$ : ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\widehat{\\nabla}V(\\theta;\\xi,\\{\\eta_{j}\\}_{j=1}^{m}):=\\left(\\frac{1}{m}\\sum_{j=1}^{m}\\nabla g_{\\eta_{j}}(\\theta,\\xi)\\right)^{\\top}\\nabla f_{\\xi}\\left(\\frac{1}{m}\\sum_{j=1}^{m}g_{\\eta_{j}}(\\theta,\\xi)\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "H8 For all $\\xi$ and $\\eta$ , assume that $f_{\\xi}(\\cdot),\\nabla f_{\\xi}(\\cdot),g_{\\eta}(\\cdot,\\xi)$ , and $\\nabla g_{\\eta}(\\cdot,\\xi)$ are respectively Lipschitz continuous with Lipschitz constants $\\ell_{f,0},\\ell_{f,1},\\ell_{g,0}$ and $\\ell_{g,1}$ . ", "page_idx": 31}, {"type": "text", "text": "H9 For all $\\theta$ and $\\xi$ , we assume that $\\mathbb{E}_{\\eta|\\xi}\\left[\\left\\|g_{\\eta}(\\theta,\\xi)-\\mathbb{E}_{\\eta|\\xi}\\left[g_{\\eta}(\\theta,\\xi)\\right]\\right\\|^{2}\\right]\\leq\\sigma_{g}^{2}.$ ", "page_idx": 31}, {"type": "text", "text": "Lemma C.5. ([40, Lemma 2.2]) Under Assumptions $H8$ and $H9$ , the following holds: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{E}\\left[\\widehat{\\nabla}V(\\theta;\\xi,\\{\\eta_{j}\\}_{j=1}^{m})\\right]-\\nabla V(\\theta)\\right\\|^{2}\\leq\\frac{\\ell_{g,0}^{2}\\ell_{f,1}^{2}\\sigma_{g}^{2}}{m}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Lemma C.6. Under Assumption $H\\!\\mathcal{B}_{:}$ , we have: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\|\\nabla V(\\theta)-\\nabla V(\\theta^{\\prime})\\|\\le\\left(\\ell_{g,1}\\ell_{f,0}+\\ell_{g,0}^{2}\\ell_{f,1}\\right)\\|\\theta-\\theta^{\\prime}\\|~,}}\\\\ {{\\|\\nabla V(\\theta)\\|\\le\\ell_{g,0}\\ell_{f,0}~.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. Denoting $G_{\\theta}=\\mathbb{E}_{\\eta|\\xi}\\left[g_{\\eta}(\\theta,\\xi)\\right]$ and $\\nabla G_{\\theta}=\\mathbb{E}_{\\eta\\vert\\xi}\\left[\\nabla g_{\\eta}(\\theta,\\xi)\\right]$ , we establish the smoothness of $V$ and Boundness of $\\nabla V$ . ", "page_idx": 32}, {"type": "text", "text": "Smoothness of $V$ : ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\lVert\\nabla V(\\theta)-\\nabla V(\\theta^{\\prime})\\right\\rVert=\\left\\lVert\\mathbb{E}_{\\xi}\\left[\\nabla G_{\\theta}^{T}\\nabla f_{\\xi}\\left(G_{\\theta}\\right)\\right]-\\mathbb{E}_{\\xi}\\left[\\nabla G_{\\theta}^{T}\\nabla f_{\\xi}\\left(G_{\\theta^{\\prime}}\\right)\\right]\\right\\rVert}&{}\\\\ &{\\le\\left\\lVert\\mathbb{E}_{\\xi}\\left[\\nabla G_{\\theta}^{T}\\nabla f_{\\xi}\\left(G_{\\theta}\\right)\\right]-\\mathbb{E}_{\\xi}\\left[\\nabla G_{\\theta}^{T}\\nabla f_{\\xi}\\left(G_{\\theta}\\right)\\right]\\right\\rVert}\\\\ &{\\quad+\\left\\lVert\\mathbb{E}_{\\xi}\\left[\\nabla G_{\\theta}^{T}\\nabla f_{\\xi}\\left(G_{\\theta}\\right)\\right]-\\mathbb{E}_{\\xi}\\left[\\nabla G_{\\theta}\\nabla f_{\\xi}\\left(G_{\\theta^{\\prime}}\\right)\\right]\\right\\rVert}\\\\ &{\\le\\left\\lVert\\mathbb{E}_{\\xi}\\left[\\left(\\nabla G_{\\theta}-\\nabla G_{\\theta^{\\prime}}\\right)^{T}\\nabla f_{\\xi}\\left(G_{\\theta}\\right)\\right]\\right\\rVert}\\\\ &{\\quad+\\left\\lVert\\mathbb{E}_{\\xi}\\left[\\nabla G_{\\theta}^{T}\\left(\\nabla f_{\\xi}\\left(G_{\\theta}\\right)-\\nabla f_{\\xi}\\left(G_{\\theta^{\\prime}}\\right)\\right)\\right]\\right\\rVert}\\\\ &{\\le\\mathbb{E}_{\\xi}\\left[\\left\\lVert\\nabla G_{\\theta}-\\nabla G_{\\theta^{\\prime}}\\right\\rVert\\left\\lVert\\nabla f_{\\xi}\\left(G_{\\theta}\\right)\\right\\rVert\\right]}\\\\ &{\\quad+\\mathbb{E}_{\\xi}\\left[\\left\\lVert\\nabla G_{\\theta^{\\prime}}\\right\\rVert\\left\\lVert\\nabla f_{\\xi}\\left(G_{\\theta}\\right)-\\nabla f_{\\xi}\\left(G_{\\theta^{\\prime}}\\right)\\right\\rVert\\right]}\\\\ &{\\quad+\\mathbb{E}_{\\xi}\\left[\\left\\lVert\\nabla\\ G_{\\theta^{\\prime}}\\right\\rVert\\left\\lVert\\nabla f_{\\xi}\\left(G_{\\theta}\\right)-\\nabla f_{\\xi}\\left(G_{\\theta^{\\prime}}\\right)\\right\\rVert\\right]}\\\\ &{\\le\\ell_{g,1}\\ell_{f,0}\\left\\lVert\\theta-\\theta^{\\prime}\\right\\rVert+\\ell_{g,0}\\ell_{f,1}\\mathbb{E}_{\\xi}\\left[\\left\\lVert G_{\\theta}-G_{\\theta^{\\prime}}\\right\\rVert\\right]}\\\\ &{\\le\\ell_{g,1}\\ell_{f,0}\\left\\lVert\\theta-\\theta^{\\prime}\\right\\rVert+\\ell\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Boundness of $\\nabla V$ : ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\nabla V(\\theta)\\right\\|=\\left\\|\\mathbb{E}_{\\xi}\\left[\\nabla G_{\\theta}^{T}\\nabla f_{\\xi}\\left(G_{\\theta}\\right)\\right]\\right\\|}\\\\ &{\\qquad\\qquad\\le\\mathbb{E}_{\\xi}\\left[\\left\\|\\nabla G_{\\theta}\\right\\|\\left\\|\\nabla f_{\\xi}\\left(G_{\\theta}\\right)\\right\\|\\right]\\le\\ell_{g,0}\\ell_{f,0}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Theorem C.7. Assume that $H8$ and $H9$ hold. Let $\\gamma_{n}=c_{\\gamma}n^{-1/2}$ , $A_{n}$ denote the adaptive matrix in AMSGRAD and $\\rho_{1},\\rho_{2}\\,\\in\\,[0,1)$ . For any $n\\geq1$ , let $R\\in\\{0,\\ldots,n\\}$ be a uniformly distributed random variable. Then, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\lVert\\nabla V\\left(\\theta_{R}\\right)\\right\\rVert^{2}\\right]=\\mathcal{O}\\left(\\frac{\\log n}{\\sqrt{n}}+b_{n}\\right),\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $b_{n}$ is defined by writing $m_{k}$ as the number of conditional samples at iteration $k$ : ", "page_idx": 32}, {"type": "equation", "text": "$$\nb_{n}=\\mathcal{O}\\left(\\frac{\\sum_{k=0}^{n}\\frac{m_{k}}{\\sqrt{k}}}{\\sqrt{n}}\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. This is an immediate implication of Theorem 4.6 using Lemmas C.5 and C.6. ", "page_idx": 32}, {"type": "text", "text": "These results can also be extended to the Federated Conditional Stochastic Optimization problem [75], which is defined by: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\mathbb{R}^{d}}V(\\theta)=\\frac{1}{L}\\sum_{\\ell=1}^{L}\\mathbb{E}_{\\xi_{\\ell}}\\left[f_{\\xi_{\\ell}}^{\\ell}\\left(\\mathbb{E}_{\\eta_{l}\\mid\\xi_{\\ell}}\\left[g_{\\eta_{\\ell}}^{\\ell}(\\theta,\\xi_{\\ell})\\right]\\right)\\right],\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\mathbb{E}_{\\xi_{\\ell}}f_{\\xi_{\\ell}}^{\\ell}(\\cdot):\\mathbb{R}^{q}\\rightarrow\\mathbb{R}$ is the outer-layer function on the $\\ell$ -th device with the randomness $\\xi_{\\ell}$ , and $\\mathbb{E}_{\\eta\\ell}|\\xi_{\\ell}g_{\\eta_{\\ell}}^{\\bar{\\ell}}(\\cdot,\\xi_{\\ell})\\,:\\,\\mathbb{R}^{d}\\,\\to\\,\\mathbb{R}^{q}$ is the inner-layer function on the $\\ell$ -th device with respect to the conditional distribution of $\\eta_{\\ell}$ given $\\xi_{\\ell}$ . If the functions $f_{\\xi_{\\ell}}^{\\ell}(\\cdot)$ and $g_{\\eta_{\\ell}}^{\\ell}(\\cdot,\\xi_{\\ell})$ for all $L$ devices verify Assumptions H8 and $^\\mathrm{H9}$ , we obtain the same convergence rate. ", "page_idx": 32}, {"type": "text", "text": "The following Table 2 provides a comprehensive summary of the key points, including the verification of our assumptions and the convergence results obtained in both Stochastic Bilevel Optimization and Conditional Stochastic Optimization. ", "page_idx": 32}, {"type": "table", "img_path": "TzxSrNJE0T/tmp/d3a7d3e9a21839b576a35c1c29d7e03cd5cb298436e0d33785e853b599fe2aa4.jpg", "table_caption": ["Table 2: Bilevel and Conditional Stochastic Optimization with our biased adaptive SA framework. "], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "D Some Other Examples of Biased Gradients with Control on Bias ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In this section, we explore examples of applications using biased gradient estimators while having control over the bias. ", "page_idx": 33}, {"type": "text", "text": "D.1 Self-Normalized Importance Sampling ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Let $\\pi$ be a probability measure on a measurable space $(\\mathsf{X},{\\mathcal{X}})$ . The objective is to estimate $\\pi(f)=$ $\\mathbb{E}_{\\pi}[f(X)]$ for a measurable function $f:\\mathcal{X}\\to\\mathbb{R}^{d}$ such that $\\pi(|f|)<\\infty$ . Assume that $\\pi(\\mathrm{d}x)\\propto$ $w(x)\\lambda({\\mathrm{d}}x)$ , where $w$ is a positive weight function and $\\lambda$ is a proposal probability distribution, and that $\\begin{array}{r}{\\lambda(w)=\\int w(x)\\lambda(\\mathrm{d}x)<\\infty}\\end{array}$ . For a function $f:\\mathcal{X}\\to\\mathbb{R}^{d}$ such that $\\pi(|f|)<\\infty$ , the identity ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\pi(f)=\\frac{\\lambda(\\omega f)}{\\lambda(\\omega)}\\;,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "leads to the Self-Normalized Importance Sampling (SNIS) estimator: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\Pi_{N}f\\left(X^{1:N}\\right)=\\sum_{i=1}^{N}\\omega_{N}^{i}f\\left(X^{i}\\right),\\quad\\omega_{N}^{i}=\\frac{w\\left(X^{i}\\right)}{\\sum_{\\ell=1}^{N}w\\left(X^{\\ell}\\right)}\\;,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $X^{1:N}=\\left(X^{1},\\cdot\\cdot\\cdot,X^{N}\\right)$ are independent draws from $\\lambda$ and the $\\omega_{N}^{i}$ are called the normalized weights. [1] shows that the bias of the SNIS estimator can be expressed as: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{E}\\left[\\Pi_{N}f\\left(X^{1:N}\\right)-\\pi(f)\\right]\\right\\|\\leq\\frac{12}{N}\\frac{\\lambda\\left(\\omega^{2}\\right)}{\\lambda(\\omega)^{2}}\\;.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "This particular type of estimator can be found in the domain of Monte Carlo methods, particularly in the context of Bayesian inference and Sequential Monte Carlo methods. ", "page_idx": 33}, {"type": "text", "text": "D.2 Sequential Monte Carlo Methods ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "We focus here in the task of estimating the parameters, denoted as $\\theta$ , in Hidden Markov Models. In this context, the hidden Markov chain is denoted by $(X_{t})_{t\\geq0}$ . The distribution of $X_{0}$ has density $\\chi$ with respect to the Lebesgue measure $\\mu$ and for all $t\\geq0$ , the conditional distribution of $X_{t+1}$ given $X_{0:t}$ has density $m_{\\theta}(X_{t},\\bar{\\cdot})$ . It is assumed that this state is partially observed through an observation process $(Y_{t})_{0\\leq t\\leq T}$ . The observations $Y_{0:t}$ are assumed to be independent conditionally on $X_{0:t}$ and, for all $0\\leq t\\leq\\bar{T}$ , the distribution of $Y_{t}$ given $X_{0:t}$ depends on $X_{t}$ only and has density $g_{\\theta}(X_{t},\\cdot)$ with respect to the Lebesgue measure. The joint distribution of hidden states and observations is given by ", "page_idx": 33}, {"type": "equation", "text": "$$\np_{\\theta}(x_{0:T},y_{0:T})=\\chi(x_{0})g_{\\theta}(x_{0},y_{0})\\prod_{t=0}^{T-1}m_{\\theta}(x_{t},x_{t+1})g_{\\theta}(x_{t+1},y_{t+1})~.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Our objective is to maximize the likelihood of the model: ", "page_idx": 33}, {"type": "equation", "text": "$$\np_{\\theta}(y_{0:T})=\\int p_{\\theta}(x_{0:T},y_{0:T})\\,\\mathrm{d}x_{0:T}\\ .\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "To use a gradient-based method for this maximization problem, we need to compute the gradient of the objective function. Under simple technical assumptions, by Fisher\u2019s identity, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\nabla_{\\theta}\\log p_{\\theta}(y_{0:T})=\\int\\nabla_{\\theta}\\log p_{\\theta}(x_{0:T},y_{0:T})p_{\\theta}(x_{0:T}|y_{0:T})\\mathrm{d}x_{0:T}}\\quad}&{}\\\\ &{=\\mathbb{E}_{x_{0:T}\\sim p_{\\theta}(.|y_{0:T})}\\left[\\nabla_{\\theta}\\log p_{\\theta}(x_{0:T},y_{0:T})\\right]}\\\\ &{=\\mathbb{E}_{x_{0:T}\\sim p_{\\theta}(.|y_{0:T})}\\left[\\displaystyle\\sum_{t=0}^{T-1}s_{t,\\theta}\\left(x_{t},x_{t+1}\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $s_{t,\\theta}\\left(x,x^{\\prime}\\right)\\,=\\,\\nabla_{\\theta}\\log\\{m_{\\theta}\\left(x,x^{\\prime}\\right)g_{\\theta}\\left(x,y_{t+1}\\right)\\}$ for $t\\,>\\,0$ and by convention $s_{0,\\theta}\\left(x,x^{\\prime}\\right)\\,=$ $\\nabla_{\\boldsymbol{\\theta}}\\log g_{\\boldsymbol{\\theta}}\\left(x,y_{0}\\right)$ . Given that the gradient of the log-likelihood represents the smoothed expectation of an additive functional, one may opt for Online Smoothing algorithms to mitigate computational costs. The estimation of the gradient $\\nabla_{\\theta}\\log p_{\\theta}\\big(y_{0:T}\\big)$ is given by: ", "page_idx": 34}, {"type": "equation", "text": "$$\nH_{\\theta}\\left(y_{0:T}\\right)=\\sum_{i=1}^{N}\\frac{\\omega_{T}^{i}}{\\Omega_{T}}\\tau_{T,\\theta}^{i}\\ ,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\{\\tau_{T,\\theta}^{i}\\}_{i=1}^{N}$ are particle approximations obtained using particles $\\{\\left(\\xi_{T}^{i},\\omega_{T}^{i}\\right)\\}_{i=1}^{N}$ targeting the filtering distribution $\\phi_{T}$ , i.e. the conditional distribution of $x_{T}$ given $y_{0:T}$ . In the Forward-only implementation of FFBSm [20], the particle approximations $\\{\\bar{\\tau_{T,\\theta}^{i}}\\}_{i=1}^{N}$ are computed using the following formula, with an initialization of $\\tau_{0}^{i}=0$ for all $i\\in[1,N]$ : ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\tau_{t+1,\\theta}^{i}=\\sum_{j=1}^{N}\\frac{\\omega_{t}^{j}m_{\\theta}(\\xi_{t}^{j},\\xi_{t+1}^{i})}{\\sum_{\\ell=1}\\omega_{t}^{\\ell}m_{\\theta}(\\xi_{t}^{\\ell},\\xi_{t+1}^{i})}\\left\\{\\tau_{t,\\theta}^{j}+s_{t,\\theta}(\\xi_{t}^{j},\\xi_{t+1}^{i})\\right\\},\\quad t\\in\\mathbb{N}\\;.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The estimator of the gradient $H_{\\theta}\\left(y_{0:T}\\right)$ computed by the Forward-only implementation of FFBSm is biased. The bias and MSE of this estimator are of order $\\mathcal{O}\\left(1/N\\right)$ [20], where $N$ corresponds to the number of particles used to estimate it. Using alternative recursion methods to compute $\\{\\tau_{T,\\theta}^{i}\\}_{i=1}^{N}$ results in different algorithms, such as the particle-based rapid incremental smoother (PARIS) [64] and its pseudo-marginal extension [34] and Parisian particle Gibbs (PPG) [13]. In such cases, one can also control the bias and MSE of the estimator. ", "page_idx": 34}, {"type": "text", "text": "D.3 Policy Gradient for Average Reward over Infinite Horizon ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Consider a finite Markov Decision Process (MDP) denoted as $(S,{\\mathcal{A}},R,P)$ , where $\\boldsymbol{S}$ represents the state space, $\\boldsymbol{\\mathcal{A}}$ denotes the action space, $R:S\\times A\\to[0,R_{\\mathrm{max}}]$ is a reward function, and $P$ is the transition model. The agent\u2019s decision-making process is characterized by a parametric family of policies $\\{\\pi_{\\theta}\\}_{\\theta\\in\\mathbb{R}^{d}}$ , employing the soft-max parameterization. The reward function is given by: ", "page_idx": 34}, {"type": "equation", "text": "$$\nV(\\theta):=\\mathbb{E}_{(S,A)\\sim v_{\\theta}}\\left[\\mathrm{R}(S,A)\\right]=\\sum_{(s,a)\\in S\\times A}v_{\\theta}(s,a)\\mathrm{R}(s,a)\\;,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $v_{\\theta}$ represents the unique stationary distribution of the state-action Markov Chain sequence $\\{(S_{t},A_{t})\\}_{t\\geq1}$ generated by the policy $\\pi_{\\theta}$ . Let $\\lambda\\in(0,1)$ be a discount factor and $T$ be sufficiently large, the estimator of the gradient of the objective function $V$ is given by: ", "page_idx": 34}, {"type": "equation", "text": "$$\nH_{\\theta}\\left(S_{1:T},A_{1:T}\\right)=\\mathrm{R}\\left(S_{T},A_{T}\\right)\\sum_{i=0}^{T-1}\\lambda^{i}\\nabla\\log\\pi_{\\theta}\\left(A_{T-i};S_{T-i}\\right)\\ ,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $(S_{1:T},A_{1:T}):=(S_{1},A_{1},\\dots,S_{T},A_{T})$ is a realization of state-action sequence generated by the policy $\\pi_{\\theta}$ . It\u2019s important to note that this gradient estimator is biased, and the bias is of order $\\mathcal{O}(1-\\lambda)$ [44]. ", "page_idx": 34}, {"type": "text", "text": "D.4 Zeroth-Order Gradient ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Consider the problem of minimizing the objective function $V$ . The zeroth-order gradient method is particularly valuable in scenarios where direct access to the gradient of the objective function is ", "page_idx": 34}, {"type": "text", "text": "challenging or computationally expensive. The zeroth-order gradient oracle obtained by Gaussian smoothing [61] is given by: ", "page_idx": 35}, {"type": "equation", "text": "$$\nH_{\\theta}\\left(X\\right)=\\frac{V(\\theta+\\tau X)-V(\\theta)}{\\tau}X~,\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $\\tau>0$ is a smoothing parameter and $X\\sim{\\mathcal{N}}(0,I_{d})$ a random Gaussian vector. [61, Lemma 3] provide the bias of this estimator: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{E}\\left[H_{\\theta}\\left(X\\right)\\right]-\\nabla V\\left(\\theta\\right)\\right\\|\\leq\\frac{\\tau}{2}L(d+3)^{3/2}\\ .\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The application of these zeroth-order gradient methods can be found in generative adversarial networks [58, 16]. ", "page_idx": 35}, {"type": "text", "text": "D.5 Compressed Stochastic Approximation: Coordinate Sampling ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "The coordinate descent method is based on the iteration: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\theta_{n+1}=\\theta_{n}-\\gamma_{n+1}H_{\\theta_{n}}\\left(X_{n+1}\\right)_{j_{n}}e_{j_{n}}\\mathrm{~,~}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $\\{e_{1},\\ldots,e_{d}\\}$ is the canonical basis of $\\mathbb{R}^{d}$ and $H_{\\theta_{n}}\\left(X_{n+1}\\right)_{j}$ is the $j$ -th coordinate of the gradient. The randomized coordinate selection rule chooses $j_{n}$ uniformly from the set $\\{1,2,\\ldots,d\\}$ . Alternatively, the Gauss-Southwell selection rule [63] uses: ", "page_idx": 35}, {"type": "equation", "text": "$$\nj_{n+1}:=\\operatorname*{argmax}_{j\\in\\{1,...,d\\}}\\left|H_{\\theta_{n}}\\left(X_{n+1}\\right)_{j}\\right|\\,.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "This corresponds to a greedy selection procedure since at each iteration we choose the coordinate with the largest directional derivative. Another approach to choosing $j_{n}$ is Coordinate Sampling [53], a variant of the stochastic gradient descent algorithm that incorporates a selection step by sampling to perform random coordinate descent. The distribution of $\\zeta_{n+1}$ , which selects the coordinate, is characterized by the probability weights vector $(w_{n}^{(1)},\\ldots,w_{n}^{(d)})$ defined as: ", "page_idx": 35}, {"type": "equation", "text": "$$\nw_{n}^{(j)}=\\mathbb{P}(\\zeta_{n+1}=j|\\mathcal{F}_{n}),\\quad j\\in\\{1,\\ldots,d\\}\\;.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "This distribution of $\\zeta_{n+1}$ is referred to as the coordinate sampling policy. The Stochastic Coordinate Gradient Descent algorithm is defined by: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\theta_{n+1}=\\theta_{n}-\\gamma_{n+1}D(\\zeta_{n+1})H_{\\theta_{n}}\\left(X_{n+1}\\right)\\;,\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $D(k)\\,=\\,e_{k}e_{k}^{\\top}\\,\\in\\,\\mathbb{R}^{d\\times d}$ has its entries equal to 0 except for the $(k,k)$ entry, which is 1. Observe that the distribution of the random matrix $D(\\zeta_{n+1})$ is fully characterized by the matrix Dn = E[D(\u03b6n+1)|Fn] = Diag(w(n1 ), . . . , w(nd )). In this context, An represents a diagonal matrix Dn where the diagonal terms characterize the probability weights for sampling each coordinate. These weights typically depend on preceding iterations and even on current gradients. In this case, we always have $\\beta_{n+1}\\leq1$ and to control the minimum eigenvalue, we only require a lower bound on the probability weights. This method can be easily extended to incorporate biased gradients and adaptive steps by introducing $\\bar{A}_{n}=D_{n}A_{n}$ , where $A_{n}$ represents the adaptive matrix as before, and $D_{n}$ is the matrix of probability weights. ", "page_idx": 35}, {"type": "text", "text": "E Experiment details and supplementary results ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "E.1 Experiment with a Synthetic Time-Dependent Bias ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "To illustrate Theorem 4.1 and the impact of bias, we consider in Figure 3 a simple least squares objective function $V(\\theta)=\\|A\\theta\\|^{2}/2$ in dimension $d=10$ . We artificially add to every gradient a zero-mean Gaussian noise with variance $\\sigma^{2}=0.01$ and a bias term $r_{n}=C_{r}n^{-r}$ at each iteration $n$ . We use Adagrad with a learning rate $\\gamma=1/2$ , $\\beta=0$ and $\\lambda=0$ . Then, the bound of Theorem 4.1 is of the form $\\mathcal{O}(n^{-1/2}+n^{-r})$ . ", "page_idx": 36}, {"type": "image", "img_path": "TzxSrNJE0T/tmp/af37ae02003290f5af99654de8925d93ea219471723c51dda6aa8ffa76c357af.jpg", "img_caption": ["Figure 3: Value of $V(\\theta_{n})-V(\\theta^{*})$ (on the left) and $\\|\\nabla V(\\theta_{n})\\|^{2}$ (on the right) with Adagrad for different values of $r_{n}=n^{-r}$ and a learning rate $\\gamma_{n}=n^{-1/2}$ . The dashed curve corresponds to the expected convergence rate $O(n^{-1/4})$ for $r=1/4$ and $O(n^{-1/2})$ for $r\\geq1/2$ . "], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "We explore different values of $r_{n}\\in\\{1,n^{-1/4},n^{-1/2},n^{-1},n^{-2},0\\}$ , where $r_{n}=1$ corresponds to constant bias, $r_{n}=0$ for an unbiased gradient, and the others exhibit decreasing bias. First, note that the impact of a constant bias term ${r}_{n}=1$ ) on the risk and the norm of gradients never vanishes. From $r_{n}=1$ to $r_{n}=n^{-1/2}$ , the effect of the bias decreases until a threshold is reached where there is no significant improvement. The convergence rate in the case $r_{n}=n^{-1/2}$ is then the same as in the case without bias, illustrating the fact that in this case the dominating term comes from the learning rate. ", "page_idx": 36}, {"type": "text", "text": "E.2 Additional Experiments of IWAE ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "In this section, we provide detailed information about the experiments on CIFAR-10. We also conduct additional experiments on the FashionMNIST dataset. For all exp\u221aeriments, we use Adagrad, RMSProp, and Adam with a learning rate decay given by $\\gamma_{n}\\,=\\,C_{\\gamma}/\\bar{\\sqrt{n}}$ , where $C_{\\gamma}\\,=\\,0.01$ for Adagrad and $C_{\\gamma}=0.001$ for RMSProp and Adam. The momentum parameters are set to $\\rho_{1}=0.9$ and $\\rho_{2}~=~0.999$ , and the regularization parameter $\\delta$ is fixed at $5\\,\\times\\,10^{-2}$ . The impact of this regularization parameter will be illustrated later. ", "page_idx": 36}, {"type": "text", "text": "Datasets. We conduct our experiments on two datasets: FashionMNIST [76] and CIFAR-10. The FashionMNIST dataset is a variant of MNIST and consists of $28\\mathrm{x}28$ pixel images of various fashion items, with 60,000 images in the training set and 10,000 images in the test set. CIFAR-10 consists of $32\\mathtt{x32}$ pixel images categorized into 10 different classes. The dataset is divided into 60,000 images in the training set and 10,000 images in the test set. ", "page_idx": 36}, {"type": "text", "text": "Models. For FashionMNIST, we use a fully connected neural network with a single hidden layer consisting of 400 hidden units and ReLU activation functions for both the encoder and the decoder. The latent space dimension is set to 20. We use 256 images per iteration (235 iterations per epoch). For CIFAR-10 and CIFAR-100, we use a Convolutional Neural Network (CNN) architecture with 3 Convolutional layers and 2 fully connected layers with ReLU activation functions. The latent space dimension is set to 100. For both datasets, we use 256 images per iteration (196 iterations per epoch). ", "page_idx": 36}, {"type": "text", "text": "We estimate the log-likelihood using the VAE, IWAE, and BR-IWAE models, all of which are trained for 100 epochs. Training is conducted using the SGD, SGD with momentum, Adagrad, RMSProp, and Adam algorithms with a decaying learning rate, as mentioned before. For SGD, we employ the clipping method to clip the gradients to prevent excessively large steps. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "For this experiment, we set $k\\,=\\,5$ samples in both IWAE and BR-IWAE, while restricting the maximum iteration of the MCMC algorithm to 5 and the burn-in period to 2 for BR-IWAE. For comparison, we estimate the Negative Log-Likelihood using these three models with SGD, SGD with momentum, Adagrad, RMSProp, and Adam, and the results are presented in Table 3. Similar to the case of CIFAR-10, we observe that IWAE outperforms VAE, while BR-IWAE outperforms IWAE by reducing bias in all cases. The adaptive methods surpass SGD, and momentum further improves their performances. Consequently, Adam excels among all algorithms due to its adaptive steps and momentum. ", "page_idx": 37}, {"type": "table", "img_path": "TzxSrNJE0T/tmp/ded62ed383259cefb641e9c780ee1e81515348b9cf2e1cb282b2be8a66ccbcaf.jpg", "table_caption": ["Table 3: Comparison of Negative Log-Likelihood on the FashionMNIST Test Set (Lower is Better). "], "table_footnote": [], "page_idx": 37}, {"type": "text", "text": "Similarly, as we did in the case of CIFAR-10, we incorporate a time-dependent bias that decreases by choosing a bias of order $\\mathcal{O}(n^{-\\alpha})$ at iteration $n$ . We vary the value of $\\alpha$ for both FashionMNIST and CIFAR-100. ", "page_idx": 37}, {"type": "image", "img_path": "TzxSrNJE0T/tmp/2df8190bea196c5a0d4cfdc900a386c746d684a96a8c6847bb5cd640e0e1cf3c.jpg", "img_caption": ["Figure 4: IWAE on the FashionMNIST Dataset with Adagrad for different values of $\\alpha$ . Bold lines represent the mean over 5 independent runs. "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "TzxSrNJE0T/tmp/bceaa21db127dc4c1579a461339dafe6293652b68556cdd3f1e91d79c45de9ea.jpg", "img_caption": ["Figure 5: IWAE on the FashionMNIST Dataset with RMSProp for different values of $\\alpha$ . Bold lines represent the mean over 5 independent runs. "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "TzxSrNJE0T/tmp/79b5a172ab205a68521e5949e7516138b8c3bbf8073a96695060d84945785c50.jpg", "img_caption": ["Figure 6: IWAE on the FashionMNIST Dataset with Adam for different values of $\\alpha$ . Bold lines represent the mean over 5 independent runs. "], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "All figures are plotted on a logarithmic scale for better visualization and with respect to the number of epochs. The\u221a dashed curve corresponds to the expected convergence rate $O(n^{-1/4})$ for $\\alpha=1/8$ , and $O(\\log n/{\\sqrt{n}})$ for $\\alpha=1/4$ , as well as for $\\alpha=1/2$ , just as in the case of CIFAR-10. We can clearly observe that for all cases, convergence is achieved when $n$ is sufficiently large. In the case of the FashionMNIST dataset, the bound seems tight, and the convergence rate of $\\bar{O(n^{-1/2})}$ does not seem to be possible to reach, in contrast to the case of CIFAR-10 where the curves corresponding to $\\alpha=1/4$ and $\\alpha=1/2$ approach the $\\mathcal{O}(n^{-1/2})$ convergence rate. For all figures, with a larger $\\alpha$ , the convergence in both the squared gradient norm and negative log-likelihood occurs more rapidly. ", "page_idx": 38}, {"type": "text", "text": "Additional Experiments on CIFAR-10 Dataset. ", "page_idx": 38}, {"type": "image", "img_path": "TzxSrNJE0T/tmp/a192faab02ca7ddecc4981f068e1c426b21034dd01f7ef1bab8e55575737b655.jpg", "img_caption": ["Figure 7: Negative Log-Likelihood on the test set on the CIFAR-10 Dataset for IWAE with Adagrad, RMSProp, and Adam. Bold lines represent the mean over 5 independent runs. "], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "The effect of $C_{\\gamma}$ . ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Figure 8 illustrates the convergence in both the squared gradient norm and the negative log-likelihood for $C_{\\gamma}\\,=\\,0.001$ and $C_{\\gamma}\\,=\\,0.01$ in Adagrad. In the case of the squared gradient norm, we have only plotted the results for $C_{\\gamma}\\,=\\,0.001$ for better visualization, and the plot for $C_{\\gamma}\\,=\\,0.01$ was already presented in Figure 2. It is clear that when $C_{\\gamma}$ is set to 0.001, the convergence of the negative log-likelihood is slower. Similarly, the convergence in the squared gradient norm for $C_{\\gamma}=0.001$ achieves convergence, but it is slower compared to the case of $C_{\\gamma}=0.01$ . ", "page_idx": 38}, {"type": "image", "img_path": "TzxSrNJE0T/tmp/d4f21cf512d0b0a253f5fc5f10c6d8855cde06490056759b6b0d7fc767c764a0.jpg", "img_caption": ["Figure 8: IWAE on the CIFAR-10 Dataset with Adagrad for different values of $\\alpha$ and $C_{\\gamma}$ . Bold lines represent the mean over 5 independent runs. "], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "The Impact of regularization parameter $\\delta$ . ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "In Section A.7, we discussed the impact of the regularization parameter $\\delta$ in Adam. It has been empirically observed that the performance of adaptive methods can be sensitive to the choice of this parameter. Here, we illustrate the impact of this regularization parameter in IWAE. To achieve this, we plot the test loss for different sets of values for $\\delta\\in\\{10^{-8},\\dot{1}0^{-5},10^{-3},10^{-2},5\\times10^{-2},10^{-1}\\}$ in Figure 9. ", "page_idx": 39}, {"type": "image", "img_path": "TzxSrNJE0T/tmp/e982cf012baf0da6efdb0fc5d0f907e3be0fba62466e6eb0bfb8e3fb39883268.jpg", "img_caption": ["Figure 9: IWAE on the CIFAR-10 Dataset with Adam for different values of $\\delta$ . Lines represent the mean over 5 independent runs. "], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "Our experimental results align with prior work [77, 67, 73], affirming the consistent impact of $\\delta$ .   \nNotably, we find that employing $\\delta=\\dot{5}\\times10^{-2}$ yields improved performance in IWAE. ", "page_idx": 39}, {"type": "text", "text": "The Impact of Bias over Time. ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Our experiments illustrate the negative log-likelihood with respect to epochs, and we observed that a higher value of $\\alpha$ leads to faster convergence. The key point to consider when tuning $\\alpha$ is that while convergence may be faster in terms of iterations, it may lead to higher computational costs. To illustrate this, we set a fixed time limit of 1000 seconds and tested different values of $\\alpha$ , plotting the test loss as a function of time in Figure 10. It is clear that with $\\alpha=1/8$ , the convergence is always slower, whereas choosing $\\alpha=1/4$ achieves faster convergence than $\\dot{\\alpha}=1/2$ . While the difference may seem small here, with more complex models, the disparity becomes significant. Therefore, it is essential to tune the value of $\\alpha$ to attain fast convergence and reduce computational time. ", "page_idx": 39}, {"type": "text", "text": "In this paper, all simulations were conducted using the Nvidia Tesla T4 GPU. The total computing hours required for the results presented in this paper are estimated to be around 100 to 200 hours of GPU usage. ", "page_idx": 39}, {"type": "image", "img_path": "TzxSrNJE0T/tmp/0e10fb03c88b79d2f59088a3911d150a44c570bff2d42424dd6eb2838a8e645f.jpg", "img_caption": ["Figure 10: Negative Log-Likelihood on the test set of the CIFAR-10 Dataset for IWAE with Adagrad (on the left) RMSProp (on the right) for Different Values of $\\alpha$ over time (in seconds). Bold lines represent the mean over 5 independent runs. "], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: The paper\u2019s contributions are mentioned in the abstract and clearly detailed in the introduction. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 40}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: In our paper, we discuss the limitations of our results. Furthermore, for each assumption, we also discuss where it can be verified and provide the limitations of these assumptions. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution ", "page_idx": 40}, {"type": "text", "text": "is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. ", "page_idx": 41}, {"type": "text", "text": "\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 41}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper provides a comprehensive set of assumptions with discussions, along with complete proofs for each theoretical result in the Appendix. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 41}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: Our work is primarily theoretical, and the experiments are conducted to illustrate our results. The paper provides detailed descriptions of the experimental setup, parameters, and methodologies, ensuring that the main results can be reproduced and verified. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case ", "page_idx": 41}, {"type": "text", "text": "of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. \u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 42}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Justification: The code supporting our experiments is available on GitHub, as referenced in the Experiments section. All data used in our study is publicly accessible (CIFAR dataset). Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 42}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Justification: The paper provides comprehensive details on the experimental setup, including hyperparameters, all optimizer algorithms, and other relevant algorithms with pseudo code, ensuring the reproducibility of the results. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 43}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: All algorithms used to illustrate the results are run 5 times, and the mean and significance of all results are plotted, ensuring an appropriate representation of statistical significance. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 43}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: The paper provides detailed information on the computer resources required for all experiments, including the type of compute workers (CPU or GPU), and approximate time of execution. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 43}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: The research conducted in the paper aligns with the NeurIPS Code of Ethics, ensuring adherence to ethical guidelines throughout the study. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 44}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: Given that the paper is primarily theoretical and the experiments are conducted solely for illustrative purposes, the work does not involve significant broader societal impacts to discuss. The focus of the paper is on theoretical contributions rather than practical applications with societal implications. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 44}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The paper is theoretical and does not involve the release of data or models that have a high risk for misuse. Therefore, no safeguards were necessary. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 44}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 45}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not use existing assets; therefore, no creators or original owners need to be credited, and no license or terms of use need to be mentioned or respected. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 45}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not introduce new assets; therefore, there is no documentation provided alongside any assets. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 45}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing experiments or research with human subjects, so there are no instructions provided to participants or details about compensation. Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 46}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: The paper does not involve research with human subjects; thus, there are no potential risks, disclosure of risks, or IRB approvals to describe. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 46}]