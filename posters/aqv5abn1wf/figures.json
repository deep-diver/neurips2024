[{"figure_path": "aQv5AbN1wF/figures/figures_1_1.jpg", "caption": "Figure 1: Under our derived scaling \u00b5P-SSM, Mamba achieves feature learning in all three SSM layers. In contrast, both Standard Parametrization (SP) and \u00b5P (heuristic) lead to instability or vanishing updates for either the latent states x(i) or the output signal y(i) or both in each SSM layer. The figures above illustrate the scalings when l = 1, but they exhibit the same trend across recurrence steps. We simultaneously scale up Nx and Nu. We run each experiment 10 times, and the shaded areas indicate the standard deviation of these runs. Both Zero-Order Hold (ZOH) and Euler discretization of By are studied and indicated in the subtitle.", "description": "This figure compares the performance of three different scaling methods (\u00b5P-SSM, Standard Parameterization, and \u00b5P (heuristic)) on a Mamba model with three SSM layers.  It shows the log2 of the L2 norm of the latent states (x) and output signals (y) for each layer, and the log2 of the L2 norm of their updates (\u0394x and \u0394y). The \u00b5P-SSM method demonstrates stable feature learning across all layers, while the other two methods experience either instability or vanishing updates, indicating failure to learn features effectively. The results are shown for both Zero-Order Hold (ZOH) and Euler discretization methods of the state transition matrix.", "section": "1 Introduction"}, {"figure_path": "aQv5AbN1wF/figures/figures_5_1.jpg", "caption": "Figure 2: Illustration of the Mamba S6 Layer. The computation is modularized into three components: selection, discretization, and per-channel linear recurrence. Mamba introduces a selection mechanism where weight matrices B1, Ci depend on the inputs u. These weight matrices are then separated into per-channel parameters, and discretized using either the ZOH or Euler methods. The discretized, per-channel weights are then applied in a linear recurrence, allowing each channel to perform computations in parallel. Trainable parameters are shown in blue.", "description": "This figure illustrates the architecture of the Mamba S6 layer, a key component of the Mamba model.  The computation is broken down into three stages: Selection, Discretization, and Per-Channel Linear Recurrence.  The selection stage dynamically generates weight matrices based on the input.  Discretization then applies either Zero-Order Hold (ZOH) or Euler methods. Finally, per-channel linear recurrences enable parallel processing for each input channel.  The diagram clearly shows the flow of information and the location of trainable parameters (shown in blue).", "section": "3.2 Forward signal propagation through a S6 Mamba layer"}, {"figure_path": "aQv5AbN1wF/figures/figures_7_1.jpg", "caption": "Figure 1: Under our derived scaling \u00b5P-SSM, Mamba achieves feature learning in all three SSM layers. In contrast, both Standard Parametrization (SP) and \u00b5P (heuristic) lead to instability or vanishing updates for either the latent states x(i) or the output signal y(i) or both in each SSM layer. The figures above illustrate the scalings when l = 1, but they exhibit the same trend across recurrence steps. We simultaneously scale up Nx and Nu. We run each experiment 10 times, and the shaded areas indicate the standard deviation of these runs. Both Zero-Order Hold (ZOH) and Euler discretization of By are studied and indicated in the subtitle.", "description": "This figure compares the performance of three different scaling methods (\u00b5P-SSM, Standard Parametrization, and \u00b5P (heuristic)) for training Structured State Space Models (SSM). The \u00b5P-SSM method, proposed by the authors, demonstrates stable feature learning across all three layers of the SSM. In contrast, the other two methods show instability or vanishing updates, indicating a failure to learn features effectively.  The figure showcases results for both Zero-Order Hold (ZOH) and Euler discretization methods for the state transition matrix.", "section": "1 Introduction"}, {"figure_path": "aQv5AbN1wF/figures/figures_8_1.jpg", "caption": "Figure 1: Under our derived scaling \u00b5P-SSM, Mamba achieves feature learning in all three SSM layers. In contrast, both Standard Parameterization (SP) and \u00b5P (heuristic) lead to instability or vanishing updates for either the latent states x(i) or the output signal y(i) or both in each SSM layer. The figures above illustrate the scalings when l = 1, but they exhibit the same trend across recurrence steps. We simultaneously scale up Nx and Nu. We run each experiment 10 times, and the shaded areas indicate the standard deviation of these runs. Both Zero-Order Hold (ZOH) and Euler discretization of By are studied and indicated in the subtitle.", "description": "This figure compares the performance of three different scaling methods (\u00b5P-SSM, Standard Parameterization, and \u00b5P Heuristic) on a structured state space model called Mamba.  The results demonstrate that \u00b5P-SSM is the only method which enables feature learning in the model across three different layers, while other methods lead to issues such as instability or vanishing updates. The figure also shows results obtained using two different numerical integration methods for the state update equation (Zero-Order Hold and Euler).", "section": "1 Introduction"}, {"figure_path": "aQv5AbN1wF/figures/figures_26_1.jpg", "caption": "Figure 1: Under our derived scaling \u00b5P-SSM, Mamba achieves feature learning in all three SSM layers. In contrast, both Standard Parameterization (SP) and \u00b5P (heuristic) lead to instability or vanishing updates for either the latent states x(i) or the output signal y(i) or both in each SSM layer. The figures above illustrate the scalings when l = 1, but they exhibit the same trend across recurrence steps. We simultaneously scale up Nx and Nu. We run each experiment 10 times, and the shaded areas indicate the standard deviation of these runs. Both Zero-Order Hold (ZOH) and Euler discretization of By are studied and indicated in the subtitle.", "description": "The figure compares the performance of three different scaling methods (Standard Parameterization, Heuristic \u00b5P, and proposed \u00b5P-SSM) for training Structured State Space Models (SSM).  It shows that only the \u00b5P-SSM scaling allows for successful feature learning in all three layers of the model, as evidenced by stable and non-vanishing updates to latent states and output signals.  The other two methods suffer from instability or vanishing updates, highlighting the importance of the proposed scaling method.", "section": "1 Introduction"}, {"figure_path": "aQv5AbN1wF/figures/figures_27_1.jpg", "caption": "Figure 3: Test loss against learning rate on Mamba with varying widths (Nu and Nx). Using \u00b5P-SSM scaling leads to substantially improved test performance compared to the SP scaling. Compared to \u00b5P (heuristic), \u00b5P-SSM scaling provides greater stability when utilizing large learning rates. Notably, we observe stable learning rate transfer from small to large model widths. Performance improves monotonically across widths in structured SSMs under \u00b5P-SSM scaling, as opposed to standard scaling where performance actually drops with scale after a certain width.", "description": "The figure displays the test loss of the Mamba model with different scaling methods (Standard Parametrization, \u00b5P (heuristic), and \u00b5P-SSM) against different learning rates and varying model widths. \u00b5P-SSM shows significantly better performance and stability, particularly at higher learning rates, compared to the other methods.  It also demonstrates stable hyperparameter transfer across model sizes, unlike the other methods.", "section": "4 \u00b5P-SSM implies stability and feature learning in Mamba"}]