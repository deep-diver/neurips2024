{"references": [{"fullname_first_author": "L. Bottou", "paper_title": "Optimization methods for large-scale machine learning", "publication_date": "2018-01-01", "reason": "This paper is foundational to the field of deep learning optimization, providing background on SGD and its variants."}, {"fullname_first_author": "M. Gurbuzbalaban", "paper_title": "The heavy-tail phenomenon in SGD", "publication_date": "2021-01-01", "reason": "This paper is directly compared to in the current work and offers a contrasting analysis of heavy-tailed behavior in SGD, highlighting the differences in approach and results."}, {"fullname_first_author": "C. Martin", "paper_title": "Traditional and heavy tailed self regularization in neural network models", "publication_date": "2019-01-01", "reason": "This paper establishes a link between heavy-tailed parameter distributions and generalization performance in neural networks, a key concept explored in the current work."}, {"fullname_first_author": "C. Paquette", "paper_title": "Homogenized Stochastic Gradient Descent", "publication_date": "2022-01-01", "reason": "This paper introduces the homogenized stochastic gradient descent (hSGD) method which is the core subject of analysis in the current paper."}, {"fullname_first_author": "U. Simsekli", "paper_title": "A tail-index analysis of stochastic gradient noise in deep neural networks", "publication_date": "2019-01-01", "reason": "This paper provides a foundational analysis of heavy tails in SGD, and its findings are validated and extended in this current work."}]}