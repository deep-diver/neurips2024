[{"figure_path": "EFrgBP9au6/tables/tables_5_1.jpg", "caption": "Table 1: Comparison of continuous-time models of SGD", "description": "This table compares three different continuous-time models of stochastic gradient descent (SGD): Gaussian Ornstein-Uhlenbeck (OU), \u03b1-stable OU, and homogenized SGD.  The comparison is based on the type of local gradient noise (Gaussian additive, non-Gaussian additive, and Gaussian additive/multiplicative, respectively), the resulting global parameter distribution (Gaussian, non-Gaussian \u03b1-stable, and non-Gaussian with a Student-t distribution as a proxy), and the tail index of the parameter distribution (+\u221e, (0,2), and (1, \u221e), respectively).  The table highlights the different characteristics of each model and their implications for understanding the heavy-tailed behavior observed in SGD.", "section": "Comparison to existing literature"}, {"figure_path": "EFrgBP9au6/tables/tables_8_1.jpg", "caption": "Table 2: Kolmogorov-Smirnov test of theoretical distributions against observed SGD iterates of the linear regression/random feature model. The null hypothesis Ho is that two distributions are identical, the alternative H\u2081 is that they are not identical.", "description": "This table presents the results of Kolmogorov-Smirnov tests comparing the fit of Student-t and \u03b1-stable distributions to empirical data from SGD iterations in a linear regression model with synthetic and real-world datasets.  The p-values indicate whether the null hypothesis (that the distributions are the same) can be rejected at a significance level of 0.05.", "section": "4 Experiments"}, {"figure_path": "EFrgBP9au6/tables/tables_20_1.jpg", "caption": "Table 3: Parameters used for Figure 1", "description": "This table presents the hyperparameters used for the experiments in Figure 1.  It shows the data used (X, Y, Z), the dimension (d), the number of iterations (K), the learning rate (\u03b3), the threshold for the learning rate (\u03b3\u0303), the regularization parameter (\u03b4), the batch size (B), the maximum eigenvalue (\u03bb\u2081), and the upper (\u03b7*) and lower (\u03b7) bounds of the asymptotic tail index. These parameters were used to generate the results shown in Figure 1, illustrating the heavy-tailed behavior of SGD in linear regression and random feature models. ", "section": "4 Experiments"}, {"figure_path": "EFrgBP9au6/tables/tables_20_2.jpg", "caption": "Table 4: Parameters used for Figure 2", "description": "This table shows the hyperparameters used in the experiments for Figure 2 of the paper.  The hyperparameters include the dimension of the data (d), the number of iterations in the SGD algorithm (K), the learning rate (\u03b3), the regularization parameter (\u03b4), and the batch size (B). Three different datasets (X, Y, Z) were used, and the table shows the parameter settings for each dataset.", "section": "4 Experiments"}]