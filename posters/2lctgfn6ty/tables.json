[{"figure_path": "2LctgfN6Ty/tables/tables_7_1.jpg", "caption": "Table 1: Merlinite-7B trained on UltraFeedback Binarized. AOT results in the best performing LLM as compared to the alternative alignment algorithms on AlpacaEval, and is competitive across the other benchmarks that are evaluated in the zero shot regime.", "description": "This table presents the results of evaluating the performance of different Large Language Model (LLM) alignment algorithms on various benchmark datasets.  The algorithms compared include Alignment via Optimal Transport (AOT), Direct Preference Optimization (DPO), Kahneman-Tversky Optimization (KTO), and Identity Policy Optimization (IPO). The base LLM used in this experiment is Merlinite-7B, and the dataset used is the UltraFeedback Binarized dataset.  The table shows the performance scores for each algorithm on several metrics, including AlpacaEval, ARC, Hellaswag, MMLU, TruthfulQA, Winogrande, and GSM8K.  The results highlight that AOT achieves state-of-the-art performance on AlpacaEval compared to other methods.", "section": "5 Experiments"}, {"figure_path": "2LctgfN6Ty/tables/tables_20_1.jpg", "caption": "Table 1: Merlinite-7B trained on UltraFeedback Binarized. AOT results in the best performing LLM as compared to the alternative alignment algorithms on AlpacaEval, and is competitive across the other benchmarks that are evaluated in the zero shot regime.", "description": "This table presents the results of evaluating the performance of different large language model (LLM) alignment algorithms.  The algorithms were evaluated on the Merlinite-7B LLM after training on the UltraFeedback Binarized dataset.  The table compares AOT (Alignment via Optimal Transport) to several other algorithms (DPO, KTO, IPO) across multiple benchmarks (AlpacaEval, ARC, Hellaswag, MMLU, TruthfulQA, Winogrande, GSM8K).  AOT shows the best performance on AlpacaEval and competitive results on other benchmarks, indicating its effectiveness in aligning LLMs.", "section": "5 Experiments"}, {"figure_path": "2LctgfN6Ty/tables/tables_20_2.jpg", "caption": "Table 3: OpenHermes-2.5-Mistral-7B trained on UltraFeedback Binarized", "description": "This table presents the performance of different LLM alignment approaches on the OpenHermes-2.5-Mistral-7B model, which was fine-tuned using the UltraFeedback Binarized dataset.  The results are benchmarked across multiple metrics including AlpacaEval, ARC, HellaSwag, MMLU, TruthfulQA, Winogrande, and GSM8K.  AOT demonstrates a significant improvement over the baseline model and other alignment techniques.", "section": "Experiments"}, {"figure_path": "2LctgfN6Ty/tables/tables_20_3.jpg", "caption": "Table 1: Merlinite-7B trained on UltraFeedback Binarized. AOT results in the best performing LLM as compared to the alternative alignment algorithms on AlpacaEval, and is competitive across the other benchmarks that are evaluated in the zero shot regime.", "description": "This table presents the performance comparison of different Large Language Model (LLM) alignment algorithms using the Merlinite-7B model trained on the UltraFeedback Binarized dataset. The algorithms compared are AOT (paired and unpaired), DPO, KTO, and IPO. The evaluation is performed across several benchmarks including AlpacaEval, ARC, Hellaswag, MMLU, TruthfulQA, Winogrande, and GSM8K.  The results demonstrate that AOT achieves the best overall performance, particularly excelling in AlpacaEval.", "section": "5 Experiments"}, {"figure_path": "2LctgfN6Ty/tables/tables_21_1.jpg", "caption": "Table 1: Merlinite-7B trained on UltraFeedback Binarized. AOT results in the best performing LLM as compared to the alternative alignment algorithms on AlpacaEval, and is competitive across the other benchmarks that are evaluated in the zero shot regime.", "description": "This table presents the results of evaluating different LLM alignment techniques on a set of benchmarks.  The techniques compared include AOT (paired and unpaired), DPO, KTO, and IPO.  The base model used is Merlinite-7B, and it's fine-tuned using the UltraFeedback Binarized dataset. The benchmarks include AlpacaEval (using GPT-4), ARC, Hellaswag, MMLU, TruthfulQA, Winogrande, and GSM8K. The results demonstrate that AOT outperforms the other alignment methods in AlpacaEval, and shows competitive performance on other benchmarks in zero-shot evaluation.", "section": "5 Experiments"}, {"figure_path": "2LctgfN6Ty/tables/tables_21_2.jpg", "caption": "Table 1: Merlinite-7B trained on UltraFeedback Binarized. AOT results in the best performing LLM as compared to the alternative alignment algorithms on AlpacaEval, and is competitive across the other benchmarks that are evaluated in the zero shot regime.", "description": "This table presents the results of comparing AOT with other state-of-the-art alignment approaches (DPO, KTO, and IPO) on the Merlinite-7B model trained on the UltraFeedback Binarized dataset.  The performance is measured across various benchmarks including AlpacaEval, ARC, HellaSwag, MMLU, TruthfulQA, Winogrande, and GSM8K.  AOT shows the best performance on AlpacaEval and competitive performance on other benchmarks.", "section": "5 Experiments"}, {"figure_path": "2LctgfN6Ty/tables/tables_21_3.jpg", "caption": "Table 1: Merlinite-7B trained on UltraFeedback Binarized. AOT results in the best performing LLM as compared to the alternative alignment algorithms on AlpacaEval, and is competitive across the other benchmarks that are evaluated in the zero shot regime.", "description": "This table presents the results of the evaluation of the proposed AOT method and other alignment methods on a diverse set of benchmarks. The results show that AOT achieves state-of-the-art performance on AlpacaEval and competitive results on other benchmarks. This table belongs to the \"Experiments\" section, demonstrating the performance of AOT compared to other alignment algorithms.", "section": "Experiments"}, {"figure_path": "2LctgfN6Ty/tables/tables_21_4.jpg", "caption": "Table 1: Merlinite-7B trained on UltraFeedback Binarized. AOT results in the best performing LLM as compared to the alternative alignment algorithms on AlpacaEval, and is competitive across the other benchmarks that are evaluated in the zero shot regime.", "description": "This table presents the results of evaluating the performance of the AOT model against several other models (DPO, KTO, IPO, and a baseline Merlinite-7B model) on various benchmarks.  The benchmarks include AlpacaEval, ARC, HellaSwag, MMLU, TruthfulQA, Winogrande, and GSM8K.  The results show that AOT outperforms the other models on AlpacaEval and is competitive on the other benchmarks.", "section": "5 Experiments"}, {"figure_path": "2LctgfN6Ty/tables/tables_21_5.jpg", "caption": "Table 3: OpenHermes-2.5-Mistral-7B trained on UltraFeedback Binarized", "description": "This table presents the performance of different LLM alignment methods (AOT paired, AOT unpaired, DPO, KTO, IPO) on the OpenHermes-2.5-Mistral-7B model, which was fine-tuned using the UltraFeedback Binarized dataset.  The results are evaluated across several benchmarks: AlpacaEval (using Llama3-70B as a judge), ARC, Hellaswag, MMLU, TruthfulQA, Winogrande, and GSM8K.  Each benchmark assesses different aspects of LLM capabilities, such as reasoning, commonsense understanding, knowledge, truthfulness, and grammaticality. The table allows for comparison of AOT's performance against other state-of-the-art alignment techniques.", "section": "5 Experiments"}, {"figure_path": "2LctgfN6Ty/tables/tables_22_1.jpg", "caption": "Table 1: Merlinite-7B trained on UltraFeedback Binarized. AOT results in the best performing LLM as compared to the alternative alignment algorithms on AlpacaEval, and is competitive across the other benchmarks that are evaluated in the zero shot regime.", "description": "This table presents the results of several Large Language Model (LLM) alignment techniques on a variety of benchmark datasets.  The models were fine-tuned using the UltraFeedback Binarized dataset.  The table compares the performance of Alignment via Optimal Transport (AOT) against other methods (DPO, KTO, IPO) across metrics like AlpacaEval, ARC, Hellaswag, MMLU, TruthfulQA, Winogrande, and GSM8K.  AOT consistently shows superior performance on AlpacaEval, demonstrating its effectiveness in aligning LLMs with human preferences.", "section": "5 Experiments"}, {"figure_path": "2LctgfN6Ty/tables/tables_23_1.jpg", "caption": "Table 2: Merlinite-7B trained on UltraFeedback Binarized. Here we present full version of the results, including AlpacaEval using Llama3-70B-instruct as a judge and GPT4 as a judge. The comparison reveals that although Llama3 inflates the scores, the relative order between the two judges remains the same, suggesting the use of a cheaper AlpacaEval alternative for local development.", "description": "This table presents the performance of different LLM alignment algorithms on various benchmarks.  It includes the results obtained using both Llama3-70B and GPT4 for AlpacaEval, highlighting the consistency of relative performance despite Llama3's tendency to inflate scores.  The table focuses on the Merlinite-7B model trained on the UltraFeedback Binarized dataset.", "section": "5 Experiments"}]