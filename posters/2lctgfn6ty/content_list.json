[{"type": "text", "text": "Distributional Preference Alignment of LLMs via Optimal Transport ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Igor Melnyk\u22c6,\u2217 Youssef Mroueh\u22c6, Brian Belgodere\u22c6, Mattia Rigotti, Apoorva Nitsure, ", "page_idx": 0}, {"type": "text", "text": "Mikhail Yurochkin, Kristjan Greenewald, Jiri Navratil, and Jarret Ross ", "page_idx": 0}, {"type": "text", "text": "IBM Research MIT-IBM Watson AI Lab ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Current LLM alignment techniques use pairwise human preferences at a sample level, and as such, they do not imply an alignment on the distributional level. We propose in this paper Alignment via Optimal Transport (AOT), a novel method for distributional preference alignment of LLMs. AOT aligns LLMs on unpaired preference data by making the reward distribution of the positive samples stochastically dominant in the first order on the distribution of negative samples. We introduce a convex relaxation of this first-order stochastic dominance and cast it as an optimal transport problem with a smooth and convex cost. Thanks to the one-dimensional nature of the resulting optimal transport problem and the convexity of the cost, it has a closed-form solution via sorting on empirical measures. We fine-tune LLMs with this AOT objective, which enables alignment by penalizing the violation of the stochastic dominance of the reward distribution of the positive samples on the reward distribution of the negative samples. We analyze the sample complexity of AOT by considering the dual of the OT problem and show that it converges at the parametric rate. Empirically, we show on a diverse set of alignment datasets and LLMs that AOT leads to state-of-the-art models in the 7B family of models when evaluated with Open LLM Benchmarks and AlpacaEval. Code for AOT is available in the Hugging Face TRL library https://ibm.biz/AOT_TRL. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Aligning Large Language Models (LLMs) with human preferences is a crucial step in making these models safe and having them follow instructions faithfully. By ensuring that LLMs adhere to human preferences, values, ethics, and desired behaviors we can reduce the risk of generating harmful, biased, or inappropriate content. ", "page_idx": 0}, {"type": "text", "text": "Reinforcement Learning from Human Feedback, RLHF [Christiano et al., 2017, Stiennon et al., 2020, Ouyang et al., 2022, Bai et al., 2022], achieves this by learning a reward model on human preference data, followed by fine-tuning the LLM to maximize the reward score while staying close to the initial reference policy to retain utility from the pre-trained model. Recently, new paradigms departed from RLHF towards direct preference optimization methods such as DPO [Rafailov et al., 2024], SLIC [Zhao et al., 2023], and Identity Policy optimization [Azar et al., 2024]. In these approaches, the reward is expressed in terms of the log-likelihood ratio between the LLM policy and the reference model. The training is done on paired preference data, i.e. as triplets of prompts, chosen and rejected sentences, where for each prompt a chosen and a rejected sample are available. The training objective is to maximize the margin between the log-likelihood ratio evaluated on the chosen sentence versus the log-likelihood ratio on rejected sentences. When paired preference data is not available, and the preference data instead takes the form of distinct marginals of chosen prompt/response pairs and rejected prompt/response pairs, we refer to this setup as the unpaired data setting. Ethayarajh et al. [2024] used Kahneman & Tversky\u2019s prospect theory in the unpaired setting and proposed the KTO method that maximizes the margin between the chosen reward and the average reward of rejected sentences and pushes the reward of a rejected sentence below the average reward of chosen sentences. ", "page_idx": 0}, {"type": "image", "img_path": "2LctgfN6Ty/tmp/beabb838b1acebb4a1d5409131972c8e9f384b09f1c027579c00e0c007ab4919.jpg", "img_caption": ["(a) Stochastic Dominance of Reward of Chosen on Rejected: AOT achieves a larger margin between the quantile plots of chosen and rejected rewards. "], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "2LctgfN6Ty/tmp/0ca70c4967f63c5d42edad23f9223e47359705fc6ce6b264fede70787c32d0f3.jpg", "img_caption": ["Figure 1: AOT in the paired & unpaired settings enables first-order stochastic dominance of the chosen reward distribution on the rejected distribution (a). The margin between the quantiles of chosen and rejected rewards is larger than alternative strategies. In (b), we see that AOT\u2019s policy chosen to rejected log-likelihood ratio dominates that ratio for the base model and alternative strategies. ", "(b) Stochastic Dominance of AOT\u2019s optimized policy margin (between Chosen on Rejected) on the margin of the reference policy. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we introduce a new distributional optimization method for fine-tuning LLMs from human preference data. Previous work in the paired setting focused on improving the reward of chosen sentences over rejected sentences on a per-sample basis. This procedure does not lead to a preference on a distributional level of the chosen marginal on the rejected marginal. In probabilistic terms, we would like to induce stochastic dominance of the reward of chosen sentences on the reward of rejected ones. First order Stochastic Dominance (FSD, see e.g. Ogryczak and Ruszczynski, 2002) of a random variable $X$ on a random variable $Y$ , means that all quantiles values of $X$ are larger than those of $Y$ . Our main contribution is introducing AOT, Alignment via Optimal Transport, a new method that enables distributional alignment. We do so by devising a new AOT objective function that induces in the unpaired setting FSD dominance of chosen reward\u2019s distribution over rejected reward\u2019s distribution. We call this unpaired variant uAOT. In the paired setting, we introduce pAOT that encourages a dominance of chosen to rejected log likelihood ratio of the optimized policy on that ratio for the reference base policy. We show that the AOT cost can be cast as a one-dimensional optimal transport problem that can be solved via sorting and efficiently optimized for the LLM. AOT enjoys also nice statistical proprieties and achieves the parametric rate since its objective can be seen as a smooth one-dimensional optimal transport problem. AOT achieves state-of-the-art results on the Alpaca leaderboard [Dubois et al., 2024] using the Merlinite 7B model [Sudalairaj et al., 2024] as a base and scores as the highest 7B model at the time of writing this paper. ", "page_idx": 1}, {"type": "text", "text": "To introduce the important concepts of our work pictorially, we show in Fig. 1a the quantile plots of the rewards of AOT and alternative alignment strategies (DPO, KTO) for chosen responses (in green) and rejected responses in (red). The quantile plots are estimated on a paired test set. We see that AOT leads to chosen rewards that have larger margins than those of rejected rewards across all percentiles. More importantly, this margin is larger in AOT models than in policies coming from alternative alignment strategies. We then show in Fig. 1b how the AOT aligned policy\u2019s chosen-torejected log-likelihood ratio dominates that same ratio evaluated on the base model\u2019s ratio across all percentiles. The distributional alignment induced by AOT ensures a large margin between all quantiles so that the preference is reflected not only on average but distributionally. We formalize distributional preference in the next section. ", "page_idx": 1}, {"type": "text", "text": "2 Distributional Preference via First Order Stochastic Dominance ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "First Order Stochastic Dominance For a real random variable $Z$ we denote $F_{Z}^{(-1)}:[0,1]\\to\\overline{{\\mathbb{R}}}$ the left-continuous inverse of the Cumulative Distribution Function (CDF) $F_{Z}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nQ_{Z}(p)=F_{Z}^{(-1)}(p)=\\operatorname*{inf}\\{\\eta:F_{Z}(\\eta)\\geq p\\}{\\mathrm{~for~}}p\\in[0,1].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Given two random variables $Z_{1}$ and $Z_{2}$ , we say that $Z_{1}$ dominates $Z_{2}$ in the first order if $Z_{1}$ has larger quantiles than $Z_{2}$ for all percentiles $p$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nZ_{1}\\underset{\\mathsf{F S D}}{\\succcurlyeq}Z_{2}\\iff Q_{Z_{1}}(p)\\geq Q_{Z_{2}}(p),\\quad\\forall p\\in[0,1].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Let $\\mathcal{X}$ be the space of prompts $X$ and $\\boldsymbol{\\wp}$ be the space of responses $Y$ from an LLM conditioned on a prompt $X\\in\\mathcal{X}$ . The reference LLM is represented as policy $\\pi_{\\mathrm{ref}}(Y|X)$ , i.e., as a conditional probability on $\\boldsymbol{\\wp}$ given a prompt $X\\in\\mathcal{X}$ . We note the LLM policy we are optimizing by $\\pi_{\\theta}$ where $\\theta$ is a parameter belonging to a bounded parameter space $\\Theta\\subset\\mathbf{\\dot{\\mathbb{R}}}^{d_{\\theta}}$ . For a measure $\\mu\\in\\mathcal P(\\mathcal X\\times\\mathcal P)$ and a mapping $r:\\mathcal{X}\\times\\mathcal{Y}\\to\\mathbb{R}$ , we note as $r_{\\sharp}\\mu$ the pushforward map of $\\mu$ through $r$ . In particular, for empirical measures $\\begin{array}{r}{\\mu={\\frac{1}{n}}\\sum_{i=1}^{n}\\delta_{(x_{i},y_{i})}}\\end{array}$ , we have that $\\begin{array}{r}{r_{\\sharp}\\mu=\\frac{1}{n}\\sum_{i=1}^{n}\\delta_{r(x_{i},y_{i})}}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "DPO as a Pointwise Preference Approach In Direct Preference Optimization (DPO, Rafailov et al., 2024), the reward being optimized by the LLM has the following form : ", "page_idx": 2}, {"type": "equation", "text": "$$\nr_{\\theta}(x,y)=\\beta\\log\\frac{\\pi_{\\theta}(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}+\\beta\\log(Z(x)),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $Z(x)$ is a normalization constant. DPO assumes access to a paired preference dataset $(X,Y_{+},Y_{-})\\,\\sim\\,\\mu$ where $Y_{+}$ denotes a positive (chosen) response to which we would like to assign a high reward, and $Y_{-}$ a negative (rejected) response to which we would like to assign a low reward. This can be formalized as minimizing the logarithmic sigmoid loss : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\Theta}-\\mathbb{E}_{(x,y_{+},y_{-})\\sim\\mu}\\log(\\sigma(\\beta(r_{\\theta}(x,y_{+})-r_{\\theta}(x,y_{-})))),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and since the difference is taken for the same $x$ , the normalization $Z(x)$ disappears resulting in: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}-\\mathbb{E}_{(x,y_{+},y_{-})\\sim\\mu}\\log\\left(\\sigma\\left(\\beta\\log\\left(\\frac{\\pi_{\\theta}(y_{+}|x)}{\\pi_{\\mathrm{ref}}(y_{+}|x)}\\right)-\\beta\\log\\left(\\frac{\\pi_{\\theta}(y_{-}|x)}{\\pi_{\\mathrm{ref}}(y_{-}|x)}\\right)\\right)\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We can interpret this as a pointwise constraint inducing preference for positive over negative reward outcomes as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\log\\left(\\frac{\\pi_{\\theta}(y_{+}|x)}{\\pi_{\\mathrm{ref}}(y_{+}|x)}\\right)\\geq\\log\\left(\\frac{\\pi_{\\theta}(y_{-}|x)}{\\pi_{\\mathrm{ref}}(y_{-}|x)}\\right),\\quad\\forall(x,y_{+},y_{-})\\sim\\mu.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "DPO can then be interpreted as a relaxation of this constraint through the logistic loss, which also suggests other preference optimization algorithms through relaxations using, for example, the hinge loss as proposed in SLIC [Zhao et al., 2023]. ", "page_idx": 2}, {"type": "text", "text": "2.1 Distributional Preference via Stochastic Dominance ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "bOuutri omnaail nc ionnssitgrhati nfrt oinm  tleoromksi nogf  astt othceh apsotiinc tdwoisme icnoannscter aoifn tt ihne  rEaqn. d(2o)m i sv tahraita bwlee $\\begin{array}{r}{Z_{\\theta}^{+}=\\log(\\frac{\\pi_{\\theta}(Y_{+}|X)}{\\pi_{\\mathrm{ref}}(Y_{+}|X)})}\\end{array}$ eosf ppeocisiatlilvy ev oaluutacbolme iens  tohne  tuhnep raairneddo sme ttvianrgi awbilteh $\\begin{array}{r}{Z_{\\theta}^{-}=\\log(\\frac{\\pi_{\\theta}(Y_{-}|X)}{\\pi_{\\mathrm{ref}}(Y_{-}|X)})}\\end{array}$ porf onmepgtast iavned  opuotsciotimvee sa.n dT hniesg iastive responses as required by DPO. This is indeed the same setting considered by KTO [Ethayarajh et al., 2024]. The following paragraph formalizes this unpaired distributional preference. ", "page_idx": 2}, {"type": "text", "text": "Distributional Unpaired Preference We assume here that we don\u2019t have access to triplets of prompts and positive/negative responses $(x,y_{+},y_{-})$ . Instead, we assume separate access to $\\mu_{+}\\in$ $\\mathcal{P}(\\mathcal{X}\\times\\mathcal{Y})$ , a distribution of positive prompt/response pairs $(X_{+},Y_{+})$ we would like to be highly rewarded and reinforce in the policy, and $\\mu_{-}\\in\\mathcal P(\\mathcal X\\times\\mathcal P)$ the distribution of the negative samples $(X_{-},Y_{-})$ to be associated with low reward. We define the distributional preference as follows: ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (Distributional Preference in the Unpaired Setting). A policy $\\pi$ prefers distributionally $\\mu_{+}\\ o n\\ \\mu_{-}$ with respect to a reference policy $\\pi_{\\mathrm{ref}}\\ i f$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\log\\frac{\\pi_{\\theta}(Y_{+}\\vert X_{+})}{\\pi_{\\mathrm{ref}}(Y_{+}\\vert X_{+})}\\underset{F S D}{\\succcurlyeq}\\,\\log\\frac{\\pi_{\\theta}(Y_{-}\\vert X_{-})}{\\pi_{\\mathrm{ref}}(Y_{-}\\vert X_{-})}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In other words, noting ru \u25e6\u03c0\u03b8(x, y) = log \u03c0\u03c0r\u03b8ef((yy||xx)), the distributional preference in the unpaired setting means that we have the following constraint: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left(r_{u}\\circ\\pi_{\\theta}\\right)_{\\sharp}\\mu_{+}\\underset{F S D}{\\succ}\\;\\left(r_{u}\\circ\\pi_{\\theta}\\right)_{\\sharp}\\mu_{-}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Distributional Paired Preference Note that we can rewrite Eq. (2) in the equivalent form: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log\\frac{\\pi_{\\theta}(y_{+}|x)}{\\pi_{\\theta}(y_{-}|x)}\\geq\\log\\frac{\\pi_{\\mathrm{ref}}(y_{+}|x)}{\\pi_{\\mathrm{ref}}(y_{-}|x)},\\quad\\forall(x,y_{+},y_{-})\\sim\\mu.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In order to turn this into a distributional constraint we need access to a paired preference dataset as in DPO (X, Y+, Y\u2212) \u223c\u00b5, and impose stochastic dominance of the random variable Z\u03b8 = log \u03c0\u03c0\u03b8\u03b8((YY\u2212+||XX)) indexed by the policy we are optimizing on the random variable $\\begin{array}{r}{Z_{\\mathrm{ref}}=\\log\\frac{\\pi_{\\mathrm{ref}}(Y_{+}|X)}{\\pi_{\\mathrm{ref}}(Y_{-}|X)}}\\end{array}$ indexed by the reference policy. $Z_{\\theta}$ and $Z_{\\mathrm{ref}}$ represent here the log likelihood ratio of positive to negative outcome under the policies $\\pi_{\\theta}$ and $\\pi_{\\mathrm{ref}}$ , respectively. Hence, it is desirable to constrain the policy $\\pi_{\\theta}$ to have a larger excess log probability between positive and negative outcomes than that resulting from the reference policy $\\pi_{\\mathrm{ref}}$ . ", "page_idx": 3}, {"type": "text", "text": "We define below more formally the paired distributional preference via stochastic dominance: ", "page_idx": 3}, {"type": "text", "text": "Definition 2 (Distributional Preference in the Paired Setting). We say that the policy $\\pi_{\\theta}$ distributionally dominates $\\pi_{\\mathrm{ref}}$ in terms of log probability ratio of positive and negative responses $i f$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\log\\frac{\\pi_{\\theta}(Y_{+}\\vert X)}{\\pi_{\\theta}(Y_{-}\\vert X)}\\underset{F S D}{\\approx}\\,\\log\\frac{\\pi_{\\mathrm{ref}}(Y_{+}\\vert X)}{\\pi_{\\mathrm{ref}}(Y_{-}\\vert X)}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Noting $\\begin{array}{r}{r_{p}\\circ\\pi_{\\theta}(x,y_{+},y_{-})=\\log\\frac{\\pi_{\\theta}(y_{+}|x)}{\\pi_{\\theta}(y_{-}|x)}}\\end{array}$ this can be written as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left(r_{p}\\circ\\pi_{\\theta}\\right)_{\\sharp}\\mu\\underset{F S D}{\\succ}\\;\\left(r_{p}\\circ\\pi_{\\mathrm{ref}}\\right)_{\\sharp}\\mu.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3 AOT: Alignment via Optimal Transport a Convex Relaxation Approach ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Note that the paired and unpaired distributional preference constraints in Definitions 1 and 2 can be used in LLM alignment as follows: ", "page_idx": 3}, {"type": "text", "text": "and ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{Find~}\\,\\pi_{\\theta}\\in\\mathcal{H}\\mathrm{~such~that~}(r_{u}\\circ\\pi_{\\theta})_{\\sharp}\\mu_{+}\\underset{\\mathrm{FSD}}{\\succ}\\,\\left(r_{u}\\circ\\pi_{\\theta}\\right)_{\\sharp}\\mu_{-}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname{Find}\\pi_{\\theta}\\in\\mathcal{H}\\mathrm{~such~that~}(r_{p}\\circ\\pi_{\\theta})_{\\sharp}\\mu\\underbrace{\\succ}_{\\mathrm{FSD}}\\ (r_{p}\\circ\\pi_{\\mathrm{ref}})_{\\sharp}\\mu\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $r_{u}$ are $r_{p}$ are given in Definitions 1 and 2 respectively, and $\\mathcal{H}$ is a hypothesis class. Those two problems are instances of learning with stochastic orders introduced in [Domingo-Enrich et al., 2022], but in a simpler setting since the constraints are on one-dimensional distributions and the order considered is the first order rather than the convex order as considered in [Domingo-Enrich et al., 2022]. Note that both problems are special cases of the following generic optimization problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\mathrm{Find~}}\\theta\\in\\Theta{\\mathrm{~such~that~:~}}U_{\\theta}\\underset{\\mathrm{FSD}}{\\succ}V_{\\theta}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $U_{\\theta}$ and ${\\mathit{V}}_{\\theta}$ are real-valued random variables whose distributions depend on a parameter vector $\\theta\\in\\Theta$ . Note that for our FSD paired setting, $V_{\\theta}=V$ (independent of $\\theta$ ). Let $\\mu_{U_{\\theta}}$ and $\\mu_{V_{\\theta}}$ be the probability measures of $U_{\\theta}$ and ${\\mathit{V}}_{\\theta}$ resp. ", "page_idx": 3}, {"type": "text", "text": "By the definition of FSD in Equation (1) we have: ", "page_idx": 3}, {"type": "equation", "text": "$$\nU_{\\theta}\\underset{\\mathrm{FSD}}{\\succ}V_{\\theta}\\iff Q_{U_{\\theta}}(t)\\geq Q_{V_{\\theta}}(t),\\forall t\\in[0,1].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We can relax this problem to the following minimization problem: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\Theta}\\varepsilon(\\theta):=\\int_{0}^{1}h(Q_{U_{\\theta}}(t)-Q_{V_{\\theta}}(t))d t,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $h$ is a function penalizing each quantile\u2019s violation of FSD. The objective function (7) seeks to measure the violation of FSD, so that it can be minimized or eliminated. For instance, with $h$ the $0/1$ loss (here $\\mathbb{1}$ is the indicator function): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\Theta}\\int_{0}^{1}\\mathbb{1}_{Q_{U_{\\theta}}(t)<Q_{V_{\\theta}}(t)}d t,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This loss reminds us the misclassification $0/1$ loss. Following classical convex relaxation of $0/1$ losses in binary classification [Bartlett et al., 2006], we consider surrogates $h$ of the indicator function. Our choices for $h$ are motivated by the \u201calmost-FSD\u201d notions in the literature (See Appendix F for a discussion). In practice, we use smooth convex approximations of the 0/1 loss $(\\mathbb{1}_{x<0})$ [Bartlett et al., 2006], for example for a margin $\\beta\\,>\\,0\\,\\,h(x\\bar{)}\\,=\\,(\\beta\\,-\\,x)_{+}^{2}$ the $\\beta-$ squared hinge loss or $h(x)=\\log(1+\\exp(-\\stackrel{\\cdot}{\\beta}x))$ the $\\beta$ -logistic loss. Although not a convex relaxation of the 0/1 loss, the least squares loss has been used in classification [Rosasco et al., 2004], and in the context of alignment, it was used in IPO [Azar et al., 2024] hence we use also $h(x)=(\\beta-x)^{2}$ , and refer to it as $\\beta$ -Least Squares. Further discussion of tradeoffs and beneftis of different losses is in Appendix F, and formal assumptions on $h$ needed for the statistical theory are given in Assumption 1. ", "page_idx": 4}, {"type": "text", "text": "The cost function in (7) is still computationally challenging, if we were to solve the problem via gradient descent on $\\theta$ this would require us to differentiate through the quantile operation. The following theorem from Santambrogio [2015] will be instrumental for us to cast the loss in (7) as an optimal transport problem with a convex cost $h$ : ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 (Theorem 2.9 and Proposition 2.17 in Santambrogio [2015]). Let $h:\\mathbb{R}\\to\\mathbb{R}^{+}$ be $a$ convex function we have for two real random variables $U,V$ , with measures $\\mu_{U},\\mu_{V}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\int_{0}^{1}h(Q_{U}(t)-Q_{V}(t))d t=\\operatorname*{min}_{\\gamma\\in\\Pi(\\mu_{U},\\mu_{V})}\\int h(u-v)d\\gamma(u,v)=0\\mathsf{T}_{h}(\\mu_{U},\\mu_{V})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and $\\gamma^{*}=(Q_{U},Q_{V})_{\\sharp}\\mathcal{L}_{1}([0,1])$ is a minimizer (where ${\\mathcal{L}}_{1}$ is the Lebesgue measure on $[0,1]$ ). If furthermore $h$ is strictly convex $\\gamma^{*}$ is the unique minimizer. ", "page_idx": 4}, {"type": "text", "text": "Thanks to Theorem 1 we can write the problem (7), in the following equivalent form that we call Alignment via Optimal Transport (AOT) : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\Theta}\\int_{0}^{1}h(Q_{U_{\\theta}}(t)-Q_{V_{\\theta}}(t))d t=\\operatorname*{min}_{\\theta\\in\\Theta}\\mathsf{O T}_{h}(\\mu_{U_{\\theta}},\\mu_{V_{\\theta}})=\\operatorname*{min}_{\\theta\\in\\Theta}\\operatorname*{min}_{\\gamma\\in\\Pi(\\mu_{U_{\\theta}},\\mu_{V_{\\theta}})}\\int h(u-v)d\\gamma(u,v).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This formulation reveals that we have turned the stochastic dominance constraint to an inner onedimensional optimal transport problem with a convex cost $h$ . ${\\mathsf{O T}}_{h}(\\mu_{U_{\\theta}},\\mu_{V_{\\theta}})$ can be thought as a soft measure of the violation of the stochastic dominance of $U_{\\theta}$ on $V_{\\theta}$ , hence by minimizing it as function of $\\theta$ we are ensuring the optimal $\\theta^{*}$ results in $U_{\\theta^{*}}$ dominating $V_{\\theta^{*}}$ . Such OT problems with a smooth cost have been subject to theoretical and statistical study in one dimension as well as in high dimensions. For instance, [Manole and Niles-Weed, 2024] considered smooth convex costs, and [Hundrieser et al., 2022] considered more general smooth costs. [Groppe and Hundrieser, 2023] considered entropic regularization of optimal transport with general smooth costs. ", "page_idx": 4}, {"type": "text", "text": "Computational Algorithm via Sorting We consider here empirical measures and turn to solve the inner problem for a fixed $\\theta$ . We omit $\\theta$ in what follows to simplify notation. We are interested in $0\\mathsf{T}_{h}(\\hat{\\mu}_{U},\\hat{\\mu}_{V})$ where $\\begin{array}{r}{\\hat{\\mu}_{U}=\\frac{1}{n}\\sum_{i=1}^{n}\\delta_{u_{i}}\\ \\hat{\\mu}_{V}=\\frac{1}{n}\\sum_{i=1}^{n}\\delta_{v_{i}}}\\end{array}$ . Given the convexity of $h$ and thanks to Theorem (1), the optimal coupling of $\\mathsf{O T}_{h}(\\hat{\\mu}_{U_{\\theta}},\\hat{\\mu}_{V_{\\theta}})$ is given by the north-west corner solution [Peyr\u00e9 and Cuturi, 2019] (Chapter 3, Section 3.4.2) that informally matches the i\u2212th smallest element of $U$ with the $i-$ th smallest element from $V$ . More formally, if we sort the variables $u_{i}$ and get the order statistics (from min to max) $u^{(1)}\\leq\\ldots\\leq u^{(n)}$ and same for $v_{i}$ : $v^{(1)}\\leq\\ldots\\leq v^{(n)}$ . We have: ", "page_idx": 4}, {"type": "equation", "text": "$$\n0\\mathsf T_{h}(\\hat{\\mu}_{U},\\hat{\\mu}_{V})=\\frac{1}{n}\\sum_{i=1}^{n}h(\\boldsymbol{u}^{(i)}-\\boldsymbol{v}^{(i)}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Back to (9), given empirical samples $\\begin{array}{r}{\\hat{\\mu}_{U_{\\theta}}=\\frac{1}{n}\\sum_{i=1}^{n}\\delta_{u_{\\theta}^{i}}}\\end{array}$ and $\\begin{array}{r}{\\hat{\\mu}_{V_{\\theta}}=\\frac{1}{n}\\sum_{i=1}^{n}\\delta_{v_{\\theta}^{i}}}\\end{array}$ , let u(i ), v(i)be the order statistics as function of $\\theta$ . We have therefore: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\Theta}\\mathsf{O T}_{h}(\\hat{\\mu}_{U_{\\theta}},\\hat{\\mu}_{V_{\\theta}})=\\operatorname*{min}_{\\theta\\in\\Theta}\\frac{1}{n}\\sum_{i=1}^{n}h(u_{\\theta}^{(i)}-v_{\\theta}^{(i)})\\;\\;\\;(\\mathsf{A O T})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In Appendix G, we show that the gradients of the objective (11) are asymptotically unbiased for bounded distributions (see the statement for all conditions). Note that the sorting operation in (11) is a 1-Lipschitz function with discontinuous Jacobian.2 Like the ReLU activation function, it can be easily optimized by gradient descent [Anil et al., 2019] (compare also sliced Wasserstein GANs). In practice, computing the gradient at any given step is done by first running the sorting algorithm and taking the gradient with respect to $\\theta$ with the current assignment held fixed. ", "page_idx": 5}, {"type": "text", "text": "AOT for Unpaired Preference Let $\\begin{array}{r}{\\hat{\\mu}_{+}^{n}=\\frac{1}{n}\\sum_{i=1}^{n}\\delta_{(x_{i,+},y_{i,+})}}\\end{array}$ and $\\begin{array}{r}{\\hat{\\mu}_{-}^{n}=\\frac{1}{n}\\sum_{i=1}^{n}\\delta_{(x_{i,-},y_{i,-})}}\\end{array}$ . Our convex relaxation approach for unpaired FSD alignment given in (FSD unpaired) can therefore be cast as an AOT problem (given in Equation (11)) for ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{u_{\\theta}^{i}=\\log\\frac{\\pi_{\\theta}(y_{i,+}|x_{i,+})}{\\pi_{\\mathrm{ref}}(y_{i,+}|x_{i,+})},\\ v_{\\theta}^{i}=\\log\\frac{\\pi_{\\theta}(y_{i,-}|x_{i,-})}{\\pi_{\\mathrm{ref}}(y_{i,-}|x_{i,-})},\\quad i=1,\\dots,n.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "AOT for Paired Preference Let $\\begin{array}{r}{\\hat{\\mu}^{n}\\,=\\,\\frac{1}{n}\\sum_{i=1}^{n}\\delta_{\\left(x_{i},y_{i,+},y_{i,-}\\right)}}\\end{array}$ be a paired preference empirical measure. Our convex relaxation approach for paired FSD alignment given in (FSD paired) can be there cast as an AOT problem (given in Equation (11)) for: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{u_{\\theta}^{i}=\\log\\frac{\\pi_{\\theta}\\left(y_{i,+}\\left|x_{i}\\right)}{\\pi_{\\theta}\\left(y_{i,-}\\left|x_{i}\\right)},\\;\\;v_{\\theta}^{i}=\\log\\frac{\\pi_{\\mathrm{ref}}\\left(y_{i,+}\\left|x_{i}\\right)}{\\pi_{\\mathrm{ref}}\\left(y_{i,-}\\left|x_{i}\\right)},\\quad i=1,\\ldots,n.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "AOT with Soft Sorting One caveat of the alternating optimization for AOT between $\\theta$ and solving the inner optimal transport problem with hard sorting is that the gradient with respect to the parameter $\\theta$ for fixed permutations has dependency in $\\theta$ on the order statistics level only and not through the sorting routine. To alleviate that, we propose to use SoftSorting [Blondel et al., 2020, Cuturi et al., 2019] that uses an entropic regularization to find a smoothed permutations via a Sinkhorn algorithm, which in turn allows the back-propagation on $\\theta$ to depend not only via the order statistics but also via the computational graph of SoftSorting. ", "page_idx": 5}, {"type": "text", "text": "Algorithms 1 and 2 in Appendix B summarize our AOT approach for distributional preference alignment in the unpaired and paired setting. ", "page_idx": 5}, {"type": "text", "text": "4 Statistical Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we focus on the statistical analysis of unpaired-AOT and defer paired-AOT to Appendix E since it has a similar analysis. We make the following assumptions on the OT cost $h$ , the reward $r$ , and the policy hypothesis class $\\mathcal{H}$ . ", "page_idx": 5}, {"type": "text", "text": "Assumption 1 (OT cost). Let $M,R\\,>\\,0$ be finite positive constants. We assume that the loss $h:[-M,M]\\to[0,R],$ , is convex $L$ -Lipchitz and bounded. h is a convex function (E.g. a relaxation of the 0/1 loss such that $h(t)>h(t^{\\prime})$ , for $t<0$ and $t^{\\prime}>0$ ). ", "page_idx": 5}, {"type": "text", "text": "Assumption 2 (Reward). We assume that $r$ is bounded so that $r\\circ\\pi_{\\theta}(x,y)\\in[-M,M]$ ", "page_idx": 5}, {"type": "text", "text": "Assumption 3 (Assumption on the hypothesis class of the policy). We assume $\\pi_{\\mathrm{ref}}$ , $\\pi_{\\theta}\\in\\mathcal{H}=\\{\\pi_{\\theta}:$ such that $r\\circ\\pi_{\\theta}$ differentiable in $\\theta$ and $\\begin{array}{r}{\\operatorname*{sup}_{x\\in\\mathcal{X},y\\in\\mathcal{Y}}\\|\\nabla_{\\theta}r\\circ\\pi_{\\theta}(y|x)\\|\\leq L^{\\prime},\\theta\\in\\Theta\\subset B_{2}(r_{0},\\dot{d}_{\\theta})\\}}\\end{array}$ , for $L^{\\prime},r_{0}>0.$ . . ", "page_idx": 5}, {"type": "text", "text": "Assumption 4. There exists $\\pi_{\\theta}\\in\\mathcal{H}$ such that $(r\\circ\\pi_{\\theta})_{\\sharp}\\mu_{+}\\underset{F S D}{\\geqslant}\\ (r\\circ\\pi_{\\theta})_{\\sharp}\\mu_{-}.$ ", "page_idx": 6}, {"type": "text", "text": "Assumption 1 is satisfied for example by the hinge squared loss $h(t)=(-t)_{+}^{2}$ by the logistic loss $h(t)=\\log(1+e^{-\\beta t})$ , for $t\\in[-M,M]$ . Assumption 2 on the boundedness of the rewards can be imposed by clamping the values of the logits of the policies to $[-M,M]$ , which is common practice in practical implementations of LLM alignment. Assumption 3 is a technical assumption needed to control the covering number of the $r\\circ\\mathcal{H}$ . Assumption 4 ensures the existence of the minimizer in $\\mathcal{H}$ . We overload notations in what follows and refer to $r_{u}$ and $r_{p}$ as $r$ to simplify the presentation. By our relaxation approach described in Section 3 we can relax the unpaired stochastic dominance constraint problem given in (FSD unpaired) to: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi_{\\theta}\\in\\mathcal{H}}\\int_{0}^{1}h\\left(Q_{(r\\circ\\pi_{\\theta})_{\\sharp}\\mu_{+}}(t)-Q_{(r\\circ\\pi_{\\theta})_{\\sharp}\\mu_{-}}(t)\\right)d t=\\operatorname*{min}_{\\pi_{\\theta}\\in\\mathcal{H}}0\\mathsf{T}_{h}\\left((r\\circ\\pi_{\\theta})_{\\sharp}\\mu_{+},(r\\circ\\pi_{\\theta})_{\\sharp}\\mu_{-}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Define the OT cost $c:[-M,M]\\times[-M,M]\\to[0,R]$ such that $c(z,z^{\\prime})=h(z-z^{\\prime})$ , for $z,z\\in$ $[-M,M]$ . Define the $c$ -transform of a function $\\varphi:[-{\\bar{M}},M]\\to\\mathbb{R}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\varphi^{c}(z)=\\operatorname*{inf}_{z^{\\prime}\\in[-M,M]}h(z-z^{\\prime})-\\varphi(z).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In our setting, a function is called $c$ -concave if there exists $\\psi:[-M,M]\\to\\mathbb{R}$ such that $\\varphi=\\psi^{c}$ . Define: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{F}_{c}=\\left\\{\\varphi:[-M,M]\\to[-R,R],\\varphi\\mathrm{~is~}c\\mathrm{-concave},\\mathrm{with}\\ ||\\varphi^{c}||_{\\infty}\\leq R\\right\\}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "By duality (Theorem 5.10 in [Villani, 2009]) we have: ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\sf O T}_{h}\\left((r\\circ\\pi_{\\theta})_{\\sharp}\\mu_{+},(r\\circ\\pi_{\\theta})_{\\sharp}\\mu_{-}\\right)=\\operatorname*{sup}_{\\varphi\\in\\mathcal{F}_{c}}\\int\\varphi(r\\circ\\pi_{\\theta})d\\mu_{+}-\\int\\varphi^{c}(r\\circ\\pi_{\\theta})d\\mu_{-}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Replacing the dual expression of ${0}{\\mathsf{T}}_{h}$ in $(\\mathsf{u A O T}_{h})$ ), we see that $(\\mathsf{u A O T}_{h})$ ) can be cast as a min-max problem: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi_{\\theta}\\in\\mathcal{H}}\\operatorname*{sup}_{\\varphi\\in\\mathcal{F}_{c}}\\int\\varphi(r\\circ\\pi_{\\theta})d\\mu_{+}-\\int\\varphi^{c}(r\\circ\\pi_{\\theta})d\\mu_{-}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Given samples $\\begin{array}{r}{\\hat{\\mu}_{+}^{n}=\\frac{1}{n}\\sum_{i=1}^{n}\\delta_{(x_{i,+},y_{i,+})}}\\end{array}$ and $\\begin{array}{r}{\\hat{\\mu}_{-}^{n}=\\frac{1}{n}\\sum_{i=1}^{n}\\delta_{(x_{i,-},y_{i,-})}}\\end{array}$ , the empirical problem is: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi_{\\theta}\\in\\mathcal{H}}\\operatorname*{sup}_{\\varphi\\in\\mathcal{F}_{c}}\\int\\varphi(r\\circ\\pi_{\\theta})d\\hat{\\mu}_{+}^{n}-\\int\\varphi^{c}(r\\circ\\pi_{\\theta})d\\hat{\\mu}_{-}^{n}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Recall that ${0}{\\mathsf{T}}_{h}$ is a measure of the violation of stochastic dominance of $(r\\circ\\pi_{\\theta})_{\\sharp}\\mu_{+}$ on $(r\\circ\\pi_{\\theta})_{\\sharp}\\mu_{-}$ We have the following result on the sample complexity of the violation of stochastic dominance: ", "page_idx": 6}, {"type": "text", "text": "Theorem 2 (Sample Complexity of Dominance Violation for AOT Unpaired). Let $\\pi\\theta^{*}$ be the population minimizer of $(\\mathsf{u A O T}_{h})$ and $\\pi_{\\hat{\\theta}_{n}}$ be the solution of the empirical problem (13). We have the following sample complexity bound for the violation of stochastic dominance in AOT unpaired: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}\\,{\\mathsf{O T}}_{h}\\left((r\\circ\\pi_{\\hat{\\theta}_{n}})_{\\sharp}\\mu_{+},(r\\circ\\pi_{\\hat{\\theta}_{n}})_{\\sharp}\\mu_{-}\\right)\\leq\\underbrace{{\\mathsf{O T}}_{h}\\left((r\\circ\\pi_{\\theta^{*}})_{\\sharp}\\mu_{+},(r\\circ\\pi_{\\theta^{*}})_{\\sharp}\\mu_{-}\\right)}_{:=\\;}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{+\\,2\\mathcal{R}_{n}(\\mathcal{F}_{c};(r\\circ\\pi_{\\theta^{*}})_{\\sharp}\\mu_{+})+2\\mathcal{R}_{n}(\\mathcal{F}_{c}^{c};(r\\circ\\pi_{\\theta^{*}})_{\\sharp}\\mu_{-})+2\\mathcal{R}_{n}(\\mathcal{F}_{c}\\circ r\\circ\\mathcal{H};\\mu_{+})+2\\mathcal{R}_{n}(\\mathcal{F}_{c}^{c}\\circ r\\circ\\mathcal{H};\\mu_{-}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{R}_{n}(\\mathcal{F};\\nu)=\\mathbb{E}\\operatorname*{sup}_{\\varphi\\in\\mathcal{F}}\\left|\\frac{1}{n}\\sum_{i=1}^{n}\\sigma_{i}\\varphi(Z_{i})\\right|}\\end{array}$ is the Rademacher Complexity and for $i=1\\ldots n_{!}$ $\\sigma_{i}$ are independent Rademacher random variables and $Z_{i}\\sim\\nu$ iid. ", "page_idx": 6}, {"type": "text", "text": "By considering our assumptions on the cost, the reward, and the hypothesis class, we obtain the parametric rate in $n$ : ", "page_idx": 6}, {"type": "text", "text": "Corollary 1. (Informal) Under Assumptions $^{\\,l}$ , 2 and $3$ we have: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{I.\\ \\mathbb{E}\\,\\mathsf{O T}_{h}\\left((r\\circ\\pi_{\\theta_{n}})\\sharp\\mu_{+},(r\\circ\\pi_{\\hat{\\theta}_{n}})_{\\sharp}\\mu_{-}\\right)-\\mathsf{O T}_{h}\\left((r\\circ\\pi_{\\theta^{*}})_{\\sharp}\\mu_{+},(r\\circ\\pi_{\\theta^{*}})_{\\sharp}\\mu_{-}\\right)\\lesssim n^{-\\frac{1}{2}},\\boldsymbol{u}\\mu_{-}\\left(\\theta_{n},\\varphi_{0}\\right)\\sim\\mathsf{O T}_{h}(r).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We see that under our assumptions and for the hinge loss squared, the expected violation of the desired dominance in AOT unpaired converges to zero as $n\\to\\infty$ . ", "page_idx": 7}, {"type": "text", "text": "Remark 1. While in Section 3, we used the primal formulation to compute ${0}{\\sf T}_{h}$ due to its computational appeal thanks to the sorting algorithm we used for analyzing the sample complexity the dual of ${0}{\\mathsf{T}}_{h}$ . The dual reveals the game theoretic aspect of AOT as a min-max game between the policy $\\pi_{\\theta}$ and the dual potential $\\varphi_{c}$ that imposes FSD on the preference we want to infuse to the policy. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we evaluate the performance of the proposed AOT method on a diverse set of base LLMs and datasets, comparing with currently available alternative alignment algorithms. ", "page_idx": 7}, {"type": "table", "img_path": "2LctgfN6Ty/tmp/bf0057dff856e7e93d064e682f718792f4ae327de3619df9fe17fd41f39fb4ec.jpg", "table_caption": ["LLM Alignment Alternatives We compared AOT with current state-of-the-art alignment approaches, specifically Direct Preference Optimization (DPO) [Rafailov et al., 2024], KahnemanTversky Optimization (KTO) [Ethayarajh et al., 2024] and Identity Policy Optimization (IPO) [Azar et al., 2024]. DPO and IPO operate on paired preference data, while KTO can handle both paired and unpaired prompt/response samples. "], "table_footnote": ["Table 1: Merlinite-7B trained on UltraFeedback Binarized. AOT results in the best performing LLM as compared to the alternative alignment algorithms on AlpacaEval, and is competitive across the other benchmarks that are evaluated in the zero shot regime. "], "page_idx": 7}, {"type": "text", "text": "Reference Models Traditionally, model alignment is the third and final step applied to the LLM that already has gone through original pretraining and supervised fine-tuning. For our experiments, we selected a range of models at various stages and with different levels of performance, all in the family of 7B-parameter models. Specifically, we used Merlinite-7B [Sudalairaj et al., 2024], which is a variant of Mistral-7B-v0.1 that has been instruction-tuned (SFT) on data from a synthetic data generator using a taxonomy-driven data curation process. In Appendix H we also cover other popular LLMs, such as Mistral-7B [Jiang et al., 2023], OpenHermes-2.5-Mistral-7B [Teknium, 2024], Starling [Zhu et al., 2023], Mistral-7B Jiang et al. [2023], and Llama3-8B [AI $@$ Meta, 2024]. ", "page_idx": 7}, {"type": "text", "text": "Datasets For our experiments, we used both paired and unpaired datasets. For the paired dataset, we used the UltraFeedback binarized dataset from [Tunstall et al., 2023b], containing over 60K training samples, where for each prompt, there is a pair of chosen (preferred) and rejected (not preferred) responses. This alignment dataset is widely used, and all compared alignment techniques are well-suited for it. For unpaired datasets, we used PKU BeaverTails [Ji et al., 2023] with over 300K samples and HelpSteer [Wang et al., 2023] with around 35K samples. Here, for each prompt, there is only a single response with a score defined by some attributes (e.g., safety, faithfulness, helpfulness, etc.). We used the sum of attribute values and thresholded by the median to binarize the responses into chosen and rejected. For this unpaired dataset, only KTO and our AOT are applicable. ", "page_idx": 7}, {"type": "text", "text": "Metrics To measure the performance of different alignment methods, we used popular evaluation metrics, AlpacaEval [Dubois et al., 2024] and Open LLM benchmark [Beeching et al., 2023]. We note that Alpaca uses GPT4 model as a judge to compare candidate responses to GPT4-based references on a set of 805 challenging questions. The GPT4-based evaluations are expensive, so to limit our expenses, we also employed a very strong and capable Llama3-70B-Instruct [AI $@$ Meta, 2024] as a judge. As we show in Appendix H in Table 2, the order determined by Llama3-70B-Instruct ", "page_idx": 7}, {"type": "image", "img_path": "2LctgfN6Ty/tmp/421520d4c82646fd5b5627752595898f608895e564960a88ed63067433cb44b0.jpg", "img_caption": ["Figure 2: Impact of batch size and loss type on AOT performance. The batch size is the effective number of samples in the mini-batch per GPU. We found the logistic loss to be performing better than least squared or hinge squared losses (all using $\\beta=0.01\\$ ). As we increase batch size, we also observed improvement in AOT performance, which is expected as more samples per minibatch results in a better effect of stochastic dominance (conforming Corollary 1). "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "2LctgfN6Ty/tmp/26e5579132d87723ae62e59399e663e951ea55dcbed0c084773066a634ed5e63.jpg", "img_caption": ["and GPT4 is the same (the absolute score values are different), providing a better free alternative LLM-judge for local Alpaca evaluations. For intermediate results, we also employed Tiny Benchmarks [Maia Polo et al., 2024] to approximate original metrics and provide fast feedback during the initial development. We also evaluated the aligned models on six key benchmarks from the Open LLM Leaderboard: AI2 Reasoning Challenge - ARC (grade-school science questions), HellaSwag (commonsense inference), MMLU (multi-task accuracy), TruthfulQA (tendency to reproduce falsehoods), Winogrande (commonsense reasoning) and GSM8K (grade school math word problems). Note that in all the above benchmarks we use 0-shot prompts, a more challenging setting as opposed to commonly used few-shot prompting. ", "Figure 3: Impact of $(\\beta)$ parameter on performance of different alignment algorithms. $\\beta$ controls the divergence of the policy model from the initial reference model (low beta - more divergence, high beta - less divergence). We see a general trend that with higher betas, LLMs alignment decreases the performance. Hence, for all experiments, we selected $\\beta\\,=\\,0.01$ as a default value. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Experimental Setup Our implementation is based on the HuggingFace Alignment Handbook Tunstall et al. [2023a]. As we show in Appendix in Section B, the changes needed to adapt HF TRL trainer [von Werra et al., 2020] for AOT are minimal and therefore can easily be adapted by the community. For each run our compute setup consisted of 8 H100 GPUs. We used LoRA [Hu et al., 2021] for parameter-efficient fine-tuning during alignment and the FSDP (Fully-Sharded Data-Parallel) setup to train the model over multiple GPUs. Under this setup, the training of each 7B-parameter model on the UltraFeedback dataset took approximately one hour. The evaluation on AlpacaEval and Open LLM benchmarks took one additional hour to get the final results. ", "page_idx": 8}, {"type": "text", "text": "Results In Table 1, we present the main results of comparing AOT to other baselines (KTO, DPO, and IPO) on paired UltraFeedback binarized dataset. On AlpacaEval (GPT4), our AOT unpaired approach scores $31.3\\%$ , which is a significant gain from the base Merlinite-7B model. As of time of this writing (May 22nd, 2024), this result places our AOT aligned LLM on AlpacaEval LeaderBoard ahead of such strong competitors as KTO-Mistral-PAIR [Ethayarajh et al., 2023] and other 7B-parameter models, reaching the level of Mixtral-8x22B-v0.1 (see Figure 4 in Appendix for an illustration). On other LLM benchmarks AOT performs competitively to other baselines. As mentioned earlier, these evaluations are done using 0-shot prompts, leading to a more challenging setting and resulting in overall lower performance across metrics and baselines. For other base LLMs we show their performance in Appendix H (see Tables 3, 4, 5, and 6). ", "page_idx": 8}, {"type": "text", "text": "We also examined the effect of batch size and the choice of loss function on AOT performance, results shown in Fig. 2. As the batch size increases, AlpacaEval (based on Llama3-70B-instruct) also increases in line with our theory in Corollary 1. Note that our current setup (FSDP over 8 H100 GPUs) limits our batch size to 35 samples per GPU. We have also examined the impact of beta (controlling divergence of policy from reference) on AOT performance in Fig. 3. We noticed a trend that with higher betas the performance of LLMs alignment decreases, thus we set $\\beta=0.01$ . Ablation results comparing hard and soft sorting as well as the variance of AlpacaEval scores across multiple runs in Appendix H (Tables 7 and 10) show the overall robustness of AOT . ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We present in this paper Distributional Alignment via Optimal Transport (AOT) for large language models. The AOT cost can be cast as a one-dimensional optimal transport problem with a smooth and convex cost that penalizes violations of the dominance of the chosen on rejected marginals. AOT enjoys parametric statistical rates. We showed with extensive experimentation on various paired and unpaired datasets, base models, and different loss functions, that AOT alignment robustly leads to aligned models that outperform alternative alignment strategies such as DPO, KTO and IPO on the Alpaca Benchmark, leading to the best 7B model to date on that benchmark as of the time of writing. On other benchmarks such as the open LLM leaderboard AOT leads to competitive results. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/blob/ main/MODEL_CARD.md.   \nC. Anil, J. Lucas, and R. Grosse. Sorting out lipschitz function approximation. In International Conference on Machine Learning, pages 291\u2013301. PMLR, 2019.   \nM. G. Azar, Z. D. Guo, B. Piot, R. Munos, M. Rowland, M. Valko, and D. Calandriello. A general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, pages 4447\u20134455. PMLR, 2024.   \nY. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.   \nP. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classification, and risk bounds. Journal of the American Statistical Association, 101:138 \u2013 156, 2006. URL https://api. semanticscholar.org/CorpusID:2833811.   \nE. Beeching, C. Fourrier, N. Habib, S. Han, N. Lambert, N. Rajani, O. Sanseviero, L. Tunstall, and T. Wolf. Open LLM Leaderboard. https://huggingface.co/spaces/HuggingFaceH4/ open_llm_leaderboard, 2023.   \nM. Blondel, O. Teboul, Q. Berthet, and J. Djolonga. Fast differentiable sorting and ranking. In International Conference on Machine Learning, pages 950\u2013959. PMLR, 2020.   \nP. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human preferences. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_ files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf.   \nM. Cuturi, O. Teboul, and J.-P. Vert. Differentiable ranking and sorting using optimal transport. Advances in neural information processing systems, 32, 2019.   \nE. Del Barrio, J. A. Cuesta-Albertos, and C. Matr\u00e1n. An optimal transportation approach for assessing almost stochastic order. The Mathematics of the Uncertain: A Tribute to Pedro Gil, pages 33\u201344, 2018.   \nC. Domingo-Enrich, Y. Schiff, and Y. Mroueh. Learning with stochastic orders. In The Eleventh International Conference on Learning Representations, 2022.   \nY. Dubois, B. Galambosi, P. Liang, and T. B. Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024.   \nK. Ethayarajh, W. Xu, D. Jurafsky, and D. Kiela. Human-centered loss functions (halos). Technical report, Contextual AI, 2023.   \nK. Ethayarajh, W. Xu, N. Muennighoff, D. Jurafsky, and D. Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024.   \nM. Groppe and S. Hundrieser. Lower complexity adaptation for empirical entropic optimal transport. arXiv preprint arXiv:2306.13580, 2023.   \nE. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. LoRa: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \nS. Hundrieser, T. Staudt, and A. Munk. Empirical optimal transport between different measures adapts to lower complexity. arXiv preprint arXiv:2202.10434, 2022.   \nJ. Ji, M. Liu, J. Dai, X. Pan, C. Zhang, C. Bian, C. Zhang, R. Sun, Y. Wang, and Y. Yang. Beavertails: Towards improved safety alignment of llm via a human-preference dataset. arXiv preprint arXiv:2307.04657, 2023.   \nA. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b (2023). arXiv preprint arXiv:2310.06825, 2023.   \nF. Maia Polo, L. Weber, L. Choshen, Y. Sun, G. Xu, and M. Yurochkin. tinyBenchmarks: evaluating LLMs with fewer examples. arXiv preprint arXiv:2402.14992, 2024.   \nT. Manole and J. Niles-Weed. Sharp convergence rates for empirical optimal transport with smooth costs. The Annals of Applied Probability, 34(1B):1108\u20131135, 2024.   \nW. Ogryczak and A. Ruszczynski. Dual stochastic dominance and related mean-risk models. SIAM Journal on Optimization, 13(1):60\u201378, 2002.   \nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.   \nG. Peyr\u00e9 and M. Cuturi. Computational optimal transport. Foundations and Trends in Machine Learning, 11(5-6):355\u2013607, 2019. ISSN 1935-8237. doi: 10.1561/2200000073.   \nR. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024.   \nL. Rosasco, E. De Vito, A. Caponnetto, M. Piana, and A. Verri. Are loss functions all the same? Neural Comput., 16(5):1063\u20131076, may 2004. ISSN 0899-7667.   \nF. Santambrogio. Optimal Transport for Applied Mathematicians: Calculus of Variations, PDEs, and Modeling. Birkh\u00e4user, Cham, 2015. ISBN 9783319208275. doi: 10.1007/978-3-319-20828-2.   \nN. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008\u20133021, 2020.   \nS. Sudalairaj, A. Bhandwaldar, A. Pareja, K. Xu, D. D. Cox, and A. Srivastava. Lab: Large-scale alignment for chatbots. arXiv preprint arXiv:2403.01081, 2024.   \nTeknium. Openhermes-2.5-mistral-7b. https://huggingface.co/teknium/OpenHermes-2. 5-Mistral-7B, 2024.   \nL. Tunstall, E. Beeching, N. Lambert, N. Rajani, S. Huang, K. Rasul, A. M. Rush, and T. Wolf. The alignment handbook. https://github.com/huggingface/alignment-handbook, 2023a.   \nL. Tunstall, E. Beeching, N. Lambert, N. Rajani, K. Rasul, Y. Belkada, S. Huang, L. von Werra, C. Fourrier, N. Habib, et al. Zephyr: Direct distillation of LM alignment. arXiv preprint arXiv:2310.16944, 2023b.   \nC. Villani. Optimal Transport: old and new, volume 338. Springer, 2009.   \nL. von Werra, Y. Belkada, L. Tunstall, E. Beeching, T. Thrush, N. Lambert, and S. Huang. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl, 2020.   \nZ. Wang, Y. Dong, J. Zeng, V. Adams, M. N. Sreedhar, D. Egert, O. Delalleau, J. P. Scowcroft, N. Kant, A. Swope, et al. Helpsteer: Multi-attribute helpfulness dataset for steerlm. arXiv preprint arXiv:2311.09528, 2023.   \nY. Zhao, M. Khalman, R. Joshi, S. Narayan, M. Saleh, and P. J. Liu. Calibrating sequence likelihood improves conditional language generation. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $\\equiv$ 0qSOodKmJaN.   \nB. Zhu, E. Frick, T. Wu, H. Zhu, and J. Jiao. Starling-7b: Improving llm helpfulness and harmlessness with rlaif, November 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Broader Impact and Limitations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this paper, we introduced an alignment method for LLMs that can capture rewards at the distributional level without the requirement of paired preference data. Our algorithm was derived by imposing the stochastic dominance of positive reward distribution over negative distributions through an Optimal Transport formulation. It enjoys a very simple algorithmic implementation in terms of a closed-form expression via sorting on empirical measures. Empirically, our algorithm demonstrates excellent results by allowing us to train 7B parameter models that achieve state-of-the-art evaluation results on Open LLM Benchmarks and AlpacaEval. ", "page_idx": 12}, {"type": "text", "text": "In terms of broader societal impact, we would like to highlight the benefits that our AOT algorithm will bring to RLHF by enabling a more robust distributional alignment of LLMs, improving their ability to follow instructions accurately, and aligning their responses with human values. ", "page_idx": 12}, {"type": "text", "text": "Our work shares the same limitations and possible negative broader societal impacts as the majority of RLHF work. The algorithm is fundamentally limited by the training dataset used for alignment and might, therefore, contribute to amplifying various types of bias present in the data. In addition, alignment through AOT is not enough to address aspects related to the security and safety of LLM deployment. In general, better performance on a given set of benchmarks following alignment does not imply better performance across the board in other tasks, and ad-hoc evaluation specific to each task of interest is warranted. ", "page_idx": 12}, {"type": "text", "text": "B Algorithms and Pytorch Code In Hugging Face TRL ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Algorithm 1 AOT Unpaired   \n1: Input: $\\pi_{\\theta}$ $\\,),\\pi_{\\mathrm{ref}},\\beta>0,\\varepsilon>0,$ ,   \n2: Unpaired Preference Data: \u201cChosen\u201d   \n$\\begin{array}{r}{\\hat{\\mu}_{+}^{n}\\;{=}\\;\\frac{1}{n}\\sum_{i=1}^{n}\\delta_{(x_{i,+},y_{i,+})}}\\end{array}$ and \u201cRejected\u201d   \n3: $\\begin{array}{r}{\\hat{\\mu}_{-}^{n}=\\frac{1}{n}\\sum_{i=1}^{n}\\delta_{(x_{i,-},y_{i,-})}}\\end{array}$ .   \n4: for iter $\\gets1$ , $n_{\\mathrm{iter}}$ do   \n5: Get a Positive/Negative mini-batch   \n6: $\\{(x_{i,+},y_{i,+})\\sim\\hat{\\mu}_{+}^{\\bar{n}},i=1\\ldots b\\}$   \n7: $\\{(x_{i,-},y_{i,-})\\sim\\hat{\\mu}_{-}^{i},i=1\\ldots b\\}$   \n8: Compute Rewards for $i=1\\ldots b$   \n9: $\\begin{array}{r}{u_{\\theta}^{i}=\\log\\frac{\\pi_{\\theta}(y_{i,+}|x_{i,+})}{\\pi_{\\mathrm{ref}}(y_{i,+}|x_{i,+})}}\\\\ {v_{\\theta}^{i}=\\log\\frac{\\pi_{\\theta}(y_{i,-}|x_{i,-})}{\\pi_{\\mathrm{ref}}(y_{i,-}|x_{i,-})}}\\end{array}$   \n10:   \n11: Sort Rewards   \n12: if hard sort then   \n13: $\\begin{array}{r}{(u^{(1)}\\cdot\\cdot\\cdot u^{(b)})=\\mathsf{S o r t}(u_{\\theta}^{i})}\\\\ {(v^{(1)}\\cdot\\cdot\\cdot v^{(b)})=\\mathsf{S o r t}(v_{\\theta}^{i})}\\end{array}$   \n14:   \n15: else if soft sort then   \n16: $\\begin{array}{r l}&{(u^{(1)}\\cdot\\dots u^{(b)})=\\mathsf{S o f t S o r t}(u_{\\theta}^{i},\\varepsilon)}\\\\ &{(v^{(1)}\\cdot\\dots v^{(b)})=\\mathsf{S o f t S o r t}(v_{\\theta}^{i},\\varepsilon)}\\end{array}$   \n17:   \n18: end if   \n19: Compute AOT logistic loss   \n20: $\\begin{array}{r l}&{\\ell_{\\theta}=\\bar{-\\frac{1}{b}}\\!\\sum_{i=1}^{b}\\log\\sigma(\\beta(u_{\\theta}^{(i)}-v_{\\theta}^{(i)}))}\\\\ &{\\bar{\\mathbf{U}}\\bar{\\mathbf{n}}\\mathbf{d}\\mathbf{a}\\mathbf{t}\\mathrm{e}\\;\\theta}\\end{array}$   \n21:   \n22: \u03b8 \u2190PagedAdamw32bit $\\left(\\nabla_{\\theta}\\ell(\\theta)\\right)$   \n23: end for   \n24: Return $\\pi_{\\theta}$   \nAlgorithm 2 AOT Paired   \n1: Input: \u03c0\u03b8, $\\pi_{\\mathrm{ref}},\\beta>0,\\varepsilon>0,$ ,   \n2: Paired Preference Data: ${\\hat{\\mu}}^{n}$   \n3: $\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}\\delta_{(x_{i},y_{i,+},y_{i,-})}}\\\\ {\\mathbf{for\\;iter}\\leftarrow1,n_{\\mathrm{iter}}\\;\\mathbf{do}}\\end{array}$   \n4: Get a Positive/Negative mini-batch   \n5: $\\{(x_{i},y_{i,+},y_{i,-})\\sim\\hat{\\mu}^{n},i=1\\ldots b\\}$   \n6: Compute Margins for $i=1\\ldots b$   \n7: $\\begin{array}{r l}&{u_{\\theta}^{i}=\\overline{{\\log\\frac{\\pi_{\\theta}(y_{i,+}|x_{i})}{\\pi_{\\theta}(y_{i,-}|x_{i})}}}}\\\\ &{v_{\\theta}^{i}=\\log\\frac{\\pi_{\\mathrm{ref}}(y_{i,+}|x_{i})}{\\pi_{\\mathrm{ref}}(y_{i,-}|x_{i})}}\\end{array}$   \n8:   \n9: Sort Margins   \n10: if hard sort then   \n11: $\\begin{array}{r}{(\\boldsymbol{u}^{(1)}\\cdot\\boldsymbol{\\cdot}\\cdot\\boldsymbol{u}^{(b)})=\\mathsf{S o r t}(u_{\\theta}^{i})}\\\\ {(\\boldsymbol{v}^{(1)}\\cdot\\boldsymbol{\\cdot}\\cdot\\boldsymbol{v}^{(b)})=\\mathsf{S o r t}(v_{\\theta}^{i})}\\end{array}$   \n12:   \n13: else if soft sort then   \n14: $(u^{(1)}\\cdot\\cdot\\cdot u^{(b)})=\\mathsf{S o f t S o r t}(u_{\\theta}^{i},\\varepsilon)$   \n15: $(v^{(1)}\\cdot\\cdot\\cdot v^{(b)})=\\mathsf{S o f t S o r t}(v_{\\theta}^{i},\\varepsilon)$   \n16: end if   \n17: Compute AOT logistic loss   \n18: $\\begin{array}{r l}&{\\ell_{\\theta}=\\overline{{-\\frac{1}{b}\\sum_{i=1}^{b}\\log{\\sigma(\\beta(u_{\\theta}^{(i)}-v_{\\theta}^{(i)}))}}}}\\\\ &{\\mathbf{Update}\\,\\theta}\\\\ &{\\theta\\leftarrow\\mathsf{P a g e d A d a m w32b i t}(\\nabla_{\\theta}\\ell(\\theta))}\\end{array}$   \n19:   \n20:   \n21: end for   \n22: Return $\\pi_{\\theta}$ ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "import t o r c h import t o r c h s o r t ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "def dpo_loss (   \ne l i f s e l f . l o s s _ t y p e $==$ \" AOT_unpair \" : c h o s e n _ l o g r a t i o s $=$ policy_chosen_logps \u2212 r e f e r e n c e _ c h o s e n _ l o g p s r e j e c t e d _ l o g r a t i o s $=$ p o l i c y _ r e j e c t e d _ l o g p s \u2212 r e f e r e n c e _ r e j e c t e d _ l o g p s i f s e l f . s o r t _ t y p e $==$ \" h a r d _ s o r t \" : c h o s e n _ l o g r a t i o s _ s o r t e d , $=$ t o r c h . s o r t ( c h o s e n _ l o g r a t i o s , dim $=\\!0$ ) r e j e c t e d _ l o g r a t i o s _ s o r t e d , $\\_=$ t o r c h . s o r t ( r e j e c t e d _ l o g r a t i o s , dim =0) e l i f s e l f . s o r t _ t y p e $==$ \" s o f t _ s o r t \" : c h o s e n _ l o g r a t i o $^{+}-$ s o r t e d $=$ t o r c h s o r t . s o f t _ s o r t ( c h o s e n _ l o g r a t i o s , r e g u l a r i z a t i o n _ s t r e n g t h $=0.1$ ) r e j e c t e d _ l o g r a t i o s _ s o r t e d $=$ t o r c h s o r t . s o f t _ s o r t ( r e j e c t e d _ l o g r a t i o s , r e g u l a r i z a t i o n _ s t r e n g t h $=0.1$ ) d e l t a _ s o r t e d $=$ c h o s e n _ l o g r a t i o s _ s o r t e d \u2212 r e j e c t e d _ l o g r a t i o s _ s o r t e d i f s e l f . AOT_loss $==$ \" hinge \" : l o s s e s $=$ t o r c h . r e l u ( s e l f . beta \u2212 d e l t a _ s o r t e d ) $^{**2}$ e l i f s e l f . AOT_loss $==$ \" l o g i s t i c \" : l o s s e s $=$ ( \u2212F . logsigmoid ( s e l f . beta $^*$ d e l t a _ s o r t e d ) $^\\ast$ (1 \u2212 s e l f . label_smoothing ) \u2212 F . logsigmoid ( \u2212s e l f . beta $^*$ d e l t a _ s o r t e d ) $^\\ast$ s e l f . label_smoothing ) ", "page_idx": 13}, {"type": "text", "text": "e l i f s e l f . l o s s _ t y p e $==$ \" AOT_pair \" : p i _ l o g r a t i o s $=$ policy_chosen_logps \u2212 p o l i c y _ r e j e c t e d _ l o g p s r e f _ l o g r a t i o s $=$ r e f e r e n c e _ c h o s e n _ l o g p s \u2212 r e f e r e n c e _ r e j e c t e d _ l o g p s i f s e l f . s o r t _ t y p e $\\mathbf{\\Sigma}=\\mathbf{\\Sigma}^{n}$ h a r d _ s o r t \" : p i _ l o g r a t i o s _ s o r t e d , $\\_=$ t o r c h . s o r t ( p i _ l o g r a t i o s , dim $=\\!0$ ) r e f _ l o g r a t i o s _ s o r t e d , $_-=$ t o r c h . s o r t ( r e f _ l o g r a t i o s , dim =0) e l i f s e l f . s o r t _ t y p e $==$ \" s o f t _ s o r t \" : p i _ l o g r a t i o s _ s o r t e d $=$ t o r c h s o r t . s o f t _ s o r t ( p i _ l o g r a t i o s , r e g u l a r i z a t i o n _ s t r e n g t h $=0.1$ ) r e f _ l o g r a t i o s _ s o r t e d $=$ t o r c h s o r t . s o f t _ s o r t ( r e f _ l o g r a t i o s , r e g u l a r i z a t i o n _ s t r e n g t h $=0.1$ ) d e l t a _ s o r t e d $=$ p i _ l o g r a t i o s _ s o r t e d \u2212 r e f _ l o g r a t i o s _ s o r t e d i f s e l f . AOT_loss $\\doteq\\\"\\,\\mathrm{hin}\\,\\mathrm{g}\\,\\mathrm{e}\\,^{,}$ : l o s s e s $=$ t o r c h . r e l u ( s e l f . beta \u2212 d e l t a _ s o r t e d ) $^{**2}$ e l i f s e l f . AOT_loss $==$ \" l o g i s t i c \" : l o s s e s $=$ ( \u2212F . logsigmoid ( s e l f . beta $^*$ d e l t a _ s o r t e d ) $^\\ast$ (1 \u2212 s e l f . label_smoothing ) \u2212 F . logsigmoid ( \u2212s e l f . beta $^*$ d e l t a _ s o r t e d ) $^\\ast$ s e l f . label_smoothing ) ", "page_idx": 13}, {"type": "text", "text": "C Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof of Theorem 2. Let ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\varepsilon(\\theta,\\mu_{+},\\mu_{-})=\\operatorname*{sup}_{\\varphi\\in\\mathcal{F}_{c}}\\int\\varphi(r\\circ\\pi_{\\theta})d\\mu_{+}-\\int\\varphi^{c}(r\\circ\\pi_{\\theta})d\\mu_{-}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where for $z,z^{\\prime}\\in\\mathbb{R}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\varphi^{c}(z)=\\operatorname*{inf}_{z^{\\prime}\\in[-M,M]}h(z-z^{\\prime})-\\varphi(z)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and (a function $f$ is $c-$ concave if there exits $g$ such that $f=g^{c}$ ). ", "page_idx": 14}, {"type": "text", "text": "The population problem : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi_{\\theta}\\in\\mathcal{H}}\\varepsilon(\\theta,\\mu_{+},\\mu_{-})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Given samples $\\begin{array}{r}{\\hat{\\mu}_{+}^{n}=\\frac{1}{n}\\sum_{i=1}^{n}\\delta_{(x_{+}^{i},y_{+}^{i})}}\\end{array}$ and $\\begin{array}{r}{\\hat{\\mu}_{-}^{m}=\\frac{1}{m}\\sum_{i=1}^{m}\\delta_{(x_{-}^{i},y_{-}^{i})}}\\end{array}$ , the empirical problem is : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\varepsilon(\\theta,\\hat{\\mu}_{+}^{n},\\hat{\\mu}_{-}^{m})=\\operatorname*{sup}_{\\varphi\\in\\mathcal{F}_{c}}\\int\\varphi(r\\circ\\pi_{\\theta})d\\hat{\\mu}_{+}^{n}-\\int\\varphi^{c}(r\\circ\\pi_{\\theta})d\\hat{\\mu}_{-}^{m}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and the ERM problem is : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi_{\\theta}\\in\\mathcal{H}}\\varepsilon(\\theta,\\hat{\\mu}_{+}^{n},\\hat{\\mu}_{-}^{m})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let $\\hat{\\theta}_{m,n}$ be the minimizer of the ERM we have for any $\\theta$ , by the definition of the minimizer: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\varepsilon(\\hat{\\theta}_{m,n},\\hat{\\mu}_{+}^{n},\\hat{\\mu}_{-}^{m})\\leq\\displaystyle\\operatorname*{sup}_{\\varphi\\in\\mathcal{F}_{c}}\\int\\varphi(r\\circ\\pi_{\\theta})d\\hat{\\mu}_{+}^{n}-\\int\\varphi^{c}(r\\circ\\pi_{\\theta})d\\hat{\\mu}_{-}^{m}}&{}\\\\ {\\ }&{\\leq\\displaystyle\\operatorname*{sup}_{\\varphi\\in\\mathcal{F}_{c}}\\int\\varphi(r\\circ\\pi_{\\theta})d\\mu_{+}-\\int\\varphi^{c}(r\\circ\\pi_{\\theta})d\\mu_{-}}\\\\ {\\ }&{+\\displaystyle\\operatorname*{sup}_{\\varphi\\in\\mathcal{F}_{c}}\\int\\varphi(r\\circ\\pi_{\\theta})d(\\hat{\\mu}_{+}^{n}-\\mu_{+})}\\\\ {\\ }&{+\\displaystyle\\operatorname*{sup}_{\\varphi\\in\\mathcal{F}_{c}}\\int\\varphi^{c}(r\\circ\\pi_{\\theta})d(\\mu_{-}-\\hat{\\mu}_{-}^{m})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let $\\theta^{*}$ be the minimizer of (14) for $\\theta=\\theta^{*}$ in the above inequality, and taking expectations on the randomness of the samples we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\varepsilon(\\hat{\\theta}_{m,n},\\hat{\\mu}_{+}^{n},\\hat{\\mu}_{-}^{m})\\leq\\mathbb{E}\\varepsilon(\\theta^{*},\\mu_{+},\\mu_{-})+\\mathbb{E}\\underset{\\varphi\\in\\mathcal{F}_{c}}{\\operatorname*{sup}}\\int\\varphi(r\\circ\\pi_{\\theta^{*}})d(\\hat{\\mu}_{+}^{n}-\\mu_{+})}\\\\ &{\\phantom{\\mathbb{E}}+\\mathbb{E}\\underset{\\varphi\\in\\mathcal{F}_{c}}{\\operatorname*{sup}}\\int\\varphi^{c}(r\\circ\\pi_{\\theta^{*}})d(\\mu_{-}-\\hat{\\mu}_{-}^{m})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "On the other hand by symmetrization we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}\\operatorname*{sup}_{\\varphi\\in\\mathcal{F}_{c}}\\int\\varphi(r\\circ\\pi_{\\theta^{\\ast}})d(\\hat{\\mu}_{+}^{n}-\\mu_{+})\\leq2\\mathcal{R}_{n}(\\mathcal{F}_{c}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\begin{array}{r}{{\\mathcal{R}}_{n}({\\mathcal{F}})=\\mathbb{E}\\operatorname*{sup}_{\\varphi\\in{\\mathcal{F}}}\\left|\\frac{1}{N}\\sum_{i=1}^{N}\\sigma_{i}\\varphi(X_{i})\\right|}\\end{array}$ , $\\sigma_{i}$ are independent rademacher random variables and $X_{i}\\sim(r\\circ\\pi_{\\theta^{*}})_{\\sharp}\\mu_{+}$ iid $(X_{i}\\in\\mathbb{R})$ . and similarly we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}\\operatorname*{sup}_{\\varphi\\in\\mathcal{F}_{c}}\\int\\varphi^{c}(r\\circ\\pi_{\\theta^{*}})d(\\mu_{-}-\\hat{\\mu}_{-}^{m})\\,\\le\\mathbb{E}\\operatorname*{sup}_{\\varphi^{c}\\in\\mathcal{F}_{c}^{c}}\\int\\varphi^{c}(r\\circ\\pi_{\\theta^{*}})d(\\mu_{-}-\\hat{\\mu}_{-}^{m})\\le2\\mathcal{R}_{m}(\\mathcal{F}_{c}^{c})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We have finally: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\varepsilon(\\hat{\\theta}_{m,n},\\hat{\\mu}_{+}^{n},\\hat{\\mu}_{-}^{m})\\leq\\varepsilon(\\theta^{*},\\mu_{+}.\\mu_{-})+2\\mathcal{R}_{n}(\\mathcal{F}_{c})+2\\mathcal{R}_{m}(\\mathcal{F}_{c}^{c})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Turning now to : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\varepsilon(\\widehat{\\theta}_{m,n},\\mu_{+},\\mu_{-})=\\displaystyle\\operatorname*{sup}_{\\varphi\\in\\mathcal{F}_{\\epsilon}}\\int\\varphi(r\\circ\\pi_{\\widehat{\\theta}_{m,n}})d\\mu_{+}-\\int\\varphi^{c}(r\\circ\\pi_{\\widehat{\\theta}_{m,n}})d\\mu_{-}}&{}\\\\ {\\leq\\varepsilon(\\widehat{\\theta}_{m,n},\\widehat{\\mu}_{+}^{n},\\widehat{\\mu}_{-}^{m})+\\displaystyle\\operatorname*{sup}_{\\varphi\\in\\mathcal{F}_{\\epsilon}}\\int\\varphi(r\\circ\\pi_{\\widehat{\\theta}_{m,n}})d(\\mu_{+}-\\mu_{+}^{n})}&{}\\\\ {+\\displaystyle\\operatorname*{sup}_{\\varphi\\in\\mathcal{F}_{\\epsilon}}\\int\\varphi^{c}(r\\circ\\pi_{\\widehat{\\theta}_{m,n}})d(\\mu_{-}^{m}-\\mu_{-})}&{}\\\\ {\\leq\\varepsilon(\\widehat{\\theta}_{m,n},\\widehat{\\mu}_{+}^{n},\\widehat{\\mu}_{-}^{m})+\\displaystyle\\operatorname*{sup}_{\\pi\\in\\mathcal{H}\\mathrm{~}\\varphi\\in\\mathcal{F}_{\\epsilon}}\\int\\varphi(r\\circ\\pi_{\\theta})d(\\mu_{+}-\\mu_{+}^{n})}&{}\\\\ {+\\displaystyle\\operatorname*{sup}_{\\pi\\in\\mathcal{H}\\mathrm{~}\\varphi\\in\\mathcal{F}_{\\epsilon}}\\int\\varphi^{c}(r\\circ\\pi_{\\theta})d(\\mu_{-}^{m}-\\mu_{-})}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Taking expectations we obtain: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\varepsilon(\\widehat{\\theta}_{m,n},\\mu_{+},\\mu_{-})\\leq\\mathbb{E}\\varepsilon(\\widehat{\\theta}_{m,n},\\widehat{\\mu}_{+}^{n},\\widehat{\\mu}_{-}^{m})+\\mathbb{E}\\underset{\\pi_{\\theta}\\in\\mathcal{H}}{\\operatorname*{sup}}\\underset{\\varphi\\in\\mathcal{F}_{c}}{\\operatorname*{sup}}\\int\\varphi(r\\circ\\pi_{\\theta})d(\\mu_{+}-\\mu_{+}^{n})}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ +\\mathbb{E}\\underset{\\pi_{\\theta}\\in\\mathcal{H}}{\\operatorname*{sup}}\\underset{\\varphi\\in\\mathcal{E}(\\mathcal{F}_{c})^{c}}{\\operatorname*{sup}}\\int\\varphi^{c}(r\\circ\\pi_{\\theta})d(\\mu_{-}^{m}-\\mu_{-})}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\leq\\underset{\\mathrm{optimal}}{\\leq}\\underset{\\mathrm{optimal}}{\\leq}\\mu_{+}...\\mu_{-}\\Big)\\ \\ \\ \\ \\ \\ \\ \\ \\underset{\\mathrm{one~dimensional}}{\\leq}\\underset{\\mathrm{or~complexity~with~optimal}}{\\leq}\\theta^{\\gamma}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathrm{optima}\\mathbb{F}\\mathrm{so}\\ r\\circ\\mathcal{H})+2\\mathcal{R}_{m}(\\mathcal{F}_{c}^{c}\\circ r\\circ\\mathcal{H}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{R}_{n}(\\mathcal{F}_{c}\\circ r\\circ\\mathcal{H})=\\frac{1}{n}\\mathbb{E}\\operatorname*{sup}_{\\pi_{\\theta}\\in\\mathcal{H}}\\operatorname*{sup}_{\\varphi\\in\\mathcal{F}_{c}}\\left|\\sum_{i=1}^{n}\\sigma_{i}\\varphi(r\\circ\\pi_{\\theta}(x_{i}^{+},y_{i}^{+}))\\right|\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "D Bounding Rademacher Complexities ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof of Corollary $^{\\,l}$ . Define the uniform metric entropy of a class of real valued functions $\\mathcal{F}$ on a set $\\mathcal{X}$ as the logarithm of the covering number with respect to the uniform norm $\\|.\\|_{\\infty}$ , for $\\varepsilon>0$ , this is defined as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\nN(\\varepsilon,\\mathcal{F},\\|,\\|_{\\infty}):=\\operatorname*{inf}\\left\\{n\\in\\mathbb{N}\\Big|\\mathrm{~there~exists~}f_{1}\\ldots f_{n}:\\mathcal{X}\\to\\mathbb{R}\\mathrm{~with~}\\operatorname*{sup}_{f\\in\\mathcal{F}}\\operatorname*{min}_{1\\le i\\le n}\\|f-f_{i}\\|_{\\infty}\\le\\varepsilon\\right\\}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As observed in [Hundrieser et al., 2022] the $\\mathrm{\\Phi_{c}}$ -transformation with bounded cost is a lipchitz operation under the uniform norm and since $f^{c c}=f$ we have by Lemma 2.1 in Munk : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{N}(\\varepsilon,\\mathcal{F}_{c}^{c},\\|.\\|_{\\infty})=\\mathcal{N}(\\varepsilon,\\mathcal{F}_{c},\\|.\\|_{\\infty})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now turning to the Rademacher complexity of a class $\\mathcal{F}$ , it is dominated by Dudley\u2019s entropy integral (Theorem 16 in Luxburg and Bousquet ): ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{R}_{n}(\\mathcal{F})\\leq\\operatorname*{inf}_{\\delta\\in[0,R]}\\left(2\\delta+\\sqrt{32}\\frac{1}{\\sqrt{n}}\\int_{\\delta/4}^{R}\\sqrt{\\log\\mathcal{N}(\\varepsilon,\\mathcal{F},\\|.\\|_{\\infty})}d\\varepsilon\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that the cost we are using is $c(z,z^{\\prime})=h(z-z^{\\prime})$ , for $z,z\\in[-M,M]$ . The domain on which the cost being a closed interval is convex and compact. By lipchitzity of $h$ (Assumption 1) and denoting $L$ its lipchitz constant, $c(\\u,z^{\\prime})$ is lipchitz for all $z^{\\prime}\\in[-M,M]$ . Equivalently $c(,z^{\\prime})$ is $(\\alpha,\\Lambda)$ H\u00f6lder smooth , for $\\alpha=1$ and $\\Lambda=L$ , and hence our setup falls under the Assumptions of Theorem 3.11 in [Hundrieser et al., 2022] for Holder smooth costs defined on convex and compact sets and we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\log\\mathcal{N}(\\varepsilon,\\mathcal{F}_{c},\\|.\\|_{\\infty})\\lesssim\\varepsilon^{-\\frac{d}{\\alpha}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Hence in our case $\\alpha=1$ and $d=1$ this leads to ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\log\\mathcal{N}(\\varepsilon,\\mathcal{F}_{c},\\|.\\|_{\\infty})\\lesssim\\varepsilon^{-1}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Replacing this in Equation (19) we obtain: $\\mathcal{R}_{n}(\\mathcal{F}_{c})\\lesssim n^{-\\frac{1}{2}}$ and by Equation (18) it follows that: $\\mathcal{R}_{m}(\\mathcal{F}_{c}^{c})\\lesssim m^{-\\frac{1}{2}}$ . ", "page_idx": 16}, {"type": "text", "text": "Turning now to the Rademacher Complexity of the composition of the c-concave potentials $\\mathcal{F}_{c}$ with $r\\circ\\mathcal{H}$ (the composition of a fixed reward function with the hypothesis class $\\mathcal{H}$ ). Note since the cost $c(.,z^{\\prime})$ is $L$ Lipchitiz for all $z^{\\prime}\\in[-M,M]$ we have $\\mathcal{F}_{c}$ is included in the set of $L$ lipchitz function that are bounded by $R$ (See [Hundrieser et al., 2022] Lemma A.2 ). For $\\varphi\\in\\mathcal{F}_{c}$ and $\\pi_{\\theta}\\in\\mathcal{H}$ , let us note $h_{\\varphi,\\pi_{\\theta}}=\\varphi(r\\circ\\pi_{\\theta})$ we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\big\\|h_{\\varphi,\\pi_{\\theta}}-h_{\\varphi^{\\prime},\\pi_{\\theta^{\\prime}}}\\big\\|_{\\infty}=\\operatorname*{sup}_{x\\in\\mathcal{X},y\\in\\mathcal{Y}}\\big|\\varphi(r\\circ\\pi_{\\theta}(y|x))-\\varphi^{\\prime}(r\\circ\\pi_{\\theta^{\\prime}}(y|x))\\big|\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\varphi(r\\circ\\pi_{\\theta}(x))-\\varphi^{\\prime}(r\\circ\\pi_{\\theta^{\\prime}})|=|\\varphi(r\\circ\\pi_{\\theta}(x))-\\varphi(r\\circ\\pi_{\\theta^{\\prime}})+\\varphi(r\\circ\\pi_{\\theta^{\\prime}})-\\varphi^{\\prime}(r\\circ\\pi_{\\theta^{\\prime}})|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq L\\left\\|r\\circ\\pi_{\\theta}-r\\circ\\pi_{\\theta^{\\prime}}\\right\\|_{\\infty}+\\left\\|\\varphi-\\varphi^{\\prime}\\right\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we used lipchitzity of $\\varphi\\in\\mathcal{F}_{c}$ and hence we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|h_{\\varphi,\\pi_{\\theta}}-h_{\\varphi^{\\prime},\\pi_{\\theta^{\\prime}}}\\right\\|_{\\infty}\\le L\\left\\|r\\circ\\pi_{\\theta}-r\\circ\\pi_{\\theta^{\\prime}}\\right\\|_{\\infty}+\\left\\|\\varphi-\\varphi^{\\prime}\\right\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We have therefore the following bound on the covering number of the composition: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{N}(\\varepsilon,\\mathcal{F}_{c}\\circ r\\circ\\mathcal{H},\\|.\\|_{\\infty})\\leq\\mathcal{N}\\left(\\frac{\\varepsilon}{2},\\mathcal{F}_{c},\\|.\\|_{\\infty}\\right)\\mathcal{N}\\left(\\frac{\\varepsilon}{2L},r\\circ\\mathcal{H},\\|.\\|_{\\infty}\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Plugging this in Equation (19) we obtain: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathfrak{I}_{n}(\\mathcal{F}_{c}\\circ r\\circ\\mathcal{H})\\leq\\operatorname*{inf}_{\\delta\\in[0,R]}\\left(2\\delta+\\sqrt{32}\\frac{1}{\\sqrt{n}}\\int_{\\delta/4}^{R}\\sqrt{\\log\\mathcal{N}(\\varepsilon/2,\\mathcal{F}_{c},\\Vert.\\Vert_{\\infty})+\\log N\\left(\\frac{\\varepsilon}{2L},r\\circ\\mathcal{H},\\Vert.\\Vert_{\\infty}\\right)}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that for $a,b>0$ we have ${\\sqrt{a+b}}\\leq{\\sqrt{a}}+{\\sqrt{b}}$ , hence we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathfrak{I}_{n}(\\mathcal{F}_{c}\\circ r\\circ\\mathcal{H})\\leq\\operatorname*{inf}_{\\delta\\in[0,R]}\\left(2\\delta+\\sqrt{32}\\frac{1}{\\sqrt{n}}\\int_{\\delta/4}^{R}\\sqrt{\\log\\mathcal{N}(\\varepsilon/2,\\mathcal{F}_{c},\\Vert.\\Vert_{\\infty})}+\\sqrt{\\log\\mathcal{N}\\left(\\frac{\\varepsilon}{2L},r\\circ\\mathcal{H},\\Vert.\\Vert_{\\infty}\\right)}\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We know by lipchitizty of the cost and being in one dimension that : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\log\\mathcal{N}(\\varepsilon,\\mathcal{F}_{c},\\|.\\|_{\\infty})\\lesssim\\varepsilon^{-1}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By lipchitizity of $r\\circ\\pi_{\\theta}$ and using Assumption 3 we have therefore: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\log\\mathcal{N}\\left(\\frac{\\varepsilon}{2L},r\\circ\\mathcal{H},\\|.\\|_{\\infty}\\right)\\leq\\log\\mathcal{N}\\left(\\frac{\\varepsilon}{2L L^{\\prime}},B_{2}(r_{0},d_{\\theta}),\\|.\\|_{\\infty}\\right)\\leq d_{\\theta}\\log\\frac{2r_{0}L^{\\prime}L}{\\varepsilon}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We have therefore: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\delta\\in[0,R]}2\\delta+4\\frac{\\sqrt{2}}{\\sqrt{n}}\\int_{\\delta/4}^{R}K_{1}\\left(\\frac{\\varepsilon}{2}\\right)^{-1/2}d\\varepsilon+4\\frac{\\sqrt{2}}{\\sqrt{n}}\\int_{\\delta/4}^{R}\\sqrt{d_{\\theta}\\log\\frac{2r_{0}L L^{\\prime}}{\\varepsilon}}d\\varepsilon\\lesssim n^{-\\frac{1}{2}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "(For $\\delta=0$ , the upper bound is obtained.) ", "page_idx": 16}, {"type": "text", "text": "For 2) By assumption 4, there exists $\\pi_{\\theta^{\\ast}}$ , such that we have for $\\begin{array}{r l r}{h}&{{}=}&{(-x)_{+}^{2}}\\end{array}$ : ${\\mathsf{O T}}_{h}\\left((r\\circ\\pi_{\\theta^{*}})_{\\sharp}\\mu_{+},(r\\circ\\pi_{\\theta^{*}})_{\\sharp}\\mu_{-}\\right)=0$ . ", "page_idx": 16}, {"type": "text", "text": "E AOT paired ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Similarly following the relaxation approach described in Section 3 and using the dual representation of the OT problem we can relax the paired stochastic dominance constraint problem given in (FSD paired) to: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi_{\\theta}\\in\\mathcal{H}}\\operatorname*{sup}_{\\varphi\\in\\mathcal{F}_{c}}\\int\\varphi(r\\circ\\pi_{\\theta})d\\mu-\\int\\varphi^{c}(r\\circ\\pi_{\\mathrm{ref}})d\\mu,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mu$ is the paired measure representing $(X,Y_{+},Y_{-})$ . let $\\pi_{\\theta^{*}}$ be the minimizer of (20). Considering Problem (20), with empirical samples $\\begin{array}{r}{\\hat{\\mu}^{n}=\\frac{1}{n}\\sum_{i=1}^{n}\\delta_{(x_{i},y_{i,+},y_{i,-})}}\\end{array}$ , denote $\\pi_{\\hat{\\theta}_{n}}$ its minimizer we have : ", "page_idx": 16}, {"type": "text", "text": "Theorem 3 (Sample Complexity of Dominance Violation for AOT Paired). The following sample complexity bound for the violation of stochastic dominance in AOT paired holds: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r l}{\\mathbb{E}\\,{\\mathsf{O T}}_{h}\\left((r\\circ\\pi_{\\hat{\\theta}_{n}})_{\\sharp}\\mu,(r\\circ\\pi_{\\mathrm{ref}})_{\\sharp}\\mu\\right)\\leq\\underbrace{{\\mathsf{O T}}_{h}\\left((r\\circ\\pi_{\\theta^{*}})_{\\sharp}\\mu,(r\\circ\\pi_{\\mathrm{ref}})_{\\sharp}\\mu\\right)}_{{\\cal O p t i m a l}\\,A l m o s t\\,F S D\\,{\\cal W}\\,d a l t i o n}}&{}\\\\ {+\\underbrace{2\\mathcal{R}_{n}(\\mathcal{F}_{c};(r\\circ\\pi_{\\theta^{*}})_{\\sharp}\\mu_{+})+4\\mathcal{R}_{n}(\\mathcal{F}_{c}^{c};(r\\circ\\pi_{\\mathrm{ref}})_{\\sharp}\\mu)}_{O n e\\,d i m e n s i o n a l\\,O T\\,s a m p l e c o m p l e x i t y\\,w i t h\\,o p t i m a l\\,\\theta^{*}}}&{}\\\\ {+\\underbrace{2\\mathcal{R}_{n}(\\mathcal{F}_{c}\\circ r\\circ\\mathcal{H};\\mu)}_{\\mathcal{I}_{n}(\\mathcal{F}_{c}\\circ r\\circ\\mathcal{H};\\mu))},}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "$\\begin{array}{r}{\\mathcal{R}_{n}(\\mathcal{F};\\nu)=\\mathbb{E}\\operatorname*{sup}_{\\varphi\\in\\mathcal{F}}\\left|\\frac{1}{n}\\sum_{i=1}^{n}\\sigma_{i}\\varphi(Z_{i})\\right|}\\end{array}$ is the Rademacher Complexity and for $i=1\\ldots n_{!}$ $\\sigma_{i}$ are independent rademacher random variables and $Z_{i}\\sim\\nu$ iid. ", "page_idx": 17}, {"type": "text", "text": "Similarly under Assumptions $1,2$ and 3 we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\,0\\mathsf{T}_{h}\\left((r\\circ\\pi_{\\hat{\\theta}_{n}})_{\\sharp}\\mu,(r\\circ\\pi_{\\mathrm{ref}})_{\\sharp}\\mu\\right)-0\\mathsf{T}_{h}\\left((r\\circ\\pi_{\\theta^{\\bullet}})_{\\sharp}\\mu,(r\\circ\\pi_{\\mathrm{ref}})_{\\sharp}\\mu\\right)\\lesssim n^{-\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Theorem 3. The proof can be simply obtained by inspecting the proof of Theorem 1, and we omit it. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Remark 2. Although ${0}{\\mathsf{T}}_{h}$ is one dimensional, an entropic regularization of ${0}{\\mathsf{T}}_{h}$ has computational advantages as discussed in Section 3. Results from [Groppe and Hundrieser, 2023] can be leveraged to obtain sample complexity bounds and we obtain under our assumptions also a parametric rate of $n^{-{\\frac{1}{2}}}$ . The main insight in [Groppe and Hundrieser, 2023] is in introducing for an entropic regularization parameter $\\varepsilon>0$ , the smoothed $(c,\\varepsilon,\\mu)$ transform and replacing the spaces $\\mathcal{F}_{c}$ by $\\mathcal{F}_{c,\\varepsilon}$ . In the $I D$ case, up to constants, these spaces have the same covering numbers. ", "page_idx": 17}, {"type": "text", "text": "F Choice of violation penalty function $h$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Recall we proposed the following three classes of loss functions $h$ in the main text: ", "page_idx": 17}, {"type": "text", "text": "1. Area of violation (\u201cclassification\u201d) Setting $h(x)$ to be the 0-1 loss $\\left(\\mathbb{1}_{x<0}\\right)$ measures the fraction of the interval [0, 1] where a violation occurs, paralleling classification losses which count the number of misclassification.   \n2. Wasserstein-1 violation Setting $h(x)$ to be the hinge loss $(-x)_{+}$ reduces to measuring the Wasserstein-1 distance from $U_{\\theta}$ to the nearest distribution that has FSD over $V_{\\theta}$ .   \n3. Wasserstein-2 violation Setting $h(x)$ to be the squared hinge loss $(-x)_{+}^{2}$ reduces to measuring the Wasserstein-2 distance from $U_{\\theta}$ to the nearest distribution that has FSD over $V_{\\theta}$ . ", "page_idx": 17}, {"type": "text", "text": "Besides measuring different quantities, the optimization-theoretic properties of each are different: ", "page_idx": 17}, {"type": "text", "text": "1. The 0-1 loss This loss does not penalize the size of the violations, making gradient-based optimization difficult as large violations have no gradient. Additionally, if FSD were not achievable (e.g. a strong teacher policy), not penalizing the size of the violations could result in risky policies. ", "page_idx": 17}, {"type": "text", "text": "2. The hinge loss, i.e. the Wasserstein-1 violation measure. By its nature, the gradient of small violations is just as large as the gradient of large violations, which may be beneficial for convergence to an FSD result. When FSD is impossible to achieve, this loss will also have the effect of encouraging sparse violations while still penalizing the size of the violations, similar to the L1 norm in the classic Lasso algorithm. Smooth relaxations Smooth relaxations of the hinge, e.g., the logistic loss described in the main text, have a nonzero gradient at zero and continue to have a gradient for small positive values. This has the benefit of encouraging quantiles to continue improving after surpassing those of the reference. ", "page_idx": 17}, {"type": "text", "text": "3. The squared hinge loss, i.e. the Wasserstein-2 violation measure. As a quadratic loss, it no longer prefers sparse violations (c.f. L2 norm regularization). Indeed, the gradient signal vanishes as the violation becomes small, meaning that dense violations are likely and slow to be removed. A potential failure mode would be that the new policy has a small violation, but none of the quantiles outperform the baseline. Relaxation Introducing a bias $\\beta$ as in the main text addresses this issue, ensuring the gradient at 0 is nonzero. ", "page_idx": 18}, {"type": "text", "text": "G Gradients of the sorting-based objective (11) ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We here repeat the sorting-based objective (11) that we propose, adding $n$ as a superscript for clarity. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\Theta}\\mathsf{O T}_{h}(\\hat{\\mu}_{U_{\\theta}}^{(n)},\\hat{\\mu}_{V_{\\theta}}^{(n)})=\\operatorname*{min}_{\\theta\\in\\Theta}\\frac{1}{n}\\sum_{i=1}^{n}h(u_{\\theta}^{(i)}-v_{\\theta}^{(i)}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We use gradient-based optimization approaches in practice to find a minimizing $\\theta$ . We have the following theorem showing the gradients are asymptotically unbiased (and thus friendly to stochastic gradient methods). ", "page_idx": 18}, {"type": "text", "text": "Theorem 4. Let $h^{\\prime}=d h/d t$ be $L$ -lipschitz and the gradients of $u_{\\theta}$ and v\u03b8 with respect to $\\theta$ have $^{\\,l}$ -norm bounded by $M$ , i.e. $\\|\\nabla_{\\theta}u_{\\theta}\\|_{1}\\leq M,\\|\\nabla_{\\theta}v_{\\theta}\\|_{1}\\leq M$ for all $\\theta\\in\\Theta$ . Suppose further that the support of $U_{\\theta}$ , $V_{\\theta}$ are bounded and their distributions have densities (i.e. are atomless).3Then the gradient of the objective in (21) is asymptotically unbiased as $n\\to\\infty$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. Observe that the gradients take the form ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta}0\\mathsf{T}_{h}(\\hat{\\mu}_{U_{\\theta}}^{(n)},\\hat{\\mu}_{V_{\\theta}}^{(n)})=\\nabla_{\\theta}\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}h(u_{\\theta}^{(i)}-v_{\\theta}^{(i)})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}h^{\\prime}(u_{\\theta}^{(i)}-v_{\\theta}^{(i)})\\nabla_{\\theta}(u_{\\theta}^{(i)}-v_{\\theta}^{(i)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that as described in the main text, here, the current gradient is determined by the current state of the sorting of the samples, but it is not necessary to differentiate through the sorting algorithm itself as it changes only discretely. ", "page_idx": 18}, {"type": "text", "text": "We wish to understand the convergence of the bias of these gradients as $n\\to\\infty$ . ", "page_idx": 18}, {"type": "text", "text": "We can write (noting that the $n$ atoms of the empirical $F_{n,V_{\\theta}}$ are distinct with probability 1) ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}h^{\\prime}(u_{\\theta}^{(i)}-v_{\\theta}^{(i)})\\nabla_{\\theta}(u_{\\theta}^{(i)}-v_{\\theta}^{(i)})=\\underbrace{\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}h^{\\prime}(u_{\\theta}^{(i)}-F_{n,V_{\\theta}}^{-1}(F_{n,U_{\\theta}}(u_{\\theta}^{(i)})))\\nabla_{\\theta}(u_{\\theta}^{(i)})}_{I}}\\\\ &{\\phantom{\\underbrace{-\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}h^{\\prime}(F_{n,U_{\\theta}}^{-1}(F_{n,V_{\\theta}}(v_{\\theta}^{(i)}))-v_{\\theta}^{(i)})\\nabla_{\\theta}(v_{\\theta}^{(i)})}_{I I}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let\u2019s consider the first term, the analysis for the second term is the same. Note that since $h^{\\prime}$ is $L$ -Lipschitz ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left|h^{\\prime}\\left(u-F_{n,V_{\\theta}}^{-1}(F_{n,U_{\\theta}}(u))\\right)-h^{\\prime}\\left(u-F_{V_{\\theta}}^{-1}(F_{U_{\\theta}}(u))\\right)\\right|\\le L\\left|F_{n,V_{\\theta}}^{-1}(F_{n,U_{\\theta}}(u))-F_{V_{\\theta}}^{-1}(F_{U_{\\theta}}(u))\\right|.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let ", "page_idx": 18}, {"type": "equation", "text": "$$\nI^{\\prime}=\\frac{1}{n}\\sum_{i=1}^{n}h^{\\prime}(u_{\\theta}^{(i)}-F_{V_{\\theta}}^{-1}(F_{U_{\\theta}}(u_{\\theta}^{(i)})))\\nabla_{\\theta}(u_{\\theta}^{(i)}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "3In our alignment setting, this is should not be an issue as our scores are real-valued. ", "page_idx": 18}, {"type": "text", "text": "Note $I^{\\prime}$ is a simple empirical average and hence unbiased in the sense that $\\mathbb{E}[I^{\\prime}]$ is constant with $n$ . Now by (24) we can write ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|I-I^{\\prime}\\|_{1}\\leq\\frac{L}{n}\\sum_{i=1}^{n}\\left|F_{n,V_{\\theta}}^{-1}(F_{n,U_{\\theta}}(u_{\\theta}^{(i)}))-F_{V_{\\theta}}^{-1}(F_{U_{\\theta}}(u_{\\theta}^{(i)}))\\right|\\|\\nabla_{\\theta}(u_{\\theta}^{(i)})\\|_{1}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We seek to bound this quantity as $n\\to\\infty$ . By Cauchy-Schwarz, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|I-I^{\\prime}\\|_{1}^{2}\\leq L^{2}\\left(\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\left(F_{n,V_{\\theta}}^{-1}(F_{n,U_{\\theta}}(u_{\\theta}^{(i)}))-F_{V_{\\theta}}^{-1}(F_{U_{\\theta}}(u_{\\theta}^{(i)}))\\right)^{2}\\right)\\left(\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\|\\nabla_{\\theta}(u_{\\theta}^{(i)})\\|_{1}^{2}\\right)}\\\\ &{\\qquad\\leq\\displaystyle\\frac{L^{2}M^{2}}{n}\\sum_{i=1}^{n}\\left(F_{n,V_{\\theta}}^{-1}(F_{n,U_{\\theta}}(u_{\\theta}^{(i)}))-F_{V_{\\theta}}^{-1}(F_{U_{\\theta}}(u_{\\theta}^{(i)}))\\right)^{2}}\\\\ &{\\qquad=\\displaystyle\\frac{L^{2}M^{2}}{n}\\sum_{i=1}^{n}\\left(v_{\\theta}^{(i)}-F_{V_{\\theta}}^{-1}(F_{U_{\\theta}}(u_{\\theta}^{(i)}))\\right)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we\u2019ve assumed that $\\begin{array}{r}{\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\|\\nabla_{\\theta}(u_{\\theta}^{(i)})\\|_{1}^{2}\\right)\\leq M^{2}}\\end{array}$ . Observe that $F_{V_{\\theta}}^{-1}(F_{U_{\\theta}}(\\cdot))$ is the optimal transport plan from $U_{\\theta}$ to ${\\mathit{V}}_{\\theta}$ and is monotonic nondecreasing, hence $F_{V_{\\theta}}^{-1}(F_{U_{\\theta}}(u_{\\theta}^{(i)}))$ are simply a new set of independently drawn order statistics of $v_{\\theta}$ , i.e. $v_{\\theta}^{(i,2)}$ ", "page_idx": 19}, {"type": "text", "text": "We thus have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|I-I^{\\prime}\\|_{1}^{2}\\sim\\frac{L^{2}M^{2}}{n}\\sum_{i=1}^{n}\\left(v_{\\theta}^{(i)}-v_{\\theta}^{(i,2)}\\right)^{2}}\\\\ {\\displaystyle=L^{2}M^{2}\\int\\left([F_{n,V_{\\theta}}^{(1)}]^{-1}(t)-[F_{n,V_{\\theta}}^{(2)}]^{-1}(t)\\right)^{2}d t}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where F n(i,)V are independently realized empirical quantile functions. ", "page_idx": 19}, {"type": "text", "text": "Theorem 3.1 of Del Barrio et al. [2018] with the subsequent Remark 3.2.1 therein applies directly to this regime, with the following restated result: ", "page_idx": 19}, {"type": "text", "text": "Theorem 5 (Special case of Theorem 3.1 in light of Remark 3.2.1 in Del Barrio et al. [2018]). If $F$ and $G$ are CDFs of $^{\\,l}$ -dimensional distributions with bounded support and $G^{-1}$ is continuous on $(0,1)$ , then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int_{0}^{1}\\left(F_{n}^{-1}-G_{m}^{-1}\\right)^{2}-\\int_{0}^{1}\\left(F^{-1}-G^{-1}\\right)^{2}\\rightarrow_{p}0\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "as $n,m\\rightarrow\\infty$ . ", "page_idx": 19}, {"type": "text", "text": "Applying Theorem 5 to our setting and noting that for us the second integral is zero, we have that if $V_{\\theta}$ has bounded support, ", "page_idx": 19}, {"type": "equation", "text": "$$\nL^{2}M^{2}\\int\\left([F_{n,V_{\\theta}}^{(1)}]^{-1}(t)-[F_{n,V_{\\theta}}^{(2)}]^{-1}(t)\\right)^{2}d t\\rightarrow_{p}0\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\to_{p}$ indicates convergence in probability as $n\\to\\infty$ . This implies that $I-I^{\\prime}$ converges to 0 in probability. The proof for $I I$ is identical. Since the gradient (23) is then a sum of two unbiased terms and two terms that converge in probability to zero, the gradient is asymptotically unbiased. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "H Additional Experiments ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Tables 2, 3, 4, 5 and 6 are ablations on the reference base model used (Merlinite-7B, OpenHermes-2.5-Mistral-7B, Sarling-LM-7B-alpha, Meta-LLama-3-8B-Instruct, Mistral-7B-Instruct-v0.2). In these tables, we report only the AlpacaEval using Llama3-70B as a judge to reduce the costs of evaluations. Note that we observed that while the absolute scoring of Llama3-70B is different than GPT4, as it can be seen in Table 2, it preserves the rankings of the models. We see across all these models a better performance of the distributional alignment AOT on AlpacaEval and a competitive performance on other benchmarks. ", "page_idx": 19}, {"type": "table", "img_path": "2LctgfN6Ty/tmp/3c003efbdbc714221039315fcdd6c765b33911859e4766318b5ec0e4f902a53a.jpg", "table_caption": [], "table_footnote": ["Table 2: Merlinite-7B trained on UltraFeedback Binarized. Here we present full version of the results, including AlpacaEval using Llama3-70B-instruct as a judge and GPT4 as a judge. The comparison reveals that although Llama3 inflates the scores, the relative order between the two judges remains the same, suggesting the use of a cheaper AlpacaEval alternative for local development. "], "page_idx": 20}, {"type": "table", "img_path": "2LctgfN6Ty/tmp/b990e65eb871841d2bbc17317d366ae8c61d117ba43a0e520b6bbaa1827e4dc7.jpg", "table_caption": [], "table_footnote": ["Table 3: OpenHermes-2.5-Mistral-7B trained on UltraFeedback Binarized "], "page_idx": 20}, {"type": "table", "img_path": "2LctgfN6Ty/tmp/a154c7d10f274ffcaae5466ec38d4fbdca0c8bbbeff3457d2d14d799199b6bce.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "2LctgfN6Ty/tmp/2b96f9a99da79991e83db366337f0fb1a01b628d84968621dc584bd2f87aba89.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "2LctgfN6Ty/tmp/0f040c5de7023bf624939c2bdf8d367c43a8c3d758dd4954664929d21783e507.jpg", "table_caption": ["Table 5: Meta-Llama-3-8B-Instruct trained on UltraFeedback Binarized "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "2LctgfN6Ty/tmp/b6323dd560b2d4f6641f044969c4dd61eaeb66aff37b278adbdc3e01e839540e.jpg", "table_caption": ["Table 7 is an ablation on the sorting that is used in AOT with Merlinite-7B as a reference model. We see that hard and soft sorting are on par in terms of overall performance. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Tables 8 and 9 give a comparison between AOT and KTO on unpaired datasets (HelpSteer and PKU binarized) we see that overall AOT leads to a better performance than KTO. ", "page_idx": 21}, {"type": "table", "img_path": "2LctgfN6Ty/tmp/9b01a8b0e3982e0c67a6d5706a3f56c6be54fc8c4d8f3a97c996d38e265e633f.jpg", "table_caption": ["Table 7: The effect of sort type on performance in AOT alignment "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "2LctgfN6Ty/tmp/cf31720b61312c8d6f74ba7498b3653e7abd0e7e112be27e5547bc54f0a3d35a.jpg", "table_caption": ["Table 8: Merlinite-7B trained on unpaired HelpSteer (binarized) "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Finally Figure 4 puts in context our best model Merlinite-7B-uAOT as the best 7B-family model on AlpacaEval leaderboard at the time of writing this paper. Finally, we give in Table 10 the variance of the evaluation across 4 different random seeds for training and evaluation each alignment strategy, we see very small variance in AOT, especially the unpaired variant. ", "page_idx": 21}, {"type": "table", "img_path": "2LctgfN6Ty/tmp/1da20353b674292ceef4fed9df99ed094bebf6a459f81d48f2e34837d6f27450.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 4: Our AOT algorithm gives a strong boost to Merlinite-7B model on AlpacaEval leaderboard (as of May 22nd, 2024). The original Merlinite-7B score is 17.1, and after the alignment, the model gained $83\\%$ . ", "page_idx": 22}, {"type": "table", "img_path": "2LctgfN6Ty/tmp/6790e9abd9bae1d0d4a1bb8bf671199e037cb79c2a079746a536ccef306da18d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 10: Merlinite-7B trained on UltraFeedback Binarized. We evaluated the stability (variance) of the model evaluation on AlpacaEval by running 4 separate training and evaluation cycles, then computing the mean and standard deviations. The results are stable, especially for AOT unpaired, showing a low deviation from the mean. ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Our main claims in the abstract and introduction summarize our methodological contributions and results, which are properly expanded upon in the rest of the paper. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The first section in the appendix of the paper identifies the main limitations of our work. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitations, while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when the image resolution is low, or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed not to penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: The Appendix includes proofs of all stated Theorems. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: All information needed to reproduce our results are clearly stated in the paper.   \nWe will also release code to reproduce the main results upon acceptance of the paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We will release the code to reproduce our main experimental results under open source upon acceptance. During the review process the code is shared in supplementary material. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: All details necessary details to reproduce our experimental results are provided in the paper. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We report error bars for the main results on AlpacaEval using LLaMA3-70B as a judge (GPT4-based evaluations would incur significant cost for this type of experiments). As indicated, error bars are computed as a variance across 4 runs. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We mentioned the hardware resources utilized for our experiments in the relevant sections of the paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our research was conducted in full conformity of the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We discuss societal impacts of our work in the context of improving on current RLHF techniques. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 27}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Creators of original work are properly cited. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Documented code for our proposed alignment algorithm will be provided upon acceptance of the paper. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}]