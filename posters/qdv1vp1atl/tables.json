[{"figure_path": "qdV1vp1AtL/tables/tables_2_1.jpg", "caption": "Table 1: Rate required to communicate a given/any sample from P", "description": "This table summarizes the minimum number of bits required to communicate a sample from a probability distribution P to a remote decoder, under two scenarios: matched and mismatched source coding and sample communication. In matched source coding, the encoder and decoder share prior knowledge of distribution P, enabling optimal compression at the entropy H(P).  In mismatched source coding, the decoder uses a different distribution Q, increasing the communication rate by the Kullback-Leibler (KL) divergence between P and Q. In sample communication, the goal is only for the receiver to obtain any sample from P, even if the specific sample from the encoder is different.  In the matched case, no bits are required since shared knowledge of P enables both to sample it locally, while in mismatched scenario, only the KL-divergence cost needs to be paid.", "section": "4 Communication of samples"}, {"figure_path": "qdV1vp1AtL/tables/tables_8_1.jpg", "caption": "Table 2: Accuracy and communication cost of FedPM for different simulation scenarios. Values are averaged over 20 runs, with standard deviation bellow 0.003.", "description": "This table presents the results of applying four different Federated Probabilistic Mask Training (FedPM) schemes. The first scheme (FedPM) uses 10 clients per round and communicates 1 sample per client, achieving 80.25% test accuracy with 0.6058 bits/parameter. The second scheme (FedPM) uses only 1 client per round and 1 sample per client, resulting in lower accuracy (75.16%) but significantly less communication (0.0340 bits/parameter). The third scheme (FedPM) increases the number of samples per client to 7, while maintaining 1 client per round, leading to an accuracy of 80.28% and communication cost of 0.3955 bits/parameter. Finally, the fourth scheme (FedPM w. USC) incorporates the proposed universal sample coding (USC), using 7 samples per client and achieving the highest test accuracy (80.39%) with the lowest communication cost (0.2482 bits/parameter).", "section": "Federated learning (FL)"}, {"figure_path": "qdV1vp1AtL/tables/tables_8_2.jpg", "caption": "Table 3: Per token cost of sending samples from 13B model. Entropy is a lower bound for source coding, while the KL-divergence serves as a bound for sample communication.", "description": "This table presents the per-token communication cost when sending samples from a 13-billion parameter language model (13B).  It compares the cost of plain text transmission (15.617 bits/token), which is the entropy of the full distribution, to the costs achieved using different sizes of auxiliary models (Q) to perform sample communication. The KL-divergence between the 13B model and each auxiliary model is given, showing how much the communication cost can be reduced by using an appropriately sized auxiliary model in sample communication.", "section": "8 Limitations and further work"}]