[{"figure_path": "nzzxLPJENZ/figures/figures_1_1.jpg", "caption": "Figure 1: The agreement between human judgment and LLMs in Eval-P benchmark in out-of-distribution evaluation. Auto-J serves as the \"one-step\" evaluation baseline, while Fennec is the \"multi-step\" baseline. The Initial, SFT, and DPO models were trained using our generated data.", "description": "This figure displays the agreement rates between human judgments and different LLMs in the Eval-P benchmark, specifically focusing on out-of-distribution evaluation.  It compares three models (Initial, SFT, and DPO) trained using a novel data generation method against two baselines: Auto-J (a one-step evaluation method) and Fennec (a multi-step evaluation method). The x-axis represents the number of inference branches used, demonstrating how the agreement rate changes with the complexity of the evaluation process. The higher the agreement rate, the closer the model's judgment aligns with human evaluation. The figure shows that our proposed models outperform the baselines, particularly when increasing the number of inference branches.", "section": "1 Introduction"}, {"figure_path": "nzzxLPJENZ/figures/figures_3_1.jpg", "caption": "Figure 2: Compared to single-chain inference, we adopt a multi-branch based approach to train the Initial model. Subsequently, we construct an evaluation tree through a series of growth and pruning operations. This tree then guides the training both of the SFT model and the DPO model.", "description": "The figure illustrates the overall process of training the three models: Initial, SFT, and DPO.  It starts with a single-chain inference approach using the Initial model to generate an evaluation tree. This tree is then pruned and expanded through growth operations to create a structured dataset. This structured dataset is used to train the SFT model which focuses on branch prediction consistency. Finally, preference data from the evaluation tree is used to train the DPO model to improve the efficiency of finding crucial evaluation criteria.", "section": "4 Method"}, {"figure_path": "nzzxLPJENZ/figures/figures_4_1.jpg", "caption": "Figure 3: The figure illustrates how the training dataset of the SFT and DPO models is sampled from an evaluation subtree based on a specific criterion.", "description": "This figure shows the process of creating training datasets for the Supervised Fine-Tuning (SFT) model and the Direct Preference Optimization (DPO) model.  It starts with an evaluation tree, where each path represents a complete evaluation trajectory. The nodes represent evaluation judgments.  The dataset generation process uses two consistency checks: self-consistency (the same criterion and scoring guideline should yield the same judgment) and position-consistency (swapping response positions should not affect judgment). Samples satisfying both constraints are used for SFT training, while those violating either constraint are used for DPO training. The figure visually depicts how the training data is sampled from the evaluation tree based on the consistency and inconsistency of evaluation results.", "section": "4 Method"}, {"figure_path": "nzzxLPJENZ/figures/figures_6_1.jpg", "caption": "Figure 4: The scenario contains seven categories, including Summarization, Exam Questions, Rewriting, Code, Functional Writing, Creative Writing, General Communication, NLP Tasks, and Others.", "description": "This figure shows the distribution of different scenario categories in the training datasets (in-distribution, out-of-distribution) and the test dataset (Eval-P). Each panel represents a different dataset, showing the percentage of each scenario category.  The figure helps illustrate the diversity (or lack thereof) of scenarios present in each dataset and can be used to understand the model's generalization capabilities across different types of tasks.", "section": "5.4 Scenario analysis"}, {"figure_path": "nzzxLPJENZ/figures/figures_8_1.jpg", "caption": "Figure 5: The agreement and consistency rates of ID and OOD models with different branches.", "description": "This figure shows the performance of the In-distribution (ID) and Out-of-distribution (OOD) models in terms of agreement and consistency across different numbers of inference branches (1, 3, and 5).  Agreement refers to how well the model's judgments align with human judgments, while consistency measures the model's predictive stability when response positions are swapped.  The figure illustrates that the DPO model generally shows stronger performance and improved stability across a variety of scenarios and branching levels. This supports the paper's main finding that their tree-based method effectively enhances the evaluation process, reducing costs and improving efficiency.", "section": "5.7 Instability problem in in-distribution training"}, {"figure_path": "nzzxLPJENZ/figures/figures_13_1.jpg", "caption": "Figure 2: Compared to single-chain inference, we adopt a multi-branch based approach to train the Initial model. Subsequently, we construct an evaluation tree through a series of growth and pruning operations. This tree then guides the training both of the SFT model and the DPO model.", "description": "This figure illustrates the overall framework of the proposed method.  It starts with training an initial model using a multi-branch approach.  Then, an evaluation tree is constructed using a breadth-first search, and this tree is used to guide the subsequent training of the SFT (Supervised Fine-Tuning) model and the DPO (Direct Preference Optimization) model, which are designed to improve the model's multi-step reasoning capabilities and efficiency, respectively.  The figure visually summarizes the process of creating and using the evaluation tree to achieve a more effective evaluation of AI responses.", "section": "4 Method"}]