[{"type": "text", "text": "Efficient Evaluation of LLMs via Branching Preference Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Large language models (LLMs) have made significant advances across various   \n2 generative tasks, progressing toward achieving near-human levels of intelligence.   \n3 However, in many scenarios, LLMs face the challenge of insufficient human   \n4 evaluation or even the inability to evaluate reliably. Particularly, in complex   \n5 dialogue scenarios involving diverse and intricate user intents, LLMs as evaluators   \n6 of AI responses exhibit a substantial gap compared to humans. Moreover, due   \n7 to the scarcity of high-quality evaluation data, LLMs exhibit deficiencies in their   \n8 evaluation capabilities. In this work, we conceptualize the evaluation process as   \n9 a decision tree, where each node represents an evaluation action, and each path   \n10 from the root to a leaf node represents a trajectory of evaluation reasoning. We   \n11 demonstrate that within a limited search space, there exist better decision-making   \n12 behaviors that facilitate the model in making reasonable and accurate judgments.   \n13 Specifically, we propose a tree-based data sampling method to generate supervised   \n14 data and preference pairs derived from the evaluation tree. Furthermore, we   \n15 introduce preference learning based on the DPO algorithm, which empowers the   \n16 fine-grained evaluation model to explore and learn better branching strategies within   \n17 budget-limited scenarios. Our model significantly reduces the dependency on   \n18 labeled data and demonstrates strong performance across three different evaluation   \n19 settings: in-distribution, out-of-distribution, and transfer evaluation. Experiments   \n20 indicate that our model can reduce inference costs by $90\\%$ compared to conducting   \n21 searches across the entire evaluation tree, thereby significantly enhancing efficiency. ", "page_idx": 0}, {"type": "text", "text": "22 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "23 Dialogue evaluation capability [6] is one of the fundamental abilities of human social interaction,   \n24 involving the comprehension and interpretation of user intentions, as well as providing reasonable   \n25 judgments on the correctness of different responses. Automated evaluation can assist humans to   \n26 supervise powerful LLMs and is an essential component for superalignment and weak-to-strong   \n27 generalization techniques [4]. However, human evaluations [3, 22] are labor-intensive and time  \n28 consuming, making it difficult to widely adopt. Traditional automated evaluation approaches [18, 39,   \n29 8] are limited by inherent deficiencies, such as string and semantic matching methods often yield   \n30 subpar accuracy and lack of interpretability. The advent of large language models offers promise for   \n31 automatically evaluating dialogue quality [19, 41, 15], owing to their high consistency with humans   \n32 in intent understanding.   \n33 Nevertheless, automated evaluation remains a challenging issue due to the diversity of tasks and   \n34 scenarios it may encounter. The user queries often encompass multiple intentions [38], which cannot   \n35 typically be addressed using a single evaluation criterion. However, related research [35, 42] often   \n36 attempts to treat evaluation as a simplistic \u2019one-step\u2019 reasoning problem, causing even the most   \n37 powerful large language models to struggle to provide reasonable and accurate results. It is essential   \n38 for the evaluation model to adapt to different scenarios and provide critical evaluation criteria.   \n39 In this work, we do not introduce any human prior for evaluation scenarios and criteria,   \n40 which are commonly used for designing and collecting training data in related studies [14, 11].   \n41 The real-world conversational scenarios are often characterized by complexity and unpredictability,   \n42 making it challenging to derive generalizable rules. Additionally, human priors frequently introduce   \n43 biases [12, 20], making these evaluation methods poorly generalized due to a lack of adaptability   \n44 and scalability. Therefore, we explore automatically sampling scenarios from large-scale datasets   \n45 and employ LLMs to automatically generate evaluation criteria, aiming to eliminate human labor   \n46 as much as possible. Another significant challenge is the lack of ground truth labels and human   \n47 feedback during the training data collection process. The insufficient of available supervised data for   \n48 evaluation tasks also prevents them to scale effectively.   \n49 Despite various challenges, we discover that the evaluation model is constrained in its ability   \n50 to identify crucial evaluation criteria, but this limitation can be mitigated by increasing the   \n51 number of considered criteria. As shown in Figure 1, the Initial model can achieve nearly a   \n52 10 point improvement in the agreement metric by increasing the number of evaluation branches.   \n53 This findings motivate us to design tree-based data sampling methods to generate training data and a   \n54 branching preference learning algorithm to improve \u201cmulti-step\u201d inference capability. Specifically,   \n55 we employ a breadth-first growth approach to construct an evaluation tree, where each path from the   \n56 root to a leaf node represents a complete evaluation trajectory. We collect high-quality evaluation   \n57 trajectories from the search space of the evaluation tree and trained an SFT model, which exhibited   \n58 superior performance and prediction consistency. Furthermore, we refine these evaluation trajectories   \n59 and train a DPO model [24], which can effectively prioritize and output crucial evaluation criteria,   \n60 thereby enhancing the model\u2019s inference effectiveness.   \n61 We mainly evaluate our models in three settings: in-distribution, out-of-distribution, and transfer   \n62 evaluation. Specifically, we use the datasets from the Chatbot Arena 1 as in-distribution data, and   \n63 collect data from large-scale dialogue datasets without human priors as out-of-distribution data. In our   \n64 experiments, we demonstrate that (1) our model outperforms several recent open-source evaluation   \n65 models and methods across all three settings, (2) there is a noticeable improvement in the evaluation   \n66 model\u2019s capability when progressively training the Initial model, the SFT model, and the DPO   \n67 model, and (3) as shown in Figure 1, our DPO model achieves the best performance even when using   \n68 only a single evaluation criterion (single inference branch). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "nzzxLPJENZ/tmp/2aed75171fd8a6b0ecb5208cc25723d2ff7d993dd5818172ce46d66dd39a87df.jpg", "img_caption": ["Figure 1: The agreement between human judgment and LLMs in Eval-P benchmark in out-ofdistribution evaluation. Auto-J serves as the \u201cone-step\u201d evaluation baseline, while Fennec is the \u201cmulti-step\u201d baseline. The Initial, SFT, and DPO models were trained using our generated data. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "69 2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "70 Automated dialogue evaluation [6] has long been a significant challenge in the field of generative AI.   \n71 Recent work [10, 7, 35, 41] has demonstrated that LLMs can act as automated evaluators, serving   \n72 as alternatives to human judges. However, LLMs still exhibit issues such as positional bias and   \n73 prediction inconsistency [34, 40]. Many studies have relied heavily on human priors [14, 11], thereby   \n74 neglecting to explore the model generalization capabilities. In contrast, our research focuses on   \n75 examining the performance with different data distributions and investigates how to bridge this gap.   \n76 We consider automated evaluation as a complex reasoning task and aim to improve model performance   \n77 by optimizing reasoning trajectories. When handling such tasks, LLMs typically utilize decision   \n78 trees [37, 23] to model the reasoning process. They often employ search algorithms like $\\mathrm{A^{*}}$ [21, 13] or   \n79 Monte Carlo Tree Search (MCTS) [29, 31] to identify the optimal reasoning path within the candidate   \n80 decision. However, these methods generally rely on deterministic reward signals or feedback, which   \n81 are absent in our settings. We demonstrate that the ensemble boundary of the evaluation branches   \n82 provides a feasible reward signal to verify the accuracy of the reasoning trajectories. Based on this,   \n83 we can guide the model to generate a substantial amount of high-quality data.   \n84 Automated evaluation is also a pivotal technology within scalable oversight, aiming to enhance   \n85 humans\u2019 ability to supervise models. For example, humans may ask models to critique the outputs of   \n86 other models [9, 28] or use models to help decompose a problem into simpler subproblems [17]. In   \n87 contrast to improving human supervision, we focus on how to conduct reliable automated evaluations.   \n88 Certainly, our proposed evaluation methods and results can also be combined with human oversight   \n89 to provide even better performance. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "90 3 Problem Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "91 In this work, our primary focus is on evaluating AI responses, particularly in analyzing query   \n92 and response pairs within given datasets to determine which response is better 2. Traditional ap  \n93 proaches [41, 35] regard the evaluation task as a \u201cone-step\u201d classification (\u201cwin\u201d or \u201close\u201d or \u201ctie\u201d) or   \n94 generation problem, where the final scores or explanations are assigned by a reward model or the   \n95 evaluation model. However, with complex reasoning tasks or scenarios, a given query may involve   \n96 multiple intents, whether explicit or implicit [38], yet the generated responses by AI often overlook   \n97 some of these intents, constrained by the model\u2019s capabilities. Therefore, multiple evaluation criteria   \n98 are required [19] to verify whether the responses address the query requirements and align with user   \n99 intentions. Considering the complexity and diversity of dialogue tasks, it remains an intractable   \n100 challenge to gather comprehensive and accurate evaluation criteria. ", "page_idx": 2}, {"type": "text", "text": "101 3.1 Conducting evaluation through multi-step reasoning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "102 We try to view the evaluation task as a complex reasoning task, a multi-step generative problem, which   \n103 entails: (1) initially seeking suitable evaluation criteria, then (2) generating scoring guidelines based   \n104 on these criteria, and finally (3) conducting comprehensive judgment based on the aforementioned   \n105 criteria and scoring guidelines. Formally, given a dialogue $\\mathcal{X}$ , we will use an evaluation model to   \n106 sequentially obtain the criterion $\\mathcal{C}$ , scoring guideline $\\boldsymbol{S}$ , and judgment $\\mathcal{I}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{C}\\sim\\pi_{\\theta}(\\mathcal{C}|\\mathrm{Prompt}_{\\mathcal{C}},X),S\\sim\\pi_{\\theta}(S|\\mathrm{Prompt}_{\\mathcal{S}},\\mathcal{C},X),\\mathcal{I}\\sim\\pi_{\\theta}(\\mathcal{I}|\\mathrm{Prompt}_{\\mathcal{I}},S,\\mathcal{C},X),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "107 where $\\pi_{\\theta}$ represents the evaluation policy, the prompt please refer to Appendix A.3. Similar to related   \n108 multi-branch evaluation [27, 16] methodologies, we refer to different reasoning paths as \u201cevaluation   \n109 branch\u201d, where each branch represents a decision-making process. Unlike previous methods [19]   \n110 that relied on enumerating criteria, our goal is for the evaluation model to automatically generate   \n111 crucial and high-priority criteria. ", "page_idx": 2}, {"type": "text", "text": "112 3.2 Focusing on two challenges ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "113 A natural approach is to first construct a candidate set of criteria and then derive suitable results based   \n114 on these criteria. To address this task, we focus on the following two challenges:   \n115 \u2022 How to construct an appropriate candidate set? Our aim is to develop a candidate set that   \n116 includes multiple evaluation branches enriched with high-quality evaluation opinions. By   \n117 training and optimizing this candidate space to advance desired behaviors, we can swiftly   \n118 identify appropriate and critical judgments during the inference process.   \n119 \u2022 How to rank the judgments? We also need to establish a ranking among different evaluation   \n120 branches to optimize the candidate space. In contrast to recent studies [13], our evaluation   \n121 dataset lacks ground truth labels or environmental feedback to act as reward signals. The cost   \n122 of obtaining these signals is prohibitive, requiring not only expensive human labor but also ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "image", "img_path": "nzzxLPJENZ/tmp/63cf7807ef478f48d768f6f8a68ee750273b1fe64d8ef31e87f4ca39801cc984.jpg", "img_caption": ["Figure 2: Compared to single-chain inference, we adopt a multi-branch based approach to train the Initial model. Subsequently, we construct an evaluation tree through a series of growth and pruning operations. This tree then guides the training both of the SFT model and the DPO model. ", "123 facing issues of low consistency among humans in many ambiguous problems. Therefore, 124 we need to design an innovative and cost-effective approach to address this challenge. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "125 4 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "126 Figure 2 illustrates an overview of our method, which involves three stages for model training: First,   \n127 we train the Initial model to construct the evaluation tree; Then, we sample different evaluation   \n128 branches as supervised data to train the SFT model, enhancing branch prediction consistency; Finally,   \n129 we collect preference data to train the DPO model, ensuring rapid sampling of critical branches. ", "page_idx": 3}, {"type": "text", "text": "130 4.1 Collecting dialogue dataset ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "131 Evaluation models typically rely on robust generalization capabilities to effectively handle diverse   \n132 dialogue tasks. Consequently, the distribution of training data significantly affects performance on   \n133 unseen tasks encountered during real-world evaluations. To address this, we sampled from a large  \n134 scale dialogue dataset rather than a specific data source. We then apply the K-Means algorithm [2] to   \n135 cluster the data. Subsequently, we sample data from these clusters, ensuring that the training dataset   \n136 encompasses a diverse set of dialogue scenarios. More details refer to Appendix A.1 ", "page_idx": 3}, {"type": "text", "text": "137 4.2 Training initial model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "138 We aim to construct a dataset from scratch for evaluation, consisting of dialogues paired with their   \n139 corresponding evaluation trees. Each tree contains different reasoning paths during the evaluation of   \n140 dialogues. The root node of this tree represents the dialogue data, and each path from the root node to   \n141 a leaf node signifies an evaluation branch. Each evaluation branch comprises three decision-making   \n142 behavior nodes: criterion $\\mathcal{C}$ , scoring guideline $\\boldsymbol{S}$ , and judgment $\\mathcal{I}$ . To simulate this decision process,   \n143 we introduce a multi-branch training approach [16] to train an LLM as the initial policy $\\pi_{\\mathrm{Initial}}$ . We   \n144 employ GPT-4 (gpt-4-0125-preview) [1] to generate corresponding multi-branch training data   \n145 to enhance quality. This approach ensures that the model can auto-regressively generate evaluation   \n146 branches using Equation 1. ", "page_idx": 3}, {"type": "text", "text": "147 4.3 Generating evaluation tree ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "148 We expand the branch candidates sampled from the policy $\\pi_{\\mathtt{I n i t i a l}}$ using the breadth-first growth,   \n149 thereby including as many high-quality evaluation paths as possible. Due to the different paradigms   \n150 of SFT and DPO, we employ consistency pruning to split the sampling space to obtain training data:   \n151 \u2022 Breadth-first Growth: The evaluation tree contains two distinct growth manner: for   \n152 criterion $\\mathcal{C}$ node, we use LLM\u2019s brainstorming capability to generate $k$ relevant criteria; for   \n153 scoring guideline $\\boldsymbol{S}$ and judgment $\\mathcal{I}$ node, we use sampling method by adjusting the LLM\u2019s   \n154 temperature and top- $\\boldsymbol{\\cdot}$ parameters. To simplify, we utilize the Initial model $\\pi_{\\mathrm{Initial}}$ to   \n155 generate a complete binary tree for each subtree with a criteria node as its root. Furthermore,   \n156 since the evaluation task requires testing the model\u2019s consistency by swapping response   \n157 positions, we can obtain $k\\times8$ different evaluation branches. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "image", "img_path": "nzzxLPJENZ/tmp/712075845a276357d76e6685fecd882dfb04895e7366982d18a5bc79b84009b1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3: The figure illustrates how the training dataset of the SFT and DPO models is sampled from an evaluation subtree based on a specific criterion. ", "page_idx": 4}, {"type": "text", "text": "158   \n159   \n160   \n161   \n162   \n163 ", "page_idx": 4}, {"type": "text", "text": "\u2022 Consistency Pruning: Prior to pruning, we introduce two different consistency constraints: self-consistency, meaning the same criterion $\\mathcal{C}$ and scoring guideline $\\boldsymbol{S}$ should yield the same judgment $\\mathcal{I}$ , and positional consistency, meaning that swapping positions should not affect the judgment $\\mathcal{I}$ . Subsequently, we obtain SFT training data from evaluation branches in the evaluation tree that meet both consistency constraints, and DPO training data from nodes that do not satisfy these constraints. ", "page_idx": 4}, {"type": "text", "text": "164 4.4 Collecting preference labels ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "165 Although we can obtain SFT and DPO data from the consistency sampling space, this data lacks   \n166 correctness verification. Typically, preference data requires human annotation to establish ranking   \n167 sequences, a time-consuming process that is not suitable for scaling. Therefore, we propose two   \n168 alternative approaches to label each evaluation branch with its correctness: ", "page_idx": 4}, {"type": "text", "text": "\u2022 Branch Ensemble: Considering that there are only three final labels for judgment (\u201cwin\u201d or \u201close\u201d or \u201ctie\u201d), we use an ensemble result of evaluation branches to obtain the consensus label. The ensemble method provides a lower bound of judge error without incurring additional costs. For SFT data, we filter out data that is inconsistent with the ensemble results. For DPO pair data, we select samples consistent with the ensemble results as \u201cchosen\u201d samples, and those inconsistent as \u201crejected\u201d samples.   \n\u2022 LLM-as-a-Judge: Some highly aligned LLMs, such as GPT-4, possess powerful annotation capabilities. Therefore, we use LLMs to determine which sample in the DPO pairs data is more reasonable as the \u201cchosen\u201d sample. In our experiments, we found that this method has only a $20\\%$ disagreement rate compared to the Branch Ensemble method. We analyze this method in Section 5.4 ", "page_idx": 4}, {"type": "text", "text": "180 As shown in Figure 3, we combine consistency pruning and automated labeling to collect the cor  \n181 responding preference data. Through the labeling of judgments, we can also obtain preference   \n182 information for criterion $\\mathcal{C}$ and scoring guideline $\\boldsymbol{S}$ based on the final judgment $\\mathcal{I}$ decisions. Specifi  \n183 cally, we prioritize predicting criteria that lead to correct judgments and select the scoring guidelines   \n184 with the highest overall scores as the \u201cchosen\u201d samples. Additionally, we randomly sample from the   \n185 filtered data to create the training set, thereby controlling training costs and efficiency. ", "page_idx": 4}, {"type": "text", "text": "186 4.5 Training SFT model and DPO model ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "187 We use the Initial model as the starting point to train the SFT model $\\pi_{\\mathtt{S F T}}$ using supervised learning,   \n188 which reduces inconsistent predictions compared to the initial policy. Then, we take the SFT model as   \n189 the initialization to train the DPO model $\\pi_{\\mathrm{DP0}}$ using Direct Preference Optimization, which can learn   \n190 the decision priorities of different branches, with the objective: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\sf D P O}(\\pi_{\\sf D P O}|\\pi_{\\sf S F T})=-\\mathbb{E}_{(x,y_{c},y_{r})}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_{\\sf D P O}(y_{c}|x)}{\\pi_{\\sf S F T}(y_{c}|x)}-\\beta\\log\\frac{\\pi_{\\sf D P O}(y_{r}|x)}{\\pi_{\\sf S F T}(y_{r}|x)}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "191 where the $(x,y)$ represents data pair of different decision tasks in Equation 1, $y_{c}$ represents the   \n192 \u201cchosen\u201d sample, and $y_{r}$ represents the \u201crejected\u201d sample.   \n193 During the inference process, we create a single branch for each criterion to conduct evaluation, and   \n194 control the number of generated branches $k$ to adjust the inference efficiency. Since the DPO model   \n195 employs sampling optimization, it usually achieves optimal performance with only a few branches. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "table", "img_path": "nzzxLPJENZ/tmp/8c9be04e7dc509910648454cd89594104953bb7502c238ccac3089044d6c26e9.jpg", "table_caption": [], "table_footnote": ["Table 1: The Initial, SFT, and DPO are our trained models from three training stages. We select the best performance results by varying branches. Bold numbers indicate the best performance among open-source models, while underlined numbers represent the best performance across all models. "], "page_idx": 5}, {"type": "text", "text": "196 5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "197 As the most popular LLM evaluation platform recently, Chatbot Arena demonstrates high alignment   \n198 with human judgments in pairwise response evaluations. We collect its open-source human judgment   \n199 benchmark, Eval-P and MT-bench, to serve as the test set. We gather training data comprising both   \n200 dialogue data and evaluation data for the following three evaluation scenarios:   \n201 1. In-distribution evaluation: We apply the Fennec [16] training data to train the In  \n202 distribution (ID) model, which included 3K dialogue data from Auto-J [14], along with   \n203 evaluation data annotated by GPT-4. This training data is a multi-branch dataset, meaning   \n204 that a single dialogue includes multiple evaluation branches.   \n205 2. Out-of-distribution evaluation: We collect 5M large-scale dialogue data and extracted   \n206 7K samples from it to serve as out-of-distribution (OOD) training data. GPT-4 annotate 3K   \n207 evaluation samples from this dataset for the Initial model training.   \n208 3. Transfer evaluation: We use 3K OOD training data (which includes evaluation data) and   \n209 2K ID dialogue data (which did not include evaluation data) to train the transfer model.   \n210 For each benchmark, we employ Agreement (AGR) and Consistency (CNS) as performance metrics.   \n211 Consistency measures the prediction consistency of the evaluation model when the positions of the   \n212 responses are swapped. Agreement quantifies the proportion of evaluations that meet the criteria   \n213 for swap consistency and align with human judgments. In many cases, the \u201ctie\u201d label indicates   \n214 an inability to distinguish performance under some evaluation criteria. However, it may still be   \n215 distinguishable under specific evaluation criteria. Therefore, we also present the model performance   \n216 on the test data without \u201ctie\u201d label. For more details, please refer to Appendix A.2 ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "217 5.1 In-distribution evaluation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "218 The results are shown in Table 1, where methods marked with $\\dagger$ denote our reimplementations. Since   \n219 the Initial model leverages the Fennec training data for initialization, its performance can be   \n220 regarded as its in-distribution evaluation baseline. As observed, the SFT and DPO models exhibit   \n221 significant performance improvements over most baseline methods on both the Auto-J and Fennec   \n222 datasets, achieving the highest agreement score of 57.18. In the multi-turn dialogue evaluation on   \n223 MT-bench, the Fennec dataset comprises only single-turn dialogues, which constrains its effectiveness   \n224 in handling multi-turn context information. Additionally, we observed the instabilities problems   \n225 during the training process, which hindered the DPO model from outperforming the Initial model.   \n226 A more comprehensive analysis of these instability problems is provided in Section 5.7. ", "page_idx": 5}, {"type": "image", "img_path": "nzzxLPJENZ/tmp/12eb2233cc27b271f4039b9a6cc0579030990feb66a4c04abb7ca202eeac1a70.jpg", "img_caption": ["(a) The scenarios of ID dataset (b) The scenarios of OOD dataset (c) The scenarios of Eval-P ", "Figure 4: The scenario contains seven categories, including Summarization, Exam Questions, Rewriting, Code, Functional Writing, Creative Writing, General Communication, NLP Tasks, and Others. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "227 5.2 Out-of-distribution evaluation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "228 In terms of OOD evaluation, the Initial model performs worse than the baseline model on both   \n229 Eval-P and MT-bench benchmarks, due to the distribution shift in the dialogue dataset. With   \n230 RLHF [22] training, the SFT model significantly surpasses the Initial model in consistency rate   \n231 and also enhances the agreement rate. Notably, the DPO model achieves superior performance with   \n232 only three branches, thereby reducing inference latency by over $60\\%$ . In evaluation settings without   \n233 \u201ctie\u201d labels, the advantage of the DPO model becomes more apparent, significantly outperforming   \n234 other models, including proprietary model GPT-4. This demonstrates that the DPO model can   \n235 effectively distinguish between responses using critical criteria, even when employing only 3 branch   \n236 for inference. Furthermore, our models are capable of handling multi-turn dialogue scenarios,   \n237 achieving performance that surpasses the in-distribution models. These extremely strong results   \n238 indicate that our model excels at identifying more crucial criteria to help distinguish the difference of   \n239 AI\u2019s responses. ", "page_idx": 6}, {"type": "text", "text": "240 5.3 Transfer evaluation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "241 The purpose of transfer evaluation is to evaluate the model\u2019s capability to adapt to in-distribution data,   \n242 thus mitigating the problem of training data distribution shift. It can be observed that both the SFT   \n243 and DPO models demonstrate improvements across multiple benchmarks compared to the Initial   \n244 model. Notably, in both OOD and transfer evaluation settings, the DPO model consistently achieves   \n245 better performance than the SFT model, while also reducing the number of inference branches.   \n246 Although the transfer model does not surpass the OOD model, it still achieves closed performance.   \n247 In Section 5.4, we provide a detailed analysis of the different scenarios that lead to these models   \n248 exhibiting significantly different performance characteristics despite their close overall performance. ", "page_idx": 6}, {"type": "text", "text": "249 5.4 Scenario analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "250 To investigate the impact of scenario categories distribution in the training data, we need to analyze   \n251 the scenarios within the OOD, ID training sets, and the Eval-P test set. For this purpose, we   \n252 employ the scenario classifier trained by Auto-J, which effectively categorizes dialogue data into 58   \n253 different scenarios. Figure 4 presents the distribution of scenarios. It can be observed that Auto-J\u2019s   \n254 training set is well-balanced across the predefined scenarios, closely matching the distribution of the   \n255 Eval-P test set. In contrast, within the OOD data, the \"Others\" category exceeds $30\\%$ , and \"General   \n256 Communication\" surpasses $50\\%$ . The significant differences in scenario distributions between the   \n257 OOD data and the test set can lead to performance variations in test cases.   \n258 From the evaluation results of fine-grained scenarios, we can derive several interesting observations   \n259 from Table 2: (1) The ID and Transfer models significantly outperform the OOD model in Summa  \n260 rization and Exam Questions, which are notably lacking in the OOD training data. (2) The OOD   \n261 model performs significantly better than the ID and Transfer models in the General Communication   \n262 and \u201cOthers\u201d categories. (3) For writing-related text generation tasks, the OOD model achieves   \n263 performance that is comparable to the ID model. These results indicate that the type and quantity of   \n264 tasks remain crucial in evaluation tasks. Therefore, the evaluation model can achieve combinatorial   \n265 generalization capability by increasing the number of scenarios or tasks. When GPT-4 serves as a   \n266 judge to provide preference labels, it achieves improvement in code and NLP tasks compared with   \n267 DPO model but also affects performance in other scenarios. ", "page_idx": 6}, {"type": "table", "img_path": "nzzxLPJENZ/tmp/f75b2e523e68f9d284572054d1bfaeb932aa73dcc9ef1d5f39a27b5dc796f658.jpg", "table_caption": ["Table 2: Agreement rates for different scenario groups and overall results. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "268 5.5 Dialogue correction ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "269 The critical capability of evaluation is to identify and   \n270 rectify flaws in dialogues, thereby enhancing the overall   \n271 quality of the original AI responses. Therefore, we test   \n272 our model\u2019s ability to evaluate and correct dialogues   \n273 generated by the Alpaca-13B [30] and the LLaMA2-   \n274 7B Chat [32] models in MT-Bench. Unlike previous   \n275 pairwise evaluations, MT-Bench presents a multi-turn   \n276 dialogue and uses GPT-4 to assign scores (ranging from   \n277 1 to 10) to different AI responses, subsequently giving   \n278 the model ranking relationship based on these scores.   \n279 Specifically, to elicit the model\u2019s correction ability, we Table 3: Results of dialogue correction.   \n280 construct 3k correction pairs and incorporate them into   \n281 the evaluation training set. When performing corrections, we first generate a judgment for the   \n282 responses and then modify those with scores below 3. As illustrated in Table 3, the modification   \n283 rates for Alpaca are all above $95\\%$ , indicating that the quality of responses generated by weak   \n284 models is generally subpar. After refinement, both Alpaca-13B and LLaMA2-7B Chat model achieve   \n285 better scores. Moreover, the correction results of the DPO model outperform those of the SFT model,   \n286 demonstrating that better evaluation feedback can lead to significant improvements in evaluation   \n287 quality. These results not only demonstrate the effectiveness of our model in identifying and correcting   \n288 dialogue flaws but also highlight its potential to substantially improve the performance of dialogue   \n289 systems through robust evaluation. ", "page_idx": 7}, {"type": "table", "img_path": "nzzxLPJENZ/tmp/c78e49a3eb95cfb0d473db16e92d886c239aa8b59148269fbdb9fd38d16da8ca.jpg", "table_caption": [""], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "290 5.6 Impact of Initial model data scale ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "291 In our investigations, we strive to reduce reliance on   \n292 both human annotators and GPT-4. Specifically, in   \n293 the current work, we trained an Initial model using   \n294 annotation data generated by GPT-4 without any addi  \n295 tional supervision. We evaluated the performance of the Initial model trained on different sizes   \n296 of data on the Eval-P benchmark. As shown in Table 4, the model reaches its best performance   \n297 at 2k data, without considering the influence of GPT-4\u2019s annotation quality. Based on the assump  \n298 tion that LLMs primarily unlock their potential during alignment phase, we believe that enhancing   \n299 performance hinges on increasing the variety of tasks rather than merely expanding the dataset. ", "page_idx": 7}, {"type": "table", "img_path": "nzzxLPJENZ/tmp/df317b4340beb8ad0065fdf6e8c7995d8e0ae840ad89cc88573452d48866ad6a.jpg", "table_caption": ["Table 4: Results of different data scale. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "nzzxLPJENZ/tmp/7f48335c4a2d31fbc32ab184a808bf9beaca9704ea0917e61a11081881debe5f.jpg", "img_caption": ["Figure 5: The agreement and consistency rates of ID and OOD models with different branches. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "300 5.7 Instability problem in in-distribution training ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "301 The Direct Preference Optimization (DPO) algorithm [24] aims to optimize the selection of various   \n302 branching preferences within the SFT model. In out-of-distribution evaluations, the DPO model   \n303 demonstrates stable performance improvements in both agreement and consistency compared to the   \n304 SFT model, as shown in Figure 5. However, in in-distribution evaluations, the SFT model consistently   \n305 outperforms the DPO model in terms of the consistency rate. Additionally, SFT model does not achieve   \n306 better performance by increasing the number of branches. We believe the primary reason for training   \n307 instability is that the training data for DPO algorithm and the initial model come from the same   \n308 distribution. As a result, the SFT and DPO models fail to obtain more stable supervision signals and   \n309 may even overfit the training dataset. In contrast, OOD training incorporates a more diverse data   \n310 distribution, which helps the model avoid converging to local optima during training. ", "page_idx": 8}, {"type": "text", "text": "311 6 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "312 6.1 Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "313 Currently, our model faces some limitations: (1) It cannot handle cases where all AI responses are   \n314 incorrect, which should not be labeled as a \u201ctie\u201d. (2) The model\u2019s result parsing relies heavily on   \n315 regular expressions, which can lead to format errors. To address these issues, we plan to make several   \n316 improvements, including expanding our task settings and utilizing the functional calling feature of   \n317 LLMs. Additionally, our model\u2019s performance is constrained by the amount of training data and   \n318 parameters. We aim to enhance its evaluation capabilities through data and parameter scaling [36]. ", "page_idx": 8}, {"type": "text", "text": "319 6.2 Future work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "320 Our work demonstrates that the evaluation model generates diverse judgments for dialogue content   \n321 based on different criteria. To align more closely with human behavior, we prioritize key judgments   \n322 in the evaluation model\u2019s outputs. In future, we try to further expand the criteria space to uncover a   \n323 variety of decision paths. Additionally, we aim to find more accurate preference selection methods to   \n324 replace ensemble methods, thereby achieving a better alignment with human behavior. ", "page_idx": 8}, {"type": "text", "text": "325 7 Broader Impact ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "326 Our work focuses on the task of automatic evaluation, specifically exploring how to learn better   \n327 evaluation strategies from an evaluation tree. We demonstrate that automated evaluation criteria   \n328 can replace human priors, and by combining branch decision-making with DPO training, we have   \n329 achieved robust evaluation performance. We conduct detailed experiments covering a broad range   \n330 of real-world scenarios to discuss how to enhance model evaluation capabilities from scratch. With   \n331 our work, we hope to inform further research into better understanding and developing improved   \n332 evaluation methodologies for LLMs. ", "page_idx": 8}, {"type": "text", "text": "333 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "334 [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni   \n335 Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4   \n336 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n337 [2] Mohiuddin Ahmed, Raihan Seraj, and Syed Mohammed Shamsul Islam. The k-means algorithm:   \n338 A comprehensive survey and performance evaluation. Electronics, 9(8):1295, 2020.   \n339 [3] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn   \n340 Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless   \n341 assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862,   \n342 2022.   \n343 [4] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschen  \n344 brenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-to-strong gener  \n345 alization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390,   \n346 2023.   \n347 [5] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv   \n348 preprint arXiv:2307.08691, 2023.   \n349 [6] Jan Deriu, Alvaro Rodrigo, Arantxa Otegi, Guillermo Echegoyen, Sophie Rosset, Eneko Agirre,   \n350 and Mark Cieliebak. Survey on evaluation methods for dialogue systems. Artificial Intelligence   \n351 Review, 54:755\u2013810, 2021.   \n352 [7] Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos   \n353 Guestrin, Percy S Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for   \n354 methods that learn from human feedback. Advances in Neural Information Processing Systems,   \n355 36, 2024.   \n356 [8] Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire.   \n357 arXiv preprint arXiv:2302.04166, 2023.   \n358 [9] Geoffrey Irving, Paul Christiano, and Dario Amodei. Ai safety via debate. arXiv preprint   \n359 arXiv:1805.00899, 2018.   \n360 [10] Dongfu Jiang, Yishan Li, Ge Zhang, Wenhao Huang, Bill Yuchen Lin, and Wenhu Chen.   \n361 Tigerscore: Towards building explainable metric for all text generation tasks. arXiv preprint   \n362 arXiv:2310.00752, 2023.   \n363 [11] Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo   \n364 Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al. Prometheus: Inducing fine-grained   \n365 evaluation capability in language models. arXiv preprint arXiv:2310.08491, 2023.   \n366 [12] Arie W Kruglanski and Icek Ajzen. Bias and error in human judgment. European Journal of   \n367 Social Psychology, 13(1):1\u201344, 1983.   \n368 [13] Lucas Lehnert, Sainbayar Sukhbaatar, Paul Mcvay, Michael Rabbat, and Yuandong Tian.   \n369 Beyond a\\*: Better planning with transformers via search dynamics bootstrapping. arXiv   \n370 preprint arXiv:2402.14083, 2024.   \n371 [14] Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. Generative   \n372 judge for evaluating alignment. arXiv preprint arXiv:2310.05470, 2023.   \n373 [15] Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, and Chongyang Tao. Leveraging large   \n374 language models for nlg evaluation: A survey. arXiv preprint arXiv:2401.07103, 2024.   \n375 [16] Xiaobo Liang, Haoke Zhang, Helan hu, Juntao Li, Jun Xu, and Min Zhang. Fennec: Fine  \n376 grained language model evaluation and correction extended through branching and bridging,   \n377 2024.   \n378 [17] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan   \n379 Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s verify step by step. arXiv preprint   \n380 arXiv:2305.20050, 2023.   \n381 [18] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization   \n382 branches out, pp. 74\u201381, 2004.   \n383 [19] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval:   \n384 Nlg evaluation using gpt-4 with better human alignment. In Proceedings of the 2023 Conference   \n385 on Empirical Methods in Natural Language Processing, pp. 2511\u20132522, 2023.   \n386 [20] Todor Markov, Chong Zhang, Sandhini Agarwal, Florentine Eloundou Nekoul, Theodore Lee,   \n387 Steven Adler, Angela Jiang, and Lilian Weng. A holistic approach to undesired content detection   \n388 in the real world. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37,   \n389 pp. 15009\u201315018, 2023.   \n390 [21] P Russel Norvig and S Artificial Intelligence. A modern approach. Prentice Hall Upper   \n391 Saddle River, NJ, USA: Rani, M., Nayak, R., & Vyas, OP (2015). An ontology-based adaptive   \n392 personalized e-learning system, assisted by software agents on cloud storage. Knowledge-Based   \n393 Systems, 90:33\u201348, 2002.   \n394 [22] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,   \n395 Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to   \n396 follow instructions with human feedback. Advances in neural information processing systems,   \n397 35:27730\u201327744, 2022.   \n398 [23] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong,   \n399 Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master $16000+$   \n400 real-world apis. arXiv preprint arXiv:2307.16789, 2023.   \n401 [24] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and   \n402 Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model.   \n403 Advances in Neural Information Processing Systems, 36, 2024.   \n404 [25] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimiza  \n405 tions toward training trillion parameter models. In SC20: International Conference for High   \n406 Performance Computing, Networking, Storage and Analysis, pp. 1\u201316. IEEE, 2020.   \n407 [26] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System   \n408 optimizations enable training deep learning models with over 100 billion parameters. In   \n409 Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &   \n410 Data Mining, pp. 3505\u20133506, 2020.   \n411 [27] Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian Li.   \n412 Branch-solve-merge improves large language model evaluation and generation. arXiv preprint   \n413 arXiv:2310.15123, 2023.   \n414 [28] William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan   \n415 Leike. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802,   \n416 2022.   \n417 [29] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess  \n418 che, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mas  \n419 tering the game of go with deep neural networks and tree search. nature, 529(7587):484\u2013489,   \n420 2016.   \n421 [30] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy   \n422 Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.   \n423 https://github.com/tatsu-lab/stanford_alpaca, 2023.   \n424 [31] Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, and Dong Yu. To  \n425 ward self-improvement of llms via imagination, searching, and criticizing. arXiv preprint   \n426 arXiv:2404.12253, 2024.   \n427 [32] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,   \n428 Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open   \n429 foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n430 [33] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes   \n431 Belkada, Shengyi Huang, Leandro von Werra, Cl\u00e9mentine Fourrier, Nathan Habib, et al. Zephyr:   \n432 Direct distillation of lm alignment. arXiv preprint arXiv:2310.16944, 2023.   \n433 [34] Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and   \n434 Zhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926,   \n435 2023.   \n436 [35] Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya   \n437 Jiang, Rui Xie, Jindong Wang, Xing Xie, et al. Pandalm: An automatic evaluation benchmark   \n438 for llm instruction tuning optimization. arXiv preprint arXiv:2306.05087, 2023.   \n439 [36] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani   \n440 Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large   \n441 language models. arXiv preprint arXiv:2206.07682, 2022.   \n442 [37] Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin   \n443 Chen, Ruobing Xie, Yankai Lin, et al. Advancing llm reasoning generalists with preference   \n444 trees. arXiv preprint arXiv:2404.02078, 2024.   \n445 [38] Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. Evaluating   \n446 large language models at evaluating instruction following. arXiv preprint arXiv:2310.07641,   \n447 2023.   \n448 [39] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore:   \n449 Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019.   \n450 [40] Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu,   \n451 and Yongbin Li. Wider and deeper llm networks are fairer llm evaluators. arXiv preprint   \n452 arXiv:2308.01862, 2023.   \n453 [41] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,   \n454 Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and   \n455 chatbot arena. Advances in Neural Information Processing Systems, 36, 2024.   \n456 [42] Lianghui Zhu, Xinggang Wang, and Xinlong Wang. Judgelm: Fine-tuned large language models   \n457 are scalable judges. arXiv preprint arXiv:2310.17631, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "458 A Appendix / supplemental material ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "459 A.1 Training data collection and clustering ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "460 We collect 5M data points from various open-source datasets as described in Table 5. We deduplicate   \n461 the queries within this dataset. To obtain the semantic representations of all queries, we utilize a   \n462 sentence-embedding model angle-llama- $7b$ -nli- $\\nu2^{3}$ . Subsequently, we employ the $\\mathbf{k}\\cdot$ -means algorithm   \n463 for unsupervised clustering to differentiate between dialogues from distinct scenarios. The $\\mathrm{k}$ -means   \n464 algorithm is implemented using cuML4. The number of clusters is 1,000, and the maximum number   \n465 of iterations is 300. We uniformly sample from each cluster to obtain a final training set comprising   \n466 7K instances. Our work generates responses to all queries using open-source models, subsequently   \n467 forming pairs of responses through a random selection process. The models employed include   \n468 Mistral-7B-Instruct- $\\dot{\\nu}\\dot{0}.2^{5}$ , Qwen1.5-7B-Chat6, Llama-2-7b-Chat7, Qwen1.5-72B-Chat8 and Mixtral  \n469 8x7B-Instruct- $\\nu\\theta.I^{9}$ . ", "page_idx": 12}, {"type": "table", "img_path": "nzzxLPJENZ/tmp/dfa1ce14b50338bb4db0b24fa5e6ffc5a0a1a006abf816be419400ae9158650a.jpg", "table_caption": ["Table 5: The details of open-source datasets utilized in our work. "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "470 A.2 Training details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "471 Table 6 presents the detailed training data statistics.   \n472 Here, $\\clubsuit$ represents the dialogue data collected from   \n473 Fennec, $\\spadesuit$ represents the dialogue data collected from   \n474 large-scale open-source dialogue data, and $\\diamondsuit$ represents   \n475 the evaluation data annotated using GPT-4. In the ID   \n476 settings, the same dialogue data is used across different   \n477 training stages, while the different dialogue data is used   \n478 for other setups. Given the diverse nature of dialogue   \n479 tasks, the assumption of data distribution is highly influenced by the collection strategies and data   \n480 deduplication methods employed. These processes inherently vary, and it is challenging to guarantee   \n481 that each dataset comprehensively represents distinct domains or tasks. We utilize Zephyr-7B   \n482 Chat10 [33] as the backbone to train our evaluation model. We employ DeepSpeed [26] library,   \n483 Zero Redundancy Optimizer (ZeRO) [25] Stage 3, FlashAttention [5], and the bfloat16 (BF16) and   \n484 tfloat32 (TF32) mix computation precision on 8 NVIDIA A100 GPUs. The number of gradient   \n485 accumulation steps is 32, the learning rate of the initial model and SFT model is 1e-5, and the learning   \n486 rate of the DPO model is 5e-7. The number of epochs is 1 in each training stage. We set $\\beta$ to 0.1   \n487 when training the DPO model. ", "page_idx": 13}, {"type": "table", "img_path": "nzzxLPJENZ/tmp/21c942152d25daefce0a1f8dbbacd5cef1aae7b49cd28a443dee3384dd5175c2.jpg", "table_caption": ["Table 6: Training dataset statistics. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "488 A.3 Prompts ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "489 Table 7-10 shows different prompts. Table 7 shows the prompt for response correction, and Table 8   \n490 elaborates on the prompts that GPT-3.5 and GPT-4 models use to generate the testing results. Table 9   \n491 is employed for preference generation powered by gpt-4-0125-preview. Table 10 presents the prompts   \n492 for multi-step evaluation, which is also used to generate the training data for our initial model using   \n493 gpt-4-0125-preview. ", "page_idx": 13}, {"type": "image", "img_path": "nzzxLPJENZ/tmp/0c96e9cbecfe7c4de3a3bf965dccbd6a05e25d430fda3f5008e6913ee056fcbd.jpg", "img_caption": ["Table 7: Prompt for response correction. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "494 A.4 Case study ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "495 Table 11 and 12 provide two cases of pairwise response comparison. We compare the judgments pro  \n496 duced by GPT-4, our Initial Model, and our DPO Model, presenting the primary outputs generated   \n497 by each model. Notably, the criteria provided by the DPO Model in both instances exhibit greater   \n498 accuracy, and the final judgments rendered by the DPO Model are demonstrably more reasonable. ", "page_idx": 13}, {"type": "text", "text": "SYSTEM MESSAGE\u2014 ", "page_idx": 14}, {"type": "text", "text": "Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user\u2019s instructions and answers the user\u2019s question better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Begin your evaluation by comparing the two responses and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie. ", "page_idx": 14}, {"type": "text", "text": "USER MESSAGE\u2014 ", "page_idx": 14}, {"type": "text", "text": "[User Question]   \n{question}   \n[The Start of Assistant A\u2019s Answer] {answer a}   \n[The End of Assistant A\u2019s Answer] [The Start of Assistant B\u2019s Answer] {answer b}   \n[The End of Assistant B\u2019s Answer] ", "page_idx": 14}, {"type": "text", "text": "Table 8: Pairwise comparison prompt for baseline models. ", "page_idx": 14}, {"type": "table", "img_path": "nzzxLPJENZ/tmp/af49235734499efb86eeaaf546bb04d44890231b0b8871884f5cf578d6fe7e19.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 10: Prompts for multi-step evaluation. The criteria in Scoring Guidelines and Pairwise-eval is regularly extracted from the output of the first step. The scoring guidelines in Pairwise-eval are the output of the second step. ", "page_idx": 15}, {"type": "table", "img_path": "nzzxLPJENZ/tmp/196006836d7ffc39e767ef6f0b0d0a4302a2d5465015b8bcc0f7b8dda30e6867.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "1. Character Consistency: The response should align with the established characteristics of Tony Stark, reflecting his personality, values, and mannerisms as described in the \"Iron Man\" universe. ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "2. Creativity: The response should demonstrate creativity in how it presents the character\u2019s thoughts and feelings, potentially offering unique insights into what it means to be Iron Man from Tony Stark\u2019s perspective. ", "page_idx": 16}, {"type": "text", "text": "3. Emotional Intelligence: The response should reflect Tony Stark\u2019s emotional intelligence, showing how he processes and responds to the query with depth and understanding. ", "page_idx": 16}, {"type": "text", "text": "1. Relevance to Query: The response must directly address the user\u2019s request to embody Tony Stark and answer the specific question about his favorite part of being Iron Man. ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "2. Character Consistency: Evaluate how accurately the response reflects Tony Stark\u2019s character traits, values, and mannerisms as seen in the \"Iron Man\" universe. ", "page_idx": 16}, {"type": "text", "text": "3. Creativity: The response should demonstrate creativity in how it presents Tony Stark\u2019s thoughts and feelings about being Iron Man, potentially offering unique insights into the character. ", "page_idx": 16}, {"type": "text", "text": "Analysis of Response A: Response A focuses on the technical and creative aspects of being Iron Man, emphasizing the design and innovation of the suits and the excitement of flying and saving the world. However, it lacks depth in exploring Tony Stark\u2019s personality, values, and mannerisms, ... It provides a very surface-level portrayal of Tony Stark, missing the opportunity to engage with the character\u2019s complexity or the nuances of his personality. Response B Score: 3 ", "page_idx": 16}, {"type": "text", "text": "Analysis of Response B: Response B offers a slightly more nuanced view of Tony Stark\u2019s motivations, focusing on his desire to make a difference and protect the vulnerable. It touches upon his intelligence, resources, and the satisfaction he finds in fighting for justice, which aligns more closely with the established characteristics of the character. However, while it provides a clearer picture of Stark\u2019s motivations and values, it still lacks the rich detail and creative engagement found in the highest scoring responses. ... ", "page_idx": 16}, {"type": "text", "text": "Comparison: Both responses have their strengths and weaknesses in portraying Tony Stark\u2019s character. Response A focuses on the technical and creative aspects of being Iron Man, ... ", "page_idx": 16}, {"type": "text", "text": "Judgement by Our Initial Model (Criteria 1) $(\\checkmark)$ Response A Score: 3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Analysis of Response A: Response A moderately satisfies the criteria by embodying Tony Stark\u2019s persona, focusing on his love for technology and innovation. ... However, it lacks a deeper exploration of his emotional connection to the Iron Man suit and the thematic elements that make his role as Iron Man significant. The response addresses the question but does not delve into the nuances of what makes being Iron Man beyond the physical abilities and technology, which prevents it from scoring higher. Response B Score: 4 Analysis of Response B: Response B effectively embodies Tony Stark\u2019s character by highlighting his intelligence, resourcefulness, and the emotional satisfaction he derives from using his abilities to protect the world. ... It slightly lacks the depth of exploring specific aspects of the Iron Man suit itself, which could have provided a more comprehensive insight into Stark\u2019s character. Comparison: Both responses do a good job of embodying Tony Stark\u2019s character, but they focus on different aspects. Response A leans more towards the technical and creative aspects of Stark\u2019s personality, emphasizing his love for technology and innovation. Response B, on the other hand, ", "page_idx": 16}, {"type": "text", "text": "Discuss antitrust laws and their impact on market competition. Compare the antitrust laws in US and China along with some case studies.\" ", "page_idx": 17}, {"type": "text", "text": "Antitrust laws are designed to promote competition in markets and protect consumers from anticompetitive practices. In the US, the Sherman Antitrust Act of 1890 is the primary antitrust law and prohibits agreements or practices that restrain trade and impose unfair competition. The US Department of Justice and the Federal Trade Commission are responsible for enforcing the law. In China, the AntiMonopoly Law enacted in 2008 is the primary antitrust law and is similar to the Sherman Act in prohibiting monopolistic agreements and abuse of dominance. Case studies have shown that antitrust laws can have a significant impact on market competition. For example, in 2000 the US Department of Justice flied a lawsuit against Microsoft alleging that the company had abused its monopoly in the computer operating system market. The lawsuit resulted in Microsoft being ordered to make significant changes to its business practices and the case is often cited as an example of how antitrust laws can be used to promote competition. In China, ... . Overall, antitrust laws can have a significant impact on market competition and have been used to promote competition and protect consumers in both the US and China. ", "page_idx": 17}, {"type": "text", "text": "Antitrust laws are a set of regulations designed to promote and maintain fair competition in markets. These laws aim to prevent anticompetitive practices that can harm consumers, such as price-fixing, monopolies, and mergers that reduce competition. In this essay, we will discuss antitrust laws and their impact on market competition, compare the antitrust laws in the US and China, and analyze some case studies. Antitrust laws have a significant impact on market competition. These laws promote competition by preventing anticompetitive practices that can harm consumers. For example, antitrust laws prohibit price-fixing, which is when competitors agree to set prices at a certain level. This practice can lead to higher prices for consumers and reduced competition. Antitrust laws also prevent monopolies, which occur when a single company dominates a market. Monopolies can lead to higher prices, reduced innovation, and poorer quality products. Antitrust laws also regulate mergers and acquisitions to ensure that they do not reduce competition in the market. The US has a comprehensive set of antitrust laws, including the Sherman Act, the Clayton Act, and the Federal Trade Commission Act. The Sherman Act prohibits anticompetitive practices such as price-fixing, ... ", "page_idx": 17}, {"type": "text", "text": "Judgment by GPT-4 (\u2717) ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Both assistants address the core aspects of antitrust laws, their impact on market competition, and provide comparisons between the US and China along with case studies. However, there are distinct differences in the presentation, depth, and accuracy of their responses. Assistant A Pros: 1. Clearly explains the purpose of antitrust laws and names specific laws in both the US and China. 2. Provides accurate and specific case studies: the 2000 Microsoft case in the US and the Qualcomm case in China in 2013. 3. Offers a concise conclusion summarizing the impact of antitrust laws. Assistant A Cons: ... Therefore, the final verdict is: [[A]] ", "page_idx": 17}, {"type": "text", "text": "1. Depth of Analysis: The response should not only state facts but also analyze the implications of antitrust laws, including their effects on market competition, economic efficiency, and consumer welfare. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "2. Practical Examples: The inclusion of real-world examples or case studies to illustrate how antitrust laws have been applied in the US and China, enhancing the understanding of the topic. ", "page_idx": 17}, {"type": "text", "text": "2. Accuracy: Information provided about antitrust laws, their implications, and the comparison between the US and China\u2019s regulatory frameworks must be factually correct and up-to-date. ", "page_idx": 17}, {"type": "text", "text": "Analysis of Response A: Response A provides a basic overview ... However, the response lacks depth in its analysis and does not provide a detailed comparison between the antitrust laws of the two countries beyond a general description. The case study is mentioned but not analyzed in detail, which limits the response\u2019s effectiveness in fully addressing the user query.   \nResponse B Score: 4   \nAnalysis of Response B: Response B offers a more detailed analysis of antitrust laws, including a discussion on the impact of these laws on market competition and a comparison between the US and Chinese antitrust frameworks. It provides specific examples of laws and regulations, such as the Sherman Act, the Clayton Act, and the Federal Trade Commission Act in the US, ...   \nComparison: Both responses address the user query by discussing antitrust laws and their impact on market com", "page_idx": 17}, {"type": "text", "text": "petition. Response B is stronger because ... ", "page_idx": 17}, {"type": "text", "text": "Response A Score: 4 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "499 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "01 Question: o the   \n502 paper\u2019s contributions and scope?   \n503 Answer: [Yes]   \n504 Justification: The key experiment results are summarized in the end of the introduction.   \n505 principal contributions are clearly stated in the end of the abstract.   \n506 Guidelines:   \n507 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n508 made in the paper.   \n509 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n510 contributions made in the paper and important assumptions and limitations. A No or   \n11 NA answer to this question will not be perceived well by the reviewers.   \n12 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n13 much the results can be expected to generalize to other settings.   \n514 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n15 are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "6 2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The limitations of this study are discussed in Section 6.1. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that   \n2 the paper has limitations, but those are not discussed in the paper.   \n3 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n24 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n5 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n6 model well-specification, asymptotic approximations only holding locally). The authors   \n7 should reflect on how these assumptions might be violated in practice and what the   \n8 implications would be.   \n9 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n0 only tested on a few datasets or with a few runs. In general, empirical results often   \ndepend on implicit assumptions, which should be articulated.   \n2 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \nFor example, a facial recognition algorithm may perform poorly when image resolution   \n4 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n5 used reliably to provide closed captions for online lectures because it fails to handle   \n6 technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n38 and how they scale with dataset size.   \n9 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n0 address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by   \n2 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n3 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n4 judgment and recognize that individual actions in favor of transparency play an impor  \n5 tant role in developing norms that preserve the integrity of the community. Reviewers   \n6 will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "547 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "548 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n549 a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "551 Justification: This study does not include theoretical results.   \n552 Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "563 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "64 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n65 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n66 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: All implementation details are provided in Appendix A.2. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "605 Answer: [Yes]   \n606 Justification: We provide a detailed README to describe how to reproduce the main results   \n607 of this study.   \n608 Guidelines:   \n609 \u2022 The answer NA means that paper does not include experiments requiring code.   \n610 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n611 public/guides/CodeSubmissionPolicy) for more details.   \n612 \u2022 While we encourage the release of code and data, we understand that this might not be   \n613 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n614 including code, unless this is central to the contribution (e.g., for a new open-source   \n615 benchmark).   \n616 \u2022 The instructions should contain the exact command and environment needed to run to   \n617 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n618 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n619 \u2022 The authors should provide instructions on data access and preparation, including how   \n620 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n621 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n622 proposed method and baselines. If only a subset of experiments are reproducible, they   \n623 should state which ones are omitted from the script and why.   \n624 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n625 versions (if applicable).   \n626 \u2022 Providing as much information as possible in supplemental material (appended to the   \n627 paper) is recommended, but including URLs to data and code is permitted.   \n628 6. Experimental Setting/Details   \n629 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n630 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n631 results?   \n632 Answer: [Yes]   \n633 Justification: All implementation details are provided in Appendix A.2.   \n634 Guidelines:   \n635 \u2022 The answer NA means that the paper does not include experiments.   \n636 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n637 that is necessary to appreciate the results and make sense of them.   \n638 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n639 material.   \n640 7. Experiment Statistical Significance   \n641 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n642 information about the statistical significance of the experiments?   \n643 Answer: [Yes]   \n644 Justification: we run every method for 3 independent trials.   \n645 Guidelines:   \n646 \u2022 The answer NA means that the paper does not include experiments.   \n647 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n648 dence intervals, or statistical significance tests, at least for the experiments that support   \n649 the main claims of the paper.   \n650 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n651 example, train/test split, initialization, random drawing of some parameter, or overall   \n652 run with given experimental conditions).   \n653 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n654 call to a library function, bootstrap, etc.)   \n655 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n656 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n657 of the mean.   \n658 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n659 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n660 of Normality of errors is not verified.   \n661 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n662 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n663 error rates).   \n664 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n665 they were calculated and reference the corresponding figures or tables in the text.   \n666 8. Experiments Compute Resources   \n667 Question: For each experiment, does the paper provide sufficient information on the com  \n668 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n669 the experiments?   \n670 Answer: [Yes]   \n671 Justification: We include the information of computational resources in Appendix A.2.   \n672 Guidelines:   \n673 \u2022 The answer NA means that the paper does not include experiments.   \n674 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n675 or cloud provider, including relevant memory and storage.   \n676 \u2022 The paper should provide the amount of compute required for each of the individual   \n677 experimental runs as well as estimate the total compute.   \n678 \u2022 The paper should disclose whether the full research project required more compute   \n679 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n680 didn\u2019t make it into the paper).   \n681 9. Code Of Ethics   \n682 Question: Does the research conducted in the paper conform, in every respect, with the   \n683 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n684 Answer: [Yes]   \n685 Justification: We reviewed the NeurIPS Code of Ethics.   \n686 Guidelines:   \n687 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n688 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n689 deviation from the Code of Ethics.   \n690 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n691 eration due to laws or regulations in their jurisdiction).   \n692 10. Broader Impacts   \n693 Question: Does the paper discuss both potential positive societal impacts and negative   \n694 societal impacts of the work performed?   \n695 Answer: [Yes]   \n696 Justification: We discuss the impacts on Section 7.   \n697 Guidelines:   \n698 \u2022 The answer NA means that there is no societal impact of the work performed.   \n699 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n700 impact or why the paper does not address societal impact.   \n701 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n702 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n703 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n704 groups), privacy considerations, and security considerations.   \n705 \u2022 The conference expects that many papers will be foundational research and not tied   \n706 to particular applications, let alone deployments. However, if there is a direct path to   \n707 any negative applications, the authors should point it out. For example, it is legitimate   \n708 to point out that an improvement in the quality of generative models could be used to   \n709 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n710 that a generic algorithm for optimizing neural networks could enable people to train   \n711 models that generate Deepfakes faster.   \n712 \u2022 The authors should consider possible harms that could arise when the technology is   \n713 being used as intended and functioning correctly, harms that could arise when the   \n714 technology is being used as intended but gives incorrect results, and harms following   \n715 from (intentional or unintentional) misuse of the technology.   \n716 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n717 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n718 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n719 feedback over time, improving the efficiency and accessibility of ML).   \n720 11. Safeguards   \n721 Question: Does the paper describe safeguards that have been put in place for responsible   \n722 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n723 image generators, or scraped datasets)?   \n724 Answer: [No]   \n725 Justification: This study poses no such risks.   \n726 Guidelines:   \n727 \u2022 The answer NA means that the paper poses no such risks.   \n728 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n729 necessary safeguards to allow for controlled use of the model, for example by requiring   \n730 that users adhere to usage guidelines or restrictions to access the model or implementing   \n731 safety filters.   \n732 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n733 should describe how they avoided releasing unsafe images.   \n734 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n735 not require this, but we encourage authors to take this into account and make a best   \n736 faith effort.   \n737 12. Licenses for existing assets   \n738 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n739 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n740 properly respected?   \n741 Answer: [NA]   \n742 Justification: We properly cited all benchmarks used in this study.   \n743 Guidelines:   \n744 \u2022 The answer NA means that the paper does not use existing assets.   \n745 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n746 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n747 URL.   \n748 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n749 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n750 service of that source should be provided.   \n751 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n752 package should be provided. For popular datasets, paperswithcode.com/datasets   \n753 has curated licenses for some datasets. Their licensing guide can help determine the   \n754 license of a dataset.   \n755 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n756 the derived asset (if it has changed) should be provided.   \n757 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n758 the asset\u2019s creators.   \n759 13. New Assets   \n760 Question: Are new assets introduced in the paper well documented and is the documentation   \n761 provided alongside the assets?   \n762 Answer: [Yes]   \n763 Justification: We provide a new evaluation training dataset.   \n764 Guidelines:   \n765 \u2022 The answer NA means that the paper does not release new assets.   \n766 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n767 submissions via structured templates. This includes details about training, license,   \n768 limitations, etc.   \n769 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n770 asset is used.   \n771 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n772 create an anonymized URL or include an anonymized zip file.   \n773 14. Crowdsourcing and Research with Human Subjects   \n774 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n775 include the full text of instructions given to participants and screenshots, if applicable, as   \n776 well as details about compensation (if any)?   \n777 Answer: [NA]   \n778 Justification: This study does not involve crowdsourcing nor research with human subjects.   \n779 Guidelines:   \n780 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n781 human subjects.   \n782 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n783 tion of the paper involves human subjects, then as much detail as possible should be   \n784 included in the main paper.   \n785 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n786 or other labor should be paid at least the minimum wage in the country of the data   \n787 collector.   \n788 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n789 Subjects   \n790 Question: Does the paper describe potential risks incurred by study participants, whether   \n791 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n792 approvals (or an equivalent approval/review based on the requirements of your country or   \n793 institution) were obtained?   \n794 Answer: [NA]   \n795 Justification: This study does not involve crowdsourcing nor research with human subjects.   \n796 Guidelines:   \n797 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n798 human subjects.   \n799 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n800 may be required for any human subjects research. If you obtained IRB approval, you   \n801 should clearly state this in the paper.   \n802 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n803 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n804 guidelines for their institution.   \n805 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n806 applicable), such as the institution conducting the review. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}]