[{"figure_path": "GYqs5Z4joA/figures/figures_3_1.jpg", "caption": "Figure 1: The pipeline of Jaccard Attention Spike Neural Network: Raw sEMG Data is first encoded into Spike Signals using ConvLIF. These signals pass through ConvLIF layers with N and 2N channels. The processed data then goes through the Spiking Jaccard Attention mechanism.", "description": "This figure illustrates the architecture of the proposed Spiking Jaccard Attention Neural Network.  Raw surface electromyography (sEMG) data is initially converted into spike trains using a convolutional leaky integrate-and-fire (ConvLIF) layer. These spike trains then pass through multiple ConvLIF layers with varying numbers of channels (N and 2N) to extract features.  The extracted features then undergo a Spiking Jaccard Attention mechanism before classification by a multi-layer perceptron (MLP) and finally a LIF classifier layer. A membrane potential memory component is also shown, highlighting a key element of the Source-Free Domain Adaptation strategy.", "section": "4.1 Jaccard Attentive Spiking Neural Network"}, {"figure_path": "GYqs5Z4joA/figures/figures_4_1.jpg", "caption": "Figure 2: Comparison of MA-SNN and Spiking Jaccard Attention Modules. MA-SNN [67] uses fully connected layers with pooling but lacks a querying mechanism, leading to continuous intermediate values and lower efficiency. Our Spiking Jaccard Attention uses spike values for intermediate representations, enhancing efficiency and accuracy.", "description": "This figure compares the proposed Spiking Jaccard Attention (SJA) module with the existing MA-SNN attention module.  MA-SNN uses fully connected layers and pooling, resulting in continuous intermediate values and reduced efficiency. In contrast, SJA utilizes spike values, which improves efficiency and accuracy by enabling direct computation of Jaccard similarity on the sparse spike trains.", "section": "4.1 Jaccard Attentive Spiking Neural Network"}, {"figure_path": "GYqs5Z4joA/figures/figures_5_1.jpg", "caption": "Figure 3: Computation flow of Spiking Source-Free Domain Adaptation. The process starts with selecting the k-nearest samples from the membrane potential memory using the Pearson correlation coefficient. Probabilistic Label Generation then produces pseudo-labels based on these k samples. Gradients are computed with Smooth NLL and KL divergence loss. The membrane potential memory list is updated at each epoch's end.", "description": "This figure illustrates the Spiking Source-Free Domain Adaptation (SSFDA) process.  Raw sEMG data is fed into the JASNN model. The model's membrane potential is recorded and stored in a 'Membrane Potential Memory List'.  For each new unlabeled sample, the k-nearest neighbors from this memory are found using Pearson correlation.  A probabilistic approach generates pseudo-labels based on these neighbors (using the mode or random selection).  The model is then trained using a loss function combining Smooth Negative Log-Likelihood (SNLL) and Kullback-Leibler (KL) divergence. The memory list is updated after each training epoch.", "section": "4.2 Spiking Source-Free Domain Adaptation based on Membrane Potential Memory"}, {"figure_path": "GYqs5Z4joA/figures/figures_8_1.jpg", "caption": "Figure 4: Comparison of performance before and after applying SSFDA for various methodologies: Figures 4a and 4b are Violin Plots demonstrating this disparity.", "description": "Violin plots showing the accuracy of three different domain adaptation methods (Ours, Duan et al., Huang et al.) before and after applying Spiking Source-Free Domain Adaptation (SSFDA) on two different scenarios.  The plots compare performance when the model is trained on forearm posture P1 and tested on forearm postures P2 (4a) and P3 (4b). The results illustrate the impact of SSFDA in improving robustness and generalizability across different forearm postures.", "section": "5.4 Result of Spiking Source-Free Domain Adaptation"}, {"figure_path": "GYqs5Z4joA/figures/figures_9_1.jpg", "caption": "Figure 5: Inference speed and RAM usage comparison between spike and float data for Raw Attention [60], Efficient Attention [54], and our Spiking Jaccard Attention: The first column shows inference time for float data, and the second for spike data. The third and fourth columns show RAM usage for these data types. The x-axis represents different data row counts, and the y-axis is logarithmic to highlight performance differences. Each experiment was conducted 100 times, with averaged results.", "description": "This figure compares the inference speed and RAM usage of three different attention mechanisms (Raw Attention, Efficient Attention, and Spiking Jaccard Attention) using both spike and float data types.  The results demonstrate that the Spiking Jaccard Attention method is significantly more efficient than the other two methods in terms of both speed and memory usage, especially as the amount of data increases.", "section": "5.5 Efficiency Analysis of Spiking Jaccard Attention"}, {"figure_path": "GYqs5Z4joA/figures/figures_16_1.jpg", "caption": "Figure 6: Overview of our dataset: the compilation contains sEMG data for ten distinct actions, each across three postures. Varied background colors represent distinct forearm postures, while the digits ranging from 0 to 9 correspond to specific gesture actions. The 'Rest' label at the top denotes a static hand gesture when no action is being performed.", "description": "This figure shows an overview of the dataset used in the SpGesture research.  It displays ten different hand gestures performed across three different forearm postures (horizontal, diagonal, and elevated). Each gesture is represented by a number (0-9), and each forearm posture has a unique color-coded background. The 'REST' column shows images of the hand in a relaxed state before and after performing each gesture.", "section": "3 Preliminaries"}, {"figure_path": "GYqs5Z4joA/figures/figures_17_1.jpg", "caption": "Figure 7: Summary of our collected data: The three postures on the left illustrate distinct preparatory arm actions. The central data graph represents the sEMG data captured from a subject under the three postures, with unique colors assigned to different channels. The data graph on the right showcases the acquired data after Root Mean Square (RMS) processing.", "description": "The figure shows a summary of the collected sEMG data. The left side shows three different forearm postures. The middle section shows the raw sEMG data from one subject for the three postures and the right section shows the RMS features extracted from the sEMG data.", "section": "A.1 Details of Data Collection"}, {"figure_path": "GYqs5Z4joA/figures/figures_19_1.jpg", "caption": "Figure 1: The pipeline of Jaccard Attention Spike Neural Network: Raw sEMG Data is first encoded into Spike Signals using ConvLIF. These signals pass through ConvLIF layers with N and 2N channels. The processed data then goes through the Spiking Jaccard Attention mechanism.", "description": "This figure illustrates the architecture of the proposed Jaccard Attentive Spiking Neural Network (JASNN).  Raw surface electromyography (sEMG) data is initially converted into spike signals using a convolutional leaky integrate-and-fire (ConvLIF) encoder. These spike signals then pass through multiple ConvLIF layers with varying numbers of channels (N and 2N), extracting features and increasing dimensionality. Finally, a Spiking Jaccard Attention mechanism is applied to focus on the most relevant features before classification.", "section": "4.1 Jaccard Attentive Spiking Neural Network"}, {"figure_path": "GYqs5Z4joA/figures/figures_21_1.jpg", "caption": "Figure 4: Comparison of performance before and after applying SSFDA for various methodologies: Figures 4a and 4b are Violin Plots demonstrating this disparity.", "description": "This figure presents a comparison of model performance before and after applying Spiking Source-Free Domain Adaptation (SSFDA) using three different methodologies. The violin plots show the distribution of accuracy across different subjects for two different scenarios: (a) comparing model trained on posture 1's performance on posture 2, and (b) comparing model trained on posture 1's performance on posture 3. The results demonstrate the effectiveness of SSFDA in improving model robustness by reducing the performance gap between different forearm postures.  The proposed method shows significantly improved accuracy and reduced variance compared to other methods.", "section": "5.4 Result of Spiking Source-Free Domain Adaptation"}, {"figure_path": "GYqs5Z4joA/figures/figures_22_1.jpg", "caption": "Figure 4: Comparison of performance before and after applying SSFDA for various methodologies: Figures 4a and 4b are Violin Plots demonstrating this disparity.", "description": "This figure compares the performance of three different pseudo-label generation methods (Ours, Duan et al., and Huang et al.) for Spiking Source-Free Domain Adaptation (SSFDA) before and after applying the adaptation technique.  Violin plots are used to show the distribution of accuracy across 14 subjects for two scenarios: when transferring models trained on forearm posture P1 to postures P2 (Figure 4a) and P3 (Figure 4b).  The plots demonstrate that the proposed method (Ours) shows a significant improvement in accuracy and reduced variability across subjects compared to the other methods, after applying SSFDA.", "section": "5.4 Result of Spiking Source-Free Domain Adaptation"}, {"figure_path": "GYqs5Z4joA/figures/figures_22_2.jpg", "caption": "Figure 1: The pipeline of Jaccard Attention Spike Neural Network: Raw sEMG Data is first encoded into Spike Signals using ConvLIF. These signals pass through ConvLIF layers with N and 2N channels. The processed data then goes through the Spiking Jaccard Attention mechanism.", "description": "This figure illustrates the architecture of the proposed Jaccard Attentive Spiking Neural Network (JASNN).  Raw surface electromyography (sEMG) data is initially converted into spike signals using a convolutional leaky integrate-and-fire (ConvLIF) encoder. These spike signals then pass through multiple ConvLIF layers, with varying numbers of channels (N and 2N). The processed data is subsequently fed into a Spiking Jaccard Attention module, which enhances the network's ability to focus on relevant features and improve its performance.", "section": "4.1 Jaccard Attentive Spiking Neural Network"}]