[{"figure_path": "BCA9NMZkLS/figures/figures_0_1.jpg", "caption": "Figure 1: The average 1-shot performance across four groups of NLP tasks We compare the scaling abilities of DeBERTa (four sizes in red) with GPT-3 (eight sizes in blue). Even though these models rely on different training objectives, they scale in a similar log-linear manner overall. Yet, on a task-by-task basis, the pretraining methods lead to substantial differences between them.", "description": "This figure compares the scaling performance of DeBERTa and GPT-3 models on four NLP tasks: language understanding, language modeling, translation, and question answering.  The x-axis represents the model size (in billions of parameters), and the y-axis represents the average performance across the tasks. The figure shows that both DeBERTa and GPT-3 exhibit a similar log-linear scaling behavior, meaning their performance increases with model size. However, there are significant differences in the performance of the two models on individual tasks, suggesting that the pre-training methods have a substantial impact on task-specific performance. ", "section": "Scaling of in-context learning performance (1-shot)"}, {"figure_path": "BCA9NMZkLS/figures/figures_1_1.jpg", "caption": "Figure 2: Illustration of the proposed methods for using a masked language model for text generation and ranking. We show how to adapt a masked language model for in-context-learning tasks through simple input reformatting, requiring no additional training. LEFT: Text generation is achieved by 1) appending [MASK] tokens to the input prompt, 2) predicting the next token for the first mask, and 3) iteratively appending new masks and predicting tokens. RIGHT: A similar approach is used to retrieve a pseudo-log-likelihood score of a text sequence that can be used to rank multiple sequences by their individual likelihoods. Both methods maintain the model's original architecture while enabling new capabilities through careful input formatting.", "description": "This figure illustrates the proposed methods for text generation and ranking using a masked language model.  The left side demonstrates text generation by iteratively replacing masked tokens ([MASK]) with predicted tokens. The right side shows how to rank text sequences by calculating pseudo log-likelihood scores, summing the individual likelihoods of each masked token prediction within the sequence.  Importantly, both methods are based on modifying the input prompt and require no model retraining.", "section": "Method: text generation and ranking with masked language models"}, {"figure_path": "BCA9NMZkLS/figures/figures_3_1.jpg", "caption": "Figure 1: The average 1-shot performance across four groups of NLP tasks. We compare the scaling abilities of DeBERTa (four sizes in red) with GPT-3 (eight sizes in blue). Even though these models rely on different training objectives, they scale in a similar log-linear manner overall. Yet, on a task-by-task basis, the pretraining methods lead to substantial differences between them.", "description": "This figure compares the performance of DeBERTa and GPT-3 on four NLP tasks (language understanding, language modeling, translation, and question answering) using a 1-shot setting.  It shows how performance scales with the size (number of parameters) of each model.  While both models show similar log-linear scaling, their relative performance varies across the different tasks, highlighting the impact of the different pre-training methods.", "section": "Scaling of in-context learning performance (1-shot)"}, {"figure_path": "BCA9NMZkLS/figures/figures_5_1.jpg", "caption": "Figure 1: The average 1-shot performance across four groups of NLP tasks. We compare the scaling abilities of DeBERTa (four sizes in red) with GPT-3 (eight sizes in blue). Even though these models rely on different training objectives, they scale in a similar log-linear manner overall. Yet, on a task-by-task basis, the pretraining methods lead to substantial differences between them.", "description": "This figure compares the scaling performance of DeBERTa and GPT-3 across four NLP task categories (language understanding, language modeling, translation, and question answering).  It shows that despite their different training objectives (masked vs. causal language modeling), both models exhibit similar log-linear scaling trends as model size increases. However, the figure also highlights that their performance varies considerably across specific tasks, suggesting that the pretraining methods significantly influence the models' capabilities.", "section": "Scaling of in-context learning performance (1-shot)"}, {"figure_path": "BCA9NMZkLS/figures/figures_19_1.jpg", "caption": "Figure 5: The average performance on the SuperGLUE benchmarks as a function of number of shots As opposed to the other SuperGLUE few-shot results, where we select the number of shots for each subtask according to the performance on its training split, here all subtasks are evaluated with the same number of shots. In this way, we can compare DeBERTa 1.5B directly to Figure 3.8 from Brown et al. (2020), which gives the same evaluation for GPT 175B (unfortunately not for smaller, more comparable, models).", "description": "This figure shows the average performance across all SuperGLUE tasks with varying numbers of in-context examples (shots).  It compares DeBERTa's performance to that of GPT-3, highlighting the similar trend despite using different model architectures. The consistent performance across all tasks demonstrates the effectiveness of in-context learning with masked language models.", "section": "4.1 Language understanding (SuperGLUE)"}, {"figure_path": "BCA9NMZkLS/figures/figures_20_1.jpg", "caption": "Figure 1: The average 1-shot performance across four groups of NLP tasks We compare the scaling abilities of DeBERTa (four sizes in red) with GPT-3 (eight sizes in blue). Even though these models rely on different training objectives, they scale in a similar log-linear manner overall. Yet, on a task-by-task basis, the pretraining methods lead to substantial differences between them.", "description": "This figure compares the performance scaling of DeBERTa and GPT-3 across four NLP task categories (language understanding, language modeling, translation, and question answering) using a single example (1-shot).  Despite different training objectives, both models show similar log-linear scaling with model size. However, individual task performances reveal significant differences due to the distinct pretraining methods.", "section": "Scaling of in-context learning performance (1-shot)"}]