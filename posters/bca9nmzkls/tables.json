[{"figure_path": "BCA9NMZkLS/tables/tables_6_1.jpg", "caption": "Table 1: Natural language understanding results All results in this table are evaluated with accuracy (higher is better). The table shows the performance of the largest available DeBERTa (1.4 billion parameters) and of a similarly-sized GPT-3 model, the best results are boldfaced. The average score is calculated over averaged task scores (in case a task uses more than one metric).", "description": "This table compares the performance of the largest DeBERTa model (1.4 billion parameters) and a similarly sized GPT-3 model on eight natural language understanding tasks from the SuperGLUE benchmark.  The tasks assess various aspects of language understanding, including reading comprehension, textual entailment, and commonsense reasoning.  Performance is measured by accuracy, with the best results for each task highlighted in bold.  An average accuracy score is also provided, combining the results across all tasks.", "section": "4.1 Language understanding (SuperGLUE)"}, {"figure_path": "BCA9NMZkLS/tables/tables_7_1.jpg", "caption": "Table 2: Results of text completion, language modeling and Winograd-style tasks All tasks are measured with accuracy, we show the performance of the largest available DeBERTa (1.4 billion parameters) and of a similarly-sized GPT-3 model, the best results are boldfaced.", "description": "This table compares the performance of the largest DeBERTa model (1.4 billion parameters) and a similarly sized GPT-3 model on four language modeling tasks: HellaSwag, StoryCloze, Winograd, and Winogrande.  The results are presented for three different scenarios: zero-shot (no examples), one-shot (one example), and few-shot (multiple examples). Accuracy is used as the evaluation metric, with the best scores for each task and scenario highlighted in bold.", "section": "4.2 Language modeling, Winograd-style and text completion tasks"}, {"figure_path": "BCA9NMZkLS/tables/tables_7_2.jpg", "caption": "Table 3: Machine translation results We report SacreBLEU scores (Post, 2018) with signature BLEU+case.mixed+numrefs.1+smooth.exp+tok.intl+version.1.2.20 (higher is better). The table shows the performance of the largest available DeBERTa (1.4 billion parameters) and of a similarly-sized GPT-3 model, the best results are boldfaced.", "description": "This table presents the results of machine translation experiments comparing DeBERTa and GPT-3 models.  The evaluation metric is SacreBLEU, and the table shows scores for different language pairs (DE-EN, EN-DE, FR-EN, EN-FR, RO-EN, EN-RO) under zero-shot, one-shot, and few-shot learning conditions.  The best scores for each language pair and setting are highlighted in bold.", "section": "4.3 Machine Translation"}, {"figure_path": "BCA9NMZkLS/tables/tables_8_1.jpg", "caption": "Table 4: Closed-book question answering and commonsense reasoning The first three tasks are measured with the exact-match accuracy and the rest is measured with classification accuracy. The table shows the performance of the largest available DeBERTa (1.4 billion parameters) and of a similarly-sized GPT-3 model, the best results are boldfaced. A detailed description of the evaluation method is given in Appendix E, full results are in Appendix F.", "description": "This table presents a comparison of DeBERTa and GPT-3's performance on seven question-answering and commonsense reasoning tasks.  The tasks are categorized into closed-book question answering and commonsense reasoning.  For each model and task, performance is measured using either exact match accuracy or classification accuracy, depending on the nature of the task. The table shows the performance for zero-shot, one-shot, and few-shot settings to highlight how the models' performance changes with the number of in-context examples provided.", "section": "4.4 Closed-book question answering and commonsense reasoning"}, {"figure_path": "BCA9NMZkLS/tables/tables_15_1.jpg", "caption": "Table 1: Natural language understanding results All results in this table are evaluated with accuracy (higher is better). The table shows the performance of the largest available DeBERTa (1.4 billion parameters) and of a similarly-sized GPT-3 model, the best results are boldfaced. The average score is calculated over averaged task scores (in case a task uses more than one metric).", "description": "This table presents a comparison of the performance of DeBERTa and GPT-3 on several natural language understanding tasks from the SuperGLUE benchmark.  The results are broken down by model size (1.4B parameters for DeBERTa and a similarly sized GPT-3 model) and evaluation type (0-shot, 1-shot, and few-shot).  The average accuracy across all tasks is reported, along with the individual task accuracies.  Higher scores indicate better performance.", "section": "4.1 Language understanding (SuperGLUE)"}, {"figure_path": "BCA9NMZkLS/tables/tables_16_1.jpg", "caption": "Table 1: Natural language understanding results All results in this table are evaluated with accuracy (higher is better). The table shows the performance of the largest available DeBERTa (1.4 billion parameters) and of a similarly-sized GPT-3 model, the best results are boldfaced. The average score is calculated over averaged task scores (in case a task uses more than one metric).", "description": "This table presents the results of evaluating DeBERTa and GPT-3 models on various natural language understanding tasks from the SuperGLUE benchmark.  It compares their performance using three different shot settings (0-shot, 1-shot, few-shot) and shows the accuracy for each model on each task, highlighting the best performing model for each task. The average accuracy across all tasks is also provided.", "section": "4.1 Language understanding (SuperGLUE)"}, {"figure_path": "BCA9NMZkLS/tables/tables_16_2.jpg", "caption": "Table 1: Natural language understanding results All results in this table are evaluated with accuracy (higher is better). The table shows the performance of the largest available DeBERTa (1.4 billion parameters) and of a similarly-sized GPT-3 model, the best results are boldfaced. The average score is calculated over averaged task scores (in case a task uses more than one metric).", "description": "This table presents the results of evaluating DeBERTa and GPT-3 on natural language understanding tasks from the SuperGLUE benchmark.  The tasks are assessed using accuracy.  The table shows the performance of the largest DeBERTa model (1.4 billion parameters) compared to a similarly sized GPT-3 model.  Results are shown for zero-shot, one-shot, and few-shot settings, with the best performance in each setting bolded. An average accuracy score is also provided.", "section": "4.1 Language understanding (SuperGLUE)"}, {"figure_path": "BCA9NMZkLS/tables/tables_17_1.jpg", "caption": "Table 5: Ablation study of different generation methods applied to DeBERTa Evaluated using one-shot setting and the largest DeBERTa 1.5B model on German-to-English translation with SacreBLEU score.", "description": "This table presents the results of an ablation study on different text generation methods using the DeBERTa 1.5B model for German-to-English translation.  The study compares autoregressive generation with varying numbers of masks (1, 2, 3, and 4) against Markov-chain Monte Carlo methods (with random and mask initializations). The results are measured using the SacreBLEU score with a one-shot setting, showing the impact of the number of masks on translation quality.", "section": "B Ablation study of text generation"}, {"figure_path": "BCA9NMZkLS/tables/tables_18_1.jpg", "caption": "Table 6: Ablation study of different ranking methods applied to DeBERTa Evaluated using zero-shot setting and the largest DeBERTa 1.5B model on ReCORD.", "description": "This table presents the results of an ablation study comparing different ranking methods using the DeBERTa 1.5B model on the ReCoRD dataset.  The study evaluates the performance of different pseudo log-likelihood approaches, varying the number of masked tokens, and compares them to a method by Kauf and Ivanova (2023) and the exact unidirectional log-likelihood calculation. The performance is measured using Exact Match (EM) and F1 scores, providing insight into the impact of masking strategies on ranking accuracy.", "section": "C Ablation study of ranking implementation"}, {"figure_path": "BCA9NMZkLS/tables/tables_25_1.jpg", "caption": "Table 7: Results of all evaluations performed in this paper The second and third column shots the dataset splits and evaluation metrics, both of them replicating the GPT-3 evaluation setup. Note the BLEU scores used for evaluating translation are SacreBLEU scores with signature BLEU+case.mixed+numrefs.1+smooth.exp+tok.intl+version.1.2.20.", "description": "This table presents the complete results of all the experiments performed in the paper.  It shows the performance of DeBERTa models (with different sizes) on various NLP tasks, categorized into different groups (language understanding, language modeling, translation, question answering).  For each task, the table shows results broken down by the number of shots (0-shot, 1-shot, and few-shot) and the evaluation metrics used. The metrics vary depending on the specific task (accuracy, F1 score, BLEU score, etc.). The table also shows results using a comparable GPT-3 model, allowing for direct comparison.", "section": "F All results"}]