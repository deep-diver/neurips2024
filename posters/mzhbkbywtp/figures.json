[{"figure_path": "mZHbkbYWTp/figures/figures_4_1.jpg", "caption": "Figure 1: The invalidity of the unbiasedness condition. (a) A toy example illustrating estimation bias. The black line represents the dynamics of a partially observable process, xt, while the red line shows the state estimate, \u00eet, biased by random internal fluctuations from the noise term nt (orange arrow). (b-c) E[xt|\u00eet], for t = 8, as a function of \u00eet for \u03c3\u03b7 = 0.0, 0.6, respectively, using the solutions from [1]. The conditional expectation E[xt|\u00eet] is computed through Monte Carlo simulations (dots \u00b1 error bars: mean \u00b1 1std). The gray straight line represents the identity line, where E[xt|\u00eet] = \u00eet.", "description": "This figure demonstrates the bias in state estimation when internal noise is present. Panel (a) shows a simple example where internal noise causes the estimated state (red line) to deviate significantly from the true state (black line), resulting in a biased estimate. Panels (b) and (c) quantify this bias by plotting the conditional expectation of the true state given the estimated state for different levels of internal noise. The deviation from the identity line (gray line) indicates the magnitude of the bias, which is more pronounced with higher internal noise.", "section": "State-of-the-Art Solutions for the LQMG Model: Causes of Suboptimality"}, {"figure_path": "mZHbkbYWTp/figures/figures_6_1.jpg", "caption": "Figure 2: Enhanced performance and different solutions with internal noise. (a) Expected accumulated cost E[J], computed by averaging the quantity from Eq. 3 over 50k trials, as a function of the internal noise strength \u03c3\u03b7, for TOD [1] and GD (Section 3.1) algorithms (mean \u00b1 1SEM from Monte Carlo simulations, error bars not visible as too small). The expected value aligns with our theoretical estimate of E[J], as derived in Section 3.1. (b-c) Optimal control and filter gains, Lt and Kt, for TOD and GD and algorithms. We also present the solutions derived from the analytical counterpart of the numerical GD algorithm to demonstrate that they match the optimal solutions (\u2018Fixed Point Optimization with Moments Propagation\u2019 \u2013 FPOMP \u2013 algorithm, see Section 3.3).", "description": "This figure compares the performance of three algorithms (TOD, GD, and FPOMP) for solving an optimal control problem with internal noise.  Panel (a) shows the expected accumulated cost as a function of internal noise strength for each algorithm, demonstrating GD's superiority. Panels (b) and (c) display the optimal control and filter gains (Lt and Kt) over time for each algorithm, highlighting the differences in their gain modulation strategies with varying internal noise levels. The results illustrate that the GD algorithm achieves lower costs by adapting its gains more effectively to internal noise compared to TOD.", "section": "Experiments: Enhanced Performance with the GD Algorithm"}, {"figure_path": "mZHbkbYWTp/figures/figures_7_1.jpg", "caption": "Figure 2: Enhanced performance and different solutions with internal noise. (a) Expected accumulated cost E[J], computed by averaging the quantity from Eq. 3 over 50k trials, as a function of the internal noise strength \u03c3\u03b7, for TOD [1] and GD (Section 3.1) algorithms (mean \u00b1 1SEM from Monte Carlo simulations, error bars not visible as too small). The expected value aligns with our theoretical estimate of E[J], as derived in Section 3.1. (b-c) Optimal control and filter gains, Lt and Kt, for TOD and GD and algorithms. We also present the solutions derived from the analytical counterpart of the numerical GD algorithm to demonstrate that they match the optimal solutions (\u2018Fixed Point Optimization with Moments Propagation\u2019 \u2013 FPOMP \u2013 algorithm, see Section 3.3).", "description": "This figure compares the performance of the TOD and GD algorithms in a one-dimensional reaching task with varying levels of internal noise.  Panel (a) shows the accumulated cost for both algorithms, demonstrating that GD significantly outperforms TOD, especially with higher internal noise. Panels (b) and (c) illustrate how the optimal control and filter gains (Lt and Kt) change as internal noise increases, revealing differences in how the two algorithms adapt.  The inclusion of FPOMP results further validates the GD algorithm's accuracy.", "section": "Experiments: Enhanced Performance with the GD Algorithm"}, {"figure_path": "mZHbkbYWTp/figures/figures_14_1.jpg", "caption": "Figure 1: The invalidity of the unbiasedness condition. (a) A toy example illustrating estimation bias. The black line represents the dynamics of a partially observable process, xt, while the red line shows the state estimate, \u00eet, biased by random internal fluctuations from the noise term nt (orange arrow). (b-c) E[xt|\u00eet], for t = 8, as a function of \u00eet for \u03c3\u03b7 = 0.0, 0.6, respectively, using the solutions from [1]. The conditional expectation E[xt|\u00eet] is computed through Monte Carlo simulations (dots \u00b1 error bars: mean \u00b1 1std). The gray straight line represents the identity line, where E[xt|\u00eet] = \u00eet.", "description": "This figure demonstrates the issue of unbiasedness in the estimation process when internal noise is present. Panel (a) shows a toy example illustrating how internal noise can bias the state estimate. Panels (b) and (c) show the conditional expectation E[xt|\u00eet] as a function of the state estimate \u00eet for different levels of internal noise (\u03c3\u03b7 = 0.0 and \u03c3\u03b7 = 0.6). The results show that the unbiasedness condition E[xt|\u00eet] = \u00eet does not hold in the presence of internal noise.", "section": "State-of-the-Art Solutions for the LQMG Model: Causes of Suboptimality"}, {"figure_path": "mZHbkbYWTp/figures/figures_17_1.jpg", "caption": "Figure 2: Enhanced performance and different solutions with internal noise. (a) Expected accumulated cost E[J], computed by averaging the quantity from Eq. 3 over 50k trials, as a function of the internal noise strength \u03c3<sub>\u03b7</sub>, for TOD [1] and GD (Section 3.1) algorithms (mean \u00b1 1SEM from Monte Carlo simulations, error bars not visible as too small). The expected value aligns with our theoretical estimate of E[J], as derived in Section 3.1. (b-c) Optimal control and filter gains, Lt and Kt, for TOD and GD and algorithms. We also present the solutions derived from the analytical counterpart of the numerical GD algorithm to demonstrate that they match the optimal solutions (\u2018Fixed Point Optimization with Moments Propagation\u2019 \u2013 FPOMP \u2013 algorithm, see Section 3.3).", "description": "This figure compares the performance of the TOD and GD algorithms in solving the Linear-Quadratic-Multiplicative-Gaussian (LQMG) optimal control problem with internal noise. Panel (a) shows that the GD algorithm consistently achieves a lower expected accumulated cost than the TOD algorithm across different levels of internal noise. Panels (b) and (c) illustrate how the optimal control (L<sub>t</sub>) and filter (K<sub>t</sub>) gains vary as a function of internal noise for both algorithms, revealing distinct modulation patterns.  The inclusion of FPOMP results demonstrates the numerical GD and analytical methods produce near-identical results.", "section": "Experiments: Enhanced Performance with the GD Algorithm"}, {"figure_path": "mZHbkbYWTp/figures/figures_18_1.jpg", "caption": "Figure 2: Enhanced performance and different solutions with internal noise. (a) Expected accumulated cost E[J], computed by averaging the quantity from Eq. 3 over 50k trials, as a function of the internal noise strength \u03c3 \u03b7 , for TOD [1] and GD (Section 3.1) algorithms (mean \u00b1 1SEM from Monte Carlo simulations, error bars not visible as too small). The expected value aligns with our theoretical estimate of E[J], as derived in Section 3.1. (b-c) Optimal control and filter gains, Lt and Kt, for TOD and GD and algorithms. We also present the solutions derived from the analytical counterpart of the numerical GD algorithm to demonstrate that they match the optimal solutions (\u2018Fixed Point Optimization with Moments Propagation\u2019 \u2013 FPOMP \u2013 algorithm, see Section 3.3).", "description": "This figure compares the performance of the TOD and GD algorithms in terms of accumulated cost and control/filter gains across different levels of internal noise. The results indicate a substantial performance improvement of GD algorithm compared to the TOD, particularly with higher internal noise, while providing optimal control and filter gain modulations.", "section": "Experiments: Enhanced Performance with the GD Algorithm"}, {"figure_path": "mZHbkbYWTp/figures/figures_19_1.jpg", "caption": "Figure 7: Eigenvector decomposition of the dynamics. We show here a qualitative representation of the eigenvectors of the matrix M<sub>t</sub> in the plane (\u0393<sub>t</sub>, \u03a9<sub>t</sub>). The black arrow represents the \"shared\" eigenvector \n<binary data, 1 bytes><binary data, 1 bytes><binary data, 1 bytes><binary data, 1 bytes>, while the blue (green) arrow represents \u03c9<sub>2</sub> for TOD (GD) solution. Note that the optimal L<sub>t</sub> are negative, while the optimal K<sub>t</sub> are positive (Fig. 2).", "description": "This figure shows a qualitative representation of the eigenvectors of the matrix M<sub>t</sub> in the plane (\u0393<sub>t</sub>, \u03a9<sub>t</sub>).  The eigenvectors' angles are used to explain how the optimal control and filter gains modulate with internal noise (\u03c3<sub>\u03b7</sub>) in the GD algorithm, leading to better noise filtering and generalization compared to the TOD algorithm.  The black arrow represents the \"shared\" eigenvector, while blue and green arrows represent the second eigenvector for the TOD and GD algorithms, respectively. Note that the optimal L<sub>t</sub> are negative, and K<sub>t</sub> are positive.", "section": "A.5.2 One-Dimensional Case: Understanding the Qualitative Differences"}, {"figure_path": "mZHbkbYWTp/figures/figures_19_2.jpg", "caption": "Figure 8: Enhanced performance when optimizing control at fixed filter gains and zero internal noise. We plot the expected accumulated cost E[J], computed by averaging the quantity from Eq. 3 over 50k trials, as a function of the scaling matrix D, with error bars (mean \u00b1 1SEM from Monte Carlo simulations, error bars not visible as too small), for the two algorithms TOD and GD.", "description": "The figure shows the expected accumulated cost (E[J]) as a function of the scaling matrix D for two different algorithms: TOD and GD.  The results demonstrate that GD outperforms TOD, especially when the filter gains are held constant at suboptimal values.  This highlights the importance of the unbiasedness condition for accurate cost estimation and control.", "section": "3.2 Experiments: Enhanced Performance with the GD Algorithm"}, {"figure_path": "mZHbkbYWTp/figures/figures_22_1.jpg", "caption": "Figure 9: High-dimensional task. (a) Expected accumulated cost as a function of \u03c3<sub>\u03b7</sub> for TOD (blue dots) and GD (green dots) algorithms. We see that even in this high-dimensional task, GD solutions outperform the ones from [1]. To compute the expected cost, we used Algorithm 1 (but the results are confirmed by Monte Carlo simulations). (b) Pseudo-determinant of the control gains L (averaged over time), denoted as ||L||<sub>p</sub> as a function of \u03c3<sub>\u03b7</sub> for TOD (blue dots) and GD (green dots) algorithms.", "description": "This figure shows the results of a high-dimensional experiment (m=10, p=4, k=10) comparing the performance of the proposed GD algorithm to the TOD algorithm from [1], in terms of accumulated cost (a) and control gain magnitude (b), as a function of internal noise (\u03c3<sub>\u03b7</sub>).  The GD algorithm demonstrates consistently lower costs and a more nuanced modulation of control gains in response to varying internal noise levels.", "section": "3.2 Experiments: Enhanced Performance with the GD Algorithm"}, {"figure_path": "mZHbkbYWTp/figures/figures_27_1.jpg", "caption": "Figure 10: Accumulated cost difference. Difference of E[J] for GD and FPOMP solutions (computed by averaging the quantity from Eq. 3 over 50k trials), as a function of \u03c3\u03b7, with error bars (SEM).", "description": "This figure compares the accumulated costs obtained from the Gradient Descent (GD) and Fixed Point Optimization with Moments Propagation (FPOMP) algorithms across various levels of internal noise (\u03c3<sub>\u03b7</sub>). The difference in accumulated costs (E[J<sub>GD</sub> - J<sub>FPOMP</sub>]) is plotted against the internal noise levels. The shaded area represents the standard error of the mean (SEM).  The plot demonstrates that the two algorithms yield very similar results in terms of accumulated cost.", "section": "A.8.1 One-Dimensional Case"}, {"figure_path": "mZHbkbYWTp/figures/figures_28_1.jpg", "caption": "Figure 2: Enhanced performance and different solutions with internal noise. (a) Expected accumulated cost E[J], computed by averaging the quantity from Eq. 3 over 50k trials, as a function of the internal noise strength \u03c3\u03b7, for TOD [1] and GD (Section 3.1) algorithms (mean \u00b1 1SEM from Monte Carlo simulations, error bars not visible as too small). The expected value aligns with our theoretical estimate of E[J], as derived in Section 3.1. (b-c) Optimal control and filter gains, Lt and Kt, for TOD and GD and algorithms. We also present the solutions derived from the analytical counterpart of the numerical GD algorithm to demonstrate that they match the optimal solutions (\u2018Fixed Point Optimization with Moments Propagation\u2019 \u2013 FPOMP \u2013 algorithm, see Section 3.3).", "description": "This figure compares the performance of three algorithms (TOD, GD, and FPOMP) in solving an optimal control problem with internal noise. Panel (a) shows the expected accumulated cost as a function of internal noise strength for each algorithm. Panels (b) and (c) show the optimal control and filter gains, respectively, as a function of internal noise strength for each algorithm. The figure demonstrates that the GD and FPOMP algorithms significantly outperform the TOD algorithm, especially in the presence of internal noise.", "section": "Experiments: Enhanced Performance with the GD Algorithm"}]