{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is All you Need", "publication_date": "2017-12-03", "reason": "This paper introduced the Transformer architecture, which is the foundation of many modern time-series forecasting models, including the one proposed in this paper."}, {"fullname_first_author": "Haixu Wu", "paper_title": "Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting", "publication_date": "2021-12-03", "reason": "This paper proposed the Autoformer model, which is a state-of-the-art time-series forecasting model that this paper builds upon and compares against."}, {"fullname_first_author": "Yuqi Nie", "paper_title": "A Time Series is Worth 64 Words: Long-term Forecasting with Transformers", "publication_date": "2023-05-01", "reason": "This paper introduced the PatchTST model, which uses a multi-scale patch design, a method that is also used and compared against in this paper."}, {"fullname_first_author": "Abhimanyu Das", "paper_title": "A decoder-only foundation model for time-series forecasting", "publication_date": "2023-10-26", "reason": "This paper introduced the TimesFM model, a pre-trained universal time series foundation model that this paper benchmarks against."}, {"fullname_first_author": "Gerald Woo", "paper_title": "Unified training of universal time series forecasting transformers", "publication_date": "2024-02-08", "reason": "This paper introduced the MOIRAI model, another pre-trained universal time series foundation model that this paper benchmarks against."}]}