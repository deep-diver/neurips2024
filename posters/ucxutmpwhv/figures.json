[{"figure_path": "ucXUtMPWhv/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of the ElasTST Architecture. ElasTST employs (a) structured attention masks for placeholders to ensure consistent outputs across varied forecasting horizons. It incorporates (b) tunable ROPE customized to time series periodicities, enhancing its robustness. The architecture also integrates a (c) multi-scale patch assembly that merges fine-grained and coarse-grained details for improved forecasting accuracy. Furthermore, we implement (d) training horizon reweighting scheme during the training phase, which effectively simulates random sampling of forecasting horizons, reducing the need for additional sampling efforts.", "description": "This figure provides a detailed overview of the ElasTST architecture, highlighting its key components: structured attention masks for handling varied forecasting horizons, tunable rotary position embedding (ROPE) for adapting to time series periodicities, multi-scale patch assembly for integrating fine-grained and coarse-grained information, and a horizon reweighting scheme for efficient training across multiple horizons. Each component is illustrated with a subfigure, explaining its role in achieving robust and consistent forecasting performance.", "section": "3 Elastic Time-Series Transformers"}, {"figure_path": "ucXUtMPWhv/figures/figures_7_1.jpg", "caption": "Figure 2: Performance of trained once and inference over varying forecasting horizons. Models except TimesFM and MOIRAI are trained with a forecasting horizon of 720 and tasked with predicting across multiple horizons. A vertical red dashed line distinguishes between their seen horizons (96, 192, 336, 720) and unseen horizon (1024). We use a dashed line to denote the datasets on which the model was pre-trained, e.g., both TimesFM and MOIRAI have leveraged Traffic datasets for their pre-training. The ETT encompasses averaged results from datasets ETTh1, ETTh2, ETTm1, and ETTm2. Models lack inherent elasticity use a truncation strategy for shorter forecasts, and the foundation models use their pre-trained checkpoints and recommended configurations for inference.", "description": "This figure compares the performance of various time series forecasting models across different forecasting horizons.  Models were trained on a horizon of 720, then tested on shorter and longer horizons to assess their robustness and ability to generalize. The figure shows that ElasTST maintains consistent accuracy across all horizons, outperforming other models which struggle when predicting beyond their training horizon.", "section": "4.1 Main Results"}, {"figure_path": "ucXUtMPWhv/figures/figures_7_2.jpg", "caption": "Figure 2: Performance of trained once and inference over varying forecasting horizons. Models except TimesFM and MOIRAI are trained with a forecasting horizon of 720 and tasked with predicting across multiple horizons. A vertical red dashed line distinguishes between their seen horizons (96, 192, 336, 720) and unseen horizon (1024). We use a dashed line to denote the datasets on which the model was pre-trained, e.g., both TimesFM and MOIRAI have leveraged Traffic datasets for their pre-training. The ETT encompasses averaged results from datasets ETTh1, ETTh2, ETTm1, and ETTm2. Models lack inherent elasticity use a truncation strategy for shorter forecasts, and the foundation models use their pre-trained checkpoints and recommended configurations for inference.", "description": "This figure compares the performance of ElasTST and other models across various forecasting horizons, including unseen horizons. It demonstrates ElasTST's robustness and ability to generalize well compared to models trained for specific horizons.", "section": "4.1 Main Results"}, {"figure_path": "ucXUtMPWhv/figures/figures_8_1.jpg", "caption": "Figure 2: Performance of trained once and inference over varying forecasting horizons. Models except TimesFM and MOIRAI are trained with a forecasting horizon of 720 and tasked with predicting across multiple horizons. A vertical red dashed line distinguishes between their seen horizons (96, 192, 336, 720) and unseen horizon (1024). We use a dashed line to denote the datasets on which the model was pre-trained, e.g., both TimesFM and MOIRAI have leveraged Traffic datasets for their pre-training. The ETT encompasses averaged results from datasets ETTh1, ETTh2, ETTm1, and ETTm2. Models lack inherent elasticity use a truncation strategy for shorter forecasts, and the foundation models use their pre-trained checkpoints and recommended configurations for inference.", "description": "This figure compares the performance of ElasTST and other time series forecasting models across various forecasting horizons, both seen and unseen during training.  It highlights ElasTST's ability to maintain strong performance across different horizons without needing per-horizon training.", "section": "4.1 Main Results"}, {"figure_path": "ucXUtMPWhv/figures/figures_8_2.jpg", "caption": "Figure 10: The performance of difference patch size selection.", "description": "The figure shows the effect of different patch sizes on forecasting performance across various forecasting horizons.  It compares single patch sizes (8, 16, 32, 64) with multi-patch configurations (combining these sizes). The results suggest that a multi-patch approach (e.g., 8, 16, and 32) generally outperforms single patch sizes, achieving a better balance between capturing short-term dynamics and long-term trends. However, adding very large patches does not always improve performance and might even be counterproductive.", "section": "4.2 Ablation Study"}, {"figure_path": "ucXUtMPWhv/figures/figures_9_1.jpg", "caption": "Figure 6: Impact of forecasting horizon selection during the training phase.", "description": "This figure visualizes the impact of different training horizon lengths on forecasting performance across various inference horizons. It compares the results of training with fixed horizons (96, 192, 336, 720) against the results of training with randomly sampled horizons and the proposed reweighting scheme. The results are displayed using CRPS (Continuous Ranked Probability Score) as the evaluation metric for five different datasets.  The figure aims to demonstrate the effectiveness of the proposed reweighting approach in simulating varied-horizon training using a single fixed training horizon, thus reducing computational costs and enhancing the model's generalization ability across various forecast lengths.", "section": "4 Experiments"}, {"figure_path": "ucXUtMPWhv/figures/figures_17_1.jpg", "caption": "Figure 7: Performance gain of each model design across the forecasting horizon. A relative performance greater than 1 indicates a gain, while values less than 1 indicate a drop. Results are averaged across all datasets, with the vertical red dashed line marking the training horizon.", "description": "This figure shows the relative performance improvement of ElasTST compared to its variants without the structured attention mask, multi-patch assembly, or tunable RoPE. The x-axis represents the forecasting horizon, and the y-axis shows the ratio of the MAE for each variant to the MAE for the full ElasTST model. Values above 1 indicate that ElasTST outperforms the variant, while values below 1 indicate the opposite. The red dashed line marks the training horizon. This visualization helps to understand the contribution of each design element to the model's performance across different forecasting horizons.", "section": "4.2 Ablation Study"}, {"figure_path": "ucXUtMPWhv/figures/figures_17_2.jpg", "caption": "Figure 2: Performance of trained once and inference over varying forecasting horizons. Models except TimesFM and MOIRAI are trained with a forecasting horizon of 720 and tasked with predicting across multiple horizons. A vertical red dashed line distinguishes between their seen horizons (96, 192, 336, 720) and unseen horizon (1024). We use a dashed line to denote the datasets on which the model was pre-trained, e.g., both TimesFM and MOIRAI have leveraged Traffic datasets for their pre-training. The ETT encompasses averaged results from datasets ETTh1, ETTh2, ETTm1, and ETTm2. Models lack inherent elasticity use a truncation strategy for shorter forecasts, and the foundation models use their pre-trained checkpoints and recommended configurations for inference.", "description": "This figure compares the performance of ElasTST and other forecasting models across various forecasting horizons, both seen (during training) and unseen (during inference). It highlights ElasTST's robustness in maintaining performance across different horizons while other models struggle, especially in extrapolating to unseen horizons.", "section": "4.1 Main Results"}, {"figure_path": "ucXUtMPWhv/figures/figures_18_1.jpg", "caption": "Figure 2: Performance of trained once and inference over varying forecasting horizons. Models except TimesFM and MOIRAI are trained with a forecasting horizon of 720 and tasked with predicting across multiple horizons. A vertical red dashed line distinguishes between their seen horizons (96, 192, 336, 720) and unseen horizon (1024). We use a dashed line to denote the datasets on which the model was pre-trained, e.g., both TimesFM and MOIRAI have leveraged Traffic datasets for their pre-training. The ETT encompasses averaged results from datasets ETTh1, ETTh2, ETTm1, and ETTm2. Models lack inherent elasticity use a truncation strategy for shorter forecasts, and the foundation models use their pre-trained checkpoints and recommended configurations for inference.", "description": "This figure compares the performance of various time series forecasting models across different forecasting horizons.  Models are trained on a single horizon (720) and then tested on a range of horizons.  The figure shows that ElasTST maintains consistent accuracy across both seen and unseen horizons, while other models show degradation in performance for unseen horizons.", "section": "4.1 Main Results"}, {"figure_path": "ucXUtMPWhv/figures/figures_19_1.jpg", "caption": "Figure 10: The performance of difference patch size selection.", "description": "The figure shows the impact of different patch sizes on forecasting performance across various forecasting horizons and training horizons.  It compares the performance of single patch configurations (using only one patch size: 8, 16, 32, or 64) to multi-patch configurations which combine multiple patch sizes (e.g., 8, 16, and 32). The results are presented as NMAE values across different forecasting horizons for each training horizon (96, 192, 336, and 720).", "section": "4.2 Ablation Study"}, {"figure_path": "ucXUtMPWhv/figures/figures_20_1.jpg", "caption": "Figure 2: Performance of trained once and inference over varying forecasting horizons. Models except TimesFM and MOIRAI are trained with a forecasting horizon of 720 and tasked with predicting across multiple horizons. A vertical red dashed line distinguishes between their seen horizons (96, 192, 336, 720) and unseen horizon (1024). We use a dashed line to denote the datasets on which the model was pre-trained, e.g., both TimesFM and MOIRAI have leveraged Traffic datasets for their pre-training. The ETT encompasses averaged results from datasets ETTh1, ETTh2, ETTm1, and ETTm2. Models lack inherent elasticity use a truncation strategy for shorter forecasts, and the foundation models use their pre-trained checkpoints and recommended configurations for inference.", "description": "This figure compares the performance of ElasTST and several other models across various forecasting horizons, including an unseen horizon.  It shows that ElasTST maintains strong performance even on unseen horizons, while others struggle, particularly those requiring per-horizon training or relying on autoregressive decoding. The figure highlights ElasTST's robustness and generalization capabilities.", "section": "4.1 Main Results"}, {"figure_path": "ucXUtMPWhv/figures/figures_21_1.jpg", "caption": "Figure 1: Overview of the ElasTST Architecture. ElasTST employs (a) structured attention masks for placeholders to ensure consistent outputs across varied forecasting horizons. It incorporates (b) tunable ROPE customized to time series periodicities, enhancing its robustness. The architecture also integrates a (c) multi-scale patch assembly that merges fine-grained and coarse-grained details for improved forecasting accuracy. Furthermore, we implement (d) training horizon reweighting scheme during the training phase, which effectively simulates random sampling of forecasting horizons, reducing the need for additional sampling efforts.", "description": "This figure provides a comprehensive overview of the ElasTST architecture, highlighting its key components: structured attention masks for handling varied forecasting horizons, tunable Rotary Position Embedding (ROPE) for adapting to time series periodicities, a multi-scale patch assembly for integrating both fine-grained and coarse-grained information, and a horizon reweighting scheme to simulate varied-horizon training.", "section": "3 Elastic Time-Series Transformers"}]