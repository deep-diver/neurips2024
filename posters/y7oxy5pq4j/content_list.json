[{"type": "text", "text": "RobIR: Robust Inverse Rendering for High-Illumination Scenes ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ziyi Yang1 Yanzhen Chen1 Xinyu Gao1 Yazhen Yuan2 Yu Wu2 Xiaowei Zhou1 Xiaogang Jin1\u2020 ", "page_idx": 0}, {"type": "text", "text": "1State Key Lab of CAD&CG, Zhejiang University 2Tencent ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Implicit representation has opened up new possibilities for inverse rendering. However, existing implicit neural inverse rendering methods struggle to handle strongly illuminated scenes with significant shadows and slight reflections. The existence of shadows and reflections can lead to an inaccurate understanding of the scene, making precise factorization difficult. To this end, we present RobIR, an implicit inverse rendering approach that uses ACES tone mapping and regularized visibility estimation to reconstruct accurate BRDF of the object. By accurately modeling the indirect radiance field, normal, visibility, and direct light simultaneously, we are able to accurately decouple environment lighting and the object\u2019s PBR materials without imposing strict constraints on the scene. Even in high-illumination scenes with shadows and specular reflections, our method can recover high-quality albedo and roughness with no shadow interference. RobIR outperforms existing methods in both quantitative and qualitative evaluations. Code is available at https://github.com/ingra14m/RobIR. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Inverse rendering, the task of extracting the geometry, materials, and lighting of a 3D scene from 2D images, is a longstanding challenge in computer graphics and computer vision. Previous methods, such as providing geometry for the entire scene [35, 46], modeling shape representation [21, 34, 52, 14] or pre-providing multiple known light information [10], have achieved plausible results using prior information. To achieve clear albedo and roughness decomposition, factors such as light obscuration, reflection, or refraction must be taken into account. Among these, hard and soft shadows are particularly challenging to eliminate, as they play a critical role not only in obtaining cleaner material but also in accurately modeling geometry and light sources. Although some data-driven approaches [22, 39] have performed plausible shadow removal at the image level, these methods are not generally applicable for inverse rendering. ", "page_idx": 0}, {"type": "text", "text": "Since the advent of NeRF [32], implicit representation has garnered significant interest in portraying scenes as neural radiance fields. By applying implicit neural representation to inverse rendering [3, 19, 54], plausible factorization can be achieved in simple scenes with weak light intensity. Thanks to NeRFactor [55] and its relevant work [8], which extend previous works by explicitly representing visibility, implicit inverse rendering can be improved with simple shadow removal and clear edge in albedo and roughness. Recently, InvRender [56] has taken the scene factorization problem to a new level by modeling indirect illumination, serving as the baseline in our experiment. ", "page_idx": 0}, {"type": "text", "text": "However, in high-illumination scenarios with strong shadows or subtle specular reflections, the current methods for implicit inverse rendering have shown limitations in accurately modeling each decomposed part for BRDF estimation. Especially, it will lead to shadow baking in albedo and roughness, thereby causing serious artifacts in relighting and other downstream applications. To deal with such scenes, the following challenges arise in order to obtain high-quality physically based rendering (PBR) materials. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "First, previous methods for inverse rendering struggle to correctly decouple environment lighting, shadows, and the object\u2019s PBR materials. While these methods perform well in scenes with weak light intensity, where shadows and specular reflections are minimal, they struggle to accurately reconstruct BRDF of the object in scenarios with intense lighting. As shown in Fig. 4 and Fig. 5, shadow and specular reflection lead to poor albedo and messy environment map. To address the aforementioned challenge, we propose a novel approach that applies Academy Color Encoding System (ACES) [1] tone mapping [1] to nonlinearly and monotonically convert the PBR color output from the rendering equation to a range within [0, 1]. Specifically, we introduce a scaled parameter $\\gamma$ to adjust the standard ACES tone mapping curve for specific scenes, better adapting to varying lighting conditions. Unlike previous methods, which either directly output PBR color within $[0,1]$ [56], or convert linear PBR color outputted within [0, 1] to sRGB color also lying in $[0,1]$ [17, 26], our method can calculate PBR color over a broader value range. For areas with extremely strong or weak lighting, ACES tone mapping can reduce information loss in reconstruction through more refined contrast control, thereby better estimating BRDF without baking shadow or specular highlights. ", "page_idx": 1}, {"type": "text", "text": "Second, existing methods encounter difficulties in accurately modeling visibility. Typically these methods [56, 17] model the visibility field $V(\\mathbf{x},\\omega)$ through a learned SDF field and sphere tracing, which takes position and view direction as inputs. However, the visibility field is not compatible with direct light modeled based on Spherical Gaussian (SG), resulting in many stubborn shadows remaining at the edges. To address this, we introduce a regularized visibility estimation (RVE) distilled from the visibility field to directly predict the visibility for each SG to achieve more accurate visibility. This technique significantly contributes to the BRDF estimation, enabling the separation of environment maps, albedo, and roughness without the baked shadows. We also apply octree tracing instead of sphere tracing to improve the precision of the visibility field modeling. ", "page_idx": 1}, {"type": "text", "text": "In summary, the major contributions of our work are: ", "page_idx": 1}, {"type": "text", "text": "\u2022 A novel scene-dependent ACES tone mapping for inverse rendering. It enables the high-quality albedo and roughness reconstruction in scenes with intense lighting and strong shadows.   \n\u2022 A novel regularized visibility estimation designed for direct SGs. It improves the visibility accuracy for each direct SG and reduces shadow residue, enhancing the overall BRDF quality of the ill-posed inverse rendering.   \n\u2022 The first neural field-based inverse rendering framework to achieve robust shadow removal in BRDF estimation under high-illumination scenes. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Implicit Neural Representation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Neural rendering has gained popularity due to its ability to produce photorealistic images. Recently, NeRF [32] enables photo-realistic novel view synthesis using MLPs. It can handle complex light scattering and reconstruct high-quality scenes for downstream tasks. ", "page_idx": 1}, {"type": "text", "text": "Subsequent work has enhanced NeRF\u2019s efficiency in various ways, elevating it to new heights and enabling its use in other domains. Structure-based techniques [51, 13, 37, 15, 9, 12] have explored ways to improve inference or training efficiency by caching or distilling implicit neural representation into the efficient data structure. Hybrid methods [25, 27, 42, 43, 7] aim to improve the efficiency by incorporating explicit voxel-based data structures. Among them, Instant-NGP [33] achieves minute training by additionally incorporating hash encoding. In addition, some follow-up methods [36, 47, 50] are dedicated to recovering clear surfaces for scenes with complex solid objects by modeling a learnable SDF network, the value of which indicates the minimum distance between the input coordinate and surfaces in the scene. ", "page_idx": 1}, {"type": "text", "text": "In our work, we employ NeuS [47], an SDF-based volume rendering framework, to learn geometry priors for inverse rendering. Furthermore, drawing inspiration from PlenOctree [51], we construct an Octree tracer from the SDF to improve inference efficiency and accuracy compared to sphere tracing. ", "page_idx": 1}, {"type": "image", "img_path": "y7oxY5pq4j/tmp/3009ab05863948173d952e1ed757a9ef86a96ebc7a95819c7192fd075ff89903.jpg", "img_caption": ["Figure 1: The pipeline of our method. During the pre-processing stage, we reconstruct the scene as an implicit representation by NeuS [47]. From the implicit representation, we extract scene priors such as normal, visibility, and indirect illumination. During BRDF estimation, we optimize environmental lighting, the scaled parameter $\\gamma$ , albedo $a$ , and roughness $r$ , to minimize reconstruction loss under the constraint of the rendering equation. After 100 epochs, we perform regularized visibility estimation and employ an MLP to learn the visibility ratio $\\bar{Q}$ of the direct SGs to obtain more accurate visibility specified for SGs, which is critical for eliminating stubborn shadows at the edges and boundaries. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2.2 Inverse Rendering ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Inverse rendering is a process in computer graphics that aims to derive an understanding of the physical properties of a scene from a set of images. Because the problem is highly ill-posed, most previous works have incorporated priors such as illumination, shape, and shadow, as well as additional observations such as scanned geometry [35, 38, 20] and known light conditions [10]. Simplified approaches, such as those assuming outdoor and natural light [40] or white light [30], aim to reduce the number of fitting parameters in an ill-posed problem. ", "page_idx": 2}, {"type": "text", "text": "Recently, there has been a surge of interest in implicit inverse rendering, building on the success of NeRF and its fully differentiable implicit representation. To model spatially-varying bidirectional reflectance distribution function (SVBRDF) under more casual capture conditions, many recent methods [3, 19, 5, 4, 49, 53, 54] have relied on implicit representation. Other works [55, 41, 48, 17, 26] have focused on physical-based modeling for complex scenes via visibility prediction. L-Tracing [8] introduced a new algorithm for estimating visibility without training, while NeRFactor [55] proposed a canonical normal and BRDF smoothness to address NeRF\u2019s poor geometric quality. InvRender [56] extends previous work by modeling indirect illumination. Relightable-GS [11] and GS-IR [24], based on the representation of 3D-GS [18], have achieved real-time inverse rendering. However, none of these methods are able to decouple shadows and materials under high-illuminance conditions. ", "page_idx": 2}, {"type": "text", "text": "2.3 The Rendering Equation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "For non-emitted object, the color $c$ of the surface point $\\mathbf{X}$ is calculated by the rendering equation: ", "page_idx": 2}, {"type": "equation", "text": "$$\nc\\left(\\mathbf{x},\\omega_{o}\\right)=\\int_{\\Omega}{f_{r}\\left(\\omega_{o},\\omega_{i},\\mathbf{x}\\right)L\\left(\\mathbf{x},\\omega_{i}\\right)\\left(\\omega_{i}\\cdot\\mathbf{n}\\right)}d\\omega_{i},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $c(\\mathbf{x},\\omega_{\\mathbf{o}})$ is the output color leaving point $\\mathbf{x}$ in the view direction $\\omega_{\\mathbf{o}}$ , $f_{r}(\\mathbf{x},\\omega_{\\mathbf{i}},\\omega_{\\mathbf{o}})$ is the BRDF function, $L\\big(\\mathbf{x},\\omega_{\\mathbf{i}}\\big)$ is the incoming radiance at point $\\mathbf{x}$ from direction $\\omega_{\\mathbf{i}}$ , and $\\mathbf{n}$ is the surface normal. Following PhySG [54] and InvRender [56], we use spherical Gaussians (SGs) to efficiently approximate the rendering equation shown in Eq. (1). An SG is a spherical function that takes the following form: ", "page_idx": 2}, {"type": "equation", "text": "$$\nG(\\omega;\\pmb{\\xi},\\lambda,\\pmb{\\mu})=\\pmb{\\mu}e^{\\lambda(\\omega\\cdot\\pmb{\\xi}-1)},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\xi\\,\\in\\,R^{3}$ is the lobe axis, $\\lambda\\,\\in\\,R^{1}$ is the lobe sharpness, and $\\pmb{\\mu}\\in R^{3}$ is the lobe amplitude.   \nPlease refer to the supplementary material for the complete details. ", "page_idx": 2}, {"type": "text", "text": "In NeuS [47], we can determine the surface point $\\mathbf{X}$ along a specific direction using sphere tracing. By substituting the color function with the shading function based on Eq. (1), we can achieve BRDF decomposition through image loss. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given a set of multi-view RGB images with known camera poses as input, our target is to reconstruct BRDF of the object even under high-illuminance scenes. As shown in Fig. 1, the pipeline of RobIR consists of two stages. In the pre-processing stage, we train NeuS $S(x,\\omega)$ as the representation of the scene, which can provide scene priors like normals, visibility, and indirect illumination (Sec. 3.2). In the BRDF estimation stage, we fix the scene priors and optimize the direct illumination and scaled parameter to compute an accurate BRDF of the object under the constraint of rendering equation (Sec. 3.3). To improve the visibility accuracy for direct illumination and decomposition stability, we introduce the regularized visibility estimation after 100 epochs (Sec. 3.4). ", "page_idx": 3}, {"type": "text", "text": "3.2 Stage 1: Pre-processing ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this stage, we adopt the same neural SDF representation and the volume rendering as NeuS [47] to reconstruct the scene. Then we can obtain the necessary prior information for the BRDF estimation stage, such as normal, visibility, and indirect illumination from NeuS. ", "page_idx": 3}, {"type": "text", "text": "Normal smoothing. In our framework, the accuracy of normal is crucial for BRDF estimation. However, we observed that normals estimated from NeuS tend to be noisy. To overcome this, we drew inspiration from Ref-NeRF [45] and employ a spatial MLP $\\mathbb{N}(\\mathbf{x})$ to predict smooth normals aligned with the density gradient normals (See Fig. 2) obtained from NeuS using $\\mathcal{L}_{2}$ loss. We further employ a smooth loss to fix the broken normals caused by specular reflection: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{n o r m}=\\|\\mathbb{N}(\\mathbf{x})-\\hat{n}\\|_{2}^{2}+\\|\\mathbb{N}(\\mathbf{x})-\\mathbb{N}(\\mathbf{x}+\\epsilon)\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbb{N}$ denotes the normal at point $\\mathbf{X}$ learned by MLP, $\\hat{\\boldsymbol{n}}$ denotes the supervision normal from NeuS, and $\\epsilon$ is a $0.02\\times$ Gaussian noise. ", "page_idx": 3}, {"type": "text", "text": "Visibility and indirect illumination. With the availability of NeuS SDF, we can use sphere tracing to model secondary shading effects such as visibility and indirect illumination. However, performing sphere tracing requires a significant amount of time and memory. Inspired by PlenOctree [51], we use an octree tracer derived from the NeuS SDF, replacing sphere tracing to accelerate the tracing and achieve more precise intersection results. Moreover, We can further improve the inference efficiency by compressing the visibility and indirect illumination field into MLP. ", "page_idx": 3}, {"type": "text", "text": "As for indirect illumination, we follow InvRender [56] and model the indirect radiance field $L_{I}({\\bf x},\\omega_{i})$ using $M=24$ SGs under the supervision of NeuS radiance field. At point $\\mathbf{X}$ , we first perform octree tracing along direction $\\omega_{i}$ to get the second intersection point $\\hat{x}$ . Then the indirect radiance field can be supervised by the out-going radiance $S(\\hat{x},-\\omega_{i})$ from NeuS. Then, the indirect illumination $L_{I}$ is computed by: ", "page_idx": 3}, {"type": "equation", "text": "$$\nL_{I}(\\mathbf{x},\\omega;\\Gamma)=\\sum_{j=1}^{M}G(\\omega;\\Gamma(\\mathbf{x},\\gamma)),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where we use an MLP $\\Gamma$ to output the $j$ th indirect SG parameters, and $\\gamma$ denotes the scaled parameter, which will be illustrated in Sec. 3.3. ", "page_idx": 3}, {"type": "text", "text": "As for visibility, we learn an MLP that maps the point $\\mathbf{x}$ and direction $\\omega$ to visibility $V(\\mathbf{x},\\omega)$ , which is supervised by the result of octree tracer from point $\\mathbf{x}$ along direction $\\omega$ . The $\\mathcal{L}_{i n d i r}$ and $\\mathcal{L}_{\\nu i s}$ are optimized by ${\\mathcal{L}}_{1}$ and binary cross entropy loss as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{i n d i r}=\\Vert\\hat{L}_{I}-L_{I}\\Vert_{1},\\mathcal{L}_{v i s}=\\mathrm{BCE}(V(\\mathbf{x},\\omega),\\hat{V}(\\mathbf{x},\\omega)),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathrm{BCE}(p_{i}\\parallel y_{i})$ represents the binary cross-entropy (BCE) loss, $\\hat{L}_{I}$ is the radiance value at the second intersection point $\\hat{x}$ obtained by querying NeuS, and $\\hat{V}({\\bf x},\\omega)$ is obtained using an octree tracer from point $\\mathbf{x}$ along direction $\\omega$ . ", "page_idx": 3}, {"type": "text", "text": "3.3 Stage 2: BRDF Estimation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "So far, we have faithfully reconstructed the prior information of the scene such as the normal, visibility and the indirect illumination. In this stage, we aim to accurately evaluate the rendering equation in order to precisely estimate the surface BRDF i.e. albedo $a$ , roughness $r$ and direct environment light with the fixed priors from stage 1. However, previous approaches tend to leave shadow and specular reflection in PBR materials under scenes with high illumination. Thus, we apply a scene-specific ACES tone mapping to the PBR color output by the rendering equation. The ACES tone mapping can calculate the PBR color over a broader value range, better estimating BRDF without baking shadow through more refined contrast control. We adopt SGs to efficiently approximate the rendering equation as PhySG [54]. See complete SGs approximation in the supplementary materials. ", "page_idx": 3}, {"type": "image", "img_path": "y7oxY5pq4j/tmp/f39b0b9de9527450d35e369e489fd7198b0406371d0571c3a24f8e7a0c3ebc44.jpg", "img_caption": ["Figure 2: Smooth loss to fix broken part. "], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "y7oxY5pq4j/tmp/f65a278887667aefc34c6036ee92ed34d31b2920a6cb5bb29ee892d47574a8f7.jpg", "img_caption": ["Figure 3: Visualization of direct SGs. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Scene-specific ACES tone mapping. We adopt the widely used the ACES tone mapping [1], which is a type of high dynamic range (HDR) tone mapping. Several recent works [16, 31] have incorporated HDR tone mapping into NeRF for specific applications. Specifically, we apply the ACES tone mapping $\\mathcal{F}$ to convert the PBR color $e$ lying in $\\lbrack0,+\\infty)$ to color lying in $[0,1]$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{F}(e)=\\frac{(2.51e+0.03)e}{(2.43e+0.59)e+0.14},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "whereas the ACES inverse tone mapping ${\\mathcal{F}}_{\\mathrm{I}}$ is given by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{F}_{\\mathrm{I}}(c)=\\frac{0.59c-0.03+\\sqrt{-1.0127c^{2}+1.3702c+0.0009}}{2(2.51-2.43c)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Given that the light intensity varies across different scenes, applying ACES tone mapping universally is not feasible. Thus, we introduce an additional learnable parameter $\\gamma\\in(0,1]$ . This scaled parameter modifies the ACES tone mapping curve, enabling it to automatically adapt to each scene\u2019s unique illumination intensity. The resulting deformed tone mapping function is defined as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{F}^{\\gamma}(e)=\\gamma^{-0.2}\\mathcal{F}(e),\\mathcal{F}_{\\mathrm{I}}^{\\gamma}(c)=\\mathcal{F}_{\\mathrm{I}}(c\\cdot\\gamma^{0.2}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Indirect illumination with scaled parameter. In Sec. 3.2, we model the indirect illumination under the supervision from NeuS\u2019s radiance field. To convert indirect illumination to the same value range as BRDF estimation, we need to map the supervised values from NeuS through ACES inverse tone mapping $\\mathcal{F}_{\\mathrm{I}}^{\\gamma}$ . Since we are not certain of the $\\gamma$ that best fits the scene during stage 1, we train indirect illumination using randomly sampled $\\gamma$ to obtain indirect illumination under all possible $\\gamma$ settings. Consequently, the loss function $\\mathcal{L}_{i n d i r}$ in Eq. (5) is then revised to include $\\gamma$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{i n d i r}=\\|\\mathcal{F}_{\\mathrm{I}}^{\\gamma}(\\hat{L}_{I})-L_{I}\\|_{1}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then in stage 2, we stop training the indirect illumination and treat $\\gamma$ as a learnable parameter. The optimal $\\gamma$ for the current scene will be determined as the decomposition model converges. ", "page_idx": 4}, {"type": "text", "text": "BRDF estimation. We use the simplified Disney BRDF [6] model with albedo, roughness, and environment light as parameters and assume dielectric materials with a fixed Fresnel term value of $F_{0}\\,=\\,0.02$ . During the BRDF estimation stage, we adopt $N\\,=\\,128$ learnable SGs to model direct illumination and represent the PBR materials using an encoder-decoder network. The network initially encodes the input surface point $\\mathbf{X}$ into its corresponding latent code $\\mathbf{z}$ and then decodes it into albedo a and roughness $\\mathbf{r}$ . To further reduce noise in materials, we incorporate the smooth loss similar to Eq. (3) to both the albedo and roughness, and apply sparsity loss to $\\mathbf{z}$ to ensure that most of the channels are close to zero: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{s m o o t h}=||\\mathbb{D}(\\mathbf{z}),\\mathbb{D}(\\mathbf{z}+\\epsilon)||_{2}^{2},\\mathcal{L}_{s p a r s e}=\\mathrm{KL}(\\mathbf{z}\\mid|\\ 0.05),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbb{D}$ is the decoder of the PBR material network, $\\begin{array}{r}{\\mathrm{KL}(\\rho\\parallel\\hat{\\rho})=\\rho l o g\\frac{\\rho}{\\hat{\\rho}}+(1-\\rho)l o g\\frac{1-\\rho}{1-\\hat{\\rho}}}\\end{array}$ represents Kullback-Leibler (KL) divergence loss that measures the relative entropy of two distributions. ", "page_idx": 4}, {"type": "image", "img_path": "y7oxY5pq4j/tmp/92c01582389888a80363d88c6d7326d651681832e43aa6996d69d8705bf826f9.jpg", "img_caption": ["Figure 4: Albedo in synthetic scenes. We compare our method to InvRender [56], NVDiffrec [34], TensoIR [17], NeRO [26], Relightable-GS [11], and GS-IR [24]. The results show that our method outperforms previous approaches without baking specular highlights and shadows into albedo. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "SGs approximation for rendering equation. In RobIR, we follow PhySG [54] and adopt SGs to approximate the rendering equation in Eq. (1): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle f_{r}\\left(\\omega_{o},\\omega_{i},\\mathbf{x}\\right)=\\frac{\\mathbf{a}}{\\pi}+f_{s}\\left(\\omega_{o},\\omega_{i},\\mathbf{r}\\right)}}\\\\ {{\\displaystyle\\omega_{i}\\cdot\\mathbf{n}\\approx G\\left(\\omega_{i};0.0315,\\mathbf{n},32.7080\\right)-31.7003,}}\\\\ {{\\displaystyle L\\left(\\mathbf{x},\\omega_{i}\\right)=\\sum_{k=1}^{N}G\\left(\\omega_{i};\\xi_{k},\\lambda_{k},\\eta(\\mathbf{x}){\\mu}_{k}\\right)+\\sum_{j=1}^{M}G_{I}(\\omega_{i};\\Gamma(\\mathbf{x},\\gamma)),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $G$ is the direct SGs learned in this stage, $G_{I}$ is the indirect SGs learned in stage 1, $\\mathbf{n}$ is the surface normal, \u03b7(x) =  iS=0  GS(\u03c9iG)(V\u03c9 (x),\u03c9i) signifies the visibility for direct SGs obtained by randomly sampling $S$ directions, $f_{s}$ denotes the specular component that can be converted to a single SG. Then, we can integrate the multiplication of these SGs in closed-form [29] to compute the final PBR color $\\omega_{o}$ . For more details about $f_{s}$ , please see the supplementary materials. ", "page_idx": 5}, {"type": "text", "text": "3.4 Regularized Visibility Estimation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "One of our primary goals is to achieve clean albedo with no residual shadows, which are typically caused by inaccurate visibility. Despite all efforts of the previous modeling, a small amount of stubborn visibility errors still exist. Therefore, after 100 epochs of BRDF estimation, we introduce regularized visibility estimation, directly using an MLP $\\tilde{Q}(\\mathbf{x},\\tau)$ to predict the visibility of $\\mathbf{X}$ relative to $N$ direct SGs instead of $\\eta$ calculated through previously learned visibility network $V(\\mathbf{x},\\omega)$ . Specifically, $\\tilde{Q}(\\mathbf{x},\\tau)$ is a visibility prediction network learned from scratch under the supervision of $\\eta$ , while $\\tau$ represents the $N\\times N$ identity matrix used to add information for $_\\mathrm{N}$ direct SGs and $\\mathbf{x}\\in R^{3}$ is expanded to $R^{N\\times3}$ to predict visibility for each direct SG. Since visibility errors primarily occur at the edges, which are also sparse in the scene, we leverage the edge loss to make the residual sparse: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{e d g e}=\\mathrm{KL}(\\tilde{Q}(\\mathbf{x},\\tau)-\\eta(\\mathbf{x})\\parallel0.01).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In the first 100 epochs, we fix $V(\\mathbf{x},\\omega)$ using $\\eta$ to obtain a stable visibility estimate, avoiding the early collapse of BRDF estimation caused by directly using $\\tilde{Q}(\\mathbf{x},\\tau)$ . After 100 epochs, with a rough BRDF estimation in place, we introduce regularized visibility estimation. By using $V(\\mathbf{x},\\omega)$ to distill $\\tilde{Q}(\\mathbf{x},\\tau)$ , we directly predict the visibility of point $\\mathbf{X}$ relative to direct SGs, circumventing errors caused by the sampling direction when calculating $\\eta$ . Thus, we can achieve a more accurate visibility estimate designed for direct SGs (See Fig. 3). ", "page_idx": 5}, {"type": "table", "img_path": "y7oxY5pq4j/tmp/4519bdc5c9773ac4843d97c557880ffe5a27a98cdfa3e25b05162016798a3ec3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 1: Quantitative evaluations. We present the results of the synthetic scenes. We color each cell as best , second best , and third best . Our method can produce high-quality albedo, roughness, and environment map while maintaining the relighting fidelity. ", "page_idx": 6}, {"type": "text", "text": "(a) nvdiffrec (b) InvRender (c) TensoIR (d) GS-IR (e) Relight-GS (f) ours (g) gt ", "page_idx": 6}, {"type": "image", "img_path": "y7oxY5pq4j/tmp/1a191cba0121a3c45d2da2305b653096eec998cd7503882936d2d2aa4b6e7484.jpg", "img_caption": ["Figure 5: Environment map. Compared to existing approaches, our method can truly achieve high-quality environment light decoupling, avoiding messy results. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Final loss. After incorporating regularized visibility estimation into inverse rendering, our final loss function in the BRDF estimation stage is: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\|\\mathcal{F}^{\\gamma}(C_{p b r}),C_{g t}\\|_{2}^{2}+\\lambda_{s m o o t h}\\mathcal{L}_{s m o o t h}+\\lambda_{s p a r s e}\\mathcal{L}_{s p a r s e}+\\lambda_{e d g e}\\mathcal{L}_{e d g e},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $C_{p b r}$ is the physically-based color from the rendering equation, $\\mathcal{F}^{\\gamma}$ is the scene-specific ACES tone mapping, $C_{g t}$ is the ground-truth color. In our experiments, $\\lambda_{s m o o t h},\\,\\lambda_{s p a r s e}$ , and $\\lambda_{e d g e}$ are set to 0.001, 0.01, and 1.0 respectively. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we present the experimental evaluation of our methods. To assess the effectiveness of our approach, we collect synthetic and real-world datasets from NeRF and NeuS without any post-processing. In addition, we use Blender to render our own datasets to further demonstrate the superiority of our methods in high-illumination scenes. It should be noted that unlike previous methods [17, 55] that used a hotdog scene with reduced illumination, we use the original hotdog from NeRF [32] without reduced illumination. See more comparison in the supplementary materials. ", "page_idx": 6}, {"type": "text", "text": "Our model hyperparameters consisted of a batch size of 1024, with 200k iterations for the NeuS training. The model was implemented in PyTorch and optimized with the Adam optimizer at a learning rate of $5e^{-4}$ . All tests were conducted on a single Tesla V100 GPU with 32GB memory. The training time without NeuS is around 5 hours. ", "page_idx": 6}, {"type": "image", "img_path": "y7oxY5pq4j/tmp/518d50cd120c57456545c9b144817f892d5248c1c60e2591ea27681d013e7b43.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 6: Roughness in synthetic scenes. The results show that our method can achieve clean roughness, even in scenes with intense shadow interference. ", "page_idx": 6}, {"type": "image", "img_path": "y7oxY5pq4j/tmp/71d7fb719748e32c97ec8e7663c3cfda2948ebd3117ac7a10c417c80f98799ba.jpg", "img_caption": ["Figure 7: Comparisons on real-world scenes. Columns 2 to 5 are albedo, the last four columns are roughness. Even in complex real-world scenarios, our method can robustly decouple shadow and material, resulting in high-quality albedo and roughness. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "y7oxY5pq4j/tmp/d3eb065366bcb11452e2f0affaaf1f7fce67eea619b451d45702a98e86225ac7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 8: Ablation. We conduct ablation experiments on the key components in the BRDF estimation stage. The ablation results emphasize the critical importance of each component in our proposed framework for attaining high-quality albedo. ", "page_idx": 7}, {"type": "text", "text": "4.1 Comparisons with previous methods ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compare our method with previous state-of-the-art neural field-based inverse rendering approaches: NVDiffrec [34], InvRender [56], TensoIR [17], NeRO [26], Relightable-GS [11], and GS-IR [24]. ", "page_idx": 7}, {"type": "text", "text": "As shown in Fig. 4 and Fig. 6, our method can truly achieve robust BRDF estimation, correctly decoupling shadows, ambient lighting, and PBR materials without baking shadows and specular highlights into albedo and roughness. Other methods tend to bake shadows into albedo, which also affects the correct decomposition of object roughness, reflecting their inability to properly separate the various components of BRDF estimation. Even in more challenging real-world scenarios shown in Fig. 7, our method can achieve robust decomposition results without baking shadows and specular highlights into albedo and roughness. ", "page_idx": 7}, {"type": "text", "text": "The estimated environment maps are shown in Fig. 5. Our method can accurately estimate the position of the light source and generate more precise light intensity in high-illumination scenes. As far as we know, we are the first to incorporate the accuracy of the estimated environment map into the quality assessment of neural field-based inverse rendering. ", "page_idx": 7}, {"type": "text", "text": "Tab. 1 shows the accuracy of the albedo, roughness, relighting, and environment map averaged over synthetic scenes. We did not measure the relighting of Relightable-GS because it does not support relighting of a single object. The term \"Log\" refers to the use of sigmoid mapping instead of ACES. We can observe that our method achieve the best results in all inverse rendering tasks. Inaccurate BRDF estimation significantly affects the results of relighting, causing methods with high-quality reconstruction to bake shadows and thus leading to a decline in rendering quality during relighting. Overall, our approach can achieve robust inverse rendering in high-illumination scenes. ", "page_idx": 7}, {"type": "text", "text": "4.2 Ablation Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We perform an ablation study to analyze the importance of the key components in our proposed method. As illustrated in Fig. 8, our method is unable to eliminate both shadows or specular reflection in the absence of ACES tone mapping. Without regularized visibility estimation, inaccurate predictions of direct SGs visibility results in residual shadows. The \"Log Tone\" result indicates that ACES is a more effective tone mapping than the sigmoid to remove shadow within our framework. Finally, our full method can correctly estimate BRDF of the object, resulting in the best performance. ", "page_idx": 7}, {"type": "image", "img_path": "y7oxY5pq4j/tmp/643ad4030873fb7e95939d426679a7c44a4d3968d6c2b4314243255abb5259dd.jpg", "img_caption": ["Figure 9: De-shadow. Given an input image from a specific viewpoint, our proposed method can accurately remove shadows caused by direct light occlusion without sacrificing rendering quality. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "y7oxY5pq4j/tmp/50a53a69707b0d27af769b309f4b4f357f5a0ebe8ec1deb372ff3883c49ce9a1.jpg", "img_caption": ["Figure 10: Relighting. Our method not only achieves high-quality relighting results in scenarios with specular highlights but can also robustly decouple shadows, obtaining high-quality relighting outcomes without baked shadows even in scenes with severe shadows. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.3 Application ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "De-shadowing. De-shadowing is a challenging task in the field of inverse rendering, often requiring strong priors and large data-driven models. Our proposed method correctly understands various lighting effects and is capable of effectively eliminating strong and irregular shadows, particularly in scenes with intense lighting. As shown in Fig. 9, by setting the visibility of direct SGs to 1, we can remove the shadow caused by direct light occlusion. It should be noted that our method cannot remove the areas with reflections and the dark regions caused by the backlighting phenomenon. ", "page_idx": 8}, {"type": "text", "text": "Relighting. To demonstrate the practical utility of the materials from our method, we conducted relighting experiments. As shown in Fig. 10, our estimated BRDF results can be accurately relighted in various lighting environments without shadow or illumination artifacts. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusions and Discussions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We presented a novel inverse rendering framework for estimating BRDF of the object under highillumination scenes. The key innovation lies in the use of ACES tone mapping, which shifts the calculation of PBR color to a wider value range, significantly reducing the impact of shadows and specular parts on BRDF estimation. In addition, regularized visibility estimation are employed to ensure more acuurate visibility for direct SGs. Experiment results on both synthetic and real-world data show that our method outperforms previous approaches in eliminating shadows and specular reflection under high-illumination scenes. ", "page_idx": 8}, {"type": "text", "text": "Currently, the proposed method has some limitations. First, non-solid, translucent, and thin objects cannot be correctly handled due to the limitations of NeuS. Second, the employment of SGs to model both direct and indirect lighting presents challenges in dealing with anisotropic objects, consequently leading to our method\u2019s deficiency in incorporating the metallic learnable parameters present in the Disney BRDF model. Third, we have not considered scenes with dynamic lighting like [28, 44]. Finally, our method\u2019s prior information is limited to multi-view images. We will consider integrating with LLM models in the future work. ", "page_idx": 8}, {"type": "text", "text": "6 Acknowlegements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by Key R&D Program of Zhejiang (No. 2024C01069). We thank Wenxin Sun for her help in pipeline illustration. We also thank Yuan Liu and Wen Zhou for the constructive suggestions. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Walter Arrighetti. The academy color encoding system (aces): A professional color-management framework for production, post-production and archival of still and motion pictures. Journal of Imaging, 3(4):40, 2017.   \n[2] Sai Bi, Zexiang Xu, Kalyan Sunkavalli, Milo\u0161 Ha\u0161an, Yannick Hold-Geoffroy, David Kriegman, and Ravi Ramamoorthi. Deep reflectance volumes: Relightable reconstructions from multi-view photometric images. In European Conference on Computer Vision, pages 294\u2013311. Springer, 2020.   \n[3] Mark Boss, Raphael Braun, Varun Jampani, Jonathan T Barron, Ce Liu, and Hendrik Lensch. Nerd: Neural reflectance decomposition from image collections. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12684\u201312694, 2021.   \n[4] Mark Boss, Andreas Engelhardt, Abhishek Kar, Yuanzhen Li, Deqing Sun, Jonathan Barron, Hendrik Lensch, and Varun Jampani. Samurai: Shape and material from unconstrained real-world arbitrary image collections. Advances in Neural Information Processing Systems, 35:26389\u201326403, 2022.   \n[5] Mark Boss, Varun Jampani, Raphael Braun, Ce Liu, Jonathan Barron, and Hendrik Lensch. Neural-pil: Neural pre-integrated lighting for reflectance decomposition. Advances in Neural Information Processing Systems, 34:10691\u201310704, 2021.   \n[6] Brent Burley and Walt Disney Animation Studios. Physically-based shading at disney. In Acm Siggraph, volume 2012, pages 1\u20137. vol. 2012, 2012.   \n[7] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In European Conference on Computer Vision (ECCV), 2022.   \n[8] Ziyu Chen, Chenjing Ding, Jianfei Guo, Dongliang Wang, Yikang Li, Xuan Xiao, Wei Wu, and Li Song. L-tracing: Fast light visibility estimation on neural surfaces by sphere tracing. In European Conference on Computer Vision, pages 217\u2013233. Springer, 2022.   \n[9] Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and Andrea Tagliasacchi. Mobilenerf: Exploiting the polygon rasterization pipeline for efficient neural field rendering on mobile architectures. arXiv preprint arXiv:2208.00277, 2022.   \n[10] Ziang Cheng, Hongdong Li, Yuta Asano, Yinqiang Zheng, and Imari Sato. Multi-view 3d reconstruction of a texture-less smooth surface of unknown generic reflectance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16226\u201316235, 2021.   \n[11] Jian Gao, Chun Gu, Youtian Lin, Hao Zhu, Xun Cao, Li Zhang, and Yao Yao. Relightable 3d gaussian: Real-time point cloud relighting with brdf decomposition and ray tracing. arXiv:2311.16043, 2023.   \n[12] Xinyu Gao, Ziyi Yang, Yunlu Zhao, Yuxiang Sun, Xiaogang Jin, and Changqing Zou. A general implicit framework for fast nerf composition and rendering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1833\u20131841, 2024.   \n[13] Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien Valentin. Fastnerf: High-fidelity neural rendering at 200fps. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14346\u201314355, 2021.   \n[14] Jon Hasselgren, Nikolai Hofmann, and Jacob Munkberg. Shape, Light, and Material Decomposition from Images using Monte Carlo Rendering and Denoising. arXiv:2206.03380, 2022.   \n[15] Peter Hedman, Pratul P Srinivasan, Ben Mildenhall, Jonathan T Barron, and Paul Debevec. Baking neural radiance fields for real-time view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5875\u20135884, 2021.   \n[16] Xin Huang, Qi Zhang, Ying Feng, Hongdong Li, Xuan Wang, and Qing Wang. Hdr-nerf: High dynamic range neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18398\u201318408, 2022.   \n[17] Haian Jin, Isabella Liu, Peijia Xu, Xiaoshuai Zhang, Songfang Han, Sai Bi, Xiaowei Zhou, Zexiang Xu, and Hao Su. Tensoir: Tensorial inverse rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.   \n[18] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00fchler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4), July 2023.   \n[19] Julian Knodt, Joe Bartusek, Seung-Hwan Baek, and Felix Heide. Neural ray-tracing: Learning surfaces and reflectance for relighting and view synthesis. arXiv preprint arXiv:2104.13562, 2021.   \n[20] Hendrik PA Lensch, Jan Kautz, Michael Goesele, Wolfgang Heidrich, and Hans-Peter Seidel. Imagebased reconstruction of spatial appearance and geometric detail. ACM Transactions on Graphics (TOG), 22(2):234\u2013257, 2003.   \n[21] Tzu-Mao Li, Miika Aittala, Fr\u00e9do Durand, and Jaakko Lehtinen. Differentiable monte carlo ray tracing through edge sampling. ACM Transactions on Graphics (TOG), 37(6):1\u201311, 2018.   \n[22] Zhengqin Li, Mohammad Shafiei, Ravi Ramamoorthi, Kalyan Sunkavalli, and Manmohan Chandraker. Inverse rendering for complex indoor scenes: Shape, spatially-varying lighting and svbrdf from a single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2475\u20132484, 2020.   \n[23] Zhengqin Li, Zexiang Xu, Ravi Ramamoorthi, Kalyan Sunkavalli, and Manmohan Chandraker. Learning to reconstruct shape and spatially-varying reflectance from a single image. ACM Transactions on Graphics (TOG), 37(6):1\u201311, 2018.   \n[24] Zhihao Liang, Qi Zhang, Ying Feng, Ying Shan, and Kui Jia. Gs-ir: 3d gaussian splatting for inverse rendering. arXiv preprint arXiv:2311.16473, 2023.   \n[25] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. Advances in Neural Information Processing Systems, 33:15651\u201315663, 2020.   \n[26] Yuan Liu, Peng Wang, Cheng Lin, Xiaoxiao Long, Jiepeng Wang, Lingjie Liu, Taku Komura, and Wenping Wang. Nero: Neural geometry and brdf reconstruction of reflective objects from multiview images. In SIGGRAPH, 2023.   \n[27] Julien N. P. Martel, David B. Lindell, Connor Z. Lin, Eric R. Chan, Marco Monteiro, and Gordon Wetzstein. Acorn: Adaptive coordinate networks for neural scene representation. ACM Trans. Graph. (SIGGRAPH), 40(4), 2021.   \n[28] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi, Jonathan T Barron, Alexey Dosovitskiy, and Daniel Duckworth. Nerf in the wild: Neural radiance fields for unconstrained photo collections. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7210\u20137219, 2021.   \n[29] Julian Meder and Beat D. Br\u00fcderlin. Hemispherical gaussians for accurate light integration. In International Conference on Computer Vision and Graphics, 2018.   \n[30] Abhimitra Meka, Mohammad Shafiei, Michael Zollh\u00f6fer, Christian Richardt, and Christian Theobalt. Real-time global illumination decomposition of videos. ACM Transactions on Graphics, 40(3), aug 2021.   \n[31] Ben Mildenhall, Peter Hedman, Ricardo Martin-Brualla, Pratul P Srinivasan, and Jonathan T Barron. Nerf in the dark: High dynamic range view synthesis from noisy raw images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16190\u201316199, 2022.   \n[32] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020.   \n[33] Thomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. ACM Trans. Graph., 41(4):102:1\u2013102:15, July 2022.   \n[34] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex Evans, Thomas M\u00fcller, and Sanja Fidler. Extracting triangular 3d models, materials, and lighting from images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8280\u20138290, 2022.   \n[35] Merlin Nimier-David, Zhao Dong, Wenzel Jakob, and Anton Kaplanyan. Material and lighting reconstruction for complex indoor scenes with texture-space differentiable rendering. 2021.   \n[36] Michael Oechsle, Songyou Peng, and Andreas Geiger. Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5589\u20135599, 2021.   \n[37] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14335\u201314345, 2021.   \n[38] Carolin Schmitt, Simon Donne, Gernot Riegler, Vladlen Koltun, and Andreas Geiger. On joint estimation of pose, geometry and svbrdf from a handheld scanner. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3493\u20133503, 2020.   \n[39] Soumyadip Sengupta, Jinwei Gu, Kihwan Kim, Guilin Liu, David W Jacobs, and Jan Kautz. Neural inverse rendering of an indoor scene from a single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8598\u20138607, 2019.   \n[40] Shuang Song and Rongjun Qin. A novel intrinsic image decomposition method to recover albedo for aerial images in photogrammetry processing, 2022.   \n[41] Pratul P Srinivasan, Boyang Deng, Xiuming Zhang, Matthew Tancik, Ben Mildenhall, and Jonathan T Barron. Nerv: Neural reflectance and visibility fields for relighting and view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7495\u20137504, 2021.   \n[42] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5459\u20135469, 2022.   \n[43] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Improved direct voxel grid optimization for radiance fields reconstruction. arXiv preprint arXiv:2206.05085, 2022.   \n[44] Jiaming Sun, Xi Chen, Qianqian Wang, Zhengqi Li, Hadar Averbuch-Elor, Xiaowei Zhou, and Noah Snavely. Neural 3d reconstruction in the wild. In ACM SIGGRAPH 2022 Conference Proceedings, pages 1\u20139, 2022.   \n[45] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T Barron, and Pratul P Srinivasan. Ref-nerf: structured view-dependent appearance for neural radiance fields. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5481\u20135490. IEEE, 2022.   \n[46] Delio Vicini, S\u00e9bastien Speierer, and Wenzel Jakob. Differentiable signed distance function rendering. ACM Transactions on Graphics (TOG), 41(4):1\u201318, 2022.   \n[47] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. NeurIPS, pages 27171\u201327183, 2021.   \n[48] Wenqi Yang, Guanying Chen, Chaofeng Chen, Zhenfang Chen, and Kwan-Yee K Wong. Ps-nerf: Neural inverse rendering for multi-view photometric stereo. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part I, pages 266\u2013284. Springer, 2022.   \n[49] Yao Yao, Jingyang Zhang, Jingbo Liu, Yihang Qu, Tian Fang, David McKinnon, Yanghai Tsin, and Long Quan. Neilf: Neural incident light field for physically-based material estimation. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXXI, pages 700\u2013716. Springer, 2022.   \n[50] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. Multiview neural surface reconstruction by disentangling geometry and appearance. Advances in Neural Information Processing Systems, 33:2492\u20132502, 2020.   \n[51] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plenoctrees for real-time rendering of neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5752\u20135761, 2021.   \n[52] Jason Zhang, Gengshan Yang, Shubham Tulsiani, and Deva Ramanan. Ners: neural reflectance surfaces for sparse-view 3d reconstruction in the wild. Advances in Neural Information Processing Systems, 34:29835\u201329847, 2021.   \n[53] Kai Zhang, Fujun Luan, Zhengqi Li, and Noah Snavely. Iron: Inverse rendering by optimizing neural sdfs and materials from photometric images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5565\u20135574, 2022.   \n[54] Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and Noah Snavely. Physg: Inverse rendering with spherical gaussians for physics-based material editing and relighting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5453\u20135462, 2021.   \n[55] Xiuming Zhang, Pratul P Srinivasan, Boyang Deng, Paul Debevec, William T Freeman, and Jonathan T Barron. Nerfactor: Neural factorization of shape and reflectance under an unknown illumination. ACM Transactions on Graphics (TOG), 40(6):1\u201318, 2021.   \n[56] Yuanqing Zhang, Jiaming Sun, Xingyi He, Huan Fu, Rongfei Jia, and Xiaowei Zhou. Modeling indirect illumination for inverse rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18643\u201318652, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "This supplementary document provides some implementation details and further results that accompany the paper. ", "page_idx": 12}, {"type": "text", "text": "\u2022 Section A introduces the differences between the dataset used by our method and those used by previous methods.   \n\u2022 Section B introduces more details of the SG approximation for the rendering equation.   \n\u2022 Section C provides additional results, including more visualizations and results on more datasets. ", "page_idx": 12}, {"type": "text", "text": "A High-illumination Dataset ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Currently, neural field-based inverse rendering methods, such as InvRender [56], NeRFactor [55], and TensoIR [17], generally use scenes with almost no high-intensity ambient light (See Fig. 13). The advantage of these scenes is that the object\u2019s BRDF estimation is not affected by self-occlusion shadows, making albedo and color quite similar. As a result, even if each part of the BRDF estimation is somewhat messy, plausible results can still be obtained. However, when the scene has intense illumination and shadows, these methods will fail to correctly model the object\u2019s BRDF. Therefore, to more accurately evaluate the robustness of inverse rendering, we choose a more challenging high-illumination dataset. ", "page_idx": 12}, {"type": "text", "text": "B SG Approximation for the Rendering Equation ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Following the methodology from [54], we employ the inner product of SGs to approximate the computation of the rendering equation. The position $\\mathbf{X}$ is dropped in the following equation due to the distant illumination assumption. Specifically, the term $\\omega_{i}\\cdot\\mathbf{n}$ is approximated by a SG as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\omega_{i}\\cdot\\mathbf{n}\\approx G\\left(\\omega_{i};0.0315,\\mathbf{n},32.7080\\right)-31.7003.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "As for the specular component $f_{s}$ , we employ the simplified Disney BRDF model as previous methods [6, 23, 2]: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{s}\\left(\\omega_{o},\\omega_{i}\\right)=\\mathcal{M}\\left(\\omega_{o},\\omega_{i}\\right)\\mathcal{D}(\\mathbf{h}),}\\\\ &{\\qquad\\qquad\\quad\\mathbf{h}=\\frac{\\omega_{o}+\\omega_{i}}{\\left\\|\\omega_{o}+\\omega_{i}\\right\\|_{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\mathcal{M}$ represents the Fresnel with shadowing effects, and $\\mathcal{D}$ is the normalized distribution function. To simplify the computation, we assume an isotropic specular BRDF, and adapt $\\mathcal{D}$ and $\\mathcal{M}$ as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{M}\\left(\\omega_{o},\\omega_{i}\\right)=\\frac{\\mathcal{F}\\left(\\omega_{o},\\omega_{i}\\right)\\mathcal{G}\\left(\\omega_{o},\\omega_{i}\\right)}{4\\left(\\mathbf{n}\\cdot\\omega_{o}\\right)\\left(\\mathbf{n}\\cdot\\omega_{i}\\right)}}\\\\ &{\\mathcal{F}\\left(\\omega_{o},\\omega_{i}\\right)=s+\\left(1-s\\right)\\cdot2^{-(5.55473\\omega_{o}\\cdot\\mathbf{h}+6.8316)(\\omega_{o}\\cdot\\mathbf{h})},}\\\\ &{\\mathcal{G}\\left(\\omega_{o},\\omega_{i}\\right)=\\frac{\\omega_{o}\\cdot\\mathbf{n}}{\\omega_{o}\\cdot\\mathbf{n}\\cdot\\mathbf{n}}\\cdot\\frac{\\omega_{i}\\cdot\\mathbf{n}}{\\omega_{i}\\cdot\\mathbf{n}\\cdot\\mathbf{n}\\left(1-k\\right)+k},}\\\\ &{\\hphantom{\\mathcal{G}\\left(\\omega_{o},\\omega_{i}\\right)=}\\frac{k}{8}\\frac{\\left(r+1\\right)^{2}}{},}\\\\ &{\\hphantom{\\mathcal{G}\\left(\\omega_{o},\\omega_{i}\\right)=}\\mathcal{D}\\left(\\mathbf{h};\\mathbf{n},\\frac{2}{r^{4}},\\frac{1}{\\pi r^{4}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $s\\in[0,1]^{3}$ is the specular factor, and $r$ denotes the roughness. Finally, we can compute the rendering equation through the fast inner product of SGs [29]. ", "page_idx": 12}, {"type": "image", "img_path": "y7oxY5pq4j/tmp/e3043f00d7cb89d76aa0368dd46fbd2a36e5799cbfce58c0e54119d05d0d2e6c.jpg", "img_caption": ["Figure 11: Other results of our method. In each scene, we present the input ground-truth image (a), our rendering result (b), normal (c), light (d), albedo (e), and roughness (f) obtained through our method. These experiments illustrate the generalizability of our method across diverse datasets and demonstrate its ability to produce high-quality results. "], "img_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "y7oxY5pq4j/tmp/40d1d968bd19272ca9f1e09c0cb34e4cf1700ec820b7e782a2e1d1fb97511949.jpg", "img_caption": ["Figure 12: Helmet Relighting. Our method achieves high-quality relighting results in scenarios with specular highlights and slight specular reflections. "], "img_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "y7oxY5pq4j/tmp/3cf44453022a1b3d7821908e9aa42b7fa71a09d70e6188b045b4682572618c57.jpg", "img_caption": ["Figure 13: Dataset Comparison. We choose a more challenging high-illumination dataset, which exposed the inability of previous neural field-based inverse rendering methods to decouple shadows from the object\u2019s PBR materials. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "y7oxY5pq4j/tmp/2398b153391dcd3cd52b79a5f9c94b4f1c5f2c2f8383565c36b6c70689c8a8b8.jpg", "img_caption": ["Figure 14: Hotdog Relighting. Our method achieves high-quality relighting results in scenarios with severe shadows. "], "img_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "y7oxY5pq4j/tmp/f6ade77c1f5ca7c10cef5c37bf78431b5541360d74373adcd67550f5e843bdf8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Table 2: Quantitative albedo comparison on synthetic dataset. We compare our method to several previous approaches: NVDiffrec [14], InvRender [56], TensoIR [17], Relightable-GS [11] and GS-IR [24]. We report PSNR, SSIM, LPIPS(VGG) and color each cell as best second best and third best . ", "page_idx": 14}, {"type": "text", "text": "C Additional Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "More qualitative results. Our method can effectively remove shadows baked into albedo and roughness, thanks to our accurate modeling of each decomposition component. Therefore, our method can certainly handle scenes with less intense lighting. Fig. 11 shows the results of our method on real-world datasets and some synthetic datasets, including scenes with shadows and specular, as well as diffuse objects. Our method can robustly perform inverse rendering in any situation without baking shadows and illumination into PBR materials. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Per-scene albedo results. We present the complete metrics of our method compared to other methods in Tab. 2. The estimated albedo in our method surpasses existing SOTA methods in every synthetic scene. ", "page_idx": 15}, {"type": "text", "text": "More relighting. We show more relighting results in Fig. 12 and Fig. 14. The two scenes demonstrate that our method can accurately estimate the BRDF of the object under scenes with specular highlights and severe shadows. ", "page_idx": 15}, {"type": "text", "text": "Additional comparison with NVDiffrecMC. We show more albedo, roughness, and environment map comparison with NVDiffrecMC [14] in Figs. 15-17. ", "page_idx": 15}, {"type": "image", "img_path": "y7oxY5pq4j/tmp/a039d7ce87717a963cfd6670026ee8084a32c7f592798c893592defd07d0548e.jpg", "img_caption": ["Figure 15: Albedo comparison with NvDiffRecMC on synthetic scenes. NvDiffRecMC cannot achieve the decouple of shadow, indirect illumination, and the PBR materials of the objects. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "y7oxY5pq4j/tmp/ddfab7e7f8cfcc4c47af46ef3bfda4dda3f108c963cd2cd4113ee01a597664bc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 16: Roughness comparison with NvDiffRecMC. NvDiffRecMC cannot obtain high-quality roughness in high illumination scenes. ", "page_idx": 15}, {"type": "image", "img_path": "y7oxY5pq4j/tmp/6069f7724f0cc3fb749eeec50431c55a59092ad407a3fa40304545cdca089706.jpg", "img_caption": ["Figure 17: Environment map comparison with NvDiffRecMC. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Visualization and evaluation on tone mapping. We first visualize the vanilla ACES and sRGB tone mapping in Fig. 18, indicating that ACES curve has much wider input range. Then in Fig. 19, we show the scene-specific tone mapping curve with different $\\gamma$ , enabling the ACES curve to fti other settings with different tone mapping methods. Finally, we evaluate the optimized ACES curve (with $\\gamma=0.42)$ ) in chessboard scene with GT tone mapping (sRGB) in Fig. 20. The results show that our scene-specific ACES tone mapping can stretch to sRGB curve, demostrating the effectiveness of our method. ", "page_idx": 16}, {"type": "image", "img_path": "y7oxY5pq4j/tmp/ee1cbb8e4bfedc56bd16670c8ba1e1519e86f7a6104d38c5fe5e6a2bce8d40d2.jpg", "img_caption": ["Figure 18: Comparison on ACES and sRGB curve. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "y7oxY5pq4j/tmp/2e9fa3c713e590013f72ad6905ecc16b75bf0b1b38d3ddbc4741715ef43ef3a9.jpg", "img_caption": ["Figure 19: Visualization of ACES tone mapping with different $\\gamma$ . "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "y7oxY5pq4j/tmp/93a47e47ca090ea32d1c081eba20b3395c84624ac3d9706faa4d9be35b95a9f7.jpg", "img_caption": ["Figure 20: Evaluation tone mapping in chessboard. The ACES tone mapping with $\\gamma\\,=\\,0.42$ matches well with the sRGB curve. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: I am sure that the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: I am sure that the paper discuss the limitations of the work. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Yes, the paper does. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Yes, the paper does. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: The code can be released upon acceptance, but now it\u2019s not a clean version. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper contains details about the training model. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We evaluate the results through PSNR, SSIM and LPIPS. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: They are presented in the paper. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Yes, we do. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 20}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Yes, they do. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 21}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]