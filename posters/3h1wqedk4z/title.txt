Enhancing Large Language Models through Adaptive Tokenizers