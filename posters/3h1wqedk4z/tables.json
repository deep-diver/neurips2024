[{"figure_path": "3H1wqEdK4z/tables/tables_5_1.jpg", "caption": "Table 1: Performance Comparison of Different Tokenization Methods.", "description": "This table presents a comparison of the performance of different tokenization methods, including BPE, BytePiece, Unigram, and the proposed ADAT method, across various metrics such as PPL and several downstream tasks (ARC-C, ARC-E, BoolQ, Lambda, LogiQA, PIQA, SciQ, SST-2, Winogrande).  The results show improvements achieved by ADAT over traditional methods.  The average percentage improvement is also highlighted.", "section": "4 Experiments"}, {"figure_path": "3H1wqEdK4z/tables/tables_6_1.jpg", "caption": "Table 2: Evaluation on Different Scale Model Size.", "description": "This table presents the performance comparison of the Unigram and ADAT tokenization methods across different model sizes (70M, 160M, and 410M parameters).  It shows the perplexity (PPL) and accuracy scores on eight common language modeling benchmark datasets (ARC-C, ARC-E, BoolQ, Lambda, LogiQA, PIQA, SciQ, SST-2, Winogrande) for each model size and tokenization method. The results demonstrate the scalability and effectiveness of the ADAT method across varying model sizes.", "section": "4.2 Tokenization Methods Evaluation"}, {"figure_path": "3H1wqEdK4z/tables/tables_6_2.jpg", "caption": "Table 3: Cross-Model Adaptability of Vocabularies.", "description": "This table shows the performance of different sized language models (70M, 160M, 410M parameters) when using three different vocabularies: a standard Unigram vocabulary and vocabularies optimized for the 70M and 410M model sizes.  The results demonstrate the cross-model adaptability of the vocabularies generated by the adaptive tokenizer approach, showing that vocabularies trained on one model size can generalize effectively to other model sizes,  improving performance.", "section": "4.4 Cross-Model Adaptability"}, {"figure_path": "3H1wqEdK4z/tables/tables_7_1.jpg", "caption": "Table 4: Impact of Vocabulary Size on Model Performance Across Different Model Sizes.", "description": "This table presents the results of experiments comparing the performance of models trained with different vocabulary sizes (50,000 and 30,000) using two different tokenization methods: Unigram and ADAT.  It shows accuracy and perplexity (PPL) scores for two different model sizes (70M and 160M parameters). The table highlights how the ADAT method improves accuracy and reduces perplexity compared to the Unigram method, demonstrating its effectiveness across various vocabulary and model sizes.", "section": "4.5 Model and Vocabulary Size"}, {"figure_path": "3H1wqEdK4z/tables/tables_8_1.jpg", "caption": "Table 5: Ablation Studies Results.", "description": "This table presents the ablation study results, showing the impact of variations in the inference corpus size used for calculating token loss, initial vocabulary sizes, momentum strategy, and the balance function F(a, b) on the accuracy of the proposed adaptive tokenizer method.  Different configurations are tested to isolate the effects of each of these factors on the overall performance.", "section": "4.6 Ablation Study"}, {"figure_path": "3H1wqEdK4z/tables/tables_9_1.jpg", "caption": "Table 2: Evaluation on Different Scale Model Size.", "description": "This table presents the results of evaluating the performance of different model sizes (70M, 160M, 410M parameters) using various tokenization methods. The metrics used are Perplexity (PPL) and several downstream task evaluation scores, which aim to assess the model's linguistic capabilities.  The results allow for a comparison of the performance impact of the tokenization strategies across different model scales.", "section": "4.2 Tokenization Methods Evaluation"}, {"figure_path": "3H1wqEdK4z/tables/tables_13_1.jpg", "caption": "Table 7: Performance comparison of different tokenization methods.", "description": "This table presents the compression rate achieved by different tokenization methods (BPE, BytePiece, ADAT+By, Unigram, ADAT+U) on the Pythia 70M model.  The compression rate is a measure of how efficiently the tokenization method reduces the size of the vocabulary while preserving linguistic information. Lower values indicate higher compression.", "section": "A.1 Compression Rate results"}, {"figure_path": "3H1wqEdK4z/tables/tables_13_2.jpg", "caption": "Table 8: Results in the 1B model.", "description": "This table shows the average performance comparison between the Unigram model and the proposed ADAT method on a 1B parameter model. ADAT demonstrates improved performance over Unigram, indicating its scalability across different model sizes.", "section": "A.2 Results of ADAT in the 1B model"}, {"figure_path": "3H1wqEdK4z/tables/tables_13_3.jpg", "caption": "Table 9: Comparison between Vocabulary of ADAT and Unigram.", "description": "This table presents the overlap ratios between vocabularies generated by ADAT and Unigram for different model sizes (70M, 160M, and 410M).  The overlap ratio indicates the similarity between the two sets of tokens.  It shows that there are significant differences between the vocabularies generated by the two methods, and that the overlap is generally higher for larger model sizes, indicating that ADAT's vocabularies become more similar to Unigram as model size increases. This also indirectly explains that the tokenizer generated by ADAT-410M is more suitable for Pythia-160M compared to the tokenizer generated by ADAT-70M.", "section": "A.3 Analysis of Differences Between Vocabularies"}, {"figure_path": "3H1wqEdK4z/tables/tables_14_1.jpg", "caption": "Table 10: Comparison with Initial Vocabulary with vocabulary size 100k.", "description": "This table shows the distribution of tokens in the initial vocabulary (100k) that fall into different score intervals, comparing the Unigram and ADAT (70M-50k) methods. The results indicate that ADAT relies not only on token frequency but also on the prediction difficulty of tokens during LLM training.", "section": "A.4 Impact of Training Epochs"}, {"figure_path": "3H1wqEdK4z/tables/tables_14_2.jpg", "caption": "Table 11: Impact of Training Epochs on Model Performance.", "description": "This table presents the results of an experiment investigating the effect of varying the number of training epochs on the performance of a model using vocabularies developed over different numbers of epochs.  The relationship between training duration and vocabulary efficacy is evaluated. The table shows accuracy for models trained for 3, 5, and 7 epochs, indicating a clear relationship between training duration and the final model's accuracy, with gains diminishing after 5 epochs. ", "section": "4.4 Impact of Training Epochs"}, {"figure_path": "3H1wqEdK4z/tables/tables_14_3.jpg", "caption": "Table 12: Results for different Infer Data Volume Tokens and Initial Vocabulary Sizes.", "description": "This table presents the results of an ablation study investigating the impact of varying inference data volume tokens and initial vocabulary sizes on model performance.  It shows accuracy scores for different combinations of inference data volume (1M, 10M, 100M, and 1000M tokens) and initial vocabulary size (75K, 100K, 150K, and 200K tokens). The data illustrates how the model's performance is affected by the amount of data used for loss calculation and by the initial vocabulary size during the model training process.", "section": "A.5 Expanded Analysis on Infer Data Volume Tokens and Initial Vocabulary Size"}, {"figure_path": "3H1wqEdK4z/tables/tables_15_1.jpg", "caption": "Table 13: Specifications of different LLMs used in the paper.", "description": "This table presents the specifications of the three different sizes of large language models (LLMs) used in the paper's experiments.  For each model size (70M, 160M, and 410M parameters), it lists the number of layers, the model dimension, the number of heads, the learning rate used during training, and the batch size.", "section": "A.7 Societal Impact"}]