[{"figure_path": "Fp3JVz5XE7/tables/tables_7_1.jpg", "caption": "Table 1: mIoU scores for BlackFed v1 and v2 in comparison with individual and FL-based training strategies for natural datasets. \"Local\" represents test data from the center. \"OOD\" represents mean mIoU on test data from rest of the centers. For FedAvg and Combined Training, just one model is trained. Hence, its performance is noted only in each of the local test datasets. For Cityscapes, we only present the average local and OOD performance across centers for brevity. The supplementary contains an expanded version for Cityscapes.", "description": "This table compares the performance of BlackFed v1 and v2 against individual training and other federated learning methods on two datasets (CAMVID and Cityscapes).  It shows the mean Intersection over Union (mIoU) scores for both \"Local\" (performance on the same center's test data) and \"Out-of-Distribution\" (OOD, average performance across test data from other centers) settings.  The results highlight BlackFed's ability to improve OOD performance while maintaining strong local performance.", "section": "Experiments and Results"}, {"figure_path": "Fp3JVz5XE7/tables/tables_7_2.jpg", "caption": "Table 2: mIoU scores for BlackFed v1 and v2 in comparison with individual and FL-based training strategies for medical datasets. \"Local\" represents test data from the center. \"OOD\" represents mean mIoU on test data from rest of the centers. For FedAvg and Combined Training, just one model is trained. Hence, its performance is noted only in each of the local test datasets.", "description": "This table compares the performance of BlackFed v1 and v2 against individual training and other federated learning methods (FedAvg, FedSeg, and FedPer) on two medical image segmentation datasets (ISIC and Polypgen).  The \"Local\" column shows the performance of each client's model on its own test set, while the \"OOD\" (Out-of-Distribution) column indicates the average performance across all other clients' test datasets. This helps assess the generalization ability of each method.  The table demonstrates how BlackFed approaches the performance of methods that allow for full gradient or model sharing while maintaining better privacy.", "section": "4 Experiments and Results"}, {"figure_path": "Fp3JVz5XE7/tables/tables_7_3.jpg", "caption": "Table 3: mIoU scores for BlackFed v1 and v2 in comparison with individual and FL-based training strategies for natural datasets. \"Local\" represents test data from the center. \"OOD\" represents mean mIoU on test data from rest of the centers. For FedAvg and Combined Training, just one model is trained. Hence, its performance is noted only in each of the local test datasets. For Cityscapes, we only present the average local and OOD performance across centers for brevity. The supplementary contains an expanded version for Cityscapes.", "description": "This table compares the performance of BlackFed v1 and v2 against other methods (individual training, combined training, white-box training, FedAvg, FedSeg, and FedPer) for four datasets (CAMVID, Cityscapes, ISIC, and Polypgen).  It shows the mean Intersection over Union (mIoU) scores for both \"Local\" (test data from the same center) and \"Out-of-Distribution\" (OOD, test data from other centers). The OOD performance is crucial for evaluating the generalization capabilities of federated learning approaches.", "section": "4 Experiments and Results"}, {"figure_path": "Fp3JVz5XE7/tables/tables_8_1.jpg", "caption": "Table 4: Comparison of client and server-side GFLOPS for different algorithms.", "description": "This table compares the computational cost (GFLOPS) of different algorithms for client and server sides using three different model architectures: DeepLabv3, Segformer, and UNext.  It shows that the proposed 'Ours' method significantly reduces the computational burden on the client side while shifting more processing to the server.", "section": "4 Experiments and Results"}, {"figure_path": "Fp3JVz5XE7/tables/tables_8_2.jpg", "caption": "Table 5: Average MIoU scores for different training strategies of BlackFed. Optimizing the client followed by the server improves performance, which is further improved by maintaining the server-side hashmap.", "description": "This table presents the average mean Intersection over Union (mIoU) scores achieved by different training strategies within the BlackFed framework, across four different datasets: CAMVID, ISIC, Cityscapes, and Polypgen.  It compares three approaches:\n1. Optimizing the server, then the client.\n2. Optimizing the client, then the server (BlackFed v1).\n3. Optimizing the client, then the server, while maintaining a server-side hashmap to mitigate catastrophic forgetting (BlackFed v2).\nThe results show that optimizing the client first, followed by the server, improves performance, and this improvement is enhanced further by employing the server-side hashmap in BlackFed v2.", "section": "4 Experiments and Results"}, {"figure_path": "Fp3JVz5XE7/tables/tables_8_3.jpg", "caption": "Table 6: Average mIoU scores for BlackFed v1 and v2 in comparison with individual and FL-based training strategies.", "description": "This table presents the mean Intersection over Union (mIoU) scores achieved by the proposed BlackFed approach (versions 1 and 2), compared against individual training and other federated learning (FL) methods like FedAvg.  The results are shown for different numbers of client and server epochs, allowing analysis of the impact of these hyperparameters on model performance.  \"Local\" indicates performance on a client's own data, while \"OOD\" represents out-of-distribution performance, showcasing the model's generalization capabilities.", "section": "4 Experiments and Results"}, {"figure_path": "Fp3JVz5XE7/tables/tables_14_1.jpg", "caption": "Table 7: Data counts for CAMVID, ISIC and Polypgen datasets", "description": "This table presents the number of data samples available for training, validation, and testing in each of the four datasets used in the paper.  The datasets are split across multiple clients, representing different institutions or data sources. For each dataset, the table shows the number of samples in each split for each client (C1, C2, etc.).  This illustrates the distribution of data among the clients, which is important in the context of federated learning.", "section": "4 Experiments and Results"}, {"figure_path": "Fp3JVz5XE7/tables/tables_14_2.jpg", "caption": "Table 8: Data counts for Cityscapes dataset", "description": "This table shows the number of images in the training, validation, and testing sets for each of the 18 centers (clients) in the Cityscapes dataset used in the federated learning experiments.  The data is not publicly available and is split based on the location of the images.", "section": "4 Experiments and Results"}, {"figure_path": "Fp3JVz5XE7/tables/tables_14_3.jpg", "caption": "Table 1: mIoU scores for BlackFed v1 and v2 in comparison with individual and FL-based training strategies for natural datasets. \"Local\" represents test data from the center. \"OOD\" represents mean mIoU on test data from rest of the centers. For FedAvg and Combined Training, just one model is trained. Hence, its performance is noted only in each of the local test datasets. For Cityscapes, we only present the average local and OOD performance across centers for brevity. The supplementary contains an expanded version for Cityscapes.", "description": "This table presents the mean Intersection over Union (mIoU) scores achieved by different federated learning methods on four datasets (CAMVID, Cityscapes, ISIC, and Polypgen).  It compares the performance of BlackFed versions 1 and 2 against individual training, combined training (all data together), a white-box approach (with gradient sharing), and other established FL methods like FedAvg and FedSeg.  The \"Local\" column shows performance on data from the same institution, and \"OOD\" (Out-of-Distribution) represents average performance across data from other institutions, indicating generalization ability.  Note that Cityscapes results are averaged across all 18 centers for brevity in the main table.", "section": "4 Experiments and Results"}, {"figure_path": "Fp3JVz5XE7/tables/tables_15_1.jpg", "caption": "Table 1: mIoU scores for BlackFed v1 and v2 in comparison with individual and FL-based training strategies for natural datasets. \"Local\" represents test data from the center. \"OOD\" represents mean mIoU on test data from rest of the centers. For FedAvg and Combined Training, just one model is trained. Hence, its performance is noted only in each of the local test datasets. For Cityscapes, we only present the average local and OOD performance across centers for brevity. The supplementary contains an expanded version for Cityscapes.", "description": "This table presents the mean Intersection over Union (mIoU) scores achieved by different federated learning methods on four datasets (CAMVID, Cityscapes, ISIC, and Polypgen).  It compares the performance of BlackFed v1 and v2 against individual training, combined training (all data aggregated), white-box federated training (gradients shared), and existing federated methods (FedAvg, FedSeg, and FedPer).  The \"Local\" column shows performance on the test data from the same client used for training, while the \"OOD\" (Out-of-Distribution) column shows performance on test data from other clients, evaluating generalization ability.  The table demonstrates BlackFed's ability to match or surpass individual and some federated learning approaches, especially in the OOD setting, without sharing gradients or model architectures.", "section": "4 Experiments and Results"}]