[{"figure_path": "oTEttMIymz/figures/figures_2_1.jpg", "caption": "Figure 1: The overview of our method. (a) We leverage dense initialization for achieving Gaussian locations, and optimize the locations and Gaussian attributes with three constraints or strategies: (b) Binocular Stereo Consistency Loss. We construct a binocular view pair by translating an input view with camera positions, where we constrain on the view consistency of binocular view pairs in a self-supervised manner. (c) Opacity Decay Strategy is designed to decay the Gaussian opacity during training for regularizing them. (d) The Color Reconstruction Loss.", "description": "This figure illustrates the proposed method's pipeline.  It shows how dense initialization is used for Gaussian locations, and then these locations and attributes are optimized using three constraints:  binocular stereo consistency loss (creating binocular pairs by translating input views), opacity decay strategy (regularizing Gaussians by decaying opacity during training), and color reconstruction loss.", "section": "3 Methods"}, {"figure_path": "oTEttMIymz/figures/figures_4_1.jpg", "caption": "Figure 2: Illustration of the Gaussian opacity decay strategy.", "description": "This figure illustrates how the opacity decay strategy works. Initially, all Gaussians have similar opacity. As the training progresses, Gaussians closer to the surface have higher opacity gradients, causing their opacity to increase. However, Gaussians further from the surface have lower opacity gradients, leading to their opacity decreasing and eventually being pruned.  This helps to filter out redundant Gaussians and refine the 3D Gaussian representation of the scene.", "section": "3.3 Opacity Decay Strategy"}, {"figure_path": "oTEttMIymz/figures/figures_5_1.jpg", "caption": "Figure 3: Visual comparison on LLFF dataset.", "description": "This figure shows a visual comparison of novel view synthesis and depth rendering results on the LLFF dataset for different methods including RegNeRF, FreeNeRF, SparseNeRF, FSGS, DNGaussian, and the proposed method. The top row displays the rendered images, while the bottom row shows the corresponding depth maps.  It highlights the superior quality and accuracy of the proposed method in both image generation and depth estimation compared to existing state-of-the-art techniques.", "section": "4.4 Comparisons"}, {"figure_path": "oTEttMIymz/figures/figures_6_1.jpg", "caption": "Figure 3: Visual comparison on LLFF dataset.", "description": "This figure compares the visual results of novel view synthesis from several state-of-the-art methods on the LLFF dataset.  Each row shows a different scene, with various methods presented side-by-side and compared to the ground truth (GT). The goal is to demonstrate the improved rendering quality and fidelity achieved by the proposed \"Ours\" method compared to existing techniques. This visualization highlights the differences in rendering accuracy, especially in terms of detail preservation, noise reduction, and overall visual realism.", "section": "4 Comparisons"}, {"figure_path": "oTEttMIymz/figures/figures_7_1.jpg", "caption": "Figure 5: Visual comparison on Blender dataset.", "description": "This figure presents a visual comparison of novel view synthesis results on the Blender dataset for four different methods: FSGS, DNGaussian, the proposed method, and the ground truth.  The top row shows results for a microphone scene, while the bottom row focuses on a potted plant scene.  Red boxes highlight areas where differences between the methods are most apparent. The comparison demonstrates the superior quality and detail preservation achieved by the proposed method compared to the baselines.", "section": "4.4 Comparisons"}, {"figure_path": "oTEttMIymz/figures/figures_9_1.jpg", "caption": "Figure 6: Visual comparison of depth maps before and after using view consistency loss.", "description": "This figure compares depth maps generated with and without the view consistency loss. The left two images show depth maps from the \"orchids\" scene of the LLFF dataset, while the right two images show depth maps from the \"leaves\" scene.  In both cases, the images on the right (using view consistency loss) show a significantly improved alignment of the depth values with the actual surfaces of the objects, resulting in a much more accurate representation of the scene's 3D structure.", "section": "3.2 Binocular Stereo Consistency Constraint"}, {"figure_path": "oTEttMIymz/figures/figures_9_2.jpg", "caption": "Figure 7: Visual comparison of novel view images and Gaussian point clouds.", "description": "This figure shows a comparison of novel view images and Gaussian point clouds generated using different methods.  Specifically, it highlights the impact of different initialization strategies (sparse vs. dense), and the use of an opacity decay strategy on the quality of the resulting novel views and the distribution of Gaussian points in the 3D scene.  The top row displays the rendered images, while the bottom row shows the distributions of Gaussian points.  The red boxes highlight regions of particular interest to illustrate the differences between the methods and the ground truth (GT).", "section": "3 Methods"}, {"figure_path": "oTEttMIymz/figures/figures_15_1.jpg", "caption": "Figure 3: Visual comparison on LLFF dataset.", "description": "This figure compares the visual results of novel view synthesis and depth rendering for several scenes in the LLFF dataset using different methods: RegNeRF, FreeNeRF, SparseNeRF, FSGS, DNGaussian, and the proposed method.  The results highlight the superior quality and depth accuracy of the proposed method compared to the baselines, especially in areas with fine details and complex geometries. The ground truth (GT) images are also provided for comparison.", "section": "4.4 Comparisons"}, {"figure_path": "oTEttMIymz/figures/figures_15_2.jpg", "caption": "Figure A: Visual comparison on LLFF dataset with 3 input views.", "description": "This figure shows a visual comparison of novel view synthesis results on the LLFF dataset using 3 input views.  It compares the results of the proposed method against the DNGaussian method and the ground truth. Each row represents a different scene from the dataset, showcasing the quality of novel view generation for each approach.  The differences highlight the improved accuracy and detail preservation in the proposed method compared to the baseline.", "section": "A More Visualizations"}, {"figure_path": "oTEttMIymz/figures/figures_16_1.jpg", "caption": "Figure 7: Visual comparison of novel view images and Gaussian point clouds.", "description": "This figure shows a visual comparison of novel view images and Gaussian point clouds generated using different initialization methods. The top row shows the results of using sparse initialization, where the Gaussian point clouds are not well-distributed and artifacts are present in the novel views. The middle row shows the results of using opacity decay. The artifacts and noisy points are reduced, resulting in better quality novel views. The bottom row shows the ground truth.", "section": "4.5 Ablation Study"}, {"figure_path": "oTEttMIymz/figures/figures_17_1.jpg", "caption": "Figure 3: Visual comparison on LLFF dataset.", "description": "This figure shows a qualitative comparison of novel view synthesis results on the LLFF dataset.  The results from different methods (RegNeRF, FreeNeRF, SparseNeRF, FSGS, DNGaussian, and the proposed method) are compared to the ground truth.  The images demonstrate the ability of each method to generate realistic novel views from sparse inputs, highlighting the differences in rendering quality and the level of detail preserved.", "section": "4 Experiments"}, {"figure_path": "oTEttMIymz/figures/figures_18_1.jpg", "caption": "Figure E: The warped images and error maps when using different source views.", "description": "This figure shows a comparison of warped images and corresponding error maps when different source views are used for the binocular stereo consistency loss. The columns represent different source view types: shifted camera position, unseen view, and an adjacent training view. The rows display the reference image, warped image, and error map.  The experiment highlights how using a source view that is too far from the reference view or has a significant rotation, can significantly increase the error due to depth estimation inaccuracies and occlusions.", "section": "4.5 Ablation Study"}]