[{"type": "text", "text": "Binocular-Guided 3D Gaussian Splatting with View Consistency for Sparse View Synthesis ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Liang Han1, Junsheng Zhou1, Yu-Shen Liu1\u2217, Zhizhong Han2 ", "page_idx": 0}, {"type": "text", "text": "School of Software, Tsinghua University, Beijing, China1 Department of Computer Science, Wayne State University, Detroit, USA2 ", "page_idx": 0}, {"type": "text", "text": "{hanl23,zhou-js24}@mails.tsinghua.edu.cn liuyushen@tsinghua.edu.cn h312h@wayne.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Novel view synthesis from sparse inputs is a vital yet challenging task in 3D computer vision. Previous methods explore 3D Gaussian Splatting with neural priors (e.g. depth priors) as an additional supervision, demonstrating promising quality and efficiency compared to the NeRF based methods. However, the neural priors from 2D pretrained models are often noisy and blurry, which struggle to precisely guide the learning of radiance fields. In this paper, We propose a novel method for synthesizing novel views from sparse views with Gaussian Splatting that does not require external prior as supervision. Our key idea lies in exploring the self-supervisions inherent in the binocular stereo consistency between each pair of binocular images constructed with disparity-guided image warping. To this end, we additionally introduce a Gaussian opacity constraint which regularizes the Gaussian locations and avoids Gaussian redundancy for improving the robustness and efficiency of inferring 3D Gaussians from sparse views. Extensive experiments on the LLFF, DTU, and Blender datasets demonstrate that our method significantly outperforms the state-of-the-art methods. Project page is available at: https://hanl2010.github.io/Binocular3DGS/. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "3D reconstruction technologies [28, 19] have demonstrated significant advances in synthesizing realistic novel views given a set of dense input views. To explore the challenging task in harsh real-world situations where only sparse inputs are available, some studies learn NeRF [28] with specially designed constraints [17, 44, 43, 49] and regularizations [30, 54, 50] on the view scarcity. However, NeRF-based methods often suffer from slow training and inference speeds, leading to high computational costs that restrict their practical applications. ", "page_idx": 0}, {"type": "text", "text": "3D Gaussian Splatting (3DGS) [19] has achieved notable advantages in rendering quality and efficiency. However, 3DGS is still facing severe challenges with inputting sparse views, where the unstructured 3D Gaussians with limited constraints tend to overfit the given few views, resulting in geometric inaccuracies for scene learning. Some recent studies [31, 71, 22, 51] on sparse view synthesis based on 3DGS employ the commonly-used depth priors from pre-trained models as additional constraints on the Gaussians geometries. However, the neural priors are often noisy and blurry, which struggle to precisely guide the learning of radiance fields. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we aim to design a method that does not require external prior as supervision, which directly explores the self-supervisions from the few input views for improving the quality and efficiency of sparse 3DGS. We justify that the key factors in achieving this goal include 1) learning more accurate scene geometry of Gaussians which leads to consistent views synthesis, and 2) avoiding redundant Gaussians near the surface for better efficiency and filtering noises. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "For learning more accurate scene geometry of Gaussians, we explore the self-supervisions inherent in the binocular stereo consistency to constrain on the rendered depth of 3DGS solely utilized from given input views and synthesized novel views. Our key insight lies in the observation that binocular image pairs implicitly involve the property of view consistency, as demonstrated in the binocular stereo vision methods [12, 11, 58]. Specifically, we first translate the camera of one input view slightly to the left or right to obtain a translational view, from which we render the image and depth from 3DGS. The rendered image and the input one thus form a left-right view pair as in binocular stereo vision. We then leverage the rendered depth and the known camera intrinsic to compute the disparity of the view pair. We conduct the supervisions by warping the rendered image of the translational view to the viewpoint of the input image using the disparity, and constrains on the consistency between the warped and input images. ", "page_idx": 1}, {"type": "text", "text": "To further reduce redundant Gaussians near the scene surface and enhance the quality and efficiency of novel view synthesis, we propose a decay schema for the opacity of Gaussians. Specifically, we simply apply a decay coefficient to the opacity property of the Gaussians, penalizing the opacity during training. To this end, Gaussians with lower opacity gradients (i.e., where the increase in opacity is smaller than the decay) are pruned, while Gaussians with higher opacity gradients (i.e., where the increase in opacity is greater than the decay) are retained. As the optimization process continues, redundant Gaussians are flitered out, and those newly generated (copied or split) Gaussians that are closer to the scene surface are retained, resulting in cleaner and more robust Gaussians. This opacity decay strategy significantly reduces artifacts in novel views and decreases the number of Gaussians, improving both the rendering quality and optimization efficiency of 3DGS. ", "page_idx": 1}, {"type": "text", "text": "Additionally, to achieve better geometry initialization for improving 3DGS quality when conducting optimization on sparse views, we use pre-trained keypoints matching network [42] to generate a dense initialization point cloud. The dense point cloud describes the geometry of the scene more accurately, preventing Gaussians from appearing far from the scene surface, especially in low-texture areas where the distribution of Gaussians is subject to limited constraints. ", "page_idx": 1}, {"type": "text", "text": "In summary, our main contributions are as follows. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a novel method for synthesizing novel views from sparse views with Gaussian Splatting that does not require external prior as supervision. We explore the self-supervisions inherent in the binocular stereo consistency to constrain the rendered depth, solely obtained from existing input views and synthesized views.   \n\u2022 We propose an opacity decay strategy which significantly regularizes the learning of Gaussians and reduces redundancy among Gaussians, leading to better rendering quality and optimization efficiency for novel view synthesis from sparse view with Gaussian Splatting.   \n\u2022 Extensive experiments on widely-used forward-facing and 360-degree scene datasets demonstrate that our method achieves state-of-the-art results compared to existing sparse novel view synthesis methods. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Neural Radiance Field ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Neural implicit functions have made great progress in surface reconstruction [45, 65, 16, 64, 56, 62, 63, 60], 3D representation [67, 53, 68, 23, 26, 66, 24] and generation [7, 70, 69, 47, 61, 25]. Detailed and realistic 3D scene representation has always been the research goal in the field of computer vision, and neural radiance fields (NeRFs) [28] has brought fundamental innovation to this domain. NeRF can reconstruct high-quality 3D scenes from sparse 2D images and generate realistic images from arbitrary viewpoints by representing scenes as continuous volume radiance functions. ", "page_idx": 1}, {"type": "text", "text": "However, NeRF requires a large number of views as input during the training stage and exhibits limitations in both training and inference speed. Consequently, the following researches have primarily focused on addressing these bottlenecks by improving computational efficiency [10, 29, 39, ", "page_idx": 1}, {"type": "image", "img_path": "oTEttMIymz/tmp/73432d9b15c45e63ab7c3fba113ba5bb3e897551de016c1ea7e12ec8ad800003.jpg", "img_caption": ["Figure 1: The overview of our method. (a) We leverage dense initialization for achieving Gaussian locations, and optimize the locations and Gaussian attributes with three constraints or strategies: (b) Binocular Stereo Consistency Loss. We construct a binocular view pair by translating an input view with camera positions, where we constrain on the view consistency of binocular view pairs in a self-supervised manner. (c) Opacity Decay Strategy is designed to decay the Gaussian opacity during training for regularizing them. (d) The Color Reconstruction Loss. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "19, 15, 4, 5, 9] or reducing the number of input views [30, 17, 44, 43, 6, 49, 37, 21, 8, 38, 36, 40, 50], while continuously striving for enhanced rendering quality [1, 2, 3]. ", "page_idx": 2}, {"type": "text", "text": "Notably, recent approach 3D Gaussian Splatting [19] has shown promising results in achieving real-time rendering capabilities without compromising rendering quality. ", "page_idx": 2}, {"type": "text", "text": "3D Gaussian Splatting [19] is an emerging method for novel view synthesis, which reconstructs high quality scenes rapidly by utilizing a set of 3D Gaussians to represent the radiance field in the scene. This method performs excellently when dealing with real-world scenes, particularly excelling in handling high-frequency details. Moreover, 3D Gaussian Splatting demonstrates significant advantages in inference speed and offers more intuitive editing and interpretability capabilities. ", "page_idx": 2}, {"type": "text", "text": "2.2 Novel View Synthesis from Sparse Views ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Recent researches [30, 17, 44, 43, 6, 49, 37, 21, 8, 38, 36, 40, 50] have explored various approaches to generate novel views from sparse input images, focusing on enhancing both rendering quality and efficiency. Methods such as NeRF[28] and 3DGS [19] have been refined through various techniques, resulting in continuous improvement in the quality of novel view synthesis. ", "page_idx": 2}, {"type": "text", "text": "Using depth priors obtained from pre-trained networks [55, 32, 33] to supervise neural radiance fields is a widely used technique [38, 44]. Some methods [30, 54, 50, 8] introduce regularization terms in NeRF to address the problem, such as frequency regularization [54] and ray entropy regularization [20]. Additionally, there are also some methods [50, 49, 17] that leverage pre-trained models such as Diffusion model to enhance the rendering quality of novel views. However, most methods incur high costs during training and inference. While some methods have improved inference efficiency through generalizable models [5, 57, 52] or by using voxel grids [40], they often sacrifice rendering quality. ", "page_idx": 2}, {"type": "text", "text": "Currently, with the advent of 3D Gaussian splatting, some methods use 3DGS for sparse view synthesis, such as FSGS [71], SparseGS [51], DNGaussian [22] and CoherentGS [31]. These methods utilize depth priors obtained from pre-trained models [32] as supervision. Depth constraints enable the unstructured Gaussians to approximately distribute along the scene surface, thereby enhancing the quality of novel view images. However, the depth priors from pre-trained models often contain significant errors and cannot make the Gaussians distribute to optimal positions. In contrast, our method employs view consistency constraints based on binocular vision, resulting in more accurate depth information and thereby achieving a more optimal distribution of Gaussians. Some concurrent studies [48, 34] also employed the concept of binocular stereo in 3DGS [19], but they feed binocular images into a pre-trained network to obtain depth priors, instead of self-supervision. ", "page_idx": 2}, {"type": "text", "text": "3 Methods ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The pipeline of our method is depicted in Figure 1. In this section, we first review the 3D representation method based on 3D Gaussians. Then, we explain how to construct stereo view pair and utilize it to enforce view consistency in a self-supervised manner. Next, we introduce the opacity decay strategy and the dense initialization method for 3D Gaussians. Finally, we present the overall loss function used for optimization. ", "page_idx": 3}, {"type": "text", "text": "3.1 Review of 3D Gaussian Splatting ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Gaussian splatting [19] represents scene with a set of 3D Gaussians. Each 3D Gaussian is defined by a central location $\\mu\\in\\mathbb{R}^{3}$ , and a covariance matrix $\\Sigma\\in\\mathbb{R}^{3\\times3}$ . Formally, it is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\nG_{i}(x)=e^{-\\frac{1}{2}(x-\\mu_{i})^{T}\\Sigma^{-1}(x-\\mu_{i})},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the covariance matrix $\\Sigma$ has physical meaning only when it is positive semi-definite. Therefore, it can be decomposed into $\\Sigma={R\\dot{S}\\dot{S}^{T}R^{T}}$ , $S\\in\\mathbb{R}^{\\bar{3}}$ is a diagonal scaling matrix with 3 parameters, $R\\in\\mathbb{R}^{4}$ is a rotation matrix analytically expressed with quaternions. In addition, for rendering the image, each Gaussian also stores an opacity value $\\alpha\\in\\mathbb R$ and a color feature $f\\in\\mathbb{R}^{k}$ . ", "page_idx": 3}, {"type": "text", "text": "The 3D Gaussian is projected into the 2D image space when rendering an image, the projected 2D Gaussian is sorted by its depth value, and then alpha blending is used to calculate the color of each pixel, ", "page_idx": 3}, {"type": "equation", "text": "$$\nc=\\sum_{i=1}^{n}c_{i}\\alpha_{i}^{\\prime}\\prod_{j=1}^{i-1}(1-\\alpha_{j}^{\\prime}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $c_{i}$ is the color computed from feature $f$ and $\\alpha^{\\prime}$ is the opacity of the 2D Gaussian, which is obtained by multiplying the covariance $\\Sigma^{\\prime}$ of the 2D Gaussian by the opacity $\\alpha$ of the corresponding 3D Gaussian. The 2D covariance matrix $\\Sigma^{\\prime}$ is calculated by $\\dot{\\Sigma^{\\prime}}=\\bar{J}W\\dot{\\Sigma}W^{T}J^{T}$ , where $J$ is the Jacobian of the affine approximation of the projective transformation. $W$ is the view transformation matrix. ", "page_idx": 3}, {"type": "text", "text": "The optimization process of Gaussian splatting is initialized with a set of 3D Gaussians from a sparse point cloud from SfM. Subsequently, the density of the Gaussian set is optimized and adaptively controlled. During optimization, a fast tile-based renderer is employed, allowing competitive training and inference times compared to the fast NeRF based methods. ", "page_idx": 3}, {"type": "text", "text": "3.2 Binocular Stereo Consistency Constraint ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The key factor in improving the rendering quality of 3DGS is to guide the Gaussians to be distributed close to the exact scene surfaces. To this end, we aim to design a constraint on the rendered depth of 3DGS which directly represents the Gaussian geometry. Previous studies commonly adopt the depth priors from pretrained models to guide the depth of 3DGS. However, the depth priors are often noisy and blurry, especially for complex scenes, which fails in providing accurate guidance. ", "page_idx": 3}, {"type": "text", "text": "In this paper, we propose a novel prior-free method which leverages the binocular stereo consistency in 3D Gaussian splatting. We constrain on the rendered depth of 3DGS solely obtained by the existing input views and the novel views rendered by 3DGS. Specifically, we first render a novel view from a view point which is translated from one of the input views, leading to a pair of binocular vision images. We then shift the novel view to the perspective of the input image using disparity. Finally, we constrain the rendered image using the consistency between the input image and the warped image. ", "page_idx": 3}, {"type": "text", "text": "Specifically, given an image view $I_{l}$ as input, we translate its corresponding camera position $C_{l}$ to the right by a distance $d_{c a m}$ and obtain the translational camera position $O_{r}$ . We then obtain a novel rendered image $I_{r}$ by rendering 3D Gaussians from $O_{r}$ . The images $I_{l}$ and $I_{r}$ form a binocular stereo image pair, where $I_{l}$ is the left image and $I_{r}$ is the right one, respectively. Simultaneously, we can obtain the rendered depth $D_{l}$ corresponding to the left image $I_{l}$ by rendering 3DGS from $O_{l}$ . According to the geometric theory of binocular stereo vision, the relationship between disparity $d$ and depth $D_{l}$ is given by $d=f\\cdot d_{c a m}/D_{l}$ , where $d\\in\\mathbb{R}^{h\\times w}$ and each value $d_{i j}$ in $d$ indicates the horizontal shift required to align a pixel in the right image with its counterpart in the left image. $f$ is the focal length of the camera. Then, we can use the disparity $d$ to move each pixel of the right image ", "page_idx": 3}, {"type": "text", "text": "$I_{r}$ , obtaining the reconstructed left image $I_{s h i f t e d}$ , as denoted by ", "page_idx": 4}, {"type": "equation", "text": "$$\nI_{s h i f t e d}[i,j]=I_{r}[i-d_{i},j-d_{j}].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In practice, to make the reconstructed left image $I_{s h i f t e d}$ differentiable, we use a bilinear sampler to interpolate on the right image $I_{r}$ to obtain each pixel of $I_{s h i f t e d}$ . The sampling coordinates for each pixel are directly obtained from the disparity. Formally, we define the loss function by ", "page_idx": 4}, {"type": "equation", "text": "$$\nL_{c o n s i s}=\\frac{1}{N}\\sum_{i,j}\\left|I_{i j}^{l}-I_{i j}^{s h i f t e d}\\right|.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To achieve better optimization with the loss function, we do not use a fixed camera position shift in practice. Instead, we randomly translate the input camera to the left or right by a distance within a range $[-d_{m a x},d_{m a x}]$ in each iteration. ", "page_idx": 4}, {"type": "text", "text": "Although some methods [38, 6, 21] use unseen views or neighboring training views as source views and warp them to the reference view for consistency constraints, they do not account for the impact of the distance and angle between the source and reference views on the effectiveness of the supervision. When the source view is rotated or moved away from the reference view, the warped image suffers severe distortion due to depth errors and occlusions. Compared to the ground truth (GT) image, this leads to larger errors, hindering the convergence of the image-warping loss. In contrast, slight camera translations that cause minor changes in view without rotation and negligible impact from occlusions, allow the errors between the warped image and the GT image to primarily stem from depth errors, facilitating better optimization of depth. For the results using different source views, please refer to the Appendix. ", "page_idx": 4}, {"type": "text", "text": "3.3 Opacity Decay Strategy ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We further justify that relying solely on the depth constraints does not always lead to correct Gaussian geometries that are closely aligned with the exact scene surfaces. The reason is that the rendered depth varies with changes in the scale and opacity of the Gaussians, rather than being solely determined by their positions. While 3DGS flexibly optimizes the scale and opacity during training, which leads to deviations in the novel views. To address this issue, we design a simple strategy by applying a decay coefficient $\\lambda$ to the opacity $\\alpha$ of the Gaussians, penalizing the opacity during training. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\alpha}=\\lambda\\alpha,0<\\lambda<1.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We illustrate the opacity decay strategy in Figure 2. Assuming all Gaussians are in the initialized state, due to the accumulation of constraints from multiple views, Gaussians near the scene surface typically have larger opacity gradients, allowing opacity to rise rapidly and construct the scene surface. However, some Gaussians far from the surface fail to get their opacity decreased and be pruned due to insufficient multiview consistent constraints, eventually affecting the rendering quality of novel views. ", "page_idx": 4}, {"type": "text", "text": "We aim to improve and stabilize the optimization of 3DGS by filtering out the far away Gaussians which indicates incorrect geometry while remain the ones close to the exact scene surfaces. This is achieved by applying the opacity decay strategy. We justify that the strategy does not lead all Gaussians\u2019 opacity going down, such as the ones on the surface, since these Gaussians\u2019 opacity progressively increase. As illustrated in Figure 2, the Gaussians far from the scene surface have lower opacity gradients due ", "page_idx": 4}, {"type": "image", "img_path": "oTEttMIymz/tmp/a4dbfece892407db6c577071893a66284fc02127e7a72cc77d3801c11b795fa0.jpg", "img_caption": ["Opacity increase due to gradient backward Opacity decay "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: Illustration of the Gaussian opacity decay strategy. ", "page_idx": 4}, {"type": "text", "text": "to fewer constraints, meaning their opacity increases less than those of the Gaussians on the scene surface. Under the opacity decay strategy, the opacity of Gaussians with lower opacity gradients gradually decreases until they are pruned. Conversely, the increase in opacity for Gaussians near the scene surface exceeds the decay magnitude, ultimately achieving a balance between the opacity increase and the decay, thereby preserving Gaussians close to the surface. ", "page_idx": 4}, {"type": "table", "img_path": "oTEttMIymz/tmp/a464eb6e85ed8cc390fd51858983c4006744d394102a379845884b8c504131d1.jpg", "table_caption": ["Table 1: Quantitative comparison on LLFF. We evaluate the NeRF-based and the 3DGS-based methods, our method achieves the best results in all metrics under different input-view settings. "], "table_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "oTEttMIymz/tmp/27e741c948d5ebd9f81964dcda622312c7825235738a5540858ce651dad1aaad.jpg", "img_caption": ["Figure 3: Visual comparison on LLFF dataset. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "3.4 Initialization from dense point clouds ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Previous 3DGS methods [19, 71, 51] usually utilize a sparse point cloud generated by Structure from Motion (SfM) [35] to initialize 3D Gaussians. However, the point cloud produced by sparse views is too sparse to adequately describe the scene to be reconstructed. Although the splitting strategy in 3DGS can replicate new Gaussians to cover the under-reconstructed area, they are subject to limited geometric constraints and cannot adhere well to the scene surfaces, especially for low-texture areas where the distribution of Gaussians may be arbitrary. We therefore seek a robust approach to achieve better geometry initialization for improving 3DGS quality when optimizing from sparse views. Note that this prior is just used for initialization, and we do not use any this kind of priors during learning 3D Gaussians. ", "page_idx": 5}, {"type": "text", "text": "To achieve this, we use a pre-trained keypoints matching network [42] to generate a dense initialization point cloud. Specifically, we arbitrarily select two images from the input images, input them into the matching network, and obtain matching points. We then leverage the triangulation method [14], along with the camera parameters corresponding to these images, to project the matching points into 3D space. This forms a dense point cloud, providing a more robust initialization for the Gaussians. ", "page_idx": 5}, {"type": "text", "text": "Compared with the sparse point cloud, the dense point cloud describes the geometry of the scene more accurately, preventing Gaussians from appearing far from the scene surface and ultimately leading to improved quality in novel view synthesis. ", "page_idx": 5}, {"type": "table", "img_path": "oTEttMIymz/tmp/2453523f49199ef1f002c1a3bdcf202c96961b24b7f20416d1654431556dbea0.jpg", "table_caption": ["Table 2: Quantitative comparison on DTU. We evaluate the NeRF-based and the 3DGS-based methods, our method achieves the best results in most metrics under different input-view settings. "], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "oTEttMIymz/tmp/5ad6e0874905da4d895572a588bb41df317b78a9385e7e78489358c8a0fa307a.jpg", "img_caption": ["Figure 4: Visual comparison on DTU dataset. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "3.5 Training Loss ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The final loss function consists of two parts: the proposed binocular stereo consistency loss $L_{c o n s i s}$ as introduced in Eq. (4) and the commonly-used color reconstruction loss $L_{c o l o r}$ of 3DGS [19]. We define the overall loss function by ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\cal L}={\\cal L}_{c o n s i s}+{\\cal L}_{c o l o r},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $L_{c o l o r}$ is composed of an $L_{1}$ loss and a structural similarity loss $L_{D-S S I M}$ , as denoted by ", "page_idx": 6}, {"type": "equation", "text": "$$\nL_{c o l o r}=(1-\\beta)L_{1}+\\beta L_{D-S S I M}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conduct experiments on three public datasets, including the LLFF dataset [27], the DTU dataset [18] and the NeRF Blender Synthetic dataset (Blender) [28]. Following prior works [30, 54, 17], we used 3, 6, and 9 views as training sets for the LLFF and DTU datasets, and 8 images for training on the Blender dataset. The selection of test images remained consistent with previous works [17, 30, 54]. The downsampling rates for the LLFF, DTU, and Blender datasets are 8, 4, and 2, respectively. ", "page_idx": 6}, {"type": "text", "text": "4.2 Implementation details ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Since the LLFF and DTU are datasets of forward-facing scenes, they cannot be constrained by views from other directions during the optimization process. On the other hand, Blender is a dataset of 360-degree scenes, 8 images from different viewpoints are used as input during training, providing stronger constraints. Therefore, for the LLFF and DTU datasets, we utilize a pre-trained matching network PDC-Net $^+$ [42] to obtain keypoints from input images, which are then used as the dense initialization point clouds. For the Blender dataset, we adopt random initialization as in the original 3DGS [19]. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Moreover, based on the analysis above, we train the LLFF and DTU datasets for 30,000 iterations, while the Blender dataset is trained for 7,000 iterations. Since the view consistency constraint based on binocular stereo vision needs to be performed on the basis of the training views that can already be rendered with high quality, we add the view consistency loss at 20,000 iterations for the LLFF and DTU datasets, and at 4,000 iterations for the Blender dataset. The maximum distance $d_{m a x}$ for camera shift is set to 0.4, the opacity decay coefficient $\\lambda$ is set to 0.995, and the $\\beta$ in the loss function 7 is set to 0.2 as in the original 3DGS [19]. ", "page_idx": 7}, {"type": "table", "img_path": "oTEttMIymz/tmp/6549f41171276234755d8fbe1c0d20fbb912ef089be5e8d7030af58d83e2af62.jpg", "table_caption": ["Table 3: Quantitative comparison on Blender for 8 input views. We evaluate the NeRF-based and the 3DGS-based methods, our method achieves comparable results to the state of the art methods. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.3 Baseline ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We choose some state-of-the-art NeRF-based and 3DGS-based sparse view synthesis methods for comparison. NeRF-based methods include DietNeRF [17], RegNeRF [30], FreeNeRF [54], SparseNeRF [44], ReconFusion [49] and MuRF [52]. 3DGS-based methods include DNGaussian [22] and FSGS [71]. Additionally, we compare against the original 3DGS [19]. ", "page_idx": 7}, {"type": "text", "text": "4.4 Comparisons ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "LLFF. Table 1 shows the quantitative results of the LLFF dataset with 3, 6, and 9 input views, respectively. Our method surpasses all baseline methods in terms of PSNR, SSIM [46], and LPIPS [59] scores under different number of input views. When the number of input views increases to 9, it is almost adequate to provide sufficient color constraints. However, the evaluation scores of DNGaussian [22] do not show a significant improvement, indicating that it does not perform well with an increased number of input views. This is because errors in the depth prior negatively affect the optimization process. ", "page_idx": 7}, {"type": "text", "text": "Figure 3 presents the visual comparison of novel view synthesis and depth rendering for the leaves and orchids scenes in the LLFF dataset. From the rendered results of novel views, several state-of-the-art methods are all perceptually acceptable. However, regarding the depth maps, RegNerf [30], FreeNerf [54], and FSGS [71] exhibit poor quality, indicating significant errors in their geometric estimation. SparseNeRF [44] and DNGaussian [22] indirectly utilize relative depth information from depth priors to supervise geometry estimation, hence achieving adequate rendering depth. Although FSGS [71] also exploits depth priors, it attempts to directly employ ", "page_idx": 7}, {"type": "image", "img_path": "oTEttMIymz/tmp/2555edfcfd52e5523c5d66fdda368939f56cf0ce3e97b37f164afbc18ef5630d.jpg", "img_caption": ["Figure 5: Visual comparison on Blender dataset. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "depth prior information by using depth correlation loss. It does not yield accurate scene geometry, instead, inaccurate depth prior information exacerbates scene geometry. In contrast, our method utilizes self-supervised view consistency loss to obtain more precise geometry, thereby recovering more structural details in novel views. ", "page_idx": 7}, {"type": "text", "text": "DTU. Table 2 shows the quantitative results of the DTU dataset under 3, 6, and 9 input views respectively. MuRF [52] performs better on the DTU dataset when using 3 views as input. However, generalizable methods require large scale datasets and time-consuming training to learn a prior. Figure 4 illustrates the visual comparison of novel view synthesis for three scenes in the DTU dataset. ", "page_idx": 7}, {"type": "table", "img_path": "oTEttMIymz/tmp/930363ae4d25ab9011c9d842bae3d314b4d32a404b11748f67972d624f2a679f.jpg", "table_caption": ["Table 4: Ablation studies on LLFF and DTU dataset with 3 input views. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "To distinctly discern the differences among various methods, we select the synthesized images far from the training views for comparison in each scene. From the rendered results of novel views, it can be observed that NeRF-based methods produce blurry results, while FSGS [71] and DNGaussian [22] exhibit numerous artifacts due to insufficient constraints on Gaussians. Our method outperforms previous state-of-the-art methods on most evaluation metrics and achieves better visual quality as well. ", "page_idx": 8}, {"type": "text", "text": "Blender. We evaluate our method on 360-degree scenes using the Blender dataset. Table 3 presents the quantitative results under 8 input views. Our approach outperforms all baselines in the PSNR score, and achieves comparable SSIM and LPIPS. We could not reproduce the results reported in the FSGS [71] on the Blender dataset and used the own test results in Table 3. Figure 5 illustrates the visual comparison results of several 3DGS-based methods. It can be observed that FSGS [71] exhibits noticeable artifacts in the synthesized images due to insufficient Gaussian constraints, and the rendering results of DNGaussian [22] also has some distortions. In contrast, our method performs better in terms of detail preservation. ", "page_idx": 8}, {"type": "text", "text": "4.5 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To verify the effectiveness of the view consistency loss, opacity decay strategy and the dense initialization for Gaussians, we isolate each of these modules separately while keeping the other modules unchanged. We then evaluate the metrics and illustrate the visual results. As shown in Tables 4 and 5, the performance ", "page_idx": 8}, {"type": "table", "img_path": "oTEttMIymz/tmp/fcae6aaa033a8bc42c74a79d0fad0b8b46ea22252b5da4e8851cb21855fcc883.jpg", "table_caption": ["Table 5: Ablation studies on Blender dataset with 3 input views. "], "table_footnote": ["decreases when removing any of the modules we proposed. "], "page_idx": 8}, {"type": "text", "text": "Effectiveness of view consistency loss. To verify the effectiveness of the view consistency loss, we compare the depth maps from novel views. Figure 6 shows the visual comparison of rendered depth maps. The comparison in the figure shows that it is evident that the view consistency loss significantly improves the estimation of scene geometry. ", "page_idx": 8}, {"type": "text", "text": "Effectiveness of opacity decay. Figure 7 (b) and (c) show the comparison of novel view synthesis results and the visualization of Gaussian centers for the trex scene in LLFF before and after applying the opacity decay. It can be seen that without the opacity decay, Gaussians appear far from the surfaces and there is noise near the surfaces. After applying the opacity decay strategy, the Gaussians far from the scene surfaces are eliminated, and the noisy point clouds are significantly reduced, thereby further improving the rendering quality of novel views. ", "page_idx": 8}, {"type": "text", "text": "Effectiveness of dense initialization. Figure 7 (a) and (c) show the novel view images and the final Gaussian point clouds obtained using different initialization point cloud. It can be seen that without dense point cloud initialization, the spatial positions of the Gaussians become arbitrary in some low-texture regions or areas occluded in the training views due to insufficient constraints, resulting in reduced quality of the novel view images. ", "page_idx": 8}, {"type": "image", "img_path": "oTEttMIymz/tmp/53d88eac2a9b2a14b2d2057f61b0f7165c64fe91eca9fcb34ecd6c79473e34cf.jpg", "img_caption": ["Figure 6: Visual comparison of depth maps before and after using view consistency loss. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "oTEttMIymz/tmp/a9bd60305ca3307beb30ebff50ed7296e4205d3cb802c4248205c4f3effb3e49.jpg", "img_caption": ["Figure 7: Visual comparison of novel view images and Gaussian point clouds. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose a novel method for novel view synthesis from sparse views with 3DGS. We construct a self-supervised multi-view consistency constraint using the rendered and input images, and introduce a Gaussian opacity decay and a dense point cloud initialization strategy. These constraints ensure that the Gaussians are distributed as closely as possible to the scene surfaces and fliter out those far from the surfaces. Our approach enables the unstructured Gaussians to accurately represent scene geometry even with sparse input views, resulting in high-quality novel rendering images. Extensive experiments on the LLFF, DTU and Blender datasets demonstrate that our method outperforms existing state-of-the-art sparse view synthesis methods. ", "page_idx": 9}, {"type": "text", "text": "Limitation. Since our method utilizes the view consistency constraints for estimating scene depth, some scene areas with low texture may lead to the inaccurate depth estimation (e.g. the white background areas in the DTU dataset), thus failing to constrain the corresponding Gaussians. This results in the white Gaussians potentially occluding the object in the novel views. In contrast, DNGaussian [22] uses the depth prior estimated by the pre-trained network to constrain the Gaussians, preventing this scenario from happening. The visual comparisons are presented in the Appendix. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by National Key R&D Program of China (2022YFC3800600), the National Natural Science Foundation of China (62272263, 62072268), and in part by Tsinghua-Kuaishou Institute of Future Media Data. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan. Mip-NeRF: A multiscale representation for anti-aliasing neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5855\u20135864, 2021. ", "page_idx": 9}, {"type": "text", "text": "[2] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. MipNeRF 360: Unbounded anti-aliased neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5470\u20135479, 2022.   \n[3] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Zip-NeRF: Anti-aliased grid-based neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19697\u201319705, 2023.   \n[4] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. TensoRF: Tensorial radiance fields. In European Conference on Computer Vision, pages 333\u2013350. Springer, 2022.   \n[5] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su. MVSNeRF: Fast generalizable radiance field reconstruction from multi-view stereo. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14124\u2013 14133, 2021.   \n[6] Di Chen, Yu Liu, Lianghua Huang, Bin Wang, and Pan Pan. GeoAug: Data augmentation for few-shot nerf with geometry constraints. In European Conference on Computer Vision, pages 322\u2013337. Springer, 2022.   \n[7] Gene Chou, Yuval Bahat, and Felix Heide. Diffusion-SDF: Conditional generative modeling of signed distance functions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2262\u20132272, 2023.   \n[8] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. Depth-supervised NeRF: Fewer views and faster training for free. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12882\u201312891, 2022.   \n[9] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahb\u00e6k Warburg, Benjamin Recht, and Angjoo Kanazawa. K-Planes: Explicit radiance fields in space, time, and appearance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12479\u201312488, 2023.   \n[10] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5501\u20135510, 2022.   \n[11] Cl\u00e9ment Godard, Oisin Mac Aodha, and Gabriel J Brostow. Unsupervised monocular depth estimation with left-right consistency. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 270\u2013279, 2017.   \n[12] Cl\u00e9ment Godard, Oisin Mac Aodha, Michael Firman, and Gabriel J Brostow. Digging into self-supervised monocular depth estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3828\u20133838, 2019.   \n[13] Antoine Gu\u00e9don and Vincent Lepetit. SuGaR: Surface-aligned gaussian splatting for efficient 3D mesh reconstruction and high-quality mesh rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5354\u20135363, 2024.   \n[14] Richard Hartley and Andrew Zisserman. Multiple View Geometry in Computer Vision. Cambridge university press, 2003.   \n[15] Wenbo Hu, Yuling Wang, Lin Ma, Bangbang Yang, Lin Gao, Xiao Liu, and Yuewen Ma. TriMipRF: Tri-Mip representation for efficient anti-aliasing neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19774\u201319783, 2023.   \n[16] Han Huang, Yulun Wu, Junsheng Zhou, Ge Gao, Ming Gu, and Yu-Shen Liu. NeuSurf: Onsurface priors for neural surface reconstruction from sparse input views. In Proceedings of the AAAI Conference on Artificial Intelligence, 2024.   \n[17] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting NeRF on a Diet: Semantically consistent few-shot view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5885\u20135894, 2021.   \n[18] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola, and Henrik Aan\u00e6s. Large scale multi-view stereopsis evaluation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 406\u2013413, 2014.   \n[19] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00fchler, and George Drettakis. 3D gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4):1\u201314, 2023.   \n[20] Mijeong Kim, Seonguk Seo, and Bohyung Han. InfoNeRF: Ray entropy minimization for few-shot neural volume rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12912\u201312921, 2022.   \n[21] Min-Seop Kwak, Jiuhn Song, and Seungryong Kim. GeCoNeRF: Few-shot neural radiance fields via geometric consistency. 2023.   \n[22] Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Xin Ning, Jun Zhou, and Lin Gu. DNGaussian: Optimizing sparse-view 3D gaussian radiance fields with global-local depth normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20775\u201320785, 2024.   \n[23] Shujuan Li, Junsheng Zhou, Baorui Ma, Yu-Shen Liu, and Zhizhong Han. NeAF: Learning neural angle fields for point normal estimation. In Proceedings of the AAAI Conference on Artificial Intelligence, 2023.   \n[24] Shujuan Li, Junsheng Zhou, Baorui Ma, Yu-Shen Liu, and Zhizhong Han. Learning continuous implicit field with local distance indicator for arbitrary-scale point cloud upsampling. In Proceedings of the AAAI Conference on Artificial Intelligence, 2024.   \n[25] Baorui Ma, Haoge Deng, Junsheng Zhou, Yu-Shen Liu, Tiejun Huang, and Xinlong Wang. GeoDream: Disentangling 2D and geometric priors for high-fidelity and consistent 3D generation. arXiv preprint arXiv:2311.17971, 2023.   \n[26] Baorui Ma, Junsheng Zhou, Yu-Shen Liu, and Zhizhong Han. Towards better gradient consistency for neural signed distance functions via level set alignment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17724\u201317734, 2023.   \n[27] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM Transactions on Graphics, 38(4):1\u201314, 2019.   \n[28] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99\u2013106, 2021.   \n[29] Thomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. ACM Transactions on Graphics, 41(4):1\u201315, 2022.   \n[30] Michael Niemeyer, Jonathan T Barron, Ben Mildenhall, Mehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. RegNeRF: Regularizing neural radiance fields for view synthesis from sparse inputs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5480\u20135490, 2022.   \n[31] Avinash Paliwal, Wei Ye, Jinhui Xiong, Dmytro Kotovenko, Rakesh Ranjan, Vikas Chandra, and Nima Khademi Kalantari. CoherentGS: Sparse novel view synthesis with coherent 3D gaussians. In European Conference on Computer Vision, 2024.   \n[32] Ren\u00e9 Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12179\u2013 12188, 2021.   \n[33] Ren\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(3):1623\u20131637, 2020.   \n[34] Sadra Safadoust, Fabio Tosi, Fatma G\u00fcney, and Matteo Poggi. Self-evolving depth-supervised 3D gaussian splatting from rendered stereo pairs. 2024.   \n[35] Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 4104\u20134113, 2016.   \n[36] Seunghyeon Seo, Yeonjin Chang, and Nojun Kwak. FlipNeRF: Flipped reflection rays for few-shot novel view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22883\u201322893, 2023.   \n[37] Nagabhushan Somraj and Rajiv Soundararajan. Vip-NeRF: Visibility prior for sparse input neural radiance fields. In ACM SIGGRAPH 2023 Conference Proceedings, pages 1\u201311, 2023.   \n[38] Jiuhn Song, Seonghoon Park, Honggyu An, Seokju Cho, Min-Seop Kwak, Sungjin Cho, and Seungryong Kim. D\u00e4RF: Boosting radiance fields from sparse input views with monocular depth adaptation. Advances in Neural Information Processing Systems, 36, 2024.   \n[39] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5459\u20135469, 2022.   \n[40] Jiakai Sun, Zhanjie Zhang, Jiafu Chen, Guangyuan Li, Boyan Ji, Lei Zhao, and Wei Xing. VGOS: Voxel grid optimization for view synthesis from sparse inputs. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, pages 1414\u20131422, 2023.   \n[41] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8922\u20138931, 2021.   \n[42] Prune Truong, Martin Danelljan, Radu Timofte, and Luc Van Gool. PDC-Net+: Enhanced probabilistic dense correspondence network. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.   \n[43] Prune Truong, Marie-Julie Rakotosaona, Fabian Manhardt, and Federico Tombari. SPARF: Neural radiance fields from sparse and noisy poses. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4190\u20134200, 2023.   \n[44] Guangcong Wang, Zhaoxi Chen, Chen Change Loy, and Ziwei Liu. SparseNeRF: Distilling depth ranking for few-shot novel view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9065\u20139076, 2023.   \n[45] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. NeuS: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. Advances in Neural Information Processing Systems, 34:27171\u201327183, 2021.   \n[46] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600\u2013 612, 2004.   \n[47] Xin Wen, Junsheng Zhou, Yu-Shen Liu, Hua Su, Zhen Dong, and Zhizhong Han. 3D shape reconstruction from 2D images with disentangled attribute flow. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3803\u20133813, 2022.   \n[48] Yaniv Wolf, Amit Bracha, and Ron Kimmel. GS2Mesh: Surface reconstruction from gaussian splatting via novel stereo views. In European Conference on Computer Vision, 2024.   \n[49] Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson, Pratul P Srinivasan, Dor Verbin, Jonathan T Barron, Ben Poole, et al. ReconFusion: 3D reconstruction with diffusion priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21551\u201321561, 2024.   \n[50] Jamie Wynn and Daniyar Turmukhambetov. DiffusioNeRF: Regularizing neural radiance fields with denoising diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4180\u20134189, 2023.   \n[51] Haolin Xiong, Sairisheek Muttukuru, Rishi Upadhyay, Pradyumna Chari, and Achuta Kadambi. SparseGS: Real-time $360^{\\circ}$ sparse view synthesis using gaussian splatting. arXiv preprint arXiv:2312.00206, 2023.   \n[52] Haofei Xu, Anpei Chen, Yuedong Chen, Christos Sakaridis, Yulun Zhang, Marc Pollefeys, Andreas Geiger, and Fisher Yu. MuRF: Multi-baseline radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20041\u201320050, 2024.   \n[53] Siming Yan, Zhenpei Yang, Haoxiang Li, Chen Song, Li Guan, Hao Kang, Gang Hua, and Qixing Huang. Implicit autoencoder for point-cloud self-supervised representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14530\u2013 14542, 2023.   \n[54] Jiawei Yang, Marco Pavone, and Yue Wang. FreeNeRF: Improving few-shot neural rendering with free frequency regularization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8254\u20138263, 2023.   \n[55] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth Anything: Unleashing the power of large-scale unlabeled data. pages 10371\u201310381, 2024.   \n[56] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces. In Advances in Neural Information Processing Systems, 2021.   \n[57] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelNeRF: Neural radiance fields from one or few images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4578\u20134587, 2021.   \n[58] Ning Zhang, Francesco Nex, George Vosselman, and Norman Kerle. Lite-Mono: A lightweight cnn and transformer architecture for self-supervised monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18537\u201318546, 2023.   \n[59] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 586\u2013595, 2018.   \n[60] Wenyuan Zhang, Yu-Shen Liu, and Zhizhong Han. Neural signed distance function inference through splatting 3D gaussians pulled on zero-level set. In Advances in Neural Information Processing Systems, 2024.   \n[61] Junsheng Zhou, Yu-Shen Liu, and Zhizhong Han. Zero-shot scene reconstruction from single images with deep prior assembly. In Advances in Neural Information Processing Systems, 2024.   \n[62] Junsheng Zhou, Baorui Ma, Shujuan Li, Yu-Shen Liu, Yi Fang, and Zhizhong Han. CAP-UDF: Learning unsigned distance functions progressively from raw point clouds with consistencyaware field optimization. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.   \n[63] Junsheng Zhou, Baorui Ma, Shujuan Li, Yu-Shen Liu, and Zhizhong Han. Learning a more continuous zero level set in unsigned distance fields through level set projection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023.   \n[64] Junsheng Zhou, Baorui Ma, and Yu-Shen Liu. Fast learning of signed distance functions from noisy point clouds via noise to noise mapping. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.   \n[65] Junsheng Zhou, Baorui Ma, Liu Yu-Shen, Fang Yi, and Han Zhizhong. Learning consistencyaware unsigned distance functions progressively from raw point clouds. In Advances in Neural Information Processing Systems, 2022.   \n[66] Junsheng Zhou, Baorui Ma, Wenyuan Zhang, Yi Fang, Yu-Shen Liu, and Zhizhong Han. Differentiable registration of images and lidar point clouds with voxelpoint-to-pixel matching. In Advances in Neural Information Processing Systems, 2023.   \n[67] Junsheng Zhou, Jinsheng Wang, Baorui Ma, Yu-Shen Liu, Tiejun Huang, and Xinlong Wang. Uni3D: Exploring Unified 3D Representation at Scale. In International Conference on Learning Representations, 2024.   \n[68] Junsheng Zhou, Xin Wen, Baorui Ma, Yu-Shen Liu, Yue Gao, Yi Fang, and Zhizhong Han. 3DOAE: Occlusion auto-encoders for self-supervised learning on point clouds. IEEE International Conference on Robotics and Automation, 2024.   \n[69] Junsheng Zhou, Weiqi Zhang, and Yu-Shen Liu. DiffGS: Functional gaussian splatting diffusion. In Advances in Neural Information Processing Systems, 2024.   \n[70] Junsheng Zhou, Weiqi Zhang, Baorui Ma, Kanle Shi, Yu-Shen Liu, and Zhizhong Han. UDiFF: Generating conditional unsigned distance fields with optimal wavelet diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.   \n[71] Zehao Zhu, Zhiwen Fan, Yifan Jiang, and Zhangyang Wang. FSGS: Real-time few-shot view synthesis using gaussian splatting. In European Conference on Computer Vision, pages 145\u2013163. Springer, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "image", "img_path": "oTEttMIymz/tmp/a1de5c147348f6931595bfad279dd34c960a92f5e75a20ef812856bf191f5ed6.jpg", "img_caption": ["Figure A: Visual comparison on LLFF dataset with 3 input views. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "oTEttMIymz/tmp/7c7dbe5a7cd8a084a432fc9377ee4fe3cf2fee9628beac3ccc34af5ded016570.jpg", "img_caption": ["Figure B: Visual comparison on DTU dataset with 3 input views. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "A More Visualizations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Additional Results on LLFF ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Figure A shows the visualization comparison of our method and DNGaussian [22] across all scenes in the LLFF [27] dataset. Each scene is trained using the same 3 input views. Although both methods achieve perceptually acceptable results, the novel view rendering results of our method are closer to the Ground Truth. ", "page_idx": 15}, {"type": "table", "img_path": "oTEttMIymz/tmp/63ea383961f4df644f3d4a36b02f2ad1e93598129d14b816130925f4ea2f0dd9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "oTEttMIymz/tmp/82d84dd3d080ac3af3030ca403e5ad12c2fd8c10ba148659a34e14b14949b9c1.jpg", "img_caption": ["Figure C: Visual comparison of using different regularization components for opacity. "], "img_footnote": ["Table A: Quantitative comparison of using different regularization components for opacity. "], "page_idx": 16}, {"type": "text", "text": "A.2 Additional Results on DTU ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Figure B shows an additional visual comparison between our method and DNGaussian [22] on the DTU [18] dataset. Each scene is trained using the same 3 input views. To clearly distinguish the differences between the two methods, we select a novel unseen view that is far from the training views for comparison. As analyzed in the limitations section of the main text, our method cannot effectively constrain the white background in training views, resulting in objects being occluded by white Gaussians in some scenes. Even so, the rendering quality of the novel views from our method is still better than the results from DNGaussian. ", "page_idx": 16}, {"type": "text", "text": "B More Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Entropy regularization instead of Opacity Decay ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The Sugar [13] method applies entropy regularization to the opacity of Gaussians to bring Gaussians closer to the surface, similar to our proposed opacity decay strategy. Therefore, we also evaluate the performance of opacity entropy regularization on the LLFF dataset. Following Sugar [13], the opacity threshold for Gaussians pruning is set to 0.5. Table A shows the metrics with opacity entropy regularization. It can be seen that our opacity decay strategy has advantages. Figure C illustrates the novel view synthesis using different opacity regularization strategies. It can be seen that entropy regularization leads to noticeable overftiting with sparse view inputs, resulting in lower quality novel view rendering. ", "page_idx": 16}, {"type": "text", "text": "B.2 Training the DTU with Mask ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To eliminate the undesirable effects of the solid color background on novel view rendering results in the DTU dataset, we perform additional experiments by using masks to filter out the background in the training views. The evaluation results are shown in Table B. Figure D illustrates the visual comparison between our method and DNGaussian [22] when using masks in the training views. It can be seen that without the solid color background, our method shows a significant improvement, while the performance of DNGaussian decreases. ", "page_idx": 16}, {"type": "table", "img_path": "oTEttMIymz/tmp/e75a31dbf7efd18eaa6a30bd1be1cab8364a5da24853b9128a9084ce074a4447.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table B: Quantitative comparison of using background masks for input views on DTU dataset. ", "page_idx": 17}, {"type": "image", "img_path": "oTEttMIymz/tmp/d1fe6dfc7eacd06276560f192b901299f99c3f9e17240900532627f8e786b3eb.jpg", "img_caption": ["Figure D: Visual comparison of using background masks for input views on DTU dataset. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "C Ablation studies for hyperparameters ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 Ablation studies for hyperparameter $\\lambda$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We perform ablation studies on the LLFF [27] and DTU [18] datasets with the value of $\\lambda$ ranging from 0.96 to 1.0, and the results are shown in Table C. We find that the best performance is achieved when the value of $\\lambda$ is 0.995 on both datasets. Therefore we set $\\lambda$ to 0.995 for all datasets. ", "page_idx": 17}, {"type": "table", "img_path": "oTEttMIymz/tmp/1a345f9f886194901e21ea5bfe83c0e6a1e4953ed1114994e2472d07d8d391ee.jpg", "table_caption": ["Table C: Ablation studies for $\\lambda$ on LLFF dataset with 3 input views. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "C.2 Ablation studies for hyperparameter $d_{m a x}$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We perform ablation studies for $d_{m a x}$ on the LLFF[27] and DTU [18] datasets. The value of $d_{m a x}$ gradually increases from 0.1 to 0.8, we obtain the results as shown in Table D. We find that there is almost no performance increase on both the LLFF and DTU datasets when $d_{m a x}$ is greater than 0.4, therefore we set the $d_{m a x}$ to 0.4 for all datasets. ", "page_idx": 17}, {"type": "text", "text": "D The impact of initialization on overall performance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We conduct experiments on the LLFF [27] and DTU [18] datasets with several different initialization, including random initialization, sparse initialization, and using the LoFTR [41] matching network to construct initial point clouds. The results are shown in Table E. ", "page_idx": 17}, {"type": "table", "img_path": "oTEttMIymz/tmp/73129176e18d9980d768d783c98822d1cca9100258e4d7466de4340fb1c05b4c.jpg", "table_caption": ["Table D: Ablation studies for $d_{m a x}$ on LLFF and DTU dataset with 3 input views "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "oTEttMIymz/tmp/63a8e976dcbff00ceabc02d80b7fbe93e6f830b56e49ce3b59391d4aef79f1e2.jpg", "table_caption": ["Table E: Impact of different initializations on performance. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "PDCNet+ [42] can obtain more matching points than LoFTR [41], so the initialized point cloud obtained by PDCNet+ results in better performance. For the LLFF [27] dataset, the input views have large overlap, SfM methods can generate point clouds with better quality than LoFTR. Therefore, using sparse point clouds generated by SfM as initialization yields better performance than LoFTR. However, for the DTU [18] dataset, where there is small overlap among input views, SfM-generated point clouds are too sparse. Thus, the performance is even worse than random initialization. ", "page_idx": 18}, {"type": "text", "text": "E The impact of different source views on overall performance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Figure E shows the warped images and error maps when using different source views. It can be observed that when the source view is rotated relative to the reference view or is far away from the reference view, the warped image suffers from significant distortions due to depth errors and occlusions. This results in a large error compared to the GT image. ", "page_idx": 18}, {"type": "image", "img_path": "oTEttMIymz/tmp/0898eaa2cdb8b93eda7b4e86b56f1eb943da6c64ab622adcc1d76484a70bbe72.jpg", "img_caption": ["Figure E: The warped images and error maps when using different source views. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table F shows the quantitative comparison using different source images on the LLFF and DTU datasets. The binocular vision-based view consistency method achieves the best performance. ", "page_idx": 19}, {"type": "table", "img_path": "oTEttMIymz/tmp/3301825f405ba9043dd77d3b3065fff837f5bf68d7422d675b125ca581f9cbfe.jpg", "table_caption": ["Table F: Quantitative comparison using different source images on the LLFF and the DTU dataset. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "F Implementations Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Since our method uses an opacity decay strategy, we do not use the opacity reset strategy from the original 3DGS, and we also remove the scale threshold for Gaussians pruning from the original 3DGS. ", "page_idx": 19}, {"type": "text", "text": "We train our model on an RTX 3090 GPU, running 30,000 iterations per scene on the LLFF and DTU datasets, which takes approximately 10 minutes each. For the Blender dataset, we train for 7,000 iterations per scene, which takes approximately 3 minutes each. The storage space required for the Gaussian point clouds is about one-third of that required by the original 3DGS. ", "page_idx": 19}, {"type": "text", "text": "G Dataset Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "LLFF. The LLFF [27] is a forward-facing dataset with 8 scenes. Following previous works [30, 54], we take every 8th image as the novel views for evaluation. The input views are evenly sampled from the remaining views. During training and evaluation, the images are downsampled by a factor of 8, resulting in a resolution of $378\\!\\times\\!504$ for each image. ", "page_idx": 19}, {"type": "text", "text": "DTU. The DTU [18] dataset consists of 124 scenes. We follow previous works [30, 54] to train and evaluate our method on 15 test scenes. The test scene IDs are: 8, 21, 30, 31, 34, 38, 40, 41, 45, 55, 63, 82, 103, 110, and 114. In each scene, we use images with the following IDs as input views: 25, 22, 28, 40, 44, 48, 0, 8, 13. The first 3 and 6 image IDs correspond to the input views in 3-view and 6-view settings respectively. We use 25 images as novel views for evaluation, with the following IDs: 1, 2, 9, 10, 11, 12, 14, 15, 23, 24, 26, 27, 29, 30, 31, 32, 33, 34, 35, 41, 42, 43, 45, 46, 47. During training and evaluation, the images are downsampled by a factor of 4, resulting in a resolution of 300 $\\times\\,400$ for each image. ", "page_idx": 19}, {"type": "text", "text": "Blender. The Blender [28] dataset consists of 8 synthetic scenes. For fair comparison, we follow previous works [30, 54] and use 8 images as input views for each scene, with the following IDs: 26, 86, 2, 55, 75, 93, 16, 73, 8. The 25 test views are sampled evenly from the testing images for evaluation. During training and evaluation, the images are downsampled by a factor of 2, resulting in a resolution of $400\\times400$ for each image. ", "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The abstract and introduction clearly state the contributions and scope of the paper. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: Our paper includes a dedicated \"Limitation\" paragraph where potential limitations are discussed. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our paper does not include theoretical results. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We clearly describe all the information needed to reproduce the experimental results in the paper, including our method, hyperparameter values, and datasets. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We use the publicly datasets and provided sufficient instructions on how to use them for our method in the supplementary materials. The source code will be publicly available. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We specify hyperparameters in the \"Implementation Details\" section of the paper, with any unspecified hyperparameters being the same as the baselines. The supplementary materials include detailed information on data splits. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [No] ", "page_idx": 22}, {"type": "text", "text": "Justification: We report the average performance in the experimental results. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide the information of GPU resource we used in the supplementary materials. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our research conform with the NeurIPS Code of Ethics in every respect. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our paper proposes a method for synthesizing novel view images from sparse input views. The aim is to observe the objects in the input images from new perspectives. There is no generation of new or fake data involved. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our paper poses no such risks. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We cite the original papers of the code and datasets we used. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our paper does not release new assets. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}]