[{"type": "text", "text": "Analysing the Generalisation and Reliability of Steering Vectors ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Daniel Tan1 David Chanin1 Aengus Lynch1 Adri\u00e0 Garriga-Alonso2 ", "page_idx": 0}, {"type": "text", "text": "Brooks Paige1 Dimitrios Kanoulas1,3 ", "page_idx": 0}, {"type": "text", "text": "Robert Kirk1 1 AI Centre, Department of Computer Science, University College London 2 FAR AI 3 Archimedes/Athena RC Correspondence to: daniel.tan.22@ucl.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Steering vectors (SVs) are a new approach to efficiently adjust language model behaviour at inference time by intervening on intermediate model activations. They have shown promise in terms of improving both capabilities and model alignment. However, the reliability and generalisation properties of this approach are unknown. In this work, we rigorously investigate these properties, and show that steering vectors have substantial limitations both in- and out-of-distribution. In-distribution, steerability is highly variable across different inputs. Depending on the concept, spurious biases can substantially contribute to how effective steering is for each input, presenting a challenge for the widespread use of steering vectors. We additionally show steerability is also mostly a property of the dataset rather than the model by measuring steerability across multiple models. Out-of-distribution, while steering vectors often generalise well, for several concepts they are brittle to reasonable changes in the prompt, resulting in them failing to generalise well. Similarity in behaviour between distributions somewhat predicts generalisation performance, but there is more work needed to understand when and why steering vectors generalise correctly. Overall, our findings show that while steering can work well in the right circumstances, there remain many technical difficulties of applying steering vectors to robustly guide models\u2019 behaviour at scale. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Steering Vectors (SVs) [30, 33, 43, 18] have been recently proposed as a technique for guiding language model behaviour at inference time. Existing work has shown promising results in using these SVs to detect and guide models towards high-level traits such as honesty [43], sycophancy [30], and positive sentiment [31]. They have also been shown to be useful for improving model capabilities [39, 18, 34] and red-teaming [29]. SVs are of interest as they enjoy a number of practical beneftis over other model adjustment techniques that require adding more information into the context window [5, 38, In-Context Learning], or performing training to adjust model parameters (fine-tuning). Recent work shows that steering vectors can be learned in an unsupervised way [20], thus removing another obstacle for their use. It may even be possible for different steering vectors to be used in combination for multiple behaviours [34, 36]. It would thus be very important and useful in practice if steering vectors were truly effective. ", "page_idx": 0}, {"type": "image", "img_path": "v8X70gTodR/tmp/817ebc6df2eb934cda2c265e6a19963c27fbfe91f2679a9237988c0108a08f04.jpg", "img_caption": ["Figure 1: Steering effects are not reliable, and often steer in the opposite direction. We show per-sample steerability and the fraction of anti-steerable examples for a representative sample of 13 datasets (out of 40 total). Many dataset have a high variation in per-sample steerability, and several datasets produce the opposite behaviour for almost $50\\%$ of inputs. For all datasets see Figure 16. Some dataset names have been shortened. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "However, existing work has mostly evaluated SVs in-distribution, and looked at aggregate behaviour. It is unknown how reliable the change in behaviour caused by SVs is, and how well SVs generalise to different system or user prompts. In this paper, we extensively evaluate the in-distribution reliability and out-of-distribution generalisation of SVs, extending analysis in Rimsky et al. [30] to a much broader variety of behaviours from the Model Written Evals (MWE) datasets by Perez et al. [26]. In addition, we consider targeted distribution shifts in the form of inserting prompts via the user message or system message. This setting mimics the practically important setting where we will need to apply SVs to different system and user prompts, and where we would require SVs to generalise well to be robustly useful. ", "page_idx": 1}, {"type": "text", "text": "Our first key result is that for many behaviours studied, steering is unreliable (Figure 1 and Section 5). For all behaviours evaluated, steerability takes on a large range of values across different inputs, including negative values, where SVs produce the opposite of the desired behaviour. Previous work [30, 33, 43, 18] does not study this variance, which potentially leads to over-optimistic claims on performance due to a lack of error bars. In explaining this variance, we demonstrate a novel type of bias, steerability bias, in which models are easier to steer towards outputs with a certain property (i.e. answer position or token choice). The lack of steerability and high variance in steering performance demonstrates that in many cases, a steering vector extracted may not correspond to the intended concept, and applying steering vectors may only be effective in the presence of spurious factors associated with the prompt template or other potential biases. ", "page_idx": 1}, {"type": "text", "text": "Our second set of results focuses on the out-of-distribution setting. Here, we find that SVs generalise reasonably well across different prompt settings, but the generalisation behaviour is not perfect or entirely predictable (Section 6). SVs generalise better over some shifts than others and generally perform worse out-of-distribution vs in-distribution. We investigate what causes this difference in generalisation properties, finding that (i) steerability is mostly a dataset-level property, with similar datasets being steerable and producing generalisable SVs for two different models; and (ii) SVs generalise better when model behaviour is similar in the source and target prompt setting. This relationship is a potential issue for SVs, as SVs will need to be applied to guide models towards behaviours they do not normally produce. ", "page_idx": 1}, {"type": "text", "text": "Overall, our findings indicate that steering vectors in their current form are not a panacea for aligning model behavior at inference time. Despite their promise, more work is required to ensure that steering vectors reliably produce the desired behaviour in a generalisable way and are practically useful. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Steering Vectors (SVs, also known as activation engineering) and related ideas were introduced by Turner et al. [33], Zou et al. [43], Liu et al. [18]. SVs can be seen as an inference-time intervention [16] technique in the representation engineering [43] toolkit, which is an umbrella term for the broad approach of improving the transparency and controllability of neural networks by examining and intervening on population-level representations and activations of the network. [28]. Rimsky et al. [30] recently introduced Contrastive Activation Addition (CAA), a specific technique for extracting and applying SVs which we use in this work, due to its simplicity, effectiveness and popularity in the community. Rimsky et al. [30] demonstrate the effectiveness of CAA in-distribution on several AI alignment-relevant behaviours, while we test on a much broader range of behaviours, investigate the reliability of the steering intervention, and examine out-of-distribution generalisation of SVs. We describe the CAA method in more detail in Section 3. ", "page_idx": 2}, {"type": "text", "text": "Compared to fine-tuning [42, 27, 23], steering vectors don\u2019t involve changing model parameters, hence potentially avoiding catastrophic forgetting [3, 19]. Compared to in-context learning [5, 38, ICL], steering does not require adding tokens to the prompt, saving inference cost and enabling it to scale beyond the length of the context window. Furthermore, Zou et al. [43] show that steering interventions are robust to adversarial attacks capable of breaking prompt-based and fine-tuning-based alignment methods [26, 37, 13]. ", "page_idx": 2}, {"type": "text", "text": "An extended related work section including discussing the relationship between SVs and the Linear Representation Hypothesis [25, LRH] and other works that evaluate the generalisation behaviour of model adjustment methods can be found in Appendix B. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Rimsky et al. [30] propose Contrastive Activation Addition (CAA) to extract and apply steering vectors on datasets. We follow this protocol in our experiments, and so we summarise the main steps here. ", "page_idx": 2}, {"type": "text", "text": "Multiple-Choice Contrastive Prompts. We construct a prompt consisting of a question or statement followed by two multiple-choice options labelled \u201c(A)\u201d and \u201c(B)\u201d. The model is tasked with reading the question and available options $(x)$ , then choosing one of the options $(y_{+}$ or $y_{-}$ ). For some datasets these two options are statements, and for others the two options are either \u201cYes\u201d or \u201cNo\u201d. A typical example is shown in Figure 9. During preprocessing, we randomise whether $\\mathbf{\\omega_{A}},$ or $\\mathbf{\\omega}^{\\bullet}\\mathbf{B}^{\\bullet}$ (and \u2018Yes\u2019 or \u2018No\u2019 where appropriate) are used as the positive $y_{+}$ or negative $y_{-}$ options, to ensure that we do not simply extract a steering vector for e.g. the token \u2018A\u2019 vs the token $\\mathbf{\\omega}^{\\bullet}\\mathbf{B}^{\\bullet}$ . ", "page_idx": 2}, {"type": "text", "text": "Steering Vector Extraction. For a given dataset $\\mathcal{D}$ consisting of triples of the form $(x,y_{+},y_{-})$ , and a given layer $L$ , activations are extracted from the residual stream at the multiple-choice option token position for the positive and negative option, to get $a_{L}(x,y_{+})$ and $a_{L}(x,y_{-})$ respectively. We extract a steering vector $v_{M D}$ using the mean difference (MD) of positive and negative activations: ", "page_idx": 2}, {"type": "equation", "text": "$$\nv_{M D}=\\frac{1}{|\\mathcal{D}|}\\sum_{(x,y_{-},y_{+})\\in\\mathcal{D}}\\left[a_{L}(x,y_{+})-a_{L}(x,y_{-})\\right]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We note that other aggregation methods have been proposed, but literature does not suggest these perform better than mean-difference. We discuss alternatives in Appendix D.4. ", "page_idx": 2}, {"type": "text", "text": "Steering Intervention. To apply a steering intervention at layer $L$ using a steering vector $v_{L}$ , we add $\\lambda\\ast v_{L}$ into the activations at the last token position at layer $L$ during model inference. Here, $\\lambda$ is a multiplier that controls the strength of the steering intervention. For any metric of (change in) behaviour, we can evaluate that metric for a range of $\\lambda\\mathbf{s}$ to ascertain the effectiveness of a steering intervention; more details including our specific choice of metric are discussed subsequently in Section 4.2 ", "page_idx": 2}, {"type": "text", "text": "4 Experiment Design ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1 Datasets and Prompts ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Datasets. We focus on the Model-Written Evaluations (MWE) datasets [26], a large dataset consisting of prompts from over 100 distinct categories designed to evaluate many specific aspects of models\u2019 behaviour. Each category contain 1000 samples generated by an LLM, covering a variety of persona and behaviors. For each of these datasets, we construct a 40-10-50 train-val-test split. We also include TruthfulQA [17] and the sycophancy dataset [26], as they were used in CAA [30]. The validation split is used for hyperparameter selection; we discuss this in Section 4.3. We randomly choose three persona datasets from each MWE persona dataset category, while keeping the sycophancy, TruthfulQA, and AI risk datasets used in CAA for a total of 40 datasets. ", "page_idx": 3}, {"type": "text", "text": "Distribution Shifts. To evaluate how well steering vectors generalise to out-of-distribution settings, we construct systematic distribution shifts by injecting additional text into the prompts. We design the prompts to elicit more or less of the target behaviour through direct instruction. Sample prompt injections are shown in Table 1. As we investigate instruction-tuned models, there are two valid prompt injection strategies: (i) replacing the default system prompt with the injection, and (ii) pre-pending the injection to the user prompt. We evaluate in both settings for completeness. To evaluate generalisation across these distribution shifts, we extract a SV in one of the prompt settings (e.g. BASE), and apply it to steer behaviour in another setting (e.g. SYS-POS), and denote this BASE $\\rightarrow\\mathrm{{SYS}}$ -POS. $\\mathrm{BASE}\\rightarrow\\mathrm{BASE}$ hence represents the standard in-distribution evaluation. ", "page_idx": 3}, {"type": "text", "text": "To measure OOD generalisation, we define relative steerability. This measures how well a steering vector $v_{A}$ trained on dataset variation $\\mathcal{D}_{A}$ works on dataset variation $\\mathcal{D}_{B}$ with multipliers $\\Lambda$ as: ", "page_idx": 3}, {"type": "equation", "text": "$$\ns_{r e l}(v_{A},\\mathcal{D}_{B},\\Lambda)=\\frac{s(v_{A},\\mathcal{D}_{B},\\Lambda)}{s(v_{B},\\mathcal{D}_{B},\\Lambda)}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "4.2 Metrics ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To measure the effectiveness of steering, we need a metric of the model\u2019s propensity to exhibit a behavioural trait (e.g. sycophancy [30], truthfulness [17], helpfulness [2]). Given a propensity metric, we then define propensity curves and steerability as summary metrics of the steering vector\u2019s effectiveness. ", "page_idx": 3}, {"type": "text", "text": "Propensity. In our multiple choice setting, the model exhibits a target trait by outputting the positive option (either \u201cA\u201d or \u201cB\u201d, see Figure 9). As such, a natural metric is to compare the logits of the positive and negative tokens (either A or B) respectively. We define the logit-difference propensity metric $m_{L D}$ as the logit of the positive token minus the logit of the negative token. Concretely: ", "page_idx": 3}, {"type": "equation", "text": "$$\nm_{L D}=\\mathrm{Logit}(y_{+})-\\mathrm{Logit}(\\mathrm{y_{-}})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Rimsky et al. [30] instead uses the normalised probability of the positive answer, which is the same except for a softmax applied to the logits. We note that normalised probabilities are a monotonic function of the logit difference, so propensity is order-invariant between these two methods. However, logit-difference is likely to be more linear with respect to the model\u2019s intermediate activations (as it doesn\u2019t include a softmax), facilitating downstream analysis. ", "page_idx": 3}, {"type": "text", "text": "We note that propensity can be measured per-sample or in aggregate. Aggregate propensity is useful for measuring broad changes in behaviour across a distribution, and we primarily use this metric when studying steering vector generalisation in Section 6. A concern is that this loses granular per-sample information; we analyse per-sample propensity in detail when steering in-distribution in Section 5. ", "page_idx": 3}, {"type": "text", "text": "Propensity Curve. To get a sense of how well steering works as a function of the multiplier $\\lambda$ , we compute $m_{L D}$ for various values of $\\begin{array}{r l r}{\\lambda}&{{}\\in}&{\\Lambda}&{{}=}\\end{array}$ $\\{-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5\\}$ . We refer to this as a propensity curve, which was ", "page_idx": 3}, {"type": "image", "img_path": "v8X70gTodR/tmp/03fde85892caa2188ce1d7e103d22f7b745a52a5af6c53b802cc09f94aa16d0d.jpg", "img_caption": ["Fig4ure 2: Example propensity curve and steerability fit for high steerability (left), and low (right). "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Steerability. To summarise a propensity curve, we propose a steerability metric. Given a steering vector $v$ , dataset $\\mathcal{D}$ , and multipliers $\\Lambda=[\\lambda_{0}\\cdot\\cdot\\cdot\\lambda_{n}]$ , we define steerability $s(v,\\mathcal{D},\\Lambda)$ as the slope of a mean-squares line fti to the mean LD scores for $v$ steering $\\mathcal{D}$ at each $\\lambda_{i}\\in\\Lambda$ . The steerability score takes values $s\\in\\mathbb{R}$ . A high positive steerability score indicates that the steering vector is effective. Conversely, a negative steerability score indicates that the steering vector has the opposite of the intended effect. See Figure 2 for a visual example. ", "page_idx": 4}, {"type": "text", "text": "4.3 Steering Vector Extraction ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Models. Following previous work, we focus on steering instruction-tuned models. We include Llama- $\\cdot2.7\\mathfrak{b}$ -Chat [32] as it was used in previous work. In order to draw conclusions that generalise beyond a single model, we also consider Qwen-1.5-14b-Chat [1], which differs in many aspects, including architecture, parameter count, and training data distributions. ", "page_idx": 4}, {"type": "text", "text": "Steering Layer. The choice of which layer to steer at is an important hyperparameter. Loosely, we expect that each layer captures a different level of abstraction in the model\u2019s internal computation [8], and steering will work best if we choose the layer that best matches the target concept\u2019s level of abstraction. In order to determine the optimal layer, we sweep over all layers using the validation split. In line with Rimsky et al. [30], we find that the optimal choice of layer is remarkably consistent across many datasets. Thus, we fix layer 13 for Llama and layer 21 for Qwen for all subsequent experiments. Layer response curves used in selecting the optimal layer are presented in Appendix D.7. ", "page_idx": 4}, {"type": "text", "text": "5 Evaluating Steering Vector Reliability ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We first evaluate how reliably SV produce the desired change in model behaviour in-distribution. For SVs to be useful they need to robustly shift the model\u2019s behaviour in the desired direction for all inputs, rather than working on some inputs and not on others. However, we find that for many datasets this is not the case: steerability has high variance, with many inputs being steered in the opposite direction to what is intended. ", "page_idx": 4}, {"type": "text", "text": "Steerability Varies Widely Across and Within Concepts. We find that both the sign and magnitude of steerability can vary widely within a concept and across different concepts. As shown in Figure 1, steering has a range of behaviours for different datasets. For some datasets with high median steerability (e.g. corrigible-neutral-HHH), the distribution is unimodal; high probability mass is concentrated around the median (though still with high variance). At the low end of median steerability, it is more common for the distribution to be bimodal, with there being two clusters of steerability which are located further away from the median (e.g. myopic-reward). In some cases, steerability is negative for one of these clusters, which means that the steering vector is having the opposite of the intended effect on these examples. We term this phenomenon anti-steerability. Many of these datasets have almost half of the inputs being anti-steerable, implying that the effect of steering is highly unreliable. ", "page_idx": 4}, {"type": "text", "text": "Steering is Affected by Spurious Factors. In order to understand the high variance in steerability, we take a closer look at datasets with a high fraction of anti-steerable examples. In these cases, we hypothesise that the steering vector extracted encodes spurious factors, as opposed to the underlying behaviour. Hence, we study whether there are biases that predict steerability. ", "page_idx": 4}, {"type": "text", "text": "Due to the multiple-choice template used for steering vector extraction, one such potential bias is towards whether A or B was used to represent the positive option. In the case of the \u2018persona\u2019 datasets, where the responses are always either Yes or No, another potential bias is whether Yes or No represents the positive option. Neither of these biases are present in the training data, as we have randomised the data during steering vector extraction such that the examples are split equally between the two (or four) choices. Despite this, we find these two biases are present (Figure 14) and are often highly predictive of the steerability, explaining a large part of the variance in per-example steerability (Figure 4). ", "page_idx": 4}, {"type": "image", "img_path": "v8X70gTodR/tmp/96b24544360b4da63a2682a1c488f9dba8ad5bc1bec306543021a6b702bd25c0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 3: Models exhibit large dataset-dependent steerability bias. The figure shows mean steerability per dataset for each way in which the positive option is presented. And entirely unbiased result would have all bars being identical. Despite datasets being balanced amongst all possible combinations of options, the mean steerability differs greatly between these splits. While there is a general trend towards preferring \u2018Yes\u2019 vs \u2018No\u2019, there is still a lot of dataset-dependent variation, and there is no clear trend for \u2018A\u2019 vs \u2018B\u2019. For full results see Figure 17. Note that some datasets have only two bars, indicating that only the \u2018A\u2019/\u2018B\u2019 split is relevant. ", "page_idx": 5}, {"type": "text", "text": "This bias is different from the standard position or token bias exhibited by LLMs [40, 35], as it is a steerability bias: the model is more steerable towards the positive answer when it a particular position or token compared to the other position or token. The preferred token or position is different for each dataset; for example corrigible-neutral-HHH has a B-steerability bias, whereas self-aware-lm has an A-steerability bias (see Figure 14). This is problematic, as it is not fixable by simple dataset debiasing (which was already performed) or logit calibration adjustments [41] (as they effect propensity, not the change in propensity, i.e. steerability). Further, it implies that there may be other steerability biases present in models, determining when they are more or less steerable towards specific answers or behaviours. Indeed, there is still a high degree of unexplained variance present in many datasets in Figure 4. ", "page_idx": 5}, {"type": "text", "text": "Some Behaviours are Un-Steerable. We empirically observe that many behaviours turn out to be unsteerable, as measured by median steerability shown in Figure 1. One possible explanation is that the datasets we used were too small or low-quality. Other explanations include that unsteerable behaviours are not linearly represented in the model, or that they correspond to multiple separate behaviours within the model\u2019s ontology. In the latter case, it would be interesting to develop methods to disentangle these separate sub-behaviours in an unsupervised way. We consider follow-up investigations for these two hypotheses to be promising directions for future work. ", "page_idx": 5}, {"type": "text", "text": "6 Steering Out-of-Distribution ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "SVs will often be applied in situations different from when they are extracted, particularly when the system and user prompt changes, and so we aim to analyse how well SVs generalise in this setting. We find that SVs generalise reasonably well but not perfectly, with some prompt changes having ", "page_idx": 5}, {"type": "image", "img_path": "v8X70gTodR/tmp/d179bcf4527b97b66511802ee4a9e830002729317f4dee03cff97da374ed2b83.jpg", "img_caption": ["Figure 4: SVs exhibit high variance, some of which is explained by spurious factors. The figure shows variance in per-sample steerability by dataset, with attributions to known spurious factors annotated. Marginal Var Explained refers to the variance explained by the \u2019Yes\u2019/\u2019No\u2019 split after removing variance from the \u2019A\u2019/\u2019B\u2019 split. For some datasets, spurious factors (orange, green) explain a large percentage of the variance, while for others, most of the variance remains unexplained. For full results see Figure 18. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "v8X70gTodR/tmp/f6b5c7af681e409a46956bcff8b617ac55226c3ce40e300d1b7c294ed030e954.jpg", "img_caption": ["BASE $\\rightarrow$ USER_NEG BASE $\\rightarrow$ USER_POS SYS_POS $\\rightarrow$ USER_NEG SYS_NEG $\\rightarrow$ USER_POS "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 5: In-distribution and out-of-distribution steerability are reasonably well-correlated.   \nWe show OOD vs ID steerability for Llama-2-7b (left; $=\\rho=0.891]$ ) and Qwen-1.5-14b (right;   \n$\\rho=0.694)$ . While OOD steerability seems correlated with ID steerability, we observe that there are some points far above or below the $x=y$ line, and this is more noticeable for the Qwen model.   \nThroughout, $\\rho$ refers to Spearman\u2019s rank correlation coefficient. ", "page_idx": 6}, {"type": "text", "text": "better generalisation that others. We investigate what affects when SVs will generalise, finding that it is mostly a property of the dataset, and that the similarity in behaviour of the unsteered model in the source and target prompt setting is also predictive of SV generalisation. ", "page_idx": 6}, {"type": "text", "text": "OOD Settings. For each dataset, we define the ID setting to be when we extract the steering vector from the BASE train split and evaluate it on the BASE test split, as defined in Table 1. We define four OOD distribution shifts. Firstly, we consider the cases where a user prompts the model to stimulate or suppress the target behaviour (BASE $\\rightarrow$ USER_NEG, BASE $\\rightarrow$ USER_POS). Additionally, we hypothesise that the model\u2019s base propensity affects the effectiveness of steering vectors. Therefore, we also study the case where the user instruction conflicts with the system prompt for the model, as encapsulated by system prompts (SYS_POS $\\rightarrow$ USER_NEG, SYS_NEG $\\rightarrow$ USER_POS). ", "page_idx": 6}, {"type": "image", "img_path": "v8X70gTodR/tmp/846430374d72d99a518352c24bbde53f6f5e3ff15f0d22994847e90e5fff9c9c.jpg", "img_caption": ["BASE $\\rightarrow$ USER_NEG BASE $\\rightarrow$ USER_POS SYS_POS $\\rightarrow$ USER_NEG SYS_NEG $\\to$ USER_POS "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 6: In-distribution and out-of-distribution variance in steerability are somewhat correlated. We show OOD vs ID variance in steerability for Llama-2-7b (left; $\\rho~=~0.535)$ and Qwen-1.5.-14b (right; $\\rho=0.341$ ). Generally, variance is slightly lower OOD than ID (as the slope of the lines is $<1$ , although results are somewhat noisy. ", "page_idx": 7}, {"type": "text", "text": "ID and OOD Steerability are Correlated. Figure 5 shows that steerability ID and OOD are correlated. We would expect that unsteerable concepts in-distribution are unlikely to steer out-ofdistribution, but it is promising for the usefulness of steering vectors that, conditioned on steering vectors working in-distribution, they continue to work well out-of-distribution. However, generalisation is not perfect, and on average steerability is worse OOD than ID, particularly for Qwen. The correlation for Qwen is also weaker than for Llama. ", "page_idx": 7}, {"type": "text", "text": "We also examine how the variance in steerability we demonstrated in Section 5 changes OOD. Figure 6 shows that ID and OOD variance are reasonably well-correlated, with OOD variance perhaps slightly lower than ID variance. This is somewhat surprising, although it may be explained by slightly lower steerability OOD (as lower steerability means lower variance in steerability as shown in Figure 20). ", "page_idx": 7}, {"type": "text", "text": "Steerability is Mostly a Property of the Dataset. We compare aggregate in-distribution and out-of-distribution steerability between Llama and Qwen in Figure 7. We find that both ID and OOD steerabilities are highly correlated across models, despite them having different sizes, architectures, and training procedures. The consistency between different model architectures indicates that the ", "page_idx": 7}, {"type": "image", "img_path": "v8X70gTodR/tmp/da90221be670a88536c6842fa7045cbe168f1fa9862e7659886e52aef98cc9d9.jpg", "img_caption": ["Figure 7: Steerability is mostly a property of the dataset. We show the correlation between steerability in Llama-2-7b and Qwen-1.5-14b both ID (left; $\\rho=0.769)$ ) and OOD (right; $\\rho=0.586)$ ). Given steerability is highly correlated between Llama and Qwen despite differences in architecture, size and training data, this suggests steerability is mostly a property of the dataset rather that the model. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "v8X70gTodR/tmp/51e099cead42b91b86ba0010f56916d2c059b5a3c4c8409e8d5115a7e12ea22b.jpg", "img_caption": ["Unsteered LD delta between training datasets "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 8: Propensity similarity is correlated with SV generalisation.. We plot relative steerability (Equation (2)) against the difference in unsteered training dataset $m_{L D}$ (Equation (3)) for Llama2-7B (left; $\\rho\\,=\\,-0.26)$ and Qwen-1.5-14b (right; $\\rho\\,=\\,-0.46)$ . In general we see a weak correlation, although it is stronger for Qwen than Llama. We fliter out any datapoints where the base steerability of the dataset variation is less than 0.25, as having low baseline steerability means any relative steerability score is likely just noise. ", "page_idx": 8}, {"type": "text", "text": "effectiveness of a steering vector is mostly a property of the dataset used to extract the dataset, as opposed to the model used. This may also be evidence that different models converge to similar ontologies [11, 10]. ", "page_idx": 8}, {"type": "text", "text": "Model Propensity is Predictive of Steering Generalisation. While steerability and SV generalisation is mostly a dataset-level property, there is still variation in generalisation performance that is not captured by dataset; for example, SVs generalise better over some shifts than others for the same dataset. In Figure 8 show that the similarity in the propensity of the model in two prompt settings is correlated with the relative steerability (Equation (2)), a measure of generalisation. In other words, if the model behaves similar in two prompt settings, then SVs will transfer better between those two settings than if the model behaves differently in the two settings. We show a similar result but for SV cosine similarity in Appendix E.3. ", "page_idx": 8}, {"type": "text", "text": "7 Discussion and Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our work is the first to report and analyse the variance in steerability at a per-example level, and in doing so reveal a major limitation in SV reliability. In Section 5, we demonstrated SVs\u2019s effects on model behaviour are often unreliable, with some concepts being unsteerable and some SVs producing the opposite behaviour to what is desired. We found that this unreliability is often driven by tokenand position-steerability bias, a new type of bias we discovered that is distinct from standard token and position biases in LLMs. Although these are very simple biases which can be easily understood, simple interventions in data preprocessing fail to address the problem, and there are likely to be other steerability biases that will affect the effectiveness and reliability of SVs. In Appendix E.2 we show that this variance is partially a dataset property rather than a model property, implying that future work investigating what causes these biases should at least partially focus on the dataset, as well as analysing whether other techniques for extracting and applying SVs can mitigate these biases. ", "page_idx": 8}, {"type": "text", "text": "In Section 6 we evaluated the generalisation properties of SVs, finding that while they often generalise reasonably well (conditioned on their in-distribution performance being good), generalisation is not always perfect. We find that SV generalisation is mostly a property of the dataset, and is correlated by the similarity in un-steered propensity of the model in the source and target setting. This correlation is problematic, as often we would want to apply steering vectors to guide model behaviour towards something it does not normally do, but in these scenarios SVs tend to generalise less well. Investigating methods to improve SV generalisation, and investigating scenarios where they generalise better or worse is important future work. ", "page_idx": 8}, {"type": "text", "text": "Overall, while steering vectors are a promising approach to efficiently guiding model behaviour at inference time, they are currently not a panacea to guarantee model helpful, harmless, and honest behavior, and substantial work is needed to improve their reliability and understand their generalisation properties. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen Technical Report, 2023. URL https://arxiv.org/abs/2309.16609. ", "page_idx": 10}, {"type": "text", "text": "[2] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional AI: Harmlessness from AI Feedback, 2022. URL https://arxiv.org/abs/ 2212.08073. ", "page_idx": 10}, {"type": "text", "text": "[3] Dan Biderman, Jose Gonzalez Ortiz, Jacob Portes, Mansheej Paul, Philip Greengard, Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, Cody Blakeney, and John P. Cunningham. LoRA Learns Less and Forgets Less, 2024. URL https://arxiv.org/ abs/2405.09673. ", "page_idx": 10}, {"type": "text", "text": "[4] Sid Black, Lee Sharkey, Leo Grinsztajn, Eric Winsor, Dan Braun, Jacob Merizian, Kip Parker, Carlos Ram\u00f3n Guevara, Beren Millidge, Gabriel Alfour, and Connor Leahy. Interpreting Neural Networks through the Polytope Lens, 2022. URL https://arxiv.org/abs/2211.12312. ", "page_idx": 10}, {"type": "text", "text": "[5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6- 12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. ", "page_idx": 10}, {"type": "text", "text": "[6] Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering Latent Knowledge in Language Models Without Supervision, 2022. URL https://arxiv.org/abs/2212.03827. ", "page_idx": 10}, {"type": "text", "text": "[7] Joshua Clymer, Garrett Baker, Rohan Subramani, and Sam Wang. Generalization Analogies: A Testbed for Generalizing AI Oversight to Hard-To-Measure Domains, 2023. URL https: //arxiv.org/abs/2311.07723. ", "page_idx": 10}, {"type": "text", "text": "[8] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. https://transformer-circuits.pub/2021/framework/index.html. ", "page_idx": 10}, {"type": "text", "text": "[9] Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. Toy Models of Superposition, 2022. URL https://arxiv.org/abs/2209.10652. ", "page_idx": 10}, {"type": "text", "text": "[10] Wes Gurnee, Theo Horsley, Zifan Carl Guo, Tara Rezaei Kheirkhah, Qinyi Sun, Will Hathaway, Neel Nanda, and Dimitris Bertsimas. Universal Neurons in GPT2 Language Models, 2024. URL https://arxiv.org/abs/2401.12181.   \n[11] Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. The Platonic Representation Hypothesis, 2024. URL https://arxiv.org/abs/2405.07987.   \n[12] Dieuwke Hupkes, Mario Giulianelli, Verna Dankers, Mikel Artetxe, Yanai Elazar, Tiago Pimentel, Christos Christodoulopoulos, Karim Lasri, Naomi Saphra, Arabella Sinclair, Dennis Ulmer, Florian Schottmann, Khuyagbaatar Batsuren, Kaiser Sun, Koustuv Sinha, Leila Khalatbari, Maria Ryskina, Rita Frieske, Ryan Cotterell, and Zhijing Jin. A taxonomy and review of generalization research in NLP. 5(10):1161\u20131174. ISSN 2522- 5839. doi: 10.1038/s42256-023-00729-y. URL https://www.nature.com/articles/ s42256-023-00729-y.   \n[13] Fengqing Jiang, Zhangchen Xu, Luyao Niu, Zhen Xiang, Bhaskar Ramasubramanian, Bo Li, and Radha Poovendran. ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs, 2024. URL https://arxiv.org/abs/2402.11753.   \n[14] Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward Grefenstette, and Roberta Raileanu. Understanding the Effects of RLHF on LLM Generalisation and Diversity, 2023. URL https://arxiv.org/abs/2310.06452.   \n[15] Kenneth Li, Aspen K. Hopkins, David Bau, Fernanda Vi\u00e9gas, Hanspeter Pfister, and Martin Wattenberg. Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task, 2022. URL https://arxiv.org/abs/2210.13382.   \n[16] Kenneth Li, Oam Patel, Fernanda Vi\u00e9gas, Hanspeter Pfister, and Martin Wattenberg. InferenceTime Intervention: Eliciting Truthful Answers from a Language Model, 2023. URL https: //arxiv.org/abs/2306.03341.   \n[17] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3214\u20133252, Dublin, Ireland, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.229. URL https://aclanthology.org/2022.acl-long.229.   \n[18] Sheng Liu, Haotian Ye, Lei Xing, and James Zou. In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering, 2023. URL https: //arxiv.org/abs/2311.06668.   \n[19] Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning, 2023. URL https://arxiv.org/abs/2308.08747.   \n[20] Andrew Mack and Alex Turner. Mechanistically eliciting latent behaviors in language models. 2024. URL https://www.lesswrong.com/posts/ioPnHKFyy4Cw2Gr2x/ mechanistically-eliciting-latent-behaviors-in-language-1.   \n[21] Alex Mallen and Nora Belrose. Eliciting Latent Knowledge from Quirky Language Models, 2023. URL https://arxiv.org/abs/2312.01037.   \n[22] Samuel Marks and Max Tegmark. The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets, 2023. URL https://arxiv.org/ abs/2310.06824.   \n[23] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3470\u20133487, Dublin, Ireland, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.244. URL https://aclanthology.org/2022.acl-long.244. ", "page_idx": 11}, {"type": "text", "text": "[24] Neel Nanda, Andrew Lee, and Martin Wattenberg. Emergent Linear Representations in World Models of Self-Supervised Sequence Models, 2023. URL https://arxiv.org/abs/2309. 00941. ", "page_idx": 12}, {"type": "text", "text": "[25] Kiho Park, Yo Joong Choe, and Victor Veitch. The Linear Representation Hypothesis and the Geometry of Large Language Models, 2023. URL https://arxiv.org/abs/2311.03658.   \n[26] Ethan Perez, Sam Ringer, Kamil\u02d9e Luko\u0161i\u00afut\u02d9e, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, Andy Jones, Anna Chen, Ben Mann, Brian Israel, Bryan Seethor, Cameron McKinnon, Christopher Olah, Da Yan, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, Guro Khundadze, Jackson Kernion, James Landis, Jamie Kerr, Jared Mueller, Jeeyoon Hyun, Joshua Landau, Kamal Ndousse, Landon Goldberg, Liane Lovitt, Martin Lucas, Michael Sellitto, Miranda Zhang, Neerav Kingsland, Nelson Elhage, Nicholas Joseph, Noem\u00ed Mercado, Nova DasSarma, Oliver Rausch, Robin Larson, Sam McCandlish, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Jack Clark, Samuel R. Bowman, Amanda Askell, Roger Grosse, Danny Hernandez, Deep Ganguli, Evan Hubinger, Nicholas Schiefer, and Jared Kaplan. Discovering Language Model Behaviors with Model-Written Evaluations, 2022. URL https: //arxiv.org/abs/2212.09251.   \n[27] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct Preference Optimization: Your Language Model is Secretly a Reward Model, 2023. URL https://arxiv.org/abs/2305.18290.   \n[28] Goutham Rajendran, Simon Buchholz, Bryon Aragam, Bernhard Sch\u00f6lkopf, and Pradeep Ravikumar. Learning Interpretable Concepts: Unifying Causal Representation Learning and Foundation Models, 2024. URL https://arxiv.org/abs/2402.09236.   \n[29] Nina Rimsky. Red-teaming language models via activation engineering, 2023. URL https://www.alignmentforum.org/posts/iHmsJdxgMEWmAfNne/ red-teaming-language-models-via-activation-engineering.   \n[30] Nina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexander Matt Turner. Steering Llama 2 via Contrastive Activation Addition, 2023. URL https://arxiv. org/abs/2312.06681.   \n[31] Curt Tigges, Oskar John Hollinsworth, Atticus Geiger, and Neel Nanda. Linear Representations of Sentiment in Large Language Models, 2023. URL https://arxiv.org/abs/2310. 15154.   \n[32] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models, 2023. URL https://arxiv.org/abs/2307.09288.   \n[33] Alexander Matt Turner, Lisa Thiergart, David Udell, Gavin Leech, Ulisse Mini, and Monte MacDiarmid. Activation Addition: Steering Language Models Without Optimization, 2023. URL https://arxiv.org/abs/2308.10248.   \n[34] Teun van der Weij, Massimo Poesio, and Nandi Schoots. Extending activation steering to broad skills and multiple behaviours, 2024.   \n[35] Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large Language Models are not Fair Evaluators, 2023. URL https://arxiv.org/abs/2305.17926.   \n[36] Zihao Wang, Lin Gui, Jeffrey Negrea, and Victor Veitch. Concept Algebra for (Score-Based) Text-Controlled Generative Models, 2023. URL https://arxiv.org/abs/2302.03693.   \n[37] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How Does LLM Safety Training Fail?, 2023. URL https://arxiv.org/abs/2307.02483.   \n[38] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, 2022. URL https://arxiv.org/abs/2201.11903.   \n[39] Zhengxuan Wu, Aryaman Arora, Zheng Wang, Atticus Geiger, Dan Jurafsky, Christopher D. Manning, and Christopher Potts. ReFT: Representation Finetuning for Language Models, 2024. URL https://arxiv.org/abs/2404.03592.   \n[40] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena, 2023. URL https: //arxiv.org/abs/2306.05685.   \n[41] Han Zhou, Xingchen Wan, Lev Proleev, Diana Mincu, Jilin Chen, Katherine Heller, and Subhrajit Roy. Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering, 2023. URL https://arxiv.org/abs/2309.17249.   \n[42] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-Tuning Language Models from Human Preferences, 2019. URL https://arxiv.org/abs/1909.08593.   \n[43] Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J. Zico Kolter, and Dan Hendrycks. Representation Engineering: A Top-Down Approach to AI Transparency, 2023. URL https://arxiv.org/abs/2310.01405. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Hardware Requirements ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "All experiments were performed using an A100 with 40 GB of VRAM. ", "page_idx": 14}, {"type": "text", "text": "B Extended Related Work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Steering Vectors and the Linear Representation Hypothesis ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The effectiveness of steering vectors in- and out-of-distribution has implications for the linear representation hypothesis (LRH) [25]. A key prediction of the LRH is that each atomic feature is associated with a single global direction in activation space, and that intervening by adding or subtracting this direction can influence the model\u2019s understanding and / or behaviour. Previous work that validates the LRH mostly considers the in-distribution (ID) setting [21, 6, 22, 24, 15]. However, this is only evidence of local linearity, which is satisfied by all continuous functions within a sufficiently small neighbourhood. The LRH in fact makes a stronger claim: that representations are globally linear. For SVs to generalise well OOD, this stronger claim has to be true \u2014 although it may not be sufficient, and the reverse implication doesn\u2019t necessarily hold, as the concepts that are linearly represented might not be human-interpretable or extractable with SV approaches. ", "page_idx": 14}, {"type": "text", "text": "Therefore, our analysis can be seen as extending existing validations of the LRH to the more challenging out-of-distribution (OOD) setting. Crucially, our proposed experimental protocol can differentiate the LRH from competing frameworks which allow for local, but not global, linearity [9, 4]. While we primarily focus on the practical usefulness of SVs in this paper, we want to highlight the additional value of our work from the perspective of the LRH. ", "page_idx": 14}, {"type": "text", "text": "B.2 Evaluating Generalisation Behaviour of Model Adjustment Procedures ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Several existing works evaluate the generalisation properties of model adjustment techniques. Hupkes et al. [12] introduce a taxonomy of work investigating generalisation in NLP. In the language of their taxonomy, the investigation in Section 6 has a practical motivation and uses a generated covariate shift for testing generalisation across domains at the fine-tuning (or in our case SV-training) stage. Kirk et al. [14] investigate the generalisation properties of different fine-tuning approaches in summarisation and instruction-following settings. Clymer et al. [7] investigate generalisation of different methods for training reward models across a range of generated natural shifts, and find that methods based on similar ideas to SVs often generalise reasonably well. In contrast, our work investigates the generalisation properties of SVs specifically on a wide range of datasets that capture desirable properties we would want to steer models towards or away from, and investigates generalising over different prompts, rather than different inputs. ", "page_idx": 14}, {"type": "text", "text": "C Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "While our study uses a large number of datasets, even more variety in the type of behaviour being steered towards could be studied to ensure our results are robust and reliable. As part of this, going beyond the multiple-choice-question format would improve the usefulness and practical implications of our work further. ", "page_idx": 14}, {"type": "text", "text": "While our work demonstrates multiple limitations of steering vectors and somewhat explains them, these failures are still not entirely understood. Further, it is unclear what potential mitigations could be applied to fix these issues. However, we believe this work is still important as it serves as a robust and rigorous demonstration of these issues that future work can build on to improve the reliability and generalisation of steering vectors. ", "page_idx": 14}, {"type": "text", "text": "We only analyse two models in this work, although they use different sizes, architectures, training data and algorithms. Our results hold across both these models, but experimenting on even more models, particularly at larger sizes, would improve the robustness and reliability of our results. ", "page_idx": 14}, {"type": "table", "img_path": "v8X70gTodR/tmp/62b3fc93fd6a05510c3e7e85c196baf42362ec98466e04454d2307afb307c51c.jpg", "table_caption": ["Table 1: Positive, negative, and baseline variations for the \u201cBelieves it is not being watched by humans\u201d MWE dataset. Each variation has two settings, where the instruction is prefixed to each sample prompt or used as the system message. ", "Figure 9: Sample constrastive prompt used to train a steering vector "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "D Methodological Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "D.1 Contrastive Prompts ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Contrastive prompts consist of a question or statement followed by two options, denoted by \u2018(A)\u2019 and \u2018(B)\u2019 respectively. We show a typical contrastive prompt in Figure 9. ", "page_idx": 15}, {"type": "text", "text": "D.2 Prompt Template ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For each model under consideration, we use the standard instruction-formatting template. Concretely, we add a system prompt and enclose the input in instruction tags. An example of this for Llama-7b is shown in Figure 10. ", "page_idx": 15}, {"type": "text", "text": "D.3 Out-of-distribution Prompt Injections ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Table 1 shows example prompt injections which define the distribution shifts for a specific dataset.   \nWe describe the full list of persona prompts used in Table 2, Table 3. ", "page_idx": 15}, {"type": "text", "text": "D.4 Aggregation Methods ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Besides MD, other aggregation methods have been proposed in the literature, such as principal components analysis (PCA) and logistic regression (LG) [43]. Here, we focus on mean-difference as it naturally results in steering vectors which have an appropriate magnitude relative to model activations , such that a steering multiplier of 1 has the same semantic meaning across different layers and different concepts. return steering vectors with unit norm, which could be different from the natural scale of the model. Thus, additional work would be required to tune appropriate steering magnitudes. We stick with MD, as Rimsky et al. [30] show it is a strong baseline, and PCA has been found to give a very similar direction to MD [31]. ", "page_idx": 15}, {"type": "table", "img_path": "v8X70gTodR/tmp/5d8143b9437d0351158354a3c0af3b9e2b19a89a9c2e8ebc7fe700b9bf403c16.jpg", "table_caption": ["Table 2: Positive and negative persona prompts (1 of 2) "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "v8X70gTodR/tmp/c257d59703ab81938e11d6a6554131a010c8b946e38309e4f8bc9a1d9d809f9e.jpg", "table_caption": ["Table 3: Positive and negative persona prompts (2 of 2) "], "table_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "v8X70gTodR/tmp/6f05ed1ca119f776875b21c58a8e8862b8e18a2eec3044ce7ddc1965c2eebf7f.jpg", "img_caption": ["Figure 10: Full instruction-formatted prompt for Llama-7b "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "D.5 MCQ vs Open-Ended Generation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For evaluating the effectiveness of steering vectors, the setting of ultimate interest is the open-ended generation setting. However, it is difficult to obtain an objective metric of whether steering vectors are effective in this setting. Previous work [30, 43] finds that multiple-choice propensity generally correlates with open-ended propensity. Hence, our analysis primarily focuses on the multiple-choice setting, with examples being prompt-engineered to select one of the multiple-choice options. ", "page_idx": 18}, {"type": "text", "text": "D.6 Logit-Difference Propensity ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Following standard practice in the mechanistic interpretability literature, we use the difference in logits between a correct and incorrect answer as the metric of propensity. Our use here is justified by two points: (i) Firstly, the correct and wrong answers are unambiguous. We find that, when the prompts are formatted in multiple-choice format, the two highest logits consistently correspond to the option tokens A and B, indicating that it is valid to consider only these two logits. (ii) Secondly, previous work [30] finds that the logit-difference usually corresponds to generation. Conditioned on the response beginning with A or B, the remainder of the response is typically consistent with the option selected. We interpret this as evidence that the model \u2018decides\u2019 which behaviour to adopt at the A/B token position. ", "page_idx": 18}, {"type": "text", "text": "D.7 Optimal Layer Selection ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Figure 11 and Figure 12 we report layer response curves plotted for a subset of datasets across all layers of Llama-2-7b-chat and Qwen-1.5-14b-chat respectively. We find that, across many datasets, the choice of optimal layer is remarkably consistent, justifying the use of a single layer for steering. ", "page_idx": 18}, {"type": "text", "text": "One concern with this approach is that datasets which have low steerability were simply steered optimally at other layers. To address this, we re-run the layer sweep on the worst-performing datasets, shown in Figure 13. We find that the optimal layer remains the same for these datasets, confirming that low steerability is not merely due to having steered at the wrong layer. ", "page_idx": 18}, {"type": "text", "text": "D.8 Optimal Multiplier Selection ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In our experiments, we fix a range of $(-1.5,1.5)$ within which we select multipliers to perform contrastive activation addition. To justify this choice, we ablate the range of multipliers used in Figure 15. We find that the overall trends in steerability remain highly consistent across multiplier ranges, giving us confidence that the conclusions on steerability are robust to the choice of multiplier. ", "page_idx": 18}, {"type": "text", "text": "D.9 OOD Steering Vector Magnitude ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "When steering in-distribution, we expect that the extracted steering vector is already of a magnitude that is scaled appropriately relative to the model\u2019s activations. However, when extracting steering ", "page_idx": 18}, {"type": "image", "img_path": "v8X70gTodR/tmp/60c01f06eda44b931f557477e05398f0b35697dbfd2303a00b902d485ad352d3.jpg", "img_caption": ["Figure 11: Steerability scores for multiple datasets as a function of layer choice for Llama2-7B. Layer 13 has the highest steerabilty score for many datasets investigated. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "v8X70gTodR/tmp/1eb7d1775424270e63e464d24870062ee4de365da7e2e2d781bb230a95c397bc.jpg", "img_caption": ["Figure 12: Steerability scores for multiple datasets as a function of layer choice for Qwen 1.5 14B. Layer 21 has the highest steerabilty score for many datasets investigated. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "v8X70gTodR/tmp/1804754c3d22b83c8c32a2162b7e74a7727ab887fa063b41d5d73a5f338a4f39.jpg", "img_caption": ["Figure 13: Re-running the layer sweep on Qwen and Llama with the worst-performing datasets. The optimal layer remains the same for almost all datasets. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "v8X70gTodR/tmp/b9285874dde695944e091b2e4a99540d08a49b8fb2a2a2b02f0a55441d6f89cf.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 14: Models exhibit large dataset-dependent steerability bias. The figure shows mean steerability per dataset for each way in which the positive option is presented. And entirely unbiased result would have all bars being identical. Despite datasets being balanced amongst all possible combinations of options, the mean steerability differs greatly between these splits. While there is a general trend towards preferring \u2018Yes\u2019 vs \u2018No\u2019, there is still a lot of dataset-dependent variation, and there is no clear trend for \u2018A\u2019 vs \u2018B\u2019. For full results see Figure 17. Note that some datasets have only two bars, indicating that only the \u2018A\u2019/\u2018B\u2019 split is relevant. ", "page_idx": 20}, {"type": "text", "text": "vectors on different dataset variants, the resulting steering vectors may be of different magnitudes. Unaddressed, this could create situations where a steering vector appears to steer better or worse than another steering vector, when in reality it is simply an artifact of one steering vector having a larger or smaller magnitude than another steering vector. Thus, we normalise the magnitudes of all steering vectors to the magnitude of the baseline steering vector, such that we can fairly compare these steering vectors using interventions of the same multiplier on the same evaluation dataset. ", "page_idx": 20}, {"type": "text", "text": "Steerability for different multiplier ranges ", "page_idx": 21}, {"type": "image", "img_path": "v8X70gTodR/tmp/34bd8a667871e0e22a11cd87bccfa673aebbcfc44834689183ab68ed43c044ba.jpg", "img_caption": ["Figure 15: Steerability when calculated with different multiplier ranges. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "E Supplementary Results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "E.1 In-Distribution Steerability ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We present the equivalent of Figure 1, Figure 14, Figure 4 for all datasets evaluated. See Figure 16, Figure 17, Figure 18 respectively. ", "page_idx": 21}, {"type": "text", "text": "E.2 Steerability Variance Across Models ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In Figure 19 we show that the high variance in steerability we demonstrate in Section 5 is somewhat correlated across models, implying this variance is partially a property of the dataset rather than a specific model. This implies that improving the reliability of SVs requires either more substantial adjustments to models, or improvements to dataset quality or SV extraction. ", "page_idx": 21}, {"type": "text", "text": "In Figure 20 we show steerability and steerability variance are somewhat correlated, for both models, but the relationship is somewhat noisy. ", "page_idx": 21}, {"type": "text", "text": "In addition, we include additional steerability correlations for Gemma-2-2b-it and Llama-3.1-70b in Figure 21 ", "page_idx": 21}, {"type": "text", "text": "E.3 OOD steering vector similarities ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In Figure 22, we produce similar plots to Figure 8 but using cosine similarity of SVs rather than relative steerability as the y-axis. We find that dataset variations that have similar unsteered LD result in more similar steering vectors, analogous to the result in Figure 8. ", "page_idx": 21}, {"type": "image", "img_path": "v8X70gTodR/tmp/3da017eb5430059ed686195b2a08b96af3d34fe6aa5bbd4e53b27511b8251da7.jpg", "img_caption": ["Figure 16: Per-sample steerability and the fraction of anti-steerable examples, visualised per dataset for all 40 datasets "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "v8X70gTodR/tmp/c90f2917bdd798758d576f461d86d589b3950a767fcba9a4607b721d0685e0ac.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 17: Aggregate (mean) steerability, split by option type, as well as option splits within the dataset, for all 40 datasets. ", "page_idx": 23}, {"type": "image", "img_path": "v8X70gTodR/tmp/e569ce6b4c82130db99fc22462edd87c553ad01cbaa81d8de43ead80c7fbfc9c.jpg", "img_caption": ["Figure 18: Variance in steerability by dataset. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "v8X70gTodR/tmp/e75fc824b0df8410d36f3dc5f550486f1cd89ce84ec8794f55cccf2f79e350c5.jpg", "img_caption": ["Figure 19: Steerability Variance is somewhat correlated across models. The figure shows correlation between steerability variance in Llama-2-7b and Qwen-1.5-14b both ID (left; $\\rho=0.465)$ and OOD (right; $\\rho=0.491)$ ). While the variance is still correlated, many datasets exhibit higher or lower variance in steerability under one model than the other, indicating that models may differ in the degree to which they incorporate spurious factors into linear concept representations. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "v8X70gTodR/tmp/ea2c0c25caee3da0c6b18bdb09dd5ca57208249d874784284f89bb76a956a4ce.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 20: Mean steerability vs variance in steerability in both models, across datasets and distribution shifts. ", "page_idx": 25}, {"type": "image", "img_path": "v8X70gTodR/tmp/bba04db1ee00bb8952e3c2dca43e004f7577821fee318c073c8af5edc55296ae.jpg", "img_caption": ["Steerability between different models (ID) ", "Figure 21: Steerability correlations ID between Gemma-2-2b, Llama-2-7b, Qwen-1.5-14b, Llama3.1-70b. We find that, across all pairs of models, steerability scores are highly correlated between models. Here, we use the Spearman correlation (as defined by sklearn.stats) "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "v8X70gTodR/tmp/2de14f59ada81afa44fb174bb7aac330dde9871543f4375903b359eb6259526d.jpg", "img_caption": ["Figure 22: Cosine similarities between steering vector variations for all datasets for Llama2-7B-chat (left; $\\rho=-0.63)$ and Qwen-1.5-14b (right; $\\rho=-0.86)$ . The $\\mathbf{X}$ -axis is the delta in unsteered logit-diff propensity between the dataset variations. A small LD delta means that both variations have similar unsteered LD. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We feel that the claims are accurate. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Yes, we discuss the limitations in Appendix C ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: We do not have any theoretical results. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We will release the codebase publicly with instructions for reproduction. We also describe our experimental setting extensively in Sections 3 and 4 and Appendix D. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide access to the code and data. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Implementation details are discussed in Sections 3 and 4 and Appendix D. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We report relevant error bars and variance in steerability in Section 5, and show error bars in Section 6 also. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We describe necessary computational resources in Appendix A. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our research does not cause any of the harms listed in the guidelines. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our work has no obvious broader societal impact beyond generally making machine learning models more capable or aligned. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: We do not release novel data or models. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: All data and models are available open-source and have been cited where appropriate. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 31}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We do not release new assets. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We do not perform experiments with human subjects ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We do not perform research with human subjects. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]