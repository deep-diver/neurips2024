[{"figure_path": "tmQH8prqLc/figures/figures_8_1.jpg", "caption": "Figure 1: Results for CIFAR-10 dataset.", "description": "The figure shows the training and testing performance of different optimization algorithms on the CIFAR-10 dataset.  The algorithms compared include Adam, AdaBelief, SGD, STORM, STORM+, META-STORM, and the proposed Ada-STORM.  The plots show training loss, training accuracy, testing loss, and testing accuracy over 200 epochs.  The results illustrate the comparative performance of the algorithms in terms of convergence speed and final accuracy.", "section": "6 Experiments"}, {"figure_path": "tmQH8prqLc/figures/figures_9_1.jpg", "caption": "Figure 1: Results for CIFAR-10 dataset.", "description": "This figure presents the results of the CIFAR-10 image classification experiment.  It shows four subplots: training loss, training accuracy, testing loss, and testing accuracy. Each subplot displays the performance of various optimization methods (Adam, AdaBelief, SGD, STORM, STORM+, META-STORM, and Ada-STORM) over 200 epochs. The x-axis represents the epoch number, and the y-axis represents the corresponding metric (loss or accuracy). Error bars are included to indicate the variability of the results. Ada-STORM shows promising performance in testing accuracy.", "section": "6 Experiments"}, {"figure_path": "tmQH8prqLc/figures/figures_9_2.jpg", "caption": "Figure 3: Results for WikiText-2 dataset.", "description": "The figure shows the results of training a 2-layer Transformer language model on the WikiText-2 dataset using various optimization methods.  The plots show training and testing loss and perplexity over 40 epochs.  The methods compared are Adam, AdaBelief, SGD, STORM, STORM+, META-STORM, and Ada-STORM. Ada-STORM demonstrates comparable performance for training loss and accuracy to other methods, but outperforms others in testing loss, which indicates better generalization ability.", "section": "6 Experiments"}]