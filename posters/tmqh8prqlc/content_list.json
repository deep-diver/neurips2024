[{"type": "text", "text": "Adaptive Variance Reduction for Stochastic Optimization under Weaker Assumptions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Wei Jiang', Sifan Yangl2, Yibo Wang12, Lijun Zhang12, ", "page_idx": 0}, {"type": "text", "text": "'National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China 2School of Artificial Intelligence, Nanjing University, Nanjing, China {jiangw, yangsf, wangyb, zhanglj}@lamda.nju.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper explores adaptive variance reduction methods for stochastic optimization based on the STORM technique. Existing adaptive extensions of STORM rely on strong assumptions like bounded gradients and bounded function values, or suffer an additional ${\\mathcal{O}}(\\log T)$ term in the convergence rate. To address these limitations, we introduce a novel adaptive STORM method that achieves an optimal convergence rate of ${\\mathcal{O}}(T^{-1/3})$ for non-convex functions with our newly designed learning rate strategy. Compared with existing approaches, our method requires weaker assumptions and attains the optimal convergence rate without the additional ${\\mathcal{O}}(\\log T)$ term. We also extend the proposed technique to stochastic compositional optimization, obtaining the same optimal rate of ${\\mathcal{O}}(T^{-1/3})$ . Furthermore, we investigate the non-convex finite-sum problem and develop another innovative adaptive variance reduction method that achieves an optimal convergence rate of $\\mathcal{O}(\\bar{n}^{1/4}T^{-1/2})$ ,where $n$ represents the number of component functions. Numerical experiments across various tasks validate the effectiveness of our method. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper investigates the stochastic optimization problem ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{d}}f(\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $f:\\mathbb{R}^{d}\\mapsto\\mathbb{R}$ is a smooth non-convex function. We assume that only noisy estimations of its gradient $\\nabla f(\\mathbf{x})$ can be accessed, denoted as $\\nabla f(\\mathbf{x};\\boldsymbol{\\xi})$ , where $\\xi$ represents the random sample drawn from a stochastic oracle such that $\\mathbb{E}[\\nabla f(\\mathbf{x};\\boldsymbol{\\xi})]=\\nabla f(\\mathbf{x})$ ", "page_idx": 0}, {"type": "text", "text": "Problem (1) has been comprehensively investigated in the literature [Duchi et al., 2011, Kingma and Ba, 2015, Loshchilov and Hutter, 2017], and it is well-known that the classical stochastic gradient descent (SGD) achieves a convergence rate of ${\\mathcal{O}}(T^{-1/4})$ ,where $T$ denotes the iteration number [Ghadimi and Lan, 2013]. To further improve the convergence rate, variance reduction methods have been developed, and attain an improved rate of ${\\mathcal{O}}(T^{-1/3})$ under a slightly stronger smoothness assumption [Fang et al., 2018, Wang et al., 2019]. However, these methods necessitate the use of a huge batch size in each iteration, which is often impractical to use. To eliminate the need for large batches, a momentum-based variance reduction method \u2014 STORM [Cutkosky and Orabona, 2019] is introduced, which achieves a convergence rate of $\\mathcal{O}(T^{-1/3}\\log T)$ ", "page_idx": 0}, {"type": "text", "text": "Although aforementioned methods are equipped with convergence guarantees, their analyses rely on delicate configurations of hyper-parameters, such as the learning rate and the momentum parameter. To set them properly, the algorithm typically needs to know the value of the smoothness parameter $L$ the gradient upper bound $G$ , and the variance upper bound $\\sigma$ , which are often unknown in practice. Specifically, most algorithms require the learning rate $\\eta_{t}$ smaller than $\\mathcal{O}(1/L)$ , and for the STORM method, setting the momentum parameter to $\\mathcal{O}(\\bar{L}^{2}\\eta_{t}^{2})$ is crucial for ensuring convergence [Cutkosky and Orabona, 2019]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To overcome this limitation, many adaptive algorithms have been developed, aiming to obtain convergence guarantees without prior knowledge of problem-dependent parameters such as $L$ $G$ and $\\sigma$ . Based on the STORM method, Levy et al. [2021] develop the $\\mathrm{STORM+}$ algorithm, which attains the optimal ${\\mathcal{O}}(T^{-1/3})$ convergence rate under the assumption of bounded function values and gradients. To remove the need for the bounded function values assumption, Liu et al. [2022] propose the META-STORM algorithm to attain an $\\mathcal{O}(T^{-1/3}\\log T)$ convergence rate, but it still requires the bounded gradients assumption and includes an additional ${\\mathcal{O}}(\\log T)$ term. In summary, despite advancements in this field, existing adaptive STORM-based methods either depend on strong assumptions or suffer an extra ${\\mathcal{O}}(\\log T)$ term compared with the lower bound [Arjevani et al., 2023]. Hence, a fundamental question to be addressed is: ", "page_idx": 1}, {"type": "text", "text": "Is it possible to develop an adaptive STORM method that achieves the optimal convergence rate fornon-convexfunctionsunderweakerassumptions? ", "page_idx": 1}, {"type": "text", "text": "We give an affirmative answer to the above question by devising a novel optimal Adaptive STORM method (Ada-STORM). The learning rate of our algorithm is set to be inversely proportional to a specific power of the iteration number $T$ in the initial iterations, and then changes adaptively based on the cumulative sum of past gradient estimations. In this way, we are able to adjust the learning rate dynamically according to the property of stochastic gradients, and ensure a small learning rate in the beginning. Leveraging this strategy, Ada-STORM achieves an optimal convergence rate of ${\\mathcal{O}}(T^{-1/3})$ for non-convex functions. Notably, our analysis does not require the function to have bounded values and bounded gradients, which is a significant advancement over existing methods [Levy et al., 2021, Liu et al., 2022]. Additionally, our convergence rate does not contain the extra ${\\mathcal{O}}(\\log T)$ term, which is often present in STORM-based methods [Cutkosky and Orabona, 2019, Liu et al., 2022]. To highlight the versatility of our approach and its potential impact in the field of stochastic optimization, we further extend our technique to develop optimal adaptive methods for compositional optimization. ", "page_idx": 1}, {"type": "text", "text": "Finally, we investigate adaptive variance reduction for the non-convex finite-sum problems. Inspired by SAG algorithm [Roux et al., 2012], we incorporate an additional term in the STORM estimator, which measures the difference of past gradients between the selected component function and the overall objective. By changing the learning rate according to the sum of past gradient estimations, we are able to obtain an optimal convergence rate of ${\\mathcal{O}}(n^{\\bar{-}1/4}T^{-1/2})$ for finite-sum problems in an adaptive manner, where $n$ is the number of component functions. Our result is better than the previous convergencerate of ${\\mathcal{O}}(n^{-1/4}T^{-1/2}\\log(n T))$ obtained by adaptive SPIDER method [Kavis et al., 2022]. In summary, compared with existing methods, this paper enjoys the following advantages: ", "page_idx": 1}, {"type": "text", "text": "\u00b7 For stochastic non-convex optimization, our method achieves the optimal convergence rate of ${\\mathcal{O}}(T^{-1/3})$ under more relaxed assumptions. Specifically, it does not require the bounded function values or the bounded gradients, and does not include the additional ${\\mathcal{O}}(\\log T)$ term in the convergence rate.   \n\u00b7 Our learning rate design and the analysis exhibit broad applicability. We substantiate this claim by obtaining an optimal rate of ${\\mathcal{O}}(T^{-1/3})$ for stochastic compositional optimization, using the technique proposed in this paper.   \n\u00b7 For non-convex finite-sum optimization, we further improve our adaptive algorithm to attain an optimal convergence rate of ${\\mathcal{O}}(n^{1/4}T^{-1/2})$ , which outperforms the previous result by eliminating the ${\\mathcal{O}}(\\operatorname{log}(n T))$ factor. ", "page_idx": 1}, {"type": "text", "text": "A comparison between our method and other STORM-based algorithms is shown in Table 1. Numerical experiments on different tasks also validate the effectiveness of the proposed method. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "This section briefly reviews related work on stochastic variance reduction methods and adaptive stochastic algorithms. ", "page_idx": 1}, {"type": "table", "img_path": "tmQH8prqLc/tmp/3a63b0f2660e1126c0e1576ea7df7d15f64442bbc438203cd7a96abe8cd1c313.jpg", "table_caption": ["Table 1: Summary of results for STORM-based methods. Here, NC denotes non-convex, Comp. indicates compositional optimization, FS represents finite-sum optimization, and BG/BF refers to requiring bounded gradients or bounded function values assumptions. Adaptive means the method does not require to know problem-dependent parameters, i.e., $L,G$ and $\\sigma$ "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "2.1 Stochastic variance reduction methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Variance reduction has been widely used in stochastic optimization to reduce the gradient estimation error and thus improve the convergence rates. The idea of variance reduction can be traced back to the SAG algorithm [Roux et al., 2012], which incorporates a memory of previous gradient values to ensure variance reduction and achieves a linear convergence rate for strongly convex finite-sum optimization. To avoid the storage of past gradients, SVRG [Zhang et al., 2013, Johnson and Zhang, 2013] proposes to calculate the full gradient periodically, obtaining the same convergence rate as the SAG algorithm. Subsequent advancement has been made by the SARAH method [Nguyen et al., 2017], which derives better convergence for smooth convex functions. ", "page_idx": 2}, {"type": "text", "text": "In the context of non-convex objectives, Fang et al. [2018] introduce the SPIDER estimator, which improves the convergence rate from ${\\mathcal{O}}(T^{-1/4})$ ${\\mathcal{O}}(T^{-1/3})$ in stochastic settings, and to $O(n^{1/4}T^{-1/2})$ in finite-sum scenarios, with $n$ representing the number of components in the finite-sum. Following this, the SpiderBoost algorithm [Wang et al., 2019] refines the SPIDER approach by employing a larger constant step size and adapting it for composite optimization problems. However, a common limitation among these methods is their reliance on large batch sizes for each iteration, posing practical challenges due to high computational demands. To mitigate this issue, Cutkosky and Orabona [2019] introduce the STORM method, a momentum-based technique that achieves an $\\mathcal{O}(T^{-1/3}\\log T)$ convergence rate without using large batches. Concurrently, Tran-Dinh et al. [2019] obtain the same result using a similar algorithm but through a different analysis. ", "page_idx": 2}, {"type": "text", "text": "2.2  Adaptive stochastic algorithms ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "For stochastic optimization, it is well-known that the SGD algorithm can obtain a convergence rate Oof ${\\mathcal{O}}(T^{-1/4})$ for non-convex objective functions with well-designed learning rates [Ghadimi and Lan, 2013]. Instead of using pre-defined iteration-based learning rates, many stochastic methods propose to adjust the learning rate based on past stochastic gradients. One of the foundational works is the AdaGrad algorithm [Duchi et al., 2011], which proves to be effective for sparse data. Further advancements include RMSprop [Tieleman and Hinton, 2012] and Adam [Kingma and Ba, 2015], demonstrating broad effectiveness across a wide range of machine learning problems. Later, the Super-Adam [Huang et al., 2021] algorithm further improves the Adam algorithm via the variance reduction technique STORM [Cutkosky and Orabona, 2019] and obtains a convergence rate of $\\mathcal{O}(T^{-1/3}\\log T)$ . Nevertheless, to obtain the corresponding convergence rates, these methods still require knowledge of certain problem-dependent parameters to set hyper-parameters accurately, hence not adaptive.2 To solve this problem, many research aims to develop fully adaptive SGD methods that maintain the optimal convergence rate without knowing problem-specific parameters [Orabona, 2014, Chen et al., 2022, Carmon and Hinder, 2022, Ivgi et al., 2023, Yang et al., 2023]. ", "page_idx": 2}, {"type": "table", "img_path": "tmQH8prqLc/tmp/3e7925e2ca592d536a863d95cce9aad93ea51b2f0a08b20d068365a0378e0067.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Recently, adaptive adaptations of STORM have received considerable attention. A notable development is the introduction of $\\mathrm{STORM+}$ [Levy et al., 2021], which presents a fully adaptive version of STORM while attaining an optimal convergence rate. To circumvent the bounded function values assumption in $\\mathrm{STORM+}$ , the META-STORM [Liu et al., 2022] approach is developed, equipped with a nearly optimal bound. However, META-STORM still requires the bounded gradients assumption, and it includes an additional ${\\mathcal{O}}(\\log T)$ in the convergence rate. Consequently, adaptive STORM with the optimal convergence rate and under mild assumptions still needs further explorations. ", "page_idx": 3}, {"type": "text", "text": "3   Adaptive variance reduction for non-convex optimization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we develop an adaptive STORM method for non-convex functions. We first outline the assumptions used, and then present our proposed method and analyze its convergence rate. ", "page_idx": 3}, {"type": "text", "text": "3.1 Assumptions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We introduce the following assumptions, which are standard and commonly adopted in the stochastic optimization [Nguyen et al., 2017, Fang et al., 2018, Cutkosky and Orabona, 2019, Li et al., 2021]. ", "page_idx": 3}, {"type": "text", "text": "Assumption 1 (Average smoothness) ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|\\nabla f(\\mathbf{x};\\boldsymbol{\\xi})-\\nabla f(\\mathbf{y};\\boldsymbol{\\xi})\\right\\|^{2}\\right]\\leq L^{2}\\|\\mathbf{x}-\\mathbf{y}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Assumption 2 (Bounded variance) ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|\\nabla f(\\mathbf{x};\\boldsymbol{\\xi})-\\nabla f(\\mathbf{x})\\right\\|^{2}\\right]\\leq\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Assumption 3 $f_{*}=\\operatorname*{inf}_{\\mathbf{x}}{f(\\mathbf{x})}\\geq-\\infty$ and $f\\left(\\mathbf{x}_{1}\\right)-f_{*}\\leq\\Delta_{f}$ for the initial solution $\\mathbf{x}_{1}$ ", "page_idx": 3}, {"type": "text", "text": "Note that some additional assumptions are required in other STORM-based methods. Specifically, STORM [Cutkosky and Orabona, 2019], STORM $^+$ [Levy et al., 2021], and META-STORM [Liu et al., 2022] assume the bounded gradients. Moreover, $\\mathrm{STORM+}$ makes an additional assumption of the bounded function values. ", "page_idx": 3}, {"type": "text", "text": "3.2  The proposed method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this subsection, we aim to develop an adaptive STORM method that achieves an optimal convergence rate for non-convex functions under weaker assumptions. Our algorithm framework is the same as the original STORM [Cutkosky and Orabona, 2019], and the only difference is the setup of the momentum parameter $\\beta_{t}$ and the learning rate $\\eta_{t}$ . First, we present the STORM algorithm in Algorithm1. ", "page_idx": 3}, {"type": "text", "text": "The core idea of STORM lies in a carefully devised variance reduced estimator $\\mathbf{v}_{t}$ , which effectively tracks the gradient f(xt). For the frstiteration (t = 1), we set v =\u2265 B1 bVf(x1;Si), which is estimated within a batch $B_{0}=T^{1/3}$ . Note that we use large batch only in the first iteration, and constant batch size in other iterations. In subsequent iterations $(t\\geq2)$ ),estimator $\\mathbf{v}_{t}$ is defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{v}_{t}=(1-\\beta_{t})\\mathbf{v}_{t-1}+\\beta_{t}\\nabla f(\\mathbf{x}_{t};\\xi_{t})+\\left(1-\\beta_{t}\\right)\\left(\\nabla f(\\mathbf{x}_{t};\\xi_{t})-\\nabla f(\\mathbf{x}_{t-1};\\xi_{t})\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the first two terms are similar to the momentum SGD, and the last term serves as the error correction, which ensures the variance reduction effect. By choosing the values of $\\beta_{t}$ and $\\eta_{t}$ carefully, STORM ensures that the estimation error $\\mathbb{E}[\\|\\mathbf{v}_{t}-\\nabla f(\\mathbf{x}_{t})\\|^{2}]$ would decrease gradually. In the original STORM paper, these parameters are set up as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\eta_{t}=\\frac{k}{\\left(w+\\sum_{i=1}^{t}\\Vert\\nabla f(\\mathbf{x}_{t};\\xi_{t})\\Vert^{2}\\right)^{1/3}},\\quad\\beta_{t}=c\\eta_{t}^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $k=\\mathcal{O}(G^{2/3}L^{-1})$ $w=\\mathcal{O}(G^{2})$ and $c=\\mathcal{O}(L^{2})$ . The settings of these hyper-parameters are crucial to the convergence analysis of STORM. However, it is worth noting that $L$ is the smoothness parameter and $G$ is the gradient upper bound, which are often difficult to determine in practice. ", "page_idx": 4}, {"type": "text", "text": "To address this problem, our approach defines the hyper-parameters as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\eta_{t}=\\operatorname*{min}\\left\\{\\frac{1}{T^{1/3}},\\frac{1}{T^{(1-\\alpha)/3}\\left(\\sum_{i=1}^{t}\\left\\Vert\\mathbf{v}_{i}\\right\\Vert^{2}\\right)^{\\alpha}}\\right\\},\\quad\\beta_{t}=\\beta=T^{-2/3},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $0<\\alpha<1/3$ . Notably, our method does not rely on the parameters $L$ and $G$ , and also does not need the bounded gradients or bounded function values assumptions that are common in other methods. Although our formulation initially requires knowledge of the iteration number $T$ ,this can be effectively circumvented using the doubling trick, which will be explained later. The above learning rate $\\eta_{t}$ can also be expressed in an alternative, more illustrative manner: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\eta_{t}=\\left\\{\\begin{array}{l l}{\\frac{1}{T^{1/3}}}&{\\mathrm{if~}\\sum_{i=1}^{t}\\left\\Vert\\mathbf{v}_{i}\\right\\Vert^{2}\\leq T^{1/3};}\\\\ {\\frac{1}{T^{\\left(1-\\alpha\\right)/3}\\left(\\sum_{i=1}^{t}\\left\\Vert\\mathbf{v}_{i}\\right\\Vert^{2}\\right)^{\\alpha}}}&{\\mathrm{else}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This formulation ensures that the learning rate starts sufficiently small in the initial stages and then changes dynamically based on the gradient estimator $\\mathbf{v}_{t}$ . This design makes our learning rate setup and convergence analysis distinctly different from previous methods. Next, we present the following theoretical guarantee for our algorithm. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 Under Assumptions 1, 2 and 3, Algorithm $^{\\,l}$ with hyper-parameters in equation (3) guarantees that: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\|\\nabla f(\\mathbf{x}_{\\tau})\\|\\right]\\leq\\mathcal{O}\\left(\\frac{\\Delta_{f}^{\\frac{1}{2(1-\\alpha)}}+\\sigma^{\\frac{1}{1-\\alpha}}+L^{\\frac{1}{2\\alpha}}}{T^{1/3}}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Remark: To ensure that $\\mathbb{E}[\\|\\nabla f(\\mathbf{x}_{\\tau})\\|]\\leq\\epsilon$ , the overall complexity is $\\mathcal{O}(\\epsilon^{-3})$ , which is known to be optimal up to constant factors [Arjevani et al., 2023]. Compared with existing STORM-based algorithms [Cutkosky and Orabona, 2019, Levy et al., 2021, Liu et al., 2022], our method does not have the extra ${\\mathcal{O}}(\\log T)$ term in the convergence rate, and our analysis does not require bounded gradients or bounded function values assumptions. Also note that the selection of $\\alpha$ does not affect the order of $T$ , and larger $\\alpha$ leads to better dependence on parameter $L$ and worse reliance on parameters $\\Delta$ and $\\sigma$ . Considering we require that $0\\bar{<}\\,\\alpha<1/3$ , we can simply set $\\alpha=0.3$ in practice. ", "page_idx": 4}, {"type": "text", "text": "3.3 The doubling trick ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "While we have attained the optimal convergence rate using the proposed adaptive STORM method, it requires knowing the total number of iterations $T$ in advance. Here, we show that we can avoid this requirement by using the doubling trick, which divides the algorithm into several stages and increases the iteration number in each stage gradually. Specifically, we design a multi-stage algorithm over $k=\\{1,2,\\cdots\\,,K\\}$ stages. At the beginning of each new stage, we reset $\\mathbf{x}_{t}=\\mathbf{x}_{0}$ . In each stage $k$ the STORM algorithm is executed for $2^{k-1}$ iterations, effectively doubling the iteration numbers after each stage. In any step $t$ , we first identify the current stage as $1+\\lfloor\\log t\\rfloor$ and then calculate the iteration number for this stage as $I_{t}=2^{\\lfloor\\log t\\rfloor}$ . Then, we can set the hyper-parameters as: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\eta_{t}=\\operatorname*{min}\\left\\{\\frac{1}{I_{t}^{1/3}},\\frac{1}{I_{t}^{(1-\\alpha)/3}\\left(\\sum_{i=I_{t}}^{t}\\Vert\\mathbf{v}_{i}\\Vert^{2}\\right)^{\\alpha}}\\right\\},\\quad\\beta_{t}=I_{t}^{-2/3},\\quad I_{t}=2^{\\lfloor\\log t\\rfloor}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This approach eliminates the need to predetermine the iteration number $T$ . By using the doubling trick, we can still obtain the same optimal convergence rate as stated in the following theorem. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2 Under Assumptions 1, 2 and 3, Algorithm $^{\\,l}$ with hyper-parameters in equation (4) guaranteesthat: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\|\\nabla f(\\mathbf{x}_{\\tau})\\|\\right]\\leq\\mathcal{O}\\left(\\frac{\\Delta_{f}^{\\frac{1}{2(1-\\alpha)}}+\\sigma^{\\frac{1}{1-\\alpha}}+L^{\\frac{1}{2\\alpha}}}{T^{1/3}}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4  Extension to stochastic compositional optimization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To demonstrate the broad applicability of our proposed technique, we extend it to stochastic compositional optimization [Wang et al., 2017a,b, Yuan et al., 2019, Zhang and Xiao, 2019, 2021, Jiang et al., 2023, 2024a], formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{d}}F(\\mathbf{x})=f(g(\\mathbf{x})),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $f$ and $g$ are smooth functions. We assume that we can only access to unbiased estimations of $\\nabla f(\\mathbf{x})$ \uff0c $\\nabla g(\\mathbf{x})$ and $g\\mathbf{(x)}$ , denoted as $\\nabla f(\\mathbf{x};\\xi),\\nabla g(\\mathbf{x};\\zeta)$ and $g(\\mathbf{x};\\zeta)$ . Here $\\xi$ and $\\zeta$ symbolize the random sample drawn for a stochastic oracle such that $\\mathbb{E}[\\nabla f(\\mathbf{x};\\xi)]=\\nabla f(\\mathbf{x}),$ $\\mathbb{E}[\\dot{g}(\\bar{\\mathbf{x}};\\zeta)]=g(\\mathbf{x})$ and $\\mathbb{E}[\\nabla g(\\mathbf{x};\\zeta)]=\\nabla g(\\mathbf{x})$ ", "page_idx": 5}, {"type": "text", "text": "Existing variance reduction methods $[\\mathrm{Hu}$ et al., 2019, Zhang and Xiao, 2019, Qi et al., 2021] are able to obtain optimal ${\\mathcal{O}}(T^{-1/3})$ convergence rates for problem (5), but they require the knowledge of smoothness parameter and the gradient upper bound to set up hyper-parameters. In this section, we aim to achieve the same optimal convergence rate without prior knowledge of problem-dependent parameters. We develop our adaptive algorithm for this problem as follows. In each step $t$ the algorithm maintains an inner function estimator $\\mathbf{u}_{t}$ in the style of STORM, i.e., ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{u}_{t}=(1-\\beta)\\mathbf{u}_{t-1}+g(\\mathbf{x}_{t};\\zeta_{t})-(1-\\beta)g(\\mathbf{x}_{t-1};\\zeta_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then, we construct a gradient estimator $\\mathbf{v}_{t}$ based on $\\mathbf{u}_{t}$ also in the style of STORM: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf v_{t}=(1-\\beta)\\mathbf v_{t-1}+\\nabla f(\\mathbf u_{t};\\xi_{t})\\nabla g(\\mathbf x_{t};\\zeta_{t})-(1-\\beta)\\nabla f(\\mathbf u_{t-1};\\xi_{t})\\nabla g(\\mathbf x_{t-1};\\zeta_{t}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "After that, we apply gradient descent using the gradient estimator $\\mathbf{v}_{t}$ . The whole algorithm is presented $\\begin{array}{r}{\\mathbf{u}_{1}=\\sum_{i=1}^{B_{0}^{-}}\\frac{1}{B_{0}}\\bar{g}(\\mathbf{x}_{1};\\zeta_{1}^{i})}\\end{array}$ $\\begin{array}{r}{\\mathbf{v}_{1}=\\sum_{i=1}^{B_{0}}\\frac{1}{B_{0}}\\nabla f(\\bar{\\mathbf{u}_{1}};\\xi_{1}^{i})\\nabla g(\\mathbf{x}_{1};\\zeta_{1})}\\end{array}$ $B_{0}=T^{1/3}$ Next, we list common assumptions used in the literature of compositional optimization [Wang et al., 2017a,b, Yuan et al., 2019, Zhang and Xiao, 2019, 2021]. ", "page_idx": 5}, {"type": "text", "text": "Assumption 4 (Average smoothness and Lipschitz continuity) ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\|\\nabla f(\\mathbf{x};\\xi)-\\nabla f(\\mathbf{y};\\xi)\\|^{2}\\right]\\leq L\\|\\mathbf{x}-\\mathbf{y}\\|^{2};\\;\\mathbb{E}\\left[\\|f(\\mathbf{x};\\xi)-f(\\mathbf{y};\\xi)\\|^{2}\\right]\\leq C\\|\\mathbf{x}-\\mathbf{y}\\|^{2};}\\\\ &{\\mathbb{E}\\left[\\|\\nabla g(\\mathbf{x};\\zeta)-\\nabla g(\\mathbf{y};\\zeta)\\|^{2}\\right]\\leq L\\|\\mathbf{x}-\\mathbf{y}\\|^{2};\\;\\mathbb{E}\\left[\\|g(\\mathbf{x};\\zeta)-g(\\mathbf{y};\\zeta)\\|^{2}\\right]\\leq C\\|\\mathbf{x}-\\mathbf{y}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Assumption 5 (Bounded variance) ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|g(\\mathbf{x};\\zeta)-g(\\mathbf{x})\\right\\|^{2}\\right]\\leq\\sigma^{2};\\mathbb{E}\\left[\\left\\|\\nabla g(\\mathbf{x};\\zeta)-\\nabla g(\\mathbf{x})\\right\\|^{2}\\right]\\leq\\sigma^{2};\\mathbb{E}\\left[\\left\\|\\nabla f(\\mathbf{x};\\xi)-\\nabla f(\\mathbf{x})\\right\\|^{2}\\right]\\leq\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "table", "img_path": "tmQH8prqLc/tmp/72847a2495f9d3fdf71d7b616b05fbeaa0fc8735342c0da0db05cf13ce09aa19.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Assumption 6 $F_{*}=\\operatorname*{inf}_{\\mathbf{x}}F(\\mathbf{x})\\geq-\\infty$ and $F\\left(\\mathbf{x}_{1}\\right)-F_{*}\\leq\\Delta_{F}$ for the initial solution $\\mathbf{x}_{1}$ ", "page_idx": 6}, {"type": "text", "text": "Remark: In Assumption 4, we further require standard Lipschitz continuity assumption, which is essential and widely required in the literature for stochastic compositional optimization [Wang et al., 2017b, Yuan et al., 2019, Jiang et al., 2022a,b]. This assumption is inherently introduced by the compositional optimization itself rather than by our adaptive techniques. ", "page_idx": 6}, {"type": "text", "text": "With the above assumptions, our algorithm enjoys the following guarantee. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3 Under Assumptions 4, 5 and $^{6}$ . our Algorithm 2 ensures that: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\|\\nabla F(\\mathbf{x}_{\\tau})\\|\\right]\\leq\\mathcal{O}\\left(T^{-1/3}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Remark: This rate matches the state-of-the-art (SOTA) results in stochastic compositional optimization [Hu et al., 2019, Zhang and Xiao, 2019, Qi et al., 2021], and our method achieve this in an adaptive manner. Note that our convergence rate aligns with the lower bound for single-level problems [Arjevani et al., 2023] and is thus unimprovable. ", "page_idx": 6}, {"type": "text", "text": "5   Adaptive variance reduction for finite-sum optimization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we further improve our adaptive variance reduction method to obtain an enhanced convergence rate for non-convex finite-sum optimization, which is in the form of ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{d}}F(\\mathbf{x})=\\frac{1}{n}\\sum_{i=1}^{n}f_{i}(\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "whereeach $f_{i}(\\cdot)$ is a smooth non-convex function. Existing adaptive method for this problem [Kavis et al., 2022] achieves a convergence rate of ${\\mathcal{O}}(n^{1/4}T^{-1/2}\\log(n T))$ based on the variance reduction technique SPIDER [Fang et al., 2018], suffering from an extra ${\\mathcal{O}}(\\log(n T))$ term compared with the corresponding lower bound [Fang et al., 2018, Li et al., 2021]. ", "page_idx": 6}, {"type": "text", "text": "To obtain the optimal convergence rate for finite-sum optimization, we incorporate techniques from the SAG algorithm [Roux et al., 2012] into the STORM estimator. Specifically, in each step $t$ we start by randomly sample $i_{t}$ from the set $\\{1,2,\\cdots\\,,n\\}$ . Then, we construct a variance reduction gradient estimator as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{v}_{t}=(1-\\beta)\\mathbf{v}_{t-1}+\\nabla f_{i_{t}}(\\mathbf{x}_{t})-(1-\\beta)\\nabla f_{i_{t}}(\\mathbf{x}_{t-1})-\\beta\\left(g_{t}^{i_{t}}-\\frac{1}{n}\\sum_{i=1}^{n}g_{t}^{i}\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the first three terms align with the original STORM method, and the last term, inspired by the SAG algorithm, deals with the finite-sum structure. Here, $g_{t}$ tracks the gradient as ", "page_idx": 6}, {"type": "equation", "text": "$$\ng_{t+1}^{i}=\\left\\{\\begin{array}{l l}{\\nabla f_{i_{t}}(\\mathbf{x}_{t})}&{i=i_{t}}\\\\ {g_{t}^{i}}&{i\\neq i_{t}}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "By such a design, we can ensure that the estimation error $\\mathbb{E}[\\left\\|\\mathbf{v}_{t}-\\nabla F(\\mathbf{x}_{t})\\right\\|^{2}]$ reduces gradually. The whole algorithm is stated in Algorithm 3. In this case, we set the hyper-parameters as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\eta_{t}=\\frac{1}{n^{\\frac{1-\\alpha}{2}}\\left(\\sum_{i=1}^{t}\\left\\Vert\\mathbf{v}_{i}\\right\\Vert^{2}\\right)^{\\alpha}},\\quad\\beta=\\frac{1}{n},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Algorithm 3 STORM for Finite-sum Optimization (SAG-type) ", "page_idx": 7}, {"type": "table", "img_path": "tmQH8prqLc/tmp/f2fae83cdf27dcce1aaf2565c181f92686d839f99e5b0132407ae7b2bd5b7eb2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "where $0<\\alpha<1/3$ . The learning rate $\\eta_{t}$ is non-increasing and changes according to the gradient estimations, and the momentum parameter $\\beta$ remains unchanged throughout the learning process. Next, we show that our method enjoys the optimal convergence rate under the following assumptions, which are standard and widely adopted in existing literature [Fang et al., 2018, Wang et al., 2019, Li et al., 2021]. ", "page_idx": 7}, {"type": "text", "text": "Assumption 7 (Smoothness\uff09 For each $i\\in\\{1,2,\\cdots\\,,m\\},$ function $f_{i}$ is $L$ -smooth such that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla f_{i}(\\mathbf{x})-\\nabla f_{i}(\\mathbf{y})\\|\\leq L\\|\\mathbf{x}-\\mathbf{y}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Assumption 8 $F_{*}=\\operatorname*{inf}_{\\mathbf{x}}F(\\mathbf{x})\\geq-\\infty$ and $F\\left(\\mathbf{x}_{1}\\right)-F_{*}\\leq\\Delta_{F}$ for the initial solution $\\mathbf{x}_{1}$ ", "page_idx": 7}, {"type": "text", "text": "Theorem 4 Under Assumptions 7 and $\\boldsymbol{\\vartheta}$ our Algorithm $^3$ guarantees that: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\|\\nabla F({\\mathbf x}_{\\tau})\\|\\right]\\leq\\mathcal{O}\\left(\\frac{n^{1/4}}{T^{1/2}}\\left(\\Delta_{F}^{\\frac{1}{2(1-\\alpha)}}+L^{\\frac{1}{2\\alpha}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Remark: Our result matches the lower bound for non-convex finite-sum problems [Fang et al., 2018, Li et al., 2021], and makes an improvement over the existing adaptive method, i.e., AdaSpider [Kavis et al., 2022]. Specifically, the convergence rate of the AdaSpider algorithm $\\mathcal{O}\\left(n^{1/4}T^{-1/2}\\left(L^{2}+\\Delta_{F}\\right)\\cdot\\log\\left(\\bar{1}+n T L\\right)\\right)$ ,and oureultis btterthan theiswhen $\\begin{array}{r}{\\frac14<\\alpha<\\frac13}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "We can avoid storing past gradients by following the SVRG method [Zhang et al., 2013, Johnson and Zhang, 2013] to compute the full gradient periodically and incorporate it into STORM estimator. Instead of storing the past gradients as in SAG algorithm, we can avoid this storage cost by incorporating elements from the SVRG method. Specifically, we compute a full batch gradient at the first stepandevery $I$ iteration(weset $I=n$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\nabla f(\\mathbf{x}_{\\tau})=\\frac{1}{n}\\sum_{i=1}^{n}\\nabla f_{i}(\\mathbf{x}_{\\tau}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "For other iterations, we randomly select an index $i_{t}$ from the set $\\{1,2,\\cdots\\,,n\\}$ and compute: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf v_{t}=(1-\\beta)\\mathbf v_{t-1}+\\nabla f_{i_{t}}(\\mathbf x_{t})-(1-\\beta)\\nabla f_{i_{t}}(\\mathbf x_{t-1})-\\beta\\left(\\nabla f_{i_{t}}(\\mathbf x_{\\tau})-\\nabla f(\\mathbf x_{\\tau})\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Note that the first three terms match the original STORM estimator, and the last term, inspired from SVRG, deals with the finite-sum structure. Compared with equation (8) in Algorithm 3, the difference isthat weuse $\\left(\\nabla f_{i_{t}}(\\mathbf{x}_{\\tau})-\\nabla f(\\mathbf{x}_{\\tau})\\right)$ instead of $\\begin{array}{r}{\\left(g_{t}^{i_{t}}-\\frac{1}{n}\\sum_{i=1}^{n}g_{t}^{i}\\right)}\\end{array}$ in the last term. The detailed procedure is outlined in Algorithm 4. This strategy maintains the same optimal rate, as stated below: ", "page_idx": 7}, {"type": "text", "text": "Theorem 5 Under Assumptions 7 and 8, our Algorithm $^{4}$ guarantees that: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\|\\nabla F({\\mathbf x}_{\\tau})\\|\\right]\\leq\\mathcal{O}\\left(\\frac{n^{1/4}}{T^{1/2}}\\left(\\Delta_{F}^{\\frac{1}{2(1-\\alpha)}}+L^{\\frac{1}{2\\alpha}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Remark: The obtained convergence rate is in the same order as the results in Theorem 4, and Algorithm 4 does not require storing past gradients anymore. ", "page_idx": 7}, {"type": "text", "text": "Algorithm 4 STORM for Finite-sum Optimization (SVRG-type) ", "page_idx": 8}, {"type": "image", "img_path": "tmQH8prqLc/tmp/d8c7892c44f407bafcf31d71273e7eac6b253f004d7fa0ff0b63fe3c25189df8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "", "img_caption": ["Figure 1: Results for CIFAR-10 dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we evaluate the performance of the proposed Ada-STORM method via numerical experiments on image classification tasks and language modeling tasks. In the experiments, we compare our method with STORM [Cutkosky and Orabona, 2019], STORM $^+$ [Levy et al., 2021] and METASTORM [Liu et al., 2022], as well as SGD, Adam [Kingma and Ba, 2015] and AdaBelief [Zhuang et al., 2020]. We use the default implementation of SGD and Adam from Pytorch [Paszke et al., 2019]. For $\\mathrm{STORM+}$ , we follow its original implementation?, and build STORM, META-STORM and our Ada-STORM based on it. When it comes to hyper-parameter tuning, we simply set $\\alpha=0.3$ forour algorithm. For other methods, we either set the hyper-parameters as recommended in the original papers or tune them by grid search. For example, we search the learning rate of SGD, Adam and AdaBelief from the set $\\{1e{-5,1e{-4,1e{-3,1e{-2,1e{-1}}}\\}}$ and select the best one for each method. All the experiments are conducted on eight NVIDIA Tesla V100 GPUs. ", "page_idx": 8}, {"type": "text", "text": "Image classification task  First, we conduct numerical experiments on multi-class image classification tasks to evaluate the performance of the proposed method. Specifically, we train ResNet18 and ResNet34 models [He et al., 2016] on the CIFAR-10 and CIFAR-100 datasets [Krizhevsky, 2009] respectively. For all optimizers, we set the batch size as 256 and train for 200 epochs. We plot the loss value and the accuracy against the epochs on the CIFAR-10 and CIFAR-100 datasets in Figure 1 and Figure 2. It is observed that, for training loss and training accuracy, our Ada-STORM algorithm achieves comparable performance with respect to other methods, and it outperforms the others in terms of testing loss and thus obtains a better testing accuracy. ", "page_idx": 8}, {"type": "text", "text": "Language modeling task  Then, we perform experiments on language modeling tasks. Concretely, we train a 2-layer Transformer [Vaswani et al., 2017] over the WiKi-Text2 dataset [Merity, 2016]. We use 256 dimensional word embeddings, 512 hidden unites and 2 heads. The batch size is set as 20 and all methods are trained for 40 epochs with dropout rate 0.1. We also clip the gradients by norm 0.25 in case of the exploding gradient. We report both the loss and perplexity versus the number of epochs in Figure 3. From the results, we observe that our method converges more quickly than other methods and obtains a slightly better perplexity compared with others, indicating the effectiveness of the proposed method. ", "page_idx": 8}, {"type": "image", "img_path": "tmQH8prqLc/tmp/43c932ff11f9098353d8d02a9d12ef7dda84ff8204d2d9a706458e8750641881.jpg", "img_caption": ["Figure 2: Results for CIFAR-100 dataset. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "tmQH8prqLc/tmp/4ca772118ab84ffda630e1c04d9c82059046d7f87a85cf439208b9b0c31e9d5d.jpg", "img_caption": ["Figure 3: Results for WikiText-2 dataset. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose an adaptive STORM method to achieve the optimal convergence rate for nonconvex functions. Compared with existing methods, our algorithm requires weaker assumptions and does not have the additional ${\\mathcal{O}}(\\log T)$ term in the convergence rate. The proposed technique can also be employed to develop optimal adaptive algorithms for compositional optimization. Furthermore, we investigate an adaptive method for non-convex finite-sum optimization, obtaining an improved convergencerate of $\\bar{\\mathcal{O}}(n^{1/4}T^{-1/2})$ . Given that STORM algorithm has already been used in many areas such as bi-level optimization [Yang et al., 2021], federated learning [Das et al., 2022], min-max optimization [Xian et al., 2021], sign-based optimization [Jiang et al., 2024b], etc., the proposed methods may also inspire the development of adaptive algorithms in these fields. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was partially supported by National Key R&D Program of China (2021ZD0112802), NSFC (62122037), and the Postgraduate Research & Practice Innovation Program of Jiangsu Province (No. KYCX24_0231). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Y. Arjevani, Y. Carmon, J. C. Duchi, D. J. Foster, N. Srebro, and B. Woodworth. Lower bounds for non-convex stochastic optimization. Mathematical Programming, 199(1):165-214, 2023.   \nY. Carmon and O. Hinder. Making SGD parameter-free. In Proceedings of the 35rd Annual Conference on Learning Theory, pages 2360-2389, 2022.   \nK. Chen, J. Langford, and F. Orabona. Better parameter-free stochastic optimization with ODE updates for coin-betting. In Proceedings of the 36th AAAI Conference on Artificial Intelligence, pages6239-6247,2022.   \nA. Cutkosky and F. Orabona. Momentum-based variance reduction in non-convex SGD. In Advances in Neural Information Processing Systems 32, pages 15210-15219, 2019.   \nR. Das, A. Acharya, A. Hashemi, S. Sanghavi, I. S. Dhillon, and U. Topcu. Faster non-convex federated learning via global and local momentum. In Proceedings of the 38th Conference on Uncertainty in Artificial Intelligence, pages 496-506, 2022.   \nJ. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(61):2121-2159, 2011.   \nC. Fang, C. J. Li, Z. Lin, and T. Zhang. SPIDER: Near-optimal non-convex optimization via stochastic path-integrated differential estimator. In Advances in Neural Information Processing Systems 31, pages 689-699, 2018.   \nS. Ghadimi and G. Lan. Stochastic first- and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.   \nK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 770-778, 2016.   \nW. Hu, C. J. Li, X. Lian, J. Liu, and H. Yuan. Effcient smooth non-convex stochastic compositional optimization via stochastic recursive gradient descent. In Advances in Neural Information Processing Systems 32, pages 6543-6553, 2019.   \nF. Huang, J. Li, and H. Huang. Super-adam: Faster and universal framework of adaptive gradients. In Advances in Neural Information Processing Systems 34, pages 9074-9085, 2021.   \nM. Ivgi, O. Hinder, and Y. Carmon, Dog is SGD's best friend: A parameter-free dynamic stp size schedule. InProceedings of the 4Oth International Conference on Machine Learning, pages 14465-14499, 2023.   \nW. Jiang, G. Li, Y. Wang, L. Zhang, and T. Yang. Multi-block-single-probe variance reduced estimator for coupled compositional optimization. In Advances in Neural Information Processing Systems 35, pages 32499-32511, 2022a.   \nW. Jiang, B. Wang, Y. Wang, L. Zhang, and T. Yang. Optimal algorithms for stochastic multi-level compositional optimization. In Proceedings of the 39th International Conference on Machine Learning, pages 10195-10216, 2022b.   \nW. Jiang, J. Qin, L. Wu, C. Chen, T. Yang, and L. Zhang. Learning unnormalized statistical models via compositional optimization. In Procedings of the 4Oth International Conference on Machine Learning, pages 15i05-15124, 2023.   \nW. Jiang, S. Yang, W. Yang, Y. Wang, Y. Wan, and L. Zhang. Projection-free variance reduction methods for stochastic constrained multi-level compositional optimization. In Proceedings of the 4lst International Conference on Machine Learning, pages 21962-21987, 2024a.   \nW. Jiang, S. Yang, W. Yang, and L. Zhang. Effcient sign-based optimization: Accelerating convergence via variance reduction. In Advances in Neural Information Processing Systems 37, 2024b.   \nR. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems 26, 2013.   \nA. Kavis, S. Skoulakis, K. Antonakopoulos, L. T. Dadi, and V. Cevher. Adaptive stochastic variance reduction for non-convex finite-summinimization. In Advances in Neural Information Processing Systems 35, pages 23524-23538, 2022.   \nD. P. Kingma and J. L. Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.   \nA. Krizhevsky. Learning multiple layers of features from tiny images. Masters Thesis, Deptartment of Computer Science, University of Toronto, 2009.   \nK. Y. Levy, A. Kavis, and V. Cevher. STORM $^+$ : Fully adaptive SGD with recursive momentum for nonconvex optimization. In Advances in Neural Information Processing Systems 34, pages 20571-20582, 2021.   \nZ. Li, H. Bao, X. Zhang, and P Richtarik. Page: A simple and optimal probabilistic gradient estimator for nonconvex optimization. In Proceedings of the 38th International Conference on Machine Learning, pages 6286-6295, 2021.   \nZ. Liu, T. D. Nguyen, T. H. Nguyen, A. Ene, and H. L. Nguyen. Meta-storm: Generalized fullyadaptive variance reduced SGD for unbounded functions. ArXiv e-prints, arXiv:2209.14853, 2022.   \nI. Loshchilov and F. Hutter. SGDR: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations, 2017.   \nH. B. McMahan and M. Streeter. Adaptive bound optimization for online convex optimization. In Proceedings of the 23rd Annual Conference on Learning Theory, 2010.   \nS. Merity. The wikitext long term dependency language modeling dataset. Salesforce Metamind, 9, 2016.   \nL. M. Nguyen, J. Liu, K. Scheinberg, and M. Takac. SARAH: A novel method for machine learning problems using stochastic recursive gradient. In Proceedings of the 34th International Conference on Machine Learning, pages 2613-2621, 2017.   \nF. Orabona. Simultaneous model selection and optimization through parameter-free stochastic learning. In Advances in Neural Information Processing Systems 27, pages 1116-1124, 2014.   \nA. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B.Steier, LFang JBai, and S.Chintala Ptrch: An merative stylehig-erformance learning library. In Advances in Neural Information Processing Systems 32, 2019.   \nQ. Qi, Z. Guo, Y. Xu, R. Jin, and T. Yang. An online method for a class of distributionally robust optimization with non-convex objectives. In Advances in Neural Information Processing Systems 34, pages 10067-10080, 2021.   \nN. L. Roux, M. Schmidt, and F. R. Bach. A stochastic gradient method with an exponential convergence rate for finite training sets. In Advances in Neural Information Processing Systems 25, pages 2672-2680, 2012.   \nT. Tieleman and G Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 4, pages 26-31, 2012.   \nQ. Tran-Dinh, N. H. Pham, D. T. Phan, and L. M. Nguyen. Hybrid stochastic gradient descent algorithms for stochastic nonconvex optimization. ArXiv e-prints, arXiv:1905.05920, 2019.   \nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin. Attention is allyou need. In Advances in Neural Information Processing Systems 30, pages 5998-6008, 2017.   \nM. Wang, E. X. Fang, and H. Liu. Stochastic compositional gradient descent: algorithms for minimizing compositions of expected-value functions. Mathematical Programming, 161(1-2): 419-449, 2017a.   \nM. Wang, J. Liu, and E. X. Fang. Accelerating stochastic composition optimization. Journal of Machine Learning Research, 18:105:1-105:23, 2017b.   \nZ.Wang,K.Ji, Y.Zhou, Y Liang, and V.Tarokh Spideroost and momentum: Faster variance reduction algorithms. In Advances in Neural Information Processing Systems 32, pages 2406-2416, 2019.   \nW. Xian, F. Huang, Y. Zhang, and H. Huang. A faster decentralized algorithm for nonconvex minimax problems. In Advances in Neural Information Processing Systems 34, pages 25865-25877, 2021.   \nJ. Yang, K. Ji, and Y. Liang. Provably faster algorithms for bilevel optimization. In Advances in Neural Information Processing Systems 34, pages 13670-13682, 2021.   \nJ. Yang, X. Li, I. Fatkhullin, and N. He. Two sides of one coin: the limits of untuned SGD and the power of adaptive methods. In Advances in Neural Information Processing Systems 36, pages 1206-1216, 2023.   \nH. Yuan, X. Lian, C. J. Li, J. Liu, and W. Hu. Efficient smooth non-convex stochastic compositional optimization via stochastic recursive gradient descent. In Advances in Neural Information Processing Systems 32, pages 14905-14916, 2019.   \nJ. Zhang and L. Xiao. A stochastic composite gradient method with incremental variance reduction. In Advances in Neural Information Processing Systems 32, pages 9075-9085, 2019.   \nJ. Zhang and L. Xiao. Multilevel composite stochastic optimization via nested variance reduction. SIAM Journal on Optimization, 31(2):1131-1157, 2021.   \nL. Zhang, M. Mahdavi, and R. Jin. Linear convergence with condition number independent access of full gradients. In Advances in Neural Information Processing Systems 26, pages 980-988, 2013.   \nJ. Zhuang, T. Tang, Y. Ding, S. C. Tatikonda, N. Dvornek, X. Papademetris, and J. Duncan. Adabelief optimizer: Adapting stepsizes by the belief in observed gradients. In Advances in Neural Information Processing Systems 33, pages 18795-18806, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Proof of Theorem 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "First, we introduce the following lemma, which is frequently used in our proof. ", "page_idx": 13}, {"type": "text", "text": "Lemma 1 Suppose $c_{i}$ is positive for $i=\\{1,2,\\cdots\\,,n\\}$ , and let $0<\\alpha<1$ . We can ensure that: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{n}c_{i}\\right)^{1-\\alpha}\\leq\\sum_{i=1}^{n}{\\frac{c_{i}}{\\left(\\sum_{j=1}^{i}c_{j}\\right)^{\\alpha}}}\\leq{\\frac{1}{1-\\alpha}}\\left(\\sum_{i=1}^{n}c_{i}\\right)^{1-\\alpha}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof 1 The proof mainly follows McMahan and Streeter [2010] and a similar analysis also appears in Levy et al. [2021]. First, we prove the right part by Induction. ", "page_idx": 13}, {"type": "text", "text": "$(I)$ For $n=1$ we can easily show that the hypothesis holds: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{c_{1}}{c_{1}^{\\alpha}}=c_{1}^{1-\\alpha}\\leq\\frac{1}{1-\\alpha}c_{1}^{1-\\alpha}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "(2) Next, assuming that the hypothesis holds for $n=t-1$ , then we show that it also holds for $n=t$ Defne $Z=\\textstyle\\sum_{i=1}^{t}c_{i}$ and $X=c_{t}$ For $n=t$ wehave ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{i=1}^{t}\\frac{c_{i}}{\\left(\\sum_{j=1}^{i}c_{j}\\right)^{\\alpha}}=\\sum_{i=1}^{t-1}\\frac{c_{i}}{\\left(\\sum_{j=1}^{i}c_{j}\\right)^{\\alpha}}+\\frac{c_{t}}{\\left(\\sum_{j=1}^{t}c_{j}\\right)^{\\alpha}}}\\\\ {\\displaystyle\\leq\\frac{1}{1-\\alpha}\\left(\\sum_{i=1}^{t-1}c_{i}\\right)^{1-\\alpha}+\\frac{c_{t}}{\\left(\\sum_{j=1}^{t}c_{j}\\right)^{\\alpha}}}\\\\ {\\displaystyle=\\frac{1}{1-\\alpha}(Z-X)^{1-\\alpha}+\\frac{X}{Z^{\\alpha}}:=h(X).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Taking the derivative concerning $x$ ,we know that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{d h(X)}{d X}=\\frac{1}{Z^{\\alpha}}-\\frac{1}{(Z-X)^{\\alpha}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which indicates that $h(X)$ decreases as $X$ increasing. Since $0\\le X\\le Z$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{0\\leq X\\leq Z}h(X)=h(0)=\\frac{1}{1-\\alpha}Z^{1-\\alpha}=\\frac{1}{1-\\alpha}\\left(\\sum_{i=1}^{t}c_{i}\\right)^{1-\\alpha},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which implies that the hypothesis is true for $n=t$ ", "page_idx": 13}, {"type": "text", "text": "Combining $(I)$ and (2), we finish the proof for the right part. Then, we give the proof of the left part asfollows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}{\\frac{c_{i}}{\\left(\\sum_{j=1}^{i}c_{j}\\right)^{\\alpha}}}\\geq\\sum_{i=1}^{n}{\\frac{c_{i}}{\\left(\\sum_{j=1}^{n}c_{j}\\right)^{\\alpha}}}=\\left(\\sum_{i=1}^{n}c_{i}\\right)^{1-\\alpha}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Thus, we finish the proof of this lemma. ", "page_idx": 13}, {"type": "text", "text": "Next, we can obtain the following guarantee for our algorithm. ", "page_idx": 13}, {"type": "text", "text": "Lemma 2 Our method enjoys the following guarantee: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\left[\\eta_{t}\\left\\lVert\\mathbf{v}_{t}\\right\\rVert^{2}\\right]}\\\\ &{\\leq\\left(2\\Delta_{f}+\\sigma^{2}\\right)+\\mathbb{E}\\underbrace{\\left[\\frac{2L^{2}}{\\beta}\\sum_{t=1}^{T}\\eta_{t}^{3}\\left\\lVert\\mathbf{v}_{t}\\right\\rVert^{2}\\right]}_{:=1}+\\mathbb{E}\\underbrace{\\left[L\\sum_{t=1}^{T}\\eta_{t}^{2}\\left\\lVert\\mathbf{v}_{t}\\right\\rVert^{2}\\right]}_{:=1}+\\mathbb{E}\\underbrace{\\left[2\\beta\\sigma^{2}\\displaystyle\\sum_{t=1}^{T}\\eta_{t}\\right]}_{:=1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbb{E}_{\\theta+1}}\\left[\\left\\|\\nabla f(\\mathbf{x}_{t+1})-\\nabla_{t+1}\\right\\|^{2}\\right]}\\\\ &{=\\mathbb{E}_{\\theta+1}\\left[\\left\\|(1-\\beta)\\nabla_{t}+\\nabla f(\\mathbf{x}_{t+1}|\\xi_{t+1})-(1-\\beta)\\nabla f(\\mathbf{x}_{t+1}\\xi_{t+1})-\\nabla f(\\mathbf{x}_{t+1})\\right\\|^{2}\\right]}\\\\ &{=\\mathbb{E}_{\\theta+1}\\left[(1-\\beta)(\\nabla_{t}-\\nabla f(\\mathbf{x}_{t}))+(\\nabla f(\\mathbf{x}_{t})-\\nabla f(\\mathbf{x}_{t+1})+\\nabla f(\\mathbf{x}_{t+1}\\xi_{t+1})-\\nabla f(\\mathbf{x}_{t};\\xi_{t+1}))\\right]}\\\\ &{\\qquad+\\beta(\\nabla f(\\mathbf{x}_{t};\\xi_{t+1})-\\nabla f(\\mathbf{x}_{t}))\\right\\|^{2}]}\\\\ &{\\le\\mathbb{E}_{\\theta+1}\\left[(1-\\beta)^{2}\\|\\nabla_{t}-\\nabla f(\\mathbf{x}_{t})\\|^{2}\\right]+\\mathbb{E}_{\\theta+1}\\left[\\left\\|\\nabla f(\\mathbf{x}_{t})-\\nabla f(\\mathbf{x}_{t+1})\\right\\|}\\\\ &{\\qquad+\\nabla f(\\mathbf{x}_{t+1}\\xi_{t+1})-\\nabla f(\\mathbf{x}_{t}\\xi_{t+1})+\\beta(\\nabla f(\\mathbf{x}_{t};\\xi_{t+1})-\\nabla f(\\mathbf{x}_{t}))\\right\\|^{2}\\right]}\\\\ &{\\le\\mathbb{E}_{\\theta+1}\\left[(1-\\beta)^{2}\\|\\nabla_{t}-\\nabla f(\\mathbf{x}_{t})\\|^{2}\\right]}\\\\ &{\\qquad+2\\mathbb{E}_{\\theta+1}\\left[\\|\\nabla f(\\mathbf{x}_{t})-\\nabla f(\\mathbf{x}_{t+1})+\\nabla f(\\mathbf{x}_{t+1};\\xi_{t+1})-\\nabla f(\\mathbf{x}_{t};\\xi_{t+1})\\|^{2}\\right]}\\\\ &{\\quad+2\\mathbb{E}_{\\theta+1}\\left[\\beta(\\nabla f(\\mathbf{x}_{t};\\xi_{t+1})-\\nabla f(\\mathbf{x}_{t}))\\right]\\|}\\\\ &{\\le(1-\\beta)^{2}\\mathbb{E}_{\\theta+1}\\left[\\|\\nabla_{t}-\\nabla f( \n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that $\\eta_{t}$ is independent of random variable $\\xi_{t+1}$ . So we can guarantee that: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\xi_{t+1}}\\left[\\eta_{t}\\left\\|\\nabla f(\\mathbf{x}_{t+1})-\\mathbf{v}_{t+1}\\right\\|^{2}\\right]\\le(1-\\beta)\\eta_{t}\\|\\mathbf{v}_{t}-\\nabla f(\\mathbf{x}_{t})\\|^{2}\\,+2\\beta^{2}\\sigma^{2}\\eta_{t}+2L^{2}\\eta_{t}^{3}\\|\\mathbf{v}_{t}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "After rearranging, we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\eta_{t}\\left\\Vert\\mathbf{v}_{t}-\\nabla f(\\mathbf{x}_{t})\\right\\Vert^{2}}\\\\ &{\\le\\displaystyle\\frac{\\eta_{t}}{\\beta}\\left\\Vert\\mathbf{v}_{t}-\\nabla f(\\mathbf{x}_{t})\\right\\Vert^{2}-\\mathbb{E}_{\\xi_{t+1}}\\left[\\frac{\\eta_{t}}{\\beta}\\left\\Vert\\mathbf{v}_{t+1}-\\nabla f(\\mathbf{x}_{t+1})\\right\\Vert^{2}\\right]+2\\beta\\sigma^{2}\\eta_{t}+\\frac{2L^{2}}{\\beta}\\eta_{t}^{3}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Letting $\\mathcal{H}_{t}$ be the history to time $t$ i.e., $\\mathcal{H}_{t}=\\{\\xi_{1},\\cdots,\\xi_{t}\\}$ , we ensure that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathcal{H}_{t}}\\left[\\eta_{t}\\left\\Vert\\mathbf{v}_{t}-\\nabla f(\\mathbf{x}_{t})\\right\\Vert^{2}\\right]\\leq\\mathbb{E}_{\\mathcal{H}_{t}}\\left[\\frac{\\eta_{t}}{\\beta}\\left\\Vert\\mathbf{v}_{t}-\\nabla f(\\mathbf{x}_{t})\\right\\Vert^{2}\\right]-\\mathbb{E}_{\\mathcal{H}_{t+1}}\\left[\\frac{\\eta_{t}}{\\beta}\\left\\Vert\\mathbf{v}_{t+1}-\\nabla f(\\mathbf{x}_{t+1})\\right\\Vert^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\;2\\beta\\sigma^{2}\\mathbb{E}_{\\mathcal{H}_{t}}\\left[\\eta_{t}\\right]+\\frac{2L^{2}}{\\beta}\\mathbb{E}_{\\mathcal{H}_{t}}\\left[\\eta_{t}^{3}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By summing up and noting that $\\eta_{t}$ is non-increasing such that $\\eta_{t+1}\\leq\\eta_{t}$ , we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{\\mathcal{H}_{t}}\\left[\\eta_{t}\\left\\Vert\\mathbf{v}_{t}-\\nabla f(\\mathbf{x}_{t})\\right\\Vert^{2}\\right]}\\\\ &{\\displaystyle\\leq\\mathbb{E}_{\\mathcal{H}_{1}}\\left[\\frac{\\eta_{1}}{\\beta}\\left\\Vert\\mathbf{v}_{1}-\\nabla f(\\mathbf{x}_{1})\\right\\Vert^{2}\\right]+2\\beta\\sigma^{2}\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{\\mathcal{H}_{t}}\\left[\\eta_{t}\\right]+\\frac{2L^{2}}{\\beta}\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{\\mathcal{H}_{t}}\\left[\\eta_{t}^{3}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since we use a large batch size in the first iteration, that is, $B_{0}=T^{1/3}$ , we can now ensure that $\\begin{array}{r}{\\mathbb{E}_{\\mathcal{H}_{1}}\\left[\\left\\|\\mathbf{v}_{1}-\\nabla f(\\mathbf{x}_{1})\\right\\|^{2}\\right]\\le\\frac{\\sigma^{2}}{B_{0}}=\\frac{\\sigma^{2}}{T^{1/3}}}\\end{array}$ $\\eta_{1}\\le T^{-1/3}$ and $\\beta=T^{-2/3}$ the first term of the above inequality is less than $\\sigma^{2}$ $S o$ , we can finally have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}\\left[\\eta_{t}\\left\\Vert\\mathbf{v}_{t}-\\nabla f(\\mathbf{x}_{t})\\right\\Vert^{2}\\right]\\leq\\sigma^{2}+2\\beta\\sigma^{2}\\sum_{t=1}^{T}\\mathbb{E}\\left[\\eta_{t}\\right]+\\frac{2L^{2}}{\\beta}\\sum_{t=1}^{T}\\mathbb{E}\\left[\\eta_{t}^{3}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Also, due to the smoothness of $f(\\mathbf{x})$ weknow that: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{f({\\mathbf x}_{t+1})\\le f({\\mathbf x}_{t})+\\langle\\nabla f\\left({\\mathbf x}_{t}\\right),{\\mathbf x}_{t+1}-{\\mathbf x}_{t}\\rangle+\\displaystyle\\frac{L}{2}\\left\\Vert{\\mathbf x}_{t+1}-{\\mathbf x}_{t}\\right\\Vert^{2}}\\\\ {\\quad=f\\left({\\mathbf x}_{t}\\right)-\\eta_{t}\\left\\langle\\nabla f\\left({\\mathbf x}_{t}\\right),{\\mathbf v}_{t}\\right\\rangle+\\displaystyle\\frac{\\eta_{t}^{2}L}{2}\\left\\Vert{\\mathbf v}_{t}\\right\\Vert^{2}}\\\\ {\\quad=f\\left({\\mathbf x}_{t}\\right)-\\eta_{t}\\left\\langle\\nabla f\\left({\\mathbf x}_{t}\\right),{\\mathbf v}_{t}\\right\\rangle+\\displaystyle\\frac{\\eta_{t}}{2}\\left\\Vert\\nabla f\\left({\\mathbf x}_{t}\\right)\\right\\Vert^{2}+\\displaystyle\\frac{\\eta_{t}}{2}\\left\\Vert{\\mathbf v}_{t}\\right\\Vert^{2}-\\displaystyle\\frac{\\eta_{t}}{2}\\left\\Vert\\nabla f\\left({\\mathbf x}_{t}\\right)\\right\\Vert^{2}}\\\\ {\\quad\\quad\\quad-\\displaystyle\\frac{\\eta_{t}}{2}\\left\\Vert{\\mathbf v}_{t}\\right\\Vert^{2}+\\displaystyle\\frac{\\eta_{t}^{2}L}{2}\\left\\Vert{\\mathbf v}_{t}\\right\\Vert^{2}}\\\\ {\\quad=f({\\mathbf x}_{t})+\\displaystyle\\frac{\\eta_{t}}{2}\\left\\Vert\\nabla f({\\mathbf x}_{t})-{\\mathbf v}_{t}\\right\\Vert^{2}-\\displaystyle\\frac{\\eta_{t}}{2}\\left\\Vert\\nabla f({\\mathbf x}_{t})\\right\\Vert^{2}-\\displaystyle\\frac{\\eta_{t}}{2}\\left\\Vert{\\mathbf v}_{t}\\right\\Vert^{2}+\\displaystyle\\frac{\\eta_{t}^{2}L}{2}\\left\\Vert{\\mathbf v}_{t}\\right\\Vert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By summing up and re-arranging, we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\eta_{t}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\leq2f(\\mathbf{x}_{1})-2f(\\mathbf{x}_{T+1})+\\sum_{t=1}^{T}\\eta_{t}\\left\\Vert\\nabla f(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\right\\Vert^{2}+\\sum_{t=1}^{T}\\eta_{t}^{2}L\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, by using equation (13) and the fact that $f(\\mathbf{x}_{1})-f_{*}\\le\\Delta_{f}$ wehave: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}\\left[\\eta_{t}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right]\\leq2\\Delta_{f}+\\sigma^{2}+2\\beta\\sigma^{2}\\sum_{t=1}^{T}\\mathbb{E}\\left[\\eta_{t}\\right]+2L^{2}\\sum_{t=1}^{T}\\mathbb{E}\\left[\\frac{\\eta_{t}^{3}}{\\beta}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right]+L\\sum_{t=1}^{T}\\mathbb{E}\\left[\\eta_{t}^{2}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which finishes the proof of Lemma 2. ", "page_idx": 15}, {"type": "text", "text": "To effectively bound each term in the above lemma, we divide the algorithm into two stages. Suppose that starting from iteration $t\\,=\\,s$ .the condition $\\begin{array}{r}{\\sum_{i=1}^{t}\\|\\mathbf{v}_{i}\\|^{2}\\geq T^{1/3}}\\end{array}$ beginsto hold.We refer to iterations $t=\\{1,2,\\cdots\\,,s-1\\}$ as the first stage, and $\\bar{t}=\\{s,\\cdots,T\\}$ as the second stage. ", "page_idx": 15}, {"type": "text", "text": "Bounding LHS: In the first stage, we know that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{s-1}\\eta_{t}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}=\\frac{1}{T^{1/3}}\\sum_{t=1}^{s-1}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For the second stage, our analysis leads to: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\sum_{n=1}^{T}\\|\\mathbf v\\|_{\\infty}^{2}=\\displaystyle\\sum_{i=1}^{T}\\frac{\\|\\mathbf v_{i}\\|^{2}}{T^{1+\\frac{1}{\\gamma}}\\left(\\sum_{i=1}^{k}\\|\\mathbf v_{i}\\|^{2}\\right)^{\\alpha}}}&{}\\\\ {\\sum_{t=1}^{T}\\frac{\\|\\mathbf v\\|_{\\infty}^{2}}{T^{1+\\frac{1}{\\gamma}}\\left(T^{1+\\frac{1}{\\gamma}}+\\sum_{i=1}^{k}\\|\\mathbf v_{i}\\|^{2}\\right)^{\\alpha}}}&{}\\\\ {\\sum_{t=1}^{T}\\frac{\\|\\mathbf v\\|_{\\infty}^{2}}{T^{1+\\frac{1}{\\gamma}}\\left(T^{1+\\frac{1}{\\gamma}}+\\left(\\sum_{i=1}^{k}\\|\\mathbf v_{i}\\|^{2}\\right)^{\\alpha}\\right)}}&{}\\\\ {\\sum_{t=1}^{T}\\frac{\\|\\mathbf v\\|_{\\infty}^{2}}{T^{1+\\frac{1}{\\gamma}}\\left(T^{1+\\frac{1}{\\gamma}}+\\operatorname*{lims}\\left\\{T^{1+\\frac{1}{\\gamma}}\\right\\}^{2}\\right)^{\\alpha}}}&{}\\\\ {\\sum_{t=1}^{T}\\frac{\\|\\mathbf v\\|_{\\infty}^{2}}{T^{1+\\frac{1}{\\gamma}}}\\frac{T^{1}\\left\\{\\mathbf v\\|\\mathbf v\\|_{\\infty}^{2}}\\left(\\sum_{i=1}^{k}\\|\\mathbf v_{i}\\|^{2}\\right)^{\\alpha-1}\\right\\}}&{}\\\\ {\\left.\\frac{1}{2}\\frac{1}{2}\\operatorname*{min}\\left\\{\\frac{1}{T^{1+\\frac{1}{\\gamma}}}\\sum_{i=1}^{T}\\|\\mathbf v\\|_{\\infty}^{2}\\left(\\frac{1}{T^{1+\\frac{1}{\\gamma}}}\\sum_{i=1}^{k}\\|\\mathbf v_{i}\\|^{2}\\right)^{1-\\alpha}\\right\\}}\\\\ {=\\frac{1}{2}\\operatorname*{min}\\left\\{\\frac{1}{T^{1+\\frac{1}{\\gamma}}\\sum_{i=1}^{k}\\|\\mathbf v\\|_{\\infty}^{2}},\\left(\\frac{1}{T^{1+\\frac{1}{\\gamma}}}\\sum_{i=1}^{k}\\|\\mathbf v_{i}\\|^{2}\\right)^{1-\\alpha}\\right\\}}\\end{array\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the first inequality stems from $\\begin{array}{r}{\\sum_{t=1}^{s-1}\\left\\|\\mathbf{v}_{t}\\right\\|^{2}\\;\\leq\\;T^{1/3}}\\end{array}$ , the second inequality results from $(x+y)^{\\alpha}\\leq x^{\\alpha}+y^{\\bar{\\alpha}}$ for positive $x,y$ and $0<\\alpha<1/3$ , and the forth inequality applies Lemma 1. Next, we bound (A), (B), (C) as follows. ", "page_idx": 16}, {"type": "text", "text": "Bounding (A): In the first stage, with $\\eta_{t}=T^{-1/3}$ \uff0c $\\beta=T^{-2/3}$ , and $\\begin{array}{r}{\\sum_{t=1}^{s-1}\\left\\|\\mathbf{v}_{t}\\right\\|^{2}\\leq T^{1/3}}\\end{array}$ , we can derive: ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\frac{2L^{2}}{\\beta}}\\sum_{t=1}^{s-1}\\eta_{t}^{3}\\left\\|\\mathbf{v}_{t}\\right\\|^{2}={\\frac{2L^{2}}{T^{1/3}}}\\sum_{t=1}^{s-1}\\left\\|\\mathbf{v}_{t}\\right\\|^{2}\\leq2L^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For the second stage, the analysis gives: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{2L^{2}}{\\beta}\\sum_{t=s}^{T}\\eta_{t}^{3}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\leq2L^{2}\\sum_{t=s}^{T}\\frac{\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}}{T^{\\frac{1-3\\alpha}{3}}\\left(\\sum_{i=s}^{t}\\left\\Vert\\mathbf{v}_{i}\\right\\Vert^{2}\\right)^{3\\alpha}}\\leq\\frac{2L^{2}}{1-3\\alpha}\\left(\\frac{1}{T^{1/3}}\\sum_{t=s}^{T}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right)^{1-3\\alpha},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the second inequality uses Lemma 1. Then we also have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{2L^{2}}{1-3\\alpha}\\left(\\frac{1}{T^{1/3}}\\sum_{t=s}^{T}\\left\\|\\mathbf{v}_{t}\\right\\|^{2}\\right)^{1-3\\alpha}=\\displaystyle\\frac{2L^{2}}{1-3\\alpha}(8-24\\alpha)^{1-3\\alpha}\\left(\\frac{1}{(8-24\\alpha)}\\frac{1}{T^{1/3}}\\sum_{t=s}^{T}\\left\\|\\mathbf{v}_{t}\\right\\|^{2}\\right)^{1-3\\alpha}}\\\\ &{\\quad\\leq3\\alpha\\left(\\frac{2L^{2}}{1-3\\alpha}(8-24\\alpha)^{1-3\\alpha}\\right)^{\\frac{1}{3\\alpha}}+\\displaystyle\\frac{1}{8T^{1/3}}\\sum_{t=s}^{T}\\left\\|\\mathbf{v}_{t}\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the inequality employs Young's inequality, such that $x y\\le3\\alpha x^{\\frac{1}{3\\alpha}}+(1-3\\alpha)y^{\\frac{1}{1-3\\alpha}}$ for positive $x,y$ .Very similarly, we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{2L^{2}}{1-3\\alpha}\\left(\\displaystyle\\frac{1}{T^{1/3}}\\displaystyle\\sum_{t=s}^{T}\\|\\mathbf{v}_{t}\\|^{2}\\right)^{1-3\\alpha}}\\\\ &{=\\!\\frac{2L^{2}}{1-3\\alpha}\\left(\\displaystyle\\frac{8-24\\alpha}{1-\\alpha}\\right)^{\\frac{1-3\\alpha}{1-\\alpha}}\\left(\\frac{1-\\alpha}{8-24\\alpha}\\right)^{\\frac{1-3\\alpha}{1-\\alpha}}\\left(\\displaystyle\\frac{1}{T^{1/3}}\\displaystyle\\sum_{t=s}^{T}\\|\\mathbf{v}_{t}\\|^{2}\\right)^{1-3\\alpha}}\\\\ &{\\leq\\!\\frac{2\\alpha}{1-\\alpha}\\left(\\displaystyle\\frac{2L^{2}}{1-3\\alpha}\\left(\\frac{8-24\\alpha}{1-\\alpha}\\right)^{\\frac{1-3\\alpha}{1-\\alpha}}\\right)^{\\frac{1-\\alpha}{2\\alpha}}+\\frac{1}{8}\\left(\\displaystyle\\frac{1}{T^{1/3}}\\displaystyle\\sum_{t=s}^{T}\\|\\mathbf{v}_{t}\\|^{2}\\right)^{1-\\alpha},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the second inequality employs Young's inequality, such that ry  @ $\\begin{array}{r}{x y\\leq\\frac{2\\alpha}{1-\\alpha}x^{\\frac{1-\\alpha}{2\\alpha}}+\\frac{1-3\\alpha}{1-\\alpha}y^{\\frac{1-\\alpha}{1-3\\alpha}}}\\end{array}$ for positive $x,y$ . Combining all above, we know that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{(\\mathrm{A})\\leq}2L^{2}+3\\alpha\\left(\\frac{2L^{2}}{1-3\\alpha}(8-24\\alpha)^{1-3\\alpha}\\right)^{\\frac{1}{3\\alpha}}}}\\\\ {{\\displaystyle{\\qquad+\\,\\frac{2\\alpha}{1-\\alpha}\\left(\\frac{2L^{2}}{1-3\\alpha}\\left(\\frac{8-24\\alpha}{1-\\alpha}\\right)^{\\frac{1-3\\alpha}{1-\\alpha}}\\right)^{\\frac{1-\\alpha}{2\\alpha}}+\\frac{\\Gamma}{8}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Bounding (B): In the first stage, with the learning rate set at $\\eta_{t}=T^{-1/3}$ and $\\begin{array}{r}{\\sum_{t=1}^{s-1}\\|\\mathbf{v}_{t}\\|^{2}\\leq T^{1/3}}\\end{array}$ weobserve: ", "page_idx": 16}, {"type": "equation", "text": "$$\nL\\sum_{t=1}^{s-1}\\eta_{t}^{2}\\left\\|\\mathbf{v}_{t}\\right\\|^{2}=\\frac{L}{T^{2/3}}\\sum_{t=1}^{s-1}\\left\\|\\mathbf{v}_{t}\\right\\|^{2}\\leq\\frac{L}{T^{1/3}}\\leq L.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For the second stage, our analysis reveals: ", "page_idx": 16}, {"type": "equation", "text": "$$\nL\\sum_{t=s}^{T}\\eta_{t}^{2}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\leq L\\sum_{t=s}^{T}\\frac{\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}}{T^{\\frac{2(1-\\alpha)}{3}}\\left(\\sum_{i=s}^{t}\\left\\Vert\\mathbf{v}_{i}\\right\\Vert^{2}\\right)^{2\\alpha}}\\leq\\frac{L}{1-2\\alpha}\\left(\\frac{1}{T^{1/3}}\\sum_{t=s}^{T}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right)^{1-2\\alpha},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the second inequality leverages Lemma 1, and we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\frac{L}{1-2\\alpha}\\left(\\frac{1}{T^{1/3}}\\sum_{t=s}^{T}\\|\\mathbf{v}_{t}\\|^{2}\\right)^{1-2\\alpha}}\\\\ &{=\\!\\frac{L}{1-2\\alpha}(8-16\\alpha)^{1-2\\alpha}\\frac{1}{(8-16\\alpha)^{1-2\\alpha}}\\left(\\frac{1}{T^{1/3}}\\sum_{t=s}^{T}\\|\\mathbf{v}_{t}\\|^{2}\\right)^{1-2\\alpha}}\\\\ &{\\le\\!2\\alpha\\left(\\frac{(8-16\\alpha)^{1-2\\alpha}L}{1-2\\alpha}\\right)^{\\frac{1}{2\\alpha}}\\!+\\!\\frac{1}{8T^{1/3}}\\sum_{t=s}^{T}\\|\\mathbf{v}_{t}\\|^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the inequality is due to Young's inequality, such that $x y\\,\\le\\,2\\alpha x^{\\frac{1}{2\\alpha}}+(1-2\\alpha)y^{\\frac{1}{1-2\\alpha}}$ for positive $x,y$ .Very similarly, we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{L}{1-2\\alpha}\\left(\\displaystyle\\frac{1}{T^{1/3}}\\displaystyle\\sum_{t=s}^{T}\\|\\mathbf{v}_{t}\\|^{2}\\right)^{1-2\\alpha}}\\\\ &{=\\!\\!\\frac{L}{1-2\\alpha}\\left(\\frac{8-16\\alpha}{1-\\alpha}\\right)^{\\frac{1-2\\alpha}{1-\\alpha}}\\left(\\frac{1-\\alpha}{8-16\\alpha}\\right)^{\\frac{1-2\\alpha}{1-\\alpha}}\\left(\\displaystyle\\frac{1}{T^{1/3}}\\displaystyle\\sum_{t=s}^{T}\\|\\mathbf{v}_{t}\\|^{2}\\right)^{1-2\\alpha}}\\\\ &{\\le\\!\\!\\frac{\\alpha}{1-\\alpha}\\left(\\displaystyle\\frac{L}{1-2\\alpha}\\left(\\frac{8-16\\alpha}{1-\\alpha}\\right)^{\\frac{1-2\\alpha}{1-\\alpha}}\\right)^{\\frac{1-\\alpha}{\\alpha}}\\!+\\frac{1}{8}\\left(\\displaystyle\\frac{1}{T^{1/3}}\\displaystyle\\sum_{t=s}^{T}\\|\\mathbf{v}_{t}\\|^{2}\\right)^{1-\\alpha},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Wwhe the s $\\begin{array}{r}{x y\\leq\\frac{\\alpha}{1-\\alpha}x^{\\frac{1-\\alpha}{\\alpha}}+\\frac{1-2\\alpha}{1-\\alpha}y^{\\frac{1-\\alpha}{1-2\\alpha}}}\\end{array}$ $x,y$ . Combining all the above, we know that ", "page_idx": 17}, {"type": "equation", "text": "$$\n(\\mathrm{B})\\le L+2\\alpha\\left(\\frac{(8-16\\alpha)^{1-2\\alpha}L}{1-2\\alpha}\\right)^{\\frac{1}{2\\alpha}}+\\frac{\\alpha}{1-\\alpha}\\left(\\frac{L}{1-2\\alpha}\\left(\\frac{8-16\\alpha}{1-\\alpha}\\right)^{\\frac{1-2\\alpha}{1-\\alpha}}\\right)^{\\frac{1-\\alpha}{\\alpha}}+\\frac{\\Gamma}{8}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Bounding (C): Given that $\\beta=T^{-2/3}$ and $\\eta_{t}\\le T^{-1/3}$ , we can easily know that $\\mathrm{(C)}\\le2\\sigma^{2}$ So far, we bound all terms in Lemma 2, and we can deduce ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{L}\\mathbf{H}\\mathbf{S}\\geq\\mathbb{E}\\left[\\frac{1}{T^{1/3}}\\sum_{t=1}^{s-1}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right]+\\mathbb{E}\\left[\\frac{\\Gamma}{2}\\right];\\quad\\mathbf{R}\\mathbf{H}\\mathbf{S}\\leq\\mathbb{E}\\left[\\frac{\\Gamma}{4}\\right]+C_{0},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle C_{0}=\\left(2\\Delta_{f}+3\\sigma^{2}+L+2L^{2}\\right)+3\\alpha\\left(\\frac{2L^{2}}{1-3\\alpha}\\left(8-24\\alpha\\right)^{1-3\\alpha}\\right)^{\\frac{1}{3\\alpha}}}\\ ~}\\\\ {{\\displaystyle~~~~~+\\frac{2\\alpha}{1-\\alpha}\\left(\\frac{2L^{2}}{1-3\\alpha}\\left(\\frac{8-24\\alpha}{1-\\alpha}\\right)^{\\frac{1-3\\alpha}{1-\\alpha}}\\right)^{\\frac{1-\\alpha}{2\\alpha}}+2\\alpha\\left(\\frac{\\left(8-16\\alpha\\right)^{1-2\\alpha}L}{1-2\\alpha}\\right)^{\\frac{1}{2\\alpha}}}\\ ~}\\\\ {{\\displaystyle~~~~~+\\frac{\\alpha}{1-\\alpha}\\left(\\frac{L}{1-2\\alpha}\\left(\\frac{8-16\\alpha}{1-\\alpha}\\right)^{\\frac{1-2\\alpha}{1-\\alpha}}\\right)^{\\frac{1-\\alpha}{\\alpha}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "These suggest that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{s-1}\\left\\lVert\\mathbf{v}_{t}\\right\\rVert^{2}\\right]\\leq C_{0}T^{1/3};\\quad\\mathbb{E}\\left[\\Gamma\\right]\\leq4C_{0}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "With the definition of $\\Gamma$ , we know that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{min}\\left\\{\\frac{1}{T^{1/3}}\\sum_{t=s}^{T}\\left\\|\\mathbf{v}_{t}\\right\\|^{2},\\left(\\frac{1}{T^{1/3}}\\sum_{t=s}^{T}\\left\\|\\mathbf{v}_{t}\\right\\|^{2}\\right)^{1-\\alpha}\\right\\}\\right]\\leq4C_{0},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which indicates the following by applying Jensen's inequality: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=s}^{T}\\|\\mathbf{v}_{t}\\|\\right]\\leq\\operatorname*{max}\\{(4C_{0})^{\\frac{1}{2}},(4C_{0})^{\\frac{1}{2(1-\\alpha)}}\\}T^{-1/3}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Also, because of Jensen's inequality, we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\displaystyle\\frac{1}{T}\\sum_{t=1}^{s-1}\\lVert\\mathbf{v}_{t}\\rVert\\right]\\leq\\sqrt{\\mathbb{E}\\left[\\left(\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=1}^{s-1}\\lVert\\mathbf{v}_{t}\\rVert\\right)^{2}\\right]}=\\sqrt{\\mathbb{E}\\left[\\displaystyle\\frac{1}{T^{2}}\\left(\\displaystyle\\sum_{t=1}^{s-1}\\lVert\\mathbf{v}_{t}\\rVert\\right)^{2}\\right]}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\sqrt{\\mathbb{E}\\left[\\displaystyle\\frac{s}{T^{2}}\\displaystyle\\sum_{t=1}^{s-1}\\lVert\\mathbf{v}_{t}\\rVert^{2}\\right]}\\leq\\sqrt{\\mathbb{E}\\left[\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=1}^{s-1}\\lVert\\mathbf{v}_{t}\\rVert^{2}\\right]}}\\\\ &{\\qquad\\qquad\\leq\\sqrt{\\displaystyle\\frac{1}{T}C_{0}T^{1/3}}=\\sqrt{C_{0}}T^{-1/3}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Summing up, we have proven that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\mathbf{v}_{t}\\|\\right]\\leq\\operatorname*{max}\\{3(C_{0})^{\\frac{1}{2}},(C_{0})^{\\frac{1}{2}}+(4C_{0})^{\\frac{1}{2(1-\\alpha)}}\\}T^{-1/3}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Finally, we finish our proof by introducing the following lemma. ", "page_idx": 18}, {"type": "text", "text": "Lemma 3 Suppose $0<\\beta<1$ ,our method ensures that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}\\left[\\left\\Vert\\mathbf{v}_{t}-\\nabla f(\\mathbf{x}_{t})\\right\\Vert^{2}\\right]\\leq3\\sigma^{2}T^{1/3}+\\mathbb{E}\\left[\\frac{2L^{2}}{\\beta}\\sum_{t=1}^{T}\\eta_{t}^{2}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof 3 First note that we have already proven the following in equation $(I I)$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\Vert\\nabla f(\\mathbf{x}_{t+1})-\\mathbf{v}_{t+1}\\Vert^{2}\\right]\\leq(1-\\beta)\\mathbb{E}\\left[\\Vert\\mathbf{v}_{t}-\\nabla f(\\mathbf{x}_{t})\\Vert^{2}\\right]+2\\beta^{2}\\sigma^{2}+2L^{2}\\mathbb{E}\\left[\\eta_{t}^{2}\\Vert\\mathbf{v}_{t}\\Vert^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "After rearranging the items, we can get the following: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\left\\Vert\\mathbf{v}_{t}-\\nabla f(\\mathbf{x}_{t})\\right\\Vert^{2}\\right]\\leq\\frac{1}{\\beta}\\left(\\mathbb{E}\\left[\\left\\Vert\\mathbf{v}_{t}-\\nabla f(\\mathbf{x}_{t})\\right\\Vert^{2}\\right]-\\mathbb{E}\\left[\\left\\Vert\\mathbf{v}_{t+1}-\\nabla f(\\mathbf{x}_{t+1})\\right\\Vert^{2}\\right]\\right)+2\\beta\\sigma^{2}}&{}\\\\ {+\\left.\\frac{2L^{2}}{\\beta}\\mathbb{E}\\left[\\eta_{t}^{2}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By summing up, we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}\\left[\\Vert\\mathbf{v}_{t}-\\nabla f(\\mathbf{x}_{t})\\Vert^{2}\\right]\\leq\\frac{1}{\\beta}\\mathbb{E}\\left[\\Vert\\mathbf{v}_{1}-\\nabla f(\\mathbf{x}_{1})\\Vert^{2}\\right]+2\\beta\\sigma^{2}T+\\frac{2L^{2}}{\\beta}\\sum_{t=1}^{T}\\mathbb{E}\\left[\\eta_{t}^{2}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since we use a large batch size in the first iteration, that is, $B_{0}=T^{1/3}$ , we can now ensure that $\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|\\mathbf{v}_{1}-\\nabla f(\\mathbf{x}_{1})\\bar{\\|}^{2}\\right\\|^{2}\\right]\\le\\frac{\\sigma^{2}}{B_{0}}=\\frac{\\sigma^{2}}{T^{1/3}}}\\end{array}$ Note that $\\begin{array}{r}{\\beta=\\frac{1}{T^{2/3}}}\\end{array}$ $\\sigma^{2}T^{1/3}$ and the second term reduces to $2\\sigma^{2}T^{1/3}$ . To this end, we ensure ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}\\left[\\left\\Vert\\mathbf{v}_{t}-\\nabla f(\\mathbf{x}_{t})\\right\\Vert^{2}\\right]\\leq3\\sigma^{2}T^{1/3}+\\frac{2L^{2}}{\\beta}\\sum_{t=1}^{T}\\mathbb{E}\\left[\\eta_{t}^{2}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus we finish the proof for this lemma. ", "page_idx": 18}, {"type": "text", "text": "Here, we bound the term $\\begin{array}{r}{\\frac{2L^{2}}{\\beta}\\sum_{t=1}^{T}\\eta_{t}^{2}\\left\\|\\mathbf{v}_{t}\\right\\|^{2}}\\end{array}$ a follow In the fist ta wh $\\begin{array}{r}{\\eta_{t}=\\frac{1}{T^{1/3}}}\\end{array}$ , we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{2L^{2}}{\\beta}\\sum_{t=1}^{s-1}\\eta_{t}^{2}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}=\\frac{2L^{2}}{\\beta}\\sum_{t=1}^{s-1}\\frac{1}{T^{2/3}}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\leq2L^{2}T^{1/3}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For the second stage, the analysis gives: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{2L^{2}}{\\beta}\\displaystyle\\sum_{t=s}^{T}\\eta_{t}^{2}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\leq2L^{2}T^{2/3}\\displaystyle\\sum_{t=s}^{T}\\frac{\\|\\mathbf{v}_{t}\\|^{2}}{T^{\\frac{2-2\\alpha}{3}}(\\sum_{t=s}^{t}\\|\\mathbf{v}_{t}\\|)^{2\\alpha}}}\\\\ &{\\phantom{\\leq}\\leq\\frac{2L^{2}T^{2}2\\alpha/3}{1-2\\alpha}\\frac{((1-2\\alpha)/L)^{1-2\\alpha}}{((1-2\\alpha)/L)^{1-2\\alpha}}\\left(\\displaystyle\\sum_{t=s}^{T}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right)^{1-2\\alpha}}\\\\ &{\\phantom{\\leq}\\leq2\\alpha\\left(\\frac{2L^{2}T^{2\\alpha/3}((1-2\\alpha)/L)^{1-2\\alpha}}{1-2\\alpha}\\right)^{1/2\\alpha}+L\\displaystyle\\sum_{t=s}^{T}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}}\\\\ &{\\phantom{\\leq}\\leq2\\alpha\\left(\\frac{2L^{2}((1-2\\alpha)/L)^{1-2\\alpha}}{1-2\\alpha}\\right)^{\\frac{1}{2\\alpha}}T^{1/3}+L\\displaystyle\\sum_{t=1}^{T}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the second inequality uses Lemma 1, and the third one employs Young's inequality, such that $x y\\leq2\\alpha x^{\\frac{1}{2\\alpha}}+(1-\\dot{2}\\alpha)y^{\\frac{1}{1-2\\alpha}}$ forpositive $x,y$ We alsoknow that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{2L^{2}}{\\beta}\\displaystyle\\sum_{t=s}^{T}\\eta_{t}^{2}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\leq2L^{2}T^{2/3}\\displaystyle\\sum_{t=s}^{T}\\frac{\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}}{T^{\\frac{2-2\\alpha}{3}}(\\sum_{i=s}^{t}\\left\\Vert\\mathbf{v}_{i}\\right\\Vert)^{2\\alpha}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{2L^{2}T^{2\\alpha/3}}{1-2\\alpha}\\left(\\frac{1-2\\alpha}{(1-\\alpha)L}T^{-\\frac{9}{1-\\alpha}}\\left(\\frac{1-\\alpha}{1-2\\alpha}T^{\\frac{9}{3}}L\\right)^{\\frac{1-2\\alpha}{1-\\alpha}}\\left(\\displaystyle\\sum_{t=s}^{T}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right)^{1-2\\alpha}\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.\\leq\\frac{\\alpha}{1-\\alpha}\\left(\\frac{2L^{2}T^{2\\alpha/3}}{1-2\\alpha}\\left(\\frac{1-2\\alpha}{(1-\\alpha)L}T^{-\\frac{9}{3}}\\right)^{\\frac{1-2\\alpha}{1-\\alpha}}\\right)^{\\frac{1-\\alpha}{\\alpha}}+T^{\\frac{8}{3}}L\\left(\\displaystyle\\sum_{t=s}^{T}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right)^{1-\\alpha}}\\\\ &{\\qquad\\qquad\\leq\\frac{\\alpha}{1-\\alpha}\\left(\\frac{2L^{2}}{1-2\\alpha}\\left(\\frac{1-2\\alpha}{(1-\\alpha)L}\\right)^{\\frac{1-2\\alpha}{1-\\alpha}}\\right)^{\\frac{1-\\alpha}{\\alpha}}T^{1/3}+T^{\\frac{9}{3}}L\\left(\\displaystyle\\sum_{t=s}^{T}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right)^{1-\\alpha},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the second inequality uses Lemma 1, and the third one employs Young's inequality, such that $\\begin{array}{r}{x y\\leq\\frac{\\alpha}{1-\\alpha}x^{\\frac{1-\\alpha}{\\alpha}}+\\frac{1-2\\alpha}{1-\\alpha}y^{\\frac{1-\\alpha}{1-2\\alpha}}}\\end{array}$ for posive $x,y$ ", "page_idx": 19}, {"type": "text", "text": "As a result, we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\left[\\left\\lVert\\mathbf{v}_{t}-\\nabla f(\\mathbf{x}_{t})\\right\\rVert^{2}\\right]}\\\\ &{\\leq\\left(3\\sigma^{2}+4C_{0}L+L^{\\frac{1+2\\alpha}{2\\alpha}}\\left(\\frac{2(1-2\\alpha)^{1-2\\alpha}}{1-2\\alpha}\\right)^{\\frac{1}{2\\alpha}}+L^{\\frac{1}{\\alpha}}\\left(\\frac{2}{1-2\\alpha}\\left(\\frac{1-2\\alpha}{1-\\alpha}\\right)^{\\frac{1-2\\alpha}{1-\\alpha}}\\right)^{\\frac{1-\\alpha}{\\alpha}}\\right)T^{1/3}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By integrating these findings, we can finally have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})\\|\\right]\\leq\\frac{1}{T}\\mathbb{E}\\left[\\sum_{t=1}^{T}\\|\\mathbf{v}_{t}\\|\\right]+\\frac{1}{T}\\left[\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\|\\right]\\leq\\frac{C^{\\prime}}{T^{1/3}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{C^{\\prime}=\\operatorname*{max}\\{3(C_{0})^{\\frac{1}{2}},(C_{0})^{\\frac{1}{2}}+(4C_{0})^{\\frac{1}{2(1-\\alpha)}}\\}}\\\\ {\\quad\\quad+\\sqrt{3\\sigma^{2}+4C_{0}L+L^{\\frac{1+2\\alpha}{2\\alpha}}\\left(\\frac{2(1-2\\alpha)^{1-2\\alpha}}{1-2\\alpha}\\right)^{\\frac{1}{2\\alpha}}+L^{\\frac{1}{\\alpha}}\\left(\\frac{2}{1-2\\alpha}\\left(\\frac{1-2\\alpha}{1-\\alpha}\\right)^{\\frac{1-2\\alpha}{1-\\alpha}}\\right)^{\\frac{1-\\alpha}{\\alpha}}}}\\\\ {=\\mathcal{O}\\left(\\Delta_{f}^{\\frac{1}{2(1-\\alpha)}}+\\sigma^{\\frac{1}{1-\\alpha}}+L^{\\frac{1}{2\\alpha}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with $C_{0}$ defined in equation (15). We find that larger $\\alpha$ leads to better dependence on $L$ and worse reliance on parameters $\\Delta$ and $\\sigma$ . For $\\alpha\\rightarrow\\textstyle{\\frac{1}{3}}$ , we can obtain that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})\\|\\right]\\leq\\mathcal{O}\\left(\\frac{\\Delta_{f}^{3/4}+\\sigma^{3/2}+L^{3/2}}{T^{1/3}}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since we require $\\begin{array}{r}{0<\\alpha<\\frac{1}{3}}\\end{array}$ , in practice, we can use $\\alpha=0.3$ instead, which leads to a convergence rate of $\\mathcal{O}\\left(\\frac{\\Delta_{f}^{5/7}+\\sigma^{10/7}+L^{5/3}}{T^{1/3}}\\right)$ ", "page_idx": 20}, {"type": "text", "text": "B Proof of Theorem 2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Since $2^{0}\\,+\\,2^{1}\\,+\\,\\cdot\\,\\cdot\\,+\\,2^{K-1}\\,<\\,2^{K}$ , running the algorithm for $T$ iterations guarantees at least $K=\\lfloor\\log(T)\\rfloor$ complete stages. In the theoretical analysis, we can simply use the output of the last complete stage $K=\\lfloor\\log(T)\\rfloor$ , which has been at least run for $2^{K-1}\\geq\\dot{T}/4$ iterations. According to the analysis of Theorem 1, we have already known that running the Algorithm 1 for $T/4$ iterations leads to the following guarantee: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert\\nabla f(\\mathbf{x}_{\\tau})\\Vert\\right]\\leq\\mathcal{O}\\left(\\frac{\\Delta_{f}^{\\frac{1}{2(1-\\alpha)}}+\\sigma^{\\frac{1}{1-\\alpha}}+L^{\\frac{1}{2\\alpha}}}{(T/4)^{1/3}}\\right)=\\mathcal{O}\\left(\\frac{\\Delta_{f}^{\\frac{1}{2(1-\\alpha)}}+\\sigma^{\\frac{1}{1-\\alpha}}+L^{\\frac{1}{2\\alpha}}}{T^{1/3}}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which is on the same order of the original convergence rate. ", "page_idx": 20}, {"type": "text", "text": "C Proof of Theorem 3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "According to equation (14), we have already proven that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\eta_{t}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\leq2F(\\mathbf{x}_{1})-2F(\\mathbf{x}_{T+1})+\\sum_{t=1}^{T}\\eta_{t}\\left\\Vert\\nabla F(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\right\\Vert^{2}+\\sum_{t=1}^{T}\\eta_{t}^{2}L\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then we bound the term $\\begin{array}{r}{\\sum_{t=1}^{T}\\eta_{t}\\left\\|\\nabla F(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\right\\|^{2}}\\end{array}$ as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\Vert\\nabla F(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\right\\Vert^{2}\\leq2\\left\\Vert\\nabla f(g(\\mathbf{x}_{t}))\\nabla g(\\mathbf{x}_{t})-\\nabla f(\\mathbf{u}_{t})\\nabla g(\\mathbf{x}_{t})\\right\\Vert^{2}+2\\left\\Vert\\nabla f(\\mathbf{u}_{t})\\nabla g(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\right\\Vert^{2}}\\\\ {\\leq2C^{2}L^{2}\\left\\Vert g(\\mathbf{x}_{t})-\\mathbf{u}_{t}\\right\\Vert^{2}+2\\left\\Vert\\mathbf{v}_{t}-\\nabla f(\\mathbf{u}_{t})\\nabla g(\\mathbf{x}_{t})\\right\\Vert^{2}.\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Define that $G_{t}=\\nabla f(\\mathbf{u}_{t})\\nabla g(\\mathbf{x}_{t})$ , then we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}\\left[\\eta_{t}\\left\\Vert\\nabla F(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\right\\Vert^{2}\\right]\\leq2C^{2}L^{2}\\sum_{t=1}^{T}\\mathbb{E}\\left[\\eta_{t}\\left\\Vert g(\\mathbf{x}_{t})-\\mathbf{u}_{t}\\right\\Vert^{2}\\right]+2\\sum_{t=1}^{T}\\mathbb{E}\\left[\\eta_{t}\\left\\Vert\\mathbf{v}_{t}-G_{t}\\right\\Vert^{2}\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For the term $\\begin{array}{r}{\\sum_{t=1}^{T}\\mathbb{E}\\left[\\eta_{t}\\left\\Vert\\mathbf{v}_{t}-G_{t}\\right\\Vert^{2}\\right]}\\end{array}$ , following the very similar analysis of equation (11), we have the following guarantee: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\xi_{t+1},\\zeta_{t+1}}\\left[\\left\\|\\mathbf{v}_{t+1}-G_{t+1}\\right\\|^{2}\\right]}\\\\ &{\\le(1-\\beta)\\left\\|\\mathbf{v}_{t}-G_{t}\\right\\|^{2}}\\\\ &{\\qquad+2\\beta^{2}\\mathbb{E}_{\\xi_{t+1},\\zeta_{t+1}}\\left[\\left\\|\\nabla f(\\mathbf{u}_{t+1};\\xi_{t+1})\\nabla g(\\mathbf{x}_{t+1};\\zeta_{t+1})-\\nabla f(\\mathbf{u}_{t+1})\\nabla g(\\mathbf{x}_{t+1})\\right\\|^{2}\\right]}\\\\ &{\\qquad+\\left.2\\mathbb{E}_{\\xi_{t+1},\\zeta_{t+1}}\\left[\\left\\|\\nabla f(\\mathbf{u}_{t+1};\\xi_{t+1})\\nabla g(\\mathbf{x}_{t+1};\\zeta_{t+1})-\\nabla f(\\mathbf{u}_{t};\\xi_{t+1})\\nabla g(\\mathbf{x}_{t};\\zeta_{t+1})\\right\\|^{2}\\right]}\\\\ &{\\le(1-\\beta)\\left\\|\\mathbf{v}_{t}-G_{t}\\right\\|^{2}+4C^{2}\\sigma^{2}\\beta^{2}+4C^{2}L^{2}\\mathbb{E}\\left[\\eta_{t}^{2}\\left\\|\\mathbf{v}_{t}\\right\\|^{2}\\right]+4C^{2}L^{2}\\mathbb{E}\\left[\\left\\|\\mathbf{u}_{t+1}-\\mathbf{u}_{t}\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "That is to say: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\left[\\left.\\eta_{t}\\left\\lVert\\mathbf{v}_{t}-G_{t}\\right\\rVert^{2}\\right]}\\\\ &{\\le\\mathbb{E}\\left[\\frac{\\eta_{1}}{\\beta}\\left\\lVert\\mathbf{v}_{1}-G_{1}\\right\\rVert^{2}\\right]+4C^{2}\\sigma^{2}\\beta\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\eta_{t}\\right]+\\frac{4C^{2}L^{2}}{\\beta}\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\left[\\left.\\eta_{t}^{3}\\left\\lVert\\mathbf{v}_{t}\\right\\rVert^{2}\\right]}\\\\ &{\\qquad+\\frac{4C^{2}L^{2}}{\\beta}\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\left[\\left.\\eta_{t}\\left\\lVert\\mathbf{u}_{t+1}-\\mathbf{u}_{t}\\right\\rVert^{2}\\right\\rvert\\right.}\\\\ &{\\le\\!\\left.(1+4C^{2})\\sigma^{2}+\\frac{4C^{2}L^{2}}{\\beta}\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\left[\\left.\\eta_{t}^{3}\\left\\lVert\\mathbf{v}_{t}\\right\\rVert^{2}\\right]+\\frac{4C^{2}L^{2}}{\\beta}\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\left[\\left.\\eta_{t}\\left\\lVert\\mathbf{u}_{t+1}-\\mathbf{u}_{t}\\right\\rVert^{2}\\right\\rvert\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last inequality due to the fact that $\\beta=T^{-2/3}$ \uff0c $\\eta_{t}\\le T^{-1/3}$ , and we use a large batch size $T^{1/3}$ in the frst iteration. Next, wefurtherbound the tem $\\begin{array}{r}{\\sum_{t=1}^{T}\\mathbb{E}\\left[\\eta_{t-1}\\left\\|\\mathbf{u}_{t}-\\mathbf{u}_{t-1}\\right\\|^{2}\\right]}\\end{array}$ First, we can ensure that: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{\\zeta_{t}}\\left[\\|\\mathbf{u}_{t}-\\mathbf{u}_{t-1}\\|^{2}\\right]}\\\\ &{=\\mathbb{E}_{\\zeta_{t}}\\left[\\|\\beta(g(\\mathbf{x}_{t};\\zeta_{t})-\\mathbf{u}_{t-1})+(1-\\beta)(g(\\mathbf{x}_{t};\\zeta_{t})-g(\\mathbf{x}_{t-1};\\zeta_{t}))\\|^{2}\\right]}\\\\ &{=\\mathbb{E}_{\\zeta_{t}}\\left[\\|\\beta(g(\\mathbf{x}_{t-1})-\\mathbf{u}_{t-1})+(g(\\mathbf{x}_{t};\\zeta_{t})-g(\\mathbf{x}_{t-1};\\zeta_{t}))+\\beta(g(\\mathbf{x}_{t-1};\\zeta_{t})-g(\\mathbf{x}_{t-1}))\\|^{2}\\right]}\\\\ &{\\leq3\\beta^{2}\\left\\|g(\\mathbf{x}_{t-1})-\\mathbf{u}_{t-1}\\right\\|^{2}+3C^{2}\\eta_{t-1}^{2}\\left\\|\\mathbf{v}_{t-1}\\right\\|^{2}+3\\beta^{2}\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "So we know that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{\\beta}\\sum_{t=1}^{T}\\mathbb{E}\\left[\\eta_{t}\\left\\|\\mathbf{u}_{t+1}-\\mathbf{u}_{t}\\right\\|^{2}\\right]}\\\\ {\\displaystyle\\leq3\\beta\\sum_{t=1}^{T}\\mathbb{E}\\left[\\eta_{t}\\left\\|g(\\mathbf{x}_{t})-\\mathbf{u}_{t}\\right\\|^{2}\\right]+3C^{2}\\mathbb{E}\\left[\\sum_{t=1}^{T}\\frac{\\eta_{t}^{3}}{\\beta}\\left\\|\\mathbf{v}_{t}\\right\\|^{2}\\right]+3\\beta\\sigma^{2}\\mathbb{E}\\left[\\sum_{t=1}^{T}\\eta_{t}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "So far, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\left[\\eta_{t}\\left\\|\\nabla F(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\right\\|^{2}\\right]\\leq26C^{2}L^{2}\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\left[\\eta_{t}\\left\\|\\mathbf{u}_{t}-g(\\mathbf{x}_{t})\\right\\|^{2}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\displaystyle\\frac{8C^{2}L^{2}+24C^{4}L^{2}}{\\beta}\\sum_{t=1}^{T}\\mathbb{E}\\left[\\eta_{t}^{3}\\left\\|\\mathbf{v}_{t}\\right\\|^{2}\\right]+(2+8C^{2}+24C^{2}L^{2})\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Next, we can bound $\\begin{array}{r}{\\sum_{t=1}^{T}\\eta_{t}\\left\\Vert\\mathbf{u}_{t}-g(\\mathbf{x}_{t})\\right\\Vert^{2}}\\end{array}$ following equation (11), as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\Vert\\mathbf{u}_{t}-g(\\mathbf{x}_{t})\\right\\Vert^{2}\\leq\\frac{1}{\\beta}\\left(\\left\\Vert\\mathbf{u}_{t}-g(\\mathbf{x}_{t})\\right\\Vert^{2}-\\mathbb{E}_{\\zeta_{t+1}}\\left[\\left\\Vert\\mathbf{u}_{t+1}-g(\\mathbf{x}_{t+1})\\right\\Vert^{2}\\right]\\right)+2\\beta\\sigma^{2}+\\frac{2C^{2}\\eta_{t}^{2}}{\\beta}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "So we can have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\sum_{t=1}^{T}\\eta_{t}\\left\\Vert\\mathbf{u}_{t}-g(\\mathbf{x}_{t})\\right\\Vert^{2}\\right]\\leq\\mathbb{E}\\left[\\frac{\\eta_{1}}{\\beta}\\left\\Vert\\mathbf{u}_{1}-g(\\mathbf{x}_{1})\\right\\Vert^{2}\\right]+2\\beta\\sigma^{2}\\mathbb{E}\\left[\\sum_{t=1}^{T}\\eta_{t}\\right]+\\frac{2C^{2}}{\\beta}\\sum_{t=1}^{T}\\mathbb{E}\\left[\\eta_{t}^{3}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right]}\\\\ {\\displaystyle\\leq3\\sigma^{2}+\\frac{2C^{2}}{\\beta}\\sum_{t=1}^{T}\\mathbb{E}\\left[\\eta_{t}^{3}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last inequality due to the fact that $\\beta=T^{-2/3}$ \uff0c $\\eta_{t}\\le T^{-1/3}$ , and we use a large batch size $T^{1/3}$ in the frst iteration. Combining all, we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\left[\\eta_{t}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right]\\leq2\\Delta_{F}+(2+8C^{2}+102C^{2}L^{2})\\sigma^{2}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad+\\left(8C^{2}L^{2}+76C^{4}L^{2}\\right)\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\frac{\\eta_{t}^{3}}{\\beta}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right]+L\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\eta_{t}^{2}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Treating $\\Delta_{F},C,L,\\sigma$ as constant, the above inequality is very similar to Lemma 2. Thus following the very similar analysis after Lemma 2, we can show that: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\mathbf{v}_{t}\\|\\right]\\leq\\mathcal{O}(T^{-1/3}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "According to previous analysis, we also have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\left[\\left\\Vert\\nabla f(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\right\\Vert^{2}\\right]}\\\\ &{\\le26C^{2}L^{2}\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\left[\\left\\Vert\\mathbf{u}_{t}-g(\\mathbf{x}_{t})\\right\\Vert^{2}\\right]+\\frac{8C^{2}L^{2}+24C^{4}L^{2}}{\\beta}\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\eta_{t}^{2}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right]}\\\\ &{\\qquad\\quad+\\left(2+8C^{2}+24C^{2}L^{2}\\right)\\sigma^{2}T^{1/3}}\\\\ &{\\leq(2+8C^{2}+102C^{2}L^{2})\\sigma^{2}T^{1/3}+8C^{2}L^{2}+76C^{4}L^{2}\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\frac{\\eta_{t}^{2}}{\\beta}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Wwhich s simlart Lema3 and leads to $\\begin{array}{r}{\\sum_{t=1}^{T}\\mathbb{E}\\left[\\left\\Vert\\nabla f(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\right\\Vert/T\\right]\\leq\\mathcal{O}(T^{-1/3})}\\end{array}$ following the same analysis. Combing all these together, we can deduce that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla F(\\mathbf{x}_{t})\\|\\le\\mathcal{O}(T^{-1/3}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which finishes the proof of Theorem 3. ", "page_idx": 22}, {"type": "text", "text": "D Proof of Theorem 4 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Due to the smoothness of $F(\\mathbf{x})$ , we have proven the following in equation (14): ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\eta_{t}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\leq2F(\\mathbf{x}_{1})-2F(\\mathbf{x}_{T+1})+\\sum_{t=1}^{T}\\eta_{t}\\left\\Vert\\nabla F(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\right\\Vert^{2}+\\sum_{t=1}^{T}\\eta_{t}^{2}L\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For the LHS, we have the following guarantee: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\eta_{t}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}=\\sum_{t=1}^{T}\\frac{\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}}{n^{\\frac{1-\\alpha}{2}}\\left(\\sum_{i=1}^{t}\\left\\Vert\\mathbf{v}_{i}\\right\\Vert^{2}\\right)^{\\alpha}}\\geq\\left(\\frac{1}{\\sqrt{n}}\\sum_{t=1}^{T}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right)^{1-\\alpha}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then, we bound the terms in the RHS. First, we have the following lemma. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{T}\\eta_{t}\\left\\Vert\\nabla F(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\right\\Vert^{2}\\right]\\leq2\\beta\\mathbb{E}\\left[\\sum_{t=1}^{T}\\eta_{t}\\left\\Vert\\nabla F(\\mathbf{x}_{t+1})-\\mathbf{z}_{t+1}\\right\\Vert^{2}\\right]+2L^{2}\\mathbb{E}\\left[\\sum_{t=1}^{T}\\frac{\\eta_{t}^{3}}{\\beta}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof 4 According to the definition of $\\mathbf{z}_{t}$ ,theestimator $\\mathbf{v}_{t}$ can be expressed as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{v}_{t}=\\left(1-\\beta\\right)\\mathbf{v}_{t-1}+\\beta\\mathbf{z}_{t}+\\left(1-\\beta\\right)\\left(\\nabla f_{i_{t}}(\\mathbf{x}_{t})-\\nabla f_{i_{t}}(\\mathbf{x}_{t-1})\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that $\\mathbb{E}_{i_{t+1}}\\left[\\mathbf{z}_{t+1}\\right]=\\nabla F(\\mathbf{x}_{t+1}),$ and we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mu_{k}}\\left[\\left\\|\\nabla F(\\mathbf{x}_{i+1})-\\mathbf{v}_{i+1}\\right\\|^{2}\\right]}\\\\ &{=\\mathbb{E}_{\\mu_{k}}\\left[\\left\\|(1-\\beta)\\nabla_{k}+\\beta\\alpha_{1}+(1-\\beta)\\left(\\nabla f_{k+1}(\\mathbf{x}_{i}+1)-\\nabla f_{k+1}(\\mathbf{x}_{i})\\right)-\\nabla F(\\mathbf{x}_{i+1})\\right\\|^{2}\\right]}\\\\ &{=\\mathbb{E}_{\\mu_{k}}\\left[\\|(1-\\beta)(\\nabla_{k}-\\nabla F(\\mathbf{x}_{i}))+\\beta(\\mu_{k+1}-\\nabla F(\\mathbf{x}_{i+1}))\\right]}\\\\ &{\\quad+(1-\\beta)\\left(\\nabla f_{k+1}(\\mathbf{x}_{i})-\\nabla F_{k+1}(\\mathbf{x}_{i})+\\nabla F(\\mathbf{x}_{i})-\\nabla F(\\mathbf{x}_{i+1})\\right)\\|^{2}\\right]}\\\\ &{\\le\\|(1-\\beta)(\\nabla_{k}-\\nabla F(\\mathbf{x}_{i}))\\|^{2}+\\mathbb{E}_{\\mu_{k}}\\left[\\|\\beta(\\mu_{k+1}-\\nabla F(\\mathbf{x}_{i+1}))\\right.}\\\\ &{\\quad+(1-\\beta)\\left(\\nabla f_{k+1}(\\mathbf{x}_{i+1})-\\nabla f_{k+1}(\\mathbf{x}_{i})+\\nabla F(\\mathbf{x}_{i})-\\nabla F(\\mathbf{x}_{i+1})\\right)\\|^{2}\\right]}\\\\ &{\\le(1-\\beta)^{2}\\|\\nabla_{k}-\\nabla F(\\mathbf{x}_{i})\\|^{2}+2\\beta^{2}\\mathbb{E}_{\\mu_{k}+1}\\left[\\mathbb{E}_{\\mu_{k}}(1-\\nabla F(\\mathbf{x}_{i+1}))\\right]^{2}}\\\\ &{\\quad+2(1-\\beta)^{2}\\mathbb{E}_{\\mu_{k}}\\left[\\left\\|\\nabla f_{k+1}(\\mathbf{x}_{i})-\\nabla f_{k+1}(\\mathbf{x}_{i})+\\nabla F(\\mathbf{x}_{i})-\\nabla F(\\mathbf{x}_{i+1})\\right\\|^{2}\\right]}\\\\ &{\\le(1-\\beta)^{2}\\mathbb{E}_{\\mu_{k}}\\left[\\left\\|\\nabla f_{k}(\\mathbf{x}_{i})+(2\\beta)^{2}\\mathbb{E}_{\\mu_{k+1}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Rearrange the items and multiply the both sides by $\\eta_{t}$ , we can get the following: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\eta_{t}\\left\\|\\mathbf{v}_{t}-\\nabla F(\\mathbf{x}_{t})\\right\\|^{2}\\right]\\leq\\mathbb{E}\\left[\\frac{\\eta_{t}}{\\beta}\\left\\|\\mathbf{v}_{t}-\\nabla F(\\mathbf{x}_{t})\\right\\|^{2}\\right]-\\mathbb{E}\\left[\\frac{\\eta_{t}}{\\beta}\\left\\|\\mathbf{v}_{t+1}-\\nabla F(\\mathbf{x}_{t+1})\\right\\|^{2}\\right]}\\\\ {+\\left.2\\beta\\mathbb{E}\\left[\\eta_{t}\\left\\|\\mathbf{z}_{t+1}-\\nabla F(\\mathbf{x}_{t+1})\\right\\|^{2}\\right]+\\frac{2L^{2}}{\\beta}\\mathbb{E}\\left[\\eta_{t}^{3}\\left\\|\\mathbf{v}_{t}\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Notethat $\\eta_{t}$ is non-increasing. By summing up, we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\left[\\eta_{t}\\left\\|\\mathbf{v}_{t}-\\nabla F(\\mathbf{x}_{t})\\right\\|^{2}\\right]}\\\\ &{\\displaystyle\\leq\\mathbb{E}\\left[\\frac{\\eta_{1}}{\\beta}\\left\\|\\mathbf{v}_{1}-\\nabla F(\\mathbf{x}_{1})\\right\\|^{2}\\right]+2\\beta\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\left[\\eta_{t}\\left\\|\\mathbf{z}_{t+1}-\\nabla F(\\mathbf{x}_{t+1})\\right\\|^{2}\\right]+\\frac{2L^{2}}{\\beta}\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\left[\\eta_{t}^{3}\\left\\|\\mathbf{v}_{t}\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since we use a full batch in the first iteration, we can finish the proof of this lemma. ", "page_idx": 23}, {"type": "text", "text": "Next, we bound two terms in the above lemma. ", "page_idx": 23}, {"type": "text", "text": "Lemma5Wecan ensure that ", "page_idx": 23}, {"type": "equation", "text": "$$\n2\\beta\\sum_{t=1}^{T}\\mathbb{E}\\left[\\eta_{t}\\left\\Vert\\nabla F(\\mathbf{x}_{t+1})-\\mathbf{z}_{t+1}\\right\\Vert^{2}\\right]\\leq12L^{2}\\sum_{t=1}^{T}\\mathbb{E}\\left[\\frac{\\eta_{t}^{3}}{\\beta}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof 5 ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathbb{E}_{i+\\mathrm{1}}\\left[\\eta_{t}\\left\\|\\nabla F({\\mathbf x}_{t+1})-{\\mathbf z}_{t+1}\\right\\|^{2}\\right]}}\\\\ {~~}\\\\ {{\\displaystyle=\\mathbb{E}_{i+\\mathrm{1}}\\left[\\eta_{t}\\left\\|\\nabla F({\\mathbf x}_{t+1})-\\nabla f_{i+\\mathrm{t}}({\\mathbf x}_{t+1})+g_{t+1}^{i+\\mathrm{t}}-\\frac{1}{n}\\sum_{i=1}^{n}g_{t+1}^{i}\\right\\|^{2}\\right]}}\\\\ {~~}\\\\ {{\\displaystyle=\\mathbb{E}_{i+\\mathrm{1}}\\left[\\eta_{t}\\left\\|\\nabla f_{i+\\mathrm{t}}({\\mathbf x}_{t+1})-g_{t+1}^{i+\\mathrm{t}}-\\left(\\nabla F({\\mathbf x}_{t+1})-\\frac{1}{n}\\sum_{i=1}^{n}g_{t+1}^{i}\\right)\\right\\|^{2}\\right]}}\\\\ {~~}\\\\ {{\\displaystyle\\le\\mathbb{E}_{i+\\mathrm{1}}\\left[\\eta_{t}\\left\\|\\nabla f_{i+\\mathrm{t}}({\\mathbf x}_{t+1})-g_{t+1}^{i+\\mathrm{t}}\\right\\|^{2}\\right]}}\\\\ {~~}\\\\ {{\\displaystyle=\\frac{1}{n}\\sum_{i=1}^{n}\\eta_{t}\\left\\|\\nabla f_{i}({\\mathbf x}_{t+1})-g_{t+1}^{i}\\right\\|^{2},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last equation is due to the fact that $i_{t+1}$ is randomly sample from $\\{1,2,\\cdots\\,,n\\}$ .Notethat we also have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\eta_{t}\\left\\|\\nabla f_{i}(\\mathbf{x}_{i+1})-g_{t+1}^{i}\\right\\|^{2}=\\mathbb{E}_{t+1}\\left[\\eta_{t}\\left\\|\\nabla f_{t+1}(\\mathbf{x}_{t+1})-g_{t+1}^{i+1}\\right\\|^{2}\\right]}\\\\ &{\\le\\mathbb{E}_{t+1}\\left[\\eta_{t}(1+2n)\\left\\|\\nabla f_{t+1}(\\mathbf{x}_{t+1})-\\nabla f_{t+1}(\\mathbf{x}_{t})\\right\\|^{2}+\\eta_{t}(1+\\frac{1}{2n})\\left\\|\\nabla f_{t+1}(\\mathbf{x}_{t})-g_{t+1}^{i+1}\\right\\|^{2}\\right]}\\\\ &{\\le\\mathbb{E}_{t+1}\\left[(1+2n)L^{2}\\eta_{t}^{3}\\left\\|\\mathbf{v}_{t}\\right\\|^{2}+\\eta_{t}(1+\\frac{1}{2n})\\left\\|\\nabla f_{t+1}(\\mathbf{x}_{t})-g_{t+1}^{i+1}\\right\\|^{2}\\right]}\\\\ &{\\le\\mathbb{E}_{t+1}\\left[(1+2n)L^{2}\\eta_{t}^{3}\\left\\|\\mathbf{v}_{t}\\right\\|^{2}\\right]}\\\\ &{\\qquad+\\eta_{t}(1+\\frac{1}{2n})\\left((1-\\frac{1}{n})\\left\\|\\nabla f_{t+1}(\\mathbf{x}_{t})-g_{t}^{i+1}\\right\\|^{2}+\\frac{1}{n}\\left\\|\\nabla f_{t+1}(\\mathbf{x}_{t})-\\nabla f_{t+1}(\\mathbf{x}_{t})\\right\\|^{2}\\right)\\right]}\\\\ &{\\le\\mathbb{E}_{t+1}\\left[3L^{2}\\eta_{t}^{3}\\left\\|\\mathbf{v}_{t}\\right\\|^{2}+\\eta_{t}(1-\\frac{1}{2n})\\left\\|\\nabla f_{t+1}(\\mathbf{x}_{t})-g_{t}^{i+1}\\right\\|^{2}\\right]}\\\\ &{\\le3n L^{2}\\eta_{t}^{3}\\left\\|\\mathbf{v}_{t}\\right\\|^{2}+\\Big(1-\\frac{1}{2n}\\Big)\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\eta_{t}\\left\\|\\nabla f_{t}(\\mathbf{x}_{t})-g_{t}^{i}\\right\\|^{ \n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "That is to say, we can ensure that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\eta_{t+1}\\left\\|\\nabla f_{i}(\\mathbf{x}_{t+1})-g_{t+1}^{i}\\right\\|^{2}\\leq\\displaystyle\\sum_{i=1}^{n}\\eta_{t}\\left\\|\\nabla f_{i}(\\mathbf{x}_{t+1})-g_{t+1}^{i}\\right\\|^{2}}\\\\ {\\displaystyle\\leq3n L^{2}\\eta_{t}^{3}\\left\\|\\mathbf{v}_{t}\\right\\|^{2}+\\left(1-\\frac{1}{2n}\\right)\\frac{1}{n}\\sum_{i=1}^{n}\\eta_{t}\\left\\|\\nabla f_{i}(\\mathbf{x}_{t})-g_{t}^{i}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By rearranging and summing up, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{2n}\\sum_{t=1}^{T}\\frac{1}{n}\\sum_{i=1}^{n}\\eta_{t}\\left\\Vert\\nabla f_{i}(\\mathbf{x}_{t})-g_{t}^{i}\\right\\Vert^{2}\\leq3n L^{2}\\sum_{t=1}^{T}\\eta_{t}^{3}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}+\\frac{1}{n}\\sum_{i=1}^{n}\\eta_{1}\\left\\Vert\\nabla f_{i}(\\mathbf{x}_{1})-g_{1}^{i}\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since we use $a$ full batch $n$ in the first iteration, the second term equals zero, and thus we obtain: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\frac{1}{n}\\sum_{i=1}^{n}\\eta_{t}\\left\\Vert\\nabla f_{i}(\\mathbf{x}_{t})-g_{t}^{i}\\right\\Vert^{2}\\leq6n^{2}L^{2}\\sum_{t=1}^{T}\\eta_{t}^{3}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}=\\frac{6L^{2}}{\\beta^{2}}\\sum_{t=1}^{T}\\eta_{t}^{3}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which leads to the result of this lemma. ", "page_idx": 24}, {"type": "text", "text": "Lemma 6 We have the following guarantee: ", "page_idx": 24}, {"type": "equation", "text": "$$\n2L^{2}\\sum_{t=1}^{T}\\frac{\\eta_{t}^{3}}{\\beta}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\leq\\frac{2\\alpha}{1-\\alpha}\\left(\\frac{2L^{2}}{1-3\\alpha}\\left(\\frac{24-72\\alpha}{1-\\alpha}\\right)^{\\frac{1-3\\alpha}{1-\\alpha}}\\right)^{\\frac{1-\\alpha}{2\\alpha}}+\\frac{1}{24}\\left(\\frac{1}{\\sqrt{n}}\\sum_{t=1}^{T}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right)^{1-\\alpha}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof 6 ", "text_level": 1, "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2L^{2}\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\frac{\\eta_{t}^{3}}{\\|\\mathbf{v}_{t}\\|^{2}}\\|\\mathbf{v}_{t}\\|^{2}}\\\\ &{=2L^{2}n\\displaystyle\\sum_{t=1}^{T}\\|\\mathbf{v}_{t}\\|^{2}=2L^{2}\\displaystyle\\sum_{t=1}^{T}\\frac{\\|\\mathbf{v}_{t}\\|^{2}}{n^{\\frac{1}{2}-\\frac{2\\alpha}{2}}\\left(\\sum_{t=1}^{t}\\|\\mathbf{v}_{t}\\|^{2}\\right)^{3/\\alpha}}}\\\\ &{\\le\\displaystyle\\frac{2L^{2}}{1-3\\alpha}\\left(\\frac{1}{\\sqrt{n}}\\displaystyle\\sum_{t=1}^{T}\\|\\mathbf{v}_{t}\\|^{2}\\right)^{1-3\\alpha}}\\\\ &{=\\displaystyle\\frac{2L^{2}}{1-3\\alpha}\\left(\\frac{24-72\\alpha}{1-\\alpha}\\right)^{\\frac{1-3\\alpha}{2}}\\left(\\frac{1-\\alpha}{24-72\\alpha}\\right)^{\\frac{1-3\\alpha}{1-\\alpha}}\\left(\\frac{1}{\\sqrt{n}}\\displaystyle\\sum_{t=1}^{T}\\|\\mathbf{v}_{t}\\|^{2}\\right)^{1-3\\alpha}}\\\\ &{\\le\\displaystyle\\frac{2\\alpha}{1-\\alpha}\\left(\\frac{2L^{2}}{1-3\\alpha}\\left(\\frac{24-72\\alpha}{1-\\alpha}\\right)^{\\frac{1-5\\alpha}{1-\\alpha}}\\right)^{\\frac{1}{2\\alpha}}+\\frac{1}{24}\\left(\\frac{1}{\\sqrt{n}}\\displaystyle\\sum_{t=1}^{T}\\|\\mathbf{v}_{t}\\|^{2}\\right)^{1-\\alpha}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last inequality employs Young's inequality, such that cy \u2264 2a $\\begin{array}{r}{x y\\leq\\frac{2\\alpha}{1-\\alpha}x^{\\frac{1-\\alpha}{2\\alpha}}+\\frac{1-3\\alpha}{1-\\alpha}y^{\\frac{1-\\alpha}{1-3\\alpha}}}\\end{array}$ for positive $x,y$ ", "page_idx": 25}, {"type": "text", "text": "Lemma 7 We can ensure the following guarantee: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{T}\\eta_{t}^{2}L\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right]\\leq\\frac{\\alpha}{1-\\alpha}\\left(\\frac{L}{1-2\\alpha}\\left(\\frac{4-8\\alpha}{1-\\alpha}\\right)^{\\frac{1-2\\alpha}{1-\\alpha}}\\right)^{\\frac{1-\\alpha}{\\alpha}}+\\frac{1}{4}\\mathbb{E}\\left[\\left(\\frac{1}{\\sqrt{n}}\\sum_{t=1}^{T}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right)^{1-\\alpha}\\right]\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof 7 ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\eta_{t}^{2}L\\left\\|\\mathbf{v}_{t}\\right\\|^{2}=L\\displaystyle\\sum_{t=1}^{T}\\frac{\\|\\mathbf{v}_{t}\\|^{2}}{n^{1-\\alpha}\\left(\\sum_{t=1}^{t}\\|\\mathbf{v}_{t}\\|^{2}\\right)^{2\\alpha}}}\\\\ &{\\le\\displaystyle\\frac{L}{1-2\\alpha}\\frac{1}{n^{1-\\alpha}}\\left(\\displaystyle\\sum_{t=1}^{T}\\|\\mathbf{v}_{t}\\|^{2}\\right)^{1-2\\alpha}}\\\\ &{\\leq\\displaystyle\\frac{L}{1-2\\alpha}\\left(\\displaystyle\\frac{1}{\\sqrt{n}}\\sum_{t=1}^{T}\\|\\mathbf{v}_{t}\\|^{2}\\right)^{1-2\\alpha}}\\\\ &{=\\displaystyle\\frac{L}{1-2\\alpha}\\left(\\frac{4-8\\alpha}{1-\\alpha}\\right)^{\\frac{1-2\\alpha}{1-\\alpha}}\\left(\\frac{1-\\alpha}{4-8\\alpha}\\right)^{\\frac{1-2\\alpha}{1-\\alpha}}\\left(\\frac{1}{\\sqrt{n}}\\displaystyle\\sum_{t=1}^{T}\\|\\mathbf{v}_{t}\\|^{2}\\right)^{1-2\\alpha}}\\\\ &{\\leq\\displaystyle\\frac{\\alpha}{1-\\alpha}\\left(\\frac{L}{1-2\\alpha}\\left(\\frac{4-8\\alpha}{1-\\alpha}\\right)^{\\frac{1-2\\alpha}{1-\\alpha}}\\right)^{\\frac{1-\\alpha}{\\alpha}}+\\frac{1}{4}\\left(\\displaystyle\\frac{1}{\\sqrt{n}}\\displaystyle\\sum_{t=1}^{T}\\|\\mathbf{v}_{t}\\|^{2}\\right)^{1-\\alpha},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Wwhere te lastineguaity employsYong inaguality suchthat $\\begin{array}{r}{x y\\leq\\frac{\\alpha}{1-\\alpha}x^{\\frac{1-\\alpha}{\\alpha}}+\\frac{1-2\\alpha}{1-\\alpha}y^{\\frac{1-\\alpha}{1-2\\alpha}}}\\end{array}$ for positive $x,y$ . Combing all these, we have already proven that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\eta_{t}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right]\\geq\\mathbb{E}\\left[\\left(\\displaystyle\\frac{1}{\\sqrt{n}}\\displaystyle\\sum_{t=1}^{T}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right)^{1-\\alpha}\\right],}\\\\ {\\displaystyle\\sum_{t=1}^{T}\\eta_{t}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\leq C_{3}+\\displaystyle\\frac{3}{4}\\mathbb{E}\\left[\\left(\\displaystyle\\frac{1}{\\sqrt{n}}\\displaystyle\\sum_{t=1}^{T}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right)^{1-\\alpha}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{C_{3}=\\!2\\Delta_{F}+\\displaystyle\\frac{14\\alpha}{1-\\alpha}\\left(\\displaystyle\\frac{2L^{2}}{1-3\\alpha}\\left(\\displaystyle\\frac{24-72\\alpha}{1-\\alpha}\\right)^{\\frac{1-3\\alpha}{1-\\alpha}}\\right)^{\\frac{1-\\alpha}{2\\alpha}}}}\\\\ {{+\\displaystyle\\frac{\\alpha}{1-\\alpha}\\left(\\displaystyle\\frac{L}{1-2\\alpha}\\left(\\displaystyle\\frac{4-8\\alpha}{1-\\alpha}\\right)^{\\frac{1-2\\alpha}{1-\\alpha}}\\right)^{\\frac{1-\\alpha}{\\alpha}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "So we can know that: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left(\\frac{1}{\\sqrt{n}}\\sum_{t=1}^{T}\\left\\|\\mathbf{v}_{t}\\right\\|^{2}\\right)^{1-\\alpha}\\right]\\leq4C_{3}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which indicate that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left(\\frac{1}{T}\\sum_{t=1}^{T}\\left\\lVert\\mathbf{v}_{t}\\right\\rVert^{2}\\right)^{1-\\alpha}\\right]\\leq4C_{3}\\left(\\frac{\\sqrt{n}}{T}\\right)^{1-\\alpha}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "as well as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\mathbf{v}_{t}\\|\\right]\\leq(4C_{3})^{\\frac{1}{2(1-\\alpha)}}\\cdot\\frac{n^{1/4}}{T^{1/2}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "To finish the proof, we also have to show the following lemma. ", "page_idx": 26}, {"type": "text", "text": "Lemma 8 ", "text_level": 1, "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\|\\nabla F(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\|^{2}\\right]}\\\\ &{\\leq\\!\\frac{\\alpha\\sqrt{n}}{1-\\alpha}\\left(\\frac{14L^{2}}{1-2\\alpha}\\left(\\frac{2-4\\alpha}{(1-\\alpha)L}\\right)^{\\frac{1-2\\alpha}{1-\\alpha}}\\right)^{\\frac{1-\\alpha}{\\alpha}}+\\frac{n^{\\frac{\\alpha}{2}}L}{2}\\mathbb{E}\\left[\\left(\\displaystyle\\sum_{t=1}^{T}\\|\\mathbf{v}_{t}\\|^{2}\\right)^{1-\\alpha}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof 8 According to the previous proof, we know that: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\left[\\left\\Vert\\nabla F(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\right\\Vert^{2}\\right]}\\\\ &{\\displaystyle\\leq2\\beta\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\left[\\left\\Vert\\nabla F(\\mathbf{x}_{t+1})-\\mathbf{z}_{t+1}\\right\\Vert^{2}\\right]+2L^{2}\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\left[\\frac{\\eta_{t}^{2}}{\\beta}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right]\\leq14n L^{2}\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\left[\\eta_{t}^{2}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Also, we can deduce that: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{14n L^{2}\\displaystyle\\sum_{t=1}^{T}\\eta_{t}^{2}\\left\\lVert\\mathbf{v}_{t}\\right\\rVert^{2}=14L^{2}n^{\\alpha}\\displaystyle\\sum_{t=1}^{T}\\frac{\\left\\lVert\\mathbf{v}_{t}\\right\\rVert^{2}}{\\left(\\sum_{i=1}^{t}\\left\\lVert\\mathbf{v}_{i}\\right\\rVert^{2}\\right)^{2\\alpha}}}\\\\ &{\\phantom{2p c}\\leq\\frac{14L^{2}n^{\\alpha}}{1-2\\alpha}\\left(\\displaystyle\\sum_{t=1}^{T}\\left\\lVert\\mathbf{v}_{t}\\right\\rVert^{2}\\right)^{1-2\\alpha}}\\\\ &{=\\frac{14L^{2}n^{\\alpha}}{1-2\\alpha}\\left(\\displaystyle\\frac{2-4\\alpha}{(1-\\alpha)n^{\\frac{1}{2}}L}\\right)^{\\frac{1-2\\alpha}{(1-\\alpha)}}\\left(\\displaystyle\\frac{(1-\\alpha)n^{\\frac{9}{2}}L}{2-4\\alpha}\\right)^{\\frac{1-2\\alpha}{1-\\alpha}}\\left(\\displaystyle\\sum_{t=1}^{T}\\left\\lVert\\mathbf{v}_{t}\\right\\rVert^{2}\\right)^{1-2\\alpha}}\\\\ &{\\leq\\frac{\\alpha}{1-\\alpha}\\left(\\frac{14L^{2}n^{\\alpha}}{1-2\\alpha}\\left(\\displaystyle\\frac{2-4\\alpha}{(1-\\alpha)n^{\\frac{9}{2}}L}\\right)^{\\frac{1-2\\alpha}{1-\\alpha}}\\right)^{\\frac{1-\\alpha}{\\alpha}}+\\frac{n^{\\frac{\\alpha}{2}}L}{2}\\left(\\displaystyle\\sum_{t=1}^{T}\\left\\lVert\\mathbf{v}_{t}\\right\\rVert^{2}\\right)^{1-\\alpha}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "wheq $\\begin{array}{r}{x y\\leq\\frac{\\alpha}{1-\\alpha}x^{\\frac{1-\\alpha}{\\alpha}}+\\frac{1-2\\alpha}{1-\\alpha}y^{\\frac{1-\\alpha}{1-2\\alpha}}}\\end{array}$ for positive ", "page_idx": 26}, {"type": "text", "text": "As a result, we can ensure that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E}\\left[\\left\\lVert\\nabla F(\\mathbf{x}_{t})\\right\\rVert\\right]\\le\\displaystyle\\frac{1}{T}\\mathbb{E}\\left[\\sum_{t=1}^{T}\\left\\lVert\\mathbf{v}_{t}\\right\\rVert\\right]+\\displaystyle\\frac{1}{T}\\left[\\sum_{t=1}^{T}\\left\\lVert\\nabla f(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\right\\rVert\\right]}&{}\\\\ &{\\displaystyle\\le\\frac{n^{1/4}}{T^{1/2}}\\left(\\left(\\frac{14L^{2}}{1-2\\alpha}\\left(\\frac{2-4\\alpha}{(1-\\alpha)L}\\right)^{\\frac{1-2\\alpha}{1-\\alpha}}\\right)^{\\frac{1-\\alpha}{2\\alpha}}+\\sqrt{2C_{3}L}+(4C_{3})^{\\frac{1}{2(1-\\alpha)}}\\right)}\\\\ &{\\displaystyle=\\mathcal{O}\\left(\\left(\\Delta_{F}^{\\frac{1}{2(1-\\alpha)}}+L^{\\frac{1}{2\\alpha}}\\right)\\frac{n^{1/4}}{T^{1/2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "E Proof of Theorem 5 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The analysis is very similar to that of Theorem 4, and the difference only appears in Lemma 5. For this new method, we can also prove the same lemma: ", "page_idx": 27}, {"type": "text", "text": "Lemma 9 ", "text_level": 1, "page_idx": 27}, {"type": "equation", "text": "$$\n2\\beta\\sum_{t=1}^{T}\\mathbb{E}\\left[\\eta_{t}\\left\\Vert\\nabla F(\\mathbf{x}_{t+1})-\\mathbf{z}_{t+1}\\right\\Vert^{2}\\right]\\leq12L^{2}\\sum_{t=1}^{T}\\mathbb{E}\\left[\\frac{\\eta_{t}^{3}}{\\beta}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}\\right].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof 9 This time, we have $\\begin{array}{r}{\\mathbf{z}_{t+1}=\\nabla f_{i_{t+1}}(\\mathbf{x}_{t+1})-\\nabla f_{i_{t+1}}(\\mathbf{x}_{\\tau})+\\nabla F(\\mathbf{x}_{\\tau}).}\\end{array}$ .And we can know that: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{i_{t+1}}\\left[\\eta_{t}\\left\\|\\nabla F(\\mathbf{x}_{t+1})-\\mathbf{z}_{t+1}\\right\\|^{2}\\right]}\\\\ &{=\\!\\mathbb{E}_{i_{t+1}}\\left[\\eta_{t}\\left\\|\\nabla F(\\mathbf{x}_{t+1})-\\nabla f_{i_{t+1}}(\\mathbf{x}_{t+1})+\\nabla f_{i_{t+1}}(\\mathbf{x}_{\\tau})-\\nabla F(\\mathbf{x}_{\\tau})\\right\\|^{2}\\right]}\\\\ &{\\le\\!\\mathbb{E}_{i_{t+1}}\\left[\\eta_{t}\\left\\|\\nabla f_{i_{t+1}}(\\mathbf{x}_{t+1})-f_{i_{t+1}}(\\mathbf{x}_{\\tau})\\right\\|^{2}\\right]}\\\\ &{\\le\\!\\eta_{t}L^{2}\\left\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{\\tau}\\right\\|^{2}}\\\\ &{\\le\\!\\eta_{t}L^{2}I\\displaystyle\\sum_{i=\\tau}^{t}\\left\\|\\mathbf{x}_{i+1}-\\mathbf{x}_{i}\\right\\|^{2}}\\\\ &{\\le\\!\\eta_{t}L^{2}I\\displaystyle\\sum_{i=\\tau}^{t}\\eta_{i}^{2}\\|\\mathbf{y}_{i}\\|^{2}\\le L^{2}I\\displaystyle\\sum_{i=\\tau}^{t}\\eta_{i}^{3}\\left\\|\\mathbf{v}_{i}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By summing up, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle2\\beta\\sum_{t=1}^{T}\\mathbb{E}\\left[\\eta_{t}\\left\\|\\nabla F(\\mathbf{x}_{t+1})-\\mathbf{z}_{t+1}\\right\\|^{2}\\right]}\\\\ {\\displaystyle\\leq2\\beta\\mathbb{E}\\left[\\sum_{t=1}^{T}L^{2}I\\sum_{i=\\tau}^{t}\\eta_{i}^{3}\\left\\|\\mathbf{v}_{i}\\right\\|^{2}\\right]\\leq2\\beta L^{2}I^{2}\\mathbb{E}\\left[\\sum_{t=1}^{T}\\eta_{t}^{3}\\left\\|\\mathbf{v}_{t}\\right\\|^{2}\\right]\\leq2L^{2}\\mathbb{E}\\left[\\sum_{t=1}^{T}\\frac{\\eta_{t}^{3}}{\\beta}\\left\\|\\mathbf{v}_{t}\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The other analysis is exactly the same as that of Theorem 4. ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The claims presented in the abstract and introduction accurately represent the contributions and scope of the paper. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: The theoretical results demonstrated in the paper rely on specific assumptions, which have been clearly stated in the main text. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper provides assumptions and proofs for each theoretical result. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper discloses the information necessary to reproduce the main experimental results. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [No] ", "page_idx": 30}, {"type": "text", "text": "Justification: Due to privacy concerns and ongoing research, we do not include the code. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper describes the training and testing details ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The paper reports error bars. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have provided the relevant information. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The research conducted in the paper conforms with the NeurIPS Code of Ethics. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This is primarily a theoretical paper with no potential negative social impact. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The creators or original owners of assets used in the paper are properly credited and the license and terms of use explicitly are properly respected. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 32}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]