[{"figure_path": "LnJ2EGKTXh/tables/tables_2_1.jpg", "caption": "Table 1: Pass@1 results of different LLMs on ROBOEVAL computed with greedy decoding T = 0 and nucleus sampling T = 0.2.", "description": "This table presents the pass@1 scores achieved by various Large Language Models (LLMs) on the ROBOEVAL benchmark.  Pass@1 represents the percentage of tasks where the LLM generated the correct program on its first attempt. The evaluation is performed using two different decoding methods: greedy decoding (T=0) and nucleus sampling (T=0.2). The models are categorized as either proprietary (GPT-4, GPT-3.5, Gemini) or open-source (Codellama, Starcoder, Deepseek-Coder, Llama). The table also shows the number of parameters for each model and its licensing status.", "section": "3.2 RQ1: Is ROBO-INSTRUCT Effective at Generating Training Data to Fine-Tune a Small Language Model for Generating Domain-Specific Robot Programs?"}, {"figure_path": "LnJ2EGKTXh/tables/tables_3_1.jpg", "caption": "Table 1: Pass@1 results of different LLMs on ROBOEVAL computed with greedy decoding T = 0 and nucleus sampling T = 0.2.", "description": "This table presents the performance comparison of various Large Language Models (LLMs) on the ROBOEVAL benchmark.  The pass@1 metric represents the percentage of times the LLM generates a correct robot program on the first attempt.  The models are evaluated under two decoding strategies: greedy decoding (T=0) and nucleus sampling (T=0.2). The table shows the number of parameters for each LLM and whether it is proprietary or open-source. This allows for a comparison of performance between different LLMs, considering their size and licensing.", "section": "3.2 RQ1: Is ROBO-INSTRUCT Effective at Generating Training Data to Fine-Tune a Small Language Model for Generating Domain-Specific Robot Programs?"}, {"figure_path": "LnJ2EGKTXh/tables/tables_6_1.jpg", "caption": "Table 1: Pass@1 results of different LLMs on ROBOEVAL computed with greedy decoding T = 0 and nucleus sampling T = 0.2.", "description": "This table presents the performance comparison of various Large Language Models (LLMs) on the ROBOEVAL benchmark, which evaluates code generation for robots.  The comparison is done using two different decoding methods: greedy decoding (T=0) and nucleus sampling (T=0.2).  The table shows the Pass@1 score (the percentage of times the top-ranked generated program is correct), the number of parameters in each LLM, and whether the LLM is proprietary or open-source.  It includes results for both proprietary models (GPT-4, GPT-3.5, Gemini-1.0-Pro) and several open-source models (Codellama-Python, Starcoder2, Deepseek-Coder, Llama-3-Inst).  Results for models fine-tuned using Self-Instruct and Robo-Instruct are also provided for comparison.", "section": "3.2 RQ1: Is ROBO-INSTRUCT Effective at Generating Training Data to Fine-Tune a Small Language Model for Generating Domain-Specific Robot Programs?"}, {"figure_path": "LnJ2EGKTXh/tables/tables_6_2.jpg", "caption": "Table 2: Pass@1 results of different LLMs on ROBOEVAL computed with greedy decoding T = 0 and nucleus sampling T = 0.2.", "description": "This table presents the performance comparison of different LLMs on the ROBOEVAL benchmark.  It shows the Pass@1 scores (the percentage of tasks where the top-ranked program is correct) for various models, including a baseline Codellama model and variations incorporating different components of the ROBO-INSTRUCT framework (ROBOSIM and INSTALIGN).  The impact of each component is shown through improvement percentages.  Results are given for both greedy decoding (T=0) and nucleus sampling (T=0.2), showcasing the effectiveness of the framework under different generation strategies. The percentage of invalid programs generated by each method is also reported, highlighting how ROBO-INSTRUCT reduces this number.", "section": "3.3 RQ2: How Do ROBOSIM and InstAlign Impact the Effectiveness of ROBO-INSTRUCT?"}, {"figure_path": "LnJ2EGKTXh/tables/tables_14_1.jpg", "caption": "Table 3: Dataset Statistics", "description": "This table presents a quantitative comparison of the datasets generated by ROBO-INSTRUCT and SELF-INSTRUCT.  The metrics shown are dataset size, n-gram diversity (using 4-grams), the number of unique synthesized locations, and the number of unique synthesized objects. These metrics offer insight into the diversity and quality of the training data produced by each method.", "section": "3.1 ROBOEVAL: A Domain-Specific Robot Code Generation Benchmark"}]