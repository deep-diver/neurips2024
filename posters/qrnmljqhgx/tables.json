[{"figure_path": "qRnmLJQHgx/tables/tables_7_1.jpg", "caption": "Table 1: Out-of-the-box (zero-shot) performance. We show the performance for a common subset of tasks: surface normals and depth estimation on DIODE [90], semantic and instance segmentation on COCO [57], k-NN retrieval on ImageNet-1K [79], and 3D human keypoint estimation on 3DPW [91]. We compare to a set of strong baselines and specialist models, including our pseudo labelers. The model learned to solve all the tasks without a loss of performance, is significantly better than the baselines, and is competitive with pseudo labelers, while being a single model for all tasks. Compared to 4M-7, the 4M-21 model preserved its performance while solving 3x more tasks. X denotes that a given model cannot solve the task out-of-the-box. * shows the tokenizer reconstruction quality and provides an estimate on the performance upper bound due to tokenization. See fig. 14 for qualitative comparisons. Best results are bolded, second best underlined.", "description": "This table presents a comparison of the performance of the 4M-21 model and several other models on four common vision tasks.  The tasks are surface normal and depth estimation, semantic and instance segmentation, k-NN retrieval, and 3D human pose estimation.  The comparison includes pseudo-labelers, strong baselines, and other multi-task models.  The results demonstrate that 4M-21 performs competitively with, or even surpasses, specialized models on the tasks and notably maintains its performance while solving three times more tasks compared to the 4M-7 model.", "section": "3 Evaluating out-of-the-box capabilities"}, {"figure_path": "qRnmLJQHgx/tables/tables_8_1.jpg", "caption": "Table 2: Unimodal transfer study. We transfer 4M-21 and baselines to ImageNet-1K [79] classification, ADE20K [106] semantic segmentation, NYUv2 [84] depth estimation, and ARKitScenes [9] (ARKS) 3D object detection. We observe that 4M-21 1) does not lose performance for the transfer tasks that are similar to the seven modalities of 4M, i.e. first three columns of the results, while being able to solve many more, and 2) leads to improved performance for novel tasks that are more different from 4M modalities, e.g. 3D object detection (last column). The improvements are further verified in the multimodal transfer results (Table 3) showing the usefulness of new modalities. Best results per task are bolded, second best underlined.", "description": "This table presents the results of a unimodal transfer learning experiment.  The authors transferred the encoders of various models (including their proposed 4M-21 model and several baselines) to four downstream tasks: ImageNet-1K classification, ADE20K semantic segmentation, NYUv2 depth estimation, and ARKitScenes 3D object detection. The table shows that 4M-21 maintains performance on tasks similar to those in its pre-training while improving on novel tasks, highlighting the benefits of training on a wider range of modalities.  The table includes pre-training data, encoder parameters, and performance metrics for each task.", "section": "4 Transfer experiments"}, {"figure_path": "qRnmLJQHgx/tables/tables_8_2.jpg", "caption": "Table 3: Multimodal transfer study. We transfer both 4M-21 and 4M (pre-trained on CC12M) to NYUv2 and Hypersim segmentation, and 3D object detection on ARKitScenes. All models are able to use optionally available depth when it is of high quality (Hypersim & ARKitScenes), while our model achieves the best results. Best results are bolded, second best underlined.", "description": "This table compares the performance of 4M-21 and 4M models on three multimodal transfer tasks: NYUv2 semantic segmentation, Hypersim semantic segmentation, and ARKitScenes 3D object detection.  The results show how the models perform using only RGB images as input, and how performance changes when depth information is also available.  4M-21 consistently outperforms 4M across all tasks and model sizes. The use of depth data is shown to improve results for the Hypersim and ARKitScenes tasks.", "section": "Transfer experiments"}, {"figure_path": "qRnmLJQHgx/tables/tables_22_1.jpg", "caption": "Table 4: Pre-training data and modality mixture ablation: We ablate different pre-training modality and dataset choices on B models. * represents the models initialized from the corresponding 4M models trained on COYO700M.", "description": "This table presents ablation studies on different pre-training data and modality combinations for the 4M-21 model (size B). It compares the performance of models trained with various combinations of CC12M, COYO700M, and C4 datasets, highlighting the impact of dataset diversity and modality mix on downstream transfer tasks.", "section": "C Additional Ablations"}, {"figure_path": "qRnmLJQHgx/tables/tables_23_1.jpg", "caption": "Table 5: Ensembling ablation: We ablate ensembling multiple predictions on DIODE normals and COCO semantic segmentation compared to no ensembling. As the results suggest, ensembling in all cases improves the quantitative results.", "description": "This table presents the ablation study on ensembling multiple predictions for improving the performance. The results demonstrate that ensembling multiple predictions for surface normal estimation on DIODE and semantic segmentation on COCO datasets improves the quantitative results (mean angle error and mean IoU).", "section": "C Additional Ablations"}, {"figure_path": "qRnmLJQHgx/tables/tables_28_1.jpg", "caption": "Table 1: Out-of-the-box (zero-shot) performance. We show the performance for a common subset of tasks: surface normals and depth estimation on DIODE [90], semantic and instance segmentation on COCO [57], k-NN retrieval on ImageNet-1K [79], and 3D human keypoint estimation on 3DPW [91]. We compare to a set of strong baselines and specialist models, including our pseudo labelers. The model learned to solve all the tasks without a loss of performance, is significantly better than the baselines, and is competitive with pseudo labelers, while being a single model for all tasks. Compared to 4M-7, the 4M-21 model preserved its performance while solving 3x more tasks. X denotes that a given model cannot solve the task out-of-the-box. * shows the tokenizer reconstruction quality and provides an estimate on the performance upper bound due to tokenization. See fig. 14 for qualitative comparisons. Best results are bolded, second best underlined.", "description": "This table presents a quantitative comparison of the 4M-21 model's zero-shot performance on several common vision tasks against various strong baselines and specialist models.  It highlights the model's ability to perform well on multiple tasks simultaneously without performance degradation, outperforming baselines and matching the performance of specialized models.  The results are compared using standard metrics for each task and consider tokenizer limitations. ", "section": "Evaluating out-of-the-box capabilities"}, {"figure_path": "qRnmLJQHgx/tables/tables_28_2.jpg", "caption": "Table 1: Out-of-the-box (zero-shot) performance. We show the performance for a common subset of tasks: surface normals and depth estimation on DIODE [90], semantic and instance segmentation on COCO [57], k-NN retrieval on ImageNet-1K [79], and 3D human keypoint estimation on 3DPW [91]. We compare to a set of strong baselines and specialist models, including our pseudo labelers. The model learned to solve all the tasks without a loss of performance, is significantly better than the baselines, and is competitive with pseudo labelers, while being a single model for all tasks. Compared to 4M-7, the 4M-21 model preserved its performance while solving 3x more tasks. X denotes that a given model cannot solve the task out-of-the-box. * shows the tokenizer reconstruction quality and provides an estimate on the performance upper bound due to tokenization. See fig. 14 for qualitative comparisons. Best results are bolded, second best underlined.", "description": "This table presents a comparison of the performance of 4M-21 and several other models on four different vision tasks.  The tasks are surface normal estimation, depth estimation, semantic segmentation, and 3D human keypoint estimation.  The table shows that 4M-21 achieves comparable or better results than specialized models for each task, demonstrating its strong zero-shot capabilities.  The table also includes the performance of pseudo-labelers, which are used to train the models, and several multi-task baselines.", "section": "3 Evaluating out-of-the-box capabilities"}, {"figure_path": "qRnmLJQHgx/tables/tables_29_1.jpg", "caption": "Table 1: Out-of-the-box (zero-shot) performance. We show the performance for a common subset of tasks: surface normals and depth estimation on DIODE [90], semantic and instance segmentation on COCO [57], k-NN retrieval on ImageNet-1K [79], and 3D human keypoint estimation on 3DPW [91]. We compare to a set of strong baselines and specialist models, including our pseudo labelers. The model learned to solve all the tasks without a loss of performance, is significantly better than the baselines, and is competitive with pseudo labelers, while being a single model for all tasks. Compared to 4M-7, the 4M-21 model preserved its performance while solving 3x more tasks. X denotes that a given model cannot solve the task out-of-the-box. * shows the tokenizer reconstruction quality and provides an estimate on the performance upper bound due to tokenization. See fig. 14 for qualitative comparisons. Best results are bolded, second best underlined.", "description": "This table presents a comparison of the performance of the 4M-21 model against several strong baselines and specialist models on four common vision tasks: surface normal and depth estimation, semantic and instance segmentation, k-NN retrieval, and 3D human keypoint estimation.  The results demonstrate that 4M-21 achieves strong out-of-the-box performance, matching or exceeding specialist models while being a single model for all tasks.  The table highlights the benefits of training a single, any-to-any model on a large number of modalities without any loss in performance compared to specialized single/few task models.", "section": "Evaluating out-of-the-box capabilities"}, {"figure_path": "qRnmLJQHgx/tables/tables_29_2.jpg", "caption": "Table 1: Out-of-the-box (zero-shot) performance. We show the performance for a common subset of tasks: surface normals and depth estimation on DIODE [90], semantic and instance segmentation on COCO [57], k-NN retrieval on ImageNet-1K [79], and 3D human keypoint estimation on 3DPW [91]. We compare to a set of strong baselines and specialist models, including our pseudo labelers. The model learned to solve all the tasks without a loss of performance, is significantly better than the baselines, and is competitive with pseudo labelers, while being a single model for all tasks. Compared to 4M-7, the 4M-21 model preserved its performance while solving 3x more tasks. X denotes that a given model cannot solve the task out-of-the-box. * shows the tokenizer reconstruction quality and provides an estimate on the performance upper bound due to tokenization. See fig. 14 for qualitative comparisons. Best results are bolded, second best underlined.", "description": "This table presents a comparison of the performance of the 4M-21 model against several strong baselines and specialized models on four common computer vision tasks. The tasks evaluated are surface normal and depth estimation, semantic and instance segmentation, k-NN retrieval, and 3D human keypoint estimation.  The results demonstrate that the 4M-21 model performs competitively with or better than specialized models on all tasks, while being a single model capable of handling all of them. This showcases the model's strong out-of-the-box capabilities, especially when compared to its predecessor (4M-7) which could only solve a third of the number of tasks. The table also includes an analysis of the upper bound of performance, considering the limitations of tokenization.", "section": "3 Evaluating out-of-the-box capabilities"}, {"figure_path": "qRnmLJQHgx/tables/tables_30_1.jpg", "caption": "Table 1: Out-of-the-box (zero-shot) performance. We show the performance for a common subset of tasks: surface normals and depth estimation on DIODE [90], semantic and instance segmentation on COCO [57], k-NN retrieval on ImageNet-1K [79], and 3D human keypoint estimation on 3DPW [91]. We compare to a set of strong baselines and specialist models, including our pseudo labelers. The model learned to solve all the tasks without a loss of performance, is significantly better than the baselines, and is competitive with pseudo labelers, while being a single model for all tasks. Compared to 4M-7, the 4M-21 model preserved its performance while solving 3x more tasks. X denotes that a given model cannot solve the task out-of-the-box. * shows the tokenizer reconstruction quality and provides an estimate on the performance upper bound due to tokenization. See fig. 14 for qualitative comparisons. Best results are bolded, second best underlined.", "description": "This table presents the zero-shot performance of 4M-21 on four vision tasks (surface normal and depth estimation, semantic and instance segmentation, k-NN retrieval, 3D human pose estimation).  It compares the model against strong baselines and specialist models, highlighting its ability to perform these tasks without specialized training. The results demonstrate comparable or superior performance to specialized models, even with 3 times more tasks than the comparable 4M-7 model.", "section": "Evaluating out-of-the-box capabilities"}, {"figure_path": "qRnmLJQHgx/tables/tables_32_1.jpg", "caption": "Table 1: Out-of-the-box (zero-shot) performance. We show the performance for a common subset of tasks: surface normals and depth estimation on DIODE [90], semantic and instance segmentation on COCO [57], k-NN retrieval on ImageNet-1K [79], and 3D human keypoint estimation on 3DPW [91]. We compare to a set of strong baselines and specialist models, including our pseudo labelers. The model learned to solve all the tasks without a loss of performance, is significantly better than the baselines, and is competitive with pseudo labelers, while being a single model for all tasks. Compared to 4M-7, the 4M-21 model preserved its performance while solving 3x more tasks. X denotes that a given model cannot solve the task out-of-the-box. * shows the tokenizer reconstruction quality and provides an estimate on the performance upper bound due to tokenization. See fig. 14 for qualitative comparisons. Best results are bolded, second best underlined.", "description": "This table presents a comparison of the 4M-21 model's zero-shot performance on several vision tasks against various baselines and specialized models.  It demonstrates the model's ability to perform multiple tasks simultaneously without significant performance loss compared to single-task models.  The table highlights the model's improvement over previous versions (4M-7) by solving three times more tasks without performance degradation.", "section": "Evaluating out-of-the-box capabilities"}, {"figure_path": "qRnmLJQHgx/tables/tables_33_1.jpg", "caption": "Table 1: Out-of-the-box (zero-shot) performance. We show the performance for a common subset of tasks: surface normals and depth estimation on DIODE [90], semantic and instance segmentation on COCO [57], k-NN retrieval on ImageNet-1K [79], and 3D human keypoint estimation on 3DPW [91]. We compare to a set of strong baselines and specialist models, including our pseudo labelers. The model learned to solve all the tasks without a loss of performance, is significantly better than the baselines, and is competitive with pseudo labelers, while being a single model for all tasks. Compared to 4M-7, the 4M-21 model preserved its performance while solving 3x more tasks. X denotes that a given model cannot solve the task out-of-the-box. * shows the tokenizer reconstruction quality and provides an estimate on the performance upper bound due to tokenization. See fig. 14 for qualitative comparisons. Best results are bolded, second best underlined.", "description": "This table presents a comparison of the 4M-21 model's zero-shot performance against various baselines and specialized models across four common vision tasks: surface normal and depth estimation, semantic and instance segmentation, k-NN retrieval, and 3D human keypoint estimation.  It demonstrates that 4M-21 achieves competitive performance with specialized models while being a single model capable of handling all tasks, and shows significantly improved performance over existing multitask models.", "section": "3.3 Evaluating out-of-the-box capabilities"}]