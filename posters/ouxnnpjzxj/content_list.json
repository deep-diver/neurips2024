[{"type": "text", "text": "Perplexity-aware Correction for Robust Alignment with Noisy Preferences ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Keyi Kong1,\u2217 Xilie $\\mathbf{X}\\mathbf{u}^{2,*}$ Di Wang3 Jingfeng Zhang4,5,\u2020 Mohan Kankanhalli2 ", "page_idx": 0}, {"type": "text", "text": "1Shandong University 2National University of Singapore 3King Abdullah University of Science and Technology 4The University of Auckland 5RIKEN Center for Advanced Intelligence Project (AIP) luxinyayaya@mail.sdu.edu.cn, xuxilie@comp.nus.edu.sg di.wang@kaust.edu.sa, jingfeng.zhang@auckland.ac.nz, mohan@comp.nus.edu.sg ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Alignment techniques are critical in ensuring that large language models (LLMs) output helpful and harmless content by enforcing the LLM-generated content to align with human preferences. However, the existence of noisy preferences (NPs), where the responses are mistakenly labelled as chosen or rejected, could spoil the alignment, thus making the LLMs generate useless and even malicious content. Existing methods mitigate the issue of NPs from the loss perspective by adjusting the alignment loss based on a clean validation dataset. Orthogonal to these lossoriented methods, we propose perplexity-aware correction (PerpCorrect) from the data perspective for robust alignment which detects and corrects NPs based on the differences between the perplexity of the chosen and rejected responses (dubbed as PPLDiff). Intuitively, a higher PPLDiff indicates a higher probability of the NP because a rejected/chosen response which is mistakenly labelled as chosen/rejected is less preferable to be generated by an aligned LLM, thus having a higher/lower perplexity. PerpCorrect works in three steps: (1) PerpCorrect aligns a surrogate LLM using the clean validation data to make the PPLDiff able to distinguish clean preferences (CPs) and NPs. (2) PerpCorrect further aligns the surrogate LLM by incorporating the reliable clean training data whose PPLDiff is extremely small and reliable noisy training data whose PPLDiff is extremely large after correction to boost the discriminatory power. (3) Detecting and correcting NPs according to the PPLDiff obtained by the aligned surrogate LLM to obtain a denoised training dataset for robust alignment. Comprehensive experiments validate that our proposed PerpCorrect can achieve state-of-the-art alignment performance under NPs. Notably, PerpCorrect demonstrates practical utility by requiring only a modest amount of validation data and being compatible with various alignment techniques. Our code is available at PerpCorrect. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Alignment enables the safe utilization of the remarkable capabilities acquired by large language models (LLMs) through self-supervised learning on vast corpora [6, 24, 4]. It refers to the process of ensuring that the contents generated by LLMs are helpful, harmless, and aligned with human values and preferences [19]. Reinforcement Learning from Human Feedback (RLHF) [9] has emerged as a primary technique for achieving alignment. Current technical routes [39, 40, 29] require a reward model to simulate human preference and use it to optimize the policy model outputs with ", "page_idx": 0}, {"type": "image", "img_path": "OUXnnPJzXJ/tmp/530166498171d47e89b4e93c8f5db1d9633bac120f9dcd478fac537df483e5ca.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "", "img_caption": ["Figure 1: We evaluated various robust alignment methods under different proportions of noisy preferences using the Llama2-7B model, on the Golden HH dataset. The reward accuracy of both the vanilla DPO and PPO method significantly decreases as the proportion of noisy preferences increases. Our method, perplexity-aware correction (PerpCorrect), outperforms both the DPO and PPO series baselines across different proportions of noisy preferences. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Proximal Policy Optimization (PPO) [27]. Current offline techniques such as Direct Preference Optimisation (DPO) [26], Sequence Likelihood Calibration with Human Feedback (SLiC) [38] and Identity-Preference Optimisation (IPO) [3], could directly align LLMs without intensively training a reward model as employed in RLHF. ", "page_idx": 1}, {"type": "text", "text": "Recent studies [34, 8] have shown there exist noisy preferences (NPs) that may lead to significant degradation in alignment performance. The issue of NPs, where the label of the actually chosen/rejected responses in training datasets is flipped as rejected/chosen, can arise from the biases of annotators [34] andt h e malicious noise injection [5]. As shown in Figure 1, when NPs are randomly injected into the training dataset, the conventional alignment method (e.g., DPO [26] and PPO [9]) will have significantly degraded alignment performance measured by the reward accuracy. Such performance degradation could result in the generation of useless and even malicious content [34]. Therefore, it necessitates developing robust alignment methods that can utilize datasets with NPs to effectively align the LLMs with human preferences. ", "page_idx": 1}, {"type": "text", "text": "Existing robust alignment methods are proposed from the loss perspective, which adjust the alignment loss using a clean validation dataset to mitigate the issue of NPs. In particular, the conservative DPO (cDPO) [21] and robust-DPO (rDPO) [8] both estimate the proportion of NPs using the clean validation data via cross-validation and then adjust the original DPO loss based on the estimated proportion of NPs. However, Mitchell [21] and Chowdhury et al. [8] overlooked the essential differences between noisy and clean preferences, which is critical for mitigating the issue of NPs. ", "page_idx": 1}, {"type": "text", "text": "To this end, we propose Perplexity-aware Correction (PerpCorrect) for robust alignment from the data perspective by leveraging the differences between noisy and clean preferences for robust alignment. PerpCorrect detects and corrects NPs based on the difference between the perplexity of the chosen response and that of the rejected counterparts (dubbed as PPLDiff) obtained by an aligned surrogate LLM using the clean validation set. If an NP is detected, PerpCorrect will correct it by filpping the label of the rejected/chosen responses as chosen/rejected. Intuitively, rejected responses which are mistakenly labelled as chosen have a higher perplexity since they are less consistent with human preferences and thus have a lower probability of being generated after alignment. Therefore, a higher value of PPLDiff indicates a higher probability of the preferences being noisy. In this way, PerpCorrect leverages the differences between noisy and clean preferences (CPs) identified by PPLDiff to detect NPs. ", "page_idx": 1}, {"type": "text", "text": "To make the PPLDiff able to distinguish CPs and NPs, PerpCorrect requires an aligned surrogate LLM for calculating PPLDiff. The density of PPLDiff obtained on the noisy training dataset using an unaligned surrogate LLM, which can be fitted as a normal distribution centered around zero (evidenced in Figure 2a), cannot discriminate CPs and NPs. Therefore, we align a surrogate LLM using the clean validation data. The density of PPLDiff obtained by the aligned surrogate LLM in Figure 2b can be ftited into two distinguishable normal distributions, thus being able to differentiate CPs and NPs. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "However, there still exists a large overlap between two normal distributions after aligning only on the clean validation dataset, which could result in an unsatisfactory accuracy of NP detection. To this end, we iteratively align the model using more reliable clean training data with extremely low PPLDiff (located in the green area in Figure 2c) and reliable noisy training data with extremely large PPLDiff (located in the red area in Figure 2c) sampled from noisy training datasets. Finally, the two normal distributions are significantly separated as shown in Figure 2d, which indicates that PPLDiff has an enhanced discriminatory power. ", "page_idx": 2}, {"type": "text", "text": "Beneftiing from the strong discriminatory power of PPLDiff calculated by the aligned surrogate LLM, PerpCorrect outputs a denoised training dataset for robust alignment by first detecting NPs based on a PPLDiff threshold and then correcting them. The data, whose PPLDiff is below a certain threshold (i.e., the black dotted line in Figure 2d) selected as the $\\mathbf{X}$ -coordinate of the two normal distributions\u2019 intersection, are identified as NPs and thus corrected by flipping the response\u2019s label. Notably, our proposed PerpCorrect is compatible with various alignment methods as well as robust alignment methods [21, 8] since the metric PPLDiff is agnostic to training algorithms and only requires an arguably small number of clean validation data $(\\mathord{\\sim}50)$ , thus making it practical. ", "page_idx": 2}, {"type": "text", "text": "Comprehensive empirical results, evaluated using the Llama2-7B [32] and phi-2 [20] models on the OpenAssistant Conversations (OASST1) [17] and Golden HH [7] datasets, validate the effectiveness of our proposed PerpCorrect method in robustifying alignment with NPs. We empirically validate that PerpCorrect consistently obtains state-of-the-art performance among various proportions of NPs. Besides, we empirically demonstrate that PerpCorrect can effectively robustify various alignment techniques and robust alignment methods, validating its compatibility. ", "page_idx": 2}, {"type": "text", "text": "2 Literature Review and Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we introduce the related work about LLM alignment and provide preliminaries about the noisy preferences, perplexity, as well as various alignment methods. ", "page_idx": 2}, {"type": "text", "text": "2.1 LLM Alignment ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In the domain of aligning LLMs with human preferences, pairwise preference methods are favored due to their lower cognitive burden on evaluators. Traditional online alignment approaches [32, 24, 29] involve training reward models from these preferences to provide signals in reinforcement learning. Recent offilne alignment methods like Direct Preference Optimization (DPO) [26], Sequence Likelihood Calibration (SLiC) [38], and Identify Preference Optimization (IPO) [3] streamlined this process by directly using preference pairs to train LLMs, thus enhancing performance and reducing computational costs. Additionally, methods like RRHF [37] align LLMs using multiple ranked preferences, Kahneman-Tversky Optimization (KTO) [13] align LLMs using a single preference labeled as good or bad, and Rejection Sampling Optimization (RSO) [18] address DPO\u2019s limitation in sampling preference pairs from the optimal policy through rejection sampling. However, NPs, arising from the biased human feedback, can determine the alignment performance [24, 34]. Robust alignment methods like conservative DPO (cDPO) [21], robust DPO (rDPO) [8] have been proposed to address these issues from the loss perspective. Our approach focuses on the data perspective to address these issues of NPs and is orthogonal to these robust alignment methods. ", "page_idx": 2}, {"type": "text", "text": "2.2 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Noisy preferences (NPs). NPs refer to preference data in training datasets, whose label of the actually chosen/rejected responses is flipped as rejected/chosen. Let D = {(x(i), y(wi ), yl(i ))}i be the preference dataset consisting of $N\\in\\mathbb{N}$ preference data points. For each preference data point $(x,\\bar{y}_{w},y_{l})\\in\\mathcal{D}$ , $x$ is the prompt input to LLMs, $y_{w}$ is the chosen response, and $y_{l}$ is the rejected response. We let $\\tilde{\\mathcal{D}}=\\{(x^{(i)},\\tilde{y}_{w}^{(i)},\\tilde{y}_{l}^{(i)})\\}_{i=1}^{N}$ be the noisy preference dataset (i.e., preference dataset ", "page_idx": 2}, {"type": "image", "img_path": "OUXnnPJzXJ/tmp/a08986f8b16a3a6dd4f63458fa9c18c771818f78e6d70ec79e6543360cee99a8.jpg", "img_caption": ["(a) Noisy and clean preferences cannot be distinguished by PPLDiff. "], "img_footnote": [], "page_idx": 3}, {"type": "image", "img_path": "OUXnnPJzXJ/tmp/9d07714625c4a6c6cdb8dabcc494cc86d079e377f9290d8a82a23a76c9f8db60.jpg", "img_caption": ["(b) The large overlap between two distributions leads to flawed NP detection. "], "img_footnote": [], "page_idx": 3}, {"type": "image", "img_path": "OUXnnPJzXJ/tmp/8aafc51fabff3d3e145933af508e14c2f74d71aa7e979c84eff24d2465671d11.jpg", "img_caption": ["(c) Aligning the surrogate LLM using extra reliable training data. "], "img_footnote": [], "page_idx": 3}, {"type": "image", "img_path": "OUXnnPJzXJ/tmp/abb82cbae8eb89ad6ec9f4e1477909a712f46a9bb24ce3299a279077e1ba4f20.jpg", "img_caption": ["(d) Separating and correcting noisy preferences based on the threshold. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: We visualized the PPLDiff under the entire PerpCorrect process using Llama2-7B on Golden HH dataset with $20\\%$ noisy preferences. We use the green dotted line to represent the normal distribution formed by clean data, the red dotted line represents the normal distribution formed by noisy data, and the black dotted line represents the threshold. ", "page_idx": 3}, {"type": "text", "text": "consisting noisy preferences) and denote preference data points that are not noisy as clean preferences (CPs). Following Chowdhury et al. [8], we obtain the noisy preference dataset $\\tilde{\\mathcal{D}}$ using the standard random noise model [23] with the probability $\\varepsilon\\,\\in\\,(0,50\\bar{\\%})$ to change the data point into noisy preferences, i.e. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{P}_{(\\boldsymbol{x}^{(i)},\\tilde{\\boldsymbol{y}}_{w}^{(i)},\\tilde{\\boldsymbol{y}}_{l}^{(i)})\\sim\\tilde{D}}\\left[(\\boldsymbol{x}^{(i)},\\tilde{\\boldsymbol{y}}_{w}^{(i)},\\tilde{\\boldsymbol{y}}_{l}^{(i)})=(\\boldsymbol{x}^{(i)},\\boldsymbol{y}_{l}^{(i)},\\boldsymbol{y}_{w}^{(i)})\\right]=\\varepsilon.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Perplexity (PPL). PPL [15] measures the probability that the LLM generates a sentence. A lower PPL of a sentence indicates that the LLM has generated this sentence with a high probability. PPL is defined as the average negative log-likelihood of a sequence, i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{PPL}(s;\\theta)=\\exp(-\\frac{1}{t}\\sum_{i=1}^{t}\\log\\pi_{\\theta}(s_{i}|s_{<i})),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $s$ is a sequence composed of $t$ tokens and $\\log\\pi_{\\theta}{\\left(s_{i}\\vert s_{<i}\\right)}$ denotes the log-likelihood of the $i$ -th token given the preceding tokens $s_{<i}$ calculated by an LLM $\\pi_{\\theta}$ . ", "page_idx": 3}, {"type": "text", "text": "Technical details of alignment methods. There are usually three phases in RLHF pipeline [34, 26]: (1) supervised fine-tuning (SFT); (2) reward modeling; (3) reinforcement learning (RL) optimization. In the SFT phase, an LLM is fine-tuned via supervised learning on high-quality task-related data. ", "page_idx": 3}, {"type": "text", "text": "We denote the LLM after the SFT phase as $\\pi_{\\mathrm{SFT}}$ . In the reward modeling phase, the reward model is introduced to simulate human preferences. Given a preference dataset, a reward model $r_{\\omega}(x,y)$ parameterized by $\\omega$ , which takes prompt $x$ and response $y$ as input and outputs a real number representing the reward score, can be optimized via minimizing the following loss function: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{R}(r_{\\omega},\\mathcal{D})=-\\mathbb{E}_{(x,y_{w},y_{l})\\sim\\mathcal{D}}\\left[\\log\\sigma(r_{\\omega}(x,y_{w})-r_{\\omega}(x,y_{l}))\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\sigma$ is the logistic function. In the RL optimization phase, the objective function is as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\theta}\\mathbb{E}_{x\\sim\\mathcal{D},y\\sim\\pi_{\\theta}(y|x)}[r_{\\omega}(x,y)-\\beta\\cdot(\\log\\pi_{\\theta}(y|x)-\\log\\pi_{\\mathrm{ref}}(y|x))],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\pi_{\\theta}(y|x)$ represents the probability that the LLM parameterized by $\\theta>0$ generates the response $y$ given the prompt $x$ , $\\pi_{\\mathrm{ref}}$ is a reference LLM to maintain the generation ability of the aligned model, and $\\beta$ is a hyper-parameter to ensure the similarity between $\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}\\mid\\boldsymbol{x})$ and $\\pi_{\\mathrm{ref}}(y\\mid x)$ . We take $\\pi_{\\mathrm{SFT}}$ as the reference LLM $\\pi_{\\mathrm{ref}}$ following Ouyang et al. [24]. ", "page_idx": 4}, {"type": "text", "text": "Recently, offilne alignment methods directly leverages preferences in preference datasets, bypassing the need to learn a reward model in RLHF. The LLM parameters are optimized by minimizing the following loss function: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\pi_{\\boldsymbol{\\theta}};\\pi_{\\mathrm{ref}})=\\mathbb{E}_{(x,y_{w},y_{l})\\sim\\mathcal{D}}\\left[\\mathcal{G}(x,y_{w},y_{l};\\boldsymbol{\\theta})\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the function $\\mathcal{G}$ changes with the alignment method. To be specific, DPO [26] uses a BCE loss, SLiC [38] uses a hinge loss, and IPO [3] uses a square loss: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{G}_{\\mathrm{DPO}}(x,y_{w},y_{l};\\theta)=-\\log\\sigma\\left(\\beta\\log\\frac{\\pi_{\\theta}\\left(y_{w}\\mid x\\right)}{\\pi_{\\mathrm{ref}}\\left(y_{w}\\mid x\\right)}-\\beta\\log\\frac{\\pi_{\\theta}\\left(y_{l}\\mid x\\right)}{\\pi_{\\mathrm{ref}}\\left(y_{l}\\mid x\\right)}\\right),}\\\\ &{\\mathcal{G}_{\\mathrm{SLiC}}(x,y_{w},y_{l};\\theta)=\\operatorname*{max}\\Big\\{0,1-\\left(\\beta\\log\\frac{\\pi_{\\theta}\\left(y_{w}\\mid x\\right)}{\\pi_{\\mathrm{ref}}\\left(y_{w}\\mid x\\right)}-\\beta\\log\\frac{\\pi_{\\theta}\\left(y_{l}\\mid x\\right)}{\\pi_{\\mathrm{ref}}\\left(y_{l}\\mid x\\right)}\\right)\\Big\\},}\\\\ &{\\mathcal{G}_{\\mathrm{IPO}}(x,y_{w},y_{l};\\theta)=\\left(\\beta\\log\\frac{\\pi_{\\theta}\\left(y_{w}\\mid x\\right)}{\\pi_{\\mathrm{ref}}\\left(y_{w}\\mid x\\right)}-\\beta\\log\\frac{\\pi_{\\theta}\\left(y_{l}\\mid x\\right)}{\\pi_{\\mathrm{ref}}\\left(y_{l}\\mid x\\right)}-\\frac{1}{2}\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To mitigate the issue of NPs, cDPO [21] and rDPO [8] adjust the DPO loss based on the estimated proportion of NPs $\\varepsilon^{\\prime}$ using a clean validation dataset $\\dot{\\mathcal{D}_{\\mathrm{val}}}=\\{(\\boldsymbol{x}^{(i)},\\boldsymbol{y}_{w}^{(i)},\\boldsymbol{y}_{l}^{(i)})\\}_{i=1}^{N_{\\mathrm{val}}}$ consisting of $N_{\\mathrm{val}}\\in\\mathcal{N}$ clean preference data points, i.e. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{G}_{\\mathrm{cDPO}}(x,\\tilde{y}_{w},\\tilde{y}_{l};\\theta)=(1-\\varepsilon^{\\prime})\\mathcal{G}_{\\mathrm{DPO}}(x,\\tilde{y}_{w},\\tilde{y}_{l};\\theta)+\\varepsilon^{\\prime}\\mathcal{G}_{\\mathrm{DPO}}(x,\\tilde{y}_{l},\\tilde{y}_{w};\\theta),}\\\\ &{\\mathcal{G}_{\\mathrm{rDPO}}(x,\\tilde{y}_{w},\\tilde{y}_{l};\\theta)=\\frac{\\left(1-\\varepsilon^{\\prime}\\right)\\mathcal{G}_{\\mathrm{DPO}}(x,\\tilde{y}_{w},\\tilde{y}_{l};\\theta)-\\varepsilon^{\\prime}\\mathcal{G}_{\\mathrm{DPO}}(x,\\tilde{y}_{l},\\tilde{y}_{w};\\theta)}{1-2\\varepsilon^{\\prime}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3 Perplexity-aware Correction for Robust Alignment ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This section introduces Perplexity-aware Correction (PerpCorrect) for robust alignment with NPs. In Section 3.1, we introduce a novel metric called PPLDiff and then illustrate the pipeline of PerpCorrect to detect and correct NPs based on PPLDiff. In Section 3.2, we demonstrate how to adapt our proposed PerpCorrect with various alignment methods to achieve robust alignment. ", "page_idx": 4}, {"type": "text", "text": "3.1 Perplexity-aware Correction (PerpCorrect) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this subsection, we introduce PerpCorrect which employs a novel metric called PPLDiff as the foundation for detecting and correcting NPs. The algorithm of PerpCorrect is described in Algorithm 2. ", "page_idx": 4}, {"type": "text", "text": "PPLDiff. PPLDiff measures the difference between the PPL of chosen response and that of the rejected response. Given a preference data point $(x,\\tilde{y}_{w},\\tilde{y}_{l})\\in\\tilde{\\cal D}$ sampled from the noisy training dataset $\\tilde{\\mathcal D}$ and an LLM $\\pi_{\\theta}$ , PPLDiff is defined as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{PPLDiff}(x,\\tilde{y}_{w},\\tilde{y}_{l};\\theta)=\\log\\mathrm{PPL}([x;\\tilde{y}_{w}];\\theta)-\\log\\mathrm{PPL}([x;\\tilde{y}_{l}];\\theta),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $[x;y]$ indicates the concatenation of the prompt $x$ and the response $y$ . Intuitively, if a data point is a clean preference, the $\\mathrm{PPL}([x;\\tilde{y}_{w}];\\theta)$ will be lower than $\\mathrm{PPL}([x;\\tilde{y}_{l}];\\theta)$ because the sequence $[x;\\tilde{y}_{w}]$ is more aligned with human values and thus has a higher probability of being generated by aligned LLMs. As a result, it PPLDiff will be lower compared to NPs, which $\\mathrm{PPL}([x;\\tilde{y}_{w}];\\theta)$ is higher than $\\mathrm{PPL}([x;\\tilde{y}_{l}];\\theta)$ . This difference allows us to distinguish CPs and NPs based on PPLDiff. ", "page_idx": 4}, {"type": "text", "text": "Aligning a surrogate LLM only using clean validation data. Here, we utilize a clean validation dataset $\\mathcal{D}_{\\mathrm{val}}$ to obtain an aligned surrogate LLM to make PPLDiff able to distinguish CPs and NPs. We empirically find that the PPLDiff values of CPs and NPs calculated by an unaligned LLM in the noisy training dataset were initially indistinguishable as shown in Figure 2a, making it impossible to differentiate the NPs from CPs. This is because an unaligned LLM lacks the necessary preferences to distinguish NPs and CPs. ", "page_idx": 5}, {"type": "text", "text": "Therefore, we introduce a surrogate LLM $\\pi\\theta^{\\prime}$ parameterized by $\\theta^{\\prime}$ to replace the unaligned LLM and use it for calculating PPLDiff. We optimize the surrogate LLM $\\pi_{\\theta^{\\prime}}$ using the clean validation dataset $\\mathcal{D}_{\\mathrm{val}}$ as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\theta^{\\prime}}\\mathbb{E}_{(x,y_{w},y_{l})\\sim{\\mathcal{D}}_{\\mathrm{val}}}\\left[{\\mathcal{G}}_{\\mathrm{DPO}}(x,y_{w},y_{l};\\theta^{\\prime})\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "After aligning the surrogate LLM, the PPLDiff values of NPs calculated by the surrogate LLM $\\pi_{\\theta^{\\prime}}$ are significantly increased and those of CPs are significantly decreased, forming two distinct distributions as shown in Figure 2b. This is because the aligned surrogate LLM is trained to generate responses that align with human preferences, enhancing its ability to distinguish between NPs and CPs based on PPLDiff. ", "page_idx": 5}, {"type": "text", "text": "To separate CPs and NPs in the noisy training dataset without knowing the oracle preferences, we leverage the Levenberg-Marquardt (LM) algorithm to find two normal distributions that fti the density of PPLDiff calculated by the aligned surrogate LLM. Specifically, the LM algorithm returns the constants $\\bar{\\varepsilon},\\bar{\\mu},\\bar{\\sigma}$ that satisfies the following condition: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h(x|\\bar{\\varepsilon},\\bar{\\mu},\\bar{\\sigma})=(1-\\bar{\\varepsilon})f_{\\mathrm{clean}}(x|\\bar{\\mu},\\bar{\\sigma}^{2})+\\varepsilon f_{\\mathrm{noisy}}(x|-\\bar{\\mu},\\bar{\\sigma}^{2}),}\\\\ &{\\mathrm{where}\\quad f(x|\\mu,\\sigma^{2})=\\displaystyle\\frac{1}{\\sqrt{2\\sigma^{2}\\pi}}\\exp(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that $x$ is the PPLDiff value and $h(x|\\bar{\\varepsilon},\\bar{\\mu},\\bar{\\sigma})$ is the superposition of these two normal distribution. We denote $f_{\\mathrm{clean}}(x|\\bar{\\mu},\\bar{\\sigma}^{2})$ as the normal distribution ftiting the PPLDiff of CPs and $f_{\\mathrm{noisy}}(x|-\\bar{\\mu},\\bar{\\sigma}^{2})$ as the normal distribution fitting the PPLDiff of NPs since the PPLDiff of NPs is intuitively higher than that of $\\mathrm{CPs}$ . In this way, we can obtain two distinguishable normal distributions to separate NPs and CPs as shown in the green and red dotted lines of Figure 2b without knowing the oracle preferences. ", "page_idx": 5}, {"type": "text", "text": "Further aligning the surrogate LLM using extra reliable training data from noisy training datasets. After aligning only using the clean validation datasets, the discriminatory power of the PPLDiff is still far from satisfactory because of the large overlap between the two normal distributions. Therefore, we align the surrogate LLM with more reliable training data to make the PPLDiff of CPs and that of NPs more separable. We iteratively align the surrogate LLM $\\pi_{\\theta^{\\prime}}$ using more reliably clean training data whose PPLDiff is extremely small and reliably noisy training data whose PPLDiff is extremely large after correction by flipping the label of the response. ", "page_idx": 5}, {"type": "text", "text": "Specifically, at epoch $t\\,\\in\\,\\mathbb{N}$ , we select $(t-1)\\cdot\\alpha\\cdot|\\tilde{\\mathcal{D}}|$ of the training data along with the clean validation data for further alignment where $\\alpha\\,\\in\\,(0,1)$ is the selection ratio and $|\\tilde{\\mathcal{D}}|\\,=\\,N$ is the number of data points in noisy training dataset. As shown in Lines 33\u201345 of Algorithm 2, the selected reliable training dataset $\\mathcal{D}_{t}^{\\prime}$ consists of $(t-1)\\cdot\\alpha\\cdot(1-\\bar{\\varepsilon})\\cdot|\\tilde{D}|$ reliably clean training data whose PPLDiff values are smallest $\\left(t-1\\right)\\cdot\\alpha\\cdot\\left(1-\\bar{\\varepsilon}\\right)$ percent and $(t-1)\\cdot\\alpha\\cdot\\bar{\\varepsilon}\\cdot|\\tilde{D}|$ reliably noisy training data after correction. Note that the reliably clean training data are the data points whose PPLDiff values are smallest $(t-1)\\cdot\\alpha\\cdot(1-\\bar{\\varepsilon})$ percent (located in the green area of Figure 2c), and the reliably noisy training data whose PPLDiff values are largest $\\left(t-1\\right)\\cdot\\alpha\\cdot\\bar{\\varepsilon}$ percent (located in the red area of Figure 2c) among all the training data points. ", "page_idx": 5}, {"type": "text", "text": "Detecting and correcting NPs based on PPLDiff to output a denoised training dataset. Based on the PPLDiff calculated by the aligned surrogate LLM, PerpCorrect detects and corrects NPs whose PPLDiff value is lower than a certain threshold. We take the $\\mathbf{X}$ -coordinate of the intersection of the two normal distributions as the threshold (the black dotted line in Figure 2d). As shown in Lines 23\u201331, data points whose PPLDiff values are larger than this threshold are identified as CPs (the green area in Figure 2d), and other data points are identified as NPs requiring correction (the red area in Figure 2d). In this way, we can obtain a denoised training dataset for robust alignment. ", "page_idx": 5}, {"type": "text", "text": "1: Input: Noisy training dataset $\\tilde{\\mathcal{D}}$ , clean validation dataset $\\mathcal{D}_{\\mathrm{val}}$ , and pre-trained LLM $\\pi_{\\theta}$ parameterized by $\\theta$ ", "page_idx": 6}, {"type": "text", "text": "2: Output: Robust alignment model $\\pi_{\\theta}$ ", "page_idx": 6}, {"type": "text", "text": "3: // Stage I: Supervised fine-tuning (SFT)   \n4: $\\pi_{\\theta}\\leftarrow$ Supervised fine-tuned LLM $\\pi_{\\theta}$ . (Details in Appendix C.3)   \n5: // Stage II: Perplexity-aware correction using the surrogate LLM   \n6: $\\tilde{\\mathcal{D}}_{\\mathrm{denoised}}$ , $\\varepsilon_{\\mathrm{denoised}}^{\\prime}\\leftarrow$ Perplexity-aware Correction $(\\pi_{\\theta},\\tilde{\\mathcal{D}},\\mathcal{D}_{\\mathrm{val}})$ (Details in Algorithm 2)   \n7: // Stage III: Alignment with denoised dataset   \n8: $\\pi_{\\theta}\\leftarrow$ Aligned LLM $\\pi_{\\theta}$ using $\\tilde{\\mathcal{D}}_{\\mathrm{denoised}}$ and $\\varepsilon_{\\mathrm{denoised}}^{\\prime}$ (Details in Appendix C.3) ", "page_idx": 6}, {"type": "text", "text": "Further, we select an optimal denoised training dataset to further enhance the performance of robust alignment according to the intersection area of the two normal distributions. We denote the intersection area of two normal distributions as the estimated NP proportion of the denoised training dataset, i.e., ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\varepsilon_{P C}^{\\prime}=\\int_{-\\operatorname{inf}}^{+\\operatorname{inf}}\\operatorname*{min}\\{(1-\\bar{\\varepsilon})f_{\\mathrm{clean}}(x|\\bar{\\mu},\\bar{\\sigma}^{2}),\\bar{\\varepsilon}f_{\\mathrm{noisy}}(x|-\\bar{\\mu},\\bar{\\sigma}^{2})\\}\\mathrm{d}x,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\varepsilon_{P C}^{\\prime}$ calculates the ratio of noisy data points which are not detected by PerpCorrect (i.e., the green area enclosed by the black and red lines in Figure 2d) and the clean data points which are mistakenly detected by PerpCorrect (i.e., the red area enclosed by the black and green lines in Figure 2d). In this way, $\\varepsilon_{P C}^{\\prime}$ can efficiently calculate the NP proportion of the denoised training dataset. We take the denoised training dataset with the smallest $\\varepsilon_{P C}^{\\prime}$ among multiple iterations as the optimal one for robust alignment to boost alignment performance. ", "page_idx": 6}, {"type": "text", "text": "3.2 Robust Alignment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Here, we introduce how to adapt PerpCorrect to robustify various alignment methods and demonstrate the algorithm of robust alignment via PerpCorrect in Algorithm 1. In general, the pipeline of the robust alignment based on PerpCorrect contains three stages: SFT, PerpCorrect, and alignment. We will first conduct SFT, following Christiano et al. [9], to boost the performance of a pre-trained LLM by boosting its skills for specific tasks. Next, we will conduct PerpCorrect to detect and correct NPs and output an optimal denoised training dataset $\\tilde{\\mathcal{D}}_{\\mathrm{denoised}}$ the smallest $\\varepsilon_{P C}^{\\prime}$ in Eq. 15. Finally, we can obtain an aligned LLM from the SFT model using the denoised training dataset $\\tilde{\\mathcal{D}}_{\\mathrm{denoised}}$ via alignment (i.e., Line 8 in Algorithm 1). ", "page_idx": 6}, {"type": "text", "text": "Because our proposed PerpCorrect is agnostic to alignment methods and model structures, PerpCorrect is applicable to robustify both online alignment methods such as RLHF (PPO) [9] and offline alignment methods including DPO [26], SLiC [38], and IPO [3]. Besides, our proposed PerpCorrect is compatible with existing loss-oriented robust alignment methods, such as cDPO [21] and rDPO [8], based on the estimated proportion of NPs. Note that cDPO and rDPO require conducting computationally expensive cross-validation to tune the estimated proportion of NPs. We can efficiently estimate the proportion of NPs by utilizing the fitted normal distributions during PerpCorrect, i.e., $\\varepsilon_{P C}^{\\prime}$ in Eq. 15. Therefore, we can combine PerpCorrect with a wide range of existing alignment methods to achieve robust alignment with NPs. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we demonstrate that our proposed PerpCorrect achieves state-of-the-art alignment performance under different proportion of NPs and have good compatibility with other alignment methods. In Section 4.1, PerpCorrect combined with DPO [26] achieves state-of-the-art alignment performance than existing baselines (Section 4.1), including DPO [26], cDPO [21], and rDPO [8]. In Section 4.2, we further analyze the impact of the number of validation data and verified the compatibility of PerpCorrect with online and offilne alignment methods and robust alignment methods. The training details and compute resources are reported in Appendix C.1. ", "page_idx": 6}, {"type": "text", "text": "Table 2: Average reward accuracy of PPO series alignment methods using Llama2-7B on the Golden HH dataset. The standard deviation of reward accuracy is reported in Table 9. ", "page_idx": 7}, {"type": "table", "img_path": "OUXnnPJzXJ/tmp/63390944462634b8acaf93a026dacb75fff5b3a85f6215935edc54ade2c92553.jpg", "table_caption": ["Table 1: Average reward accuracy of DPO series alignment methods using Llama2-7B on the Golden HH dataset. The standard deviation of reward accuracy is reported in Table 8. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "OUXnnPJzXJ/tmp/312af7df68d730d94ca7838e87963e928af79d39c3ab97cb5c086f64743e72a6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 3: Average reward accuracy of DPO series alignment methods using phi-2 on the Golden HH dataset. The standard deviation of reward accuracy is reported in Table 10. ", "page_idx": 7}, {"type": "text", "text": "Table 4: Average reward accuracy of DPO series alignment methods using phi-2 on the OASST1 dataset. The standard deviation of reward accuracy is reported in Table 11. ", "page_idx": 7}, {"type": "table", "img_path": "OUXnnPJzXJ/tmp/830b90faf05e38949afb30f44dc8ae6af0d220099d9946191b518218516b4b34.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Datasets. We utilize two preference datasets, namely OpenAssistant Conversations (OASST1) [17] and Golden HH [7]. The processed OASST1 dataset comprises 17,939 training samples and 951 testing samples and the processed Golden HH dataset consists of 12,066 training samples and 654 testing samples. The description and processing details of these datasets are provided in Appendix C.2. ", "page_idx": 7}, {"type": "text", "text": "Models. Our evaluation leverages two distinct series of open-sourced LLMs with different parameter sizes: Llama2-7B [32] and phi-2 [20]. We acquire the checkpoints from their official repositories on Hugging Face. The LLMs used for PerpCorrect and those for robust alignment share the same model structure and initialization. ", "page_idx": 7}, {"type": "text", "text": "Baselines. We adopt vanilla DPO [26] and two robust alignment methods, cDPO [21] and rDPO [8], as baselines. For their detailed implementation, we utilize and adapt the transformers and TRL libraries provided by the Hugging Face community. ", "page_idx": 7}, {"type": "text", "text": "Metrics. In accordance with Chowdhury et al. [8], we employ the winning rate of policy generations against the selected preferences on the test dataset as our primary metric. This metric applies to vanilla DPO [26], cDPO [21], rDPO [8], as well as other offline alignment methods including SLiC [38] and IPO [3]. Additionally, we utilize the winning rate of the reward model score for the chosen preferences on the test dataset as our metric for vanilla PPO [24], cPPO [21, 34], and rPPO [8]. These two metrics are collectively called reward accuracy. ", "page_idx": 7}, {"type": "text", "text": "4.1 PerpCorrect Achieves the State-of-the-Art Robust Alignment Performance ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The empirical results demonstrate that our method, PerpCorrect, achieves state-of-the-art robust alignment performance, surpassing existing baselines such as vanilla DPO [26], cDPO [21], and rDPO [8]. This is evident across various proportions of noisy preferences $\\varepsilon$ using different datasets and LLMs. ", "page_idx": 7}, {"type": "text", "text": "Comparison using different LLMs. Tables 1 and 3 demonstrate the average reward accuracy of the DPO series alignment methods on the Golden HH [7] dataset using Llama2-7B [32] and phi-2 [20]. At a proportion of the NPs $\\varepsilon\\,=\\,40\\%$ , PerpCorrect increases the reward accuracy by $41.77\\%$ (from $53.15\\%$ to $94.92\\%$ ) using Llama2-7B and by $41.41\\%$ (from $54.98\\%$ to $96.39\\%$ ) using phi-2. The empirical result validates that our proposed PerpCorrect can be used on different sizes of LLMs and achieve better alignment performance than baselines. ", "page_idx": 7}, {"type": "text", "text": "Comparison on different datasets. Tables 3 and 4 demonstrate the average reward accuracy of the DPO series alignment methods on the Golden HH [7] and OASST1 [17] datasets using phi-2 [20]. ", "page_idx": 7}, {"type": "table", "img_path": "OUXnnPJzXJ/tmp/85fed0e5a049b175bc900c5ecc1b93abe921b489c59060d02ed703a9afa1b41a.jpg", "table_caption": ["Table 5: Impact of the number of clean validation data evaluated on the Golden HH dataset using Llama2-7B with a proportion of NPs $\\varepsilon=40\\%$ . "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "OUXnnPJzXJ/tmp/c94e2796d8748845a2ac0660fe4de7feaa19a1fa290dd580293aa6172f5b846e.jpg", "table_caption": ["Table 6: Average reward accuracy and improvements of the offline and robust alignment methods, as well as those combined with PerpCorrect, using Llama2-7B on the Golden HH dataset. The standard deviation of reward accuracy and improvements is reported in Table 12. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "The empirical results reveal a significant discrepancy in average reward accuracy between the more complex OASST1 dataset and the Golden HH dataset. The performance of other robust alignment methods is found to be unsatisfactory on the OASST1 dataset, often not surpassing the vanilla DPO. In contrast, our method PerpCorrect consistently maintains strong alignment performance across varying proportions of noisy preferences. In general, our method PerpCorrect can achieve better alignment performance than baselines across different datasets. ", "page_idx": 8}, {"type": "text", "text": "4.2 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Impact of the number of clean validation data. Table 5 illustrates the impact of the number of clean validation data points. We conducted experiments on the Golden HH dataset using Llama2- 7B with a proportion of NPs $\\varepsilon=40\\%$ . The empirical results indicate that as the number of clean validation data points increases, the performance of our method, PerpCorrect, also improves. However, when the number is too large, the improvement in performance is not obvious, and the cost of manual annotation significantly increases. ", "page_idx": 8}, {"type": "text", "text": "Compatibility with online alignment method RLHF (PPO). We adopt vanilla PPO [24], cPPO [21, 34], and rPPO [8] as baselines. Table 2 shows the alignment performance of PPO series alignment methods on the Golden HH [7] dataset using Llama2-7B. Although vanilla PPO has good performance when the proportion of NPs is low, it still declines significantly when the proportion is high. PerpCorrect maintains desirable alignment performances when the proportion of NPs is high. Our empirical results show that PerpCorrect has desirable compatibility with online alignment method RLHF (PPO). ", "page_idx": 8}, {"type": "text", "text": "Compatibility with various offline alignment methods. Table 6 presents the average reward accuracy and improvements of original offilne alignment methods compared to those combined with PerpCorrect. Our experiments, conducted on the Golden HH dataset using Llama2-7B, reveal that the reward accuracy of SLiC [38] and IPO [3] both significantly decrease as the proportion of NPs increases, similar to vanilla DPO [26]. However, our method PerpCorrect enhances their alignment performance across different proportions of NPs. Notably, IPO combined with PerpCorrect achieves the best alignment performance. We conjecture the main reason is that the proportion of NPs in the denoised dataset is very low and IPO performs better than other methods under a low proportion of NPs. These empirical results demonstrate that our method has good compatibility with various offilne alignment methods. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Compatibility with robust alignment methods. Table 6 shows the average reward accuracy and improvements of robust alignment methods compared to those combined with PerpCorrect. Our method, PerpCorrect, can significantly enhance the performance of cDPO [21], and provide a modest improvement for rDPO [8] under almost all proportion of NPs. The empirical results show that our method has good compatibility with robust alignment methods. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper proposes a method called perplexity-aware correction (PerpCorrect), as an effective approach for robust alignment with noisy preferences (NPs). PerpCorrect utilizes a surrogate LLM to calculate a novel metric, PPLDiff, and further detects and corrects NPs from clean preferences (CPs) based on it. PerpCorrect consists of three steps: (1) PerpCorrect aligns a surrogate LLM using the clean validation dataset, enabling PPLDiff to distinguish between CPs and NPs. (2) PerpCorrect enhances the discrimination power of PPLDiff by aligning the surrogate LLM with more reliable training data. (3) PerpCorrect detects and corrects NPs from CPs based on a calculated threshold and obtains a denoised training dataset. The paper further proposes a robust alignment pipeline, consisting of three stages SFT, PerpCorrect, and alignment, to achieve robust alignment with NPs. The experimental results validate that PerpCorrect achieves state-of-the-art alignment performance and has good compatibility with other online, offline, and robust alignment methods. Therefore, PerpCorrect can be an effective method to mitigate the impact of NPs and can be used for robust alignment. Future research directions include: (1) Improving the time efficiency of PerpCorrect and (2) Reducing the amount of clean validation data required to achieve the same alignment performance. ", "page_idx": 9}, {"type": "text", "text": "Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We discuss some limitations of this work to stimulate further research in this direction. Our limitations mainly stem from two aspects: time efficiency issues caused by multiple calculations of PPLDiff and repeated training of a surrogate LLM, and the need for a validation dataset. ", "page_idx": 9}, {"type": "text", "text": "Time efficiency. Iteratively calculating the PPLDiff value for each data point and aligning a surrogate LLM is time-consuming. Selecting reliably training data and denoising the training dataset requires that the PPLDiff value be calculated for each data point during each epoch, which may cause unnecessary calculations for CPs and NPs that can already be clearly distinguished. Besides, aligning a surrogate LLM with same size as the LLM for alignment multiple times is time-consuming. The detailed discussion is in the Appendix B. ", "page_idx": 9}, {"type": "text", "text": "Validation dataset. PerpCorrect requires a validation dataset for aligning a surrogate LLM. However, manually annotating a validation dataset is complex and labor-intensive in practice. As shown in Table 5, there is a significant disparity in alignment performance when comparing the use of 10 clean samples to 50 clean samples. Exploring how to use fewer clean samples or even no clean samples to achieve the same or better performance is a problem worth further investigation. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research is supported by the National Research Foundation, Singapore under its Strategic Capability Research Centres Funding Initiative, the funding BAS/1/1689-01-01, URF/1/4663-01-01, REI/1/5232-01-01, REI/1/5332-01-01, and URF/1/5508-01-01 from KAUST, and funding from KAUST - Center of Excellence for Generative AI, under award number 5940. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] 01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation models by 01.ai, 2024. URL https://arxiv.org/abs/2403.04652.   \n[2] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, M\u00e9rouane Debbah, \u00c9tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. The falcon series of open language models, 2023. URL https://arxiv.org/abs/2311.16867.   \n[3] Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and R\u00e9mi Munos. A general theoretical paradigm to understand learning from human preferences, 2023.   \n[4] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback, 2022. URL https://arxiv.org/abs/2204.05862.   \n[5] Tim Baumg\u00e4rtner, Yang Gao, Dana Alon, and Donald Metzler. Best-of-venom: Attacking rlhf by injecting poisoned preference data, 2024.   \n[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[7] Tianchi Cai, Xierui Song, Jiyan Jiang, Fei Teng, Jinjie Gu, and Guannan Zhang. Ulma: Unified language model alignment with human demonstration and point-wise preference, 2024.   \n[8] Sayak Ray Chowdhury, Anush Kini, and Nagarajan Natarajan. Provably robust DPO: Aligning language models with noisy feedback. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id $\\cdot$ yhpDKSw7yA.   \n[9] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.   \n[10] Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. Safe RLHF: Safe reinforcement learning from human feedback. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id $\\cdot$ TyFrPOKYXw.   \n[11] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024.   \n[12] Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with $\\nu$ -usable information. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 5988\u20136008. PMLR, 17\u201323 Jul 2022. URL https://proceedings.mlr.press/v162/ethayarajh22a. html.   \n[13] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization, 2024.   \n[14] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang. Chatglm: A family of large language models from glm-130b to glm-4 all tools, 2024.   \n[15] Fred Jelinek, Robert L Mercer, Lalit R Bahl, and James K Baker. Perplexity\u2014a measure of the difficulty of speech recognition tasks. The Journal of the Acoustical Society of America, 62(S1): S63\u2013S63, 1977.   \n[16] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mistral 7b, 2023. URL https://arxiv.org/abs/2310.06825.   \n[17] Andreas K\u00f6pf, Yannic Kilcher, Dimitri von R\u00fctte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Rich\u00e1rd Nagyf,i et al. Openassistant conversations-democratizing large language model alignment. Advances in Neural Information Processing Systems, 36, 2024.   \n[18] Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J Liu, and Jialu Liu. Statistical rejection sampling improves preference optimization. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id $\\cdot$ xbjSwwrQOe.   \n[19] Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo, Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li. Trustworthy llms: a survey and guideline for evaluating large language models\u2019 alignment, 2024.   \n[20] Microsoft. Phi-2: The surprising power of small language models, 2023. URL https://www.microsoft.com/en-us/research/blog/ phi-2-the-surprising-power-of-small-language-models.   \n[21] Eric Mitchell. A note on dpo with noisy preferences and relationship to ipo, 2023. URL https://ericmitchell.ai/cdpo.pdf.   \n[22] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assisted question-answering with human feedback, 2022. URL https://arxiv.org/abs/2112.09332.   \n[23] Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with noisy labels. Advances in neural information processing systems, 26, 2013.   \n[24] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730\u201327744, 2022.   \n[25] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.   \n[26] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=HPuSIXJaa9. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[27] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. ", "page_idx": 12}, {"type": "text", "text": "[28] Damien Sileo. tasksource: Structured dataset preprocessing annotations for frictionless extreme multi-task learning and evaluation. arXiv preprint arXiv:2301.05948, 2023. URL https: //arxiv.org/abs/2301.05948.   \n[29] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008\u20133021, 2020.   \n[30] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. ", "page_idx": 12}, {"type": "text", "text": "[31] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, L\u00e9onard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram\u00e9, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Chris Welty, Christopher A. Choquette-Choo, Danila Sinopalnikov, David Weinberger, Dimple Vijaykumar, Dominika Rogozin\u00b4ska, Dustin Herbison, Elisa Bandy, Emma Wang, Eric Noland, Erica Moreira, Evan Senter, Evgenii Eltyshev, Francesco Visin, Gabriel Rasskin, Gary Wei, Glenn Cameron, Gus Martins, Hadi Hashemi, Hanna Klimczak-Plucin\u00b4ska, Harleen Batra, Harsh Dhand, Ivan Nardini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway, Jetha Chan, Jin Peng Zhou, Joana Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernandez, Joost van Amersfoort, Josh Gordon, Josh Lipschultz, Josh Newlan, Ju yeong Ji, Kareem Mohamed, Kartikeya Badola, Kat Black, Katie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish Greene, Lars Lowe Sjoesund, Lauren Usui, Laurent Sifre, Lena Heuermann, Leticia Lago, Lilly McNealus, Livio Baldini Soares, Logan Kilpatrick, Lucas Dixon, Luciano Martins, Machel Reid, Manvinder Singh, Mark Iverson, Martin G\u00f6rner, Mat Velloso, Mateo Wirth, Matt Davidow, Matt Miller, Matthew Rahtz, Matthew Watson, Meg Risdal, Mehran Kazemi, Michael Moynihan, Ming Zhang, Minsuk Kahng, Minwoo Park, Mof iRahman, Mohit Khatwani, Natalie Dao, Nenshad Bardoliwalla, Nesh Devanathan, Neta Dumai, Nilay Chauhan, Oscar Wahltinez, Pankil Botarda, Parker Barnes, Paul Barham, Paul Michel, Pengchong Jin, Petko Georgiev, Phil Culliton, Pradeep Kuppala, Ramona Comanescu, Ramona Merhej, Reena Jana, Reza Ardeshir Rokni, Rishabh Agarwal, Ryan Mullins, Samaneh Saadat, Sara Mc Carthy, Sarah Cogan, Sarah Perrin, S\u00e9bastien M. R. Arnold, Sebastian Krause, Shengyang Dai, Shruti Garg, Shruti Sheth, Sue Ronstrom, Susan Chan, Timothy Jordan, Ting Yu, Tom Eccles, Tom Hennigan, Tomas Kocisky, Tulsee Doshi, Vihan Jain, Vikas Yadav, Vilobh Meshram, Vishal Dharmadhikari, Warren Barkley, Wei Wei, Wenming Ye, Woohyun Han, Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao Gong, Zichuan Wei, Victor Cotruta, Phoebe Kirk, Anand Rao, Minh Giang, Ludovic Peran, Tris Warkentin, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, D. Sculley, Jeanine Banks, Anca Dragan, Slav Petrov, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Armand Joulin, Kathleen Kenealy, Robert Dadashi, and Alek Andreev. Gemma 2: Improving open language models at a practical size, 2024. URL https://arxiv.org/abs/2408.00118. ", "page_idx": 12}, {"type": "text", "text": "[32] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, ", "page_idx": 12}, {"type": "text", "text": "Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. ", "page_idx": 13}, {"type": "text", "text": "[33] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. URL https://arxiv.org/abs/2307.09288. ", "page_idx": 13}, {"type": "text", "text": "[34] Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, Songyang Gao, Nuo Xu, Yuhao Zhou, Xiaoran Fan, Zhiheng Xi, Jun Zhao, Xiao Wang, Tao Ji, Hang Yan, Lixing Shen, Zhan Chen, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, and Yu-Gang Jiang. Secrets of rlhf in large language models part ii: Reward modeling, 2024.   \n[35] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024. URL https://arxiv.org/abs/2407.10671.   \n[36] Jingwei Yi, Rui Ye, Qisi Chen, Bin Benjamin Zhu, Siheng Chen, Defu Lian, Guangzhong Sun, Xing Xie, and Fangzhao Wu. Open-source can be dangerous: On the vulnerability of value alignment in open-source LLMs, 2024. URL https://openreview.net/forum?id= NIouO0C0ex.   \n[37] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank responses to align language models with human feedback without tears. arXiv preprint arXiv:2304.05302, 2023.   \n[38] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. Slic-hf: Sequence likelihood calibration with human feedback, 2023.   \n[39] Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, Limao Xiong, Lu Chen, Zhiheng Xi, Nuo Xu, Wenbin Lai, Minghao Zhu, Cheng Chang, Zhangyue Yin, Rongxiang Weng, Wensen Cheng, Haoran Huang, Tianxiang Sun, Hang Yan, Tao Gui, Qi Zhang, Xipeng Qiu, and Xuanjing Huang. Secrets of rlhf in large language models part i: Ppo, 2023.   \n[40] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. ", "page_idx": 13}, {"type": "text", "text": "A Broader Impacts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our proposed PerpCorrect and robust alignment pipeline offers a solution for achieving state-of-theart performance in robust alignment under noisy preferences. PerpCorrect is designed to effectively reduce malicious noise in the dataset and mitigate biases introduced by human annotators, ensuring that the trained language model (LLM) is accurately aligned with true human preferences. ", "page_idx": 14}, {"type": "text", "text": "Moreover, we recognize a potential risk: if malicious users exploit our method for reverse training, they might compromise the security mechanisms of existing open-source LLMs. Existing research has demonstrated the possibility of reverse training [36]. ", "page_idx": 14}, {"type": "text", "text": "B Time Efficiency Analysis ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The additional computational overhead is primarily attributed to PerpCorrect (Section 3.1). Table 7 presents both theoretical and empirical runtime comparisons, where $X$ represents the theoretical time required for Alignment (Section 3.2) or other baselines. ", "page_idx": 14}, {"type": "text", "text": "In theory, during the PerpCorrect process, we need to calculate PPLDiff and train the surrogate model in each epoch. The computation time introduced by PerpCorrect is approximately $\\textstyle{\\frac{T}{3}}$ that of the Alignment or other baselines. ", "page_idx": 14}, {"type": "text", "text": "The calculation of PPLDiff in each epoch requires only ${\\textstyle\\frac{1}{3}}\\mathrm{of}$ the time needed for robust alignment. The primary computational load in robust alignment arises from the complexity of forwarding and back-propagation, while the complexities of gradient updates and parameter updates are relatively low. Additionally, back-propagation takes twice as long as forwarding. In addition, the calculation of PPLDiff only requires forwarding. ", "page_idx": 14}, {"type": "text", "text": "For surrogate model training, PerpCorrect utilized data points that represented $t\\times\\alpha$ of the total dataset during epoch $t$ . Since both $t$ and $\\alpha$ are small, the time required for surrogate model training can be approximately ignored. ", "page_idx": 14}, {"type": "text", "text": "In practice, Our entire robust alignment pipeline ${\\sim}24$ hours) takes only twice as long as the baseline $_{\\sim12}$ hours). We set $T=5$ and $\\alpha=2$ , and used the AdamW optimizer. The practical efficiency of the PerpCorrect is due to the use of fp32 precision by the AdamW optimizer, which increases the GPU\u2019s calculation time during the robust alignment process. ", "page_idx": 14}, {"type": "table", "img_path": "OUXnnPJzXJ/tmp/777a3ccc8fa530551fae5409439ba6eb32f3c32621731ad5382703cd61111ba5.jpg", "table_caption": ["Table 7: Comparison of theoretical and practical running times for PerpCorrect and baselines. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "C Implementation details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Training details and compute resources. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We utilized the Qlora method [11] for fine-tuning the LLMs, executed on RTX 4090 GPUs with $24\\ \\mathrm{GB}$ of memory. Hyperparameters were set as follows: lora_rank $=32$ , lora_dropout $=0.1$ , and lora_alpha $=~16$ . For SFT, we use the alpaca dataset [30] and set learning_rate $=~2e\\mathrm{~-~}4$ and batch_size $=~20$ . For our PerpCorrect stage II, we set $\\beta\\:=\\:0.1$ , learning_rate $\\mathit{\\Theta}=\\;1e\\mathrm{~-~3~}$ , batch_size $=4$ , $T=5$ , and $\\alpha=0.02$ . For our PerpCorrect stage III and all other alignment methods, we set $\\beta=0.1$ , learning_rate $=3e-4$ , and batch_size $=20$ . Other details not mentioned, we follow the default setting in TRL library. Each experiment, involving a specific method and proportion of NPs, could be completed using a single RTX 4090 GPU within 24 hours on the Golden HH dataset and within 72 hours on the OASST1 dataset. ", "page_idx": 14}, {"type": "text", "text": "C.2 Description and Processing Details of the Datasets ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "OpenAssistant Conversations Dataset (OASST1). The original OASST1 dataset [17] is an assistant-style conversation corpus generated and annotated by humans. It consists of over 10,000 fully annotated conversations in 35 different languages. Sileo [28] converted these conversations into a preference dataset comprising 17,966 training samples and 952 testing samples. After flitering out conversations with one or fewer letters, we obtained a preference dataset with 17,939 training samples and 951 testing samples. ", "page_idx": 15}, {"type": "text", "text": "Golden HH. The Golden HH dataset [7] is a variant of the Anthropic Helpful and Harmless (HH) dataset [4]. It originally contains 42,537 training samples and 2,312 testing samples as part of a preference dataset. Each sample has two keys: one representing the prompt $x$ and the chosen response $y_{w}$ , and the other representing the prompt $x$ and the rejected response $y_{l}$ . We first converted the dataset into a triple form: prompt $x$ , chosen response $y_{w}$ , and rejected response $y_{l}$ , retaining only one-turn conversation data. After filtering out samples with one or fewer letters, we obtained a preference dataset with 12,066 training samples and 654 testing samples. ", "page_idx": 15}, {"type": "text", "text": "C.3 Detailed Robust Alignment via Perplexity-aware Correction ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Supervised Fine-Tuning (SFT). The objective of Supervised Fine-Tuning (SFT) is to enhance the performance of a pre-trained large language model (LLM) by refining its abilities for specific tasks. As demonstrated by prior work [9, 25, 24], this can be achieved by utilizing supervised fine-tuning with a specialized dataset tailored to the target task. The SFT dataset is annotated with labels, providing examples that are directly relevant to the task. Specifically, for each data point $(x,y)$ in the SFT dataset, $x$ represents the prompt given to the LLM, and $y$ represents the expected response that the model should generate based on the prompt $x$ . The process involves fine-tuning the LLM by maximizing the log-likelihood of the correct responses $y$ given the prompts $x$ . Through this method, the model learns to produce more accurate and task-specific outputs, thereby significantly improving its performance on the given task. ", "page_idx": 15}, {"type": "text", "text": "Perplexity-aware Correction (PerpCorrect). We demonstrate the entire PerpCorrect algorithm in Algorithm 2. ", "page_idx": 15}, {"type": "text", "text": "Alignment. We can achieve alignment using the denoised training dataset $\\tilde{\\mathcal{D}}_{\\mathrm{denoised}}$ with an estimated proportion of NPs $\\varepsilon_{\\mathrm{denoised}}^{\\prime}$ . For offline alignment methods such as DPO, SLiC, and IPO, we can directly optimize the LLM using the denoised training dataset D\u02dcdenoised based on the loss functions defined in Eqs. 6\u20138. For loss-based robust alignment methods, including cDPO and rDPO, we set $\\varepsilon^{\\prime}\\,=\\,\\varepsilon_{\\mathrm{denoised}}^{\\prime}$ and then optimize the LLM using the denoised training dataset $\\tilde{\\mathcal{D}}_{\\mathrm{denoised}}$ according to the loss functions mentioned in Eqs. 9 and 10. For the online alignment method RLHF (PPO), we first train a reward model using the denoised training dataset $\\tilde{\\mathcal{D}}_{\\mathrm{denoised}}$ based on the loss function described in Eq. 3. Subsequently, we further optimize the LLM using PPO according to the objective function detailed in Eq. 4. ", "page_idx": 15}, {"type": "text", "text": "D Extended Experimental Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "D.1 Experiment Statistical Significance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Tables 8\u201312 demonstrates the standard deviation of the reward accuracy reported in Tables 1\u20134 and 6. ", "page_idx": 15}, {"type": "text", "text": "D.2 Average PPLDiff Values of Data from Different Datasets Calculated by Unaligned LLMs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We randomly selected 10,000 data points from each dataset and calculated PPLDiff using different LLMs. The datasets and LLMs are downloaded from the Huggingface website. The average PPLDiff values are reported in the Table 13. ", "page_idx": 15}, {"type": "text", "text": "Table 9: Standard deviation of reward accuracy for PPO series alignment methods using Llama2- 7B on the Golden HH dataset. The average reward accuracy is reported in Table 2. ", "page_idx": 16}, {"type": "table", "img_path": "OUXnnPJzXJ/tmp/a29180b20970a3ae724a9c86818845ea7c9492242e4c61aa8d3abdfff4eee865.jpg", "table_caption": ["Table 8: Standard deviation of reward accuracy for DPO series alignment methods using Llama2- 7B on the Golden HH dataset. The average reward accuracy is reported in Table 1. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "OUXnnPJzXJ/tmp/a69b818da8d80122957ff110f3ecc8ab87f09b3b2e3a82b73494a7d23f1b4753.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 10: Standard deviation of reward accuracy for DPO series alignment methods using phi-2 on the Golden HH dataset. The average reward accuracy is reported in Table 3. ", "page_idx": 16}, {"type": "text", "text": "Table 11: Standard deviation of reward accuracy for DPO series alignment methods using phi-2 on the OASST1 dataset. The average reward accuracy is reported in Table 4. ", "page_idx": 16}, {"type": "table", "img_path": "OUXnnPJzXJ/tmp/e09448a61fce76cd7b28003cbccba079f7fe248db994364456ffe08db637f9bb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "OUXnnPJzXJ/tmp/61cd698dc115b8ea87514874134317df71ad861a78e4e501f91414c72c700312.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "OUXnnPJzXJ/tmp/7d8400e5356dee6cce1659435cb4632d1ccf433471eb92e6ffa234ddf13f46aa.jpg", "table_caption": ["Table 12: Standard deviation of reward accuracy and improvements of the offilne and robust alignment methods, as well as those combined with PerpCorrect, using Llama2-7B on the Golden HH dataset. The average reward accuracy is reported in Table 6. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "OUXnnPJzXJ/tmp/40a02fe45f19c84f2538bec4a59ef1542c68c3cd8cf87cab4f986376015aac0e.jpg", "table_caption": ["Table 13: Average PPLDiff values of randomly selected data points across datasets calculated by different LLMs. \"Avg.\" refers to the average PPLDiff value over all the datasets. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "1: Input: Noisy training dataset $\\tilde{\\mathcal D}$ , clean validation dataset $\\mathcal{D}_{\\mathrm{val}}$ , LLM $\\pi_{\\theta}$ parameterized by $\\theta$   \n2: Output: Denoised training dataset $\\tilde{\\mathcal{D}}_{\\mathrm{denoised}}$ and estimated proportion of NPs \u03b5\u2032denoised   \n3: $\\pi_{\\theta^{\\prime}}\\leftarrow\\pi_{\\theta}$ , $\\mathcal{D}_{0}^{\\prime}\\gets\\emptyset$ , $\\varepsilon_{\\mathrm{denoised}}^{\\prime}\\leftarrow1$ , $\\tilde{\\mathcal{D}}_{\\mathrm{denoised}}\\leftarrow\\tilde{\\mathcal{D}}$ ,   \n4: for epoch $t=0,...,T$ do   \n5: // Aligning the surrogate LLM   \n6: $\\pi_{\\theta^{\\prime}}\\leftarrow$ Alignment $(\\pi_{\\theta^{\\prime}},D_{t}^{\\prime}\\cup D_{\\mathrm{val}})$   \n7: // Calculating the PPLDiff values for each data point   \n8: $\\Omega\\gets\\emptyset$   \n9: fo $\\begin{array}{r l}&{\\mathbf{r}\\left(\\tilde{x},\\tilde{y}_{w},\\tilde{y}_{l}\\right)\\in\\dot{\\mathcal{D}}\\;\\mathbf{do}}\\\\ &{z\\gets\\log\\mathrm{PPL}(x+\\tilde{y}_{w};\\theta^{\\prime})-\\log\\mathrm{PPL}(x+\\tilde{y}_{l};\\theta^{\\prime})}\\\\ &{\\Omega\\gets\\Omega\\cup\\{\\left(\\tilde{x},\\tilde{y}_{w},\\tilde{y}_{l},z\\right)\\}}\\end{array}$   \n10:   \n11:   \n12: end for   \n13: // Fitting PPLDiff density of noisy training dataset   \n14: \u03b5\u00af, $\\bar{\\mu}$ , $\\bar{\\sigma}\\gets$ Fitted parameters using Levenberg-Marquard algorithm with $\\Omega$   \n15: // Estimating NPs proportion of the denoised training dataset   \n16: $\\varepsilon_{P C}^{\\prime}\\leftarrow$ Estimated proportion of NPs using the Eq.15 based on $\\bar{\\varepsilon},\\bar{\\mu},\\bar{\\sigma}$   \n17: // Ke eping denoised training dataset with the smallest \u03b5\u2032denoised   \n18: if $\\varepsilon_{P C}^{\\prime}<\\varepsilon_{\\mathrm{denoised}}^{\\prime}$ then   \n19: $\\varepsilon_{\\mathrm{denoised}}^{\\prime}\\leftarrow\\varepsilon_{P C}^{\\prime}$   \n20: // Calculating the Threshold $\\tau$   \n21: $\\tau\\gets\\mathbf{X}$ -coordinate of the intersection of the two normal distributions $(\\bar{\\varepsilon},\\bar{\\mu},\\bar{\\sigma})$   \n22: $//$ Distinguishing CPs and NPs based on the threshold $\\tau$ and correcting NPs   \n23: $\\tilde{\\mathcal{D}}_{\\mathrm{CPs}}\\gets\\emptyset$ , $\\tilde{\\mathcal{D}}_{\\mathrm{NPs}}\\gets\\emptyset$   \n24: for $(\\tilde{x},\\tilde{y}_{w},\\tilde{y}_{l},z)\\in\\Omega$ do   \n25: if $z>\\tau$ then   \n26: $\\tilde{D}_{\\mathrm{CPs}}\\leftarrow\\tilde{D}_{\\mathrm{CPs}}\\cup\\{(\\tilde{x},\\tilde{y}_{w},\\tilde{y}_{l})\\}$   \n27: else   \n28: $\\tilde{D}_{\\mathrm{NPs}}\\gets\\tilde{D}_{\\mathrm{NPs}}\\cup\\{(\\tilde{x},\\tilde{y}_{l},\\tilde{y}_{w})\\}$   \n29: end if   \n30: end for   \n31: $\\tilde{D}_{\\mathrm{Donised}}\\leftarrow\\tilde{D}_{\\mathrm{CPs}}\\cup\\tilde{D}_{\\mathrm{NPs}}$   \n32: end if   \n33: $\\mathcal{D}_{\\mathrm{Clean}}\\leftarrow\\emptyset,\\mathcal{D}_{\\mathrm{Noisy}}\\leftarrow\\emptyset$   \n34: // Calculating the left bound $\\tau_{l}$ and the right bound $\\tau_{r}$   \n35: $\\tau_{l}\\leftarrow(t-1)\\cdot\\alpha\\cdot(1-\\bar{\\varepsilon})\\cdot|\\tilde{\\mathcal{D}}|$ -th smallest PPLDiff value in $\\Omega$   \n36: $\\tau_{r}\\gets(t-1)\\cdot\\alpha\\cdot\\bar{\\varepsilon}\\cdot|\\tilde{D}|$ -th largest PPLDiff value in $\\Omega$   \n37: // Finding extra reliable training data   \n38: for $(\\tilde{x},\\tilde{y}_{w},\\tilde{y}_{l},z)\\in\\Omega$ do   \n39: if $z<\\tau_{l}$ then   \n40: $\\mathcal{D}_{\\mathrm{Clean}}\\leftarrow\\mathcal{D}_{\\mathrm{Clean}}\\cup\\{(\\tilde{x},\\tilde{y}_{w},\\tilde{y}_{l})\\}$   \n41: end if   \n42: if $z>\\tau_{r}$ then   \n43: $\\mathcal{D}_{\\mathrm{Noisy}}\\leftarrow\\mathcal{D}_{\\mathrm{Noisy}}\\cup\\{(\\tilde{x},\\tilde{y}_{l},\\tilde{y}_{w})\\}$   \n44: end if   \n45: end for   \n46: $\\mathcal{D}_{t+1}^{\\prime}\\leftarrow\\mathcal{D}_{\\mathrm{Clean}}\\cup\\mathcal{D}_{\\mathrm{Noisy}}$   \n47: end for ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Our introduction covers our contributions, main methods and experimental results. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We discussed the efficiency issues and data volume requirements of our method PerpCorrect in the Conclusions section. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We show all the experiment detail in the Experiments section and Appendix. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide open access to our code using Github. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We show the entire experimental details in the Experiments section and Appendix and provide open access to the code using Github. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We report the standard deviation in the Appendix. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide detailed sufficient information on the computer resources in the Appendix. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We follow the Code of Ethics. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We discuss the potential impacts in Appendix. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our robust alignment method does not have a high risk for misuse. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The models and datasets the we used are open-sourced, and we follow their license and terms of use. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our training code are open-source on GitHub. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}]