[{"figure_path": "XYDMAckWMa/tables/tables_6_1.jpg", "caption": "Table 1: Correspondence between some methods which can reduced to FM framework and our theoretical descriptions of them.", "description": "This table summarizes several existing methods related to the Flow Matching (FM) framework and provides explicit expressions for their vector fields and scores.  It shows how different probability paths and their corresponding choices of q(z) and \u03bct(z) lead to variations in the vector field and score functions,  highlighting the relationships between the proposed Explicit Flow Matching (ExFM) method and the existing techniques. The table helps clarify the theoretical connections and differences between ExFM and related approaches.", "section": "2 Main idea"}, {"figure_path": "XYDMAckWMa/tables/tables_7_1.jpg", "caption": "Table 2: ExFM and CFM metrics comparison table on toy 2D data.", "description": "This table presents a comparison of the performance of Explicit Flow Matching (ExFM) and Conditional Flow Matching (CFM) on eight different 2D toy datasets.  The metrics used for comparison are MSE training loss and Energy Distance. The table shows that ExFM consistently achieves lower values for both metrics, indicating its superior performance in terms of training efficiency and model accuracy.", "section": "3 Numerical Experiments"}, {"figure_path": "XYDMAckWMa/tables/tables_7_2.jpg", "caption": "Table 3: NLL comparison for ExFM, CFM and OT-CFM methods over 10 000 learning steps, mean and std taken from 10 sampling iterations.", "description": "This table presents a comparison of the negative log-likelihood (NLL) values achieved by three different methods: ExFM, CFM, and OT-CFM.  The comparison is made across five tabular datasets after 10,000 learning steps. For each dataset and method, the mean and standard deviation of NLL values calculated from 10 sampling iterations are reported. This allows for an assessment of the relative performance of each method in terms of likelihood.", "section": "Numerical Experiments"}, {"figure_path": "XYDMAckWMa/tables/tables_8_1.jpg", "caption": "Table 4: ExFM-S evaluation on four toy datasets (\u03bc \u00b1 \u03c3 over three seeds). For comparison we take I-CFM, OT-CFM, and ExFM (no values for moons \u2192 8gaussians due to the absence of explicit formula for po). Performance in generative modeling (W2) and dynamic OT optimality (NPE) is assessed. The best result for each metric is highlighted in bold. Instances where we outperform CFM are underscored.", "description": "This table presents a comparison of the performance of the stochastic version of ExFM (ExFM-S) against other methods (I-CFM, OT-CFM, and ExFM) on four 2D toy datasets.  The metrics used are the 2-Wasserstein distance (W2) and the optimal transport plan evaluation (NPE). Results are shown as mean \u00b1 standard deviation over three random seeds. ExFM-S shows competitive performance across all metrics and datasets, outperforming other methods in several cases. The absence of results for ExFM in the 'moons \u2192 8gaussians' case is due to the lack of an explicit formula for po (the initial probability density).", "section": "Stochastic ExFM (ExFM-S) on toy 2D data"}, {"figure_path": "XYDMAckWMa/tables/tables_32_1.jpg", "caption": "Table 5: Learning parameters for Tabular datasets.", "description": "This table presents the hyperparameters used for training the models on five tabular datasets: POWER, GAS, HEPMASS, BSDS300, and MINIBOONE.  For each dataset, it specifies the architecture of the Multilayer Perceptron (MLP) used, indicated by the number of neurons in each hidden layer, and the learning rate (LR) employed during training.  These hyperparameters were chosen based on the characteristics of each dataset and the computational resources available to optimize performance.", "section": "H.2 Tabular"}, {"figure_path": "XYDMAckWMa/tables/tables_32_2.jpg", "caption": "Table 6: NLL comparison for ExFM, CFM and OT-CFM methods over 10 000 learning steps, mean and std taken from 10 sampling iterations.", "description": "This table presents a comparison of the negative log-likelihood (NLL) values obtained using three different methods: ExFM, CFM, and OT-CFM.  The results are averaged over 10 sampling iterations and show the mean and standard deviation of the NLL after 10,000 learning steps.  The comparison is done across five different datasets: POWER, GAS, HEPMASS, BSDS300, and MINIBOONE. Lower NLL values indicate better performance.", "section": "3 Numerical Experiments"}, {"figure_path": "XYDMAckWMa/tables/tables_33_1.jpg", "caption": "Table 1: Correspondence between some methods which can reduced to FM framework and our theoretical descriptions of them.", "description": "This table summarizes several existing methods that can be expressed within the Flow Matching (FM) framework.  It provides a comparison between these methods, highlighting the explicit expressions for the vector field (VF) and score (S) derived in the paper.  This allows for a direct comparison between the proposed Explicit Flow Matching (ExFM) method and other approaches in terms of their theoretical underpinnings and analytical tractability. The VF and S components are key to understanding the efficiency and convergence properties of different flow-based generative models.", "section": "2 Main idea"}, {"figure_path": "XYDMAckWMa/tables/tables_34_1.jpg", "caption": "Table 8: FID comparison for ExFM, CFM and OT-CFM methods over 400 000 learning steps, mean and std taken from 4 sampling iterations.", "description": "This table presents a comparison of the Fr\u00e9chet Inception Distance (FID) scores achieved by three different methods: ExFM, CFM, and OT-CFM. The FID score is a metric used to evaluate the quality of generated images by comparing them to real images. Lower FID scores indicate better image quality. The table shows the FID scores at different steps (iterations) of the training process.  The results demonstrate the performance of each method over a long training period.", "section": "H.3 ExFM-S evaluation"}]