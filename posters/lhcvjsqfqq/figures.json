[{"figure_path": "lHcvjsQFQq/figures/figures_4_1.jpg", "caption": "Figure 1: Illustration of soft TV-distance. (left) f functions, (right) corresponding derivatives f'.", "description": "This figure illustrates the soft TV-distance, a modification of the total variation (TV) distance used to address the lack of an invertible derivative in the original TV distance.  The left panel shows a comparison of the f functions for the TV distance and the proposed soft TV-distance. The right panel shows a comparison of their corresponding derivatives (f'). The soft TV-distance uses a smooth approximation of the TV distance's derivative, making it suitable for use in the DrilDICE algorithm.", "section": "3.3 Soft TV-Distance"}, {"figure_path": "lHcvjsQFQq/figures/figures_5_1.jpg", "caption": "Figure 2: Four Rooms environment and a deterministic expert (red arrows).", "description": "This figure shows the Four Rooms environment used in the experiments. The environment is a grid world with four distinct rooms connected by doorways. The goal is to navigate from a starting location (orange square) to a goal location (green square). The red arrows indicate the path taken by the deterministic expert policy used to collect the expert demonstration dataset.", "section": "4.2 Toy Domain Experiment: Four Rooms domain with Imbalanced Datasets"}, {"figure_path": "lHcvjsQFQq/figures/figures_6_1.jpg", "caption": "Figure 3: Visualizations of policy behaviors and weights. (a) dp(s) : dataset state distribution, (b) d\u03c0\u03b5 (s) : behavior of BC policy, (c) d(s)w*be(s): corrected state distribution that maximizes C\u03c0, (d) behavior of drnew (s): DrilDICE policy (BC weighted with w*be(s)).", "description": "This figure visualizes the state distributions and policy behaviors of different methods in the Four Rooms environment. Panel (a) shows the state distribution of the dataset (dp(s)). Panel (b) shows the state distribution induced by the behavioral cloning (BC) policy (d\u03c0E(s)). Panel (c) presents the corrected state distribution (d(s)w*be(s)) obtained by weighting the original dataset distribution using the weights (w*be(s)) derived from the DrilDICE algorithm. Finally, panel (d) shows the state distribution of the DrilDICE policy (d\u03c0new(s)), which is a result of behavioral cloning with the weights from DrilDICE.", "section": "4.2 Toy Domain Experiment: Four Rooms domain with Imbalanced Datasets"}, {"figure_path": "lHcvjsQFQq/figures/figures_8_1.jpg", "caption": "Figure 4: Illustrative examples for generating datasets for Scenario 2 and 3.", "description": "This figure illustrates how datasets are generated for scenarios 2 and 3 in the paper. Scenario 2 simulates time-dependent data collection, where data is collected with varying frequencies at different timesteps. This is shown in subfigure (a) where the x-axis represents timesteps and the y-axis represents the counts of collected data at each timestep, with different colored bars representing different parameter combinations. Scenario 3 simulates segmented trajectory data collection, where only short segments of the trajectories are collected instead of complete trajectories.  Subfigure (b) shows this by representing the trajectory indices on the y-axis, and timesteps on the x-axis. The dark purple blocks indicate the segments of trajectories collected for training.", "section": "4.4 Covariate Shift Scenarios"}, {"figure_path": "lHcvjsQFQq/figures/figures_9_1.jpg", "caption": "Figure 5: Performance comparison on Scenario 3 (segmented dataset) along the number of segments. The points and shaded areas indicate the means and standard errors measured over 5 repetitions.", "description": "This figure compares the performance of different imitation learning methods (BC, DemoDICE, AW-BC, DR-BC, OptiDICE-BC, and DrilDICE) on the segmented dataset scenario of the D4RL benchmark. The x-axis represents the number of segments used, and the y-axis shows three different metrics: normalized score, worst-25% performance, and target MSE.  The shaded areas represent standard errors from 5 repetitions, showing the variability of the results. The figure helps to understand how the performance of each method changes as the length of the trajectories used for training changes.  DrilDICE generally shows better performance with more segments.", "section": "4.4.2 Results"}, {"figure_path": "lHcvjsQFQq/figures/figures_13_1.jpg", "caption": "Figure A: Example of [21]. Table A: Policies and their corresponding state distributions.", "description": "This figure shows an example MDP with three states (S1, S2, S3) and two actions (a1, a2).  The expert policy (\u03c0E) and an imitator policy (\u03c0^) are defined, along with their corresponding state distributions (dE, d\u03c0). The transition probabilities are deterministic.  The figure also illustrates a data distribution (dD1) that differs from the expert's distribution, highlighting a covariate shift scenario. This example is used to demonstrate how the standard behavioral cloning approach's loss minimization does not guarantee a reduction in the performance gap between the expert and the imitator.", "section": "A Suboptimality Bounds with Arbitrary State Distributions"}, {"figure_path": "lHcvjsQFQq/figures/figures_14_1.jpg", "caption": "Figure B: Visualization of f functions and derivatives f' corresponding to f-divergence choices. We also visualize corresponding w*(e) = max((f')-1(e), 0), which is a closed form solution of the inner maximization in Eq. 7.", "description": "This figure visualizes the f-divergence functions (left), their derivatives (middle), and the closed-form solution of the inner maximization problem (right). Different f-divergence choices (KL, \u03c7\u00b2, soft \u03c7\u00b2, TV, soft TV) are compared.  The derivatives are important because they are used to obtain the closed-form solution. The closed-form solution is a crucial part of the DrilDICE algorithm for efficiently solving the optimization problem.", "section": "Summary of f-Divergence Choices"}, {"figure_path": "lHcvjsQFQq/figures/figures_16_1.jpg", "caption": "Figure C: Performance comparison on complete trajectory scenarios. The points/shaded areas indicate the means/standard errors measured over 5 repetitions.", "description": "This figure compares the performance of different imitation learning methods (BC, DemoDICE, AW-BC, DR-BC, OptiDICE-BC, and DrilDICE) on complete expert trajectories with varying numbers of trajectories (1, 5, 10, 50).  The y-axis represents the normalized score, and the x-axis shows the number of trajectories used in training.  The shaded areas represent the standard errors of the mean, indicating the variability in performance across multiple repetitions of the experiment. The results show DrilDICE achieves superior performance and data efficiency compared to other methods, especially with smaller datasets.", "section": "E.1 Performance comparison on complete trajectories"}, {"figure_path": "lHcvjsQFQq/figures/figures_16_2.jpg", "caption": "Figure 5: Performance comparison on Scenario 3 (segmented dataset) along the number of segments. The points and shaded areas indicate the means and standard errors measured over 5 repetitions.", "description": "This figure compares the performance of different imitation learning methods (BC, DemoDICE, AW-BC, OptiDICE-BC, DR-BC, and DrilDICE) on the segmented dataset in Scenario 3.  The x-axis represents the number of segments used in training, and the y-axis shows the normalized score and target MSE. The points represent the average performance over 5 repetitions, and the shaded areas show the standard error. The results demonstrate how the performance of each method changes as the number of segments increases.", "section": "4.4.2 Results"}, {"figure_path": "lHcvjsQFQq/figures/figures_17_1.jpg", "caption": "Figure 5: Performance comparison on Scenario 3 (segmented dataset) along the number of segments. The points and shaded areas indicate the means and standard errors measured over 5 repetitions.", "description": "This figure compares the performance of different imitation learning methods (BC, DemoDICE, AW-BC, DR-BC, OptiDICE-BC, and DrilDICE) on the segmented dataset scenario of Scenario 3 from the paper.  The x-axis represents the number of segments used in training, and the y-axis shows three performance metrics: normalized score, worst-25% performance, and target MSE. The points represent the average performance over 5 repetitions, and the shaded area shows the standard error. The figure demonstrates how the performance of each method changes as the number of segments increases, highlighting the relative effectiveness of DrilDICE across different metrics and dataset conditions.", "section": "4.4.2 Results"}, {"figure_path": "lHcvjsQFQq/figures/figures_18_1.jpg", "caption": "Figure 5: Performance comparison on Scenario 3 (segmented dataset) along the number of segments. The points and shaded areas indicate the means and standard errors measured over 5 repetitions.", "description": "This figure compares the performance of different algorithms (BC, DR-BC, OptiDICE-BC, and DrilDICE) across three metrics (normalized score, worst-25% performance, and target MSE) as the number of segments in the dataset varies.  Each algorithm is tested on three different continuous control tasks (hopper, walker2d, and halfcheetah). The shaded areas represent standard error across 5 repetitions.  The results show that DrilDICE consistently outperforms other methods in terms of overall performance and robustness, particularly when the number of segments is low.", "section": "4.4 Results"}]