[{"type": "text", "text": "Mitigating Covariate Shift in Behavioral Cloning via Robust Stationary Distribution Correction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Seokin Seo1, Byung-Jun Lee2,3, Jongmin Lee4, HyeongJoo Hwang1, Hongseok Yang1, Kee-Eung Kim1 ", "page_idx": 0}, {"type": "text", "text": "1KAIST, 2Korea University, 3Gauss Labs Inc., 4UC Berkeley siseo@ai.kaist.ac.kr, byungjunlee@korea.ac.kr, jongmin.lee@berkeley.edu, hjhwang@ai.kaist.ac.kr, hongseok.yang@kaist.ac.kr, kekim@kaist.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider offline imitation learning (IL), which aims to train an agent to imitate from the dataset of expert demonstrations without online interaction with the environment. Behavioral Cloning (BC) has been a simple yet effective approach to offilne IL, but it is also well-known to be vulnerable to the covariate shift resulting from the mismatch between the state distributions induced by the learned policy and the expert policy. Moreover, as often occurs in practice, when expert datasets are collected from an arbitrary state distribution instead of a stationary one, these shifts become more pronounced, potentially leading to substantial failures in existing IL methods. Specifically, we focus on covariate shift resulting from arbitrary state data distributions, such as biased data collection or incomplete trajectories, rather than shifts induced by changes in dynamics or noisy expert actions. In this paper, to mitigate the effect of the covariate shifts in BC, we propose DrilDICE, which utilizes a distributionally robust BC objective by employing a stationary distribution correction ratio estimation (DICE) to derive a feasible solution. We evaluate the effectiveness of our method through an extensive set of experiments covering diverse covariate shift scenarios. The results demonstrate the efficacy of the proposed approach in improving the robustness against the shifts, outperforming existing offline IL methods in such scenarios. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Imitation learning (IL) aims to recover the expert behavior from the dataset of demonstrations. The standard IL setting assumes that the imitator is allowed to interact with the environment during training, as it provides valuable information regarding state transitions. On the other hand, offline IL requires training without online interactions, reflecting scenarios where the interactions are either infeasible or expensive [5, 12, 13, 28]. Despite recent works on offline IL that explore scenarios involving supplementary datasets, such as suboptimal demonstrations [5, 14], behavioral cloning (BC) remains a compelling option in practice since BC does not require additional datasets except expert demonstrations. However, the efficacy of BC can be compromised when the imitator policy\u2019s behavior deviates from the underlying data distribution. This phenomenon, known as covariate shift, presents a significant challenge, often resulting in performance degradation. Consequently, a substantial body of IL research is dedicated to addressing this covariate shift issue [5, 24]. ", "page_idx": 0}, {"type": "text", "text": "A common approach for solving offline IL problems is to use the so-called distribution matching objective or its variant [11, 12, 14, 15, 31], which aims to align the stationary distribution of the imitator policy with that of the expert policy. However, this approach crucially assumes that expert demonstrations are sampled from the stationary distribution of the expert policy. In practice, this assumption may break, that is, sampled expert demonstrations may be from a shifted version of the stationary distribution. For instance, expert demonstrations may be collected by first sampling states from a distribution that is different from the stationary distribution of the expert policy, and then labeling sampled states with expert actions. In this case, expert demonstrations are not samples from the stationary distribution. As a result, the offilne IL algorithms based on the distribution-matching approach might perform badly in this covariate shift case. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we address the issue of the covariate shift in the dataset of expert demonstrations, particularly when the data distribution does not match the stationary distribution of the expert policy. We begin by arguing that BC objective is more natural to consider the shift than the distribution matching objective. Then, inspired by the principle of distributionally robust optimization [2, 6, 20, 25], we propose an adversarial objective for offline IL that addresses covariate shifts in BC training. Instead of simply considering the worst-case with respect to all possible distributions on state-action pairs, our objective considers only those distributions that arise as the stationary distributions of policies, i.e., distributions that satisfy the Bellman flow constraint. Leveraging the techniques from the stationary distribution correction ratio (DICE) algorithm, we introduce DrilDICE, which efficiently solves our optimization problem and computes an imitator robust to covariate shifts of the dataset. In addition, we suggest the practical covariate shift scenarios that may arise in offline IL applications. Under those problem settings, we compare our approach with baselines and demonstrate that our approach can imitate the agent robust to covariate shift of our interests. ", "page_idx": 1}, {"type": "text", "text": "2 Background and Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Offline Imitation Learning with Arbitrary State Distributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider a Markov decision process (MDP) without rewards, which is defined as a tuple of $\\langle S,A,T,p_{0},\\gamma\\rangle$ where $\\boldsymbol{S}$ is a state space, $\\boldsymbol{\\mathcal{A}}$ is an action space, $T:S\\times A\\to\\Delta(S)$ is a transition distribution, $p_{0}\\,\\in\\,\\Delta(S)$ is an initial state distribution and $\\gamma\\,\\in\\,[0,1)$ is a discounted factor. We focus on the class of deterministic policies $\\Pi\\,=\\,\\{\\pi\\,:\\,S\\,\\to\\,{\\dot{A}}\\}$ . Given a policy $\\pi\\,\\in\\,\\Pi$ , the stationary distribution $d_{\\pi}$ of $\\pi$ is defined as $\\begin{array}{r}{d_{\\pi}(s,a)\\stackrel{*}{=}(1-\\gamma)\\sum_{t=0}^{\\infty}\\gamma^{t}\\operatorname*{Pr}(s_{t}\\stackrel{.}{=}s,\\dot{a}_{t}=a)}\\end{array}$ where $s_{0}\\sim p_{0},a_{t}=\\pi(s_{t}),s_{t+1}\\sim\\mathcal{T}(s_{t},a_{t})$ . For convenience, we use $\\pi_{E}$ to denote the policy of an expert and write simply $d_{E}$ for the stationary distribution $d_{\\pi_{E}}$ of $\\pi_{E}$ . ", "page_idx": 1}, {"type": "text", "text": "Assuming the existence of an expert policy $\\pi_{E}\\in\\Pi$ , the goal of imitation learning (IL) is to recover $\\pi_{E}$ by utilizing some demonstrations of the expert. Specifically, offilne $\\mathrm{IL}$ prohibits online interactions and relies solely on a given offilne dataset that consists of demonstrations of the expert. We consider that the demonstration dataset $\\mathcal{D}$ is the collection of $(s,a,s^{\\prime})$ triplets where $s$ is sampled from an arbitrary state distribution $d_{D}(s)$ , $a$ is determined by $\\pi_{E}(s)$ and $s^{\\prime}$ is sampled from $T(\\bar{s}^{\\prime}|s,a)$ . Note that conventional $\\mathrm{IL}$ approaches assume that $d_{D}$ is close to the expert\u2019s state distribution $d_{E}$ . However, unlike previous studies, we focus the imitation learning with an arbitrary state distribution $d_{D}$ , without assuming that $d_{D}$ to be a state stationary distribution of any policy. ", "page_idx": 1}, {"type": "text", "text": "As we mentioned in the introduction, one common approach for offline $\\mathrm{IL}$ is to use the distribution matching objective or its variant [11, 12, 14, 15] and to find an imitator policy whose stationary distribution matches that of the expert policy. But as we will show later in the paper, the algorithms based on this approach do not perform well when there is a covariate shift in the given demonstration dataset $d_{D}(s,a)$ in a way explained in the previous paragraph. In the paper, we propose an offilne $\\mathrm{IL}$ algorithm that achieves good performance in the presence of such a covariate shift by using a version of distributionally robust optimization. ", "page_idx": 1}, {"type": "text", "text": "2.2 Covariate Shift in Imitation Learning ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Covariate shift is a widely used term in machine learning [1, 7]. Traditionally, it refers to the phenomenon where the distribution of covariates (inputs) during training differs from that during testing, i.e. given a covariate variable $X\\in\\mathcal{X}$ and a response variable $Y\\in\\mathcal{V}$ , ", "page_idx": 1}, {"type": "equation", "text": "$$\np_{\\mathrm{train}}(Y|X=x)=p_{\\mathrm{test}}(Y|X=x)\\quad\\forall x\\in\\mathcal{X},\\qquad p_{\\mathrm{train}}(X)\\neq p_{\\mathrm{test}}(X).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The distribution shift that is considered in this paper has this form where state $s$ and action $a$ correspond to $X$ and $Y$ , and $p_{\\mathrm{train}}$ and $p_{\\mathrm{{test}}}$ correspond to the distribution $d_{D}$ of the given demonstration dataset and the stationary distribution $d_{E}$ of the expert policy $\\pi_{E}$ . ", "page_idx": 1}, {"type": "text", "text": "One significant branch of research addressing the issue of covariate shift is distributionally robust optimization (DRO). DRO involves considering an uncertainty set of the distribution and optimizing for the worst-case loss within this set [6, 20]. Recent studies have proposed method to attain the worstcase supremum by using kernel methods [25] or the integral probability metric such as Wasserstein metric [10]. Additionally, in the field of imitation learning, DRO has been applied to achieve robust learning in the presence of noisy experts [2] or transition shifts [19]. ", "page_idx": 2}, {"type": "text", "text": "A closely related problem studied in $\\mathrm{IL}$ is robust imitation learning, which focuses on learning a policy that can tolerate various shifts in the transition distribution $T$ of the MDP [4, 19]. Another related scenario is the noisy expert scenario, where the demonstration dataset is corrupted by noise, resulting in the distribution $d_{D}$ of the dataset being a noisy version of $d_{E}$ [6, 23]. These issues are usually addressed by using robust optimization techniques [6, 19], which are also utilized in our approach. However, it is important to note that the types of distribution shifts considered in our work and in prior researches differ within the context of offline IL. ", "page_idx": 2}, {"type": "text", "text": "To emphasize, we are not focused on addressing covariate shifts induced by transition shifts or noisy demonstrations. Instead, our work investigates a type of covariate shift where the dataset still consists of expert actions, but states are not sampled from the stationary distribution of the expert policy: the distribution $d_{D}(s)$ of the samples in the given demonstration dataset differs from $\\bar{d}_{E}(s)$ , but both distributions share the expert policy given states, i.e., $\\pi_{D}(s)=\\pi_{E}(s)$ for all $s\\in S$ . ", "page_idx": 2}, {"type": "text", "text": "3 Mitigating Covariate Shift in BC ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Robustness to Covariate Shift ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Assume that we have a free access to $\\pi_{E}$ , such that we can optimize our trained policy $\\pi$ to be close to it on any state. We can write the BC objective for optimizing $\\pi$ as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi}\\;\\mathbb{E}_{s\\sim d_{E}}[\\ell(\\pi(s),\\pi_{E}(s))]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for some supervised learning loss $\\ell$ (e.g. squared error, cross-entropy, ...). Previous studies on imitation learning have assumed that $d_{D}\\approx d_{E}$ . Under this assumption, BC approach of: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi}\\mathbb{E}_{s\\sim d_{D}}[\\ell(\\pi(s),\\pi_{E}(s))]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "does show strong performance if we have sufficient data. Otherwise, if the expert data is not sufficient, we can also incorporate transition information and benefit from the distribution matching (DM) approach, which aims to get a stationary distribution $d_{\\mathrm{DM}}$ that is close to data distribution $d_{D}$ with the following objective: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l}{d_{\\mathrm{DM}}:=\\displaystyle\\arg\\operatorname*{max}_{d\\in\\Delta(S)}\\,-\\mathbb{D}(d(s,a)||d_{D}(s,a))}\\\\ {\\mathrm{~s.~t.~}\\displaystyle\\sum_{a}d(s,a)=(1-\\gamma)\\rho_{0}(s)+\\gamma\\displaystyle\\sum_{\\bar{s},\\bar{a}}T(s|\\bar{s},\\bar{a})d(\\bar{s},\\bar{a})\\quad\\forall s\\in\\mathcal{S}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbb{D}$ is any divergence between two probability distributions. The solution of the optimization problem $d_{\\mathrm{DM}}$ can be used for the state distribution for BC instead of $d_{E}$ , and it is widely known that this can complement lack of data with additional transition information, leading to improved performance. Nevertheless, under harsh covariate shift, i.e., when the state distribution of $d_{D}$ diverges from that of $d_{E}$ , both two approaches above can fail miserably, as there is no guarantee on improving policy performance when $d_{E}$ and $d_{D}$ are significantly different. See Section A for an example of failure cases. ", "page_idx": 2}, {"type": "text", "text": "To overcome the covariate shift, we adopt a distributionally robust objective, i.e., for an arbitrary distribution $d$ , we know $\\begin{array}{r}{\\mathbb{E}_{s\\sim d_{E}}[\\ell(\\pi(s),\\bar{\\pi}_{E}(s))]\\le\\operatorname*{max}_{d\\in\\bar{\\Delta}(S)}\\mathbb{E}_{s\\sim d}[\\ell(\\pi_{E}(s),\\pi(s))]}\\end{array}$ , which holds with $d_{\\pi}$ as well, and we can instead optimize for ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi}\\operatorname*{max}_{d\\in\\mathcal{Q}}\\mathbb{E}_{s\\sim d}[\\ell(\\pi(s),\\pi_{E}(s))],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "which corresponds to an upper-bound minimization for optimizing $\\pi$ with respect to some uncertainty set $\\mathcal{Q}\\subseteq\\Delta(S)$ . However, when $\\mathcal{Q}$ is choosed as $\\Delta(S)$ , this objective can easily lead to overly pessimistic solutions, as we seek for any extreme state distribution $d$ that maximizes the policy loss, and it will easily place all its probability on a single state. Hence, a choice of $\\mathcal{Q}$ is crucial to obtain an appropriate solution of distributionally robust optimization. ", "page_idx": 2}, {"type": "text", "text": "3.2 Towards Less Pessimistic Robust Objective ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "For notational convenience, denote $C_{\\pi}(s):=\\ell(\\pi(s),\\pi_{E}(s))$ . In the objective (3), the worst-case policy loss is considered among an uncertainty set $\\mathcal{Q}$ . However, when $\\mathcal{Q}=\\Delta(S)$ , it would be too pessimistic and the objective can be improved when we consider a smaller choice of $\\mathcal{Q}$ . To keep the fact that it is an upper-bound minimization for Eq. 1, which is the loss with $d_{E}$ , the expert state distribution $d_{E}$ should still be contained in $\\mathcal{Q}$ , even if we aim to use a smaller set. ", "page_idx": 3}, {"type": "text", "text": "To facilitate this, we impose two constraints on $\\mathcal{Q}$ with respect to a state distribution $d$ : (1) $d$ should satisfy Bellman flow constraint, (2) $d$ should be close enough to $d_{D}$ to prevent $d$ diverging too far from $\\dot{d}{_D}^{1}$ . The first constraint is a constraint that should be satisfied by $d_{E}$ , and thus it effectively reduces the potential distribution set for $d$ if we impose it properly. The second constraint may not seem necessary, but as we estimate the policy loss from finite samples from $d_{D}$ , diverging too far from $d_{D}$ will reduce effective number of samples. Then, by introducing $f$ -divergence regularization to enforce the second constraint, we consider the following constrained optimization problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{min}_{\\pi}}&{\\displaystyle\\operatorname*{max}_{d\\in\\Delta(S)}~~\\mathbb{E}_{s\\sim d}[C_{\\pi}(s)]-\\alpha{\\mathbb D}_{f}(d(s,a)||d_{D}(s,a))}\\\\ {\\mathrm{s.t.}}&{\\displaystyle\\sum_{a}d(s,a)=(1-\\gamma)\\rho_{0}(s)+\\gamma\\displaystyle\\sum_{\\bar{s},\\bar{a}}T(s|\\bar{s},\\bar{a})d(\\bar{s},\\bar{a})\\quad\\forall s\\in S}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbb{D}_{f}$ denotes $f$ -divergence with a convex function $f$ . We develop a practical algorithm in a stationary distribution correction ratio estimation (DICE) framework style. From Eq. 4, we can take Lagrangian $\\nu$ to solve the inner constrained optimization, i.e. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\underset{d\\in\\Delta(S)}{\\operatorname*{max}}\\,\\underset{\\nu}{\\operatorname*{min}}\\sum_{s,a}d(s,a)C_{\\pi}(s)-\\alpha\\mathbb{D}_{f}(d(s,a)||d_{D}(s,a))}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\displaystyle\\sum_{s}\\nu(s)[(1-\\gamma)\\rho_{0}(s)+\\gamma\\displaystyle\\sum_{\\bar{s},\\bar{a}}T(s|\\bar{s},\\bar{a})d(\\bar{s},\\bar{a})-\\displaystyle\\sum_{a}d(s,a)]}\\\\ &{=\\displaystyle\\underset{w\\in\\mathcal{W}}{\\operatorname*{max}}\\,\\underset{\\nu}{\\operatorname*{min}}(1-\\gamma)\\mathbb{E}_{s\\sim\\rho_{0}}[\\nu(s)]+\\mathbb{E}_{(s,a,s^{\\prime})\\sim d_{D}}\\left[-\\alpha f\\left(w(s,a)\\right)+w(s,a)e_{\\pi,\\nu}(s,a,s^{\\prime})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{e_{\\pi,\\nu}\\::=\\:(C_{\\pi}(s)+\\gamma\\sum_{s^{\\prime}}T(s^{\\prime}|s,a)\\nu(s^{\\prime})-\\nu(s)).}\\end{array}$ , $\\begin{array}{r}{w(s,a)\\,:=\\,\\frac{d(s,a)}{d_{D}(s,a)}}\\end{array}$ and $\\mathcal{W}$ is a class of functions $w:S\\times A\\to\\mathbb{R}_{\\geq0}$ . ", "page_idx": 3}, {"type": "text", "text": "By Slater\u2019s condition [3], since the strong duality holds on Eq. 5, we can change the minimax into a maximin optimization as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\nu}\\operatorname*{max}_{w\\in\\mathcal{W}}(1-\\gamma)\\mathbb{E}_{s\\sim\\rho_{0}}[\\nu(s)]+\\mathbb{E}_{(s,a,s^{\\prime})\\sim d_{D}}\\left[-\\alpha f\\left(w(s,a)\\right)+w(s,a)e_{\\pi,\\nu}(s,a,s^{\\prime})\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and the closed-form solution $w^{*}$ can be obtained by solving for the inner maximization. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. (Lee et al. [16]) The closed-form solution for the inner maximization of Eq. (6) is ", "page_idx": 3}, {"type": "equation", "text": "$$\nw_{\\pi,\\nu}^{*}(s,a)=\\operatorname*{max}\\left(0,(f^{\\prime})^{-1}\\left(\\frac{e_{\\pi,\\nu}(s,a)}{\\alpha}\\right)\\right)\\quad\\forall s,a\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "After obtaining $w_{\\pi,\\nu}^{*}$ , the minimax optimization (6) is converted into a minimization problem with respect to $\\nu$ . As a result, each objective for $\\nu,\\pi$ can be summarized as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\nu}{\\operatorname*{min}}}&{(1-\\gamma)\\mathbb{E}_{s\\sim\\rho_{0}}[\\nu(s)]+\\mathbb{E}_{(s,a,s^{\\prime})\\sim d_{D}}\\left[-\\alpha f\\left(w_{\\pi,\\nu}^{*}(s,a)\\right)+w_{\\pi,\\nu}^{*}(s,a)e_{\\pi,\\nu}(s,a,s^{\\prime})\\right]}\\\\ {\\underset{\\pi}{\\operatorname*{min}}}&{\\mathbb{E}_{(s,a)\\sim d_{D}}\\left[w_{\\pi,\\nu}^{*}(s,a)C_{\\pi}(s)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Hence, the problem can be practically addressed by alternatively optimizing $\\nu$ and $\\pi$ following recent developments in DICE approaches [16, 17]. We call this distributionally robust optimization approach to covariate shift in offilne $\\mathrm{IL}$ as DrilDICE (Distributionally Robust Imitation Learning via DICE). ", "page_idx": 3}, {"type": "image", "img_path": "lHcvjsQFQq/tmp/1cf69927cc3754c7753f431142806a42f9b11e1618b35357cc63aa9872903788.jpg", "img_caption": ["Figure 1: Illustration of soft TV-distance. (left) $f$ functions, (right) corresponding derivatives $f^{\\prime}$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.3 Soft TV-Distance ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To facilliate a fair comparison with the uncertainty set defined by total variation (TV) distance, as utilized in [19], we introduce the soft-TV distance as a specific choice of $f$ -divergence of DrilDICE. It is important to note that Proposition 1 requires $(f^{\\prime})^{-1}$ , the invertable derivative of $f$ . However, the generator function of TV-distance $f_{\\mathrm{TV}}(x):={\\textstyle\\frac{1}{2}}|x-1|$ , lacks an invertible derivative as illustrated in Figure 1, rendering the direct application of TV distance in DrilDICE challenging. ", "page_idx": 4}, {"type": "text", "text": "We technically overcome this limitation by relaxing the derivative function of TV-distance. Given that the derivative function of TV-distance manifests as a step-function, we choose to relax this function by employing the tanh function. Specifically, we utilize the log-cosh function [22] to $f$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{\\mathrm{SoffV}}(x)=\\frac{1}{2}\\log(\\cosh{(x-1)}),\\quad(f_{\\mathrm{SoffV}}^{\\prime})^{-1}(y)=\\operatorname{tanh}^{-1}{(2y)}+1.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "By plugging $f_{\\mathrm{SoftTV}}^{\\prime}$ into Eq. 7, we can obtain the closed-form solution $w_{\\pi,\\nu}^{*}$ of the inner maximization objective (6) tailored to a specific choice of $f_{\\mathrm{Soft-TV}}$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nw_{\\pi,\\nu}^{*}(s,a)=\\mathrm{ReLU}\\left(\\operatorname{tanh}^{-1}\\left(\\frac{2e_{\\pi_{\\nu}}(s,a,s^{\\prime})}{\\alpha}\\right)+1\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathrm{ReLU}(x):=\\operatorname*{max}(x,0)$ . This choice of $f$ enables DrilDICE to obtain a closed form solution of inner maximization problem while maintaining similar properties of TV-distance. For other possible choice of $f$ -divergence in DrilDICE, see Section B in the supplementary material. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Comparison on Baselines Objectives ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To provide clear descriptions about the baselines used for comparison in our experiments, we summarize the relevant objectives in Table 1. ", "page_idx": 4}, {"type": "text", "text": "Adversarial Weighted BC (AW-BC) We refer to adversarial weighting by following the terminology from [27] as the minimax objective without constraints. As we discussed in Section 3.2, the adversarial weighting objective is also an upper-bound of the target objective. However, it tends to be overly pessimistic because it considers the entire state distribution space, including distributions that do not correspond to the stationary distribution of any policy. ", "page_idx": 4}, {"type": "text", "text": "Distributionally Robust BC (DR-BC) To adjust the level of the robustness, DR-BC [19] can be employed in this context. Despite originally designed to address transition shifts, DR-BC remains relevant to our scenario since it considers the uncertainty set over state distributions by adopting a robustness radius hyperparameter $\\rho$ . Notably, the $f$ -divergence constraint of DrilDICE is functionally analogous to the robustness radius constraint $D_{\\mathrm{TV}}(d\\|d_{D})\\,\\le\\,\\rho$ . However, our approach, which incorporates Bellman flow constraints, addresses a more restricted set when constrained by the equivalent radius level, thereby offering a tighter upper-bound to Eq. 1. ", "page_idx": 4}, {"type": "table", "img_path": "lHcvjsQFQq/tmp/a16859729e61d12ad4677ce682a5bfa8cf38f2b5f53982bf4e290433fe11b57c.jpg", "table_caption": [], "table_footnote": ["Table 1: Objective comparisons for related approaches. Denote a stationary distributions class as $\\bar{\\mathcal{Q}}$ , i.e., $\\begin{array}{r}{\\bar{\\mathcal{Q}}:=\\lbrace\\bar{d}\\in\\Delta(S):\\dot{\\cdot}d(s)=(1-\\gamma)\\rho_{0}(s)\\dot{+}\\gamma\\sum_{\\bar{s},\\bar{a}}T(s|\\bar{s},\\bar{a})d(\\bar{s},\\bar{a})\\forall s\\in S\\rbrace}\\end{array}$ "], "page_idx": 5}, {"type": "text", "text": "Best-case Weighting If the sign of the cost function in the worst-case weighting objective is converted, the objective seeks to find a cost-minimizing stationary distribution that is close to the data-collecting policy. We call this objective as the best-case weighting. When $d_{E}\\in\\mathcal{Q}$ , the best-case weighting minimize the lower bound of the target objective since $\\mathrm{min}_{d\\in\\mathcal{Q}}\\,\\mathbb{E}_{d}[C_{\\pi}(s)]\\leq\\mathbb{E}_{d_{E}}[C_{\\pi}(s)]$ , which is not relevant to minimize the target objective. Despite there is no direct connection, to ensure that performance of our method are not due to side effects of algorithm\u2019s implementation, we include this method for performance comparison in the following experiment section. ", "page_idx": 5}, {"type": "text", "text": "4.2 Toy Domain Experiment: Four Rooms domain with Imbalanced Datasets ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.2.1 Experiment Settings ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Four Rooms environment Four Rooms is a gridworld environment which aims to find a path from a starting state to a goal state. Each cell in the grid represents a state and the starting state and the goal state are marked by the orange and the green box in Figure 2 respectively. The agent can choose one of 4 actions from each state: UP,DOWN, LEFT, RIGHT, and the rooms are numbered as shown in Figure 2. We pre-collect expert dataset $D_{E}$ by using a deterministic expert policy $\\pi_{E}$ and online interactions. ", "page_idx": 5}, {"type": "text", "text": "Covariate shift scenarios To investigate that our approach can effectively addresses the covariate shift, we need the dataset that notably deviates from the expert stationary distribution $d_{E}$ . A possible realistic scenario of the dataset deviation occurs when data collectors gather data at different frequencies for each state or action. For example, if a data collection device (e.g. cameras or sensors) operates at different recording frequencies in each room, the frequency of observing a room in the dataset will not match $d_{E}$ . To simulate this scenario, we design datasets where the marginal room (or action) distribution of dataset deviates from $d_{E}$ . By manipulating the marginal proportion of a predetermined variable in dataset, we generate a deviated dataset from the original dataset. We consider a total of 8 problem settings by manipulating the marginal distribution of four rooms and four actions. In each scenario, we set a marginal distribution $p_{i}(\\bar{u})$ of the variable to be manipulated $u$ to a certain fixed probability and resample transitions to configure the dataset $D_{i}$ through the following process. (See Section C in the supplementary material) ", "page_idx": 5}, {"type": "image", "img_path": "lHcvjsQFQq/tmp/0c5d8862972ea1327891f34459ef28db5658f1869511af488aeea1cc57f56488.jpg", "img_caption": ["Figure 2: Four Rooms environment and a deterministic expert (red arrows). "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\nD_{i}=\\{(s,a)|u\\sim p_{i}(u),s\\sim D_{E}(s|u),a=\\pi_{E}(s)\\}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Implementations and baselines Since we are not interested in trivial scenarios where the policy can easily minimize the supervised loss over entire state space to zero (i.e. $\\mathrm{max}_{s\\in S}\\,C_{\\pi}(s)=\\bar{0})$ , we conduct experiments using function approximators instead of a tabular setting. We initialize $\\pi_{0}$ for the cost function $C_{\\pi}$ as BC policy. To solve optimization problems, each algorithm is implemented with a two-step procedure: (1) optimize each objective by using the cost $C_{\\pi_{0}}$ determined by an ", "page_idx": 5}, {"type": "table", "img_path": "lHcvjsQFQq/tmp/482fed3622d36736f17da0856d8fe3a074b806bf7d8260aca078804673b7a744.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 2: Comparison of different methods across manipulated datasets in Four Rooms environment. $(p(u)=0.4)$ Each experiment is repeated with 50 times and the average values with their standard errors are reported. The best average values are highlighted in bold. ", "page_idx": 6}, {"type": "image", "img_path": "lHcvjsQFQq/tmp/85cd40a0a345b45b8affaccdad4ea90c6fa55501b42b794fe67f3d8ef6fb8bdc.jpg", "img_caption": ["Figure 3: Visualizations of policy behaviors and weights. (a) $d_{D}(s)$ : dataset state distribution, (b) $d_{\\pi_{\\mathrm{bc}}}$ (s) : behavior of BC policy, (c) $d(s)w_{\\pi\\mathrm{bc}}^{*}(s)$ : corrected state distribution that maximizes $C_{\\pi}$ , (d) behavior of $d_{\\pi_{\\mathrm{new}}}(s)$ : DrilDICE policy (BC weighted with $w_{\\pi_{\\mathrm{bc}}}^{*}(s)\\}$ ). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "initial policy $\\pi_{0}$ and obtain $w_{\\pi_{0}}^{*}$ , (2) train $\\pi$ with the weighted BC. For policy and weight models, we use linear approximators with RBF features, which exploit distances from representative points. For more details, see Section C. For choices of $f$ -divergence for OptiDICE-BC and DrilDICE, we select KL-divergence instead of the soft TV-distance for a simplicity of convex optimization solver implementation. We consider following baselines: ", "page_idx": 6}, {"type": "text", "text": "\u2022 BC: a standard behavioral cloning without any regularization.   \n\u2022 DemoDICE [14] : a representative distribution matching approach to offline IL. We compare a special case of DemoDICE that does not exploits supplementary datasets.   \n\u2022 AW-BC: adversarial weighting method without Bellman flow constraints and robustness radius.   \n\u2022 DR-BC [19]: distributionally robust BC method without Bellman flow constraints.   \n\u2022 OptiDICE-BC [16] $:$ a representrative method as the best-case weighting. ", "page_idx": 6}, {"type": "text", "text": "Evaluation metrics The following metrics are measured with 100 episodes for each trained policy. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Normalized score: a normalized episode return that scales from 0 (random score) up to 100 (expert score) averaged by 100 episodes.   \n\u2022 Worst- $.25\\%$ performance : the normalized scores averaged by the worst $25\\%$ episodes.   \n\u2022 Target 0-1 loss: the averaged 0-1 loss (i.e. $\\mathbb{E}_{d_{\\pi}}[\\mathbb{I}(\\pi(s)\\neq\\pi_{E}(s))])^{2}$ ", "page_idx": 6}, {"type": "table", "img_path": "lHcvjsQFQq/tmp/ecce5f455d5bd8eb65d79556b6fcae3bac12e9107b42ab97736b9915acb82acb.jpg", "table_caption": [], "table_footnote": ["Table 3: Performance comparison on Scenario 1 (rebalanced dataset). $p(D_{1})$ determines the proportion of dataset $D_{1}$ , which is close to the representative point. Each experiment is repeated with 5 times and the average normalized scores with their standard errors are reported. The highest mean performance scores are highlighted in bold. "], "page_idx": 7}, {"type": "text", "text": "4.3 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As depicted Table 2, DrilDICE outperformed baselines in the Four Rooms environment under various covariate shift scenarios. Our approach achieved the highest normalized scores across all scenarios and consistently demonstrated superior performance in the worst- $.25\\%$ performance. In scenarios involving Room 3, where the original dataset $D_{E}$ had only $8.9\\%$ coverage, the probability of observing states in Room 3 was increased to $40\\%$ , causing significant covariate shifts. This led to a significant degradation in the robust performance of BC (worst- $25\\%$ ). However, DrilDICE successfully improved the worst- $.25\\%$ performance in Room 3 to levels comparable to other scenarios, demonstrating its robustness to covariate shifts induced by dataset deviations. ", "page_idx": 7}, {"type": "text", "text": "Figure 3 illustrates the behaviors of BC and DrilDICE in the Room 3 scenario. With an increased dataset proportion visiting Room 3, as shown in Figure 3-(a), $\\pi_{\\mathrm{bc}}$ performs accurately in Room 3, predicting the correct expert actions in nearly all states (26 out of 27) in Room 3. However, its performance remains suboptimal in other rooms. DrilDICE, leveraging the cost derived from $\\pi_{\\mathrm{bc}}$ , computes the corrected state distribution through the worst-case weighting, denoted as $w_{\\mathrm{{bc}}}^{*}$ , as depicted in Figure 3-(c). Notably, states mispredicted by $\\pi_{\\mathrm{bc}}$ or those with high probabilities for the dataset distribution tend to obtain relatively higher weight values, contributing more to loss optimization. Consequently, DrilDICE effectively trains the agent by correcting suboptimal behaviors. ", "page_idx": 7}, {"type": "text", "text": "4.4 D4RL Dataset with Covariate-Shifted Expert Demonstrations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "4.4.1 Covariate Shift Scenarios ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To investigate our approach also benefits distributionally robust training in more complex tasks, we conduct experiments on continuous control domains. In addition, we devise three practical problem settings that could possibly occur in the real-world and simulate those scenarios by restructuring the given dataset to simulate these dataset deviation scenarios (For experiments on standard scenarios, refer to Section E). We utilize hopper-expert, walker2d-expert, halfcheetah-expert datasets included in D4RL benchmark [9] as original datasets. We utilize the soft TV-distance defined in Section 3.3 as $f$ -divergence for OptiDICE-BC and DrilDICE for these scenarios (For experiments with another choice of $f$ -divergence, see Section E.4). See Section D for implementation details. ", "page_idx": 7}, {"type": "text", "text": "Scenario 1: Rebalanced dataset From a practical perspective, a data collector may encounter scenarios in which the costs associated with taking an action or visiting a specific state exhibit substantial costs varying across different states or actions. In such scenarios, the data collected in states or actions with lower costs is likely to be observed, while those with higher costs tend to be underrepresented. To simulate these circumstances, we partition the state or action space and manipulate their mixture ratio to generate an imbalanced dataset, under assuming the costs of collecting data points in each group is significantly different, hence their proportion is shifted from the original dataset. ", "page_idx": 7}, {"type": "image", "img_path": "lHcvjsQFQq/tmp/bd4b753a665d7bd4df477f13b45b55dfe9d38c8f870bf8e4b74a8772bf54acf6.jpg", "img_caption": ["", "Figure 4: Illustrative examples for generating datasets for Scenario 2 and 3. ", ""], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "lHcvjsQFQq/tmp/53ccf72ec9eb08492f4b87e4cf4602a9d936a22639d992c065ac7194965028c8.jpg", "table_caption": [], "table_footnote": ["Table 4: Performance comparison on Scenario 2 (time-dependently collected dataset). Each experiment is repeated 5 times, and the average normalized scores with their standard errors are reported. The highest mean performance scores are highlighted in bold. "], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "In order to implement this scenario, we measure the distance between each state and the representative point of the states, then split the dataset into two groups $D_{1}$ , $D_{2}$ based on the statistics of distances: $D_{1}$ comprises states close to the point, while $D_{2}$ is consists of states farther from the point. Subsequently, we resample datasets according to a predetermined proportion of $p(D_{1})$ in $\\{0.1,0.5,0.9\\}$ by uniformly sampling different numbers of samples from each group. ", "page_idx": 8}, {"type": "text", "text": "Scenario 2: Time-dependently collected dataset One of well-known practical issues of covariate shift in supervised learning and time-series forecasting communities is dataset shift caused by seasonal or time-dependent variables [8, 18, 26]. In real-world scenarios, data collection agents affected by seasonality (e.g., temperature, humidity) can cause the data to be collected far from the expert agent\u2019s full demonstration. For instance, consider a sensor with the collecting frequency is sensitive to the temperature, making it prone to frequent breakdowns in the summer, and this sensor should collect expert data throughout the entire year. However, due to its frequent failures during the summer, the collected dataset will deviate from $d_{E}$ . ", "page_idx": 8}, {"type": "text", "text": "Motivated by these scenarios, we simulate time-dependent covariate shift scenarios by subsampling timesteps of the trajectory in extensive manners. To model the timestep sampling distribution, we utilize Beta distribution $\\dot{\\mathrm{B}}(a,b)$ . Since the support of $\\textstyle\\mathrm{\\mathrm{B}}(a,b)$ is [0, 1], with multiplying with the maximum timestep and discretization, we can sample timesteps in various shapes of distributions by adjusting parameters $a,b$ . We conduct experiments on four parameter combinations, $(a,b)\\in$ $\\{(1,\\bar{1}),(1,\\bar{5}),(5,1),(5,5)\\}$ and the timestep distribution notably varies as depicted in Figure 4a. ", "page_idx": 8}, {"type": "text", "text": "Scenario 3: Segmented trajectory dataset As similarly explored in [30], when decision-making horizons are extensively long, it is not feasible to gather multiple long trajectories keep tracking from initial states to ensure the stationarity of the expert dataset. Instead, it is more practical to gather shorter segments of expert demonstrations in such scenarios. We simulate this scenario by utilizing short segments of the pre-collected trajectories. To collect this, for each trajectory in the original dataset, we sample the starting timestep of the segment by using a subsampling method similar to ", "page_idx": 8}, {"type": "image", "img_path": "lHcvjsQFQq/tmp/c876c433eeeea18371aae2953955fe4ffda2cd35175821633f0d1812cb7d00e6.jpg", "img_caption": ["Figure 5: Performance comparison on Scenario 3 (segmented dataset) along the number of segments. The points and shaded areas indicate the means and standard errors measured over 5 repetitions. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Scenario 2. Then, we extract consequent segments with fixed-length timesteps and configure the segmented trajectory dataset similar to Figure 4b. In this scenario, we adjust the number of segments used for training to investigate the relationship between performance and the number of segments. ", "page_idx": 9}, {"type": "text", "text": "4.4.2 Results ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Scenario 1 Table 3 summarizes the performance comparison on the rebalanced datasets. From the table, we observe that DrilDICE overally outperforms other methods across different proportions and tasks and outperforms 14 out of 15 problem settings with signficant margins. DemoDICE, AW-BC and OptiDICE-BC fail to outperform BC more than half of the problem settings, which demonstrates the distribution matching and the best-case weighting are not robust to these shift scenarios. Moreover, we can conclude a robustness of DrilDICE is not gifted by side effects from the implementation of DICE algorithms at least this scenario. DR-BC, which does not incorporate Bellman flow constraints, also exhibits improved robustness, outperforming BC in 9 out of 15 problem settings. However, DrilDICE consistently surpasses DR-BC across all settings, illustrating that the inclusion of Bellman flow constraints is crucial for effectively addressing the covariate shift of our interest. ", "page_idx": 9}, {"type": "text", "text": "Scenario 2 As depicted in Table 4, DrilDICE shows robust overall performance, achieving the highest mean performance in 11 out of 12 settings. Remarkably, while all baseline methods, including DR-BC, fail to surpass BC more than half of the problem settings, only DrilDICE consistently demonstrates exceptional robustness. This emphasizes that the critical role of incorporating Bellman flow constraint to enhance robustness in scenarios involving this type of covariate shift. ", "page_idx": 9}, {"type": "text", "text": "Scenario 3 Figure 5 presents a performance comparison across three metrics while varying the number of segments. As illustrated, DrilDICE\u2019s normalized score consistently increases as the number of segments grows, and the target MSE exhibits a steady decreases maintaining levels that are lower or comparable to those of other approaches. The robust performance metrics also display consistent behavior, with an exception of walker2d task. Interestingly, we observed that a weak correlation between the target MSE and the episode return (see Figure E in the supplementary material). ", "page_idx": 9}, {"type": "text", "text": "In summary, by constraining the uncertainty set to a plausible set, DrilDICE effectively minimizes target MSE and enables BC to imitate the agent robust to various covariate shift scenarios. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusions and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose DrilDICE, a offline IL approach that is robust to the covariate shift caused by the data distribution deviated from the stationary distribution of the expert. By optimizing the Bellman-flowconstrained worst-case objective, our approach effectively minimize the surrogate loss of the expected error w.r.t non-shifted target distribution. We also suggests an extensive set of practical covariate shift scenarios of our interest and empirically show that DrilDICE successfully imitate the expert robust to those shift scenarios. For a limitation, we don\u2019t consider uncertainty from transition shift or noisy demonstrations. We expect that extending our approach into such scenario will be beneficial on many practical scenarios, such as Sim2Real or transfer learning tasks. Moreover, we believe exploring a real-world application of our method is also an attractive extend of the future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by IITP grant funded by MSIT of Korea (No. RS-2020-II200940, No. RS2022-II220311, No. RS-2019-II190075, No. RS-2019-II190079, No. RS-2024-00397310, No. RS2024-00343989, No. RS-2024-00457882), IITP-ITRC (IITP-2024-RS-2024-00436857), NRF of Korea (No. RS-2024-00451162, No. RS-2023-00279680), BK21 Four project, KAIST-NAVER Hypercreative AI Center, and NSF AI4OPT AI Centre. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Niall Adams. Dataset shift in machine learning. Journal of the Royal Statistical Society Series A: Statistics in Society, 2009.   \n[2] Mohammad Ali Bashiri, Brian Ziebart, and Xinhua Zhang. Distributionally robust imitation learning. Advances in Neural Information Processing Systems, 2021.   \n[3] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.   \n[4] Jongseong Chae, Seungyul Han, Whiyoung Jung, Myungsik Cho, Sungho Choi, and Youngchul Sung. Robust imitation learning against variations in environment dynamics. In Proceedings of the 39th International Conference on Machine Learning. PMLR, 2022.   \n[5] Jonathan Chang, Masatoshi Uehara, Dhruv Sreenivas, Rahul Kidambi, and Wen Sun. Mitigating covariate shift in imitation learning via offline data with partial coverage. Advances in Neural Information Processing Systems, 2021.   \n[6] Ruidi Chen and Ioannis Ch. Paschalidis. Distributionally robust learning. Foundations and Trends\u00ae in Optimization, 2020.   \n[7] Xiangli Chen, Mathew Monfort, Anqi Liu, and Brian D Ziebart. Robust covariate shift regression. In Proceedings of the 18th International Conference on Artificial Intelligence and Statistics. PMLR, 2016.   \n[8] Yasaman Ensaf,i Saman Hassanzadeh Amin, Guoqing Zhang, and Bharat Shah. Time-series forecasting of seasonal items sales using machine learning \u2013 a comparative analysis. International Journal of Information Management Data Insights, 2022.   \n[9] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.   \n[10] Rui Gao, Xi Chen, and Anton J. Kleywegt. Wasserstein distributionally robust optimization and variation regularization. Operations Research, 2020.   \n[11] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in Neural Information Processing Systems, 2016.   \n[12] Daniel Jarrett, Ioana Bica, and Mihaela van der Schaar. Strictly batch imitation learning by energy-based distribution matching. Advances in Neural Information Processing Systems, 2020.   \n[13] Shengyi Jiang, Jingcheng Pang, and Yang Yu. Offline imitation learning with a misspecified simulator. Advances in Neural Information Processing Systems, 2020.   \n[14] Geon-Hyeong Kim, Seokin Seo, Jongmin Lee, Wonseok Jeon, HyeongJoo Hwang, Hongseok Yang, and Kee-Eung Kim. DemoDICE: Offilne imitation learning with supplementary imperfect demonstrations. In International Conference on Learning Representations, 2022.   \n[15] Ilya Kostrikov, Ofir Nachum, and Jonathan Tompson. Imitation learning via off-policy distribution matching. arXiv preprint arXiv:1912.05032, 2019.   \n[16] Jongmin Lee, Wonseok Jeon, Byungjun Lee, Joelle Pineau, and Kee-Eung Kim. OptiDICE: Offilne policy optimization via stationary distribution correction estimation. In Proceedings of the 24th International Conference on Machine Learning. PMLR, 2021.   \n[17] Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. DualDICE: Behavior-agnostic estimation of discounted stationary distribution corrections. Advances in Neural Information Processing Systems, 2019.   \n[18] Richard Paap, Philip Hans Franses, and Henk Hoek. Mean shifts, unit roots and forecasting seasonal time series. International Journal of Forecasting, 1997.   \n[19] Kishan Panaganti, Zaiyan Xu, Dileep Kalathil, and Mohammad Ghavamzadeh. Distributionally robust behavioral cloning for robust imitation learning. In 62nd IEEE Conference on Decision and Control, 2023.   \n[20] Hamed Rahimian and Sanjay Mehrotra. Distributionally robust optimization: A review. arXiv preprint arXiv:1908.05659, 2019.   \n[21] Stephane Ross and Drew Bagnell. Efficient reductions for imitation learning. In Proceedings of the 13th International Conference on Artificial Intelligence and Statistics. PMLR, 2010.   \n[22] Resve A. Saleh and A.K.Md. Ehsanes Saleh. Statistical properties of the log-cosh loss function used in machine learning. arXiv preprint arXiv:2208.04564, 2022.   \n[23] Fumihiro Sasaki and Ryota Yamashina. Behavioral cloning from noisy demonstrations. In International Conference on Learning Representations, 2020.   \n[24] Jonathan Spencer, Sanjiban Choudhury, Arun Venkatraman, Brian Ziebart, and J Andrew Bagnell. Feedback in imitation learning: The three regimes of covariate shift. arXiv preprint arXiv:2102.02872, 2021.   \n[25] Matthew Staib and Stefanie Jegelka. Distributionally robust optimization and generalization in kernel methods. Advances in Neural Information Processing Systems, 2019.   \n[26] Xuezhi Wang, Tzu-Kuo Huang, and Jeff Schneider. Active transfer learning under model shift. In Proceedings of the 31st International Conference on Machine Learning. PMLR, 2014.   \n[27] Junfeng Wen, Chun-Nam Yu, and Russell Greiner. Robust learning under uncertain test distributions: Relating covariate shift to model misspecification. In Proceedings of the 31st International Conference on Machine Learning. PMLR, 2014.   \n[28] Haoran Xu, Xianyuan Zhan, Honglei Yin, and Huiling Qin. Discriminator-weighted offline imitation learning from suboptimal demonstrations. In Proceedings of the 39th International Conference on Machine Learning. PMLR, 2022.   \n[29] Haoran Xu, Li Jiang, Jianxiong Li, Zhuoran Yang, Zhaoran Wang, Victor Wai Kin Chan, and Xianyuan Zhan. Offline RL with no OOD actions: In-sample learning via implicit value regularization. In International Conference on Learning Representations, 2023.   \n[30] Kai Yan, Alex Schwing, and Yu-Xiong Wang. A simple solution for offline imitation from observations and examples with possibly incomplete trajectories. Advances in Neural Information Processing Systems, 2024.   \n[31] Xin Zhang, Yanhua Li, Ziming Zhang, and Zhi-Li Zhang. $f$ -GAIL: Learning $f$ -divergence for generative adversarial imitation learning. Advances in Neural Information Processing Systems, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Supplementary Material ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Contents ", "page_idx": 12}, {"type": "text", "text": "A Suboptimality Bounds with Arbitrary State Distributions 14 ", "page_idx": 12}, {"type": "text", "text": "B Summary of $f$ -Divergence Choices 15 ", "page_idx": 12}, {"type": "text", "text": "C Experimental Settings for Four Rooms Environment 16 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "C.1 Marginal probability of the original dataset 16   \nC.2 Implementation details . . 16   \nC.3 Hyperparameters 16 ", "page_idx": 12}, {"type": "text", "text": "D Experimental Settings for D4RL Benchmark 16 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "D.1 Implementation details 16   \nD.2 Hyperparameters . 16   \nD.3 Number of sub-trajectories 17 ", "page_idx": 12}, {"type": "text", "text": "E Additional Experimental Results 17 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "E.1 Performance comparison on complete trajectories . . . 17   \nE.2 Performance comparison on lower quality segments . . . . 17   \nE.3 Weak correlation between target MSE and normalized score 18   \nE.4 Performance comparison with a different $f$ -divergence choice 18 ", "page_idx": 12}, {"type": "text", "text": "F Licenses 19 ", "page_idx": 12}, {"type": "text", "text": "G Computation Resources 19 ", "page_idx": 12}, {"type": "text", "text": "H Broader Impacts 19 ", "page_idx": 12}, {"type": "text", "text": "A Suboptimality Bounds with Arbitrary State Distributions ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we present an example of BC where the performance guarantee fails when the data state distribution $d_{D}$ differs from expert stationary $d_{E}$ . For simplicity, here we focus on the finite-horizon average state distribution, but this can be straightforwardly extended to the infinitehorizon discounted state distribution. Consider a finite-horizon setting with a horizon $H\\ \\geq\\ 2$ and denote the state distribution $d_{\\pi}^{i}$ of $\\pi$ at timestep $i$ . Define the $H$ -horizon state distribution $d_{\\pi}$ of $\\pi$ as $\\begin{array}{r}{d_{\\pi}(s)\\;:=\\;\\frac{1}{H}\\sum_{i=1}^{H}d_{\\pi}^{i}(s)}\\end{array}$ for all $s~\\in~s$ . Denote the performance of $\\pi$ as ${\\cal J}(\\pi)~:=~~~$ $\\begin{array}{r}{\\sum_{t=0}^{H-1}\\mathbb{E}_{d_{\\pi}^{t}}[R(s_{t},a_{t})]\\;=\\;H\\mathbb{E}_{d_{\\pi}}[R(s,a)]}\\end{array}$ mwehaesreu $R\\,:\\,S\\times A\\,\\rightarrow\\,[0,1]$ piosl iac ytr ei sr efrwoamr dt hfeu necxtpieornt. $\\ell$ $\\pi$ policy for the state $s\\in S$ . Here, consider 0-1 loss, i.e. $\\ell(\\pi(s),\\pi_{E}(s))=\\mathbb{I}[\\pi(s)\\neq\\pi_{E}(s)]$ . ", "page_idx": 13}, {"type": "text", "text": "Proposition 2. (Ross and Bagnell [21]) The suboptimality of an imitator policy $\\hat{\\pi}$ is bounded by ", "page_idx": 13}, {"type": "equation", "text": "$$\nJ(\\pi_{E})-J(\\hat{\\pi})\\leq H^{2}\\mathbb{E}_{s\\sim d_{E}}[\\ell(\\hat{\\pi}(s),\\pi_{E}(s))]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In our problem setting, we assume the lack of access to dataset sampled from $d_{E}(s)$ , instead deal with dataset sampled from an arbitrary distribution $d_{D}(s)$ . When applying BC approach to minimize $\\mathbb{E}_{s\\sim d_{D}}[\\ell(\\pi(s),\\bar{\\pi}_{E}(s))]$ , it is possible to construct an example where this expected loss fails to upper bound the policy suboptimality, $J(\\pi_{E})-J(\\pi)$ . To illustrate, we revisit [21] and consider an example that tightens the inequality (11). Assume $0<\\epsilon\\leq1/H,0<\\delta<1/H$ . ", "page_idx": 13}, {"type": "image", "img_path": "lHcvjsQFQq/tmp/31d82fbbcb8a20f2b8221ee0c92d7075e210e42b919a67bc949789129bc27c4a.jpg", "img_caption": ["ple of [21]. Table A: Policies and their corresponding state distributions "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Consider an MDP illustrated in Figure A, where $\\cal{S}=\\{s_{1},s_{2},s_{3}\\},\\cal{A}=\\{a_{1},a_{2}\\}$ , the transition $T$ is deterministic with $T(s_{1},a_{1})=s_{2},T(s_{1},a_{2})=s_{3},T(s_{2},a_{1})=s_{3},T(s_{2},a_{2})=s_{2},T(s_{3},a_{1})=$ $s_{2},T(s_{3},a_{2})\\;=\\;s_{3}$ and $s_{1}$ is the initial state. We define the expert policy $\\pi_{E}$ and an imitator policy $\\hat{\\pi}$ and derive their corresponding $H$ -horizon state distributions $d_{E},d_{\\hat{\\pi}}$ in Table A. Define $R(s,a)=\\mathbb{I}[a=\\pi_{E}(s)]$ for all $s\\in{\\mathcal{S}},a\\in{\\mathcal{A}}$ . Then, $J(\\pi_{E}),J(\\hat{\\pi})$ would be: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{I(\\pi_{E})=H\\sum_{s\\in S}d_{E}(s)\\sum_{a\\in A}\\pi_{E}(a|s)R(s,a)=H\\left(\\frac{1}{H}+\\frac{H-1}{H}\\right)=H}}}\\\\ {{\\displaystyle{J(\\hat{\\pi})=H\\sum_{s\\in S}d_{\\hat{\\pi}}(s)\\sum_{a\\in A}\\hat{\\pi}(a|s)R(s,a)=H\\left(\\frac{1}{H}\\cdot(1-\\epsilon H)+\\frac{H-1}{H}\\cdot(1-\\epsilon H)\\right)=H-\\epsilon H^{2}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Hence, $J(\\pi_{E})-J(\\hat{\\pi})=\\epsilon H^{2}$ . Note that $\\begin{array}{r}{\\mathbb{E}_{s\\sim d_{E}}[\\ell(\\hat{\\pi},\\pi_{E})]=\\frac{1}{H}\\cdot\\epsilon H+\\frac{H-1}{H}\\cdot0+0\\cdot1=\\epsilon}\\end{array}$ , therefore the both left-hand and right-hand sides of the equality (11) are equal to $\\epsilon H^{2}$ . ", "page_idx": 13}, {"type": "text", "text": "However, considering the expect loss with respect to $d_{D_{1}}$ , where $d_{D_{1}}(s_{1})\\,=\\,\\delta,d_{D_{1}}(s_{2})\\,=\\,1\\,-$ $\\delta,d_{D_{1}}(s_{3})=0$ as described in Table A, we can calculate $\\mathbb{E}_{s\\sim d_{D_{1}}}[\\ell(\\hat{\\pi},\\pi_{E})]=\\delta\\epsilon H$ . Then, ", "page_idx": 13}, {"type": "equation", "text": "$$\nJ(\\pi_{E})-J(\\hat{\\pi})=\\epsilon H^{2}>\\delta\\epsilon H^{3}=H^{2}\\mathbb{E}_{s\\sim d_{D_{1}}}[\\ell(\\hat{\\pi},\\pi_{E})]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\delta<1/H$ . As $\\delta$ decreases, the right-hand side also decreases, allowing it to be made arbitrarily small regardless of the left-hand side. This implies that there exist a scenario in which minimizing BC loss with an arbitrary data state distribution does not guarantee a reduction in the performance gap between the expert and the imitator. ", "page_idx": 13}, {"type": "text", "text": "Although straightforward, we emphasize that distributionally robust optimization (DRO) approaches can minimize the suboptimality under the realizability assumption, i.e, $d_{E}\\in\\mathcal{Q}$ . ", "page_idx": 14}, {"type": "text", "text": "Remark 1. Given an uncertainty set $\\mathcal{Q}\\subseteq\\Delta(S)$ with $d_{E}\\,\\in\\,\\mathcal{Q},$ , we can guarantee that $J(\\pi_{E})\\;-$ $J(\\pi)\\le H^{2}\\operatorname*{sup}_{d\\in\\mathcal{Q}}\\mathbb{E}_{s\\sim d}[l(\\pi(s),\\pi_{E}(s))]$ . Furthermore, given $\\mathcal{Q}_{1}\\subseteq\\mathcal{Q}_{2}\\subseteq\\Delta(S)$ with $d_{E}\\in\\mathcal{Q}_{1}$ , the upper-bound of $\\mathcal{Q}_{1}$ would be tighter than $\\mathcal{Q}_{2}$ , i.e. ", "page_idx": 14}, {"type": "equation", "text": "$$\nJ(\\pi_{E})-J(\\pi)\\leq H^{2}\\operatorname*{sup}_{d\\in\\mathcal{Q}_{1}}\\mathbb{E}_{s\\sim d}[l(\\pi(s),\\pi_{E}(s))]\\leq H^{2}\\operatorname*{sup}_{d\\in\\mathcal{Q}_{2}}\\mathbb{E}_{s\\sim d}[l(\\pi(s),\\pi_{E}(s))]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let $\\mathcal{Q}_{\\rho}\\;:=\\;\\{d\\;\\in\\;\\Delta(S)\\;:\\;D_{\\mathrm{TV}}(d,d_{D})\\;\\leq\\;\\rho\\}$ and $\\mathcal{Q}_{\\rho}^{\\mathrm{DrilDICE}}\\;=\\;\\{d\\;\\in\\;\\Delta({\\cal S})\\;:\\;D_{\\mathrm{TV}}(d,d_{D})\\;\\leq$ $\\rho,d$ satisfies Bellman flow constraints}. Since $d_{E}$ satisfies Bellman flow constraints, if $d_{E}\\,\\in\\,\\mathcal{Q}_{\\rho}$ , ttihgehnt $d_{E}\\,\\in\\,\\mathcal{Q}_{\\rho}^{\\mathrm{DrilDICE}}$ fowri tthh $\\mathcal{Q}_{\\rho}^{\\mathrm{DrilDICE}}\\subseteq\\mathcal{Q}_{\\rho}$ c. oTmhpeanr,e tdh teo  utnhcate rotfa . set of $\\mathcal{Q}_{\\rho}^{\\mathrm{DrilDICE}}$ provides a more $\\mathcal{Q}_{\\rho}$ ", "page_idx": 14}, {"type": "text", "text": "B Summary of $f$ -Divergence Choices ", "text_level": 1, "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\stackrel{\\mathrm{relt}\\quad\\rightarrow\\quad}{\\mathrm{sotr}}{\\mathrm{.}}^{2}(x)}&{:=}&{\\left\\{\\!\\!\\!\\begin{array}{l l}{x\\log x-x+1}&{\\mathrm{~if~}0<x<1}\\\\ {(x-1)^{2}}&{\\mathrm{~if~}x\\geq1}\\end{array}\\right.\\mathrm{~and~}\\,(f_{\\mathrm{sot}\\cdot\\chi^{2}}^{\\prime})^{-1}(y)}&{:=}&{\\left\\{\\!\\!\\!\\begin{array}{l l}{\\exp(y)}&{\\mathrm{~if~}y<0}\\\\ {y+1}&{\\mathrm{~if~}y\\geq0}\\end{array}\\right.}\\\\ {\\mathrm{~3eLU}(x)}&{:=\\operatorname*{max}(0,x),\\,\\mathrm{ELU}(x):=\\left\\{\\!\\!\\!\\begin{array}{l l}{\\exp(x)-1}&{\\mathrm{~if~}x<0}\\\\ {x}&{\\mathrm{~if~}x\\geq0}\\end{array}\\right.\\mathrm{~Then,~}f\\mathrm{-divergence~choices~can}}\\\\ &{}&{\\qquad\\cdot\\quad\\ldots\\ldots\\ldots}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "table", "img_path": "lHcvjsQFQq/tmp/2a5faeb9fa1503e73ae3d24add1b714975cb179a5d546f5fe5f65ae015674230.jpg", "table_caption": ["Define ", "Table B: Summary of $f$ -divergences and their associated functions. "], "table_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "lHcvjsQFQq/tmp/3349cc837337760954ab4cf20a1ef030ce89e7fa43eaaebf63e94f01418a622e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure B: Visualization of $f$ functions and derivatives $f^{\\prime}$ corresponding to $f$ -divergence choices. We also visualize corresponding $w_{f}^{*}(e)=\\operatorname*{max}((f^{\\prime})^{-1}(\\mathring{e}),0)$ , which is a closed form solution of the inner maximization in Eq. 7. ", "page_idx": 14}, {"type": "text", "text": "C Experimental Settings for Four Rooms Environment ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Marginal probability of the original dataset ", "page_idx": 15}, {"type": "table", "img_path": "lHcvjsQFQq/tmp/6a465007fb5ce2f746eef6ea342e5e6baf1d9f17e8d5bedc874f69580221c8b4.jpg", "table_caption": [], "table_footnote": ["Table C: Marginal probability of $D_{E}$ over rooms and actions. "], "page_idx": 15}, {"type": "text", "text": "C.2 Implementation details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "\u2022 The number of transitions: $1000+|S|^{3}$   \n\u2022 RBF feature bandwidth: 10   \n\u2022 The number of representative points for RBF feature : $5\\times5$   \n\u2022 $C_{\\pi}$ : 1-0 loss ", "page_idx": 15}, {"type": "text", "text": "C.3 Hyperparameters ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "\u2022 $\\alpha\\in\\{100,50,20,10,5,2,1,0.5,0.2,0.1\\}$ (for OptiDICE-BC, DrilDICE) \u2022 $\\rho\\in\\{100,50,20,10,5,2,1,0.5,0.2,0.1\\}$ (for DR-BC) \u2022 Logit margin maximum: log(100) ", "page_idx": 15}, {"type": "text", "text": "D Experimental Settings for D4RL Benchmark ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "D.1 Implementation details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "\u2022 Since the number of $s_{0}$ is much smaller than $s$ , we follow heuristics to estimate $\\mathbb{E}_{s\\sim d_{D}}[\\nu(s)]$ instead of $\\mathbb{E}_{s\\sim\\rho_{0}}[\\nu(s)]$ in the objective (8).   \n\u2022 In Scenario 3, to determine a starting timestep of each segment, we sample timesteps by using Geometric distribution with $p=5\\times10^{-3}$ .   \n\u2022 $C_{\\pi}$ : Mean Squared Error (MSE) loss ", "page_idx": 15}, {"type": "text", "text": "D.2 Hyperparameters ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "lHcvjsQFQq/tmp/f8862da663cd140ec4b129d0391533c1fcdf9578e6467c198534a52bc678a81c.jpg", "table_caption": [], "table_footnote": ["Table D: Summary of hyperparameters used in D4RL benchmark experiments. "], "page_idx": 15}, {"type": "table", "img_path": "lHcvjsQFQq/tmp/df905c32c5123a53fa02fd0bc8db6621bfb0a929e670c11e928878c52f801a17.jpg", "table_caption": ["D.3 Number of sub-trajectories "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table E: Number of sub-trajectories used in each scenario. Each sub-trajectory consists of one initial transition and 50 subsampled transitions from a complete trajectory. To enhance dataset support, one complete trajectory is appended for each specified count of sub-trajectories. ", "page_idx": 16}, {"type": "text", "text": "E Additional Experimental Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "E.1 Performance comparison on complete trajectories ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To compare performance of IL methods on standard scenarios, we have conducted additional experiments using complete expert trajectories using D4RL expert-v2 dataset varying the number of trajectories in $\\{1,5,10,50\\}$ . The results are detailed in Figure C. As demonstrated, DrilDICE can deal with sampling errors of small datasets, showing a superior data efficiency compared to other methods. ", "page_idx": 16}, {"type": "image", "img_path": "lHcvjsQFQq/tmp/ff5be0483dd490edd6b3ca1473199403a76df2240b9974b2115aeaf97c345e5b.jpg", "img_caption": ["Figure C: Performance comparison on complete trajectory scenarios. The points/shaded areas indicate the means/standard errors measured over 5 repetitions. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "E.2 Performance comparison on lower quality segments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Despite our primary focus on expert-quality datasets, we conducted experiments with additional datasets on Scenario 3 (segment datasets) to ensure consistency across different datasets. Rather than employing the original dataset, D4RL expert-v2, we use D4RL medium-v2 quality demonstrations as our imitation standard for comparison. The results are depicted in Figure D. The results indicate that both DrilDICE and DR-BC demonstrate competitive imitation performance compared to other baselines, with a consistent decrease in target MSE as the number of segments increases. ", "page_idx": 16}, {"type": "image", "img_path": "lHcvjsQFQq/tmp/4216160c9396254e0b40716db67f8051a177c9d638768d5ab5aa6c3a7bc793a5.jpg", "img_caption": ["Figure D: Performance comparison on Scenario 3 (segmented dataset) with D4RL medium-v2 datasets. Each black dashed-dot line expresses the averaged normalized score of the used dataset. The points/shaded areas indicate the means/standard errors measured over 5 repetitions. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "lHcvjsQFQq/tmp/f3a485d656eb5b1304473f01674e20d251229a81d2ec501fefebd935e797704f.jpg", "img_caption": ["Figure E: Correlation between the target MSE and normalized scores. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "E.4 Performance comparison with a different $f$ -divergence choice ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To evaluate the effect of $f$ -divergence choice on imitation performance, we additionally evaluate OptiDICE-BC and DrilDICE with the soft $\\chi^{2}$ -divergence, as originally utilized in [16], across all considered scenarios. The results for scenarios 1-3 are presented in Table F, G and Figure F respectively. These results indicate that the choice of the soft TV-distance significantly enhances the performance of DrilDICE compared to using soft- $\\chi^{2}$ . To explain this performance gain, we hypothesize that the soft-TV distance provides more discriminative weighting of samples based on their long-term policy errors. As shown in Figure B, conventional $f$ -divergences (e.g. KL, soft- $\\chi^{2}$ , ...) make less pronounced to the long-term policy error, yet the soft-TV distance responds sensitively to changes in $e$ , resulting in a more pronounced weight $w$ . This enables BC loss to more selectively focus on critical samples with significant magnitude of long-term policy errors, thereby effectively enhancing performance, akin to the benefits observed in Sparse Q-Learning [29]. ", "page_idx": 17}, {"type": "table", "img_path": "lHcvjsQFQq/tmp/55d5c6400976c1b72db1e51555d761edde58d57dc019b399740f56c8cbea8279.jpg", "table_caption": [], "table_footnote": ["Table F: Performance comparison on Scenario 1 (rebalanced dataset) including a different choice of $f$ -divergence (soft $\\chi^{2}$ -divergence) for OptiDICE-BC and DrilDICE. "], "page_idx": 17}, {"type": "table", "img_path": "lHcvjsQFQq/tmp/f367d3d10b46b9f013071dd248de0198de047cb9e44dbf4760392818297840ee.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table G: Performance comparison on Scenario 2 (time-dependently collected dataset) including a different choice of $f$ -divergence (soft $\\chi^{2}$ -divergence) for OptiDICE-BC and DrilDICE. ", "page_idx": 18}, {"type": "image", "img_path": "lHcvjsQFQq/tmp/91ae1e010207a17d668487750912669f211281edfb185414be7a8b2da19a68af.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure F: Performance comparison on Scenario 3 (segmented dataset) including a different choice of $f$ -divergence (soft $\\chi^{2}$ -divergence) for OptiDICE-BC and DrilDICE. ", "page_idx": 18}, {"type": "text", "text": "F Licenses ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For all experiments in Four Rooms environment, we use CVXPY and convex optimization solver MOSEK with academic licenses. We also use Mujoco free licenses. Our code has been developed upon MIT licenses. ", "page_idx": 18}, {"type": "text", "text": "G Computation Resources ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We used Google cloud computing engine with $100\\,{\\mathsf{c}}2$ -standard-4 instances that have the following system specification: ", "page_idx": 18}, {"type": "text", "text": "\u2022 Series : C2   \n\u2022 Family $:$ compute-optimized   \n\u2022 vCPU : 4   \n\u2022 Memory $:$ 16 GB   \n\u2022 CPU Manufacturer $:$ Intel   \n\u2022 CPU Platform $:$ Intel Cascade Lake ", "page_idx": 18}, {"type": "text", "text": "\u2022 CPU Base Frequency : 3.1 GHz \u2022 CPU Turbo Frequency $:$ 3.8 GHz \u2022 CPU Max. Turbo Frequency $:$ 3.9 GHz \u2022 Network Bandwidth : 10 Gbps \u2022 Max. Disk Size : 257 TB \u2022 Max. Number of Disks : 128 ", "page_idx": 18}, {"type": "text", "text": "H Broader Impacts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Since we study distributionally robust learning, our approach can be used in fairness or bias reduction. As we discussed in experiment section, our algorithm is also beneficial when the training agent in the cost-constrained environment. Further, we expect our research contributes to build more trustworthy and robust AI systems. ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Yes, we clearly reflect our scope and contribution in abstract and introduction. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Yes, please see Section 5 ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: We does not claim any theoretical results. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide hyperparameters and code in the supplementary materials. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We used D4RL dataset, which is public, and provide toy domain code to generate datasets. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Yes, we include training and test details in Appendix. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Yes, we report standard errors for all experiments we did. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Yes, we provide resource information in the appendix ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Yes, we read and conducted research with Code of Ethics. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We describe broader impacts in the appendix ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Experiments of our research grounded only on simulators. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We describe License in Appendix F. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 23}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: we does not release new assets ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our research does not involve crowdsourcing. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our research does not involve crowdsourcing. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]