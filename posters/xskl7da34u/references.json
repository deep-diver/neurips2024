{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 Technical Report", "publication_date": "2023-03-08", "reason": "This paper is a comprehensive technical report on GPT-4, a large language model that is highly relevant to the field of multimodal large language models (MLLMs) and is foundational to many of the techniques used in the current paper."}, {"fullname_first_author": "William Fedus", "paper_title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity", "publication_date": "2022-MM-DD", "reason": "This paper introduces Switch Transformers, an architecture that enables the training of extremely large language models by employing a mixture-of-experts approach which is crucial to the MoME architecture proposed in this paper."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning Transferable Visual Models From Natural Language Supervision", "publication_date": "2021-MM-DD", "reason": "CLIP, introduced in this paper, is a foundational vision-language model that is used extensively in multimodal learning and is directly relevant to the MoME's approach of using multiple vision encoders."}, {"fullname_first_author": "Maxime Oquab", "paper_title": "DINOv2: Learning Robust Visual Features Without Supervision", "publication_date": "2023-04-07", "reason": "DINOv2 is a self-supervised vision model that is used as one of the vision encoders in the proposed MoME model; its performance and architectural choices directly impact the overall performance and efficiency of the model."}, {"fullname_first_author": "Kenton Lee", "paper_title": "Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding", "publication_date": "2023-MM-DD", "reason": "Pix2Struct is another important vision encoder used in MoME.  This paper's contributions in visual understanding are critical to the overall effectiveness of the multimodal model, particularly in tasks involving document understanding."}]}