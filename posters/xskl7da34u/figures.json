[{"figure_path": "Xskl7Da34U/figures/figures_1_1.jpg", "caption": "Figure 1: VL data distribution visualization and model performance comparisons. Experimental results in (a) show that a generalist model trained on a mixed dataset underperforms most specialist models trained on separate task groups. The feature distributions visualized in (b) and (c) show significant discrepancies across VL tasks in both images and instructions.", "description": "This figure shows a comparison between specialist and generalist multimodal large language models (MLLMs) on various vision-language (VL) tasks.  Panel (a) demonstrates the performance gap, illustrating that generalist models trained on a mixed dataset significantly underperform compared to specialist models trained on individual task groups. Panels (b) and (c) provide visual representations of the sentence semantics and image embeddings, respectively, highlighting the substantial differences in feature distributions across the different task groups.  These variations underscore the challenges in training effective generalist MLLMs due to task interference.", "section": "3 Analysis of Task Interference"}, {"figure_path": "Xskl7Da34U/figures/figures_3_1.jpg", "caption": "Figure 2: The overall architecture of the proposed MoME. The model obtains compressed and self-enhanced visual features from distinct vision encoders through adaptive deformable transformation (a) and aggregates them by dynamic routing (b). The MoLE blocks (c) are integrated into each FFN layer of LLM to improve multitasking capability with little cost.", "description": "This figure illustrates the architecture of the Mixture of Multimodal Experts (MoME) model.  It shows how MoME combines visual and language experts to improve performance on multimodal tasks.  The adaptive deformable transformation (ADT) module processes features from multiple vision encoders (CLIP, DINO, Pix2Struct), creating compressed and self-enhanced visual features. Dynamic routing then aggregates these features based on the input instructions.  Finally, the Mixture of Language Experts (MoLE) modules are integrated into the feed-forward network (FFN) layers of the large language model (LLM) to enhance its multitasking abilities with minimal additional computational cost.", "section": "3.2 Architecture"}, {"figure_path": "Xskl7Da34U/figures/figures_3_2.jpg", "caption": "Figure 3: Comparison of MLLMs with different vision encoders.", "description": "The figure shows the performance comparison of three different MLLMs, each using a different vision encoder (CLIP-ViT, DINOv2, and Pix2Struct).  It demonstrates that models with different vision encoders excel in specific tasks, showcasing the variation in capabilities across encoders and highlighting the need for an adaptive approach.", "section": "3.2.1 Mixture of Vision Experts"}, {"figure_path": "Xskl7Da34U/figures/figures_6_1.jpg", "caption": "Figure 4: Distribution of vision experts routing results. In each bar, the lengths of different colors represent the frequency with which each expert is selected.", "description": "This figure visualizes the distribution of vision expert selection frequencies across various vision-language tasks.  Each bar represents a specific task, and the segments within each bar show the proportion of times each vision expert (CLIP, DINO, Pix2Struct) was chosen by the MoVE (Mixture of Vision Experts) module to process the visual features for that task.  The lengths of the colored segments directly indicate the frequency of expert selection; longer segments mean that expert was more frequently used for that particular task.  The chart provides insights into the specialization of different vision experts for different task types.", "section": "4.1 Analysis on MoVE"}, {"figure_path": "Xskl7Da34U/figures/figures_7_1.jpg", "caption": "Figure 2: The overall architecture of the proposed MoME. The model obtains compressed and self-enhanced visual features from distinct vision encoders through adaptive deformable transformation (a) and aggregates them by dynamic routing (b). The MoLE blocks (c) are integrated into each FFN layer of LLM to improve multitasking capability with little cost.", "description": "This figure illustrates the architecture of the Mixture of Multimodal Experts (MoME) model.  It shows three main components: (a) Adaptive Deformable Transformation, which processes visual features from different encoders to create a unified representation; (b) Dynamic Routing, which aggregates these features based on the input instruction; and (c) Mixture of Language Experts (MoLE), which integrates into each feed-forward network (FFN) layer of the Large Language Model (LLM) to improve multitasking and enhance the model's comprehension with minimal computational overhead. The overall design aims to dynamically mix vision and language experts based on input instructions, improving the generalist capabilities of Multimodal Large Language Models (MLLMs).", "section": "3.2 Architecture"}, {"figure_path": "Xskl7Da34U/figures/figures_8_1.jpg", "caption": "Figure 6: Visualization of samples along with their routing result distributions. The MOVE distributions on the left represent Pix2Struct, DINOv2, and CLIP-ViT from top to bottom. MoLE is on the bottom, with different colors indicating different experts.", "description": "This figure visualizes examples from four different vision-language tasks (REC, REG, Document, and General) alongside their respective MoVE (Mixture of Vision Experts) and MoLE (Mixture of Language Experts) routing result distributions.  The MoVE section displays the contribution of each vision encoder (Pix2Struct, DINOv2, and CLIP-ViT) to the final feature representation, showing how each encoder specializes in particular tasks. The MoLE section shows how each language expert contributes to processing different task types.  The visualization highlights the dynamic selection of vision and language experts based on the task's unique requirements, illustrating MoME's adaptive nature in handling different types of vision-language tasks.", "section": "4 Experimental Results and Analysis"}, {"figure_path": "Xskl7Da34U/figures/figures_13_1.jpg", "caption": "Figure 7: Multitask learning and evaluation datasets and their corresponding categories.", "description": "This figure shows the categorization of 24 datasets used in the paper for multitask learning and evaluation.  The datasets are grouped into four categories: General, REC, REG, and Document. Each category contains several datasets, some of which are used for both training and evaluation, while others are only used for evaluation. The color-coding distinguishes between datasets used for training only, evaluation only, or both.", "section": "3.1 Analysis of Task Interference"}, {"figure_path": "Xskl7Da34U/figures/figures_16_1.jpg", "caption": "Figure 8: Distribution of all language experts routing results across all tasks.", "description": "This figure visualizes the distribution of expert usage across different layers of the language model (LLM) for various vision-language tasks. Each bar represents a specific task, and the segments within each bar show the percentage of times each expert was used in each layer of the LLM for that task.  This helps to illustrate the specialization of experts across various tasks and demonstrates the dynamic routing capabilities of the MoLE module.", "section": "4.2 Analysis on MOLE"}]