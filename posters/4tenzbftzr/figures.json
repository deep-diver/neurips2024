[{"figure_path": "4TENzBftZR/figures/figures_1_1.jpg", "caption": "Figure 1: Practical applications of iVideoGPT, which is designed to scale, allowing pre-training on millions of human and robotic manipulation trajectories. This results in a single, versatile foundation of interactive world models, adaptable to a wide range of downstream tasks.", "description": "The figure illustrates the applications of iVideoGPT.  iVideoGPT is pre-trained on a large dataset of human and robot manipulation trajectories, creating a scalable and versatile world model.  This model can then be applied to a variety of downstream tasks, including video prediction, visual planning, and model-based reinforcement learning. The figure shows example videos from each of these downstream applications.", "section": "1 Introduction"}, {"figure_path": "4TENzBftZR/figures/figures_2_1.jpg", "caption": "Figure 2: Conceptual comparison among architectures, illustrated using a single context frame (T0 = 1) for simplicity. (a) Recurrent architectures for world models like Dreamer [29] and MuZero [80] provide step-level interactivity but limited scalability. (b) Recent video generation advancements like VideoGPT [101] and Stable Video Diffusion [8, 7] use non-causal temporal modules that can only offer trajectory-level interactivity. (c) Our model utilizes an autoregressive transformer that separately maps each step into a sequence of tokens, achieving both scalability and interactivity.", "description": "This figure compares three different architectural approaches for world models: recurrent models, video generation models, and interactive video prediction models.  Recurrent models, like Dreamer and MuZero, offer step-level interactivity but struggle with scalability. Video generation models, such as VideoGPT and Stable Video Diffusion, use non-causal temporal modules, limiting interactivity to trajectory-level. In contrast, the proposed iVideoGPT utilizes an autoregressive transformer, achieving both scalability and step-level interactivity by mapping each step to a sequence of tokens.", "section": "2 Problem Formulation"}, {"figure_path": "4TENzBftZR/figures/figures_3_1.jpg", "caption": "Figure 3: Architecture of iVideoGPT, simplified to show only a single context frame (T0 = 1). (a) Compressive tokenization utilizes a conditional VQGAN that discretizes future frames conditioned on context frames to handle temporal redundancy, significantly reducing the number of video tokens. (b) An autoregressive transformer integrates multimodal signals-visual observations, actions, and rewards-into a sequence of tokens, enabling interactive agent experiences through next-token prediction. Actions and rewards are optional and not included in action-free video pre-training.", "description": "This figure shows the architecture of iVideoGPT. The left panel (a) illustrates the compressive tokenization method, which uses a conditional VQGAN to efficiently encode visual information. The right panel (b) illustrates how the model processes multimodal signals (visual observations, actions, rewards) using an autoregressive transformer. This enables interactive prediction of the next token, which is crucial for model-based reinforcement learning applications.", "section": "3 Interactive VideoGPT"}, {"figure_path": "4TENzBftZR/figures/figures_5_1.jpg", "caption": "Figure 4: Qualitative evaluation: video prediction results of iVideoGPT on Open X-Embodiment, RoboNet, and VP2. Zoom in for details. Extended examples can be found in Appendix B.1.", "description": "This figure shows the qualitative results of the iVideoGPT model on three different video datasets: Open X-Embodiment, RoboNet, and VP2.  Each dataset represents different types of videos, with Open X-Embodiment showing robot manipulation trajectories, RoboNet showing a variety of robotic arm movements, and VP2 showcasing both robotic and human interactions. For each dataset, the figure displays the ground truth video sequence and iVideoGPT's predictions side-by-side for comparison. The results demonstrate the model's capability to accurately predict future frames based on provided context frames.", "section": "4 Experiments"}, {"figure_path": "4TENzBftZR/figures/figures_6_1.jpg", "caption": "Figure 5: Visual MPC results on the VP2 benchmark. We report the mean and min/max performance of iVideoGPT over 3 control runs. On the right, we show the mean scores averaged across all tasks except flat block due to low simulator performance, normalized by the performance of the simulator.", "description": "The figure shows the success rates of iVideoGPT and other visual predictive control models on various tasks from the VP2 benchmark.  It compares the performance across multiple tasks, illustrating the effectiveness of iVideoGPT and highlighting its strengths and weaknesses relative to other models. The right side shows normalized mean scores, which account for differences in task difficulty and simulator performance.", "section": "4.2 Visual Planning"}, {"figure_path": "4TENzBftZR/figures/figures_6_2.jpg", "caption": "Figure 2: Conceptual comparison among architectures, illustrated using a single context frame (To = 1) for simplicity. (a) Recurrent architectures for world models like Dreamer [29] and MuZero [80] provide step-level interactivity but limited scalability. (b) Recent video generation advancements like VideoGPT [101] and Stable Video Diffusion [8, 7] use non-causal temporal modules that can only offer trajectory-level interactivity. (c) Our model utilizes an autoregressive transformer that separately maps each step into a sequence of tokens, achieving both scalability and interactivity.", "description": "This figure compares three different types of architectures for world models: recurrent models, video generation models, and the proposed interactive video prediction model. Recurrent models offer step-level interactivity but suffer from scalability issues. Video generation models, on the other hand, provide trajectory-level interactivity but are not designed for interactive scenarios. The proposed model combines scalability and interactivity by using an autoregressive transformer that integrates multimodal signals (visual observations, actions, and rewards) into a sequence of tokens, allowing for interactive agent experiences via next-token prediction.", "section": "2 Problem Formulation"}, {"figure_path": "4TENzBftZR/figures/figures_7_1.jpg", "caption": "Figure 7: Visual model-based RL on Meta-world. (Left) Aggregated results report interquartile mean and 95% confidence interval (CI) [2] across a total of 30 runs over six tasks. (Right) Individual results for each task, report mean and 95% CI across five runs, measuring success rates over 20 evaluation episodes. PT denotes pre-training.", "description": "This figure shows the results of visual model-based reinforcement learning experiments conducted on six robotic manipulation tasks from the Meta-World benchmark.  The left panel presents aggregated results across all six tasks, illustrating the success rate of different model-based RL approaches (DrQ-v2, DreamerV3, and MBPO with and without pre-training using iVideoGPT) over the course of environment steps. The right panel displays the individual results for each of the six tasks, showing success rates with confidence intervals.  The results highlight the superior sample efficiency of the model-based methods, particularly when using iVideoGPT for world model pre-training.", "section": "4.3 Visual Model-based Reinforcement Learning"}, {"figure_path": "4TENzBftZR/figures/figures_8_1.jpg", "caption": "Figure 8: Zero-shot prediction by pre-trained transformer in iVideoGPT. Without fine-tuning, the transformer predicts natural movements of a different robot gripper using the pre-trained tokenizer (second row) but accurately predicts for the correct gripper type with an adapted tokenizer (third row).", "description": "This figure demonstrates the zero-shot prediction capability of the pre-trained transformer in iVideoGPT. The first row shows the ground truth video of a robot manipulating objects. The second row shows the prediction results without fine-tuning.  The pre-trained tokenizer is used and predicts a natural movement, although with a different robot gripper than the training set. The third row shows the results with fine-tuning on the tokenizer only.  Here, the model accurately predicts the correct gripper movements from the training set. The fourth row shows the prediction using a fully fine-tuned model for comparison. This experiment highlights the model's ability to transfer knowledge and adapt to new situations with minimal fine-tuning.", "section": "4.4 Model Analysis"}, {"figure_path": "4TENzBftZR/figures/figures_8_2.jpg", "caption": "Figure 9: Model analysis. (a) Video prediction results with various fine-tuning strategies and data sizes on BAIR. (b) Validation losses for the 138M and 436M transformer models on the pre-training dataset. (c) Computational efficiency and reconstruction quality of different tokenizers.", "description": "This figure shows the results of model analysis experiments.  (a) demonstrates the performance of few-shot adaptation on the BAIR dataset with different fine-tuning strategies (from scratch, no fine-tuning, tokenizer fine-tuned, full fine-tuned) and varying dataset sizes. (b) illustrates the model scaling behavior by comparing the validation losses of 138M and 436M transformer models during pre-training. (c) presents a comparison of the tokenization efficiency and reconstruction quality among different tokenizers (4x4, 16x16, and the proposed compressive tokenizer).", "section": "4.4 Model Analysis"}, {"figure_path": "4TENzBftZR/figures/figures_9_1.jpg", "caption": "Figure 10: Context-dynamics decoupling in our compressive tokenization. By removing cross-attention from future frames to context frames, the decoder can still reconstruct a trajectory that moves in the same way as the original, but the visual context is almost entirely missing.", "description": "This figure demonstrates the effectiveness of the proposed compressive tokenization method in separating contextual information from dynamic information. By removing the cross-attention mechanism between the context frames and future frames in the decoder, the reconstruction still captures the dynamic motion of objects but loses the detailed visual context from the original sequence. This highlights the ability of the model to efficiently focus on the essential motion information while compressing the amount of information needed to be processed, which is a crucial component for scalability and efficiency.", "section": "3.1 Architecture"}, {"figure_path": "4TENzBftZR/figures/figures_21_1.jpg", "caption": "Figure 11: Qualitative evaluation on the Open X-Embodiment dataset for DreamerV3-XL pre-trained on the same dataset as ours.", "description": "This figure compares the qualitative video prediction results of iVideoGPT and DreamerV3-XL. Both models were pre-trained on the Open X-Embodiment dataset. The figure shows that iVideoGPT generates significantly more realistic and coherent video predictions compared to DreamerV3-XL, especially in terms of object motion and interaction. This highlights iVideoGPT's superior performance in capturing and generating complex spatiotemporal dynamics.", "section": "B Extended Experimental Results"}, {"figure_path": "4TENzBftZR/figures/figures_22_1.jpg", "caption": "Figure 12: Additional qualitative evaluation on the Open X-Embodiment dataset for action-free video prediction.", "description": "This figure shows additional qualitative results of action-free video prediction on the Open X-Embodiment dataset.  It provides a visual comparison between ground truth video frames and those predicted by the iVideoGPT model.  Multiple examples across various scenarios are shown to illustrate model performance on a variety of actions.", "section": "B.1 Qualitative Evaluation"}, {"figure_path": "4TENzBftZR/figures/figures_23_1.jpg", "caption": "Figure 4: Qualitative evaluation: video prediction results of iVideoGPT on Open X-Embodiment, RoboNet, and VP2. Zoom in for details. Extended examples can be found in Appendix B.1.", "description": "This figure shows the qualitative results of video prediction using iVideoGPT on three different datasets: Open X-Embodiment, RoboNet, and VP2.  It compares the model's predictions against ground truth videos for action-free and goal-conditioned settings. Each dataset shows example trajectories where the video model predicts a sequence of frames from initial context frames. The results visually demonstrate the iVideoGPT model's ability to generate realistic and accurate video predictions for robotic manipulation.", "section": "4 Experiments"}, {"figure_path": "4TENzBftZR/figures/figures_23_2.jpg", "caption": "Figure 8: Zero-shot prediction by pre-trained transformer in iVideoGPT. Without fine-tuning, the transformer predicts natural movements of a different robot gripper using the pre-trained tokenizer (second row) but accurately predicts for the correct gripper type with an adapted tokenizer (third row).", "description": "This figure demonstrates the zero-shot prediction capability of the pre-trained iVideoGPT transformer.  The first row shows the ground truth video frames of a robot manipulating objects. The second row shows the model's predictions without any fine-tuning; while it predicts a plausible movement, the gripper type is incorrect.  The third row showcases the predictions after only fine-tuning the tokenizer (the part of the model responsible for converting images into tokens) with the data for the correct gripper type. This achieves more accurate predictions with only partial model retraining. The final row provides predictions after fine-tuning the entire model, showing the performance improvement resulting from full retraining.", "section": "4.4 Model Analysis"}, {"figure_path": "4TENzBftZR/figures/figures_23_3.jpg", "caption": "Figure 8: Zero-shot prediction by pre-trained transformer in iVideoGPT. Without fine-tuning, the transformer predicts natural movements of a different robot gripper using the pre-trained tokenizer (second row) but accurately predicts for the correct gripper type with an adapted tokenizer (third row).", "description": "This figure demonstrates the zero-shot prediction capabilities of the pre-trained iVideoGPT transformer on the unseen BAIR dataset.  The top row shows the ground truth video frames. The second row displays the model's predictions without any fine-tuning, revealing its ability to predict natural movements even with a different robot gripper than those seen during pre-training. This highlights the separation of scene context and motion dynamics in the model. The third row showcases the results after adapting the tokenizer for the unseen gripper, leading to more accurate predictions. Finally, the bottom row illustrates the results with full fine-tuning, demonstrating that only the tokenizer needs adapting for excellent zero-shot performance.", "section": "4.4 Model Analysis"}, {"figure_path": "4TENzBftZR/figures/figures_24_1.jpg", "caption": "Figure 1: Practical applications of iVideoGPT, which is designed to scale, allowing pre-training on millions of human and robotic manipulation trajectories. This results in a single, versatile foundation of interactive world models, adaptable to a wide range of downstream tasks.", "description": "The figure shows different applications of the iVideoGPT model.  The model is pre-trained on a large dataset of human and robot manipulation videos. This allows it to be used as a foundation for various downstream tasks, such as video prediction, visual planning, and model-based reinforcement learning. The scalability of the iVideoGPT model is highlighted, showing its adaptability to different applications.", "section": "1 Introduction"}, {"figure_path": "4TENzBftZR/figures/figures_24_2.jpg", "caption": "Figure 17: Additional qualitative evaluation on the RoboNet dataset, in high resolution (256 \u00d7 256).", "description": "This figure displays several examples of video prediction results from the iVideoGPT model on the RoboNet dataset.  It specifically showcases the model's performance at a higher resolution (256x256 pixels) compared to some of the other results shown in the paper. The figure is arranged in rows, with each row representing a different video sequence.  Each row shows the ground truth frames (labeled \"Ground truth\") next to the frames generated by iVideoGPT (labeled \"Predicted\"). This allows for a direct visual comparison of the model's predictions to the actual video footage, highlighting the model's ability to generate high-resolution videos that accurately depict the robotic manipulation tasks shown.", "section": "4 Experiments"}, {"figure_path": "4TENzBftZR/figures/figures_25_1.jpg", "caption": "Figure 1: Practical applications of iVideoGPT, which is designed to scale, allowing pre-training on millions of human and robotic manipulation trajectories. This results in a single, versatile foundation of interactive world models, adaptable to a wide range of downstream tasks.", "description": "The figure shows how iVideoGPT, a scalable interactive world model, is trained on a large dataset of human and robot manipulation trajectories.  This pre-training allows iVideoGPT to be used for a variety of downstream tasks, such as video prediction, visual planning, and model-based reinforcement learning.  The versatility is highlighted by showing examples of human manipulation, robotic manipulation, video prediction, visual planning, and model-based reinforcement learning applications.", "section": "1 Introduction"}, {"figure_path": "4TENzBftZR/figures/figures_25_2.jpg", "caption": "Figure 3: Architecture of iVideoGPT, simplified to show only a single context frame (T0 = 1). (a) Compressive tokenization utilizes a conditional VQGAN that discretizes future frames conditioned on context frames to handle temporal redundancy, significantly reducing the number of video tokens. (b) An autoregressive transformer integrates multimodal signals-visual observations, actions, and rewards-into a sequence of tokens, enabling interactive agent experiences through next-token prediction. Actions and rewards are optional and not included in action-free video pre-training.", "description": "This figure illustrates the architecture of iVideoGPT.  Panel (a) shows the compressive tokenization process using a conditional VQGAN to efficiently represent video frames by encoding only essential dynamics information in future frames, conditioned on the context frames from the past.  Panel (b) depicts the autoregressive transformer that processes visual observations, actions (optional), and rewards (optional) as a sequence of tokens to produce interactive video predictions.  This architecture enables scalable interactive prediction.", "section": "3.1 Architecture"}, {"figure_path": "4TENzBftZR/figures/figures_26_1.jpg", "caption": "Figure 20: Human study. Videos generated by three models, VideoGPT, MCVD, and iVideoGPT, on the action-free BAIR dataset are presented to human users, who label their preferences based on the physical naturalness and feasibility of robot-object interactions.", "description": "This figure shows the results of a human evaluation comparing video prediction results from three different models: VideoGPT, MCVD, and iVideoGPT.  Human participants were shown videos generated by each model and asked to rate their preference based on how natural and feasible the robot-object interactions looked.  The results are presented as percentages showing which model was preferred more often in pairwise comparisons.", "section": "B.2 Human Study"}, {"figure_path": "4TENzBftZR/figures/figures_27_1.jpg", "caption": "Figure 7: Visual model-based RL on Meta-world. (Left) Aggregated results report interquartile mean and 95% confidence interval (CI) [2] across a total of 30 runs over six tasks. (Right) Individual results for each task, report mean and 95% CI across five runs, measuring success rates over 20 evaluation episodes. PT denotes pre-training.", "description": "This figure shows the results of visual model-based reinforcement learning experiments conducted on the Meta-world benchmark.  The left panel presents an aggregated view of the success rates across six different tasks, showing the interquartile mean and 95% confidence intervals over 30 runs.  The right panel provides a more detailed breakdown of the performance for each task individually, displaying the mean success rate and 95% confidence intervals over 5 runs with each run consisting of 20 episodes.  The results are compared between different model-based RL algorithms, including one that leverages the pre-trained iVideoGPT world model.", "section": "4.3 Visual Model-based Reinforcement Learning"}, {"figure_path": "4TENzBftZR/figures/figures_29_1.jpg", "caption": "Figure 11: Qualitative evaluation on the Open X-Embodiment dataset for Dreamerv3-XL pre-trained on the same dataset as ours.", "description": "This figure shows a qualitative comparison of video prediction results between iVideoGPT and DreamerV3-XL. Both models were pre-trained on the same Open X-Embodiment dataset, allowing for a fair comparison of their performance. The figure showcases sample video predictions generated by both models, enabling visual inspection of their respective strengths and limitations.  The goal is to visually demonstrate the superiority of iVideoGPT in generating realistic and coherent video predictions, compared to DreamerV3-XL.", "section": "B Extended Experimental Results"}, {"figure_path": "4TENzBftZR/figures/figures_29_2.jpg", "caption": "Figure 2: Conceptual comparison among architectures, illustrated using a single context frame (To = 1) for simplicity. (a) Recurrent architectures for world models like Dreamer [29] and MuZero [80] provide step-level interactivity but limited scalability. (b) Recent video generation advancements like VideoGPT [101] and Stable Video Diffusion [8, 7] use non-causal temporal modules that can only offer trajectory-level interactivity. (c) Our model utilizes an autoregressive transformer that separately maps each step into a sequence of tokens, achieving both scalability and interactivity.", "description": "This figure compares three different architectures for world models: recurrent models, video generation models, and the proposed interactive video prediction model.  Recurrent models, while interactive, lack scalability. Video generation models are scalable but only offer trajectory-level interactivity. The proposed model uses an autoregressive transformer to achieve both scalability and step-level interactivity.", "section": "2 Problem Formulation"}]