[{"figure_path": "4TENzBftZR/tables/tables_6_1.jpg", "caption": "Table 1: Video prediction results on the BAIR robot pushing and RoboNet datasets. We report the mean and standard deviation for each metric calculated over three runs. \"-\" marks that the value is not reported in the original papers. LPIPS and SSIM scores are scaled by 100 for convenient display.", "description": "This table presents a quantitative comparison of iVideoGPT's video prediction performance against several state-of-the-art methods on two benchmark datasets: BAIR robot pushing and RoboNet.  The metrics used for comparison include Fr\u00e9chet Video Distance (FVD), Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS).  Results are shown separately for action-free and action-conditioned video prediction, and for different resolutions (64x64 and 256x256). The table highlights iVideoGPT's competitive performance, especially in terms of FVD, indicating its ability to generate realistic and temporally consistent videos.", "section": "4.1 Video Prediction"}, {"figure_path": "4TENzBftZR/tables/tables_6_2.jpg", "caption": "Table 1: Video prediction results on the BAIR robot pushing and RoboNet datasets. We report the mean and standard deviation for each metric calculated over three runs. \"-\" marks that the value is not reported in the original papers. LPIPS and SSIM scores are scaled by 100 for convenient display.", "description": "This table presents a comparison of the iVideoGPT model's performance on video prediction tasks against other state-of-the-art methods.  The results are shown for two datasets: BAIR robot pushing and RoboNet.  Metrics used for comparison include FVD (Fr\u00e9chet Video Distance), PSNR (Peak Signal-to-Noise Ratio), SSIM (Structural Similarity Index), and LPIPS (Learned Perceptual Image Patch Similarity).  Results are provided for both 64x64 and 256x256 resolutions and with and without action conditioning.", "section": "4.1 Video Prediction"}, {"figure_path": "4TENzBftZR/tables/tables_16_1.jpg", "caption": "Table 2: Hyperparameters of iVideoGPT architectures.", "description": "This table details the hyperparameters used in the architecture of the Interactive VideoGPT model. It is broken down into two sections: VQGAN and Transformer.  The VQGAN section lists parameters such as resolution, the number of down and up blocks and layers, channel dimensions, embedding size, codebook size, normalization method and group size, and activation function. The Transformer section lists the parameters such as number of layers, number of heads, hidden dimension, feedforward dimension, dropout rate, and activation function.  The table shows separate specifications for low-resolution (64x64) and high-resolution (256x256) model variations.", "section": "A.1 Architecture"}, {"figure_path": "4TENzBftZR/tables/tables_17_1.jpg", "caption": "Table 3: Hyperparameters of iVideoGPT training and evaluation.", "description": "This table details the hyperparameters used for training and evaluating the iVideoGPT model. It is broken down by model resolution (low and high), training phase (pre-training and fine-tuning), and dataset (BAIR, RoboNet, and VP2).  Specific hyperparameters listed include GPU training days, training steps, discriminator start point, batch size, sequence length, number of context frames, number of sampled future frames, learning rate, learning rate scheduling method, weight decay, gradient clipping value, warmup steps, loss balancing method, optimizer, mixed precision, and sampling parameters (temperature and top-k).", "section": "A.1 Architecture"}, {"figure_path": "4TENzBftZR/tables/tables_19_1.jpg", "caption": "Table 4: iVideoGPT pre-training data mixture from the Open X-Embodiment [70] and Something-Something-V2 [25] datasets.", "description": "This table details the composition of the pre-training dataset for the iVideoGPT model.  It lists various datasets used, the number of trajectories from each, the step size used when sampling frames from those trajectories, and the weighting assigned to each dataset in the overall training mixture. The datasets represent a diverse range of robotic and human manipulation tasks, aiming to provide comprehensive coverage of various scene dynamics and object interactions.", "section": "3.2 Pre-Training"}, {"figure_path": "4TENzBftZR/tables/tables_21_1.jpg", "caption": "Table 5: Hyperparameters of model-based RL with iVideoGPT.", "description": "This table lists the hyperparameters used for the model-based reinforcement learning experiments in the paper.  It breaks down the settings for both the model rollout phase and the model training phase.  Specific hyperparameters include the batch size, horizon, training intervals, sequence length, and learning rate, amongst others. The table shows that the real data ratio was set to 0.5.", "section": "3.3 Fine-Tuning"}, {"figure_path": "4TENzBftZR/tables/tables_26_1.jpg", "caption": "Table 1: Video prediction results on the BAIR robot pushing and RoboNet datasets. We report the mean and standard deviation for each metric calculated over three runs. \"-\" marks that the value is not reported in the original papers. LPIPS and SSIM scores are scaled by 100 for convenient display.", "description": "This table presents a comparison of video prediction results on two datasets (BAIR and RoboNet) using several different methods, including iVideoGPT.  Metrics used to evaluate performance are FVD (lower is better), PSNR (higher is better), SSIM (higher is better), and LPIPS (lower is better).  The table shows the mean and standard deviation of results from three runs for each method.", "section": "4.1 Video Prediction"}, {"figure_path": "4TENzBftZR/tables/tables_26_2.jpg", "caption": "Table 7: Visual model-based RL on Meta-world. (Left) Aggregated results report interquartile mean and 95% confidence interval (CI) [2] across a total of 30 runs over six tasks. (Right) Individual results for each task, report mean and 95% CI across five runs, measuring success rates over 20 evaluation episodes. PT denotes pre-training.", "description": "This table presents the results of visual model-based reinforcement learning experiments conducted on six robotic manipulation tasks from the Meta-World benchmark.  The left side shows aggregated results (interquartile mean and 95% confidence interval across 30 runs), while the right side details individual task performance (mean and 95% confidence interval across 5 runs) in terms of success rate over 20 evaluation episodes.  The impact of pre-training (PT) on model performance is also highlighted.", "section": "4.3 Visual Model-based Reinforcement Learning"}, {"figure_path": "4TENzBftZR/tables/tables_27_1.jpg", "caption": "Table 8: Generation efficiency with various tokenizers, measured on an RTX 4090 GPU with a batch size of 1.", "description": "This table shows the computational efficiency of different tokenization methods (4x4, 16x16, and the proposed method) for video generation.  It reports the time taken for tokenization, generation, and detokenization steps, as well as the GPU memory usage for each method. The results highlight the computational efficiency of the proposed compressive tokenization method.", "section": "B.5 Computational Efficiency"}]