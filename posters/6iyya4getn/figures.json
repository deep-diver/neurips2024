[{"figure_path": "6IyYa4gETN/figures/figures_1_1.jpg", "caption": "Figure 1: Existing methods can only support generating a target image from one source image and one target pose.", "description": "This figure illustrates the limitations of existing pose-guided person generation methods.  It shows that current techniques can only handle the scenario where a single target image is generated from a single source image and a single target pose.  The figure visually contrasts this limitation with the capabilities of the proposed IMAGPose method, which can handle more complex scenarios (generating multiple target images or using multiple source images).", "section": "1 Introduction"}, {"figure_path": "6IyYa4gETN/figures/figures_3_1.jpg", "caption": "Figure 2: The IMAGPose is a unified conditional framework designed to generate high-fidelity and high-quality target person images under various conditions. IMAGPose aims to address the issue of detail texture loss, achieve an alignment of person images and poses, and ensure the person images' local fidelity and global consistency.", "description": "This figure illustrates the architecture of the IMAGPose model, a unified conditional framework for pose-guided person generation.  It highlights the three main modules: Feature-Level Conditioning (FLC), Image-Level Conditioning (ILC), and Cross-View Attention (CVA). The FLC module addresses the issue of missing texture details by combining low-level and high-level features. The ILC module handles diverse user scenarios by injecting variable numbers of source images and using a masking strategy. The CVA module ensures image consistency by using a combination of global and local cross-attention.  The figure shows the flow of data through these modules, starting from the source image and target pose(s) and culminating in the generation of high-quality target person images.", "section": "3 Method"}, {"figure_path": "6IyYa4gETN/figures/figures_4_1.jpg", "caption": "Figure 3: The masking strategy flexibly unify different user scenarios.", "description": "This figure illustrates how the masking strategy in the IMAGPose framework adapts to different user scenarios.  Panel (a) shows the process of generating multiple target images from multiple target poses. The input includes multiple target poses and a single source image. The multiple target images are combined into a single 'joint image', then masked according to the masking strategy. A joint pose is also created. Panel (b) demonstrates the process of generating a single target image from multiple source images and a single target pose.  This involves masking a single target image and combining it with multiple source images to create a 'joint image', along with the creation of a 'joint pose'. The masking strategy ensures that the model can handle various inputs flexibly, allowing for different combinations of source images and target poses.", "section": "3.3 Image-Level Conditioning Module"}, {"figure_path": "6IyYa4gETN/figures/figures_4_2.jpg", "caption": "Figure 4: Illustration of the CVA module.", "description": "The figure illustrates the Cross-View Attention (CVA) module's architecture.  The input feature is first split into four smaller local features. Each local feature then undergoes a projection, self-attention, and another projection before being joined back together to form a single output feature.  This process allows the CVA module to capture both global and local relationships within the image.", "section": "3.4 Cross-View Attention Module"}, {"figure_path": "6IyYa4gETN/figures/figures_6_1.jpg", "caption": "Figure 5: Qualitative comparisons with several state-of-the-art models on the DeepFashion dataset.", "description": "This figure presents a qualitative comparison of IMAGPose with other state-of-the-art models on the DeepFashion dataset.  Each row shows the ground truth (GT), source image, target pose, and the results generated by various models including ADGAN, PISE, GFLA, DPTN, CASD, NTED, PIDM, PoCoLD, CFLD, PCDMs, and IMAGPose. The figure allows for visual evaluation of the different models' performance in terms of image quality, clothing details, and pose accuracy.", "section": "4.1 Main Results"}, {"figure_path": "6IyYa4gETN/figures/figures_7_1.jpg", "caption": "Figure 6: User study results on DeepFashion in terms of R2G, G2R and Jab metric. Higher values in these three metrics indicate better performance.", "description": "This figure presents a bar chart summarizing the results of a user study conducted on the DeepFashion dataset to evaluate the performance of IMAGPose and other state-of-the-art methods.  The chart displays the percentage of images misclassified as real (G2R), the percentage of real images misclassified as generated (R2G), and the percentage of times a method's generated images were judged as superior (Jab). Higher values in all three metrics indicate better performance.  The results show that IMAGPose significantly outperforms other methods across all three metrics.", "section": "4.1 Main Results"}, {"figure_path": "6IyYa4gETN/figures/figures_7_2.jpg", "caption": "Figure 7: Quantitative comparison of IMAGPose under different user scenarios on the DeepFashion dataset.", "description": "This figure shows the quantitative comparison results of IMAGPose under three different user scenarios on the DeepFashion dataset.  Scenario T1 replicates the target pose in the joint pose; T2 replicates the source image in the joint image; and T3 uses multiple different source images.  The graph plots SSIM, LPIPS, and FID scores against training steps for each scenario. This demonstrates IMAGPose's ability to adapt and maintain competitive performance across diverse user inputs.", "section": "4.1 Main Results"}, {"figure_path": "6IyYa4gETN/figures/figures_7_3.jpg", "caption": "Figure 8: Results of speed and performance.", "description": "This figure shows a comparison of the speed and performance (SSIM) of IMAGPose and several other state-of-the-art methods.  IMAGPose demonstrates significantly faster generation speeds while maintaining competitive SSIM scores compared to methods like PoCoLD, PIDM, CFLD, and PCDMs. The results highlight the efficiency gains achieved by IMAGPose's architecture.", "section": "4.2 Ablations"}, {"figure_path": "6IyYa4gETN/figures/figures_8_1.jpg", "caption": "Figure 12: More visual comparison of our model\u2019s uniformity across different user scenarios.", "description": "This figure shows visual results comparing the performance of IMAGPose across three different user scenarios.  The first three columns demonstrate the generation of a single target image from a single source image and a single target pose, with variations in the way the input data is handled (T1, T2, T3). The last column (IMAGPose*) shows the generation of multiple target images with different poses simultaneously, from a single source image. The figure visually demonstrates the model's ability to maintain consistency and photorealism across the different conditions.", "section": "4.1 Main Results"}, {"figure_path": "6IyYa4gETN/figures/figures_14_1.jpg", "caption": "Figure 10: More qualitative comparisons between IMAGPose and SOTA methods on the DeepFashion dataset.", "description": "This figure shows a qualitative comparison of IMAGPose against several state-of-the-art methods on the DeepFashion dataset.  Each row presents a different example, showing the ground truth (GT) image and the results generated by PoCoLD, CFLD, PCDMs, and IMAGPose. The comparison highlights the differences in clothing detail, pose accuracy, and overall image quality between the different models, demonstrating IMAGPose's superior performance in generating high-fidelity and detailed person images.", "section": "C.1 More Qualitative Comparisons for IMAGPose"}, {"figure_path": "6IyYa4gETN/figures/figures_15_1.jpg", "caption": "Figure 11: (a) The schematic diagram of the common frameworks based on existing diffusion models can only support generating a target image from a single source image and a single target pose. (b) During the development of IMAGPose, we devised a proprietary model to address the scenarios of generating multiple target images with different poses simultaneously.", "description": "This figure illustrates the limitation of existing diffusion models for image generation and how IMAGPose addresses it. (a) shows the standard approach of existing models that only generates a single image using one source image and one target pose. (b) shows how IMAGPose can generate multiple images from a single source image with multiple target poses simultaneously.", "section": "3 Method"}, {"figure_path": "6IyYa4gETN/figures/figures_17_1.jpg", "caption": "Figure 12: More visual comparison of our model's uniformity across different user scenarios.", "description": "This figure shows a visual comparison of IMAGPose's performance across three different user scenarios. The first column shows the results for the default setting (one source image and one target pose), the second column shows results for generating a target image from multiple source images, and the third column shows results for generating multiple target images from a single source image.  The results demonstrate the model's consistency and ability to generate high-quality images across various scenarios.", "section": "4.1 Main Results"}, {"figure_path": "6IyYa4gETN/figures/figures_18_1.jpg", "caption": "Figure 13: An example question used in our user study for pose-guided person image synthesis.", "description": "This figure shows an example question from a user study evaluating the realism of images generated by the IMAGPose model.  Participants were asked to determine if the shown image of a woman was real or fake.  This is one example from a larger set of questions used to assess the model's performance compared to other models and human perception.", "section": "C.3 User Study"}]