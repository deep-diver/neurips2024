[{"figure_path": "lwpfH9wVkO/figures/figures_9_1.jpg", "caption": "Figure 1: Experimental results for binarised MNIST. (a) The PAC-Bayes bound on the total risk decreases when tuning the posterior via Theorem 2. (b) This is achieved by a shift in the empirical error probabilities. (c) The bound on kl(Rs(Q)||RD(Q)) is not substantially increased, meaning we still retain good control of RD(Q) after optimizing Q for this particular choice of l.", "description": "This figure shows experimental results for the binarised MNIST dataset.  Subfigure (a) displays the decrease in the PAC-Bayes bound on the total risk as the posterior is tuned using Theorem 2.  Subfigure (b) illustrates the corresponding shift in the empirical error probabilities.  Finally, subfigure (c) demonstrates that the bound on the KL-divergence between the empirical and true risk vectors does not increase significantly, indicating that good control of the true risk is maintained despite optimizing for a specific loss vector.", "section": "6 Numerical experiments"}, {"figure_path": "lwpfH9wVkO/figures/figures_16_1.jpg", "caption": "Figure 2: MNIST (first column) and HAM10000 (second column) experiments.", "description": "This figure shows the results of applying the proposed method to the MNIST and HAM10000 datasets.  The plots display the PAC-Bayes bound on the total risk, empirical error probabilities, PAC-Bayes bound on KL(Rs(Q)||RD(Q)), and KL(Q||P) over training epochs.  The first column presents results for MNIST, and the second column for HAM10000.  The plots illustrate how the algorithm adjusts the posterior distribution to minimize the bound on the total risk, while keeping the KL divergence between the posterior and prior relatively low.", "section": "6 Numerical experiments"}]