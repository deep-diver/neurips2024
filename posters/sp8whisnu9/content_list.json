[{"type": "text", "text": "Synthesize, Partition, then Adapt: Eliciting Diverse Samples from Foundation Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yeming Wen\u2217& Swarat Chaudhuri Department of Computer Science The University of Texas at Austin ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Presenting users with diverse responses from foundation models is crucial for enhancing user experience and accommodating varying preferences. However, generating multiple high-quality and diverse responses without sacrificing accuracy remains a challenge, especially when using greedy sampling. In this work, we propose a novel framework, Synthesize-Partition-Adapt (SPA), that leverages the abundant synthetic data available in many domains to elicit diverse responses from foundation models. By leveraging signal provided by data attribution methods such as influence function, SPA partitions data into subsets, each targeting unique aspects of the data, and trains multiple model adaptations optimized for these subsets. Experimental results demonstrate the effectiveness of our approach in diversifying foundation model responses while maintaining high quality, showcased through the HumanEval and MBPP tasks in the code generation domain and several tasks in the natural language understanding domain, highlighting its potential to enrich user experience across various applications. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Transformer-based foundation models have revolutionized the fields of natural language processing (NLP) and code generation with their remarkable abilities a wide range of understanding and generation tasks (Vaswani et al., 2017; Devlin et al., 2019; Brown et al., 2020; Chen et al., 2021). These models are typically pre-trained on vast amounts of text data and then undergo instruction fine-tuning \u2014 a post-training process \u2014 to improve alignment with user expectations and enhance the overall user experience (Ouyang et al., 2022). Due to the high cost of human-annotated data, synthetically generated datasets (Wang et al., 2022b) such as OSS-Instruct (Wei et al., 2023) and Alpaca (Taori et al., 2023) have become an important component of instruction tuning, demonstrating strong effectiveness in improving foundation model performance. ", "page_idx": 0}, {"type": "image", "img_path": "sp8wHIsnu9/tmp/20283ff0207e2c366fec0f623ddbf7b859baef08f11b4c93fa1128ba67c3077d.jpg", "img_caption": ["Figure 1: A user is expecting two diverse templates from the foundation model. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "To date, these synthetic datasets have been primarily used to align foundation models with instructions or to induce certain preferable behaviors. In this paper, we focus on a different use of synthetic data: in improving the diversity of foundation models\u2019 outputs. Diversifying the generated responses is crucial for accommodating diverse user preferences and enhancing user satisfaction. Consider the scenario illustrated in Fig. 1, where a user prompts a foundation model with \u201cGive me a personal website template\u201d. In this case, we would prefer the model to generate two diverse templates while maintaining good quality, providing users with a variety of styles and layouts. Conventional methods for improving diversity, such as temperature sampling (Ackley et al., 1985; Hinton et al., 2015; Wang et al., 2019, 2023), rely on sampling techniques that anneal the probabilistic distribution of outputs. These methods often trade off diversity for quality, as the generated responses may deviate from the learned distribution and produce hallucination or less coherent outputs (Lee, 2023). Moreover, these techniques are not applicable when using greedy sampling, which is often preferred for its simplicity and precision. This highlights the need for approaches that not only align foundation model outputs with user expectations but also elicit diverse responses without sacrificing quality. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we present a framework, Synthesize-Partition-Adapt (SPA), that achieves these objectives. The framework partitions the synthetic data and adapts foundation models to these partitions in the post-training stage. By leveraging the inherent diversity in the training data, this approach can generate diverse responses without compromising accuracy. The potential of partition-and-adapt approach is further amplified by the increasing availability of large-scale synthetic datasets because the utility of instruction-tuning a single model on the entire dataset diminishes. In particular, we show that influence function (Koh & Liang, 2017) can be an effective signal to partition synthetic datasets into subsets, each targeting unique aspects that elicit distinct model behaviors. However, SPA is not limited to influence function and can be extended to other partitioning strategies. By training multiple adaptations on these subsets using parameter-efficient fine-tuning techniques, such as LoRA (Hu et al., 2021), we enable the generation of diverse and accurate responses. ", "page_idx": 1}, {"type": "text", "text": "To demonstrate the effectiveness of our approach, we conduct experiments on a range of tasks in both the code generation and natural language understanding domains. We evaluate our method on the HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) datasets for code generation, as well as several natural language understanding tasks. The results showcase the ability of our approach to diversify model responses while maintaining high accuracy, highlighting its potential to enrich user experience across various applications. ", "page_idx": 1}, {"type": "text", "text": "To summarize, the main contributions of this paper are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose SPA, a novel framework that leverages synthetic data, data partitioning, and model adaptation to elicit diverse responses from foundation models.   \n\u2022 We demonstrate the effectiveness of SPA in diversifying foundation model responses while maintaining sampling quality through extensive experiments on code generation and natural language understanding tasks.   \n\u2022 We highlight the potential of SPA to leverage the increasing availability of large-scale synthetic datasets for improving the diversity of foundation model responses. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Instruction Fine-tuning ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "By fine-tuning foundation models on human-annotated data that demonstrates desired behaviors, instruction tuning aims to improve the alignment between the model\u2019s outputs and the user\u2019s intentions (Ouyang et al., 2022; Wei et al., 2021; Sanh et al., 2022). Let $\\bar{D}=\\bar{(x_{i},y_{i})_{i=1}^{N}}$ denote a dataset of input-output pairs, where $x_{i}$ represents the input instruction and $y_{i}$ represents the corresponding desired output. The objective of instruction tuning is to minimize the following loss function: $\\begin{array}{r}{\\dot{\\mathcal{L}(\\theta)}~=~-\\frac{1}{N}\\sum_{i=1}^{N}\\log_{\\theta}(\\dot{y_{i}}|x_{i})}\\end{array}$ where $\\theta$ represents the parameters of the foundation model, and $p_{\\theta}(y_{i}|x_{i})$ is the probability of generating the target response given the input . ", "page_idx": 1}, {"type": "text", "text": "Classical approaches for instruction tuning typically require a substantial amount of parallel labeled data of NL intents and gold model responses. Collecting large-scale, high-quality annotated datasets is often time-consuming and expensive. To mitigate this issue, researchers have explored the use of synthetic data for instruction tuning. By leveraging techniques such as data augmentation (Wei & Zou, 2019; Sennrich et al., 2016) and back-translation (Edunov et al., 2018), synthetic data can be generated at scale, providing a cost-effective alternative to human-annotated datasets. Furthermore, synthetic instruction-following data can also be generated from the foundation model itself (Wang et al., 2022a; Honovich et al., 2022; Taori et al., 2023; Peng et al., 2023; Wen et al., 2024, inter alia). ", "page_idx": 1}, {"type": "text", "text": "2.2 Data Attribution and influence function ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Data attribution methods aim to quantify the importance or influence of individual training points on a model\u2019s predictions. One such method is the influence function (Koh & Liang, 2017). Formally, let ${\\mathcal{L}}(\\theta)$ denote the loss function of the model, where $\\theta$ represents the model parameters. The influence of a training point $z$ on the model\u2019s parameters $\\theta$ is given by $\\mathcal{Z}(z)=-H_{\\theta}^{-1}\\nabla_{\\theta}\\mathcal{L}(z,\\theta)$ . where $H_{\\theta}$ is the Hessian matrix of the loss function with respect to the model parameters, and $\\nabla_{\\theta}\\mathcal{L}(z,\\theta)$ is the gradient of the loss function with respect to the model parameters, evaluated at the training point $z$ . Next, the influence of elevating the weight of $z$ on the loss associated with a test point $z_{t e s t}$ is: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Z}(z,z_{t e s t})=-\\nabla_{\\theta}\\mathcal{L}(z_{t e s t},\\hat{\\theta})^{\\top}H_{\\hat{\\theta}}^{-1}\\nabla_{\\theta}\\mathcal{L}(z,\\hat{\\theta})}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "It is impossible to calculate the full Hessian $H_{\\theta}^{-1}$ matrix in deep neural networks. Koh & Liang (2017) developed a simple and efficient implementation that requires only oracle access to gradients and Hessian-vector products. This implementation makes it feasible to apply influence function to large-scale models. However, the vast parameter space of foundation models presents an even greater challenge, rendering the direct application of influence function impractical. In response to this, recent advancements in Grosse et al. (2023) have further refined the methodology, enabling the application of influence function to large language models. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given a user input $\\mathbf{x}$ , our goal is to generate a diverse set of high-quality responses ${\\bf y}_{1},{\\bf y}_{2},...,{\\bf y}_{K}$ from a foundation model $\\mathcal{M}$ . One approach to generating diverse responses is to sample from the model multiple times using techniques like temperature sampling: $\\mathbf{y}_{k}\\,=\\,\\mathcal{M}(\\mathbf{x};\\theta,\\tau)$ , where $k\\,=\\,1,2,...,K$ and $\\theta$ represents the model parameters and $\\tau$ is the temperature hyperparameter. However, this approach often trades off diversity for quality as studied in Chung et al. (2023). An alternative approach is to train multiple model adaptations $\\mathcal{M}_{1},\\mathcal{M}_{2},...,\\mathcal{M}_{K}$ and sample one response from each adaptation: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\mathbf{y}}_{k}=\\mathcal{M}_{k}({\\mathbf{x}};\\theta_{k}),\\quad k=1,2,...,K,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\theta_{k}$ represents the parameters of the $k$ -th model adaptation. By training each adaptation on a different subset of the data that captures unique aspects and yields distinct model behaviors, we can generate diverse responses while maintaining their quality. Moreover, this approach allows us to elicit diverse samples even with greedy sampling, which is often preferred for maximum precision. ", "page_idx": 2}, {"type": "image", "img_path": "sp8wHIsnu9/tmp/1e79f968b23b96ab77e6a80357cc0460f7852987d79ee24423e1ead6855851cf.jpg", "img_caption": ["Figure 2: $p a s s(\\varpi1$ on HumanEval after fine-tuning on some percentage of OSSInstruct dataset (Wei et al., 2023) using LORA. The plot demonstrates the diminishing returns observed with increasing amounts of data used for parameter efficient fine-tuning. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Traditionally, training multiple model adaptations has been considered unfavorable due to the repeated training process, which can be computationally expensive and time-consuming. However, with the increasing popularity of instruction tuning, it has become common practice to go through a post-training stage using instruction data before deploying the model to users. This post-training stage presents an opportunity to train multiple model adaptations without incurring significant additional costs, making the approach more feasible and practical in real-world scenarios. ", "page_idx": 2}, {"type": "text", "text": "As the volume of synthetic data grows, the utility of fine-tuning a single model on the entire dataset diminishes due to the diminishing returns in the post-training stage, as demonstrated in Fig. 2. The $p a s s@1$ accuracy after fine-tuning on the entire synthetic dataset using LORA is roughly the same as only consuming $15\\%$ of the data2. This creates an opportunity to leverage the abundant synthetic data to train multiple model adaptations, each specializing in a specific subset of the data. In this work, we propose the Synthesize, Partition, then Adapt (SPA) framework to address the diverse response generation problem. SPA leverages existing synthetic datasets, data partitioning techniques, and parameter-efficient fine-tuning methods to train multiple model adaptations. By sampling from the collection of these adaptations, SPA generates diverse and high-quality responses, enhancing the overall user experience. ", "page_idx": 2}, {"type": "image", "img_path": "sp8wHIsnu9/tmp/a17273e30a39b7dbd6db784f4f3272799120d573a4cf4137a41c91d6e8e2aa46.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 3: An illustration of the Synthesize, Partition, then Adapt (SPA) framework. SPA partitions synthetic dataset according to data attribution scores, which can be obtained using various methods such as influence function or lexical overlap. Multiple foundation model adaptations are then trained on each subset. Sampling from the collection of these model adaptations can present users with diverse responses. SPA is not limited to a specific attribution method. ", "page_idx": 3}, {"type": "text", "text": "4 Partitioning Synthetic Data and Training Adaptations ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We present the technical details of our proposed SPA framework for training multiple adaptations. We leverage an existing synthetic dataset $\\dot{\\mathcal{D}}=\\{\\mathbf{x}_{i},\\mathbf{y}_{i}\\}_{i=1}^{N}$ for the purpose of this study. The use of an existing synthetic dataset allows us to focus on the effectiveness of the Partition then Adapt steps in eliciting diverse samples, while demonstrating the flexibility of our framework to work with various synthetic datasets. Fig. 3 provides an overview of the framework. After obtaining the synthetic data, our approach consists of three main steps: (1) computing data attribution scores for synthetic data points, (2) partitioning the synthetic dataset based on these scores, and (3) training multiple foundation model adaptations using parameter-efficient fine-tuning techniques like LORA. ", "page_idx": 3}, {"type": "text", "text": "4.1 Computing Data Attribution Scores ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Consider a pre-trained foundation model $\\mathcal{M}$ with parameters $\\theta$ . Our goal is to leverage the synthetic dataset $\\mathcal{D}$ to train a set of $K$ foundation model adaptations $\\{\\mathcal{M}_{k}\\}_{k=1}^{K}$ . Each adaptation focuses on a specific subset of the data that yields similar model behaviors. To partition the synthetic dataset, we employ data attribution methods that measure the importance of each training point to the model\u2019s predictions. Although we use influence function as an example to label the data, the SPA framework is not limited to influence function and can be extended to other data attribution methods, such as lexical overlap or TRAK (Park et al., 2023). To calculate the influence function, we first fine-tune the pre-trained foundation model $\\mathcal{M}$ on the synthetic dataset $\\mathcal{D}$ . The fine-tuning process optimizes the model parameters $\\theta$ to minimize the loss function ${\\mathcal{L}}(\\theta)$ on the synthetic dataset using LORA (Hu et al., 2021): $\\begin{array}{r}{\\mathcal{L}(\\boldsymbol{\\theta})\\;=\\;\\frac{1}{N}\\sum_{i=1}^{N}\\ell(\\mathbf{y}_{i},\\mathcal{M}(\\mathbf{x}_{i};\\boldsymbol{\\theta}))}\\end{array}$ , where $\\ell(\\cdot,\\cdot)$ is a suitable loss function, such as cross-entropy loss for language modeling tasks. This fine-tuning process yields the optimized model parameters $\\hat{\\theta}$ . ", "page_idx": 3}, {"type": "text", "text": "Next, we select a set of $M$ test queries $\\{(\\mathbf{x}_{t}^{(m)},\\mathbf{y}_{t}^{(m)})\\}_{m=1}^{M}$ , which can be a collection of questions requiring various expertise knowledge to solve. For each test query $(\\mathbf{x}_{t}^{(m)},\\mathbf{y}_{t}^{(m)})$ , we compute the influence score of each synthetic data point $(\\mathbf{x}_{i},\\mathbf{y}_{i})\\in\\mathcal{D}$ using Eq. (1): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{\\boldsymbol{Z}}((\\mathbf{x}_{i},\\mathbf{y}_{i}),(\\mathbf{x}_{t}^{(m)},\\mathbf{y}_{t}^{(m)}))=-\\nabla_{\\theta}\\ell(\\mathbf{y}_{t}^{(m)},\\mathcal{M}(\\mathbf{x}_{t}^{(m)};\\hat{\\boldsymbol{\\theta}}))^{\\top}\\boldsymbol{H}_{\\hat{\\boldsymbol{\\theta}}}^{-1}\\nabla_{\\theta}\\ell(\\mathbf{y}_{i},\\mathcal{M}(\\mathbf{x}_{i};\\hat{\\boldsymbol{\\theta}})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To efficiently compute the influence scores, we employ the stochastic estimation method proposed by Koh $\\&$ Liang (2017), which approximates the inverse Hessian-vector product using conjugate gradients. Although even this method is generally infeasible in foundation models due to their vast parameter space, the use of LORA (Hu et al., 2021) makes it feasible by significantly reducing the number of trainable parameters. The computational cost of estimating the influence of a test query between the entire dataset $\\mathcal{D}$ is the same as calculating the gradient of $\\mathcal{D}$ . Another option to address this issue is to use the K-FAC approximation of the Hessian, as proposed by Grosse et al. (2023). We focus on the LORA approach and leave the exploration of K-FAC and other approximations for future work. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "4.2 Partitioning Synthetic Dataset ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "After computing the data attribution scores for each synthetic data point with respect to the $M$ test points, we obtain an influence matrix $\\mathbf{I}\\in\\mathbb{R}^{N\\times M}$ , where $\\mathbf{I}_{i,m}$ represents the attribution score of the $i$ -th synthetic data point for the $m$ -th test point. To partition the synthetic dataset $\\mathcal{D}$ into $K$ subsets $\\{\\mathcal{D}_{k}\\}_{k=1}^{K}$ , a clustering algorithm can be applied to solve the following objective: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\left\\{\\mathcal{D}_{k}\\right\\}_{k=1}^{K}}\\sum_{k=1}^{K}\\sum_{\\left(\\mathbf{x}_{i},\\mathbf{y}_{i}\\right)\\in\\mathcal{D}_{k}}\\sum_{\\left(\\mathbf{x}_{j},\\mathbf{y}_{j}\\right)\\in\\mathcal{D}_{k}}\\left|\\mathbf{I}_{i,:}-\\mathbf{I}_{j,:}\\right|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{I}_{i},$ : denotes the $i$ -th row of the influence matrix $\\mathbf{I}$ , subject to $\\textstyle\\bigcup_{k=1}^{K}D_{k}={\\mathcal{D}}$ and $\\mathcal{D}_{k}\\cap\\mathcal{D}_{k^{\\prime}}=\\emptyset$ for all $k\\neq k^{\\prime}$ . In this work, we assume partitions are disjoint for t he simplicity of the study. ", "page_idx": 4}, {"type": "text", "text": "The clustering algorithm assigns each synthetic data point $\\left(\\mathbf{x}_{i},\\mathbf{y}_{i}\\right)$ to one of the $K$ subsets based on the similarity of its influence scores across the $M$ test points. This partitioning ensures that data points within each subset have similar impacts on the model\u2019s predictions. The choice of the clustering algorithm may depend on the specific characteristics of the dataset. For simplicity and ease of implementation, in this study, we use a ranking heuristic to partition the synthetic dataset. The details of this heuristic will be explained in the experiment section $\\S5.1$ . However, it is important to note that our SPA framework is not limited to any specific clustering algorithm. ", "page_idx": 4}, {"type": "text", "text": "4.3 Training Multiple Adaptations with LORA ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Once the synthetic dataset is partitioned into $K$ subsets, we train a foundation model $\\mathcal{M}_{k}$ for each subset $\\mathcal{D}_{k}$ using parameter-efficient fine-tuning techniques like LORA ( $\\operatorname{Hu}$ et al., 2021). LORA adapts the pre-trained foundation model parameters $\\theta$ by learning low-rank matrices $\\mathbf{A}_{k}\\,\\in\\,\\mathbb{R}^{r\\times d}$ and $\\mathbf{B}_{k}\\,\\in\\,\\dot{\\mathbb{R}}^{d\\times r}$ for each weight matrix $\\mathbf{W}\\,\\in\\,\\mathbb{R}^{d\\times d}$ in the pre-trained foundation model, where $r\\ll d$ is the rank of the adaptation matrices. ", "page_idx": 4}, {"type": "text", "text": "The adapted weight matrix $\\mathbf{W}_{k}$ for the foundation model adaptation $\\mathcal{M}_{k}$ is computed as: $\\mathbf{W}_{k}=$ $\\mathbf{W}+\\mathbf{B}_{k}\\mathbf{A}_{k}$ . During the fine-tuning process, only the adaptation matrices ${\\bf A}_{k}$ and $\\mathbf{B}_{k}$ are learned, while the pre-trained weights $\\mathbf{W}$ remain frozen. This significantly reduces the number of trainable parameters, making it feasible to train multiple foundation model adaptations with limited computational resources. The training objective for each foundation model adaptation $\\mathcal{M}_{k}$ is given by $\\begin{array}{r}{\\operatorname*{min}_{\\theta_{k}}\\frac{1}{|\\mathcal{D}_{k}|}\\sum_{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\in\\mathcal{D}_{k}}\\ell(\\mathbf{y}_{i},\\breve{\\mathcal{M}}_{k}(\\mathbf{\\tilde{x}}_{i};\\theta_{k}))}\\end{array}$ where $\\theta_{k}$ represents the parameters of $\\mathcal{M}_{k}$ , which include the pre-trained weights $\\theta$ and the LoRA adaptation matrices $\\mathbf{A}_{k},\\mathbf{B}_{k}$ . By training multiple foundation model adaptations using LORA, we can efficiently adapt the pre-trained foundation model to different subsets of the synthetic data, each focusing on a specific aspect of the data that yields similar model behaviors. This approach enables the creation of a diverse set of specialized models that capture different knowledge or expertise present in the synthetic data, while leveraging the knowledge acquired during the pre-training phase. ", "page_idx": 4}, {"type": "text", "text": "Inference with Multiple Adaptations During inference, given a user input $\\mathbf{x}$ , our goal is to generate a diverse set of responses by leveraging the multiple foundation model adaptations trained on different subsets of the synthetic data. To achieve this, we randomly sample a foundation model adaptation $\\mathcal{M}_{k}$ from the set of $K$ adaptations $\\{\\mathcal{M}_{k}\\}_{k=1}^{K}$ and generate the output y using the selected adaptation. By randomly sampling from the set of adaptations, we can generate a diverse set of responses for the user input $\\mathbf{x}$ . This approach ensures that the generated responses are not only diverse but also maintain reasonable quality. It is worth noting that this approach is compatible with various sampling techniques, such as temperature scaling, top- $\\cdot\\mathbf{k}$ and top-p sampling, which can further enhance the diversity of the generated responses. ", "page_idx": 4}, {"type": "text", "text": "To generate multiple diverse responses for the user input x, we can repeat the random sampling process multiple times, each time selecting a different adaptation and generating a response. This allows us to present the user with a set of alternative responses that capture different perspectives or styles, enhancing the overall user experience. Unlike temperature sampling, which can degrade the quality of the generated responses, our approach maintains the quality of each response by leveraging the specialized knowledge captured by each adaptation. Moreover, our approach can generate diverse samples even when greedy sampling is used. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we present the experimental setup and results for evaluating the effectiveness of our proposed SPA framework in improving the diversity of foundation model outputs. We conduct experiments on both code generation tasks such as HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) and several natural language understanding tasks. ", "page_idx": 5}, {"type": "text", "text": "5.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Base Model and Synthetic Dataset For the code generation experiments, we use CodeLLaMA 7B (Rozie\\`re et al., 2023) as the base foundation model. CodeLLaMA is a state-of-the-art language model specifically designed for code-related tasks, pre-trained on a large corpus of code and natural language data. For the synthetic dataset, we utilize the OSS-Instruct dataset (Wei et al., 2023), which consists of 75,000 code-related question-answering pairs generated by GPT-3.5 Turbo (OpenAI, 2023). In the natural language understanding domain, we employ Llama-2 13B (Touvron et al., 2023) as the base foundation model. Llama-2 is a powerful language model trained on a diverse range of web-scale data, demonstrating strong performance across various natural language understanding tasks. For the synthetic dataset, we use Platypus (Lee et al., 2023), which focuses on improving LLMs\u2019 STEM and logic knowledge. Platypus consists of a curated sub-selection of public text datasets, comprising approximately 25,000 question-answer pairs. ", "page_idx": 5}, {"type": "text", "text": "Data Attribution Scores We compare two methods for computing data attribution scores: influence function and lexical overlap. ", "page_idx": 5}, {"type": "text", "text": "For the influence-based method, we hand-write 12 examples that cover a wide range of knowledge for each domain. For each of these examples, we calculate the influence score with respect to each training example in the corresponding synthetic dataset using Equation 3. We then select the top 8 test queries whose distribution of influence scores over the dataset has the highest variance. This ensures that the selected test queries have diverse impacts on the synthetic dataset, capturing different aspects of the domain knowledge. The resulting influence matrices $\\mathbf{I}_{c o d e}\\in\\mathbb{R}^{8\\times75,\\dot{0}00}$ and $\\mathbf{I}_{n l u}\\in\\mathbb{R}^{8\\times25,000}$ are used for partitioning the OSS-Instruct and Platypus datasets, respectively. ", "page_idx": 5}, {"type": "text", "text": "For the lexical overlap method, we compute the BM25 score (Robertson et al., 1994) between each training example and the hand-written test queries. The BM25 score is calculated as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nI(z,z_{q u e r y})=\\sum_{t\\in z_{q u e r y}}\\log\\frac{N+1}{N_{t}}\\cdot\\bigg(\\frac{(k_{1}+1)f(z,t)}{k_{1}\\Big((1-b)+b\\cdot\\frac{L(z)}{L_{a v g}}\\Big)+f(z,t)}+1\\bigg)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $f(z,t)$ is the overlap count, $N$ is the number of training examples, $L(z)$ is the length of the example, and $L_{a v g}$ is the average example length. We adopted the framework and the hyperparameters in Lv & Zhai (2011). While we focus on influence function in this work, exploring the effectiveness of alternative data attribution methods like BM25 could be an interesting direction for future research. More details are provided in Appendix A. ", "page_idx": 5}, {"type": "text", "text": "Partitioning the Synthetic Datasets To train multiple foundation model adaptations, we first set the hyperparameter $K$ , which represents the total number of adaptations. We use $K=8$ for both code generation and natural langauge understanding domain. For each data point in the synthetic dataset, we aim to find the test queries that provides the most influence. Formally, for each synthetic data point $(\\mathbf{x}_{i},\\mathbf{y}_{i})$ , we assign it to the subset $\\mathcal{D}_{k}^{*}$ corresponding to the test point with the highest influence score or the BM25 score: $k^{*}\\;=\\;\\arg\\operatorname*{max}_{k\\in\\{1,...,K\\}}\\mathbf{I}_{k,i}$ . where ${\\bf I}_{k,i}$ represents either the influence matrix or the BM25 score matrix. This process partitions the OSS-Instruct dataset into $K$ groups for code generation and the Platypus dataset into $K$ groups for natural language understanding. Each group is associated with a specific test example that has the highest influence on the data points within the group. ", "page_idx": 5}, {"type": "table", "img_path": "sp8wHIsnu9/tmp/18b422bff01e91ea2d3930485893c5be2bfe0affcc64f42af1cf9cd19b2e7045.jpg", "table_caption": [], "table_footnote": ["Table 1: Results on the HumanEval and MBPP. $\\tau$ denotes the temperature used for sampling. SPA with influence function achieves the best performance in terms of diversity score and avg. KL divergence) while maintaining comparable pass $@1$ performance to the single adaptation baseline. pass $@5$ measures sample quality but also has a positive correlation with diversity. "], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "With the partitioned synthetic dataset, we train $K$ model adaptations using the LORA technique, as described in $\\S4.3$ . Each adaptation $\\mathcal{M}_{k}$ is trained on the corresponding subset $\\mathcal{D}_{k}$ of the synthetic dataset, focusing on the specific coding knowledge captured by the associated test point. ", "page_idx": 6}, {"type": "text", "text": "Evaluation Metrics We use the following two metrics to assess the diversity: ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "1. Average KL Divergence: Let $P_{i}$ and $P_{j}$ be the probability distributions of the generated responses from two model adaptations $i$ and $j$ , respectively. The KL divergence between $P_{i}$ and $P_{j}$ is defined as DKL(Pi \u2225Pj) =  x Pi(x) log PPji((xx)). The average KL divergence is calculated by averaging the pairwise KL divergence between all possible pairs of model adaptations. A higher average KL divergence indicates greater diversity among the model adaptations, ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\mathrm{Average~KL~Divergence}}={\\frac{1}{\\binom{K}{2}}}\\sum_{i=1}^{N-1}\\sum_{j=i+1}^{N}D_{K L}(P_{i}\\parallel P_{j})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "2. Sample Diversity: The average KL divergence evaluates the diversity at the distributional level. We also consider the sample diversity which measures the uniqueness of individual responses. We calculate the diversity score among $K$ randomly generated samples for each problem. The diversity score is defined as the proportion of unique samples within the generated set. Specifically, it is calculated by taking one minus the ratio of the number of duplicate pairs to the total number of generated pairs. ", "page_idx": 6}, {"type": "text", "text": "Baselines We consider two baselines in the evaluation: (1) Single Adaptation, where a single model adaptation is trained on the entire synthetic dataset using LORA, and (2) Multiple Adaptations (random), where multiple adaptations are trained on randomly partitioned subsets of the synthetic dataset using LORA. Hyperparameters used to train adaptations are provided in Appendix A. ", "page_idx": 6}, {"type": "text", "text": "5.2 Code Generation Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In the code generation domain, we evaluate the performance of our proposed methodology on two popular code generation benchmarks: HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). HumanEval consists of 164 hand-written programming problems with corresponding test cases, while MBPP contains 399 held-out programming problems collected from online resources3. These benchmarks assess the ability to generate functionally correct code. ", "page_idx": 6}, {"type": "text", "text": "pass $@k$ metric In addition to the diversity metrics, we also evaluate the sample quality by pass $@1$ and pass $@5$ , measuring the percentage of problems for which at least one of the $k$ generated samples passes all the test cases. Note that the pass $@5$ metric has a strong correlation to the diversity of the samples. More diverse samples generally lead to higher pass $@5$ for $k>1$ . ", "page_idx": 6}, {"type": "text", "text": "Tab. 1 presents the evaluation results of our SPA framework and the baselines on the HumanEval and MBPP benchmarks. For the multiple adaptation methods, including random partitioning, lexical overlap, and influence function, we use greedy decoding $(\\tau=0)$ ) to generate samples. For the single adaptation baseline, we use a temperature of $\\tau\\:=\\:0.1$ to induce some diversity in the generated samples, as greedy decoding would not produce any diversity in this case. ", "page_idx": 6}, {"type": "image", "img_path": "sp8wHIsnu9/tmp/01398ed8a3aff192485a070a375da244badee6dbf695e46b8a3a773fad515c5f.jpg", "img_caption": ["Figure 4: How sampling temperature affects $p a s s@1$ , pass $@5$ , and Diversity Score for different methods on the HumanEval benchmark. The results are averaged over 4 checkpoints. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Our primary focus is on comparing the diversity metrics, namely the Diversity Score and the Average KL Divergence (avg. KL), across the different methods. SPA with influence function achieves the highest Diversity Scores of $85\\%$ and $78\\%$ on HumanEval and MBPP, respectively, indicating that the generated samples are more unique and diverse compared to the other methods. Similarly, SPA with influence function yields the highest Average KL Divergence of 0.017 and 0.020 on HumanEval and MBPP, demonstrating greater diversity at the distributional level. ", "page_idx": 7}, {"type": "text", "text": "The random partitioning and lexical overlap approaches also improve upon the single adaptation baseline in terms of diversity metrics, but to a lesser extent than influence function. In particular, the lexical overlap induces more diversity than the random adaptations baseline. This suggests that even simpler data attribution methods can be beneficial for enhancing diversity when training multiple specialized adaptations. It is worth noting that the pass $@5$ scores, while primarily measuring sample quality, also have a positive correlation with diversity. SPA with influence function achieves the highest $p a s s@5$ scores of $69.05\\%$ and $73.68\\%$ on HumanEval and MBPP, indicating that the generated samples not only exhibit greater diversity but also maintain high quality. ", "page_idx": 7}, {"type": "text", "text": "In summary, these results underscore the effectiveness of our SPA framework in generating diverse code samples without compromising quality. By leveraging influence function for data partitioning and training multiple adaptations using LORA, SPA enables the generation of diverse and accurate code solutions, even when using greedy decoding. We also showed that training more adaptations than 8 did not lead to more diversity in Appendix B. ", "page_idx": 7}, {"type": "text", "text": "Impact of Temperature Fig. 4 presents the impact of temperature on $p a s s@1$ , pass $@5$ , and Diversity Score for different methods on the HumanEval benchmark. The first plot shows that all methods, including Single, Random, Lexical, and Influence, exhibit similar patterns in terms of $p a s s@1$ performance. They achieve maximum accuracy (around $50.2\\%)$ ) when $\\tau=0$ and gradually decrease to approximately $46.5\\%$ as the temperature increases to 0.5. ", "page_idx": 7}, {"type": "text", "text": "However, both pass $@5$ and Diversity Score improve for all methods as the temperature increases, which is expected as higher temperatures encourage the model to generate more diverse samples. Notably, SPA with influence function (Influence) maintains its advantage over other methods across all temperature values, outperforming Single, Random, and Lexical methods. Although the performance gap between Influence and other methods narrows as the temperature increases due to the inherent diversity promotion of higher temperatures, Influence still maintains a lead at $\\tau=0.5$ . ", "page_idx": 7}, {"type": "text", "text": "5.3 Natural Language Understanding Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To demonstrate the effectiveness of SPA in the natural language understanding domain, we evaluate its performance on several diverse tasks, including Big-Bench Hard (BBH) (Suzgun et al., 2022), GPQA (Rein et al., 2023), MMLU (Hendrycks et al., 2020), and WinoGrande (Sakaguchi et al., 2019). For tasks that involve multiple-choice questions, we asked the model to continue generating text even after producing an answer choice for the purpose of measuring sample diversity. As shown in Fig. 5, SPA with influence function consistently achieves higher diversity scores and average KL divergence compared to the lexical overlap and random adaptation across all tasks. Interestingly, random adaptations achieve better diversity than lexical overlap on the GPQA task, suggesting that the effectiveness of partitioning methods may change depending on the task. ", "page_idx": 7}, {"type": "image", "img_path": "sp8wHIsnu9/tmp/3194501dea32b0981c131e5d6d712ed356e743e2be8107a213ebb0399e7e1a5f.jpg", "img_caption": ["Figure 5: Average KL divergence and diversity score on various natural language understanding tasks. SPA with influence function consistently outperforms the lexical overlap and random adaptations, demonstrating its effectiveness in generating diverse samples across different NLU tasks. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "The diversity scores and average KL divergence values vary across tasks, reflecting the inherent differences in the nature and complexity of each task. Tasks like MMLU, which cover a wide range of subjects, tend to yield higher average KL divergence. We also notice that a larger gap in average KL divergence does not necessarily translate to a proportionally greater difference in diversity scores. This suggests that while average KL divergence captures the dissimilarity between the generated sample distributions, it may not always directly correlate with the actual diversity of the samples. Nonetheless, the consistent improvement achieved by SPA with influence function highlights its robustness and adaptability to various natural language understanding challenges. ", "page_idx": 8}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Sampling-based methods have been widely explored to generate diverse text from language models. One of the most common approaches is temperature sampling (Ackley et al., 1985; Hinton et al., 2015). Several studies have investigated the impact of temperature on model sampling and its effect on the diversity-quality trade-off (Caccia et al., 2018; Renze & Guven, 2024; Wang et al., 2023). Higher temperatures lead to more diverse but potentially less coherent samples, while lower temperatures produce more conservative and deterministic outputs. When using high temperatures, human interventions can help to correct errors during the sampling process (Chung et al., 2023). Dynamic temperature strategies have also been explored during the model training and inference stages (Lin et al., 2018; Zhang et al., 2018; Wang et al., 2019; Chang et al., 2023). ", "page_idx": 8}, {"type": "text", "text": "Besides adjusting temperature, top- $k$ , top- $\\cdot p$ (nucleus) sampling (Holtzman et al., 2019) and their variants are common sampling methods (Fan et al., 2018; Meister et al., 2022; Hewitt et al., 2022; Ravfogel et al., 2023), which restrict the sampling space or dynamically adjust the number of tokens considered at each step. Another line of works studied how to formulate quality-diversity trade-off as a search or RL problem (Naik et al., 2023; Lim et al., 2024; Mudgal et al., 2023; Bradley et al., 2023; Ji et al., 2023). ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In summary, we proposed SPA, which that leverages synthetic data, data partitioning, and model adaptation to elicit diverse responses from foundation models. By partitioning synthetic datasets into subsets that capture unique aspects of the data and training multiple model adaptations optimized for these subsets, SPA enables the generation of diverse and high-quality responses. ", "page_idx": 8}, {"type": "text", "text": "Limitation One main challenges is the computational cost associated with influence function, which require several extra epochs of backward passes to estimate. Future work could explore more efficient data attribution methods, such as TRAK (Park et al., 2023) and K-FAC (Grosse et al., 2023). ", "page_idx": 8}, {"type": "text", "text": "Additionally, the ranking heuristics used to approximate Eq. (4) can be replaced by more advanced clustering algorithms. Additionally, serving multiple LoRA adaptations poses significant computational challenge in real-time serving framework. Recent works such as S-LoRA and FLoRA (Sheng et al., 2023; Wen & Chaudhuri, 2024) can be considered to accommodate this overhead. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "David H. Ackley, Geoffrey E. Hinton, and Terrence J. Sejnowski. A learning algorithm for boltzmann machines. Cogn. Sci., 9:147\u2013169, 1985. URL https://api.semanticscholar.org/ CorpusID:12174018. ", "page_idx": 9}, {"type": "text", "text": "Naman Agarwal, Brian Bullins, and Elad Hazan. Second-order stochastic optimization for machine learning in linear time. J. Mach. Learn. Res., 18:116:1\u2013116:40, 2016. URL https://api. semanticscholar.org/CorpusID:10569090. ", "page_idx": 9}, {"type": "text", "text": "Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. ", "page_idx": 9}, {"type": "text", "text": "Herbie Bradley, Andrew Dai, Hannah Benita Teufel, Jenny Zhang, Koen Oostermeijer, Marco Bellagente, Jeff Clune, Kenneth O. Stanley, Gre\u00b4gory Schott, and Joel Lehman. Quality-diversity through ai feedback. ArXiv, abs/2310.13032, 2023. URL https://api.semanticscholar. org/CorpusID:264405960. ", "page_idx": 9}, {"type": "text", "text": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877\u20131901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. ", "page_idx": 9}, {"type": "text", "text": "Massimo Caccia, Lucas Caccia, William Fedus, H. Larochelle, Joelle Pineau, and Laurent Charlin. Language gans falling short. ArXiv, abs/1811.02549, 2018. URL https://api. semanticscholar.org/CorpusID:53208122. ", "page_idx": 9}, {"type": "text", "text": "Chung-Ching Chang, D. Reitter, Renat Aksitov, and Yun-Hsuan Sung. Kl-divergence guided temperature sampling. ArXiv, abs/2306.01286, 2023. URL https://api.semanticscholar.org/ CorpusID:259063711. ", "page_idx": 9}, {"type": "text", "text": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, David W. Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, Suchir Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. ArXiv, abs/2107.03374, 2021. URL https://api.semanticscholar.org/CorpusID:235755472. ", "page_idx": 9}, {"type": "text", "text": "John Joon Young Chung, Ece Kamar, and Saleema Amershi. Increasing diversity while maintaining accuracy: Text data generation with large language models and human interventions. In Annual Meeting of the Association for Computational Linguistics, 2023. URL https: //api.semanticscholar.org/CorpusID:259096160. ", "page_idx": 9}, {"type": "text", "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In North American Chapter of the Association for Computational Linguistics, 2019. URL https://api.semanticscholar.org/ CorpusID:52967399.   \nSergey Edunov, Myle Ott, Michael Auli, and David Grangier. Understanding back-translation at scale. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun\u2019ichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 489\u2013500, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1045. URL https://aclanthology.org/D18-1045.   \nAngela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Annual Meeting of the Association for Computational Linguistics, 2018. URL https://api. semanticscholar.org/CorpusID:44134226.   \nRoger Baker Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, Evan Hubinger, Kamil.e Lukovsiut.e, Karina Nguyen, Nicholas Joseph, Sam McCandlish, Jared Kaplan, and Sam Bowman. Studying large language model generalization with influence functions. ArXiv, abs/2308.03296, 2023. URL https://api.semanticscholar.org/CorpusID:260682872.   \nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Xiaodong Song, and Jacob Steinhardt. Measuring massive multitask language understanding. ArXiv, abs/2009.03300, 2020. URL https://api.semanticscholar.org/CorpusID:221516475.   \nJohn Hewitt, Christopher D. Manning, and Percy Liang. Truncation sampling as language model desmoothing. In Conference on Empirical Methods in Natural Language Processing, 2022. URL https://api.semanticscholar.org/CorpusID:253157390.   \nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. ArXiv, abs/1503.02531, 2015. URL https://api.semanticscholar.org/CorpusID: 7200347.   \nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. ArXiv, abs/1904.09751, 2019. URL https://api.semanticscholar.org/ CorpusID:127986954.   \nOr Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language models with (almost) no human labor. ArXiv, abs/2212.09689, 2022.   \nJ. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. ArXiv, abs/2106.09685, 2021. URL https://api.semanticscholar.org/CorpusID:235458009.   \nHaozhe Ji, Pei Ke, Hongning Wang, and Minlie Huang. Language model decoding as direct metrics optimization. ArXiv, abs/2310.01041, 2023. URL https://api.semanticscholar.org/ CorpusID:263605885.   \nPang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In International Conference on Machine Learning, 2017. URL https://api.semanticscholar. org/CorpusID:13193974.   \nAriel N. Lee, Cole J. Hunter, and Nataniel Ruiz. Platypus: Quick, cheap, and powerful refinement of llms. ArXiv, abs/2308.07317, 2023. URL https://api.semanticscholar.org/CorpusID: 260886870.   \nMinhyeok Lee. A mathematical investigation of hallucination and creativity in gpt models. Mathematics, 2023. URL https://api.semanticscholar.org/CorpusID:258768397.   \nBryan Lim, Manon Flageat, and Antoine Cully. Large language models as in-context ai generators for quality-diversity. ArXiv, abs/2404.15794, 2024. URL https://api.semanticscholar. org/CorpusID:269362584.   \nJunyang Lin, Xu Sun, Xuancheng Ren, Muyu Li, and Qi Su. Learning when to concentrate or divert attention: Self-adaptive attention temperature for neural machine translation. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun\u2019ichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2985\u20132990, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/ v1/D18-1331. URL https://aclanthology.org/D18-1331.   \nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https:// openreview.net/forum?id $\\equiv$ 1qvx610Cu7.   \nYuanhua Lv and ChengXiang Zhai. Lower-bounding term frequency normalization. In International Conference on Information and Knowledge Management, 2011. URL https://api. semanticscholar.org/CorpusID:14029221.   \nJames Martens. Deep learning via hessian-free optimization. In International Conference on Machine Learning, 2010. URL https://api.semanticscholar.org/CorpusID:11154521.   \nClara Meister, Tiago Pimentel, Gian Wiher, and Ryan Cotterell. Typical decoding for natural language generation. ArXiv, abs/2202.00666, 2022. URL https://api.semanticscholar.org/ CorpusID:246442062.   \nSidharth Mudgal, Jong Lee, Harish Ganapathy, YaGuang Li, Tao Wang, Yanping Huang, Zhifeng Chen, Heng-Tze Cheng, Michael Collins, Trevor Strohman, Jilin Chen, Alex Beutel, and Ahmad Beirami. Controlled decoding from language models. ArXiv, abs/2310.17022, 2023. URL https: //api.semanticscholar.org/CorpusID:264491118.   \nRanjita Naik, Varun Chandrasekaran, Mert Yuksekgonul, Hamid Palangi, and Besmira Nushi. Diversity of thought improves reasoning abilities of llms. 2023. URL https://api. semanticscholar.org/CorpusID:267938465.   \nOpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. URL https://api. semanticscholar.org/CorpusID:257532815.   \nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155, 2022.   \nSung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. Trak: Attributing model behavior at scale. In International Conference on Machine Learning, 2023. URL https://api.semanticscholar.org/CorpusID:257757261.   \nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. ArXiv, abs/2304.03277, 2023.   \nShauli Ravfogel, Yoav Goldberg, and Jacob Goldberger. Conformal nucleus sampling. In Annual Meeting of the Association for Computational Linguistics, 2023. URL https://api. semanticscholar.org/CorpusID:258479879.   \nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: A graduate-level google-proof q&a benchmark. ArXiv, abs/2311.12022, 2023. URL https://api.semanticscholar.org/CorpusID: 265295009.   \nMatthew Renze and Erhan Guven. The effect of sampling temperature on problem solving in large language models. ArXiv, abs/2402.05201, 2024. URL https://api.semanticscholar.org/ CorpusID:267547769.   \nStephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. Okapi at trec-3. In Text Retrieval Conference, 1994. URL https://api.semanticscholar. org/CorpusID:41563977. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Baptiste Rozie\\`re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Tan, Yossi Adi, Jingyu Liu, Tal Remez, Je\u00b4re\u00b4my Rapin, Artyom Kozhevnikov, I. Evtimov, Joanna Bitton, Manish P Bhatt, Cristian Canto\u00b4n Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D\u2019efossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code. ArXiv, abs/2308.12950, 2023. URL https://api.semanticscholar.org/CorpusID:261100919. ", "page_idx": 12}, {"type": "text", "text": "Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande. Communications of the ACM, 64:99 \u2013 106, 2019. URL https://api.semanticscholar.org/ CorpusID:198893658. ", "page_idx": 12}, {"type": "text", "text": "Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id $\\cdot$ 9Vrb9D0WI4. ", "page_idx": 12}, {"type": "text", "text": "Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models with monolingual data. In Katrin Erk and Noah A. Smith (eds.), Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 86\u201396, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/ P16-1009. URL https://aclanthology.org/P16-1009. ", "page_idx": 12}, {"type": "text", "text": "Ying Sheng, Shiyi Cao, Dacheng Li, Coleman Hooper, Nicholas Lee, Shuo Yang, Christopher Chou, Banghua Zhu, Lianmin Zheng, Kurt Keutzer, Joseph E. Gonzalez, and Ion Stoica. Slora: Serving thousands of concurrent lora adapters. ArXiv, abs/2311.03285, 2023. URL https://api.semanticscholar.org/CorpusID:265033787. ", "page_idx": 12}, {"type": "text", "text": "Mirac Suzgun, Nathan Scales, Nathanael Scha\u00a8rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, , and Jason Wei. Challenging bigbench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. ", "page_idx": 12}, {"type": "text", "text": "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. ", "page_idx": 12}, {"type": "text", "text": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. ", "page_idx": 12}, {"type": "text", "text": "Andrew Trotman, Antti Puurula, and Blake Burgess. Improvements to bm25 and language models examined. Proceedings of the 19th Australasian Document Computing Symposium, 2014. URL https://api.semanticscholar.org/CorpusID:207220720. ", "page_idx": 12}, {"type": "text", "text": "Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing Systems, 2017. URL https://api.semanticscholar.org/CorpusID:13756489. ", "page_idx": 12}, {"type": "text", "text": "Chi Wang, Susan Liu, and Ahmed Hassan Awadallah. Cost-effective hyperparameter optimization for large language model generation inference. ArXiv, abs/2303.04673, 2023. URL https: //api.semanticscholar.org/CorpusID:257405357. ", "page_idx": 13}, {"type": "text", "text": "Pei-Hsin Wang, Sheng-Iou Hsieh, Shih-Chieh Chang, Yu-Ting Chen, Jia-Yu Pan, Wei Wei, and DaChang Juan. Contextual temperature for language modeling. ArXiv, abs/2012.13575, 2019. URL https://api.semanticscholar.org/CorpusID:214250287. ", "page_idx": 13}, {"type": "text", "text": "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. ArXiv, abs/2212.10560, 2022a. ", "page_idx": 13}, {"type": "text", "text": "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions, 2022b. ", "page_idx": 13}, {"type": "text", "text": "Jason Wei and Kai Zou. EDA: Easy data augmentation techniques for boosting performance on text classification tasks. In Kentaro Inui, Jing Jiang, Vincent $\\mathrm{Ng}$ , and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 6382\u20136388, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1670. URL https://aclanthology.org/D19-1670. ", "page_idx": 13}, {"type": "text", "text": "Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. ArXiv, abs/2109.01652, 2021. URL https://api.semanticscholar.org/CorpusID:237416585. ", "page_idx": 13}, {"type": "text", "text": "Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Source code is all you need. arXiv preprint arXiv:2312.02120, 2023. ", "page_idx": 13}, {"type": "text", "text": "Yeming Wen and Swarat Chaudhuri. Batched low-rank adaptation of foundation models. In The Twelfth International Conference on Learning Representations, 2024. URL https:// openreview.net/forum?id=w4abltTZ2f. ", "page_idx": 13}, {"type": "text", "text": "Yeming Wen, Pengcheng Yin, Kensen Shi, Henryk Michalewski, Swarat Chaudhuri, and Alex Polozov. Grounding data science code generation with input-output specifications. ArXiv, abs/2402.08073, 2024. URL https://api.semanticscholar.org/CorpusID:267637235. ", "page_idx": 13}, {"type": "text", "text": "Xu Zhang, Felix X. Yu, Svebor Karaman, Wei Zhang, and Shih-Fu Chang. Heated-up softmax embedding. ArXiv, abs/1809.04157, 2018. URL https://api.semanticscholar.org/ CorpusID:52193504. ", "page_idx": 13}, {"type": "text", "text": "A Experimental Setup Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This section provides additional details on the experimental setup that were not included in the main content due to space constraints. ", "page_idx": 13}, {"type": "text", "text": "Computing Data Attribution Scores For the lexical overlap method, we use a publicly available BM25 (Lv & Zhai, 2011; Trotman et al., 2014) implementation written in Python and released under https://pypi.org/project/rank-bm25/. We used the default hyperparameters. ", "page_idx": 13}, {"type": "text", "text": "When calculating the influence function, we employ the conjugate gradient method with LiSSA approximation (Martens, 2010; Agarwal et al., 2016). We leverage a publicly available implementation from https://github.com/alstonlo/torch-influence/. For the OSS-Instruct dataset, we use a damping factor of 0.001, a depth of 120, and 500 repeats, following the guideline that the product of depth and repeats should be roughly equal to the dataset size. For the Platypus dataset, we use a depth of 120 and 200 repeats. It is worth noting that computing the influence function is also intensive with LORA. Each column of the I matrix in Eq. (4) requires approximately one epoch of backward passes over the entire synthetic dataset. On the OSS-Instruct dataset, this takes roughly 5 hours using a single A100 80GB GPU. However, this is offline computation which is consumed before deploying the model to users. ", "page_idx": 13}, {"type": "text", "text": "After obtaining the data attribution matrix, we observe that using the ranking heuristic presented in $\\S5.1$ leads to imbalanced partitions. To achieve more balanced partitions, we normalize the data attribution matrix before applying the heuristics. We leave the exploration of more advanced clustering algorithms, such as k-means, for future work. ", "page_idx": 14}, {"type": "text", "text": "Details for Model Adaptations In this section, we provide details on the computing resources and hyperparameters used for training the model adaptations in both the code generation and natural language understanding domains. For the code generation experiments, we use a machine with 3 A100 40GB GPUs and train each partition for 400 steps, which takes approximately 80 minutes (each partition). The hyperparameters are mostly adopted from https://github.com/ bigcode-project/starcoder/tree/main. The base model is CodeLLaMA-7B-Python, and we use bf16 precision to accelerate training. The per-device train batch size is set to 1, with a gradient accumulation step of 20. We use a learning rate of 2e-4 with a cosine learning rate scheduler and 20 warmup steps. For the LORA hyperparameters, we use a rank $(r)$ of 16, an alpha of 16. ", "page_idx": 14}, {"type": "text", "text": "In the natural language understanding domain, we train each partition for 400 steps, which takes approximately 40 minutes, using the Llama-2 13B model as the base model. The training time is shorter compared to the code generation domain because the Platypus dataset is much smaller than OSS-Instruct. The hyperparameters are mostly adopted from https://github.com/ arielnlee/Platypus. We use a per-device batch size of 1 and a gradient accumulation step of 4. The learning rate is set to 1e-4, with a total of 20 warmup steps. ", "page_idx": 14}, {"type": "text", "text": "All the computational costs mentioned in this section, including the time and resources required for computing data attribution scores and training model adaptations, are offline. These costs are incurred before deploying the models to users, and they do not affect the inference time. ", "page_idx": 14}, {"type": "image", "img_path": "sp8wHIsnu9/tmp/1ed4a902c379e80b7dd60bf488f2e928398781a8c0063032472391aee51829be.jpg", "img_caption": ["Figure 6: Diversity score as function of the number of adaptations on the HumanEval benchmark. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Impact of Number of Adaptations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we investigate the impact of the number of model adaptations on the diversity of the generated responses. We focus on the HumanEval benchmark in the code generation domain and vary the number of adaptations from 8 to 12. The results are presented in Fig. 6. ", "page_idx": 14}, {"type": "text", "text": "As shown in Fig. 6, the diversity score remains relatively stable as the number of adaptations increases from 8 to 12, regardless of the partitioning method used. These results suggest that increasing the number of adaptations beyond a certain point may not necessarily lead to an improvement in the diversity of the generated responses. ", "page_idx": 14}, {"type": "text", "text": "C Test Queries ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this appendix, we provide the hand-written test queries used in our experiments for both the code generation and text generation domains. These examples were utilized to compute data attribution scores. Most of the examples are generated by GPT-4 (OpenAI, 2023). ", "page_idx": 14}, {"type": "text", "text": "C.1 Code Generation Domain ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1. \"\"\"Title: Longest Palindromic Subsequence 2 Query: Write a function to find the longest palindromic subsequence in a given $\\hookrightarrow$ string. 3 Solution: 4 \"\"\" def longest_palindromic_subsequence(s): 6 $\\mathbf{\\deltan_{\\alpha}}=\\mathbf{\\alpha}\\mathtt{l e n}(\\mathbf{s})$ 7 dp $=$ [[0] $^*$ n for _ in range(n)] ", "page_idx": 14}, {"type": "text", "text": "8   \n9 for i in range(n):   \n10 dp[i][i] $\\mathit{\\Theta}=\\mathit{\\Theta}_{1}$   \n11   \n12 for length in range(2, $\\mathtt{n}{+}1$ ):   \n13 for i in range $(\\mathtt{n-l e n g t h+1}$ ):   \n14 $\\dot{\\textbf{\\scriptsize j}}=\\dot{\\textbf{\\i}}+$ length - 1   \n15 if $\\bf\\ S\\left[i\\right]\\tau=\\tau\\bf\\{s\\}\\,[j]$ and length $\\scriptstyle\\mathbf{\\omega}=\\ 2$ :   \n16 dp[i][j] $=~2$   \n17 e $\\mathrm{\\downarrowif~\\s\\left[i\\right]~\\==~\\s\\left[j\\right]}$ :   \n18 $\\mathrm{\\tt~dp}\\left[\\dot{\\mathfrak{i}}\\right]\\;\\left[\\dot{\\mathfrak{j}}\\right]\\;\\;=\\;\\;\\dot{\\mathsf{d p}}\\left[\\dot{\\mathfrak{i}}\\!+\\!\\!1\\right]\\;\\left[\\dot{\\mathfrak{j}}\\!-\\!\\!1\\right]\\;\\;+\\;\\;2$   \n19 else:   \n20 dp[i][j] $=$ max(dp[i+1][j], dp[i][j-1])   \n21   \n22 return dp[0][n-1]   \n23   \n24 2. \"\"\"Title: Nth Fibonacci Number   \n25 Query: Implement a function to calculate the nth Fibonacci number using dynamic   \n$\\hookrightarrow$ programming.   \n26 Solution:   \n27 \"\"\"   \n28 def fibonacci(n):   \n29 if $\\textbf{n}<=\\textbf{0}$ :   \n30 return 0   \n31 elif $\\textbf{n}==\\textbf{1}$ :   \n32 return 1   \n33   \n34 $\\begin{array}{l}{\\pmb{\\mathrm{~f~i~b~}}=\\pmb{\\mathrm{~[0]~}}*\\mathbf{\\eta}(\\pmb{\\mathrm{n~}}+\\mathbf{\\eta}1)}\\\\ {\\pmb{\\mathrm{~f~i~b~}}[1]\\;\\;=\\;\\;1}\\end{array}$   \n35   \n36   \n37 for i in range(2, $\\textbf{n}+\\textbf{1})$ :   \n38 fib[i] $=$ fib[i - 1] $^+$ fib[i - 2]   \n39   \n40 return fib[n]   \n41   \n42 3. \"\"\"Title: Sum of Two Largest Elements   \n43 Query: Create a function that takes a list of integers and returns the sum of the   \n$\\hookrightarrow$ two largest elements in the list.   \n44 Solution:   \n45 \"\"\"   \n46 def sum_of_two_largest(nums):   \n47 if len(nums) $<~2$ :   \n48 return sum(nums)   \n49   \n50 largest $=$ second_largest $=$ float('-inf')   \n51   \n52 for num in nums:   \n53 if num $>$ largest:   \n54 second_largest $=$ largest   \n55 largest $=$ num   \n56 elif num $>$ second_largest:   \n57 second_largest $=$ num   \n58   \n59 return largest $^+$ second_largest   \n60   \n61 4. \"\"\"Title: Maximum Subarray Sum   \n62 Query: Implement a function to find the maximum subarray sum in a given array of   \n$\\hookrightarrow$ integers.   \n63 Solution:   \n64 \"\"\"   \n65 def max_subarray_sum(nums):   \n66 max_sum $=$ float('-inf')   \n67 current_sum $=~0$   \n68   \n69 for num in nums:   \n70 current_sum $=$ max(num, current_sum $^+$ num)   \n71 max_sum $=$ max(max_sum, current_sum)   \n72   \n73 return max_sum   \n74   \n75 5. \"\"\"Title: First Non-Repeating Character   \n76 Query: Create a function that takes a string and returns the first non-repeating   \n$\\hookrightarrow$ character in the string.   \n77 Solution:   \n78 \"\"\"   \n79 def first_non_repeating_character(s):   \n80 char_count $\\bar{\\mathbf{\\alpha}}=\\{\\}$   \n81   \n82 for char in s:   \n83 char_count[char] $=$ char_count.get(char, 0) + 1   \n84   \n85 for char in s:   \n86 if char_count[char] $\\circleddash$ :   \n87 return char   \n88   \n89 return None   \n90   \n91 6. \"\"\"Title: Merge Two Sorted Lists   \n92 Query: Write a function to merge two sorted lists into a single sorted list.   \n93 Solution:   \n94 \"\"\"   \n95 def merge_sorted_lists(list1, list2):   \n96 merged_list $\\mathrm{~\\ensuremath~{~\\mu~=~}~}\\left[\\boldsymbol{\\mathrm{1}}\\right]$   \n97 $\\dot{\\textbf{\\textsf{i}}}=\\dot{\\textbf{\\textsf{j}}}=\\textbf{\\textsf{0}}$   \n98   \n99 while i < len(list1) and j < len(list2):   \n100 if list1[i] $<=$ list2[j]:   \n101 merged_list.append(list1[i])   \n102 $\\textbf{i}+=\\textbf{1}$   \n103 else:   \n104 merged_list.append(list2[j])   \n105 $\\ j\\ +=\\ 1$   \n106   \n107 while i $<$ len(list1):   \n108 merged_list.append(list1[i])   \n109 i $+=~1$   \n110   \n111 while j < len(list2):   \n112 merged_list.append(list2[j])   \n113 j $+=~1$   \n114   \n115 return merged_list   \n116   \n117 7. \"\"\"Title: Remove Prime Numbers from List   \n118 Query: Create a function that takes a list of integers and returns a new list with   \n$\\hookrightarrow$ all the prime numbers removed.   \n119 Solution:   \n120 \"\"\"   \n121 def is_prime(num):   \n122 if num $<~2$ :   \n123 return False   \n124 for i in range(2, $\\mathrm{int}\\left(\\bf\\mathrm{num}\\;\\;**\\;\\;0\\.5\\right)\\;+\\;\\;1)$ :   \n125 if num % $\\dot{\\textbf{1}}\\!=\\!=\\!\\textbf{0}$ :   \n126 return False   \n127 return True   \n128   \n129 def remove_prime_numbers(nums):   \n130 return [num for num in nums if not is_prime(num)]   \n131   \n132 8. \"\"\"Title: Longest Common Substring   \n133 Query: Write a function to find the longest common substring between two given   \n$\\hookrightarrow$ strings.   \n134 Solution:   \n135 \"\"\"   \n136 def longest_common_substring(str1, str2):   \n137 m, $\\textbf{n}=~\\mathtt{l e n}(\\mathtt{s t r}1)$ , len(str2)   \n138 $\\mathbf{dp}~=~[\\,[0]~*~\\,(\\mathbf{n}~+~1)\\,$ for _ in range $\\mathrm{~(~m~+~1)~}$ ]   \n139 max_length $=~0$   \n140 end_index $=~0$   \n141   \n142 for i in range(1, $\\texttt{m+1}$ :   \n143 for j in range(1, $\\texttt{n+1})$ :   \n144 if str $\\downarrow\\left[\\dot{\\bf{i}}\\mathrm{~\\boldmath~-~}\\mathrm{~\\boldmath~1~}\\right]\\;\\;=\\;\\;{\\bf{s t r}}2\\left[\\dot{\\bf{j}}\\mathrm{~\\boldmath~-~}\\mathrm{~\\boldmath~1~}\\right]\\,:$ :   \n145 d $\\mathrm{~\\tt~{[p~[i]~[j]}~}=\\mathrm{~\\tt~{dp}~[i~-~1]~[j~-~1]~\\,~+~1~}$   \n146 if dp[i][j] $>$ max_length:   \n147 max_length $=$ dp[i][j]   \n148 end_index $=\\ \\dot{\\mathsf{x}}$   \n149 else:   \n150 $\\tt d p[i]\\,[j]\\ =\\ 0\\$   \n151   \n152 start_index $=$ end_index - max_length   \n153 return str1[start_index : end_index]   \n154   \n155 9. \"\"\"Title: Kth Largest Element in an Unsorted Array   \n156 Query: Implement a function to find the kth largest element in an unsorted array.   \n157 Solution:   \n158 \"\"\"   \n159 def kth_largest_element(nums, k):   \n160 k = len(nums) - k   \n161   \n162 def partition(left, right):   \n163 pivot $=$ nums[right]   \n164 $\\dot{\\textbf{1}}=$ left - 1   \n165   \n166 for j in range(left, right):   \n167 if nums[j] $<=$ pivot:   \n168 i $+=~1$   \n169 nums[i], nums[j] $=$ nums[j], nums[i]   \n170   \n171 nums[i + 1], nums[right] $=$ nums[right], nums[i + 1]   \n172 return i $+\\_1$   \n173   \n174 def quick_select(left, right):   \n175 if left $==$ right:   \n176 return nums[left]   \n177   \n178 pivot_index $=$ partition(left, right)   \n179   \n180 if $\\textbf{k}==$ pivot_index:   \n181 return nums[k]   \n182 elif k $<$ pivot_index:   \n183 return quick_select(left, pivot_index - 1)   \n184 else:   \n185 return quick_select(pivot_index $+\\_1$ , right)   \n186   \n187 return quick_select(0, len(nums) - 1)   \n188   \n189 10. \"\"\"Title: Product of Array Elements   \n190 Query: Create a function that takes a list of integers and returns the product of   \n$\\hookrightarrow$ all the elements.   \n191 Solution:   \n192 \"\"\"   \n193 def product_of_elements(nums):   \n194 product $\\mathit{\\Theta}=\\mathit{\\Theta}1$   \n195 for num in nums:   \n196 product $\\ast=$ num   \n197 return product   \n198   \n199 11. \"\"\"Title: Binary Search   \n200 Query: Implement a function to perform binary search on a sorted list of integers.   \n201 Solution:   \n202   \n203 def binary_search(nums, target):   \n204 left $=~0$   \n205 right $=$ len(nums) - 1   \n206   \n207 while left $<=$ right:   \n208 mid $=$ (left $^+$ right) // 2   \n209   \n210 if nums[mid] $==$ target:   \n211 return mid   \n212 elif nums[mid] $<$ target:   \n213 left $=$ mid $+\\ 1$   \n214 else:   \n215 right $=$ mid - 1   \n216   \n217 return -1   \n218   \n219 12. \"\"\"Title: Find Missing Number   \n220 Query: Create a function that takes a list of integers from 0 to n (inclusive) with   \n$\\hookrightarrow$ one number missing and returns the missing number.   \n221 Solution:   \n222 \"\"\"   \n223 def find_missing_number(nums):   \n224 n = len(nums)   \n225 expected_sum $=$ $\\mathbf{\\eta}\\left(\\mathbf{n}\\mathbf{\\alpha}\\ast\\mathbf{\\eta}\\left(\\mathbf{n_{\\alpha}}+\\mathbf{\\eta}1\\right)\\right.$ ) // 2   \n226 actual_sum $=$ sum(nums)   \n227 return expected_sum - actual_sum ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "C.2 Text Generation Domain ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1 1. Title: Economic Impacts of the Black Death   \n2 Query: Explain the economic impacts of the Great Mortality in medieval Europe.   \n3 Response: The the Great Mortality drastically reduced the population of Europe, $\\hookrightarrow$ leading to severe labor shortages, higher wages, lower prices for land, and a $\\hookrightarrow$ shift in economic power from the feudal lords to the working class and $\\hookrightarrow$ merchants.   \n4   \n5 2. Title: Photosynthesis Process   \n6 Query: Describe the process of photosynthesis and its importance to the Earth's $\\hookrightarrow$ ecosystem.   \n7 Response: Photosynthesis is the process by which green plants and some other $\\hookrightarrow$ organisms use sunlight to synthesize nutrients from carbon dioxide and water. It $\\hookrightarrow$ generates oxygen as a byproduct, which is vital for most life forms on Earth.   \n8   \n9 3. Title: Calculating Travel Distance   \n10 Query: If a car travels at 60 miles per hour for 3 hours, how far has it gone? $\\hookrightarrow$ Explain your calculation.   \n11 Response: The car has traveled 180 miles, calculated as 60 miles/hour \\* 3 hours.   \n12   \n13 4. Title: Utilitarianism vs Deontological Ethics   \n14 Query: Discuss the main differences between utilitarianism and deontological ethics.   \n15 Response: Utilitarianism focuses on the outcomes or consequences of actions to $\\hookrightarrow$ determine morality, while deontological ethics considers the actions themselves $\\hookrightarrow$ and the adherence to duties or rules as the basis for morality.   \n16   \n17 5. Title: Advancements in Quantum Computing   \n18 Query: What are the key advancements in quantum computing over the last decade?   \n19 Response: Key advancements include the development of quantum supremacy, error $\\hookrightarrow$ correction, and the creation of more stable qubits, enhancing computing power $\\hookrightarrow$ and reliability.   \n20   \n21 6. Title: Wedding Traditions in India   \n22 Query: Compare the wedding traditions of Northern and Southern India.   \n23 Response: Northern Indian weddings often feature elaborate rituals like Sangeet and $\\hookrightarrow$ Mehendi, while Southern Indian weddings are marked by rituals like Kashi Yatra $\\hookrightarrow$ and Oonjal. Both have vibrant traditions but differ in cultural practices and $\\hookrightarrow$ attire.   \n24   \n25 7. Title: Deforestation in the Amazon   \n26 Query: What are the primary causes of the Amazon rainforest's deforestation and what $\\hookrightarrow$ measures are being taken to address it?   \n27 Response: Primary causes include agriculture, logging, and infrastructure $\\hookrightarrow$ development. Measures to address this include enforcement of laws, satellite $\\hookrightarrow$ monitoring, and international cooperation on sustainable practices.   \n28   \n29 8. Title: Theme of Ambition in Macbeth   \n30 Query: Analyze the theme of ambition in Shakespeare's 'Macbeth'. Response: Ambition in 'Macbeth' serves as both a driving force and a tragic flaw for $\\hookrightarrow$ the characters, particularly Macbeth, leading to his rise and eventual downfall $\\hookrightarrow$ as he succumbs to the ambition spurred by the prophecy and his wife\u2019s $\\hookrightarrow$ encouragement.   \n32   \n33 9. Title: Global Impact of Renewable Energy   \n34 Query: Discuss the global impact of renewable energy sources on climate change. Response: Renewable energy sources like solar and wind have a significant impact on $\\hookrightarrow$ mitigating climate change by reducing dependence on fossil fuels, decreasing $\\hookrightarrow$ greenhouse gas emissions, and promoting sustainability. Countries adopting $\\hookrightarrow$ renewable energy contribute to a global reduction in carbon footprints, which $\\hookrightarrow$ \u2192 can help meet the goals set by international climate agreements like the Paris $\\hookrightarrow$ Agreement.   \n36 10. Title: The Role of Artificial Intelligence in Healthcare Query: Explain how artificial intelligence is transforming healthcare.   \n39 Response: Artificial intelligence in healthcare is transforming the industry by $\\hookrightarrow$ enhancing diagnostic accuracy, improving treatment personalization, and $\\hookrightarrow$ optimizing operational efficiencies. AI applications include predictive $\\hookrightarrow$ analytics for patient management, automated imaging and diagnostics, and $\\hookrightarrow$ robot-assisted surgeries, leading to faster, more accurate patient care and $\\hookrightarrow$ reduced healthcare costs.   \n40 11. Title: Cultural Significance of Food in Japan Query: Describe the cultural significance of food in Japan and how it reflects $\\hookrightarrow$ Japanese society. Response: Food in Japan is deeply intertwined with the nation's culture, reflecting $\\hookrightarrow$ aspects of beauty, seasonality, and regional diversity. Traditional dishes like $\\hookrightarrow$ sushi and bento embody aesthetic principles and social customs, such as respect $\\hookrightarrow$ for nature and meticulous attention to detail. Food rituals, such as tea $\\hookrightarrow$ \u2192 ceremonies, also highlight the importance of mindfulness and harmony in Japanese $\\hookrightarrow$ society.   \n44   \n45 12. Title: Economic Effects of Globalization   \n46 Query: Analyze the economic effects of globalization on developing countries.   \n47 Response: Globalization has both positive and negative economic effects on $\\hookrightarrow$ developing countries. On the positive side, it allows access to international $\\hookrightarrow$ markets, increases capital inflow, and promotes technology transfer, leading to $\\hookrightarrow$ job creation and economic growth. However, it can also lead to economic $\\hookrightarrow$ dependency, cultural homogenization, and the potential exploitation of local $\\hookrightarrow$ resources and labor, which might exacerbate inequalities and social tensions. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We propose the SPA framework which is the main contribution of this paper. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: The limitation is stated in the Section 6. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: No theoretical result in this paper. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Hyper-parameters are given in the appendix. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: In the supplementary material. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: See Section 5. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The results are averaged over 4 checkpoints. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The information is given in the Appendix. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: No violation. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This work is foundational research on how to generate diverse samples. It is not directly applied in any product. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. \u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: We use existing dataset and models. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We cited the used datasets, models and the repos. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 24}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The code is included in the supplementary material. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]