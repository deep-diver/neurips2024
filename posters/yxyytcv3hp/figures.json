[{"figure_path": "YxyYTcv3hp/figures/figures_2_1.jpg", "caption": "Figure 1: Sample data x with Gaussian noise (TG) and black pixels (TB) perturbations, illustrating feature removal and performance comparison.", "description": "This figure demonstrates the challenge in evaluating the effectiveness of feature unlearning in federated learning (FL).  Three images are shown: the original image (x), an image with the mouth region replaced by Gaussian noise (xG), and an image with the mouth region replaced by a black block (xB). The accuracy of a model trained on these perturbed images is significantly lower than the accuracy of a model trained on the original image, highlighting the difficulty of creating a suitable ground truth for evaluating feature unlearning in FL. ", "section": "3.2 Challenges for Feature Unlearning in FL"}, {"figure_path": "YxyYTcv3hp/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of our proposed Ferrari framework: Initiated by the feature unlearning request from the unlearn client Cu, the server initializes the trained global model \u03b8 to Cu for local feature unlearning. Upon completion, Cu uploads the unlearned model \u03b8u to the server. Local feature unlearning minimizes the Lipschitz constant L between the original input and its perturbed feature subset, reducing feature sensitivity yet preserving the overall model performance.", "description": "This figure illustrates the Ferrari framework's workflow.  A client requests feature unlearning (step 1). The server sends the model to the client (step 2). The client performs local feature unlearning, minimizing feature sensitivity by reducing the model's output change rate relative to changes in the specified input feature (the slope in the graph, step 3). The client then uploads the updated model back to the server (step 4).", "section": "4 The Proposed Method"}, {"figure_path": "YxyYTcv3hp/figures/figures_5_1.jpg", "caption": "Figure 3: Pixel-pattern backdoor feature.", "description": "This figure shows examples of pixel-pattern backdoor features added to different datasets. A small pattern is added to images in MNIST, FMNIST, CIFAR-10, CIFAR-20, and CIFAR-100.  The top row shows the original image, and the bottom row shows the image with the added backdoor trigger. This trigger is intended to cause the model to misclassify images regardless of their actual content.", "section": "5.1 Experimental Setup"}, {"figure_path": "YxyYTcv3hp/figures/figures_5_2.jpg", "caption": "Figure 4: Biased datasets distribution.", "description": "This figure shows the distribution of biased datasets used in the experiments for biased feature unlearning.  The top row shows examples from the CMNIST dataset, illustrating bias towards color patterns. For example, the digit '3' is shown in blue and green, and the digit '8' is shown in green and blue. The bottom row shows examples from the CelebA dataset, illustrating bias towards gender and smiling attributes. For example, images of men and women are shown, along with images with or without smiles, highlighting the bias present within the training data.", "section": "5.1 Experimental Setup"}, {"figure_path": "YxyYTcv3hp/figures/figures_7_1.jpg", "caption": "Figure 5: MIA reconstruction on CelebA (unlearned mouth)", "description": "This figure shows the results of a model inversion attack (MIA) on the CelebA dataset after unlearning the \"mouth\" feature.  It compares the original image (\"Target\") with the reconstructions generated by the Baseline model (which didn't undergo unlearning), the Retrain model (which was retrained without the mouth feature), and the Ferrari model (the proposed unlearning method). The goal is to demonstrate that Ferrari effectively prevents reconstruction of the unlearned feature (mouth), protecting privacy, while the Baseline and Retrain models fail to do so.", "section": "5.3.1 Sensitive Feature Unlearning"}, {"figure_path": "YxyYTcv3hp/figures/figures_8_1.jpg", "caption": "Figure 5: MIA reconstruction on CelebA (unlearned mouth)", "description": "This figure shows the results of a model inversion attack (MIA) on the CelebA dataset after unlearning the \"mouth\" feature.  It compares the MIA reconstructions from the Baseline model (which did not undergo unlearning), a Retrained model (trained without the mouth feature), and the Ferrari model (which used the proposed federated feature unlearning method).  The goal is to visually demonstrate the effectiveness of Ferrari in preventing reconstruction of the unlearned feature.  Successful unlearning should result in poor reconstruction of the \"mouth\" in the Ferrari model's results, indicating that the model no longer retains information about this feature.", "section": "5.3.1 Sensitive Feature Unlearning"}, {"figure_path": "YxyYTcv3hp/figures/figures_9_1.jpg", "caption": "Figure 7: Computational complexity analysis comparing the runtime(s) and FLOPs for each unlearning method.", "description": "This figure compares the computational efficiency of different federated unlearning methods.  It shows a bar chart visualizing the runtime (in seconds) and FLOPs (floating point operations) for each method.  The methods are: Retrain, Fine-tune, FedCDP, FedRecovery, and the proposed Ferrari method.  The results demonstrate that the Ferrari method has significantly lower runtime and FLOPs compared to other methods, suggesting its superior efficiency for federated feature unlearning.", "section": "5.4 Computational Complexity"}, {"figure_path": "YxyYTcv3hp/figures/figures_9_2.jpg", "caption": "Figure 8: Ablation and hyper-parameter analysis on Ferrari backdoor feature unlearning. Solid line: Dr; dashed line: Du.", "description": "This figure shows the ablation study and hyperparameter analysis of the proposed Ferrari framework for backdoor feature unlearning.  The ablation study (a) compares the performance using the Lipschitz loss function (as used in Ferrari) against a Non-Lipschitz variant to highlight the importance of the bounded optimization provided by Lipschitz continuity. The hyperparameter analysis (b) shows the effects of varying the standard deviation (\u03c3) of the Gaussian noise added during perturbation on the accuracy of the retain (Dr) and unlearn (Du) datasets. Finally (c) shows the effect of using different proportions of the unlearn client's dataset (Du) on the accuracy of the retain and unlearn datasets.  The results highlight the importance of the Lipschitz loss and the impact of the hyperparameters on the effectiveness of feature unlearning.", "section": "5.5 Ablation Study and Hyper-parameter Analysis"}, {"figure_path": "YxyYTcv3hp/figures/figures_18_1.jpg", "caption": "Figure 9: MNIST", "description": "The figure displays attention maps for MNIST dataset, showcasing the attention patterns of the baseline, retrained, and Ferrari (proposed method) models on digits 0-9.  It helps visualize how each model focuses on different parts of the digit images, illustrating the change in attention patterns after feature unlearning using the Ferrari model.", "section": "5.3.2 Backdoor Feature Unlearning"}, {"figure_path": "YxyYTcv3hp/figures/figures_19_1.jpg", "caption": "Figure 10: FMNIST", "description": "This figure shows the attention map analysis for backdoor samples across model iterations of baseline, retrain, and unlearn model using the proposed Ferrari method on the FMNIST dataset.  The GradCAM attention maps are shown for each class, and for each model (Baseline, Retrain, Ferrari). This visualization helps understand how each model focuses on different features when making predictions, and how the unlearning process affects the model's attention mechanism.", "section": "A.3.1 Backdoor Feature Unlearning"}, {"figure_path": "YxyYTcv3hp/figures/figures_19_2.jpg", "caption": "Figure 11: CIFAR-10", "description": "This figure shows the attention maps for CIFAR-10 dataset across different iterations of baseline, retrain, and unlearn models using the proposed Ferrari method.  The attention maps visualize the regions of the input image that the model focuses on when making predictions.  The red boxes highlight the backdoor trigger in the input images. The baseline model strongly focuses on the backdoor trigger, while the retrain and Ferrari models show a reduced focus on the trigger and more focus on relevant features for classification.", "section": "5.3.2 Backdoor Feature Unlearning"}, {"figure_path": "YxyYTcv3hp/figures/figures_19_3.jpg", "caption": "Figure 6: Attention map analysis for bias and unbias samples across model iterations of baseline, retrain, and unlearn model using our proposed Ferrari to unlearn \u2018mouth\u2019 on CelebA dataset.", "description": "This figure shows the attention maps generated by GradCAM for the baseline model, the retrained model, and the model after applying the Ferrari method to unlearn the \"mouth\" feature in the CelebA dataset.  The top row displays images from the biased dataset, while the bottom row shows images from the unbiased dataset.  Each column represents a different example image, allowing a visual comparison of how the attention shifts across models.  The goal is to demonstrate Ferrari's effectiveness in removing attention from the targeted feature (mouth) without drastically impacting the overall model performance.", "section": "5.3.3 Biased Feature Unlearning"}, {"figure_path": "YxyYTcv3hp/figures/figures_20_1.jpg", "caption": "Figure 5: MIA reconstruction on CelebA (unlearned mouth)", "description": "This figure shows the results of a model inversion attack (MIA) on the CelebA dataset after unlearning the \"mouth\" feature.  It compares the reconstructions generated by the baseline model (which still retains information about the mouth), the retrained model (trained without the mouth feature), and the Ferrari model (using the proposed method).  The goal is to demonstrate Ferrari's effectiveness in preventing the reconstruction of the unlearned feature, thereby protecting privacy.", "section": "5.3.1 Sensitive Feature Unlearning"}, {"figure_path": "YxyYTcv3hp/figures/figures_21_1.jpg", "caption": "Figure 5: MIA reconstruction on CelebA (unlearned mouth)", "description": "This figure shows the results of a model inversion attack (MIA) performed on the CelebA dataset after unlearning the \"mouth\" feature.  It visually compares the reconstructed images of the mouth feature from the baseline model, the retrained model, and the Ferrari model (the proposed method).  The comparison highlights the effectiveness of the Ferrari framework in preventing reconstruction of the unlearned feature, thereby enhancing privacy.", "section": "5.3.1 Sensitive Feature Unlearning"}, {"figure_path": "YxyYTcv3hp/figures/figures_22_1.jpg", "caption": "Figure 5: MIA reconstruction on CelebA (unlearned mouth)", "description": "This figure shows the results of a model inversion attack (MIA) on the CelebA dataset after unlearning the \"mouth\" feature.  It compares the reconstructed images from the Baseline model (before unlearning), the Retrain model (trained on data without the mouth feature), and the Ferrari model (the proposed unlearning method). The goal is to demonstrate that Ferrari effectively prevents the reconstruction of the unlearned feature, thus preserving privacy.", "section": "5.3.1 Sensitive Feature Unlearning"}, {"figure_path": "YxyYTcv3hp/figures/figures_23_1.jpg", "caption": "Figure 15: Lipschitz and Non-Lipschitz loss analysis on backdoor feature unlearning.", "description": "This figure shows the comparison of Lipschitz and Non-Lipschitz loss functions during the backdoor feature unlearning process on different datasets. The Lipschitz loss shows a steady decrease, while the Non-Lipschitz loss fluctuates significantly, highlighting the importance of Lipschitz continuity in stabilizing the unlearning process.", "section": "A.4 Lipschitz and Non-Lipschitz Loss Analysis"}, {"figure_path": "YxyYTcv3hp/figures/figures_24_1.jpg", "caption": "Figure 17: Scability analysis of client numbers on the CIFAR-10 dataset on the accuracy of retain client dataset  and unlearn client dataset ", "description": "This figure shows the scalability analysis of the proposed Ferrari framework on the CIFAR-10 dataset. It compares the accuracy of the retain dataset (Dr) and the unlearn dataset (Du) across different numbers of clients (10, 20, and 50) for three methods: Baseline, Retrain, and Ferrari.  The results demonstrate the robustness of Ferrari's performance even with a large number of clients, indicating its suitability for large-scale federated learning settings.", "section": "5.4 Computational Complexity"}]