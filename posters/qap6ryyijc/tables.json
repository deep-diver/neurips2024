[{"figure_path": "qAP6RyYIJc/tables/tables_20_1.jpg", "caption": "Table 1: Example model responses. { ... -> ... } indicates the original and requested output.", "description": "This table shows example outputs from three different large language models (LLMs): gpt-j-6b, Llama-3-8b, and mamba-1.4b.  For each model, a specific layer is selected and an \"edit request\" is shown, indicating a factual correction to be applied. The table then shows the original hallucinated response and the edited model's corrected response. This highlights the models' ability to correct specific hallucinations without retraining, by showing successful implementation of the stealth edits discussed in the paper.", "section": "D.1 Example stealth edits"}, {"figure_path": "qAP6RyYIJc/tables/tables_20_2.jpg", "caption": "Table 2: Scaling jet-packs to 10,000 simultaneous edits from the MCF dataset.", "description": "This table presents the results of scaling up the jet-pack method to correct 10,000 hallucinations simultaneously.  It shows the performance across three different language models (Llama-3-8b, gpt-j-6b, mamba-1.4b) at specific layers within those models. Metrics reported include the edit success rate, perplexity ratio, detector false positive rates (using Wikipedia and other MCF prompts as test sets), and the worst-case false positive rates guaranteed by Theorem 2 (using both Wikipedia and other MCF prompts to estimate dimensionality). The results show high success rates and low false positive rates, demonstrating the scalability and selectivity of the jet-pack approach.", "section": "Extended Experimental Results"}, {"figure_path": "qAP6RyYIJc/tables/tables_24_1.jpg", "caption": "Table 3: Detector false positive rate (FPR) for 1,000 edits sampled from MCF, evaluated on prompts from Pile-10k.", "description": "This table presents the false positive rates (FPR) of 1000 edits sampled from the Multi-CounterFact (MCF) dataset.  The FPR is evaluated using prompts from the Pile-10k dataset. The table shows the FPR mean and standard deviation for three different language models (llama-3-8b, mamba-1.4b, and gpt-j-6b) at specific layers.  The low FPR values demonstrate the high selectivity of the edits, meaning that the edits correctly respond only to the intended prompts and not others.", "section": "D Extended Experimental Results"}, {"figure_path": "qAP6RyYIJc/tables/tables_25_1.jpg", "caption": "Table 4: Evaluation on Pile-10k for gpt-j-6b layer 18.", "description": "This table presents the evaluation results on the Pile-10k dataset for the gpt-j-6b model at layer 18. It compares the word and byte perplexity scores for the original model, three models with single in-place edits, and a model where one neuron has been set to zero.  The results demonstrate that in-place edits and removing a single neuron do not significantly affect the model's perplexity on this dataset.", "section": "D.5 Impact of model's ability over various domains"}, {"figure_path": "qAP6RyYIJc/tables/tables_25_2.jpg", "caption": "Table 5: Evaluation on TinyBenchmarks for gpt-j-6b layer 18.", "description": "This table presents the results of evaluating five different language models on the TinyBenchmarks suite of tasks. The models are: the original, unmodified model; a model with one neuron set to zero; and three models each with a single in-place edit randomly selected from the Multi-CounterFact (MCF) dataset.  The table shows the accuracy scores for four tasks: tinyARC (flexible-extract or strict-match filters), tinyHellaswag, and tinyWinogrande.  The consistent performance across the models suggests that the edits had minimal impact on overall model capabilities.", "section": "D.5 Impact of model's ability over various domains"}, {"figure_path": "qAP6RyYIJc/tables/tables_26_1.jpg", "caption": "Table 6: Evaluation on MMLU Pro for gpt-j-6b layer 18.", "description": "This table presents the results of evaluating five different language models on the MMLU Pro benchmark. The models are the original model, a model with one neuron set to zero, and three models with a single in-place edit randomly selected from the Multi-CounterFact (MCF) dataset.  The table shows the accuracy (ACC) achieved by each model on the benchmark, demonstrating the minimal impact of the edits on the model's overall performance.", "section": "D.5 Impact of model's ability over various domains"}]