{"references": [{"fullname_first_author": "Vaswani et al.", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduces the transformer architecture, the foundation of most large language models discussed in the paper."}, {"fullname_first_author": "Brown et al.", "paper_title": "Language models are few-shot learners", "publication_date": "2020-10-26", "reason": "This paper introduces the concept of few-shot learning in language models, which is highly relevant to the model editing techniques discussed."}, {"fullname_first_author": "Gao et al.", "paper_title": "A framework for few-shot language model evaluation", "publication_date": "2024-07-01", "reason": "This paper provides a framework for evaluating few-shot learning in language models, which is crucial for assessing the effectiveness of the proposed model editing techniques."}, {"fullname_first_author": "Meng et al.", "paper_title": "Locating and editing factual associations in GPT", "publication_date": "2022-12-01", "reason": "This paper introduces methods for editing factual associations in GPT models, which is closely related to the concept of stealth editing discussed in this paper."}, {"fullname_first_author": "Higham et al.", "paper_title": "On adversarial examples and stealth attacks in artificial intelligence systems", "publication_date": "2020-07-01", "reason": "This paper discusses stealth attacks in AI systems, which serves as the foundation for the discussion of stealth attacks against language models in this paper."}]}