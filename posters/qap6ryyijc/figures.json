[{"figure_path": "qAP6RyYIJc/figures/figures_0_1.jpg", "caption": "Figure 1: Intrinsic dimension n(D, \u03b4) estimated using 20,000 prompts sampled from Wikipedia. Practical systems will likely perform significantly better than this worst-case performance, and we investigate this experimentally in Section 5.", "description": "This figure shows the estimated intrinsic dimension for three different large language models (LLMs): Llama-3-8b, gpt-j-6b, and mamba-1.4b.  The intrinsic dimension is a measure of the effective dimensionality of the model's feature space and is calculated using 20,000 prompts sampled from Wikipedia.  The x-axis represents the separation threshold (\u03b4), while the y-axis represents the estimated intrinsic dimension.  The plots show that the intrinsic dimension varies across different layers of the models and is generally lower than the actual dimensionality of the feature space, particularly for larger values of \u03b4. The authors argue that this metric is crucial for predicting the success of stealth editing methods and the vulnerability of the model to stealth attacks.  The figure suggests that practical systems are likely to perform better than the worst-case performance shown in the plot, and the authors will investigate this experimentally in Section 5.", "section": "4 Theoretical foundations"}, {"figure_path": "qAP6RyYIJc/figures/figures_6_1.jpg", "caption": "Figure 1: Intrinsic dimension n(D, \u03b4) estimated using 20,000 prompts sampled from Wikipedia. Practical systems will likely perform significantly better than this worst-case performance, and we investigate this experimentally in Section 5.", "description": "This figure shows the estimated intrinsic dimension for three different large language models (LLMs): GPT-J 6B, Llama 3 8B, and Mamba 1.4B. The intrinsic dimension is a measure of the complexity of the model's feature space. A higher intrinsic dimension indicates a more complex feature space, which makes it more difficult to edit the model selectively. The figure shows that the intrinsic dimension varies across different layers of the model, and that it is generally higher in later layers. This suggests that it may be easier to edit the model selectively in earlier layers.", "section": "4 Theoretical foundations"}, {"figure_path": "qAP6RyYIJc/figures/figures_7_1.jpg", "caption": "Figure 2: Performance of in-place edits for correcting hallucinations. See Section 5 for details.", "description": "This figure shows the performance of in-place edits for correcting hallucinations in three different language models (gpt-j-6b, Llama-3-8b, mamba-1.4b) across two datasets (MCF and ZsRE).  The x-axis represents the normalized depth of the edit layer in the model. The y-axis shows different metrics: edit success rate, perplexity ratio, detector false positive rate (FPR), and the theoretically guaranteed FPR from Theorem 2. The results illustrate the selectivity and effectiveness of in-place stealth edits for correcting specific hallucinations.  The standard deviations are represented by shaded areas in each plot.", "section": "5 Experimental results"}, {"figure_path": "qAP6RyYIJc/figures/figures_8_1.jpg", "caption": "Figure 3: Jet-pack edits for correcting hallucinations in MCF. See Section 5 for details.", "description": "This figure shows the results of using jet-pack edits to correct multiple hallucinations in the MCF dataset.  It presents the edit success rate, perplexity ratio, detector false positive rate (FPR), and the theoretically guaranteed FPR from Theorem 2. The plots are separated by model (gpt-j-6b, Llama-3-8b, mamba-1.4b) and number of edits (1000, 4000) and show how these metrics vary depending on the edit layer depth (normalized).  Coloured shaded areas represent standard deviations, showing the variability in results across different edits. The lines show the mean values. The plots also include estimates of the intrinsic dimensionality, showing how it affects the accuracy of the edits.", "section": "5 Experimental results"}, {"figure_path": "qAP6RyYIJc/figures/figures_8_2.jpg", "caption": "Figure 2: Performance of in-place edits for correcting hallucinations. See Section 5 for details.", "description": "This figure displays the results of in-place edits used to correct hallucinations in language models.  It shows how the success rate of these edits varies depending on the depth of the layer in which they're implemented, across three different language models (gpt-j-6b, Llama-3-8b, mamba-1.4b) and two datasets (MCF, ZsRE). Additionally, it presents the perplexity ratio (a measure of how much the model's overall performance is affected by the edit), the detector false positive rate (the proportion of non-trigger prompts that falsely activate the edit detector), and the theoretical FPR guaranteed by Theorem 2 (based on the intrinsic dimensionality of the data).", "section": "5 Experimental results"}, {"figure_path": "qAP6RyYIJc/figures/figures_8_3.jpg", "caption": "Figure 2: Performance of in-place edits for correcting hallucinations. See Section 5 for details.", "description": "This figure shows the performance of in-place edits for correcting hallucinations in three different language models (gpt-j-6b, Llama-3-8b, mamba-1.4b) across two datasets (MCF and ZsRE).  The x-axis represents the normalized depth of the layer in which the edit is applied.  The y-axis shows three metrics: edit success rate, perplexity ratio, and detector false positive rate.  The edit success rate indicates the percentage of successful edits. The perplexity ratio measures the change in the model's perplexity after the edit. The false positive rate indicates the proportion of non-trigger prompts that incorrectly activate the edited neuron. The figure demonstrates that the performance of in-place edits is dependent on the model architecture and the layer depth.  The colored shaded areas represent the standard deviation for each metric, indicating the variability in performance across different edits. The theoretical worst-case false positive rates from Theorem 2 are included for comparison.", "section": "Experimental results"}, {"figure_path": "qAP6RyYIJc/figures/figures_9_1.jpg", "caption": "Figure 2: Performance of in-place edits for correcting hallucinations. See Section 5 for details.", "description": "This figure shows the results of in-place edits for correcting hallucinations in language models.  It presents several metrics: edit success rate (the proportion of successful edits), perplexity ratio (measuring the impact of the edits on overall model performance), false positive rate (FPR, the proportion of non-trigger prompts that falsely activate the edited response), and the theoretically guaranteed FPR based on the intrinsic dimensionality.  The results are shown for three different models (gpt-j-6b, Llama-3-8b, mamba-1.4b) and two datasets (MCF and ZsRE). The x-axis represents the edit layer depth, normalized to show results across different model depths.", "section": "5 Experimental results"}, {"figure_path": "qAP6RyYIJc/figures/figures_16_1.jpg", "caption": "Figure 7: Performance of a constructed bias direction for layer 17 of the Llama model, as the number of training inputs varies.", "description": "This figure shows how well a constructed bias direction, as described in Section B.1.5 of the paper, performs as the number of training feature vectors used to construct it increases. The plot shows the mean and standard deviation of the test feature projection onto the bias direction for different numbers of training vectors.  The x-axis represents the number of training feature vectors used and the y-axis represents the mean projection and standard deviation of the test feature vectors onto this direction. The plot shows that the mean projection remains relatively stable around 1 as the number of training vectors increases, indicating that the bias direction is effectively isolating a region near the intended trigger. The standard deviation decreases as the number of training vectors increases, suggesting that the constructed bias direction becomes more precise as more training data is used.", "section": "B.1.5 Constructing a surrogate bias for Llama and Mamba families of models"}, {"figure_path": "qAP6RyYIJc/figures/figures_19_1.jpg", "caption": "Figure 8: Estimated Theorem 2 worse case false positive rate for \u03b8\u2208 {0.05, 0.005}.", "description": "This figure shows the estimated Theorem 2 worst-case false positive rates for two different threshold values (\u03b8 = 0.05 and \u03b8 = 0.005) across three different language models (gpt-j-6b, llama-3-8b, mamba-1.4b) and two different datasets (MCF and ZsRE).  For each threshold, there are two subplots: one showing the false positive rates computed using prompts from the MCF and ZsRE datasets; and the other showing the false positive rates using prompts from the Wikipedia dataset. For each model and dataset, the plots show how the false positive rates change as the depth of the edit layer in the model increases. The plots also show the theoretical worst-case false positive rates guaranteed by Theorem 2, which are computed using estimates of the intrinsic dimensionality of the feature vectors in the model.", "section": "5 Experimental results"}, {"figure_path": "qAP6RyYIJc/figures/figures_21_1.jpg", "caption": "Figure 2: Performance of in-place edits for correcting hallucinations. See Section 5 for details.", "description": "This figure displays the results of in-place edits used to correct hallucinations in language models.  It shows the edit success rate, perplexity ratio, false positive rate, and compares empirical results to theoretical guarantees (from Theorem 2) across different models (gpt-j-6b, Llama-3-8b, mamba-1.4b) and datasets (MCF, ZsRE). The x-axis represents the normalized depth of the layer where the edit was applied.  The plots illustrate how the effectiveness of in-place edits varies depending on model architecture, dataset, and layer depth. The shaded regions represent the standard deviation.", "section": "5 Experimental results"}, {"figure_path": "qAP6RyYIJc/figures/figures_21_2.jpg", "caption": "Figure 2: Performance of in-place edits for correcting hallucinations. See Section 5 for details.", "description": "This figure shows the results of in-place edits for correcting hallucinations in three different language models (gpt-j-6b, Llama-3-8b, mamba-1.4b) using two datasets (MCF, ZsRE).  The x-axis represents the normalized depth of the layer in which the edit is applied. The y-axis shows three metrics: Edit Success Rate, Perplexity Ratio, and Detector False Positive Rate. The results illustrate the effectiveness of in-place edits in correcting specific hallucinations while minimizing changes to the overall model behavior. The plot also includes theoretical false positive rates predicted by Theorem 2, based on the intrinsic dimensionality of the datasets. This comparison helps to demonstrate the link between theoretical predictions and empirical results. The error bars represent the maximum standard deviation across various prompts in both datasets. ", "section": "5 Experimental results"}, {"figure_path": "qAP6RyYIJc/figures/figures_22_1.jpg", "caption": "Figure 2: Performance of in-place edits for correcting hallucinations. See Section 5 for details.", "description": "This figure presents the results of experiments testing the efficacy of in-place edits for correcting hallucinations in three different large language models: Llama 3 8b, GPT-J, and Mamba 1.4b.  The experiments used two datasets, MCF and ZsRE, each containing factual prompts and expected responses. The figure shows the edit success rate, perplexity ratio, detector false positive rate, and the theoretical worst-case false positive rate (guaranteed by Theorem 2) as a function of the depth of the layer in which the edit was implanted. The results demonstrate the high selectivity of the edits, with low false positive rates in intermediate and later layers for all models.  The edit success rate and perplexity ratio vary by layer and model, reflecting the challenges of manipulating different network structures.  The figure clearly shows that the worst-case theoretical false positive rate underestimates the actual performance. ", "section": "5 Experimental results"}, {"figure_path": "qAP6RyYIJc/figures/figures_22_2.jpg", "caption": "Figure 2: Performance of in-place edits for correcting hallucinations. See Section 5 for details.", "description": "This figure shows the performance of in-place edits for correcting hallucinations in three different language models (gpt-j-6b, Llama-3-8b, mamba-1.4b) using two datasets (MCF and ZsRE).  It presents three key metrics across different depths of the model layers: Edit Success Rate (the percentage of successful edits), Perplexity Ratio (comparing the perplexity of the edited model to the original model to gauge the impact of the edit), and Detector FPR (False Positive Rate, measuring how often non-target prompts trigger the edit). The results illustrate the effectiveness of in-place edits, particularly in deeper layers, showing high success rates with minimal impact on overall model performance and low false positive rates.", "section": "5 Experimental results"}, {"figure_path": "qAP6RyYIJc/figures/figures_23_1.jpg", "caption": "Figure 2: Performance of in-place edits for correcting hallucinations. See Section 5 for details.", "description": "This figure shows the performance of in-place edits for correcting hallucinations in three different large language models (LLMs): GPT-J, Llama, and Mamba.  The results are presented across two datasets (MCF and ZsRE), and various metrics are shown: edit success rate, perplexity ratio, detector false positive rate (FPR), and the theoretical FPR guaranteed by Theorem 2. The x-axis represents the edit layer depth (normalized), indicating the position of the edit within the model's architecture.  The results demonstrate the selectivity of in-place edits, showing that they can successfully correct hallucinations without significantly impacting other model functions, especially in deeper layers. The theoretical and empirical results align, confirming the importance of intrinsic dimension in the effectiveness of these edits.", "section": "5 Experimental results"}, {"figure_path": "qAP6RyYIJc/figures/figures_24_1.jpg", "caption": "Figure 2: Performance of in-place edits for correcting hallucinations. See Section 5 for details.", "description": "This figure displays the results of in-place edits used to correct hallucinations in language models.  It shows the edit success rate, perplexity ratio, false positive rate of the detector neuron (FPR), and the theoretically guaranteed FPR from Theorem 2 for three different language models (gpt-j-6b, Llama-3-8b, mamba-1.4b) and two datasets (MCF and ZsRE). The x-axis represents the normalized depth of the edit layer within the model.  The plot demonstrates that the success rate is quite high and the FPR is very low, especially in the later layers of the model, illustrating the method\u2019s effectiveness in correcting hallucinations without significantly affecting other model behaviour.  The theoretical bounds provided by Theorem 2 serve as a further validation of the methods' selectivity.", "section": "5 Experimental results"}]