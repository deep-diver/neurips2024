[{"figure_path": "2WQjNXZbhR/figures/figures_3_1.jpg", "caption": "Figure 1: Visualization of classifier boundaries for two models (a single quadratic neuron with 2-dimensional inputs and a two-layer MLP comprising 2-ReLU(10)-1) across two distinct tasks. For each task, the impact of varying training sample sizes is examined. Model comparisons are performed under uniform conditions, utilizing identical random seeds and hyperparameters for fairness. Training is executed using gradient descent with a learning rate of 0.1 over 10,000 epochs. The term \"theoretical\" denotes the optimal classifier\u2019s boundary as specified in Equation (2), while \u201cnumerical\u201d represents the empirical classification boundary obtained from the model.", "description": "This figure compares the decision boundaries of a single quadratic neuron and a two-layer Multilayer Perceptron (MLP) for two binary classification tasks with varying numbers of training samples.  The left side shows results for a task where the two classes have identical covariance matrices, while the right side depicts a task with non-identical covariance.  The plots show that the quadratic neuron converges to the theoretically optimal boundary much faster (with fewer training examples) than the MLP, especially in the more challenging task (non-identical covariance). This highlights the advantage of quadratic neurons in capturing correlations within the data.", "section": "3 Quadratic neurons possess enhanced generalization capabilities"}, {"figure_path": "2WQjNXZbhR/figures/figures_4_1.jpg", "caption": "Figure 2: Performance of two models on few-shot learning tasks with MNIST and Arabic MNIST datasets. The first model consists of 10 quadratic neurons with a 784-dimensional input, while the second model is a MLP with the configuration of 784-ReLU(8000)-10. Both models are evaluated under identical conditions using the same training protocol, which includes SGD with a learning rate of 0.1 and a batch size of 100 across 20 epochs. The term \u2018Sample size\u2019 refers to the ratio of the number of training samples to the full training set. Experiments are conducted for each model and sample ratio across ten runs, and the resulting test accuracy is depicted through an error bar plot (error bar represents the upper and lower bound for test accuracy).", "description": "The figure shows the performance comparison of two models (quadratic neuron model and two-layer MLP) on few-shot learning tasks using MNIST and Arabic MNIST datasets. The results demonstrate that the quadratic neuron model outperforms the two-layer MLP, especially when trained with limited samples.  The graph plots test accuracy against the sample size (the ratio of training samples to the whole training set).", "section": "3.2 Few-shot learning experiments on MNIST and Arabic MNIST"}, {"figure_path": "2WQjNXZbhR/figures/figures_4_2.jpg", "caption": "Figure 3: Schematic of the biological interpretation of Dit-CNNs. Dit-CNNs is inspired by neural networks in the visual system. For example, different types of cone cells encode various color (channel) information, and retinal ganglion cells receive inputs from multiple types of cone cells [22], the responses can be modeled as having receptive fields (convolutional kernels) related to different color channels (W1 * X1, W2 * X2, W3 * X3). When multiple channel inputs are present, traditional CNNs simply linearly sum the corresponding responses. In contrast, dendritic neurons integrate these inputs with an additional quadratic term based on the dendritic quadratic integration rule. This approach leads to the formulation of Dit-CNNs after simplification.", "description": "This figure illustrates how Dit-CNNs are inspired by the biological visual system.  Different cone cells process different color channels, sending signals to retinal ganglion cells. Traditional CNNs sum these signals linearly, but Dit-CNNs add a quadratic term reflecting dendritic integration, leading to their unique architecture.", "section": "4 Integrating quadratic neurons into CNNs with biological plausibility"}, {"figure_path": "2WQjNXZbhR/figures/figures_6_1.jpg", "caption": "Figure 4: Visualization of some performance results presented in Table 2 (left) and Table 3 (right).", "description": "This figure visualizes the performance results from Tables 2 and 3.  The left panel shows the test accuracy of different ResNet models (ResNet, Dit-ResNet, QResNet, QuadraResNet) on CIFAR-10, plotted against network depth. The right panel displays the top-1 accuracy of various ConvNeXt models (ConvNeXt, Dit-ConvNeXt, DeepMAD, QuadraNet) on ImageNet-1K, plotted against network size. The Dit-CNN models consistently show improved performance compared to the baselines across both datasets.", "section": "4.1 Evaluations on CIFAR-10 and CIFAR-100"}, {"figure_path": "2WQjNXZbhR/figures/figures_8_1.jpg", "caption": "Figure 5: Left: Structure of ConvNeXt highlighting three candidate layers (in red) for integrating quadratic neurons. Right: ImageNet-1k performance on different Dit-ConvNeXt-T, blue dots indicates top-1 accuracy (left blue vertical axis) while green dots indicates the accuracy post-removal of the quadratic term from Dit-ConvNeXt-T after training (right green vertical axis).", "description": "This figure shows the architecture of ConvNeXt with three candidate layers highlighted in red where quadratic neurons were integrated. The right panel shows the ImageNet-1k performance comparison between original Dit-ConvNeXt-T and Dit-ConvNeXt-T after removing the quadratic term. The results demonstrate that the quadratic term significantly contributes to improved accuracy, particularly in Block 3.", "section": "4.3.2 Incorporate quadratic neurons with minimal computational overhead"}, {"figure_path": "2WQjNXZbhR/figures/figures_18_1.jpg", "caption": "Figure 6: Comparison of Eigenvectors between covariance matrices \u03a3j and quadratic weights Aj for j \u2208 {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}. Left: Visualization of eigenvectors corresponding to the largest eigenvalue of \u03a3j alongside the most similar eigenvectors of Aj. Right: Cosine similarity metrics for ten eigenvector pairs depicted on the left.", "description": "This figure compares eigenvectors from covariance matrices (\u03a3j) and quadratic weights (Aj) for each digit class (0-9) in the MNIST dataset. The left panel visualizes these eigenvectors, showing a strong similarity between those from \u03a3j (representing data distribution) and Aj (learned by quadratic neurons). The right panel quantifies this similarity using cosine similarity, demonstrating that quadratic neurons effectively capture data correlations inherent in the MNIST dataset.", "section": "3.2 Few-shot learning experiments on MNIST and Arabic MNIST"}]