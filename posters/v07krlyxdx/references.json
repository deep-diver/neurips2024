{"references": [{"fullname_first_author": "I. J. Goodfellow", "paper_title": "Explaining and harnessing adversarial examples", "publication_date": "2014", "reason": "This paper is foundational to the study of adversarial robustness in deep learning, introducing the concept and highlighting its importance for AI safety and trustworthiness."}, {"fullname_first_author": "M. Hein", "paper_title": "Formal guarantees on the robustness of a classifier against adversarial manipulation", "publication_date": "2017", "reason": "This work provides theoretical guarantees for robustness against adversarial examples, using Lipschitz continuity as a key concept, which is central to the current paper's approach."}, {"fullname_first_author": "J. Cohen", "paper_title": "Certified adversarial robustness via randomized smoothing", "publication_date": "2019", "reason": "This paper presents a method for certifying the adversarial robustness of neural networks, which is related to and provides a contrast to the certification method proposed in the current paper."}, {"fullname_first_author": "A. Madry", "paper_title": "Towards deep learning models resistant to adversarial attacks", "publication_date": "2017", "reason": "This influential paper introduces the concept of adversarial training as a defense against adversarial attacks, a method which the current paper expands on and complements."}, {"fullname_first_author": "N. Tishby", "paper_title": "The information bottleneck method", "publication_date": "2000", "reason": "This paper introduces the information bottleneck method for understanding model generalization and robustness by analyzing information flow, relevant to the paper's focus on knowledge continuity."}]}