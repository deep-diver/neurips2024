[{"figure_path": "FXdMgfCDer/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of the proposed TPP approach. During training, for each graph task t, the task prototype pt is generated by applying Laplacian smoothing on the graph Gt and added to P = {p\u00b9, ..., pt-1}. At the same time, the graph prompt It and the classification head \u03c6t for this task are optimized on Gt through a frozen pre-trained GNN. During inference, the task ID of the test graph is first inferred (i.e., task identification). Then, the graph prompt and the classifier of the predicted task are retrieved to perform the node classification in GCIL. The GNN is trained on G1 and remains frozen for subsequent tasks.", "description": "This figure illustrates the training and inference processes of the proposed Task Profiling and Prompting (TPP) approach for Graph Class-Incremental Learning (GCIL). During training, a task prototype is generated for each task using Laplacian smoothing, and a task-specific graph prompt is learned using a pre-trained GNN.  The GNN remains frozen after initial training.  During inference, the task ID is predicted using the task prototypes and the corresponding graph prompt and classifier are retrieved to classify nodes in the test graph.", "section": "3.2 Overview of The Proposed TPP Approach"}, {"figure_path": "FXdMgfCDer/figures/figures_4_1.jpg", "caption": "Figure 1: (a) Classification space of two graph tasks when no task ID is provided. The classification space is split into two separate spaces in Task 1 in (b) and Task 2 in (c) when the task ID can be accurately predicted. This helps alleviate the inter-task class separation issue. To mitigate catastrophic forgetting, we learn a graph prompt for each task that absorbs task-specific discriminative information for better class separation within each task, as shown in (d) and (e) respectively. This essentially results in a separate classification model for each task, achieving fully forget-free GCIL models.", "description": "This figure illustrates the effect of task ID prediction and graph prompting on the classification space of two graph tasks.  (a) shows the overlapping classification spaces when task IDs aren't provided. (b) and (c) demonstrate how accurate task ID prediction separates the classification spaces.  (d) and (e) show how graph prompting further improves class separation within each task, effectively creating separate classification models and mitigating catastrophic forgetting.", "section": "1 Introduction"}, {"figure_path": "FXdMgfCDer/figures/figures_8_1.jpg", "caption": "Figure 1: (a) Classification space of two graph tasks when no task ID is provided. The classification space is split into two separate spaces in Task 1 in (b) and Task 2 in (c) when the task ID can be accurately predicted. This helps alleviate the inter-task class separation issue. To mitigate catastrophic forgetting, we learn a graph prompt for each task that absorbs task-specific discriminative information for better class separation within each task, as shown in (d) and (e) respectively. This essentially results in a separate classification model for each task, achieving fully forget-free GCIL models.", "description": "This figure illustrates the impact of task ID prediction and graph prompting on the classification space in graph class-incremental learning (GCIL).  (a) shows overlapping classes from two tasks without task IDs. (b) and (c) show the separated classification spaces when accurate task ID prediction is used. (d) and (e) demonstrate the effect of graph prompting in further separating classes within each task, resulting in forget-free learning.", "section": "1 Introduction"}]