[{"type": "text", "text": "BLAST: Block-Level Adaptive Structured Matrices for Efficient Deep Neural Network Inference ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Changwoo Lee Soo Min Kwon Qing Qu Hun-Seok Kim University of Michigan {cwoolee,kwonsm,qingqu,hunseok}@umich.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large-scale foundation models have demonstrated exceptional performance in language and vision tasks. However, the numerous dense matrix-vector operations involved in these large networks pose significant computational challenges during inference. To address these challenges, we introduce the Block-Level Adaptive STructured (BLAST) matrix, designed to learn and leverage efficient structures prevalent in the weight matrices of linear layers within deep learning models. Compared to existing structured matrices, the BLAST matrix offers substantial flexibility, as it can represent various types of structures that are either learned from data or computed from pre-existing weight matrices. We demonstrate the efficiency of using the BLAST matrix for compressing both language and vision tasks, showing that (i) for medium-sized models such as ViT and GPT-2, training with BLAST weights boosts performance while reducing complexity by $70\\%$ and $40\\%$ , respectively; and (ii) for large foundation models such as Llama-7B and DiT-XL, the BLAST matrix achieves a $2\\mathbf{x}$ compression while exhibiting the lowest performance degradation among all tested structured matrices. Our code is available at https://github.com/changwoolee/BLAST. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Foundation models built on large deep neural networks (DNNs) have demonstrated remarkable performance in vision and language tasks. However, the size of these large networks poses both computational and storage challenges, especially in resource-constrained environments such as edge devices. The size of a single DNN often exceeds the capacity of the supporting hardware devices [1\u20135]. For example, Llama-70B [1] demands at least 140GB of memory solely for loading its weights in half-precision floating point representation, while the state-of-the-art commercial GPU only accommodates 80GB of memory. Furthermore, inference with these networks involves numerous dense matrix-vector operations, which can be limiting when computing power is constrained. ", "page_idx": 0}, {"type": "text", "text": "Fortunately, large (overparameterized) DNNs often exhibits parameter redundancy, where the intrinsic dimension of the weights is much lower than the ambient dimension. As such, the weights should be structured, possessing hidden properties such as low-rankness [6\u20139] or sparsity [10, 11]. Hence, it is possible to replace (or factorize) these dense existing weight matrices with structured ones without degrading performance [10\u201312]. However, using structured matrices that do not align with the true underlying structure of the weight matrices can result in significant performance degradation. We demonstrate this point in Figure 1 where we attempt to capture the structure of a diffusion model transformer (DiT) [13] using the low-rank structure to generate synthetic images. In Figure 1, we compress the model\u2019s linear layers by approximately $50\\%$ of the total number of parameters using low-rank weight matrices via singular value decomposition (SVD) and generate images with the compressed model (see Section 4.2 and Appendix C.3 for details). As shown in Figure 1 (middle), simply using the low-rank structure introduces unwanted artifacts in the generated images. ", "page_idx": 0}, {"type": "image", "img_path": "n0arS0DDot/tmp/586aae05383accc3e581277bad08d137734a88972889e39fe39881c171e51494.jpg", "img_caption": ["Figure 1: Examples of generated images using DiT [13] starting from the same noise vectors and a deterministic solver. The original model is compressed by $50\\%$ through BLAST or low-rank matrices and re-trained for 10 epochs on ImageNet. The images from the model compressed via BLAST preserve the quality of the images of the original model, whereas the images generated by the low-rank model contain more undesired artifacts. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "To address this issue, many flexible structures for modeling DNN weights have been proposed to minimize the misalignment between imposed and true low-dimensional structures. For example, Dao et al. [14] proposed the Monarch matrix, a specific type of Block Low-Rank (BLR) structure [15], in which all blocks share the same rank, intended for use in the linear layers of transformers [16]. Matrix multiplication with a Monarch matrix can be performed efficiently using batched matrix multiplication routines. Additionally, Chen et al. [17] investigated a block sparse plus low-rank structure. However, all of these methods still suffer from the fact that the underlying structure of each weight matrix is not known a priori. By imposing one of these structures, performance degradation may still occur due to misalignment. Recently, Lee and Kim [12] introduced a data-driven design called Generalized Block Low-Rank (GBLR). This approach employs multiple rank-1 blocks with various sizes and locations learned from data via differentiable masks. Unfortunately, the GBLR matrix is optimized for custom-designed hardware, as the learned block patterns are random. It has limited usability on general GPUs as the computation of GBLR matrices does not accelerate well on them. ", "page_idx": 1}, {"type": "text", "text": "In this work, we introduce the Block-Level Adaptive Structured (BLAST) matrix, a versatile and efficient design tailored to uncover various low-dimensional structures in the weight matrices of DNNs for accelerated inference on GPUs. Our matrix structure leverages shared bases across block matrices with block-wise diagonal coupling factors. This structure encapsulates different structures such as low-rank, block low-rank, block-diagonal matrices, and their combinations. BLAST matrices can be applied to the training scenario from scratch or compression after training. For training from scratch, we let the linear layers of the DNN to directly adopt the BLAST structure and learn its factors from data. The factors of the BLAST matrix are constructed to have well-defined gradients, allowing them to be optimized using popular methods like stochastic gradient descent (SGD) or Adam [18]. For compressing existing weights, we propose a factorization algorithm to learn the BLAST factors from pre-trained weights. The compression performance can be further improved by updating the BLAST factors using data, a process we call \u201cre-training\u201d. ", "page_idx": 1}, {"type": "text", "text": "We demonstrate the efficiency of BLAST by training Vision Transformers (ViT) [19] and GPT-2 [20] from scratch on various datasets, showing that it can reduce complexity by $70\\%$ and $40\\%$ , respectively. We also compress existing ViT and Diffusion Transformer (DiT) [13] models with BLAST matrices by $70\\%$ and $50\\%$ , respectively, demonstrating that BLAST compression (and re-training) achieves higher accuracy $/$ quality compared to existing methods for ViT and DiT (see Figure 1). For the language tasks, we compress Llama-7B [1] by $50\\%$ via BLAST and re-train on 0.49B tokens, showing the lowest accuracy degradation with significant inference speedup on a NVIDIA A100 GPU. Overall, our contributions can be summarized as follows: ", "page_idx": 1}, {"type": "image", "img_path": "n0arS0DDot/tmp/3b2cb3b5cab817c96ba41b26264fc898c621df78ff0f816677642c239efe0f7a.jpg", "img_caption": ["Figure 2: Existing structured matrices and our proposed BLAST matrix. The unique structure of BLAST allows for flexible matrix structures while enabling faster matrix multiplication compared to existing matrices. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "\u2022 We propose a novel block-structured matrix called BLAST that encompasses a wide range of matrix structures, allowing for faster matrix multiplication. Various existing structured matrices such as Low-Rank, Monarch [14], and Block Diagonal matrices can be expressed using the BLAST matrix. \u2022 We provide gradient descent-based methods to find the BLAST factors for DNN weights. We empirically show that standard DNN training with the BLAST weight matrices effectively recovers the original accuracy while achieving up to a $70\\%$ reduction in computational complexity. \u2022 In cases where pre-trained dense weights are available, we propose a preconditioned gradient descent factorization algorithm to decompose the weights to BLAST factors for compression and further re-training. Our experimental results show that pre-trained foundation models for vision or language tasks can be compressed by $50\\%$ using BLAST matrices. ", "page_idx": 2}, {"type": "text", "text": "Notation and Organization. We use $\\sigma_{1}(X)$ to denote the largest singular value of the matrix $\\mathbf{\\deltaX}$ The notation $\\odot$ indicates Hadamard product. ", "page_idx": 2}, {"type": "text", "text": "The rest of the paper is organized as follows. In Section 2, we introduce the BLAST matrix and discuss its properties. In Section 3, we propose a methodology to train/compress DNNs with BLAST weight matrices. In Section 4, we demonstrate the effectiveness of the BLAST weights in improving efficiency without noticeable accuracy degradation. We discuss related works in Section 5, and conclude in Section 6. ", "page_idx": 2}, {"type": "text", "text": "2 Block-Level Adaptive Structured (BLAST) Matrix ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Consider a square matrix1 $A\\,\\in\\,\\mathbb{R}^{n\\times n}$ for some $\\textit{n}\\in\\mathbb{N}$ , which has an unknown intrinsic lowdimensional structure. We first equally partition the matrix $\\pmb{A}$ into $b\\times b$ blocks of size $p\\times p$ where $b,p\\in\\mathbb{N}$ are constants such that $n=b p$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A=\\left[\\!\\!\\begin{array}{c c c c}{A_{1,1}}&{A_{1,2}}&{\\cdot\\cdot\\cdot}&{A_{1,b}}\\\\ {A_{2,1}}&{A_{2,2}}&{\\cdot\\cdot}&{A_{2,b}}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {A_{b,1}}&{A_{b,2}}&{\\cdot\\cdot}&{A_{b,b}}\\end{array}\\!\\!\\right],\\quad A_{i,j}\\in\\mathbb{R}^{p\\times p},\\quad i,j\\in[b].}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Then, the BLAST matrix parameterizes each block matrix $A_{i,j}$ using three factors: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A_{i,j}=U_{i}S_{i,j}V_{j}^{T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $U_{i},V_{j}\\,\\in\\,\\mathbb{R}^{p\\times r}$ are the left and the right factors, respectively, and $S_{i,j}\\,=\\,\\mathrm{diag}(s_{i,j})$ is an $r\\times r$ diagonal matrix whose diagonal entries are $\\pmb{s}_{i,j}\\in\\mathbb{R}^{r}$ . We provide a visual representation on the rightmost side of Figure 2, and illustrate how this structure differs from other types of matrices. While the BLAST structure may appear similar to SVD, there are two notable differences: (i) the left and right factors do not need to be orthonormal, and (ii) the diagonal entries do not need to be positive. These distinctions make it more flexible in capturing different types of low-rank structures. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Factor Sharing: The left factor matrix $U_{i}$ of size $r p$ is shared across $b$ blocks at the $i^{\\mathrm{th}}$ row, i.e., $A_{i,1},\\ldots,A_{i,b}$ . Likewise, the right factor $V_{j}$ is shared across the blocks at the $j^{\\mathrm{th}}$ column. On the other hand, the diagonal factor ${\\boldsymbol{s}}_{i,j}$ of size $r$ is specific to each block $A_{i,j}$ . Hence the total number of parameters of an $n\\times n$ BLAST matrix with $b\\times b$ number of blocks of rank $r$ is $2r p b+r b^{2}=2{\\dot{n}}r+r b^{2}$ . This reduces the number of parameters $b$ times by enforcing the blocks at the same row or column share the same bases. ", "page_idx": 3}, {"type": "text", "text": "\u2022 Individual Diagonal Factors: The individual diagonal factors of each block matrix are the source of the adaptivity and flexibility of the BLAST matrix. By changing the values of the diagonal factors, the BLAST matrix can encompass a wide variety of matrix structures. These factors can be estimated using gradient descent, since $s_{i,j}$ is a real-valued vector and $\\begin{array}{r}{\\pmb{A}_{i,j}=U_{i}\\mathrm{diag}(s_{i,j})V_{j}^{T}}\\end{array}$ is linear to si,j. ", "page_idx": 3}, {"type": "text", "text": "Low-Rank Matrices as Special Cases of BLAST To demonstrate how the BLAST matrix can capture different types of structures, we present an example showing how the BLAST matrix can encompass a low-rank matrix. Consider the case where all the diagonal factors are ones, i.e., ${\\boldsymbol{s}}_{i,j}=\\mathbf{1}_{r}$ for all $i,j=1,2,\\dots,b$ . Then, we can write the block matrix as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nU V^{T}={\\left[\\begin{array}{l}{U_{1}}\\\\ {U_{2}}\\\\ {\\vdots}\\\\ {U_{b}}\\end{array}\\right]}\\left[V_{1}\\quad V_{2}\\quad\\cdot\\cdot\\cdot V_{b}\\right]={\\left[\\begin{array}{l l l l l}{U_{1}V_{1}^{T}}&{U_{1}V_{2}^{T}}&{\\cdot\\cdot\\cdot}&{U_{1}V_{b}^{T}}\\\\ {U_{2}V_{1}^{T}}&{U_{2}V_{2}^{T}}&{\\cdot\\cdot\\cdot}&{U_{2}V_{b}^{T}}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {U_{b}V_{1}^{T}}&{U_{b}V_{2}^{T}}&{\\cdot\\cdot}&{U_{b}V_{b}^{T}}\\end{array}\\right]}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Hence, if the true underlying structure is low-rank, we can expect the BLAST matrix to learn this specific structure. Similarly, we show in Section A.1 that the BLAST matrix can construct low-rank, block-diagonal, and block low-rank matrices through different diagonal parameters. A combination of these canonical structured matrices, such as a low-rank with block-diagonal matrix, can also be achieved by simply concatenating the factors of each matrix. ", "page_idx": 3}, {"type": "text", "text": "Matrix Multiplication DNNs involve numerous matrix-vector (matrix-matrix) multiplications in the form of $\\textbf{\\textit{y}}=$ $\\mathbf{\\deltaAx}$ $\\begin{array}{r l r}{\\mathbf{\\nabla}Y}&{{}=}&{A X)}\\end{array}$ ). Algorithm 1 depicts the BLAST matrix-vector multiplication procedure. Consider the partitioned input vector $\\textbf{\\em x}~=~[\\pmb{x}_{1}^{T},\\pmb{x}_{2}^{T},\\cdot~\\cdot~,\\pmb{x}_{b}^{T}]^{T}$ and the partitioned output vector $\\textit{\\textbf{y}}=$ $[{\\pmb y}_{1}^{T},{\\pmb y}_{2}^{T},\\therefore\\;,{\\pmb y}_{b}^{T}]^{T}$ . The $i^{\\mathrm{th}}$ partitioned output vector $\\pmb{y}_{i}$ is then computed by the sum of the $b$ block-wise matrix-vector multiplications along $j=1,\\ldots,b$ : ", "page_idx": 3}, {"type": "table", "img_path": "n0arS0DDot/tmp/17a9339f224366c72d2cdc671930514d54c4d3c92c8bc27fa27624af2db525b7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "equation", "text": "$$\ny_{i}=\\sum_{j=1}^{b}A_{i,j}x_{j}=\\sum_{j=1}^{b}U_{i}S_{i,j}V_{j}^{T}x_{j}=U_{i}\\left(\\sum_{j=1}^{b}S_{i,j}\\left(V_{j}^{T}x_{j}\\right)\\right),\\quad i=1,\\ldots,b.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The number of multiplications required to perform the matrix-vector multiplication $\\pmb{y}\\,=\\,A\\pmb{x}$ is $(2n+b^{2})r$ . The matrix multiplication $z_{j}=\\mathring{V}_{j}^{T}\\pmb{x}_{j}$ , $j=1,\\dots,b$ is computed once and shared across $i=1,\\dots,b$ , whereas the matrix multiplications in Line 3 and Line 6 of Algorithm 1 can be executed in parallel, e.g., by torch.bmm in PyTorch [21]. An implementation of Algorithm 1 for general matrix or tensor inputs can be found in Appendix A. ", "page_idx": 3}, {"type": "text", "text": "3 Applications of BLAST Matrices ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "There are two main applications of BLAST matrices: (i) training from scratch with the BLAST structure and (ii) compression of pre-trained weights using BLAST factorization. ", "page_idx": 3}, {"type": "text", "text": "3.1 Training from Scratch using BLAST Matrices ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To train a DNN on a dataset, parameters are typically initialized randomly and updated through stochastic gradient descent. In this setting, BLAST can replace dense weights to learn structures from the training data. Instead of using random dense weight matrices, the model is initialized with random BLAST factors $U_{i},V_{j},s_{i,j}$ . Since the forward and the backward path of the linear layer involving the weight matrix is composed of three linear operations as in Equation (3), the derivatives of the minibatch loss can be back-propagated by automatic differentiation frameworks [21]. Hence, all of the trainable parameters of BLAST can be updated using conventional optimizers (e.g., Adam [18] or AdamW [22]) without additional treatment. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.2 Compressing Weights via BLAST Factorization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "BLAST Factorization via Gradient Descent Given pre-trained dense weights of a DNN, we can compress the weights using BLAST matrices. Let $\\pmb{A}$ denote the weight matrix and $A_{i,j}$ denote its blocks. We estimate the BLAST factors of $A_{i,j}$ by finding the factors of the BLAST matrix that minimize the Frobenius norm error between the original weight matrix and the BLAST structure: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\ell(\\boldsymbol{U}_{*},\\boldsymbol{V}_{*},\\boldsymbol{s}_{*,*})=\\sum_{i=1}^{b}\\sum_{j=1}^{b}\\frac{1}{2}\\left\\|\\boldsymbol{A}_{i,j}-\\boldsymbol{U}_{i}\\mathrm{diag}(\\boldsymbol{s}_{i,j})\\boldsymbol{V}_{j}^{T}\\right\\|_{F}^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $^*$ denotes the collection of all $b$ components along the axis. This problem shares many characteristics with the classical matrix factorization problem [23\u201326], and hence we can solve for the factors using alternating gradient descent starting from small random initialization (e.g., Line 1 of Algorithm 2) [27, 8]. That is, the $k^{\\mathrm{th}}$ gradient descent step is composed of three alternating updates with a step size $\\eta>0$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U_{i}^{(k+1)}\\leftarrow U_{i}^{(k)}-\\eta_{U_{i}^{(k)}}\\cdot\\nabla_{U_{i}^{(k)}}\\ell\\left(U_{*}^{(k)},V_{*}^{(k)},s_{*,*}^{(k)}\\right),}\\\\ &{V_{j}^{(k+1)}\\leftarrow V_{j}^{(k)}-\\eta_{V_{j}^{(k)}}\\cdot\\nabla_{V_{j}^{(k)}}\\ell\\left(U_{*}^{(k+1)},V_{*}^{(k)},s_{*,*}^{(k)}\\right),}\\\\ &{s_{i,j}^{(k+1)}\\leftarrow s_{i,j}^{(k)}-\\eta_{s_{i,j}^{(k)}}\\cdot\\nabla_{s_{i,j}^{(k)}}\\ell\\left(U_{*}^{(k+1)},V_{*}^{(k+1)},s_{*,*}^{(k)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "With properly chosen step sizes, Equations (5) to (7) always decrease the loss value whenever the current variables do not have any infinite entries and the gradient is non-zero. Using notations V\u00af i(k) $\\left[S_{i,1}^{(k)}V_{1}^{(k)T}\\cdot\\cdot\\cdot S_{i,b}^{(k)}V_{b}^{(k)T}\\right]^{T}$ and $\\bar{U}_{j}^{(k)}\\,=\\,\\left[(U_{1}^{(k+1)}S_{1,j}^{(k)})^{T}\\cdot\\cdot\\cdot(U_{b}^{(k+1)}S_{b,j}^{(k)})^{T}\\right]^{T}$ to indicate the concatenation of the right and left factors scaled by the diagonal components, the loss is monotonically non-increasing as in the following theorem. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Let $A_{i,j}\\in\\mathbb{R}^{p\\times p}$ be a target block and $U_{i}^{(k)},V_{j}^{(k)}\\in\\mathbb{R}^{p\\times r}$ )\u2208Rp\u00d7r, and si(,kj) $\\pmb{s}_{i,j}^{(k)}\\in\\mathbb{R}^{r}$ be factors of a block in the BLAST matrix to be optimized. With the step sizes $0<\\eta_{U_{i}^{(k)}}\\leq1/\\sigma_{1}\\left(\\bar{V}_{i}^{(k)T}\\bar{V}_{i}^{(k)}\\right)$ 0 $\\begin{array}{r}{\\mid\\,<\\,\\eta_{V_{j}^{(k)}}\\,\\leq\\,1/\\sigma_{1}\\left(\\bar{U}_{j}^{(k)T}\\bar{U}_{j}^{(k)}\\right),\\,0\\,<\\,\\eta_{s_{i,j}^{(k)}}\\,\\leq\\,1/\\sigma_{1}\\left((U_{i}^{(k+1)T}U_{i}^{(k+1)})\\odot(V_{j}^{(k+1)T}V_{j}^{(k+1)})\\right)\\!,}\\end{array}$ the gradient descent updates in Equations (5) $t o$ (7) monotonically non-increase the loss: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\ell(\\pmb{U}_{*}^{(k+1)},\\pmb{V}_{*}^{(k+1)},\\pmb{s}_{*,*}^{(k+1)})\\leq\\ell(\\pmb{U}_{*}^{(k)},\\pmb{V}_{*}^{(k)},\\pmb{s}_{*,*}^{(k)}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The proof of Theorem 1 is in Section B, which is an application of the descent lemma from classical optimization theory. ", "page_idx": 4}, {"type": "text", "text": "Blast Factorization via Preconditioned Gradient Descent Recall that in order to estimate the BLAST factors given a pre-trained weight $\\pmb{A}$ , we need to choose a rank $r$ . Since we do not know the rank of $\\pmb{A}$ a priori, we may have to overestimate the rank. However, overestimating the rank may slow down the convergence rate for solving Equation (4). To illustrate this, we performed an empirical analysis of the convergence behavior on a synthetically generated target low-rank matrix $\\pmb{A}$ , whose dimension is $256\\times256$ with a true rank of $r^{*}=8$ . For the analysis, we computed the factors of a BLAST matrix with $b=16$ for various values of $r$ . We used linearly decaying step sizes $\\eta^{(k)}=\\eta_{U_{i}^{(k)}}=\\eta_{V_{j}^{(k)}}=\\eta_{s_{i,j}^{(k)}}$ . When $r=r^{*}=8$ (the ranks of the left and right factors $U_{*},V_{*}$ match the actual rank of $\\pmb{A}$ ), gradient descent finds a low-rank solution with minimal error within 30 iterations, as shown by the blue curve in Figure 3-left. However, in the case of $r=32>r^{*}$ where the BLAST factors are overparameterized, we observed slower convergence and a substantial residual error after 100 steps as shown by the blue curve in Figure 3-right. This behavior is consistent with previous observations of slow convergence in ill-conditioned matrix factorization problems [23, 24]. ", "page_idx": 4}, {"type": "image", "img_path": "n0arS0DDot/tmp/f7b784badeecc24c352741a5266a2197e5bac16b09281b6780a2cfc89ab10ab9.jpg", "img_caption": ["Figure 3: Convergence of the BLAST factorization with and without the preconditioning steps on noiseless low-rank matrix factorization with rank $r^{\\star}$ . Left: The BLAST parameter $r=r^{\\star}$ , Right: $r\\,>\\,r^{\\star}$ . When $r\\,>\\,r^{*}$ , the convergence rate of GD without the preconditioning is slowed down, while GD with the preconditioning (PrecGD) can recover the ground truth with small error. "], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "n0arS0DDot/tmp/1351a4155451b78409e81dec853f986134d8c4a0cf8319a5ea10e22f86832dd3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "The convergence rate of solving the overparameterized low-rank factorization by gradient descent can be improved via inexpensive preconditioners [23, 24] which effectively decrease the condition number at each iteration. Inspired by the preconditioned gradient descent for low-rank factorization, we generalize the idea to solve our problem by multiplying preconditioning matrices to the gradients in Equations (5) to (7). We summarize the preconditioned gradient descent method for the BLAST factorization in Algorithm 2, where the following preconditioning matrices are used: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{U_{i}}^{(k)}=\\left(\\bar{V}_{i}^{(k)T}\\bar{V}_{i}^{(k)}+\\delta I\\right)^{-1},P_{V_{j}}^{(k)}=\\left(\\bar{U}_{j}^{(k)T}\\bar{U}_{j}^{(k)}+\\delta I\\right)^{-1},}\\\\ &{P_{s_{i,j}}^{(k)}=\\left(W_{i,j}^{(k)}+\\delta I\\right)^{-1},\\,W_{i,j}^{(k)}=\\left(U_{i}^{(k+1)T}U_{i}^{(k+1)}\\right)\\odot\\left(V_{j}^{(k+1)T}V_{j}^{(k+1)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "$\\delta$ is proportional to the square root of the error in Equation (4). The derivations are presented in Appendix A.2. Figure 3 shows that preconditioning improves the convergence of the overparameterized BLAST factorization. The preconditioned gradient descent (yellow curve) finds the points with low error after 100 steps, whereas the gradient descent without preconditioning fails to achieve a small error. More empirical studies on the BLAST factorization with or without preconditioning can be found in Appendix D.1. We summarize the compression procedure in Algorithm 2. The computational complexity of this compression algorithm is $O(n r^{2}+\\bar{r}^{3})$ where the cubic dependency on $r$ is from the matrix inversion steps. We emphasize that these matrix inversion steps are not substantial computational bottlenecks because $r$ is smaller than $n$ as in prior work [23, 24]. ", "page_idx": 5}, {"type": "text", "text": "After BLAST compression outlined in Algorithm 2, the BLAST factors can be used directly for inference to save both storage and computational costs. However, we observe that, instead of using the estimated factors directly, using them as initial points and refining the estimates by re-training the model with BLAST factors can further improve the performance of the compressed model. We refer to this process as \u201cre-training\u201d after BLAST compression. ", "page_idx": 5}, {"type": "image", "img_path": "n0arS0DDot/tmp/b8120efb80bab0be3e0e88c4fc54494bdfaab0c01cb1ffaa096dce52180fd35c.jpg", "img_caption": ["Figure 4: CIFAR-10/100 accuracy of ViT-S trained from scratch with different structured matrices. "], "img_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "n0arS0DDot/tmp/ee57674e0576d36ea404f811c6d28280da7b28b720d2bc597c066f99cce1afd1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 1: ImageNet validation accuracy and relative FLOPs of ViT-Base trained from scratch models with different structured weight matrices. The image and the patch sizes are $224\\times224$ and $16\\times16$ , respectively. $\\mathrm{BLAST_{3}}$ indicates the BLAST matrix with $3\\times3$ number of blocks. ", "page_idx": 6}, {"type": "text", "text": "4 Experimental Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate the BLAST matrix under two settings: (i) training from scratch with random initialization in the BLAST format, and (ii) re-training after compressing the dense weights to BLAST matrices via Algorithm 2. We compare the performance of BLAST with both non-adaptive and adaptive structured matrices. Among the non-adaptive approaches, we include low-rank (LR) matrices, Monarch for block low-rank (BLR) [14], and Pixelfly [17] or Block-Diagonal for block sparse matrices. For the adaptive and learnable structured matrix category, we evaluate Gaudi-GBLR [12]. We report the number of floating point operations (FLOPs) by counting the number of multiplications. The BLAST matrix with $b\\times b$ number of blocks is denoted by ${\\mathrm{BLAST}}_{b}$ . We used the same hyperparameter $r$ for every target weight matrix by setting it to meet the computational budget of the DNN. All experimental details can be found in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "4.1 Training from Scratch ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Image Classification We train the reduced-size Vision Transformers (ViT) [19] with $\\mathrm{BLAST_{3}}$ (BLAST with $b=3$ ) weight matrices on CIFAR-10, 100 [28], and ImageNet-1k[29] for 310 epochs from random initialization, and compare with other structured matrices. In the CIFAR-10 and CIFAR-100 benchmarks, BLAST outperforms several non-adaptive baselines, such as Low-Rank, Pixelfly, and Monarch with higher accuracy at the same FLOPs complexity (Figure 4). Gaudi-GBLR presents the most favorable accuracy-to-FLOPs tradeoff due to its capability of learning the adaptive resource/budget allocation for each weight matrix, which is a feature that our BLAST setting lacks in this particular evaluation (as we force it to use the same $r$ for all matrices). ", "page_idx": 6}, {"type": "image", "img_path": "n0arS0DDot/tmp/7755b854dff1c39b2c45898ae02c88b621308a9a19506c30c5814e1d54c6c06c.jpg", "img_caption": ["Figure 5: Pre-training result: WikiText 103 test perplexity-FLOPs trade-off curves from GPT-2 with different types of weight matrices. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "However, in the context of ImageNet-1k in Table 1,   \nweight matrices trained using BLAST with $b\\,=\\,3$ attain the highest levels of accuracy with the least FLOPs. This superior performance of BLAST (despite the common $r$ for all matrices) over Gaudi-GBLR can be attributed to its simpler training process with fewer hyperparameters. In contrast, the more complex training requirements of Gaudi-GBLR, which involve smoothness annealing and proximal gradient descent, may lead to suboptimal results for a large model such as ViT-Base in Table 1. ", "page_idx": 6}, {"type": "text", "text": "Language Model Evaluation We validate the training performance of BLAST weights on language models. We replace the weights of GPT-2 [20] with random ${\\tt B L A S T}_{6}$ matrices and trained the network from scratch on the WikiText 103 [30] dataset for 100 epochs. In Figure 5, we compare the test set perplexity of BLAST with the perplexity from low-rank, block-diagonal, Monarch, and Gaudi-GBLR matrices. Similar to the ImageNet training, we found that BLAST achieves the best perplexity-FLOPs trade-off. Compared to Gaudi-GBLR, BLAST obtains a significant perplexity gain. We attribute this improvement to the simple training process of BLAST which requires less hyperparameter tuning than that of Gaudi-GBLR. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.2 Compression and Re-training ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we discuss the performance of BLAST weights when pre-trained dense weights are available. We first compress the dense weights using Algorithm 2 and re-train the model on the training data with the cross-entropy loss. ", "page_idx": 7}, {"type": "text", "text": "ViT on ImageNet Classification We compress the weights of the vision transformer (ViT) trained on ImageNet training set by $\\mathrm{BLAST_{3}}$ and $\\mathrm{BLAST_{12}}$ using Algorithm 2 and re-train the models for 35 epochs. The accuracy-FLOPs trade-off curve of each model is presented in Figure 6. Both BLAST compressed & re-trained models outperform other baselines, even though BLAST models did not use the adaptive budget allocation, unlike Gaudi-GBLR. It is observed that the accuracy of the BLAST models slightly increases from $b=3$ to $b=12$ . ", "page_idx": 7}, {"type": "text", "text": "Diffusion Models We compress the weights of a Diffusion Transformer (DiT) [13] pre-trained on ImageNet using BLAST matrices and compare its performance to SVD-based low-rank approximation. For both techniques, we match the compression ratio such that both decrease the total number of model parameters by $50\\%$ , and re-train each model for 10 epochs on the ImageNet training set. We evaluate the models by generating a total of 50,000 images using the original, low-rank, and BLAST compressed models, and compute the FID [31], sFID [32], and IS [33] metrics with respect to the ImageNet validation set. The objective is to observe if the compressed model can generate images as realistic as the original uncompressed model. ", "page_idx": 7}, {"type": "image", "img_path": "n0arS0DDot/tmp/cd2c57cef7cff6635de094b14b018752fa0e18a3a06614b2c8729ad544daaf8b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "n0arS0DDot/tmp/be81457ab2f04d7f75d98ed389a35f2e37855d8d4460f329fe879afa45e175ba.jpg", "table_caption": ["Figure 6: Compression and re-training result: ImageNet accuracy-FLOPs tradeoff curves from ViT-B with different types of weight matrices. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 2: Performance comparison for compressing the weight matrices of a diffusion model followed by re-training. FID and IS scores were computed with respect to a validation dataset. CR stands for Compression Ratio. ", "page_idx": 7}, {"type": "text", "text": "In Table 2, we show quantitatively that the model compressed via BLAST significantly outperforms the model compressed via SVD. The low-rank compressed model often generates unrealistic images, leading to poor metrics such as the inception score. Figure 1 also contrasts how the BLAST matrices contribute to maintaining high perceptual quality as well as a close instance-wise resemblance with the uncompressed model outputs. Due to space limitations, we defer additional qualitative results and experimental setup to Appendix D.2. ", "page_idx": 7}, {"type": "text", "text": "Large Language Models (LLMs) We compress the weights of Llama-7B [1] with BLAST matrices using Algorithm 2 by $20\\%$ and $50\\%$ , and re-train the models for 400 steps on a subset of SlimPajama [34] dataset using 0.49B tokens. The number of blocks $b$ in the BLAST matrices is fixed at 16, and we use $r\\,=\\,1024$ for the attention modules and $r\\,=\\,1488$ for the MLP modules to achieve a $50\\%$ compression ratio. We test the WikiText-2 perplexity and the zero-shot task classification accuracy on common sense reasoning datasets including PIQA[35], HellaSwag[36], WinoGrande[37], BoolQ[38], OpenBookQA[39], ARC-easy and challenge [40]. We report the performance of Low-Rank, Monarch, and Block-Diagonal weight matrices after compression at the same rate and re-training. In Table 3, the first row presents the performance of the original Llama-7B model. On $50\\%$ compression ratio in the last five rows, the Monarch and Block-Diagonal matrices fail to recover the acceptable performance. Compared to Low-Rank weights, BLAST weights achieve the lowest performance degradation in WikiText-2 perplexity and zero-shot classification accuracy. The accuracy of each common sense reasoning benchmark and extended results can be found in Appendix D.3. ", "page_idx": 7}, {"type": "text", "text": "We provide an analysis to quantify the performance impact of compression and re-training. We first quantify the weight compression performance at $20\\%$ compression ratio in Table 3. Although the compression ratio is moderate, Low-Rank and Monarch compression without re-training suffer from significant performance loss, at most ${4\\bf{x}}$ higher perplexity and $25\\%$ accuracy degradation. On the other hand, $\\mathrm{BLAST_{16}}$ compression without re-training maintains reasonable accuracy and perplexity. This shows that the flexible and adaptive structure of BLAST matrices captures more information than other types of structured matrices. Yet, BLAST compression also exhibits noticeable performance degradation on a more intensive compression ratio (see the yellow curve in Figure 7). We provide more compression-only results on Diffusion Models and LLMs in Appendix D. ", "page_idx": 7}, {"type": "table", "img_path": "n0arS0DDot/tmp/6ad8c68f90ea85027b8aec109c94c23731692dde5c6bf29eb3075cd5f1863bfd.jpg", "table_caption": [], "table_footnote": ["Table 3: Zero-shot performance of LLaMA-7B after compression and retraining. Avg. 0-Shot Accuracy stands for the average accuracy of the zero-shot classification task. CR denotes compression ratio. Bold indicates the best performance under the same compression ratio. BLA $S\\mathrm{T}_{b}$ indicates the BLAST matrix with $b\\times b$ number of blocks. "], "page_idx": 8}, {"type": "image", "img_path": "n0arS0DDot/tmp/303c77d807f5fbd0fbce099ac578f7e1a0faec043b526f9f336e79704efa016f.jpg", "img_caption": ["Figure 7: Average zero-shot accuracy vs. compression ratio curves of Llama-7B by $\\mathrm{BLAST_{16}}$ before and after re-training. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "n0arS0DDot/tmp/f755558c7fc87687b20265b8da08abe283b15a8a21c816dc74d2a56175539acf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We find that the re-training stage is crucial for converting a pre-trained model into an efficient version without losing significant accuracy when the compression ratio is high. In Figure 7, we show the average zero-shot classification accuracy of the compressed models with $50\\%$ compression. The models with BLAST weights before re-training (yellow curve) exhibit substantial accuracy degradation at higher compression ratios. However, re-training (blue curve) effectively recovers performance using only 0.49B tokens and 400 steps. ", "page_idx": 8}, {"type": "text", "text": "LLM Runtime Analysis We evaluate the runtime of the Llama-7B compressed by the BLAST matrices on the text generation task. For evaluation, we let the model generate the sequences of length $L=10$ , 100, and 1000 ten times and report the average and the standard deviation of model runtime in Table 4. The instruction we use to generate the desired sequence length is \u201cIncreasing sequence: one,\u201d and the model generates the text sequence such as \u201ctwo, three, \u201d and so on, with a batch size of 1. All runtime evaluation tests were conducted on a single 40GB NVIDIA A100 GPU after compiling the script using torch.compile(). The $20\\%$ compressed model shows a $12\\%{\\sim}15\\%$ runtime reduction without any library function customization for BLAST matrix multiplication. The speedup when $b=2$ is higher than when $b=16$ because a larger number of blocks increases the computation overhead to perform Equation (3). Notably, the $50\\%$ compression ratio provides $32\\%{\\sim}35\\%$ runtime reduction when $b=16$ . We note that the test is highly memory-bandwidthbounded. Thus the speedup reported in Table 4 can be mostly attributed to the reduction of parameters (i.e., memory accesses) rather than the reduction in FLOPs due to BLAST compression. ", "page_idx": 8}, {"type": "text", "text": "5 Related Works ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Structured Matrix with Shared Bases Sharing the bases of block-structured matrices has recently drawn interest due to its considerable memory savings. BLAST matrices exemplify this approach. ", "page_idx": 8}, {"type": "text", "text": "Ashcraft et al. [41] extend the BLR [15] format to $\\mathrm{BLR^{2}}$ , incorporating shared bases and block-wise low-rank coupling factors determined through LQ or QR factorization. Similarly, Yen et al. [42] apply the concept of shared bases in the Block-Diagonal preconditioner for DNN weights. While BLAST also shares the bases of blocks, it is distinct in having a diagonal coupling matrix, as shown in Equation (2). The design of BLAST matrices aims to enhance efficiency and learn a variety of structures, from low-rank to high-rank block matrices. More importantly, identifying the diagonal coupling factors in BLAST matrices does not necessitate QR decomposition. Instead, they can be updated via gradient descent, making this approach well-suited for modeling the weight matrices in deep learning models. ", "page_idx": 9}, {"type": "text", "text": "DNN Weight Pruning and Decomposition Earlier work on DNN pruning [43\u201345] identifies less important parameters from the Hessian or magnitude to sparsify the weight matrix. Unlike general sparse matrices, a group sparse matrix skips computation in a group-wise manner by pruning channels [46, 47]. Sparse GPT [11] successfully prunes large language models with $50\\%$ sparsity without significantly degrading the performance of the original model. The model can achieve actual speedup utilizing 2:4 sparsity [48] on specific GPUs. However, 2:4 sparsity requires accelerators with specific architectures (e.g., NVIDIA A100 GPUs) and supports only the $50\\%$ compression ratio. On the other hand, BLAST is device-agnostic since it can be implemented with basic matrix arithmetic operations and offers diverse compression ratios. ", "page_idx": 9}, {"type": "text", "text": "Low-rank matrices have been widely adopted for CNN compression [49, 50] and Transformer compression [51, 52]. Additionally, Butterfly [53] and Monarch [14] factorization methods model high-rank but low-dimensional structures of the weights. Specifically, a Monarch matrix is a generalized version of a Butterfly matrix, yielding a wider spectrum of structured matrices. The number of blocks plays a key role in determining the rank of the Monarch matrix as a whole and does not generalize to another Monarch matrix with fewer blocks. Unlike Monarch, BLAST with $b\\times b$ blocks can express Monarch matrices with the same or fewer number of blocks, including the global low-rank matrix, i.e., $b=1$ . ", "page_idx": 9}, {"type": "text", "text": "Learning Low-dimensional Structures of DNN Weights Similar to the BLAST matrix, a GaudiGBLR matrix [12] enables learning low-dimensional structures by gradient descent in the generalized structured matrix space. Gaudi-GBLR overlaps a variable number of zero-padded rank-1 blocks to model high-rank submatrices. Although Gaudi-GBLR can express a wider spectrum of matrices than BLAST, the matrix-vector multiplication for Gaudi-GBLR is less efficient because GPUs and typical neural processors cannot handle zero-padded vectors efficiently. In contrast, the BLAST matrix-vector operation does not involve zero padding, allowing for more efficient execution in hardware for the same FLOPs (as shown in Figure 4, Figure 6, and Table 1). ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we introduced the BLAST matrix designed to improve the inference efficiency of large DNNs. The BLAST matrix represents various low-dimensional structures of the weight matrices with fewer parameters, while enabling efficient matrix-vector products. The BLAST factors are either learnable from data or estimated from existing weights using our preconditioned factorization algorithm. Our results on both language and vision tasks highlight the effectiveness of BLAST. ", "page_idx": 9}, {"type": "text", "text": "Limitations and Future Work The BLAST matrix-vector product consists of three steps, as detailed in Equation (3), which may degrade hardware-execution parallelism. In our evaluation, we used the same computational budget $r$ for all matrices. Learning an adaptive budget per layer or matrix (e.g., via overparameterization [54, 7]) could further improve BLAST performance, which is left for future work. The proposed method has not been evaluated on tiny $\\langle{<}100\\mathbf{M}$ parameters) or extremely large $\\scriptstyle(>10\\mathrm{{B}}$ parameters) DNNs. Additionally, optimizing runtime and power consumption via BLAST matrices with customized library functions and/or hardware accelerators also remains as future work. Furthermore, a deeper theoretical investigation into the behaviors of BLAST matrices would provide a more comprehensive understanding of their capabilities and limitations. Applying advanced re-training techniques, such as knowledge distillation [55] or iterative compression and distillation [56], to the BLAST compression pipeline is also left for future work. Finally, beyond the weight structures, we expect BLAST can also help understand and exploit low-dimensional data manifolds [57\u201359] in future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The arXiv version of the paper can be found at https://arxiv.org/abs/2410.21262. This work was supported in part by COGNISENSE, one of seven centers in JUMP 2.0, a Semiconductor Research Corporation (SRC) program sponsored by DARPA. SMK and QQ acknowledge funding support from NSF CAREER CCF-2143904, and NSF CCF-2212066. We thank Salar Fattahi, Reetuparna Das, Mingyu Yang, Sara Shoouri, Shrikant Arvavasu, Jayeon Yi, Andrew Larson, Pierre Abillama, Alireza Khadem, Yufeng Gu, and Can Yaras for helpful discussion and feedback. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [2] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4015\u20134026, 2023. [3] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n[4] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [5] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[6] Minyoung Huh, Hossein Mobahi, Richard Zhang, Brian Cheung, Pulkit Agrawal, and Phillip Isola. The low-rank simplicity bias in deep networks. Transactions on Machine Learning Research, 2022. [7] Can Yaras, Peng Wang, Wei Hu, Zhihui Zhu, Laura Balzano, and Qing Qu. The law of parsimony in gradient descent for learning deep linear networks. arXiv preprint arXiv:2306.01154, 2023.   \n[8] Soo Min Kwon, Zekai Zhang, Dogyoon Song, Laura Balzano, and Qing Qu. Efficient lowdimensional compression of overparameterized models. In Sanjoy Dasgupta, Stephan Mandt, and Yingzhen Li, editors, Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, volume 238 of Proceedings of Machine Learning Research, pages 1009\u20131017. PMLR, 02\u201304 May 2024. URL https://proceedings.mlr.press/v238/ min-kwon24a.html.   \n[9] Peng Wang, Xiao Li, Can Yaras, Zhihui Zhu, Laura Balzano, Wei Hu, and Qing Qu. Understanding deep representation learning via layerwise feature compression and discrimination. arXiv preprint arXiv:2311.02960, 2023.   \n[10] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=rJl-b3RcF7.   \n[11] Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot. In International Conference on Machine Learning, pages 10323\u201310337. PMLR, 2023.   \n[12] Changwoo Lee and Hun-Seok Kim. Differentiable learning of generalized structured matrices for efficient deep neural networks. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=pAVJKp3Dvn.   \n[13] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195\u20134205, 2023.   \n[14] Tri Dao, Beidi Chen, Nimit S Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, and Christopher R\u00e9. Monarch: Expressive structured matrices for efficient and accurate training. In International Conference on Machine Learning, pages 4690\u20134721. PMLR, 2022.   \n[15] Patrick Amestoy, Cleve Ashcraft, Olivier Boiteau, Alfredo Buttari, Jean-Yves L\u2019Excellent, and Cl\u00e9ment Weisbecker. Improving multifrontal methods by means of block low-rank representations. SIAM Journal on Scientific Computing, 37(3):A1451\u2013A1474, 2015.   \n[16] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[17] Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher Re. Pixelated butterfly: Simple and efficient sparse training for neural network models. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id=Nfl-iXa-y7R.   \n[18] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020.   \n[20] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n[21] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.   \n[22] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum? id=Bkg6RiCqY7.   \n[23] Tian Tong, Cong Ma, and Yuejie Chi. Accelerating ill-conditioned low-rank matrix estimation via scaled gradient descent. Journal of Machine Learning Research, 22(150):1\u201363, 2021.   \n[24] Jialun Zhang, Salar Fattahi, and Richard Y Zhang. Preconditioned gradient descent for overparameterized nonconvex matrix factorization. Advances in Neural Information Processing Systems, 34:5985\u20135996, 2021.   \n[25] Xingyu Xu, Yandi Shen, Yuejie Chi, and Cong Ma. The power of preconditioning in overparameterized low-rank matrix sensing. In International Conference on Machine Learning, pages 38611\u201338654. PMLR, 2023.   \n[26] Tian Ye and Simon S Du. Global convergence of gradient descent for asymmetric low-rank matrix factorization. Advances in Neural Information Processing Systems, 34:1429\u20131439, 2021.   \n[27] Dominik St\u00f6ger and Mahdi Soltanolkotabi. Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction. Advances in Neural Information Processing Systems, 34:23831\u201323843, 2021.   \n[28] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical Report 0, University of Toronto, Toronto, Ontario, 2009. URL https://www.cs. toronto.edu/\\~kriz/learning-features-2009-TR.pdf.   \n[29] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211\u2013252, 2015. doi: 10.1007/s11263-015-0816-y.   \n[30] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.   \n[31] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.   \n[32] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter Battaglia. Generating images with sparse representations. In International Conference on Machine Learning, pages 7958\u20137968. PMLR, 2021.   \n[33] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016.   \n[34] Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, and Nolan Dey. SlimPajama: A 627B token cleaned and deduplicated version of RedPajama. https://www.cerebras.net/blog/ slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama, 2023. URL https://huggingface.co/datasets/cerebras/SlimPajama-627B.   \n[35] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432\u20137439, 2020.   \n[36] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019.   \n[37] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99\u2013106, 2021.   \n[38] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.   \n[39] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.   \n[40] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.   \n[41] Cleve Ashcraft, Alfredo Buttari, and Theo Mary. Block low-rank matrices with shared bases: Potential and limitations of the blr $\\mathord{\\sim}2$ format. SIAM Journal on Matrix Analysis and Applications, 42(2):990\u20131010, 2021.   \n[42] Jui-Nan Yen, Sai Surya Duvvuri, Inderjit S Dhillon, and Cho-Jui Hsieh. Block low-rank preconditioner with shared basis for stochastic optimization. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id= JzQlGqBm8d.   \n[43] Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. Advances in neural information processing systems, 2, 1989.   \n[44] Babak Hassibi and David Stork. Second order derivatives for network pruning: Optimal brain surgeon. Advances in neural information processing systems, 5, 1992.   \n[45] Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding. In Yoshua Bengio and Yann LeCun, editors, 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv. org/abs/1510.00149.   \n[46] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks. In Proceedings of the IEEE international conference on computer vision, pages 1389\u20131397, 2017.   \n[47] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large language models. Advances in neural information processing systems, 36:21702\u201321720, 2023.   \n[48] Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu, and Paulius Micikevicius. Accelerating sparse deep neural networks. arXiv preprint arXiv:2104.08378, 2021.   \n[49] Cheng Tai, Tong Xiao, Xiaogang Wang, and Weinan E. Convolutional neural networks with lowrank regularization. In Yoshua Bengio and Yann LeCun, editors, 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1511.06067.   \n[50] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014.   \n[51] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id=nZeVKeeFYf9.   \n[52] Habib Hajimolahoseini, Walid Ahmed, Mehdi Rezagholizadeh, Vahid Partovinia, and Yang Liu. Strategies for applying low rank decomposition to transformer-based models. In 36th Conference on Neural Information Processing Systems (NeurIPS2022), 2022.   \n[53] Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher R\u00e9. Learning fast algorithms for linear transforms using butterfly factorizations. In International conference on machine learning, pages 1517\u20131527. PMLR, 2019.   \n[54] Can Yaras, Peng Wang, Laura Balzano, and Qing Qu. Compressible dynamics in deep overparameterized low-rank learning & adaptation. In Forty-first International Conference on Machine Learning, 2024.   \n[55] Geoffrey Hinton. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.   \n[56] Sharath Turuvekere Sreenivas, Saurav Muralidharan, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. Llm pruning and distillation in practice: The minitron approach. arXiv preprint arXiv:2408.11796, 2024.   \n[57] Peng Wang, Huijie Zhang, Zekai Zhang, Siyi Chen, Yi Ma, and Qing Qu. Diffusion models learn low-dimensional distributions via subspace clustering. arXiv preprint arXiv:2409.02426, 2024.   \n[58] Gen Li and Yuling Yan. Adapting to unknown low-dimensional structures in score-based diffusion models. arXiv preprint arXiv:2405.14861, 2024.   \n[59] Siyi Chen, Huijie Zhang, Minzhe Guo, Yifu Lu, Peng Wang, and Qing Qu. Exploring lowdimensional subspace in diffusion models for controllable image editing. In The Thirtyeighth Annual Conference on Neural Information Processing Systems, 2024. URL https: //arxiv.org/abs/2409.02374.   \n[60] Roger A Horn and Charles R Johnson. Matrix analysis. Cambridge university press, 2012.   \n[61] S\u00e9bastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and Trends\u00ae in Machine Learning, 8(3-4):231\u2013357, 2015.   \n[62] Roger A Horn and Charles R Johnson. Topics in matrix analysis. Cambridge university press, 1994.   \n[63] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018.   \n[64] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.   \n[65] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[66] Runyu Peng, Yunhua Zhou, Qipeng Guo, Yang Gao, Hang Yan, Xipeng Qiu, and Dahua Lin. Data-freeweight compress and denoise for large language models. arXiv preprint arXiv:2402.16319, 2024.   \n[67] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPof,i Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac\u2019h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/ 10256836. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Details on BLAST Matrix and Factorization ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Matrix Multiplication Code Implementation Following the conventional setting in Transformers [16], the input tensor is assumed to have batch, sequence, and channel dimensions. The left and right factors are multiplied using the batched matrix multiplication routine, whereas the diagonal factors are multiplied via broadcasting and summation. ", "page_idx": 15}, {"type": "image", "img_path": "n0arS0DDot/tmp/3f9c74e97892d0ee9d792271757dd356cda9ccc3fdf88d850956a69d822ef7a9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 8: Pseudocode of BLAST Matrix Multiplication. The function bmm stands for the batched matrix multiplication routine (e.g., torch.bmm [21]). ", "page_idx": 15}, {"type": "text", "text": "A.1 More Special Cases of BLAST Matrix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Block diagonal matrix A block-diagonal matrix is a BLAST matrix when $r\\,=\\,p$ and $s_{i,j}\\,=$   \n$\\int\\mathbf{1}_{r}$ ioft $i=j$ se since ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left[\\begin{array}{c c c c c}{A_{1,1}}&&&\\\\ &{A_{2,2}}&&\\\\ &&{\\ddots}&\\\\ &&&{A_{b,b}}\\end{array}\\right]=\\left[\\begin{array}{c c c c c}{U_{1}\\mathrm{diag}(s_{1,1})V_{1}^{T}}&&&\\\\ &{U_{2}\\mathrm{diag}(s_{2,2})V_{2}^{T}}&&\\\\ &&{\\ddots}&&\\\\ &&&{U_{b}\\mathrm{diag}(s_{b,b})V_{b}^{T}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "When $r<p$ , BLAST matrices model block-diagonal matrices which have low-rank diagonal blocks. ", "page_idx": 15}, {"type": "text", "text": "Block low-rank (BLR) matrix For ease of understanding, let us consider a BLR matrix of 9 $\\textstyle{\\frac{n}{3}}\\times{\\frac{n}{3}}$ rank-1 blocks. Each block is composed of the unique bases $\\pmb{A}_{i,j}=\\pmb{u}_{i,j}\\pmb{v}_{i,j}^{T}$ . Now consider $U_{i}=[\\pmb{u}_{i,1},\\pmb{u}_{i,2},\\pmb{u}_{i,3}]$ and $V_{j}=[v_{1,j},v_{2,j},v_{3,j}]$ . Then, the BLR matrix is a BLAST matrix with $r=b=3$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left[\\!\\!{u_{1,1}}v_{1,1}^{T}\\quad{u_{1,2}}v_{1,2}^{T}\\quad{u_{1,3}}v_{1,3}^{T}\\right]=\\left[\\!\\!{U_{1}S_{1}V_{1}^{T}\\quad U_{1}S_{2}V_{2}^{T}\\quad U_{1}S_{3}V_{3}^{T}}\\!\\!\\right]}\\\\ {\\left[\\!\\!{u_{2,1}}v_{2,1}^{T}\\quad{u_{2,2}}v_{2,2}^{T}\\quad{u_{2,3}}v_{2,3}^{T}\\right]=\\left[\\!\\!{U_{2}S_{1}V_{1}^{T}\\quad U_{2}S_{2}V_{2}^{T}\\quad U_{2}S_{3}V_{3}^{T}}\\!\\!\\right]}\\\\ {\\left[\\!\\!{u_{3,1}}v_{3,1}^{T}\\quad{u_{3,2}}v_{3,2}^{T}\\quad{u_{3,3}}v_{3,3}^{T}\\right]}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $S_{1}=\\mathrm{diag}([1,0,0]),S_{2}=\\mathrm{diag}([0,1,0])$ , and $S_{3}=\\mathrm{diag}([0,0,1])$ . ", "page_idx": 15}, {"type": "text", "text": "To model a $n\\times n$ general $b\\times b$ partitioned BLR matrix where the rank of each block is $t$ , let us use the BLAST matrix with $b\\times b$ blocks and $r=b t$ . The factors have the following shapes: ", "page_idx": 15}, {"type": "equation", "text": "$$\nU_{i},V_{j}\\in\\mathbb{R}^{p\\times(b t)},\\quad s_{i,j}\\in\\mathbb{R}^{b t}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "$s_{i,j,k}=\\binom{1}{0}\\begin{array}{r l}{\\mathrm{if}\\ t(j-1)+1\\leq k<t j+1}\\\\ {\\mathrm{otherwise}}\\end{array}$ , the BLAST matrix can model the BLR matrix. ", "page_idx": 15}, {"type": "text", "text": "Note that the number of parameters of the BLAST matrix is $2n r+r b^{2}$ , whereas that of the BLR matrix in this case is $b^{2}\\cdot^{-}(p+p)t=2(p b)(b t)=2n r$ . In other words, the BLAST matrix models various matrices with the cost of $r b^{2}$ . ", "page_idx": 15}, {"type": "text", "text": "A.2 Derivation of Preconditioning Matrices ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We rewrite the loss function in Equation (4) below: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\ell(U_{*},V_{*},s_{*,*})=\\sum_{i=1}^{b}\\sum_{j=1}^{b}\\frac{1}{2}\\left\\|A_{i,j}-U_{i}\\mathrm{diag}(\\pmb{s}_{i,j})V_{j}^{T}\\right\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We first derive the gradients of $U_{i},V_{j}$ , and ${\\boldsymbol{s}}_{i,j}$ , then discuss the preconditioning matrix for each factor. ", "page_idx": 16}, {"type": "text", "text": "A.2.1 Gradients ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Here we derive the gradients of Equation (4) with respect to the BLAST factors. We begin with introducing the short-handed notation for the concatenated factors: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{V}_{i}^{T}=[S_{i,1}V_{1}^{T}\\cdot\\cdot\\cdot S_{i,b}V_{b}^{T}],}\\\\ &{\\bar{U}_{j}=[(U_{1}S_{1,j})^{T}\\cdot\\cdot\\cdot(U_{b}S_{b,j})^{T}]^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "That is, the matrix $\\bar{V_{i}}$ is composed by concatenating $V_{j}^{T_{\\mathrm{s}}}$ horizontally along $j=1,2,\\dots,b$ after scaling them with $S_{i,j}$ . $\\bar{U}_{j}$ is defined similarly by concatenating the scaled $U_{i}\\mathbf{s}$ vertically. ", "page_idx": 16}, {"type": "text", "text": "Now we derive the gradients below. ", "page_idx": 16}, {"type": "text", "text": "Gradient of $U_{i}$ We only have to consider the loss term related to $U_{i}$ . Therefore, we have the following gradient expression: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\nabla_{U_{i}}\\ell(U_{*},V_{*},s_{*,*})=\\nabla U_{i}\\sum_{j=1}^{b}\\frac{1}{2}\\left\\|A_{i,j}-U_{i}\\mathrm{diag}(s_{i,j})V_{j}^{T}\\right\\|_{F}^{2}}}\\\\ &{=\\nabla U_{i}\\frac{1}{2}\\left\\|A_{i,*}-U_{i}\\bar{V}_{i}^{T}\\right\\|_{F}^{2}}\\\\ &{=(U_{i}\\bar{V}_{i}^{T}-A_{i,*})\\bar{V}_{i},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where for the second equality we used the concatenated version of the first line. ", "page_idx": 16}, {"type": "text", "text": "Gradient of $V_{j}$ follows the similar derivation as Equation (10): ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\nabla_{V_{j}}\\ell(U_{*},V_{*},s_{*,*})=\\nabla_{V_{j}}\\sum_{i=1}^{b}\\frac{1}{2}\\left\\|A_{i,j}-U_{i}\\mathrm{diag}(s_{i,j})V_{j}^{T}\\right\\|_{F}^{2}}}\\\\ &{=\\nabla_{V_{j}}\\frac{1}{2}\\left\\|A_{*,j}-\\bar{U}_{j}V_{j}^{T}\\right\\|_{F}^{2}}\\\\ &{=(\\bar{U}_{j}V_{j}^{T}-A_{*,j})^{T}\\bar{U}_{j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Gradient of ${\\boldsymbol{s}}_{i,j}$ We consider the block-wise loss for the gradient: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nabla_{s_{i,j}}\\ell(\\boldsymbol{U}_{*},\\boldsymbol{V}_{*},s_{*,*})=\\nabla_{s_{i,j}}\\frac{1}{2}\\left\\|\\boldsymbol{A}_{i,j}-\\boldsymbol{U}_{i}\\mathrm{diag}(s_{i,j})\\boldsymbol{V}_{j}^{T}\\right\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since the Frobenius norm can be expressed by a matrix trace, the loss is written as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|A_{i,j}-U_{i}\\mathrm{diag}(s_{i,j})V_{j}^{T}\\right\\|_{F}^{2}=\\mathrm{Tr}\\left(\\left(A_{i,j}-U_{i}S_{i,j}V_{j}^{T}\\right)^{T}\\left(A_{i,j}-U_{i}S_{i,j}V_{j}^{T}\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathrm{Tr}\\left(V_{j}S_{i,j}U_{i}^{T}U_{i}S_{i,j}V_{j}^{T}-2A_{i,j}^{T}U_{i}S_{i,j}V_{j}^{T}+A_{i,j}^{T}A_{i,j}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathrm{Tr}\\left(S_{i,j}V_{j}^{T}V_{j}S_{i,j}U_{i}^{T}U_{i}-2S_{i,j}V_{j}^{T}A_{i,j}^{T}U_{i}+A_{i,j}^{T}A_{i,j}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\operatorname{Tr}(X)$ is the trace of $\\mathbf{\\deltaX}$ . Note that the derivative of product in trace is given by $\\nabla_{X}\\mathrm{Tr}(X Y)=$ ${\\mathbf{}}Y^{T}$ for any two conformal matrices $\\mathbf{\\deltaX}$ and $\\mathbf{Y}$ . Therefore, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nabla_{S_{i,j}}\\frac{1}{2}\\left\\|A_{i,j}-U_{i}\\mathrm{diag}(s_{i,j})V_{j}^{T}\\right\\|_{F}^{2}=U_{i}^{T}U_{i}S_{i,j}V_{j}^{T}V_{j}-U_{i}^{T}A_{i,j}V_{j}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now Equation (12) becomes as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\nabla_{s_{i,j}}}\\frac{1}{2}\\left\\|{A_{i,j}-U_{i}\\mathrm{diag}(s_{i,j})V_{j}^{T}}\\right\\|_{F}^{2}=\\mathrm{diag}\\left(U_{i}^{T}U_{i}S_{i,j}V_{j}^{T}V_{j}-U_{i}^{T}A_{i,j}V_{j}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The first term on the right hand side is further arranged by using the fact that $\\mathrm{diag}\\left(X Y^{T}\\right)\\ =$ $(X\\odot Y)\\,\\mathbf{1}$ for any two matrices $X,Y$ of the same size. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname{diag}\\left(\\left(U_{i}^{T}U_{i}S_{i,j}\\right)\\left(V_{j}^{T}V_{j}\\right)\\right)=\\left[\\left(U_{i}^{T}U_{i}\\mathrm{diag}(s_{i,j})\\right)\\odot\\left(V_{j}^{T}V_{j}\\right)\\right]\\mathbf{1}_{r}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\left(\\left(U_{i}^{T}U_{i}\\right)\\odot\\left(V_{j}^{T}V_{j}\\right)\\right)s_{i,j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, the gradient of $\\boldsymbol{s}_{i,j}$ is now expressed as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\nabla_{s_{i,j}}\\ell(\\boldsymbol{U}_{*},V_{*},\\boldsymbol{s}_{*,*})=\\left((\\boldsymbol{U}_{i}^{T}\\boldsymbol{U}_{i})\\odot(V_{j}^{T}V_{j})\\right)\\boldsymbol{s}_{i,j}-\\operatorname{diag}\\left(\\boldsymbol{U}_{i}^{T}\\boldsymbol{A}_{i,j}V_{j}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A.2.2 Preconditioning Matrices ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Now let us derive the preconditioning matrices used in Algorithm 2. ", "page_idx": 17}, {"type": "text", "text": "Preconditioning matrix $P_{U_{i}}$ for $U_{i}$ Let $V_{j}$ and $\\boldsymbol{s}_{i,j}$ are given. Let us consider the case when $U_{i}$ is at the stationary point $\\hat{U}$ which satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\nabla_{\\hat{U}}\\frac{1}{2}\\|A_{i,*}-\\hat{U}\\bar{V}_{i}^{T}\\|_{F}^{2}=\\hat{U}\\bar{V}_{i}^{T}\\bar{V}_{i}-A_{i,*}\\bar{V}_{i}}}\\\\ &{=O,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $^o$ is the zero matrix. This gives us the normal equation ", "page_idx": 17}, {"type": "equation", "text": "$$\nA_{i,*}\\bar{V}_{i}=\\hat{U}\\bar{V}_{i}^{T}\\bar{V}_{i}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now consider a preconditioned gradient descent with $P_{U_{i}}\\in\\mathbb{R}^{r\\times r}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U_{i}^{\\prime}=U_{i}-\\left(U_{i}\\bar{V}_{i}^{T}\\bar{V}_{i}-A_{i,*}\\bar{V}_{i}\\right)P_{U_{i}}}\\\\ &{\\qquad=U_{i}-\\left(U_{i}\\bar{V}_{i}^{T}\\bar{V}_{i}-\\hat{U}\\bar{V}_{i}^{T}\\bar{V}_{i}\\right)P_{U_{i}}}\\\\ &{\\qquad\\qquad=U_{i}-\\left(U_{i}-\\hat{U}\\right)\\bar{V}_{i}^{T}\\bar{V}_{i}P_{U_{i}}}\\\\ &{\\Longrightarrow U_{i}^{\\prime}-\\hat{U}_{i}=\\left(U_{i}-\\hat{U}\\right)\\left(I-\\bar{V}_{i}^{T}\\bar{V}_{i}P_{U_{i}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Suppose $\\bar{V}_{i}^{T}\\bar{V}_{i}$ is invertible. Then the ideal preconditioner is ", "page_idx": 17}, {"type": "equation", "text": "$$\nP_{U_{i}}^{\\star}=(\\bar{V}_{i}^{T}\\bar{V}_{i})^{-1}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "since it brings $U_{i}^{\\prime}$ to the stationary point $\\hat{U}$ . However, directly use the inverse of $\\bar{V}_{i}^{T}\\bar{V}$ might result in numerical instability or complete breakdown of the algorithm when the matrix is singular. By following [24], we use the regularized version ", "page_idx": 17}, {"type": "equation", "text": "$$\nP_{U_{i}}=\\left(\\bar{V}_{i}^{T}\\bar{V}_{i}+\\delta I\\right)^{-1}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\delta$ is chosen by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\delta=\\delta_{0}\\cdot\\sqrt{\\ell(U_{\\ast},V_{\\ast},s_{\\ast,\\ast})},\\quad\\delta_{0}>0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Preconditioning matrix $P_{V_{j}}$ for $V_{j}$ The preconditioner for $V_{j}$ can be derived by following the similar steps for $P_{U_{i}}$ . Here we present the result: ", "page_idx": 17}, {"type": "equation", "text": "$$\nP_{V_{j}}=\\left(\\bar{U}_{j}^{T}\\bar{U}_{j}+\\delta I\\right)^{-1}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\delta$ is chosen by Equation (19). ", "page_idx": 17}, {"type": "text", "text": "Preconditioning matrix $P_{s_{i,j}}$ for ${\\boldsymbol{s}}_{i,j}$ We again consider the stationary point $\\hat{\\pmb{s}}$ or the diagonal version $\\hat{\\boldsymbol{S}}=\\mathrm{diag}(\\hat{\\boldsymbol{s}})$ which has a zero gradient: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\nabla_{\\hat{s}}\\frac{1}{2}\\|A_{i,j}-U_{i}\\hat{S}V_{j}^{T}\\|_{F}^{2}=\\left((U_{i}^{T}U_{i})\\odot(V_{j}^{T}V_{j})\\right)\\hat{s}-\\operatorname{diag}\\left(U_{i}^{T}A_{i,j}V_{j}\\right)=\\mathbf{0}}\\\\ &{\\Longrightarrow\\left((U_{i}^{T}U_{i})\\odot(V_{j}^{T}V_{j})\\right)\\hat{s}=\\operatorname{diag}\\left(U_{i}^{T}A_{i,j}V_{j}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, Equation (15) can be written as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla_{s_{i,j}}\\ell(U_{*},V_{*},s_{*,*})=\\left((U_{i}^{T}U_{i})\\odot(V_{j}^{T}V_{j})\\right)(s_{i,j}-\\hat{s}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now consider the preconditioned gradient descent: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad s_{i,j}^{\\prime}=s_{i,j}-P_{s_{i,j}}\\left((U_{i}^{T}U_{i})\\odot(V_{j}^{T}V_{j})\\right)(s_{i,j}-\\hat{s})}\\\\ &{\\Longrightarrow s_{i,j}^{\\prime}-\\hat{s}=s_{i,j}-\\hat{s}-P_{s_{i,j}}\\left((U_{i}^{T}U_{i})\\odot(V_{j}^{T}V_{j})\\right)(s_{i,j}-\\hat{s})}\\\\ &{\\qquad\\qquad=\\left(I-P_{s_{i,j}}\\left((U_{i}^{T}U_{i})\\odot(V_{j}^{T}V_{j})\\right)\\right)(s_{i,j}-\\hat{s})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The Hadamard product of two positive definite matrices are still positive definite (see [60, Theorem 7.5.3], also known as Schur\u2019s Product Theorem). Hence, if both $U_{i}^{T}U_{i}$ and $V_{j}^{T}\\dot{V}_{j}$ are positive definite so that invertible, $(U_{i}^{T}U_{i})\\odot(V_{j}^{T}V_{j})$ is also invertible. The ideal preconditioner when both matrices are invertible is therefore ", "page_idx": 18}, {"type": "equation", "text": "$$\nP_{s_{i,j}}^{\\star}=\\left((U_{i}^{T}U_{i})\\odot(V_{j}^{T}V_{j})\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Same as for $P_{U_{i}}$ and $P_{V_{j}}$ , we also consider the regularized version: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\boldsymbol{P}_{s_{i,j}}=\\left((\\boldsymbol{U}_{i}^{T}\\boldsymbol{U}_{i})\\odot(\\boldsymbol{V}_{j}^{T}\\boldsymbol{V}_{j})+\\delta\\boldsymbol{I}\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Here, $\\delta$ is chosen by Equation (19). ", "page_idx": 18}, {"type": "text", "text": "B Proof of Theorem 1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We first introduce the following properties: ", "page_idx": 18}, {"type": "text", "text": "Lemma 2. [61, Lemma 3.4] Assume $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ is convex and continuously differentiable, and its gradient is $L$ -Lipschitz continuous. Then for any $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{n}$ , one has ", "page_idx": 18}, {"type": "equation", "text": "$$\nf(\\pmb{y})-f(\\pmb{x})-\\langle\\nabla f(\\pmb{x}),\\pmb{y}-\\pmb{x}\\rangle\\leq\\frac{L}{2}\\|\\pmb{y}-\\pmb{x}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. See [61, Lemma 3.4]. ", "page_idx": 18}, {"type": "text", "text": "Lemma 3. Assume $f\\,:\\,\\mathbb{R}^{n}\\,\\rightarrow\\,\\mathbb{R}$ is convex and continuously differentiable, and its gradient is $L$ -Lipschitz continuous. Consider a gradient descent update ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{x}^{(k+1)}=\\pmb{x}^{(k)}-\\eta\\cdot\\nabla f(\\pmb{x}^{(k)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, with the step size $\\begin{array}{r}{0<\\eta\\le\\frac{1}{L}}\\end{array}$ , the following holds: ", "page_idx": 18}, {"type": "equation", "text": "$$\nf(\\pmb{x}^{(k+1)})\\leq f(\\pmb{x}^{(k)})-\\frac{1}{2L}\\|\\nabla f(\\pmb{x}^{(k)})\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "That is, the gradient descent update does not increase the function value. ", "page_idx": 18}, {"type": "text", "text": "Proof. By Lemma 2, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f({\\pmb x}^{(k+1)})\\leq f({\\pmb x}^{(k)})+\\langle\\nabla f({\\pmb x}^{(k)}),{\\pmb x}^{(k+1)}-{\\pmb x}^{(k)}\\rangle+\\frac L2\\|{\\pmb x}^{(k+1)}-{\\pmb x}^{(k)}\\|_{2}^{2}}\\\\ &{\\qquad\\qquad=f({\\pmb x}^{(k)})-\\eta\\cdot\\|\\nabla f({\\pmb x}^{(k)})\\|_{2}^{2}+\\frac{\\eta^{2}L}{2}\\|\\nabla f({\\pmb x}^{(k)})\\|_{2}^{2}}\\\\ &{\\qquad\\qquad=f({\\pmb x}^{(k)})-\\eta\\cdot\\left(1-\\frac{\\eta L}{2}\\right)\\|\\nabla f({\\pmb x}^{(k)})\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\leq f({\\pmb x}^{(k)})-\\frac{\\eta}{2}\\|\\nabla f({\\pmb x}^{(k)})\\|_{2}^{2}\\quad(\\mathrm{since~}\\eta L\\leq1)}\\\\ &{\\qquad\\qquad\\leq f({\\pmb x}^{(k)})-\\frac{1}{2L}\\|\\nabla f({\\pmb x}^{(k)})\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now we prove Theorem 1, which we restate below. ", "page_idx": 19}, {"type": "text", "text": "Theorem 1. Let $A_{i,j}\\in\\mathbb{R}^{p\\times p}$ be a target block and $U_{i}^{(k)},V_{j}^{(k)}\\in\\mathbb{R}^{p\\times r}$ , and $\\pmb{s}_{i,j}^{(k)}\\in\\mathbb{R}^{r}$ be factors of a block in the BLAST matrix to be optimized. With the step sizes $0<\\eta_{U_{i}^{(k)}}\\leq\\bar{1}/\\sigma_{1}\\left(\\bar{V}_{i}^{(k)T}\\bar{V}_{i}^{(k)}\\right)$ $\\eta_{V_{j}^{(k)}}\\,\\le\\,1/\\sigma_{1}\\left(\\bar{U}_{j}^{(k)T}\\bar{U}_{j}^{(k)}\\right),\\,0\\,<\\,\\eta_{s_{i,j}^{(k)}}\\,\\le\\,1/\\sigma_{1}\\left((U_{i}^{(k+1)T}U_{i}^{(k+1)})\\odot(V_{j}^{(k+1)T}V_{j}^{(k+1)})\\right)$ , the gradient descent updates in Equations (5) $t o$ (7) monotonically non-increase the loss: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\ell(\\pmb{U}_{*}^{(k+1)},\\pmb{V}_{*}^{(k+1)},\\pmb{s}_{*,*}^{(k+1)})\\leq\\ell(\\pmb{U}_{*}^{(k)},\\pmb{V}_{*}^{(k)},\\pmb{s}_{*,*}^{(k)}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. To prove Theorem 1, we first show that each step of Equations (5) to (7) satisfies Lemma 3 under the given conditions. Then we resemble the results to construct the bound. ", "page_idx": 19}, {"type": "text", "text": "Gradient Descent Update on $U_{i}$ Let us denote the loss term regarding $U_{i}$ by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\ell(\\boldsymbol{U}_{i})=\\frac{1}{2}\\left\\|\\boldsymbol{A}_{i,*}-\\boldsymbol{U}_{i}\\bar{\\boldsymbol{V}}_{i}^{T}\\right\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since (i) the Frobenius norm is convex, (ii) all linear mappings are convex, and (iii) a composition of two convex functions are convex, $\\left\\Vert\\mathbf{A}_{i,*}-U_{i}\\bar{V}_{i}^{T}\\right\\Vert_{F}^{2}$ is a convex function of $U_{i}$ . Also, the gradient we derived in Equation (10) always exists and has the following vectorized form: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{vec}(\\nabla\\ell(U_{i}))=\\mathrm{vec}(U_{i}\\bar{V}_{i}^{T}\\bar{V}_{i})-\\mathrm{vec}(A_{i,*}\\bar{V}_{i})}\\\\ &{\\qquad\\qquad\\qquad=((\\bar{V}_{i}^{T}\\bar{V}_{i})^{T}\\otimes I)u_{i}-\\mathrm{vec}(A_{i,*}\\bar{V}_{i}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\otimes$ denotes the Kronecker product and $\\pmb{{u}}_{i}\\equiv\\mathrm{vec}(\\pmb{U}_{i})$ . The Lipschitz constant of the gradient is the largest singular value of the matrix $(\\bar{V}_{i}^{T}\\bar{V}_{i})^{T}\\otimes I$ , which is the largest singular value of $\\bar{V}_{i}^{T}\\bar{V}_{i}$ since the Kronecker product of two matrices of singular values $\\pmb{\\Sigma}_{1}$ and $\\pmb{\\Sigma}_{2}$ has the singular values of $\\pmb{\\Sigma}_{1}\\otimes\\pmb{\\Sigma}_{2}$ (see [62, Theorem 4.2.15]). ", "page_idx": 19}, {"type": "text", "text": "Therefore, from Lemma 3, we obtain the following bound: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\ell(\\pmb{U}_{i}^{(k+1)})\\leq\\ell(\\pmb{U}_{i}^{(k)})-\\frac{\\|\\nabla_{\\pmb{U}_{i}^{(k)}}\\ell(\\pmb{U}_{i}^{(k)})\\|_{F}^{2}}{2\\sigma_{1}\\left(\\bar{V}_{i}^{(k)T}\\bar{V}_{i}^{(k)}\\right)},\\quad i=1,\\ldots,b.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Gradient Descent Update on $V_{j}$ The loss function $\\begin{array}{r}{\\ell(V_{j})=\\frac{1}{2}\\|A_{*,j}-\\bar{U}_{j}\\bar{U}_{j}^{T}\\|_{F}^{2}}\\end{array}$ with respect to $V_{j}$ is convex to $V_{j}$ and the gradient of $V_{j}$ in Equation (11) also always exists and can be rewritten as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{vec}(\\nabla\\ell(V_{j}))=\\mathrm{vec}(V_{j}\\bar{U}_{j}^{T}\\bar{U}_{j})-\\mathrm{vec}(A_{*,j}^{T}\\bar{U}_{j})}\\\\ &{\\qquad\\qquad\\qquad=((\\bar{U}_{j}^{T}\\bar{U}_{j})\\otimes I)v_{j}-\\mathrm{vec}(A_{*,j}^{T}\\bar{U}_{j}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The Lipschitz constant of the gradient is again the largest singular value of $\\bar{U}_{j}^{T}\\bar{U}_{j}$ . We have the bound from Lemma 3 similar to Equation (23): ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\ell(V_{j}^{(k+1)})\\leq\\ell(V_{j}^{(k)})-\\frac{\\|\\nabla_{V_{j}^{(k)}}\\ell(V_{i}^{(k)})\\|_{F}^{2}}{2\\sigma_{1}\\left(\\bar{U}_{j}^{(k)T}\\bar{U}_{j}^{(k)}\\right)},\\quad j=1,\\ldots,b.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Gradient Descent Update on $\\boldsymbol{s}_{i,j}$ The loss function ", "text_level": 1, "page_idx": 19}, {"type": "equation", "text": "$$\n\\ell(\\boldsymbol{s}_{i,j})=\\frac{1}{2}\\left\\|\\boldsymbol{A}_{i,j}-\\boldsymbol{U}_{i}\\mathrm{diag}(\\boldsymbol{s}_{i,j})\\boldsymbol{V}_{j}^{T}\\right\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "is also convex in ${\\boldsymbol{s}}_{i,j}$ since $\\mathrm{diag(\\cdot)}$ is a convex mapping. We know the gradient exists from Equation (15): ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla\\ell(s_{i,j})=\\mathrm{diag}\\left(U_{i}^{T}U_{i}\\mathrm{diag}(s_{i,j})V_{j}^{T}V_{j}\\right)-\\mathrm{diag}\\left(U_{i}^{T}A_{i,j}V_{j}\\right)}\\\\ &{\\qquad\\qquad=\\left(\\left(U_{i}^{T}U_{i}\\right)\\odot\\left(V_{j}^{T}V_{j}\\right)\\right)s_{i,j}-\\mathrm{diag}\\left(U_{i}^{T}A_{i,j}V_{j}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and the Lipschitz constant of the gradient is $\\sigma_{1}\\left(\\left(U_{i}^{T}U_{i}\\right)\\odot\\left(V_{j}^{T}V_{j}\\right)\\right)$ . The bound from Lemma 3 for the diagonal factors is as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\ell(s_{i,j}^{(k+1)})\\leq\\ell(s_{i,j}^{(k)})-\\frac{\\|\\nabla_{s_{i,j}^{(k)}}\\ell(s_{i,j}^{(k)})\\|_{2}^{2}}{2\\sigma_{1}\\left(\\left(U_{i}^{T}U_{i}\\right)\\odot\\left(V_{j}^{T}V_{j}\\right)\\right)},\\quad i,j=1,\\ldots,b.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Combining Equations (23) to (25), we retrieve the bound in Theorem 1: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell(U_{*}^{(k+1)},V_{*}^{(k)},s_{*,*}^{(k)})\\le\\ell(U_{*}^{(k)},V_{*}^{(k)},s_{*,*}^{(k)}),}\\\\ &{\\quad\\ell(U_{*}^{(k+1)},V_{*}^{(k+1)},s_{*,*}^{(k)})\\le\\ell(U_{*}^{(k+1)},V_{*}^{(k)},s_{*,*}^{(k)}),}\\\\ &{\\quad\\ell(U_{*}^{(k+1)},V_{*}^{(k+1)},s_{*,*}^{(k+1)})\\le\\ell(U_{*}^{(k+1)},V_{*}^{(k+1)},s_{*,*}^{(k)})}\\\\ &{\\Rightarrow\\ell(U_{*}^{(k+1)},V_{*}^{(k+1)},s_{*,*}^{(k+1)})\\le\\ell(U_{*}^{(k)},V_{*}^{(k)},s_{*,*}^{(k)})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "C Experimental Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we provide the experimental details. Throughout the experiments, we used 8 NVIDIA A40 GPUs or 4 NVIDIA L40S GPUs for training and evaluation, and a single NVIDIA A100 GPU with 40GB memory for runtime evaluation. ", "page_idx": 20}, {"type": "text", "text": "C.1 Datasets and Benchmarks ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Image Datasets For image classification tasks, we use CIFAR-10 [28], CIFAR-100 [28], and ImageNet-1k [29] datasets for our experiments. CIFAR-10 and 100 contain 50,000 training and 10,000 test images, each of which is $32\\times32$ color images of 10 and 100 classes, respectively. ImageNet-1k consists of 1,281,167 training and 50,000 validation images of 1,000 classes. ", "page_idx": 20}, {"type": "text", "text": "Common Sense Reasoning Benchmarks For our large language model evaluation, we use the following common sense reasoning benchmarks: Physical Interaction: Question Answering (PIQA) [35], HellaSwag[36], WinoGrande[37], BoolQ[38], OpenBookQA[39], AI2\u2019s Reasoning Challenge (ARC)-easy and challenge [40]. PIQA targets the task of physical common sense reasoning, with 16,000 examples for training, 2,000 for development, and 3,000 for testing. HellaSwag is composed of 10k questions that are specifically hard for the machines, though trivial for humans $95\\%$ accuracy). Winogrande is a large-scale dataset of $44\\mathrm{k}$ pronoun resolution problems. BoolQ consists of 15,942 yes/no questions. OpenBookQA is modeled after open book exams for assessing human understanding of a subject, containing 5,957 multiple-choice elementary-level science questions (4,957 train, 500 dev, 500 test). AI2\u2019s Reasoning Challenge (ARC) dataset consists of 7,787 multiple-choice science exam questions, and the questions are categorized into \u201ceasy\u201d and \u201cchallenging\u201d subsets. ", "page_idx": 20}, {"type": "text", "text": "C.2 CIFAR-10/100 and ImageNet-1k Image Classification Training ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For CIFAR-10 and CIFAR-100 training, we trained the ViT-Small models with $4\\times4$ -sized patches [19]. We trained ViT-Base models with $16\\times16$ -sized patches for ImageNet-1k training. All models were trained by the AdamW [22] optimizer. ", "page_idx": 20}, {"type": "text", "text": "In the ViT models, we replaced the weight matrices of query, key, and value projection layers in one attention module and those in the feed-forward modules. In addition, we stacked the weights of query, key, and value weights and modeled them by one BLAST matrix. ", "page_idx": 20}, {"type": "text", "text": "The BLAST factors were randomly initialized to have the desired standard deviation $\\sigma=0.02$ while having zero-mean, where the standard deviation 0.02 was also used for initializing the weights of the original-sized ViTs. Specifically, we initialized the factors as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U_{i}\\sim\\mathcal{N}(\\mathbf{0},\\sqrt{0.02}I),\\quad\\forall i=1,2,\\ldots,b,}\\\\ &{V_{j}\\sim\\mathcal{N}(\\mathbf{0},\\sqrt{0.02}I),\\quad\\forall j=1,2,\\ldots,b,}\\\\ &{s_{i,j}\\sim\\mathrm{Unif}(0.0,2.0),\\quad\\forall i,j=1,2,\\ldots,b,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For all models, we applied AutoAugment [63]. We summarize the training hyperparameters in Table 5. ", "page_idx": 21}, {"type": "table", "img_path": "n0arS0DDot/tmp/54d6d2a35c16be320a25f728d1e0b62cd84a455c4663564ec0b4203c235f024c.jpg", "table_caption": [], "table_footnote": ["Table 5: Hyperparameters used in training from scratch. "], "page_idx": 21}, {"type": "text", "text": "C.3 Compression and Re-training ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "ViT on ImageNet-1k For ImageNet-1k compression and re-training, we followed a similar strategy to the ImageNet training with minor changes, summarized in Table 6. The ViT-Base with $16\\times16.$ - sized patches was chosen as a baseline model. We decomposed the pre-trained weight matrices of ViT-Base by Algorithm 2 with $K=300$ and $\\delta_{0}=0.1$ . Also, we linearly decayed the step size from 1.0 to 0.0. Then, all compressed models were trained by the AdamW [22] optimizer. ", "page_idx": 21}, {"type": "table", "img_path": "n0arS0DDot/tmp/3453f461b71145e3a7168e8c701173ffd81ee756c7ed5e2b87fd908bde50b834.jpg", "table_caption": ["Table 6: Hyperparameters used in re-training. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Diffusion Model We compressed the DiT-XL model with $2\\times2.$ -sized patches [13], pre-trained on the $256\\times256$ ImageNet training samples. A DiT model is a variant of Vision Transformer [19] with additional adaptive layer normalization (adaLN) [64]. Here, we compressed the stacked query, key, and value weights, the first fully connected layer of the feed-forward module, and the adaLN projection layers by $\\mathrm{BLAST_{9}}$ or low-rank matrices. All weights were compressed by Algorithm 2 with $K=500$ and $\\delta_{0}=0.1$ . We linearly decayed the step size from 1.0 to 0.0. The parameters of the BLAST and the low-rank matrices were set to have the desired compression ratio in total, i.e., we remove $50\\%$ (or $20\\%$ ) of the total parameters out of the network. We present the summary in Tables 7 and 8. We generated 50,000 images using the original, the low-rank-compressed, and the BLAST-compressed DiT. ", "page_idx": 21}, {"type": "table", "img_path": "n0arS0DDot/tmp/47bf75445c201fe1fb0d94e329601fbe18db8f010bbcacc9c6b9c5d0736a4ca1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "FID, sFID and IS Evaluation We sampled the novel images using DDPM sampler [65] for FID, sFID, and IS evaluation in Table 2. The step size was set to 250 for each model. Then, the FID, sFID, and IS were computed between each pool of generated images and the 50,000 ImageNet validation images to estimate the distributional discrepancy between the target and the generated samples. ", "page_idx": 21}, {"type": "text", "text": "Large Language Model For the large language model compression, we used the Llama-7B pretrained model, publicly available at https://huggingface.co/huggyllama/llama-7b. For the $50\\%$ compression ratio, we compressed all the weights in the main modules of Llama-7B with BLAST with the parameters described in Table 9. For the $20\\%$ and $10\\%$ compression ratios, we compressed the weights of Q_proj, K_proj, gate_proj, up_proj layers to match the target compression ratio of the total parameter counts. Also, by following [66], we compress Q_proj and K_proj layers for the first 10 attention modules. The parameters used in the experiment are summarized in Tables 8, 9 and 11. All weights were compressed by Algorithm 2 with $K=300$ and $\\delta_{0}=0.1$ . We linearly decayed the step size from 1.0 to 0.0. The factorization process takes 3.38 GPU hours for the BLAST weights of $b=16$ on NVIDIA A40 GPUs. ", "page_idx": 21}, {"type": "table", "img_path": "n0arS0DDot/tmp/42643c6b0f28c3feca1c8f3754416c45ee3be58eaa7024d50f0cf1f7843a141d.jpg", "table_caption": [], "table_footnote": ["Table 8: Hyperparameters used for the $\\mathrm{BLAST_{9}}$ -compressed DiT-XL/2 with $20\\%$ compression ratio. $m,n$ : size of the original matrix, $b$ : number of row/column partitions, $r$ : BLAST rank parameter, Layer Indices: indices of layers that the BLAST matrix replaces the weight. "], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "To re-train the compressed Llama models, we used a subset2 of the SlimPajama dataset [34] for 400 steps using 0.47B tokens. The global batch size was set to 576, and the models were trained on 4 NVIDIA L40S GPUs. See Table 6 for details. ", "page_idx": 22}, {"type": "table", "img_path": "n0arS0DDot/tmp/1edab9ced2d3141fc5b458468fb14a606d3a5e92f022d8cb23f57dcdc6a65eb6.jpg", "table_caption": ["We used Language Model Evaluation Harness3 [67] for the zero-shot classification accuracy evaluation. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "n0arS0DDot/tmp/cb7444a76fdcbe9718edd8cce3ebb7e018f2300b675a9195b0b1f9480b111c18.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 10: Hyperparameters used for the $\\mathrm{BLAST_{16}}$ -compressed Llama-7B with $20\\%$ compression ratio. $m,n$ : size of the original matrix, $b$ : number of row/column partitions, $r$ : BLAST rank parameter, Layer Indices: indices of layers that the BLAST matrix replaces the weight. ", "page_idx": 22}, {"type": "text", "text": "D Additional Experimental Results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "D.1 Synthetic Experiments on BLAST Factorization ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this experiment, we test the factorization algorithms discussed in Section 3, similar to Figure 3 but with a different target matrix. To be specific, we synthesize a $256\\times256$ -sized $\\mathrm{BLAST_{16}}$ (i.e., $b=16$ ) target matrix with $r^{*}=8$ . Then, we compare the error curve along the iterates of the gradient descent without preconditioning (GD) in Equations (5) to (7), and the preconditioned gradient descent (PrecGD) in Algorithm 2. Here, we consider the exact parameterization setting when $r=r^{*}=8$ and the over-parameterized setting $r=32>r^{*}$ . In Figure 9, unlike the low-rank target matrix, GD does not converge in both cases. However, the preconditioned version easily finds the low-error solution for the exact parameterization. For the overparameterized case, the preconditioned gradient descent method in Algorithm 2 achieved in two orders of magnitude improvement from the simple GD. ", "page_idx": 22}, {"type": "table", "img_path": "n0arS0DDot/tmp/3196a73a1601540697c3927499b1cb5670b6ee1e86033472a7695e570313fbe8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 11: Hyperparameters used for the $\\mathrm{BLAST_{16}}$ -compressed Llama-7B with $10\\%$ compression ratio. $m,n$ : size of the original matrix, $b$ : number of row/column partitions, $r$ : BLAST rank parameter, Layer Indices: indices of layers that the BLAST matrix replaces the weight. ", "page_idx": 23}, {"type": "image", "img_path": "n0arS0DDot/tmp/96f49f6d77e3fd33375236aabd3192f253be9d08e718bc89314add7011208828.jpg", "img_caption": ["BLAST $\\rightarrow$ BLAST. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 9: Plots of normalized reconstruction errors using the BLAST factorization with GD and GD with preconditioning steps (PrecGD) in both exact and rank overparameterized settings, when the target matrix is $\\mathrm{BLAST_{16}}$ . Left: Reconstruction errors when $r=r^{*}$ . Right: Reconstruction errors when $r>r*$ . ", "page_idx": 23}, {"type": "image", "img_path": "n0arS0DDot/tmp/3227b6bd2c0a457877170adc4c0bf57425c955af4cf5fd524b20be79ab481974.jpg", "img_caption": ["Figure 10: Examples of generated images using both low-rank and BLAST decompositions. Both methods compress the original model by $20\\%$ . "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "D.2 Additional Results on Diffusion Model Compression ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We include extended experimental results of the diffusion model compression in Section 4.2 ", "page_idx": 23}, {"type": "text", "text": "Compression-only In Figures 10 and 11, the additional image samples of original uncompressed, low-rank-compressed, and $\\mathrm{BLAST_{9}}$ -compressed DiT [13] models are presented. The images in the same column were sampled using 250 DDIM steps, starting from the same noise vector. The compression ratio was set to $20\\%$ for both models. The figures show that the outputs of the model compressed by BLAST maintain similar features and perceptual quality to the outputs of the original DiT. ", "page_idx": 23}, {"type": "text", "text": "Compression and re-training We present additional samples from the low-rank and BLAST DiT models at the $50\\%$ compression ratio after re-training in Figure 13. Similar to Figure 1, the images generated by the low-rank DiT lose significant image quality, whereas the images from the BLAST DiT preserve the quality and semantics of the original samples. ", "page_idx": 23}, {"type": "image", "img_path": "n0arS0DDot/tmp/810d7f3f1884992d919919612ba16c63e5fbcb180f4c3a6337a8a6a94cca23ad.jpg", "img_caption": ["Figure 11: More examples of generated images using both low-rank and BLAST decompositions. Both methods compress the original model by $20\\%$ . "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "n0arS0DDot/tmp/1cc5055544f45781807a78647ff2b9ad9b038056eab0ae813afd5d0d4ba34d68.jpg", "img_caption": ["Figure 12: Comparison of the images generated by the BLAST and low-rank compressed models. Overall, the low-rank approximated model often generates unrealistic images, which contributes to low scores evident in Table 2. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Evidence of low performance of low-rank-compressed model Some images generated by the $20\\%$ low-rank-compressed model (Figure 12-left) are highly unrealistic and have inferior quality compared to the images generated by original and BLAST-compressed models (Figure 12-right). We observe that these samples contribute to the low scores in Table 2. These samples were computed using 250 DDPM steps, as done by the original DiT work [13]. ", "page_idx": 24}, {"type": "text", "text": "D.3 Additional Results on Large Language Model Compression ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Compression-only We report the performance of LLM-Pruner [47] and Joint Rank- $k$ [66] for the same compression task. LLM-Pruner [47] identifies sparse weights with data to pinpoint unimportant neurons. Joint Rank- $k$ [66] performs a low-rank approximation jointly on weight matrices with similar column spaces by stacking them and applying a truncated SVD. ", "page_idx": 24}, {"type": "text", "text": "In Table 12, we compare the performance degradation of LLM-Pruner, Joint Rank- $k$ , Low-Rank, $\\mathbf{B}\\mathbf{L}\\mathbf{A}\\mathbf{S}\\mathbf{T}_{2}$ , and $\\mathrm{BLAST_{16}}$ , as well as their absolute performance. The first four rows represent the zero-shot performance of Llama-7B [1] from the literature, while the row marked with an asterisk $(^{*})$ indicates our results. ", "page_idx": 24}, {"type": "text", "text": "For $10\\%$ compression, Joint Rank- $k$ achieved the lowest performance degradation, although $\\mathrm{BLAST_{16}}$ also exhibited a similar performance drop. When the model is compressed by $20\\%$ , $\\mathrm{BLAST_{16}}$ surpasses Joint Rank- $k$ [66], LLM-Pruner [47], and low-rank schemes. The zero-shot accuracy versus compression ratio curve in Figure 7 shows that BLAST compression results in less performance drop for the same compression ratio. ", "page_idx": 24}, {"type": "image", "img_path": "n0arS0DDot/tmp/f9a42b726e9a5067bf8b4ef16fb0754d11f8888796117e19af21362f6a1fb8a0.jpg", "img_caption": ["Figure 13: Examples of generated images using DiT [13] starting from the same noise vectors and a deterministic solver. The original model is compressed by $50\\%$ through BLAST or Low-Rank matrices and re-trained for 10 epochs on ImageNet. The images from the model compressed via BLAST preserves the quality of the images of the original model, whereas the images generated by the low-rank model contain artifacts. "], "img_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "n0arS0DDot/tmp/f6e3c59fde763f8b59429c94761783ecccccf9072b00122c237dfe86df7bdba8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Table 12: Zero-shot performance of LLaMA-7B with various compression methods without retraining. All models are not post-trained. CR denotes compression ratio. Bold indicates the best performance under the same compression ratio. Underline refers to the lowest performance drop. ${\\mathrm{BLAST}}_{b}$ indicates the BLAST matrix with $b\\times b$ number of blocks. The mark \u2217represents the results from our experiment. ", "page_idx": 25}, {"type": "table", "img_path": "n0arS0DDot/tmp/8abb429aea04a2bebd9831211ccdbc675db9d762f09de0d7583de57ba59dc471.jpg", "table_caption": ["Compression and Re-training In Table 13, we present the performance of each common sense reasoning benchmark which we report their average in Table 3. "], "table_footnote": ["Table 13: Zero-shot performance of LLaMA-7B with various compression methods after re-training. CR denotes compression ratio. ${\\mathrm{BLAST}}_{b}$ indicates the BLAST matrix with $b\\times b$ number of blocks. "], "page_idx": 25}, {"type": "text", "text": "E Broader Impact ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Our proposed method targets improving the efficiency of the DNN inference. This might have a negative social impact by promoting the accessibility and usability of malicious DNNs such as DeepFake. However, at the same time, we expect the BLAST matrix will bring a tremendous positive social impact. First, it contributes to sustainability by cutting down the energy consumption for the DNN inference. Moreover, the BLAST matrix can improve the accessibility of AI-based medical, educational, and social services by providing a foundation for running those models on mobile devices. Therefore, we believe BLAST will give tangible benefits to our society. ", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Section 1 ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Section 6 ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Section 3 and B. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Appendix C and section 4 ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: Appendix C and section 4. Our code is available at https://github.com/ changwoolee/BLAST. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Appendix C and section 4. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We reported the standard deviation in Table 4. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Section 4 and appendix C. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our paper is written upon widely-used publicly available datasets and models.   \nWe discuss potential harmful impact in Appendix E. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Appendix E. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Instead of providing a decomposed Llama-7B model, we provide the code flie to reproduce the result if one can access the original safeguarded model. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Appendix C. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 30}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Documentation is available in the project directory at https://github.com/ changwoolee/BLAST. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: We did not conduct experiments with human subjects nor croudsourcing. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: We did not conduct experiments with human subjects nor croudsourcing. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]