[{"figure_path": "n0arS0DDot/figures/figures_1_1.jpg", "caption": "Figure 1: Examples of generated images using DiT [13] starting from the same noise vectors and a deterministic solver. The original model is compressed by 50% through BLAST or low-rank matrices and re-trained for 10 epochs on ImageNet. The images from the model compressed via BLAST preserve the quality of the images of the original model, whereas the images generated by the low-rank model contain more undesired artifacts.", "description": "This figure compares image generation results from a diffusion model (DiT) using three different weight matrices: the original full-rank matrix, a low-rank approximation, and a BLAST matrix.  All three models were compressed to 50% of their original parameter size and then retrained on ImageNet. The figure shows that the BLAST compressed model generates images of comparable quality to the original model, while the low-rank compressed model produces images with noticeable artifacts.", "section": "1 Introduction"}, {"figure_path": "n0arS0DDot/figures/figures_2_1.jpg", "caption": "Figure 2: Existing structured matrices and our proposed BLAST matrix. The unique structure of BLAST allows for flexible matrix structures while enabling faster matrix multiplication compared to existing matrices.", "description": "This figure compares various existing structured matrices (Monarch, Low-Rank, Block-Diagonal, GBLR) with the proposed BLAST matrix.  It visually illustrates how each matrix type structures the weight matrix A, highlighting the unique block-wise adaptive structure of BLAST.  The visual representation emphasizes the factor sharing and individual diagonal factors which give BLAST its flexibility and efficiency in matrix multiplication.", "section": "2 Block-Level Adaptive Structured (BLAST) Matrix"}, {"figure_path": "n0arS0DDot/figures/figures_5_1.jpg", "caption": "Figure 9: Plots of normalized reconstruction errors using the BLAST factorization with GD and GD with preconditioning steps (PrecGD) in both exact and rank overparameterized settings, when the target matrix is BLAST16. Left: Reconstruction errors when r = r*. Right: Reconstruction errors when r > r*. ", "description": "This figure compares the convergence rate of the gradient descent (GD) method and the preconditioned gradient descent (PrecGD) method for BLAST matrix factorization. Two scenarios are considered: one where the rank of the BLAST matrix (r) is equal to the true rank of the target matrix (r*), and another where r is greater than r*. The left panel shows that for the exact rank case (r=r*), both GD and PrecGD converge to a low error, but PrecGD converges faster. The right panel illustrates that when the rank is over-parameterized (r>r*), GD fails to converge, while PrecGD still achieves low error.  This demonstrates the effectiveness of the PrecGD method for improving the efficiency of BLAST matrix factorization, especially in scenarios where the true rank is unknown.", "section": "D.1 Synthetic Experiments on BLAST Factorization"}, {"figure_path": "n0arS0DDot/figures/figures_5_2.jpg", "caption": "Figure 1: Examples of generated images using DiT [13] starting from the same noise vectors and a deterministic solver. The original model is compressed by 50% through BLAST or low-rank matrices and re-trained for 10 epochs on ImageNet. The images from the model compressed via BLAST preserve the quality of the images of the original model, whereas the images generated by the low-rank model contain more undesired artifacts.", "description": "This figure compares the image generation quality of three different models: the original DiT model, a low-rank compressed model, and a BLAST compressed model. All models were compressed to 50% of their original size and re-trained on ImageNet. The images generated by the BLAST model maintain a high level of quality compared to the original model, while the low-rank model produces images with noticeable artifacts.", "section": "1 Introduction"}, {"figure_path": "n0arS0DDot/figures/figures_6_1.jpg", "caption": "Figure 4: CIFAR-10/100 accuracy of ViT-S trained from scratch with different structured matrices.", "description": "This figure shows the accuracy and relative FLOPs (floating point operations) for different structured weight matrices when training a Vision Transformer (ViT-S) model from scratch on CIFAR-10 and CIFAR-100 datasets.  It compares the performance of BLAST with other structured matrices like Low-Rank, Monarch, Pixelfly, and Gaudi-GBLR.  The x-axis represents the relative FLOPs, indicating the computational cost compared to a dense ViT-S model (100% FLOPs), and the y-axis shows the classification accuracy.  The results demonstrate the effectiveness of the BLAST matrix in achieving a high accuracy with reduced computational cost compared to other tested methods.", "section": "4.1 Training from Scratch"}, {"figure_path": "n0arS0DDot/figures/figures_6_2.jpg", "caption": "Figure 5: Pre-training result: WikiText 103 test perplexity-FLOPs trade-off curves from GPT-2 with different types of weight matrices.", "description": "This figure shows the performance of different weight matrix types in GPT-2 model pre-training on the WikiText-103 dataset.  The x-axis represents the relative FLOPs (floating-point operations), indicating the computational cost. The y-axis shows the test perplexity, a measure of how well the model predicts the next word in a sequence\u2014lower is better. The plot compares the performance of a dense weight matrix (baseline), low-rank matrices, block diagonal matrices, Monarch matrices, Gaudi-GBLR matrices, and the proposed BLAST matrices.  The BLAST matrix achieves the best perplexity-FLOPs trade-off, indicating its efficiency in compressing the model while maintaining performance.", "section": "4.1 Training from Scratch"}, {"figure_path": "n0arS0DDot/figures/figures_7_1.jpg", "caption": "Figure 4: CIFAR-10/100 accuracy of ViT-S trained from scratch with different structured matrices.", "description": "This figure compares the performance of different structured weight matrices on CIFAR-10 and CIFAR-100 image classification tasks when training Vision Transformers (ViT-S) from scratch.  The x-axis represents the relative FLOPs (floating-point operations) indicating computational efficiency. The y-axis shows the classification accuracy achieved. By comparing the accuracy against relative FLOPs, we can assess the trade-off between model efficiency and performance for different methods.  The methods being compared include dense weight matrices, low-rank matrices, Monarch matrices, Gaudi-GBLR matrices, and the proposed BLAST matrices with different block sizes (BLAST3 and BLAST12).  The figure illustrates how BLAST achieves a better accuracy-efficiency trade-off, outperforming other methods with lower computational cost.", "section": "4.1 Training from Scratch"}, {"figure_path": "n0arS0DDot/figures/figures_8_1.jpg", "caption": "Figure 7: Average zero-shot accuracy vs. compression ratio curves of Llama-7B by BLAST16 before and after re-training.", "description": "This figure shows the performance of Llama-7B language model after compression with BLAST matrices. The x-axis represents the compression ratio (%), while the y-axis shows the average zero-shot accuracy (%). Three lines are plotted: uncompressed model, BLAST16 compression only, and BLAST16 compression with re-training. The graph shows that re-training after compression significantly improves the model's performance, especially at higher compression ratios.", "section": "4.2 Compression and Re-training"}, {"figure_path": "n0arS0DDot/figures/figures_15_1.jpg", "caption": "Figure 1: Examples of generated images using DiT [13] starting from the same noise vectors and a deterministic solver. The original model is compressed by 50% through BLAST or low-rank matrices and re-trained for 10 epochs on ImageNet. The images from the model compressed via BLAST preserve the quality of the images of the original model, whereas the images generated by the low-rank model contain more undesired artifacts.", "description": "This figure compares the image generation results of a diffusion model (DiT) using three different weight matrices: the original, full-weight matrix, a low-rank approximation, and the proposed BLAST matrix. All models were compressed to 50% of their original size and then re-trained on ImageNet.  The BLAST matrix demonstrates superior performance, producing images with much higher fidelity than the low-rank approximation.", "section": "1 Introduction"}, {"figure_path": "n0arS0DDot/figures/figures_23_1.jpg", "caption": "Figure 3: Convergence of the BLAST factorization with and without the preconditioning steps on noiseless low-rank matrix factorization with rank r*. Left: The BLAST parameter r = r*, Right: r > r*. When r > r*, the convergence rate of GD without the preconditioning is slowed down, while GD with the preconditioning (PrecGD) can recover the ground truth with small error.", "description": "This figure shows the convergence of the BLAST factorization algorithm with and without preconditioning.  The left panel shows the case where the rank parameter (r) of the BLAST matrix is equal to the true rank of the target matrix (r*), demonstrating fast convergence with or without preconditioning.  The right panel shows the case where r > r*, illustrating slower convergence for gradient descent (GD) without preconditioning, but much faster convergence with preconditioned gradient descent (PrecGD). This highlights the benefit of preconditioning when the rank is overestimated.", "section": "3.2 Compressing Weights via BLAST Factorization"}, {"figure_path": "n0arS0DDot/figures/figures_23_2.jpg", "caption": "Figure 1: Examples of generated images using DiT [13] starting from the same noise vectors and a deterministic solver. The original model is compressed by 50% through BLAST or low-rank matrices and re-trained for 10 epochs on ImageNet. The images from the model compressed via BLAST preserve the quality of the images of the original model, whereas the images generated by the low-rank model contain more undesired artifacts.", "description": "The figure compares the image generation quality of a diffusion model (DiT) using three different methods: the original uncompressed model, a model compressed using low-rank matrices, and a model compressed using BLAST matrices.  All models are compressed by 50% and then retrained. The images generated by the BLAST compressed model maintain high fidelity to the original images, while the low-rank compressed model produces images with noticeable artifacts.", "section": "1 Introduction"}, {"figure_path": "n0arS0DDot/figures/figures_24_1.jpg", "caption": "Figure 1: Examples of generated images using DiT [13] starting from the same noise vectors and a deterministic solver. The original model is compressed by 50% through BLAST or low-rank matrices and re-trained for 10 epochs on ImageNet. The images from the model compressed via BLAST preserve the quality of the images of the original model, whereas the images generated by the low-rank model contain more undesired artifacts.", "description": "This figure compares image generation results from a diffusion model (DiT) using three different approaches: the original uncompressed model, a model compressed using low-rank matrices, and a model compressed using the proposed BLAST matrices.  All models were compressed to 50% of their original size and then re-trained on ImageNet. The images generated by the BLAST-compressed model maintain a high level of visual quality compared to the original, while the low-rank compressed model introduces noticeable artifacts.", "section": "1 Introduction"}, {"figure_path": "n0arS0DDot/figures/figures_24_2.jpg", "caption": "Figure 1: Examples of generated images using DiT [13] starting from the same noise vectors and a deterministic solver. The original model is compressed by 50% through BLAST or low-rank matrices and re-trained for 10 epochs on ImageNet. The images from the model compressed via BLAST preserve the quality of the images of the original model, whereas the images generated by the low-rank model contain more undesired artifacts.", "description": "This figure compares image generation results from a diffusion model (DiT) using three different weight matrix compression methods. The original, uncompressed model is compared against models compressed using BLAST and low-rank matrices.  The results demonstrate that BLAST maintains image quality better than the low-rank method, which introduces noticeable artifacts.", "section": "1 Introduction"}, {"figure_path": "n0arS0DDot/figures/figures_25_1.jpg", "caption": "Figure 1: Examples of generated images using DiT [13] starting from the same noise vectors and a deterministic solver. The original model is compressed by 50% through BLAST or low-rank matrices and re-trained for 10 epochs on ImageNet. The images from the model compressed via BLAST preserve the quality of the images of the original model, whereas the images generated by the low-rank model contain more undesired artifacts.", "description": "This figure compares image generation results from a diffusion model (DiT) using three different weight matrix compression methods: no compression (original), low-rank compression, and BLAST compression.  Each method reduced the model size by 50%.  The images demonstrate that the BLAST compression method preserves image quality far better than low-rank compression, which introduces significant visual artifacts.", "section": "1 Introduction"}]