[{"figure_path": "FcUyz33OED/figures/figures_1_1.jpg", "caption": "Figure 1: Overall motivation. We advocate for the model to broaden its attention to diverse patterns, mitigating the tendency to overfit to localized patterns. The left segment of the figure contrasts the confusion matrices of the standard transformer with our approach. The right portion showcases the performances of both the standard transformer and our methodology when confronted with objects exhibiting similar local patterns. Tokens with high / low contributions to classification are in red / blue, respectively. Standard transformer tends to overfit to localized patterns. While our method, by modulating tokens with significant contributions, enables the model to garner features from a varied spectrum of target segments, thereby ensuring greater robustness.", "description": "This figure illustrates the core idea of the proposed method. The left panel shows confusion matrices to compare the standard transformer and the proposed method. The right panel shows the classification results of both methods when dealing with objects containing similar local patterns. The proposed method is more robust against overfitting to localized patterns.", "section": "1 Introduction"}, {"figure_path": "FcUyz33OED/figures/figures_2_1.jpg", "caption": "Figure 2: (a) Process of progressive adversarial dropping. The first line means token learned in each stage. (b) Visualization of token weights learned by the classifier. Compared with standard transformer, ours has an advantage in mining sample patterns.", "description": "Figure 2(a) shows the progressive adversarial dropping process of the APCT model. The model iteratively identifies and drops tokens based on their significance, encouraging the model to learn from more diverse patterns.  Figure 2(b) visually compares the token weights learned by the standard transformer and the APCT model. The APCT model shows a more balanced distribution of weights, indicating that it learns from a broader range of patterns.", "section": "3 Methodology"}, {"figure_path": "FcUyz33OED/figures/figures_4_1.jpg", "caption": "Figure 3: Overall architecture of our algorithm, composed of two key modules: Adversarial Significance Identifier and Target-guided Promptor. The former evaluates token significance within the context of the global perception, with the help of dominant feature indexing process from an auxiliary supervising loss that can bolster the precision of the index selection, then producing dropping rate for tokens. Subsequently, Target-guided Promptor enhances key dropout probabilities influenced by rate above, driving the model to explore auxiliary segments for pivotal information. This mechanism mitigates the propensity of the model to overfit to localized patterns.", "description": "This figure shows the overall architecture of the Target-Guided Adversarial Point Cloud Transformer (APCT). It consists of three stages, each containing multiple blocks.  The core components are the Adversarial Significance Identifier and the Target-guided Promptor. The identifier determines the importance of each token and assigns a dropping rate. The prompter then uses these rates to selectively drop tokens during the self-attention mechanism, forcing the network to focus on a broader range of patterns and preventing overfitting.", "section": "3 Methodology"}, {"figure_path": "FcUyz33OED/figures/figures_5_1.jpg", "caption": "Figure 4: Workflow of Adversarial Significance Identifier. Squares with light/dark blue means low/high values, respectively.", "description": "This figure illustrates the process of the Adversarial Significance Identifier module.  The input is a set of tokens, each with C-dimensional features. The module uses an auxiliary supervision process to identify the most significant tokens (indicated by darker blue squares) for each feature channel. These significant tokens are used to generate per-token dropout rates, with higher rates assigned to more significant tokens. The dropout rates are then used by the Target-guided Promptor in subsequent stages to selectively drop these key tokens, forcing the network to learn from less dominant features and achieve greater robustness.", "section": "3.2 Adversarial Significance Identifier"}, {"figure_path": "FcUyz33OED/figures/figures_8_1.jpg", "caption": "Figure 7: Comparative Curves of our algorithm and vanilla dropout strategy.", "description": "This figure shows the comparison results between the proposed APCT method and the vanilla dropout method. The x-axis represents different dropout ratios, and the y-axis represents the mean corruption error (mCE). The curves for different methods and configurations show that the proposed APCT consistently outperforms the vanilla dropout method in terms of robustness.", "section": "4.4 Ablation Study"}, {"figure_path": "FcUyz33OED/figures/figures_15_1.jpg", "caption": "Figure 10: Visualization of samples in ModelNet-C, which is constructed by seven types of corruptions with five levels of severity. Listed examples are from severity level 2.", "description": "This figure shows examples of point cloud corruptions in the ModelNet-C dataset.  It illustrates seven different types of corruption (Scale, Jitter, Drop-Global, Drop-Local, Add-Global, Add-Local, Rotate), each with five levels of severity. The image displays the effects of each corruption type at severity level 2 on a single airplane model. The leftmost image shows the clean airplane model, and subsequent images show the progressive degradation caused by each corruption type.", "section": "4.1 Results on ModelNet-C"}, {"figure_path": "FcUyz33OED/figures/figures_16_1.jpg", "caption": "Figure 11: Side-by-side comparison of the confusion matrices for two distinct architectures: the standard transformer-based point cloud model (top row) and ours (bottom row).", "description": "This figure compares confusion matrices of a standard transformer-based point cloud model and the proposed APCT model under various corruption types (Scale, Jitter, Drop-Global, Drop-Local, Add-Global, Add-Local, Rotate) from the ModelNet-C dataset. Each sub-figure shows the confusion matrix for a specific corruption type. The top row represents the standard model, while the bottom row represents the APCT model. The color intensity in the matrices indicates the frequency of misclassifications between different classes.", "section": "A.11 Comparative Analysis of Confusion Matrices"}, {"figure_path": "FcUyz33OED/figures/figures_16_2.jpg", "caption": "Figure 12: More visualization results of patterns learned by the classifier on ModelNet-C. Tokens with high / low scores are in red / blue, respectively.", "description": "This figure visualizes the patterns learned by the classifier on the ModelNet-C dataset.  It shows examples of point clouds after being processed by a standard transformer and the proposed APCT method.  The color intensity of each point represents its contribution to the final classification, with red indicating high contribution and blue indicating low contribution. The figure aims to demonstrate that the APCT method learns a more diverse set of patterns compared to the standard transformer, making it more robust to real-world corruptions.", "section": "A.12 More Visualizations of learned patterns"}]