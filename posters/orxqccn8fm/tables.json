[{"figure_path": "orxQccN8Fm/tables/tables_4_1.jpg", "caption": "Table 1: A state-less counter-example with three actions where IRL-based fine-tune (4) shows regularization effect over SFT (1) to maintain weights over unseen data in the demonstration dataset D. Here we assume r \u2208 [0, R].", "description": "This table presents a simplified example with one state and three actions to illustrate the difference between SFT (Supervised Fine-Tuning) and IRL (Inverse Reinforcement Learning) methods in handling unseen data.  SFT assigns all probability to the observed data point, while IRL distributes the probability more evenly, demonstrating its regularization effect and robustness to unseen data.", "section": "3 Reward Learning and Policy Fine Tuning from Demonstration Data"}, {"figure_path": "orxQccN8Fm/tables/tables_6_1.jpg", "caption": "Table 2: Table summarizing the computational costs of proposed methods.", "description": "This table compares the memory usage and computation time for Algorithm 1 and Algorithm 2, relative to standard Supervised Fine-Tuning (SFT).  Algorithm 1, which involves training a reward model and a policy model simultaneously, requires more memory (Forward+Backward) and has a computation time equivalent to 2SFTs plus the time required for generating continuations. Algorithm 2, which implicitly learns the reward model through self-generation, is more efficient, requiring less memory (Backward) and a computation time comparable to a single SFT plus generation time. ", "section": "4 Discussions"}, {"figure_path": "orxQccN8Fm/tables/tables_9_1.jpg", "caption": "Table 3: Test performance of SPIN [Chen et al., 2024] and IRFT (Algorithm 2) based on pythia-1.4b across HuggingFace Open LLM Leaderboard datasets. We keep training for 2 epochs after each generation process and K are calculated after this rule.", "description": "This table presents the performance of the Supervised Fine-Tuning (SFT), Self-Play Fine-tune (SPIN), and Implicit Reward-learning Fine-Tune (IRFT) methods on the pythia-1.4b model across various tasks from the HuggingFace Open LLM Leaderboard.  It compares the performance using different numbers of training epochs and shows the impact of the proposed IRFT method on model performance. The results demonstrate the improved performance of IRFT, especially when compared to the baseline SFT method.", "section": "5.3 Results of IRFT (Algorithm 2)"}, {"figure_path": "orxQccN8Fm/tables/tables_9_2.jpg", "caption": "Table 3: Test performance of SPIN [Chen et al., 2024] and IRFT (Algorithm 2) based on pythia-1.4b across HuggingFace Open LLM Leaderboard datasets. We keep training for 2 epochs after each generation process and K are calculated after this rule.", "description": "This table presents the performance comparison between Standard Fine-tuning (SFT), Self-Play Fine-tuning (SPIN), and the proposed Implicit Reward Fine-tuning (IRFT) methods on the Pythia-1.4b model.  The evaluation is conducted across multiple tasks from the HuggingFace Open LLM Leaderboard. The table shows the performance metrics for each method, including accuracy and exact match scores, and varying the number of iterations (T and K) for the IRFT algorithm.", "section": "5.3 Results of IRFT (Algorithm 2)"}, {"figure_path": "orxQccN8Fm/tables/tables_19_1.jpg", "caption": "Table 3: Test performance of SPIN [Chen et al., 2024] and IRFT (Algorithm 2) based on pythia-1.4b across HuggingFace Open LLM Leaderboard datasets. We keep training for 2 epochs after each generation process and K are calculated after this rule.", "description": "This table presents the results of experiments comparing the performance of the Self-Play Fine-tune (SPIN) algorithm and the proposed Implicit Reward-learning Fine-tuning (IRFT) algorithm (Algorithm 2 in the paper) on the Pythia-1.4b language model.  The models were evaluated across various tasks from the HuggingFace Open LLM Leaderboard. The table shows the performance metrics (accuracy, exact match, etc.) achieved by different versions of the model (with different values of T, a hyperparameter that controls the training process).  This table helps illustrate the effectiveness of the IRFT algorithm in improving the performance of the LLM for multiple tasks.", "section": "5.3 Results of IRFT (Algorithm 2)"}, {"figure_path": "orxQccN8Fm/tables/tables_20_1.jpg", "caption": "Table 3: Test performance of SPIN [Chen et al., 2024] and IRFT (Algorithm 2) based on pythia-1.4b across HuggingFace Open LLM Leaderboard datasets. We keep training for 2 epochs after each generation process and K are calculated after this rule.", "description": "This table shows the performance comparison of different methods on the HuggingFace Open LLM Leaderboard.  The methods compared include Standard Supervised Fine-Tuning (SFT), Self-Play Fine-Tune (SPIN), and the proposed Implicit Reward Fine-Tuning (IRFT) method with different numbers of training epochs.  The results are presented across various tasks included in the leaderboard, highlighting the improvement in performance achieved by the proposed method over SFT and SPIN.", "section": "5.3 Results of IRFT (Algorithm 2)"}, {"figure_path": "orxQccN8Fm/tables/tables_20_2.jpg", "caption": "Table 7: Test performance of different versions of the base model zephyr-7b-sft-full.", "description": "This table compares the performance of different versions of the base model, zephyr-7b-sft-full, across various tasks from the HuggingFace Open LLM Leaderboard.  It highlights the impact of using different versions of the evaluation code and model on the overall results, emphasizing the importance of consistent evaluation methodology for accurate comparisons.", "section": "5.3 Results of IRFT (Algorithm 2)"}, {"figure_path": "orxQccN8Fm/tables/tables_20_3.jpg", "caption": "Table 8: Win-rate of models trained by different methods. In this table we test with LoRA (r = 64).", "description": "This table shows the win-rate of models trained using different methods: SFT, SPIN (which is equivalent to IRFT with T=1), and IRFT with T=5.  The win-rate represents the percentage of times the reward of the model's generated continuation is higher than the reward of the reference model's continuation.  The results demonstrate the improvement in the ability to distinguish between preferred and rejected continuations as the method progresses from SFT to IRFT with T=5, showcasing the efficacy of the reward learning approach.", "section": "5.2 Results of RFT (Algorithm 1)"}]