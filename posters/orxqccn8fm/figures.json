[{"figure_path": "orxQccN8Fm/figures/figures_2_1.jpg", "caption": "Figure 1: Left: Difference between SFT and the two proposed methods: RFT (Algorithm 1) and IRFT (Algorithm 2); Right: Log probability gap between the chosen/preferred continuation and the rejected/non-preferred continuations for different methods. All methods only consume the chosen/preferred data, but RFT and IRFT can effectively distinguish between chosen and rejected continuations; see Example 2 in Sec. 3 for the detailed settings.", "description": "This figure compares three methods for Large Language Model (LLM) alignment: Supervised Fine-Tuning (SFT), Reward Fine-Tuning (RFT), and Implicit Reward Fine-Tuning (IRFT).  The left panel shows the conceptual differences in the methods' approaches. SFT uses only behavior cloning on demonstration data. RFT incorporates Inverse Reinforcement Learning (IRL) to learn a reward model simultaneously with a policy model. IRFT leverages self-generated data to implicitly learn a reward model and refine the policy. The right panel shows the log probability gap between chosen/preferred and rejected/non-preferred continuations.  RFT and IRFT show a greater ability to distinguish between these than SFT, even when trained only on chosen/preferred data. This highlights the effectiveness of the proposed reward learning methods in LLM alignment.", "section": "3 Reward Learning and Policy Fine Tuning from Demonstration Data"}, {"figure_path": "orxQccN8Fm/figures/figures_8_1.jpg", "caption": "Figure 1: Left: Difference between SFT and the two proposed methods: RFT (Algorithm 1) and IRFT (Algorithm 2); Right: Log probability gap between the chosen/preferred continuation and the rejected/non-preferred continuations for different methods. All methods only consume the chosen/preferred data, but RFT and IRFT can effectively distinguish between chosen and rejected continuations; see Example 2 in Sec. 3 for the detailed settings.", "description": "The figure illustrates the performance of three methods: SFT, RFT, and IRFT in distinguishing between preferred and rejected continuations.  The left panel shows a schematic representation of the three methods, highlighting the differences in their approach to learning from data.  The right panel displays the log probability gap, which quantifies the ability of each method to differentiate between the preferred and rejected continuations.  RFT and IRFT, which incorporate reward learning, significantly outperform SFT, demonstrating their superior ability to learn preferences from the demonstration data.", "section": "3 Reward Learning and Policy Fine Tuning from Demonstration Data"}]