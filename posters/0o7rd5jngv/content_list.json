[{"type": "text", "text": "Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mingze Wang School of Mathematical Sciences, Peking University, Beijing, China mingzewang@stu.pku.edu.cn ", "page_idx": 0}, {"type": "text", "text": "WeinanE Center for Machine Learning Research and School of Mathematical Sciences, Peking University, Beijing, China AI for Science Institute, Beijing, China weinan@math.pku.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We conduct a systematic study of the approximation properties of Transformer for sequence modeling with long, sparse and complicated memory. We investigate the mechanisms through which different components of Transformer, such as the dot-product self-attention, positional encoding and feed-forward layer, affect its expressive power, and we study their combined effects through establishing explicit approximation rates. Our study reveals the roles of critical parameters in the Transformer, such as the number of layers and the number of attention heads. These theoretical insights are validated experimentally and offer natural suggestions for alternative architectures. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, Transformer networks (Vaswani et al., 2017) have emerged as foundational models, setting new benchmarks across various domains, including natural language processing (NLP), computer vision (CV), and protein folding. Despite their impressive practical achievements, the underlying mechanisms and theoretical foundations of Transformer networks remain largely elusive. ", "page_idx": 0}, {"type": "text", "text": "Transformer networks encompass various components, posing challenges to their comprehensive understanding. A typical Transformer comprises multiple layers, each consisting of a multi-head self-attention (Attn) sub-layer and a feed-forward network (FFN) sub-layer, integrated with residual blocks. FFN is a two-layer nonlinear network, while Attn includes dot-product (DP) and positional encoding (PE). To get a better understanding of how Transformer works in practice, we need to study several key issues. These include: ", "page_idx": 0}, {"type": "text", "text": "(i) How do the key hyper-parameters, for example, the number of layers, the number of Attn heads and the with of FFN layers, affect the performance of the Transformer network? ", "page_idx": 0}, {"type": "text", "text": "(ii) How do the Attn and FFN layers contribute differently to the overall performance? (ii) How does DP attention work, and is the DP structure necessary? ", "page_idx": 0}, {"type": "text", "text": "(iv) How effcient is PE in modeling long-range correlations? ", "page_idx": 0}, {"type": "text", "text": "Extensive empirical research on Transformer components has led to the proposal of numerous alternatives to the current structure of Transformer. For example, several relative positional encodings (RPE) (Shaw et al., 2018; Raffel et al., 2020; Su et al., 2024; Press et al., 2022) have been proposed to substitute the original absolute positional encoding (APE), yielding superior performance in challenging tasks like length generalization (Ontanon et al., 2022; Csordas et al., 2021; Anil et al., 2022). Additionally, the necessity of the computationally expensive DP in Attn layers has been widely questioned, and researchers proposed numerous alternatives of DP that show considerable efficacy in specific tasks (Kitaev et al., 2020; Wang et al., 2020; Choromanski et al., 2020; Tay et al., 2021; Allen-Zhu and Li, 2023). Nonetheless, these explorations have not yielded a satisfactory theoretical understanding of the mechanisms of these components. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, we investigate the expressive power of Transformer and the underlying mechanisms of its components for sequence modeling. Our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "We categorize three types of sequence modeling tasks with varying complexity, which are relevant to a broad spectrum of application areas. Task I: Modeling fixed, long but sparse memories. This is relevant to sparse Boolean functions and the traditional $n$ gram model in NLP. Task Il: Modeling adaptive, long but sparse memories. This is relevant to multi-step reasoning tasks as well as various NLP tasks such as dependency parsing, sentiment analysis, and continuation writing. Task IIl: Modeling essentially sparse memories. Examples include feature representation in CV and wavelet analysis in classical signal processing. ", "page_idx": 1}, {"type": "text", "text": "For these sequence modeling tasks, we theoretically investigate the expressive power of Transformer and its variants, establishing explicit approximation rates. Our meticulous analysis provides theoretical insights into the underlying mechanisms of Transformer components. Specifically, ", "page_idx": 1}, {"type": "text", "text": "\u00b7 The distinct roles of the number of layers, the number of Attn heads, and the width of FFN layers. Deeper Transformer are capable of handling memories with more intricate interrelationships, such as nested relationships $\\mathrm{Thm}\\left.4.4\\right)$ . In contrast, for memories lacking such interrelationships, single-layer Transformer with sufficient number of Attn heads and FFN widthshould suffice $\\mathrm{Thm}\\,4.1\\$ . This is quite intuitive: If the content of the next token relies on a few previous tokens in an independent way, we can treat each such dependence by a separate attention head. There is no need for many layers. Additionally, increasing the depth can also alleviate the reliance on the number of heads and width (Prop 4.5). ", "page_idx": 1}, {"type": "text", "text": "\u00b7 The different roles of Attn layers and FFN layers. Our results consistently suggest that: FFN layers are tasked with approximating nonlinear memory functions and the readout function, while Attn layers are responsible for extracting the tokens from these memory locations. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 The functionality and necessity of DP. For the relatively simple Task I, DP is not necessary and can be omitted ${\\mathrm{Thm}}\\,3.1\\$ 0. However, for the more complex Task II, the cooperation between DP and RPE provides the needed interaction between the temporal space and the token space, crucial for the extraction of adaptive memories $\\operatorname{Thm}4.1$ and 4.4). Additionally, for Task II, while the nonlinearity provided by DP is necessary (Prop 4.2), a computationally efficient alternative to DP exists, as we show in Prop 4.3. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 The efficiency of RPE in modeling long-range correlations. Our results consistently suggest that the primary role of RPE is to approximate the memory kernels. Specifically, for Task II, we demonstrate that Transformer with suitable RPE can handle heavy-tailed memories, thus overcoming the Curse of Memory faced by recurrent neural networks (Thm 5.1). Moreover, our findings give theoretical support to the choice of RPE in practice. ", "page_idx": 1}, {"type": "text", "text": "Finally, we conduct experiments to validate our theoretical insights. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Basic notations. We use bold-faced letters for vectors or matrices and lowercase letters for scalars, e.g. $\\pmb{x}\\,=\\,(\\pmb{x}_{1},\\cdot\\cdot\\cdot\\,,\\pmb{x}_{d})^{\\top}\\,\\in\\,\\mathbb{R}^{d}$ and $\\pmb{W}\\,=\\,(W_{i j})_{m\\times n}\\,\\in\\,\\mathbb{R}^{m\\times n}$ . The standard Euclidean inner product between two vectors is denoted by $\\langle\\cdot,\\cdot\\rangle$ , and the $l_{p}$ norm of a vector is represented by $\\left\\|\\cdot\\right\\|_{p}$ We employ standard big-O notations $\\mathcal{O},\\Omega,\\Theta$ to hide absolute positive constants and use $\\Tilde{\\mathcal{O}},\\Tilde{\\Omega},\\Tilde{\\Theta}$ to further hide logarithmic constants. For any positive integer $n$ , let $[n]=\\{1,\\cdot\\cdot\\cdot,n\\}$ . Denote by $\\mathbb{I}\\{E\\}$ the indicator function for an event $E$ . Denote by $a\\vee b=\\operatorname*{max}\\{{\\bar{a}},{\\bar{b}}\\}$ for real number $a,b$ ", "page_idx": 1}, {"type": "text", "text": "2.1  Sequence modeling with long but sparse memories ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Sequence modeling. For convenience, we consider input sequences of infinite length $(t\\in\\mathbb{Z})$ .It is important to note, however, that our theoretical framework can be adapted to finite-length input sequences by masking distant tokens. Formally, the output sequence $\\overbar{\\mathbf{Y}}\\,=\\,(\\mathbf{y}_{t})_{t\\in\\mathbb{Z}}\\,\\in\\,\\mathbb{R}^{c\\times\\ddot{\\mathbb{Z}}}$ is generated from the input sequence $\\pmb{X}=(\\pmb{x}_{t})_{t\\in\\mathbb{Z}}\\in\\mathcal{X}\\,\\subset\\,\\mathbb{R}^{d\\times\\bar{\\mathbb{Z}}}$ via an unknown mapping $\\mathbf{H}.\\left(\\cdot\\right)$ dependent on the input sequence up to the prediction time, and this can be expressed as: ", "page_idx": 2}, {"type": "equation", "text": "$$\ny_{t}=\\mathbf{H}_{t}(X)=f(x_{t},x_{t-1},x_{t-2},\\cdot\\cdot\\cdot),\\quad t\\in\\mathbb{Z}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Our objective is to learn the mapping $\\mathbf{H}.\\left(\\cdot\\right)$ .Additionally, we define the norm $\\begin{array}{r l}{\\|\\mathbf{H}\\|}&{{}:=}\\end{array}$ $\\operatorname*{sup}_{t\\in\\mathbb{Z}}\\operatorname*{iup}_{X\\in\\mathcal{X}}\\|\\mathbf{H}_{t}(X)\\|$ . Without loss of generality, we assume $\\|\\pmb{x}_{t}\\|_{2}\\,\\leq\\,1$ for any $\\boldsymbol{X}\\in\\mathcal{X}$ and set the output dimension $c=1$ for simplicity. ", "page_idx": 2}, {"type": "text", "text": "Long but sparse memories. To model such sequences, we define three types of memories: fixed, long but sparse memories; adaptive, long but sparse memories; and essentially sparse memories. These memory types are prevalent in sequence modeling tasks across diverse domains such as NLP, CV, signal processing, and sparse function representation. In Section 3, 4, and 5, we will formally define these different types and investigate Transformer's capacity to model them. ", "page_idx": 2}, {"type": "text", "text": "2.2  Transformer architecture ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Transformer network. Transformer (Vaswani et al., 2017) is a network architecture designed for processing sequences and generating predictions. Given an input sequence $\\mathbf{\\deltaX}$ , Transformer executes the following steps. Initially, each $d$ -dimensional (dim) input token is transformed into a $D$ -dim vector through an embedding mapping such as ${\\pmb x}_{t}^{(0)}={\\pmb W}_{E}{\\pmb x}_{t}+{\\pmb b}_{E}$ where $W_{E}\\in\\mathbb{R}^{D\\times d},\\pmb{b}_{E}\\in\\mathbb{R}^{D}$ Subsequently, a typical $L$ -layer Transformer with residual block operates according to the formulation: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\pmb X}^{(l-\\frac{1}{2})}={\\pmb X}^{(l-1)}+{\\mathbf A}{\\mathbf t}{\\mathbf n}^{(l)}({\\pmb X}^{(l-1)}),\\quad l\\in[L];}\\\\ {{\\pmb X}^{(l)}={\\pmb X}^{(l-\\frac{1}{2})}+{\\mathbf F}{\\mathbf F}{\\mathbf N}^{(l)}({\\pmb X}^{(l-\\frac{1}{2})}),\\quad l\\in[L].}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "At the $l$ -th layer, $\\mathbf{FFN}^{(l)}(\\cdot)$ denotes a standard (point-wise) two-layer ReLU networks with $m$ neurons: fra given iput $\\pmb{x}\\in\\mathbb{R}^{D}$ $\\begin{array}{r}{\\mathbf{FFN}^{(l)}(\\pmb{x})=\\sum_{k=1}^{m}\\pmb{a}_{k}^{(l)}\\sigma\\big(\\pmb{b}_{k}^{(l)\\top}\\pmb{x}+c_{k}^{(l)}\\big)}\\end{array}$ where $\\sigma(\\cdot)$ isthe activation function such as ReLU. Additionally, in the final ( $L$ -th) FFN layer, the residual block is omitted, commonly referred to as the readout function. Moreover, $\\mathbf{Attn}^{(l)}(\\cdot)$ refers to a multi-head self-attention, as elaborated below. ", "page_idx": 2}, {"type": "text", "text": "Multi-head self-attention. Our focus lies on standard dot-product Attn, denoted as $\\mathbf{Attn}^{(l)}(\\cdot)$ and consistingof $H$ heads. When applied to an input sequence $\\mathbf{\\deltaX}$ , Attn operates as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{Attn}^{(l)}(\\pmb{X})=W_{O}^{(l)}\\sum_{h=1}^{H}W_{V}^{(l,h)}\\pmb{X}\\mathrm{softmax}_{c}\\left(\\left\\langle W_{Q}^{(l,h)}\\pmb{X},\\pmb{W}_{K}^{(l,h)}\\pmb{X}\\right\\rangle+\\pmb{R}^{(l,h)}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "$\\pmb{W}_{Q}^{(l,h)},\\pmb{W}_{K}^{(l,h)},\\pmb{W}_{V}^{(l,h)},\\pmb{W}_{O}^{(l,h)}$ matrices of the $(l,h)$ -th head, respectively. softmaxe represents taking softmax normalization across column. Furthermore, $\\pmb{R}^{(l,h)}\\in\\mathbb{R}^{\\mathbb{Z}\\times\\mathbb{Z}}$ denotes the relative positional encoding matrix, which satisfies $R_{t,s}^{(l,h)}=-\\infty$ for $t<s$ in the next-token prediction paradigm. Consequently, the $t$ -th output of Attn is expressed as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{Attn}_{t}^{(l)}(\\mathbf{\\boldsymbol{X}})=W_{O}^{(l)}\\sum_{h=1}^{H}\\sum_{s=0}^{+\\infty}\\frac{W_{V}^{(l,h)}x_{t-s}\\exp\\left(\\left\\langle W_{Q}^{(l,h)}x_{t},W_{K}^{(l,h)}x_{t-s}\\right\\rangle+R_{t,t-s}^{(l,h)}\\right)}{\\sum_{j=0}^{+\\infty}\\exp\\left(\\left\\langle W_{Q}^{(l,h)}x_{t},W_{K}^{(l,h)}x_{t-j}\\right\\rangle+R_{t,t-j}^{(l,h)}\\right)}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Logarithmic and Power relative positional encoding. As highlighted in Section A, among various types of RPEs, the RPEs used in T5 and KERPLE(log) demonstrate superior performance over Alibi, significantly outperforming other RPEs and APEs in the length generalization task (Kazemnejad et al., 2023; Chi et al., 2022). This finding motivates our focus on the T5-type, KERPLE(log), and Alibi-type RPEs throughout this paper. All of these RPE matrices are Toeplitz, with the form of ", "page_idx": 2}, {"type": "text", "text": "$R_{t,s}=r(t-s)$ . Notably, for T5 and KERPLE(log), $r(t-s)$ undergoes an initial linear decrease followed by a logarithmic decrease as the relative distance $t-s$ increases (Please refer to Section G.1 for more details). In contrast, for Alibi, $r(t-s)$ decreases linearly. Inspired by these discussions, we examine the following RPEs with different decay rates: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\phi_{\\mathrm{log}}(z)=\\left\\{\\!\\!\\begin{array}{l l}{{-\\log z,\\,}}&{{z\\geq1}}\\\\ {{-\\infty,\\,}}&{{\\mathrm{otherwise}}}\\end{array}\\!\\!;\\right.\\;\\;\\;\\phi_{\\mathrm{lin}}(z)=\\left\\{\\!\\!\\!\\begin{array}{l l}{{-z,\\,}}&{{z\\geq0}}\\\\ {{-\\infty,\\,}}&{{\\mathrm{otherwise}}}\\end{array}\\!\\!.\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We will study Transformer with $\\phi_{\\mathrm{type}}~\\mathrm{RPE}~(\\mathrm{type}\\in\\{\\mathrm{log},\\mathrm{lin}\\})$ . Specifically,the RPE in the $(l,h)$ -th head (3) is as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nR_{t,s}^{(l,h)}:=p^{(l,h)}\\phi_{\\tt t y p e}(t-s),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $p^{(l,h)}\\in\\mathbb{R}_{+}$ is a trainable parameter. ", "page_idx": 3}, {"type": "text", "text": "Remark 2.1. For standard Transformer (2) incorporating Atn (3) with RPE (4), the parameters are: the embedding matrx $W_{E}$ $\\mathbf{a}_{k}^{(l)},b_{k}^{(l)},c_{k}^{(l)}$ 2 in the FFN layers; W(l,h), W(.h),W $W_{Q}^{(l,h)},W_{K}^{(l,h)},W_{V}^{(l,\\bar{h})},p^{(l,h)},W_{O}^{(l)}$ in the Attn layers. Notably, the number of parameters is independent of the sequence length, thus enabling the model to handle input sequences of arbitrary length. ", "page_idx": 3}, {"type": "text", "text": "Remark 2.2. In the subsequent sections, we will analyze Transformer and its variants. For the sake of brevity, some shorthand notations are introduced here. For examples, Transformer (2) using $\\phi_{\\mathrm{log}}/\\phi_{\\mathrm{lin}}$ ${\\pmb W}_{Q}^{(l,h)},{\\pmb W}_{K}^{(l,h)}={\\bf0}$ ", "page_idx": 3}, {"type": "text", "text": "2.3  Expressive power via approximation theory ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This paper delves into the expressive power of Transformer through the lens of approximation theory, with a specific focus on establishing explicit approximation rates for Transformers in modeling long but sparse memories. ", "page_idx": 3}, {"type": "text", "text": "Approximation rates v.s. universal approximation. In approximation theory, results are generally categorized into two types: universal approximation (density-type) and approximation rates (Jacksontype) (Jackson, 1930). Universal approximation investigates whether the hypothesis class is dense in the target class. Although this property is fundamental, it does not offer detailed insights into approximation efficiency. In contrast, approximation rates go deeper, emphasizing the efficiency of the approximation. A typical example within this framework is the approximation theory of two-layer neural networks (2NNs). ", "page_idx": 3}, {"type": "text", "text": "Barron space of 2NNs. The well-known universal approximation result for 2NNs asserts that 2NNs can approximate any continuous function (Barron, 1992; 1993; 1994). Nonetheless, this result lacks a characterization of the approximation efficiency, i.e., how many neurons are needed to achieve a certain approximation accuracy? This gap was addressed by the Barron space theory $\\mathrm{E}$ et al., 2019; 2021; Ma et al., 2020). It is established that for any function within Barron space $f\\,\\in\\,\\mathcal B$ (Appendix G.2), 2NNs with $m$ neurons (denoted by $\\mathcal{H}_{m}$ ) can approximate them efficiently, at a rate of $\\operatorname*{inf}_{f_{m}\\in\\mathcal{H}_{m}}\\|f-f_{m}\\|\\leq\\mathcal{O}(\\|f\\|_{B}/\\sqrt{m})$ , remarkably independent of the input dimension $d$ thus avoiding the Curse of Dimensionality (Bellman, 1966; Bach, 2017). ", "page_idx": 3}, {"type": "text", "text": "3Fixed, long but $M$ -sparse memories ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1  Problem formulation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Fixed, long but $M$ -sparse memories. In this section, we investigate a fundamental category of long but sparse memories. Our focus is on scenarios where the positions of the sparse memories remain fixed and are independent of the tokens. The target function is represented by: ", "page_idx": 3}, {"type": "equation", "text": "$$\ny_{t}=f(\\mathbf{x}_{t},\\mathbf{x}_{t-T_{1}},\\cdot\\cdot\\cdot\\mathbf{\\theta},\\mathbf{x}_{t-T_{M}}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $1\\leq T_{1}<\\cdot\\cdot\\cdot<T_{M}<+\\infty$ signify the fixed positions of the memories. Despite the memories being fixed (token-independent) and sparse (finite $M$ ), the task can still be complex due to the potentially long-range memories $(T_{1},\\cdot\\cdot\\cdot\\,,T_{M}$ can be large enough). ", "page_idx": 3}, {"type": "text", "text": "Examples. (I) For Boolean inputs, (5) aligns with sparse Boolean functions, also studied in (Edelman et al., 2022; Bhattamishra et al., 2022). Notably, Bhattamishra et al. (2022) observed that Transformers outperform LSTMs in learning sparse parities. (Il) Selecting the simplest case of $T_{i}\\;=\\;i$ in (5) corresponds to the traditional $n$ -gram model, which consists of short and sparse memories. ", "page_idx": 4}, {"type": "text", "text": "Target class. We focus on target functions described in (5). The readout function $f$ is considered within the standard Barron space $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , i.e., which can be effectively approximated by 2NNs. Moreover, we assume that $f$ is Lipschitz, denoted by $f\\in\\mathcal{L}$ . Thus, we can focus more on investigating the memory extraction power of Transformer. Formally, we define the target class for modeling fixed, long but $M$ -sparse memories as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{H}^{\\mathrm{Fix}}:=\\big\\{\\mathbf{H}:\\;\\mathbf{H}_{t}(X)=(5),\\;\\mathrm{where}\\;1\\leq T_{1}<\\cdot\\,\\cdot<T_{M}<+\\infty,f\\in\\mathcal{B}\\cap\\mathcal{L}\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Transformer hypothesis class. As mentioned in Section 1, one of our main aims is to study the necessity and roles of different components in Transformer, such as DP and RPE. This section focuses on the \u201csimplest\u2019 one-layer Transformer and investigates whether it can effectively model this task. Formally, our hypothesis class includes all one-layer $D P$ -free Transformers, confi gured with $H$ Attn heads and FFN width $m$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{T F}_{(1,H,m)}^{\\mathrm{DPF,type}}:=\\big\\{\\mathbf{TF}:\\mathbf{TF}\\;\\mathrm{is~a~1-layer},\\,H\\mathrm{-head},\\,m\\mathrm{-width}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\;\\mathrm{dot-product-free~Transformer~with~type-RPE}\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.2   Theoretical results and insights ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Theorem 3.1 (Approximation rate). For any target $\\mathbf{H}\\in\\mathcal{H}^{\\mathrm{Fix}}$ (6), rate $n\\in\\mathbb{N}_{+}$ and $H,m\\in\\mathbb{N}_{+}$ there exists a 1-layer Transformer TF E TF(1.H,ty) FDPE,type (T) and a constant C(n) such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{H}-\\mathbf{T}\\mathbf{F}\\|\\|\\leq\\mathcal{E}_{\\mathrm{FFN}}+\\|f\\|_{\\mathrm{Lip}}\\,\\mathcal{E}_{\\mathrm{Attn}}(\\mathrm{type}),}\\\\ &{a n d\\,\\mathcal{E}_{\\mathrm{Attn}}(\\mathrm{type})=\\left\\{\\mathcal{O}\\Big(\\frac{C(n)}{H^{n}}\\left(\\sum_{i=1}^{M}e^{0.01T_{i}}\\right)^{n+1}\\Big)\\,,\\mathrm{type}=\\operatorname*{lim}_{\\mathbf{\\theta}}\\right.}\\\\ &{\\left.\\mathcal{O}\\Big(\\frac{C(n)}{H^{n}}\\left(\\sum_{i=1}^{M}T_{i}^{1.01}\\right)^{n+1}\\right)\\,,\\mathrm{type}=\\log\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1 establishes the approximation rate of one-layer DP-free Transformer for modeling fixed, long but sparse memories. Here, the model complexity is governed by the number of Attn heads $H$ and the width of FFN layers $m$ , while the target complexity arises from the lengths of the memories $T_{1},\\cdot\\cdot\\cdot,T_{M}$ and the complexity of the readout function $f$ . The approximation error comprises two components: the error in the FFN component $\\mathcal{E}_{\\mathrm{FFN}}$ and the error in the Attn component $\\bar{\\mathcal{E}}_{\\mathrm{Attn}}(\\tt t y p e)$ The error $\\mathcal{E}_{\\mathrm{FFN}}$ aligns with classical results, showcasing its effectiveness in approximating Barron functions. On the other hand, $\\mathcal{E}_{\\mathrm{Attn}}(\\tt t y p e)$ hinges on the capacity of the Attn block for modeling long-range memories. Specifically, with increasing memory length, the necessary number of Attn heads grows at a small exponential rate for lin-RPE and at a polynomial rate for log-RPE. ", "page_idx": 4}, {"type": "text", "text": "The proof of Theorem 3.1 is deferred to Appendix B. We can draw some insights from Theorem 3.1 and its proof. ", "page_idx": 4}, {"type": "text", "text": "Different roles of the Attn layer and the FFN layer. The Attn and FFN layers fulfll distinct roles in this task. Specifically, the FFN layer efficiently approximates the nonlinear readout function $f$ ,while the Attn layer is responsible for extracting the token $\\mathbf{\\Delta}x_{t-T_{i}}$ by approximating the memory kernel $\\mathbb{I}\\{\\cdot=T_{i}\\}$ . These components together enable effective modeling of fixed, long, but sparse memories. ", "page_idx": 4}, {"type": "text", "text": "Non-necessity of DP. Theorem 3.1 suggests that the DP component in Attn is not necessary and can be omitted for modeling fixed, long but sparse memories. This is due to the relative simplicity of modeling fixed memory kernels. In a more complex scenario in Section 4, the role of the dot-product becomes important. In contrast to Edelman et al. (2022), which utilizes the property of DP to prove that Transformer can model sparse Boolean functions, our result reveals that one-layer Transformer can successfully tackle the same task even without the dot product in the attention layer. ", "page_idx": 4}, {"type": "text", "text": "Effect of RPE types on expressivity. Our result indicates that the type of the RPE used in the Attn layer subtly influences the Transformer's ability to model long-range memories. As the range of the memory increases, the required head number grows at a slightly exponential rate for lin-RPE and at a polynomial rate for log-RPE. The subtle difference is attributed to the relative simplicity of approximating the memory kernel $\\mathbb{I}\\{\\cdot=T_{i}\\}$ . We will explore a more complex task in Section 5, where the impact of different types of RPE becomes even more pronounced. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4 $K$ -Adaptive, long but $M$ -sparse memories ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1   Problem formulation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we delve into a more complex modeling scenario closely aligned with typical language processing tasks. ", "page_idx": 5}, {"type": "text", "text": "$K$ Adaptive,longbut $M$ -sparse memories. This section investigates the scenario where the positions of the sparse memories are \u201cadaptive\", meaning they depend on the input tokens. The target function is formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\ny_{t}=f(\\pmb{x}_{t},\\pmb{x}_{t-t_{1}},\\cdot\\cdot\\cdot\\mathbf{\\nabla},\\pmb{x}_{t-t_{M}}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the positions of the memory tokens $t_{1},\\cdot\\cdot\\cdot,t_{M}$ follow a nested relationship: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{t_{1}=g_{1}(\\pmb{x}_{t});t_{2}=g_{2}(\\pmb{x}_{t},\\pmb{x}_{t-t_{1}});\\cdot\\cdot\\cdot;t_{K+1}=g_{K+1}(\\pmb{x}_{t},\\pmb{x}_{t-t_{1}},\\cdot\\cdot\\cdot,\\pmb{x}_{t-t_{K}});}\\\\ {\\cdot\\cdot\\cdot;t_{M}=g_{M}(\\pmb{x}_{t},\\pmb{x}_{t-t_{1}},\\cdot\\cdot\\cdot,\\pmb{x}_{t-t_{K}}).\\qquad\\qquad\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, $M$ denotes the number of memory tokens, and $K$ measures the nesting complexity in the memory structure. We assume that memory functions $g_{i}$ generate positive integers for the input tokens, and there exist maximum values $T_{i}$ such that $g_{i}\\leq T_{i}$ . In this adaptive framework, each position of the memory token depends on multiple input tokens and is nested within other memory structures, leading to potential influence of later memory tokens by the earlier ones. ", "page_idx": 5}, {"type": "text", "text": "To facilitate understanding, we first consider a warm-up case, i.e., $K=0$ in (8). In this case, the positions of memories only depend on the current token, without interaction with each other. It can berepresented as: ", "page_idx": 5}, {"type": "equation", "text": "$$\ny_{t}=f(\\pmb{x}_{t},\\pmb{x}_{t-t_{1}},\\cdot\\cdot\\cdot\\mathbf{\\nabla},\\pmb{x}_{t-t_{M}}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $t_{i}=g(\\pmb{x}_{i}),i\\in[M]$ ", "page_idx": 5}, {"type": "text", "text": "Target class. The target classes for modeling adaptive, long but sparse memories in both warm-up and general cases are as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{H}_{(1,M)}^{\\mathrm{Adap}}:=\\big\\{\\mathbf{H}:\\,\\mathbf{H}_{t}(X)=(9),\\mathrm{~where~}g_{i}\\in\\mathcal{B},1\\leq g_{i}\\leq T_{i},i\\in[M];f\\in\\mathcal{B}\\cap\\mathcal{L}\\big\\}.}\\\\ &{\\quad\\mathcal{H}_{(K,M)}^{\\mathrm{Adap}}:=\\big\\{\\mathbf{H}:\\,\\mathbf{H}_{t}(X)=(8),\\mathrm{~where~}g_{i}\\in\\mathcal{B},1\\leq g_{i}\\leq T_{i},i\\in[M];f\\in\\mathcal{B}\\cap\\mathcal{L}\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Examples. Adaptive memories are commonly encountered in practical scenarios. (I) Adaptive sparse Boolean functions, e.g.. $y_{t}=x_{t}\\cdot x_{t-g(x_{t})}\\cdot x_{t-g(x_{t-g(x_{t})})}$ , where $X\\in\\{\\pm1\\}^{\\mathbb{Z}}$ \uff0c $g(x)=1$ for $x=1$ and $g(x)=2$ for $x=-1$ . This fits within our framework (8) with $K=M=2$ (II) Multi-step reasoning, e.g., modeling the $K$ -adaptive, long, but $K$ -sparse memories contains a complicated $K$ -step reasoning task, which require the sequential search following the rule ( $\\left(\\cdot\\cdot\\cdot\\left(\\left(x_{t}\\mapsto x_{t-t_{1}}\\right)\\mapsto\\right.$ $x_{t-t_{2}}\\cdot\\cdot\\cdot\\,\\cdot\\,\\rangle\\;\\mapsto\\;x_{t-t_{K-1}}\\big)\\;\\mapsto\\;x_{t-t_{K}}$ : (III) In $N\\!L P$ tasks like dependency parsing, part-of-speech tagging, sentiment analysis, or continuation writing, the positions of relevant prefix tokens usually depend on the context itself, and can vary depending the content. Additionally, the nested structure is a fundamental characteristic of natural language (Hawkins, 2021). ", "page_idx": 5}, {"type": "text", "text": "Transformer hypothesis class. Some previous works Yun et al. (2019); Kim et al. (2022) treated the softmax with normalization as an approximation of hardmax, suggesting the potential importance of the normalization. In contrast, in this section, we remove the normalization in the denominator of softmax and investigate its ability for sequence modeling. Additionally, to address the discreteness of time and memory values, we consider Transformer with specific precision, as detailed in Appendix C. The precision technique is widely used in LLM training (Kalamkar et al., 2019), such as BFloat16. Formally, the hypothesis class is defined as follows, encompassing all normalization-free $L$ -layer Transformer, configured with $H$ Attn heads and FFN width $m$ and using type-RPE and specific precision. ", "page_idx": 5}, {"type": "text", "text": "Transformer with type-RPE and specific precision} ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1 (Approximation rate, warm-up case). For any target $\\mathbf{H}\\in\\mathcal{H}_{(1,M)}^{\\mathrm{Adap}}$ Hadap (8), rate n E N+, and $H,m\\in\\mathbb{N}_{+}$ there exists a two-layer Transformer $\\mathbf{TF}\\in\\mathcal{T F}_{(2,H,m)}^{\\tt t y p e}$ (12) and a constant $C(n)$ such that: if the width satisfes $m\\geq\\left\\{\\tilde{\\Omega}\\big(\\sum_{i=1}^{M}\\left\\|g_{i}\\right\\|_{\\mathcal{B}}^{2}\\big)\\right.}\\\\ {\\left.\\tilde{\\Omega}\\big(\\sum_{i=1}^{M}\\left\\|\\log g_{i}\\right\\|_{\\mathcal{B}}^{2}T_{i}^{2}\\big)\\right.}$ \uff0c $\\mathtt{t y p e}=\\operatorname*{lin}$ , then the following \uff0c $\\mathtt{t y p e}=\\mathtt{l o g}$ \uff0c approximation rate holds: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{H}-\\mathbf{T}\\mathbf{F}\\|\\|\\leq\\mathcal{E}_{\\mathrm{FFN}}+\\|f\\|_{\\mathrm{Lip}}\\,\\mathcal{E}_{\\mathrm{Attn}}(\\mathrm{type}),}\\\\ &{m d\\,\\mathcal{E}_{\\mathrm{Attn}}(\\mathrm{type})=\\left\\{\\mathcal{O}\\Big(\\frac{C(n)}{H^{n}}\\,\\Big({\\sum_{i=1}^{M}{e^{0.01T_{i}}}}\\Big)^{n+1}\\Big)\\quad,\\mathrm{type}=\\operatorname*{lin}_{\\textstyle\\atop{\\scriptstyle\\mathrm{,~type}=\\log}}\\right.}\\\\ &{m d\\,\\mathcal{E}_{\\mathrm{Attn}}(\\mathrm{type})=\\left\\{\\mathcal{O}\\Big(\\frac{C(n)}{H^{n}}\\,\\Big({\\sum_{i=1}^{M}{T_{i}^{1.01}}}\\Big)^{n+1}\\Big)\\quad,\\mathrm{type}=\\log\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In Theorem 4.1, we present the approximation rate of two-layer Transformer for the warm-up case: modeling 1-adaptive, long but $M$ -sparse memories. This theorem reveals that the approximation error comprises two distinct components: the error in the FFN component $\\mathcal{E}_{\\mathrm{FFN}}$ and the error in the Attn component $\\mathcal{E}_{\\mathrm{Attn}}(\\tt t y p e)$ . A critical difference from 3.1 is the presence of the condition related to the width $m$ of FFN layers. This term arises from using the FFN layer to approximate the memory function $g_{i}$ . Owing to the discreteness of memory $g_{i}$ and the implementation of rounding operations, the approximation within rounding accuracy all achieves zero error after rounding, while it can not get correct rounding beyond this accuracy. In contrast, the error $\\mathcal{E}_{\\mathrm{FFN}}$ is caused by using FFN to approximate the readout function $f$ , the same as $\\mathcal{E}_{\\mathrm{FFN}}$ in Theorem 3.1. ", "page_idx": 6}, {"type": "text", "text": "The proof of Theorem 4.1 can be found in Appendix C.1. Theorem 4.1 and its proof offer several critical insights into the underlying mechanism of Transformer. ", "page_idx": 6}, {"type": "text", "text": "Distinct roles of Attn layers and FFN layers. Our proof elucidates that the FFN layers are tasked with approximating the readout function $f$ and memory functions $g_{i}$ , while the Attn layers are responsible for the extraction of the adaptive memories. It is essential to clarify the difference between \u201capproximating memory functions\u201d and \u201cmemory extraction\". The former refers to utilizing some function to estimate the memory function $g_{i}$ , whereas the latter pertains to extracting the token $\\pmb{x}_{t-g_{i}(\\pmb{x}_{t})}$ from the memory location. ", "page_idx": 6}, {"type": "text", "text": "Cooperation between DP and RPE. In the 2-nd Attn layer, the extraction of the memory functions is achieved through an interplay between DP and RPE. Specifically, this is done through a nice interaction between the temporal space (provided by RPE) and the token space (provided by ${\\cal D}P_{.}$ Please refer to Appendix C.1 for more details. ", "page_idx": 6}, {"type": "text", "text": "Rethinking DP in Attn. Our proof highlights that the core mechanism of Attn is to provide a nice interaction between the temporal space and the token space through the cooperation of DP and RPE. This leads us to the following question: Is DP in Attn necessary and replaceable? The following two propositions provide some hints. ", "page_idx": 6}, {"type": "text", "text": "Proposiion 42 DPvs.DP-fre (infomal) Theexs target $\\mathbf{H}\\in\\mathcal{H}_{(1,1)}^{\\mathrm{Adap}}$ (10) such that: (A) For any $\\epsilon>0$ there exists a 1-layer Atn AttnDP such that $\\left\\|\\left\\|\\mathbf{H}-\\mathbf{Attn}^{\\mathrm{{DP}}}\\right\\|\\right\\|\\leq\\epsilon.$ (B) For any 1-layer DP-fre Atn AttnDPF F, a uniform lower bound holds: $\\begin{array}{r}{\\left\\|\\mathbf{H}-\\mathbf{A}\\mathbf{t}\\mathbf{n}^{\\mathrm{{DPF}}}\\right\\|\\geq\\frac{2}{3}}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "Proposition 4.2 reveal a significant distinction in the expressiveness of two network types for modeling adaptive, long, but sparse memories. Specifically, 1-layer Attn with DP can effectively model this task, while 1-layer DP-free Attn provably fails. This finding underscores the essential role of DP in providing the necessary nonlinearity for Attn to model adaptive memories. The formal version of Proposition 4.2 and its proof can be found in Appendix C.2. ", "page_idx": 6}, {"type": "text", "text": "Proposition 4.3 (Substitute for DP (informal). There exists a substitute structure for $D P,$ requiring only $\\mathcal{O}(D)$ parameters(comparedto $\\mathcal{O}(D^{2})$ in standard $D P$ )that can effectivelymodel $\\textbf{H}\\in$ $\\mathcal{H}_{(1,M)}^{\\mathrm{adap}}$ $D P$   \nthe same approximation rate as stated in Section 4.1. ", "page_idx": 6}, {"type": "text", "text": "Proposition 4.3 demonstrates the existence of a structurally simpler yet effective alternative to traditional DP for modeling (10). This alternative is proposed based on our insights into the role of Attn in facilitating the interaction between the temporal space and the token space. Specifically, we propose a more direct structure to achieve this interaction. More details are deferred to Appendix C.3. ", "page_idx": 7}, {"type": "text", "text": "4.3   Theoretical results and insights: The general case ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "TheorAppxatira eral caeran $\\mathbf{H}\\,\\in\\,\\mathcal{H}_{(K,M)}^{\\mathrm{Adap}}$ rate $n\\,\\in\\,\\mathbb{N}_{+}$ \uff0c and $H,m~\\in~\\mathbb{N}_{+}$ ,there exists an $L$ layer $(L\\ =\\ K\\,+1\\,+\\,\\mathbb{I}\\{M\\ \\geq\\ K\\,+\\,\\mathrm{i}\\}$ )Transformer TF E TFtyPH FH,m) (12anda constant C(n) such that: if the width satisfies if the with satisfies m \u2265 $\\mathbf{\\mu}_{m\\setminus\\mathbf{\\Lambda}}\\int\\tilde{\\Omega}\\big(\\operatorname*{max}_{i\\in[K]}\\vee\\sum_{i=K+1}^{M}\\big\\|g_{i}\\big\\|_{\\mathcal{B}}^{2}\\big)$ $\\begin{array}{r}{\\d^{m}=\\left\\{\\tilde{\\Omega}\\Big(\\operatorname*{max}_{i\\in[K]}\\vee\\overline{{\\sum_{i=K+1}^{M^{\\star\\star}}\\|\\log g_{i}}}\\big\\|_{\\mathcal{B}}^{2}\\,T_{i}^{2}\\big)\\,,\\d\\mathrm{type}=\\log\\right.}\\end{array}$ ,type = lin, then te fllowing aproximation rate holds: ", "page_idx": 7}, {"type": "equation", "text": "$\\lVert\\mathbf{H}-\\mathbf{TF}\\rVert\\leq\\mathcal{E}_{\\mathrm{FFN}}+\\lVert f\\rVert_{\\mathrm{Lip}}\\,\\mathcal{E}_{\\mathrm{Attn}}(\\mathrm{type})$ ", "text_format": "latex", "page_idx": 7}, {"type": "equation", "text": "$$\n\\scriptstyle\\mathrm{\\tilde{\\Sigma}_{F F N}}=\\tilde{O}\\left(\\frac{\\|f\\|_{B}}{\\sqrt{m}}\\right),\\scriptstyle\\mathcal{E}_{\\mathrm{Attn}}(\\mathbf{type})=\\left\\{\\mathcal{O}\\Big(\\frac{C(n)}{H^{n}}\\sqrt{\\sum_{l=1}^{K}\\epsilon^{0.02(n+1)T_{l}}+\\left(\\sum_{l=K+1}^{M}\\epsilon^{0.01T_{l}}\\right)^{2n+2}}\\Big),\\scriptstyle\\mathrm{type}=\\operatorname*{lim}_{\\scriptstyle\\mathrm{\\tilde{\\Sigma}_{f}}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In Theorem 4.4, we establish the approximation rate of deep Transformer for modeling $K$ -adaptive, long but $M$ -sparse memories. Similar to that in Theorem 4.1, the approximation error divides into two distinct terms. A key difference from Theorem 4.1 is the impact of the nested relationships among the memory functions on the required number of layers, Attn heads, and the width of FFN layers. The nested structure within the initial $K$ memories mandates sequential processing in the first $K$ layers one by one. If $M\\geq K+1$ , then in the $K+1$ -th layer, the remaining $M-K$ non-nested memory functions $t_{K+1},\\cdot\\cdot\\cdot\\,,t_{M}$ are concurrently processed. The proof of Theorem 4.4 is deferred to Appendix D.1. ", "page_idx": 7}, {"type": "text", "text": "Distinct roles of the number of layers $L$ , the number of Attn heads $H$ , and the width of FFN layers $m$ . Theorem 4.4 and its proof highlight the distinct roles of three key hyper-parameters of Transformer: $L,\\,H$ , and $m$ . Deeper Transformer are capable of handling the memories with more intricate nested relationships, requiring a $K+1$ layer network for a nesting complexity of $K$ . In contrast, the number of heads and width needed is dictated by the individual complexity of memory functions themselves $(\\|g_{i}\\|_{\\boldsymbol{B}},\\|\\log g_{i}\\|_{\\boldsymbol{B}},$ $T_{i}$ for memory $g_{i}$ ), necessitating that each layer's Attn heads and FFN width are sufficient to capture the memory functions extracted in that layer. This understanding is quite intuitive: If the content of the next token relies on a few previous tokens in an independent way, we can treat each such dependence with a separate attention head. There is no need for many layers. ", "page_idx": 7}, {"type": "text", "text": "Mitigating required head and width with depth. Recalling Theorem 4.1, the memories lacking nested relationships can be efficiently approximated by 2-layer Transformer with a sufficient number of heads and width. The subsequent proposition further explores how increasing the depth of Transformer can influence its efficiency for modeling memories without nested relationships. ", "page_idx": 7}, {"type": "text", "text": "$\\mathbf{H}\\in\\mathcal{H}_{(1,M)}^{\\mathrm{Adap}}$ $n\\in\\mathbb{N}_{+}$ $H,m\\in\\mathbb{N}_{+}$ , the exs an $M+1$ layer Transformer $\\mathbf{TF}\\in{\\mathcal{T F}}_{(M+1,H,m)}^{\\mathtt{t y p e}}$ (12) and a constant $C(n)$ $m\\,\\geq\\,\\left\\{\\tilde{\\Omega}\\Big(\\operatorname*{max}_{i\\in[K]}\\|g_{i}\\|_{\\mathcal{B}}^{2}\\Big)\\right.$ , type = lin,then the $\\mathtt{t y p e}=\\mathtt{l o g}$ following approximation rate holds: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\lVert\\mathbf{H}-\\mathbf{T}\\mathbf{F}\\rVert\\leq\\mathcal{E}_{\\mathrm{FFN}}+\\lVert f\\rVert_{\\mathrm{Lip}}\\,\\mathcal{E}_{\\mathrm{Attn}}(\\mathrm{type}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{I\\,\\mathcal{E}_{\\mathrm{Attn}}(\\mathrm{type})=\\left\\{\\mathcal{O}\\Big(\\frac{C(n)}{H^{n}}\\sqrt{\\sum_{l=1}^{K}e^{0.02(n+1)T_{l}}}\\Big)\\,,\\mathrm{type}=\\operatorname*{lim}_{\\mathbf{\\alpha}}\\right.}\\\\ {\\left.\\mathcal{O}\\Big(\\frac{C(n)}{H^{n}}\\sqrt{\\sum_{l=1}^{K}T_{l}^{2.02(n+1)}}\\Big)\\,\\,\\,,\\mathrm{type}=\\log\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Upon comparing Proposition 4.5 with Theorem 4.1, a notable distinction becomes evident between 2-layer and $M+1$ -layer Transformer in terms of the requirement of the number of Attn heads and the width of FFN layers. Specifically, for 2-layer Transformer, the required width is proportionally linked to the sum of all the memory functions? complexity $(\\|g_{i}\\|_{B}\\,,\\|\\log g_{i}\\|_{B}\\,,T_{i}$ for memory function $g_{i}$ In contrast, for $M+1$ -layer Transformer, the required width correlates with the maximum complexity of the memory functions, much lower than that for 2-layer Transformer. Similarly, the required number of heads for $M+1$ -layer Transformer is much fewer than that for 2-layer Transformer. Please refer to Appendix D.2 for a detailed comparison. The observation suggests that increased depth can significantly reduce the demands on the number of heads and the width. The underlying reason is that deep networks can distribute the memories across different layers for processing, with each layer focusing on approximating only a single memory function. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5  Essentially $M$ -sparse memories ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "5.1  Problem formulation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In language tasks, each token possesses clear semantic meaning. As a result, the structure of the memory is sparse in the original space. This aligns well with our modeling assumptions discussed in Section 3 and 4. However, in other machine learning tasks, we may encounter situations where the input tokens lack distinct semantic meaning. This might happen in image processing or classical signal processing. In these situations, the memory structure could potentially be dense in the original space. Nonetheless, the memory structure might exhibit sparsity in some transformed domain. We call such memory structure \u201cessentially sparse\". In this section, we study the situation in which the memory structure in long-ranged but essentially sparse. For simplicity, we consider the situation in which the positions of the memory kernels are fixed. The analysis can be easily extended to the situation with an adaptive memory structure. ", "page_idx": 8}, {"type": "text", "text": "Fixed, essentially $M$ -sparse memory. Consider the following situation: ", "page_idx": 8}, {"type": "equation", "text": "$$\ny_{t}=f\\left(\\left(\\pmb{X}*\\rho_{1}\\right)(t),\\cdot\\cdot\\cdot,\\left(\\pmb{X}*\\rho_{M}\\right)(t)\\right),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Where $\\rho_{1}(\\cdot),\\cdot\\cdot\\cdot\\,,\\rho_{M}(\\cdot)\\,\\in\\,\\ell^{1}(\\mathbb{N})$ serve as memory kermes and $\\begin{array}{r}{(X\\ast\\rho_{k})(t)=\\sum_{s=0}^{+\\infty}x_{t-s}\\rho_{k}(s)}\\end{array}$ denotes the convolution of the inputs with kernel $\\rho_{k}$ ", "page_idx": 8}, {"type": "text", "text": "Target class and Transformer hypothesis class. The target class for modeling essentially sparse memories is defined as: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{H}^{\\mathrm{Ess}}:=\\big\\{\\mathbf{H}:\\,\\mathbf{H}_{t}(X)=(13),\\mathrm{~where~}\\rho_{1},\\cdot\\cdot\\cdot,\\rho_{M}\\in\\ell^{1}(\\mathbb{N}),\\,f\\in\\mathcal{B}\\cap\\mathcal{L}\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "For the hypothesis class, we consider one-layer dot-product-free Transformer with Attn head number $H$ and FFN width $m$ , as defined in (7). ", "page_idx": 8}, {"type": "text", "text": "Examples. Essentially sparse memories are prevalent in real-world scenarios: ", "page_idx": 8}, {"type": "text", "text": "(1) Image Tasks. In CV, a fundamental objective is identifying and representing meaningful \u201cfeatures\", such as ears, nose, etc. These features can often be modeled using convolution kernels, leading to a task in the form $y=f\\left(X*\\rho_{\\mathrm{eye}},X*\\rho_{\\mathrm{nose}},X*\\rho_{\\mathrm{ear}}\\right)$ . This is an extension of the task we discussed above, in which the kernel functions $\\{\\rho_{j}\\}$ are data-dependent (\"adaptive\u201d in the terminology used in the previous section). ", "page_idx": 8}, {"type": "text", "text": "(I) Signal processing. In signal processing, it is commonly the case that the signals are highly sparse under Wavelet or Fourier transforms. For instance, let $\\psi(\\cdot)$ be a wavelet function and define $\\begin{array}{r}{\\psi_{a,b}(t)\\;:=\\;\\psi(\\frac{t-b}{a})/\\sqrt{|a|}}\\end{array}$ :Then we have $y\\;=\\;f\\left(X*\\psi_{a_{1},b_{1}},\\cdot\\cdot\\cdot\\;,X*\\psi_{a_{M},b_{M}}\\right)$ where $(a_{1},b_{1}),\\cdot\\cdot\\cdot\\,,(a_{M},b_{M})$ might be data-dependent. ", "page_idx": 8}, {"type": "text", "text": "(III) Mathematical calculation. Consider algebraic operations where memory_ exhibits sparsity under specific linear transformations. For example, $y_{t}\\ =\\ 10x_{t}\\,+\\,x_{t-4}/(\\sum_{s=0}^{100}w_{s}x_{t-10-s})\\,-$ $\\textstyle\\sum_{s=0}^{+\\infty}v_{s}x_{t-100-s}$ can be represented in our framework as $y=f\\left(\\pmb{X}*\\rho_{1},\\cdot\\cdot\\cdot,\\pmb{X}*\\rho_{4}\\right)$ ,whereach $\\rho_{i}$ represents a specific linear transformation. ", "page_idx": 8}, {"type": "text", "text": "5.2  Theoretical results and insights ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Theorem 5.1 (Approximation rates). ", "page_idx": 8}, {"type": "text", "text": "(A) Consider $\\mathcal{H}^{\\mathrm{Ess}}$ (14) with exponentially decayed memory kernels, i.e., there exists $\\beta>0$ suchthat ", "page_idx": 8}, {"type": "text", "text": "$\\rho_{1}(t),\\cdot\\cdot\\cdot\\mathrm{\\Phi},\\rho_{M}(t)=\\mathcal{O}(e^{-\\beta t})$ . Then for any target $\\mathbf{H}\\in{\\mathcal{H}}^{\\mathrm{Ess}}$ , rate $n\\in[\\lfloor99\\beta\\rfloor]$ and $H,m\\in\\mathbb{N}_{+}$ there exists a 1-layer $D P$ -free Transformer $\\mathbf{TF}\\in\\mathcal{T F}_{(1,H,m)}^{\\mathrm{DPF,lin}}$ F(,H;m) 7) and a constant C(n) such that ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{H}-\\mathbf{T}\\mathbf{F}\\right\\|\\leq\\mathcal{E}_{\\mathrm{FFN}}+\\left\\|\\boldsymbol{f}\\right\\|_{\\mathrm{Lip}}\\cdot\\mathcal{E}_{\\mathrm{Attn}};\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "$(B)$ Consider $\\mathcal{H}^{\\mathrm{Ess}}$ (14) with polynomially decayed memory kernels, i.e., there exists $\\beta>1$ such that $\\rho_{1}(t),\\cdot\\cdot\\cdot\\;,\\rho_{M}(t)\\,=\\,\\mathcal{O}(\\dot{t}^{-\\beta})$ .Thenforanytarget $\\mathbf{H}\\in\\mathcal{H}^{\\mathrm{Ess}}$ rate $n\\,\\in\\,[\\lfloor0.99\\beta\\rfloor\\,-\\,1]$ and $H,m\\in\\mathbb{N}_{+}$ ,therexists a l-layer $D P$ -freTransformer $\\mathbf{TF}\\in\\mathcal{T F}_{(1,H,m)}^{\\mathrm{DPF,log}}$ (7)anda constant $C(n)$ such that ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|{\\bf H}-{\\bf T}{\\bf F}\\|\\leq\\mathcal{E}_{\\mathrm{FFN}}+\\|f\\|_{\\mathrm{Lip}}\\cdot\\mathcal{E}_{\\mathrm{Attn}};}\\\\ &{\\varsigma_{\\mathrm{FFN}}=\\tilde{\\mathcal{O}}\\left(\\frac{\\|f\\|_{\\mathrm{B}}}{\\sqrt{m}}\\right),\\mathcal{E}_{\\mathrm{Attn}}=\\mathcal{O}\\left(\\frac{C(n)M^{n+1}}{H^{n}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Theorem 5.1 illustrates that one-layer DP-free Transformer with lin-RPE is effective in modeling essentially sparse memories with exponentially decayed kernels, and one-layer DP-free Transformer with log-RPE can efficiently model the memories with polynomially decayed kernels. A key difference between Theorem 5.1 and Theorem 3.1 lies in the memory kernels they address. In Theorem 5.1, the Attn layer should approximate general memory kernels $\\rho_{i}(\\cdot)$ , instead of approximating indicator kernels $\\mathbb{I}\\{\\bar{\\cdot}=T_{i}\\}$ in Theorem 3.1. The proof of Theorem 5.1 can be found in Appendix E. ", "page_idx": 9}, {"type": "text", "text": "Overcoming the Curse of Memory $\\mathbf{(CoM)}$ . For recurrent neural networks (RNN), it was discovered (Li et al., 2021; 2022) that both approximation and optimization become exceedingly difficult when the target has long-term memory. This phenomenon is referred as the \u201ccurse of memory'\", or \"CoM'\". It was shown in (Li et al., 2021; 2022) that RNN requires an exponentially large number of neurons to approximate targets with heavy-tailed memory kernels, such as the ones that exhibit polynomial decay. In contrast, Theorem 5.1 reveals that Transformer with log-RPE efficiently handles polynomial decaying memory kernels, requiring only a polynomial number of neurons for effective approximation. This finding theoretically elucidates the superior performance of T5's RPE and KERPLE(log) in length generalization task in practice (Section G.1). ", "page_idx": 9}, {"type": "text", "text": "6  Experimental Validation ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "As summarized in Section 1, our theoretical analysis reveals novel insights into the expressive power and mechanisms of Transformer. To validate these insights, we conduct experiments ranging from simple toy models to more complex language model pre-training. Due to space constraints, detailed experimental validation and practical implications of our insights are presented in Appendix H. ", "page_idx": 9}, {"type": "text", "text": "7  Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we investigate theoretically the expressive power and the mechanisms of Transformer for modeling long but sparse memories. Our analysis establishes explicit approximation rates and offers much-needed insights into the functionalities of the various components of Transformer. However, we still have a long way to go for a full theoretical understanding of Transformer. For instance, although we have investigated the mechanisms of Transformer in terms of expressive power, the evolution of the mechanisms during the training process remains elusive. Recent studies revealed that Transformer exhibits multi-phase learning dynamics (Boix-Adsera et al., 2023) and undergoes phase transitions (Olsson et al., 2022) during training, akin to the phenomenon of learning with increasing complexity in classical neural networks (Kalimeris et al., 2019; Xu et al., 2019; Rahaman et al., 2019; Abbe et al., 2023a; Wang and Ma, 2023). These and other issues will be studied in future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported in part by the National Key Basic Research Program of China (No. 2015CB856000). We thank Prof. Qianxiao Li, Prof. Lei Wu, Dr. Zhong Li, and Dr. Hongkang Yang for helpful discussions and anonymous reviewers for their valuable suggestions. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. Sgd learning on neural networks: leap complexity and saddle-to-saddle dynamics. In The Thirty Sixth Annual Conference on Learning Theory, pages 2552-2623. PMLR, 2023a. 10   \nEmmanuel Abbe, Samy Bengio, Aryo Lotf, and Kevin Rizk. Generalization on the unseen, logic reasoning and degree curriculum. International Conference on Machine Learning, 2023b. 17   \nEkin Akyirek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022.17   \nZeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 1, context-free grammar. arXiv preprint arXiv:2305.13673, 2023. 2, 17   \nCem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length generalization in large language models. Advances in Neural Information Processing Systems, 35:38546-38556, 2022. 2,17   \nFrancis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal of Machine Learning Research, 18(1):629-681, 2017. 4, 64   \nYu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. arXiv preprint arXiv:2306.04637, 2023.17   \nAndrew R Barron. Neural net approximation. In Proc. 7th Yale Workshop on Adaptive and Learning Systems, volume 1, pages 69-72, 1992. 4, 64   \nAndrew R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information theory, 39(3):930-945, 1993. 4, 64   \nAndrew R Barron. Approximation and estimation bounds for artificial neural networks. Machine Learning, 14(1):115-133, 1994. 4, 64   \nRichard Bellman. Dynamic programming. Science, 153(3731):34-37, 1966. 4, 64   \nIz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. 17   \nSatwik Bhattamishra, Kabir Ahuja, and Navin Goyal. Onthe ability and lmitations of transformers to recognize formal languages. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020. 17   \nSatwik Bhattamishra, Arkil Patel, Varun Kanade, and Phil Blunsom. Simplicity bias in transformers and their ability to learn sparse boolean functions. arXiv preprint arXiv:2211.12316, 2022. 5   \nEnric Boix-Adsera, Etai Littwin, Emmanuel Abbe, Samy Bengio, and Joshua Susskind. Transformers learn through gradual rank increase. arXiv preprint arXiv:2306.07042, 2023. 10   \nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. 17   \nEmmanuel J Candes and Michael B Wakin. An introduction to compressive sampling. IEEE signal processing magazine, 25(2):21-30, 2008. 17   \nTa-Chung Chi, Ting-Han Fan, Peter J Ramadge, and Alexander Rudnicky. Kerple: Kernelized relative positional embedding for length extrapolation. Advances in Neural Information Processing Systems, 35:8386-8399, 2022. 3, 17   \nKrzysztof Choromanski, Valeri Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. 2, 17   \nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1-113, 2023.17   \nRobert Csordas, Kazuki Irie, and Jurgen Schmidhuber. The devil is in the detail: Simple tricks improve systematic generalization of transformers. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021. 2, 17   \nMostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal transformers. International Conference on Learning Representations, 2019. 16   \nDavid L Donoho. Compressed sensing. IEEE Transactions on information theory, 52(4):1289-1306, 2006.17   \nWeinan E, Chao Ma, and Lei Wu. A priori estimates of the population risk for two-layer neural networks. Communications in Mathematical Sciences, 17(5):1407-1425, 2019. 4, 64   \nWeinan E, Chao Ma, and Lei Wu. The barron space and the fow-induced function spaces for neural network models. Constructive Approximation, pages 1-38, 2021. 4, 64   \nBenjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creationin self-attntion mechanisms. In International Conference on Machine Learning, pages 5793-5831. PMLR, 2022. 5, 16   \nNelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. https://transformer-circuits.pub/2021/framework/index.html. 17   \nGuhao Feng, Yuntian Gu, Bohang Zhang, Haotian Ye, Di He, and Liwei Wang. Towards revealing the mystery behind chain of thought: a theoretical perspective. arXiv preprint arXiv:2305.15408, 2023.17   \nW Nelson Francis and Henry Kucera. Brown corpus manual. Letters to the Editor, 5(2):7, 1979. 17   \nShivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? acase studyof simple function classes. Advances in Neural Information Processing Systems, 35:30583-30598, 2022. 17   \nAngeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dimitris Papailiopoulos. Looped transformers as programmable computers. International Conference on Machine Learning, 2023. 16   \nAaron Gokaslan and Vanya Cohen. Openwebtext corpus. http: //Skylion007 .github.io/ OpenWebTextCorpus, 2019. 66   \nMichael Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156-171, 2020. 17   \nJeff Hawkins. A thousand brains: A new theory of intelligence. Basic Books, 2021. 6   \nDunham Jackson. The theory of approximation, volume 11. American Mathematical Soc., 1930. 4, 65   \nHaotian Jiang and QianxiaoLi. Approximation theory of transformer networks for sequence modeling. arXiv preprint arXiv:2305.18475, 2023. 17   \nDhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, et al. A study of bfloat16 for deep learning training. arXiv preprint arXiv:1905.12322. 2019. 6. 23   \nDimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan Yang, Boaz Barak, and Haofeng Zhang. Sgd on neural networks learns functions of increasing complexity. Advances in neural information processing systems, 32, 2019. 10   \nAmirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers. Advances in Neural Information Processing Systems, 2023. 3, 17   \nJunghwan Kim, Michell Kim, and Barzan Mozafari. Provable memorization capacity of transformers. In The Eleventh International Conference on Learning Representations, 2022. 6   \nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020. 2, 17   \nZhong Li, Jiequn Han, Qianxiao Li, and Weinan E. On the curse of memory in recurrent neural networks: Approximation and optimization analysis. International Conference on Learning Representations, 2021. 10   \nZhong LiJqun Han,WenanE,and Qianxiao Li.Aproximation and optimizatin theoryfoinear continuous-time recurrent neural networks. Journal of Machine Learning Research, 23(42):1-85, 2022.10   \nZonglin Li, Chong You, Srinadh Bhojanapalli, Daliang Li, Ankit Singh Rawat, Sashank Reddi, Ke Ye, Felix Chern,Felix Yu, Ruiqi Guo, et al. The lazy neron phenomenon: On mergence of activation sparsity in transformers. In Conference on Parsimony and Learning (Recent Spotlight Track), 2023. 17   \nBingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers learn shortcuts to automata. arXiv preprint arXiv:2210.10749, 2022. 17   \nChao Ma and Lexing Ying. Why self-attention is natural for sequence-to-sequence problems? a perspective from symmetries. arXiv preprint arXiv:2210.06741, 2022. 17   \nChao Ma, Stphan Wojtowytsch, Lei Wu, and Weinan E. Towards a mathematical understanding of neural network-based machine learning: what we know and what we don't. arXiv preprint arXiv:2009.10713, 2020. 4, 64, 65   \nArvind Mahankali, Tatsunori B Hashimoto, and Tengyu Ma. One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention. arXiv preprint arXiv:2307.03576, 2023. 17   \nWilliam Merrill and Ashish Sabharwal. The expressive power of transformers with chain of thought. arXiv preprint arXiv:2310.07923, 2023a. 17   \nWilliam Merrill and Ashish Sabharwal. The parallelism tradeoff: Limitations of log-precision transformers. Transactions of the Association for Computational Linguistics, 11:531-545, 2023b. 17   \nWilliam Merrill Ashish Sabharwal, and Noah A Smith. Saturated transformers are constant-depth threshold circuits. Transactions of the Association for Computational Linguistics, 10:843-856, 2022.17   \nYves Meyer. Wavelets and Operators: Volume 1. Cambridge university press, 1992. 17   \nTetsuya Nasukawa and Jeonghee Yi. Sentiment analysis: Capturing favorability using natural language processing, In Proceedings of the 2nd international conference on Knowledge capture, pages 70-77,2003. 17   \nJoakim Nivre and Mario Scholz. Deterministic dependency parsing of english text. In COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics, pages 64-70, 2004.17   \nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022. 10, 17 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Santiago Ontanon, Joshua Ainslie, Vaclav Cvicek, and Zachary Fisher. Making transformers solve compositional tasks. Proceedings of the 60Oth Annual Meeting of the Association for Computational Linguistics,2022.2,17 ", "page_idx": 13}, {"type": "text", "text": "OpenA1. Gpt-4 technical report. https://cdn.openai.com/papers/gpt-4.pdf, 2023. 17 ", "page_idx": 13}, {"type": "text", "text": "Jorge Perez, Pablo Barcel6, and Javier Marinkovic. Attention is turing complete. The Journal of Machine Learning Research,22(1):3463-3497,2021. 16 ", "page_idx": 13}, {"type": "text", "text": "Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. International Conference on Learning Representations, 2022. 1,17   \nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020. 1, 17, 64   \nNasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua Bengio, and Aaron Courville. On the spectral bias of neural networks. In International Conference on Machine Learning, pages 5301-5310. PMLR, 2019. 10   \nClaude Elwood Shannon. A mathematical theory of communication. The Bell system technical journal, 27(3):379-423, 1948. 17   \nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics, 2018.1, 17   \nLingfeng Shen, Aayush Mishra, and Daniel Khashabi. Do pretrained transformers really learn in-context by gradient descent? arXiv preprint arXiv:2310.08540, 2023. 17   \nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 1, 17   \nYi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer: Rethinking self-attention for transformer models. In International Conference on Machine Learning, pages 10183-10192. PMLR, 2021. 2,17   \nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 17   \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 1, 3, 17,66   \nJohannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pages 35151-35174. PMLR, 2023. 17   \nJohannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi, Nicolas Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Max Vladymyrov, Razvan Pascanu, et al. Uncovering mesa-optimization algorithms in transformers. arXiv preprint arXiv:2309.05858, 2023. 17   \nLean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. Label words are anchors: An information fow perspective for understanding in-context learning. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023. 17   \nMingze Wang and Chao Ma. Understanding multi-phase optimization dynamics and rich nonlinear behaviors of relu networks. Advances in Neural information Processing Systems, 2023. 10   \nSinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. 2, 17 Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: a case study on approximating turing machines with transformers. Advances in Neural Information Processing Systems, 35:12071-12083, 2022a. 16 Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022b. 17 Gail Weiss, Yoav Goldberg, and Eran Yahav. Thinking like transformers. In International Conference on Machine Learning, pages 11080-11090. PMLR, 2021. 17 BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Francois Yvon, et al. Bloom: A   \n176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.   \n17 Zhi-Qin John Xu, Yaoyu Zhang, Tao Luo, Yanyang Xiao, and Zheng Ma. Frequency principle: Fourier analysis sheds light on deep neural networks. Communications in Computational Physics,   \n2019.10 Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions?  arXiv preprint arXiv:1912.10077,2019. 6, 16 Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:17283-17297, 2020. 17 Zhongwang Zhang, Zhiwei Wang, Junjie Yao, Zhangchen Zhou, Xiaolong Li, Zhi-Qin John Xu, et al. Anchor function: a type of benchmark functions for studying language models. arXiv preprint arXiv:2401.08309, 2024. 17 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "ADetailed Related Works 16 ", "page_idx": 15}, {"type": "text", "text": "B Proof of Section 3 18   \nB.1  Proof of Theorem 3.1 18   \nPro01 01 Secuon 4.2 23   \nC.1 Proof of Theorem 4.1 23   \nC.2 Proof of Proposition 4.2 33   \nC.3 Proof of Proposition 4.3 . 35   \nDProof of Section 4.3 38   \nD.1 Proof of Theorem 4.4 38   \nD.2 Proof of Proposition 4.5 . 48   \nE Proof of Section 5 50   \nE.1 Proof of Theorem 5.1 50   \nF Key Lemmas about Approximation 55   \nF.1 Approximation by the sum of exponential decay 55   \nF.2  Approximation by the sum of polynomial decay 59 ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "G Some Background and Proof Preparation 64 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "G.1 T5's relative positional encoding 64   \nG.2 Barron space theory . . 64   \nG.3  Useful approximation lemmas 65   \nH Experiments 66   \nH.1  Restatement of our theoretical insights 66   \nH.2  Experimental Validation 66   \nH.3Practical Implications 69 ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Detailed Related Works ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Theoretical results of Transformer. We first review the expressive power results of Transformer. Yun et al. (2019) first proved the universal approximation property (UAP) of Transformer, highlighting the crucial role of PE in breaking permutation invariance. Edelman et al. (2022) demonstrated that Transformer can approximate fixed sparse functions. Dehghani et al. (2019); P\u00e9rez et al. (2021); Wei et al. (2022a) explored the Turing-completeness of infinite-precision and finite-precision Transformer. Giannou et al. (2023) showed that looped Transformer can implement practical computer programs. ", "page_idx": 15}, {"type": "text", "text": "Jiang and Li (2023) provided explicit approximation rates for Transformer in sequences modeling with inherent graph structures. Liu et al. (2022) found that Transformer can execute finite-state automata. Ma and Ying (2022) asserted the natural suitability of Attn for achieving permutation equivariance. Besides these affirmative results, several studies characterized the expressivity limitation of Transformers, particularly in modeling formal languages or simulating circuits (Hahn, 2020; Weiss et al., 2021; Bhattamishra et al., 2020; Merrill and Sabharwal, 2023b; Merrill et al., 2022). Additionally Feng et al. (2023); Merrill and Sabharwal (2023a) examined the expressivity of Transformer using Chain of Thought prompting (Wei et al., 2022b). Moreover, some studies showed that the in-context learning ability of Transformer is attainable by simulating gradient-based iterations across various layers (Garg et al., 2022; Akyirek et al., 2022; von Oswald et al., 2023; Von Oswald et al., 2023; Mahankali et al., 2023; Bai et al., 2023; Shen et al., 2023). Besides, experimental studies also provide insights into the mechanisms of Transformer through induction head (Elhage et al., 2021; Olsson et al., 2022), information flow (Wang et al., 2023), anchor functions (Zhang et al., 2024), etc. ", "page_idx": 16}, {"type": "text", "text": "Positional encoding. One core component of Transformer is the PE, which facilitates the representation of input sequence order. Theoretically, Transformer without PE lacks UAP and is restricted to representing permutation-invariant functions. PE was first introduced in Vaswani et al. (2017). It has limitations in encoding unseen positions. To overcome this difficulty, Shaw et al. (2018) introduced RPE. Subsequent studies proposed various different RPE types. Notable examples include T5's RPE (Raffel et al., 2020), Rotary RPE (Su et al., 2024) (utilized in PaLM (Chowdhery et al., 2023) and LlaMA (Touvron et al., 2023), Alibi RPE (Press et al., 2022) (employed in BLOOM (Workshop et al., 2022)), and KERPLE (Chi et al., 2022). A prevailing belief is that RPEs can outperform APEs in the \"length generalization task\" (Ontanon et al., 2022; Csordas et al., 2021)- the ability to generalize from smaller training contexts to larger ones, a critical challenge for Large Language Models (Anil et al., 2022; Abbe et al., 2023b). However, Press et al. (2022) revealed that the commonly used Rotary RPE may exhibit suboptimal performance in this task. The recent work (Kazemnejad et al., 2023; Chi et al., 2022) conducted systematic experiments comparing the length generalization capabilities of Transformers with various RPEs and APEs, suggesting that the RPEs used in T5 and KERPLE(log) demonstrate superior performance over other types. ", "page_idx": 16}, {"type": "text", "text": "Rethinking dot-product. Another critical component of Transformer is the DP structure. Due to its quadratic cost as a function of the sequence length, the necessity of DP has always been questioned. Numerous variants of DP have been proposed, demonstrating competitive performance across diverse tasks. Representative examples include Longformer (Beltagy et al., 2020), Big Bird (Zaheer et al., 2020), Reformer (Kitaev et al., 2020), Linformer (Wang et al., 2020), Performer (Choromanski et al., 2020), Synthesizer (Tay et al., 2021), etc. In particular, a recent study (Alen-Zhu and Li, 2023) compared standard and DP-free Transformers in modeling \"context-free grammar\". Their findings suggested that the presence of DP has a marginal impact on performance. These evidences motivate us to rethink thenecessity of DP in Transformer. ", "page_idx": 16}, {"type": "text", "text": "Sparsity (Donoho, 2006; Candes and Wakin, 2008) has gained considerable attention in sequence modeling. In classical signal processing, there is a prevailing notion that valuable signals are extremely sparse. For example, when representing an image, one often finds that only a few wavelet coefficients hold significant values in wavelet space (Meyer, 1992). In NLP, the starting point off the traditional $n$ -gram model (Shannon, 1948) is that the next token only relies on a few previous tokens. Such models, however, overlook long-range information, often resulting in suboptimal performance. For NLP tasks such as dependency parsing (Nivre and Scholz, 2004), sentiment analysis (Nasukawa and Yi, 2003), part-of-speech tagging (Francis and Kucera, 1979), and continuation writing (Brown et al., 2020; OpenAI, 2023), it is indeed often the case that only a limited subset of preceding information is crucial for accurate prediction. However, these relevant information can be quite distant. For instance, the resolution of a mystery novel may hinge on elements introduced at the outset. Moreover, for Transformer networks, extensive research into the visualization and interpretability has revealed that (i) the learned activation maps of FFN layers are extremely sparse (Li et al., 2023); (ii) the learned self-attention matrices exhibit notable sparsity, yet it does not closely resemble a diagonal configuration (Elhage et al., 2021). These observations suggest that the prediction of the next token is influenced by a small number of previous tokens which might be far away. Therefore, being able to represent sparse but long-range dependence is important for sequence modeling. ", "page_idx": 16}, {"type": "text", "text": "B Proof of Section 3 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this subsection, we give the detailed proofs of fixed, long but sparse memory: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\pmb{{y}}_{t}=\\pmb{{f}}(\\pmb{{x}}_{t},\\pmb{{x}}_{t-T_{1}},\\cdot\\cdot\\cdot\\cdot\\mid,\\pmb{{x}}_{t-T_{M}}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $1\\leq T_{1}<\\cdot\\cdot\\cdot<T_{M}<+\\infty$ signify the fixed positions of the memories. ", "page_idx": 17}, {"type": "text", "text": "Theorem B.1 (Restatement of Theorem 3.1). For any target $\\mathbf{H}\\in\\mathcal{H}^{\\mathrm{Fix}}$ (6), rate $n\\,\\in\\,\\mathbb{N}_{+}$ ,and $H,m\\in\\mathbb{N}_{+}$ , there exists a 1-layer Transformer $\\mathbf{TF}\\in{\\mathcal{T F}}_{(1,H,m)}^{\\mathrm{DPF,type}}$ (7)anda constant $C(n)$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\lVert\\mathbf{H}-\\mathbf{T}\\mathbf{F}\\rVert\\leq\\mathcal{E}_{\\mathrm{FFN}}+\\lVert f\\rVert_{\\mathrm{Lip}}\\,\\mathcal{E}_{\\mathrm{Attn}}(\\mathrm{type}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{E}_{\\mathrm{FFN}}=\\tilde{\\mathcal{O}}\\left(\\frac{\\|f\\|_{B}}{\\sqrt{m}}\\right)}\\end{array}$ and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\mathrm{Attn}}(\\mathrm{type})=\\left\\{\\mathcal{O}\\left(\\frac{C(n)}{H^{n}}\\left(\\sum_{i=1}^{M}e^{0.01T_{i}}\\right)^{n+1}\\right),\\quad\\mathrm{type=lin}\\atop\\mathcal{O}\\left(\\frac{C(n)}{H^{n}}\\left(\\sum_{i=1}^{M}T_{i}^{1.01}\\right)^{n+1}\\right),\\quad\\mathrm{~type=log}\\right..\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Theorem B.1. ", "page_idx": 17}, {"type": "text", "text": "First, we choose the embedding dimension $D\\;=\\;(M+1)d$ , and select the simple embedding $\\begin{array}{r}{W_{E}=(I_{d\\times d},\\mathbf{0})^{\\top}\\in\\mathbb{R}^{D\\times d},\\pmb{b}_{E}^{\\top}=\\mathbf{0}\\in\\mathbb{R}^{D}}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "Then for any input sequence $X=({\\mathbf{\\boldsymbol{x}}}_{t})_{t\\in\\mathbb{Z}}$ , the token after embedding satisfies: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{x}_{t}^{E}=W_{E}\\pmb{x}_{t}+\\pmb{b}_{E}=(\\pmb{x}_{t}^{\\top},\\pmb{0}^{\\top})^{\\top}\\in\\mathbb{R}^{D}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For one-layer Dot-product-fee Tranfor $\\mathbf{TF}\\;\\in\\;\\mathcal{T F}_{(1,H,m)}^{\\mathrm{DPF,type}}$ with $\\phi_{\\tt t y p e}$ , the output token $\\mathbf{TF}_{t}(X)$ of $t$ -th input token $\\pmb{x}_{t}$ satisfies: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{x}_{t}^{(1/2)}=\\pmb{x}_{t}^{(0)}+\\pmb{W}_{O}^{(1)}\\sum_{h=1}^{H}\\mathbf{A}\\mathbf{t}\\mathbf{n}_{t}^{(1,h)}(\\pmb{X}^{(0)}),}\\\\ &{\\pmb{x}_{t}^{(1)}=\\mathbf{F}\\mathbf{F}\\mathbf{N}^{(1)}(\\pmb{x}_{t}^{(1/2)})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{Attn}_{t}^{(1,h)}(X)=W_{V}^{(1,h)}\\sum_{s=0}^{+\\infty}\\frac{x_{t-s}\\exp\\left(p^{(1,h)}\\phi_{\\mathrm{type}}(s)\\right)}{\\sum_{j=0}^{+\\infty}\\exp\\left(p^{(1,h)}\\phi_{\\mathrm{type}}(j)\\right)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This proof can be summarized as the following process: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\cdots\\quad x_{t}^{E}\\quad\\cdots.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\cdots\\quad x_{t}^{(1)}\\approx f(x_{t},x_{t-T_{1}},\\cdot\\cdot\\cdot\\,,x_{t-T_{M}})\\quad\\cdot\\cdot\\cdot\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now we give the formal proof. ", "page_idx": 17}, {"type": "text", "text": "Step I. Extract the memory locations by (Dot-product-free) Attn layer. ", "page_idx": 17}, {"type": "text", "text": "We consider to use $H_{k}$ attention hads (from $\\textstyle\\sum_{i=1}^{k-1}H_{i}+1$ th head to $\\textstyle\\sum_{i=1}^{k}H_{i}$ th head to exract it, and it satisfies to $\\textstyle\\sum_{k=1}^{M}H_{k}=H$ ", "page_idx": 17}, {"type": "text", "text": "For simplicity, we denote the following projection matrices: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{P^{(k)}:=(\\mathbf{0}_{d\\times k d}}&{{}I_{d\\times d}\\quad\\mathbf{0})\\in\\mathbb{R}^{d\\times D},\\quad1\\leq k\\leq M.}\\\\ {P_{\\perp}^{(k)}:=\\bigg(\\!I_{k d\\times k d}\\!}&{{}\\mathbf{0}_{d\\times d}\\quad\\qquad\\mathbf{0}}\\\\ {\\mathbf{0}\\quad\\mathbf{0}_{d\\times d}\\quad I_{(M-k)d\\times(M-k)d}\\bigg)\\in\\mathbb{R}^{M d\\times D},\\quad1\\leq k\\leq M.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now we consider the extraction of $k$ -th memory $\\mathbf{\\boldsymbol{x}}_{t-T_{k}}$ $(1\\leq k\\leq M)$ ", "page_idx": 18}, {"type": "text", "text": "\u00b7 Case $\\mathtt{t y p e}=\\operatorname*{lin}.$ ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "By Lemma F.1, for any rate $n\\in\\mathbb{N}_{+}$ , there exists an constant $C(n)$ and a function ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\phi_{k}^{\\mathrm{exp}}(t)=\\sum_{\\sum_{i=1}^{k-1}H_{i}+1\\le h\\le\\sum_{i=1}^{k}H_{i}}\\alpha_{h}e^{-\\beta_{h}t}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "such that $\\beta_{h}>0$ and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\mathbb{I}\\{\\cdot=T_{k}\\}-\\phi_{k}^{\\mathrm{exp}}(\\cdot)\\|_{\\ell_{1}(\\mathbb{N})}=\\sum_{s=0}^{+\\infty}|\\mathbb{I}\\{s=T_{k}\\}-\\phi_{k}^{\\mathrm{exp}}(s)|\\leq C(n)\\frac{e^{0.01(n+1)T_{k}}}{H_{k}^{n}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, for these attention heads $\\begin{array}{r}{(\\sum_{i=1}^{k-1}H_{i}+1\\le h\\le\\sum_{i=1}^{k}H_{i})}\\end{array}$ , we can choose ", "page_idx": 18}, {"type": "equation", "text": "$$\np^{(1,h)}=\\beta_{h},\\quad W_{V}^{(1,h)}=\\alpha_{h}\\,\\left(\\sum_{j=0}^{+\\infty}\\exp(-\\beta_{h}j)\\right)\\delta_{(k+1,1)}^{d\\times d},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\delta^{(k+1,1)}\\in\\mathbb{R}^{D\\times D}$ means that: it equals to $\\pmb{I}_{d\\times d}$ for the $(k+1,1)$ -th $d\\times d$ blocks, and ${\\mathbf{0}}_{d\\times d}$ for the other $d\\times d$ blocks. ", "page_idx": 18}, {"type": "text", "text": "Then it holds that: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}\\mathbf{A}\\mathbf{t}\\mathbf{t}\\mathbf{n}_{t}^{(1,h)}(X^{(0)})=\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}\\alpha_{h}\\sum_{s=0}^{+\\infty}e^{-\\beta_{h}s}\\left(\\mathbf{0}_{t-s}\\right)\\in\\mathbb{R}^{D}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This implies: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P^{(k)}\\displaystyle\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}\\mathbf{A}\\mathbf{t}\\mathbf{n}_{t}^{(1,h)}(X^{(0)})=\\displaystyle\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}\\alpha_{h}\\sum_{s=0}^{+\\infty}e^{-\\beta_{h}s}\\pmb{x}_{t-s},}\\\\ &{P_{\\bot}^{(k)}\\displaystyle\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}\\mathbf{A}\\mathbf{t}\\mathbf{n}_{t}^{(1,h)}(X^{(0)})=\\mathbf{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "moreover, the following estimate holds: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|P^{(k)}\\displaystyle\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}\\mathbf{A}\\mathbf{t}\\mathbf{n}_{t}^{(1,h)}(X^{(0)})-x_{t-T_{k}}\\right\\|_{2}}\\\\ &{=\\left\\|\\displaystyle\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}\\alpha_{h}\\displaystyle\\sum_{s=0}^{+\\infty}e^{-\\beta_{h}s}\\pmb{x}_{t-s}-\\pmb{x}_{t-T_{k}}\\right\\|_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\left\\|\\displaystyle\\sum_{s=0}^{+\\infty}\\left(\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}\\alpha_{h}e^{-\\beta_{h}s}-\\mathbb{I}\\{s=T_{k}\\}\\right)x_{t-s}\\right\\|_{2}}\\\\ &{\\le\\displaystyle\\sum_{s=0}^{+\\infty}\\left|\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}\\alpha_{h}e^{-\\beta_{h}s}-\\mathbb{I}\\{s=T_{k}\\}\\right|}\\\\ &{=\\|\\phi_{k}^{\\exp}(\\cdot)-\\mathbb{I}\\{\\cdot=T_{k}\\}\\|_{\\ell_{1}(\\mathbb{N})}\\le C(n)\\frac{e^{0.01(n+1)T_{k}}}{H_{k}^{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "\u00b7 Case $\\mathtt{t y p e}=\\mathtt{l o g}$ ", "page_idx": 19}, {"type": "text", "text": "By Lemma F.4, for any rate $n\\in\\mathbb{N}_{+}$ , there exists an constant $C(n)$ and a function ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\phi_{k}^{\\mathrm{poly}}(t)=\\sum_{\\sum_{i=1}^{k-1}H_{i}+1\\le h\\le\\sum_{i=1}^{k}H_{i}}\\alpha_{h}t^{-\\beta_{h}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "such that $\\beta_{h}>1$ and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{I}\\{\\cdot=T_{k}\\}-\\phi_{k}^{\\mathrm{poly}}(\\cdot)\\right\\|_{\\ell_{1}(\\mathbb{N}_{+})}=\\sum_{s=1}^{+\\infty}\\left|\\mathbb{I}\\{s=T_{k}\\}-\\phi_{k}^{\\mathrm{poly}}(s)\\right|\\le C(n)\\frac{T_{k}^{1,01(n+1)}H_{k}^{n}(s)}{.}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, for these attention heads $\\begin{array}{r}{(\\sum_{i=1}^{k-1}H_{i}+1\\le h\\le\\sum_{i=1}^{k}H_{i})}\\end{array}$ we can choose ", "page_idx": 19}, {"type": "equation", "text": "$$\np^{(1,h)}=\\beta_{h},\\;\\;\\;{\\pmb W}_{V}^{(1,h)}=\\alpha_{h}\\left(\\sum_{j=1}^{+\\infty}j^{-\\beta_{h}}\\right)\\delta^{(k+1,1)},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\delta^{(k+1,1)}\\in\\mathbb{R}^{D\\times D}$ means that: it equals to $\\pmb{I}_{d\\times d}$ for the $(k+1,1)$ -th $d\\times d$ blocks, and ${\\mathbf{0}}_{d\\times d}$ for the other $d\\times d$ blocks. ", "page_idx": 19}, {"type": "text", "text": "Then it holds that: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P^{(k)}\\underset{h=\\sum_{i=1}^{k-1}H_{i}+1}{\\sum_{i=1}^{k}H_{i}}\\mathbf{A}\\mathbf{t}\\mathbf{n}_{t}^{(1,h)}(\\boldsymbol{X}^{(0)})=\\underset{h=\\sum_{i=1}^{k-1}H_{i}+1}{\\sum_{i=1}^{k}H_{i}}\\alpha_{h}\\underset{s=1}{\\sum_{s=1}^{k-1}}s^{-\\beta_{h}}\\pmb{x}_{t-s},}\\\\ &{P_{\\perp}^{(k)}\\underset{h=\\sum_{i=1}^{k-1}H_{i}+1}{\\sum_{i=1}^{k}H_{i}}\\mathbf{A}\\mathbf{t}\\mathbf{n}_{t}^{(1,h)}(\\boldsymbol{X}^{(0)})=\\mathbf{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "moreover, the following estimate holds: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\displaystyle\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}P^{(k)}\\mathbf{A}\\mathbf{H}\\mathbf{t}^{(1,h)}(X^{(0)})-x_{t-T_{k}}\\right\\|_{2}}\\\\ &{=\\left\\|\\displaystyle\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}\\alpha_{h}\\displaystyle\\sum_{s=1}^{+\\infty}s^{-\\beta_{h}}\\mathbf{x}_{t-s}-x_{t-T_{k}}\\right\\|_{2}}\\\\ &{=\\left\\|\\displaystyle\\sum_{s=1}^{+\\infty}\\left(\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}\\alpha_{h}s^{-\\beta_{h}}-\\mathbb{I}\\{s=T_{k}\\}\\right)x_{t-s}\\right\\|_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\displaystyle\\sum_{s=1}^{+\\infty}\\left|\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}\\alpha_{h}s^{-\\beta_{h}}-\\mathbb{I}\\{s=T_{k}\\}\\right|}\\\\ &{=\\displaystyle\\left\\|\\phi_{k}^{\\mathrm{poly}}(\\cdot)-\\mathbb{I}\\{\\cdot=T_{k}\\}\\right\\|_{\\ell_{1}(\\mathbb{N}_{+})}\\leq\\mathcal{O}\\left(C(n)\\frac{T_{k}^{1.01(n+1)}}{H_{k}^{n}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then we combine the results for all $k\\in[M]$ for these two cases. By choose $W_{O}=I_{D}$ , we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left|\\frac{\\mathbf{x}_{t}^{(1)}}{\\mathbf{x}_{t}^{(1)}}-\\left(\\frac{\\mathbf{x}_{t+1}^{(1)}}{x_{t+1}^{(1)}}\\right)\\right|_{2}\\right.}\\\\ &{=\\left|\\left|\\frac{\\mathbf{x}_{t}^{(1)}}{\\mathbf{x}_{t}^{(1)}}\\right|+\\frac{\\mathbf{x}_{t}^{(1)}}{\\mathbf{x}_{t+1}^{(1)}}\\mathrm{Ant}^{(1,\\lambda)}(X)-\\left(\\frac{\\mathbf{x}_{t+1}^{(1)}}{\\mathbf{x}_{t+1}^{(1)}}\\right)\\right|_{2}-\\left|\\frac{\\mathbf{x}_{t}^{(1)}}{\\mathbf{x}_{t}^{(1)}}\\mathrm{Ant}^{(1,\\lambda)}(X)-\\left(\\frac{\\mathbf{x}_{t+1}^{(1)}}{\\mathbf{x}_{t+1}^{(1)}}\\right)\\right|_{2}\\right|}\\\\ &{=\\left|\\left|\\frac{\\mathbf{x}_{t}^{(1)}}{\\mathbf{x}_{t}^{(1)}}\\right|\\left(\\frac{\\mathbf{x}_{t}^{(1)}}{\\mathbf{x}_{t+1}^{(1)}}\\mathrm{Ant}^{(1,\\lambda)}(X)-\\left(\\frac{\\mathbf{x}_{t+1}^{(1)}}{\\mathbf{x}_{t+1}^{(1)}}\\right)\\right|_{2}\\right|}\\\\ &{\\leq\\frac{1}{\\mathbf{x}_{t}^{(1)}}\\left|\\left|\\frac{\\mathbf{x}_{t+1}^{(1)}}{\\mathbf{x}_{t}\\geq\\overline{{x}}_{t+1}^{(1)}}\\mathrm{Ant}^{(1,\\lambda)}(X)-\\left(\\frac{\\mathbf{x}_{t+1}^{(1)}}{\\mathbf{x}_{t}^{(1)}}\\right)\\right|_{2}}\\\\ &{\\qquad\\times\\left|\\frac{1}{\\mathbf{x}_{t+1}^{(1)}}\\right|\\left|\\frac{\\mathbf{x}_{t+1}^{(1)}}{\\mathbf{x}_{t}\\geq\\overline{{x}}_{t+1}^{(1)}}\\mathrm{Ant}^{(1,\\lambda)}(X)-\\mathbf{x}_{t-1}\\right|_{2}\\right|}\\\\ &{\\qquad\\times\\frac{1}{\\mathbf{x}_{t}^{(1)}}\\left|\\left|\\frac\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Consequently, one detail is to assign the head number $\\{H_{k}\\}_{k=1}^{M}$ such that the error'sum $\\mathcal{E}_{\\mathrm{Attn}}(\\tt t y p e)$ is as small as possible. Our way is solving the minimization problem: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{min}_{H_{1},\\cdots,H_{M}}:\\mathcal{E}_{\\mathrm{Attn}}(\\boldsymbol{\\mathbf{t}}\\boldsymbol{\\mathbf{y}}\\boldsymbol{\\mathbf{p}}\\mathbf{e})}&{}\\\\ {\\displaystyle\\mathrm{s.t.}\\,\\sum_{k=1}^{M}H_{k}=H,}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which suggests that we should choose the head number: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{H_{k}=\\frac{e^{0.01T_{k}}}{\\sum_{j=1}^{M}e^{0.01T_{j}}}H,\\quad k\\in[M],\\quad\\mathrm{type}=\\mathrm{lin};}&\\\\ &{H_{k}=\\frac{T_{k}^{1.01}}{\\sum_{j=1}^{M}T_{j}^{1.01}}H,\\quad k\\in[M],\\quad\\mathrm{type}=\\mathrm{log}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, we obtain the bound in Step I: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\mathrm{Attn}}(\\mathrm{type})\\leq\\left\\{\\begin{array}{l l}{\\displaystyle\\frac{C(n)}{H^{n}}\\left(\\sum_{k=1}^{M}e^{0.01T_{k}}\\right)^{n+1}\\mathrm{,~type=lin}}\\\\ {\\displaystyle\\frac{C(n)}{H^{n}}\\left(\\sum_{k=1}^{M}T_{k}^{1.01}\\right)^{n+1}\\mathrm{,~type=log}}\\end{array}\\right.\\mathrm{~.}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Furthermore, by choosing $\\mathcal{E}_{\\mathrm{Attn}}(\\sf t y p e)\\le1$ , it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|x_{t}^{(1/2)}\\right\\|_{\\infty}\\leq\\left\\|x_{t}^{(1/2)}-\\left({x_{t-t_{1}}\\atop\\mathscr{x}_{t-t_{M}}}\\right)\\right\\|_{\\infty}+\\left\\|\\binom{x_{t}}{\\mathscr{x}_{t-t_{M}}}\\right\\|_{\\infty}\\leq\\mathcal{E}_{\\mathrm{Attn}}(\\mathrm{type})+1\\leq2.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Step II. Approximate the readout function by FFN layer. ", "page_idx": 21}, {"type": "text", "text": "In this step, we aim to approximate the function $f$ using two-layer network. By Lemma G.6, there exists a two layer neural network with $m$ neurons defined on $\\mathbb{R}^{\\check{D}}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{FFN}^{(1)}(\\pmb{y})=\\sum_{k=1}^{m}a_{k}\\sigma(\\pmb{b}_{k}^{\\top}\\pmb{y}+c_{k})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal E_{\\mathrm{FFN}}:=\\left\\|\\mathrm{FFN}^{(1)}-f\\right\\|_{L^{\\infty}([-2,2]^{D})}\\leq\\tilde{\\mathcal O}\\left(\\frac{\\|f\\|_{\\mathcal B}}{\\sqrt{m}}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The final bound. ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For any $t$ and $X\\in\\mathcal{X}$ , it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~\\left\\Vert\\mathbf{H}_{t}(X)-x_{t}^{(1)}\\right\\Vert=\\left|f\\left(x_{t},x_{t-t_{1}},\\cdots x_{t-t_{M}}\\right)-\\mathrm{FFN}^{(1)}\\left(x_{t}^{(1/2)}\\right)\\right|}\\\\ &{=\\left|f\\left(x_{t},x_{t-t_{1}},\\cdots x_{t-t_{M}}\\right)-f\\left(x_{t}^{(1/2)}\\right)+f\\left(x_{t}^{(1/2)}\\right)-\\mathrm{FFN}^{(1)}\\left(x_{t}^{(1/2)}\\right)\\right|}\\\\ &{\\leq\\left|f\\left(x_{t},x_{t-t_{1}},\\cdots x_{t-t_{M}}\\right)-f\\left(x_{t}^{(1/2)}\\right)\\right|+\\left|f\\left(x_{t}^{(1/2)}\\right)-\\mathrm{FFN}^{(1)}\\left(x_{t}^{(1/2)}\\right)\\right|}\\\\ &{\\leq\\left\\Vert f\\right\\Vert_{\\mathrm{Lip}}\\left\\Vert\\left(x_{t}^{\\top},x_{t-t_{1}}^{\\top},\\cdots x_{t-t_{M}}^{\\top}\\right)-x_{t}^{(1/2)}\\right\\Vert_{2}+\\left\\Vert f-\\mathrm{FFN}^{(1)}\\right\\Vert_{L^{\\infty}\\left([-2,2]^{D}\\right)}}\\\\ &{\\leq\\left\\Vert f\\right\\Vert_{\\mathrm{Lip}}\\cdot\\mathcal{E}_{\\mathrm{Attn}}(\\mathrm{type})+\\mathcal{E}_{\\mathrm{FFN}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\mathrm{FFN}}=\\tilde{\\mathcal{O}}\\left(\\frac{\\|f\\|_{B}}{\\sqrt{m}}\\right);\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\mathrm{Attn}}(\\mathrm{type})=\\left\\{\\mathcal{O}\\left(\\frac{C(n)}{H^{n}}\\Big(\\sum_{k=1}^{M}e^{0.01T_{k}}\\Big)^{n+1}\\right),\\quad\\mathrm{type=lin}\\right.}\\\\ {\\left.\\mathcal{O}\\left(\\frac{C(n)}{H^{n}}\\Big(\\sum_{k=1}^{M}T_{k}^{1.01}\\Big)^{n+1}\\right),\\quad\\mathrm{~type=log~}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Due to the arbitrariness of $t$ and $\\mathbf{\\deltaX}$ , the proof is completed. ", "page_idx": 21}, {"type": "text", "text": "C Proof of Section 4.2 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we give the detailed proofs of the approximation theory of Transformer for modeling the warm-up case of adaptive, long but sparse memory: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\pmb{{y}}_{t}=\\pmb{{f}}(\\pmb{{x}}_{t},\\pmb{{x}}_{t-t_{1}},\\cdot\\cdot\\cdot\\textit{,}\\pmb{{x}}_{t-t_{M}}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the adaptive memory satisfies to: ", "page_idx": 22}, {"type": "equation", "text": "$$\nt_{k}=g_{k}(\\pmb{x}_{t}),\\quad k\\in[M].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Moreover, $g_{k}(\\cdot)$ generate positive integers for the input tokens, and there exist maximum values $T_{k}$ such that $1\\leq g_{k}({\\mathbf{\\boldsymbol{x}}}_{t})\\leq T_{k}$ holds for any $\\pmb{x}_{t}$ and $k\\in[M]$ ", "page_idx": 22}, {"type": "text", "text": "To tackle the discrete values of the time and the memory values $g_{k}(\\pmb{x}_{t})$ , a modified version of standard FFN, termed \u201cFFN with precision\", us cibsudered. This approach ensures that the output of FFN undergoes a simple rounding operation. Notably, the precision technique is widely used in LLM training (Kalamkar et al., 2019), such as BFloat16. Specifically, for Transformer using RPE with type, we use the following FFN with precision: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\widetilde{\\mathrm{FFN}}(\\pmb{x}):=[\\mathrm{FFN}(\\pmb{x})],}&{\\mathrm{type}=\\operatorname*{lin};}\\\\ &{\\widetilde{\\mathrm{FFN}}(\\pmb{x}):=\\log\\left[\\exp\\left(\\mathrm{FFN}(\\pmb{x})\\right)\\right],}&{\\mathrm{type}=\\log,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $[\\cdot]$ signifies rounding to the nearest integer, i.e., $[x]=\\arg\\operatorname*{min}_{n\\in\\mathbb{Z}}\\left|n-x\\right|(x\\in\\mathbb{R}).$ ", "page_idx": 22}, {"type": "text", "text": "It is important to note that the rounding obtained by using the operator $\\log[\\exp(z)]$ , used in (15), is quite fine, which is much finer than the vanilla rounding obtained by $[z]$ . To elaborate, the following proposition is presented: ", "page_idx": 22}, {"type": "text", "text": "Proposition C.1. For any $z\\geq1$ , the following holds: ", "page_idx": 22}, {"type": "equation", "text": "$$\n(i)\\mid\\log[\\exp(z)]-z\\mid\\leq\\frac{1}{2\\operatorname*{min}\\{e^{z},[e^{z}]\\}};\\quad(i i)\\mid[z]-z\\mid\\leq\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "C.1 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Theorem C.2 (Restatement of Theorem 4.1). For any target $\\mathbf{H}\\in\\mathcal{H}_{(1,M)}^{\\mathrm{Adap}}$ (8), rate $n\\in\\mathbb{N}_{+}$ and $H,m\\in\\mathbb{N}_{+}$ , there exists a wo-layerTransformer $\\mathbf{TF}\\in\\mathcal{T F}_{(2,H,m)}^{\\mathrm{NF,type}}$ (12) and a constant $C(n)$ such that: if the width satisfies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{m\\geq\\left\\{\\tilde{\\Omega}\\Big(\\sum_{i=1}^{M}\\left\\|g_{i}\\right\\|_{\\mathcal B}^{2}\\Big),\\right.\\quad\\quad\\quad\\quad\\mathrm{type=lin}}\\\\ {\\tilde{\\Omega}\\Big(\\sum_{i=1}^{M}\\left\\|\\log g_{i}\\right\\|_{\\mathcal B}^{2}T_{i}^{2}\\Big),\\quad\\mathrm{type=log}}\\end{array},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "then the following approximation rate holds: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\lVert\\mathbf{H}-\\mathbf{T}\\mathbf{F}\\rVert\\leq\\mathcal{E}_{\\mathrm{FFN}}+\\lVert f\\rVert_{\\mathrm{Lip}}\\,\\mathcal{E}_{\\mathrm{Attn}}(\\mathrm{type}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "$\\begin{array}{r}{\\mathcal{E}_{\\mathrm{FFN}}=\\tilde{\\mathcal{O}}\\left(\\frac{\\|f\\|_{B}}{\\sqrt{m}}\\right)}\\end{array}$ and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\mathrm{Attn}}(\\mathrm{type})=\\left\\{\\mathcal{O}\\left(\\frac{C(n)}{H^{n}}\\left(\\sum_{i=1}^{M}e^{0.01T_{i}}\\right)^{n+1}\\right),\\quad\\mathrm{type=lin}\\atop\\mathcal{O}\\left(\\frac{C(n)}{H^{n}}\\left(\\sum_{i=1}^{M}T_{i}^{1.01}\\right)^{n+1}\\right),\\quad\\mathrm{~type=log}\\right..\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof of Theorem C.2. ", "page_idx": 22}, {"type": "text", "text": "First, we choose the embedding dimension $D=(M+1)(d+1)$ , and select a simple embedding $\\begin{array}{r}{W_{E}=(I_{d\\times d},\\mathbf{0})^{\\top}\\in\\mathbb{R}^{D\\times d},b_{E}^{\\top}=\\mathbf{0}\\in\\mathbb{R}^{D}}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "Then for any input sequence $X=({\\mathbf{\\boldsymbol{x}}}_{t})_{t\\in\\mathbb{Z}}$ , the token after embedding satisfies: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(0)}=W_{E}\\pmb{x}_{t}+\\pmb{b}_{E}=(\\pmb{x}_{t}^{\\top},\\pmb{0}^{\\top})^{\\top}\\in\\mathbb{R}^{D}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To tackle the discrete values of $g_{m}(\\pmb{x}_{t})$ , we utilize FFN, FFN with precision (15). It ensures that the output of FFN undergoes a simple rounding operation. ", "page_idx": 23}, {"type": "text", "text": "Thsfolaa $\\mathbf{TF}\\in\\mathcal{T F}_{(2,H,m)}^{\\mathrm{NF,type}}$ with $\\phi_{\\tt t y p e}$ $\\pmb{x}_{t}^{(2)}$ of $t$ -th input token satisfies: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{t}^{(1/2)}=x_{t}^{(0)}+W_{O}^{(1)}\\displaystyle\\sum_{h=1}^{H}\\mathbf{A}\\mathbf{ttn}_{t}^{(1,h)}(\\boldsymbol{X}^{(0)}),}\\\\ &{\\quad x_{t}^{(1)}=x_{t}^{(1/2)}+\\widehat{\\mathbf{F}\\mathbf{F}\\mathbf{N}}^{(1)}(x_{t}^{(1/2)}),}\\\\ &{x_{t}^{(3/2)}=x_{t}^{(1)}+W_{O}^{(2)}\\displaystyle\\sum_{h=1}^{H}\\mathbf{A}\\mathbf{ttn}_{t}^{(2,h)}(\\boldsymbol{X}^{(1)}),}\\\\ &{\\quad x_{t}^{(2)}=\\mathbf{F}\\mathbf{F}\\mathbf{N}^{(2)}(x_{t}^{(3/2)}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{A}\\mathbf{t}\\mathbf{t}\\mathbf{n}_{t}^{(l,h)}(\\boldsymbol{X})=W_{V}^{(l,h)}\\sum_{s=0}^{+\\infty}\\pmb{x}_{t-s}\\exp\\left(\\left\\langle W_{Q}^{(l,h)}\\pmb{x}_{t},W_{K}^{(l,h)}\\pmb{x}_{t-s}\\right\\rangle+p^{(l,h)}\\phi_{\\mathrm{type}}(s)\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This proof can be summarized as the following process: ", "page_idx": 23}, {"type": "text", "text": "\u00b7 Case type = lin. Step $\\begin{array}{r l}&{x_{t}^{(0)}}\\\\ &{\\textrm{L.s t A t l n}\\;\\downarrow}\\\\ &{x_{t}^{(1/2)}=x_{t}^{(0)}}\\\\ &{\\textrm{L.l.s t F P}\\downarrow}\\\\ &{x_{t}^{(1)}=(x_{t}^{\\top},0^{\\top},g_{1}(x_{t}),\\cdots,g_{M}(x_{t}),1)^{\\top}}\\\\ &{\\textrm{L.o s t A t l n}\\;\\downarrow}\\\\ &{x_{t}^{(3/2)}\\approx(x_{t}^{\\top},x_{t-g_{1}(x_{t})}^{\\top},\\cdots,x_{t-g_{M}(x_{t})}^{\\top},g_{1}(x_{t}),\\cdots,g_{M}(x_{t}),1)^{\\top}}\\\\ &{\\textrm{V.o s t F P}\\downarrow}\\\\ &{x_{t}^{(2)}\\approx f(x_{t},x_{t-g_{1}(x_{t})},\\cdots,x_{t-g_{M}(x_{t})})}\\end{array}$ Step1 Step Il Step I ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{t}^{(0)}}\\\\ &{\\;\\;\\mathrm{Att~in:}}\\\\ &{x_{t}^{(1/2)}=x_{t}^{(0)}}\\\\ &{\\mathrm{tFFN~}\\downarrow}\\\\ &{\\quad x_{t}^{(1)}=(x_{t}^{\\top},0^{\\top},\\log g_{1}(x_{t}),\\cdots,\\log g_{M}(x_{t}),\\log2)^{\\top}}\\\\ &{\\;\\;\\mathrm{tAtm~}\\;\\downarrow}\\\\ &{x_{t}^{(3/2)}\\approx(x_{t}^{\\top},x_{t-g_{1}(x_{t})}^{\\top},\\cdots,x_{t-g_{M}(x_{t})}^{\\top},\\log g_{1}(x_{t}),\\cdots,\\log g_{M}(x_{t}),\\log2)^{\\top}}\\\\ &{\\mathrm{tFFN~}\\downarrow}\\\\ &{x_{t}^{(2)}\\approx f(x_{t},x_{t-g_{1}(x_{t})},\\cdots,x_{t-g_{M}(x_{t})})}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now we give the formal proof. ", "page_idx": 23}, {"type": "text", "text": "Step I. Identity map. ", "page_idx": 23}, {"type": "text", "text": "For the frs Atn layer, we only need to do the identy mapby taking $W_{0}^{(1)}=\\mathbf{0}$ Then $\\pmb{x}_{t}^{(1/2)}=\\pmb{x}_{t}^{(0)}$ Step II. Approximate the adaptive memory function by the first FFN layer. ", "page_idx": 23}, {"type": "text", "text": "\u00b7 Case type = lin. Our main idea is that using the first FFN layer to express $(\\pmb{x}_{t}^{\\top},\\pmb{0}^{\\top},\\bar{g}_{1}(\\pmb{x}_{t}),\\cdot\\cdot\\cdot\\mathrm{~,~}g_{M}(\\pmb{x}_{t}),1)^{\\top}$ exactly. First, we consider to approximate the $r$ -th memory function $g_{r}(x)$ by standard FFN. For any $r\\in[M]$ , by Lemma G.6, there exists a two-layer neural network with $m_{r}$ neurons ", "page_idx": 24}, {"type": "equation", "text": "$$\nf_{(1,r)}^{\\mathrm{2NN}}(\\pmb{x})=\\sum_{k=1}^{m_{r}}a_{k}^{(1,r)}\\sigma\\left(\\pmb{b}_{k}^{(1,r)}\\ ^{\\top}\\pmb{x}+c_{k}^{(1,r)}\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "defined on $\\mathbb{R}^{d}$ such that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|g_{r}-f_{(1,r)}^{\\mathrm{2NN}}\\right\\|_{L^{\\infty}([-1,1]^{D})}\\leq\\tilde{\\mathcal{O}}\\left(\\frac{\\|g_{r}\\|_{B}}{\\sqrt{m_{r}}}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, if we choose ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{O}}\\left(\\frac{\\|g_{r}\\|_{\\mathcal{B}}}{\\sqrt{m_{r}}}\\right)<\\frac{1}{2},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "the following holds: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left|g_{r}(\\pmb{x}_{t})-f_{(1,r)}^{\\mathrm{2NN}}(\\pmb{x}_{t})\\right|\\leq\\left\\|g_{r}-f_{(1,r)}^{\\mathrm{2NN}}\\right\\|_{L^{\\infty}([-1,1]^{d})}<\\frac{1}{2},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Noticing $g_{r}(\\mathbf{x}_{t})\\in\\mathbb{N}_{+}$ , we have $\\left[f_{(1,r)}^{2\\mathrm{NN}}({\\pmb x}_{t})\\right]=g_{r}({\\pmb x}_{t})$ , which implies: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\widetilde{f_{(1,r)}^{\\mathrm{2NN}}}({\\pmb x}_{t})=\\Big[f_{(1,r)}^{\\mathrm{2NN}}({\\pmb x}_{t})\\Big]=g_{r}({\\pmb x}_{t}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Consequently, in order to construct the form $(\\mathbf{0}^{\\top},g_{1}(\\pmb{x}_{t}),\\cdot\\cdot\\cdot\\mathbf{\\nabla},g_{M}(\\pmb{x}_{t}),1)^{\\top}\\in\\mathbb{R}^{D}$ , we need to arrange the parameters a,\\*), blt,- and $c_{k}^{(1,r)}\\left(k\\in[m_{r}],r\\in[M]\\right)$ appropraely. Denote $\\bar{b}_{k}^{(1,r)}\\,=\\,({\\pmb{b}}_{k}^{(1,r)}\\,\\top,{\\bf0}^{\\top})^{\\top}\\,\\in\\,\\mathbb{R}^{D}$ for $k\\,\\in\\,[m_{r}],r\\,\\in\\,[M]$ . Consider the fllowing two-layer neural network with $1+\\textstyle\\sum_{r=1}^{M}m_{r}$ neurons defned on $\\mathbb{R}^{D}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{FFN}^{(1)}(\\pmb{x})=\\sum_{r=1}^{M}\\sum_{\\substack{1+\\sum_{j=0}^{r-1}m_{j}\\leq k\\leq\\sum_{j=0}^{r}m_{j}}}e_{D-M+r-1}a_{k}^{(1,r)}\\sigma\\left(\\bar{b}_{k}^{(1,r)\\,\\top}\\pmb{x}+c_{k}^{(1,r)}\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "It is easy to verify that for any (1/2),it holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad{\\bf F}\\mathbf{F}\\mathbf{N}^{(1)}({\\pmb x}_{t}^{(1/2)})=\\mathbf{F}{\\bf F}{\\bf N}^{(1)}({\\pmb x}_{t}^{(0)})}\\\\ &{=\\displaystyle\\sum_{r=1}^{M}\\sum_{1+\\sum_{j=0}^{r-1}m_{j}\\leq k\\leq\\sum_{j=0}^{r}m_{j}}e_{D-M+r-1}a_{k}^{(1,r)}\\sigma\\left(\\bar{b}_{k}^{(1,r)}\\tau_{{\\pmb x}_{t}}^{(0)}+c_{k}^{(1,r)}\\right)+e_{D}\\cdot1\\cdot\\sigma(0+1)}\\\\ &{=\\displaystyle\\sum_{r=1}^{M}\\sum_{1+\\sum_{j=0}^{r-1}m_{j}\\leq k\\leq\\sum_{j=0}^{r}m_{j}}e_{D-M+r-1}a_{k}^{(1,r)}\\sigma\\left(b_{k}^{(1,r)}\\tau_{{\\pmb x}_{t}}+c_{k}^{(1,r)}\\right)+e_{D}\\cdot1\\cdot\\sigma(0+1)}\\\\ &{=\\displaystyle\\sum_{r=1}^{M}e_{D-M+r-1}f_{r}^{2\\mathrm{NN}}({\\pmb x}_{t})+e_{D}}\\\\ &{=\\displaystyle\\sum_{r=1}^{M}e_{D-M+r-1}f_{r}^{2\\mathrm{NN}}({\\pmb x}_{t})+e_{D}}\\\\ &{=(0,f_{0}^{2\\mathrm{NN}}({\\pmb x}_{t}),\\cdots,f_{0,M}^{2\\mathrm{NN}}({\\pmb x}_{t}),1)^{\\top}\\in\\mathbb{R}^{D}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Moreover, it satisfies that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf{F}\\mathbf{F}\\mathbf{N}}^{(1)}(\\mathbf{\\boldsymbol{x}}_{t}^{(1/2)})=\\Big[\\mathbf{F}\\mathbf{F}\\mathbf{N}^{(1)}(\\mathbf{\\boldsymbol{x}}_{t}^{(1/2)})\\Big]\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\!(\\mathbf{0}_{d}^{\\top},\\left[f_{(1,1)}^{\\mathrm{2NN}}(\\pmb{x}_{t})\\right],\\cdot\\cdot\\cdot\\cdot,\\left[f_{(1,M)}^{\\mathrm{2NN}}(\\pmb{x}_{t})\\right],1)^{\\top}}\\\\ &{=\\!(\\mathbf{0}_{d}^{\\top},g_{1}(\\pmb{x}_{t}),\\cdot\\cdot\\cdot\\cdot,g_{M}(\\pmb{x}_{t}),1)^{\\top}\\in\\mathbb{R}^{D}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus, we have achieved our goal in this step: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t}^{(1)}=\\pmb{x}_{t}^{(1/2)}+\\widetilde{\\mathbf{FFN}}^{(1)}(\\pmb{x}_{t}^{(1/2)})=(\\pmb{x}_{t}^{\\top},\\pmb{0}^{\\top},g_{1}(\\pmb{x}_{t}),\\cdot\\cdot\\cdot\\cdot,g_{M}(\\pmb{x}_{t}),1)^{\\top}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "\u00b7Case $\\sf t y p e~{\\Delta~=~}~\\mathrm{lc}$ og.Our main idea is that using the first FFN layer to express $({\\pmb x}_{t}^{\\top},{\\pmb0}^{\\top},\\operatorname{log}g_{1}({\\pmb x}_{t}),\\bar{\\cdot}\\cdot\\cdot\\cdot,\\log g_{M}({\\pmb x}_{t}),\\log2)^{\\top}$ exactly. First, we consider to approximate the $r$ -th memory function $\\log g_{r}(x)$ by standard FFN. For any $r\\in[M]$ , by Lemma G.6, there exists a two-layer neural network with $m_{r}$ neurons ", "page_idx": 25}, {"type": "equation", "text": "$$\nf_{(1,r)}^{\\mathrm{2NN}}(\\pmb{x})=\\sum_{k=1}^{m_{r}}a_{k}^{(1,r)}\\sigma\\left(\\pmb{b}_{k}^{(1,r)}\\ ^{\\top}\\pmb{x}+c_{k}^{(1,r)}\\right)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "defined on $\\mathbb{R}^{d}$ such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\|\\log g_{r}-f_{(1,r)}^{\\mathrm{2NN}}\\right\\|_{L^{\\infty}([-1,1]^{D})}\\leq\\tilde{\\mathcal{O}}\\left(\\frac{\\|\\log g_{r}\\|_{\\mathcal{B}}}{\\sqrt{m_{r}}}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore, if we choose ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{O}}\\left(\\frac{\\lVert\\log g_{r}\\rVert_{B}}{\\sqrt{m_{r}}}\\right)<\\frac{1}{4T_{r}},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "the following holds: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|\\log g_{r}(\\pmb{x}_{t})-f_{(1,r)}^{\\mathrm{2NN}}(\\pmb{x}_{t})\\right|\\leq\\left\\|g_{r}-f_{(1,r)}^{\\mathrm{2NN}}\\right\\|_{L^{\\infty}([-1,1]^{d})}<\\frac{1}{4T_{r}},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which ensures ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\exp\\left(f_{(1,r)}^{\\mathrm{2NN}}(x_{t})\\right)-g_{r}(x_{t})\\right|=\\left|\\exp\\left(f_{(1,r)}^{\\mathrm{2NN}}(x_{t})\\right)-\\exp\\left(\\log\\left(g_{r}(x_{t})\\right)\\right)\\right|}\\\\ &{\\le\\exp\\left(\\operatorname*{max}\\left\\{f_{(1,r)}^{\\mathrm{2NN}}(x_{t}),\\log\\left(g_{r}(x_{t})\\right)\\right\\}\\right)\\left|f_{(1,r)}^{\\mathrm{2NN}}(x_{t})-\\log\\left(g_{r}(x_{t})\\right)\\right|}\\\\ &{\\le\\exp\\left(\\log g_{r}(x_{t})+\\frac{1}{4}\\right)\\frac{1}{4T_{r}}}\\\\ &{\\le\\!e^{1/4}\\cdot T_{r}\\cdot\\frac{1}{4T_{r}}<\\frac{1}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Noticing $g_{r}(\\mathbf{x}_{t})\\in\\mathbb{N}_{+}$ , we have $\\left[\\exp\\left(f_{(1,r)}^{\\mathrm{2NN}}(\\pmb{x}_{t})\\right)\\right]=g_{r}(\\pmb{x}_{t})$ which implies: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widetilde{f_{(1,r)}^{\\mathrm{2NN}}}(\\pmb{x}_{t})=\\log\\left[\\exp\\left(f_{(1,r)}^{\\mathrm{2NN}}\\right)\\right]=\\log g_{r}(\\pmb{x}_{t}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Consequently, in order to construct the form $(\\mathbf{0}^{\\top},\\log g_{1}(\\mathbf{x}_{t}),\\cdots,\\log g_{M}(\\mathbf{x}_{t}),\\log2)^{\\top}$ we need to arrange the parameters $a_{k}^{(1,r)},b_{k}^{(1,r)}$ , and $c_{k}^{(1,r)}\\left(k\\in[m_{r}],r\\in[M]\\right)$ appropriately. Denote $\\bar{\\pmb{b}}_{k}^{(1,r)}\\,=\\,(\\pmb{b}_{k}^{(1,r)^{\\top}},\\pmb{0}^{\\top})^{\\top}\\,\\in\\,\\mathbb{R}^{D}$ for $k\\,\\in\\,[m_{r}],r\\,\\in\\,[M]$ Conside the folowing two-layer neural network with $1+\\textstyle\\sum_{r=1}^{M}m_{r}$ neurons defined on $\\mathbb{R}^{D}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbf{FFN}^{(1)}(\\pmb{x})=\\sum_{r=1}^{M}\\sum_{\\substack{1+\\sum_{j=0}^{r-1}m_{j}\\leq k\\leq\\sum_{j=0}^{r}m_{j}}}e_{D-M+r-1}a_{k}^{(1,r)}\\sigma\\left(\\bar{b}_{k}^{(1,r)\\,\\top}\\pmb{x}+c_{k}^{(1,r)}\\right)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "It is easy to verify that for any x(1/2) , it holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbf{F}\\mathbf{F}\\mathbf{N}^{(1)}(x_{t}^{(1/2)})=\\mathbf{F}\\mathbf{F}^{\\mathrm{N}^{(1)}}(x_{t}^{(0)})}\\\\ &{=\\displaystyle\\sum_{r=1}^{M}\\sum_{1+\\sum_{j=0}^{r-1}m_{j}\\leq k\\leq\\sum_{j=0}^{r}m_{j}}e_{D-M+r-1}a_{k}^{(1,r)}\\sigma\\left(\\bar{b}_{k}^{(1,r)}\\ \\tau_{\\alpha}^{(0)}+c_{k}^{(1,r)}\\right)+e_{D}\\cdot1\\cdot\\sigma(0+\\log2)}\\\\ &{=\\displaystyle\\sum_{r=1}^{M}\\sum_{1+\\sum_{j=0}^{r-1}m_{j}\\leq k\\leq\\sum_{j=0}^{r}m_{j}}e_{D-M+r-1}a_{k}^{(1,r)}\\sigma\\left(b_{k}^{(1,r)}\\ \\tau_{\\alpha}+c_{k}^{(1,r)}\\right)+e_{D}\\cdot1\\cdot\\sigma(0+\\log2)}\\\\ &{=\\displaystyle\\sum_{r=1}^{M}e_{D-M+r-1}f_{r}^{\\mathrm{2NN}}(\\mathbf{x}_{t})+e_{D}\\log2}\\\\ &{=\\displaystyle(\\mathbf{0}_{r}^{\\top}f_{(1)}^{\\mathrm{2NN}}(\\mathbf{x}_{t}),\\dots,f_{(1,M)}^{\\mathrm{2NN}}(\\mathbf{x}_{t}),\\log2)^{\\top}\\in\\mathbb{R}^{D}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Moreover, it satisfies that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\mathbf{FFN}}^{(1)}(x_{t}^{(1/2)})=\\log\\left[\\exp\\left(\\mathbf{FFN}^{(1)}(x_{t}^{(1/2)})\\right)\\right]}\\\\ &{=\\!(\\mathbf{0}_{d}^{\\top},\\log\\left[\\exp\\left(f_{(1,1)}^{\\mathrm{2NN}}(x_{t})\\right)\\right],\\cdots,\\log\\left[\\exp\\left(f_{(1,M)}^{\\mathrm{2NN}}(x_{t})\\right)\\right],\\log2,\\mathbf{0}^{\\top})^{\\top}}\\\\ &{=\\!(\\mathbf{0}_{d}^{\\top},\\log g_{1}(x_{t}),\\cdots,\\log g_{M}(x_{t}),\\log2)^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Thus, we have achieved our goal in this step: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(1)}=\\pmb{x}_{t}^{(1/2)}+\\widetilde{\\mathbf{FFN}}^{(1)}(\\pmb{x}_{t}^{(1/2)})=(\\pmb{x}_{t}^{\\top},\\pmb{0}^{\\top},\\log g_{1}(\\pmb{x}_{t}),\\cdots,\\log g_{M}(\\pmb{x}_{t}),\\log2)^{\\top}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "As established in the proof above, the width $m$ must satisfy: ", "page_idx": 26}, {"type": "equation", "text": "$$\nm\\geq1+\\sum_{r=1}^{M}m_{r}=\\left\\{\\tilde{\\Omega}\\left(\\sum_{r=1}^{M}\\|g_{r}\\|_{\\mathcal B}^{2}\\right),\\right.\\quad\\quad\\quad\\quad\\mathrm{type=lin}\\,}\\\\ {\\tilde{\\Omega}\\left(\\sum_{r=1}^{M}\\|\\log g_{r}\\|_{\\mathcal B}^{2}\\,T_{r}^{2}\\right),\\quad\\mathrm{type=log}\\,}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Step Il1. Extract the adaptive memories by the second Attn layer. ", "page_idx": 26}, {"type": "text", "text": "We conside to use $H_{k}$ atention heads (rom $\\textstyle\\sum_{i=1}^{k-1}H_{i}+1$ -th head to $\\textstyle\\sum_{i=1}^{k}H_{i}$ -th head to extract it, and it satisfies to $\\textstyle\\sum_{k=1}^{M}H_{k}=H$ ", "page_idx": 26}, {"type": "text", "text": "For simplicity, we denote the following projection matrices in $\\mathbb{R}^{D\\times D}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad\\quad P^{(k)}:=\\bigl(\\mathbf{0}_{d\\times k d}\\quad I_{d\\times d}\\quad\\mathbf{0}\\bigr)\\in\\mathbb{R}^{d\\times D},\\quad1\\leq k\\leq M;}\\\\ &{P_{\\perp}^{(k)}:=\\left(\\!\\!\\begin{array}{c c c}{I_{k d\\times k d}}&{\\mathbf{0}_{d\\times d}}&{\\mathbf{0}}\\\\ {\\mathbf{0}}&{\\mathbf{0}_{d\\times d}}&{I_{(D-(k+1)d)\\times(D-(k+1)d)}}\\end{array}\\!\\!\\right)\\in\\mathbb{R}^{(D-d)\\times D},\\quad1\\leq k\\leq M;}\\\\ &{Q^{(M)}:=\\bigl(I_{(M+1)d\\times(M+1)d}&{\\mathbf{0}\\bigr)\\in\\mathbb{R}^{(M+1)d\\times D}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Now we consider the extraction of $k$ -th adaptive memory $\\pmb{x}_{t-g_{k}(\\pmb{x}_{t})}$ $1\\leq k\\leq M)$ ", "page_idx": 26}, {"type": "text", "text": "\u00b7 Case $\\mathtt{t y p e}=\\operatorname*{lin}.$ ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "By Lemma F.2, for any rate $n\\in\\mathbb{N}_{+}$ , there exists a constant $C(n)$ and a function ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\phi_{k}^{\\mathrm{exp}}(t;B)=\\sum_{\\scriptstyle\\sum_{i=1}^{k-1}{H_{i}+1\\le h\\le\\sum_{i=1}^{k}{H_{i}}}}\\alpha_{h}\\exp\\left(-\\beta_{h}(t-B)\\right)}}\\\\ &{}&{=\\sum_{\\scriptstyle\\sum_{i=1}^{k-1}{H_{i}+1\\le h\\le\\sum_{i=1}^{k}{H_{i}}}}\\alpha_{h}\\exp\\left(\\beta_{h}B-\\beta_{h}t\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "such that $\\beta_{h}>0$ and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{1\\leq B\\leq T_{k}}\\|\\mathbb{I}\\{\\cdot=B\\}-\\phi_{k}^{\\mathrm{exp}}(\\cdot;B)\\|_{\\ell_{1}(\\mathbb{N})}\\leq\\frac{C(n)e^{0.01(n+1)T_{k}}}{H_{k}^{n}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Moreover, Noticing that $1\\leq g_{k}({\\mathbf{\\boldsymbol{x}}}_{t})\\leq T_{k}$ holds for any $\\pmb{X}=(\\pmb{x}_{t})_{t\\in\\mathbb{Z}}\\in\\mathcal{X}$ , the following holds: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{\\pmb{X}}{\\operatorname*{sup}}\\,\\|\\mathbb{I}\\{\\cdot=g_{k}(\\pmb{x}_{t})\\}-\\phi_{k}^{\\mathrm{exp}}(\\cdot;g_{k}(\\pmb{x}_{t}))\\|_{\\ell_{1}(\\mathbb{N})}}\\\\ &{\\leq\\underset{1\\leq B\\leq T_{k}}{\\operatorname*{sup}}\\,\\|\\mathbb{I}\\{\\cdot=B\\}-\\phi_{k}^{\\mathrm{exp}}(\\cdot;B)\\|_{\\ell_{1}(\\mathbb{N})}\\leq\\frac{C(n)e^{0.01(n+1)T_{k}}}{H_{k}^{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore, for these attention heads $\\begin{array}{r}{(\\sum_{i=1}^{k-1}H_{i}+1\\le h\\le\\sum_{i=1}^{k}H_{i})}\\end{array}$ , we can choose: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad p^{(2,h)}=\\beta_{h},\\quad W_{O}^{(1)}=I_{D\\times D},\\quad W_{V}^{(2,h)}=\\alpha_{h}\\delta_{(k+1,1)}^{(d\\times d)}\\in\\mathbb{R}^{D\\times D},}\\\\ &{W_{Q}^{(2,h)}=\\sqrt{\\beta_{h}}\\delta_{(D-M+k-1,1)}^{(1\\times1)}\\in\\mathbb{R}^{D\\times D},\\quad W_{K}^{(2,h)}=\\sqrt{\\beta_{h}}\\delta_{(D,1)}^{(1\\times1)}\\in\\mathbb{R}^{D\\times D},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where 8(rxr) means that: it equals to $I_{r\\times r}$ for the $(p_{1},p_{2})$ -th $r\\times r$ blocks, and $\\mathbf{0}_{r\\times r}$ for the other $r\\times r$ blocks. ", "page_idx": 27}, {"type": "text", "text": "Then it is easy to verify: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left\\langle W_{Q}^{(2,h)}x_{t}^{(1)},W_{K}^{(2,h)}x_{t-s}^{(1)}\\right\\rangle+p^{(2,h)}\\phi_{\\mathrm{lin}}(s)=-\\beta_{h}\\Big(s-g_{k}({\\pmb x}_{t})\\Big),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which implies: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\underset{h=\\sum_{i=1}^{k-1}H_{i}+1}{\\sum_{i=1}^{k}}\\mathbf{A}\\mathbf{tm}_{t}^{(2,h)}(\\pmb{X}^{(1)})=\\underset{h=\\sum_{i=1}^{k-1}H_{i}+1}{\\sum_{i=1}^{k}}\\alpha_{h}\\sum_{s=0}^{+\\infty}e^{-\\beta_{h}(s-g_{k}(\\mathbf{x}_{t}))}\\left(\\pmb{x}_{t-s}^{0_{k d}}\\right)\\in\\mathbb{R}^{D},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then it holds that: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P^{(k)}\\displaystyle\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}\\mathbf{A}\\mathbf{t}\\mathbf{t}\\mathbf{n}_{t}^{(2,h)}(X^{(0)})=\\displaystyle\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}\\alpha_{h}\\sum_{s=0}^{+\\infty}e^{-\\beta_{h}(s-g_{k}(\\mathbf{x}_{t}))}x_{t-s},}\\\\ &{P_{\\perp}^{(k)}\\displaystyle\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}\\mathbf{A}\\mathbf{t}\\mathbf{t}\\mathbf{n}_{t}^{(2,h)}(X^{(0)})=\\mathbf{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "moreover, the following estimate holds: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\underset{h=\\sum_{i=1}^{k-1}H_{i}+1}{\\sum_{i=1}^{k}}P^{(k)}\\mathbf{Attn}_{t}^{(2,h)}(\\boldsymbol{X})-\\pmb{x}_{t-g_{k}(\\pmb{x}_{t})}\\right\\|_{2}}\\\\ &{=\\left\\|\\underset{h=\\sum_{i=1}^{k-1}H_{i}+1}{\\sum_{i=1}^{k}}\\alpha_{h}\\sum_{s=0}^{+\\infty}e^{-\\beta_{h}(s-g_{k}(\\pmb{x}_{t}))}\\pmb{x}_{t-s}-\\pmb{x}_{t-g_{k}(\\pmb{x}_{t})}\\right\\|_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\left\\|\\displaystyle\\sum_{s=0}^{+\\infty}\\left(\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}\\alpha_{h}e^{-\\beta_{h}(s-g_{k}(x_{t}))}-\\mathbb{I}\\{s=g_{k}(x_{t})\\}\\right)x_{t-s}\\right\\|_{2}}\\\\ &{\\le\\displaystyle\\sum_{s=0}^{+\\infty}\\left|\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}\\alpha_{h}e^{-\\beta_{h}(s-g_{k}(x_{t}))}-\\mathbb{I}\\{s=g_{k}(x_{t})\\}\\right|}\\\\ &{=\\|\\phi_{k}^{\\mathrm{exp}}(\\cdot;g_{k}(x_{t}))-\\mathbb{I}\\{:=g_{k}(x_{t})\\}\\|_{\\ell_{1}(\\mathbb{N})}\\le\\frac{C(n)e^{0.01(n+1)T_{k}}}{H_{k}^{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "\u00b7 Case $\\mathtt{t y p e}=\\mathtt{l o g}$ ", "page_idx": 28}, {"type": "text", "text": "By Lemma F.5, for any rate $n\\in\\mathbb{N}_{+}$ , there exists a constant $C(n)$ and a function ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi_{k}^{\\mathrm{poly}}(t;B)=\\sum_{\\substack{\\sum_{i=1}^{k-1}H_{i}+1\\leq h\\leq\\sum_{i=1}^{k}H_{i}}}\\alpha_{h}(t/B)^{-\\beta_{h}}}\\\\ &{\\qquad\\qquad\\qquad=\\sum_{\\substack{\\sum_{i=1}^{k-1}H_{i}+1\\leq h\\leq\\sum_{i=1}^{k}H_{i}}}\\alpha_{h}\\exp\\left(\\beta_{h}\\log B-\\beta_{h}\\log t\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "such that $\\beta_{h}>1$ and ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{1\\leq B\\leq T_{k}}\\left\\|\\mathbb{I}\\{\\cdot=B\\}-\\phi_{k}^{\\mathrm{poly}}(\\cdot;B)\\right\\|_{\\ell_{1}(\\mathbb{N}_{+})}\\leq\\frac{C(n)T_{k}^{1.01(n+1)}}{H_{k}^{n}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Moreover, Noticing that $1\\leq g_{k}({\\mathbf{\\boldsymbol{x}}}_{t})\\leq T_{k}$ holds for any $\\pmb{X}=(\\pmb{x}_{t})_{t\\in\\mathbb{Z}}\\in\\mathcal{X}$ the following holds: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\b{X}}{\\operatorname*{sup}}\\left\\|\\mathbb{I}\\{\\cdot=g_{k}(\\mathbf{x}_{t})\\}-\\phi_{k}^{\\mathrm{poly}}(\\cdot;g_{k}(\\pmb{x}_{t}))\\right\\|_{\\ell_{1}(\\mathbb{N}_{+})}}\\\\ &{\\leq\\underset{1\\leq B\\leq T_{k}}{\\operatorname*{sup}}\\left\\|\\mathbb{I}\\{\\cdot=B\\}-\\phi_{k}^{\\mathrm{poly}}(\\cdot;B)\\right\\|_{\\ell_{1}(\\mathbb{N}_{+})}\\leq\\frac{C(n)T_{k}^{1.01(n+1)}}{H_{k}^{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Therefore, for these attention heads $\\begin{array}{r}{(\\sum_{i=1}^{k-1}H_{i}+1\\le h\\le\\sum_{i=1}^{k}H_{i})}\\end{array}$ , we can choose: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{p^{(2,h)}=\\beta_{h},\\quad W_{O}^{(1)}=I_{D\\times D},\\quad W_{V}^{(2,h)}=\\alpha_{h}\\delta_{(k+1,1)}^{(d\\times d)}\\in\\mathbb{R}^{D\\times D},\\quad}\\\\ &{}&{W_{Q}^{(2,h)}=\\sqrt{\\beta_{h}}\\delta_{(D-M+k-1,1)}^{(1\\times1)}\\in\\mathbb{R}^{D\\times D},\\quad W_{K}^{(2,h)}=\\frac{\\sqrt{\\beta_{h}}}{\\log2}\\delta_{(D,1)}^{(1\\times1)}\\in\\mathbb{R}^{D\\times D},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Where 8(rxr) means that: itqualstIforthe (P,)thrX rblcks, and Ox for the other $r\\times r$ blocks. ", "page_idx": 28}, {"type": "text", "text": "Then it is easy to verify: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\langle W_{Q}^{(2,h)}x_{t}^{(1)},W_{K}^{(2,h)}x_{t-s}^{(1)}\\right\\rangle+p^{(2,h)}\\phi_{\\log}(s)=-\\beta_{h}\\log\\Big(s/g_{k}(x_{t})\\Big),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which implies: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\underset{h=\\sum_{i=1}^{k-1}H_{i}+1}{\\sum_{i=1}^{k}}\\mathbf{A}\\mathbf{tm}_{t}^{(2,h)}(X^{(1)})=\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}\\alpha_{h}\\sum_{s=0}^{+\\infty}(s/g_{k}(x_{t}))^{-\\beta_{h}}\\left(\\pmb{x}_{t-s}^{0}\\right)\\in\\mathbb{R}^{D},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then it holds that: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P^{(k)}\\displaystyle\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}\\mathbf{A}\\mathbf{t}\\mathbf{t}\\mathbf{n}_{t}^{(2,h)}(X^{(0)})=\\displaystyle\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}\\alpha_{h}\\sum_{s=0}^{+\\infty}(s/g_{k}(x_{t}))^{-\\beta_{h}}\\mathbf{x}_{t-s},}\\\\ &{P_{\\perp}^{(k)}\\displaystyle\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}\\mathbf{A}\\mathbf{t}\\mathbf{t}\\mathbf{n}_{t}^{(2,h)}(X^{(0)})=\\mathbf{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "moreover, the following estimate holds: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\begin{array}{c}{\\frac{\\sum_{k=1}^{n}m_{k}}{\\sum_{k=1}^{n}m_{k}}}\\\\ {\\vdots}\\\\ {m_{k}}\\end{array}\\right\\|_{L^{\\frac{2}{3}}}\\!\\!\\!\\!\\!\\!\\!\\!}\\\\ &{=\\left\\|\\begin{array}{c}{\\frac{\\sum_{k=1}^{n}m_{k}}{\\sum_{k=1}^{n}m_{k+1}}\\!\\!\\!\\!\\!\\!\\!\\!}\\\\ {m_{k}}\\end{array}\\right\\|_{L^{\\frac{2}{3}}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}\\\\ &{=\\left\\|\\begin{array}{c}{\\frac{\\sum_{k=1}^{n}m_{k}}{\\sum_{k=1}^{n}m_{k+1}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}\\\\ {m_{k}}\\end{array}\\right\\|_{L^{\\frac{2}{3}}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}\\\\ &{=\\left\\|\\begin{array}{c}{\\displaystyle\\sum_{i=0}^{n}\\left(\\sum_{k=1}^{n}\\!\\!\\!\\!\\!\\!\\!\\sum_{i=1}^{n}m_{k}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}\\\\ {\\vdots}\\\\ {m_{k}\\!\\!\\!\\!\\!\\!\\!\\!\\sum_{i=0}^{n}\\left(\\sum_{k=1}^{n}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!-\\!\\!m_{k}\\!\\!\\!\\!\\!\\!\\!\\!_{i}}+\\!\\!\\!\\!\\!\\!\\!\\!\\!\\sum_{i=1}^{n}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!+m_{k}\\!\\!\\!\\!\\!\\!\\!\\!\\int_{0}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!-m_{k}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\int_{1}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then we combine the estimate for all $k\\in[M]$ for these two cases. It holds that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|Q^{(H)}x_{t+\\frac{1}{2}}^{(2,\\bar{\\lambda})}-\\left(\\frac{x_{t+\\frac{1}{2}}}{x_{t-\\frac{3}{2}}(u_{t})}\\right)\\right|_{\\frac{1}{2}}}\\\\ &{=\\left|\\left(\\frac{x_{t}}{u_{t}}\\right)+\\sum_{k=1}^{M}Q^{(3)}A\\mathrm{HG}u_{k}^{(2,\\bar{\\lambda})}(X^{(1)})-\\left(\\frac{x_{t}}{x_{t-\\frac{1}{2}}(u_{k})}\\right)\\right|_{\\frac{1}{2}}}\\\\ &{=\\left|\\frac{M}{\\left|\\sum_{k=1}^{M}}Q^{(3)}A\\mathrm{HG}u_{k}^{(2,\\bar{\\lambda})}(X)-\\left(\\frac{u_{t}}{x_{t-\\frac{1}{2}}(u_{k})}\\right)\\right|_{\\frac{1}{2}}}\\\\ &{=\\left|\\left|\\frac{\\dot{X}}{\\left|\\sum_{k=1}^{M}}\\left(\\frac{\\sum_{k=1}^{M}R}{\\sum_{k=1}^{M}\\frac{\\xi_{k}}{\\xi_{k}+\\frac{1}{2}}(u_{k})}\\right)\\right|_{\\frac{1}{2}}}\\\\ &{\\leq\\frac{M}{\\left|\\sum_{k=1}^{M}\\right|_{\\frac{1}{2}}}+\\frac{Q^{(3)}A\\mathrm{HG}u_{k}^{(2,\\bar{\\lambda})}(X)-\\left(\\frac{u_{k}}{x_{t}-\\frac{1}{2}(u_{k})}\\right)}{\\left|\\xi_{k}+\\frac{1}{2}(u_{k})\\right|_{\\frac{1}{2}}}\\right|_{\\frac{1}{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\displaystyle\\sum_{k=1}^{M}\\left\\|\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}P^{(k)}\\mathbf{A}\\mathbf{t}\\mathbf{n}_{t}^{(2,h)}(X)-\\pmb{x}_{t-g_{k}(\\pmb x_{t})}\\right\\|_{2}}\\\\ &{\\le\\!\\mathcal{E}_{\\mathrm{Attn}}(\\mathbf{t}\\mathbf{y}\\mathbf{p}\\mathbf{e}):=\\left\\{\\frac{C(n)^{e^{0.01(n+1)T_{k}}}}{H_{k}^{n}},\\quad\\mathrm{type=lin}}\\\\ &{\\le\\!\\mathcal{E}_{\\mathrm{Attn}}(\\mathbf{t}\\mathbf{y}\\mathbf{p}\\mathbf{e}):=\\left\\{\\frac{C(n)T_{k}^{1.01(n+1)}}{H_{k}^{n}},\\quad\\mathrm{~type=log}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Similar to the proof of Theorem B.1, we choose the head number: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{H_{k}=\\frac{e^{0.01T_{k}}}{\\sum_{j=1}^{M}e^{0.01T_{j}}}H,\\quad k\\in[M],\\quad\\mathrm{type}=\\mathrm{lin};}&\\\\ &{H_{k}=\\frac{T_{k}^{1.01}}{\\sum_{j=1}^{M}T_{j}^{1.01}}H,\\quad k\\in[M],\\quad\\mathrm{type}=\\mathrm{log}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Thus, we obtain the final bound in Step III: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\mathrm{Attn}}^{\\mathrm{Soft}}(\\mathrm{type})\\leq\\left\\{\\begin{array}{l l}{\\displaystyle\\frac{C(n)}{H^{n}}\\left(\\sum_{k=1}^{M}e^{0.01T_{k}}\\right)^{n+1},\\:\\mathrm{type}=\\operatorname*{lin}}\\\\ {\\displaystyle\\frac{C(n)}{H^{n}}\\left(\\sum_{k=1}^{M}T_{k}^{1.01}\\right)^{n+1},\\:\\mathrm{type}=\\log}\\end{array}\\right.\\;.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Furthermore, by choosing $\\mathcal{E}_{\\mathrm{Attn}}(\\sf t y p e)\\le1$ , it holds that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|Q^{(M)}x_{t}^{(3/2)}\\right\\|_{\\infty}\\leq\\left\\|Q^{(M)}x_{t}^{(3/2)}-\\left(x_{t-g_{1}(x_{t})}^{\\alpha_{t}}\\right)\\right\\|_{\\infty}+\\left\\|\\left(\\binom{x_{t}}{x_{t-g_{1}(x_{t})}}\\right\\|_{\\infty}\\right.}\\\\ {\\left.\\qquad\\qquad\\qquad\\leq\\xi_{\\mathrm{Attn}}(\\mathbf{type})+1\\leq2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Step IV. Approximate the nonlinear function by 2-nd FFN layer. ", "page_idx": 30}, {"type": "text", "text": "In this step, we aim to approximate the function $f$ using two-layer network. By Lemma G.6, there exists a two-layer neural network with $m$ neurons ", "page_idx": 30}, {"type": "equation", "text": "$$\nf_{(2)}^{\\mathrm{2NN}}(\\pmb{x})=\\sum_{k=1}^{m}a_{k}^{(2)}\\pmb{\\sigma}\\left(\\pmb{b}_{k}^{(2)}\\ ^{\\top}\\pmb{x}+c_{k}^{(2)}\\right)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "defined on $\\mathbb{R}^{(M+1)d}$ such that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left\\|f-f_{(2)}^{\\mathrm{2NN}}\\right\\|_{L^{\\infty}([-2,2]^{(M+1)d})}\\leq\\tilde{\\mathcal{O}}\\left(\\frac{\\|f\\|_{\\mathcal{B}}}{\\sqrt{m}}\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Denote $\\bar{\\pmb{b}}_{k}^{(2)}=({\\pmb{b}}_{k}^{(2)^{\\top}},\\mathbf{0}^{\\top})^{\\top}\\in\\mathbb{R}^{D}$ for $k\\in[m]$ Andwecosidtfoin w-layl network with $m$ neurons defined on $\\mathbb{R}^{D}$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathrm{FFN}^{(2)}(\\pmb{x}):=\\sum_{k=1}^{m}a_{k}^{(2)}\\pmb{\\sigma}\\left(\\bar{\\pmb{b}}_{k}^{(2)}\\ ^{\\top}\\pmb{x}+c_{k}^{(2)}\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "It is easy to verify: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathrm{FFN}^{(2)}\\left({\\pmb x}_{t}^{(3/2)}\\right)=f_{(2)}^{\\mathrm{2NN}}\\left({\\pmb Q}^{(M)}{\\pmb x}_{t}^{(3/2)}\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Moreover, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal E_{\\mathrm{FFN}}^{(2)}:=\\left\\|f-\\mathrm{FFN}^{(2)}\\right\\|_{L^{\\infty}([-2,2]^{(M+1)d})}\\leq\\tilde{\\mathcal O}\\left(\\frac{\\|f\\|_{\\mathcal B}}{\\sqrt{m}}\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The final bound. ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "For any $t$ and $\\|X\\|\\in\\mathcal{X}$ , it holds that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left|\\mathbf{H}_{t}(X)-x_{t}^{(2)}\\right|\\right|=\\left|f\\left(x_{t},x_{t-g_{1}(\\mathbf{x}_{t})},\\cdots\\mathbf{x}_{t-g_{M}(\\mathbf{x}_{t})}\\right)-\\mathrm{FFN}^{(2)}\\left(\\pmb{x}_{t}^{(3/2)}\\right)\\right|}\\\\ &{=\\left|f\\left(x_{t},x_{t-t_{1}},\\cdots x_{t-t_{M}}\\right)-f_{(2)}^{2\\mathrm{NN}}\\left(Q^{(M)}x_{t}^{(3/2)}\\right)\\right|}\\\\ &{=\\left|f\\left(x_{t},x_{t-g_{1}(\\mathbf{x}_{t})},\\cdots x_{t-g_{M}(\\mathbf{x}_{t})}\\right)-f\\left(Q^{(M)}x_{t}^{(3/2)}\\right)\\right|}\\\\ &{\\qquad+f\\left(Q^{(M)}x_{t}^{(3/2)}\\right)-f_{(2)}^{2\\mathrm{NN}}\\left(Q^{(M)}x_{t}^{(3/2)}\\right)\\Big|}\\\\ &{\\leq\\left|f\\left(x_{t},x_{t-t_{1}},\\cdots x_{t-t_{M}}\\right)-f\\left(Q^{(M)}x_{t}^{(3/2)}\\right)\\right|+\\left|f\\left(Q^{(M)}x_{t}^{(3/2)}\\right)-f_{(2)}^{2\\mathrm{NN}}\\left(Q^{(M)}x_{t}^{(3/2)}\\right)\\right|}\\\\ &{\\leq\\left\\|f\\right\\|_{\\mathrm{Lip}}\\left\\|\\left(x_{t}^{\\top},x_{t-t_{1}}^{\\top},\\cdots x_{t-t_{M}}^{\\top}\\right)^{\\top}-Q^{(M)}x_{t}^{(3/2)}\\right\\|_{2}+\\left\\|f-f_{(2)}^{2\\mathrm{NN}}\\right\\|_{L^{\\infty}\\left([-2,2]^{(M+1)D}\\right)}}\\\\ &{\\leq\\|f\\|_{\\mathrm{Lip}}\\cdot\\xi_{\\mathrm{Att}}+\\xi_{\\mathrm{FFN}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\mathrm{FFN}}=\\tilde{\\mathcal{O}}\\left(\\frac{\\|f\\|_{B}}{\\sqrt{m}}\\right);\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\mathrm{Attn}}(\\mathrm{type})=\\left\\{\\mathcal{O}\\left(\\frac{C(n)}{H^{n}}\\Big(\\sum_{k=1}^{M}e^{0.01T_{k}}\\Big)^{n+1}\\right),\\quad\\mathrm{type=lin}\\right.}\\\\ {\\left.\\mathcal{O}\\left(\\frac{C(n)}{H^{n}}\\Big(\\sum_{k=1}^{M}T_{k}^{1.01}\\Big)^{n+1}\\right),\\quad\\mathrm{~type=log~}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Moreover, recalling our proof in Step $\\mathrm{II}$ , we further need the hard condition: ", "page_idx": 31}, {"type": "equation", "text": "$$\nm\\geq\\left\\{\\tilde{\\Omega}\\left(\\sum_{r=1}^{M}\\left\\|g_{r}\\right\\|_{\\mathcal B}^{2}\\right),\\right.\\quad\\quad\\quad\\quad\\mathrm{type=lin}\\mathrm{~.~}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Due to the arbitrariness of $t$ and $\\mathbf{\\deltaX}$ , the proof is completed. ", "page_idx": 31}, {"type": "text", "text": "Remark C.3. The core step in this proof is Step IMl, where the extraction of the memory functions is achieved through a a nice interaction between the temporal space (provided by RPE) and the token space (provided by $D P$ ). Specifically, the memory functions $g_{i}(\\pmb{x}_{t})$ (in token space) are mapped into the temporal space $s$ , resulting in the form of $\\pmb{x}_{s-g_{i}(\\pmb{x}_{t})}$ for DP Attn with lin-RPE or $\\log(\\bar{s}/g_{i}(\\mathbf{\\bar{x}}_{t}))$ for DP Attn with log-RPE. ", "page_idx": 31}, {"type": "text", "text": "C.2 Proof of Proposition 4.2 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "For Proposition 16, we denote the following one-layer Attn hypothesis class: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad A\\mathcal{T}\\mathcal{T}\\mathcal{N}_{(1,H)}^{\\mathrm{vye}}:=\\left\\{\\mathbf{A}\\mathbf{t}\\mathbf{t}\\mathbf{n}:\\mathbf{T}\\mathbf{F}\\mathrm{~is~a~}1.\\mathrm{layer},\\,H\\mathrm{-head~(normalization-free)~Attn~with~type-RPE}\\right\\};}\\\\ &{\\quad\\hfill4T\\mathcal{T}\\mathcal{N}_{(1,H)}^{\\mathrm{DPF,type}}:=\\left\\{\\mathbf{T}\\mathbf{F}:\\mathbf{A}\\mathbf{t}\\mathbf{t}\\mathbf{n}\\mathrm{~is~a~}1.\\mathrm{layer},\\,H\\mathrm{-head~DP\\mathrm{-free~Attn~with~type-RPE}}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proposition C.4 (The formal version of Proposition 4.2). Consider 1-layer Attn. Then, there exists a $\\mathbf{H}\\in\\mathcal{H}_{(1,1)}^{a d a p}$ ", "page_idx": 32}, {"type": "text", "text": "(A) (Attn with ${\\cal D}P$ ) For any $\\epsilon>0$ there exists $a$ 1-layer Attn $\\mathbf{Attn}^{\\mathrm{DP}}\\in\\mathcal{A T T N}_{(1,H)}^{\\sf t y p e}$ such that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{H}-\\mathbf{Attn}^{\\mathrm{{DP}}}\\right\\|\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "$(B)$ (Aitn without $D P$ )For any 1-layer $D P$ Sfree Atn $\\mathbf{Attn}^{\\mathrm{DPF}}\\in\\mathcal{A T T N}_{(1,H)}^{\\mathrm{DPF,type}}$ , a uniform lower bound holds: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{H}-\\mathbf{Attn}^{\\mathrm{{DPF}}}\\right\\|\\geq{\\frac{2}{3}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof of Proposition C.4. ", "page_idx": 32}, {"type": "text", "text": "Consider the following target function $\\mathrm{H}\\in\\mathcal{H}_{1,1}^{\\mathrm{Adap}}$ . Let the input sequence $X\\in{\\mathcal{X}}=\\{-1,0,1\\}^{\\mathbb{Z}}$ and we consider the target ", "page_idx": 32}, {"type": "equation", "text": "$$\ny_{t}=\\mathrm{H}_{t}(X):=x_{t-g(x_{t})},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the adaptive memory is ", "page_idx": 32}, {"type": "equation", "text": "$$\ng(x)=\\left\\{\\!\\!\\begin{array}{l l}{{0,}}&{{x=-1}}\\\\ {{1,}}&{{x=0}}\\\\ {{2,}}&{{x=1}}\\end{array}\\right.\\;.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Part (A). The Efficiency of Attn with DP. ", "page_idx": 32}, {"type": "text", "text": "First, we choose the embedding dimension ${\\cal D}\\,=\\,2$ , and select simple embedding ${\\pmb W}_{E}\\,=\\,{\\pmb b}_{E}\\,=$ $(1,0)^{\\top}\\in\\mathbb{R}^{2\\times1}$ . Then for any input sequence $X=({\\mathbf{\\boldsymbol{x}}}_{t})_{t\\in\\mathbb{Z}}$ , the token after embedding satisfies: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\boldsymbol{x}_{t}^{(0)}=W_{E}\\boldsymbol{x}_{t}+\\boldsymbol{b}_{E}=(x_{t},1)^{\\top}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We consider one-layer normalization-free Self-attention with $\\phi_{\\mathrm{exp}}$ , which has the form: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathrm{Attn}_{t}^{\\mathrm{DP}}(X)=W_{O}\\sum_{h=1}^{H}W_{V}^{(1,h)}\\sum_{s=0}^{+\\infty}x_{t-s}^{(0)}\\exp\\left(\\left\\langle W_{Q}^{(l,h)}x_{t}^{(0)},W_{K}^{(1,h)}x_{t-s}^{(0)}\\right\\rangle+p^{(1,h)}s\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By Lemma F.2 (for $n=1$ ), there exists a constant $C>0$ and a function ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\phi^{\\mathrm{exp}}(t;B)=\\sum_{h=1}^{H}\\alpha_{h}\\exp(-\\beta_{h}(t-B))=\\sum_{h=1}^{H}\\alpha_{h}\\exp\\left(\\beta_{h}B-\\beta_{h}t\\right)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "such that $\\beta_{h}>0$ and ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\leq B\\leq2}\\|\\mathbb{I}\\{\\cdot=B\\}-\\phi^{\\mathrm{exp}}(\\cdot;B)\\|_{\\ell_{1}(\\mathbb{N})}\\leq\\frac{C e^{0.01\\cdot2\\cdot2}}{H}=\\mathcal{O}\\left(\\frac{1}{H}\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Moreover, Noticing that $0\\leq g(x_{t})\\leq2$ holds for any $\\pmb{X}=(x_{t})_{t\\in\\mathbb{Z}}\\in\\mathcal{X}$ , the following holds: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{X}{\\operatorname*{sup}}\\,\\lVert\\mathbb{I}\\{\\cdot=g(x_{t})\\}-\\phi^{\\mathrm{exp}}(\\cdot;g(x_{t}))\\rVert_{\\ell_{1}(\\mathbb{N})}}\\\\ &{\\le\\underset{0\\le B\\le2}{\\operatorname*{sup}}\\,\\lVert\\mathbb{I}\\{\\cdot=B\\}-\\phi^{\\mathrm{exp}}(\\cdot;B)\\rVert_{\\ell_{1}(\\mathbb{N})}\\le\\mathcal{O}\\left(\\frac{1}{H}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Therefore, for attention heads $(1\\leq h\\leq H)$ , we can choose: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p^{(1,h)}=\\beta_{h},\\quad W_{O}^{(1)}=(1,0)^{\\top}\\in\\mathbb{R}^{2\\times1},\\quad W_{V}^{(1,h)}=\\alpha_{h}\\delta_{(1,1)}^{(1\\times1)}\\in\\mathbb{R}^{2\\times2},}\\\\ &{\\quad W_{Q}^{(1,h)}=\\sqrt{\\beta_{h}}(1,1)^{\\top}\\in\\mathbb{R}^{2\\times1},\\quad W_{K}^{(1,h)}=\\sqrt{\\beta_{h}}(0,1)^{\\top}\\in\\mathbb{R}^{2\\times1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where 8(rxr) means that: it equals to $I_{r\\times r}$ for the $(p_{1},p_{2})$ -th $r\\times r$ blocks, and $\\mathbf{0}_{r\\times r}$ for the other $r\\times r$ blocks. ", "page_idx": 33}, {"type": "text", "text": "Then it is easy to verify: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left\\langle W_{Q}^{(1,h)}x_{t}^{(0)},W_{K}^{(1,h)}x_{t-s}^{(0)}\\right\\rangle+p^{(2,h)}s=-p^{(1,h)}\\Big(s-(x_{t}+1)\\Big)=-p^{(1,h)}\\Big(s-g(x_{t})\\Big).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus, the following estimate holds: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\mathrm{Att}_{n}^{\\mathrm{pr}}(X)-x_{t-g(x_{t})}\\right\\|_{2}}\\\\ &{=\\left\\|\\displaystyle\\sum_{h=1}^{H}\\alpha_{h}\\displaystyle\\sum_{s=0}^{+\\infty}e^{-\\beta_{h}(s-g(x_{t}))}x_{t-s}-x_{t-g(x_{t})}\\right\\|_{2}}\\\\ &{=\\left\\|\\displaystyle\\sum_{s=0}^{+\\infty}\\left(\\displaystyle\\sum_{h=1}^{H}\\alpha_{h}e^{-\\beta_{h}(s-g(x_{t}))}-\\mathbb{I}\\{s=g(x_{t})\\}\\right)x_{t-s}\\right\\|_{2}}\\\\ &{\\leq\\displaystyle\\sum_{s=0}^{+\\infty}\\left|\\displaystyle\\sum_{h=1}^{H}\\alpha_{h}e^{-\\beta_{h}(s-g_{h}(x_{t}))}-\\mathbb{I}\\{s=g(x_{t})\\}\\right|}\\\\ &{=\\|\\phi^{\\mathrm{exp}}(\\cdot;g(x_{t}))-\\mathbb{I}\\{:=g(x_{t})\\}\\|_{\\ell_{1}(\\mathbb{N})}\\leq\\mathcal{O}\\left(\\frac{1}{H}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Due to the arbitrariness of $t$ and $\\mathbf{\\deltaX}$ , the proof is completed: for any $\\epsilon>0$ , we only need to use $H=\\Omega(1/\\epsilon)$ heads to approximate it. ", "page_idx": 33}, {"type": "text", "text": "Part (B). The Hardness Result of Attn without DP. ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "We consider one-layer Dot-product-free Self-attention with $\\phi_{\\mathrm{exp}}$ or $\\phi_{\\mathrm{log}}$ . For any input $\\mathbf{\\deltaX}$ , the corresponding output can be written as: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname{Attn}_{t}^{\\mathrm{DPF}}({\\pmb X})=\\sum_{s=0}^{+\\infty}\\rho_{s}(W_{E}x_{t-s}+b_{E}).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "For simplicity, we denote: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\boldsymbol{x}_{t}^{(0)}=\\boldsymbol{W}_{E}\\boldsymbol{x}_{t}+\\boldsymbol{b}_{E},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "then we have the following estimate: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\lvert\\lvert\\mathrm{{\\lvert{H-Attn}^{D P F}}}\\rvert\\rvert=\\operatorname*{sup}_{t}\\operatorname*{sup}_{X}\\lvert\\mathrm{{H}}_{t}^{\\mathrm{{DPF}}}(X)-\\mathrm{{Attn}}_{t}(X)\\rvert\\ge\\operatorname*{sup}_{X}\\lvert\\mathrm{{H}}_{0}(X)-\\mathrm{{Attn}}_{0}^{\\mathrm{{DPF}}}(X)\\rvert\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n=\\operatorname*{sup}_{(\\cdots,x-2,x-1,x_{0})}\\left|x_{-g(x_{0})}-\\sum_{s=0}^{+\\infty}\\rho_{s}x_{-s}^{(0)}\\right|\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathrm{sup}_{1},}\\\\ &{}&{\\mathrm{sup}_{2},}\\\\ &{}&{\\mathrm{sup}_{3},}\\\\ &{}&{\\mathrm{sup}_{4},}\\\\ &{}&{\\mathrm{sup}_{5},}\\\\ &{}&{\\mathrm{sup}_{6},}\\\\ &{}&{\\mathrm{sup}_{7},}\\\\ &{}&{\\mathrm{term}_{8},}\\\\ &{}&{\\mathrm{term}_{9},}\\\\ &{}&{\\mathrm{term}_{10},}\\\\ &{}&{\\mathrm{term}_{9},}\\\\ &{}&{\\mathrm{term}_{11},}\\\\ &{}&{\\mathrm{term}_{22},}\\\\ &{}&{\\mathrm{term}_{12},}\\\\ &{}&{\\mathrm{term}_{13},}\\\\ &{}&{\\mathrm{term}_{23},}\\\\ &{}&{\\mathrm{term}_{31},}\\\\ &{}&{\\mathrm{term}_{42},}\\\\ &{}&{\\mathrm{term}_{52},}\\end{array}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "C.3 Proof of Proposition 4.3 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In this subsection, we propose a structurally simpler yet effective alternative to traditional Dot-product in Self-attention. This alternative is proposed based on our insights into the role of Attn in facilitating interaction between the temporal-space and the token-space. Specifically, we propose a more direct structure to achieve the interaction $\\dot{\\phi}_{\\sf t y p e}(s)-\\phi_{\\sf t y p e}\\big({\\pmb w}^{\\sf i}{\\pmb x}_{t}\\big)$ ", "page_idx": 34}, {"type": "text", "text": "Definition C.5 (TMX Transformer). We define the TMX ( ${}^{\\ast}t$ minus $\\pmb{x}^{\\circ}$ ) Transformer as follows. In standard Transformer (2), we modify the term ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left\\langle W_{Q}^{(l,h)}\\mathbf{x}_{t},W_{K}^{(l,h)}\\mathbf{x}_{t-s}\\right\\rangle+p^{(l,h)}\\phi_{\\mathtt{t y p e}}(s)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "in Multi-head Dot-product Self-attention to the new formulation: ", "page_idx": 34}, {"type": "equation", "text": "$$\np^{(l,h)}\\Big(\\phi_{\\mathtt{t y p e}}\\left(s\\right)-\\phi_{\\mathtt{t y p e}}\\left(w^{(l,h)}\\right.^{\\top}\\!\\mathbf{x}_{t}\\Big)\\Big),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "wheretheparameters $\\pmb{w}^{(l,h)}\\in\\mathbb{R}^{D\\times1}$ ", "page_idx": 35}, {"type": "text", "text": "Notice that in TMX Transformer, the revised term requires only $\\mathcal{O}(D)$ parameters, much less than $\\mathcal{O}(D^{2})$ in standard Dot-product Transformer. ", "page_idx": 35}, {"type": "text", "text": "Consequently, we define the following TMX Transformer hypothesis class. ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{T F}_{(1,H,m)}^{\\mathrm{TMX,type}}:=\\left\\{\\mathbf{TF}:\\mathbf{TF}\\;\\mathrm{is~a~1-layer},\\;H\\mathrm{-head},\\,m\\mathrm{-width}\\right.\\qquad\\qquad\\qquad\\qquad}\\\\ {\\left.\\qquad\\qquad\\mathrm{(normalization\\mathrm{-free)}~T M X~T r a n s f o r m e r~w i t h~t y p e\\mathrm{-}\\mathrm{RPE}}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proposition C.6 (The formal version of Proposition 4.3). Under the same conditions in Theorem 4.1, there exists a two-layer TMx Transformer $\\mathbf{TF}\\in{\\mathcal{T F}}_{(2,H,m)}^{\\mathrm{TMX,type}}$ FTMx,type (17) such that i can achieve the ame approximation rate as standard Transformer presented in Theorem 4.1. ", "page_idx": 35}, {"type": "text", "text": "Proof of Proposition C.6. ", "page_idx": 35}, {"type": "text", "text": "It is worth noting that TMX Transformer only replaces $\\left\\langle W_{Q}^{(l,h)}\\mathbf{x}_{t},W_{K}^{(l,h)}\\mathbf{x}_{t-s}\\right\\rangle+p^{(l,h)}\\phi_{\\mathtt{t y p e}}(s)$ in standad rasformer with $p^{(l,h)}\\Big(\\phi_{\\mathtt{t y p e}}\\left(s\\right)-\\phi_{\\mathtt{t y p e}}\\left(w^{(l,h)}\\right.^{\\top}\\pmb{x}_{t}\\Big)\\Big)$ Therefore, the prof is highly similar to that of Theorem C.2. We only need to prove that TMX Atn can also achieve Step I and Step IMI in the proof of Theorem C.2. ", "page_idx": 35}, {"type": "text", "text": "Step I. Step I is trivial due to the same use of the residual block. ", "page_idx": 35}, {"type": "text", "text": "Step Il1. Extract the adaptive memories by the second Attn layer. ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "We still consider to use $H_{k}$ attention heads (from $\\textstyle\\sum_{i=1}^{k-1}H_{i}+1$ -th head to $\\textstyle\\sum_{i=1}^{k}H_{i}$ -th head) to extract it, and it satisfes to $\\textstyle\\sum_{k=1}^{M}H_{k}=H$ ", "page_idx": 35}, {"type": "text", "text": "Now we consider the extraction of $k$ -th adaptive memory $\\pmb{x}_{t-g_{k}(\\pmb{x}_{t})}$ $(1\\leq k\\leq M)$ ", "page_idx": 35}, {"type": "text", "text": "\u00b7 Case $\\mathtt{t y p e}=\\operatorname*{lin}.$ ", "page_idx": 35}, {"type": "text", "text": "For the proof of standard Transformer (the proof of Theorem C.2), for the attention heads $\\begin{array}{r}{(\\sum_{i=1}^{k-1}H_{i}+1\\le h\\le\\sum_{i=1}^{k}H_{i})}\\end{array}$ wecanconstruct specific $p^{(2,h)},W_{Q}^{(2,h)},W_{K}^{(2,h)},W_{V}^{(2,h)}$ suchthat ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left\\langle W_{Q}^{(2,h)}x_{t}^{(1)},W_{K}^{(2,h)}x_{t-s}^{(1)}\\right\\rangle+p^{(2,h)}\\phi_{\\mathrm{lin}}(s)=-\\beta_{h}\\Big(s-g_{k}({\\pmb x}_{t})\\Big).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Inthis prof welytprove that weanalonrut s $\\pmb{w}^{(l,h)},\\pmb{W}_{V}^{(2,h)}$ such that ", "page_idx": 35}, {"type": "equation", "text": "$$\np^{(2,h)}\\left(\\phi_{\\mathrm{lin}}(s)-\\phi_{\\mathrm{lin}}\\left(\\pmb{w}^{(2,h)}\\right.^{\\top}\\pmb{x}_{t}^{(1)}\\right)\\right)=-\\beta_{h}\\Big(s-g_{k}(\\pmb{x}_{t})\\Big).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Recalling the proof of Theorem C.2, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(1)}=(\\pmb{x}_{t}^{\\top},\\pmb{0}^{\\top},g_{1}(\\pmb{x}_{t}),\\cdot\\cdot\\cdot\\mathrm{~,~}g_{M}(\\pmb{x}_{t}),1)^{\\top}\\in\\mathbb{R}^{D}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Therefore, we can choose ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p^{(2,h)}=\\beta_{h},\\quad w^{(2,h)}=\\delta_{(D-M+k-1,1)}^{(1\\times1)}\\in{\\mathbb R}^{D},\\quad W_{V}^{(2,h)}=\\alpha_{h}\\delta_{(k+1,1)}^{(d\\times d)}\\in{\\mathbb R}^{D\\times D}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where &(rxr) means that: it equals to $I_{r\\times r}$ for the $(p_{1},p_{2})$ -th $r\\times r$ blocks, and $\\mathbf{0}_{r\\times r}$ for the other $r\\times r$ blocks. ", "page_idx": 35}, {"type": "text", "text": "The the following holds: ", "page_idx": 35}, {"type": "equation", "text": "$$\np^{(2,h)}\\left(\\phi_{\\mathrm{lin}}(s)-\\phi_{\\mathrm{lin}}\\left(\\pmb{w}^{(2,h)}\\ ^{\\top}\\pmb{x}_{t}^{(1)}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n=-\\,p^{(2,h)}\\left(s-w^{(2,h)\\textsf{T}}\\pmb{x}_{t}^{(1)}\\right)=-\\beta_{h}\\Big(s-g_{k}(\\pmb{x}_{t})\\Big).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "\u00b7 Case $\\mathtt{t y p e}=\\mathtt{l o g}$ ", "page_idx": 36}, {"type": "text", "text": "For the proof of standard Transformer (the proof of Theorem C.2), for the attention heads $\\begin{array}{r}{(\\sum_{i=1}^{k-1}H_{i}+1\\le h\\le\\sum_{i=1}^{k}H_{i})}\\end{array}$ ,we can construct specific $p^{(2,h)},W_{Q}^{(2,h)},W_{K}^{(2,h)},W_{V}^{(2,h)}$ suchthat ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left\\langle W_{Q}^{(2,h)}x_{t}^{(1)},W_{K}^{(2,h)}x_{t-s}^{(1)}\\right\\rangle+p^{(2,h)}\\phi_{\\log}(s)=-\\beta_{h}\\log\\Big(s/g_{k}(x_{t})\\Big).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "In this proof, we only need to prove that we can also construct specific $\\pmb{w}^{(l,h)},\\pmb{W}_{V}^{(2,h)}$ such that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p^{(2,h)}\\left(\\phi_{\\log}(s)-\\phi_{\\log}\\left(w^{(2,h)}\\;^{\\top}x_{t}^{(1)}\\right)\\right)=-\\beta_{h}\\log\\Big(s/g_{k}(x_{t})\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Recalling the proof of Theorem C.2, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{x}_{t}^{(1)}=(\\pmb{x}_{t}^{\\top},\\pmb{0}^{\\top},\\log g_{1}(\\pmb{x}_{t}),\\pmb{\\cdot}\\cdot\\pmb{\\cdot},\\log g_{M}(\\pmb{x}_{t}),\\log2)^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Therefore, we can choose ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p^{(2,h)}=\\beta_{h},\\quad w^{(2,h)}=\\delta_{(D-M+k-1,1)}^{(1\\times1)}\\in{\\mathbb R}^{D},\\quad W_{V}^{(2,h)}=\\alpha_{h}\\delta_{(k+1,1)}^{(d\\times d)}\\in{\\mathbb R}^{D\\times D}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where 8(rxr) means that: it equals to $I_{r\\times r}$ for the $(p_{1},p_{2})$ -th $r\\times r$ blocks, and $\\mathbf{0}_{r\\times r}$ for the other $r\\times r$ blocks. ", "page_idx": 36}, {"type": "text", "text": "The the following holds: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad p^{(2,h)}\\left(\\phi_{\\log}(s)-\\phi_{\\log}\\left(w^{(2,h)\\textsf{T}}\\pmb{x}_{t}^{(1)}\\right)\\right)}\\\\ &{=-\\,p^{(2,h)}\\log\\left(s/\\left(w^{(2,h)\\textsf{T}}\\pmb{x}_{t}^{(1)}\\right)\\right)=-\\beta_{h}\\log\\Big(s/g_{k}(\\pmb{x}_{t})\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The rest of the proof is exactly the same as the proof of Theorem C.2, and we do not repeat it. ", "page_idx": 36}, {"type": "text", "text": "D Proof of Section 4.3 ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "D.1Proof of Theorem 4.4 ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "In this subsection, we give the detailed proofs for the general case of $K$ -adaptive, long but $M$ sparse memory: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\pmb{{y}}_{t}=\\pmb{{f}}(\\pmb{{x}}_{t},\\pmb{{x}}_{t-t_{1}},\\cdot\\cdot\\cdot\\textit{,}\\pmb{{x}}_{t-t_{M}}),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the adaptive memories satisfy: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{~}&{t_{1}=g_{1}(\\pmb{x}_{t});~~~~~}\\\\ &{~}&{t_{2}=g_{2}(\\pmb{x}_{t},\\pmb{x}_{t-t_{1}});~~~}\\\\ &{~}&{\\quad\\quad\\cdot\\cdot\\cdot}\\\\ &{~}&{t_{K+1}=g_{K+1}(\\pmb{x}_{t},\\pmb{x}_{t-t_{1}},\\cdot\\cdot\\cdot\\cdot,\\pmb{x}_{t-t_{K}});}\\\\ &{~}&{\\quad\\quad\\cdot\\cdot\\quad}\\\\ &{~}&{t_{K+2}=g_{K+2}(\\pmb{x}_{t},\\pmb{x}_{t-t_{1}},\\cdot\\cdot\\cdot,\\pmb{x}_{t-t_{K}});}\\\\ &{~}&{\\quad\\quad\\cdot\\cdot}\\\\ &{~}&{t_{M}=g_{K+1}(\\pmb{x}_{t},\\pmb{x}_{t-t_{1}},\\cdot\\cdot\\cdot,\\pmb{x}_{t-t_{K}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $1\\leq t_{k}\\leq T_{k}$ holds for any $k\\in[M]$ ", "page_idx": 37}, {"type": "text", "text": "TheoremD.1 Restatement f Theorem 4.4). Forany targetH , rate $n\\in\\mathbb{N}_{+}$ and $H,m\\in$ $\\mathbb{N}_{+}$ there exists an $L$ layer $(L=K+1+\\mathbb{I}\\{M\\geq K+1\\},$ Transformer $\\mathbf{TF}\\in\\mathcal{T F}_{(L,H,m)}^{\\mathrm{NF,type}}$ (12) and aconstant $C(n)$ such that: if the width satisfies ", "page_idx": 37}, {"type": "equation", "text": "$$\nm\\geq\\left\\{\\begin{array}{l l}{\\tilde{\\Omega}\\Big(\\displaystyle\\operatorname*{max}_{i\\in[K]}\\vee\\sum_{i=K+1}^{M}\\left\\Vert g_{i}\\right\\Vert_{\\mathcal{B}}^{2}\\Big),}&{\\mathrm{\\type=lin,}}\\\\ {\\tilde{\\Omega}\\Big(\\displaystyle\\operatorname*{max}_{i\\in[K]}\\vee\\sum_{i=K+1}^{M}\\left\\Vert\\log g_{i}\\right\\Vert_{\\mathcal{B}}^{2}T_{i}^{2}\\Big),}&{\\mathrm{\\type=log}}\\end{array}\\right.,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "then the following approximation rate holds: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\lVert\\mathbf{H}-\\mathbf{T}\\mathbf{F}\\rVert\\leq\\mathcal{E}_{\\mathrm{FFN}}+\\mathcal{E}_{\\mathrm{Attn}}(\\mathrm{type}),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\mathrm{Attn}}(\\mathrm{type})=\\left\\{\\mathcal{O}\\left(\\frac{C(n)}{H^{n}}\\sqrt{\\sum_{l=1}^{K}e^{0.02\\left(n+1\\right)T_{l}}+\\left(\\sum_{l=K+1}^{M}e^{0.01T_{l}}\\right)^{2n+2}}\\right),\\;\\mathrm{type}=\\mathrm{lin}\\;\\middle|\\;n>\\right.\\right.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof of Theorem D.1. ", "page_idx": 37}, {"type": "text", "text": "First, we choose the embedding dimension $D=(M+1)(d+1)$ , and select the same embedding matrix $W_{E}\\,=\\,(\\pmb{I}_{d},\\pmb{0})^{\\top}\\,\\in\\,\\mathbb{R}^{D\\times d},\\pmb{b}_{E}\\,=\\,\\pmb{0}=\\in\\,\\mathbb{R}^{D}$ as the proof of Theorem C.2. Moreover, we still use the network with precision FFN defined in Appendix C.1 to tackle the discrete values of memories. ", "page_idx": 37}, {"type": "text", "text": "Then for any input sequence $X=({\\mathbf{\\boldsymbol{x}}}_{t})_{t\\in\\mathbb{Z}}$ , the token after embedding satisfies: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(0)}=W_{E}\\pmb{x}_{t}+\\pmb{b}_{E}=(\\pmb{x}_{t}^{\\top},\\pmb{0}^{\\top})^{\\top}\\in\\mathbb{R}^{D}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Thus,for L-layer(L = K+1 + I{M \u2265 K+ 1) normalization-free Transformer TF E TFNF,type With $\\phi_{\\tt t y p e}$ , the output token r(K+1) of t-th input token satisfies: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(l-1/2)}=\\pmb{x}_{t}^{(l)}+\\pmb{W}_{O}^{(l)}\\sum_{h=1}^{H}\\mathbf{A}\\mathbf{t}\\mathbf{n}_{t}^{(l,h)}(\\pmb{X}^{(l-1)}),\\;1\\leq l\\leq L+1\n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\mathbf{x}_{t}^{(l)}=\\mathbf{x}_{t}^{(l-1/2)}+\\widetilde{\\mathbf{F}\\mathbf{F}\\mathbf{N}}^{(l)}(\\mathbf{x}_{t}^{(l-1/2)}),\\;1\\le l\\le L}\\\\ {\\mathbf{x}_{t}^{(L+1)}=\\mathbf{F}\\mathbf{F}\\mathbf{N}^{(L+1)}(\\mathbf{x}_{t}^{(L+1/2)}),~~~~~~~\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbf{A}\\mathbf{t}\\mathbf{t}\\mathbf{n}_{t}^{(l,h)}(\\boldsymbol{X})=W_{V}^{(l,h)}\\sum_{s=0}^{+\\infty}\\pmb{x}_{t-s}\\exp\\left(\\left\\langle W_{Q}^{(l,h)}\\pmb{x}_{t},W_{K}^{(l,h)}\\pmb{x}_{t-s}\\right\\rangle+p^{(l,h)}\\phi_{\\mathrm{type}}(s)\\right).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Since the proof of this theorem is similar to the proof of Theorem C.2, we mainly discuss the differences. ", "page_idx": 38}, {"type": "text", "text": "The proof can be summarized as the following process: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 Case $\\mathtt{t y p e}=\\operatorname*{lin}$ ", "page_idx": 38}, {"type": "text", "text": "Regime $M\\geq K+1$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(0)}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(1/2)}=\\pmb{x}_{t}^{(0)}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(1)}=\\pmb{x}_{t}^{(1/2)}+(\\mathbf{0}^{\\top},t_{1},\\mathbf{0}_{M-1}^{\\top},1)^{\\top}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(3/2)}\\approx\\pmb{x}_{t}^{(1)}+(\\mathbf{0}_{d}^{\\top},\\pmb{x}_{t-t_{1}}^{\\top},\\mathbf{0}^{\\top})^{\\top}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "equation", "text": "$$\n{\\pmb x}_{t}^{(2)}={\\pmb x}_{t}^{(3/2)}+({\\bf0}^{\\top},t_{2},{\\bf0}_{M-1}^{\\top})^{\\top}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\pmb x}_{t}^{(5/2)}\\approx{\\pmb x}_{t}^{(2)}+(\\mathbf{0}_{2d}^{\\top},{\\pmb x}_{t-t_{2}}^{\\top},\\mathbf{0}^{\\top})^{\\top}}\\\\ {\\dots\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Step $2K+1$ $K+1$ -st Attn $\\downarrow$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(K+1/2)}\\approx\\pmb{x}_{t}^{(K)}+(\\mathbf{0}_{K d}^{\\top},\\pmb{x}_{t-t_{K}}^{\\top},\\mathbf{0}^{\\top})^{\\top}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Step $2K+2$ $K+1$ -st FFN ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(K+1)}=\\pmb{x}_{t}^{(K+1/2)}+(\\pmb{0}^{\\top},t_{K+1},\\cdot\\cdot\\cdot\\cdot,t_{M},0)^{\\top}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Step $2K+3.\\ K+2{\\mathrm{-st}}\\ {\\mathrm{Attn}}\\ \\downarrow$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(K+3/2)}\\approx\\pmb{x}_{t}^{(K+1)}+(\\pmb{0}_{(K+1)d}^{\\top},\\pmb{x}_{t-t_{K+1}}^{\\top},\\cdot\\cdot\\cdot\\cdot,\\pmb{x}_{t-t_{M}}^{\\top},0)^{\\top}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Step $2K+4.\\;K+2{\\-}{\\mathrm{st}}\\;{\\mathrm{FFN}}\\;\\downarrow$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(K+2)}\\approx\\pmb{f}(\\pmb{x}_{t},\\pmb{x}_{t-t_{1}},\\cdot\\cdot\\cdot\\cdot,\\pmb{x}_{t-t_{M}})\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Regime $M=K$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(0)}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(1/2)}=\\pmb{x}_{t}^{(0)}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(1)}=\\pmb{x}_{t}^{(1/2)}+(\\mathbf{0}^{\\top},t_{1},\\mathbf{0}_{M-1}^{\\top},1)^{\\top}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(3/2)}\\approx\\pmb{x}_{t}^{(1)}+(\\mathbf{0}_{d}^{\\top},\\pmb{x}_{t-t_{1}}^{\\top},\\mathbf{0}^{\\top})^{\\top}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Step 4. 2-st FFN $\\downarrow$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n{\\pmb x}_{t}^{(2)}={\\pmb x}_{t}^{(3/2)}+\\left({\\bf0}^{\\top},t_{2},{\\bf0}_{M-1}^{\\top}\\right)^{\\top}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\pmb x}_{t}^{(5/2)}\\approx{\\pmb x}_{t}^{(2)}+(\\mathbf{0}_{2d}^{\\top},{\\pmb x}_{t-t_{2}}^{\\top},\\mathbf{0}^{\\top})^{\\top}}\\\\ {\\dots\\dots\\qquad\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Step $2K+1$ $K+1$ -st Attn $\\downarrow$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(K+1/2)}\\approx\\pmb{x}_{t}^{(K)}+(\\mathbf{0}_{K d}^{\\top},\\pmb{x}_{t-t_{K}}^{\\top},\\mathbf{0}^{\\top})^{\\top}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Step $2K+2$ . K + 1-st FFN ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(K+1)}\\approx\\pmb{f}(\\pmb{x}_{t},\\pmb{x}_{t-t_{1}},\\cdot\\cdot\\cdot\\cdot,\\pmb{x}_{t-t_{M}})\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "\u00b7 Case $\\tt t y p e=\\tt l o g,$ ", "page_idx": 39}, {"type": "text", "text": "Regime $M\\geq K+1$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(0)}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(1/2)}=\\pmb{x}_{t}^{(0)}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(3/2)}\\approx\\pmb{x}_{t}^{(1)}+(\\mathbf{0}_{d}^{\\top},\\pmb{x}_{t-t_{1}}^{\\top},\\mathbf{0}^{\\top})^{\\top}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(2)}=\\pmb{x}_{t}^{(3/2)}+(\\mathbf{0}^{\\top},\\log t_{2},\\mathbf{0}_{M-1}^{\\top})^{\\top}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\pmb x}_{t}^{(5/2)}\\approx{\\pmb x}_{t}^{(2)}+(\\mathbf{0}_{2d}^{\\top},{\\pmb x}_{t-t_{2}}^{\\top},\\mathbf{0}^{\\top})^{\\top}}\\\\ {\\dots\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Step $2K+1$ $K+1\\cdot$ -st Attn $\\downarrow$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(K+1/2)}\\approx\\pmb{x}_{t}^{(K)}+(\\pmb{0}_{K d}^{\\top},\\pmb{x}_{t-\\log t_{K}}^{\\top},\\pmb{0}^{\\top})^{\\top}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Step $2K+2.\\;K+1.\\mathrm{st}\\;\\mathrm{FFN}\\;\\downarrow$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(K+1)}=\\pmb{x}_{t}^{(K+1/2)}+(\\mathbf{0}^{\\top},\\log t_{K+1},\\cdots,\\log t_{M},0)^{\\top}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Step $2K+3$ . K + 2-st Attn $\\downarrow$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(K+3/2)}\\approx\\pmb{x}_{t}^{(K+1)}+(\\pmb{0}_{(K+1)d}^{\\top},\\pmb{x}_{t-t_{K+1}}^{\\top},\\cdot\\cdot\\cdot\\cdot,\\pmb{x}_{t-t_{M}}^{\\top},0)^{\\top}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Step $2K+4.\\;K+2{\\-}{\\mathrm{st}}\\;{\\mathrm{FFN}}\\;\\downarrow$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(K+2)}\\approx\\pmb{f}(\\pmb{x}_{t},\\pmb{x}_{t-t_{1}},\\cdot\\cdot\\cdot\\cdot,\\pmb{x}_{t-t_{M}})\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Regime $M=K$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad x_{t}^{(0)}}\\\\ &{\\mathrm{st}\\,\\mathrm{Atu}\\,\\,\\downarrow}\\\\ &{\\qquad x_{t}^{(1/2)}=x_{t}^{(0)}}\\\\ &{\\qquad\\mathrm{st}\\,\\mathrm{FFN}\\,\\,\\downarrow}\\\\ &{\\qquad x_{t}^{(1)}=x_{t}^{(1/2)}+(\\mathbf{0}^{\\top},\\log t_{1},\\mathbf{0}_{M-1}^{\\top},\\log2)^{\\top}}\\\\ &{\\qquad\\mathrm{st}\\,\\mathrm{Atu}\\,\\,\\downarrow}\\\\ &{\\qquad x_{t}^{(3/2)}\\approx x_{t}^{(1)}+(\\mathbf{0}_{d}^{\\top},x_{t-t_{1}}^{\\top},\\mathbf{0}^{\\top})^{\\top}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\pmb{x}_{t}^{(2)}=\\pmb{x}_{t}^{(3/2)}+(\\mathbf{0}^{\\top},\\log t_{2},\\mathbf{0}_{M-1}^{\\top})^{\\top}}\\\\ &{\\therefore\\;3\\mathrm{-st}\\;\\mathrm{Attn}\\;\\downarrow}&\\\\ &{}&{\\pmb{x}_{t}^{(5/2)}\\approx\\pmb{x}_{t}^{(2)}+(\\mathbf{0}_{2d}^{\\top},\\pmb{x}_{t-t_{2}}^{\\top},\\mathbf{0}^{\\top})^{\\top}}\\\\ &{}&{\\therefore\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\pmb{x}_{t}^{(K+1/2)}\\approx\\pmb{x}_{t}^{(K)}+(\\mathbf{0}_{K d}^{\\top},\\pmb{x}_{t-t_{K}}^{\\top},\\mathbf{0}^{\\top})^{\\top}}\\\\ &{\\mathrm{5tep~}2K+2.~K+1\\mathrm{-}\\mathrm{st}\\,\\mathrm{FFN~}\\downarrow}\\\\ &{}&{\\pmb{x}_{t}^{(K+1)}\\approx\\pmb{f}(\\pmb{x}_{t},\\pmb{x}_{t-t_{1}},\\cdot\\cdot\\cdot\\cdot,\\pmb{x}_{t-t_{M}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "For simplicity, we denote the following projection matrices: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{P^{(k)}:=\\big(\\mathbf{0}_{d\\times k d}}&{{\\cal I}_{d\\times d}\\quad\\mathbf{0}\\big)\\in\\mathbb{R}^{d\\times D},\\quad1\\le k\\le M;}\\\\ {P_{\\perp}^{(k)}:=\\bigg(\\begin{array}{l l l}{{\\cal I}_{k d\\times k d}}&{\\mathbf{0}_{d\\times d}}&{\\mathbf{0}}\\\\ {\\mathbf{0}}&{\\mathbf{0}_{d\\times d}}&{{\\cal I}_{(D-(k+1)d)\\times(D-(k+1)d)}\\bigg)\\in\\mathbb{R}^{(D-d)\\times D},\\quad1\\le k\\le M;}\\\\ {\\mathbf{0}^{(k)}:=\\big({\\cal I}_{(k+1)d\\times(k+1)d}}&{\\mathbf{0}\\big)\\in\\mathbb{R}^{(k+1)d\\times D},\\quad1\\le k\\le M;}\\\\ {Q_{\\perp}^{(k)}:=\\big(\\mathbf{0}}&{{\\cal I}_{(D-(k+1)d)\\times(D-(k+1)d)}\\big)\\in\\mathbb{R}^{(D-(k+1)d)\\times D},\\quad1\\le k\\le M;}\\\\ {{\\cal R}:=\\big(\\mathbf{0}_{(M-K)d\\times(K+1)d}}&{{\\cal I}_{(M-K)d\\times(M-K)d}\\quad\\mathbf{0}\\big)\\in\\mathbb{R}^{(M-K)d\\times D};}\\\\ {{\\cal R}_{\\perp}:=\\bigg({\\cal I}_{(K+1)d\\times(K+1)d}}&{\\mathbf{0}}&{\\mathbf{0}}\\\\ {\\mathbf{0}}&{\\mathbf{0}}&{{\\cal I}_{(M+1)\\times(M+1)}\\bigg)\\in\\mathbb{R}^{(D-(M-K)d)\\times D}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Step 1 is trivial due to the use of the residual block. ", "page_idx": 40}, {"type": "text", "text": "Step 2. In the same way as Step $\\mathrm{II}$ in the proof of Theorem C.2, we obtain the conclusion in this step: If the width of FFN satisfies ", "page_idx": 40}, {"type": "equation", "text": "$$\nm\\geq\\left\\{\\tilde{\\Omega}\\left(\\left\\|g_{1}\\right\\|_{B}^{2}\\right),\\quad\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, \n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "then the following holds: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(1)}=\\pmb{x}_{t}^{(1/2)}+(\\mathbf{0}^{\\top},t_{1},\\mathbf{0}_{M}^{\\top},1)^{\\top}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Thus, (E1) holds for $l=2$ Step 3 \\~ Step 2K + 1. ", "page_idx": 40}, {"type": "text", "text": "\u00b7 Case $\\mathtt{t y p e}=\\operatorname*{lin}$ ", "page_idx": 40}, {"type": "text", "text": "- FFN layers. ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "We use $l.$ -th $(2\\leq l\\leq K)$ FFN layer to express $l$ -th memory $t_{l}$ exactly. By Lemma G.6, there exists a two-layer neural network with $m$ neurons defined on $\\mathbb{R}^{l d}$ ", "page_idx": 40}, {"type": "equation", "text": "$$\nf_{(l)}^{\\mathrm{2NN}}(\\pmb{x})=\\sum_{k=1}^{m}a_{k}^{(l)}\\pmb{\\sigma}(\\pmb{b}_{k}^{(l)}\\mathrm{\\pmb{\\omega}}^{\\top}\\pmb{x}+c_{k}^{(l)})\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "such that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left\\|g_{l}-f_{(l)}^{\\mathrm{2NN}}\\right\\|_{L^{\\infty}([-2,2]^{l d})}\\leq\\tilde{\\mathcal{O}}\\left(\\frac{\\|g_{l}\\|_{\\mathcal{B}}}{\\sqrt{m}}\\right).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "$l$ Cth FEN layer we only ned to arangethe parameters $a_{k}^{(l)},b_{k}^{(l)}$ and $\\boldsymbol{c}_{k}^{\\left(l\\right)}\\left(\\boldsymbol{k}\\right)\\in$ $[m],2\\leq l\\leq M\\`$ ", "page_idx": 40}, {"type": "text", "text": "Denote 6() $\\bar{\\pmb{b}}_{k}^{(l)}\\,=\\,\\big(\\pmb{b}_{k}^{(l)^{\\top}},\\mathbf{0}^{\\top}\\big)^{\\top}\\,\\in\\,\\mathbb{R}^{D}$ for $k\\;\\in\\;[m],2\\;\\leq\\;l\\;\\leq\\;K\\,-\\,1$ Consider the following two-layer neural network with $m$ neurons defined on $\\mathbb{R}^{D}$ ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbf{FFN}^{(l)}(\\pmb{x})=\\sum_{k=1}^{m}{e_{D-M+l-1}a_{k}^{(l)}\\sigma\\left(\\bar{\\pmb{b}}_{k}^{(l)}\\,\\bar{\\pmb{\\tau}}+\\pmb{c}_{k}^{(l)}\\right)}\\,.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "It is easy to verify ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbf F\\mathbf F\\mathbf N^{(l)}(\\pmb x)=\\left(\\mathbf0^{\\top},f_{(l)}^{\\mathrm{2NN}}\\left(\\pmb Q^{(l)}\\pmb x\\right),\\mathbf0_{D-M+l-1}^{\\top}\\right)^{\\top}\\in\\mathbb R^{D},\\quad\\forall\\pmb x\\in\\mathbb R^{D}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Notice that if the width $m$ satisfies ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{O}}\\left(\\frac{\\|g_{l}\\|_{B}}{\\sqrt{m}}\\right)<\\frac{1}{4},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "and the input $\\pmb{x}_{t}^{(l-1/2)}$ satisfies ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left\\|g_{l}\\right\\|_{\\mathrm{Lip}}\\cdot\\left\\|(\\pmb{x}_{t}^{\\top},\\cdot\\cdot\\cdot,\\pmb{x}_{t-t_{l-1}}^{\\top})^{\\top}-\\pmb{Q}^{(l-1)}\\pmb{x}_{t}^{(l-1/2)}\\right\\|_{2}\\leq\\frac{1}{4}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "the following holds: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|g_{l}(x_{t},\\cdots,x_{t-t_{l-1}})-f_{l}^{\\mathrm{2NN}}\\left(Q^{(l)}x_{t}^{(l-1/2)}\\right)\\right|}\\\\ &{\\leq\\left|g_{l}(x_{t},\\cdots,x_{t-t_{l-1}})-g_{l}(Q^{(l-1)}x_{t}^{(l-1/2)})\\right|}\\\\ &{\\quad\\quad+\\left|g_{l}(Q^{(l-1)}x_{t}^{(l-1/2)})-f_{l}^{\\mathrm{2NN}}(Q^{(l-1)}x_{t}^{(l-1/2)})\\right|}\\\\ &{\\leq\\left\\|g_{l}\\right\\|_{\\mathrm{Lip}}\\left\\|(\\pmb{x}_{t}^{\\top},\\cdots,\\pmb{x}_{t-t_{l-1}}^{\\top})^{\\top}-\\pmb{Q}^{(l-1)}x_{t}^{(l-1/2)}\\right\\|_{2}+\\left\\|g_{l}-f_{(l)}^{\\mathrm{2NN}}\\right\\|_{L^{\\infty}}}\\\\ &{<\\frac{1}{4}+\\frac{1}{4}=\\frac{1}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Noticing $t_{l}=g_{l}(\\mathbf{x}_{t},\\cdot\\cdot\\cdot\\mathbf{\\delta},\\mathbf{x}_{t-t_{l-1}})\\in\\mathbb{N}_{+}$ , we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\widetilde{f_{(l)}^{\\mathrm{2NN}}}({\\pmb Q}^{(l-1)}{\\pmb x}_{t}^{(l-1/2)})=t_{l}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Thus, it holds that: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf{F}\\mathbf{F}\\mathbf{N}}^{(l)}(\\pmb{x}_{t}^{(l-1/2)})=(\\mathbf{0}^{\\top},t_{l},\\mathbf{0}_{D-M+l-1}^{\\top})^{\\top}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Attn layers. ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "By Lemma F.2, for any rate $n\\in\\mathbb{N}_{+}$ , there exists a constant $C(n)$ and $K$ functions ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\phi_{l}^{\\mathrm{exp}}(t;B)=\\sum_{h=1}\\alpha_{l,h}\\exp(-\\beta_{l,h}(t-B))}\\quad}&{{}}\\\\ &{=\\displaystyle\\sum_{h=1}^{H}\\alpha_{l,h}\\exp\\Big(\\beta_{l,h}B-\\beta_{l,h}t\\Big),\\quad1\\le l\\le K}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "such that $\\beta_{l,h}>0$ and ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{1\\leq B\\leq T_{l}}\\|\\mathbb{I}\\{\\cdot=B\\}-\\phi_{l}^{\\mathrm{exp}}(\\cdot;B)\\|_{\\ell_{1}(\\mathbb{N})}\\leq\\frac{C(n)e^{0.01(n+1)T_{l}}}{H^{n}},~1\\leq l\\leq K.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Moreover, Noticing that $1\\;\\leq\\;g_{l}(\\cdot)\\;\\leq\\;T_{l}$ holds for any $\\pmb{X}\\;=\\;(\\pmb{x}_{t})_{t\\in\\mathbb{Z}}\\;\\in\\;\\mathcal{X}$ and $2\\le l\\le K$ , the following holds: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\b{X}}{\\operatorname*{sup}}\\,\\|\\mathbb{I}\\{\\cdot=t_{l}\\}-\\phi_{l}^{\\mathrm{exp}}(\\cdot;t_{l})\\|_{{\\ell_{1}}(\\mathbb{N})}}\\\\ &{\\leq\\underset{1\\leq B\\leq T_{l}}{\\operatorname*{sup}}\\,\\|\\mathbb{I}\\{\\cdot=B\\}-\\phi_{l}^{\\mathrm{exp}}(\\cdot;B)\\|_{{\\ell_{1}}(\\mathbb{N})}\\leq\\frac{C(n)e^{0.01(n+1)T_{l}}}{H^{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Therefore, for the attention heads $h$ $\\langle h\\in[H]\\rangle$ in each layer $l$ $(1\\leq l\\leq K)$ ), we can choose: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p^{(l+1,h)}=\\beta_{l,h},\\quad W_{O}^{(l+1)}=I,\\quad W_{V}^{(l+1,h)}=\\alpha_{l,h}\\delta_{(l+1,1)}^{(d\\times d)}\\in\\mathbb{R}^{D\\times D},}\\\\ &{\\quad\\quad\\quad\\quad W_{Q}^{(l+1,h)}=\\sqrt{\\beta_{l,h}}\\delta_{(D-M+l,1)}^{(1\\times1)}\\in\\mathbb{R}^{D\\times(D/H)},}\\\\ &{\\quad\\quad\\quad\\quad W_{K}^{(l+1,h)}=\\sqrt{\\beta_{l,h}}\\delta_{(D,1)}^{(1\\times1)}\\in\\mathbb{R}^{D\\times(D/H)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where 8(rxr) means that: it equals to $I_{r\\times r}$ for the $(p_{1},p_{2})$ -th $r\\times r$ blocks, and $\\mathbf{0}_{r\\times r}$ for the other $r\\times r$ blocks. ", "page_idx": 42}, {"type": "text", "text": "\u00b7 Case $\\tt t y p e=\\tt l o g,$ ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "- FFN layers. ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "We use $l$ -th( $2\\le l\\le K)$ FFN layer to express $l$ -th memory $t_{l}$ exactly. By Lemma G.6, there exists a two-layer neural network with $m$ neurons defined on $\\mathbb{R}^{l d}$ ", "page_idx": 42}, {"type": "equation", "text": "$$\nf_{(l)}^{\\mathrm{2NN}}(\\pmb{x})=\\sum_{k=1}^{m}a_{k}^{(l)}\\pmb{\\sigma}(\\pmb{b}_{k}^{(l)}\\mathrm{\\pmb{\\omega}}^{\\top}\\pmb{x}+c_{k}^{(l)})\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "such that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left\\|\\log g_{l}-f_{(l)}^{\\mathrm{2NN}}\\right\\|_{L^{\\infty}}\\leq\\tilde{\\mathcal{O}}\\left(\\frac{\\|\\log g_{l}\\|_{\\mathcal{B}}}{\\sqrt{m}}\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "For $l$ th FFN layer, we only need to arrange the parameers $a_{k}^{(l)},b_{k}^{(l)}$ and $\\boldsymbol{c}_{k}^{\\left(l\\right)}\\left(\\boldsymbol{k}\\right)\\in$ $[m],2\\leq l\\leq M)$   \nDenote $\\bar{\\pmb{b}}_{k}^{(l)}=(\\pmb{b}_{k}^{(l)^{\\top}},\\pmb{0}^{\\top})^{\\top}\\in\\mathbb{R}^{D}$ for $k\\,\\in\\,[m],2\\,\\le\\,l\\,\\le\\,K-1$ We consider the following $l_{\\cdot}$ -th layer 2NN with $m$ neurons defined on $\\mathbb{R}^{D}$ ", "page_idx": 42}, {"type": "equation", "text": "$$\n{\\bf F}{\\bf F}{\\bf N}^{(l)}({\\pmb x})=\\sum_{k=1}^{m}e_{D-M+l-1}a_{k}^{(r)}\\sigma\\left(\\bar{{\\pmb b}}_{k}^{(r)}{\\bf\\Delta}^{\\top}{\\pmb x}+c_{k}^{(r)}\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "It is easy to verify ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbf F\\mathbf F\\mathbf N^{(l)}(\\pmb x)=\\left(\\mathbf0^{\\top},f_{(l)}^{\\mathrm{2NN}}\\left(\\pmb Q^{(l)}\\pmb x\\right),\\mathbf0_{D-M+l-1}^{\\top}\\right)^{\\top}\\in\\mathbb R^{D},\\quad\\forall\\pmb x\\in\\mathbb R^{D}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Notice that if the width $m$ satisfies ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{O}}\\left(\\frac{\\Vert\\log g_{l}\\Vert_{B}}{\\sqrt{m}}\\right)<\\frac{1}{8T_{l}},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "and the input $\\pmb{x}_{t}^{(l-1/2)}$ satisfies ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left\\|\\log g_{l}\\right\\|_{\\mathrm{Lip}}\\cdot\\left\\|(\\pmb{x}_{t}^{\\top},\\cdot\\cdot\\cdot\\mathbf{\\nabla},\\pmb{x}_{t-t_{l-1}}^{\\top})^{\\top}-\\pmb{Q}^{(l)}\\pmb{x}_{t}^{(l-1/2)}\\right\\|_{2}\\leq\\frac{1}{8T_{l}},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "the following holds: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\left|\\log g_{l}(\\pmb{x}_{t},\\cdots,\\pmb{x}_{t-t_{l-1}})-f_{(l)}^{\\mathrm{2NN}}(\\pmb{Q}^{(l)}\\pmb{x}_{t}^{(l-1/2)})\\right|}\\\\ &{}&{\\quad\\le\\left|\\log g_{l}(\\pmb{x}_{t},\\cdots,\\pmb{x}_{t-t_{l-1}})-\\log g_{l}(\\pmb{Q}^{(l)}\\pmb{x}_{t}^{(l-1/2)})\\right|}\\\\ &{}&{\\quad+\\left|g_{l}(\\pmb{Q}^{(l)}\\pmb{x}_{t}^{(l-1/2)})-f_{(l)}^{\\mathrm{2NN}}(\\pmb{Q}^{(l)}\\pmb{x}_{t}^{(l-1/2)})\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\leq\\left\\|\\log g_{l}\\right\\|_{\\mathrm{Lip}}\\left\\|({\\pmb x}_{t}^{\\top},\\cdot\\cdot\\cdot,{\\pmb x}_{t-t_{l-1}}^{\\top})^{\\top}-{\\pmb Q}^{(l)}{\\pmb x}_{t}^{(l-1/2)}\\right\\|_{2}+\\left\\|\\log g_{l}-f_{(l)}^{\\mathrm{2NN}}\\right\\|_{L^{\\infty}}}}\\\\ {{\\displaystyle<\\frac{1}{8T_{l}}+\\frac{1}{8T_{l}}=\\frac{1}{4T_{l}},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "which ensures ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\exp\\left(f_{(l)}^{2\\mathrm{NN}}(Q^{(l)}x_{t}^{(l-1/2)})\\right)-g_{l}(x_{t},\\cdots,x_{t-t_{l-1}})\\right|}\\\\ &{=\\left|\\exp\\left(f_{(l)}^{2\\mathrm{NN}}(Q^{(l)}x_{t}^{(l-1/2)})\\right)-\\exp\\left(\\log\\left(g_{l}(x_{t},\\cdots,x_{t-t_{l-1}})\\right)\\right)\\right|}\\\\ &{\\le\\exp\\left(\\operatorname*{max}\\left\\{f_{(l)}^{2\\mathrm{NN}}(Q^{(l)}x_{t}^{(l-1/2)}),\\log\\left(g_{l}(x_{t},\\cdots,x_{t-t_{l-1}})\\right)\\right\\}\\right)}\\\\ &{\\qquad\\cdot\\left|f_{(l)}^{2\\mathrm{NN}}(Q^{(l)}x_{t}^{(l-1/2)})-\\log\\left(g_{l}(x_{t},\\cdots,x_{t-t_{l-1}})\\right)\\right|}\\\\ &{\\le\\exp\\left(\\log\\left(g_{l}(x_{t},\\cdots,x_{t-t_{l-1}})\\right)+\\frac{1}{8}\\right)\\frac{1}{4T_{r}}}\\\\ &{\\le e^{1/8}\\cdot T_{r}\\cdot\\frac{1}{4T_{r}}<\\frac{1}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Noticing $t_{l}=g_{l}(\\pmb{x}_{t},\\cdot\\cdot\\cdot\\mathrm{~,~}\\pmb{x}_{t-t_{l-1}})\\in\\mathbb{N}_{+}$ , we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\widetilde{f_{(l)}^{\\mathrm{2NN}}}(Q_{l}x_{t}^{(l-1/2)})=\\log\\left(\\exp\\left[\\mathrm{FFN}^{(l)}(x_{t},\\cdots,x_{t-t_{l-1}})\\right]\\right)=\\log t_{l}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Thus, it holds that: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf{F}\\mathbf{F}\\mathbf{N}}^{(l)}(\\pmb{x}_{t}^{(l-1/2)})=(\\mathbf{0}^{\\top},\\log t_{l},\\mathbf{0}_{D-M+l-1}^{\\top})^{\\top}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "- Attn layers. ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "By Lemma F.5, for any rate $n\\in\\mathbb{N}_{+}$ , there exists a constant $C(n)$ and $K$ functions ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi_{l}^{\\mathrm{poly}}(t;B)=\\displaystyle\\sum_{h=1}\\alpha_{l,h}(t/B)^{-\\beta_{l,h}}}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{h=1}^{H}\\alpha_{l,h}\\exp\\Big(-\\beta_{l,h}\\log(t/B)\\Big),\\quad1\\leq l\\leq K}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "such that $\\beta_{l,h}>0$ and ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{1\\leq B\\leq T_{l}}\\Big\\|\\mathbb{I}\\{\\cdot=B\\}-\\phi_{l}^{\\mathrm{poly}}(\\cdot;B)\\Big\\|_{\\ell_{1}(\\mathbb{N}_{+})}\\leq\\frac{C(n)T_{l}^{1.01(n+1)}}{H^{n}},~1\\leq l\\leq K.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Moreover, Noticing that $1\\;\\leq\\;g_{l}(\\cdot)\\;\\leq\\;T_{l}$ holds for any $\\pmb{X}\\;=\\;(\\pmb{x}_{t})_{t\\in\\mathbb{Z}}\\;\\in\\;\\mathcal{X}$ and $1\\le l\\le K$ , the following holds: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~\\underset{\\b{X}}{\\operatorname*{sup}}\\left\\|\\mathbb{I}\\{\\cdot=t_{l}\\}-\\phi_{l}^{\\mathrm{poly}}(\\cdot;t_{l})\\right\\|_{\\ell_{1}(\\mathbb{N}_{+})}}\\\\ &{\\leq\\underset{1\\leq B\\leq T_{l}}{\\operatorname*{sup}}\\left\\|\\mathbb{I}\\{\\cdot=B\\}-\\phi_{l}^{\\mathrm{poly}}(\\cdot;B)\\right\\|_{\\ell_{1}(\\mathbb{N}_{+})}\\leq\\frac{C(n)T_{l}^{1.01(n+1)}}{H^{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Therefore, for the attention heads $h$ $(h\\in[H])$ in each layer $l$ $(1\\leq l\\leq K)$ ), we can choose: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p^{(l+1,h)}=\\beta_{l,h},\\quad W_{O}^{(l+1)}=I,\\quad W_{V}^{(l+1,h)}=\\alpha_{l,h}\\delta_{(l+1,1)}^{(d\\times d)}\\in\\mathbb{R}^{D\\times D},}\\\\ &{\\quad\\quad\\quad\\quad W_{Q}^{(l+1,h)}=\\sqrt{\\beta_{l,h}}\\delta_{(D-M+l,1)}^{(1\\times1)}\\in\\mathbb{R}^{D\\times(D/H)},}\\\\ &{\\quad\\quad\\quad\\quad W_{K}^{(l+1,h)}=\\sqrt{\\beta_{l,h}}\\delta_{(D,1)}^{(1\\times1)}\\in\\mathbb{R}^{D\\times(D/H)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where 8(rxr) means that: it equals to $I_{r\\times r}$ for the $(p_{1},p_{2})$ -th $r\\times r$ blocks, and $\\mathbf{0}_{r\\times r}$ for the other $r\\times r$ blocks. ", "page_idx": 43}, {"type": "text", "text": "Similar to the estimate in Step II and Step III in the proof of Theorem C.2, it is easy to prove the following estimates by induction. ", "page_idx": 44}, {"type": "text", "text": "If the width satisfies ", "page_idx": 44}, {"type": "equation", "text": "$$\nm\\geq\\left\\{\\tilde{\\Omega}\\left(\\underset{l\\in[K]}{\\operatorname*{max}}\\left\\Vert g_{l}\\right\\Vert_{\\mathcal{B}}^{2}\\right)=\\tilde{\\Omega}\\left(\\left\\Vert g_{K}\\right\\Vert_{\\mathcal{B}}^{2}\\right),\\right.\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\left.\\mathrm{type=lin}\\,\\,,}\\\\ {\\tilde{\\Omega}\\left(\\underset{l\\in[K]}{\\operatorname*{max}}\\left\\Vert\\log g_{l}\\right\\Vert_{\\mathcal{B}}^{2}T_{l}^{2}\\right)=\\tilde{\\Omega}\\left(\\left\\Vert\\log g_{K}\\right\\Vert_{\\mathcal{B}}^{2}T_{K}^{2}\\right),\\quad\\mathrm{type=log}\\,\\,,\\quad\\quad\\quad\\quad}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and the head number satisfies ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\{\\frac{C(n)}{H^{n}}\\sqrt{\\sum_{l=1}^{K-1}e^{0.02(n+1)T_{l}}}\\leq\\frac{1}{4\\underbrace{\\operatorname*{max}_{l\\in[K-1]}\\|g_{l}\\|_{\\mathrm{Lip}}}_{l\\in[K-1]}},\\right.\\quad\\mathrm{type=lin}}\\\\ &{\\left.\\!\\!\\frac{C(n)}{H^{n}}\\sqrt{\\sum_{l=1}^{K-1}T_{l}^{2.02(n+1)}}\\leq\\frac{1}{4\\underbrace{\\operatorname*{max}_{l\\in[K-1]}\\|\\log g_{l}\\|_{\\mathrm{Lip}}}_{l\\in[K-1]}},\\quad\\mathrm{type=log}\\right.,}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "then the following estimates hold: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 (E1) for any $2\\le l\\le K$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(l)}=\\pmb{x}_{t}^{(l-1/2)}+(\\mathbf{0}^{\\top},t_{l},\\mathbf{0}_{M-l+1}^{\\top})^{\\top};\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "\u00b7 (E2) for any $1\\le l\\le K$ \uff0c ", "page_idx": 44}, {"type": "equation", "text": "$$\nP_{\\perp}^{(l)}\\pmb{x}_{t}^{(l+1/2)}=P_{\\perp}^{(l)}\\pmb{x}_{t}^{(l)};\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "\u00b7 (E3) for any $1\\le l\\le K$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|P^{(l)}\\left(\\pmb{x}_{t}^{(l+1/2)}-\\left(\\pmb{x}_{t}^{(l)}+(\\pmb{0}_{l d}^{\\top},\\pmb{x}_{t-t_{l}}^{\\top},\\pmb{0}^{\\top})^{\\top}\\right)\\right)\\right\\|_{2}\\leq\\left\\{\\frac{C(n)e^{0.01(n+1)T_{l}}}{H^{n}},\\quad\\mathrm{type=lin~};}\\\\ &{\\qquad\\left\\|Q^{(l)}\\left(\\pmb{x}_{t}^{(l+1/2)}-\\pmb{x}_{t}^{(0)}\\right)\\right\\|_{2}\\leq\\left\\{\\frac{C(n)}{H^{n}}\\sqrt{\\sum_{j=1}^{l}e^{0.02(n+1)T_{l}}},\\quad\\mathrm{type=lin~}\\right.}\\\\ &{\\qquad\\left\\|Q^{(l)}\\left(\\pmb{x}_{t}^{(l+1/2)}-\\pmb{x}_{t}^{(0)}\\right)\\right\\|_{2}\\leq\\left\\{\\frac{C(n)}{H^{n}}\\sqrt{\\sum_{j=1}^{l}T_{j}^{2.02(n+1)}},\\quad\\mathrm{type=log~}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "The Remained Steps. ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Step $2K+2$ and $2K+3$ ", "page_idx": 44}, {"type": "text", "text": "In the similar way as Step $3\\sim$ Step $2K-1$ in this proof and Step II, Step $\\mathbf{III}$ in the proof of Theorem C.2, it is easy to verify the following estimate. ", "page_idx": 44}, {"type": "text", "text": "If the width satisifes ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{m\\geq\\left\\{\\tilde{\\Omega}\\left(\\sum_{l=K+1}^{M}\\left\\|g_{l}\\right\\|_{\\mathcal B}^{2}\\right),\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathrm{type=lin}\\right.}\\\\ {\\tilde{\\Omega}\\left(\\sum_{l=K+1}^{M}\\left\\|\\log g_{l}\\right\\|_{\\mathcal B}^{2}T_{l}^{2}\\right),\\ \\ \\ \\mathrm{type=log}}\\end{array},\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and the head number satisfies ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\displaystyle\\frac{C(n)}{H^{n}}\\sqrt{\\sum_{l=1}^{K}e^{0.02(n+1)T_{l}}}\\,\\le\\frac{1}{4\\,\\underset{l\\,\\in\\,[K]}{\\operatorname*{max}}\\,\\|g_{l}\\|_{\\mathrm{Lip}}},}&{\\mathrm{\\type=lin}}\\\\ {\\displaystyle\\frac{C(n)}{H^{n}}\\sqrt{\\sum_{l=1}^{K}T_{l}^{2.02(n+1)}}\\le\\frac{1}{4\\,\\underset{l\\,\\in\\,[K]}{\\operatorname*{max}}\\,\\|\\log g_{l}\\|_{\\mathrm{Lip}}},}&{\\mathrm{\\type=log}}\\end{array}\\right.,\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "then the following estimates hold: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(K+1)}=\\pmb{x}_{t}^{(K+1/2)}+(\\pmb{0}^{\\top},t_{K+1},\\cdots,t_{M},0)^{\\top};\n$$", "text_format": "latex", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R_{\\perp}\\pmb{x}_{t}^{(K+3/2)}=R_{\\perp}\\pmb{x}_{t}^{(K+1)};}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|{R}\\left(x_{t}^{(K+3/2)}-\\left(x_{t}^{(K+1)}+(\\mathbf{0}_{(K+1)D}^{\\top},\\mathbf{x}_{t-t_{K+1}}^{\\top},\\cdots,x_{t-t_{M}}^{\\top},\\mathbf{0}^{\\top})^{\\top}\\right)\\right)\\right\\|_{2}}\\\\ &{\\leq\\left\\{{C}(n)\\left(\\frac{\\sum_{l=K+1}^{M}e^{0.01T_{l}}}{H^{\\frac{n}{n+1}}}\\right)^{n+1},\\;\\mathrm{type=lin}\\right.}\\\\ &{\\left.\\!\\!\\!\\!C(n)\\!\\left(\\frac{\\sum_{l=K+1}^{M}T_{l}^{1.01}}{H^{\\frac{n}{n+1}}}\\right)^{n+1}\\!,\\;\\mathrm{type=log}\\right.}\\end{array},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|Q^{(M)}\\left(x_{t}^{(K+3/2)}-x_{t}^{(0)}\\right)\\right\\|_{2}}\\\\ &{\\leq\\left\\{\\frac{C(n)}{H^{n}}\\sqrt{\\sum_{l=1}^{K}e^{0.02(n+1)T_{l}}+\\left(\\sum_{l=K+1}^{M}e^{0.01T_{l}}\\right)^{2n+2}},\\;\\mathrm{type}=\\operatorname*{lim}\\right.}\\\\ &{\\leq\\left\\{\\frac{C(n)}{H^{n}}\\sqrt{\\sum_{l=1}^{K}T_{l}^{2.02(n+1)}+\\left(\\sum_{l=K+1}^{M}T_{l}^{1.01}\\right)^{2n+2}},\\;\\mathrm{type}=\\log\\right.}\\end{array}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Step $2K+4$ and the final bound. ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "In the same way as Step IV in the proof of Theorem C.2, there exists FFN, such that the following estimate holds for any $t$ and $\\mathbf{\\deltaX}$ ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|H_{t}(X)-x_{t}^{(K+2)}\\right\\|\\leq\\|f\\|_{\\mathrm{Lip}}\\cdot\\left\\|Q^{(M)}\\left(x_{t}^{(K+3/2)}-x_{t}^{(0)}\\right)\\right\\|_{2}+\\mathcal{E}_{\\mathrm{FFN}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\|f\\|_{\\mathrm{Lip}}\\cdot\\mathcal{E}_{\\mathrm{Attn}}(\\mathrm{type})+\\mathcal{E}_{\\mathrm{FFN}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\mathrm{FFN}}=\\mathcal{O}\\left(\\frac{\\Vert\\boldsymbol{f}\\Vert_{B}}{\\sqrt{m}}\\right),\n$$", "text_format": "latex", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}_{\\mathrm{Attn}}(\\mathrm{type})=\\left\\|Q^{(M)}\\left(\\pmb{x}_{t}^{(K+3/2)}-\\pmb{x}_{t}^{(0)}\\right)\\right\\|_{2}}\\\\ &{=\\left\\{\\frac{C(n)}{H^{n}}\\sqrt{\\sum_{l=1}^{K}e^{0.02(n+1)T_{l}}+\\left(\\sum_{l=K+1}^{M}e^{0.01T_{l}}\\right)^{2n+2}},\\;\\mathrm{type}=\\mathrm{lin}\\right.}\\\\ &{=\\left\\{\\frac{C(n)}{H^{n}}\\sqrt{\\sum_{l=1}^{K}T_{l}^{2.02(n+1)}+\\left(\\sum_{l=K+1}^{M}T_{l}^{1.01}\\right)^{2n+2}},\\;\\mathrm{type}=\\mathrm{log}\\right.}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Recalling our analysis, we need the head number satisfies ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\displaystyle\\frac{C(n)}{H^{n}}\\sqrt{\\sum_{l=1}^{K}e^{0.02(n+1)T_{l}}}\\leq\\frac{1}{4\\operatorname*{max}_{l\\in[K]}\\|g_{l}\\|_{\\mathrm{Lip}}},}&{\\mathrm{\\type=lin}}\\\\ {\\displaystyle\\frac{C(n)}{H^{n}}\\sqrt{\\sum_{l=1}^{K}T_{l}^{2.02(n+1)}}\\leq\\frac{1}{4\\operatorname*{max}_{l\\in[K]}\\|\\log g_{l}\\|_{\\mathrm{Lip}}},}&{\\mathrm{\\type=log~}}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Due to ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\displaystyle\\frac{C(n)}{H^{n}}\\sqrt{\\sum_{l=1}^{K}e^{0.02(n+1)T_{l}}}\\leq\\mathcal{E}_{\\mathrm{Attn}}(\\boldsymbol{\\mathrm{type}}),}&{\\,\\,\\boldsymbol{\\mathrm{type}}=\\operatorname*{lin}_{\\boldsymbol{\\mathrm{1}}}}\\\\ {\\displaystyle\\frac{C(n)}{H^{n}}\\sqrt{\\sum_{l=1}^{K}T_{l}^{2.02(n+1)}}\\leq\\mathcal{E}_{\\mathrm{Attn}}(\\boldsymbol{\\mathrm{type}}),}&{\\,\\,\\boldsymbol{\\mathrm{type}}=\\log}\\end{array}\\right.,\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "when we is large enough, this condition holds naturally and do not affect the approximation rate. ", "page_idx": 45}, {"type": "text", "text": "Moreover, we need the following condition on the width: ", "page_idx": 45}, {"type": "equation", "text": "$$\nm\\geq\\left\\{\\tilde{\\Omega}\\left(\\underset{l\\in[K]}{\\operatorname*{max}}\\vee\\sum_{l=K+1}^{M}\\left\\Vert g_{l}\\right\\Vert_{\\mathcal{B}}^{2}\\right),\\quad\\qquad\\quad\\mathrm{type=lin}\\atop\\tilde{\\Omega}\\left(\\underset{l\\in[K]}{\\operatorname*{max}}\\vee\\sum_{l=K+1}^{M}\\left\\Vert\\log g_{l}\\right\\Vert_{\\mathcal{B}}^{2}T_{l}^{2}\\right),\\quad\\mathrm{type=log}\\atop{\\mathrm{type=log}},\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "\u00b7 Regime $M=K$ ", "page_idx": 46}, {"type": "text", "text": "Step $2K+2$ and the final bound. ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "In the same way as Step IV in the proof of Theorem C.2, there exists FFN, such that the following estimate holds for any $t$ and $\\mathbf{\\deltaX}$ ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbf{H}_{t}(\\boldsymbol{X})-\\boldsymbol{x}_{t}^{(K+1)}\\right\\|\\leq\\left\\|\\boldsymbol{f}\\right\\|_{\\mathrm{Lip}}\\cdot\\left\\|\\boldsymbol{Q}^{(M)}\\left(\\boldsymbol{x}_{t}^{(K+1/2)}-\\boldsymbol{x}_{t}^{(0)}\\right)\\right\\|_{2}+\\mathcal{E}_{\\mathrm{FFN}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\left\\|\\boldsymbol{f}\\right\\|_{\\mathrm{Lip}}\\cdot\\mathcal{E}_{\\mathrm{Attn}}(\\boldsymbol{\\mathrm{type}})+\\mathcal{E}_{\\mathrm{FFN}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\mathrm{FFN}}=\\mathcal{O}\\left(\\frac{\\Vert\\boldsymbol{f}\\Vert_{B}}{\\sqrt{m}}\\right),\n$$", "text_format": "latex", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{E}_{\\mathrm{Attn}}(\\mathrm{type})=\\left\\lVert Q_{M}\\left(\\pmb{x}_{t}^{(K+1/2)}-\\pmb{x}_{t}^{(0)}\\right)\\right\\rVert_{2}}\\\\ &{=\\left\\{\\frac{C(n)}{H^{n}}\\sqrt{\\sum_{l=1}^{K}e^{0.02(n+1)T_{l}}},\\mathrm{~type=\\operatorname*{lin}}\\right.}\\\\ &{\\quad\\left.\\frac{C(n)}{H^{n}}\\sqrt{\\sum_{l=1}^{K}T_{l}^{2.02(n+1)}},\\mathrm{~type=\\log}\\right.}\\end{array}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Recalling our analysis, we need the head number satisfies ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\displaystyle\\frac{C(n)}{H^{n}}\\sqrt{\\sum_{l=1}^{K-1}e^{0.02(n+1)T_{l}}}\\leq\\frac{1}{4\\operatorname*{max}_{l\\in[K-1]}\\lVert g_{l}\\rVert_{\\mathrm{Lip}}},}&{\\mathrm{~type=lin~}}\\\\ {\\displaystyle\\frac{C(n)}{H^{n}}\\sqrt{\\sum_{l=1}^{K-1}T_{l}^{2.02(n+1)}}\\leq\\frac{1}{4\\operatorname*{max}_{l\\in[K-1]}\\lVert\\log g_{l}\\rVert_{\\mathrm{Lip}}},}&{\\mathrm{~type=log~}}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Due to ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\frac{C(n)}{H^{n}}\\sqrt{\\sum_{l=1}^{K-1}e^{0.02(n+1)T_{l}}}\\leq\\mathcal{E}_{\\mathrm{Attn}}(\\boldsymbol{\\mathrm{type}}),}&{\\ \\boldsymbol{\\mathrm{type}}=\\operatorname*{lin}}\\\\ {\\frac{C(n)}{H^{n}}\\sqrt{\\sum_{l=1}^{K-1}T_{l}^{2.02(n+1)}}\\leq\\mathcal{E}_{\\mathrm{Attn}}(\\boldsymbol{\\mathrm{type}}),}&{\\ \\boldsymbol{\\mathrm{type}}=\\log}\\end{array}\\right.,\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "when $H$ is large enough, this condition holds naturally and do not affect the approximation rate. ", "page_idx": 46}, {"type": "text", "text": "Moreover, we need the following condition on the width: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r}{m\\geq\\left\\{\\begin{array}{l l}{\\tilde{\\Omega}\\left(\\underset{l\\in[K]}{\\operatorname*{max}}\\,\\|g_{l}\\|_{\\mathcal B}^{2}\\right),}&{\\mathrm{\\type=lin}}\\\\ {\\tilde{\\Omega}\\left(\\underset{l\\in[K]}{\\operatorname*{max}}\\,\\|\\log g_{l}\\|_{\\mathcal B}^{2}\\,T_{l}^{2}\\right),}&{\\mathrm{\\type=log}}\\end{array}\\right.,}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Combining these two regimes, we complete our proof. ", "page_idx": 46}, {"type": "text", "text": "D.2  Proof of Proposition 4.5 ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Proof of Proposition 4.5. ", "page_idx": 47}, {"type": "text", "text": "This proposition is a direct corollary of Theorem 4.4. It can be seen as a special case of $M=K$ in Theorem 4.4. ", "page_idx": 47}, {"type": "text", "text": "Therefore, under the same conditions, there exists a $K\\,+\\,1.$ -layer Transformer TF E $\\mathcal{T F}_{(K+1,H,m)}^{\\mathrm{NF,type}}$ (12) and a constant $C(n)$ such that if the width satisfes ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r}{m\\geq\\left\\{\\tilde{\\Omega}\\Big(\\underset{i\\in[K]}{\\operatorname*{max}}\\,\\|g_{i}\\|_{\\mathcal B}^{2}\\Big),\\right.\\quad\\quad\\quad\\quad\\quad\\mathrm{type=\\operatorname*{lim}},}\\\\ {\\tilde{\\Omega}\\Big(\\underset{i\\in[K]}{\\operatorname*{max}}\\,\\|\\log g_{i}\\|_{\\mathcal B}^{2}\\,T_{i}^{2}\\Big),\\quad\\mathrm{type=\\log}}\\end{array},\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "then the following approximation rate holds: ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\lVert\\mathbf{H}-\\mathbf{T}\\mathbf{F}\\rVert\\leq\\mathcal{E}_{\\mathrm{FFN}}+\\lVert f\\rVert_{\\mathrm{Lip}}\\,\\mathcal{E}_{\\mathrm{Attn}}(\\mathrm{type}),\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Wwhere $\\begin{array}{r}{\\mathcal{E}_{\\mathrm{FFN}}=\\tilde{\\mathcal{O}}\\left(\\frac{\\|f\\|_{B}}{\\sqrt{m}}\\right)}\\end{array}$ and", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\mathrm{Attn}}(\\mathrm{type})=\\left\\{\\mathcal{O}\\left(\\frac{C(n)}{H^{n}}\\sqrt{\\sum_{l=1}^{K}e^{0.02(n+1)T_{l}}}\\right),\\:\\mathrm{type}=\\mathrm{lin}\\quad\\right.}\\\\ {\\mathcal{O}\\left(\\frac{C(n)}{H^{n}}\\sqrt{\\sum_{l=1}^{K}T_{l}^{2.02(n+1)}}\\right),\\:\\mathrm{type}=\\mathrm{log}\\quad\\right.}\\end{array}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Comparison between Proposition 4.5 and Theorem 4.1. ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "We compare 2-layer Transformer and $M+1$ -layer Transformer regarding the requirement of the number of heads and width. ", "page_idx": 47}, {"type": "text", "text": "The required width of FFN layers. - For 2-layer Transformer, the required width of FFN layers $m_{\\mathrm{need}}^{(2)}$ is proportionally linked to the sum of all the memory functions? complexity: ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{m}_{\\mathrm{need}}^{(2)}=\\left\\{\\tilde{\\Omega}\\Big(\\underset{i\\in[K]}{\\sum}\\;\\|g_{i}\\|_{\\mathcal B}^{2}\\Big),\\right.\\quad\\quad\\quad\\quad\\mathrm{type=lin,}}\\\\ {\\tilde{\\Omega}\\Big(\\underset{i\\in[K]}{\\sum}\\;\\|\\mathrm{log}\\;g_{i}\\|_{\\mathcal B}^{2}\\,T_{i}^{2}\\Big),\\quad\\mathrm{type=log}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "-For $M+1$ -layer Transformer, the required width of FFN layers $m_{\\mathrm{need}}^{(M+1)}$ correlates with the maximum complexity of the memory functions: ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r}{m_{\\mathrm{need}}^{(M+1)}=\\left\\{\\tilde{\\Omega}\\!\\left(\\underset{i\\in[K]}{\\operatorname*{max}}\\left\\Vert g_{i}\\right\\Vert_{\\mathcal{B}}^{2}\\right)\\!,\\quad\\quad\\quad\\quad\\quad\\quad\\mathrm{type=lin,}\\right.}\\\\ {\\tilde{\\Omega}\\!\\left(\\underset{i\\in[K]}{\\operatorname*{max}}\\left\\Vert\\log g_{i}\\right\\Vert_{\\mathcal{B}}^{2}T_{i}^{2}\\right)\\!,\\quad\\mathrm{type=log}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "It is easy to see that: ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{m_{\\mathrm{need}}^{(M+1)}}{m_{\\mathrm{need}}^{(2)}}=\\frac{\\displaystyle\\operatorname*{max}\\{a_{1},\\cdots,a_{M}\\}}{\\sum_{k=1}^{M}a_{k}},}\\\\ &{\\displaystyle\\operatorname*{max}\\{a_{1},\\cdots,a_{M}\\}\\leq\\sum_{k=1}^{M}a_{k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "\u00b7 The required number of Attn heads. To achieve the same $\\mathcal{E}_{\\mathrm{Attn}}(\\mathrm{type})=\\epsilon$ ", "page_idx": 47}, {"type": "text", "text": "-for 2-layerTransformer, the required nmberof Atheads H satisfies: ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\epsilon=\\left\\{\\mathcal{O}\\left(\\frac{C(n)}{\\left(H_{\\mathrm{necd}}^{(2)}\\right)^{n}}\\sqrt{\\sum_{l=1}^{K}e^{0.02(n+1)T_{l}}}\\right),\\;\\mathrm{type}=\\operatorname*{lin}\\quad.\\right.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "- for M + 1-layer Transformer, the required number of Attn heads HM+1) satisfies: ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\epsilon=\\left\\{\\begin{array}{l l}{\\mathcal{O}\\left(\\frac{C(n)}{\\left(H_{\\mathrm{ned}}^{(M+1)}\\right)^{n}}\\left(\\sum_{i=1}^{M}e^{0.01T_{i}}\\right)^{n+1}\\right),}&{\\mathrm{~type=lin~}}\\\\ {\\mathcal{O}\\left(\\frac{C(n)}{\\left(H_{\\mathrm{ned}}^{(M+1)}\\right)^{n}}\\left(\\sum_{i=1}^{M}T_{i}^{1.01}\\right)^{n+1}\\right),}&{\\mathrm{~type=log~}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "It is easy to see that: ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\left(\\frac{{\\cal H}_{\\mathrm{need}}^{(M+1)}}{{\\cal H}_{\\mathrm{need}}^{(2)}}\\right)^{2n}=\\frac{b_{1}^{2}+\\cdot\\cdot+b_{M}^{2}}{(b_{1}+\\cdot\\cdot\\cdot b_{M})^{2}},}\\\\ &{}&{b_{1}^{2}+\\cdot\\cdot\\cdot+b_{M}^{2}\\le(b_{1}+\\cdot\\cdot\\cdot b_{M})^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "This finding suggests that increased depth can significantly reduce the demands on the number of heads and the width. The underlying reason is that deep networks can distribute memories across different layers for processing, with each layer focusing on approximating only a single memory function. ", "page_idx": 48}, {"type": "text", "text": "E Proof of Section 5 ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "E.1 Proof of Theorem 5.1 ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "In this subsection, we give the detailed proofs of the warm-up case of (fixed) essentially sparse memories as follows: ", "page_idx": 49}, {"type": "equation", "text": "$$\ny_{t}=f\\left(\\left(\\pmb{X}*\\rho_{1}\\right)(t),\\cdot\\cdot\\cdot,\\left(\\pmb{X}*\\rho_{M}\\right)(t)\\right),\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Wwhere $\\rho_{1}(\\cdot),\\cdot\\cdot\\cdot\\,,\\rho_{M}(\\cdot)\\,\\in\\,\\ell^{1}(\\mathbb{N})$ serve as memory kermes, and $\\begin{array}{r}{(X\\ast\\rho_{k})(t)=\\sum_{s=0}^{+\\infty}x_{t-s}\\rho_{k}(s)}\\end{array}$ denotes the convolution of the inputs with kernel $\\rho_{k}$ ", "page_idx": 49}, {"type": "text", "text": "Theorem E.1 (Restatement of Theorem 5.1). ", "page_idx": 49}, {"type": "text", "text": "(A) Consider $\\mathcal{H}^{\\mathrm{Ess}}$ (14) with exponentially decayed memory kernels, i.e., there exists $\\beta>0$ such that $\\rho_{1}(t),\\cdot\\cdot\\cdot\\mathrm{\\Phi},\\rho_{M}(t)=\\mathcal{O}(e^{-\\beta t})$ Then for any target $\\mathbf{H}\\in\\dot{\\mathcal{H}}^{\\mathrm{Ess}}$ ,rate $n\\in[\\lfloor99\\beta\\rfloor]$ ,and $H,m\\in\\mathbb{N}_{+}$ there exists a 1-layer $D P$ free Transformer $\\mathbf{TF}\\in\\mathcal{T F}_{(1,H,m)}^{\\mathrm{DPF,exp}}$ (7) and a constant $C(n)$ such that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\lVert\\mathbf{H}-\\mathbf{T}\\mathbf{F}\\rVert\\leq\\mathcal{E}_{\\mathrm{FFN}}+\\left\\lVert f\\right\\rVert_{\\mathrm{Lip}}\\mathcal{E}_{\\mathrm{Attn}}(\\mathrm{type});\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "$(B)$ Consider $\\mathcal{H}^{\\mathrm{Ess}}$ (14) with polynomially decayed memory kernels, i.e., there exists $\\beta>1$ such that $\\rho_{1}(t),\\cdot\\cdot\\cdot\\;,\\rho_{M}(t)\\,=\\,\\mathcal{O}(\\dot{t}^{-\\beta})$ .Thenforanytarget $\\mathbf{H}\\in\\mathcal{H}^{\\mathrm{Ess}}$ rate $n\\,\\in\\,[\\lfloor0.99\\beta\\rfloor\\,-\\,1]$ and $H,m\\in\\mathbb{N}_{+}$ ,therexists a1-layer $D P$ -freeTransformer $\\mathbf{TF}\\in{\\mathcal{T F}}_{(1,H,m)}^{\\mathrm{DPF,poly}}$ FDPE:poy (7)anda constant C(n) such that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\lVert\\mathbf{H}-\\mathbf{T}\\mathbf{F}\\rVert\\leq\\mathcal{E}_{\\mathrm{FFN}}+\\left\\lVert f\\right\\rVert_{\\mathrm{Lip}}\\mathcal{E}_{\\mathrm{Attn}}(\\mathrm{type});\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{E}_{\\mathrm{FFN}}=\\tilde{\\mathcal{O}}\\left(\\frac{\\|f\\|_{B}}{\\sqrt{m}}\\right)}\\end{array}$ and ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathcal E_{\\mathrm{Attn}}(\\sf t y p e)=\\mathcal O\\left(\\frac{C(n)M^{n+1}}{H^{n}}\\right).\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Proof of Theorem E.1. ", "page_idx": 49}, {"type": "text", "text": "The proof of this theorem is highly similar to the proof of Theorem B.1. The only difference is that the Attn layer needs to be used to approximate general memory kernel $\\rho_{k}(\\cdot)$ insteadof simple $\\mathbb{I}\\{\\cdot=T_{k}\\}$ . But for the completeness of the proof in this section, we still provide the detailed proof. ", "page_idx": 49}, {"type": "text", "text": "First, we choose the embedding dimension $D\\,=\\,M d$ and select the simple embedding $W_{E}\\,=$ $(I_{d\\times d},\\mathbf{0})^{\\top}\\in\\mathbb{R}^{D\\times d}$ \uff0c $\\mathbf{\\boldsymbol{b}}_{E}=\\mathbf{0}\\in\\mathbf{\\check{R}}^{D}$ ", "page_idx": 49}, {"type": "text", "text": "For any input sequence $X=({\\mathbf{\\boldsymbol{x}}}_{t})_{t\\in\\mathbb{Z}}$ , the token after embedding satisfies: ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{x}_{t}^{E}=W_{E}\\pmb{x}_{t}+\\pmb{b}_{E}=(\\pmb{x}_{t}^{\\top},\\pmb{0}^{\\top})^{\\top}\\in\\mathbb{R}^{D}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Then for one-layer Dot-product-free Transformer $\\mathbf{TF}\\in{\\mathcal{T F}}_{(1,H,m)}^{\\mathrm{DPF,type}}$ FDPF,type without residual blocks, the output token $\\mathbf{TF}_{t}(X)$ of $t$ -th input token $\\pmb{x}_{t}$ satisfies: ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\pmb{x}_{t}^{(1/2)}=\\pmb{W}_{O}^{(1)}\\sum_{h=1}^{H}\\mathbf{Attn}_{t}^{(1,h)}(\\pmb{X}^{(0)}),}\\\\ {\\pmb{x}_{t}^{(1)}=\\mathbf{F}\\mathbf{F}\\mathbf{N}^{(1)}(\\pmb{x}_{t}^{(1/2)})}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathbf{Attn}_{t}^{(1,h)}(X)=W_{V}^{(1,h)}\\sum_{s=0}^{+\\infty}\\frac{x_{t-s}\\exp\\left(p^{(1,h)}\\phi_{\\mathrm{type}}(s)\\right)}{\\sum_{j=0}^{+\\infty}\\exp\\left(p^{(1,h)}\\phi_{\\mathrm{type}}(j)\\right)}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "This proof can be summarized as the following process: ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\cdots\\quad x_{t}^{E}\\quad\\cdots.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Step I. Attn layer $\\downarrow$ ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{l r}{\\cdot\\cdot\\cdot}&{x_{t}^{(1/2)}\\approx((X*\\rho_{1})(t),\\cdots,(X*\\rho_{M})(t))^{\\top}}&{\\cdots}\\\\ {\\mathrm{II.\\,FFN\\,layer}\\;\\downarrow}\\\\ {\\cdot\\cdot\\cdot}&{x_{t}^{(1)}\\approx f\\,((X*\\rho_{1})(t),\\cdots,(X*\\rho_{M})(t))}&{\\cdots}\\end{array}}\\end{array}\n$$Step ", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Now we give the formal proof. ", "page_idx": 50}, {"type": "text", "text": "Step I. Extract the memory locations by (Dot-product-free) Attn layer. ", "page_idx": 50}, {"type": "text", "text": "We consider to use $H_{k}$ attention heads (from $\\textstyle\\sum_{i=1}^{k-1}H_{i}+1$ th head to $\\textstyle\\sum_{i=1}^{k}H_{i}$ th head) to extraet it, and it satisfies to $\\textstyle\\sum_{k=1}^{M}H_{k}=H$ ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad\\qquad\\quad P^{(k)}:=\\bigl(\\mathbf{0}_{d\\times(k-1)d}\\quad{\\cal I}_{d\\times d}\\quad\\mathbf{0}\\bigr)\\in\\mathbb{R}^{d\\times D},\\quad1\\le k\\le M.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathbf{0}}\\\\ &{P_{\\perp}^{(k)}:=\\left(\\!\\!\\begin{array}{c c c c}{\\!\\!\\boldsymbol{I}_{(k-1)d\\times(k-1)d}\\!\\!}&{\\!\\!\\mathbf{0}_{d\\times d}\\quad\\!\\!}&{\\!\\!\\mathbf{0}}\\\\ {\\mathbf{0}\\quad\\!\\!}&{\\!\\!\\mathbf{0}_{d\\times d}\\quad\\!\\!}&{\\!\\!\\boldsymbol{I}_{(M-k-1)d\\times(M-k-1)d}\\!\\!}\\end{array}\\!\\!\\right)\\in\\mathbb{R}^{(M-1)d\\times D},\\quad1\\le k\\le M.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Now we consider the extraction of $k$ -th memory $(X*\\rho_{k})(t)\\;(1\\leq k\\leq M)$ ", "page_idx": 50}, {"type": "text", "text": "\u00b7 Case (A). Approximating exponentially decayed memories by $\\mathtt{t y p e}=\\operatorname*{lin}$ ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Because there exists $\\beta>0$ such that $\\rho_{k}(t)=O(e^{-\\beta t})$ , by Lemma F.3, for any $n\\in[|99\\beta|]$ and $m\\in\\mathbb{N}_{+}$ , there exists an absolute constant $C(n)$ only depending on $n$ and a function ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\phi_{k}^{\\mathrm{exp}}(t)=\\sum_{\\sum_{i=1}^{k-1}H_{i}+1\\le h\\le\\sum_{i=1}^{k}H_{i}}\\alpha_{h}e^{-\\beta_{h}t}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "such that $\\beta_{h}>0$ and ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\left\\|\\rho_{k}(\\cdot)-\\phi_{k}^{\\mathrm{exp}}(\\cdot)\\right\\|_{\\ell_{1}(\\mathbb{N})}=\\sum_{s=0}^{+\\infty}|\\rho_{k}(s)-\\phi_{k}^{\\mathrm{exp}}(s)|\\leq\\frac{C(n)}{m^{n}}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Therefore, for these attention heads $\\begin{array}{r}{(\\sum_{i=1}^{k-1}H_{i}+1\\le h\\le\\sum_{i=1}^{k}H_{i})}\\end{array}$ , we can choose ", "page_idx": 50}, {"type": "equation", "text": "$$\np^{(1,h)}=\\beta_{h},\\quad{\\pmb W}_{V}^{(1,h)}=\\alpha_{h}\\,\\left(\\sum_{j=0}^{+\\infty}\\exp(-\\beta_{h}j)\\right)\\delta_{(k,1)}^{d\\times d},\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where $\\pmb{\\delta}^{(k,1)}\\in\\mathbb{R}^{D\\times D}$ means that: it equals to $\\pmb{I}_{d\\times d}$ for the $(k,1)$ -th $d\\times d$ blocks, and ${\\mathbf{0}}_{d\\times d}$ for the other $d\\times d$ blocks. ", "page_idx": 50}, {"type": "text", "text": "Then it holds that: ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\underset{h=\\sum_{i=1}^{k-1}H_{i}+1}{\\sum_{i=1}^{k}}\\mathbf{A}\\mathbf{t}\\mathbf{t}\\mathbf{n}_{t}^{(1,h)}(\\boldsymbol{X}^{(0)})=\\underset{h=\\sum_{i=1}^{k-1}H_{i}+1}{\\sum_{i=1}^{k}}\\alpha_{h}\\sum_{s=0}^{+\\infty}e^{-\\beta_{h}s}\\left(\\mathbf{\\widehat{\\alpha}}_{\\mathbf{\\widehat{\\alpha}}_{t-s}}^{\\mathbf{0}_{(k-1)d}}\\right)\\in\\mathbb{R}^{D},\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "This implies: ", "page_idx": 50}, {"type": "equation", "text": "$$\nP^{(k)}\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}\\mathbf{A}\\mathbf{t}\\mathbf{n}_{t}^{(1,h)}(X^{(0)})=\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}\\alpha_{h}\\sum_{s=0}^{+\\infty}e^{-\\beta_{h}s}\\pmb{x}_{t-s},\n$$", "text_format": "latex", "page_idx": 50}, {"type": "equation", "text": "$$\n{\\pmb P}_{\\perp}^{(k)}\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}{\\bf A t t n}_{t}^{(1,h)}({\\pmb X}^{(0)})={\\bf0},\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "moreover, the following estimate holds: ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\begin{array}{c}{\\displaystyle\\sum_{k=1}^{\\lfloor\\frac{\\gamma+1}{\\gamma}\\rfloor\\,H_{t}}\\quad P^{(k)}\\mathrm{Attm}_{1}^{(1,k)}(X^{(0)})-(X+\\rho_{k})(t)\\right\\|_{2}}\\\\ {\\displaystyle\\sum_{k=1}^{\\lfloor\\frac{\\gamma+1}{\\gamma}\\rfloor\\,H_{t}+1}\\quad\\;+\\frac{\\gamma\\,\\Delta\\,}{2}\\,}\\\\ {\\displaystyle\\prod_{h=\\sum_{i=1}^{\\lfloor\\frac{\\gamma}{\\gamma}\\rfloor}\\,H_{t}+1}\\quad\\alpha_{h}\\sum_{s=0}^{\\lfloor\\frac{\\gamma}{\\gamma}\\rfloor\\,s}x_{t}\\,x_{t-s}-\\sum_{s=0}^{\\lfloor\\frac{\\gamma}{\\gamma}\\rfloor\\,x_{t-s}\\,\\beta_{h}(s)}\\Bigg\\|_{2}}\\\\ {\\displaystyle\\prod_{s=0}^{\\lfloor\\frac{\\gamma}{\\gamma}\\rfloor}\\quad\\left(\\frac{\\sum_{s=1}^{\\lfloor\\frac{\\gamma}{\\gamma}\\rfloor\\,H_{t}}}{\\sum_{s=0}^{\\lfloor\\frac{\\gamma}{\\gamma}\\rfloor-1}\\,H_{t}+1}\\;\\alpha_{h}e^{-\\beta_{h}s}-\\mathbb{I}\\left\\{s=T_{k}\\right\\}\\right)\\,x_{t-s}\\Bigg\\|_{2}}\\\\ {\\displaystyle\\leq\\sum_{s=0}^{\\lfloor\\frac{\\gamma}{\\gamma}\\rfloor}\\left\\vert\\sum_{s=\\sum_{i=1}^{\\lfloor\\frac{\\gamma}{\\gamma}\\rfloor}\\,H_{s}+1}^{\\sum{\\lfloor\\frac{\\gamma}{\\gamma}\\rfloor}\\,H_{t}}\\;\\alpha_{h}e^{-\\beta_{h}s}-\\rho_{k}(s)\\right\\vert}\\\\ {\\displaystyle\\left\\vert\\alpha_{h}^{\\gamma}\\mathrm{An}_{\\alpha}^{\\gamma}\\right\\vert-\\rho_{k}(\\cdot)\\sum_{t=1}^{\\lfloor\\frac{\\gamma}{\\gamma}\\rfloor}\\left\\{\\phantom{\\frac{b}{b}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}_{t}\\left\\vert\\nabla_{t}(s)\\right\\vert\\leq\\frac{C(s)}{H_{t}}\\right\\vert.}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "\u00b7 Case (B). Approximating polynomially decayed memories by $\\mathtt{t y p e}=\\mathtt{l o g}$ ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Because there exists $\\beta~>~0$ such that $\\rho_{k}(t)\\,=\\,\\mathcal{O}(t^{-\\beta})$ , by Lemma F.6, for any $n\\ \\in$ $\\left[\\left[0.99\\beta\\right]-1\\right]$ and $m\\in\\mathbb{N}_{+}$ , there exists an absolute constant $C(n)$ only depending on $n$ and a function ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\phi_{k}^{\\mathrm{poly}}(t)=\\sum_{\\sum_{i=1}^{k-1}H_{i}+1\\le h\\le\\sum_{i=1}^{k}H_{i}}\\alpha_{h}t^{-\\beta_{h}}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "such that $\\beta_{h}>1$ and ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\Big\\|\\rho_{k}(\\cdot)-\\phi_{k}^{\\mathrm{poly}}(\\cdot)\\Big\\|_{\\ell_{1}(\\mathbb{N}_{+})}=\\sum_{s=1}^{+\\infty}\\Big|\\rho_{k}(s)-\\phi_{k}^{\\mathrm{poly}}(s)\\Big|\\leq\\frac{C(n)}{m^{n}}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Therefore, for these attention heads $\\begin{array}{r}{(\\sum_{i=1}^{k-1}H_{i}+1\\le h\\le\\sum_{i=1}^{k}H_{i})}\\end{array}$ we can choose ", "page_idx": 51}, {"type": "equation", "text": "$$\np^{(1,h)}=\\beta_{h},\\quad W_{V}^{(1,h)}=\\alpha_{h}\\left(\\sum_{j=1}^{+\\infty}j^{-\\beta_{h}}\\right)\\delta^{(k,1)},\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Where $\\pmb{\\delta}^{(k,1)}\\in\\mathbb{R}^{D\\times D}$ means that: it equals to $\\pmb{I}_{d\\times d}$ for the $(k,1)$ h $d\\times d$ blocks, and ${\\mathbf{0}}_{d\\times d}$ for the other $d\\times d$ blocks. ", "page_idx": 51}, {"type": "text", "text": "Then it holds that: ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}\\mathbf{A}\\mathbf{t}\\mathbf{n}_{t}^{(1,h)}(\\pmb{X}^{(0)})=\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}\\alpha_{h}\\sum_{s=1}^{+\\infty}s^{-\\beta_{h}}\\left(\\!\\!\\begin{array}{c}{\\mathbf{0}_{(k-1)d}}\\\\ {\\mathbf{x}_{t-s}}\\\\ {\\mathbf{0}}\\end{array}\\!\\!\\right),\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "This implies: ", "page_idx": 51}, {"type": "equation", "text": "$$\nP^{(k)}\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}\\mathbf{A}\\mathbf{t}\\mathbf{n}_{t}^{(1,h)}(X^{(0)})=\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}\\alpha_{h}\\sum_{s=1}^{+\\infty}s^{-\\beta_{h}}\\mathbf{x}_{t-s},\n$$", "text_format": "latex", "page_idx": 51}, {"type": "equation", "text": "$$\nP_{\\perp}^{(k)}\\sum_{h=\\sum_{i=1}^{k-1}H_{i}+1}^{\\sum_{i=1}^{k}H_{i}}\\mathbf{Attn}_{t}^{(1,h)}(X^{(0)})=\\mathbf{0},\n$$", "text_format": "latex", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\begin{array}{c}{\\displaystyle\\sum_{k=1}^{\\infty+n}\\boldsymbol{H}_{\\neq}^{(k)}\\mathbf{A}\\mathbf{H}\\mathbf{H}_{\\neq}^{(1,k)}\\big({\\mathbf{X}}^{(0)}\\big)-({\\mathbf{X}}+{\\boldsymbol{\\rho}}_{k})(t){\\bigg\\|}}\\\\ {\\displaystyle\\sum_{k=1}^{\\infty+n}\\boldsymbol{H}_{\\neq}^{(k)}+1}\\\\ {\\displaystyle\\sum_{h=1}^{\\infty+n}\\boldsymbol{H}_{\\neq}^{(k)}\\mathbf{A}_{h}^{\\star}\\mathbf{\\Lambda}_{\\times=1}^{+\\infty}\\cdots\\sum_{n=0}^{+\\infty}\\boldsymbol{\\mu}_{t}\\mathbf{\\Lambda}_{h}(s){\\bigg\\|}_{2}}\\\\ {\\displaystyle\\int_{\\Omega}\\mathbf{i}\\times\\left(\\sum_{h=1}^{\\infty+n}\\boldsymbol{H}_{h}^{(k)}\\right.}\\\\ {\\displaystyle-1\\Bigg)\\left\\{\\sum_{i=1}^{\\infty}\\left(\\sum_{h=2}^{\\infty+n}\\boldsymbol{H}_{h}+h^{\\alpha}\\mathcal{I}_{h}-\\rho_{h}(s)\\right)\\mathbf{x}_{t-*}\\right\\}_{2}}\\\\ {\\displaystyle\\leq\\sum_{s=1}^{+\\infty}\\left|\\sum_{h=2}^{\\infty+n}\\boldsymbol{H}_{\\neq}\\right.}\\\\ {\\displaystyle}\\\\ &{\\left.\\quad\\times\\int_{s=1}^{+\\infty}\\left|\\sum_{h=2}^{\\infty+n}\\boldsymbol{H}_{H}+\\boldsymbol{\\alpha}_{h}\\mathbf{x}^{-\\beta_{h}}-\\rho_{h}(s)\\right|}\\\\ {\\displaystyle=\\left\\|\\phi_{h}^{\\alpha_{h}}\\mathbf{A}^{\\star}\\right\\|-\\rho_{h}(s)\\right\\|_{6}}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Then we combine the estimate for all $k\\in[M]$ for these two cases. By choose $W_{O}=I_{D}$ , we have: ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|{\\mathbf{x}_{t}^{(1/2)}-\\left(\\begin{array}{c}{(X*\\rho_{t})(t)}\\\\ {\\vdots}\\\\ {(X*\\rho_{t})(t)}\\end{array}\\right)}\\right\\|_{2}}\\\\ &{=\\left\\|{\\mathbf{\\hat{k}}_{\\parallel}^{M}\\left(\\begin{array}{c}{\\frac{\\sum_{i=1}^{K}I_{i}}{\\sum_{i=1}^{K}\\bigg(h_{i}}}\\\\ {h_{i}\\sum_{i=1}^{K}\\bigg(h_{i}}\\end{array}\\right)+({\\mathbf{k}_{\\parallel}^{(1,k)}(X\\!)}-\\left(\\begin{array}{c}{0({t}-1)d}\\\\ {(X*\\rho_{t})(t)}\\end{array}\\right)}\\right)\\right\\|_{2}}\\\\ &{\\leq\\displaystyle{\\sum_{k=1}^{M}\\left\\|{\\mathbf{\\hat{k}}_{\\parallel}^{\\sum_{i=1}^{K}\\rho_{k}}\\mathbf{\\hat{k}}_{\\parallel}\\mathbf{t}({\\mathbf{t}},{\\mathbf{t}}^{(1,k)}(X\\!)}-\\left(\\begin{array}{c}{0({t}-1)d}\\\\ {\\vdots}\\\\ {(X*\\rho_{t})(t)}\\end{array}\\right)}\\right\\|_{2}}\\\\ &{=\\displaystyle{\\sum_{k=1}^{M}\\left\\|{\\frac{\\sum_{i=1}^{K}I_{i}}{\\sum_{i=1}^{K}\\rho_{k}}+P^{(k)}\\mathbf{A}\\mathbf{t}\\mathbf{t}({\\mathbf{t}}_{i}^{(1,k)}(X^{(0)})-(X*\\rho_{k})(t)}\\right\\|_{2}}}\\\\ &{\\leq\\xi_{k\\operatorname*{min}}=\\displaystyle{\\sum_{k=1}^{M}\\frac{C_{k}^{(1)}}{\\prod_{i=1}^{K}}\\left\\{\\begin{array}{c}{0}\\\\ {\\vdots}\\\\ {0}\\end{array}\\right\\}}\\quad\\mathrm{for}\\ b o t h{\\mathbf{\\hat{C}}}\\mathbf{\\hat{s}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Consequently, one detal s to assign the head number $\\{H_{k}\\}_{k=1}^{M}$ such that th eror's sum $\\mathcal{E}_{\\mathrm{Attn}}(\\tt t y p e)$ is as small as possible. Here, we simply choose the same $H_{k}$ ", "page_idx": 52}, {"type": "equation", "text": "$$\nH_{k}=\\frac{H}{M},\\quad k\\in[M].\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Thus, we obtain the bound in Step I: ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\mathrm{Attn}}=\\sum_{k=1}^{M}\\frac{C(n)}{H_{k}^{n}}=\\frac{C(n)M^{n+1}}{H^{n}}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Furthermore, by choosing $\\mathcal{E}_{\\mathrm{Attn}}\\leq1$ , it holds that ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\left\\|x_{t}^{(1/2)}\\right\\|_{\\infty}\\leq\\left\\|x_{t}^{(1/2)}-\\left(\\binom{(X*\\rho_{1})(t)}{\\vdots}\\right)\\right\\|_{\\infty}+\\left\\|\\binom{\\left(X*\\rho_{1}\\right)(t)}{\\vdots}\\right\\|_{\\infty}\\right\\|_{\\infty}\\leq\\mathcal{E}_{\\mathrm{Attn}}+1\\leq2.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Step II. Approximate the readout function by FFN layer. ", "page_idx": 53}, {"type": "text", "text": "In this step, we aim to approximate the function $f$ using two-layer network. By Lemma G.6, there exists a two-layer neural network with $m$ neurons defined on $\\mathbb{R}^{\\check{D}}$ ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathrm{FFN}^{(1)}(\\pmb{y})=\\sum_{k=1}^{m}a_{k}\\sigma(\\pmb{b}_{k}^{\\top}\\pmb{y}+c_{k})\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "such that ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathcal E_{\\mathrm{FFN}}:=\\left\\|\\mathrm{FFN}^{(1)}-f\\right\\|_{L^{\\infty}([-2,2]^{D})}\\leq\\tilde{\\mathcal O}\\left(\\frac{\\|f\\|_{\\mathcal B}}{\\sqrt{m}}\\right).\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "The final bound. ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "For any $t$ and $X\\in\\mathcal{X}$ , it holds that ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\vert\\mathbf{H}_{t}(X)-x_{t}^{(1)}\\right\\vert\\right\\vert=\\left\\vert f((X*\\rho_{1})(t),\\cdots,(X*\\rho_{M})(t))-\\mathrm{FFN}^{(1)}\\left(x_{t}^{(1/2)}\\right)\\right\\vert}\\\\ &{=\\left\\vert f((X*\\rho_{1})(t),\\cdots,(X*\\rho_{M})(t))-f\\left(x_{t}^{(1/2)}\\right)+f\\left(x_{t}^{(1/2)}\\right)-\\mathrm{FFN}^{(1)}\\left(x_{t}^{(1/2)}\\right)\\right\\vert}\\\\ &{\\le\\left\\vert f((X*\\rho_{1})(t),\\cdots,(X*\\rho_{M})(t))-f\\left(x_{t}^{(1/2)}\\right)\\right\\vert+\\left\\vert f\\left(x_{t}^{(1/2)}\\right)-\\mathrm{FFN}^{(1)}\\left(x_{t}^{(1/2)}\\right)\\right\\vert}\\\\ &{\\le\\left\\vert f\\right\\vert_{\\mathrm{Lip}}\\left\\vert\\left((X*\\rho_{1})(t)^{\\top},\\cdots,(X*\\rho_{M})(t)^{\\top})^{\\top}-x_{t}^{(1/2)}\\right\\vert\\right\\vert_{2}+\\left\\Vert f-\\mathrm{FFN}^{(1)}\\right\\Vert_{L^{\\infty}([-2,2]^{D})}}\\\\ &{\\le\\left\\Vert f\\right\\Vert_{\\mathrm{Lip}}\\cdot\\mathcal{E}_{\\mathrm{Attn}}+\\mathcal{E}_{\\mathrm{FFN}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\mathrm{FFN}}=\\frac{\\|f\\|_{B}}{\\sqrt{m}};\\quad\\mathcal{E}_{\\mathrm{Attn}}=\\frac{C(n)M^{n+1}}{H^{n}},\\quad\\mathrm{~for~both~}\\mathbf{C}\\mathbf{a}\\mathbf{s}\\mathbf{e}\\mathrm{~(A)~and~}\\mathbf{C}\\mathbf{a}\\mathbf{s}\\mathbf{e}\\mathrm{~(B)}.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Due to the arbitrariness of $t$ and $\\mathbf{\\deltaX}$ , the proof is completed. ", "page_idx": 53}, {"type": "text", "text": "F Key Lemmas about Approximation ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "F.1  Approximation by the sum of exponential decay ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Lemma F.1 (Exp decay, fixed Delta function). For any $T\\,\\in\\,\\mathbb{N}_{+}$ \uff0c $n,m\\,\\in\\,\\mathbb{N}_{+}$ ,there exists and absolute constant $C(n)$ only depending on $n$ and a $\\phi_{m}^{\\mathrm{exp}}(t)=\\sum_{k=1}^{m}\\alpha_{k}e^{-\\beta_{k}t}$ such that ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\|\\mathbb{I}(\\cdot=T)-\\phi_{m}^{\\mathrm{exp}}(\\cdot)\\|_{\\ell_{1}(\\mathbb{N})}\\leq\\frac{C(n)e^{0.01(n+1)T}}{m^{n}}.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where $\\beta_{k}>0$ holds for any $k\\in[m]$ ", "page_idx": 54}, {"type": "text", "text": "Proof of Lemma $F.l$ ", "page_idx": 54}, {"type": "text", "text": "Let $\\alpha,\\gamma>0$ be constants, and they will take specific values at the end of the proof. ", "page_idx": 54}, {"type": "text", "text": "First, recall the standard bump function on $[-1,1]$ ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\Psi(x):=\\left\\{{\\exp\\left({-{\\frac{1}{1-x^{2}}}}\\right)}\\,,\\,\\,x\\in(-1,1)\\quad\\right.,\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "and we can define the following constants for $T\\geq1$ ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\mu_{T}=e^{-\\alpha T},\\quad\\sigma_{T}=e^{-\\alpha T}-e^{-\\alpha(T+1)}.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Then we consider the following bump function $\\Psi_{T}\\in\\mathcal{C}^{\\infty}([0,1])$ ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\Psi_{T}(x)=\\left\\{V_{T}\\Psi\\left(\\frac{x-\\mu_{T}}{\\sigma_{T}}\\right),\\;x\\in(\\mu_{T}-\\sigma_{T},\\mu_{T}+\\sigma_{T})\\right.\\quad,\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where $V_{T}$ is a scaling constant such that $\\Psi_{T}(e^{-\\alpha T})=e^{\\gamma T}$ ", "page_idx": 54}, {"type": "text", "text": "First, we consider the approximation of $\\Psi_{T}$ on $[0,1]$ ", "page_idx": 54}, {"type": "text", "text": "Notice that $\\Psi_{T}\\,\\in\\,\\mathcal{C}^{\\infty}([0,1])$ , and $\\Psi_{T}^{(k)}(0)\\,=\\,0$ for any $k\\,\\in\\,\\mathbb{N}$ . For the standard bump function $\\Psi$ , for any $n\\in\\mathbb{N}_{+}$ , there exists an absolute constant $M(n)>0$ only depending on $n$ , such that $\\operatorname*{max}_{0\\leq k\\leq10}\\operatorname*{sup}_{x\\in[-1,1]}\\left|\\Psi^{(k)}(x)\\right|\\leq M(n).$ ", "page_idx": 54}, {"type": "text", "text": "Notice that for any $k\\in\\mathbb{N}$ and $x\\in[0,1]$ ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\Psi_{T}^{(k)}(x)=\\frac{V_{T}}{\\sigma_{T}^{k}}\\Psi^{(k)}\\left(\\frac{x-\\mu_{T}}{\\sigma_{T}}\\right).\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Therefore, the following upper bound holds: ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad M_{T}(n)=\\underset{0\\leq k\\leq n}{\\operatorname*{max}}\\frac{V_{T}}{\\sigma_{T}^{k}}M(n)=\\frac{V_{T}}{\\sigma_{T}^{n}}M(n)}\\\\ &{=\\!\\frac{e^{\\gamma T}\\cdot e}{\\left(e^{-\\alpha T}-e^{-\\alpha(T+1)}\\right)^{n}}M(n)=\\frac{M(n)e}{\\left(1-1/e\\right)^{n}}e^{(\\gamma+n\\alpha)T}:=C(n,\\alpha)e^{(\\gamma+n\\alpha)T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "By Lemma G.5, for any $m\\in\\mathbb{N}_{+}$ , there exists a polynomial $Q_{m}(x)=\\sum_{k=0}^{m-1}\\alpha_{k}x^{k}$ such that ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in[0,1]}|\\Psi_{T}(x)-Q_{m}(x)|\\leq\\frac{M_{T}(n)}{m^{n}}\\leq\\frac{C(n,\\alpha)e^{(\\gamma+n\\alpha)T}}{m^{n}}.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Now we use the transform $x=e^{-\\alpha t}$ on the function $\\Psi$ and consider ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\Phi_{T}(t):=e^{-\\gamma t}\\Psi_{T}(e^{-\\alpha t}),\\quad t\\in[0,+\\infty).\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "It is easy to verify that $\\Phi_{T}$ satisfies that ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\left.\\Phi_{T}(t)\\right|_{\\mathbb{N}}=\\mathbb{I}(t=T).\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Moreover, we consider the function ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P_{m}(t):=e^{-\\gamma t}Q_{m}(e^{-\\alpha t}),\\quad t\\in[0,+\\infty).}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Then by choosing $\\alpha=\\gamma=0.01$ , the following error estimate holds: ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left\\lVert P_{m}(\\cdot)-\\mathbb{I}(\\cdot=T)\\right\\rVert_{\\ell_{1}(\\mathbb{N})}=\\sum_{t=0}^{+\\infty}\\left\\lvert P_{m}(t)-\\Phi_{T}(t)\\right\\rvert}\\\\ &{=\\displaystyle\\sum_{t=0}^{+\\infty}e^{-\\gamma t}|Q_{m}(e^{-\\alpha t})-\\Psi_{T}(e^{-\\alpha t})|\\le\\sum_{t=0}^{+\\infty}e^{-\\gamma t}\\frac{M_{T}(n)}{m^{n}}}\\\\ &{\\le\\displaystyle\\frac{C(n,\\alpha)e^{(\\gamma+n\\alpha)T}}{m^{n}}\\sum_{t=0}^{+\\infty}e^{-\\gamma t}\\le\\frac{C(n)e^{0.01(n+1)T}}{m^{n}}\\frac{1}{1-e^{-\\gamma}}}\\\\ &{=\\displaystyle\\frac{\\tilde{C}(n)e^{0.01(n+1)T}}{m^{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Finaly noicethat $P_{m}(t)=e^{-\\gamma t}Q_{m}\\left(e^{-\\alpha t}\\right)=\\sum_{k=0}^{m-1}\\alpha_{k}e^{-(0.01+0.01k)}$ so we can selet $\\phi_{m}^{\\mathrm{exp}}(t):=$ $P_{m}(t)$ ", "page_idx": 55}, {"type": "text", "text": "Lemma F.2 (Exp decay, adaptive Delta function). For any $T\\in\\mathbb{N}$ $n,m\\,\\in\\,\\mathbb{N}_{+}$ , there exists an absolute constant $C(n)$ only depending on $n$ and a $\\phi_{m}^{\\mathrm{exp}}(t;B)=\\sum_{k=1}^{m}\\alpha_{k}e^{-\\beta_{k}(t-B)}$ such that ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{1\\leq B\\leq T}\\|\\mathbb{I}(\\cdot=B)-\\phi_{m}^{\\mathrm{exp}}(\\cdot;B)\\|_{\\ell_{1}(\\mathbb{N})}\\leq\\frac{C(n)e^{0.01(n+1)T}}{m^{n}}.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where $\\beta_{k}>0$ holds for any $k\\in[m]$ ", "page_idx": 55}, {"type": "text", "text": "Proof of Lemma $F.2$ ", "page_idx": 55}, {"type": "text", "text": "The key point of the proof is to note that the adaptability of $B$ can be eliminated by the translation operator $t-B$ ", "page_idx": 55}, {"type": "text", "text": "First, recall our proof of Lemma F.1. For the same $\\Psi_{T}(\\cdot)$ , for any $n,m\\in\\ensuremath{\\mathbb{N}}_{+}$ , there exists an absolute $m\\!-\\!1$ constant $C(n)$ only depending on $n$ and a polynomial $Q_{m}(x)=\\sum_{k=0}^{^{m}}\\alpha_{k}x^{k}$ \u2211 \u03b1kxk such that ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in[0,1]}|\\Psi_{T}(x)-Q_{m}(x)|\\leq\\frac{C(n)e^{0.01(n+1)T}}{m^{n}}.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Moreover, using the transform $x=e^{-0.01(t-B+T)}$ $\\mathit{\\Pi}^{t}\\geq0$ ) on the function $\\Psi$ and consider ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\Phi_{T}(t;B):=e^{-0.01(t-B+T)}\\Psi_{T}\\left(e^{-0.01(t-B+T)}\\right),\\quad t\\in[0,+\\infty).\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "It is easy to verify that $\\Phi_{T}(\\cdot;\\cdot)$ satisfies that ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\left.\\Phi_{T}(t;B)\\right|_{\\mathbb{N}}=\\mathbb{I}(t=B).\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "And we consider the function ", "page_idx": 55}, {"type": "equation", "text": "$$\nP_{m}(t;B):=e^{-0.01(t-B+T)}Q_{m}\\left(e^{-0.01(t-B+T)}\\right),\\quad t\\in[0,+\\infty).\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Then, for any $1\\leq B\\leq T$ , the following error estimate holds: ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\lVert P_{m}(\\cdot;B)-\\mathbb{I}(\\cdot=B)\\rVert_{\\ell_{1}(\\mathbb{N})}=\\sum_{t=0}^{+\\infty}\\lvert P_{m}(t;B)-\\Phi_{T}(t;B)\\rvert}\\\\ &{=\\displaystyle\\sum_{t=0}^{+\\infty}e^{-0.01(t-B+T)}\\left\\lvert Q_{m}\\left(e^{-0.01(t-B+T)}\\right)-\\Psi_{T}\\left(e^{-0.01(t-B+T)}\\right)\\right\\rvert}\\\\ &{\\le\\displaystyle\\sum_{t=0}^{+\\infty}e^{-0.01t}\\operatorname*{sup}_{x\\in[0,1]}\\lvert Q_{m}(x)-\\Psi_{T}(x)\\rvert}\\\\ &{\\le\\displaystyle\\frac{C(n)e^{0.01(n+1)T}}{m^{n}}\\sum_{t=0}^{+\\infty}e^{-0.01t}=\\frac{\\tilde{C}(n)e^{0.01(n+1)T}}{m^{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Due to the arbitrariness of $B$ , the proof is completed. ", "page_idx": 56}, {"type": "text", "text": "Lemma F.3 (Exp decay, fixed Delta function). Consider a exponentially decayed memory $\\rho(\\cdot)$ : there exists $\\beta>0$ such that $\\rho(t)={\\mathcal{O}}(e^{-\\beta t})$ .Thenforany $n\\in[\\lfloor99\\beta]]$ and $m\\in\\mathbb{N}_{+}$ ,thereexistsan absoluteconstant $C(n)$ onlydependingon $n$ and a $\\phi_{m}^{\\mathrm{exp}}(t)=\\sum_{k=1}^{m}\\alpha_{k}e^{-\\beta_{k}t}$ such that ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\|\\rho(\\cdot)-\\phi_{m}^{\\mathrm{exp}}(\\cdot)\\|_{\\ell_{1}(\\mathbb{N})}\\leq\\frac{C(n)}{m^{n}},\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where $\\beta_{k}>0$ holds for any $k\\in[m]$ ", "page_idx": 56}, {"type": "text", "text": "Proof of Lemma $F.3$   \nThere exists $C>0$ such that $|\\rho(t)|\\leq C e^{-\\beta t}$   \nLet $\\alpha,\\gamma>0$ be constants, and they will take specific values at the end of the proof. ", "page_idx": 56}, {"type": "text", "text": "First, recall the standard bump function on $[-1,1]$ ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\Psi(x):=\\left\\{{\\exp\\left({-{\\frac{1}{1-x^{2}}}}\\right)}\\,,\\,\\,x\\in(-1,1)\\quad\\right.,\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "and we can define the following constants for $T\\geq1$ ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\mu_{T}=e^{-\\alpha T},\\quad\\sigma_{T}=\\frac{1}{2}\\left(e^{-\\alpha T}-e^{-\\alpha(T+1)}\\right),\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "and we consider the following bump function $\\Psi_{T}\\in\\mathcal{C}^{\\infty}([0,1])$ ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\Psi_{T}(x)=\\left\\{V_{T}\\Psi\\left(\\frac{x-\\mu_{T}}{\\sigma_{T}}\\right),\\;x\\in(\\mu_{T}-\\sigma_{T},\\mu_{T}+\\sigma_{T})\\right.\\quad,\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where $V_{T}$ is a scaling constant such that $\\Psi_{T}(e^{-\\alpha T})=e^{\\gamma T}\\rho(T)$ ", "page_idx": 56}, {"type": "text", "text": "Consequently, we consider the sum of bump functions on $[0,1]$ ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\varphi(x):=\\sum_{T=1}^{+\\infty}\\Psi_{T}(x).\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "It is easy to verify that $\\left(\\mu_{T_{1}}-\\sigma_{T_{1}},\\mu_{T_{1}}+\\sigma_{T_{1}}\\right)\\cap\\left(\\mu_{T_{2}}-\\sigma_{T_{2}},\\mu_{T_{2}}+\\sigma_{T_{2}}\\right)=\\emptyset$ for any $T_{1}\\neq T_{2}$ and ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\varphi(x)=\\left\\{\\Psi_{T}(x),\\;\\mu_{T}-\\sigma_{T}\\leq x\\leq\\mu_{T}+\\sigma_{T}\\right.\\quad.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "First, we study the property of $\\varphi(\\cdot)$ ", "page_idx": 57}, {"type": "text", "text": "We denote the absolute constants $M_{k}=\\operatorname*{sup}_{x}|\\varphi^{(k)}(x)|$ . Notice that for any $k\\in\\mathbb{N}$ ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\Psi_{T}^{(k)}(x)=\\frac{V_{T}}{\\sigma_{T}^{k}}\\Psi^{(k)}\\left(\\frac{x-\\mu_{T}}{\\sigma_{T}}\\right).\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Therefore, it holds that ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{x\\in(\\mu_{T}-\\sigma_{T},\\mu_{T}+\\sigma_{T})}{\\operatorname*{sup}}|\\varphi^{(k)}(x)|=\\underset{x\\in(\\mu_{T}-\\sigma_{T},\\mu_{T}+\\sigma_{T})}{\\operatorname*{sup}}|\\Psi_{T}^{(k)}(x)|}\\\\ &{\\leq\\!\\frac{V_{T}}{\\sigma_{T}^{k}}M_{k}=\\frac{e^{\\gamma T}\\rho(T)}{\\left(e^{-\\alpha T}-e^{-\\alpha(T+1)}\\right)^{k}}M_{k}e\\leq\\frac{C M_{k}e}{(1-e^{-\\alpha})^{k}}e^{(\\gamma+k\\alpha-\\beta)T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Therefore, if $\\beta\\ge\\gamma+k\\alpha$ , then the following uniform bounds holds: ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{sup}_{x\\in(0,1]}|\\varphi^{(k)}(x)|=\\operatorname*{sup}_{T\\geq1}\\operatorname*{sup}_{x\\in(\\mu_{T}-\\sigma_{T},\\mu_{T}+\\sigma_{T})}|\\varphi^{(k)}(x)|}\\\\ &{\\leq\\displaystyle\\operatorname*{sup}_{T\\geq1}\\frac{C M_{k}e}{(1-e^{-\\alpha})^{k}}e^{(\\gamma+k\\alpha-\\beta)T}\\leq\\frac{C M_{k}e}{(1-e^{-\\alpha})^{k}}:=C(k,\\alpha).}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Consequently, we consider the smoothness of $\\Phi$ at $x=0$ ", "page_idx": 57}, {"type": "text", "text": "Recalling the previous results, for any $x\\in(0,1]$ , we have ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{|\\varphi^{(k)}(x)|}{x}\\leq C(k,\\alpha)\\frac{e^{(\\gamma+k\\alpha-\\beta)T}}{\\mu_{T}-\\sigma_{T}}=\\frac{2C(k,\\alpha)}{1-e^{-\\alpha}}e^{(\\gamma+(k+1)\\alpha-\\beta)T},\\;x\\in(\\mu_{T}-\\sigma_{T},\\mu_{T}+\\sigma_{T});}\\\\ &{\\displaystyle\\frac{|\\varphi^{(k)}(x)|}{x}=0,\\;\\mathrm{otherwise}}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Thus, byndution, it is asy to verify that for any $\\begin{array}{r}{i<\\frac{\\beta-\\gamma}{\\alpha}\\;(i\\in\\mathbb{N}),}\\end{array}$ ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\varphi^{(i)}(0)=0.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Therefore, for any $\\begin{array}{r}{n<\\frac{\\beta-\\gamma}{\\alpha}\\;(n\\in\\mathbb{N}),\\varphi^{(k)}(0)=0}\\end{array}$ holds for any $0\\leq k\\leq n$ Moreover, there exists absolute constant $C(n,\\alpha)^{\\underbar{}{}}$ such that: ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{0\\leq k\\leq n}\\operatorname*{sup}_{x\\in[0,1]}|\\varphi^{(k)}(x)|\\leq C(n,\\alpha).\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "By Lemma G.5, for any $m\\in\\mathbb{N}_{+}$ , there exists a polynomial $Q_{m}(x)=\\sum_{k=0}^{m-1}\\alpha_{k}x^{k}$ such that ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in[0,1]}|\\varphi(x)-Q_{m}(x)|\\leq{\\frac{C(n,\\alpha)}{m^{n}}}.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Now we use the transform $x=e^{-\\alpha t}\\left(t\\geq0\\right)$ on the function $\\varphi$ and consider ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\Phi(t):=\\frac{1}{e^{\\gamma t}}\\varphi\\left(\\frac{1}{e^{\\alpha t}}\\right),\\quad t\\in[0,+\\infty).\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "It is easy to verify that $\\Phi$ satisfies that ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\left.\\Phi(t)\\right|_{\\mathbb{N}}=\\rho(t)\\big|_{\\mathbb{N}}.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Moreover, we consider the function ", "page_idx": 57}, {"type": "equation", "text": "$$\nP_{m}(t):=\\frac{1}{e^{\\gamma t}}Q_{m}\\left(\\frac{1}{e^{\\alpha t}}\\right),\\quad t\\in[0,+\\infty).\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Then for any $\\begin{array}{r}{n<\\frac{\\beta-\\gamma}{\\alpha}\\;(n\\in\\mathbb{N})}\\end{array}$ , the following eror estimate holds: ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|P_{m}(\\cdot)-\\rho(\\cdot)\\|_{\\ell_{1}(\\mathbb{N})}=\\sum_{t=0}^{+\\infty}|P_{m}(t)-\\Phi(t)|}\\\\ {\\displaystyle=\\sum_{t=0}^{+\\infty}e^{-\\gamma t}\\,\\|Q_{m}\\left(e^{-\\alpha t}\\right)-\\Psi_{T}\\left(e^{-\\alpha t}\\right)\\|\\leq\\frac{C(n,\\alpha)}{m^{n}}\\sum_{t=0}^{+\\infty}e^{-\\gamma t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "By choosing $\\alpha=5\\cdot10^{-3}$ and $\\gamma=10^{-2}\\beta$ , it holds that $\\begin{array}{r}{99\\beta<\\frac{\\beta-\\gamma}{2\\alpha}=\\frac{\\beta-\\gamma}{\\alpha}}\\end{array}$ Thus, we obtain our result: for any $n\\in\\left[\\left\\lfloor99\\beta\\right\\rfloor\\right](\\beta\\geq1/99)$ , the following error estimate holds: ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\|P_{m}(\\cdot)-\\rho(\\cdot)\\|_{\\ell_{1}(\\mathbb{N})}\\leq\\frac{C(n)}{m^{n}}\\sum_{t=0}^{+\\infty}e^{-\\gamma t}=\\frac{C(n)}{m^{n}}\\frac{1}{1-e^{-10^{-2}\\beta}}=\\frac{\\tilde{C}(n)}{m^{n}}.\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "F.2  Approximation by the sum of polynomial decay ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Lemma F.4 (Poly decay, fixed Delta function). For any $T,n,m\\in\\mathbb{N}_{+}$ ,thereexistsanabsolute constant $C(n)$ only depending on $n$ and a $\\phi_{m}^{\\mathrm{poly}}(t)=\\sum_{k=1}^{m}\\alpha_{k}t^{-\\beta_{k}}$ suchthat ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\bigl\\|\\mathbb{I}(\\cdot=T)-\\phi_{m}^{\\mathrm{poly}}(\\cdot)\\bigr\\|_{\\ell_{1}(\\mathbb{N}_{+})}\\leq\\frac{C(n)T^{1.01(n+1)}}{m^{n}},\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where $\\beta_{k}>1$ holds for any $k\\in[m]$ ", "page_idx": 58}, {"type": "text", "text": "ProofofLemma $F.4.$ Let $\\alpha,\\gamma>0$ be constants, and they will take specific values at the end of the proof ", "page_idx": 58}, {"type": "text", "text": "First, recall the standard bump function on $[-1,1]$ ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\Psi(x):=\\left\\{{\\exp\\left({-{\\frac{1}{1-x^{2}}}}\\right)}\\,,\\,\\,x\\in(-1,1)\\quad\\right.,\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "and we can define the following constants for $T\\geq1$ ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\mu_{T}=\\frac{1}{T^{\\alpha}},\\quad\\sigma_{T}=\\frac{1}{T^{\\alpha}}-\\frac{1}{(T+1)^{\\alpha}}.\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Then we consider the following bump function $\\Psi_{T}\\in\\mathcal{C}^{\\infty}([0,1])$ ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\Psi_{T}(x)=\\left\\{V_{T}\\Psi\\left(\\frac{x-\\mu_{T}}{\\sigma_{T}}\\right),\\;x\\in(\\mu_{T}-\\sigma_{T},\\mu_{T}+\\sigma_{T})\\right.\\quad,\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where $V_{T}$ is a scaling constant such that $\\begin{array}{r}{\\Psi_{T}\\big(\\frac{1}{T^{\\alpha}}\\big)=T^{1+\\gamma}}\\end{array}$ ", "page_idx": 58}, {"type": "text", "text": "First, we consider the approximation of $\\Psi_{T}$ on $[0,1]$ ", "page_idx": 58}, {"type": "text", "text": "Notice that $\\Psi_{T}\\,\\in\\,\\mathcal{C}^{\\infty}([0,1])$ , and $\\Psi_{T}^{(k)}(0)\\,=\\,0$ for any $k\\,\\in\\,\\mathbb{N}$ . For the standard bump function $\\Psi$ , for any $n\\in\\mathbb{N}_{+}$ , there exists an absolute constant $M(n)>0$ only depending on $n$ , such that $\\operatorname*{max}_{0\\leq k\\leq n}\\operatorname*{sup}_{x\\in[-1,1]}\\left|\\Psi^{(k)}(x)\\right|\\leq M(n)$ ", "page_idx": 58}, {"type": "text", "text": "Notice that for any $k\\in\\mathbb{N}$ and $x\\in[0,1]$ ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\Psi_{T}^{(k)}(x)=\\frac{V_{T}}{\\sigma_{T}^{k}}\\Psi^{(k)}\\left(\\frac{x-\\mu_{T}}{\\sigma_{T}}\\right).\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Therefore, the following upper bound holds: ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad M_{T}(n)=\\underset{0\\leq k\\leq n}{\\operatorname*{max}}\\,\\frac{V_{T}}{\\sigma_{T}^{k}}M(n)=\\frac{V_{T}}{\\sigma_{T}^{n}}M(n)=\\frac{T^{1+\\gamma}e}{\\left(1/T^{\\alpha}-1/(T+1)^{\\alpha}\\right)^{n}}M(n)}\\\\ &{\\leq\\!\\frac{T^{1+\\gamma}(T+1)^{n(1+\\alpha)}M(n)e}{\\alpha^{n}}\\leq\\frac{2^{n}M(n)e}{\\alpha^{n}}T^{1+\\gamma+n(1+\\alpha)}:=C(n,\\alpha)T^{1+\\gamma+n(1+\\alpha)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "By Lemma G.5, for any $m\\in\\mathbb{N}_{+}$ , there exists a polynomial $Q_{m}(x)=\\sum_{k=0}^{m-1}\\alpha_{k}x^{k}$ such that ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in[0,1]}|\\Psi_{T}(x)-Q_{m}(x)|\\leq\\frac{M_{T}(n)}{m^{n}}\\leq\\frac{C(n,\\alpha)T^{1+\\gamma+n(1+\\alpha)}}{m^{n}}.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Now we use the transform $\\begin{array}{r}{x=\\frac{1}{t^{\\alpha}}\\,\\left(t\\geq1\\right)}\\end{array}$ on the function $\\Psi$ and consider ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\Phi_{T}(t):=\\frac{1}{t^{1+\\gamma}}\\Psi_{T}\\left(\\frac{1}{t^{\\alpha}}\\right),\\quad t\\in[1,+\\infty).\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "It is easy to verify that $\\Phi_{T}$ satisfies that ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\left.\\Phi_{T}(t)\\right|_{\\mathbb{N}_{+}}=\\mathbb{I}(t=T).\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Moreover, we consider the function ", "page_idx": 59}, {"type": "equation", "text": "$$\nP_{m}(t):=\\frac{1}{t^{1+\\gamma}}Q_{m}\\left(\\frac{1}{t^{\\alpha}}\\right),\\quad t\\in[1,+\\infty).\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Then by choosing $\\alpha=\\gamma=0.01$ , the following error estimate holds: ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|P_{m}(\\cdot)-\\mathbb{I}(\\cdot=T)\\|_{\\ell_{1}(\\mathbb{N}_{+})}=\\sum_{t=1}^{+\\infty}|P_{m}(t)-\\Phi_{T}(t)|}\\\\ {\\displaystyle=\\sum_{t=1}^{+\\infty}\\frac{1}{t^{1+\\gamma}}\\left|Q_{m}\\left(\\frac{1}{t^{\\alpha}}\\right)-\\Psi_{T}\\left(\\frac{1}{t^{\\alpha}}\\right)\\right|\\le\\sum_{t=1}^{+\\infty}\\frac{1}{t^{1+\\gamma}}\\frac{M_{T}(n)}{m^{n}}}\\\\ {\\displaystyle\\le\\frac{C(n,\\alpha)T^{1+\\gamma+n(1+\\alpha)}}{m^{n}}\\sum_{t=1}^{+\\infty}\\frac{1}{t^{1+\\gamma}}=\\frac{C(n)T^{1.01(n+1)}}{m^{n}}\\sum_{t=1}^{+\\infty}\\frac{1}{t^{1+0.01}}}\\\\ {\\displaystyle=\\frac{\\tilde{C}(n)T^{1.01(n+1)}}{m^{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Finally, notie that $P_{m}(\\cdot)$ satisfies to $\\begin{array}{l}{{P_{m}(t)\\,=\\,\\frac{1}{t^{1+\\gamma}}Q_{m}\\left(\\frac{1}{t^{\\alpha}}\\right)\\,=\\,\\displaystyle\\sum_{k=0}^{m-1}\\alpha_{k}t^{-(1.01+0.01k)}}}\\end{array}$ so we can select $\\phi_{m}^{\\mathrm{poly}}(t):=P_{m}(t)$ ", "page_idx": 59}, {"type": "text", "text": "Lemma F.5 (Poly decay, adaptive Delta function). For any $T,n,m\\in\\mathbb{N}_{+}$ ,there exists an absolute constant $C(n)$ only depending on n and a $\\phi_{m}^{\\mathrm{poly}}(t;B)=\\sum_{k=1}^{m}\\alpha_{k}(t/B)^{-\\beta_{k}}$ suchthat ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{1\\leq B\\leq T}\\left\\|\\mathbb{I}(\\cdot=B)-\\phi_{m}^{\\mathrm{poly}}(\\cdot;B)\\right\\|_{\\ell_{1}(\\mathbb{N}_{+})}\\leq\\frac{C(n)T^{1.01(n+1)}}{m^{n}},\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "where $\\beta_{k}>1$ holds for any $k\\in[m]$ ", "page_idx": 59}, {"type": "text", "text": "Proof of Lemma F.5. ", "page_idx": 59}, {"type": "text", "text": "The key point of the proof is to note that the adaptability of $B$ can be eliminated by the rescaling operator $t/B$ ", "page_idx": 59}, {"type": "text", "text": "First, recall our proof of Lemma F.4. For the same $\\Psi_{T}(\\cdot)$ , for any $n,m\\in\\ensuremath{\\mathbb{N}}_{+}$ , there exists an absolute constant $C(n)$ only depending on $n$ and a polynomial $Q_{m}(x)=\\sum_{k=0}^{m-1}\\alpha_{k}x^{k}$ such that ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in[0,1]}|\\Psi_{T}(x)-Q_{m}(x)|\\leq\\frac{C(n)T^{1.01(n+1)}}{m^{n}}.\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "We use the transform $\\begin{array}{r}{x=\\frac{1}{t^{0.01}}\\,\\,(t\\geq1)}\\end{array}$ on the function $\\Psi$ and consider ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\Phi_{T}(t;B):=\\left(\\frac{B}{t T}\\right)^{1.01}\\Psi_{T}\\left(\\left(\\frac{B}{t T}\\right)^{0.01}\\right),\\quad t\\in[1,+\\infty).\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "It is easy to verify that $\\Phi_{T}(\\cdot;\\cdot)$ satisfies that ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\Phi_{T}(t;B)\\big|_{\\mathbb{N}_{+}}=\\mathbb{I}(t=B).\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "And we consider the function ", "page_idx": 60}, {"type": "equation", "text": "$$\nP_{m}(t;B):=\\left(\\frac{B}{t T}\\right)^{1.01}Q_{m}\\left(\\left(\\frac{B}{t T}\\right)^{0.01}\\right),\\quad t\\in[1,+\\infty).\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Then, for any $1\\leq B\\leq T$ , the following error estimate holds: ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left\\lVert P_{m}(:;B)-\\mathbb{I}(\\cdot=B)\\right\\rVert_{\\ell_{1}(\\mathbb{N}_{+})}=\\sum_{t=1}^{+\\infty}\\left\\lvert P_{m}(t;B)-\\Phi_{T}(t;B)\\right\\rvert}\\\\ &{\\displaystyle=\\sum_{t=1}^{+\\infty}\\left(\\frac{B}{t T}\\right)^{1.01}\\left\\lvert Q_{m}\\left(\\left(\\frac{B}{t T}\\right)^{0.01}\\right)-\\Psi_{T}\\left(\\left(\\frac{B}{t T}\\right)^{0.01}\\right)\\right\\rvert}\\\\ &{\\displaystyle\\leq\\sum_{t=1}^{+\\infty}\\frac{1}{t^{1.01}}\\operatorname*{sup}_{x\\in[0,1]}|Q_{m}(x)-\\Psi_{T}(x)|}\\\\ &{\\displaystyle\\leq\\frac{C(n)T^{1.01(n+1)}}{m^{n}}\\sum_{t=1}^{+\\infty}\\frac{1}{t^{1.01}}=\\frac{\\tilde{C}(n)T^{1.01(n+1)}}{m^{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Due to the arbitrariness of $B$ , the proof is completed. ", "page_idx": 60}, {"type": "text", "text": "Lemma F.6 (Poly decay, fixed Delta function). Consider a polynomially decayed memory $\\rho(\\cdot)$ : there exists $\\beta>1$ such that $\\rho(t)=O(t^{-\\beta})$ .Thenforany $n\\in[\\lfloor0.99\\beta\\rfloor-1]$ and $m\\in\\mathbb{N}_{+}$ ,thereexistsan absolute constant $C(n)$ only depending on n and a $\\phi_{m}^{\\mathrm{poly}}(t)=\\sum_{k=1}^{m}\\alpha_{k}t^{-\\beta_{k}}$ such that ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\Big\\|\\rho(\\cdot)-\\phi_{m}^{\\mathrm{poly}}(\\cdot)\\Big\\|_{\\ell_{1}(\\mathbb{N}_{+})}\\leq\\frac{C(n)}{m^{n}},\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "where $\\beta_{k}>1$ holds for any $k\\in[m]$ ", "page_idx": 60}, {"type": "text", "text": "Proof of Lemma F.6.   \nThere exists $C>0$ such that $|\\rho(t)|\\le C/t^{\\beta}$   \nLet $\\alpha,\\gamma>0$ be constants, and they will take specific values at the end of the proof First, recall the standard bump function on $[-1,1]$ ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\Psi(x):=\\left\\{{\\exp\\left({-{\\frac{1}{1-x^{2}}}}\\right)}\\,,\\,\\,x\\in(-1,1)\\quad\\right.,\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "and we can define the following constants for $T\\geq1$ ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\mu_{T}=\\frac{1}{T^{\\alpha}},\\quad\\sigma_{T}=\\frac{1}{2}\\left(\\frac{1}{T^{\\alpha}}-\\frac{1}{(T+1)^{\\alpha}}\\right),\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "and we consider the following bump function $\\Psi_{T}\\in\\mathcal{C}^{\\infty}([0,1])$ ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\Psi_{T}(x)=\\left\\{V_{T}\\Psi\\left(\\frac{x-\\mu_{T}}{\\sigma_{T}}\\right),\\;x\\in(\\mu_{T}-\\sigma_{T},\\mu_{T}+\\sigma_{T})\\right.\\quad,\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "where $V_{T}$ is a scaling constant such that $\\begin{array}{r}{\\Psi_{T}\\big(\\frac{1}{T^{\\alpha}}\\big)=T^{1+\\gamma}\\rho(T)}\\end{array}$ ", "page_idx": 61}, {"type": "text", "text": "Consequently, we consider the sum of bump functions on $[0,1]$ ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\varphi(x):=\\sum_{T=1}^{+\\infty}\\Psi_{T}(x).\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "It is easy to verify that $\\left(\\mu_{T_{1}}-\\sigma_{T_{1}},\\mu_{T_{1}}+\\sigma_{T_{1}}\\right)\\cap\\left(\\mu_{T_{2}}-\\sigma_{T_{2}},\\mu_{T_{2}}+\\sigma_{T_{2}}\\right)=\\emptyset$ for any $T_{1}\\neq T_{2}$ and ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\varphi(x)=\\left\\{\\Psi_{T}(x),\\;\\mu_{T}-\\sigma_{T}\\leq x\\leq\\mu_{T}+\\sigma_{T}\\right.\\quad.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "First, we study the property of $\\varphi(\\cdot)$ ", "page_idx": 61}, {"type": "text", "text": "We denote the absolute constants $M_{k}=\\operatorname*{sup}_{x}|\\varphi^{(k)}(x)|$ . Notice that for any $k\\in\\mathbb{N}$ ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\Psi_{T}^{(k)}(x)=\\frac{V_{T}}{\\sigma_{T}^{k}}\\Psi^{(k)}\\left(\\frac{x-\\mu_{T}}{\\sigma_{T}}\\right).\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Therefore, it holds that ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{x\\in(\\mu_{T}-\\sigma_{T},\\mu_{T}+\\sigma_{T})}{\\operatorname*{sup}}|\\varphi^{(k)}(x)|=\\underset{x\\in(\\mu_{T}-\\sigma_{T},\\mu_{T}+\\sigma_{T})}{\\operatorname*{sup}}|\\Psi_{T}^{(k)}(x)|}\\\\ &{\\leq\\!\\frac{V_{T}}{\\sigma_{T}^{k}}M_{k}=\\frac{T^{1+\\gamma}\\rho(T)}{\\Big(\\frac{1}{T^{\\alpha}}-\\frac{1}{(T+1)^{\\alpha}}\\Big)^{k}}2^{k}M_{k}e}\\\\ &{\\leq\\!\\frac{(T+1)^{k(1+\\alpha)}T^{1+\\gamma-\\beta}C2^{k}M_{k}e}{\\alpha^{k}}\\leq\\frac{2^{k(2+\\alpha)}C M_{k}e}{\\alpha^{k}}T^{1+\\gamma+k(1+\\alpha)-\\beta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Therefore, if $\\begin{array}{r}{k\\le\\frac{\\beta-(1+\\gamma)}{1+\\alpha}}\\end{array}$ ,the following uniform bounds hold: ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{x\\in(0,1]}{\\operatorname*{sup}}|\\varphi^{(k)}(x)|=\\underset{T\\geq1\\,x\\in(\\mu_{T}-\\sigma_{T},\\mu_{T}+\\sigma_{T})}{\\operatorname*{sup}}|\\varphi^{(k)}(x)|}\\\\ &{\\leq\\underset{T\\geq1}{\\operatorname*{sup}}\\frac{2^{k(2+\\alpha)}C M_{k}e}{\\alpha^{k}}T^{1+\\gamma+k(1+\\alpha)-\\beta}\\leq\\frac{2^{k(2+\\alpha)}C M_{k}e}{\\alpha^{k}}:=C(k,\\alpha).}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Consequently, we consider the smoothness of $\\Phi$ at $x=0$ ", "page_idx": 61}, {"type": "text", "text": "Recalling the previous results, for any $x\\in(0,1]$ , we have ", "page_idx": 61}, {"type": "equation", "text": "$$\n{\\frac{\\varphi^{(k)}(x)|}{x}}\\leq C(k,\\alpha){\\frac{T^{1+\\gamma+k(1+\\alpha)-\\beta}}{\\mu_{T}-\\sigma_{T}}}\\leq{\\frac{C(k,\\alpha)2^{2+\\alpha}}{\\alpha}}T^{1+\\gamma+(k+1)(1+\\alpha)-\\beta},\\ x\\in(\\mu_{T}-\\sigma_{T},\\mu_{T}+\\sigma_{T}),\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Thusbyti,iseaterifytat f $\\begin{array}{r}{i<\\frac{\\beta-(1+\\gamma)}{1+\\alpha}\\left(i\\in\\mathbb{N}\\right)}\\end{array}$ ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\varphi^{(i)}(0)=0.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Therefore, for any $\\begin{array}{r}{n<\\frac{\\beta-(1+\\gamma)}{1+\\alpha}\\;(n\\in\\mathbb{N}_{+}),\\varphi^{(k)}(0)=0}\\end{array}$ holds for any $0\\leq k\\leq n$ . Moreover, the following uniform bound holds: ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{0\\leq k\\leq n}\\operatorname*{sup}_{x\\in[0,1]}|\\varphi^{(k)}(x)|\\leq C(n,\\alpha).\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "By Lemma G.5, for any $m\\in\\mathbb{N}_{+}$ , there exists a polynomial $Q_{m}(x)=\\sum_{k=0}^{m-1}\\alpha_{k}x^{k}$ such that ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in[0,1]}|\\varphi(x)-Q_{m}(x)|\\leq{\\frac{C(n,\\alpha)}{m^{n}}}.\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Now we use the transform $\\textstyle x={\\frac{1}{t^{\\alpha}}}$ $\\mathit{\\Omega}^{\\leftmoon}\\left(t\\geq1\\right)$ on the function $\\varphi$ and consider ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\Phi(t):=\\frac{1}{t^{1+\\gamma}}\\varphi\\left(\\frac{1}{t^{\\alpha}}\\right),\\quad t\\in[1,+\\infty).\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "It is easy to verify that $\\Phi$ satisfies that ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\Phi(t)\\big|_{\\mathbb{N}_{+}}=\\rho(t)\\big|_{\\mathbb{N}_{+}}.\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Moreover, we consider the function ", "page_idx": 62}, {"type": "equation", "text": "$$\nP_{m}(t):=\\frac{1}{t^{1+\\gamma}}Q_{m}\\left(\\frac{1}{t^{\\alpha}}\\right),\\quad t\\in[1,+\\infty).\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Then for any $\\begin{array}{r}{n<\\frac{\\beta-(1+\\gamma)}{1+\\alpha}\\;(n\\in\\mathbb{N})}\\end{array}$ , the following eror estimate holds: ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|P_{m}(\\cdot)-\\rho(\\cdot)\\|_{\\ell_{1}(\\mathbb{N}_{+})}=\\sum_{t=1}^{+\\infty}|P_{m}(t)-\\Phi(t)|}\\\\ {\\displaystyle=\\sum_{t=1}^{+\\infty}\\frac{1}{t^{1+\\gamma}}\\left|Q_{m}\\left(\\frac{1}{t^{\\alpha}}\\right)-\\Psi_{T}\\left(\\frac{1}{t^{\\alpha}}\\right)\\right|\\leq\\frac{C(n,\\alpha)}{m^{n}}\\sum_{t=1}^{+\\infty}\\frac{1}{t^{1+\\gamma}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "By choosing \u03b1 = 10-2 and  = 10-4\u03b2, we have 0.99\u03b2 - 1 = \u03b2\u03b1 - 1 = \u03b2-\u2193++a) < \u03b2-(\u00b1+). Thus, we obtain our result: for any $n\\in[\\lfloor0.99\\beta\\rfloor-1]$ $(\\beta\\geq2/0.99)$ , the following error estimate holds: ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\|P_{m}(\\cdot)-\\rho(\\cdot)\\|_{\\ell_{1}(\\mathbb{N}_{+})}\\leq\\frac{C(n)}{m^{n}}\\sum_{t=1}^{+\\infty}\\frac{1}{t^{1+\\gamma}}\\leq\\frac{C(n)}{m^{n}}\\sum_{t=1}^{+\\infty}\\frac{1}{t^{1+10^{-4}}}=\\frac{\\tilde{C}(n)}{m^{n}}.\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "G   Some Background and Proof Preparation ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "G.1   T5's relative positional encoding ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "The T5's Relative Positional Encoding is primary focus of this study. Its standard form in practical applications (Raffel et al., 2020) adheres to $R_{t,s}=r(t-s)$ ,where ", "page_idx": 63}, {"type": "equation", "text": "$$\n-r(n)=\\left\\{\\!\\!\\begin{array}{l l}{n,}&{\\mathrm{~if~}n<B}\\\\ {\\mathcal{B}+\\lfloor\\mathcal{B}\\cdot\\frac{\\log(n/B)}{\\log(\\mathcal{D}/B)}\\rfloor,}&{\\mathrm{~if~}\\mathcal{B}\\le n<\\mathcal{D}\\;.}\\\\ {2B-1,}&{\\mathrm{~if~}n\\ge\\mathcal{D}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Here, $\\mathcal{D}$ is a large integer, signifying the longest distance of concern, while $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ is a small integer. One can see that for $n<B$ \uff0c $r(\\cdot)$ exhibits polynomial decay, whereas for $B<n<D$ \uff0c $r(\\cdot)$ demonstrates logarithmic decay. Consequently, the overall decay rate of $r(\\cdot)$ is logarithmic. ", "page_idx": 63}, {"type": "text", "text": "The following Table further provides an example of standard T5's Relative Positional Encoding. ", "page_idx": 63}, {"type": "table", "img_path": "0o7Rd5jngV/tmp/c6f0a51caefcb5a408ccae54cd18ed6e972d48e09ef6a2c2e080617016fcbd2b.jpg", "table_caption": ["Table 1: An example of standard T5's Relative Positional Encoding "], "table_footnote": [], "page_idx": 63}, {"type": "text", "text": "G.2 Barron space theory ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "The well-known universal approximation result for 2NNs asserts that 2NNs can approximate any continuous function (Barron, 1992; 1993; 1994). Nonetheless, this result lacks a characterization of the approximation efficiency, i.e., how many neurons are needed to achieve a certain approximation accuracy? This gap was addressed by the Barron space theory (E et al., 2019; 2021). It is established that for any function within Barron space $f\\in B$ , 2NNs with $m$ neurons (denoted by $\\mathcal{H}_{m}$ ) can approximate them efficiently, at a rate of $\\operatorname*{inf}_{f_{m}\\in\\mathcal{H}_{m}}\\|f-f_{m}\\|\\leq\\mathcal{O}(\\|f\\|_{B}/\\sqrt{m})$ , remarkably independent of the input dimension $d$ , thus avoiding the Curse of Dimensionality (Bellman, 1966; Bach, 2017). Specifically, the Barron space is defined by: ", "page_idx": 63}, {"type": "text", "text": "Definition G.1 (Barron space (E et al., 2019; 2021; Ma et al., 2020)). Consider functions $f:X\\to\\mathbb{R}$ that admit the following representation: ", "page_idx": 63}, {"type": "equation", "text": "$$\nf(\\pmb{x})=\\int_{\\Omega}a\\sigma(\\pmb{b}^{\\top}\\pmb{x}+c)\\rho(\\mathrm{d}a,\\mathrm{d}b,\\mathrm{d}c),\\;\\pmb{x}\\in X.\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "For any $p\\in[1,+\\infty]$ , we define the Barron norm: ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\|f\\|_{B_{p}}:=\\operatorname*{inf}_{\\rho}\\Big(\\mathbb{E}_{\\rho}\\left[|a|^{p}(\\|b\\|_{1}+|c|)^{p}\\right]\\Big)^{1/p}.\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Then the Barron space are defined as: ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\mathcal{B}_{p}:=\\{f\\in\\mathcal{C}:\\|f\\|_{\\mathcal{B}_{p}}<+\\infty\\}.\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Proposition G.2. For any $p\\in[1,+\\infty]$ $B_{p}=B_{\\infty}$ and $\\|f\\|_{B_{p}}=\\|f\\|_{B_{\\infty}}$ ", "page_idx": 63}, {"type": "text", "text": "Remark G.3. From the Proposition above, the Barron spaces $B_{p}$ are equivalent for any $p\\in[1,+\\infty]$ Consequently, in this paper, we use $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ and $\\left\\|\\cdot\\right\\|_{B}$ to denote the Barron space and Barron norm. ", "page_idx": 63}, {"type": "text", "text": "Remark G.4. For Barron space $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ , both Direct and Inverse Approximation Theorems hold ( $\\mathrm{^E}$ et al., 2021). In this paper, we mainly utilize the Direct Approximation Theorem, stated in Lemma G.6. ", "page_idx": 63}, {"type": "text", "text": "G.3 Useful approximation lemmas ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Lemma G.5 (Jackson (1930)). Let $f\\in C^{n}([0,1])$ with $f(0)=f^{\\prime}(0)=\\cdots=f^{(n)}(0)=0$ Then for any $m\\in\\mathbb{N}_{+}$ , there exists a polynomial $Q_{m}(x)=\\sum_{k=0}^{m-1}\\alpha_{k}x^{k}$ such that ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\|f-Q_{m}\\|_{L^{\\infty}([0,1])}\\leq\\frac{M(n)}{m^{n}},\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "where M(n)=axIf()(0,1) ", "page_idx": 64}, {"type": "text", "text": "Lemma G.6 (Ma et al. (2020)). For any $f\\in B$ and $m\\in\\mathbb{N}$ there exists a two-layer ReLU neural network $f_{m}(\\pmb{x})=\\sum_{k=1}^{m}a_{k}\\sigma(\\pmb{b}_{k}^{\\top}\\pmb{x}+c_{k})$ with $m$ neurons such that ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\|f-f_{m}\\|_{L^{\\infty}([0,1]^{d})}\\leq\\tilde{\\mathcal{O}}\\left(\\frac{\\|f\\|_{\\mathcal{B}}}{\\sqrt{m}}\\right).\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "H Experiments ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "H.1   Restatement of our theoretical insights ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "As detailed in Section 1, our theoretical analysis reveals the following novel insights into the expressive power and mechanisms of Transformer: ", "page_idx": 65}, {"type": "text", "text": "Insight (1). The distinct roles of the number of layers, the number of Attn heads, and the width of FFN layers. (1a) Deeper Transformers can handle tasks with memories with more intricate interrelationships, such as nested relationships (Type II). (1b) In contrast, for tasks with memories lacking such interrelationships (Type I), a single-layer Transformer with sufficient Attn heads and FFN width should suffice. ", "page_idx": 65}, {"type": "text", "text": "Insight (2). The different roles of Attn layers and FFN layers. (2a) FFN layers are tasked with approximating nonlinear memory functions and the readout function, (2b) while Attn layers are responsible for extracting the tokens from the memory locations. ", "page_idx": 65}, {"type": "text", "text": "Insight (3). The functionality and necessity of Dot-product (DP). (3a) For the relatively simple Task I, DP is not necessary and can be omitted. (3b) However, for the more complex Task II, DP provides necessary nonlinearity: the cooperation between DP and RPE provides the needed interaction between the temporal space and the token space. ", "page_idx": 65}, {"type": "text", "text": "Insight (4). The efficiency of Relative Positional Encoding (RPE) in modeling long-range correlations. The primary role of RPE is to approximate the memory kernels. (4a) Transformer with log-type RPE can handle heavy-tailed memories. (4b) Transformer with lin-type RPE can handle light-tailed memories. ", "page_idx": 65}, {"type": "text", "text": "H.2  Experimental Validation ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "To validation of our theoretical insights $(\\mathbf{1}\\mathbf{a}){\\sim}(\\mathbf{4}\\mathbf{b})$ , we conduct 8 experiments, from simple toy models to more complex LLM pre-training. The experiments are conducted on 1 A100. ", "page_idx": 65}, {"type": "text", "text": "H.2.1 Validation of Insight (1la) ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Objective. As indicated in Section 4, numerous NLP tasks exhibit complex interrelationships among tokens and belong to our Task II. This experiment aims to verify our Insight (1a): for such tasks, increasing the number of layers $L$ is more efficient than increasing the number of Attn heads $H$ ", "page_idx": 65}, {"type": "text", "text": "Setup. Specifically, we pretrain decoder-only Transformers (Vaswani et al., 2017) with different $L$ and $H$ on the Open WebText dataset (Gokaslan and Cohen, 2019) for 10,000 iterations (approximately 1B tokens) on 1 A100, using cross-entropy loss and AdamW with the same hyperparameters. To ensure comparability, we meticulously balance the total number of parameters across both experimental setups. ", "page_idx": 65}, {"type": "text", "text": "Results and conclusion. The final validation losses are shown in Table 2. By comparing these two subtables, the benefits brought by increasing $L$ are much greater than the benefits brought by increasing $H$ $[0.802>0.136)$ , thereby corroborating our Insight (1la). ", "page_idx": 65}, {"type": "table", "img_path": "0o7Rd5jngV/tmp/876828f48124aa00c0d05f889b412209eacf6f804876c4f27f104ebc72bf7db2.jpg", "table_caption": ["Table 2: Results of the experiment supporting Insight (1a). "], "table_footnote": [], "page_idx": 65}, {"type": "text", "text": "H.2.2 Validation of Insight (1b) ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Objective. As mentioned in Section 3, sparse Boolean functions have no interactions among the memories and belong to our Task I. This experiment aims to verify our Insight (1b): for such tasks, a single-layer Transformer equipped with a sufficient number of Attn heads $H$ and FFN width $m$ suffices, and there is no need to increase the number of layers $L$ ", "page_idx": 65}, {"type": "text", "text": "Setup.  Specifically,  we train  single-layer DP-free  Transformers with different $H$ and $m$ to  learn  a  sparse  Boolean  target  function $f^{*}$ $\\begin{array}{r l r l r}{f^{\\ast}(\\pmb{x})}&{{}:=}&{g^{\\ast}(x_{48},x_{56},x_{99})}&{{}:=}&{}\\end{array}$ $\\begin{array}{r l}{\\small}&{{}\\sum_{k=1}^{64}\\mathrm{ReLU}(\\langle w_{k}^{*},(x_{48},x_{56},x_{99})\\rangle)}\\end{array}$ for input sequence $\\pmb{x}\\,=\\,(x_{1},\\cdot\\cdot\\cdot\\,,x_{1000})\\,\\in\\,\\{\\pm1\\}^{1000}$ where $\\pmb{w}_{k}^{*}$ are generated by $\\pmb{w}_{k}^{*}\\sim N(0,I_{3})$ . Training proceeds for 10,000 iterations (1M samples) using squared loss and Adam W with the same hyperparameters. ", "page_idx": 66}, {"type": "text", "text": "Results and conclusion. The final validation losses are shown in Table 3. As shown in this table, a single-layer Transformer equipped with a sufficient $H$ (32) and $m$ (256) is adequate for representing this sparse Boolean function. This empirical evidence supports our Insight (1b). ", "page_idx": 66}, {"type": "table", "img_path": "0o7Rd5jngV/tmp/fc3aecf14d7be812ae57804d1882210ed7d62b150f702dd5e741a80a73053891.jpg", "table_caption": ["Table 3: Results of the experiment supporting Insight (1b) "], "table_footnote": [], "page_idx": 66}, {"type": "text", "text": "H.2.3 Validation of Insight (2a) ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Objective. This experiment aims to verify our Insight (2a): to learn a sparse Boolean function with a \"complex\u201d readout function and \u201csimple\u201d memories, increasing the FFN width $m$ can significantly improve the performance, whereas increasing the number of Attn heads $H$ brings almost no benefit. ", "page_idx": 66}, {"type": "text", "text": "Setup. Specifically, we train single-layer DP-free Transformers with different $H$ and $m$ to learn a sparse Boolean function with a \"complex\" readout function $(g^{*})$ and a \"simple\" single memory $\\left(x_{99}\\right)$ $\\begin{array}{r}{f^{*}({\\pmb x}):=g^{*}(x_{99}):=\\sum_{k=1}^{64}\\operatorname{ReLU}({\\pmb w}_{k}^{*}\\cdot x_{99})}\\end{array}$ for any input sequence $\\pmb{x}=(x_{1},\\cdot\\cdot\\cdot\\mathrm{~,~}x_{1000})\\in$ $\\{\\pm1\\}^{1000}$ , where $\\pmb{w}_{k}^{*}$ are generated by $\\pmb{w}_{k}^{*}\\sim N(0,1)$ . Training proceeds for 10,000 iterations (1M samples) using squared loss and Adam W with the same hyperparameters. ", "page_idx": 66}, {"type": "text", "text": "Results and conclusion. The final validation losses are shown in Table 4. The tables indicate that, for learning a sparse Boolean function with a \u201ccomplex\u201d readout function and \u201csimple\u201d memories, increasing $m$ can significantly improve the performance ( $\\mathrm{{[0.49\\rightarrow0.002})}$ , almost completing this task perfectly. Conversely, increasing $H$ fails to yield substantial improvement. This empirical evidence supports our Insight (2a). ", "page_idx": 66}, {"type": "table", "img_path": "0o7Rd5jngV/tmp/07696613a6c0633ec3d223bf561b6e3c14f8f0bc5bece65173d6ff85ec15fd62.jpg", "table_caption": ["Table 4: Results of the experiment supporting Insight (2a). "], "table_footnote": [], "page_idx": 66}, {"type": "text", "text": "H.2.4 Validation of Insight (2b) ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Objective. Contrasting with Experiment (2a), this experiment aims to verify our Insight (2b): for learning a sparse Boolean function with a \u201csimple\u201d\" readout function and \u201ccomplex\u201d memories, increasing the number of Attn headers $H$ can substantially improve the performance while increasing FFNwidth $m$ will offer almost no benefit. ", "page_idx": 66}, {"type": "text", "text": "Setup. Specifically, we train single-layer DP-free Transformers with different $H$ and $m$ to learn a sparse Boolean function with a \u201csimple\u201d linear readout function $(g^{*})$ and relatively \u201ccomplex\" memories $(x_{48},x_{56},x_{99})$ $f^{*}({\\pmb x}):=g^{*}(x_{48},x_{56},x_{99}):=x_{48}+x_{56}\\dot{+}x_{99}$ for any input sequence $\\pmb{x}=(x_{1},\\cdot\\cdot\\cdot\\cdot,x_{1000})\\in\\{\\pm1\\}^{1000}$ Training processes for 10,000iterations (1M samples),using squared loss and Adam W with the same hyperparameters. ", "page_idx": 66}, {"type": "text", "text": "Results and conclusion. The final validation losses are presented in Table 5. The tables indicate that, for learning a sparse Boolean function with a \u201csimple\u201d readout function and \u201ccomplex\u201d memories, increasing $m$ can significantly improve the performance $\\left(1.16\\,\\rightarrow\\,10^{-6}\\right)$ 0, closely achieving task perfection. In contrast, increasing $m$ brings almost no benefits. This empirical evidence supports our Insight 2(b). ", "page_idx": 66}, {"type": "text", "text": "H.2.5 Validation of Insight (3a) ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Objective. As mentioned in Section 3, learning sparse Boolean functions has no interactions among the memories and belongs to our Task 1. This experiment aims to verify our insight (3a): for such tasks, a DP-free Transformer equipped with a sufficient number of Attn heads $H$ and FFN width $m$ is sufficiently capable. Moreover, there is no need to use DP structure in Attn. ", "page_idx": 66}, {"type": "table", "img_path": "0o7Rd5jngV/tmp/e5888559f30e59805abccdad2d467e09dad192bba53aa84f9fd5cdde544b8e6b.jpg", "table_caption": ["Table 5: Results of the experiment supporting Insight (2b) "], "table_footnote": [], "page_idx": 67}, {"type": "text", "text": "", "page_idx": 67}, {"type": "text", "text": "Setup. Specifically, we train single-layer DP-free Transformers with different $H$ and $m$ and with DP or without DP to learn a sparse Boolean target function $f^{*}\\colon f^{*}({\\pmb x}):=g^{*}(x_{48},x_{56},x_{99})\\ :=$ $\\begin{array}{r l}{\\small}&{{}\\sum_{k=1}^{64}\\mathrm{ReLU}(\\langle\\pmb{w}_{k}^{*},(x_{48},x_{56},x_{99})\\rangle)}\\end{array}$ foriapu seqguence $\\pmb{x}\\,=\\,(x_{1},\\cdot\\cdot\\cdot\\,,x_{1000})\\,\\in\\,\\{\\pm1\\}^{1000}$ wWhere $\\pmb{w}_{k}^{*}$ are generated by $\\pmb{w}_{k}^{*}\\sim N(0,I_{3})$ . Training proceeds for 10,000 iterations (1M samples) using squared loss and Adam W with the same hyperparameters. ", "page_idx": 67}, {"type": "text", "text": "Results and conclusion. The final validation losses are shown in Table 6. The findings illustrate that a DP-free Transformer equipped with a sufficient $H$ (32) and $m$ (256) is adept at accurately representing the given sparse Boolean function. Additionally, the Incorporation of the DP structure into the layers contributes marginally to performance enhancement. This substantiates our Insight (3a). ", "page_idx": 67}, {"type": "table", "img_path": "0o7Rd5jngV/tmp/96d2ccb669aa20fdb09b15fb4a6711ff5614b7cd955df9c835b41efc1f19f054.jpg", "table_caption": ["Table 6: Results of the experiment supporting Insight (3a) "], "table_footnote": [], "page_idx": 67}, {"type": "text", "text": "H.2.6 Validation of Insight (3b) ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "Objective. As indicated in Section 4, numerous NLP tasks exhibit complex interrelationships among tokens and belong to our Task I1. This experiment aims to verify our Insight (3b): for such tasks, the utilization of DP structure in Attn layers is necessary. ", "page_idx": 67}, {"type": "text", "text": "Setup. Specifically, we pre-train Transformers with DP or without DP on the Open WebText dataset for 10,000 iterations (approximately 1B tokens) on 1 A100, using cross-entropy loss and AdamW with the same hyperparameters. ", "page_idx": 67}, {"type": "text", "text": "Results and conclusion. The final validation losses are presented in Table 7. As shown in the table, for NLP pre-training tasks, Transformer incorporating DP structure is more efficient than Transformer withoutDP $(5.796<5.830,5.374<5.486,4.994<5.274)$ , thereby supporting our Insight 3(b). ", "page_idx": 67}, {"type": "table", "img_path": "0o7Rd5jngV/tmp/05f2a0317b6eade9619861908c552580b5c15fed7b702c76c6095185a0afba2d.jpg", "table_caption": ["Table 7: Results of the experiment supporting Insight (3b) "], "table_footnote": [], "page_idx": 67}, {"type": "text", "text": "H.2.7  Validation of Insight (4a) ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "Objective. This experiment aims to verify our Insight (4a): for learning Task III with heavy-tailed memories, Transformers with log-type RPE are efficient, whereas those with lin-type RPE fail. ", "page_idx": 67}, {"type": "text", "text": "Setup. Specifically, we train single-layer, FFN-free, DP-free Transformers with log-type RPE or lintype RPE and varying numbers of Attn heads $H$ . The target function involves a heavy-tailed memory kermel $\\rho(t)=t^{-0.5}$ $\\begin{array}{r}{\\bar{f}^{*}(\\pmb{x}):=\\sum_{s=1}^{1000}x_{s}\\rho(1000-s)}\\end{array}$ for any input sequence $\\pmb{x}=(x_{1},\\cdot\\cdot\\cdot\\mathrm{~,~}x_{1000})\\in$ $\\{\\pm1\\}^{1000}$ . Training processes for 10.000 iterations (1M samples) using squared loss and AdamW with the same hyperparameters. ", "page_idx": 67}, {"type": "text", "text": "Results and conclusion. The final validation losses are shown in Table 8. As shown in the table, to learn heavy-tailed memories, even single-head Transformer with log-type RRE can complete it perfectly $(<10^{-5})$ 0. Conversely, Transformers employing lin-type RRE exhibit limited improvement even with up to 64 heads (0.19). This empirical evidence supports our Insight (4a). ", "page_idx": 67}, {"type": "text", "text": "", "page_idx": 68}, {"type": "table", "img_path": "0o7Rd5jngV/tmp/eab48b5b6ddededce566564c9759c53a83047339238604f27ab27996bfa0f40b.jpg", "table_caption": ["Table 8: Results of the experiment supporting Insight (4a) "], "table_footnote": [], "page_idx": 68}, {"type": "text", "text": "H.2.8  Validation of Insight (4b) ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "Objective. In contrast to Experiment (4a), this experiment aims to verify that for learning our Task III with light-tailed memories, Transformers with lin-type RPE are efficient, whereas those with log-type RPE fail. ", "page_idx": 68}, {"type": "text", "text": "Setup. Specifically, we train single-layer, FFN-free, DP-free Transformers with log-type RPE or lintype RPE and varying numbers of Attn heads $H$ . The target function involves a heavy-tailed memory kermel $\\rho(t)=e^{-5t}$ $\\begin{array}{r}{\\bar{f}^{*}({\\pmb x}):=\\sum_{s=1}^{1000}x_{s}\\rho(1000-s)}\\end{array}$ for any input sequence $\\pmb{x}=(x_{1},\\cdot\\cdot\\cdot\\mathrm{~,~}x_{1000})\\in$ $\\{\\pm1\\}^{1000}$ . Training processes for 10,000 iterations (1M samples) using squared loss and Adam W with the same hyperparameters. ", "page_idx": 68}, {"type": "text", "text": "Results and conclusion. The final validation losses are shown in Table 9. As shown in the table, to learn light-tailed memories, even single-head Transformer with lin-type RRE can complete it perfectly $\\bar{(<10^{-7})}$ . Conversely, Transformers employing log-type RRE exhibit limited improvement even with upto64 heads $(5.3\\stackrel{.}{\\times}10^{-4})$ . This empirical evidence supports our Insight (4b). ", "page_idx": 68}, {"type": "table", "img_path": "0o7Rd5jngV/tmp/730f1f92a7676bafd3f2ad73c57e657f8679f836faf91490b538709014e1bbfe.jpg", "table_caption": ["Table 9: Results of the experiment supporting Insight (4b) "], "table_footnote": [], "page_idx": 68}, {"type": "text", "text": "H.3  Practical Implications ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "Our theoretical insights and empirical evidence can directly lead to the following 8 practical implications, such as the strategic selection of Transformer hyperparameters for specific tasks. ", "page_idx": 68}, {"type": "text", "text": "\u00b7 Implication (1a). (supported by Insight (1a) and Experiment (1a) For sequence modeling tasks with complex interrelations between memories, such as many NLP applications, enhancing the number of layers $L$ is more beneficial than increasing the number of Attn heads $H$ andFFNwidth $m$   \n\u00b7 Implication (1b). (supported by Insight (1b) and Experiment (1b)) For simple sequence modeling tasks with almost no memory intercorrelation, such as the learning of sparse Boolean function, improving performance necessitates only sufficient $H$ and $m$ in a single-layer Transformer, without a need to increase $L$   \n\u00b7 Implication (2a). (supported by Insight (2a) and Experiment (2a)) For sequence modeling tasks with complex readout or memory functions, increasing $m$ can significantly improve performance.   \n\u00b7 Implication (2b). (supported by Insight (2b) and Experiment (2b)) For sequence modeling tasks with multiple memories, increasing $H$ can markedly improve performance.   \n\u00b7 Implication (3a). (supported by Insight (3a) and Experiment (3a)) For simple sequence modeling tasks with almost no memory correlations, such as learning sparse Boolean functions, omitting the DP structure in Attn layers can still perform well.   \n\u00b7 Implication (3b). (supported by Insight (3b) and Experiment (3b)) ", "page_idx": 68}, {"type": "text", "text": "For sequence modeling tasks with complex correlations between memories, such as many NLP tasks, preserving the DP structure in attention layers is crucial for achieving high performance due to its indispensable nonlinearity. ", "page_idx": 69}, {"type": "text", "text": "\u00b7 Implication (4a). (supported by Insight (4a) and Experiment (4a) For sequence modeling tasks with heavy-tailed memories, the employment of log-type RPE (such as T5's RPE and KERPLE (log)) is recommended over lin-type RPE (such as Alibi).   \n\u00b7 Implication (4b). (supported by Insight (4b) and Experiment (4b)) For sequence modeling tasks with light-tailed memories, the employment of lin-type RPE (such as Alibi) is recommended over log-type RPE (such as T5's RPE and KERPLE (log)). ", "page_idx": 69}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 70}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 70}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 70}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 70}, {"type": "text", "text": "Justification: We believe that the abstract and introduction refect the contributions and scope of thepaper. ", "page_idx": 70}, {"type": "text", "text": "Guidelines: ", "page_idx": 70}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 70}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 70}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 70}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 70}, {"type": "text", "text": "Justification: In Section 7. ", "page_idx": 70}, {"type": "text", "text": "Guidelines: ", "page_idx": 70}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational effciency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 70}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 70}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 70}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 70}, {"type": "text", "text": "Justification: In Section 2, 3, 4, and 5; Appendix B, C, D, E, and F. Guidelines: ", "page_idx": 71}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 71}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 71}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 71}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 71}, {"type": "text", "text": "Justification: We believe that all of the experimental results are reproducible in our work. Guidelines: ", "page_idx": 71}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b)  If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 71}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 71}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 71}, {"type": "text", "text": "Answer: [No] ", "page_idx": 72}, {"type": "text", "text": "Justification: The code or data of the experiments are simple and easy to reproduce following the description in the paper. ", "page_idx": 72}, {"type": "text", "text": "Guidelines: ", "page_idx": 72}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 72}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 72}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 72}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 72}, {"type": "text", "text": "Justification: In Appendix H. Guidelines: ", "page_idx": 72}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 72}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 72}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 72}, {"type": "text", "text": "Answer: [No] ", "page_idx": 72}, {"type": "text", "text": "Justification: the approximation error is deterministic and there is no need to consider the error barshere. ", "page_idx": 72}, {"type": "text", "text": "Guidelines: ", "page_idx": 72}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 72}, {"type": "text", "text": "", "page_idx": 73}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 73}, {"type": "text", "text": "Answer: [Yes] Justification: In Section H. ", "page_idx": 73}, {"type": "text", "text": "", "page_idx": 73}, {"type": "text", "text": "Guidelines: ", "page_idx": 73}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 73}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 73}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 73}, {"type": "text", "text": "Justification: We have confirmed that the research is conducted with the NeurIPS Code of Ethics. ", "page_idx": 73}, {"type": "text", "text": "Guidelines: ", "page_idx": 73}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 73}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 73}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 73}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 73}, {"type": "text", "text": "Guidelines: ", "page_idx": 73}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 73}, {"type": "text", "text": "", "page_idx": 74}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 74}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 74}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 74}, {"type": "text", "text": "", "page_idx": 74}, {"type": "text", "text": "Guidelines: ", "page_idx": 74}, {"type": "text", "text": "\u00b7 I'he answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 74}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 74}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 74}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 74}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 74}, {"type": "text", "text": "Guidelines: ", "page_idx": 74}, {"type": "text", "text": "\u00b7 'The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 74}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 75}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 75}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 75}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 75}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 75}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 75}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 75}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 75}, {"type": "text", "text": "Guidelines: ", "page_idx": 75}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 75}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 75}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 75}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 75}, {"type": "text", "text": "Guidelines: ", "page_idx": 75}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 75}]