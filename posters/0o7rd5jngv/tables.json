[{"figure_path": "0o7Rd5jngV/tables/tables_63_1.jpg", "caption": "Table 1: An example of standard T5's Relative Positional Encoding", "description": "This table shows an example of the relative positional encoding used in the T5 Transformer model.  The table illustrates how the function -r(t-s) maps the relative distance (t-s) between tokens t and s to a numerical value.  For small distances (t-s < B), the encoding is linear, and for larger distances (B \u2264 t-s < D), it decays logarithmically, showing the model's handling of long-range dependencies.", "section": "G.1 T5's Relative Positional Encoding"}, {"figure_path": "0o7Rd5jngV/tables/tables_65_1.jpg", "caption": "Table 2: Results of the experiment supporting Insight (1a).", "description": "This table presents the results of an experiment designed to validate the first insight (1a) from the paper.  The experiment compares the performance of Transformer models with varying numbers of layers (L) and attention heads (H) on a language modeling task. The results show that increasing the number of layers (L) significantly reduces the validation loss, while increasing the number of attention heads (H) has a much smaller effect.  The number of parameters in each model is roughly balanced, allowing a fair comparison.", "section": "H.2 Experimental Validation"}, {"figure_path": "0o7Rd5jngV/tables/tables_66_1.jpg", "caption": "Table 3: Results of the experiment supporting Insight (1b).", "description": "The table shows the final validation losses for learning a sparse Boolean function with different numbers of attention heads (H) and FFN width (m).  The results support the insight that a single-layer Transformer with sufficient H and m can effectively model such a function, without needing additional layers.", "section": "H.2 Experimental Validation"}, {"figure_path": "0o7Rd5jngV/tables/tables_66_2.jpg", "caption": "Table 4: Results of the experiment supporting Insight (2a).", "description": "This table presents the results of an experiment designed to validate Insight 2a of the paper.  Insight 2a posits that for tasks involving a complex readout function and simple memories, increasing the FFN width (m) significantly improves performance, while increasing the number of attention heads (H) has little to no effect. The table shows the final validation loss for different combinations of H and m, demonstrating the significant performance gain achieved by increasing m and the negligible effect of increasing H.", "section": "H.2 Experimental Validation"}, {"figure_path": "0o7Rd5jngV/tables/tables_67_1.jpg", "caption": "Table 3: Results of the experiment supporting Insight (1b).", "description": "This table presents the results of an experiment designed to validate the hypothesis that for tasks with memories lacking intricate interrelationships (Type I), single-layer Transformers with sufficient Attn heads and FFN width suffice.  The experiment involved training single-layer DP-free Transformers with varying numbers of attention heads (H) and FFN widths (m) to learn a sparse Boolean target function. The validation loss is reported for each configuration, demonstrating that a single-layer model can achieve low loss with sufficient width and head count.", "section": "H.2.2 Validation of Insight (1b)"}, {"figure_path": "0o7Rd5jngV/tables/tables_67_2.jpg", "caption": "Table 6: Results of the experiment supporting Insight (3a).", "description": "This table presents the final validation loss for learning a sparse Boolean function with a simple readout function and simple memories.  The experiment compared the performance of single-layer DP-free transformers with different numbers of attention heads (H) and FFN widths (m), both with and without the dot product (DP) structure in the attention layer. The results show that a single-layer DP-free transformer with sufficient H and m can effectively learn this sparse Boolean function, and the inclusion of DP provides only a minor improvement in performance.", "section": "H.2.5 Validation of Insight (3a)"}, {"figure_path": "0o7Rd5jngV/tables/tables_67_3.jpg", "caption": "Table 7: Results of the experiment supporting Insight (3b).", "description": "This table presents the final validation losses for Transformer models with and without the dot-product (DP) structure in the attention layer. The results show that, for NLP pre-training tasks, Transformer models incorporating DP structure achieve lower validation losses than DP-free Transformer models, thus supporting the necessity of DP in achieving higher performance for complex NLP tasks.", "section": "H.2 Experimental Validation"}, {"figure_path": "0o7Rd5jngV/tables/tables_68_1.jpg", "caption": "Table 8: Results of the experiment supporting Insight (4a)", "description": "This table presents the final validation losses for learning heavy-tailed memories using single-layer, FFN-free, DP-free Transformers with log-type RPE or lin-type RPE and varying numbers of attention heads.  The results show that for heavy-tailed memories, even a single-head Transformer with log-type RPE performs well, while lin-type RPE shows limited improvement even with many heads.", "section": "H.2 Experimental Validation"}, {"figure_path": "0o7Rd5jngV/tables/tables_68_2.jpg", "caption": "Table 8: Results of the experiment supporting Insight (4a).", "description": "This table presents the results of an experiment designed to validate Insight (4a) of the paper. The experiment focuses on evaluating the performance of Transformers with different types of relative positional encoding (RPE) for sequence modeling tasks involving heavy-tailed memories.  The results show that Transformers with log-type RPE achieve significantly better performance than those with lin-type RPE, even with a small number of attention heads. This supports the claim that log-type RPE is more efficient for handling heavy-tailed memories.", "section": "H.2 Experimental Validation"}]