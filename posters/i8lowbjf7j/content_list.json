[{"type": "text", "text": "Interpretable Lightweight Transformer via Unrolling of Learned Graph Smoothness Priors ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tam Thuc Do   \nYork University   \nToronto, Canada   \ndtamthuc@yorku.ca   \nParham Eftekhar   \nYork University   \nToronto, Canada   \neftekhar@yorku.ca   \nSeyed Alireza Hosseini   \nYork University   \nToronto, Canada   \nahoseini@yorku.ca ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Gene Cheung York University Toronto, Canada genec@yorku.ca ", "page_idx": 0}, {"type": "text", "text": "Philip A. Chou packet.media Seattle, USA pachou@ieee.org ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We build interpretable and lightweight transformer-like neural networks by unrolling iterative optimization algorithms that minimize graph smoothness priors\u2014 the quadratic graph Laplacian regularizer (GLR) and the $\\ell_{1}$ -norm graph total variation (GTV)\u2014subject to an interpolation constraint. The crucial insight is that a normalized signal-dependent graph learning module amounts to a variant of the basic self-attention mechanism in conventional transformers. Unlike \u201cblack-box\u201d transformers that require learning of large key, query and value matrices to compute scaled dot products as affinities and subsequent output embeddings, resulting in huge parameter sets, our unrolled networks employ shallow CNNs to learn lowdimensional features per node to establish pairwise Mahalanobis distances and construct sparse similarity graphs. At each layer, given a learned graph, the target interpolated signal is simply a low-pass flitered output derived from the minimization of an assumed graph smoothness prior, leading to a dramatic reduction in parameter count. Experiments for two image interpolation applications verify the restoration performance, parameter efficiency and robustness to covariate shift of our graph-based unrolled networks compared to conventional transformers. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Focusing on the self-attention mechanism [1] as the basic building block\u2014where the affinity between two input tokens is computed as a transformed dot product\u2014transformers [2] learn large parameter sets to achieve state-of-the-art (SOTA) performance in a wide range of signal prediction/classification problems [3, 4], outperforming convolutional neural nets (CNNs) and recurrent neural nets (RNNs). However, there are shortcomings: i) lack of mathematical interpretability to characterize general performance1, ii) requiring substantial training datasets to train sizable parameters [12], and iii) fragility to covariate shift\u2014when training and testing data have different distributions [13]. ", "page_idx": 0}, {"type": "text", "text": "Orthogonally, algorithm unrolling [14] implements iterations of a model-based algorithm as a sequence of neural layers to build a feed-forward network, whose parameters can be learned endto-end via back-propagation from data. A classic example is the unrolling of the iterative softthresholding algorithm (ISTA) in sparse coding into Learned ISTA (LISTA) [15]. Recently, [16] showed that by unrolling an iterative algorithm minimizing a sparse rate reduction (SRR) objective, it can lead to a family of \u201cwhite-box\u201d transformer-like deep neural nets that are $100\\%$ mathematically interpretable. Inspired by [16], in this work we also seek to build white-box transformers via algorithm unrolling, but from a unique graph signal processing (GSP) perspective [17, 18, 19]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Over the last decade and a half, GSP studies spectral analysis and processing of discrete signals on structured data kernels described by finite graphs. Specifically, by assuming that the sought signal is smooth (low-pass) with respect to (w.r.t.) a particular graph, a plethora of graph-based restoration algorithms can be designed for practical applications, including image denoising [20], JPEG dequantization [21], interpolation [22], 3D point cloud denoising [23, 24] and super-resolution [25]. At the heart of GSP is the construction of a similarity graph that captures pairwise similarities between signal samples on two connected nodes. We first demonstrate that signal-dependent similarity graph learning with normalization is akin to affinity computation in the self-attention mechanism. Thus, our first contribution is to show that unrolling of a graph-based iterative algorithm2 with normalized graph learning results in an interpretable transformer-like feed-forward network. ", "page_idx": 1}, {"type": "text", "text": "Second, computation of a positive weight $w_{i,j}\\,=\\,\\exp(-d(i,j))$ of an edge $(i,j)$ connecting two graph nodes $i$ and $j$ often employs Mahalanobis distance $d(i,j)=(\\mathbf{f}_{i}-\\mathbf{f}_{j})^{\\top}\\mathbf{M}(\\mathbf{f}_{i}-\\mathbf{f}_{j})$ between representative (e.g., CNN-computed) feature vectors $\\mathbf{f}_{i}$ and $\\mathbf{f}_{j}$ , where $\\mathbf{f}_{i},\\mathbf{f}_{j}\\,\\in\\,\\mathbb{R}^{D}$ reside in lowdimensional space [27, 28]. Hence, unlike a conventional transformer that requires large key and query matrices, $\\mathbf{K}$ and $\\mathbf{Q}$ , to compute transformed dot products, the similarity graph learning module can be more parameter-efficient. Moreover, by adding a graph smoothness prior such as graph Laplacian regularizer (GLR) [18, 20] or graph total variation (GTV) [29, 30, 31] in the optimization objective, once a graph $\\mathcal{G}$ is learned, the target signal is simply computed as the low-pass filtered output derived from the minimization of the assumed graph smoothness prior. Thus, a large value matrix $\\mathbf{V}$ to compute output embeddings typical in a transformer is also not needed. Our second contribution is to demonstrate that a lightweight transformer with fewer parameters can be built via unrolling of a graph-based restoration algorithm with a chosen graph signal smoothness prior. ", "page_idx": 1}, {"type": "text", "text": "Specifically, focusing on the signal interpolation problem, we first derive linear-time graph-based algorithms by minimizing GLR or GTV, via conjugate gradient (CG) [32] or a modern adaptation of alternative method of multipliers (ADMM) for sparse linear programming (SLP) [33], respectively. In each iteration, given a learned graph, each algorithm deploys a low-pass graph fliter to interpolate the up-sampled observation vector into the target signal. We intersperse unrolled algorithm iterations with graph learning modules into a compact and interpretable neural network. We demonstrate its restoration performance, parameter efficiency ( $3\\%$ of SOTA\u2019s parameters in one case), and robustness to covariate shift for two practical applications: image demosaicking, and image interpolation. ", "page_idx": 1}, {"type": "text", "text": "Notation: Vectors and matrices are written in bold lowercase and uppercase letters, respectively. The $(i,j)$ element and the $j$ -th column of a matrix A are denoted by $A_{i,j}$ and ${\\bf a}_{j}$ , respectively. The $i$ -th element in the vector a is denoted by $a_{i}$ . The square identity matrix of rank $N$ is denoted by ${\\mathbf{I}}_{N}$ , the $M$ -by- $N$ zero matrix is denoted by $\\mathbf{0}_{M,N}$ , and the vector of all ones / zeros of length $N$ is denoted by ${\\mathbf{1}}_{N}\\mathrm{~/~}{\\mathbf{0}}_{N}$ , respectively. Operator $\\|\\cdot\\|_{p}$ denotes the $\\ell{-}p$ norm. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 GSP Definitions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "A graph $\\mathcal{G}(\\mathcal{N},\\mathcal{E},\\mathbf{W})$ is defined by a node set $\\mathcal{N}=\\{1,\\dots,N\\}$ and an edge set $\\mathcal{E}$ of size $|\\mathcal{E}|=M$ , where $(i,j)\\in\\mathcal{E}$ means nodes $i,j\\in N$ are connected with weight $w_{i,j}=W_{i,j}\\in\\mathbb{R}$ . In this paper, we consider only positive graphs $\\mathcal{G}$ with no self-loops, i.e., $w_{i,j}\\geq0,\\forall i,j$ , and $w_{i,i}=0,\\forall i$ . We assume edges are undirected, and thus adjacency matrix $\\mathbf{W}\\,\\in\\,\\mathbb{R}^{N\\times N}$ is symmetric. The combinatorial graph Laplacian matrix is defined as $\\mathbf{L}\\triangleq\\mathbf{D}-\\mathbf{W}\\in\\mathbb{R}^{N\\times N}$ , where $\\mathbf{D}\\triangleq\\mathrm{diag}(\\mathbf{W}\\mathbf{1}_{N})$ is the degree matrix, and $\\mathrm{diag}(\\mathbf{v})$ returns a diagonal matrix with $\\mathbf{v}$ along its diagonal. L for a positive graph $\\mathcal{G}$ is provably positive semi-definite (PSD), i.e., all its eigenvalues $\\lambda_{i}$ \u2019s are non-negative [19]. ", "page_idx": 1}, {"type": "text", "text": "We define also the incidence matrix $\\mathbf{C}=\\mathbb{R}^{M\\times N}$ : each $k$ -th row of $\\mathbf{C}$ corresponds to the $k$ -th edge $(i,j)\\in\\mathcal{E}$ , where $C_{k,i}=w_{i,j}$ , $C_{k,j}=-w_{i,j}$ , and $C_{k,l}=0,\\,\\forall l\\neq i,j$ . Since our assumed graph $\\mathcal{G}$ is undirected, the polarities of $C_{k,i}$ and $C_{k,j}$ are arbitrary, as long as they are opposite. ", "page_idx": 2}, {"type": "text", "text": "2.2 Graph Laplacian Regularizer ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given a positive connected graph $\\mathcal{G}$ with $N$ nodes and $M$ edges, we first define smoothness of a signal $\\mathbf{x}\\in\\mathbb{R}^{N}$ w.r.t. $\\mathcal{G}$ using the graph Laplacian regularizer (GLR) [18, 20] as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}\\|_{\\mathcal{G},2}=\\mathbf{x}^{\\top}\\mathbf{L}\\mathbf{x}=\\sum_{(i,j)\\in\\mathcal{E}}w_{i,j}(x_{i}-x_{j})^{2}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{L}$ is a combinatorial Laplacian matrix specifying graph $\\mathcal{G}$ . GLR (1) is non-negative for a positive graph, and thus is suitable as a signal prior for minimization problems [20, 21]. ", "page_idx": 2}, {"type": "text", "text": "2.3 Graph Total Variation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Instead of GLR (1), we can alternatively define graph signal smoothness using graph total variation (GTV) [29, 30, 31] $\\|\\mathbf{x}\\|_{\\mathcal{G},1}$ for signal $\\mathbf{x}\\in\\mathbb{R}^{N}$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}\\|_{\\mathcal{G},1}=\\|\\mathbf{C}\\mathbf{x}\\|_{1}\\overset{(a)}{=}\\sum_{(i,j)\\in\\mathcal{E}}w_{i,j}|x_{i}-x_{j}|\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $(a)$ is true since $\\mathcal{G}$ is positive. GTV is also non-negative for positive $\\mathcal{G}$ , and has been used as a signal prior for restoration problems such as image deblurring [34]. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Formulation & Optimization using GLR ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first assume a positive, sparse and connected graph $\\mathcal{G}$ with $N$ nodes and $M$ edges specified by graph Laplacian matrix L. By sparse, we mean that $M$ is $\\mathcal{O}(N)$ and not $\\mathcal{O}(N^{2})$ . By connected, we mean that any node $j$ can be traversed from any other node $i$ . Given $\\mathcal{G}$ , we first derive a linear-time iterative algorithm to interpolate signal $\\mathbf{x}$ by minimizing GLR given observed samples y. In Section 4, we derive an algorithm by minimizing GTV instead given y. In Section 5, we unroll iterations of one of two derived algorithms into neural layers, together with strategically inserted graph learning modules, to construct graph-based lightweight transformer-like neural nets. ", "page_idx": 2}, {"type": "text", "text": "We first employ GLR [20] as the objective to reconstruct $\\mathbf{x}\\in\\mathbb{R}^{N}$ given partial observation $\\mathbf{y}\\in\\mathbb{R}^{K}$ , where $K<N$ . Denote by $\\mathbf{H}\\in\\{0,\\^{\\bullet}1\\}^{K\\times N}$ a sampling matrix defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\nH_{i,j}=\\left\\{\\begin{array}{l l}{1}&{\\mathrm{if}\\;\\mathrm{node}\\;j\\;\\mathrm{is}\\;\\mathrm{the}\\;i\\mathrm{-th}\\;\\mathrm{sample}}\\\\ {0}&{\\mathrm{o.w.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "that picks out $K$ samples from signal $\\mathbf{x}$ . The optimization is thus ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}}\\;\\mathbf{x}^{\\top}\\mathbf{L}\\mathbf{x},\\;\\;\\;\\;\\;\\;\\mathrm{s.t.}\\;\\;\\mathbf{H}\\mathbf{x}=\\mathbf{y}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ${\\bf L}\\in\\mathbb{R}^{N\\times N}$ is a graph Laplacian matrix corresponding to a positive graph $\\mathcal{G}$ [19]. PSD L implies that $\\mathbf{x}^{\\top}\\mathbf{L}\\mathbf{x}\\geq0,\\bar{\\forall}\\mathbf{x}$ , and thus (4) has a convex objective with a linear interpolation constraint. ", "page_idx": 2}, {"type": "text", "text": "3.2 Optimization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We solve (4) via a standard Lagrangian approach [35] and write its corresponding unconstrained Lagrangian function $f(\\mathbf{x},\\pmb{\\mu})$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\nf(\\mathbf{x},\\pmb{\\mu})=\\mathbf{x}^{\\top}\\mathbf{L}\\mathbf{x}+\\pmb{\\mu}^{\\top}(\\mathbf{H}\\mathbf{x}-\\mathbf{y})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ${\\pmb{\\mu}}\\in\\mathbb{R}^{K}$ is the Lagrange multiplier vector. To minimize $f(\\mathbf{x},\\pmb{\\mu})$ in (5), we take the derivative w.r.t. $\\mathbf{x}$ and $\\pmb{\\mu}$ separately and set them to zero, resulting in the following linear system: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\underbrace{\\left[\\begin{array}{l l}{2\\mathbf{L}}&{\\mathbf{H}^{\\top}}\\\\ {\\mathbf{H}}&{\\mathbf{0}_{K,K}}\\end{array}\\right]}_{\\mathbf{P}}\\left[\\begin{array}{l}{\\mathbf{x}}\\\\ {\\mu}\\end{array}\\right]=\\left[\\begin{array}{l}{\\mathbf{0}_{N,N}}\\\\ {\\mathbf{y}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Given that the underlying graph $\\mathcal{G}$ is positive and connected, coefficient matrix $\\mathbf{P}$ is provably full-rank and thus invertible (see Appendix A.1 for a proof). Hence, (6) has a unique solution $\\mathbf{x}^{*}$ . ", "page_idx": 3}, {"type": "text", "text": "Suppose we index the sampled nodes $\\boldsymbol{S}$ in $\\mathbf{x}$ before the non-sampled nodes $\\bar{S}$ , i.e., $\\mathbf{x}=[\\mathbf{x}_{S};\\mathbf{x}_{\\bar{S}}]$ . Then $\\mathbf{H}=\\left[\\mathbf{I}_{K}\\ \\ \\mathbf{0}_{K,N-K}\\right]$ , and the second block row in (6) implies $\\mathbf{x}_{S}=\\mathbf{y}$ . Suppose we write $\\mathbf{L}\\,=\\,[\\mathbf{L}_{S,S}\\,\\,\\,\\mathbf{L}_{S,\\bar{S}};\\mathbf{L}_{\\bar{S},S}\\,\\,\\,\\mathbf{L}_{\\bar{S},\\bar{S}}]$ in blocks also. For the first block row in (6), consider only the non-sampled rows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\bigg(\\big[2\\mathbf{L}\\ \\mathbf{H}^{\\top}\\big]\\left[\\begin{array}{l}{\\mathbf{x}}\\\\ {\\mu}\\end{array}\\right]\\bigg)_{\\bar{S}}=2(\\mathbf{L}_{\\bar{S},S}\\,\\mathbf{x}_{S}+\\mathbf{L}_{\\bar{S},\\bar{S}}\\,\\mathbf{x}_{\\bar{S}})=\\mathbf{0}_{N-K}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\underline{{(\\mathbf{H}^{\\top}\\mu)}}_{\\mathcal{\\bar{S}}}=\\mathbf{0}_{N-K}$ since the non-sampled rows of $\\mathbf{H}^{\\top}$ (the non-sampled columns of $\\mathbf{H}$ ) are zeros. Thus, $\\mathbf{x}_{\\bar{S}}$ can be computed via the following system of linear equations: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{L}_{\\bar{S},\\bar{S}}\\,\\mathbf{x}_{\\bar{S}}=-\\mathbf{L}_{\\bar{S},S}\\,\\mathbf{y}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{L}_{\\bar{S},\\bar{S}}$ is a symmetric, sparse, and provably positive definite (PD) matrix (see Appendix A.2 for a proof). Thus, there exists a unique solution $\\mathbf{x}_{\\bar{S}}$ in (8). ", "page_idx": 3}, {"type": "text", "text": "Complexity: For notation simplicity, let $\\mathcal{L}=\\mathbf{L}_{\\bar{S},\\bar{S}}$ . Linear system (8) can be solved efficiently using conjugate gradient (CG), an iterative descent algorithm with complexity $\\mathcal{O}(\\mathrm{nnz}(\\mathcal{L})\\sqrt{\\kappa(\\pmb{\\mathcal{L}})}/\\log(\\epsilon))$ , where nnz(L) is the number of non-zero entries in matrix L, \u03ba(L) = \u03bb\u03bbmmianx((LL)) is the condition number of $\\mathcal{L}$ , $\\lambda_{\\operatorname*{max}}(\\mathcal{L})$ and $\\lambda_{\\operatorname*{min}}(\\mathcal{L})$ are the respective largest and smallest eigenvalues of $\\mathcal{L}$ , and $\\epsilon$ is the convergence threshold of the gradient search [32]. Because $\\mathbf{L}$ is sparse by graph construction $\\langle\\mathcal{O}(N)$ edges), $\\mathcal{L}=\\mathbf{L}_{\\bar{S},\\bar{S}}$ is also sparse, i.e., $\\mathrm{nnz}({\\mathcal{L}})={\\mathcal{O}}(N)$ . Assuming $\\kappa(\\mathcal{L})$ can be reasonably lower-bounded for PD $\\mathcal{L}$ and $\\epsilon$ is reasonably chosen, the complexity of solving (8) using CG is $\\mathcal{O}(N)$ . ", "page_idx": 3}, {"type": "text", "text": "Interpretation: To elicit a signal filtering interpretation from (6), we assume for now that $\\mathbf{L}$ is $\\mathrm{PD}^{3}$ and thus invertible. Recall that the block matrix inversion formula [36] is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{P}^{-1}=\\left(\\left[\\begin{array}{c c}{\\mathbf{A}}&{\\mathbf{B}}\\\\ {\\mathbf{C}}&{\\mathbf{D}}\\end{array}\\right]\\right)^{-1}=\\left[\\begin{array}{c c}{\\mathbf{A}^{-1}+\\mathbf{A}^{-1}\\mathbf{B}(\\mathbf{P}/\\mathbf{A})\\mathbf{C}\\mathbf{A}^{-1}}&{-\\mathbf{A}^{-1}\\mathbf{B}(\\mathbf{P}/\\mathbf{A})}\\\\ {-(\\mathbf{P}/\\mathbf{A})\\mathbf{C}\\mathbf{A}^{-1}}&{(\\mathbf{P}/\\mathbf{A})}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{P}/\\mathbf{A}=(\\mathbf{D}-\\mathbf{C}\\mathbf{A}^{-1}\\mathbf{B})^{-1}$ is the Schur complement of block $\\mathbf{A}$ of matrix $\\mathbf{P}$ . Solution $\\mathbf{x}^{*}$ can thus be computed as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{x}^{*}=\\mathbf{L}^{-1}\\mathbf{H}^{\\top}\\left(\\mathbf{H}\\mathbf{L}^{-1}\\mathbf{H}^{\\top}\\right)^{-1}\\mathbf{y}}\\\\ &{\\quad=\\mathbf{L}^{-1}\\mathbf{H}^{\\top}\\left((\\mathbf{L}^{-1})\\mathbf{\\mathcal{S}}\\right)^{-1}\\mathbf{y}=\\mathbf{L}^{-1}\\mathbf{H}^{\\top}\\mathbf{L}_{\\mathcal{S}}^{\\#}\\mathbf{y}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{L}_{S}^{\\#}\\triangleq\\left((\\mathbf{L}^{-1})s\\right)^{-1}$ and $(\\mathbf{L}^{-1})_{\\cal S}$ denotes the rows and columns of ${\\bf L}^{-1}$ corresponding to the sampled nodes. $\\mathbf{L}_{S}^{\\#}$ is a high-pass fliter similar to $\\mathbf{L}_{S}$ . Thus, we can interpret $\\mathbf{x}^{*}$ as a low-pass flitered output of up-sampled $\\mathbf{H}^{\\top}\\mathbf{L}_{S}^{\\#}$ y\u2014with low-pass filter response $r(\\mathbf{A})=\\mathbf{A}^{-1}$ where $\\mathbf{L}=\\mathbf{V}\\mathbf{A}\\mathbf{V}^{\\top}$ , $\\mathbf{A}=\\mathrm{diag}([\\lambda_{1},\\ldots,\\lambda_{N}])$ , is eigen-decomposible with frequencies $\\lambda_{k}$ \u2019s and Fourier modes $\\mathbf{v}_{k}$ \u2019s. ", "page_idx": 3}, {"type": "text", "text": "4 Problem Formulation & Optimization using GTV ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1 Problem Formulation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given a positive connected graph $\\mathcal{G}$ specified by incidence matrix $\\mathbf{C}\\in\\mathbb{R}^{M\\times N}$ and partial observation $\\mathbf{y}$ , we now employ instead GTV as the objective to interpolate target signal $\\mathbf{x}$ , resulting in ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}}~\\|\\mathbf{Cx}\\|_{1},~~~~\\mathrm{s.t.}~\\mathbf{Hx}=\\mathbf{y},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "(11) is a linear program (LP), since both the objective and the lone constraint are linear. Thus, while minimizing GLR leads to a linear system (6), minimizing GTV leads to a linear program (11). ", "page_idx": 3}, {"type": "text", "text": "4.1.1 LP in Standard Form ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "First, we rewrite LP (11) in standard form as follows. Define upper-bound variable $\\mathbf{z}\\in\\mathbb{R}^{M}$ with a pair of linear constraints $\\mathbf{z}\\geq\\pm\\mathbf{Cx}$ . This enables a linear objective $\\mathbf{1}_{M}^{\\top}\\mathbf{z}$ for a minimization problem (thus ensuring the upper bound is tight), i.e., $\\mathbf{z}\\;=\\;{\\|\\mathbf{Cx}\\|}\\mathbf{\\bar{1}}$ . Second, we introduce non-negative slack variables $\\mathbf{q}_{1},\\dot{\\mathbf{q}_{2}}\\in\\mathbb{R}^{M}$ to convert inequality constraints $\\mathbf{z}\\geq\\pm\\mathbf{Cx}$ to equality constraints $\\mathbf{z}=\\mathbf{C}\\mathbf{x}+\\mathbf{q}_{1}$ and $\\mathbf{z}=-\\mathbf{C}\\mathbf{x}+\\mathbf{q}_{2}$ . Thus, LP (11) can be rewritten as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{z},\\mathbf{x},\\mathbf{q}}\\mathbf{1}_{M}^{\\top}\\mathbf{z},\\quad\\mathrm{s.t.}\\underbrace{\\left[\\begin{array}{c c c}{\\mathbf{I}_{M}}&{-\\mathbf{C}}&{-\\big(\\mathbf{I}_{M}\\mathbf{\\Sigma}\\mathbf{0}_{M,M}\\big)}\\\\ {\\mathbf{I}_{M}}&{\\mathbf{C}}&{-\\big(\\mathbf{0}_{M,M}\\mathbf{\\Sigma}\\mathbf{I}_{M}\\big)}\\\\ {\\mathbf{0}_{K,M}}&{\\mathbf{H}}&{\\mathbf{0}_{K,2M}}\\end{array}\\right]}_{:\\mathbf{\\hat{\\Sigma}}}\\left[\\begin{array}{l}{\\mathbf{z}}\\\\ {\\mathbf{x}}\\\\ {\\mathbf{q}}\\end{array}\\right]=\\underbrace{\\left[\\begin{array}{l}{\\mathbf{0}_{M}}\\\\ {\\mathbf{0}_{M}}\\\\ {\\mathbf{y}}\\end{array}\\right]}_{:\\mathbf{\\hat{\\Sigma}}},\\quad\\mathbf{q}\\geq\\mathbf{0}_{2M}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{q}=[\\mathbf{q}_{1};\\mathbf{q}_{2}]\\in\\mathbb{R}^{2M}$ . ", "page_idx": 4}, {"type": "text", "text": "4.2 Optimization Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Because coefficient matrix $\\mathbf{A}$ is sparse, (12) is a sparse linear program (SLP). We solve SLP (12) efficiently by adopting an ADMM approach for SLP in [33]. We first define a convex but non-differentiable (non-smooth) indicator function: ", "page_idx": 4}, {"type": "equation", "text": "$$\ng({\\bf q})=\\left\\{\\begin{array}{l l}{0}&{\\mathrm{if}\\;q_{j}\\ge0,\\;\\;\\forall j}\\\\ {\\infty}&{\\mathrm{o.w.}}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We next introduce auxiliary variable $\\tilde{\\mathbf{q}}\\in\\mathbb{R}^{2M}$ and equality constraint $\\tilde{\\mathbf{q}}=\\mathbf{q}$ . We now rewrite (12) with a single equality constraint as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{z},\\mathbf{x},\\mathbf{q},\\tilde{\\mathbf{q}}}\\mathbf{1}_{M}^{\\top}\\mathbf{z}+g(\\tilde{\\mathbf{q}}),\\quad\\mathrm{s.t.}\\underbrace{\\left[\\begin{array}{c}{\\mathbf{A}}\\\\ {\\mathbf{0}_{2M,M+N}\\:\\mathbf{I}_{2M}}\\end{array}\\right]}_{\\mathbf{B}}\\left[\\begin{array}{c}{\\mathbf{z}}\\\\ {\\mathbf{x}}\\\\ {\\mathbf{q}}\\end{array}\\right]=\\left[\\begin{array}{c}{\\mathbf{b}}\\\\ {\\tilde{\\mathbf{q}}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We can now rewrite (14) into an unconstrained version using the augmented Lagrangian method as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{z},\\mathbf{x},\\mathbf{q},\\tilde{\\mathbf{q}}}\\mathbf{1}_{M}^{\\top}\\mathbf{z}+g(\\tilde{\\mathbf{q}})+\\mu^{\\top}\\left(\\mathbf{B}\\left[\\begin{array}{l}{\\mathbf{z}}\\\\ {\\mathbf{x}}\\\\ {\\mathbf{q}}\\end{array}\\right]-\\left[\\begin{array}{l}{\\mathbf{b}}\\\\ {\\tilde{\\mathbf{q}}}\\end{array}\\right]\\right)+\\frac{\\gamma}{2}\\left\\|\\mathbf{B}\\left[\\begin{array}{l}{\\mathbf{z}}\\\\ {\\mathbf{x}}\\\\ {\\mathbf{q}}\\end{array}\\right]-\\left[\\begin{array}{l}{\\mathbf{b}}\\\\ {\\tilde{\\mathbf{q}}}\\end{array}\\right]\\right\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\pmb{\\mu}\\in\\mathbb{R}^{4M+K}$ is a Lagrange multiplier vector, and $\\gamma>0$ is a scalar parameter. In the sequel, we write $\\pmb{\\mu}=[\\pmb{\\mu}_{a};\\pmb{\\mu}_{b};\\pmb{\\mu}_{c};\\pmb{\\mu}_{d};\\pmb{\\mu}_{e}]$ , where $\\pmb{\\mu}_{a},\\pmb{\\mu}_{b},\\pmb{\\mu}_{d},\\pmb{\\mu}_{e}\\in\\mathbb{R}^{M}$ and $\\pmb{\\mu}_{c}\\in\\bar{\\mathbb{R}}^{K}$ . ", "page_idx": 4}, {"type": "text", "text": "4.2.1 Optimizing Main Variables ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As typically done in ADMM, we minimize the unconstrained objective (15) alternately as follows. At iteration $t$ , when $\\tilde{\\mathbf{q}}^{t}$ and $\\pmb{\\mu}^{t}$ are fixed, the optimization for $\\mathbf{z}^{t+1}$ , $\\mathbf{x}^{t+1}$ and $\\mathbf{q}^{t+1}$ becomes ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{z},\\mathbf{x},\\mathbf{q}}\\mathbf{1}_{M}^{\\top}\\mathbf{z}+(\\mu^{t})^{\\top}\\left(\\mathbf{B}\\left[\\begin{array}{l}{\\mathbf{z}}\\\\ {\\mathbf{x}}\\\\ {\\mathbf{q}}\\end{array}\\right]-\\left[\\begin{array}{l}{\\mathbf{b}}\\\\ {\\tilde{\\mathbf{q}}}\\end{array}\\right]\\right)+\\frac{\\gamma}{2}\\left\\|\\mathbf{B}\\left[\\begin{array}{l}{\\mathbf{z}}\\\\ {\\mathbf{x}}\\\\ {\\mathbf{q}}\\end{array}\\right]-\\left[\\begin{array}{l}{\\mathbf{b}}\\\\ {\\tilde{\\mathbf{q}}}\\end{array}\\right]\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The solution to this convex and smooth quadratic optimization is a system of linear equations, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{z}^{t+1}=-\\frac{1}{\\gamma}\\mathbf{1}_{M}-\\frac{1}{2\\gamma}\\left(\\mu_{a}^{t}+\\mu_{b}^{t}+\\mu_{d}^{t}+\\mu_{e}^{t}\\right)+\\frac{1}{2}(\\tilde{\\mathbf{q}}_{1}^{t}+\\tilde{\\mathbf{q}}_{2}^{t})}\\\\ &{(\\mathbf{C}^{\\top}\\mathbf{C}+\\mathbf{H}^{\\top}\\mathbf{H})\\mathbf{x}^{t+1}=\\frac{1}{2\\gamma}\\mathbf{C}^{\\top}\\left(\\mu_{a}^{t}-\\mu_{b}^{t}+\\mu_{d}^{t}-\\mu_{e}^{t}\\right)-\\frac{1}{\\gamma}\\mathbf{H}^{\\top}\\mu_{c}^{t}-\\frac{1}{2}\\mathbf{C}^{\\top}(\\tilde{\\mathbf{q}}_{1}^{t}-\\tilde{\\mathbf{q}}_{2}^{t})+\\mathbf{H}^{\\top}\\mathbf{y}}\\\\ &{\\mathbf{q}_{1}^{t}=\\frac{1}{2}\\left(\\mathbf{z}^{t+1}-\\mathbf{C}\\mathbf{x}^{t+1}\\right)+\\frac{1}{2\\gamma}(\\mu_{a}^{t}-\\mu_{d}^{t}+\\gamma\\tilde{\\mathbf{q}}_{1}^{t})}\\\\ &{\\mathbf{q}_{2}^{t}=\\frac{1}{2}\\left(\\mathbf{z}^{t+1}+\\mathbf{C}\\mathbf{x}^{t+1}\\right)+\\frac{1}{2\\gamma}(\\mu_{b}^{t}-\\mu_{e}^{t}+\\gamma\\tilde{\\mathbf{q}}_{2}^{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "See the Appendix A.3 for a derivation. ", "page_idx": 4}, {"type": "text", "text": "Linear system (18) is solvable if the coefficient matrix ${\\pmb{\\mathcal{L}}}\\triangleq\\mathbf{L}+\\mathbf{H}^{\\top}\\mathbf{H}$ , where $\\mathbf{L}\\triangleq\\mathbf{C}^{\\top}\\mathbf{C}$ is a PSD graph Laplacian for a positive graph, is invertible. See Appendix A.4 for a proof that $\\mathcal{L}$ is PD and thus invertible . ", "page_idx": 5}, {"type": "text", "text": "Complexity: Linear system (18) again can be solved efficiently using CG with complexity $\\mathcal{O}(\\mathrm{nnz}(\\mathcal{L})\\sqrt{\\kappa(\\pmb{\\mathcal{L}})}/\\log(\\epsilon))$ [32]. Because $\\mathbf{C}$ is sparse by graph construction $(\\mathcal{O}(N)$ edges) and $\\mathbf{H}$ is sparse by definition, $\\mathcal{L}$ is also sparse, i.e., $\\mathrm{nnz}({\\mathcal{L}})\\,=\\,{\\mathcal{O}}(N)$ . Thus, assuming $\\kappa(\\mathcal{L})$ is also upper-bounded and $\\epsilon$ is reasonably chosen, the complexity of solving (18) using CG is $\\mathcal{O}(N)$ . ", "page_idx": 5}, {"type": "text", "text": "Interpretation: (18) can be interpreted as follows. $\\mathbf{H}^{\\top}\\mathbf{y}$ is the up-sampled version of observation y. $\\bar{\\mathcal{L}}\\bar{=}\\,\\mathbf{V}\\mathbf{A}\\mathbf{V}^{\\top}$ , $\\boldsymbol{\\Lambda}=\\mathrm{diag}([\\lambda_{1},\\dots,\\lambda_{N}])$ , is an eigen-decomposible generalized Laplacian matrix\u2014 Laplacian matrix $\\mathbf{C}^{\\top}\\mathbf{C}$ plus self-loops of weight 1 at sampled nodes due to diagonal matrix $\\mathbf{H}^{\\top}\\mathbf{H}$ , and like a graph Laplacian $\\mathbf{L}$ without self-loops, can be interpreted as a high-pass spectral filter [37]. $\\mathcal{L}^{-1}=\\mathbf{V}\\mathbf{A}^{-1}\\mathbf{V}^{\\top}$ is thus a low-pass spectral filter with frequency response $r(\\bar{\\mathbf{A}^{\\big}})=\\mathbf{A}^{-1}$ to interpolate output $\\mathcal{L}^{-1}\\mathbf{H}^{\\top}\\mathbf{y}$ . We interpret the remaining terms on the right-hand side of (18) as bias. ", "page_idx": 5}, {"type": "text", "text": "4.2.2 Optimizing Auxiliary Variable ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Fixing ${\\bf z}^{t+1}$ , $\\mathbf{x}^{t+1}$ and $\\mathbf{q}^{t+1}$ , the optimization for $\\tilde{\\mathbf{q}}^{t+1}$ for (15) simplifies to ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\tilde{\\mathbf{q}}}g(\\tilde{\\mathbf{q}})+\\left(\\mu_{d}^{t}\\right)^{\\top}\\left(\\mathbf{q}^{t+1}-\\tilde{\\mathbf{q}}\\right)+\\frac{\\gamma}{2}\\left\\|\\mathbf{q}^{t+1}-\\tilde{\\mathbf{q}}\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The solution for optimal $\\tilde{\\mathbf{q}}^{t+1}$ is term-by-term thresholding: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{q}_{i}^{t+1}=\\left\\{\\begin{array}{l l}{q_{i}^{t+1}+\\frac{1}{\\gamma}\\mu_{d,i}^{t}}&{\\mathrm{if~}\\,q_{i}^{t+1}+\\frac{1}{\\gamma}\\mu_{d,i}^{t}\\geq0}\\\\ {0}&{\\mathrm{o.w.}}\\end{array}\\right.,\\forall i.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "See Appendix A.5 for a derivation. ", "page_idx": 5}, {"type": "text", "text": "4.2.3 Updating Lagrange Multiplier ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The Lagrange multiplier $\\pmb{\\mu}^{t+1}$ can be updated in the usual manner in an ADMM framework [38]: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mu^{t+1}=\\mu^{t}+\\gamma\\left(\\mathbf{B}\\left[\\begin{array}{l}{\\mathbf{z}}\\\\ {\\mathbf{x}}\\\\ {\\mathbf{q}}\\end{array}\\right]-\\left[\\begin{array}{l}{\\mathbf{b}}\\\\ {\\tilde{\\mathbf{q}}}\\end{array}\\right]\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Algorithm Complexity: Given that the number of iterations until ADMM convergence is not a function of input size, it is $\\mathcal{O}(1)$ . The most time-consuming step in each ADMM iteration is the solving of linear system (18) via CG in $\\mathcal{O}(N)$ . Thus, we conclude that solving SLP (12) using the aforementioned ADMM algorithm is $\\mathcal{O}(N)$ . ", "page_idx": 5}, {"type": "text", "text": "Algorithm Comparison: Comparing the CG algorithm used to solve linear system (8) and the ADMM algorithm developed to solve SLP (12), we first observe that, given a similarity graph $\\mathcal{G}$ specified by Laplacian or incidence matrix, $\\mathbf{L}$ or $\\mathbf{C}$ , both algorithms compute the interpolated signal $\\mathbf{x}^{*}$ as a low-pass filtered output of the up-sampled input $\\mathbf{H}^{\\top}\\mathbf{y}$ in (10) and (18), respectively. This is intuitive, given the assumed graph smoothness priors, GLR and GTV. We see also that the ADMM algorithm is more intricate: in each iteration, the main variables are computed using CG, while the auxiliary variable is updated via ReLU-like thresholding. As a result, the ADMM algorithm is more amenable to deep algorithm unrolling with better performance in general (see Section 6 for details). ", "page_idx": 5}, {"type": "text", "text": "5 Graph Learning & Algorithm Unrolling ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now discuss how a similarity graph $\\mathcal{G}$ can be learned from data, specified by graph Laplacian L for GLR minimization (4) or incidence matrix $\\mathbf{C}$ for GTV minimization (11), so that the two proposed graph-based interpolations can take place. Moreover, we show how a normalized graph learning module4 performs comparable operations to the self-attention mechanism in conventional transformers. Thus, unrolling sequential pairs of graph-based iterative algorithm and graph learning module back-to-back leads to an interpretable \u201cwhite-box\u201d transformer-like neural net. ", "page_idx": 5}, {"type": "text", "text": "5.1 Self-Attention Operator in Transformer ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first review the self-attention operator in a conventional transformer architecture, defined using a transformed dot product and a softmax operation [1]. Specifically, first denote by $\\mathbf{x}_{i}\\,\\in\\,\\mathbb{R}^{E}$ an embedding for token $i$ of $N$ tokens. Affinity $e(i,j)$ between tokens $i$ and $j$ is defined as the dot product between linear-transformed embeddings $\\mathbf{Kx}_{i}$ and $\\mathbf{Qx}_{j}$ , where $\\mathbf{Q},\\mathbf{K}\\in\\overset{\\mathcal{\\mathbf{C}}}{\\mathbb{R}}^{E\\times E}$ are the query and key matrices, respectively. Using softmax, a non-linear function that maps a vector of real numbers to a vector of positive numbers that sum to 1, attention weight $a_{i,j}$ is computed as ", "page_idx": 6}, {"type": "equation", "text": "$$\na_{i,j}=\\frac{\\exp(e(i,j))}{\\sum_{l=1}^{N}\\exp(e(i,l))},~~~~e(i,j)=(\\mathbf{Q}\\mathbf{x}_{j})^{\\top}(\\mathbf{K}\\mathbf{x}_{i}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Given self-attention weights $a_{i,j}$ , output embedding $\\mathbf{y}_{i}$ for token $i$ is computed as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{y}_{i}=\\sum_{l=1}^{N}a_{i,l}\\mathbf{x}_{l}\\mathbf{V}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathbf{V}\\in\\mathbb{R}^{E\\times E}$ is a value matrix. \u201cSelf-attention\u201d here means that input embeddings are weighted to compute output embeddings. A transformer is thus a sequence of embedding-to-embedding mappings via different learned self-attention operations defined by Q, $\\mathbf{K}$ and $\\mathbf{V}$ matrices. Multi-head attention is possible when multiple query and key matrices $\\mathbf{Q}^{(m)}$ and $\\mathbf{K}^{(m)}$ are used to compute different attention weights ai(,mj )\u2019s for the same input embeddings xi and xj, and the output embedding $\\mathbf{y}_{i}$ is computed using an average of these multi-head attention weights $a_{i,l}^{(m)}$ \u2019s. ", "page_idx": 6}, {"type": "text", "text": "5.2 Computation of Graph Edge Weights ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Consider now how edge weights $w_{i,j}$ \u2019s can be computed from data to specify a finite graph $\\mathcal{G}$ [27, 28]. A low-dimensional feature vector $\\mathbf{f}_{i}^{\\phantom{\\,}}\\in\\mathbb{R}^{D}$ can be computed for each node $i$ from embedding $\\mathbf{x}_{i}\\in\\mathbb{R}^{E}$ via some (possibly non-linear) function $\\mathbf{f}_{i}=F(\\mathbf{x}_{i})$ , where typically $D\\ll E$ . Edge weight $w_{i,j}$ between nodes $i$ and $j$ in a graph $\\mathcal{G}$ can then be computed as ", "page_idx": 6}, {"type": "equation", "text": "$$\nw_{i,j}=\\exp\\left(-d(i,j)\\right),\\ \\ \\ \\ d(i,j)=(\\mathbf{f}_{i}-\\mathbf{f}_{j})^{\\top}\\mathbf{M}(\\mathbf{f}_{i}-\\mathbf{f}_{j})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $d(i,j)$ is the squared Mahalanobis distance given PSD metric matrix $\\mathbf{M}$ that quantifies the difference between nodes $i$ and $j$ . $M$ edge weights $\\{w_{i,j}\\}$ compose a graph $\\mathcal{G}$ , specified by the Laplacian matrix $\\mathbf{L}$ for GLR minimization (4) and the incidence matrix $\\mathbf{C}^{M\\times N}$ for GTV minimization (11). Because $w_{i,j}\\geq0,\\forall i,j$ , constructed graph $\\mathcal{G}$ is positive. ", "page_idx": 6}, {"type": "text", "text": "As a concrete example, consider bilateral filter (BF) weights commonly used in image filtering [42], where feature $\\mathbf{f}_{i}$ contains the 2D grid location $\\mathbf{l}_{i}$ and color intensity $p_{i}$ of pixel $i$ , and metric $\\mathbf{M}=\\mathrm{diag}([1/\\sigma_{d}^{2};1/\\sigma_{r}^{2}])$ is a diagonal matrix with weights to specify the relative strength of the domain and range filters in BF. Because BF uses input pixel intensities $p_{l}$ \u2019s to compute weighted output pixel intensities $p_{i}$ \u2019s, BF is signal-dependent, similar to self-attention weights in transformers. ", "page_idx": 6}, {"type": "text", "text": "Edge weights are often first normalized before being used for filtering. ", "page_idx": 6}, {"type": "text", "text": "Normalization: For normalization, the symmetric normalized graph Laplacian $\\mathbf{L}_{n}$ is defined as ${\\bf L}_{n}\\triangleq{\\bf D}^{-1/2}{\\bf L}{\\bf D}^{-1/2}$ , so that the diagonal entries of $\\mathbf{L}_{n}$ are all ones (assuming $\\mathcal{G}$ is connected and positive) [18]. We assume normalized $\\mathbf{L}_{n}$ is used for Laplacian $\\mathbf{L}$ in GLR minimization in (4). ", "page_idx": 6}, {"type": "text", "text": "Alternatively, the asymmetric random walk graph Laplacian $\\mathbf{L}_{r w}$ is defined as ${\\bf L}_{r w}\\triangleq{\\bf D}^{-1}{\\bf L}$ , so that the sum of each row of $\\mathbf{L}_{\\mathit{r w}}$ equals to zero [18]. Interpreting $\\mathbf{L}_{\\mathit{r w}}$ as a Laplacian matrix to a directed graph, the weight sum of edges leaving each node $i$ is one, i.e., $\\begin{array}{r}{\\sum_{l\\mid(i,l)\\in\\mathcal{E}}\\bar{w}_{i,l}=1,\\forall i}\\end{array}$ . To accomplish this, undirected edges weights $\\{w_{i,j}\\}$ are normalized to $\\{\\bar{w}_{i,j}\\}$ via ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\bar{w}_{i,j}=\\frac{\\exp(-d(i,j))}{\\sum_{l|(i,l)\\in\\mathcal{E}}\\exp(-d(i,l))}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "For GTV minimization in (11), we normalize edge weights in incidence matrix $\\mathbf{C}$ instead using (26). This results in normalized $\\bar{\\mathbf{C}}\\in\\mathbb{R}^{2M\\times N}$ for a directed graph with $2M$ directed edges. Subsequently, we define symmetric graph Laplacian $\\bar{\\mathbf{L}}=\\bar{\\mathbf{C}}^{\\top}\\bar{\\mathbf{C}}$ and generalized graph Laplacian $\\bar{\\mathcal{L}}=\\bar{\\mathbf{L}}+\\mathbf{H}^{\\top}\\mathbf{H}$ . Note that $\\begin{array}{r}{\\|\\bar{\\mathbf{C}}\\mathbf{1}\\|_{1}=\\sum_{l=1}^{N}\\bar{w}_{i,l}\\,|1-1|=0}\\end{array}$ after normalization, as expected for a total variation term on a constant signal 1. Further, note that while $\\bar{\\bf C}$ is an incidence matrix for a directed graph with $2M$ edges, $\\bar{\\bf L}$ is a graph Laplacian for an undirected graph with $M$ edges. See Fig. 3 in Appendix A.6 for an example of incidence matrix $\\mathbf{C}$ , normalized incidence matrix $\\mathbf{\\bar{C}}$ , and graph Laplacian matrix $\\bar{\\bf L}$ . ", "page_idx": 6}, {"type": "image", "img_path": "i8LoWBJf7j/tmp/101e7a2f265ca51969f4c94cc467ae478af1f1ad2aa63eb28c6550cc1b0d1db4.jpg", "img_caption": ["Figure 1: Unrolling of GTV-based signal interpolation algorithm. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Comparison to Self-Attention Operator: We see how the definitions of edge weights (25) and normalization (26) are similar to attention weights in (23). Specifically, interpreting the negative squared Mahalanobis distance $-d(i,j)$ as affinity $e(i,j)$ , normalized edge weights $\\bar{w}_{i,j}$ in (25) are essentially the same as attention weights $a_{i,j}$ in (23). There are subtle but important differences: i) how non-negative Mahalanobis distance $d(\\boldsymbol{\\bar{i}},\\boldsymbol{j})$ is computed in (25) using features $\\mathbf{f}_{i}=F(\\mathbf{x}_{i})$ and metric M versus how real-valued affinity is computed via a transformed dot product in (23), and ii) how the normalization term is computed in a one-hop neighborhood from node $i$ in (26) versus how it is computed using all $N$ tokens in (23). The first difference conceptually means that edge weight based on Mahalanobis distance $d(i,j)$ is symmetric (i.e., $\\bar{w}_{i,j}=\\bar{w}_{j,i}\\,$ ), while attention weight $a_{i,j}$ is not. Both differences have crucial complexity implications, which we will revisit in the sequel. ", "page_idx": 7}, {"type": "text", "text": "Further, we note that, given a graph $\\mathcal{G}$ , the interpolated signal $\\mathbf{x}^{*}$ is computed simply as a low-pass flitered output of the up-sampled input observation $\\mathbf{H}^{\\top}\\mathbf{y}$ via (10) or (18), depending on the assumed graph smoothness prior, GLR or GTV, while the output embedding $\\mathbf{y}_{i}$ in a conventional transformer requires value matrix $\\mathbf{V}$ in (24). This also has a complexity implication. ", "page_idx": 7}, {"type": "text", "text": "5.3 Deep Algorithm Unrolling ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We unroll $T$ sequential pairs of an iterative interpolation algorithm (GLR- or GTV-based) with a graph learning module into an interpretable neural net. See Fig. 1a for an illustration of the GTVbased algorithm unrolling, where the $t$ -th pair of ADMM block and the graph learning module have respective parameters $\\mathbf{{\\Theta}}_{t}$ and $\\Phi_{t}$ that are learned from back-propagation via a defined loss function. $\\Phi_{t}$ include parameters used to define feature function $F(\\cdot)$ and metric matrix M in (25), so the module can construct a graph $\\mathcal{G}$ specified by incidence matrix $\\mathbf{C}^{t+1}$ given signal $\\mathbf{x}^{t}$ . In our implementation, we employ a shallow CNN to map a neighborhood of pixels centered at pixel $i$ to a low-dimensional feature $\\mathbf{f}_{i}$ , with a parameter size smaller than query and key matrices, $\\mathbf{Q}$ and $\\mathbf{K}$ , in a conventional transformer. See Section 6.1 for details. ", "page_idx": 7}, {"type": "text", "text": "An ADMM block contains multiple ADMM layers that are unrolled iterations of the iterative ADMM algorithm described in Section 4.2. Each ADMM layer updates the main variables $\\mathbf{x},\\mathbf{z},\\mathbf{q},$ , auxiliary variable $\\tilde{\\mathbf{q}}$ , and Lagrange multiplier $\\pmb{\\mu}$ in turn using (17) to (22). ADMM weight parameter $\\gamma$ , as well as parameters in CG used to compute linear system (18), are learned via back-propagation. Specifically, two CG parameters $\\alpha$ and $\\beta$ that represent step size and momentum during the conjugate gradient descent step are learned. See Appendix A.7 for details. ", "page_idx": 7}, {"type": "image", "img_path": "i8LoWBJf7j/tmp/f7f1d7c4c06b47d3d7eeed545ec8d9d31c4134557f1353ff75781080b817650b.jpg", "img_caption": ["Figure 2: Demosaicking performance vs. training size for different models. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "6.1 Experimental Setup ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "All models were developed using Python 3.11. We leveraged PyTorch to implement all models and trained them using NVIDIA GeForce RTX 2080 Ti. To train each learned model, we used the DIV2K dataset, which contains 800 and 100 high-resolution (HR) training and validation images, respectively. Since the images are HR, we patchified the images into small images and used only about 1 to $4\\%$ of the patches for training and validation sets. We randomly sampled patches of $64\\times64$ pixels to train the model. To test a model, we used the McM [43], Kodak [44], and Urban100 [45] datasets, running each model on the whole images. See Appendix A.8 for more implementation details. ", "page_idx": 8}, {"type": "text", "text": "We tested model performance in two imaging applications: demosaicking and image interpolation. Demosaicking reconstructs a full-color image (each pixel contains RGB colors) from a Bayerpatterned image, where each pixel location has only one of Red, Green, or Blue. Interpolation reconstructs empty pixels missing all three colors in an image. To create input images, for the first application, we started from a full-color image and then removed color components per pixel according to the Bayer pattern. For the second application, we directly down-sampled horizontally and vertically a HR image by a factor of 2 to get the corresponding low-resolution (LR) image without any anti-aliasing filtering. This is equivalent to keeping every four pixels in the HR image. ", "page_idx": 8}, {"type": "text", "text": "6.2 Experimental Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "For the first application, we evaluated our graph-based models against two variants of RSTCANet [46], RSTCANet-B and RSTCANet-S (RST-B and RST-S for short), a SOTA framework that employs a swin transformer architecture, Menon [47], Malvar [48] and bicubic interpolation. Menon [47] employs a directional approach combined with an a posteriori decision, followed by an additional refinement step. Malvar [48] uses a linear flitering technique that incorporates inter-channel information across all channels for demosaicking. ", "page_idx": 8}, {"type": "text", "text": "The baselines for our second application are MAIN [49], a multi-scale deep learning framework for image interpolation, SwinIR [50], and bicubic interpolation. SwinIR consists of three stages: shallow feature extraction, deep feature extraction, and a final reconstruction stage; see [50] for details. We use Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM) [51] as our evaluation metrics, common in image quality assessment. ", "page_idx": 8}, {"type": "table", "img_path": "i8LoWBJf7j/tmp/0570671919411634dbae2262a488526f6d797ea06a7f298ffcb0c9f43410480b.jpg", "table_caption": ["Table 3: Interpolation performance for different models, trained on 10k sample dataset. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "i8LoWBJf7j/tmp/7a3bf704fe2efa827e236ea114affab0553832ba5c1c012cb23c6aa36c13198a.jpg", "table_caption": [], "table_footnote": ["Table 2: Demosaicking performance for different models, trained on 10k sample dataset. "], "page_idx": 9}, {"type": "text", "text": "Table 2 shows the demosaicking performance for different models, where all models were trained on the same dataset and the same number of epochs (30), using a subset of DIV2K dataset containing $10K$ of $64\\times64$ patches. We observe that our unrolled GTV model (uGTV) achieved the best overall performance, while the unrolled GLR model (uGLR) and RST-S performed similarly. Both our models (uGTV and uGLR) performed better than RST-B while employing significantly fewer parameters. Crucially, we observe that although our normalized edge weight $\\bar{w}_{i,j}$ based on Mahalanobis distance is symmetric while the self-attention weight $a_{i,j}$ is not (due to query and key matrices $\\mathbf{Q}$ and $\\mathbf{K}$ not being the same in general), the directionality in the self-attention mechanism does not appear to help improve performance of the conventional transformer further, at least for image interpolation tasks. The iterative GTV algorithm (iGTV) without parameter optimization performed the worst, demonstrating the importance of parameter learning. ", "page_idx": 9}, {"type": "text", "text": "In Fig. 2, we see the demosaicking performance of different models versus training data size. We see that for small data size, our models (uGTV and uGLR) performed significantly better than RST-B. This is intuitive, since a model with more parameters requires more training data in general. See Appendix A.9 for example visual results. ", "page_idx": 9}, {"type": "text", "text": "Next, we test robustness to covariate shift by testing models trained on noiseless data using a dataset artificially injected with Gaussian noise. Table 1 shows the demosaicking performance versus different noise variances. We observe that our models outperformed RST-B in all noisy scenarios. ", "page_idx": 9}, {"type": "text", "text": "For image interpolation, we interploated a LR image to a corresponding HR image. We trained all models on the same dataset as the first application with the same number of epochs (15). Table 3 shows that under the same training conditions, our proposed models (uGTV and uGLR) outperformed MAIN in interpolation performance in all three benchmark datasets by about 0.7 dB. Note that for this application we only interpolated Y-channel from YCbCr color space for PSNR computation. Similar to the first application, our models achieved slight performance gain while employing drastically fewer parameters; specifically, uGTV employed only about $3\\%$ of the parameters in MAIN. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "By unrolling iterative algorithms that minimize one of two graph smoothness priors\u2014 $\\ell_{2}$ -norm graph Laplacian regularizer (GLR) or $\\ell_{1}$ -norm graph total variation (GTV)\u2014we build interpretable and light-weight transformer-like neural nets for the signal interpolation problem. The key insight is that the normalized graph learning module is akin to the self-attention mechanism in a conventional transformer architecture. Moreover, the interpolated signal in each layer is simply the low-pass filtered output derived from the assumed graph smoothness prior, eliminating the need for the value matrix. Experiments in two imaging applications show that interpolation results on par with SOTA can be achieved with a fraction of the parameters used in conventional transformers. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, \u201cNeural machine translation by jointly learning to align and translate,\u201d CoRR, vol. abs/1409.0473, 2014.   \n[2] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin, \u201cAttention is all you need,\u201d Advances in neural information ", "page_idx": 9}, {"type": "text", "text": "processing systems, vol. 30, 2017. ", "page_idx": 10}, {"type": "text", "text": "85 [3] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo, \u201cSwin transformer: Hierarchical vision transformer using shifted windows,\u201d in 2021 IEEE/CVF International Conference on Computer Vision (ICCV), 2021, pp. 9992\u201310002. [4] Alexander Kolesnikov, Alexey Dosovitskiy, Dirk Weissenborn, Georg Heigold, Jakob Uszkoreit, Lucas Beyer, Matthias Minderer, Mostafa Dehghani, Neil Houlsby, Sylvain Gelly, Thomas Unterthiner, and Xiaohua Zhai, \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d in The Ninth International Conference on Learning Representations (ICLR), 2021. [5] James Vuckovic, Aristide Baratin, and R\u00e9mi Tachet des Combes, \u201cA mathematical theory of attention,\u201d ArXiv, vol. abs/2007.02876, 2020. [6] Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank Reddi, Sanjiv Kumar, and Suvrit Sra, \u201cWhy are adaptive methods good for attention models?,\u201d in Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, Eds. 2020, vol. 33, pp. 15383\u201315393, Curran Associates, Inc. [7] Charles Burton Snell, Ruiqi Zhong, Dan Klein, and Jacob Steinhardt, \u201cApproximating how single head attention learns,\u201d ArXiv, vol. abs/2103.07601, 2021. [8] Colin Wei, Yining Chen, and Tengyu Ma, \u201cStatistically meaningful approximation: a case study on approximating turing machines with transformers,\u201d in Advances in Neural Information Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds. 2022, vol. 35, pp. 12071\u201312083, Curran Associates, Inc. [9] Hyunjik Kim, George Papamakarios, and Andriy Mnih, \u201cThe lipschitz constant of self-attention,\u201d in Proceedings of the 38th International Conference on Machine Learning, Marina Meila and Tong Zhang, Eds. 18\u201324 Jul 2021, vol. 139 of Proceedings of Machine Learning Research, pp. 5562\u20135571, PMLR.   \n[10] Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang, \u201cInductive biases and variable creation in self-attention mechanisms,\u201d in Proceedings of the 39th International Conference on Machine Learning, Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, Eds. 17\u201323 Jul 2022, vol. 162 of Proceedings of Machine Learning Research, pp. 5793\u20135831, PMLR.   \n[11] Hongkang Li, Meng Wang, Sijia Liu, and Pin-Yu Chen, \u201cA theoretical understanding of shallow vision transformers: Learning, generalization, and sample complexity,\u201d in The Eleventh International Conference on Learning Representations, 2023.   \n[12] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme Ruiz, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd Van Steenkiste, Gamaleldin Fathy Elsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Collier, Alexey A. Gritsenko, Vighnesh Birodkar, Cristina Nader Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetic, Dustin Tran, Thomas Kipf, Mario Lucic, Xiaohua Zhai, Daniel Keysers, Jeremiah J. Harmsen, and Neil Houlsby, \u201cScaling vision transformers to 22 billion parameters,\u201d in Proceedings of the 40th International Conference on Machine Learning, Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, Eds. 23\u201329 Jul 2023, vol. 202 of Proceedings of Machine Learning Research, pp. 7480\u20137512, PMLR.   \n[13] Ruiqi Zhang, Spencer Frei, and Peter L. Bartlett, \u201cTrained transformers learn linear models in-context,\u201d Journal of Machine Learning Research, vol. 25, no. 49, pp. 1\u201355, 2024.   \n[14] Vishal Monga, Yuelong Li, and Yonina C. Eldar, \u201cAlgorithm unrolling: Interpretable, efficient deep learning for signal and image processing,\u201d IEEE Signal Processing Magazine, vol. 38, no. 2, pp. 18\u201344, 2021.   \n[15] Karol Gregor and Yann LeCun, \u201cLearning fast approximations of sparse coding,\u201d in Proceedings of the 27th International Conference on International Conference on Machine Learning, Madison, WI, USA, 2010, ICML\u201910, p. 399\u2013406, Omnipress.   \n[16] Yaodong Yu, Sam Buchanan, Druv Pai, Tianzhe Chu, Ziyang Wu, Shengbang Tong, Benjamin Haeffele, and Yi Ma, \u201cWhite-box transformers via sparse rate reduction,\u201d in Advances in Neural Information Processing Systems, A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds. 2023, vol. 36, pp. 9422\u20139457, Curran Associates, Inc.   \n[17] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Vandergheynst, \u201cThe emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains,\u201d in IEEE Signal Processing Magazine, May 2013, vol. 30, no.3, pp. 83\u201398.   \n[18] A. Ortega, P. Frossard, J. Kovacevic, J. M. F. Moura, and P. Vandergheynst, \u201cGraph signal processing: Overview, challenges, and applications,\u201d in Proceedings of the IEEE, May 2018, vol. 106, no.5, pp. 808\u2013828.   \n[19] G. Cheung, E. Magli, Y. Tanaka, and M. Ng, \u201cGraph spectral image processing,\u201d in Proceedings of the IEEE, May 2018, vol. 106, no.5, pp. 907\u2013930.   \n[20] J. Pang and G. Cheung, \u201cGraph Laplacian regularization for inverse imaging: Analysis in the continuous domain,\u201d in IEEE Transactions on Image Processing, April 2017, vol. 26, no.4, pp. 1770\u20131785.   \n[21] X. Liu, G. Cheung, X. Wu, and D. Zhao, \u201cRandom walk graph Laplacian based smoothness prior for soft decoding of JPEG images,\u201d IEEE Transactions on Image Processing, vol. 26, no.2, pp. 509\u2013524, February 2017.   \n[22] Fei Chen, Gene Cheung, and Xue Zhang, \u201cManifold graph signal restoration using gradient graph Laplacian regularizer,\u201d IEEE Transactions on Signal Processing, vol. 72, pp. 744\u2013761, 2024.   \n[23] Jin Zeng, Jiahao Pang, Wenxiu Sun, and Gene Cheung, \u201cDeep graph Laplacian regularization for robust denoising of real images,\u201d in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2019, pp. 1759\u20131768.   \n[24] Chinthaka Dinesh, Gene Cheung, and Ivan V. Bajic\u00b4, \u201cPoint cloud denoising via feature graph laplacian regularization,\u201d IEEE Transactions on Image Processing, vol. 29, pp. 4143\u20134158, 2020.   \n[25] Chinthaka Dinesh, Gene Cheung, and Ivan V. Baji\u00b4c, \u201cPoint cloud video super-resolution via partial point coupling and graph smoothness,\u201d IEEE Transactions on Image Processing, vol. 31, pp. 4117\u20134132, 2022.   \n[26] Yongyi Yang, Tang Liu, Yangkun Wang, Jinjing Zhou, Quan Gan, Zhewei Wei, Zheng Zhang, Zengfeng Huang, and David Wipf, \u201cGraph neural networks inspired by classical iterative algorithms,\u201d in Proceedings of the 38th International Conference on Machine Learning, Marina Meila and Tong Zhang, Eds. 18\u201324 Jul 2021, vol. 139 of Proceedings of Machine Learning Research, pp. 11773\u201311783, PMLR.   \n[27] Wei Hu, Xiang Gao, Gene Cheung, and Zongming Guo, \u201cFeature graph learning for 3D point cloud denoising,\u201d IEEE Transactions on Signal Processing, vol. 68, pp. 2841\u20132856, 2020.   \n[28] Cheng Yang, Gene Cheung, and Wei Hu, \u201cSigned graph metric learning via Gershgorin disc perfect alignment,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 10, pp. 7219\u20137234, 2022.   \n[29] Abderrahim Elmoataz, Olivier Lezoray, and S\u00c9bastien Bougleux, \u201cNonlocal discrete regularization on weighted graphs: A framework for image and manifold processing,\u201d IEEE Transactions on Image Processing, vol. 17, no. 7, pp. 1047\u20131060, 2008.   \n[30] Camille Couprie, Leo Grady, Laurent Najman, Jean-Christophe Pesquet, and Hugues Talbot, \u201cDual constrained tv-based regularization on graphs,\u201d SIAM Journal on Imaging Sciences, vol. 6, no. 3, pp. 1246\u20131273, 2013.   \n[31] Peter Berger, Gabor Hannak, and Gerald Matz, \u201cGraph signal recovery via primal-dual algorithms for total variation minimization,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 6, pp. 842\u2013855, 2017.   \n[32] J. R Shewchuk, \u201cAn introduction to the conjugate gradient method without the agonizing pain,\u201d Tech. Rep., USA, 1994.   \n[33] Sinong Wang and Ness Shroff, \u201cA new alternating direction method for linear programming,\u201d in Advances in Neural Information Processing Systems, I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds. 2017, vol. 30, Curran Associates, Inc.   \n[34] Y. Bai, G. Cheung, X. Liu, and W. Gao, \u201cGraph-based blind image deblurring from a single photograph,\u201d IEEE Transactions on Image Processing, vol. 28, no.3, pp. 1404\u20131418, 2019.   \n[35] S. Boyd and L. Vandenberghe, Convex Optimization, Cambridge, 2004.   \n[36] G. Golub and C. F. Van Loan, Matrix Computations (Johns Hopkins Studies in the Mathematical Sciences), Johns Hopkins University Press, 2012.   \n[37] E. BrianDavies, Graham M. L. Gladwell, Josef Leydold, Peter F. Stadler, and Peter F. Stadler, \u201cDiscrete nodal domain theorems,\u201d Linear Algebra and its Applications, vol. 336, pp. 51\u201360, 2000.   \n[38] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, \u201cDistributed optimization and statistical learning via the alternating direction method of multipliers,\u201d in Foundations and Trends in Optimization, 2011, vol. 3, no.1, pp. 1\u2013122.   \n[39] H. Egilmez, E. Pavez, and A. Ortega, \u201cGraph learning from data under Laplacian and structural constraints,\u201d in IEEE Journal of Selected Topics in Signal Processing, July 2017, vol. 11, no.6, pp. 825\u2013841.   \n[40] Xiaowen Dong, Dorina Thanou, Michael Rabbat, and Pascal Frossard, \u201cLearning graphs from data: A signal representation perspective,\u201d IEEE Signal Processing Magazine, vol. 36, no. 3, pp. 44\u201363, 2019.   \n[41] Saghar Bagheri, Tam Thuc Do, Gene Cheung, and Antonio Ortega, \u201cSpectral graph learning with core eigenvectors prior via iterative GLASSO and projection,\u201d IEEE Transactions on Signal Processing, vol. 72, pp. 3958\u20133972, 2024.   \n[42] C. Tomasi and R. Manduchi, \u201cBilateral flitering for gray and color images,\u201d in Proceedings of the IEEE International Conference on Computer Vision, Bombay, India, 1998.   \n[43] Lei Zhang, Xiaolin Wu, Antoni Buades, and Xin Li, \u201cColor demosaicking by local directional interpolation and nonlocal adaptive thresholding,\u201d Journal of Electronic imaging, vol. 20, no. 2, pp. 023016\u2013023016, 2011.   \n[44] Eastman Kodak, \u201cKodak lossless true color image suite (photocd pcd0992),\u201d URL http://r0k. us/graphics/kodak, vol. 6, pp. 2, 1993.   \n[45] Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja, \u201cSingle image super-resolution from transformed self-exemplars,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 5197\u20135206.   \n[46] Wenzhu Xing and Karen Egiazarian, \u201cResidual swin transformer channel attention network for image demosaicing,\u201d in 2022 10th European Workshop on Visual Information Processing (EUVIP). IEEE, 2022, pp. 1\u20136.   \n[47] Daniele Menon, Stefano Andriani, and Giancarlo Calvagno, \u201cDemosaicing with directional flitering and a posteriori decision,\u201d IEEE Transactions on Image Processing, vol. 16, no. 1, pp. 132\u2013141, 2007.   \n[48] H.S. Malvar, Li wei He, and R. Cutler, \u201cHigh-quality linear interpolation for demosaicing of bayer-patterned color images,\u201d in 2004 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2004, vol. 3, pp. iii\u2013485.   \n[49] Jiahuan Ji, Baojiang Zhong, and Kai-Kuang Ma, \u201cImage interpolation using multi-scale attention-aware inception network,\u201d IEEE Transactions on Image Processing, vol. 29, pp. 9413\u20139428, 2020.   \n[50] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte, \u201cSwinir: Image restoration using swin transformer,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops, October 2021, pp. 1833\u20131844.   \n[51] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli, \u201cImage quality assessment: from error visibility to structural similarity,\u201d IEEE Transactions on Image Processing, vol. 13, no. 4, pp. 600\u2013612, 2004.   \n[52] Yurii Nesterov, Introductory lectures on convex optimization: A basic course, vol. 87, Springer Science & Business Media, 2013. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Full-Rankness of Matrix P in (4) ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Given that the underlying graph $\\mathcal{G}$ is positive and connected, we prove that coefficient matrix $\\mathbf{P}$ in (4) is full-rank and thus invertible. We prove by contradiction: suppose $\\mathbf{P}$ is not full-rank, and there exists a vector $\\mathbf{v}\\,=\\,[\\mathbf{x};\\,\\mu]$ such that $\\mathbf{P}\\mathbf{v}\\,=\\,\\mathbf{0}_{N+K}$ . Suppose we order $K$ sampled entries $\\mathbf{x}_{S}$ before $N-K$ non-sampled entries $\\mathbf{x}_{\\bar{S}}$ in $\\mathbf{x}$ , i.e., $\\mathbf{x}=[\\mathbf{x}_{S};\\mathbf{x}_{\\bar{S}}]$ . First, given sampling matrix $\\mathbf{H}=[\\mathbf{I}_{K}\\ \\ \\mathbf{0}_{K,N-K}]\\in\\ \\{0,\\bar{1}\\}^{K\\times N}$ , focusing on the second block row of $\\mathbf{P}$ , $[{\\bf H}\\ \\ {\\bf0}_{K,K}]{\\bf v}\\,=\\,{\\bf0}_{K}$ means ${\\bf H}{\\bf x}=\\big[{\\bf I}_{k}\\ {\\bf0}_{K,N-K}\\big]\\big[{\\bf x}_{S};{\\bf x}_{\\bar{S}}\\big]={\\bf0}_{K}$ . Thus, sampled entries of $\\mathbf{x}$ must be zeros, i.e., $\\mathbf{x}_{S}=\\mathbf{0}_{K}$ . Second, suppose we write Laplacian $\\mathbf{L}$ in blocks, i.e., $\\mathbf{L}\\,=\\,[\\mathbf{L}_{S,S}\\ \\mathbf{L}_{S,\\bar{S}};\\mathbf{L}_{\\bar{S},S}\\ \\mathbf{L}_{\\bar{S},\\bar{S}}]$ . Then, the non-sampled rows of the first block row of $\\mathbf{M}\\mathbf{v}$ are ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bigg([2\\mathbf{L}\\ \\mathbf{H}^{\\top}]\\left[\\begin{array}{l}{\\mathbf{x}}\\\\ {\\mu}\\end{array}\\right]\\bigg)_{\\bar{\\cal S}}=2\\left(\\mathbf{L}_{\\bar{\\cal S},\\cal S}\\mathbf{x}_{\\cal S}+\\mathbf{L}_{\\bar{\\cal S},\\bar{\\cal S}}\\mathbf{x}_{\\bar{\\cal S}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $({\\bf H}^{\\top}\\pmb{\\mu})_{\\bar{S}}\\,=\\,{\\bf0}_{N-K}$ since non-sampled rows of $\\mathbf{H}^{\\top}\\,=\\,\\left[\\mathbf{I}_{K};\\mathbf{0}_{N-K,K}\\right]$ are all zeros. From above, we know $\\mathbf{x}_{S}=\\mathbf{0}_{K}$ , and thus $([2\\mathbf{L}\\ \\mathbf{H}^{\\top}]\\mathbf{v})_{\\bar{S}}=\\mathbf{0}_{N-K}$ implies that we require $\\mathbf{L}_{\\bar{S},\\bar{S}}\\mathbf{x}_{\\bar{S}}=\\mathbf{0}_{N}$ . However, since $\\mathbf{L}_{\\mathcal{\\Bar{S}\\Bar{S}}}$ is a combinatorial Laplacian matrix for a positive sub-graph connecting nonsampled nodes $p l u s$ at least one strictly positive self-loop (representing an edge from a non-sampled node to a sample node), given $\\mathcal{G}$ is a connected positive graph, $\\mathbf{L}_{\\Bar{S},\\Bar{S}}$ must be PD (see Appendix A.2 below). Thus, $\\nexists\\mathbf{x}_{\\bar{S}}\\neq\\mathbf{0}$ s.t. $\\mathbf{L}_{\\bar{S},\\bar{S}}\\mathbf{x}_{\\bar{S}}=\\mathbf{0}_{N-K}$ , a contradiction. Therefore, we can conclude that $\\mathbf{P}$ must be full-rank. ", "page_idx": 13}, {"type": "text", "text": "A.2 Positive Definiteness of Matrix $\\mathbf{L}_{\\bar{S},\\bar{S}}$ in (8) ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Given that the underlying graph $\\mathcal{G}$ is positive and connected, we prove that coefficient matrix $\\mathbf{L}_{\\mathcal{\\Bar{S}\\Bar{S}}}$ in (8) is PD. By definition $\\mathbf{L}\\triangleq\\mathrm{diag}(\\mathbf{W}\\mathbf{1}_{N})-\\mathbf{W}$ , where W is an adjacency matrix for a positive graph without self-loops, i.e., $W_{i,j}\\,\\geq\\,0,\\forall i\\,\\neq\\,j$ and $W_{i,i}=0,\\forall i$ . Thus, $\\begin{array}{r}{L_{i,i}=\\sum_{j}W_{i,j},\\forall i}\\end{array}$ , and $L_{i,j}=-W_{i,j}\\le0,\\forall i\\neq j$ . For sub-matrix $\\begin{array}{r}{\\mathbf{L}_{\\bar{S},\\bar{S},}L_{i,i}=\\sum_{j\\in\\bar{S}}W_{i,j}+\\sum_{j\\in S}W_{i,j},\\bar{\\forall}i\\in\\bar{S}}\\end{array}$ . Define $\\mathbf{L}_{\\bar{S},\\bar{S}}^{\\prime}$ as a graph Laplacian matrix for nodes in $\\bar{S}$ consid ering only edg es between nodes in $\\bar{S}$ , i.e., $\\begin{array}{r}{L_{i,i}=\\sum_{j\\in\\bar{\\mathcal{S}}}W_{i,j},\\forall i\\in\\bar{\\mathcal{S}}}\\end{array}$ . Define $\\mathbf{D}_{\\bar{S},\\bar{S}}^{\\prime}$ as a diagonal degree matrix for nodes in $\\bar{S}$ considering only edges between $\\bar{S}$ and $\\boldsymbol{S}$ , i.e., $\\begin{array}{r}{D_{i,i}^{\\prime}=\\sum_{j\\in\\mathcal{S}}W_{i,j},\\forall i\\in\\bar{\\mathcal{S}}}\\end{array}$ . Note that $D_{i,i}^{\\prime}\\ge0,\\forall i$ . We can now write $\\mathbf{L}_{\\mathcal{\\Bar{S}\\Bar{S}}}$ as ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\bf L}_{\\bar{S},\\bar{S}}={\\bf L}_{\\bar{S},\\bar{S}}^{\\prime}+{\\bf D}_{\\bar{S},\\bar{S}}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "$\\mathbf{L}_{\\bar{S},\\bar{S}}^{\\prime}$ is a combinatorial graph Laplacian for a positive graph without self-loops, and thus is provably PSD [19]. $\\mathbf{D}_{\\bar{S},\\bar{S}}^{\\prime}$ is a non-negative diagonal matrix, and thus is also PSD. By Weyl\u2019s inequality, $\\mathbf{L}_{\\mathcal{\\Bar{S}\\Bar{S}}}$ is also PSD. ", "page_idx": 13}, {"type": "text", "text": "We prove by contradiction: suppose $\\mathbf{L}_{\\bar{S},\\bar{S}}$ is not PD, and $\\exists\\mathbf{x}\\ \\neq\\ \\mathbf{0}$ such that $\\mathbf{x}^{\\top}\\mathbf{L}_{\\bar{S},\\bar{S}}\\mathbf{x}\\ =\\ 0$ . $\\mathbf{x}^{\\top}\\mathbf{L}_{\\bar{S},\\bar{S}}\\mathbf{x}\\,=\\,0$ iff $\\mathbf{x}^{\\top}\\mathbf{L}_{\\bar{S},\\bar{S}}^{\\prime}\\mathbf{x}\\,=\\,0$ and D\u2032S\u00af, S\u00afx = 0 simultaneously. Denote by S\u00af1 and S\u00af2 the indices of nodes in $\\bar{S}$ with and without connections to nodes in $\\boldsymbol{S}$ , respectively. $\\bar{S}_{1}\\neq\\emptyset$ , since $\\mathcal{G}$ is connected. Suppose first $\\bar{S}_{2}=\\emptyset$ . Then $\\mathbf{D}_{\\bar{S},\\bar{S}}^{\\prime}$ has strictly positive diagonal entries and is PD, and there is no $\\mathbf{x}\\neq\\mathbf{0}$ s.t. $\\mathbf{x}^{\\top}\\mathbf{D}_{\\bar{S},\\bar{S}}^{\\prime}\\mathbf{x}=\\mathbf{0}$ , a contradiction. ", "page_idx": 13}, {"type": "text", "text": "Suppose now $\\bar{S}_{2}\\neq\\emptyset$ . First, $\\mathbf{x}^{\\top}\\mathbf{D}_{\\bar{S},\\bar{S}}^{\\prime}\\mathbf{x}=0$ implies $\\mathbf{x}_{\\bar{S}_{1}}=\\mathbf{0}$ . Then, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{x}^{\\top}\\mathbf{L}_{\\bar{S},\\bar{S}}^{\\prime}\\mathbf{x}=\\sum_{i,j\\in\\bar{S}}W_{i,j}(x_{i}-x_{j})^{2}\\geq\\sum_{i\\in\\bar{S}_{1},j\\in\\bar{S}_{2}}W_{i,j}(x_{i}-x_{j})^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since each term in the sum is non-negative, the sum is zero only if for each $(i,j)\\in\\mathcal{E}$ where $i\\in\\bar{S}_{1}$ and $j\\in\\bar{S}_{2}$ , $0=x_{i}=x_{j}$ . For nodes $\\bar{k}\\in\\bar{S}_{2}$ connected only to nodes $j\\in\\bar{S}_{2}$ connected to $i\\in\\bar{S}_{1}$ , $x_{k}=x_{j}=x_{i}=0$ necessarily, and for nodes $l\\in\\bar{S}_{2}$ connected to $k\\in\\bar{S}_{2}$ must have $x_{l}=x_{k}=0$ , and so on. Thus, ${\\bf x}={\\bf0}$ , a contradiction. Thus, we can conclude that $\\mathbf{L}_{\\Bar{S},\\Bar{S}}$ is PD. ", "page_idx": 13}, {"type": "text", "text": "A.3 Derivation of (17), (18) and (19) ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We define $\\pmb{\\phi}=[\\mathbf{z};\\mathbf{x};\\mathbf{q}_{1};\\mathbf{q}_{2}]$ and rewrite the objective (16) to ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\phi}{\\operatorname*{min}}}&{\\left[\\begin{array}{c}{\\mathbf{1}_{M}}\\\\ {\\mathbf{0}_{N+2M}}\\end{array}\\right]^{\\top}\\phi+(\\mu^{t})^{\\top}\\left(\\left[\\begin{array}{c}{\\mathbf{A}}\\\\ {\\mathbf{0}_{2M,M+N}\\mathbf{I}_{2M}}\\end{array}\\right]\\phi-\\left[\\begin{array}{c}{\\mathbf{b}}\\\\ {\\tilde{\\mathbf{q}}^{t}}\\end{array}\\right]\\right)}\\\\ &{+\\left.\\frac{\\gamma}{2}\\left\\|\\underbrace{\\left[\\begin{array}{c}{\\mathbf{A}}\\\\ {\\mathbf{0}_{2M,M+N}\\mathbf{I}_{2M}}\\end{array}\\right]}_{\\mathbf{B}}\\phi-\\left[\\begin{array}{c}{\\mathbf{b}}\\\\ {\\tilde{\\mathbf{q}}^{t}}\\end{array}\\right]\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "(30) is a convex quadratic objective, and so we take the derivative w.r.t. $\\phi$ and set it to 0: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left[\\begin{array}{c}{\\mathbf{1}_{M}}\\\\ {\\mathbf{0}_{N+2M}}\\end{array}\\right]+\\left[\\mathbf{A}^{\\top}\\begin{array}{c}{\\mathbf{0}_{M+N,2M}}\\\\ {\\mathbf{I}_{2M}}\\end{array}\\right](\\mu^{t})}\\\\ &{+\\frac{\\gamma}{2}\\left(2\\left[\\mathbf{A}^{\\top}\\begin{array}{c}{\\mathbf{0}_{M+N,2M}}\\\\ {\\mathbf{I}_{2M}}\\end{array}\\right]\\left[\\begin{array}{c}{\\mathbf{A}}\\\\ {\\mathbf{0}_{2M,M+N}\\mathbf{I}_{2M}}\\end{array}\\right]\\phi-2\\left[\\mathbf{A}^{\\top}\\begin{array}{c}{\\mathbf{0}_{M+N,2M}}\\\\ {\\mathbf{I}_{2M}}\\end{array}\\right]\\left[\\begin{array}{c}{\\mathbf{b}}\\\\ {\\tilde{\\mathbf{q}}^{t}}\\end{array}\\right]\\right)=\\mathbf{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Given that $\\mathbf{B}$ is the following matrix: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{B}=\\left[\\begin{array}{c c c c}{\\mathbf{I}_{M}}&{-\\mathbf{C}}&{-\\mathbf{I}_{M}}&{\\mathbf{0}_{M}}\\\\ {\\mathbf{I}_{M}}&{\\mathbf{C}}&{\\mathbf{0}_{M}}&{-\\mathbf{I}_{M}}\\\\ {\\mathbf{0}_{K,M}}&{\\mathbf{H}}&{\\mathbf{0}_{K,M}}&{\\mathbf{0}_{K,M}}\\\\ {\\mathbf{0}_{M,M}}&{\\mathbf{0}_{M,N}}&{\\mathbf{I}_{M}}&{\\mathbf{0}_{M}}\\\\ {\\mathbf{0}_{M,M}}&{\\mathbf{0}_{M,N}}&{\\mathbf{0}_{M}}&{\\mathbf{I}_{M}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Hence, $\\mathbf{B}^{\\top}\\mathbf{B}$ is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{B}^{\\top}\\mathbf{B}=\\left[\\begin{array}{c c c c}{2\\mathbf{I}_{M}}&{\\mathbf{0}_{M,N}}&{-\\mathbf{I}_{M}}&{-\\mathbf{I}_{M}}\\\\ {\\mathbf{0}_{N,M}}&{\\mathbf{2C}^{\\top}\\mathbf{C}+\\mathbf{H}^{\\top}\\mathbf{H}}&{\\mathbf{C}^{\\top}}&{-\\mathbf{C}^{\\top}}\\\\ {-\\mathbf{I}_{M}}&{\\mathbf{C}}&{2\\mathbf{I}_{M}}&{\\mathbf{0}_{M,M}}\\\\ {-\\mathbf{I}_{M}}&{-\\mathbf{C}}&{\\mathbf{0}_{M,M}}&{2\\mathbf{I}_{M}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that adding two of row 1 to rows 3 and 4, we get $\\big[2{\\bf I}_{M}\\ {\\bf0}_{M,N}\\ {\\bf0}_{M,M}\\ {\\bf0}_{M,M}\\big]$ . ", "page_idx": 14}, {"type": "text", "text": "Solving for $\\phi$ in (31), we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma\\mathbf{B}^{\\top}\\mathbf{B}\\phi=-\\left[\\begin{array}{l}{\\mathbf{1}_{M}}\\\\ {\\mathbf{0}_{N}}\\\\ {\\mathbf{0}_{M}}\\\\ {\\mathbf{0}_{M}}\\end{array}\\right]-\\mathbf{B}^{\\top}\\left(\\left[\\begin{array}{l}{\\mu_{a}^{t}}\\\\ {\\mu_{b}^{t}}\\\\ {\\mu_{c}^{t}}\\\\ {\\mu_{c}^{t}}\\\\ {\\mu_{c}^{t}}\\end{array}\\right]-\\gamma\\left[\\begin{array}{l}{\\mathbf{0}_{M}}\\\\ {\\mathbf{0}_{M}}\\\\ {\\mathbf{y}}\\\\ {\\tilde{\\mathbf{q}}_{2}^{t}}\\\\ {\\tilde{\\mathbf{q}}_{2}^{t}}\\end{array}\\right]\\right)}\\\\ &{\\qquad\\qquad=\\left[\\begin{array}{l}{-\\mathbf{1}_{M}-\\mu_{a}^{t}-\\mu_{b}^{t}}\\\\ {\\mathbf{C}^{\\top}\\mu_{a}^{t}-\\mathbf{C}^{\\top}\\mu_{b}^{t}-\\mathbf{H}^{\\top}\\mu_{c}^{t}+\\gamma\\mathbf{H}^{\\top}\\mathbf{y}}\\\\ {\\mu_{a}^{t}-\\mu_{d}^{t}+\\gamma\\tilde{\\mathbf{q}}_{1}^{t}}\\\\ {\\mu_{b}^{t}-\\mu_{c}^{t}+\\gamma\\tilde{\\mathbf{q}}_{2}^{t}}\\end{array}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We can solve for ${\\bf z}^{t+1}$ directly; by adding two of row 1 to rows 3 and 4 of (35), we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{2\\gamma\\mathbf{z}^{t+1}=-2(\\mathbf{1}_{M})-\\mu_{a}^{t}-\\mu_{b}^{t}-\\mu_{d}^{t}-\\mu_{e}^{t}+\\gamma(\\tilde{\\mathbf{q}}_{1}^{t}+\\tilde{\\mathbf{q}}_{2}^{t})}}\\\\ {{\\quad\\mathbf{z}^{t+1}=-\\displaystyle\\frac{1}{\\gamma}\\mathbf{1}_{M}-\\displaystyle\\frac{1}{2\\gamma}\\left(\\mu_{a}^{t}+\\mu_{b}^{t}+\\mu_{d}^{t}+\\mu_{e}^{t}\\right)+\\displaystyle\\frac{1}{2}(\\tilde{\\mathbf{q}}_{1}^{t}+\\tilde{\\mathbf{q}}_{2}^{t}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Subtracting row 4 from row 3 of (35), we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\gamma\\mathbf{C}\\mathbf{x}^{t+1}+2\\gamma(\\mathbf{q}_{1}^{t+1}-\\mathbf{q}_{2}^{t+1})=\\mu_{a}^{t}-\\mu_{b}^{t}-\\mu_{d}^{t}+\\mu_{e}^{t}+\\gamma(\\tilde{\\mathbf{q}}_{1}^{t}-\\tilde{\\mathbf{q}}_{2}^{t})}\\\\ &{\\qquad\\qquad\\qquad\\gamma(\\mathbf{q}_{1}^{t+1}-\\mathbf{q}_{2}^{t+1})=-\\gamma\\mathbf{C}\\mathbf{x}^{t+1}+\\displaystyle\\frac{1}{2}\\left(\\mu_{a}^{t}-\\mu_{b}^{t}-\\mu_{d}^{t}+\\mu_{e}^{t}\\right)+\\frac{\\gamma}{2}(\\tilde{\\mathbf{q}}_{1}^{t}-\\tilde{\\mathbf{q}}_{2}^{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, row 2 can be rewritten as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma(2\\mathbf{C}^{\\top}\\mathbf{C}+\\mathbf{H}^{\\top}\\mathbf{H})\\mathbf{x}^{t+1}+\\mathbf{C}^{\\top}\\gamma(\\mathbf{q}_{1}^{t+1}-\\mathbf{q}_{2}^{t+1})=\\mathbf{C}^{\\top}\\mu_{a}^{t}-\\mathbf{C}^{\\top}\\mu_{b}^{t}-\\mathbf{H}^{\\top}\\mu_{c}^{t}+\\gamma\\mathbf{H}^{\\top}\\mathbf{y}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\gamma(\\mathbf{C}^{\\top}\\mathbf{C}+\\mathbf{H}^{\\top}\\mathbf{H})\\mathbf{x}^{t+1}=\\frac{1}{2}\\mathbf{C}^{\\top}\\left(\\mu_{a}^{t}-\\mu_{b}^{t}+\\mu_{d}^{t}-\\mu_{e}^{t}\\right)-\\mathbf{H}^{\\top}\\mu_{c}^{t}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad-\\frac{\\gamma}{2}\\mathbf{C}^{\\top}(\\widetilde{\\mathbf{q}}_{1}^{t}-\\widetilde{\\mathbf{q}}_{2}^{t})+\\gamma\\mathbf{H}^{\\top}\\mathbf{y}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Finally, from rows 3 and 4, $\\mathbf{q}_{1}^{t}$ and $\\mathbf{q}_{2}^{t}$ can be computed as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf q}_{1}^{t}=\\frac{1}{2}\\left({\\bf z}^{t+1}-{\\bf C x}^{t+1}\\right)+\\frac{1}{2\\gamma}(\\mu_{a}^{t}-\\mu_{d}^{t}+\\gamma\\tilde{{\\bf q}}_{1}^{t})}}\\\\ {{\\displaystyle{\\bf q}_{2}^{t}=\\frac{1}{2}\\left({\\bf z}^{t+1}+{\\bf C x}^{t+1}\\right)+\\frac{1}{2\\gamma}(\\mu_{b}^{t}-\\mu_{e}^{t}+\\gamma\\tilde{{\\bf q}}_{2}^{t}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A.4 Invertibility of $\\pmb{\\mathcal{L}}=\\mathbf{C}^{\\top}\\mathbf{C}+\\mathbf{H}^{\\top}\\mathbf{H}$ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Clearly $\\pmb{\\mathcal{L}}=\\mathbf{C}^{\\top}\\mathbf{C}+\\mathbf{H}^{\\top}\\mathbf{H}$ is real, symmetric, and positive semi-definite. Thus its eigenvalues are real and non-negative. To show that it is invertible, it suffices to show that its minimum eigenvalue $\\lambda_{\\operatorname*{min}}(\\mathcal{L})$ is strictly greater than zero. But $\\begin{array}{r}{\\lambda_{\\operatorname*{min}}(\\pmb{\\mathscr{L}})=\\operatorname*{min}_{\\mathbf{x}:||\\mathbf{x}||=1}\\mathbf{x}^{\\top}\\mathcal{L}\\mathbf{x}}\\end{array}$ . Hence it suffices to show that $\\mathbf{x}^{\\top}{\\boldsymbol{\\mathcal{L}}}\\mathbf{x}=0$ implies ${\\bf x}={\\bf0}$ . Now observe that $\\begin{array}{r}{\\mathbf{x}^{\\top}\\mathcal{L}\\mathbf{x}=\\sum_{(i,j)\\in\\mathcal{E}}w_{i,j}^{2}(x_{i}-x_{j})^{2}+\\sum_{i\\in\\mathcal{S}}x_{i}^{2}}\\end{array}$ , where $\\boldsymbol{S}$ is the set of nodes with constrained values. If $\\mathbf{x}^{\\top}{\\boldsymbol{\\mathcal{L}}}\\mathbf{x}=0$ then all terms must be zero, meaning that all $x_{i}$ in $\\boldsymbol{S}$ are zero, and hence all of their neighbors, and all of their neighbors, and so on. Thus, $\\mathcal{L}$ is invertible if there exists at least one node with a constrained value (i.e., a self-loop) in each connected component of the graph. ", "page_idx": 15}, {"type": "text", "text": "A.5 Derivation of (21) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We derive the solution to optimization (20). Ignoring the first convex but non-smooth term $g(\\tilde{\\mathbf{q}})$ , the remaining two terms in the objective are convex and smooth. Taking the derivative w.r.t. variable $\\tilde{\\mathbf{q}}$ and setting it to 0, we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{-\\pmb{\\mu}_{d}^{t}-\\gamma\\mathbf{q}^{t+1}+\\gamma\\tilde{\\mathbf{q}}^{*}=\\mathbf{0}_{M}}\\\\ {\\tilde{\\mathbf{q}}^{*}=\\mathbf{q}^{t+1}+\\frac{1}{\\gamma}\\pmb{\\mu}_{d}^{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This solution is valid iff $g(\\tilde{\\mathbf{q}}^{*})=0$ ; otherwise the first term $g(\\tilde{\\mathbf{q}}^{*})$ dominates and $\\tilde{\\mathbf{q}}^{*}=\\mathbf{0}_{M}$ . Given that (40) can be computed entry-by-entry separately, (21) follows. ", "page_idx": 15}, {"type": "text", "text": "A.6 Example of Edge Weight Normalization for the Incidence Matrix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Fig. 3(top), we show an example three-node undirected graph with three edge weights $w_{1,2}=1/2$ , $w_{1,3}=1/2$ , and $w_{2,3}=1/3$ , and the corresponding incidence matrix C. Normalizing edge weights using (26), we see in Fig. 3(middle) a directed graph with six edges where the sum of normalized edge weights leaving a node is 1, resulting in normalized incidence matrix $\\bar{\\mathbf{C}}$ . Finally, we see in Fig. 3(bottom) an undirected graph with three edges corresponding to graph Laplacian $\\bar{\\mathbf{L}}=\\bar{\\mathbf{C}}^{\\top}\\bar{\\mathbf{C}}$ . Note that $\\Vert\\bar{\\mathbf{C}}\\mathbf{1}_{3}\\Vert_{1}=\\mathbf{0}_{6}$ as expected. ", "page_idx": 15}, {"type": "text", "text": "A.7 Parameters Learning in Conjugate Gradient Algorithm (CG) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The linear systems that we need to solve\u2014(8) for GLR minimization and (18) for GTV minimization\u2014 have the follow form, ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\mathcal{L}}\\mathbf{x}=\\mathbf{b}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Given $\\mathcal{L}$ is PD, we consider the minimization problem, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}}Q(\\mathbf{x})=\\frac{1}{2}\\mathbf{x}^{\\top}\\mathcal{L}\\mathbf{x}-\\mathbf{b}^{\\top}\\mathbf{x}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with gradient ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\delta Q(\\mathbf{x})}{\\delta\\mathbf{x}}=\\mathcal{L}\\mathbf{x}-\\mathbf{b}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "image", "img_path": "i8LoWBJf7j/tmp/7bfe23fcaa76e3af1c34d85d627eaf67c4966bf60b428d0129b5f437a09bc948.jpg", "img_caption": ["Figure 3: 3-node graph for incidence matrix $\\mathbf{C}$ (top), 3-node graph for normalized incidence matrix $\\bar{\\bf C}$ (middle), 3-node graph for graph Laplacian $\\bar{\\mathbf{L}}=\\bar{\\mathbf{C}}^{\\top}\\bar{\\mathbf{C}}$ where $\\begin{array}{r}{s={\\frac{6}{5}}}\\end{array}$ (bottom). "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Thus, the simple gradient descent has the following update rules with $\\alpha_{t}$ commonly known as learning rate, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbf{g}^{t}=\\mathcal{L}\\mathbf{x}^{t}-\\mathbf{b}=\\mathbf{g}^{t-1}-\\alpha_{t}\\mathcal{L}\\mathbf{g}^{t-1}}\\\\ &{\\mathbf{x}^{t+1}=\\mathbf{x}^{t}-\\alpha_{t}\\mathbf{g}^{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Next, a momentum term $\\beta_{t}$ and the cumulative gradients term $\\mathbf{v}^{t}$ are added, resulting in the wellknown Accelerated Gradient Descent Algorithm [52], also known as Conjugated Gradient Descent. The new update rules are ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{g}^{t}=\\mathbf{g}^{t-1}-\\alpha_{t}\\pmb{\\mathcal{L}}\\mathbf{v}^{t-1}}\\\\ &{\\mathbf{v}^{t}=\\mathbf{g}^{t}+\\beta_{t}\\mathbf{v}^{t-1}}\\\\ &{\\mathbf{x}^{t+1}=\\mathbf{x}^{t}-\\alpha_{t}\\mathbf{v}^{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where both $\\alpha_{t}$ and $\\beta_{t}$ in each iteration $t$ are considered trainable parameters. ", "page_idx": 16}, {"type": "text", "text": "A.8 Experimental Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Initially, we developed our unrolled ADMM model without training any parameters. The target signal $\\mathbf{x}$ was estimated using linear interpolation of known values in $5\\times5$ pixel neighborhood. RGB values and pixel locations were combined to form feature vectors for computing edge weights $w_{i,j}$ , which were shared across the three channels. The metric matrix $\\mathbf{M}$ was initialized as a diagonal matrix with all entries set to 1.5. Vectors $\\begin{array}{r}{\\mu_{a},\\mu_{b},\\mu_{c},}\\end{array}$ , $\\mu_{d}$ , and $\\mu_{e}$ were initialized with all entries equal to 0.1. The parameters $\\gamma,\\alpha$ , and $\\beta$ in CG were set to 10, 0.5, and 0.3, respectively. For the learned ADMM block, the parameters $\\gamma$ , \u03b1, $\\beta$ , and the metric matrix $\\mathbf{M}$ were learned during training. A training dataset was created consisting of 5000, 10000, or 20000 image patches, each of size $64\\times64$ , to train the model. ", "page_idx": 16}, {"type": "text", "text": "All matrix multiplications in our models are implemented to take advantage of the sparsity of the constructed graphs, which were restricted to be a window of size $5\\times5=25$ nearest neighbors of each pixel. This ensures that our graphs are always connected and sparse. We stacked vertically 4 Graph Learning modules coupled with ADMM block, so that we can learn multiple graphs in parallel. This is commonly known as multi-head in the transformer architecture. We also stacked 5 graph learning modules and ADMM blocks horizontally to further learn more graphs. In all ADMM blocks, we set the number of ADMM iterations to 5 and the number of CG iterations to 10. ", "page_idx": 16}, {"type": "text", "text": "To extract high-level features, a shallow CNN was employed, consisting of four convolutional layers with 48 feature maps (12 features for each graph learning head). After each convolutional layer, a ReLU activation function was applied. Convolutional layers utilized $3\\times3$ kernels without any down-sampling, generating 48 features for each pixel. The feature maps were divided into four sets, with each set serving as the input for a Graph Learning module, allowing for the parallel production of four graphs. A simple weighted average scheme was then used to combine the outputs into a single output $\\mathbf{x}^{t}$ . Additionally, skip connections were introduced between the convolutional layers to facilitate the training process. ", "page_idx": 16}, {"type": "image", "img_path": "i8LoWBJf7j/tmp/334d66a650829c6dda06b1b7c56ea61decb2a413ea4fe48a1d98c54d2beb15c4.jpg", "img_caption": ["Figure 4: Visual demosaicking results for image Urban062. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "A.9 Additional Experimental Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Fig. 4 shows visual results for a test image for all models, including our uGTV, uGLR and the baselines. Two variants of RSTCANet, uGTV and uGLR are trained on 10000 images patches of size $64\\times64$ for 30 epochs. We observe that our two models, especially uGTV, has better performance compared to RST-B and comparable performance with RST-S in selected high-frequency area. ", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Main claims are clearly stated in the abstract and Introduction. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We state the limitation of our edge weight computation, as compared to self-attention weights in conventional transformers, in Section 5.2. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: All proofs and derivations are included in the Appendices. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Details of our experiments are explained in Section 6.1 and Appendix A.8. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: We used open source data: McM [43], Kodak [44], and Urban100 [45] datasets. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We described the experiments setup in Section 6.1 and Appendix A.8. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: We did provide various metrics for many datasets to compare our models with the baseline models. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We described the experimental setup in Section 6.1 and Appendix A.8. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: As authors, we have made every effort to conform to the NeurIPS Code of Ethics. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: The performed work is purely computational and does not have direct societal impact. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 22}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]