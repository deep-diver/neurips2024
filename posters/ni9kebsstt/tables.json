[{"figure_path": "Ni9kebsSTt/tables/tables_6_1.jpg", "caption": "Table 1: Results on text completion (upper table) and other tasks (lower table). Bold numbers indicate the best performance. PPL: Perplexity. RG: ROUGE score. Avg. Len: Average generation length. ABLEU/ARG: The difference between the max score to correct references and the max score to incorrect references. FS: FACTSCORE with length penalty.", "description": "This table presents the quantitative results of the proposed NEST model and several baseline models on various downstream tasks.  The upper half shows results for text completion tasks (WikiText-103 and Pile of Law), evaluating metrics like perplexity, ROUGE scores, and average length.  The lower half shows results for question answering (TQA, NQ, HQA, MQA), fact verification (TruthfulQA, Biography), and multi-task learning (MMLU), reporting metrics like answer-level recall, BLEU, ROUGE, FACTSCORE, and accuracy.  The results are presented for different sizes of the Llama-2-Chat language model (7B, 13B, and 70B) with and without retrieval augmentation (RA), kNN-LM, NEST, and a combination of RA and NEST.  The bold numbers indicate the best-performing model for each metric.", "section": "4.4 Main Results"}, {"figure_path": "Ni9kebsSTt/tables/tables_7_1.jpg", "caption": "Table 1: Results on text completion (upper table) and other tasks (lower table). Bold numbers indicate the best performance. PPL: Perplexity. RG: ROUGE score. Avg. Len: Average generation length. ABLUE/ARG: The difference between the max score to correct references and the max score to incorrect references. FS: FACTSCORE with length penalty.", "description": "This table presents the results of various experiments conducted on different language models using the proposed NEST method and several baseline methods for various tasks like text completion, question answering, fact verification, and multi-choice tasks.  The upper half shows the results for text completion, with metrics such as Perplexity, ROUGE scores, and average generation length. The lower half shows the results for other tasks, with metrics tailored to the specific task.  The table allows comparison of the performance of different models and methods across various downstream applications.", "section": "4.4 Main Results"}, {"figure_path": "Ni9kebsSTt/tables/tables_21_1.jpg", "caption": "Table 3: Ablation study on the validation set of WikiText-103, NQ, and Biography. ROUGE-1 is reported for WikiText-103, ALR is reported for NQ, and FACTSCORE is reported for Biography.", "description": "This ablation study shows the effect of progressively adding components of NEST to a two-stage kNN-LM baseline on three different datasets.  It demonstrates the contribution of each component: Relative Retrieval Confidence, Dynamic Span Selection, and Relaxed Speculative Decoding, to the overall performance measured by ROUGE-1, Answer-Level Recall (ALR), and FACTSCORE.", "section": "4.4 Main Results"}]