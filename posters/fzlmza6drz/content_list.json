[{"type": "text", "text": "GRAPHTRAIL: Translating GNN Predictions into Human-Interpretable Logical Rules ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Burouj Armgaan, Manthan Dalmia ", "page_idx": 0}, {"type": "text", "text": "Sourav Medya ", "page_idx": 0}, {"type": "text", "text": "Department of Computer Science & Engineering IIT Delhi, India csz228001@iitd.ac.in, manthandalmia2@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Department of Computer Science University of Illinois, Chicago, USA medya@uic.edu ", "page_idx": 0}, {"type": "text", "text": "Sayan Ranu Department of Computer Science & Engineering and Yardi School of AI IIT Delhi, India sayanranu@cse.iitd.ac.in ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Instance-level explanation of graph neural networks (GNNs) is a well-studied area. These explainers, however, only explain an instance (e.g., a graph) and fail to uncover the combinatorial reasoning learned by a GNN from the training data towards making its predictions. In this work, we introduce GRAPHTRAIL, the first end-to-end, post-hoc, global GNN explainer that translates the functioning of a black-box GNN model to a boolean formula over the (sub)graph-level concepts without relying on local explainers. GRAPHTRAIL is unique in automatically mining the discriminative subgraph-level concepts using Shapley values. Subsequently, the GNN predictions are mapped to a human-interpretable boolean formula over these concepts through symbolic regression. Extensive experiments across diverse datasets and GNN architectures demonstrate significant improvement over existing global explainers in mapping GNN predictions to faithful logical formulae. The robust and accurate performance of GRAPHTRAIL makes it invaluable for improving GNNs and facilitates adoption in domains with strict transparency requirements. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction and Related Works ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "GNNs have witnessed widespread adoption for graph-level prediction tasks due to their impressive performance [35, 14, 46, 11, 41, 18, 38, 32, 5, 12, 53, 22]. Unfortunately, like other deep-learning models, GNNs are considered black boxes due to their lack of transparency and interpretability. This lack of interpretability presents a significant barrier to their adoption in critical domains such as healthcare, finance, and law enforcement. Additionally, the ability to explain predictions is crucial for understanding potential flaws in the model and generating insights for further refinement. ", "page_idx": 0}, {"type": "text", "text": "Existing Works: To introduce interpretability of GNNs, several algorithms have been proposed in the literature [23]. Fig. G in the Appendix, which was originally presented in [23], and now updated by us with more recent works, presents the taxonomy of GNN explainability research. As observed, a vast majority of explainers focus on instance-level explanations. ", "page_idx": 0}, {"type": "text", "text": "Instance-level (or local) explainers [55, 29, 42, 58, 15, 57, 27, 45, 25, 3, 1, 51, 47, 26, 6, 19] take a graph as input and identify components within this graph\u2014-such as a subgraph\u2014that maximally influence the prediction made by a model. This instance-level focus limits its ability to extract patterns utilized by GNNs at a global level across a multitude of graphs and how these patterns are combined into a single decision-making rule. The objective of our work is to develop an end-to-end global explainer that $(I)$ mines the subgraph concepts used by a black-box GNN model, and then (2) uncovers the boolean logic used by the GNN over these concepts to make its predictions. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Works on global GNN explainers are limited [56, 16, 54, 2]. XGNN [56] and GNNInterpreter [50] are generative-modeling based global explainers. Both generate a graph that maximally aligns with a specified class label. While this generated graph likely contains important features used by the GNN in making its predictions, it may not actually be present in the dataset, limiting its utility for analyzing specific predictions. Additionally, it does not produce a human-interpretable rule explaining the class attributions made by the GNN. Finally, XGNN [56] requires domain-specific validity-rules as input, which affects its generalizability and results in inferior performance [2]. Another work [54] evaluates which base concepts are detected by the neurons of a GNN model during predictions and their relative importance. These concepts can be subgraphs or node-level properties, such as degrees. However, this method lacks the ability to automatically mine the concepts. More importantly, this method also does not generate a human-interpretable rule for the decision-making process of the GNN. ", "page_idx": 1}, {"type": "text", "text": "The closest work to ours is GLGEXPLAINER [2], which shares the objective of providing a global explanation of the GNN through a boolean formula over subgraph-level concepts. However, there are significant limitations that need to be addressed: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Dependency on instance-level explainers: GLGEXPLAINER does not mine the subgraph concepts in an end-to-end manner. Instead, it relies on an instance-level explainer (e.g. PGEXPLAINER [29]) to provide these concepts, over which it searches for the combinatorial formula mapping to the GNN predictions. This dependency creates a disconnect with the objective, as instance-level explainers lack a global understanding of the model. Our proposed approach develops an end-to-end pipeline that mines concepts based on global trends. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Lack of interpretability due to vector-level concepts: In GLGEXPLAINER, each concept in the formula corresponds to a feature vector and not a subgraph. These vectors represent the embedding of a cluster of subgraphs generated by the instance explainer. Hence, in its original form, the formula is not human-interpretable. To convert into a human-interpretable formula, GLGEXPLAINER randomly selects a subgraph from the cluster, assuming all subgraphs in a cluster are similar. Our investigation $(\\S\\,4)$ reveals that this assumption is rarely true in practice, compromising both interpretability and efficacy. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Lack of robustness: GLGEXPLAINER shows significant variation in the formula based on the training split used. As our analysis in $\\S\\,4$ reveals, due to the reliance on instance-level explanations, even when data are drawn from the same distribution, the base concept candidates vary, and consequently so does the eventual formula. ", "page_idx": 1}, {"type": "text", "text": "At this juncture, we note that our work is distinct from the line of research on explainable GNNs [61, 10, 34]. Explainable GNNs are designed to make explainable predictions rather than explaining the predictions of a black-box GNN. ", "page_idx": 1}, {"type": "text", "text": "Contributions: In this work, we present an end-to-end, post-hoc, global GNN explainer called GRAPHTRAIL (TRAnslating GNN Prediction into human-Interpretable Logical Rules)1, which addresses the limitations outlined above. Specifically, ", "page_idx": 1}, {"type": "text", "text": "\u2022 Problem formulation: We formulate the problem of translating a message-passing GNN model for graph classification into a human-interpretable logic formula over subgraph concepts. Unlike existing works, in our formulation, the concepts are not assumed to be an input generated through a decoupled algorithm. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Novel methodology: We develop GRAPHTRAIL, which uses a mix of several innovative insights. First, GRAPHTRAIL exploits the fact that a message passing GNN decomposes a graph into a set of computation trees. This enables GRAPHTRAIL to limit the exploration of concepts from an exponential subgraph search space to a linear space of computation trees. The global impact of computation trees is assessed using Shapley values, and then mapped to a boolean formula over concepts using symbolic regression. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Empirical analysis: Extensive experiments across a diverse set of datasets, GNN architectures and pooling function, demonstrate GRAPHTRAIL to significantly surpass existing global explainers in efficacy, human-interpretability, data efficiency, and robustness. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries and Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Definition 1 (Graph). A graph is defined as $\\mathcal{G}=(\\mathcal{V},\\mathcal{E},X)$ over a node set $\\mathcal{V}$ , edge set $\\mathcal{E}=\\{(u,v)$ | $u,v\\in\\mathcal{V}\\}$ and a node feature matrix $X=\\{_{\\mathbf{x}_{v}}\\mid v\\in\\mathcal{V}\\}$ where $\\mathbf{x}_{v}\\,\\in\\,\\mathbb{R}^{d}$ is the set of features characterizing each node. ", "page_idx": 2}, {"type": "text", "text": "Two graphs are termed identical if they are isomorphic to each other. ", "page_idx": 2}, {"type": "text", "text": "Definition 2 (Graph Isomorphism). Graph $\\mathcal{G}_{1}$ is isomorphic to graph $\\mathscr{G}_{2}$ (denoted as $\\mathcal{G}_{1}\\cong\\mathcal{G}_{2},$ ) if there exists a bijection between their node sets that preserves the edge connectivity and node features. Specifically, ${\\mathcal{G}}_{1}\\,\\cong\\,{\\mathcal{G}}_{2}\\;\\;\\iff\\;\\exists f\\,:\\,{\\mathcal{V}}_{1}\\,\\to\\,{\\mathcal{V}}_{2}$ such that: (1) $f$ is a bijection, (2) $\\mathbf{x}_{v}=$ $\\mathbf{x}_{f(v)}$ , where $\\mathbf{x}_{v}\\in\\mathcal{V}_{1},\\mathbf{x}_{f(v)}\\in\\mathcal{V}_{2}$ and (3) $(u,v)\\in\\mathcal{E}_{1}$ if and only $i f\\left(f(u),f(v)\\right)\\in\\mathcal{E}_{2}$ . ", "page_idx": 2}, {"type": "text", "text": "Graph $\\mathcal{G}_{1}$ is subgraph isomorphic to $\\mathcal{G}_{2}$ , denoted as $\\mathcal{G}_{1}\\subseteq\\mathcal{G}_{2}$ , if $f$ is an injection and condition (3) is modified to $(u,v)\\in\\mathcal{E}_{1}$ if $(f(u),f(v))\\in{\\mathcal{E}}_{2}$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 3 (Graph Classification). In graph classification, we are given a set of train graphs $\\mathcal{D}_{t r}=\\{\\mathcal{G}_{1},\\cdots\\,,\\bar{\\mathcal{G}_{m}}\\}$ , where each graph $\\mathcal{G}_{i}$ is tagged with a class label $\\mathcal{y}_{i}$ from the set $\\{\\mathcal{D}_{1},\\cdot\\cdot\\cdot,\\mathcal{D}_{c}\\}$ . The objective is to train a GNN model $\\Phi$ such that given a graph with an unknown class label, the label prediction error is minimized. ", "page_idx": 2}, {"type": "text", "text": "Error may be measured using any of the known metrics such as cross-entropy loss, negative loglikelihood, etc. Hereon, we implicitly assume $\\Phi$ to be a message-passing GNN [22, 14, 46, 53]. We assume the GNN $\\Phi$ returns a $c$ -dimensional distribution over the class labels, where $\\Phi(\\mathcal{G})_{j}$ is the probability of the $j$ -th class, and $\\Phi(\\mathcal{G})^{*}=\\arg\\operatorname*{max}_{j\\in\\{1,2,\\cdots,c\\}}\\{\\Phi(\\mathcal{G})_{j}\\}$ denotes the predicted class label. ", "page_idx": 2}, {"type": "text", "text": "Definition 4 (Concepts [2, 21]). Concepts refer to semantically meaningful units of information within the data that humans use to analyze and make decisions about that dataset. In the context of graph classification, these concepts correspond to subgraphs. ", "page_idx": 2}, {"type": "text", "text": "Problem 1 (Global GNN explanation through boolean logic over concepts). Let $\\mathcal{D}$ be a set of graphs, where each graph is labeled with a class from the set $\\{\\bar{y}_{1},\\cdot\\cdot\\cdot,\\mathscr{D}_{c}\\}$ . Given a trained GNN $\\Phi$ , our objective is to learn a set of c boolean formulas $\\{f_{1},\\cdot\\cdot\\cdot,f_{c}\\}$ , over subgraph-level concepts such that for any graph $\\mathcal{G}$ , $i f\\,\\Phi({\\mathcal G})^{*}={\\mathcal N}_{i}$ , then $f_{i}(\\mathcal{G})=T R U E$ and $\\forall j\\ne i$ , $f_{j}(\\mathcal{G})=F A L S E.$ . The candidate space of concepts includes all unique subgraphs of the dataset, i.e., $\\mathcal{C}=\\{S\\mid S\\subseteq\\mathcal{G},\\mathcal{G}\\in\\mathcal{D}\\}$ . ", "page_idx": 2}, {"type": "text", "text": "The proposed problem formulation surfaces two key challenges: ", "page_idx": 2}, {"type": "text", "text": "1. How do we extract the concepts? Concept extraction poses a significant challenge due to the exponential size of the candidate space. In the worst case, a graph with $n$ nodes can have $2^{n}$ possible subgraphs. Furthermore, if there are $n$ subgraphs in the dataset, identifying the unique subgraphs requires performing ${\\mathcal{O}}(n^{2})$ graph isomorphism tests. The computation cost of graph isomorphism grows exponentially with graph size.   \n2. How do we uncover the boolean logic over the concepts? The number of boolean formulas associated with a set of symbols (concepts) increases exponentially with the size of the set. Hence, the scalability challenge is further exacerbated. ", "page_idx": 2}, {"type": "text", "text": "3 GRAPHTRAIL: Proposed Global Explainer ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Fig. 1 presents the pipeline of GRAPHTRAIL. GRAPHTRAIL builds on the observation that any message passing GNN decomposes a graph of $n$ nodes into $n$ computation trees. Consequently, only the subgraphs corresponding to these computation trees are processed by the GNN and the rest of the subgraphs are irrelevant to the GNN\u2019s predictions. This property allows us to reduce the candidate space size from exponential to linear. Subsequently, the impact of each of the computation trees is assessed through its Shapley Value [43], and the top- $k$ trees are sent to the logic formulator. The Boolean logic is revealed by the symbolic regression over these computation trees. Symbolic regression aims to discover concise closed-form mathematical equations that best fit a given set of data [24, 31]. In our context, we constrain the regressor over computation trees with boolean operators and fit to the predictions of GNN. We next elaborate on each of these steps. ", "page_idx": 2}, {"type": "text", "text": "3.1 Computation Framework of Message-passing GNNs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "GNNs aggregate messages layer by layer. If $\\mathbf{x}_{v}\\in\\mathbb{R}^{|F|}$ is the input feature vector for node $v\\in\\mathcal{V}$ , then the $\\mathbf{\\overline{{0}}}^{t h}$ layer representation of node $v$ is set to $\\mathbf{h}_{v}^{0}=\\mathbf{x}_{v}$ . In each of the subsequent layers $\\ell$ , ", "page_idx": 2}, {"type": "image", "img_path": "fzlMza6dRZ/tmp/b44b765761bdf063ba6f52ca7b50e81c9f5c72862db312c1176093284e401246.jpg", "img_caption": ["Figure 1: Pipeline of the GRAPHTRAIL algorithm. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "GNNs draw messages from their neighbors $\\mathcal{N}_{v}=\\{u\\ |\\ (u,v\\in\\mathcal{E})\\}$ and aggregate as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{m}_{v}^{\\ell}(u)=\\mathrm{MsG}^{\\ell}(\\mathbf{h}_{u}^{\\ell-1},\\mathbf{h}_{v}^{\\ell-1})}\\\\ &{\\quad\\overline{{\\mathbf{m}}}_{v}^{\\ell}=\\mathrm{AGGREGATE}^{\\ell}(\\{\\mathbf{\\{m}}_{v}^{\\ell}(u),\\forall u\\in\\mathcal{N}_{v}\\})}\\\\ &{\\quad\\mathbf{h}_{v}^{\\ell}=\\mathrm{MLP}^{\\ell}(\\mathbf{h}_{v}^{\\ell-1},\\overline{{\\mathbf{m}}}_{v}^{\\ell})}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $\\mathbf{M}\\mathbf{S}\\mathbf{G}^{\\ell}$ and AGGREGATE\u2113can be either pre-defined functions (e.g., MEANPOOL) or neural networks (e.g., GAT [46]). $\\{\\!\\!\\left\\{\\cdot\\right\\}\\!\\!\\}$ denotes a multi-set since the same message may be received from multiple nodes. The $\\ell^{t h}$ -layer representation of node $v$ is the embedding generated by passing the aggregated messages through an MLP. Finally, the representation of the entire graph is computed as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{h}_{\\mathcal{G}}=\\mathrm{PooL}\\left(\\left\\{\\mathbf{h}_{v}^{L}\\mid\\forall v\\in\\mathcal{V}\\right\\}\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, POOL could represent aggregation functions such as MEANPOOL, SUMPOOL, etc., and $L$ is the total number of layers in the GNN. Following the computation of the graph embedding, it is mapped to a distribution $\\Phi({\\mathcal{G}})$ over all class labels by passing $\\mathbf{h}_{\\mathcal{G}}$ through an MLP. ", "page_idx": 3}, {"type": "text", "text": "3.2 Mapping Concepts to Rooted Computation Trees ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The message passing architecture of GNNs limits the embedding computation of a node $v$ within the computation tree rooted at $v$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 5 (Rooted Computation Tree ${\\mathcal{T}}_{v}^{L}$ ). A computation tree, also known as a receptive field, is a fundamental concept in GNNs that describes how information propagates through the graph during the neural network\u2019s operation. Formally, given a graph $\\mathcal{G}=(\\mathcal{V},\\mathcal{E},X)$ , a node $v\\in\\mathcal{V}$ , and the number of layers $L$ in the GNN, the computation tree $\\Bar{T}_{v}^{L}$ rooted at $v$ is defined as follows: ", "page_idx": 3}, {"type": "text", "text": "\u2022 Enumerate all paths (including those with repeated vertices) starting from v and extending up to $L$ hops away.   \n\u2022 Merge these paths into a tree structure where: i) the root is always $v$ , and ii) two nodes from different paths are merged if: a) they are at the same depth in their respective paths, and $b$ ) all their ancestors in the paths have already been merged. This tree represents how information flows to node v during L rounds of message passing in the GNN. ", "page_idx": 3}, {"type": "text", "text": "Fig. 2 illustrates the process of constructing the computation trees. We now highlight their properties that enable us to compress and process the concept space in a tractable manner. ", "page_idx": 3}, {"type": "image", "img_path": "fzlMza6dRZ/tmp/def2207830338a5bd0bac1e520d15f0d721a4dac37b6fdfceb4dda73ce1e2a34.jpg", "img_caption": ["Figure 2: The figure illustrates the process of constructing the computation trees of nodes $v_{1}\\in\\mathcal{G}_{1}$ and $u_{1}\\in\\mathcal{G}_{2}$ for $L=2$ . The colors of the nodes represent the node labels. Note that although $v_{1}$ and $u_{1}$ are embedded in non-isomorphic $L$ -hop neighborhoods, their computation trees are isomorphic. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Lemma 1. In an $L$ -layered GNN, the computation tree ${\\mathcal{T}}_{v}^{L}$ is sufficient to compute node embedding $\\mathbf{h}_{v}^{L}$ , $\\forall v\\in\\mathcal{V}$ . ", "page_idx": 4}, {"type": "text", "text": "Proof. In a single layer of message-passing, a node $v$ receives messages from each of its immediate neighbors. Over $L$ layers, $v$ collects messages from nodes within $L$ hops. The computation tree ${\\mathcal{T}}_{v}^{L}$ contains all such paths of length up to $L$ and hence is sufficient to compute $\\mathbf{h}_{v}^{L}$ . \u53e3 ", "page_idx": 4}, {"type": "text", "text": "Proof. The expressive power of a message-passing GNN is upper bounded by the Weisfeiler-Lehman test (1-WL) [53]. This implies that if the $L$ -hop neighborhoods of two nodes are indistinguishable by 1-WL, then their representations will be the same. The 1-WL test cannot differentiate between nodes with identical computation trees [44]. \u53e3 ", "page_idx": 4}, {"type": "text", "text": "Owing to Lemma 1 and Lemma 2, given a graph $\\mathcal{G}=(\\mathcal{V},\\mathcal{E},X)$ , we can decompose it into a multiset of computation trees $\\mathcal{T}_{\\mathcal{G}}\\,=\\,\\{\\!\\!\\{\\mathcal{T}_{v}^{L}\\:\\}\\!\\!\\mid\\,v\\,\\in\\,\\bar{\\mathcal{V}}\\}\\!\\!\\}$ , compute node embeddings $\\mathbf{h}_{v}^{L}$ , $\\forall v\\in\\mathcal{V}$ , and then aggregate them into graph embedding $\\mathbf{h}_{\\mathcal{G}}$ (Eq. 4) without incurring any loss of information. This computation structure enables us to shrink the candidate space of concepts, originally defined in Prob. 1, to the set of unique computation trees, i.e., $\\begin{array}{r}{\\mathcal{C}=\\bigcup_{\\mathcal{G}\\in\\mathcal{D}}\\mathcal{T}_{\\mathcal{G}}}\\end{array}$ . The reformulation of the concept space imparts several desirable side-effects. ", "page_idx": 4}, {"type": "text", "text": "\u2022 First, the size of the candidate space of concepts reduces from exponential to $\\sum_{\\forall\\mathcal{G}\\in\\mathcal{D}}|\\mathcal{V}|$ , which is linear to the graph size (i.e., number of nodes) and the number of graphs in  the dataset. \u2022 Second, while graph isomorphism has a computational cost exponential to the graph size, rooted-tree isomorphism can be performed in time linear to the number of edges in the tree (See App. A for details). Therefore, distilling all computation trees to only the unique ones can be done in linear time. \u2022 Third, GNNs may map non-isomorphic $L$ -hop node neighborhoods (subgraphs) to isomorphic computation trees (See Fig. 2). This further reduces the number of concept candidates and, consequently, the computational burden. ", "page_idx": 4}, {"type": "text", "text": "3.3 Mining Concepts ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To mine concepts from the candidate space, we assess their Shapley values. Shapley value [43] is a co-operative game theoretic technique where players form coalitions and receive payouts based on their contribution to the outcome. Mathematically, the Shapley value $\\psi_{i}(\\Phi)$ of player $i$ is its average marginal contribution across all possible combinations of features. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\psi_{i}(V)=\\sum_{S\\subseteq\\{1,\\cdots p\\}\\setminus\\{i\\}}{\\frac{|S|!\\;(p-|S|-1)!}{p!}}(V(S\\cup\\{i\\})-V(S))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $p$ is the total number of players, $S$ represents a subset of players excluding player $i$ and the value function $V(.)$ quantifies the outcome. ", "page_idx": 4}, {"type": "text", "text": "With the hypotheses that some combination of computation trees is responsible for GNN\u2019s predictions, we aim to leverage Shapley values in identifying this combination. Consequently, the players correspond to computation trees, and the value function corresponds to the performance of the GNN when restricted to the chosen subset of computation trees. We set the value function as the negated cross-entropy error of GNN $\\Phi$ \u2019s prediction (one may use other accuracy measures as well). Thus, Eq. 5 is re-expressed as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\psi_{i}(\\Phi)=\\sum_{\\forall S,S\\subseteq\\mathcal{C}\\backslash\\{C_{i}\\}}\\frac{|S|!\\left(|\\mathcal{C}|-|S|-1\\right)!}{|\\mathcal{C}|!}\\left(V(S\\cup\\{C_{i}\\})-V(S)\\right)}\\\\ &{\\displaystyle\\mathrm{e},\\,V(S)=\\sum_{\\forall\\mathcal{G}\\in\\mathcal{D}}\\sum_{j=1}^{c}\\mathcal{V}_{j}^{\\mathcal{G}}\\log\\left(\\Phi\\left(\\mathcal{G}^{S}\\right)_{j}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In Eq. 6, $\\mathcal{C}=\\{C_{1},\\cdots,C_{m}\\}$ is the set of unique computation trees, and $C_{i}$ denotes the $i$ -th one. Eq. 7 represents the negated cross-entropy error. In Eq. 7, we slightly overload the notation introduced in Def. 3 as follows: we assume that the class label information is maintained as a $c$ -dimensional one-hot vector. Thus, $\\boldsymbol{\\mathcal{Y}}_{j}^{\\mathcal{G}}$ is 1 if the true class label of $\\mathcal{G}$ is $j$ -th label. The GNN $\\Phi$ returns a distribution over the class labels, where $\\Phi({\\mathcal{G}}^{S})_{j}$ is the probability of the $j$ -th class, when $\\mathcal{G}$ is embedded by only considering the computation trees within $S$ . Mathematically, this means modifying Eq. 4 as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{h}_{\\mathcal{G}}^{S}=\\mathrm{PooL}\\left(\\left\\{\\mathbf{h}_{v}^{L}\\mid\\forall v\\in\\mathcal{V}\\mathrm{~and~}\\mathcal{T}_{v}^{L}\\in S\\right\\}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Finally, we select the top- $k$ computation trees with the highest Shapley value as concepts. We discuss our procedure for selecting $k$ in the next section (\u00a7 3.4). ", "page_idx": 5}, {"type": "text", "text": "3.3.1 Computational Tractability ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "While we are able to map the problem of concept mining to Shapley Value computations, several computational challenges arise. Firstly, Shapley value computation necessitates sampling all possible subsets of players (computation trees), leading to exponential computation complexity. Secondly, we need to compute $\\mathbf{h}_{\\mathcal{G}}^{S}$ , i.e., the graph embedding corresponding to each subset of trees $S$ . The na\u00efve approach involves: (1) enumerating computation trees rooted in each node, which consumes $\\mathcal{O}(|\\mathcal{E}|)$ time, (2) performing $\\mathcal{O}(|S||\\mathcal{V}|)$ tree isomorphisms to detect computation trees contained within $S$ and then (3) computing Eq. 8. Given that Shapley computation requires drawing multiple samples of $S$ (Eq. 6), the above pipeline for each sample $S$ is prohibitively slow. ", "page_idx": 5}, {"type": "text", "text": "Fortunately, there are works to circumvent the first challenge via sampling a small set of coalitions with probabilistic approximations [59, 33]. We use the sampling strategy outlined in [28] (see App. B for details). ", "page_idx": 5}, {"type": "text", "text": "The second challenge is unique since computing Shapley on computation trees of GNNs has not been studied. GraphShap [36], the only other work to study Shapley value in the context of explaining graph classification, looks at the specific class of identity-aware graphs, where nodes with unique identities recur across multiple graph views. More importantly, GraphShap is not customized for GNNs and hence the context of computation tree does not arise. ", "page_idx": 5}, {"type": "text", "text": "To address the bottleneck of computing Eq. 8, we project each graph into a concept vector. ", "page_idx": 5}, {"type": "text", "text": "Definition 6 (Concept vector). Let $\\mathcal{C}=\\{C_{1},\\cdots,C_{m}\\}$ be the set of unique computation trees. The concept vector $C_{\\mathcal{G}}\\,\\in\\,\\mathbb{Z}_{\\ge0}^{m}$ of graph $\\mathcal{G}\\,=\\,(\\nu,\\mathcal{E},X)$ is an $m$ -dimensional vector where the i-th dimension represents the number of computation trees in $\\mathcal{G}$ that are isomorphic to the $i$ -th concept candidate $C_{i}\\in\\mathcal{C}$ . Mathematically, $C_{\\mathcal{G}}[i]=|\\{v\\in\\mathcal{V}\\,|\\,T_{v}^{L}\\cong C_{i}\\}|$ |. ", "page_idx": 5}, {"type": "text", "text": "The \u201cconcept encoding\u201d box in Fig. 1 illustrates this operation. From Lemma 1, the embedding of the root node of $C_{i}$ depends solely on $C_{i}$ . Hence, we pre-compute and map each $C_{i}\\in\\mathcal{C}$ into an embedding $\\mathbf{h}_{i}$ . By combining Lemma 2 with Eq.4, we deduce that for both MEANPOOL and SUMPOOL, the two most common aggregators can be computed in $O(|S|)$ time. Specifically, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{For~SUMPOOL,}\\;\\;\\mathbf{h}_{\\mathcal{G}}^{S}=\\displaystyle\\sum_{C_{i}\\in S}C_{\\mathcal{G}}[i]\\times\\mathbf{h}_{i}}\\\\ &{\\mathrm{For~MEANPOOL,}\\;\\;\\mathbf{h}_{\\mathcal{G}}^{S}=\\displaystyle\\frac{1}{|\\mathcal{V}|}\\sum_{C_{i}\\in S}C_{\\mathcal{G}}[i]\\times\\mathbf{h}_{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Owing to Lemma 1 and Lemma 2, computing the graph embedding simply involves $|S|$ look-ups of vectors instead of fresh rounds of message-passing. ", "page_idx": 5}, {"type": "text", "text": "3.4 Generation of Logical Rules through Symbolic Regression ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Let us denote the top- $k$ computation trees with the $k$ highest Shapley values as $\\mathcal{C}^{*}$ . To discover logical rules over $\\mathcal{C}^{*}$ , we perform symbolic regression. Symbolic regression is a branch of regression analysis that aims to discover symbolic expressions from experimental data [31]. Formally, given a set of $n$ input-output pairs $\\{(\\mathbf{x}_{i},y_{i})\\}_{i=1}^{n}$ , where $\\mathbf{x}_{i}\\in\\mathbb{R}^{d}$ is an input vector, and $y_{i}\\in\\mathbb{R}$ is the output value (or label), and a set of operators, such as addition, subtraction, logarithm, trigonometric functions, boolean logic, etc., the goal is to find a symbolic equation $e$ and corresponding function $f_{e}$ such that $\\forall i$ , $y_{i}\\approx f_{e}(\\mathbf{x}_{i})$ . ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "To perform symbolic regression over computation trees, we first project the concept vector of each graph $\\mathcal{G}=(\\dot{\\mathcal{V}},\\mathcal{E},\\pmb{X})$ on the top- $k$ trees $\\mathcal{C}^{*}$ . This distilled concept vector is denoted as $C_{\\mathcal{G}}^{*}\\in\\mathbb{Z}_{\\geq0}^{k}$ , where $C_{\\mathcal{G}}^{*}[i]=|\\{v\\in\\mathcal{V}\\:|\\:T_{v}^{L}\\cong C_{i}\\}|$ , with $C_{i}$ representing the $i^{\\mathrm{th}}$ ranked computation tree in $\\mathcal{C}^{*}$ based on Shapley value. While symbolic regression can accommodate a wide array of operators, mathematical operations such as addition or multiplication are not meaningful when applied to computation trees. Hence, to facilitate human interpretability, we restrict to the boolean operators: conjunction $(\\land)$ , disjunction $(\\lor)$ , negation $(\\neg)$ , and XOR $\\left(\\oplus\\right)$ . We aim to learn a symbolic function $f_{c}$ for each class label $c$ predicted by the GNN. Let ${\\mathcal{D}}_{c}=\\{{\\mathcal{G}}\\in{\\mathcal{D}}\\mid\\Phi({\\mathcal{G}})^{*}=c\\}$ be the subset of graphs where the predicted label is $c$ . The symbolic function $f_{c}$ is identified by minimizing a multi-objective loss function that balances prediction error and model complexity. Formally, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}\\left(f_{c}\\right)=E r r o r(f_{c})+(\\alpha)C o m p l e x i t y(f_{c})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Complexity $(f_{c})$ corresponds to the number of boolean operators and variables in $f_{c}$ . $\\alpha$ is a weighting hyper-parameter. Error corresponds to the number of disagreements with the GNN. Formally, ", "page_idx": 6}, {"type": "equation", "text": "$$\nE r r o r\\left(f_{c}\\right)=\\sum_{\\forall\\mathcal{G}\\in\\mathcal{D}_{c}}1-f_{c}(\\mathcal{G})+\\sum_{\\forall\\mathcal{G}^{\\prime}\\in\\mathcal{D}\\backslash\\mathcal{D}_{c}}f_{c}(\\mathcal{G})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Since $f_{c}$ is a boolean function, it returns either TRUE (equivalently 1) or FALSE (0). ", "page_idx": 6}, {"type": "text", "text": "Symbolic regression poses a combinatorial optimization challenge, as the number of potential functions increases exponentially with the number of symbols, which is $k$ in our case. Fortunately, the area is rich with several studies [31]. We use [8], which leverages a multi-population evolutionary algorithm to efficiently optimize and identify symbolic expressions (See App. C for details). ", "page_idx": 6}, {"type": "text", "text": "As depicted in Fig. 1, following the generation of boolean functions, GRAPHTRAIL concludes. ", "page_idx": 6}, {"type": "text", "text": "Identifying $k$ \u2013 the number of concepts: The higher the value of $k$ , the more expressive symbolic regression is to fit to the prediction data by GNN. On the other hand, the computation cost grows monotonically with $k$ . To optimize this balance, we start with a small value of $k$ , and incrementally increase it until the accuracy plateaus, similar to using the patience parameter to determine the number of epochs in machine learning model training. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we evaluate GRAPHTRAIL and benchmark its performance in translating predictions of various GNN architectures across diverse datasets and pooling layers. More experiments can be found in the appendix. The codebase of GRAPHTRAIL is shared at https://github.com/idea-iitd/GraphTrail. ", "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Baselines: As discussed in $\\S\\ 1$ , GLGEXPLAINER [2] is the only existing algorithm that ftis a logical formula to GNN predictions, making it our primary baseline. Recall, GLGEXPLAINER generates a formula over vectors representing embeddings of subgraph clusters rather than individual subgraphs, rendering it non-interpretable. The formula is applied to an input graph by first processing it through an instance-level explainer, embedding the explanation subgraphs into feature space, and assigning them to the closest cluster. These cluster vectors are then considered present in the input graph. To make the formula interpretable, each cluster vector is replaced by the subgraph closest to the cluster representation in the embedding space. A subgraph (concept) $\\mathcal{G}_{S}$ in the formula is considered present in an input graph $\\mathcal{G}$ if $g_{S}\\subseteq{\\mathcal{G}}$ . We term this version GLGEXPLAINER-iso. ", "page_idx": 6}, {"type": "text", "text": "We use the authors\u2019 implementation of GLGEXPLAINER with the recommended parameters and employ PGEXPLAINER [29] as the instance-level explainer, as suggested. The implementation of PGEXPLAINER is available in Pytorch Geometric. ", "page_idx": 6}, {"type": "text", "text": "GRAPHTRAIL-S: Shapley value computation brings the computational bottleneck in the pipeline, which involves calculating the global Shapley value for each computation tree on each graph. This process can become impractical for datasets with a large number of graphs. To address this issue, we propose GRAPHTRAIL-S, an efficient version of GRAPHTRAIL. Note that we are interested in the ranking according to the Shapley values and not the values themselves. If a subset of the dataset can emulate the entire dataset, the generated ranking is expected to remain the same. Thus, GRAPHTRAIL-S creates a stratified split of the training dataset that maintains the class ratios and computes the global Shapley values using this subset. The rest of the pipeline remains unchanged, allowing for a more efficient yet effective method. This is also shown in the results from Table 1 where GRAPHTRAIL-s is slightly inferior to GRAPHTRAIL. ", "page_idx": 6}, {"type": "table", "img_path": "fzlMza6dRZ/tmp/d8ceb35f4c4fe52cca61dda27bfe9c1c5e05a65eb9714388ed42745fad67e710.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "fzlMza6dRZ/tmp/14094e48bb331a0eb0edc983c106aa6c7c9916d026a4897342f8167efbc38aae.jpg", "table_caption": ["Table 1: Average fidelity of the formulae across three seeds. The best and second best results are shown in bold and underlined, respectively. G-TRAIL and GLG represents GRAPHTRAIL and GLGEXPLAINER respectively. "], "table_footnote": ["Table 2: Comparison of Fidelity across GNN architectures and POOL layers. The best result in each dataset and category is highlighted in bold. "], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Datasets: We use four benchmark datasets listed in Table C (App. D). While NCI1 [49], MUTAG [17], and Mutagenicity [39, 20] are collections of molecules, BAMultiShapes [55] is a synthetic dataset specifically curated to benchmark global GNN explainers and associated with ground truth logical explanations. Further details are presented in App. D. ", "page_idx": 7}, {"type": "text", "text": "Evaluation framework: While we benchmark against various GNN architectures and POOL layers (Eq. 4), the default architecture is set to GAT for MUTAG and Mutagenicity and GIN for the other two. We use SUMPOOL as the default across datasets. The accuracies of the GNN architectures across all POOL choices are presented in Table D in the Appendix. Details on the hardware platform, train-val-test splits and other parameters are provided in App. E. All experiments have been repeated with three seeds and we report the means and standard deviations. ", "page_idx": 7}, {"type": "text", "text": "Metrics: We evaluate explainer faithfulness using Fidelity; the ratio of graphs where the logical formula matches the GNN class label. Additional metrics can be found in the appendix. ", "page_idx": 7}, {"type": "text", "text": "4.2 Fidelity ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Is GRAPHTRAIL faithful in translating GNN through logical rules? Table 1 provides the answer with several key observations. ", "page_idx": 7}, {"type": "text", "text": "First, both GRAPHTRAIL and GRAPHTRAIL-s significantly outperform GLGEXPLAINER. GRAPHTRAIL computes Shapley values by evaluating all subsets of concepts, while GRAPHTRAIL-s employs a faster, sampling-based method. The key reason for the superior performance of GRAPHTRAIL and GRAPHTRAIL-s over GLGEXPLAINER lies in aligning the concept definitions with the fundamental computational units of a GNN, namely computational trees. GRAPHTRAIL then mines these concepts with a global perspective by analyzing their Shapley values. In contrast, GLGEXPLAINER depends on instance-level explainers, which lack the ability to detect global patterns. ", "page_idx": 7}, {"type": "text", "text": "Second, GLGEXPLAINER-iso consistently underperforms compared to GLGEXPLAINER. This is because the clusters of instance-level subgraph explanations in GLGEXPLAINER-iso are not pure (See App. I). Although they are grouped based on embedding distance, their structures vary significantly. Representing a cluster by an exemplar subgraph fails to capture the diversity effectively. ", "page_idx": 7}, {"type": "text", "text": "Finally, GLGEXPLAINER results do not match the original results reported in [2]. In App. H, we discuss the causes, including the identification of test data leakage in the Mutagenicity data set. ", "page_idx": 7}, {"type": "text", "text": "4.3 Robustness ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Varying architectures and pooling mechanisms. In Table 2, we present the robustness of GRAPHTRAIL and GLGEXPLAINER across various GNN architectures and POOL layers. GRAPHTRAIL achieves higher fidelity than GLGEXPLAINER across all scenarios. This result further substantiates the superior ability of GRAPHTRAIL in faithfully translating GNN outcomes. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Varying $k$ . Fig. 3 explores the relationship between the number of mined concepts $(k)$ and the fidelity achieved by GRAPHTRAIL across various datasets. Recall that higher values of $k$ allow for greater expressivity in symbolic regression. Consistent with this, the results exhibit an increasing trend, with fidelity approaching saturation at $k=50$ for most datasets. ", "page_idx": 8}, {"type": "text", "text": "4.4 Data Efficiency ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In Fig. 4, we assess the explainers\u2019 effectiveness in low-data regimes by varying the train data volume on the $x$ -axis and measuring Fidelity on the $y$ -axis. We expect Fidelity to decrease with lesser data. In GRAPHTRAIL, the decrease in Fidelity with lesser data is minor, indicating robust efficacy even with limited data and that it can be used in low-resource settings. Notably, the higher average Fidelity and lower standard deviation in GRAPHTRAIL across sizes indicate enhanced stability. ", "page_idx": 8}, {"type": "image", "img_path": "fzlMza6dRZ/tmp/a83cfbab1709b321f594e0fc3330355fc6a62460e57a6fd7530be64c96676974.jpg", "img_caption": ["Figure 3: Impact of $k$ (number of concepts) on Fidelity. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "fzlMza6dRZ/tmp/95b7a8edf21f71819dde1e1ded4d0f087383c17f5c13ec41679702f1bab1382a.jpg", "img_caption": ["Figure 4: Results on the test set fidelity averaged over three seeds while varying the training data size. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.5 Visual Analysis of Rules ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Fig. 5 presents the rules inferred by GRAPHTRAIL and GLGEXPLAINER for the datasets MUTAG, Mutagenicity, and BAMultiShapes. ", "page_idx": 8}, {"type": "text", "text": "MUTAG: According to [9], compounds with rings and electron-attracting group elements conjugated with nitro groups enhance mutagenicity. GRAPHTRAIL identifies both the ring structures as well as the nitro group as identifiers of mutagenicity in this dataset. In contrast, GLGEXPLAINER fails to identify the ring structure. ", "page_idx": 8}, {"type": "text", "text": "Mutagenicity: Prior work [55, 45, 29, 2] often limits the mutagenic behavior in this dataset to $N O_{2}$ and $N H_{2}$ , i.e., the nitro and amine groups. However, in [20], eight general toxicophores are capable of identifying $75\\%$ of all mutagens in the dataset (see Fig. F [20]). As seen in Fig. 5, without any supervision, GRAPHTRAIL identifies and incorporates six of them in its formula, viz. aromatic nitro, aromatic amine, nitroso, unsubstituted heteroatom-bonded heteroatom, azo-type, and polycyclic aromatic systems. The azo-type and the unsubstituted heteroatom-bonded heteroatom groups have not been identified explicitly but are potential extensions of two of the identified structures. ", "page_idx": 8}, {"type": "text", "text": "BAMultiShapes: BAMultiShapes has known ground truth logical rules for its class labels. While GRAPHTRAIL does not perfectly recover these rules, GRAPHTRAIL\u2019s inferred concepts and their logical combination exhibit greater similarity to the ground truth compared to GLGEXPLAINER\u2019s results. It should be noted that explainers try to find what the model has learned rather than the ground truth. One striking observation is that GLGEXPLAINER\u2019s extracted concepts bear no similarity to the ground truth and two of the prototypes have no graphs in their clusters and are therefore empty. In contrast, GRAPHTRAIL identifies concepts that are isomorphic to the grid and house motifs present in the ground truth. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusions, Limitations and Future Directions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we have designed an end-to-end, post-hoc, global graph neural network (GNN) explainer called GRAPHTRAIL. We have formulated the problem of translating a message-passing GNN model for graph classification into a human-interpretable logic formula over subgraph concepts without assuming the concepts are pre-generated through a separate algorithm. GRAPHTRAIL leverages several novel insights such as the decomposition of graphs into computation trees by message-passing GNNs, which reduces the search space for concepts from exponential number of all possible subgraphs to a linear space of computation trees. GRAPHTRAIL evaluates the impact of these computation trees using Shapley values and then creates a boolean formula via symbolic regression only with the important ones. Extensive empirical analyses across diverse datasets and different GNN architectures demonstrate GRAPHTRAIL\u2019s effectiveness in capturing the underlying logical relationships within complex GNN models for a wide range of scenarios in a human-interpretable form. ", "page_idx": 8}, {"type": "image", "img_path": "fzlMza6dRZ/tmp/96032430cfacd5c6ab5be2a2fbed6f782d13c62b7d5f6b60e6a5d1789e8a8933.jpg", "img_caption": ["Figure 5: Visual inspection of the formulae generated by GRAPHTRAIL and GLGEXPLAINER. The encircled numbers in Mutagenicity are toxicophore IDs from Fig. F (in Appendix). The structures with two IDs in Mutagenicity can be extended to both toxicophores. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Limitations: While both GRAPHTRAIL and GLGEXPLAINER express rules as logical combinations of concept\u2019s presence or absence, they currently lack the ability to capture the multiplicity of a concept\u2019s occurrence within a graph. Finally, we believe attempting to interpret GNNs with subgraph level concepts creates a dilemma between interpretability and faithfulness to the computational mechanism of GNNs. To elaborate, in GRAPHTRAIL, since symbolic regression generates the formula over computation trees, we map it back to the graph space, by replacing each computation tree with the most frequent $L$ -hop ego graph that generates this tree. Note, GNNs operate at the level of computation trees. Given any formula at a subgraph level, a new formula can be generated with identical fidelity, by simply replacing each subgraph concept with one that produces the same set of computation trees (recall Fig. 2). However, when concepts are lowered to computation tree level, human interpetability is compromised. It is worth exploring how the best balance between human interpretability [13] and staying faithful to GNN computation structure can be obtained. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Burouj Armgaan acknowledges the support of the Prime Minister\u2019s Research Fellowship (PMRF) for funding this research. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Carlo Abrate and Francesco Bonchi. Counterfactual graphs for explainable classification of brain networks. In KDD, page 2495\u20132504, 2021. (Cited on p. $_1\\leftrightarrow$ )   \n[2] Steve Azzolin, Antonio Longa, Pietro Barbiero, Pietro Lio, and Andrea Passerini. Global explainability of GNNs via logic combination of learned concepts. In The Eleventh International Conference on Learning Representations, 2023. (Cited on pp. 2, 3, 7, 8, 9, 15, and ${\\bf19}\\leftrightarrow$ )   \n[3] Mohit Bajaj, Lingyang Chu, Zi Yu Xue, Jian Pei, Lanjun Wang, Peter Cho-Ho Lam, and Yong Zhang. Robust counterfactual explanations on graph neural networks. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. (Cited on pp. 1 and $15\\leftrightarrow$ )   \n[4] Federico Baldassarre and Hossein Azizpour. Explainability techniques for graph convolutional networks. arXiv preprint arXiv:1905.13686, 2019. (Cited on p. $15\\leftrightarrow$ )   \n[5] Vaibhav Bihani, Sahil Manchanda, Srikanth Sastry, Sayan Ranu, and NM Krishnan. Stridernet: A graph reinforcement learning approach to optimize atomic structures on rough energy landscapes. In ICML, 2023. (Cited on p. $_1\\leftrightarrow$ ) [6] Chirag Chhablani, Sarthak Jain, Akshay Channesh, Ian A Kash, and Sourav Medya. Gametheoretic counterfactual explanation for graph neural networks. In Proceedings of the ACM on Web Conference 2024, pages 503\u2013514, 2024. (Cited on p. $_1\\leftrightarrow$ )   \n[7] Yun Chi, Yirong Yang, and Richard Muntz. Canonical forms for labelled trees and their applications in frequent subtree mining. Knowl. Inf. Syst., 8:203\u2013234, 08 2005. (Cited on p. 15 $\\leftrightarrow$ ) [8] Miles Cranmer. Interpretable machine learning for science with pysr and symbolicregression. jl. arXiv preprint arXiv:2305.01582, 2023. (Cited on pp. 7 and $17\\leftrightarrow$ ) [9] Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Corwin Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. Journal of medicinal chemistry, 34(2):786\u2013797, 1991. (Cited on pp. 9 and $15\\leftrightarrow$ )   \n[10] Dobrik Georgiev, Pietro Barbiero, Dmitry Kazhdan, Petar Velic\u02c7kovic\u00b4, and Pietro Li\u00f2. Algorithmic concept-based explainable reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 6685\u20136693, 2022. (Cited on p. $_2\\leftrightarrow$ )   \n[11] Mridul Gupta, Hariprasad Kodamana, and Sayan Ranu. FRIGATE: Frugal spatio-temporal forecasting on road networks. In 29th SIGKDD Conference on Knowledge Discovery and Data Mining, 2023. (Cited on p. $_1\\leftrightarrow$ )   \n[12] Shubham Gupta, Sahil Manchanda, Srikanta Bedathur, and Sayan Ranu. Tigger: Scalable generative modelling for temporal interaction graphs. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 6819\u20136828, 2022. (Cited on p. $_1\\leftrightarrow$ )   \n[13] Pantea Habibi, Peyman Baghershahi, Sourav Medya, and Debaleena Chattopadhyay. Design requirements for human-centered graph neural network explanations. arXiv preprint arXiv:2405.06917, 2024. (Cited on p. $10\\leftrightarrow$ )   \n[14] William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS\u201917, page 1025\u20131035, Red Hook, NY, USA, 2017. Curran Associates Inc. (Cited on pp. 1 and $_3\\leftrightarrow$ )   \n[15] Qiang Huang, Makoto Yamada, Yuan Tian, Dinesh Singh, and Yi Chang. Graphlime: Local interpretable model explanations for graph neural networks. IEEE Transactions on Knowledge and Data Engineering, 2022. (Cited on pp. 1 and $15\\leftrightarrow$ )   \n[16] Zexi Huang, Mert Kosan, Sourav Medya, Sayan Ranu, and Ambuj Singh. Global counterfactual explainer for graph neural networks. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining, pages 141\u2013149, 2023. (Cited on pp. 2 and $15\\leftrightarrow$ )   \n[17] Sergei Ivanov, Sergei Sviridov, and Evgeny Burnaev. Understanding isomorphism bias in graph data sets. Geometric Learning and Graph Representations ICLR Workshop, 2019. (Cited on pp. 8 and $17\\leftrightarrow$ )   \n[18] Jayant Jain, Vrittika Bagadia, Sahil Manchanda, and Sayan Ranu. Neuromlr: Robust & reliable route recommendation on road networks. Advances in Neural Information Processing Systems, 34:22070\u201322082, 2021. (Cited on p. $_1\\leftrightarrow$ )   \n[19] Jaykumar Kakkad, Jaspal Jannu, Kartik Sharma, Charu Aggarwal, and Sourav Medya. A survey on explainability of graph neural networks. arXiv preprint arXiv:2306.01958, 2023. (Cited on p. $_1\\leftrightarrow$ )   \n[20] Jeroen Kazius, Ross McGuire, and Roberta Bursi. Derivation and validation of toxicophores for mutagenicity prediction. Journal of medicinal chemistry, 48(1):312\u2013320, 2005. (Cited on pp. 8, 9, and $17\\leftrightarrow$ )   \n[21] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In International conference on machine learning, pages 2668\u20132677. PMLR, 2018. (Cited on p. $3\\leftrightarrow$ )   \n[22] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. (Cited on pp. 1 and $_3\\leftrightarrow$ )   \n[23] Mert Kosan, Samidha Verma, Burouj Armgaan, Khushbu Pahwa, Ambuj Singh, Sourav Medya, and Sayan Ranu. GNNX-BENCH: Unravelling the utility of perturbation-based GNN explainers through in-depth benchmarking. In The Twelfth International Conference on Learning Representations, 2024. (Cited on p. $\\mathbf{1}\\longleftrightarrow$ )   \n[24] John R. Koza. Genetic programming: on the programming of computers by means of natural selection. MIT Press, Cambridge, MA, USA, 1992. (Cited on p. $3\\leftrightarrow$ )   \n[25] Wanyu Lin, Hao Lan, and Baochun Li. Generative causal explanations for graph neural networks. In International Conference on Machine Learning, pages 6666\u20136679. PMLR, 2021. (Cited on pp. 1 and $15\\leftrightarrow$ )   \n[26] Shengyao Lu, Keith G Mills, Jiao He, Bang Liu, and Di Niu. GOAt: Explaining graph neural networks via graph output attribution. In The Twelfth International Conference on Learning Representations, 2024. (Cited on pp. 1 and $15\\leftrightarrow$ )   \n[27] Ana Lucic, Maartje A Ter Hoeve, Gabriele Tolomei, Maarten De Rijke, and Fabrizio Silvestri. Cf-gnnexplainer: Counterfactual explanations for graph neural networks. In AISTATS, pages 4499\u20134511, 2022. (Cited on pp. 1 and $15\\leftrightarrow$ )   \n[28] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. Advances in neural information processing systems, 30, 2017. (Cited on pp. 6 and $16\\leftrightarrow$ )   \n[29] Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang Zhang. Parameterized explainer for graph neural network. Advances in neural information processing systems, 33:19620\u201319631, 2020. (Cited on pp. 1, 2, 7, 9, 15, and ${\\bf19}\\leftrightarrow$ )   \n[30] Jing Ma, Ruocheng Guo, Saumitra Mishra, Aidong Zhang, and Jundong Li. Clear: Generative counterfactual explanations on graphs. arXiv preprint arXiv:2210.08443, 2022. (Cited on p. 15 $\\leftrightarrow$ [31] Nour Makke and Sanjay Chawla. Interpretable scientific discovery with symbolic regression: a review. Artificial Intelligence Review, 57(1):2, 2024. (Cited on pp. 3, 6, 7, and $17\\leftrightarrow$ ) [32] Sahil Manchanda, Akash Mittal, Anuj Dhawan, Sourav Medya, Sayan Ranu, and Ambuj Singh. Gcomb: Learning budget-constrained combinatorial algorithms over billion-sized graphs. Advances in Neural Information Processing Systems, 33:20000\u201320011, 2020. (Cited on p. 1 $\\leftrightarrow$ [33] Sourav Medya, Tiyani Ma, Arlei Silva, and Ambuj Singh. A game theoretic approach for core resilience. In International Joint Conferences on Artificial Intelligence Organization, 2020. (Cited on p. $6\\leftrightarrow$ ) [34] Peter M\u00fcller, Lukas Faber, Karolis Martinkus, and Roger Wattenhofer. Graphchef: Learning the recipe of your dataset. In ICML 3rd Workshop on Interpretable Machine Learning in Healthcare (IMLH), 2023. (Cited on p. $_2\\leftrightarrow$ ) [35] Sunil Nishad, Shubhangi Agarwal, Arnab Bhattacharya, and Sayan Ranu. Graphreach: Positionaware graph neural network using reachability estimations. IJCAI, 2021. (Cited on p. 1 $\\leftrightarrow$ ) [36] Alan Perotti, Paolo Bajardi, Francesco Bonchi, and Andr\u00e9 Panisson. Explaining identity-aware graph classifiers through the language of motifs. In 2023 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138. IEEE, 2023. (Cited on p. $\\6\\leftrightarrow$ ) [37] Phillip E Pope, Soheil Kolouri, Mohammad Rostami, Charles E Martin, and Heiko Hoffmann. Explainability methods for graph convolutional neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10772\u201310781, 2019. (Cited on p. $15\\leftrightarrow$ ) [38] Rishabh Ranjan, Siddharth Grover, Sourav Medya, Venkatesan Chakaravarthy, Yogish Sabharwal, and Sayan Ranu. Greed: A neural framework for learning graph distance functions. In Advances in Neural Information Processing Systems, 2022. (Cited on p. $_1\\leftrightarrow$ ) [39] Kaspar Riesen and Horst Bunke. Iam graph database repository for graph based pattern recognition and machine learning. In Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR), pages 287\u2013297. Springer, 2008. (Cited on pp. 8 and $17\\leftrightarrow$ ) [40] Thomas Schnake, Oliver Eberle, Jonas Lederer, Shinichi Nakajima, Kristof T Sch\u00fctt, KlausRobert M\u00fcller, and Gr\u00e9goire Montavon. Higher-order explanations of graph neural networks via relevant walks. IEEE transactions on pattern analysis and machine intelligence, 44(11):7581\u2013   \n7596, 2021. (Cited on p. $15\\leftrightarrow$ ) [41] Rishi Shah, Krishnanshu Jain, Sahil Manchanda, Sourav Medya, and Sayan Ranu. Neurocut: A neural approach for robust graph partitioning. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD \u201924, page 2584\u20132595, New York, NY, USA, 2024. Association for Computing Machinery. (Cited on p. $_1\\leftrightarrow$ ) [42] Caihua Shan, Yifei Shen, Yao Zhang, Xiang Li, and Dongsheng Li. Reinforcement learning enhanced explainer for graph neural networks. In NeurIPS 2021, December 2021. (Cited on p. $_1\\leftrightarrow$ ) [43] L. Shapley. 7. A Value for $n$ -Person Games. Contributions to the Theory of Games II (1953)   \n307-317., pages 69\u201379. Princeton University Press, Princeton, 1997. (Cited on pp. 3, 5, and 16 $\\leftrightarrow$ [44] Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M. Borgwardt. Weisfeiler-lehman graph kernels. J. Mach. Learn. Res., 12(null):2539\u20132561, nov   \n2011. (Cited on p. $5\\leftrightarrow$ ) [45] Juntao Tan, Shijie Geng, Zuohui Fu, Yingqiang Ge, Shuyuan Xu, Yunqi Li, and Yongfeng Zhang. Learning and evaluating graph neural network explanations based on counterfactual and factual reasoning. In Proceedings of the ACM Web Conference 2022, WWW \u201922, page   \n1018\u20131027, 2022. (Cited on pp. 1, 9, and $15\\leftrightarrow$ )   \n[46] Petar Velic\u02c7kovic\u00b4, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. (Cited on pp. 1, 3, and $4\\leftrightarrow$ )   \n[47] Samidha Verma, Burouj Armgaan, Sourav Medya, and Sayan Ranu. InduCE: Inductive counterfactual explanations for graph neural networks. Transactions on Machine Learning Research, 2024. (Cited on pp. 1 and $15\\leftrightarrow$ )   \n[48] Minh $\\mathrm{Vu}$ and My T Thai. Pgm-explainer: Probabilistic graphical model explanations for graph neural networks. Advances in neural information processing systems, 33:12225\u201312235, 2020. (Cited on p. $15\\leftrightarrow$ )   \n[49] Nikil Wale and George Karypis. Comparison of descriptor spaces for chemical compound retrieval and classification. In ICDM, 2006. (Cited on pp. 8 and $17\\leftrightarrow$ )   \n[50] Xiaoqi Wang and Han Wei Shen. GNNInterpreter: A probabilistic generative model-level explanation for graph neural networks. In The Eleventh International Conference on Learning Representations, 2023. (Cited on p. $_2\\leftrightarrow$ )   \n[51] Geemi P Wellawatte, Aditi Seshadri, and Andrew D White. Model agnostic generation of counterfactual explanations for molecules. Chemical science, 13(13):3697\u20133705, 2022. (Cited on p. $_1\\leftrightarrow$ )   \n[52] Yaochen Xie, Sumeet Katariya, Xianfeng Tang, Edward Huang, Nikhil Rao, Karthik Subbian, and Shuiwang Ji. Task-agnostic graph explanations. NeurIPS, 2022. (Cited on p. $15\\leftrightarrow$ )   \n[53] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. (Cited on pp. 1, 3, and $5\\leftrightarrow$ )   \n[54] Han Xuanyuan, Pietro Barbiero, Dobrik Georgiev, Lucie Charlotte Magister, and Pietro Li\u00f3. Global concept-based interpretability for graph neural networks via neuron analysis. In AAAI, 2023. (Cited on pp. 2 and $15\\leftrightarrow$ )   \n[55] Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnnexplainer: Generating explanations for graph neural networks. Advances in neural information processing systems, 32, 2019. (Cited on pp. 1, 8, 9, 15, 17, and ${\\bf19}\\leftrightarrow$ )   \n[56] Hao Yuan, Jiliang Tang, Xia Hu, and Shuiwang Ji. Xgnn: Towards model-level explanations of graph neural networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 430\u2013438, 2020. (Cited on pp. 2 and $15\\leftrightarrow$ )   \n[57] Hao Yuan, Haiyang Yu, Shurui Gui, and Shuiwang Ji. Explainability in graph neural networks: A taxonomic survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022. (Cited on p. $_1\\leftrightarrow$ )   \n[58] Hao Yuan, Haiyang Yu, Jie Wang, Kang Li, and Shuiwang Ji. On explainability of graph neural networks via subgraph explorations. In ICML, pages 12241\u201312252. PMLR, 2021. (Cited on pp. 1 and $15\\leftrightarrow$ )   \n[59] Jiayao Zhang, Qiheng Sun, Jinfei Liu, Li Xiong, Jian Pei, and Kui Ren. Efficient sampling approaches to shapley value approximation. Proceedings of the ACM on Management of Data, 1(1):1\u201324, 2023. (Cited on p. $6\\leftrightarrow$ )   \n[60] Yue Zhang, David Defazio, and Arti Ramesh. Relex: A model-agnostic relational model explainer. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, pages 1042\u20131049, 2021. (Cited on p. $15\\leftrightarrow$ )   \n[61] Zaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, and Cheekong Lee. Protgnn: Towards self-explaining graph neural networks. AAAI, 2021. (Cited on p. $_2\\leftrightarrow$ ) ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "table", "img_path": "fzlMza6dRZ/tmp/4e070caf881008b1927f34a51aac89eaf5be73a3ba399e49a9577ec9e2621690.jpg", "table_caption": ["Appendix "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure F: The eight toxicophores in the Mutagenicity dataset that are capable of identifying $75\\%$ of all mutagens in the dataset [9]. \u201caro\u201d indicates an aromatic atom. \"arom. rings\" indicates an atom that is part of multiple aromatic rings. The toxicophores identified by GRAPHTRAIL are colored green. Toxicophores 5 and 6 are underlined as they are not explicitly identified but are potential extensions to two of the identified structures in Fig. 5. ", "page_idx": 14}, {"type": "image", "img_path": "fzlMza6dRZ/tmp/289c127f94bbddde5ddb3aba1354bf5e72772b19d0a76d95dfeddeddaa3a1d24.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure G: Taxonomy of research conducted on explaining GNN predictions. Gradient: SA [4] , Guided-BP [4] , Grad-CAM [37]; Decomposition: Excitation-BP [37], GNN-LRP [40], CAM [37], GOAT [26]; Perturbation: GNNExplainer [55], PGExplainer [29], SubgraphX [58], GEM [25], TAGExplainer [52], $\\mathrm{CF^{2}}$ [45], RCExplainer [3],CF-GNNexplainer [27], CLEAR [30], InduCE [47]; Surrogate: GraphLime [15], Relex [60], PGM-Explainer [48]; Global: XGNN [56], GLG-Explainer [2], Xuanyuan et al. [54], GCFExplainer [16]. ", "page_idx": 14}, {"type": "text", "text": "A Rooted Tree Isomorphism ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To identify the set of unique computation trees GRAPHTRAIL performs rooted tree-isomorphism. An efficient method for this task involves using canonical labels. The canonical label of a graph $\\mathcal{G}$ is a unique representation that remains unchanged under isomorphism, i.e., two graphs $\\mathcal{G}_{1}$ and $\\mathscr{G}_{2}$ have the same canonical label if and only if they are isomorphic. We construct the canonical label of a rooted computation tree through Depth-First Canonical Labeling (DFCF) [7]. From a labelled rooted unordered tree we can derive many labelled rooted ordered trees as shown in Fig. H. There is a one-to-one correspondence between a labelled rooted ordered tree and its depth-first string encoding. The ordering of the strings orders the trees and the minimum in the ordering is the canonical label. Each string is a depth-first (preorder) traversal that uses $\\mathbb{S}$ to represent a backtrack and $\\#$ to represent ", "page_idx": 14}, {"type": "image", "img_path": "fzlMza6dRZ/tmp/fbdf639a263b49d92443cfa8ac3c3e38a23d81d6fd44365e59d6fcc732913156.jpg", "img_caption": ["\\$\uff1a Backtrack# : End of string encoding"], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure H: Four rooted ordered trees obtained from the same rooted unordered tree. The alphabets denote the node labels. The strings below each tree are their depth-first traversals. ", "page_idx": 15}, {"type": "image", "img_path": "fzlMza6dRZ/tmp/c7fa3fc1fcec6485bffe245dc2114ef6378f99d019bc325b504441e4b0b69524.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure I: Example of constructing a Depth-First-Canonical-Form of a rooted tree. Each vertex is labeled with its identifier and, within parentheses, the ranks of its children, ending with a \u201chash symbol\u201d to denote the end of the encoding. The rank of the vertex at its level is placed before the parentheses. After sorting all levels, the tree is then scanned from the top down, beginning with the root, and the children of each vertex are rearranged according to the established order. ", "page_idx": 15}, {"type": "text", "text": "the end of the string encoding. In sorting, $\\#$ is greater than $\\mathbb{S}$ and both these symbols are greater than other labels. The Depth-First Canonical Form is constructed by sorting the vertices of a rooted unordered tree level by level. At each level, the vertices are sorted first by their labels and then by the ranks of their children at respective levels. Fig. I illustrates the process. ", "page_idx": 15}, {"type": "text", "text": "B Shapley Values ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "SHAP [28] represents the Shapley value explanation (SV) [43] as an additive feature attribution method and ftis a weighted linear regression model to approximate the model\u2019s output. The weights in this model correspond to the SVs (contribution of features). It specifies the explanation as ", "page_idx": 15}, {"type": "equation", "text": "$$\ng(z)=\\phi_{0}+\\sum_{j=1}^{M}\\phi_{j}z_{j}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $g$ is the model, $z\\,\\in\\,\\{0,1\\}^{M}$ is the coalition vector, $M$ is the maximum coalition size, and $\\phi_{j}\\in\\mathbb{R}$ is the SV for feature $j$ . Kernel-SHAP [28] is a SHAP variant to estimate SVs. It calculates SVs via: ", "page_idx": 15}, {"type": "text", "text": "\u2022 Coalition Sampling: It randomly selects subsets of features, coalitions $\\left(z_{k}\\right)$ . It assigns weights to each coalition using the SHAP kernel where $M=$ total features, $|z|=$ coalition size: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pi(z)=\\frac{(M-1)}{\\binom{M}{x}|z|(M-|z|)}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "\u2022 Predictions: For each coalition, Kernel-SHAP maps the coalition back to the original features and obtains a prediction afterwards. ", "page_idx": 15}, {"type": "text", "text": "C Symbolic Regression ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In symbolic regression, given a set of $n$ input-output pairs $(x_{i},y_{i})_{i=1}^{n}$ , where $x_{i}\\in\\mathbb{R}^{d}$ is an input vector, and $y\\in\\mathbb R$ is the output value (or label), and a set of operators, such as addition, subtraction, etc., the goal is to find a symbolic equation $e$ and corresponding function $f_{e}$ such that $\\forall i:y_{i}\\approx f_{e}(x_{i})$ , while also reducing the complexity (number of operators and variable) of the formula. The loss function is presented in Eq. 11. Finding the optimal formula is not computationally tractable since the number of formulas grows exponentially with the number of variables and operations. Hence, from the various approximation strategies in the literature [31] we use [8], which leverages an evolutionary algorithm. ", "page_idx": 16}, {"type": "text", "text": "The process begins by initializing $n_{p}$ populations, each with $L$ random expressions of complexity $C$ (3 in our experiments). For each $P_{i}$ , a set $M_{i}$ is created to store the expressions with the smallest loss (Eq. 11) at each complexity level within that population. Additionally, a global set $H$ is maintained to store the expressions with lowest loss at each complexity across all populations. The algorithm iterates over each population $P_{i}$ for a fixed number of epochs, allowing them to evolve. Evolution is driven by running tournaments within each population, where the expression $E$ with lowest loss is declared the winner. A copy of the winner is created and chosen for mutation or crossover, forming $E^{*}$ . Mutating an expression involves repeatedly applying random operations from a set of operations, e.g., replacing/adding operators and variables. Crossover selects the two best expressions, $E_{1},E_{2}$ from the population and swapping random sub-expressions between them, forming $E_{1}^{*},E_{2}^{*}$ . The newly formed expression(s) replace the oldest expression(s) in the population. ", "page_idx": 16}, {"type": "text", "text": "Once evolution within each population is complete, the sets $M_{i}$ and $H$ are updated with the best expressions. When the set number of epochs are complete, the expression from $H$ with minimum loss is returned. ", "page_idx": 16}, {"type": "text", "text": "D Dataset details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Table C presents four benchmark datasets used in our experiments. NCI1 [49], MUTAG [17], and Mutagenicity [39, 20] are collections of molecules. The nodes represent different atoms, and the edges are chemical bonds between them. The molecules are classified by whether they are anticancer (NCI1) or mutagenic (MUTAG and Mutagenicity). In MUTAG, Class 1 is the mutagenic class while in Mutagenicity, Class 0 is mutagenic. BAMultiShapes [55] is a synthetic dataset. The dataset is composed of 1,000 Barabasi-Albert (BA) graphs with network motifs attached in random positions. These motifs include house, grid, and wheel structures. Class 0 includes plain BA graphs as well as BA graphs augmented with a house, a grid, a wheel, or all three motifs combined. Class 1 consists of BA graphs enriched with combinations of two motifs: a house and a grid, a house and a wheel, or a wheel and a grid. ", "page_idx": 16}, {"type": "table", "img_path": "fzlMza6dRZ/tmp/3c6586b23882e27edde3ba9cdcb0942c35cab3c2ed5b4dfef057521631045799.jpg", "table_caption": ["Table C: Statistics of the datasets. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "E Further details of empirical framework ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "All experiments are performed on an Intel Xeon Gold 6248 processor with 96 cores and 1 NVIDIA A100 GPU with 40GB of memory and 377 GB RAM with Ubuntu 18.04. ", "page_idx": 16}, {"type": "text", "text": "Each dataset is split into train-validation-test sets in the proportion of 70:10:20. The accuracies of all GNN architectures are reported in Table D. All GNNs have been trained with $L=3$ layers. We use Adam optimizer with a learning rate set to 0.001. Training stops early after a warm-up of 90 ", "page_idx": 16}, {"type": "text", "text": "epochs if validation accuracy doesn\u2019t increase for 100 epochs or a total of 1000 epochs elapse. The explainers are evaluated on the test splits. ", "page_idx": 17}, {"type": "table", "img_path": "fzlMza6dRZ/tmp/8d70782a278f6ef42c95b548f146e627055b9ebc5b5ced1b25812c6ca9ca47e6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table D: Model accuracy across different GNN architectures and pooling layers for aggregating node embeddings into graph embedding (Eq. 4). ", "page_idx": 17}, {"type": "image", "img_path": "fzlMza6dRZ/tmp/4eb61645474db1d63e0f13e9872f44b8e3ccc4c4d27a43ca3b420a64a361fc4b.jpg", "img_caption": ["Figure J: Code snippet of instance explainer restricting train set to only those graphs that contain the explanation motifs. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "F Contribution of Shapley values ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To highlight the impact of using Shapley values for selecting the important computation trees, we compare the GRAPHTRAIL\u2019s fidelity when the computation trees are chosen based on the Shapley values versus when they are chosen at random. As evident from Table E, choosing random trees significantly deteriorates the fidelity, indicating the value of Shapley-based selection. ", "page_idx": 17}, {"type": "text", "text": "G Additional Experiments ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we present additional experiments to further validate our findings. The results are summarized in Tables F, G, H, I, each highlighting different aspects of the derived formulae. Each metric is the weighted average across the classes. These tables provide deeper insights and reinforce the conclusions drawn from our initial analysis. ", "page_idx": 17}, {"type": "table", "img_path": "fzlMza6dRZ/tmp/9d145ed3730aa87c75ad4c9a38efc54f391e5d1d74a82a126eb804aa2eadf75b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table E: The impact of selecting computation trees by considering Shapley values versus random selection. ", "page_idx": 17}, {"type": "table", "img_path": "fzlMza6dRZ/tmp/03a2f29e5b5516d20fb80789a09721a7efde1f16519a320cadef88bee0c9dd9d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "fzlMza6dRZ/tmp/1936f783a7e7ee8128bdf4fd45004ee5f7cd1c5d80c9beb623e1d75240bbc4ac.jpg", "table_caption": ["Table F: Weighted average precision of the generated formulas against the GNN output "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "fzlMza6dRZ/tmp/714a4b1a34c8e8495175b3bbb9e4feefedd64cd75fcc3d6da469a5d25b39d134.jpg", "table_caption": ["Table G: Weighted average recall of the generated formulas against the GNN output "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "fzlMza6dRZ/tmp/721e9a63b74b63a575fd1f7c02b9132389aab211f907fffd2249569451ea88ae.jpg", "table_caption": ["Table H: The weighted average F1-score of the generated formulas. The weighted version accounts for label imbalance and can result in an F1-score that is not between precision and recall. ", "Table I: Accuracy of the generated formulae against ground-truth labels "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "H Reproducibility Analysis of GLGEXPLAINER ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Our analysis reveals inconsistencies between the findings reported in [2] and our own observations with GLGEXPLAINER. Specifically, the reported results in [2] for Mutagenicity dataset might be inflated due to test data leakage. In this dataset, the presence of NO2 and/or NH2 motifs is indicative of a positive class label [55]. However, GLGEXPLAINER utilizes PGEXPLAINER [29] to generate instance explanations. Within GLGEXPLAINER, these explanations are created by excluding training graphs that lack these motifs (see Fig. J for the relevant code snippet from PGEXPLAINER\u2019s repository https://github.com/flyingdoog/PGExplainer/blob/master/MUTAG.ipynb). This approach results in biased explanations that overemphasize specific motifs. When all training graphs are included, GLGEXPLAINER produces a wider variety of explanations, leading to a decrease in its overall performance. ", "page_idx": 18}, {"type": "text", "text": "For other datasets, GLGEXPLAINER offers instance explanations but lacks transparency regarding the specific PGEXPLAINER settings used to generate them. Despite employing all recommended settings and extensive parameter tuning for PGEXPLAINER, we were unable to replicate the instance explanations presented by the authors of GLGEXPLAINER. Our email to the authors of GLGEXPLAINER pointing out these inconsistencies did not receive a response. ", "page_idx": 18}, {"type": "text", "text": "I Cluster Impurity of GLGEXPLAINER ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In GLGEXPLAINER, the formulae are vectors that are called prototypes. Prototypes are representatives of clusters of local explanations, and it is assumed that the clusters are pure. This essentially means that most of the elements in the same clusters will be similar. To verify this claim, we evaluate random graphs from each cluster for both classes across different seeds. Figures K, L, M, and N show the results in all four datasets across different seeds. We make two important observations. First, the clusters are not pure, and the graphs are not similar in the same cluster. For instance, in seed 45 of MUTAG (Fig. K), class 0 has four types of graphs among five. Second, across classes, there exist similar graphs, which is undesired. For example, in the same figure, seed 729 shows three graphs that are similar across classes. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "image", "img_path": "fzlMza6dRZ/tmp/451c4f2c9afce4d9dd248c2540593cfd6d7497b37424667dd17e093098f88071.jpg", "img_caption": ["Seed 729 "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "fzlMza6dRZ/tmp/5b799a663271cd5e56950c11e000ab8ed87f50ef022706a0435d86e6c4930d87.jpg", "img_caption": ["Seed 729 "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure L: A visual depiction of the impure clusters of GLGEXPLAINER in Mutagenicity. ", "page_idx": 20}, {"type": "image", "img_path": "fzlMza6dRZ/tmp/dc25819767bb4445a84d1c840634a72561c3f3c49f4cc3abd12a8026df87b440.jpg", "img_caption": ["Seed 796 "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure M: A visual depiction of the impure clusters of GLGEXPLAINER in BAMultiShapes. ", "page_idx": 21}, {"type": "image", "img_path": "fzlMza6dRZ/tmp/1d855ba5d85c6f1ba72893011ecbd3bf1f4ff587b9803412093774c83a7fae92.jpg", "img_caption": ["Seed 1983 "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure N: A visual depiction of the impure clusters of GLGEXPLAINER in NCI1. ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s   \ncontributions and scope?   \nAnswer: [Yes]   \nJustification: Sec. 2, 3, and 4.   \nGuidelines:   \n\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]   \nJustification: Sec. 5.   \nGuidelines:   \n\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a   \ncomplete (and correct) proof?   \nAnswer: [Yes]   \nJustification: Sec. 3.2   \nGuidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results. ", "page_idx": 23}, {"type": "text", "text": "\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. ", "page_idx": 23}, {"type": "text", "text": "\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Justification: Sec 4.1, Appendix B and C Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Sec. 4 and Appendix B and C. The code has been provided via a github repository. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/ CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/ CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?   \nAnswer: [Yes]   \nJustification: Sec. 4 and Appendix B and C.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate   \ninformation about the statistical significance of the experiments?   \nAnswer: [Yes]   \nJustification: Section 4   \nGuidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes]   \nJustification: Sec. 4 and Appendix C   \nGuidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] Justification: All sections. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal   \nimpacts of the work performed?   \nAnswer: [Yes]   \nJustification: Sec. 1 (positive social impact), Sec. 5 (negative social impact)   \nGuidelines:   \n\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of   \ndata or models that have a high risk for misuse (e.g., pretrained language models, image generators,   \nor scraped datasets)?   \nAnswer: [NA]   \nJustification: [NA]   \nGuidelines:   \n\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA]   \nJustification: [NA]   \nGuidelines:   \n\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation   \nprovided alongside the assets?   \nAnswer: [Yes]   \nJustification: Sec. 4   \nGuidelines:   \n\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "4. Crowdsourcing and Research with Human Subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: [NA] Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]