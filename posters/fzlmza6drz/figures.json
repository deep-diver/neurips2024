[{"figure_path": "fzlMza6dRZ/figures/figures_3_1.jpg", "caption": "Figure 1: Pipeline of the GRAPHTRAIL algorithm.", "description": "This figure presents a flowchart illustrating the different steps involved in the GRAPHTRAIL algorithm.  It starts with input graphs and progresses through several stages: creation of computation trees (CTrees), canonization of these trees to identify unique ones, computation of Shapley values to rank these unique trees (indicating their importance in the GNN's predictions), filtering of the less important trees, generation of concept vectors representing the selected trees, embedding those vectors, and finally, using symbolic regression to generate a logical formula that encapsulates the GNN's decision-making process.", "section": "3 GRAPHTRAIL: Proposed Global Explainer"}, {"figure_path": "fzlMza6dRZ/figures/figures_4_1.jpg", "caption": "Figure 2: The figure illustrates the process of constructing the computation trees of nodes v1 \u2208 G1 and u1 \u2208 G2 for L = 2. The colors of the nodes represent the node labels. Note that although v1 and u1 are embedded in non-isomorphic L-hop neighborhoods, their computation trees are isomorphic.", "description": "This figure illustrates how computation trees are constructed in a message-passing GNN. Two example graphs G1 and G2 are shown, each with a node (v1 and u1 respectively) whose computation tree is constructed for L=2 layers. The computation tree for each node includes all paths of length up to L starting from the node.  The key takeaway is that even if two nodes have non-isomorphic L-hop neighborhoods (subgraphs), their computation trees can still be isomorphic. This property is crucial to the efficiency of the GRAPHTRAIL method.", "section": "3.2 Mapping Concepts to Rooted Computation Trees"}, {"figure_path": "fzlMza6dRZ/figures/figures_8_1.jpg", "caption": "Figure 4: Results on the test set fidelity averaged over three seeds while varying the training data size.", "description": "This figure shows how the test set fidelity of GRAPHTRAIL and GLGEXPLAINER changes with varying training data sizes.  The x-axis represents the fraction of the training dataset used, while the y-axis shows the test fidelity (average over three seeds).  The plot demonstrates the robustness and data efficiency of GRAPHTRAIL, maintaining higher fidelity even with smaller training datasets compared to GLGEXPLAINER.", "section": "4 Experiments"}, {"figure_path": "fzlMza6dRZ/figures/figures_8_2.jpg", "caption": "Figure 4: Results on the test set fidelity averaged over three seeds while varying the training data size.", "description": "This figure shows the test set fidelity of GraphTrail and GLGExplainer across different fractions of the training dataset for four datasets (BAMultiShapes, MUTAG, Mutagenicity, and NCI1).  The x-axis represents the fraction of the training data used, and the y-axis shows the test fidelity. The figure demonstrates the robustness and stability of GRAPHTRAIL in low-data regimes, as its fidelity remains relatively high even with small fractions of the training data.  Conversely, GLGExplainer shows a more significant decrease in fidelity as the amount of training data decreases.", "section": "4 Experiments"}, {"figure_path": "fzlMza6dRZ/figures/figures_9_1.jpg", "caption": "Figure 5: Visual inspection of the formulae generated by GRAPHTRAIL and GLGEXPLAINER. The encircled numbers in Mutagenicity are toxicophore IDs from Fig. F (in Appendix). The structures with two IDs in Mutagenicity can be extended to both toxicophores.", "description": "This figure visually compares the logical rules (formulae) generated by GRAPHTRAIL and GLGExplainer for three different datasets: MUTAG, Mutagenicity, and BAMultiShapes.  For each dataset, it shows the top-k computation trees (subgraphs) identified as important by each method, along with the resulting boolean formula and its fidelity (how accurately it reflects the GNN's predictions).  The figure highlights that GRAPHTRAIL produces more accurate and interpretable rules, especially in the Mutagenicity dataset, where it identifies key toxicophores related to mutagenicity, unlike GLGExplainer.", "section": "Visual Analysis of Rules"}, {"figure_path": "fzlMza6dRZ/figures/figures_14_1.jpg", "caption": "Figure G: Taxonomy of research conducted on explaining GNN predictions. Gradient: SA [4], Guided-BP [4], Grad-CAM [37]; Decomposition: Excitation-BP [37], GNN-LRP [40], CAM [37], GOAT [26]; Perturbation: GNNExplainer [55], PGExplainer [29], SubgraphX [58], GEM [25], TAGExplainer [52], CF2 [45], RCExplainer [3], CF-GNNexplainer [27], CLEAR [30], InduCE [47]; Surrogate: GraphLIME [15], Relex [60], PGM-Explainer [48]; Global: XGNN [56], GLG-Explainer [2], Xuanyuan et al. [54], GCFExplainer [16].", "description": "This figure presents a comprehensive taxonomy of existing GNN explanation methods. It categorizes them into instance-level and global-level explainers, further subdividing instance-level methods into gradients/features, decomposition, perturbation, and surrogate approaches.  Each category lists specific algorithms with their corresponding references, indicating whether they are factual or counterfactual methods.", "section": "Introduction and Related Works"}, {"figure_path": "fzlMza6dRZ/figures/figures_15_1.jpg", "caption": "Figure 2: The figure illustrates the process of constructing the computation trees of nodes v1 \u2208 G1 and u1 \u2208 G2 for L = 2. The colors of the nodes represent the node labels. Note that although v1 and u1 are embedded in non-isomorphic L-hop neighborhoods, their computation trees are isomorphic.", "description": "This figure illustrates how computation trees are constructed for a two-layered Graph Neural Network (GNN).  Two example graphs G1 and G2 with nodes v1 and u1, respectively, are shown. Each node's computation tree shows how information propagates through the graph within a two-hop radius. Despite the original graphs having non-isomorphic neighbourhoods around v1 and u1, the resulting computation trees are isomorphic. This illustrates a key concept in the GRAPHTRAIL algorithm; that isomorphic computation trees are collapsed into a single concept to reduce the number of concepts for efficient Boolean logic generation.", "section": "3.2 Mapping Concepts to Rooted Computation Trees"}, {"figure_path": "fzlMza6dRZ/figures/figures_15_2.jpg", "caption": "Figure 1: Pipeline of the GRAPHTRAIL algorithm.", "description": "This figure illustrates the pipeline of the GRAPHTRAIL algorithm, which consists of several steps: 1) Graph and Computation Tree Extraction: the input graph is processed to extract computation trees, which capture the information flow within the GNN; 2) Canonization: computation trees are converted into a canonical form for efficient comparison and concept mining; 3) Shapley Value Computation: Shapley values are computed for each unique computation tree to identify the most important concepts in the GNN's decision-making process; 4) Concept Vector Generation: concept vectors are created to represent the presence or absence of the selected concepts in each graph; 5) Symbolic Regression: a boolean formula is learned that maps the concept vectors to the GNN's predictions. The resulting formula is human-interpretable and explains the GNN's decision-making process at a global level.", "section": "3 GRAPHTRAIL: Proposed Global Explainer"}, {"figure_path": "fzlMza6dRZ/figures/figures_17_1.jpg", "caption": "Figure 1: Pipeline of the GRAPHTRAIL algorithm.", "description": "The figure shows a flowchart of the GRAPHTRAIL algorithm. It starts with a graph and produces a logical formula. The main steps are: (1) canonization of computation trees (CTrees), (2) filtering based on Shapley values, (3) concept vector creation, (4) symbolic regression to get a logical formula.", "section": "3 GRAPHTRAIL: Proposed Global Explainer"}, {"figure_path": "fzlMza6dRZ/figures/figures_19_1.jpg", "caption": "Figure K: A visual depiction of the impure clusters of GLGEXPLAINER in MUTAG.", "description": "This figure visually demonstrates the impurity within the clusters generated by the GLGExplainer algorithm in the MUTAG dataset.  It shows examples from different seeds (random starting points for the algorithm) to illustrate that the clusters contain diverse graph structures, rather than being homogeneous. This impurity contradicts the assumption in GLGExplainer that clusters are pure, and weakens the reliability of the method.", "section": "Cluster Impurity of GLGEXPLAINER"}, {"figure_path": "fzlMza6dRZ/figures/figures_20_1.jpg", "caption": "Figure K: A visual depiction of the impure clusters of GLGEXPLAINER in MUTAG.", "description": "This figure visually shows the impure clusters generated by the GLGExplainer method in the MUTAG dataset.  It demonstrates that the clusters produced by GLGExplainer are not pure; they contain graphs from different classes, indicating that the method's cluster assignment does not effectively separate data points based on their class labels.  This is shown for three different random seeds (45, 357, and 729), each illustrating the lack of purity within the generated clusters. The lack of purity in these clusters directly affects the interpretability and accuracy of the GLGExplainer's logical formulas, which are built upon these clusters.", "section": "Cluster Impurity of GLGEXPLAINER"}, {"figure_path": "fzlMza6dRZ/figures/figures_21_1.jpg", "caption": "Figure M: A visual depiction of the impure clusters of GLGEXPLAINER in BAMultiShapes.", "description": "This figure visually demonstrates the impurity of clusters generated by the GLGExplainer method in the BAMultiShapes dataset.  It shows examples of graphs from different clusters, highlighting the lack of homogeneity within each cluster. The lack of purity in these clusters is a significant limitation of the GLGExplainer approach, as it impacts the interpretability and reliability of its explanations.", "section": "Cluster Impurity of GLGEXPLAINER"}, {"figure_path": "fzlMza6dRZ/figures/figures_22_1.jpg", "caption": "Figure 1: Pipeline of the GRAPHTRAIL algorithm.", "description": "The figure shows the pipeline of the GRAPHTRAIL algorithm. It starts with a graph as input. Then, the computation trees are extracted, and canonicalized. After filtering, the unique computation trees are used to compute the Shapley values. The top-k computation trees are then selected as concepts. These concepts are mapped to concept vectors and subsequently to their embeddings. Finally, these concept embeddings are used in a symbolic regression to obtain a logical formula.", "section": "3 GRAPHTRAIL: Proposed Global Explainer"}]