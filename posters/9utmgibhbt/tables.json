[{"figure_path": "9utMGIbHBt/tables/tables_5_1.jpg", "caption": "Table 1: FID scores on the CIFAR10 dataset [22]. UDPM uses 3 steps, which are equivalent in terms of complexity to 0.3 of a single denoising step used in typical diffusion models like DDPM or EDM.", "description": "This table compares the Fr\u00e9chet Inception Distance (FID) scores of UDPM against other state-of-the-art diffusion models on the CIFAR10 dataset.  It highlights UDPM's efficiency by showing that its 3 steps are computationally equivalent to only 0.3 steps of a typical diffusion model.  Lower FID scores indicate better image quality.", "section": "4.2 Class Conditional Generation"}, {"figure_path": "9utMGIbHBt/tables/tables_8_1.jpg", "caption": "Table 1: FID scores on the CIFAR10 dataset [22]. UDPM uses 3 steps, which are equivalent in terms of complexity to 0.3 of a single denoising step used in typical diffusion models like DDPM or EDM.", "description": "This table compares the Fr\u00e9chet Inception Distance (FID) scores of UDPM against other state-of-the-art diffusion models on the CIFAR10 dataset.  It highlights UDPM's superior performance, achieving a lower FID score with significantly fewer steps (3) than other models, which require 1 to 35 steps. The table emphasizes that UDPM's 3 steps are computationally equivalent to just 0.3 of a single step in traditional diffusion models, showcasing its efficiency.", "section": "4.2 Class Conditional Generation"}, {"figure_path": "9utMGIbHBt/tables/tables_17_1.jpg", "caption": "Table 2: FID scores comparison between UDPM and EDM [18] on the FFHQ [19] and AFHQv2 [6] datasets. UDPM requires 3 diffusion steps, which is equivalent to 0.3 denoising steps of EDM.", "description": "This table compares the Fr\u00e9chet Inception Distance (FID) scores achieved by UDPM and EDM on the FFHQ and AFHQv2 datasets.  FID is a metric used to evaluate the quality of generated images, with lower scores indicating better quality.  The table highlights that UDPM achieves comparable or better FID scores with significantly fewer computational steps.  Specifically, UDPM's 3 steps are equivalent to only 0.3 steps of the EDM model, showcasing UDPM's computational efficiency.", "section": "4.2 Class Conditional Generation"}, {"figure_path": "9utMGIbHBt/tables/tables_18_1.jpg", "caption": "Table 3: Training hyperparameters.", "description": "This table presents the hyperparameters used during the training phase of the UDPM model for three different datasets: CIFAR10, AFHQv2, and FFHQ.  The hyperparameters include the learning rate, the number of warmup steps, batch size, dropout rate, optimizer used (Adam), and the number of GPUs used for training.  These settings were optimized for each dataset to achieve the best results.", "section": "4 Experiments"}, {"figure_path": "9utMGIbHBt/tables/tables_18_2.jpg", "caption": "Table 4: Model hyperparameters.", "description": "This table shows the hyperparameters used for the different datasets in the UDPM model.  These hyperparameters control various aspects of the model's architecture and training, such as the number of channels, the attention resolution, and the number of blocks per scale.  Different datasets may require different hyperparameter settings to achieve optimal performance.", "section": "4.1 Unconditional Generation"}, {"figure_path": "9utMGIbHBt/tables/tables_18_3.jpg", "caption": "Table 5: Discriminator network hyperparameters.", "description": "This table details the hyperparameters used for the discriminator network in the UDPM model, categorized by dataset (CIFAR10, AFHQv2, FFHQ).  The hyperparameters include the discriminator architecture (DDGAN [41]), the number of parameters, the number of channels at each layer, the number of input channels, the number of blocks per scale, the type of blocks used (NCSN down), the gradient penalty weight (from [41]), and the dropout rate.", "section": "4. Experiments"}]