[{"figure_path": "Nycj81Z692/tables/tables_2_1.jpg", "caption": "Table 3: The main results of relational triplet extraction (RTE) and knowledge graph completion (KGC). We report the accuracy (acc) and confidence for GPT evaluation on two datasets, and report accuracy (acc) for the Human evaluation approach. The best baseline performance is underlined.", "description": "This table presents the main results of the relational triplet extraction and knowledge graph completion tasks.  It compares the performance of the proposed UrbanKGent model against 31 baseline methods across two datasets (NYC and CHI).  Both GPT-4-based evaluation and human evaluation are provided for comparison.  The accuracy and confidence levels are shown for GPT-4, while accuracy is given for human evaluation. The best-performing baseline model is underlined for each task and dataset.", "section": "5.2 Main Result"}, {"figure_path": "Nycj81Z692/tables/tables_3_1.jpg", "caption": "Table 1: The statistics of raw datasets.", "description": "This table shows the number of Area of Interest (AOI), road, Point of Interest (POI), review, and web page data collected for New York City and Chicago.  These data sources are used to construct the Urban Knowledge Graph.", "section": "2 UrbanKGC Data Description"}, {"figure_path": "Nycj81Z692/tables/tables_7_1.jpg", "caption": "Table 2: The statistics of constructed UrbanKGC dataset.", "description": "This table shows the number of records in different datasets used for the UrbanKGC task.  The datasets are categorized by city (New York City or Chicago), task type (RTE or KGC), and dataset size (Instruct, main dataset, or Large).  The Instruct datasets are smaller and used for instruction fine-tuning. The main datasets are medium-sized and used for validation, and the Large datasets are much larger and used for real-world testing. ", "section": "2 UrbanKGC Data Description"}, {"figure_path": "Nycj81Z692/tables/tables_8_1.jpg", "caption": "Table 3: The main results of relational triplet extraction (RTE) and knowledge graph completion (KGC). We report the accuracy (acc) and confidence for GPT evaluation on two datasets, and report accuracy (acc) for the Human evaluation approach. The best baseline performance is underlined.", "description": "This table presents the main experimental results of the UrbanKGent model on two tasks: Relational Triplet Extraction (RTE) and Knowledge Graph Completion (KGC).  It compares the performance of UrbanKGent against 31 baseline methods across two datasets (NYC and CHI).  The accuracy is measured using both GPT-4 and human evaluation.  The table highlights the superior performance of UrbanKGent across various models and evaluation methods, with the best performing baseline underlined.", "section": "5.2 Main Result"}, {"figure_path": "Nycj81Z692/tables/tables_8_2.jpg", "caption": "Table 4: Statistics comparison of constructed UrbanKGs in New York and Chicago between UrbanKGent and existing benchmark.", "description": "This table compares the statistics of Urban Knowledge Graphs (UrbanKGs) constructed for New York City and Chicago using the UrbanKGent framework and an existing benchmark (UUKG).  It shows the number of entities, relations, and triplets in each UrbanKG, as well as the total volume of data used for construction. The comparison highlights the efficiency of UrbanKGent, which constructs UrbanKGs with a similar scale using significantly less data.", "section": "5.2 Main Result"}, {"figure_path": "Nycj81Z692/tables/tables_14_1.jpg", "caption": "Table 5: The detailed statistic of RTE datasets. We report the maximum length, minimum length, and average length of urban text in the RTE dataset.", "description": "This table presents a statistical overview of six different datasets used for Relational Triple Extraction (RTE) tasks in the UrbanKGent paper.  It shows the maximum, minimum, and average lengths of the urban text within each dataset, along with the total number of records in each dataset. The datasets are categorized into instruction tuning datasets and testing datasets, with two datasets each for New York City and Chicago.  The large datasets are intended for real-world application. The table helps to understand the characteristics and size of the text data used for training and evaluating the RTE models.", "section": "2.1.1 Geographic Data"}, {"figure_path": "Nycj81Z692/tables/tables_14_2.jpg", "caption": "Table 3: The main results of relational triplet extraction (RTE) and knowledge graph completion (KGC). We report the accuracy (acc) and confidence for GPT evaluation on two datasets, and report accuracy (acc) for the Human evaluation approach. The best baseline performance is underlined.", "description": "This table presents the main experimental results of the paper, comparing the performance of the proposed UrbanKGent model against 31 baseline methods on two tasks: relational triplet extraction (RTE) and knowledge graph completion (KGC).  The results are shown for two datasets, and two evaluation methods are used: GPT-4 based evaluation and human evaluation.  The accuracy and confidence intervals are reported for the GPT-4 evaluations, and the accuracy is reported for the human evaluation.  The best-performing baseline method for each task and dataset is underlined.", "section": "5.2 Main Result"}, {"figure_path": "Nycj81Z692/tables/tables_15_1.jpg", "caption": "Table 1: The statistics of raw datasets.", "description": "This table presents the summary statistics of the raw datasets used in the paper, including the number of Areas of Interest (AOI), roads, Points of Interest (POI), reviews, and web pages for both New York City and Chicago.", "section": "2 UrbanKGC Data Description"}, {"figure_path": "Nycj81Z692/tables/tables_17_1.jpg", "caption": "Table 7: Illustrative RTE evaluation example when we utilize human evaluation and GPT evaluation method. We calculate the accuracy by counting the proportion of true triplets. The label for GPT evaluation method is invisible.", "description": "This table shows an example of how the accuracy of relational triplet extraction (RTE) is calculated using both human and GPT-4 evaluation methods.  Human evaluators determine the true and false triplets based on a given text and model output. GPT-4 is then used to evaluate the same text and output, providing another accuracy measure. The labels used by GPT-4 are not visible in this specific illustration.", "section": "5.2 Main Result"}, {"figure_path": "Nycj81Z692/tables/tables_19_1.jpg", "caption": "Table 8: The average Spearman correlation value between human evaluations and GPT-4 evaluations. \"Repeat X times\" refers to instructing GPT-4 to generate judgments X times, and adopting the answer that appears most frequently (e.g., True/False for the KGC task and Number of the true triplet for the RTE task) as the final decision.", "description": "This table shows the correlation consistency between human evaluation and GPT-4 evaluation under different repeat times.  The Spearman correlation is calculated to measure the consistency, with higher values indicating greater agreement.  The results demonstrate the reliability of using GPT-4 for evaluation, even when repeating the evaluation multiple times.", "section": "D.3 Evaluation Consistency"}, {"figure_path": "Nycj81Z692/tables/tables_19_2.jpg", "caption": "Table 3: The main results of relational triplet extraction (RTE) and knowledge graph completion (KGC). We report the accuracy (acc) and confidence for GPT evaluation on two datasets, and report accuracy (acc) for the Human evaluation approach. The best baseline performance is underlined.", "description": "This table presents the main results of the relational triplet extraction (RTE) and knowledge graph completion (KGC) tasks.  It compares the performance of various models, including different versions of LLMs (e.g., Llama-2, Llama-3, Vicuna, Alpaca, Mistral) and GPT-3.5/4.  Results are shown for two datasets, and include accuracy and confidence scores from GPT-4 based evaluations, as well as human-evaluated accuracy scores. The best-performing baseline model for each task and dataset is underlined.", "section": "5.2 Main Result"}, {"figure_path": "Nycj81Z692/tables/tables_20_1.jpg", "caption": "Table 10: Effect of different blocks.", "description": "This table presents the ablation study results of the proposed UrbanKGent framework. By removing different components (knowledgeable instruction generation, multi-view design in RTE, external geospatial tool invocation, and iterative trajectory self-refinement), the impact on the performance of relational triplet extraction (RTE) and knowledge graph completion (KGC) tasks are evaluated. The results are shown in terms of accuracy and confidence using GPT-4 evaluation, and accuracy using human evaluation.", "section": "E.2 RQ2: Ablation Studies"}, {"figure_path": "Nycj81Z692/tables/tables_20_2.jpg", "caption": "Table 11: Comparison among LLM-based UrbanKGC methods in four ways.", "description": "This table compares four different methods for Urban Knowledge Graph Construction (UrbanKGC) using Large Language Models (LLMs): Zero-shot learning (ZSL), In-context learning (ICL), Vanilla fine-tuning (VFT), and the proposed UrbanKGent inference method.  It shows whether each method utilizes extra knowledge, requires fine-tuning, uses tool invocation, and employs self-refinement.", "section": "5 Experiments"}, {"figure_path": "Nycj81Z692/tables/tables_21_1.jpg", "caption": "Table 12: The inference latency comparison of UrbanKGC using UrbanKGent family. We use two middle-size dataset (i.e., NYC and CHI) and two large-scale dataset (i.e., NYC-Large and CHI-Large) for UrbanKG construction.", "description": "This table shows the inference time in minutes for constructing Urban Knowledge Graphs (UKGs) using three different versions of the UrbanKGent model (7B, 8B, and 13B parameters).  The inference time is shown for two medium-sized datasets (NYC and CHI) and two larger datasets (NYC-Large and CHI-Large).  The data volume for each dataset is also provided.  The bottom row shows the average latency per 1000 records across all datasets.", "section": "5. Experiments"}, {"figure_path": "Nycj81Z692/tables/tables_21_2.jpg", "caption": "Table 13: The statistic of entity and relation ontology of constructed UrbanKGs on NYC-Large and CHI-Large dataset.", "description": "This table presents the statistics of the entity and relation ontology in the UrbanKGs constructed using the NYC-Large and CHI-Large datasets.  It shows the number of coarse-grained and fine-grained entities and relations, as well as the total number of entities and triplets in each UrbanKG.  This helps to quantify the richness and complexity of the generated knowledge graphs.", "section": "5.3 Agent Application"}]