[{"type": "text", "text": "CRITICEVAL: Evaluate Large Language Model as Critic ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tian Lan1\u2217 Wenwei Zhang2\u2217 Chen $\\mathbf{X}\\mathbf{u}^{4}$ Heyan Huang1 Dahua Lin2,3,5 Kai Chen2\u2020 Xian-Ling Mao1\u2020 ", "page_idx": 0}, {"type": "text", "text": "1School of Computer Science and Technology, Beijing Institute of Technology   \n2Shanghai AI Laboratory 3MMLab, The Chinese University of Hong Kong 4School of Medical Technology, Beijing Institute of Technology 5CPII under InnoHK https://github.com/open-compass/CriticEval ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Critique ability, i.e., the capability of Large Language Models (LLMs) to identify and rectify flaws in responses, is crucial for their applications in self-improvement and scalable oversight. While numerous studies have been proposed to evaluate critique ability of LLMs, their comprehensiveness and reliability are still limited. To overcome this problem, we introduce CRITICEVAL, a novel benchmark designed to comprehensively and reliably evaluate critique ability of LLMs. Specifically, to ensure the comprehensiveness, CRITICEVAL evaluates critique ability from four dimensions across nine diverse task scenarios. It evaluates both scalar-valued and textual critiques, targeting responses of varying quality. To ensure the reliability, a large number of critiques are annotated to serve as references, enabling GPT-4 to evaluate textual critiques reliably. Extensive evaluations of open-source and closed-source LLMs first validate the reliability of evaluation in CRITICEVAL. Then, experimental results demonstrate the promising potential of open-source LLMs, the effectiveness of critique datasets and several intriguing relationships between the critique ability and some critical factors, including task types, response qualities and critique dimensions. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Critique ability is crucial for the self-improvement [1, 2] of LLMs, as it enables the effective analysis and correction of flaws in responses [3, 4]. This capability also facilitates a more robust framework, i.e., scalable oversight [5, 6], for ensuring the AI systems growing in scale and capability remain aligned with human-desired outcomes and ethical standards. ", "page_idx": 0}, {"type": "text", "text": "So far, while numerous works have been proposed to evaluate critique ability of LLMs in downstream tasks, like common NLP tasks [7, 8] and reasoning tasks [4, 9], their comprehensiveness and reliability are limited. Specifically, existing works [10, 11, 12] typically evaluate only specific aspects of critique ability, resulting in limited evaluated critique dimensions [13, 11, 12, 10, 14], insufficient analysis of response qualities and task types [4, 9]. Besides, while GPT-4 is frequently used to evaluate textual or natural language critiques [3, 10, 13], its reliability across all critique dimensions and tasks remains unverified [10, 15]. In summary, a comprehensive and reliable benchmark for assessing critique capability of LLMs is still under-explored, significantly impeding the in-depth analysis. ", "page_idx": 0}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/35490d32b95a100ea92d0a03a6ae85cc58581805379eb36b8f92db158e939551.jpg", "table_caption": ["Table 1: Comparison between the test and dev set of benchmarks and CRITICEVAL. A complete list are described in Appendix C. The response quality in some benchmarks is unclassified (-). PR denotes the Pass Rate on reasoning and coding tasks. Our concurrent works are marked with a $\\dagger$ . "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "To fill this gap, we propose a novel benchmark, CRITICEVAL, designed to comprehensively and reliably measure critique capability of LLMs. Specifically, to ensure comprehensiveness, CRITICEVAL evaluates critique ability of LLMs from following dimensions: evaluating a single response (feedback), comparing pairs of responses (comparison), correcting the response based on feedback (correction) and evaluating one feedback of LLM (meta-feedback). These critique dimensions cover all categories of critiques in previous works [11, 13, 4] and the necessary capabilities for self-improvement of LLM [1] and scalable oversight [6]. These critique dimensions are measured under nine diverse task scenarios, including three common NLP tasks, two alignment tasks, and four math reasoning and coding tasks. Moreover, evaluated responses in each critique dimension and each task are collected using various open-source and closed-source LLMs with different capabilities, with human annotation ensuring varied quality levels. Furthermore, since both the scalar-valued and textual formats of critique are commonly used in these scenarios [3], CRITICEVAL evaluate the critiques in both formats, equipped with objective [18, 19] and subjective [10, 11] evaluations, respectively. Note that scalar-valued critiques typically refer to Likert scores and preference labels, while textual critiques refer to more fine-grained textual analysis about response quality [11, 3]. Overall, as shown in Table 1, CRITICEVAL exhibits significant advantages in comprehensiveness compared to previous benchmarks. It demonstrates great diversity in critique formats, critique dimensions, response qualities and the data size of textual critique. ", "page_idx": 1}, {"type": "text", "text": "To ensure the reliability of evaluating textual critiques in CRITICEVAL, a large number of high-quality critiques are annotated, serving as references for GPT-4 to evaluate textual critiques automatically. To annotate these textual critiques efficiently, we employ a human-in-the-loop pipeline [20], first generated by GPT-4 and then rigorously reviewed and refined by human experts. ", "page_idx": 1}, {"type": "text", "text": "Extensive evaluations of 35 widely used open-source and closed-source LLMs prove the reliability of CRITICEVAL. Specifically, GPT-4 with human-annotated reference critiques achieves close correlations with human judgments, while removing them results in significant performance loss. Additionally, critiques with higher scores consistently lead to superior improvements, illustrating a clear correlation between the real critique ability of LLMs and their evaluation scores within CRITICEVAL. Then, extensive evaluations results also demonstrate that some open-source LLMs, such as Qwen [21] and InternLM2 [22], are approaching state-of-the-art closed-source LLMs in critique capabilities, and their critique ability could be further improved through scaling strategy. Besides, the effectiveness of critique datasets is also validated. Finally, these evaluation results also reveal several intriguing phenomena: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Critique difficulty varies by task type. For instance, math reasoning and coding tasks are more challenging for feedback, comparison, while they are easier for meta-feedback. \u2022 There is an inverse relationship between the quality of critiques and responses. For example, high-quality responses pose a greater challenge to critique effectively. \u2022 Critique difficulty correlates with the critique dimensions; notably, comparison and metafeedback dimensions present greater challenges than feedback dimension. ", "page_idx": 1}, {"type": "text", "text": "These observations and phenomena promote an in-depth understanding of critique ability of LLMs.   \nWe hope the discoveries could spur future research in this field. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Application of Critique Ability ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Automatic Evaluation Automatic evaluation, also known as critique ability in recent works [3, 10], has been well studied in the past few years [23, 18]. It aims to accurately judge the evaluated responses in numerous NLP tasks and reduce the high cost of human annotations [18, 17, 24, 25, 26]. Recently, advanced LLMs, like GPT-4, have exhibited very close correlation with human judgments [18, 27, 13], assign textual critiques with corresponding quality scores, i.e., the scalar-valued score, in a chain-of-thought inference manner [28]. To further mitigate the high inference cost of closed-source LLMs, numerous works propose to improve critique ability of open-source LLMs by fine-tuning them on critique datasets generated by GPT-4 [14, 29, 30], like Auto-J [11] and UltraFeedback [12]. ", "page_idx": 2}, {"type": "text", "text": "LLM Self-improvement So far, critique ability has been widely used for self-improvement of LLMs in two stages: (1) Inference stage: Given textual critiques that analyze the flaws in the response and provide suggestions, LLMs can iteratively improve the response quality [6, 15, 31, 32, 33].; (2) Training stage: Scalar-valued critiques are frequently used to compile responses with a clear performance gap for rejective fine-tuning (RFT) or preference learning (RLHF [34]), which further enhances LLM capabilities [1, 2, 5, 35, 36]. For instance, Self-rewarding [1] improves Llama-2-70B by fine-tuning it on samples selected based on its rewards. Similarly, ChatGLM-Math [36] fine-tunes a math-critique model for scoring generated answers, which are used for rejective fine-tuning [37] and direct preference optimization [38]. ", "page_idx": 2}, {"type": "text", "text": "2.2 Benchmarking Critique Ability ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "So far, numerous meta-evaluation benchmarks have been proposed to evaluate the critique ability of models [18]. Early benchmarks mainly focus on evaluating scalar-valued critiques [30, 24] on common NLP tasks [18, 23], like translation [7] and summary [8] by computing the correlations between model and human judgments. Recent works also assess scalar-valued critiques of LLMs on reasoning and coding tasks [24, 9]. For example, CRITICBENCH [9] built from 3 reasoning tasks, analyzes important properties of critique ability of LLMs. Our concurrent work, CRITICBENCH [4] analyzes several intriguing findings among generation, critique and correction capability on responses collect from five reasoning tasks.3 Compared with these existing works, our proposed CRITICEVAL exhibits advantages on several crucial factors, like critique dimensions, response qualities and diverse task types, leading to more comprehensive evaluations for critique ability. Although CRITICBENCH [9] collect high-quality responses, their quality levels are still unclassified. ", "page_idx": 2}, {"type": "text", "text": "Beyond scalar-valued critiques, evaluating textual critiques is more challenging [10, 15]. Most existing works coarsely evaluated textual critiques using GPT-4 [11, 12], proven unreliable [10, 13]. Unlike them, our extensive results prove that GPT-4 with human-annotated critiques is reliable for evaluating textual critiques. Although our concurrent work, MetaCritique [13], demonstrates the reliability of evaluating textual critiques by verifying their Atomic Information Units, it is unclear whether their conclusions could be extended to more critique dimensions and tasks. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first formally define the key concepts and their corresponding notions in CRITICEVAL. Figure 1 shows a specific case to understand these concepts. ", "page_idx": 2}, {"type": "text", "text": "Task Input $(I)$ and Response $(R)$ represent the queries and generations of LLMs, respectively. ", "page_idx": 2}, {"type": "text", "text": "Critique aims to analyze and refine the generated responses. Formally, this paper studies the critique capabilities in four dimensions: (1) feedback $F_{s}$ involves textual analysis and a quality score. Good feedback should not only find flaws in responses but also provide helpful suggestions for correction [6]; (2) correction or refinement $C R$ aims to revise responses with or without feedback. Previous evaluations [11, 12, 9] overlook this dimension, although it is an inevitable step when letting the model improve itself [35]; (3) comparison $F_{c}$ contains a textual critique and a preference label for a pair of responses $(R_{a},R_{b})$ ; (4) Meta-feedback $F_{s}(F_{s})$ , i.e., the feedback of feedback itself [6], involves a rating score reflecting the quality of $F_{s}$ and corresponding textual analysis, which is a high-level critique dimension. Such an ability is necessary to improve critique ability of LLMs [1, 13]. Due to the complexity of the meta-feedback dimension, textual critiques are not collected in this paper, and we leave it for future research. ", "page_idx": 2}, {"type": "image", "img_path": "ZsxZ65YqL1/tmp/2442e53ccf171521dcbf440822920cebe1e57aa46123bff51cf7c5e34673f19e.jpg", "img_caption": ["Figure 1: Cases of four critique dimensions. Scalar-valued critiques are scores and preference labels. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "To the best of our knowledge, these four critiques cover all categories of critiques examined in prior research [11, 4, 13]. Although the feedback of correction and comparison dimensions are also important, they are not essential for the self-improvement of LLMs. Thus, this study mainly focuses on studying the feedback of feedback $F_{s}(F_{s})$ and leaving the rest of them for our future work. ", "page_idx": 3}, {"type": "text", "text": "4 CRITICEVAL Construction ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given the challenge of crafting scalar-valued and textual critiques from scratch, we construct CRITICEVAL using a human-in-the-loop data construction pipeline as shown in Figure 2.4 ", "page_idx": 3}, {"type": "text", "text": "4.1 Task Input Collection ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Task inputs for 9 distinct tasks are collected to evaluate critique capabilities comprehensively (Step 1 in Figure 2). Specifically, CRITICEVAL includes three widely used tasks for evaluating critique ability: (1) representative classical language tasks: summary [39], translation [40], and question-answering [41]; (2) LLM alignment: general chat scenarios [19] and harmlessness cases [35]; (3) reasoning and code capabilities: math reasoning with chain-of-thought (CoT) and program-of-thought (PoT), and coding with and without execution results. We hereinafter refer to \u201ccode w/ execution\u201d as \u201cCodeExec\u201d and \u201ccode w/o execution\u201d as \u201cCodeNE\u201d. For each task, we collect around 100 task inputs from the test sets of some widely used benchmark datasets to ensure the task input quality and avoid data contamination. Please refer to Appendix D for more details about the data source. ", "page_idx": 3}, {"type": "text", "text": "4.2 Response and Critique to be Evaluated ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "For each collected $I$ in each task, LLMs of different scales and capabilities are first employed to generate responses with diverse flaws (Step 2 (a) in Figure 2). The complete list of LLMs is in Appendix E. Then, low-, medium-, and high-quality responses with diverse quality differences are collected according to the quality score annotated by the human raters with GPT-4-turbo as the assistant (Step 2 (b)). Moreover, we also collect golden or correct responses, which have been proven challenging for critiques [15]. More details about how to select low-, medium, high-quality and correct responses can be found in Appendix F. ", "page_idx": 3}, {"type": "text", "text": "After collecting responses, we further collect critiques to be evaluated for the meta-feedback dimension by utilizing four LLMs that are known powerful for critiques (Step 3 (d) in Figure 2): (1) GPT-4; (2) GPT-3.5-turbo; (3) Auto-J-13B [11]; (4) UltraCM-13B [12]. ", "page_idx": 3}, {"type": "text", "text": "4.3 Reference Critique Generation and Annotation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "After collecting task inputs and responses, four kinds of reference critiques are collected on these responses to make the objective and subjective evaluation in our proposed CRITICEVAL more reliable. Feedback and Correction GPT-4-turbo is utilized to generate feedback and corrections sequentially (Step 3 (c) and (e) in Figure 2). The scalar-valued and textual critiques for feedback dimension are collected, denotes as \u201cscore\u201d and \u201ctext\u201d in Figure 2. Since responses in math reasoning and coding tasks pose significant challenges for critiques during our annotation, ground-truth answers are provided for GPT-4 as references to generate high-quality feedback and corrections. Then, they are carefully reviewed and revised by human annotators. ", "page_idx": 3}, {"type": "image", "img_path": "ZsxZ65YqL1/tmp/c4f421efed3c87810240c138a8f4defa5b72652b1ac04300a654cdaaade5aa6a.jpg", "img_caption": ["Figure 2: The data construction pipeline for our proposed CRITICEVAL. Step 1: 9 tasks and numerous LLMs are prepared. Step 2: LLMs are employed to generate responses, which are then meticulously reviewed by human experts. Step 3: Critiques are generated by LLMs with strong critique ability, and human experts annotate them. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Comparison Our empirical finding suggests that pairs of responses pose greater challenges for comparison if they perform similarly. Therefore, two kinds of pairs are first created: $(R_{\\mathrm{low}},R_{\\mathrm{high}})$ and $(R_{\\mathrm{med}},R_{\\mathrm{high}})$ , designated as the easy and hard samples, respectively. Then, GPT-4-turbo is prompted to provide scalar-valued and text critiques on these pairs (Step 3 (f) in Figure 2). These outputs, labeled as \u201cpreferred label\u201d and \u201ctext\u201d in Figure 2, are then refined by human annotators. ", "page_idx": 4}, {"type": "text", "text": "Meta-Feedback Since GPT-4 has been proven unreliable to evaluate critiques [10, 15], three human experts are asked to provide their quality scores for generated critiques (Step 3 (d) in Figure 2). ", "page_idx": 4}, {"type": "text", "text": "During human annotation, multiple human experts are asked to follow a rigorous annotation protocol, detailed in Appendix H.1, and the statistics of human annotation for reference critiques are described in Appendix H.6. Besides, several case studies are shown in Appendix I to facilitate a clear understanding of our proposed CRITICEVAL. ", "page_idx": 4}, {"type": "text", "text": "5 Evaluation Metrics ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "5.1 Objective Evaluation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Feedback and meta-feedback evaluation aim to evaluate the consistency between generated scores and human judgments. This setup facilitates the generation in a chain-of-thought manner, followed by the quality score of the evaluated critiques. For the meta-feedback dimension, LLMs are prompted with annotated reference critiques. The widely-used Spearman correlations [42] are computed [23, 24], which ranges from $-1$ to 1 (normalize to $(-100,100))$ . Higher scores indicate a higher consistency with human judgments. The $p$ -value of spearman correlation are recorded, and $p<0.05$ is typically considered to be statistically significant [23, 43]. ", "page_idx": 4}, {"type": "text", "text": "Comparison evaluation assesses the accuracy of LLM in deciding preferences between two responses. It is well known that current LLMs exhibit significant positional bias [17, 44, 45], i.e., LLMs tend to prefer responses based on their specific position in the prompt. We implement a rigorous verification process to mitigate the effects of positional bias. Specifically, given responses $R_{a}$ and $R_{b}$ to be compared, we obtain the comparison based on two orders, noted as $F_{c}^{a}=\\bar{F_{c}}(R_{a},R_{b})$ ", "page_idx": 4}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/d02bba054e228cb344c46b8a57635adf662c77a1fdc46fea6a43eea8989c9a83.jpg", "table_caption": ["Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with $>0.05$ $p_{\\|}$ -value are marked with $\\dagger$ $.\\,F_{s},C R,F_{c},\\dot{F}_{s}(F_{s})$ represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "and $F_{c}^{b}=F_{c}(R_{b},R_{a})$ . The objective scores are computed by: $\\begin{array}{r}{s=\\frac{1}{N}{\\sum}_{i=1}^{N}\\thinspace\\mathbb{1}(L(F_{c}^{a},F_{c}^{b}))}\\end{array}$ , where $\\mathbb{1}\\to\\{0,1\\}$ is the indicator function. $L(F_{c}^{a},F_{c}^{b})$ is true if and only if $F_{c}^{a}\\neq F_{c}^{b}$ and $F_{c}^{a},F_{c}^{b}$ align with ground-truth preference label. $N$ is the number of test samples. ", "page_idx": 5}, {"type": "text", "text": "Correction evaluation is only conducted on math reasoning and coding tasks since the revision could be easily verified with the ground-truth answers and the test cases [4, 9]. Thus, the objective evaluation metric is implemented as the pass rate: NNPas s, where N and NPass are the number of the total samples and passed samples, respectively. ", "page_idx": 5}, {"type": "text", "text": "5.2 Subjective Evaluation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The subjective evaluation aims to evaluate the quality of the generated textual critiques. Since responses in math reasoning and coding tasks can be verified, we only conduct subjective evaluations on other 5 tasks for the correction dimension. In our work, GPT-4 evaluates the generated critiques by generating the chain of thought followed by the score, with our human-annotated critiques as references. It is well-known that LLMs prefer longer generations during their automatic evaluation [46, 45]. However, Figure 7 in Appendix J proves that there is no clue that GPT-4, with our concise and precise reference critiques as input, would give higher scores to longer critiques. The subjective scores range from 1 to 10. Following previous work [20], the human-annotated reference critiques are anchored to 8, serving as a relative scoring pivot. ", "page_idx": 5}, {"type": "text", "text": "5.3 Overall Score ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The overall score of subjective and objective evaluation is computed as averaging on all the critique dimensions, and more details about computing the overall score can be found in Appendix K. ", "page_idx": 5}, {"type": "text", "text": "6 Evaluation and Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The critique abilities of representative LLMs are analyzed in this section, and the overview results are shown in Table 2. Firstly, the reliability of evaluation in CRITICEVAL are proven in Section 6.2 and Section 6.3. Then, overall analysis is described in Section 6.4. Furthermore, several intriguing phenomena about some critical factors are described, including task types (Section 6.5), the response quality (Section 6.6) and the critique dimensions (Section 6.7). Finally, we elaborate and analyze the fine-grained error patterns of model-generated critiques in Section 6.8. The complete experimental results of all evaluated LLMs on the test/dev set for each task and each critique dimension are placed in Appendix L. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "6.1 LLMs to be Evaluated ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "35 widely used open-source and closed-source LLMs of different sizes are evaluated on CRITICEVAL, including the instruction-tuned LLMs [21, 50, 22, 51], critique-tuned LLMs that fine-tuned on critique datasets generated by GPT-4 [11, 12, 30], and reward models [12, 52, 53]. Please refer to Appendix N for all evaluated LLMs and the inference details. The prompt templates for LLMs on critique dimensions are shown in Appendix I with score rubrics listed in Figure 18 in Appendix H.3. ", "page_idx": 6}, {"type": "text", "text": "6.2 Reliability of Subjective Evaluation in CRITICEVAL ", "text_level": 1, "page_idx": 6}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/ce32d38c63195489d7cc8518ca1d603d306ae3cd4aca43d10fa7a94fd1d8b288.jpg", "table_caption": ["Table 3: Results of meta-feedback dimension in CRITICEVAL dev set. $p$ -value $<0.05$ . "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "As LLMs are prompted with human-annotated critiques, their performance in meta-feedback could reveal their reliability for evaluating generated textual critiques. As shown in Table 2 $(F_{s}(F_{s})$ column) and Table 3, GPT-4-turbo achieves very high correlations (62.90, and 66.18) with human judgment. Although there is still a gap compared to the average human level ( $66.18<79.03)$ ), the strong correlations ensure the reliable evaluation for textual critique ability [23]. Furthermore, we also conduct the ablation study to prove the contribution of our human-annotated reference critiques. As shown in Table 3, it can be found that all LLMs perform worse when the reference critiques (ref.) are removed (average -13.36 performance decrease), proving their significant contribution for reliable subjective evaluation in our proposed CRITICEVAL. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/6aafa4f85f44820964bcddc23c132a52f55b6c187f85d43c9c3f14a44340197b.jpg", "table_caption": ["Table 4: Correlations in $_{C R}$ and $F_{c}$ dimensions. $p$ -value $<0.05$ . "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Moreover, except for the feedback critique dimension, we also test the reliability of subjective evaluation on the correction and comparison critique dimensions. Specifically, we ask three human annotators to annotate the quality score of 450 critiques generated by five representative LLMs (GPT-3.5-turbo, Qwen72B-Chat, InternLM2-20B-Chat, Mistral-7B and ChatGLM3- 6B) from 9 tasks in CRITICEVAL, and all the human annotators are guided by the same evaluation protocol in our subjective evaluation. The results are shown in Table 4. It can be found that GPT-4-turbo, with our human-annotated critiques as references, could achieve a very strong correlation with human judgments, close to the average human level. This observation proves the robust and reliable subjective evaluation of the textual critiques in the correction and comparison dimensions. Besides, the correlation scores on the correction and comparison critique dimension are higher than the feedback dimension. This phenomenon suggests that the feedback of the feedback is more challenging than the feedback of correction and comparison. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "6.3 More Effective Critiques Consistently Lead to Superior Corrections ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Table 5: The quality of corrections $C R$ increases as the quality of feedback increases. ", "page_idx": 6}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/a0871f3190ef3835c7956b9a529e05b0af76bc4f31e63f66f66a737df66970da.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Although the reliability of subjective evaluation has been proven in Section 6.2, it is still unknown whether real feedback critique ability of LLMs is consistent with the evaluation results in CRITICEVAL. To explore this, we prompt the InternLM2-20B-Chat and Llama2-70B-Chat models to revise responses from CRITICEVAL using three sources of feedback with varying quality levels. As illustrated in Table 5, a clear and consistent trend emerges: as the quality of the feedback increases, both the objective and subjective revision performance improves. This finding underscores that real critique ability of LLMs aligns closely with the evaluation results in our proposed CRITICEVAL, i.e., critiques of LLMs with higher scores are more accurate and effective for corrections. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "In summary, the reliability of evaluation in CRITICEVAL has been well proven. Following sections will describe the overall analysis and relationships between critique ability and several crucial factors. ", "page_idx": 7}, {"type": "text", "text": "6.4 Overall Analysis of LLMs ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As shown in Table 2, GPT-4 significantly outperforms other LLMs on most critique dimensions, while slightly underperforms our human-annotated critiques $(7.81<8)$ ). Surprisingly, open-source LLMs are approaching state-of-the-art closed-source LLMs. For example, InternLM2-20B-Chat surpasses GPT-3.5-turbo on overall subjective scores $(6.20>5.89)$ . Furthermore, there is a clear relationship that the critique ability of LLMs improves steadily as the number of parameters increases (Table 2), suggesting that the critique ability of LLMs highly correlates with their capability. ", "page_idx": 7}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/d35daf7a89ba88d390cb2831858b0bbd24a46e2dc588007eabaf831dcf78782e.jpg", "table_caption": ["Table 6: Critique-tuned LLMs results in feedback dimension. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "We also provide a clear diagram to show this relationship in Figure 14 in Appendix R. Beyond the average scores, we also categorize the textual critiques of LLMs into multiple quality intervals for more interpretable analysis, which are described in Appendix O. ", "page_idx": 7}, {"type": "text", "text": "The results of critique-tuned LLMs in the feedback dimension on the test set are shown in Table 6. It can be found that critiquetuned LLMs fine-tuned from Llama-2-13B significantly outperform even the Llama-2-70B-Chat model, proving the effectiveness of critiques datasets [11, 12]. The results of representative reward models are shown in Table 7. From these results, it can be found that reward models like UltraRM-13B achieve impressive performance in scoring the quality of responses, significantly outperforming GPT-3.5-turbo. This observation aligns with findings in recent works [14]. ", "page_idx": 7}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/b84ea25e77ecc12f2f78a92129bd16e157ffc1724ff53353fb2dc80663d0e12b.jpg", "table_caption": ["Table 7: Reward model objective results in $F_{s}$ and $F_{c}$ dimensions. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "6.5 Relationship with Task Type ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Effective critiques usually require domain knowledge and understanding of given tasks. We analyze the relationship between critique ability and task type in Table 8, which shows the average performance of all evaluated LLMs. ", "page_idx": 7}, {"type": "text", "text": "Feedback, Comparison LLMs achieve much higher scores in the first five tasks than on math reasoning and coding tasks, indicating math reasoning and code tasks are more challenging. ", "page_idx": 7}, {"type": "text", "text": "Meta-Feedback LLMs achieve much higher consistency with human judgments on code and math reasoning tasks, indicat", "page_idx": 7}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/2ec6ad88732fc5bdea497bbf1e5507337f510082644a49c5d79c73b3aa35cf58.jpg", "table_caption": ["Table 8: Two Avg. rows represent the average scores of all LLMs on the first 5 tasks and the last 4 tasks. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "ing that evaluating textual critiques in math reasoning and code tasks is more reliable. ", "page_idx": 7}, {"type": "text", "text": "Correction Math reasoning tasks are more challenging than coding tasks, and CodeExec is easier to revise than CodeNE due to the richer information in execution results. Except for math reasoning and coding tasks, the translation is the most challenging task because professional domain knowledge is required, while harmlessness is the easiest to refine since most LLMs have been trained to avoid harmful generations [35]. Furthermore, we explore the variance in correction quality on reasoning and coding tasks (Obj. in Table 9) ", "page_idx": 7}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/6079eb6251aecdf65910a8453167e8077e930f73d17de99512314530e03ec896.jpg", "table_caption": ["Table 9: Average performance of evaluated LLMs on test set. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "and other subjective tasks (Sub. in Table 9).5Specifically, three kinds of feedback are used for correction: (1) Human-annotated Feedback (HF); (2) Empty Feedback (EF), where LLMs are prompted to improve responses without any feedback; and (3) LLMs Self-generated Feedback (SF). As shown in Table 9, it can be found that self-generated feedback is beneficial to corrections on subjective evaluation $(\\mathrm{HF}(7.12)\\,>\\,\\mathrm{SF}(5.48)\\,>\\,\\mathrm{EF}(5.16))$ , while it might negatively affect corrections on objective evaluation of math reasoning and coding tasks $({\\mathrm{SF}}(13.01)<{\\mathrm{EF}}(14.44)<{\\mathrm{HF}}(43.66))$ . This observation proves that LLMs struggle in self-improvement on challenging reasoning tasks, aligning with recent findings [54, 4]. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6.6 Relationship with Response Quality ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Before analyzing the relationship between response quali- Table 10: Error pattern distribution $(\\%)$ . ties and critique ability, it is essential to categorize the error ", "page_idx": 8}, {"type": "text", "text": "patterns in responses. We highlight that the error patterns are related to the task type, complicating the classification of errors. To conduct a representative analysis of errors in all tasks, human annotators are asked to categorize errors into three patterns, which collectively encompass nearly all the cases: (1) Obvious error is easy to critique and correct, like apparent misuses of words in translation task; (2) Complex error is challenging to correct, regardless of whether critiques are easy to critique, like logical reasoning error in reasoning tasks; (3) Subtle error is hard to critique, while it is usually easier to revise than complex error, like slight misunderstandings of context in general chat. The distribution presented in Table 10 reveals distinct primary errors across different response qualities. More details about these error patterns in each task are described in Appendix P. ", "page_idx": 8}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/de3e7f73417ff504aa0bf5a4066cadfeaa4a5d2c4dd11ed4930c125992d17f36.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Given the distribution of error patterns, we analyze critique ability of LLMs on responses with varying qualities. As shown in Table 11, highquality responses are the hardest for feedback since they contain lots of subtle errors (Table 10). Note that the medium-quality responses have higher objective feedback scores than lowquality ones, which is inconsistent with our expectations. This phenomenon is because low", "page_idx": 8}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/2a7a81c58a06fe8d4c8404d0218a5e32c7bbe43f497a32c0fad9b5f8f47e6cf0.jpg", "table_caption": ["Table 11: Average performance of LLMs on the different response qualities (test set). "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "quality responses often receive very low human-annotated quality scores (near 1), while the scoring of LLMs tends to be higher, leading to a discrepancy. For the correction dimension, low-, and high-quality responses are easier to correct than medium-quality due to the most obvious and subtle errors. There are two kinds of qualities for comparison dimension: easy and hard. Most LLMs perform better on easy samples than on hard samples. Specifically, the subjective and objective scores of easy samples are 4.78 and 39.73, respectively, higher than those of hard samples (4.55 and 29.80). For the meta-feedback dimension, LLMs achieve the highest consistency with human judgments on high-quality responses while performing worst on medium-quality responses. ", "page_idx": 8}, {"type": "text", "text": "6.7 Relationship with Critique Dimensions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The average scores of all evaluated LLMs on different critique dimensions are shown in Table 12. Objective scores of comparison and correction are not recorded because they are not correlations. Several conclusions can be made: (1) correction is the easiest critique dimension, followed by feedback, and then comparison. This observation demonstrates that comparison requires accurate analysis of both responses, which is more complex than the feedback dimension; (2) As a high-level critique dimension, meta-feedback is more challenging than the feedback. ", "page_idx": 8}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/3d66eecc8dc2924172a597ca5f06be6ba937349d54c9bdd8357c1a81c8040688.jpg", "table_caption": ["Table 12: Average performance on test set. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "6.8 Fine-grained Failure Modes in Model-Generated Critiques ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This section analyzes the fine-grained failure modes in model-generated critiques across feedback, comparison and correction dimensions. As illustrated in Table 13, human annotators summarize the 12 main failure modes in model-generated critiques. Then, we compute the distribution of these failure modes of all evaluated LLMs. Figure 3 demonstrate that the most frequent failure modes are missing errors (E1, E2), lacing effective comparison analysis (E7) and worse revision than references (E10) for feedback, comparison and correction dimensions, respectively. Furthermore, as shown in Figure 4, it can be observed that missing errors/suggestions (E1, E2) and inaccurate critiques (E3, E4, E8) usually lead to lower subjective scores. ", "page_idx": 8}, {"type": "text", "text": "Table 13: Definition of Failure Modes in Feedback, Comparison and Correction critique dimensions. E1-E6 denotes the shared failure modes of feedback and comparison dimensions, and E7-E8 belong to comparison dimension. E9-E11 belong to the correction dimension. ", "page_idx": 9}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/86960830f1b0be36f440bdcbbadd044ea95f7eba18f47e2bbc597ab1e72cf90f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "ZsxZ65YqL1/tmp/8ff12567af5edf310bb6ab8eb380834eedcd192c1e0762ca4ef2176eaf25894b.jpg", "img_caption": ["Figure 4: Average subjective score of failure modes in each critique dimension. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduce a comprehensive and reliable benchmark for evaluating the critique abilities of LLMs, named CRITICEVAL. Extensive experimental results first prove the reliability of CRITICEVAL, and reveal the promising potential of open-source LLMs, the effectiveness of critique datasets and intriguing relationships between critique capabilities and some factors: task types, response qualities and critique dimensions. These observations significantly promote an in-depth understanding of the critical ability of LLMs and LLM\u2019s self-improvement. In the future, we plan to enhance our benchmark in several key areas: (1) Broadening the scope to include more tasks, such as tool-using; (2) Extending the benchmark to encompass other languages, like Chinese; (3) Improving the subjective evaluation protocol to allow for more fine-grained analysis; (2) Continue to evaluate LLMs and track their critique ability, like Llama-3 models; (5) Improving the quality of reference critiques by incorporating additional high-quality critiques from advanced LLMs if and only if their quality surpasses the existing reference critiques. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "First, the authors would like to express their sincere gratitude to all the anonymous reviewers and meta-reviewers for their insightful comments and constructive feedback. Besides, we would also like to thank many senior researchers for their valuable comments before our submission, which greatly improved the quality of our paper: Yan Wang, Yong Hu, Rongcheng Tu, Hongli Mao, Fanshu Sun and Chen Xu. The names are listed in no particular order. Furthermore, this project is funded in part by the Centre for Perceptual and Interactive Intelligence (CPII) Ltd under the Innovation and Technology Commission (ITC)\u2019s InnoHK. Dahua Lin is a PI of CPII under the InnoHK. This work is also supported by the National Natural Science Foundation of China (No. 62172039, U21B2009 and 62276110) and the MIIT Program (CEIEC-2022-ZM02-0247). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models, 2024.   \n[2] Weiwen Xu, Deng Cai, Zhisong Zhang, Wai Lam, and Shuming Shi. Reasons to reject? aligning language models with judgments, 2023.   \n[3] Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies, 2023.   \n[4] Zhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Nan Duan, and Weizhu Chen. CRITIC: Large language models can self-correct with tool-interactive critiquing. In The Twelfth International Conference on Learning Representations, 2024.   \n[5] Samuel R. Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamile\u02d9 Luko\u0161iu\u00afte\u02d9, Amanda Askell, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Christopher Olah, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, Jackson Kernion, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Liane Lovitt, Nelson Elhage, Nicholas Schiefer, Nicholas Joseph, Noem\u00ed Mercado, Nova DasSarma, Robin Larson, Sam McCandlish, Sandipan Kundu, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Timothy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, and Jared Kaplan. Measuring progress on scalable oversight for large language models, 2022.   \n[6] William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting human evaluators, 2022.   \n[7] Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and Andr\u00e9 FT Martins. Results of wmt22 metrics shared task: Stop using bleu\u2013neural metrics are better and more robust. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 46\u201368, 2022.   \n[8] Alexander R Fabbri, Wojciech Kry\u00b4sci\u00b4nski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. Summeval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:391\u2013409, 2021.   \n[9] Liangchen Luo, Zi Lin, Yinxiao Liu, Lei Shu, Yun Zhu, Jingbo Shang, and Lei Meng. Critique ability of large language models, 2023.   \n[10] Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean O\u2019Brien, Ramakanth Pasunuru, Jane DwivediYu, Olga Golovneva, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. Shepherd: A critic for language model generation, 2023.   \n[11] Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, hai zhao, and Pengfei Liu. Generative judge for evaluating alignment. In The Twelfth International Conference on Learning Representations, 2024.   \n[12] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback, 2023.   \n[13] Shichao Sun, Junlong Li, Weizhe Yuan, Ruifeng Yuan, Wenjie Li, and Pengfei Liu. The critique of critique, 2024.   \n[14] Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. Prometheus 2: An open source language model specialized in evaluating other language models. arXiv preprint arXiv:2405.01535, 2024.   \n[15] Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying Peng, Jun Wang, Yueting Zhuang, and Weiming Lu. Self-contrast: Better reflection through inconsistent solving perspectives, 2024.   \n[16] Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, and Yujiu Yang. Criticbench: Benchmarking llms for critique-correct reasoning, 2024.   \n[17] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.   \n[18] Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire, 2023.   \n[19] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 2023.   \n[20] Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, Xiaohan Zhang, Lichao Sun, Hongning Wang, Jing Zhang, Minlie Huang, Yuxiao Dong, and Jie Tang. Alignbench: Benchmarking chinese alignment of large language models, 2023.   \n[21] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.   \n[22] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, and Dahua Lin. Internlm2 technical report, 2024.   \n[23] Tian Lan, Xian-Ling Mao, Wei Wei, Xiaoyan Gao, and Heyan Huang. Pone: A novel automatic evaluation metric for open-domain generative dialogue systems. ACM Trans. Inf. Syst., 39(1), ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "nov 2020. ", "page_idx": 11}, {"type": "text", "text": "[24] Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Wang, and Lei Li. INSTRUCTSCORE: Towards explainable text generation evaluation with automatic feedback. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5967\u20135994, Singapore, December 2023. Association for Computational Linguistics.   \n[25] Junlong Li, Fan Zhou, Shichao Sun, Yikai Zhang, Hai Zhao, and Pengfei Liu. Dissecting human and llm preferences, 2024.   \n[26] Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, and Yue Zhang. Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization, 2023.   \n[27] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg evaluation using gpt-4 with better human alignment, 2023.   \n[28] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.   \n[29] Pei Ke, Bosi Wen, Zhuoer Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, Jie Tang, and Minlie Huang. Critiquellm: Scaling llm-as-critic for effective and explainable evaluation of large language model generation, 2023.   \n[30] Dongfu Jiang, Yishan Li, Ge Zhang, Wenhao Huang, Bill Yuchen Lin, and Wenhu Chen. Tigerscore: Towards building explainable metric for all text generation tasks. arXiv preprint arXiv:2310.00752, 2023.   \n[31] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback, 2023.   \n[32] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023.   \n[33] Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, Andr\u00e9 Martins, Graham Neubig, Ankush Garg, Jonathan Clark, Markus Freitag, and Orhan Firat. The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors, Proceedings of the Eighth Conference on Machine Translation, pages 1066\u20131083, Singapore, December 2023. Association for Computational Linguistics.   \n[34] Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash. Rlaif: Scaling reinforcement learning from human feedback with ai feedback, 2023.   \n[35] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional ai: Harmlessness from ai feedback, 2022.   \n[36] Yifan Xu, Xiao Liu, Xinghan Liu, Zhenyu Hou, Yueyan Li, Xiaohan Zhang, Zihan Wang, Aohan Zeng, Zhengxiao Du, Wenyi Zhao, Jie Tang, and Yuxiao Dong. Chatglm-math: Improving math problem-solving in large language models with a self-critique pipeline, 2024.   \n[37] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.   \n[38] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model, 2023.   \n[39] Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback. In NeurIPS, 2020.   \n[40] Lucia Specia, Fr\u00e9d\u00e9ric Blain, Marina Fomicheva, Erick Fonseca, Vishrav Chaudhary, Francisco Guzm\u00e1n, and Andr\u00e9 F. T. Martins. Findings of the WMT 2020 shared task on quality estimation. In Lo\u00efc Barrault, Ond\u02c7rej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-juss\u00e0, Christian Federmann, Mark Fishel, Alexander Fraser, Yvette Graham, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Andr\u00e9 Martins, Makoto Morishita, Christof Monz, Masaaki Nagata, Toshiaki Nakazawa, and Matteo Negri, editors, Proceedings of the Fifth Conference on Machine Translation, pages 743\u2013764, Online, November 2020. Association for Computational Linguistics.   \n[41] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In EMNLP, 2018.   \n[42] Jerrold Zar. Spearman Rank Correlation, volume 5. 07 2005.   \n[43] Chongyang Tao, Lili Mou, Dongyan Zhao, and Rui Yan. Ruber: An unsupervised method for automatic evaluation of open-domain dialog systems. In AAAI Conference on Artificial Intelligence, 2017.   \n[44] Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators, 2023.   \n[45] Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. Evaluating large language models at evaluating instruction following. In The Twelfth International Conference on Learning Representations, 2024.   \n[46] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. How far can camels go? exploring the state of instruction tuning on open resources. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.   \n[47] Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, and Lei Hou. Benchmarking foundation models with language-model-as-an-examiner. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.   \n[48] InternLM Team. Internlm: A multilingual language model with progressively enhanced capabilities. https://github.com/InternLM/InternLM, 2023.   \n[49] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u00e9lio Renard Lavaud, Lucile Saulnier, ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mixtral of experts, 2024. ", "page_idx": 14}, {"type": "text", "text": "[50] DeepSeek-AI. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. ", "page_idx": 14}, {"type": "text", "text": "[51] OpenAI. Gpt-4 technical report, 2023.   \n[52] Junjie Wang, Yuxiang Zhang, Lin Zhang, Ping Yang, Xinyu Gao, Ziwei Wu, Xiaoqun Dong, Junqing He, Jianheng Zhuo, Qi Yang, Yongfeng Huang, Xiayu Li, Yanghan Wu, Junyu Lu, Xinyu Zhu, Weifeng Chen, Ting Han, Kunhao Pan, Rui Wang, Hao Wang, Xiaojun Wu, Zhongshen Zeng, Chongpei Chen, Ruyi Gan, and Jiaxing Zhang. Fengshenbang 1.0: Being the foundation of chinese cognitive intelligence. CoRR, abs/2209.02970, 2022.   \n[53] Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with $\\nu$ -usable information. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 5988\u2013 6008. PMLR, 17\u201323 Jul 2022.   \n[54] Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. In The Twelfth International Conference on Learning Representations, 2024.   \n[55] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. Tool learning with foundation models, 2023.   \n[56] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt\u00e4schel, and Sebastian Riedel. Kilt: a benchmark for knowledge intensive language tasks, 2021.   \n[57] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. Siren\u2019s song in the ai ocean: A survey on hallucination in large language models, 2023.   \n[58] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024.   \n[59] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. Sglang: Efficient execution of structured language model programs, 2024.   \n[60] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback, 2022.   \n[61] Kotaro Hara, Abigail Adams, Kristy Milland, Saiph Savage, Chris Callison-Burch, and Jeffrey P Bigham. A data-driven analysis of workers\u2019 earnings on amazon mechanical turk. In Proceedings of the 2018 CHI conference on human factors in computing systems, pages 1\u201314, 2018.   \n[62] Giulio Zhou and Gerasimos Lampouras. Webnlg challenge 2020: Language agnostic delexicalisation for multilingual rdf-to-text generation. In Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), pages 186\u2013191, 2020.   \n[63] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. ArXiv, abs/2110.14168, 2021.   \n[64] Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, and Yejin Choi. The unlocking spell on base llms: Rethinking alignment via in-context learning, 2023.   \n[65] Jian Guan, Zhexin Zhang, Zhuoer Feng, Zitao Liu, Wenbiao Ding, Xiaoxi Mao, Changjie Fan, and Minlie Huang. Openmeva: A benchmark for evaluating open-ended story generation metrics. arXiv preprint arXiv:2105.08920, 2021.   \n[66] Fran\u00e7ois Mairesse, Milica Gasic, Filip Jurcicek, Simon Keizer, Blaise Thomson, Kai Yu, and Steve Young. Phrase-based statistical language generation using graphical models and active learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1552\u20131561, 2010.   \n[67] Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. Commongen: A constrained text generation challenge for generative commonsense reasoning. arXiv preprint arXiv:1911.03705, 2019.   \n[68] Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, and Minjoon Seo. FLASK: Fine-grained language model evaluation based on alignment skill sets. In The Twelfth International Conference on Learning Representations, 2024.   \n[69] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149\u20134158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.   \n[70] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020.   \n[71] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. ACL, 2017.   \n[72] Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. MathQA: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2357\u20132367, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.   \n[73] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.   \n[74] Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral, and Ashwin Kalyan. Numglue: A suite of fundamental yet challenging mathematical reasoning tasks. ACL, 2022.   \n[75] Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. TheoremQA: A theorem-driven question answering dataset. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7889\u20137901, Singapore, December 2023. Association for Computational Linguistics. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "[76] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. ", "page_idx": 16}, {"type": "text", "text": "[77] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021.   \n[78] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. A survey on large language model based autonomous agents, 2023.   \n[79] Nat McAleese, Rai Michael Pokorny, Juan Felipe Ceron Uribe, Evgenia Nitishinskaya, Maja Trebacz, and Jan Leike. Llm critics help catch llm bugs, 2024.   \n[80] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1\u201338, March 2023.   \n[81] Rajani Nazneen, Lambert Nathan, Sheon Han, Wang Jean, Nitski Osvald, Beeching Edward, and Tunstall Lewis. Can foundation models label data like humans? Hugging Face Blog, 2023. https://huggingface.co/blog/llm-v-human-data.   \n[82] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023.   \n[83] LMDeploy Contributors. Lmdeploy: A toolkit for compressing, deploying, and serving llm. https://github.com/InternLM/lmdeploy, 2023.   \n[84] Baichuan. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023.   \n[85] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, and Daxin Jiang. WizardLM: Empowering large pre-trained language models to follow complex instructions. In The Twelfth International Conference on Learning Representations, 2024.   \n[86] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90\\%*$ chatgpt quality, March 2023.   \n[87] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022.   \n[88] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback, 2024.   \n[89] Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Language models are multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations, 2023. ", "page_idx": 16}, {"type": "text", "text": "A Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A.1 Sub-optimal Reference Critiques ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Following previous work [20], our work construct CRITICEVAL with the human-in-the-loop annotation pipeline, i.e., multiple human annotators are asked to review and revise the critiques generated by GPT-4 model. Even though we have established a rigorous annotation process to ensure the high quality of annotated critiques, human annotators are inevitably influenced by GPT-4\u2019s initial generated critiques in some more open-ended tasks, like general chat and QA tasks. This problem may result in the quality of revised critiques by human annotation still being sub-optimal. To address this problem, we plan to enhance CRITICEVAL in our next version. Specifically, we will replace existing reference critiques with potential better critiques generated by advanced evaluated LLMs, like Claude- $3^{6}$ , if and only if their qualities surpasses reference critiques. ", "page_idx": 17}, {"type": "text", "text": "A.2 GPT-4 Model for Subjective Evaluation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "CRITICEVAL mainly utilizes the advanced GPT-4-turbo model for subjective evaluation. Despite integrating high-quality annotated reference critiques to guide GPT-4 toward more accurate assessments, it\u2019s essential to acknowledge that the model\u2019s evaluations may not always align perfectly with human judgment. While GPT-4 have yet to reach the level of precision of human annotation - they currently represent the most effective approach for balancing the trade-offs between evaluation cost and quality. It is still a significant challenge to accurately and automatically evaluate critiques across all scenarios. Recognizing this, we aim to address these issues in our future work by progressively refining our benchmark and evaluation protocols. ", "page_idx": 17}, {"type": "text", "text": "A.3 More Tasks for Critique ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Compared with existing benchmarks [12, 10, 14, 13], our proposed CRITICEVAL exhibits significant advantages in the diversity of evaluated task scenarios. Although we strive to cover a wide range of diverse generation tasks, there are still some tasks have yet to be considered, such as tool learning [55], knowledge-intensive tasks [56] and hallucination [57]. In our future work, we will continue to include more tasks in the next version of CRITICEVAL. ", "page_idx": 17}, {"type": "text", "text": "A.4 More LLMs to be Evaluated ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Some newly proposed LLMs and reward models [58] have not been added yet, like Llama- $\\cdot3^{7}$ and Claude-3 series models. Since our conclusions are summarized from evaluation results of over 35 open-source and closed-source LLMs, lacking these LLMs does not affect conclusions. We will continue to evaluate their critique ability in our future work. ", "page_idx": 17}, {"type": "text", "text": "A.5 Limited Inference Strategy ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this paper, all the evaluated models generate the critiques by using the greedy-search decoding method. There exist some inference strategies to potentially improve the model\u2019s performance, like structured generation [59]. The primary goal of our proposed CRITICEVAL in the current stage is to construct a comprehensive and reliable benchmark for evaluating the critique ability of LLMs, and we will explore these inference strategies to improve the critique ability in our future work. ", "page_idx": 17}, {"type": "text", "text": "B Ethical Considerations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Most task inputs in CRITICEVAL are collected from publicly available datasets, free from any possible harm toward individuals or groups. Moreover, humans carefully select and process the responses and critiques generated by LLMs to secure privacy and confidentiality. No personal identification information is involved. However, it should be noted that the task input, responses, and critiques in the Anthropic-HHH dataset [60] of the harmlessness task contain harmful materials and hate speech. Despite the risks involved, it is essential to disclose this research fully, and materials in the Anthropic-HHH dataset have been widely used for safety research in the LLM community. All raters have been paid adequate wages. The hourly wage of our human annotators is about 5.69 USD, which is much higher than average hourly wage 3.13 USD on Amazon Mechanical Turk [61]. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "C Comparison and Statistics of CRITICEVAL ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The comparison between CRITICEVAL and existing benchmarks can be found in Table 14, which proves the advantages of our proposed CRITICEVAL for critique evaluation. Compared with existing benchmarks for critique evaluation, our proposed CRITICEVAL contains 3, 608 textual natural language critique samples (textual critiques) that are well annotated by multiple human annotators, leading to a more stable and reliable assessment in our subjective evaluation. The scale of objective evaluation data (scalar data) in our dataset is second only to Chat Arena8 and SummEval [8]. However, compared to these two datasets, our dataset contains a more diverse range of critique dimensions and tasks (nine diverse tasks). Moreover, the statistics of CRITICEVAL in the test and dev set are shown in Table 15. ", "page_idx": 18}, {"type": "text", "text": "Table 14: Statistics of existing critique benchmarks, meta-evaluation benchmarks (scalar-valued critique evaluation), and CRITICEVAL. NL and Scalar denote natural language feedback and scalarvalued feedback, i.e., the preference label or Likert score [3]. CriticBench [9, 16] contain two kinds of response quality (correct and wrong). The responses in some benchmarks are not unclassified, and we set them as unclassified (-). Scalar-valued critiques in Auto-J [11] are from its Eval-P, and textual critiques are from Eval-C split. ", "page_idx": 18}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/eec3391fb53776c7bcef8704eb07c59f3fd4ca9217546008b143306b6c834663.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "D Source Data for Different Tasks ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The benchmark includes three representative classical language tasks: summary [39], translation [40], and question-answering [41]. Since a popular application of LLMs is to serve as a chatbot, where alignment is important to ensure the safe application of LLMs, we collect instructions from general chat scenarios [19] and harmlessness cases [35] to evaluate the LLMs\u2019 critique ability for alignment. Furthermore, the reasoning and code capabilities are also fundamental for augmenting LLMs as agents [78], another important and promising application of LLMs. Thus, we also collect instructions for math reasoning with chain-of-thought and program-of-thought, and coding with and without execution results. ", "page_idx": 18}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/890bac599e48e126e5d958b21944a94ed384c1ebb983896286bb7b0f30e05096.jpg", "table_caption": ["Table 15: The statistics of the test and dev set in our proposed CRITICEVAL. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "To ensure the difficulty of CRITICEVAL, we only collect coding and math reasoning questions that some 70B LLMs cannot correctly answer, which is proven effective in previous works [9]. Our motivation is to collect questions that could easily raise responses with diverse flaws. Simple questions pose challenges for us in achieving this goal since most LLMs can easily solve them. Collecting these questions that 70B LLMs cannot answer correctly makes the difficulty of questions become moderate or complex, which aligns with our motivation. ", "page_idx": 19}, {"type": "text", "text": "The details of selected datasets for 9 tasks are listed in Table 16, covering ", "page_idx": 19}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/7e339fde4f60c1df4fe47dd69f0e48953e6336464a3d1cf154e2970c525bca7b.jpg", "table_caption": ["Table 16: Source of 9 tasks in CRITICEVAL. Most tasks contain diverse samples from multiple test sets. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "the well-known NLP tasks (translation, summary, and question answering), reasoning tasks (mathematics and coding), and alignment (general chat and harmlessness). These datasets\u2019 test sets are used for CRITICEVAL construction, avoiding data contamination. For each task, we collect around 100 instructions from the test sets of some widely-used benchmark datasets to ensure the instruction quality and avoid data contamination. ", "page_idx": 19}, {"type": "text", "text": "Our dataset is under Apache 2.0 License. ", "page_idx": 19}, {"type": "text", "text": "E List of Used LLMs for Response and Critique Generation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Our study uses several LLMs with different capabilities to generate diverse feedback, listed in Table 17. Besides, we also use some critique-tuned LLMs to generate textual feedback, like Auto-J-13B and UltraCM-13B models. ", "page_idx": 19}, {"type": "text", "text": "F Details of Responses Generation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "F.1 How to Collect Low-, Medium-, High-quality Responses ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To identify the quality of these responses efficiently, GPT-4 is utilized to initially assign quality ratings ranging from 1 to 7 (Step 2 (b) in Figure 2) then let human annotators meticulously review and adjust these scores, which are used in the objective evaluation in the feedback dimension (Section 5.1). then, three responses with distinct quality differences for each $I$ are sampled based on their human-verified quality scores, including low-, medium-, and high-quality responses (noted as $R_{\\mathrm{low}},R_{\\mathrm{med}},R_{\\mathrm{high}}$ , respectively). Due to partial simple or hard queries, there might be queries where the scores of three responses are close. However, there is a distinct quality difference between low, medium, and high-quality responses overall. ", "page_idx": 19}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/2687aa05e223d4bff09de140db3aa84825a7bf869567fbb142342f9d2147877c.jpg", "table_caption": ["Table 17: The list of used LLMs for generating responses and critiques. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "The statistical of responses\u2019 quality scores on 9 tasks can be found in the Figure 5. Figure 5 demonstrates the discernible performance disparities in responses for each task. Since automatic execution leaks quality information, we do not collect the correct responses for the CodeExec task. Such variation is instrumental in analyzing the impact of response quality on the feedback. ", "page_idx": 20}, {"type": "text", "text": "F.2 Correct or Golden Response Generation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Golden or correct responses are collected for each task input $I$ , which are proven challenging for critiques [10, 15]. We use GPT-4 to generate correct responses using ground-truth rationales or codes as hints for coding and mathematical tasks. Since executions leak information about ", "page_idx": 20}, {"type": "image", "img_path": "ZsxZ65YqL1/tmp/af936697c519c54b2d77e994c44a4c251e5e12e4fc35aeb9883a76aa434c3916.jpg", "img_caption": ["Figure 5: Human annotated Likert scores (1-7). "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "response quality, correct responses are not collected for the CodeExec task. In tasks beyond coding and mathematics, GPT-4 is prompted to refine its past generations, given its feedback during multiple turns, and the last revision is collected as golden response. ", "page_idx": 20}, {"type": "text", "text": "G Reasons of Utilizing Human-in-the-Loop Data Generation Pipeline ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We construct the high-quality reference critiques by using human-in-the-loop data generation pipeline, which is motivated by two essential considerations (effectiveness and efficiency). ", "page_idx": 20}, {"type": "text", "text": "Human-written Critiques are Usually Insufficient - Sub-optimal (Effectiveness) Our trial human annotation reveals that human annotators might neglect some apparent or severe issues when writing critiques from scratch, consistent with findings in recent studies from OpenAI [79, 6]. Our experimental result in Appendix M demonstrate that neglecting issues usually leads to low-quality critiques. In contrast, despite the possibility of generating wrong critiques, LLMs like GPT-4 offer more comprehensive and detailed critiques [79]. By revising LLM\u2019s errors by human experts, the final critiques could be more comprehensive and accurate, leveraging the strengths of both human annotators and LLMs. ", "page_idx": 20}, {"type": "text", "text": "Annotating Challenging Critique Task from Scratch Cost A Lot (Efficiency) Writting critiques from scratch is a significant challenge [20, 10]. For instance, Shepherd [52] incurred an annotation cost of $8\\Phi$ per sample, leading to over $28{,}8645$ and 1,350 work hours to annotate the entire CRITICEVAL, which is unbearable for our project. Thus, we have to employ advanced LLMs to generate draft critiques, followed by human annotation. GPT-4 was chosen because our preliminary studies indicate that it is the most reliable LLM for producing draft critiques, while other LLMs are much worse. Consequently, a diverse set of LLMs introduces more noise in generated critiques, bringing more difficulties to human annotators. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "In conclusion, the human-in-the-loop pipeline achieves the trade-off between annotation cost and quality. We promise to add these details to the revised paper to emphasize the motivation of using the human-in-the-loop pipeline. ", "page_idx": 21}, {"type": "text", "text": "H Human Annotation Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "H.1 Evaluation Protocol ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Three to five human annotators annotate each scalar-valued critique in each task. Biases among human annotators may arise from factors such as the annotator\u2019s gender and professional background. To minimize the impact of annotators\u2019 biases on the quality of our human annotations, we first selected a diverse group of annotators from a crowd-sourcing platform to annotate each critique sample collaboratively. All human annotators have been paid adequate wages. The hourly wage of our human annotators is about 5.69 USD, which is much higher than average hourly wage 3.13 USD on Amazon Mechanical Turk [61]. ", "page_idx": 21}, {"type": "text", "text": "Before annotation, we designed a rigorous data annotation verification process to iteratively train these annotators, thereby ensuring the stability and reliability of the annotation quality. During annotation, these human annotators first annotate each textual critique, which are summarized by another supervisor annotator. After annotation, the supervisors (authors in our paper) conducted a $5\\%$ sample inspection. If the error rate exceeds the threshold, annotators are asked to revise their work until the error rate is lower than the threshold. ", "page_idx": 21}, {"type": "text", "text": "H.2 Inspect Errors in Datasets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "It should be noted that some underlying datasets contain inaccuracies that may lead to compounding effects during evaluation. For example, we have noticed some incorrect solutions and rationales for mathematics and coding questions during our human evaluation process. In our work, to mitigate the effects of such errors, human annotators are asked to meticulously examine each question, the provided golden answers (only for mathematics and coding tasks) and the evaluated responses and critiques. They are asked to exclude instances where the golden answers or questions are flawed or incorrect, like wrong solutions to mathematics and coding questions. ", "page_idx": 21}, {"type": "text", "text": "H.3 Score Rubrics for Different Tasks ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The annotators are entrusted with the detailed score rubrics to evaluate the different dimensions [14]. Table 18 lists the score rubrics designed for different tasks. Note that math and code tasks only need to check the correctness. ", "page_idx": 21}, {"type": "text", "text": "H.4 Internet Search ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Task inputs in the QA and chat tasks often require specific factual knowledge for responses. However, GPT-4 sometimes produces spurious knowledge or fails to effectively identify factual inaccuracies within these responses, a common issue known as hallucination [80, 57]. Consequently, we strongly urge human annotators involved in the feedback, comparison and correction annotations in QA and general chat tasks to verify factual content through internet searches. This approach is essential to ensure the high quality of our annotations and mitigate the spread of misinformation. ", "page_idx": 21}, {"type": "text", "text": "H.5 Exclude Mention of Ground-Truth ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To generate correct responses $R_{\\mathrm{corr}}$ . and critiques for challenging tasks, like mathematics and coding, we provide the ground-truth rationales as reference for GPT-4. Then, these generated responses and ", "page_idx": 21}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/e4ccd0afae62673a1404586b613d5bf4b60bc246c5a7e11b411a076616f3d041.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "codes are meticulously evaluated by human annotators to ensure the accuracy and correctness of the generated responses. Human annotators are asked to exclude any direct references of ground-truth rationals in these generated $R_{\\mathrm{corr}}$ . and critiques. ", "page_idx": 22}, {"type": "text", "text": "H.6 Statistics of Human Annotation on Reference Critiques ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The average revision rates of textual critiques for the feedback, comparison, and correction critique dimensions are $25.22\\%$ , $34.83\\%$ , and $48.37\\%$ , respectively, indicating that GPT-4\u2019s textual feedback is more likely to be accepted by human annotators, followed by the comparison dimension. In contrast, GPT-4-turbo\u2019s correction might contain more flaws that need to be corrected. Moreover, the average inner correlation score among annotators is 0.79, indicating solid inner agreement. ", "page_idx": 22}, {"type": "text", "text": "I Case Study ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "This section shows some cases in CRITICEVAL, subjective evaluation of feedback, comparison and correction critique dimensions. ", "page_idx": 23}, {"type": "text", "text": "I.1 Examples in CRITICEVAL ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this subsection, we have provided some cases about four critique dimensions: (1) feedback; (2) comparison; (3) correction; and (4) meta-feedback. From these cases, it can be found that samples in our proposed CRITICEVAL contains rich metadata. For example, the data source, quality, and used LLMs are listed. ", "page_idx": 23}, {"type": "text", "text": "# Case of feedback and correction dimension ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\"question\": \"How do I make escargot?\",   \n\"generation\": \"Place the snails in a container and cover with damp cloth or $\\hookrightarrow$ put in a plastic bag. Store in the refrigerator overnight.\",   \n\"feedback\": \"The response does not effectively answer the question. The $\\hookrightarrow$ instruction was asking for a way to make escargot, which is a dish made $\\hookrightarrow$ from cooked land snails, usually served as an appetizer in French $\\hookrightarrow$ cuisine. The response only provides information on how to store snails, $\\hookrightarrow$ not how to prepare them for cooking or how to cook them. The response $\\hookrightarrow$ should contain steps on how to clean and prepare the snails, cooking $\\hookrightarrow$ ingredients, and the cooking process itself.\",   \n\"feedback_score\": \"2\",   \n\"correction\": \"First, you need to clean the snails. To do this, put them in a $\\hookrightarrow$ bowl with salted water and leave them for a couple of hours. After this, $\\hookrightarrow$ rinse them thoroughly. Next, prepare a garlic butter mixture. Combine a $\\hookrightarrow$ stick of softened butter, 2 minced garlic cloves, a small bunch of $\\hookrightarrow$ finely chopped parsley, a pinch of salt, and a dash of pepper in a bowl. $\\hookrightarrow$ Preheat your oven to $356\\,^{\\circ}\\mathsf{F}$ $.175^{\\circ}\\mathrm{C})$ . Take each snail and place it in a $\\hookrightarrow$ snail dish. Fill the remaining space in the dish with the garlic butter $\\hookrightarrow$ mixture. Bake the snails in the preheated oven for about 15 minutes, or $\\hookrightarrow$ until the butter is bubbly and slightly browned. Serve the escargot hot, $\\hookrightarrow$ with fresh bread on the side. Enjoy your homemade escargot!\",   \n\"data_source\": \"alpaca_eval\",   \n\"metadata\": {   \n\"quality\": \"low\",   \n\"llm_name\": \"internlm-7b\"   \n} ", "page_idx": 23}, {"type": "text", "text": "} # One case of comparison dimension ", "page_idx": 23}, {"type": "text", "text": "\"question\": \"Did they ever announce the release date for the new elder $\\hookrightarrow$ scrolls game?\",   \n\"generation_a\": \"The Elder Scrolls Online: Tamriel Unlimited was released on $\\hookrightarrow$ March 10, 2015.\",   \n\"generation_b\": \"I am a language model and I cannot access any specific $\\hookrightarrow$ information regarding the release date of the new Elder Scrolls game. $\\hookrightarrow$ However, if you check the official website or social media channels of $\\hookrightarrow$ the game, you might find an announcement about the release date. Please $\\hookrightarrow$ note that release dates are often subject to change, so it is always $\\hookrightarrow$ recommended to check the latest information directly from the   \n$\\hookrightarrow$ developers.\", \"feedback\": \"Preference: Response B Explanation: The instruction requires $\\hookrightarrow$ the release date of the new Elder Scrolls game. Response A provides a $\\hookrightarrow$ release date, but it's for the wrong game, \\\"The Elder Scrolls Online: $\\hookrightarrow$ Tamriel Unlimited\\\", not the new Elder Scrolls game. Response B, while $\\hookrightarrow$ not providing a specific release date, gives a more accurate answer by $\\hookrightarrow$ acknowledging its limitations as a language model and advising the user $\\hookrightarrow$ to check the official sources for the most up-to-date information. $\\hookrightarrow$ Therefore, Response B is preferred.\",   \n\"preference\": \"ansB\",   \n\"llm_a\": \"internlm-20b\",   \n\"llm_b\": \"baichuan2-13b\"   \n\"data_source\": \"alpaca_eval\",   \n\"metadata\": {   \n\"quality_a_sub\": \"low\",   \n\"quality_b_sub\": \"high\"   \n} ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "# One case of meta-feedback dimension { ", "page_idx": 24}, {"type": "text", "text": "\"question\": \"How do I detail a car?\",   \n\"generation\": \"To detail a car, firstly ensure that you have all the $\\hookrightarrow$ necessary tools and equipment required to perform the task. This may $\\hookrightarrow$ include a screwdriver set, pliers, a ratchet set, and so on. Next, $\\hookrightarrow$ follow these steps: 1. Remove the vehicle's fuel tank (if applicable) $\\hookrightarrow$ and any other fluids or components that need to be removed for access. $\\hookrightarrow$ 2. Disconnect the battery and ground the negative terminal to prevent $\\hookrightarrow$ electrical shock. 3. Locate and remove any necessary panels, such as the $\\hookrightarrow$ hood, trunk lid, or doors, to provide access to various components. 4. $\\hookrightarrow$ Inspect and clean any visible parts of the car, including the body, $\\hookrightarrow$ wheels, and tires. Check for damage or corrosion and repair as needed. $\\hookrightarrow$ 5. Remove any accessories, such as antennas, mirrors, or windshield $\\hookrightarrow$ wipers, to get to the underlying components. 6. Locate and remove any $\\hookrightarrow$ fasteners holding the car's interior in place, such as seat belts, $\\hookrightarrow$ dashboard pieces, and floor mats. 7. Carefully remove the interior, $\\hookrightarrow$ being mindful not to damage any wires or cables. 8. Once the interior is $\\hookrightarrow$ out, remove any remaining exterior components, such as the roof, doors, $\\hookrightarrow$ or fenders. 9. Start disassembling the engine and other mechanical $\\hookrightarrow$ components, such as the transmission, differential, and suspension $\\hookrightarrow$ system. 10. Label and organize all removed parts to make reassembly $\\hookrightarrow$ easier. 11. Clean and inspect every part thoroughly, replacing any $\\hookrightarrow$ damaged or worn items with new ones. 12. Finally, put the car back $\\hookrightarrow$ together in reverse order, ensuring all bolts, screws, and other $\\hookrightarrow$ fasteners are tightened to specification. Remember to use appropriate $\\hookrightarrow$ safety precautions throughout the detailing process, such as wearing $\\hookrightarrow$ gloves, eye protection, and following manufacturer recommendations.\", \"annotated_scores\": \"5\",   \n\"llm\": \"gpt-3.5-turbo\",   \n\"source\": \"alpaca_eval\",   \n\"meta_feedback_score\": 5.0, ", "page_idx": 24}, {"type": "text", "text": "\"evaluated_feedback\": \"Feedback: The response provided a detailed $\\hookrightarrow$ step-by-step guide on how to detail a car. It covers various aspects $\\hookrightarrow$ such as removing fluids and components, cleaning and inspecting parts, $\\hookrightarrow$ and reassembling the car. The instructions are clear and well-organized. $\\hookrightarrow$ However, there are a few areas that could be improved: 1. The response $\\hookrightarrow$ could have mentioned the importance of washing the car before starting $\\hookrightarrow$ the detailing process. 2. It would be helpful to include specific $\\hookrightarrow$ cleaning products or techniques that can be used for different parts of $\\hookrightarrow$ the car. 3. The response could also mention the importance of using $\\hookrightarrow$ appropriate cleaning tools, such as microfiber towels or soft brushes, $\\hookrightarrow$ to avoid scratching the car's surface. Overall, the response is relevant $\\hookrightarrow$ and provides a comprehensive guide to detailing a car. The suggested $\\hookrightarrow$ improvements would enhance the clarity and completeness of the $\\hookrightarrow$ instructions.\", # human-annotated scores from 3 human raters \"meta_feedback_scores\": [ 6.0, 5.0, 4.0 ] } ", "page_idx": 25}, {"type": "text", "text": "I.2 Case Study Proves Reference Critique ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we present a case study comparing subjective evaluations conducted with and without the inclusion of reference feedback. Figure 6 illustrates that, without reference feedback, GPT-4 erroneously attributed very high scores to its generated feedback. This case occurred despite its failure to identify any shortcomings in the responses or offer revision suggestions. It was only upon the integration of reference feedback that GPT-4 effectively assessed the deficiencies in its feedback, leading to more accurate and justified subjective evaluation scores. ", "page_idx": 25}, {"type": "text", "text": "I.3 Subjective Evaluation Cases ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The case for feedback subjective evaluation is shown in Figure 8. The generated response contains factual errors: \u201ccool music from 1920 are King of the Jazz Band and In the Mood.\u201d. High-quality reference feedback points out these errors, while Qwen-72B-Chat\u2019s feedback fails to find them, leading to a low-quality score 2. Figure 9 shows the case for comparison subjective evaluation. The case for correction subjective evaluation is shown in Figure 10. ", "page_idx": 25}, {"type": "text", "text": "I.4 Objective Evaluation Cases ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The case for the feedback dimension is shown in Figure 11, and the case for the comparison dimension is shown in Figure 12. ", "page_idx": 25}, {"type": "text", "text": "# Question: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "I like to host guests at my home from time to time, and I am gathering recipes of different dishes and drinks to keep things interesting. I am interested in trying some Danish dishes. Can you give me a recipe for Koldsk\u00e5l? ", "page_idx": 26}, {"type": "text", "text": "# Generated Response to be Evaluated: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Here is a recipe for Koldsk\u00e5l: Ingredients: 4 Quart Jar, 3/4 cup chopped onion 1/2 cup chopped carrot 1/2 cup chopped celery $1\\ 1/2$ cups chopped cabbage 1/2 cup chopped mushrooms 1/2 cup chopped tomatoes 1/2 cup chopped zucchini 1/2 cup chopped ... (multiple dull repetitions) ", "page_idx": 26}, {"type": "text", "text": "# Reference Feedback: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The response provided is not satisfactory and contains several issues.   \n1. Factual Error: The recipe provided is not for Koldsk\u00e5l, a traditional Danish dish. Koldsk\u00e5l is a sweet dish typically made with buttermilk, eggs, sugar, vanilla, and lemon, often served with small biscuits called kammerjunkere. The ingredients listed in the response do not reflect this.   \n2. Relevance: The response does not answer the user\u2019s request for a Koldsk\u00e5l recipe. Instead, it provides a list of ingredients that appear to be for a different type of dish entirely.   \n3. Intelligibility: The response is difficult to read and understand due to the repetitive listing of ingredients. It\u2019s also missing instructions on how to prepare the dish.   \n4. Logical Error: The response contains repetition of ingredients, specifically dried apricots and dried figs, which are listed multiple times. This is not logical in a recipe context.   \nTo improve the response, the model should provide an accurate recipe for Koldsk\u00e5l, including the correct ingredients and preparation instructions. The model should also avoid repetition of ingredients and ensure the response is clear and easy to understand. ", "page_idx": 26}, {"type": "text", "text": "# Generated Feedback ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The recipe for Koldsk\u00e5l is not provided. ", "page_idx": 26}, {"type": "text", "text": "# Subjective Evaluation w/. Reference Feedback ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The generated feedback is extremely brief and fails to address the multiple issues present in the response. It only states that the recipe for Koldsk\u00e5l is not provided, which is accurate but not comprehensive. The reference feedback, on the other hand, provides a detailed analysis of the problems, including factual errors, relevance, intelligibility, and logical errors. It also offers constructive suggestions for improvement. The generated feedback does not mention the factual inaccuracy of the ingredients listed, the irrelevance of the response to the user\u2019s request, the repetitive listing of ingredients, or the lack of preparation instructions. It also does not provide any guidance on how the response could be improved. Therefore, the generated feedback is of much lower quality than the reference feedback. ", "page_idx": 26}, {"type": "text", "text": "Score: 2 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "# Subjective Evaluation w/o. Reference Feedback ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The generated feedback correctly identifies that the response does not provide a recipe for Koldsk\u00e5l. The response lists ingredients that are not related to the traditional Danish dish Koldsk\u00e5l, which is a sweet cold soup typically made with buttermilk, eggs, sugar, vanilla, and lemon. Instead, the response includes a nonsensical list of ingredients that are repeated multiple times and are more in line with a vegetable or fruit salad rather than a Danish dessert soup. The feedback is succinct and directly addresses the main issue with the response, which is the absence of an actual Koldsk\u00e5l recipe. However, the feedback could have been more detailed by explaining what Koldsk\u00e5l is and why the ingredients listed do not correspond to the dish in question. ", "page_idx": 26}, {"type": "text", "text": "Score: 8 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Figure 6: One case in subjective evaluation of QA task. GPT-4 provides a more reasonable rationale and score given the high-quality feedback as a reference. Generated feedback is brief, cannot point out flaws, and provides rich information for revision, indicating its low quality. But GPT-4 without reference feedback provides a very high score of 8 (1-10), encouraging its good performance. ", "page_idx": 26}, {"type": "text", "text": "J Analysis about Length Bias in Subjective Evaluation ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Following previous works [46, 81], the Likert scores and the average number of unique tokens in generated critiques are shown in Figure 7. It can be found that there is almost no relationship between the number of tokens and the Likert scores generated by GPT-4 in the subjective evaluation of three critique dimensions, owing to the concise and precise reference critiques in the prompt. ", "page_idx": 27}, {"type": "text", "text": "K Overall Score Computation ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The overall scores of LLMs on these four critique dimensions are calculated to reflect their overall capability. For objective evaluation, the overall scores are the average scores over four critique dimensions, ranging from 0 to 100. Since Spearman scores range from $-1$ to 1, they are first normalized into $(0,100)$ before averaging. For subjective evaluation, since meta", "page_idx": 27}, {"type": "image", "img_path": "ZsxZ65YqL1/tmp/680c4fc3d95acddd4d2f0201117923f7344b5ff7b33436c3fd1a3024e4035a2b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 7: Each dot represents one LLM\u2019s performance on the subjective evaluation. ", "page_idx": 27}, {"type": "text", "text": "feedback does not require textual critiques, the overall scores are set as the average of feedback, comparison and correction scores, ranging from 1 to 10. ", "page_idx": 27}, {"type": "text", "text": "# Evaluation Task ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "You are given: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "$\\star$ a given user query or instruction $\\star$ a generated response by a large-scale language model $\\star$ a textual feedback analyzes the response $\\star$ a reference high-quality feedback as reference ", "page_idx": 28}, {"type": "text", "text": "# Given Data ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "### Question: What is some cool music from the 1920s?   \n### Generation: Some cool music from the 1920s is King of the Jazz Band and In the Mood. ### Reference Feedback (Score of 8): The response contains a factual error. \u201cIn the Mood\u201d is not from the 1920s, but was recorded in 1939 by Glenn Miller. Also, \u201cKing of the Jazz Band\u201d is not a recognizable song title from any era. This could lead to confusion for the user. The model should ensure it provides accurate and recognizable song titles from the 1920s. ", "page_idx": 28}, {"type": "text", "text": "### Generated Feedback: The response provides two examples of cool music from the 1920s, which are \u201cKing of the Jazz Band\u201d and \u201cIn the Mood.\u201d However, it would be more helpful if the response could provide additional context or information about these songs or the genre of music they belong to. For example, mentioning that \u201cKing of the Jazz Band\u201d was performed by Paul Whiteman\u2019s orchestra, which was one of the most popular jazz bands of the time, or that \u201cIn the Mood\u201d was a hit song by Glenn Miller\u2019s orchestra and became an iconic piece of swing music, would make the response more informative and engaging. Additionally, providing a few more examples of cool music from the 1920s would give the reader a broader understanding of the musical landscape of that era. Overall, while the response is relevant and understandable, it can be improved by adding more depth and context. ", "page_idx": 28}, {"type": "text", "text": "# Very Important Detail!!! ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. If you think the generated feedback is better then reference feedback, you could provide score higher than 8. ", "page_idx": 28}, {"type": "text", "text": "2. please first generate your feedback and explanation for the generated feedback by comparing it with reference feedback, and then you must generate the score following the format \u2018Score: $\\mathbf{x}^{\\,\\ast}$ , where $\\mathbf{X}$ is the score range from 1 to 10. ", "page_idx": 28}, {"type": "text", "text": "# Chain-of-thought evaluation by GPT-4: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The generated feedback fails to identify the factual inaccuracies present. The reference feedback correctly points out that \u201cIn the Mood\u201d is not from the 1920s but from 1939, and it also notes that \u201cKing of the Jazz Band\u201d is not a recognizable song title. The generated feedback, on the other hand, does not address these issues at all. Instead, it suggests adding more context and examples to make the response more informative, which, while potentially useful, does not correct the fundamental errors. The generated feedback\u2019s suggestion to provide additional context about the songs and the era is a good one, but it is based on the assumption that the provided song titles are correct and relevant, which they are not. Therefore, the generated feedback is not helpful in improving the translation\u2019s accuracy or fluency and does not align with the high-quality reference feedback. ", "page_idx": 28}, {"type": "text", "text": "# Score: 2 ", "page_idx": 28}, {"type": "text", "text": "Figure 8: One case of evaluating feedback generation on chat task. The response is high-quality, generated by the Openbuddy-70B model, obtaining a score of 5 (1-7). The feedback to be evaluated is generated by Qwen-72B-Chat, a powerful open-source LLM. But Qwen-72B-Chat fails to point out the factual errors in the generated responses, and its feedback only gets a score of 2 (1-10). The resources to understand this example can be found in https://en.wikipedia.org/wiki/In_the_ Mood and https://en.wikipedia.org/wiki/King_of_Jazz. ", "page_idx": 28}, {"type": "text", "text": "# Evaluation Task ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "You are given: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "$\\star$ a source sentence in English $\\star$ a generated response A by a large-scale language model $\\star$ a generated response B by another large-scale language model $\\star$ a textual comparison analyzes the response $\\star$ a reference high-quality comparison-based feedback as reference ", "page_idx": 29}, {"type": "text", "text": "# Given Data ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "### Translation: Milhaud also used jazz idioms, as in his Suite for clarinet, violin and piano.### Translation A: \u7c73\u54c8\u4f0a\u5c14\u5fb7\u4e5f\u66fe\u4f7f\u7528\u7235\u58eb\u4e50\u7684\u97f3\u8c03\uff0c\u5982\u4ed6\u7684\u957f\u7b1b\u3001\u5c0f\u63d0\u7434\u548c\u94a2\u7434\u7684\u7ec4\u66f2\u3002### Translation B: \u7c73\u5c14\u8c6a\u5fb7\u4e5f\u4f7f\u7528\u7235\u58eb\u4e50\u7684\u4e60\u8bed\uff0c\u5982\u4ed6\u5728\u957f\u53f7\u3001\u5c0f\u63d0\u7434\u548c\u94a2\u7434\u7684\u56db\u91cd\u594f\u4e2d\u6240\u4f7f\u7528\u7684\u3002", "page_idx": 29}, {"type": "text", "text": "### Reference Feedback (Score of 8): Both translations have inaccuracies. In Translation A, \u201cclarinet\u201d is incorrectly translated as \u201c\u957f\u7b1b\u201d (flute). In Translation B, \u201cclarinet\u201d is incorrectly translated as \u201c\u957f \u53f7\u201d (trombone). Moreover, in Translation B, \u201cSuite\u201d is incorrectly translated as \u201c\u56db\u91cd\u594f\u201d (quartet). However, Translation A correctly translates \u201cSuite\u201d as \u201c\u7ec4\u66f2\u201d. Considering these, Translation A is slightly better due to its correct translation of \u201cSuite\u201d. ", "page_idx": 29}, {"type": "text", "text": "### Generated Feedback: Translation A is more accurate than Translation B. The original sentence mentions \u201cjazz idioms\u201d which is translated correctly as \u201c\u7235\u58eb\u4e50\u7684\u97f3\u8c03\u201d in Translation A. However, Translation B uses \u201c\u7235\u58eb\u4e50\u7684\u4e60\u8bed\u201d which does not convey the same meaning as \u201cidioms.\u201d Additionally, the original sentence mentions \u201cSuite for clarinet, violin and piano\u201d which is translated correctly as \u201c\u957f \u7b1b\u3001\u5c0f\u63d0\u7434\u548c\u94a2\u7434\u7684\u7ec4\u66f2\u201d in Translation A, but incorrectly as \u201c\u957f\u53f7\u3001\u5c0f\u63d0\u7434\u548c\u94a2\u7434\u7684\u56db\u91cd\u594f\u201d in Translation B. Therefore, my preference would be for Translation A as it is more accurate and faithful to the original sentence. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "# Very Important Detail!!! ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. If you think the generated feedback is better then reference feedback, you could provide score higher than 8. ", "page_idx": 29}, {"type": "text", "text": "2. Please first generate your feedback and explanation for the generated feedback by comparing it with reference feedback, and then you must generate the score following the format \u2018Score: $\\mathbf{x}^{\\,\\ast}$ , where $\\mathbf{X}$ is the score range from 1 to 10. ", "page_idx": 29}, {"type": "text", "text": "# Chain-of-thought evaluation by GPT-4:   \nThe generated feedback correctly identifies that Translation A is more accurate than Translation B. However, it makes a significant factual error by stating that \u201c\u957f\u7b1b\u201d is the correct translation for \u201cclarinet,\u201d which is incorrect. The correct translation for \u201cclarinet\u201d should be \u201c\u5355\u7c27\u7ba1\u201d or \u201c\u9ed1\u7ba1.\u201d This error is critical as it misleads the evaluation of the accuracy of the translations. The generated feedback also correctly points out the mistranslation of \u201cjazz idioms\u201d in Translation B. However, it fails to mention that \u201c\u7235\u58eb\u4e50\u7684\u97f3\u8c03\u201d in Translation A is also not an entirely accurate translation of \u201cjazz idioms,\u201d as \u201c\u97f3\u8c03\u201d refers more to musical tones rather than idioms or phrases. Additionally, the generated feedback does not address the error in Translation B where \u201cSuite\u201d is translated as \u201c\u56db\u91cd\u594f\u201d (quartet), which is a significant mistake as it changes the type of musical composition being discussed. The reference feedback is more accurate as it correctly identifies the mistranslation of \u201cclarinet\u201d in both responses and the mistranslation of \u201cSuite\u201d in Translation B. It also provides a clear comparison that acknowledges the correct translation of \u201cSuite\u201d in Translation A, which the generated feedback fails to do. In terms of intelligibility, both feedbacks are clear and easy to understand. They both maintain relevance by addressing the accuracy of the translations in relation to the original sentence. Considering these points, the generated feedback has a factual error and misses a critical translation error, making it less effective than the reference feedback. ", "page_idx": 29}, {"type": "text", "text": "# Score: 4 ", "page_idx": 29}, {"type": "text", "text": "Figure 9: One case of evaluating generated comparison critique on the translation task. Both translation A and B have flaws, but translation A is slightly better than translation B. The response is high-quality, generated by Qwen-7B-Chat mode. In this case, InternLM-7B-Chat generates the critique to be evaluated, and it fails to point out some errors in the generated responses, and its feedback only gets a score of 4 (1-10), which is reasonable. ", "page_idx": 29}, {"type": "image", "img_path": "ZsxZ65YqL1/tmp/e3b338cd464ae2b6d7bb785b23e435cfed26de7d25fc97eeb2077b1b7859fcc8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "# Score: 8 ", "page_idx": 30}, {"type": "text", "text": "Figure 10: One case of evaluating generated correction on the translation task. The correction is high-quality, and get a score of 8 (1-10). ", "page_idx": 30}, {"type": "image", "img_path": "ZsxZ65YqL1/tmp/e45cb774c3d6458def255ff9e8b77b9d45d28206582e427c30f542d24929887b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Figure 11: One case of objective evaluation on translation task. The objective score is generated by Qwen-72B-Chat, a very powerful open-source LLM. ", "page_idx": 31}, {"type": "image", "img_path": "ZsxZ65YqL1/tmp/7eff4ba4c7c08e8638007ea13e06df3d04131ae7b378987a43505bf6188a4f20.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Figure 12: One case of generated comparison critique on translation task. The preference label is generated by Qwen-72B-Chat, a very powerful open-source LLM. ", "page_idx": 31}, {"type": "text", "text": "L Complete Results ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "L.1 Complete Results for Different Qualities ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Table 19 and Table 20 show the overall evaluation results for different qualities granularities on the subjective evaluation of the test and dev set the feedback, comparison, and correction critique dimensions. Table 21 and Table 22 show the overall evaluation results for different qualities granularities on the objective evaluation of the test and dev set the feedback, comparison, and correction critique dimensions. ", "page_idx": 32}, {"type": "text", "text": "L.2 Complete Results for Different Tasks ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Table 23 and Table 24 show evaluation results on each tasks (test set) for the feedback and comparisonbased feedback dimensions. Table 25 and Table 26 show complete evaluation results on each tasks (dev set) for the feedback and comparison dimensions. Table 27 and Table 28 show complete evaluation results on each task (test and dev set) for the correction dimension. ", "page_idx": 32}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/d2c1aa6358f660a320b8324530aaba2d1eb02cb79e14a0f486baf29baf97d4ef.jpg", "table_caption": ["Table 19: Performance of subjective evaluation on the test set of the feedback, comparison and correction critique dimensions. "], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "Table 20: Performance of subjective evaluation on the dev set of the feedback, comparison and correction critique dimensions. ", "page_idx": 33}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/1d08e26bf91dd3dd40216a0e7d8f307609043500ab5a07982cd74ab6e32f75aa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 33}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/cfa92b4dee2a8a6936883609340168b48a2ba2045ec9ffbb2185ac6f8655a234.jpg", "table_caption": [], "table_footnote": [], "page_idx": 33}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/47cbc46be27481c82e9b5a0dbd02c390fef7e38b19387c14893f9017e5baed7f.jpg", "table_caption": ["Open-source LLM (13B-33B) "], "table_footnote": [], "page_idx": 33}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/c0b3029c46b2c631923e55e3c9cbcdd9bf0a4a16925234dc8e8c1fd9b2b6154f.jpg", "table_caption": ["Open-source LLM (> 30B) "], "table_footnote": [], "page_idx": 33}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/d1392b4883a9b7058cd95e2034e973059b0cbdd31b0b7d849a5be2ac6b9e5807.jpg", "table_caption": ["Table 21: Performance on the objective evaluation of the test set of CRITICEVAL. "], "table_footnote": [], "page_idx": 34}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/587db73c8a152dbede91a9010cdb43151a2ac1759d7c5e3f14b4dab1c761e6ff.jpg", "table_caption": ["Table 22: Performance on the objective evaluation of the dev set of CRITICEVAL. "], "table_footnote": [], "page_idx": 35}, {"type": "text", "text": "Table 23: Subjective evaluation on the test set of the feedback critique dimension. Three Avg. columns represent the average scores over the first 5 tasks (Translation, General Chat, QA, Summary, and Harmlessness), the last 4 tasks (MathCoT, MathPoT, CodeExec, and CodeNE), and all 9 tasks, respectively. ", "page_idx": 36}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/cfe6da7a0600ab266cc77934eb4df646ebac9f920b78adfd8695495b267d64b6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 36}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/0d4182cf51c1e54ab714ed962ca1aa1b7074ecbbd519cc943fa943696ba61048.jpg", "table_caption": ["Critique-tuned LLMs (13B-14B "], "table_footnote": [], "page_idx": 36}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/25a17ed58e7fbebfc69021d48a5b0689a659bc9df61252aec19a473f56d97c1c.jpg", "table_caption": ["Open-source Models (13B-33B) "], "table_footnote": [], "page_idx": 36}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/cce8e98a660883197c2dce49e499ad9cc7ecc2bc6f25d6d2dd1549060a1e5624.jpg", "table_caption": [], "table_footnote": [], "page_idx": 36}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/c28b4fe1a571efd2b69e26281287162b5fd02cca34247eaf37201d254072aba7.jpg", "table_caption": ["Table 24: Subjective evaluation results on the test set of the comparison dimension. "], "table_footnote": [], "page_idx": 37}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/694257755f41944103888fc98a54a65bb0fa72746be892e322ac612aa0f8d8b0.jpg", "table_caption": ["Table 25: Subjective evaluation results on the dev set of the feedback dimension. "], "table_footnote": [], "page_idx": 38}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/d4a06643a29e0d1a4ef63d43fe7d49d5dd11061bf62d9f017e6a675409b95874.jpg", "table_caption": ["Table 26: Subjective evaluation results on the dev set of the comparison critique dimension. "], "table_footnote": [], "page_idx": 39}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/aac3ca1ed08d633f56c1307745a3b184470c0cc270a3a97eadced0c7e229f75c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 39}, {"type": "text", "text": "Table 27: Subjective evaluation results on the test and dev set of the correction critique dimension. Due to the cost limitation, we do not provide the experimental results on these closed-source APIbased LLMs: GLM4-no-tool, ErnieBot-Pro, Baichuan2 Turbo, Qwen-Max, MiniMax-abab5. ", "page_idx": 40}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/62dbf7fa7e14e7b01b1f523bf68f2272084a7e1f7e90a1e063403b21d10ae981.jpg", "table_caption": [], "table_footnote": [], "page_idx": 40}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/1b5aef33c2ba8b57dd0f91b710a8d7ae6744680bdc1f0a299d015b1cb2dbf241.jpg", "table_caption": [], "table_footnote": ["Open-source LLMs (> 70B) "], "page_idx": 40}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/d7741bd5b7ce18f29a5a0325b1bcc10e00f385dc2f59d5a442a28a5dbf1b164f.jpg", "table_caption": ["Table 28: Objective evaluation results on the test and dev set of the correction dimension. Due to the cost limitation, we do not provide the experimental results of following closed-source API-based LLMs on dev set: GLM4-no-tool, ErnieBot-Pro, Baichuan2 Turbo, Qwen-Max, MiniMax-abab5. "], "table_footnote": [], "page_idx": 41}, {"type": "text", "text": "M Human Performance in CRITICEVAL ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "In this section, we provide more details and comparison between LLMs and human performance. Specifically, we conduct the human annotation of the subjective tasks on the CRITICEVAL test set, and the overall human-level performance is shown in Table 29. Note that the cohort and corresponding set of human critiques does not represent the best possible human performance; instead, they represent the capability of annotators selected for this human performance annotation of the CRITICEVAL test set. ", "page_idx": 41}, {"type": "text", "text": "Table 29: Comparison between Human Performance and GPT-4-turbo. ", "page_idx": 41}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/2bafd5b8a55d12b1464c8a581c8c67aa2e765a401a2d3cba6357f38c2b03b934.jpg", "table_caption": [], "table_footnote": [], "page_idx": 41}, {"type": "text", "text": "It can be found that the human-level significantly outperforms GPT-4 on the objective task, while it is inferior to GPT-4 on the subjective evaluation. Therefore, we conduct the Quantitative and Qualitative Analysis to understand the performance gap between humans and GPT-4 in subjective evaluation. ", "page_idx": 41}, {"type": "text", "text": "Quantitative Analysis We conduct the fine-grained failure modes analysis, and the distribution of each failure mode for feedback, comparison and correction dimensions are shown in Table 30, Table 31 and Table 32. The numbers in following tables indicate the frequencies of error types in GPT-4 and human-written critiques. The detailed description of each failure mode can be found in Section 6.8. As for the feedback and comparison dimensions, the distribution of E1 (missing issues), E2 (missing suggestions or low-quality suggestions), and E7 (insufficient analysis) in humanwritten critiques is significantly higher than that of GPT-4. In contrast, the distribution of other error types is significantly lower. As for the correction dimension, human annotators usually do not follow suggestions in the provided feedback (E9) and generate additional errors (E11). Through communicating with the annotators, we notice that the primary cause of this issue is that some tasks require domain-specific knowledge, and the lack of this knowledge among human annotators results in lower-quality corrections. In summary, the human-written critiques are often less comprehensive than GPT-4, significantly reducing the quality. In contrast, the mistakes in human-written critiques are significantly less than that of GPT-4. This phenomenon is consistent with our preliminary study and recent findings [79, 6], further proving the effectiveness and reasonableness of leveraging the human-in-the-loop pipeline to construct comprehensive and accurate reference critiques. ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "Table 30: Comparison between Human Performance and GPT-4-turbo in feedback dimension $(F_{s})$ . ", "page_idx": 42}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/7905142a26ff094c4179a2aa98ba3f50b6679534777c47fdc3d080ea1d99764c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 42}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/0e3412fe22413a0a59dbf47c0c599cbb7ea6042ebad471f062e00ebc08aba233.jpg", "table_caption": ["Table 31: Comparison between Human Performance and GPT-4-turbo in comparison dimension $(F_{c})$ . "], "table_footnote": [], "page_idx": 42}, {"type": "text", "text": "Table 32: Comparison between Human Performance and GPT-4-turbo in correction dimension $(C R)$ . ", "page_idx": 42}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/aa01ccd7be70aadffa989aa2b9756d84c0b49a47d35555cb000774625bff838c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 42}, {"type": "text", "text": "Qualitative Analysis We inspect the human-written critiques in the subjective evaluation tasks to understand the source of the performance difference. In general, human annotators write fewer comments than LLMs, and the comments are usually general and brief. Besides, many tasks involve domain-specific knowledge that humans may lack, but GPT-4 excels in (albeit with potential hallucinations). ", "page_idx": 42}, {"type": "text", "text": "N Evaluated LLMs ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "We extensively evaluate widely used open-source and closed-source LLMs of different sizes on CRITICEVAL to understand the current progress in this field, including (1) instruction-tuned LLMs; (2) critique-tuned LLMs; and (3) reward models. To reproduce evaluation results, the greedy search is employed for open-source LLMs, and the temperature factor is set as 0 for closed-source LLMs, i.e., decoding randomness is minimum. ", "page_idx": 42}, {"type": "text", "text": "The inference procedures of all these evaluated LLMs in this paper are conducted in an A800 server with 8 GPU cards, each with 80G CUDA memory. The vLLM [82] and LMDeploy [83] packages are used to speed up the inference, and the average inference time cost for each LLM is 1.25 hours. ", "page_idx": 42}, {"type": "text", "text": "N.1 Instruction-tuned LLMs ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "For closed-source LLMs, we test GPT-4, Claude, Gemini-Pro, PaLM, GPT-3.5-turbo, etc. For open-source LLMs, we test numerous LLM series including Mistral [49], LLaMA2 [37], Baichuan2 [84], Qwen9 [21], InternLM2 [48], WizardLM [85], Vicuna [86], Yi, and DeepSeek [50], etc. ", "page_idx": 42}, {"type": "text", "text": "N.2 Critique-tuned LLMs ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Recent works have proven that fine-tuning LLMs on critiques generated by GPT-4 significantly improves LLM\u2019s critique ability [11, 30, 24, 12]. Llama-2-13B fine-tuned on GPT-4\u2019s critique could achieve close critique ability to GPT-4, and even outperform much larger model, like Llama-2-70B. In this paper, we name the LLMs fine-tuned on critiques dataset Critique-tuned LLMs, and we comprehensively evaluate critique-tuned LLMs on our proposed CRITICEVAL: (1) TigerScore [30]; (2) Auto-J-13B [11]; and (3) UltraCM [12]. ", "page_idx": 42}, {"type": "image", "img_path": "ZsxZ65YqL1/tmp/fa5f091076d177eb4cb68bfc55de455a01781045bc300ceb0c4aeb7a15db2788.jpg", "img_caption": ["Figure 13: Interpretable analysis of the LLM\u2019s critique ability on subjective evaluation. "], "img_footnote": [], "page_idx": 43}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "However, there are two popular critique-tuned LLMs are not evaluated in CRITICEVAL. The reasons are listed as follows: (1) InstructScore [24] can only be used to evaluate limited tasks, like data2text and commonsense, thus we donot test InstructScore in our work; (2) Prometheus [14] are not evaluated because of its high dependence on the criteria question, score rubrics and reference answers, which is not fully covered in our benchmark. ", "page_idx": 43}, {"type": "text", "text": "N.3 Reward Models ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Moreover, we show CRITICEVAL can also be used to evaluate reward models [87]. There are lots of reward models that can be publicly accseed. We only evaluate three representative reward models, and leave the evaluation on other reward models in our future work [58]: (1) UltraRM-13B [12]; (2) Ziya-7B [52]; (3) SteamSHP [53]. ", "page_idx": 43}, {"type": "text", "text": "O Interpretable Analysis of the Quality of Textual Critiques ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Beyond the simplified average scores from 1 to 10 in the subjective evaluation of CRITICEVAL, we also categorize the textual critiques of each LLM into three quality intervals for more interpretable analysis: (1) Low-quality critiques (1-3); (2) Medium-quality critiques (4-6); (3) High-quality critiques (7-10). The results of five representative LLMs on feedback, correction, and comparionbased feedback dimensions are shown in Figure 13. It can be found that GPT-4-turbo exhibits strong critique ability and barely generates low-quality critiques. In contrast, the critiques generated by the Llama-2-70B-Chat are usually low-quality. Besides, the ratio of low-quality critiques generated by some LLMs, like Llama-2-70B-Chat and Qwen-72B-Chat, are very high, indicating that they have a lot of room for improvement. ", "page_idx": 43}, {"type": "text", "text": "P Error Patterns in Responses ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "In this section, we analyze the details and cases about three kinds of error patterns in responses: (1) obvious error; (2) complex error; (3) subtle error. Specifically, we ask human annotators to summarize the common error cases after they annotate all the textual critiques in CRITICEVAL, and categorize them into obvious error, complex error, and subtle errors in Table 33. It should be noted that these three error patterns may have other specific error cases in nine domains. However, it is difficult to exhaust all the error case. Thus, our motivation is to list, annotate, and analyze as many as possible to ensure the accuracy of our experimental results. ", "page_idx": 43}, {"type": "text", "text": "Q Likert Score for Responses ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Figure 5 demonstrates the discernible performance disparities in responses for each task. Since automatic execution leaks quality information, we do not collect the correct responses for the ", "page_idx": 43}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/c48c0ee829b482c5c3d9eb5a42657d4863c1bdb47e5cd82926668ff3c5390078.jpg", "table_caption": ["Table 33: Specific error cases in each error pattern and each data domain in our human annotation. Since harmful content is easy to detect, the complex error is very rare in CRITICEVAL. "], "table_footnote": [], "page_idx": 44}, {"type": "image", "img_path": "ZsxZ65YqL1/tmp/86e18b163f24853cf8b24344d17daaaef8de787df3d0fdde6bd8530780e3160d.jpg", "img_caption": ["Figure 14: The visualization of scaling law of LLM\u2019s critique ability. Under the same LLM series, the critique ability for LLMs becomes better when the size of the model scale increases. "], "img_footnote": [], "page_idx": 44}, {"type": "text", "text": "CodeExec task. Such variation is instrumental in analyzing the impact of response quality on the feedback. ", "page_idx": 44}, {"type": "text", "text": "R Visualization of Relationship between Model Scales and Critique Ability ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Following previous works [9], we provide the diagrams to demonstrate the relationship between LLM\u2019s critique ability and model scales, which are shown in Figure 14. It can be easily found that the critique ability of all LLM series steadily increase with the number of the parameters (scale) increasing. ", "page_idx": 44}, {"type": "text", "text": "S How Few-shot Examples Affect Performance ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "In this paper, we have studied the few-shot prompting strategy. However, our results demonstrate that few-shot prompting reduces performance across various LLMs. As illustrated in Table 34, when using 1-5 examples in objective feedback evaluations, we observed a significant decline in LLMs\u2019 performance as the number of examples increased. This intriguing phenomenon may be due to the complexity of the critique task, where few-shot examples might impede the LLM\u2019s understanding of the evaluated responses. Consequently, CRITICEVAL currently does not utilize few-shot prompting by default. Given the emerging interest in critique ability research, we look forward to future works investigating advanced inference strategies to improve critique ability of LLMs. ", "page_idx": 45}, {"type": "text", "text": "Table 34: The critique performance (Spearman correlation) of LLMs by few-shot prompting. ", "page_idx": 45}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/0d08d60dbd82d5aae1bbe443e606f27d97f1a2e8ca5551f6a75b22b53fda16b9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 45}, {"type": "text", "text": "T Scalability and Cost about CRITICEVAL ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "In this section, we provide the cost of constructing one task and inferencing one LLM in CRITICEVAL. ", "page_idx": 45}, {"type": "text", "text": "T.1 Construction Cost ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Collect Evaluated Responses for All Tasks ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "\u2022 Open-source LLMs: a GPU server with 8 A100 (80G) cards is used to generate evaluated responses, and the total GPU hours are 4.26 hours, approximately $82.88\\Phi$ (refer to the price of Alibaba Cloud).   \n\u2022 Closed-source LLMs: the average cost for each LLM is $0.89\\mathbb{S}$ . ", "page_idx": 45}, {"type": "text", "text": "Generate and Revise GPT-4 Critiques The cost of the human annotation is computed under these settings: (1) Four human annotators (3 annotators and one supervisor); (2) $5.69\\mathbb{S}$ hourly wage for each annotator; (3) Average 400 samples in one task. The overall construction cost are shown in Table 35, which is affordable [88]. ", "page_idx": 45}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/55c4f976d809f90520cf19fa34846a374e75eaa87afd945242d6ae30af962452.jpg", "table_caption": ["Table 35: The cost of constructing the critiques. "], "table_footnote": [], "page_idx": 45}, {"type": "text", "text": "T.2 Average Computation Cost for One LLM ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "As shown in Table 36The overall cost of the test and dev set is $13.19{+}9.94{=}23.13\\Phi$ , comparable to the evaluation cost on the AlpacaEval benchmark (5-15\\$) [19]. These costs are essential for CRITICEVAL, as they guarantee the reliability of critique evaluation. We promise to add these details to the Appendix of our revised submission. ", "page_idx": 45}, {"type": "text", "text": "U Multilingual Support ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "The primary goal of CRITICEVAL in the current stage is to construct a reliable and comprehensive evaluation for critique ability. We agree that it is essential to study multilingual critiques and intend to broaden CRITICEVAL to include other languages in future work. The following content briefly introduces our preliminary solution on how to achieve this goal. ", "page_idx": 45}, {"type": "table", "img_path": "ZsxZ65YqL1/tmp/bb7486698dcf7f404ac960ceea9f5dac0b0dba1917965e34ba91b263c61d6381.jpg", "table_caption": ["Table 36: The computation cost of inference one LLM in CRITICEVAL. "], "table_footnote": [], "page_idx": 46}, {"type": "text", "text": "Construct Multilingual CRITICEVAL Following the previous work [89], CRITICEVAL could be translated to various languages, especially low-resource languages, with human annotation for revising translation inaccuracies. the most direct way is to translate CRITICEVAL into various languages, with human annotation for revising translation inaccuracies. ", "page_idx": 46}, {"type": "text", "text": "Evaluate Multilingual CRITICEVAL While the reliability of objective evaluation could be ensured, the reliability of subjective evaluation is limited by the multilingual capability of the judge model (GPT-4). We recommend back-translating multilingual critiques into English and evaluating them within English CRITICEVAL. ", "page_idx": 46}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: We have well discussed the main claims in Section 4 and Section 6. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 47}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: The limitations are well described and discussed in Appendix A ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 47}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: The paper does not include theoretical results. Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 48}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: Our experimental results could be easily reproduced because we have already discussed the necessary details in Section 6.1, Appendix I and Appendix L. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 48}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: We upload dataset and evaluation toolkit of our proposed CRITICEVAL with this submission to reproduce our results. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 49}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: We have well described all the details about inference procedure in Section 6, Appendix H, Appendix F, Appendix E, Appendix D, Appendix N and Appendix C. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 49}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Justification: We report the statistical significance of our objective evaluation in CRITICEVAL in Section 5 and Section 6. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 49}, {"type": "text", "text": "", "page_idx": 50}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: The time cost, compute worker, CUDA memory for LLM inference in our proposed CRITICEVAL dataset are well described in Appendix N. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 50}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: We promise our research conforms with the NeurIPS Code of Ethics, which is listed in Section B, Section D. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 50}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: We have described the ethical consideration in Appendix B, and our paper doesn\u2019t raise the potential positive or negative societal impacts. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 50}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 51}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Justification: We have discussed this part of content in Appendix B. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 51}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: We have well discussed the Licenses of existing assets in Appendix D, while we do not discuss more details for the used LLMs in this paper due to the vast number of LLMs. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 51}, {"type": "text", "text": "", "page_idx": 52}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Justification: We upload the documentation and some important information about our new assets. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 52}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Justification: We have discussed this part of the content carefully in Appendix H. Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 52}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Justification: We have submitted an application to the Institutional Review Board (IRB). After evaluation, the committee determined that the human annotation for critique task posed no ethical concerns, and accordingly, we conduct the human annotation. Besides, we have adequately addressed minimal ethical concerns by following efforts: ensuring the fairness of the data collection by selecting a diverse group of annotators, providing clear guidelines, and compensating the annotators well, as described in Appendix B and Appendix H. Therefore, these efforts make sure our work does not violate the NeurIPS Code of Ethics. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 53}]