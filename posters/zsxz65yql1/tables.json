[{"figure_path": "ZsxZ65YqL1/tables/tables_1_1.jpg", "caption": "Table 1: Comparison between the test and dev set of benchmarks and CRITICEVAL. A complete list are described in Appendix C. The response quality in some benchmarks is unclassified (-). PR denotes the Pass Rate on reasoning and coding tasks. Our concurrent works are marked with a \u2020.", "description": "This table compares CRITICEVAL with other existing benchmarks for evaluating the critique capabilities of LLMs.  It shows the different critique formats (scalar, natural language), dimensions, response qualities, dataset sizes (test and development sets), and evaluation metrics (subjective, objective).  It highlights CRITICEVAL's increased comprehensiveness and reliability, including its larger dataset, multiple critique dimensions, and use of GPT-4 for reliable textual critique evaluation.", "section": "1 Introduction"}, {"figure_path": "ZsxZ65YqL1/tables/tables_5_1.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F (Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents the subjective and objective evaluation results for 35 different LLMs on the CRITICEVAL benchmark's test set.  The subjective scores are based on human evaluations of the quality of the LLMs' critiques, while the objective scores are based on the correlation between the LLM's scores and human judgments.  The table breaks down the results by critique dimension (feedback, correction, comparison, meta-feedback) and includes an overall score combining all dimensions.  Dark gray and light gray highlight the best and worst performing models, respectively.", "section": "5 Evaluation Metrics"}, {"figure_path": "ZsxZ65YqL1/tables/tables_6_1.jpg", "caption": "Table 3: Results of meta-feedback dimension in CRITICEVAL dev set. p-value < 0.05.", "description": "This table presents the results of the meta-feedback dimension evaluation on the CRITICEVAL development set.  It shows the performance of several LLMs (GPT-4-turbo, Qwen-1.5-72B, Claude-instant-1, and GPT-3.5-turbo) in evaluating textual critiques, comparing their scores with and without the use of reference critiques. The table highlights the impact of reference critiques on the reliability of LLM-based textual critique evaluation.", "section": "6.2 Reliability of Subjective Evaluation in CRITICEVAL"}, {"figure_path": "ZsxZ65YqL1/tables/tables_6_2.jpg", "caption": "Table 4: Correlations in CR and Fe dimensions. p-value < 0.05.", "description": "This table presents the correlation results between human evaluation and GPT-4 evaluation for the correction (CR) and comparison (Fc) dimensions in the CRITICEVAL benchmark.  A p-value less than 0.05 indicates statistical significance. The table shows strong correlations, suggesting the reliability of GPT-4 in evaluating textual critiques for these two dimensions.", "section": "5 Evaluation Metrics"}, {"figure_path": "ZsxZ65YqL1/tables/tables_6_3.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u2084(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents the subjective and objective evaluation results for 35 LLMs on the CRITICEVAL benchmark's test set.  It shows the performance across four critique dimensions (feedback, correction, comparison, and meta-feedback) using both scalar and textual critique formats.  The best and worst performing LLMs are highlighted, and statistically significant results (p-value > 0.05) are noted. The overall score represents an average across all dimensions.", "section": "5 Evaluation Metrics"}, {"figure_path": "ZsxZ65YqL1/tables/tables_7_1.jpg", "caption": "Table 6: Critique-tuned LLMs results in feedback dimension.", "description": "This table presents the subjective (Sub.) and objective (Obj.) evaluation results for critique-tuned LLMs (Large Language Models) on the feedback dimension of the CRITICEVAL benchmark.  Critique-tuned LLMs are models specifically fine-tuned on critique datasets. The table shows that performance varies considerably across different models, highlighting the impact of fine-tuning strategies on critique ability.  The scores likely reflect the degree to which these LLMs successfully identify and provide helpful suggestions for improving the quality of model generated text. ", "section": "6.4 Overall Analysis of LLMs"}, {"figure_path": "ZsxZ65YqL1/tables/tables_7_2.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u300f(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents the subjective and objective evaluation results for various LLMs on the CRITICEVAL benchmark's test set.  The evaluation covers four critique dimensions (feedback, correction, comparison, meta-feedback), and both subjective (human-rated) and objective (automatically computed using GPT-4) scores are provided.  The table highlights the best and worst performing models in each dimension and overall, considering various factors like LLM type (closed-source vs. open-source) and size.  Statistical significance (p-values) is indicated for objective scores. ", "section": "5 Evaluation Metrics"}, {"figure_path": "ZsxZ65YqL1/tables/tables_7_3.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u2084(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents the subjective and objective evaluation results for 35 different LLMs on the CRITICEVAL test set.  The evaluation is broken down into four critique dimensions (feedback, correction, comparison, and meta-feedback), each with both subjective and objective metrics.  Dark gray highlights the best performing model(s) in each category, and light gray highlights the worst performing model(s).  The overall score is a composite of performance across the critique dimensions.", "section": "5 Evaluation Metrics"}, {"figure_path": "ZsxZ65YqL1/tables/tables_7_4.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u2084(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents the subjective and objective evaluation results of 35 LLMs on the CRITICEVAL benchmark's test set.  It shows performance across four critique dimensions (feedback, correction, comparison, meta-feedback),  using both subjective (human-rated quality) and objective (correlation with GPT-4 judgments) metrics.  High and low-performing models are highlighted, and p-values are provided for statistical significance of the objective scores.", "section": "5 Evaluation Metrics"}, {"figure_path": "ZsxZ65YqL1/tables/tables_8_1.jpg", "caption": "Table 10: Error pattern distribution (%).", "description": "This table shows the distribution of three types of error patterns (Obvious, Complex, Subtle) across different response quality levels (Low, Medium, High).  Each cell represents the percentage of responses with a particular error pattern and quality level.  It helps illustrate the relationship between response quality and the types of errors generated.", "section": "6.6 Relationship with Response Quality"}, {"figure_path": "ZsxZ65YqL1/tables/tables_8_2.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u2084(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents the subjective and objective evaluation results for various LLMs on the CRITICEVAL benchmark's test set.  It shows scores across four critique dimensions (Feedback, Correction, Comparison, and Meta-feedback), categorized by LLM type (closed-source vs. open-source) and model size.  Dark gray highlights the best-performing LLMs, while light gray indicates the worst.  Significance levels (p-values) for objective scores are also provided. The \"Overall\" column shows the average score across all four dimensions.", "section": "5 Evaluation Metrics"}, {"figure_path": "ZsxZ65YqL1/tables/tables_8_3.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u2084(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents the subjective and objective evaluation results of 35 different LLMs on the CRITICEVAL test set.  The evaluation is broken down by four critique dimensions (feedback, correction, comparison, and meta-feedback) and shows both subjective (human-rated) and objective (GPT-4-based) scores.  Dark gray highlights the best-performing LLMs, and light gray highlights the worst-performing LLMs for each dimension.  The overall score is the average score across all four dimensions.", "section": "5 Evaluation Metrics"}, {"figure_path": "ZsxZ65YqL1/tables/tables_9_1.jpg", "caption": "Table 1: Comparison between the test and dev set of benchmarks and CRITICEVAL. A complete list are described in Appendix C. The response quality in some benchmarks is unclassified (-). PR denotes the Pass Rate on reasoning and coding tasks. Our concurrent works are marked with a \u2020.", "description": "This table compares CRITICEVAL with other existing benchmarks for evaluating the critique capabilities of LLMs.  It highlights key differences across several dimensions, including the format and number of critiques, the number of dimensions evaluated, the response quality (whether it is classified or not), the presence of subjective and objective metrics, and whether human annotation was used and if the benchmark has been publicly released.  The table helps to demonstrate CRITICEVAL's comprehensiveness and reliability compared to prior work.", "section": "1 Introduction"}, {"figure_path": "ZsxZ65YqL1/tables/tables_18_1.jpg", "caption": "Table 1: Comparison between the test and dev set of benchmarks and CRITICEVAL. A complete list are described in Appendix C. The response quality in some benchmarks is unclassified (-). PR denotes the Pass Rate on reasoning and coding tasks. Our concurrent works are marked with a \u2020.", "description": "This table compares CRITICEVAL with other existing benchmarks for evaluating the critique ability of LLMs.  It shows several key features of each benchmark, including the format of the critiques (natural language or scalar), the number of critique dimensions evaluated, the response quality (if classified), the size of the test and development datasets, whether subjective metrics (human annotation) and objective metrics were used in the evaluation, and whether the dataset is publicly available. The table highlights CRITICEVAL's comprehensiveness and reliability by showing its larger scale, more diverse task scenarios, and more robust evaluation methodology.", "section": "4 CRITICEVAL Construction"}, {"figure_path": "ZsxZ65YqL1/tables/tables_19_1.jpg", "caption": "Table 1: Comparison between the test and dev set of benchmarks and CRITICEVAL. A complete list are described in Appendix C. The response quality in some benchmarks is unclassified (-). PR denotes the Pass Rate on reasoning and coding tasks. Our concurrent works are marked with a \u2020.", "description": "This table compares CRITICEVAL with other existing benchmarks for evaluating the critique capabilities of large language models.  It compares various aspects such as the format and number of critiques, the number of dimensions evaluated, the response quality, the size of the dataset, and whether subjective or objective metrics, or human annotation are used.  The table highlights CRITICEVAL's comprehensiveness and its use of both scalar and textual critiques.", "section": "1 Introduction"}, {"figure_path": "ZsxZ65YqL1/tables/tables_19_2.jpg", "caption": "Table 1: Comparison between the test and dev set of benchmarks and CRITICEVAL. A complete list are described in Appendix C. The response quality in some benchmarks is unclassified (-). PR denotes the Pass Rate on reasoning and coding tasks. Our concurrent works are marked with a \u2020.", "description": "This table compares CRITICEVAL with other existing benchmarks for evaluating the critique capabilities of LLMs.  The comparison covers several key aspects, including the critique format (whether it's natural language or scalar), the number of critique dimensions evaluated, the response quality of the data used, the size of the test and development sets, whether objective and subjective metrics are used, whether human annotations were used, and whether the benchmark data has been publicly released.  The table highlights CRITICEVAL's advantages in terms of comprehensiveness and reliability compared to existing benchmarks.", "section": "4 CRITICEVAL Construction"}, {"figure_path": "ZsxZ65YqL1/tables/tables_20_1.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u2084(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents the subjective and objective evaluation results for 35 different LLMs on the CRITICEVAL benchmark's test set.  The results are broken down by four critique dimensions (feedback, correction, comparison, and meta-feedback) and provide both subjective (human-rated) and objective (GPT-4-based) scores.  Dark gray and light gray shading highlight the best and worst performing models, respectively. The overall score is an average across all four dimensions.", "section": "5 Evaluation Metrics"}, {"figure_path": "ZsxZ65YqL1/tables/tables_22_1.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u2084(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents a comprehensive evaluation of various Large Language Models (LLMs) on the CRITICEVAL benchmark.  It shows both subjective and objective scores for four critique dimensions (feedback, correction, comparison, and meta-feedback).  The best and worst-performing models are highlighted, and statistical significance is indicated where relevant. The overall scores represent an aggregate measure across all dimensions.", "section": "5 Evaluation Metrics"}, {"figure_path": "ZsxZ65YqL1/tables/tables_32_1.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u300f(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents the subjective and objective evaluation results for 35 different LLMs on the CRITICEVAL test set.  The evaluation is broken down into four critique dimensions: feedback, correction, comparison, and meta-feedback.  For each dimension and each LLM, subjective scores (human evaluation) and objective scores (GPT-4 based evaluation) are provided.  The table highlights the best and worst performing LLMs and indicates statistical significance where applicable.", "section": "Evaluation Metrics"}, {"figure_path": "ZsxZ65YqL1/tables/tables_33_1.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u2084(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents the subjective and objective evaluation results for 35 different LLMs on the CRITICEVAL benchmark's test set.  The evaluation is broken down by four critique dimensions (feedback, correction, comparison, meta-feedback) and includes both subjective (human-rated quality) and objective (automatically computed) scores.  Dark gray highlights the top-performing LLMs for each metric, while light gray highlights the worst-performing.  The overall score represents an average across the critique dimensions.", "section": "5 Evaluation Metrics"}, {"figure_path": "ZsxZ65YqL1/tables/tables_33_2.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u300f(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents the subjective and objective evaluation results for various large language models (LLMs) on the CRITICEVAL benchmark.  The objective metrics measure the correlation between LLM-generated critique scores and human judgments across four critique dimensions: feedback, correction, comparison, and meta-feedback.  Subjective scores represent human ratings of the quality of textual critiques.  The table highlights the best and worst performing models in each dimension, indicating the relative strengths and weaknesses of each LLM in providing critiques.", "section": "5 Evaluation Metrics"}, {"figure_path": "ZsxZ65YqL1/tables/tables_33_3.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u300f(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents the subjective and objective evaluation results for various LLMs on the CRITICEVAL benchmark's test set.  The evaluation metrics cover four critique dimensions: feedback, correction, comparison, and meta-feedback.  Performance is shown for both closed-source and open-source models, highlighting the best and worst performers.  Statistical significance (p-value) is indicated for objective feedback and meta-feedback scores. The overall score is calculated by averaging across all four dimensions.", "section": "Evaluation and Analysis"}, {"figure_path": "ZsxZ65YqL1/tables/tables_33_4.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u2084(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents the subjective and objective evaluation results for 35 different LLMs on the CRITICEVAL benchmark's test set.  The evaluation is broken down into four critique dimensions: feedback, correction, comparison, and meta-feedback.  Both scalar (numerical scores) and natural language (textual critiques) formats are included. Dark gray highlights the best-performing LLMs, and light gray highlights the worst.  The 'Overall' column shows the aggregate score across all dimensions.  The p-value indicates statistical significance.", "section": "5 Evaluation Metrics"}, {"figure_path": "ZsxZ65YqL1/tables/tables_34_1.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u2084(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents the subjective and objective evaluation results for 35 different LLMs on the CRITICEVAL benchmark's test set.  The results are broken down by four critique dimensions: feedback (Fs), correction (CR), comparison (Fc), and meta-feedback (F\u2084(Fs)).  For each dimension, both subjective and objective scores are shown, along with an overall score.  The table highlights the best and worst performing models in dark and light gray, respectively, and indicates where the p-value for objective scores exceeds 0.05.", "section": "5 Evaluation Metrics"}, {"figure_path": "ZsxZ65YqL1/tables/tables_35_1.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u300f(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents the subjective and objective evaluation results for several large language models (LLMs) on the CRITICEVAL benchmark.  The subjective evaluation uses human ratings on four dimensions of critique: feedback, correction, comparison, and meta-feedback.  Objective evaluation uses metrics like correlation and pass rate to assess the alignment of LLM critiques with human judgments.  The table highlights the best and worst performing LLMs for each metric and dimension.", "section": "5 Evaluation Metrics"}, {"figure_path": "ZsxZ65YqL1/tables/tables_36_1.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u2084(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents the subjective and objective evaluation results for various large language models (LLMs) on the CRITICEVAL benchmark's test set.  The evaluation considers four critique dimensions (feedback, correction, comparison, and meta-feedback).  Dark gray highlights the best performing model, while light gray highlights the worst.  Objective scores use correlation and pass rate metrics, and only scores with p-values below 0.05 are included. Subjective scores are averaged over multiple annotators.  The \"overall\" column represents a combined score across all dimensions.", "section": "5 Evaluation Metrics"}, {"figure_path": "ZsxZ65YqL1/tables/tables_36_2.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u300f(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents the subjective and objective evaluation results for 35 different LLMs on the CRITICEVAL benchmark's test set.  The evaluation considers four critique dimensions (feedback, correction, comparison, and meta-feedback), and uses both subjective (human-rated) and objective (GPT-4-based) metrics.  The table highlights the best and worst performing models for each dimension and overall, providing insights into the relative strengths and weaknesses of various LLMs in critiquing different tasks.", "section": "5 Evaluation Metrics"}, {"figure_path": "ZsxZ65YqL1/tables/tables_36_3.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u2019(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents the subjective and objective evaluation results for 13 different LLMs on the CRITICEVAL benchmark's test set.  The evaluation is broken down into four critique dimensions: feedback (Fs), correction (CR), comparison (Fc), and meta-feedback (F\u2019(Fs)).  Both subjective (human-rated quality) and objective (automatically computed using GPT-4) scores are reported for each dimension, along with an overall score combining all dimensions.  Dark and light gray shading highlights the best and worst performing models, respectively.", "section": "6 Evaluation and Analysis"}, {"figure_path": "ZsxZ65YqL1/tables/tables_36_4.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u2019(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents the subjective and objective evaluation results for 35 different LLMs on the CRITICEVAL benchmark's test set.  The evaluation is broken down into four critique dimensions (feedback, correction, comparison, meta-feedback), with both subjective (human-rated) and objective (automatically computed) scores provided.  The table highlights the best and worst performing models for each dimension and overall.", "section": "5 Evaluation Metrics"}, {"figure_path": "ZsxZ65YqL1/tables/tables_37_1.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u2019(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents the subjective and objective evaluation results for 35 different LLMs on the CRITICEVAL benchmark's test set.  The evaluation is broken down into four critique dimensions: feedback, correction, comparison, and meta-feedback.  Both subjective (human-rated) and objective (automatically computed using GPT-4) scores are provided for each dimension.  The table highlights the best and worst performing models for each metric and overall.", "section": "5 Evaluation Metrics"}, {"figure_path": "ZsxZ65YqL1/tables/tables_38_1.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u300f(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents the subjective and objective evaluation results for 35 different LLMs on the CRITICEVAL benchmark's test set.  The evaluation is broken down into four critique dimensions (feedback, correction, comparison, and meta-feedback) and includes both subjective (human-rated quality) and objective (correlation with GPT-4 judgments) metrics.  Darker shading indicates better performance, and a \u2020 symbol denotes objective scores with a p-value > 0.05. The 'Overall' column provides an aggregate score across all dimensions.", "section": "Evaluation and Analysis"}, {"figure_path": "ZsxZ65YqL1/tables/tables_39_1.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u2084(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents the subjective and objective evaluation results for various LLMs on the CRITICEVAL benchmark's test set.  The results are broken down by four critique dimensions (feedback, correction, comparison, meta-feedback) and include both subjective (human-rated) and objective (automatically computed) scores.  Darker shading indicates better performance, and a \u2020 symbol denotes objective scores with a p-value greater than 0.05. The overall score is the average across all dimensions.", "section": "5 Evaluation Metrics"}, {"figure_path": "ZsxZ65YqL1/tables/tables_39_2.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u2084(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents the subjective and objective evaluation results for several large language models (LLMs) on the CRITICEVAL benchmark.  The subjective scores reflect human judgment of the quality of the LLMs' critiques, while the objective scores measure the correlation between the LLMs' evaluations and human judgments. The table breaks down the results by four critique dimensions (feedback, correction, comparison, and meta-feedback), showing the performance of each LLM in each dimension and overall.  The table highlights the best and worst performing models, indicating which models excel and where improvements are needed.", "section": "6 Evaluation and Analysis"}, {"figure_path": "ZsxZ65YqL1/tables/tables_40_1.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u300f(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents the subjective and objective evaluation results for 35 different LLMs on the CRITICEVAL benchmark's test set.  The evaluation considers four critique dimensions (feedback, correction, comparison, and meta-feedback), and both subjective (human-rated quality) and objective (automatically computed using GPT-4) metrics are reported.  The table highlights the best and worst performing models for each dimension and provides an overall score.", "section": "5 Evaluation Metrics"}, {"figure_path": "ZsxZ65YqL1/tables/tables_40_2.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u2084(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents a comprehensive evaluation of various Large Language Models (LLMs) on the CRITICEVAL benchmark.  It shows both subjective and objective scores across four critique dimensions: feedback, correction, comparison, and meta-feedback.  The objective scores are based on correlations with human judgements, while subjective scores represent human ratings of the generated critiques. The table highlights the top-performing and worst-performing models in each category and provides an overall score across all dimensions.", "section": "6 Evaluation and Analysis"}, {"figure_path": "ZsxZ65YqL1/tables/tables_41_1.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u2019(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents the subjective and objective evaluation results for various LLMs on the CRITICEVAL benchmark's test set.  It shows the performance of each model across four critique dimensions (feedback, correction, comparison, meta-feedback), with scores indicating quality.  The table highlights the best and worst-performing models, and notes statistical significance (p-values) where applicable.", "section": "5 Evaluation Metrics"}, {"figure_path": "ZsxZ65YqL1/tables/tables_41_2.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u2084(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents the subjective and objective evaluation results for several closed-source and open-source LLMs on the CRITICEVAL benchmark's test set.  The evaluation considers four critique dimensions: feedback, correction, comparison, and meta-feedback.  Both scalar and textual critique formats are assessed.  Dark gray highlights the best-performing model, while light gray highlights the worst-performing model for each metric.  Objective scores are based on correlations with human judgments, while subjective scores are Likert-scale human ratings.", "section": "5 Evaluation Metrics"}, {"figure_path": "ZsxZ65YqL1/tables/tables_42_1.jpg", "caption": "Table 30: Comparison between Human Performance and GPT-4-turbo in feedback dimension (Fs).", "description": "This table presents a comparison of the performance between human annotators and the GPT-4-turbo model in the feedback dimension of the CRITICEVAL benchmark.  It shows the distribution of different failure modes (E1-E6 and \"Other\") in the feedback generated by both humans and GPT-4. Each failure mode represents a specific type of error or deficiency in the feedback. The values represent the percentage of critiques exhibiting each failure mode. This allows for a detailed analysis of the strengths and weaknesses of both human and AI-generated feedback in terms of accuracy and completeness.", "section": "6.8 Fine-grained Failure Modes in Model-Generated Critiques"}, {"figure_path": "ZsxZ65YqL1/tables/tables_42_2.jpg", "caption": "Table 31: Comparison between Human Performance and GPT-4-turbo in comparison dimension (Fc).", "description": "This table presents a comparison of the performance between human annotators and the GPT-4-turbo model on the comparison dimension of the CRITICEVAL benchmark. It breaks down the distribution of different failure modes (E1-E8, Other) in the critiques generated by both humans and GPT-4-turbo, highlighting their relative strengths and weaknesses in identifying and analyzing response quality differences.", "section": "6.8 Fine-grained Failure Modes in Model-Generated Critiques"}, {"figure_path": "ZsxZ65YqL1/tables/tables_42_3.jpg", "caption": "Table 32: Comparison between Human Performance and GPT-4-turbo in correction dimension (CR).", "description": "This table compares the performance of human annotators and the GPT-4-turbo model on the correction dimension of the CRITICEVAL benchmark.  It shows the distribution of different failure modes (E1, E2, E3, and Other) for human and GPT-4 evaluations, highlighting the differences in their critique performance. These failure modes likely represent different types of mistakes made in the corrections.", "section": "6.2 Reliability of Subjective Evaluation in CRITICEVAL"}, {"figure_path": "ZsxZ65YqL1/tables/tables_44_1.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u2084(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents a comprehensive evaluation of various LLMs across four critique dimensions (feedback, correction, comparison, meta-feedback) on the CRITICEVAL test set.  It shows both subjective (human-rated quality) and objective (automatically computed using GPT-4) scores.  The best and worst performing models are highlighted, and statistical significance (p-value) is indicated for objective measures. The overall score summarizes performance across all dimensions.", "section": "5 Evaluation Metrics"}, {"figure_path": "ZsxZ65YqL1/tables/tables_45_1.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u2084(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents the subjective and objective evaluation results for 35 different LLMs on the CRITICEVAL benchmark's test set.  The results are broken down by four critique dimensions (feedback, correction, comparison, meta-feedback) and include both subjective (human-rated) and objective (GPT-4-based) scores.  Dark gray highlights the best-performing model in each category, while light gray highlights the worst.  The overall score is an average across all dimensions.  Note that statistically insignificant objective scores are marked with a dagger symbol.", "section": "6 Evaluation and Analysis"}, {"figure_path": "ZsxZ65YqL1/tables/tables_45_2.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u300f(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents the subjective and objective evaluation results for 35 different LLMs on the CRITICEVAL benchmark's test set.  The results are broken down by four critique dimensions (feedback, correction, comparison, and meta-feedback), showing both subjective (human-rated) and objective (GPT-4-based) scores.  The table highlights the best and worst-performing LLMs in terms of overall performance and also indicates statistical significance for objective scores.  The overall score reflects the average performance across all dimensions.", "section": "6 Evaluation and Analysis"}, {"figure_path": "ZsxZ65YqL1/tables/tables_46_1.jpg", "caption": "Table 2: Subjective and objective evaluation results on the test set of CRITICEVAL. Dark gray and shade gray in this and the following tables highlight the best and worst performance. Objective feedback and meta-feedback scores with > 0.05 p-value are marked with \u2020. Fs, CR, Fc, F\u2084(Fs) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. Overall column denotes the overall score over multiple critique dimensions.", "description": "This table presents the subjective and objective evaluation results for several LLMs on the CRITICEVAL benchmark's test set.  The evaluation considers four critique dimensions: feedback, correction, comparison, and meta-feedback.  Scores are provided for both subjective (human-rated quality) and objective (GPT-4-based) evaluations, highlighting the best and worst performing models.  The overall score reflects the performance across all dimensions.", "section": "5 Evaluation Metrics"}]