[{"figure_path": "klqhrq7fvB/figures/figures_1_1.jpg", "caption": "Figure 1: Summary of our GNN scaling hypotheses studied in the present work. The baseline model is presented in dark grey, followed by different scaling hypotheses illustrated in lighter colors. We analyze the scaling behavior of message-passing networks, graph Transformers and hybrid architectures with respect to the increasing scale of width, depth, number of molecules, number of labels, and diversity of datasets.", "description": "This figure summarizes the different scaling hypotheses tested in the paper.  The authors investigated how varying the model width, depth, the number of molecules and labels in the training dataset, and the diversity of the datasets affected the performance of different GNN architectures (message-passing networks, graph transformers, and hybrid models). The baseline model is shown in dark gray, with variations in lighter colors representing the different scaling hypotheses.", "section": "3 How Do Molecular GNNs Scale?"}, {"figure_path": "klqhrq7fvB/figures/figures_5_1.jpg", "caption": "Figure 2: Effect of scaling different scaling types (columns) to test performance (rows). The standardized mean is calculated as mean of standardized scores for every task in a dataset group, i.e., a mean and standard deviation per task were calculated based on all our models in this study (signs of tasks with lower is better metrics were flipped).", "description": "This figure shows the results of scaling experiments on various aspects of the GNN models, including width, depth, number of molecules, and labels, as well as dataset diversity.  The different scaling methods are shown as columns, and the performance metrics are shown in rows. Each point represents the average standardized performance across multiple tasks.  This illustrates the impact of these factors on model performance for various molecular tasks.", "section": "4.1 Scaling Trends for Pretraining"}, {"figure_path": "klqhrq7fvB/figures/figures_6_1.jpg", "caption": "Figure 2: Effect of scaling different scaling types (columns) to test performance (rows). The standardized mean is calculated as mean of standardized scores for every task in a dataset group, i.e., a mean and standard deviation per task were calculated based on all our models in this study (signs of tasks with lower is better metrics were flipped).", "description": "This figure shows the effects of scaling different aspects of the GNN models (width, depth, number of molecules, and number of labels) on model performance. Each row represents a different performance metric (e.g., AUROC, MAE) and each column shows a different scaling factor.  The results demonstrate a positive scaling trend across all metrics for all scaling factors. The impact of each scaling factor is also shown.", "section": "4.1 Scaling Trends for Pretraining"}, {"figure_path": "klqhrq7fvB/figures/figures_7_1.jpg", "caption": "Figure 4: Comparison of our MolGPS foundation model (that combines fingerprints from the MPNN++, Transformer and hybrid GPS++ model) to the SOTA across TDC, Polaris, and MoleculeNet benchmarks. SOTA refers to the maximum value for each dataset. MolGPS establishes new SOTA on 11/22 TDC tasks and on all but one task among Polaris and MoleculeNet.", "description": "This figure compares the performance of the MolGPS model to the state-of-the-art (SOTA) across three benchmark datasets: TDC, Polaris, and MoleculeNet.  MolGPS is a foundation model that combines fingerprints from three different architectures (MPNN++, Transformer, and hybrid GPS++). The figure shows that MolGPS achieves SOTA performance on a significant portion of the tasks in each dataset, demonstrating its effectiveness as a general-purpose model for various molecular property prediction tasks.", "section": "Experiments"}, {"figure_path": "klqhrq7fvB/figures/figures_8_1.jpg", "caption": "Figure 5: Comparison of our MPNN++ probing (that leverages multiple fingerprints; with and without additional phenomics pretraining) and MolGPS (that leverages fingerprints from MPNN++, Transformer and GPS++) to various baselines across TDC benchmark collection.", "description": "This figure compares the performance of three different models (MPNN++, MPNN++ with phenomics, and MolGPS) on the TDC ADMET benchmark.  The x-axis represents the number of parameters in each model, and the y-axis represents the normalized performance across the 22 tasks in the benchmark. The figure shows that MolGPS significantly outperforms the other models, demonstrating strong scaling behavior and the benefit of incorporating phenomics data into the pretraining process.  Several baselines are also included for comparison, showing the relative performance improvements of the presented models.", "section": "4 Experiments"}, {"figure_path": "klqhrq7fvB/figures/figures_18_1.jpg", "caption": "Figure 2: Effect of scaling different scaling types (columns) to test performance (rows). The standardized mean is calculated as mean of standardized scores for every task in a dataset group, i.e., a mean and standard deviation per task were calculated based on all our models in this study (signs of tasks with lower is better metrics were flipped).", "description": "This figure presents the results of an experiment that investigates the scaling behavior of Graph Neural Networks (GNNs) for molecular graphs in various settings.  Different columns explore different scaling factors (width, depth, number of molecules, datasets, labels) while rows show different performance metrics.  The standardized mean metric is used for better comparison across various tasks.", "section": "4.1 Scaling Trends for Pretraining"}, {"figure_path": "klqhrq7fvB/figures/figures_18_2.jpg", "caption": "Figure 2: Effect of scaling different scaling types (columns) to test performance (rows). The standardized mean is calculated as mean of standardized scores for every task in a dataset group, i.e., a mean and standard deviation per task were calculated based on all our models in this study (signs of tasks with lower is better metrics were flipped).", "description": "This figure shows the effects of scaling different aspects of the models and datasets on their performance.  The rows represent the different performance metrics used, and the columns represent the different aspects being scaled (width, depth, number of molecules, datasets, labels). Each point shows the average standardized performance across multiple tasks. The lighter colors indicate better performance.", "section": "4.1 Scaling Trends for Pretraining"}, {"figure_path": "klqhrq7fvB/figures/figures_19_1.jpg", "caption": "Figure 8: Model performance on the test set throughout training for MPNN++, Transformer, and GPS++ architectures with molecule scaling. Different colors represent models with varying fraction of molecules used for training.", "description": "This figure shows the training curves for three different GNN architectures (MPNN++, Transformer, and GPS++) while varying the fraction of molecules used for training.  The x-axis represents the number of gradient steps, and the y-axis shows the loss for the different datasets used for pretraining. Each line represents a model trained with a different fraction of the total dataset (12.5%, 25%, 50%, 100%).  The figure illustrates how the model performance and training loss change with different amounts of training data and the different GNN architectures. ", "section": "4.1 Scaling Trends for Pretraining"}, {"figure_path": "klqhrq7fvB/figures/figures_20_1.jpg", "caption": "Figure 2: Effect of scaling different scaling types (columns) to test performance (rows). The standardized mean is calculated as mean of standardized scores for every task in a dataset group, i.e., a mean and standard deviation per task were calculated based on all our models in this study (signs of tasks with lower is better metrics were flipped).", "description": "This figure shows the results of experiments evaluating how different scaling factors affect the performance of various GNN models.  The rows represent different performance metrics (e.g., standardized mean for various tasks), and columns show the impact of different scaling factors like width, depth, molecule count, and dataset diversity.  The lighter and darker shades of green indicate better and worse performances respectively. The figure helps to visualize which scaling methods have the greatest effect on improving model performance for specific tasks.", "section": "4.1 Scaling Trends for Pretraining"}, {"figure_path": "klqhrq7fvB/figures/figures_21_1.jpg", "caption": "Figure 2: Effect of scaling different scaling types (columns) to test performance (rows). The standardized mean is calculated as mean of standardized scores for every task in a dataset group, i.e., a mean and standard deviation per task were calculated based on all our models in this study (signs of tasks with lower is better metrics were flipped).", "description": "This figure shows the results of scaling experiments on molecular GNNs.  It analyzes how performance changes with variations in model width, depth, number of molecules, labels, and dataset diversity.  Different rows show different performance metrics. Different columns show the effects of varying the different scaling factors (width, depth, molecules, dataset, labels). The graphs illustrate the scaling behavior of three different GNN architectures (MPNN++, Transformer, and GPS++) across multiple datasets and tasks.", "section": "4.1 Scaling Trends for Pretraining"}, {"figure_path": "klqhrq7fvB/figures/figures_22_1.jpg", "caption": "Figure 2: Effect of scaling different scaling types (columns) to test performance (rows). The standardized mean is calculated as mean of standardized scores for every task in a dataset group, i.e., a mean and standard deviation per task were calculated based on all our models in this study (signs of tasks with lower is better metrics were flipped).", "description": "This figure shows the impact of scaling different aspects of the model and data on the performance across various tasks.  It shows scaling with respect to the model's width (number of parameters), depth (number of layers), the number of molecules in the dataset, the diversity of datasets used, and the number of labels per molecule.  Different colors represent different model architectures (MPNN++, Transformer, and GPS++). The y-axis represents the average standardized performance across different tasks.  Each column represents a different scaling factor, and each row represents a different performance metric.", "section": "4.1 Scaling Trends for Pretraining"}, {"figure_path": "klqhrq7fvB/figures/figures_23_1.jpg", "caption": "Figure 2: Effect of scaling different scaling types (columns) to test performance (rows). The standardized mean is calculated as mean of standardized scores for every task in a dataset group, i.e., a mean and standard deviation per task were calculated based on all our models in this study (signs of tasks with lower is better metrics were flipped).", "description": "This figure shows the effects of scaling on the performance of different GNN architectures.  Each column represents a scaling type (width, depth, number of molecules, dataset ablation, label scaling), and each row represents a performance metric. The standardized mean is shown, which normalizes performance scores across various tasks.  The results demonstrate how changes in model size and dataset affect performance on different tasks, illustrating power law scaling behavior.  For example, increasing the model width significantly improves performance, while increasing the depth has diminishing returns after a certain point.", "section": "4.1 Scaling Trends for Pretraining"}, {"figure_path": "klqhrq7fvB/figures/figures_24_1.jpg", "caption": "Figure 2: Effect of scaling different scaling types (columns) to test performance (rows). The standardized mean is calculated as mean of standardized scores for every task in a dataset group, i.e., a mean and standard deviation per task were calculated based on all our models in this study (signs of tasks with lower is better metrics were flipped).", "description": "This figure shows the results of scaling experiments on various aspects of the GNN models.  The rows represent different performance metrics (e.g., AUROC, R-squared), while the columns represent different scaling factors (e.g., width, depth, number of molecules, dataset diversity, and number of labels). Each cell in the figure shows the standardized mean performance across multiple tasks, which allows for a comparison of scaling effects across different metrics and factors. The use of a standardized mean allows for comparisons even though the datasets involved had different scoring systems.", "section": "4.1 Scaling Trends for Pretraining"}, {"figure_path": "klqhrq7fvB/figures/figures_25_1.jpg", "caption": "Figure 1: Summary of our GNN scaling hypotheses studied in the present work. The baseline model is presented in dark grey, followed by different scaling hypotheses illustrated in lighter colors. We analyze the scaling behavior of message-passing networks, graph Transformers and hybrid architectures with respect to the increasing scale of width, depth, number of molecules, number of labels, and diversity of datasets.", "description": "This figure summarizes the different scaling hypotheses explored in the paper.  It shows how the authors investigated scaling graph neural networks (GNNs) by varying several key factors, including the model's width (number of parameters), depth (number of layers), the size of the molecular dataset (number of molecules), the number of labels per molecule, and the diversity of the datasets.  The baseline model is represented in dark grey, while variations are shown in lighter colors.", "section": "3 How Do Molecular GNNs Scale?"}, {"figure_path": "klqhrq7fvB/figures/figures_25_2.jpg", "caption": "Figure 2: Effect of scaling different scaling types (columns) to test performance (rows). The standardized mean is calculated as mean of standardized scores for every task in a dataset group, i.e., a mean and standard deviation per task were calculated based on all our models in this study (signs of tasks with lower is better metrics were flipped).", "description": "This figure shows the results of scaling experiments on molecular GNNs.  Different scaling types (width, depth, number of molecules, dataset ablation, label fraction) were tested, and their effects on various downstream tasks are shown. The standardized mean performance across tasks is presented, providing a summary of the scaling behaviors for different GNN architectures (MPNN++, Transformer, GPS++).", "section": "4.1 Scaling Trends for Pretraining"}, {"figure_path": "klqhrq7fvB/figures/figures_26_1.jpg", "caption": "Figure 9: Width Scaling: Comparison of probing and finetuning for MPNN++ (left), Transformer (center), and hybrid GPS++ (right) across different model sizes on the TDC benchmark. Darker green shades denote higher/desirable metric values. Average Spearman correlations for MPNN++, Transformer and GPS++ models show improving scaling behavior with increasing number of parameters across the TDC benchmark. The average Spearman correlation between width and performance for probing is 0.69, 0.82 and 0.73, respectively, and 0.72 when finetuning MPNN++, effectively showing that model size plays an important role in predictive performance.", "description": "This figure shows the results of scaling experiments on the TDC benchmark dataset for three different GNN architectures (MPNN++, Transformer, and GPS++).  It demonstrates the impact of increasing model width (number of parameters) on model performance, evaluated using both probing and finetuning strategies.  Darker green indicates better performance.  The Spearman correlation values quantify the strength of the relationship between model size and performance for both probing and finetuning.  This provides evidence that model size is an important factor in achieving better results.", "section": "E.1 Width Scaling"}, {"figure_path": "klqhrq7fvB/figures/figures_26_2.jpg", "caption": "Figure 2: Effect of scaling different scaling types (columns) to test performance (rows). The standardized mean is calculated as mean of standardized scores for every task in a dataset group, i.e., a mean and standard deviation per task were calculated based on all our models in this study (signs of tasks with lower is better metrics were flipped).", "description": "This figure displays the results of scaling experiments on various factors including width, depth, number of molecules, dataset size, and the number of labels.  Each row represents a different performance metric (e.g., AUPRC, Spearman, MAE), and each column a different scaling approach.  The graphs show how these changes affect performance on different downstream tasks, providing insights into how molecular GNNs scale.", "section": "4.1 Scaling Trends for Pretraining"}, {"figure_path": "klqhrq7fvB/figures/figures_27_1.jpg", "caption": "Figure 2: Effect of scaling different scaling types (columns) to test performance (rows). The standardized mean is calculated as mean of standardized scores for every task in a dataset group, i.e., a mean and standard deviation per task were calculated based on all our models in this study (signs of tasks with lower is better metrics were flipped).", "description": "This figure shows the results of scaling experiments on molecular GNNs.  It explores how changes in model width, depth, dataset size, and number of labels affect performance across various tasks (L1000, PCBA, PCQM4M). Each row represents a performance metric, and each column shows a different scaling factor. The standardized mean performance is shown for each condition, providing a comprehensive view of the scaling behavior for different GNN architectures (MPNN++, Transformer, GPS++).", "section": "4.1 Scaling Trends for Pretraining"}]