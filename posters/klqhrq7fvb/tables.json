[{"figure_path": "klqhrq7fvB/tables/tables_28_1.jpg", "caption": "Table 1: Comparison of MolGPS variants to the self-supervised GraphMVP model on the MoleculeNet dataset (test AUROC). Note that we only consider the datasets that were not part of our pretraining.", "description": "This table compares the performance of different MolGPS variants (with and without phenomics data, and different model sizes) against the self-supervised GraphMVP model on the MoleculeNet benchmark.  The test AUROC (Area Under the Receiver Operating Characteristic curve) is reported for four datasets (BACE, BBBP, Clintox, and Sider). Only datasets not used in the MolGPS pretraining are included in this comparison.", "section": "F Comparison of MolGPS to Unsupervised Methods"}, {"figure_path": "klqhrq7fvB/tables/tables_28_2.jpg", "caption": "Table 2: Comparison of MolGPS variants to the self-supervised variants of MolE and the standard MolE (supervised + self-supervised pretraining) on TDC benchmark collection. Supervised pretraining significantly improves MolE performance as seen in the normalized performance (left-most column). MolGPS models outperform all MolE models by a large margin, exhibiting the best performance in all but two tasks.", "description": "This table compares the performance of MolGPS variants (with and without phenomics data) against the self-supervised and fully supervised versions of the MolE model.  The comparison is performed on the TDC benchmark dataset, and focuses on normalized MAE scores for various tasks. The results highlight that supervised pretraining greatly improves MolE's performance, and MolGPS consistently surpasses all MolE versions across most tasks.", "section": "F Comparison of MolGPS to Unsupervised Methods"}, {"figure_path": "klqhrq7fvB/tables/tables_29_1.jpg", "caption": "Table 3: Power law constants (\u03b1 in Equation 1) for different downstream tasks from Polaris benchmark when varying the number of parameters (Figure 3).", "description": "This table presents the power law constants (\u03b1) obtained from applying Equation 1 to the results of the width scaling experiments on the Polaris benchmark (shown in Figure 3).  These constants represent how much the loss function changes with respect to changes in model parameters,  specifically for different downstream tasks (T1-T12). The values are shown separately for probing and finetuning strategies, demonstrating differences in how model scaling impacts performance based on the training approach.", "section": "G Scaling Law Details"}, {"figure_path": "klqhrq7fvB/tables/tables_29_2.jpg", "caption": "Table 4: Power law constants (\u03b2 in Equation 2) for pretraining different architectures when varying dataset sizes (Figure 2).", "description": "This table shows the power-law scaling behavior of different GNN architectures (MPNN++, Transformer, and GPS++) during pretraining. It presents the power-law constant (\u03b2) obtained from Equation 2, which relates the training loss to the dataset size. The values of \u03b2 indicate how efficiently the models use the increasing amount of training data.  Higher \u03b2 values suggest better scaling behavior.", "section": "G Scaling Law Details"}]