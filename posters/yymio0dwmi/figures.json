[{"figure_path": "YyMiO0DWmI/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison w.r.t. (a) prior single-device TTA vs. (b) our collaborative TTA. Prior TTA operates on each device independently and may be infeasible in resource-limited devices. In contrast, our collaborative TTA allows devices to share knowledge. Based on this, on different devices, one can choose to solely aggregate the shared knowledge for TTA (Follower Agents), or further conduct backpropagation for knowledge aggregation and new domain knowledge learning (Principal Agents).", "description": "This figure compares single-device and collaborative test-time adaptation (TTA).  Single-device TTA (a) shows each device adapting independently, which can be problematic for resource-constrained devices.  The proposed collaborative TTA (b) introduces a shared knowledge store, allowing devices to leverage each other's learned knowledge.  'Principal Agents' use this knowledge and learn new domain-specific knowledge through backpropagation, while 'Follower Agents' simply aggregate the shared knowledge.", "section": "1 Introduction"}, {"figure_path": "YyMiO0DWmI/figures/figures_2_1.jpg", "caption": "Figure 2: An illustration of our proposed CoLA. We maintain a shared domain vector set T to explicitly store the knowledge learned by each principal agent during adaptation. Based on T, for Principal Agents, we jointly learn the domain-specific parameters A and the reweighting term \u03b1 via backward propagation, where the learned knowledge is then stored in T. For Follower Agents, we adaptively aggregate the shared knowledge in T in a forward-only manner, based on the domain similarities, which prioritizes knowledge derived from domains that are similar to the testing domain.", "description": "This figure illustrates the CoLA framework.  It shows how shared domain knowledge vectors (T) are maintained and used by both principal and follower agents. Principal agents use backpropagation to learn new domain-specific parameters (A) and a reweighting term (\u03b1) to reprogram the shared knowledge. Follower agents use a simpler, optimization-free method to aggregate shared knowledge based on domain similarity.", "section": "3 Cross-device collaborative test-time adaptation"}, {"figure_path": "YyMiO0DWmI/figures/figures_8_1.jpg", "caption": "Figure 3: Ablation study of CoLA. In left, we compare the sample efficiency on Device2 in Table 2, Gaussian. Model's accuracy is recorded on the entire test set after adapting to N test samples. In right, we evaluate the effectiveness of Tf on seen (i.e., ImageNet-C, Gaussian) and unseen distributions (i.e., ImageNet-R/Sketch), where CoLA exploits weights of ETA+CoLA in Table 2 for Eqn. (3).", "description": "This figure presents an ablation study of the proposed CoLA method. The left subplot shows a comparison of sample efficiency on Device 2 from Table 2 using the Gaussian noise corruption type from ImageNet-C. It compares the performance of ETA with and without the proposed CoLA method, showing significant improvement with CoLA. The right subplot demonstrates the impact of the temperature scaling factor Tf (introduced in Equation 5 of the paper) on the model's accuracy. It evaluates the accuracy of the model on seen and unseen distributions using different values of Tf, showing the robustness of the method across different distribution types.", "section": "4.2 Ablation studies and more discussions"}, {"figure_path": "YyMiO0DWmI/figures/figures_19_1.jpg", "caption": "Figure 1: Comparison w.r.t. (a) prior single-device TTA vs. (b) our collaborative TTA. Prior TTA operates on each device independently and may be infeasible in resource-limited devices. In contrast, our collaborative TTA allows devices to share knowledge. Based on this, on different devices, one can choose to solely aggregate the shared knowledge for TTA (Follower Agents), or further conduct backpropagation for knowledge aggregation and new domain knowledge learning (Principal Agents).", "description": "This figure compares single-device and collaborative test-time adaptation (TTA).  Single-device TTA operates independently on each device, potentially failing on resource-constrained devices.  Collaborative TTA shares knowledge across devices, enabling two adaptation strategies: 1) knowledge reprogramming (principal agents with sufficient resources) and 2) similarity-based aggregation (follower agents with limited resources).", "section": "1 Introduction"}, {"figure_path": "YyMiO0DWmI/figures/figures_26_1.jpg", "caption": "Figure 3: Ablation study of CoLA. In left, we compare the sample efficiency on Device2 in Table 2, Gaussian. Model's accuracy is recorded on the entire test set after adapting to N test samples. In right, we evaluate the effectiveness of Tf on seen (i.e., ImageNet-C, Gaussian) and unseen distributions (i.e., ImageNet-R/Sketch), where CoLA exploits weights of ETA+CoLA in Table 2 for Eqn. (3).", "description": "This figure presents an ablation study of the proposed CoLA method. The left subplot shows a comparison of sample efficiency on Device 2 (from Table 2) using the Gaussian noise corruption from ImageNet-C.  It compares the accuracy of ETA and ETA+CoLA (with and without the temperature scaling factor T<sub>f</sub>) as a function of the number of online samples used for adaptation.  The right subplot demonstrates the impact of the temperature scaling factor T<sub>f</sub>  on the accuracy of CoLA when adapting to both seen (ImageNet-C and Gaussian noise) and unseen (ImageNet-R and ImageNet-Sketch) distributions. The results highlight CoLA's sample efficiency and robustness across different datasets.", "section": "Ablation studies and more discussions"}, {"figure_path": "YyMiO0DWmI/figures/figures_26_2.jpg", "caption": "Figure 3: Ablation study of CoLA. In left, we compare the sample efficiency on Device2 in Table 2, Gaussian. Model's accuracy is recorded on the entire test set after adapting to N test samples. In right, we evaluate the effectiveness of Tf on seen (i.e., ImageNet-C, Gaussian) and unseen distributions (i.e., ImageNet-R/Sketch), where CoLA exploits weights of ETA+CoLA in Table 2 for Eqn. (3).", "description": "This figure presents an ablation study of the proposed CoLA method. The left subplot shows a comparison of sample efficiency on Device 2 from Table 2 (Gaussian noise).  The accuracy of the model is evaluated on the complete test set after adapting to varying numbers of test samples, comparing the proposed method with and without the temperature scaling factor (T_f). The right subplot demonstrates the impact of the temperature scaling factor (T_f) on the accuracy of the method across seen (ImageNet-C, Gaussian noise) and unseen (ImageNet-R and ImageNet-Sketch) distributions. The results highlight the importance of T_f for achieving robustness and sample efficiency.", "section": "Ablation studies and more discussions"}, {"figure_path": "YyMiO0DWmI/figures/figures_27_1.jpg", "caption": "Figure 2: An illustration of our proposed CoLA. We maintain a shared domain vector set T to explicitly store the knowledge learned by each principal agent during adaptation. Based on T, for Principal Agents, we jointly learn the domain-specific parameters A and the reweighting term a via backward propagation, where the learned knowledge is then stored in T. For Follower Agents, we adaptively aggregate the shared knowledge in T in a forward-only manner, based on the domain similarities, which prioritizes knowledge derived from domains that are similar to the testing domain.", "description": "This figure illustrates the CoLA framework, showing how principal and follower agents utilize shared domain knowledge vectors (T). Principal agents learn new domain-specific parameters (A) and reweighting terms (\u03b1) via backpropagation, updating T. Follower agents optimize-free aggregate knowledge from T based on domain similarity.", "section": "3 Cross-device collaborative test-time adaptation"}]