[{"figure_path": "0Lr9HQijA1/tables/tables_6_1.jpg", "caption": "Table 1: Accuracy of different partial ratio q on CIFAR-10, CIFAR-100, and CUB-200 for partial label learning. The best and the second best results are indicated in bold and underline respectively.", "description": "This table presents the accuracy results of different partial label learning methods on three datasets: CIFAR-10, CIFAR-100, and CUB-200.  The accuracy is evaluated for various partial ratios (q), representing the probability of a negative label being flipped to a false positive label.  The table shows that the proposed method consistently outperforms other state-of-the-art methods across different datasets and partial ratios.", "section": "4.1 Partial Label Learning"}, {"figure_path": "0Lr9HQijA1/tables/tables_7_1.jpg", "caption": "Table 2: Error rate of different number of labels l on CIFAR-100, STL-10, IMDB, and Amazon Review datasets for semi-supervised learning.", "description": "This table shows the error rate achieved by different semi-supervised learning methods on four datasets (CIFAR-100, STL-10, IMDB, and Amazon Review) with varying numbers of labeled samples (l).  Lower error rates indicate better performance.  The results are averaged across three independent runs.", "section": "4.2 Semi-Supervised Learning"}, {"figure_path": "0Lr9HQijA1/tables/tables_7_2.jpg", "caption": "Table 3: Accuracy of synthetic noise on CIFAR-10 and CIFAR-100 and instance noise on Clothing1M and WebVision for noisy label learning. We use noise ratio of {0.2, 0.5, 0.8} for synthetic symmetric noise and 0.4 for asymmetric label noise. The instance noise ratio is unknown.", "description": "This table presents the accuracy results of different noisy label learning methods on four datasets: CIFAR-10, CIFAR-100, Clothing1M, and WebVision.  The noise is applied in three ways: synthetic symmetric noise, synthetic asymmetric noise, and instance noise. For synthetic noise, different noise ratios (0.2, 0.5, 0.8 for symmetric and 0.4 for asymmetric) are used.  The instance noise ratio is not specified and is inherent to the Clothing1M and WebVision datasets. The table compares the performance of several methods, including the proposed ILL framework, against baseline methods such as CE, Mixup, DivideMix, ELR, and SOP.", "section": "4.3 Noisy Label Learning"}, {"figure_path": "0Lr9HQijA1/tables/tables_8_1.jpg", "caption": "Table 4: Accuracy comparison of mixture of different imprecise labels. We report results of full labels, partial ratio q of {0.1, 0.3, 0.5} for CIFAR-10 and {0.01, 0.05, 0.1} for CIFAR-100, and noise ratio \u03b7 of {0.1, 0.2, 0.3} for CIFAR-10 and CIFAR-100.", "description": "This table presents the accuracy comparison results for the mixed imprecise label learning setting. It compares the performance of different methods under various combinations of partial labels (with different ratios) and noisy labels (with different noise ratios), using CIFAR-10 and CIFAR-100 datasets.  The results are categorized by the partial label ratio and noise ratio, allowing for a detailed analysis of method performance across different levels of label imprecision.", "section": "4.4 Mixed Imprecise Label Learning"}, {"figure_path": "0Lr9HQijA1/tables/tables_8_2.jpg", "caption": "Table 5: Robust test accuracy results of our method on more mixture of imprecise label configurations. l, q and \u03b7 are the number of labels, partial, and noise ratio.", "description": "This table presents the robust test accuracy results of the proposed method on various mixtures of imprecise label configurations.  It shows the performance across different numbers of labels (l), partial ratios (q), and noise ratios (\u03b7). The results demonstrate the robustness and effectiveness of the method in handling various challenging scenarios with different combinations of imprecise labels.", "section": "4.4 Mixed Imprecise Label Learning"}, {"figure_path": "0Lr9HQijA1/tables/tables_18_1.jpg", "caption": "Table 1: Accuracy of different partial ratio q on CIFAR-10, CIFAR-100, and CUB-200 for partial label learning. The best and the second best results are indicated in bold and underline respectively.", "description": "This table presents the accuracy results of different partial label learning methods on three datasets: CIFAR-10, CIFAR-100, and CUB-200.  The accuracy is evaluated under different partial ratios (q), representing the probability of flipping a negative label to a false positive.  The table compares the performance of the proposed ILL method against several existing partial label learning techniques (LWS, PRODEN, CC, MSE, EXP, and PICO). The best and second-best results for each dataset and partial ratio are highlighted.", "section": "4.1 Partial Label Learning"}, {"figure_path": "0Lr9HQijA1/tables/tables_22_1.jpg", "caption": "Table 7: Hyper-parameters for partial label learning used in experiments.", "description": "This table presents the hyperparameters used in the partial label learning experiments described in the paper.  It includes details such as image size, model architecture (ResNet-18), batch size, learning rate, weight decay, learning rate scheduler (cosine), number of training epochs, and the number of classes in each dataset (CIFAR-10, CIFAR-100, CUB-200).  The table provides essential information for reproducibility, allowing researchers to replicate the experimental setup.", "section": "4.1 Partial Label Learning"}, {"figure_path": "0Lr9HQijA1/tables/tables_22_2.jpg", "caption": "Table 8: Comparison with R-CR in partial label learning", "description": "This table compares the performance of the proposed ILL method against the R-CR method on partial label learning using CIFAR-10 and CIFAR-100 datasets with different partial ratios.  It highlights the competitive performance of ILL, showing it outperforms R-CR on CIFAR-10 and achieves comparable results on CIFAR-100. The results suggest that ILL is a robust and effective approach for partial label learning, even when compared against methods with more complex designs such as R-CR.", "section": "4.1 Partial Label Learning"}, {"figure_path": "0Lr9HQijA1/tables/tables_23_1.jpg", "caption": "Table 9: Comparison on instance-dependent partial label learning", "description": "This table compares the performance of different methods on instance-dependent partial label learning using various datasets: MNIST, Kuzushiji-MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100.  The results show the accuracy achieved by each method on each dataset.  The methods compared include VALEN [81], RCR [12], PiCO [13], POP [125], and the proposed method (Ours). The table highlights the proposed method's competitive performance.", "section": "D.2 Partial Label Learning"}, {"figure_path": "0Lr9HQijA1/tables/tables_23_2.jpg", "caption": "Table 1: Accuracy of different partial ratio q on CIFAR-10, CIFAR-100, and CUB-200 for partial label learning. The best and the second best results are indicated in bold and underline respectively.", "description": "This table presents the accuracy results of different partial label learning methods on three benchmark datasets: CIFAR-10, CIFAR-100, and CUB-200.  Each dataset is tested with varying amounts of partial label information (q). The table compares the performance of the proposed method against several state-of-the-art baselines. The best and second-best results for each setting are highlighted in bold and underlined, respectively, showing the superiority of the proposed method across various settings of partial label rates.", "section": "4.1 Partial Label Learning"}, {"figure_path": "0Lr9HQijA1/tables/tables_24_1.jpg", "caption": "Table 1: Accuracy of different partial ratio q on CIFAR-10, CIFAR-100, and CUB-200 for partial label learning. The best and the second best results are indicated in bold and underline respectively.", "description": "This table shows the performance comparison of different partial label learning methods on CIFAR-10, CIFAR-100, and CUB-200 datasets with different partial ratios. The partial ratio represents the probability of flipping negative labels to false positive labels.  The results are presented as accuracy, with the best and second-best results highlighted in bold and underlined, respectively. This allows for assessing the effectiveness of various approaches in handling partial label scenarios.", "section": "4.1 Partial Label Learning"}, {"figure_path": "0Lr9HQijA1/tables/tables_24_2.jpg", "caption": "Table 2: Error rate of different number of labels l on CIFAR-100, STL-10, IMDB, and Amazon Review datasets for semi-supervised learning.", "description": "This table shows the error rates achieved by different semi-supervised learning methods on four datasets (CIFAR-100, STL-10, IMDB, and Amazon Review) with varying numbers of labeled samples (l).  Lower error rates indicate better performance. The results highlight the performance differences between various methods in low-data regimes and demonstrate the effectiveness of the proposed method compared to established baselines.", "section": "4.2 Semi-Supervised Learning"}, {"figure_path": "0Lr9HQijA1/tables/tables_24_3.jpg", "caption": "Table 2: Error rate of different number of labels l on CIFAR-100, STL-10, IMDB, and Amazon Review datasets for semi-supervised learning.", "description": "This table presents the error rates achieved by various semi-supervised learning methods on four different datasets (CIFAR-100, STL-10, IMDB, and Amazon Reviews).  The error rate is shown for different numbers of labeled samples (l) used for training.  It allows comparison of the performance of the proposed method against several state-of-the-art baselines across varying dataset sizes and complexities. Lower error rates indicate better performance.", "section": "4.2 Semi-Supervised Learning"}, {"figure_path": "0Lr9HQijA1/tables/tables_25_1.jpg", "caption": "Table 1: Accuracy of different partial ratio q on CIFAR-10, CIFAR-100, and CUB-200 for partial label learning. The best and the second best results are indicated in bold and underline respectively.", "description": "This table presents the accuracy results of different partial label learning methods on three datasets: CIFAR-10, CIFAR-100, and CUB-200.  Each dataset is tested with various partial ratios (q), representing the probability of a negative label being flipped to a false positive. The table compares the performance of several methods against a fully supervised baseline, highlighting the best and second-best performers for each setting.", "section": "4.1 Partial Label Learning"}, {"figure_path": "0Lr9HQijA1/tables/tables_25_2.jpg", "caption": "Table 15: Test accuracy comparison of instance independent label noise on CIFAR-10N and CIFAR-100N for noisy label learning. The best results are indicated in bold, and the second best results are indicated in underline. Our results are averaged over three independent runs with ResNet34 as the backbone.", "description": "This table presents the test accuracy results for noisy label learning experiments conducted on CIFAR-10N and CIFAR-100N datasets.  Different types of noise are evaluated, including clean data, random noise (three variations), aggregate noise, and worst-case noise.  The results are compared across several state-of-the-art methods (CE, Forward, Co-teaching, DivideMix, ELR, CORES, and SOP) and the proposed ILL method.  The best and second-best results are highlighted, and all results are averaged over three independent runs using ResNet34 as the backbone network.", "section": "4.3 Noisy Label Learning"}, {"figure_path": "0Lr9HQijA1/tables/tables_26_1.jpg", "caption": "Table 3: Accuracy of synthetic noise on CIFAR-10 and CIFAR-100 and instance noise on Clothing1M and WebVision for noisy label learning. We use noise ratio of {0.2, 0.5, 0.8} for synthetic symmetric noise and 0.4 for asymmetric label noise. The instance noise ratio is unknown.", "description": "This table presents the accuracy results of different methods for noisy label learning on four datasets: CIFAR-10, CIFAR-100, Clothing1M, and WebVision.  The noise types include synthetic symmetric and asymmetric noise (with specified noise ratios) and instance-level noise (with unknown ratio).  It compares the performance of the proposed ILL method against several baseline methods, demonstrating its effectiveness in handling noisy labels, across a range of noise settings and datasets.", "section": "4.3 Noisy Label Learning"}, {"figure_path": "0Lr9HQijA1/tables/tables_27_1.jpg", "caption": "Table 4: Accuracy comparison of mixture of different imprecise labels. We report results of full labels, partial ratio q of {0.1, 0.3, 0.5} for CIFAR-10 and {0.01, 0.05, 0.1} for CIFAR-100, and noise ratio \u03b7 of {0.1, 0.2, 0.3} for CIFAR-10 and CIFAR-100.", "description": "This table presents the accuracy results of the proposed ILL framework on datasets CIFAR-10 and CIFAR-100, comparing its performance against several state-of-the-art methods.  The experiment involves a mixture of imprecise labels: full labels, partial labels (with varying partial ratios), and noisy labels (with varying noise ratios). The table shows the accuracy for different combinations of these label types.", "section": "4.4 Mixed Imprecise Label Learning"}, {"figure_path": "0Lr9HQijA1/tables/tables_27_2.jpg", "caption": "Table 1: Accuracy of different partial ratio q on CIFAR-10, CIFAR-100, and CUB-200 for partial label learning. The best and the second best results are indicated in bold and underline respectively.", "description": "This table presents the accuracy results of different partial label learning methods on three benchmark datasets: CIFAR-10, CIFAR-100, and CUB-200.  The accuracy is evaluated under various partial ratios (q), representing the probability of flipping negative labels into false positives. The table compares the proposed method against several existing state-of-the-art methods. The best and second-best performance for each dataset and partial ratio are highlighted.", "section": "4.1 Partial Label Learning"}]