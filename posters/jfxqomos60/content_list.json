[{"type": "text", "text": "Unravelling in Collaborative Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Aymeric Capitainel Etienne Boursier2 ", "page_idx": 0}, {"type": "text", "text": "Antoine Scheidl Eric Moulines1 Michael I. Jordan3 El-Mahdi El-Mhamdi1 ", "page_idx": 0}, {"type": "text", "text": "Alain Durmusl ", "page_idx": 0}, {"type": "text", "text": "1 Centre de Mathematiques Appliquees - CNRS - Ecole polytechnique - Palaiseau, 91120, France 2 INRIA Saclay, Universite Paris Saclay, LMO - Orsay, 91400, France 3 Inria, Ecole Normale Superieure, PSL Research University - Paris, 75, France ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Collaborative learning offers a promising avenue for leveraging decentralized data. However, collaboration in groups of strategic learners is not a given. In this work, we consider strategic agents who wish to train a model together but have sampling distributions of different quality. The collaboration is organized by a benevolent aggregator who gathers samples so as to maximize total welfare, but is unaware of data quality. This setting allows us to shed light on the deleterious effect of adverse selection in collaborative learning. More precisely, we demonstrate that when data quality indices are private, the coalition may undergo a phenomenon known as unravelling, wherein it shrinks up to the point that it becomes empty or solely comprised of the worst agent. We show how this issue can be addressed without making use of external transfers, by proposing a novel method inspired by probabilistic verification. This approach makes the grand coalition a Nash equilibrium with high probability despite information asymmetry, thereby breaking unravelling. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Collaborative learning is a framework in which multiple agents share their data and computational resources to address a common learning task [Blum et al., 2017, Kairouz et al., 2021]. A significant challenge arises when the quality of these distributions is unknown centrally and agents are strategic. Indeed, participants may be tempted to withhold or misrepresent the quality of their data to gain a competitive advantage. These strategic behaviors and their consequences have been studied extensively in the literature on information economics [Mas-Colell et al., 1995, Laffont and Martimort, 2001]. In particular, information asymmetry is known to result in adverse selection, whereby lowquality goods end up dominating the market. In the current paper we study collaborative learning from the perspective of information economics. ", "page_idx": 0}, {"type": "text", "text": "A vivid illustration of adverse selection is found in Akerlof's seminal work on the market for lemons [second-hand cars of low quality, Akerlof, 1970]. Because buyers cannot properly assess the quality of cars on the second-hand market, their inclination to pay decreases. As a consequence, sellers with high-quality cars withdraw from the market, since the proposed price falls below their reservation price. This in turn lowers the buyers\u2019 expectation regarding the average quality of cars on the market, so their willingness to pay decreases even more, which de facto crowds out additional cars. The market may therefore enter a death spiral, up to the point where only low-quality cars are exchanged in any competitive Nash equilibrium. This phenomenon is known as unravelling. The insurance market serves as another poignant example of this effect [Rothschild and Stiglitz, 1976, Einav and Finkelstein, 2011, Hendren, 2013]. In insurance, information asymmetry arises due to the fact that insurees possess private knowledge about their individual risk profiles, which insurers lack. Individuals with higher risk are more inclined to purchase policy, while those with lower risk opt out. Consequently, insurers are left with a pool of policyholders skewed towards higher risk, leading to increased premiums to cover potential losses. This, in turn, prompts low-risk individuals to exit the market, exacerbating adverse selection further\u2014a cycle reminiscent of the unravelling described by Akerlof. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We study the problem of whether collaborative learning could also fall victim to unravelling. We consider strategic agents who have access to sampling distributions of varying quality and wish to jointly train a model. They delegate the training process to a central authority who collects samples so as to maximize total welfare. We ask whether adverse selection can arise when data quality is private information. In particular, the presence of a low-quality data owner may harm the model, prompting high-quality owner to leave the collaboration and train a model entirely on their own. Their departure would decrease the average data quality even more, and create a vicious circle. In the worst case, the coalition of learners would reduce to the lowest data quality owner alone. This question is of prime importance from a practical point of view, because unravelling could jeopardize the long-run stability of collaborative models deployed at large scale. ", "page_idx": 1}, {"type": "text", "text": "Our contribution is threefold: ", "page_idx": 1}, {"type": "text", "text": "1. We provide a rigorous framework for analyzing collaborative learning with strategic agents having data distributions of varying quality. On the one hand, we leverage tools from domain adaptation to capture a notion of data quality formally. On the other hand, we model collaboration as a principal-agent problem, where the principal is an aggregator in charge of collecting samples so as to maximize social welfare. This setup allows us to derive the benchmark welfare-maximizing collaboration scheme when data quality is public information.   \n2. We show that when data quality is private, a naive aggregation strategy which consists in asking agents to declare their quality type and applying the optimal scheme results in a complete unravelling. More precisely, the set of agents willing to collaborate is either empty or made of the lowest-quality data owner alone at any pure Nash equilibrium.   \n3. We present solutions to unravelling. When transfers are allowed, the VCG mechanism suffices to re-establish optimality. When transfers are not possible, we leverage probabilistic verification techniques to design a mechanism which breaks unravelling. More precisely, we ensure that the optimal, grand coalition ranks with high probability among the Nash equilibria of the game induced by our mechanism. We demonstrate how to implement our mechanism practically in the setting of classification. ", "page_idx": 1}, {"type": "text", "text": "Related work. The issue of information asymmetry in machine learning has been an area of recent activity. Several learning settings have been considered, including bandits [Wei et al., 2024], linear regression [Donahue and Kleinberg, 2021a,b], classification [Blum et al., 2021] and empirical risk minimization [Dorner et al., 2023, Liu et al., 2023] in a federated context [Tu et al., 2022]. ", "page_idx": 1}, {"type": "text", "text": "Most of these studies focus on the sub-problem of moral hazard, where agents take actions that are unobserved by others. This situation usually results in under-provision of effort and inefficiency at the collective scale [Laffont and Martimort, 2001]. This issue appears naturally in federated learning, because model updates are performed locally. For instance, Karimireddy et al. [2022] show that heterogeneity in sampling costs results in total free-riding without a proper incentive scheme. Huang et al. [2022] show that under-provision of data points in federated learning arises from privacy concerns. Yan et al. [2023] consider a federated classification game where agents can reduce the noise in their data distribution but incur a costly effort to do so. In the same vein, Huang et al. [2023] study the case of agents who are interested in different models, and may skew their sampling measure accordingly. Saig et al. [2023] and Ananthakrishnan et al. [2023] study hidden actions when a principal delegates a predictive task to another agent, and show that thresholds or linear contracts are able to approximate the optimal contracts. ", "page_idx": 1}, {"type": "text", "text": "Adverse selection is another type of information asymmetry, where preferences of agents are unobserved rather than actions. This issue also naturally arises in collaborative learning, because the data distributions from which agents sample or about which they care may not be public. While data heterogeneity is a widely explored topic in federated learning (see for instance Gao et al. 2022 for a general survey, and Fu et al. 2023 for the specific problem of client selection), the strategic aspect has been rarely considered. Most studies doing so focus on hidden sampling costs (see, e.g.. Karimireddy et al. 2022, or Wei et al. 2024 in a bandit context), but few address the fundamental problem of distribution shift. Ananthakrishnan et al. [2023] mentions the issue of adverse selection when a principal delegates a predictive task to an agent, and provide qualitative insights about the optimal contract. However, they consider a single agent and leave aside the question of participation. Finally, Werner et al. [2024] and Tsoy and Konstantinov [2024] study the question of the stability of collaborative learning between competing agents. However, their analysis relies on ad-hoc market structures rather than information asymmetry per se. As such, our work is the first to demonstrate the effect of imperfect information on the sustainability of collaborative learning. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Organization.  Section 2 presents our model and assumptions. Section 3 studies a full information benchmark, which allows us to derive the welfare-maximizing contribution scheme. In Section 4, we turn to the more realistic case where data quality is private information. We first show that in this case, a naive aggregation method leads to total unravelling. Second, we introduce a mechanism which breaks unravelling by inducing a game where the grand coalition is a pure strategy Nash equilibrium with high probability. ", "page_idx": 2}, {"type": "text", "text": "2 Model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Statistical framework. Let $(\\mathsf{X},{\\mathcal{X}})$ and $(\\mathsf{Y},\\mathcal{Y})$ be two measurable spaces and denote by $\\mathcal{P}$ a family of probability measures on $(\\mathsf{X}\\times\\mathsf{Y},{\\mathcal{X}}\\otimes{\\mathcal{Y}})$ . We consider $[J]\\,=\\,{\\bar{\\left\\{1,\\dots,J\\right\\}}}$ agents who aim to perform a prediction task associated with a hypothesis class ${\\mathcal{G}}\\,\\subset\\,\\{g:\\mathsf{X}\\to\\mathsf{Y}\\}$ , a loss function $\\ell:\\mathsf{Y}\\times\\mathsf{Y}$ and a probability measure $P_{0}\\in\\mathcal{P}$ . Each agent seeks to minimize $\\dot{g}\\in\\mathcal{G}\\mapsto\\mathcal{R}_{P_{0}}(g)$ where for any probability distribution $P\\in\\mathcal P$ \uff0c $\\begin{array}{r}{\\mathcal{R}_{P}(g)=\\overline{{\\int\\ell(g(x),y)}}\\mathrm{d}P(x,y)}\\end{array}$ is the risk associated to $g\\in{\\mathcal{G}}$ with respect to $P$ ", "page_idx": 2}, {"type": "text", "text": "We leverage tools and results from statistical learning theory. Denote by $\\{(X_{i},Y_{i})\\}_{i\\geqslant1}$ the canonical process on $\\times\\times\\Upsilon$ and denote by $\\mathbb{P}_{P}$ and $\\mathbb{E}_{P}$ the canonical probability and expectation under which $\\mathbf{\\bar{\\{}}}(X_{i},Y_{i})\\}_{i\\geqslant1}$ are i.i.d. random variables with distribution $P\\;\\in\\;\\mathcal{P}$ . With this notation, we can introduce our assumptions on $\\mathcal{G}$ and $\\mathcal{P}$ ", "page_idx": 2}, {"type": "text", "text": "H1. For $\\delta\\,\\in\\,(0,1).$ there exist $\\alpha_{\\delta}\\,>\\,0,$ $\\beta\\,>\\,0$ and $\\gamma>0$ such that for any distribution $P\\in\\mathcal P$ hypothesis $g\\in{\\mathcal{G}}$ and $n\\geqslant1$ ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{P}_{P}\\left(\\left|\\mathcal{R}_{P}(g)-\\frac{1}{n}\\sum_{i=1}^{n}\\ell(g(X_{i}),Y_{i})\\right|\\leqslant\\frac{\\alpha_{\\delta}}{(1+n)^{\\gamma}}+\\beta\\right)\\geqslant1-\\delta\\;.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This assumption covers a wide range of situations. For instance in the classification case where $\\textsf{Y}=\\{-1,1\\}$ and $\\ell\\;:\\;(y,\\tilde{y})\\;\\mapsto\\;\\mathbb{1}_{\\{y\\tilde{y}\\leqslant0\\}}$ \uff0c $\\alpha_{\\delta}~=~\\sqrt{\\ln(1/\\delta)/2}$ \uff0c $\\beta~=~2\\mathrm{RAD}(\\mathcal{G})$ and $\\gamma\\,=\\,1/2$ where $\\mathbf{RAD}(\\mathcal{G})$ is the empirical Rademacher complexity of $\\mathcal{G}$ [Bousquet et al., 2003]. Similarly, the Bayesian PAC approach in the linear regression context with bounded loss leads to $\\alpha_{\\delta}=$ $\\mathrm{KL}(\\rho\\|\\pi)+\\ln(1/\\delta).$ $\\bar{\\beta}=\\|\\ell\\|_{\\infty}^{2}/8$ and $\\gamma=1$ where $\\rho$ and $\\pi$ are any distributions on $\\mathcal{G}$ [Shalaeva et al., 2019, Corollary 4]. ", "page_idx": 2}, {"type": "text", "text": "For ease of notation, we let $\\mathcal{R}_{j}(g)$ serve as a shorthand for $\\mathcal{R}_{P_{j}}(g)$ . It is moreover assumed that agent $j\\in[J]$ cannot directly sample from $P_{0}$ , but has instead access to a distribution $P_{j}\\in\\mathcal{P}$ which deviates from $P_{0}$ according to the $\\mathcal{G}$ -divergence: ", "page_idx": 2}, {"type": "text", "text": "H2. For any $j\\in[J],\\,P_{j}\\in\\mathcal{P}$ has finite $\\mathcal{G}$ -divergence: $\\begin{array}{r}{\\theta_{j}=\\operatorname*{sup}_{g\\in\\mathcal{G}}\\vert\\mathcal{R}_{j}(g)-\\mathcal{R}_{0}(g)\\vert<+\\infty\\ .}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "Intuitively, for any $j\\in[J],\\,\\theta_{j}\\geqslant0$ models the bias incurred by having access to samples from $P_{j}$ instead of the target distribution $P_{0}$ . More precisely, the risk excess associated with empirical risk minimization (ERM) based on samples from $P_{j}$ is in the worst case at least $\\theta_{j}$ : A poor sampling distribution $P_{j}$ which corresponds to a high $\\theta_{j}$ in the previous expression\u2014-might be the consequence of low-quality sensors or degraded experimental conditions resulting in noisier data points. ", "page_idx": 2}, {"type": "text", "text": "The class of discrepancies appearing in $\\mathbf{H}2$ has been considered in the domain adaptation literature [see, e.g., Ben-David et al., 2010, Kifer et al., 2004, Konstantinov and Lampert, 2019]. It provides a natural framework to analyze the behavior of models trained on diverse distributions, and is practically appealing since it can be easily estimated in the context of classification [Ben-David et al., 2010]. We make the following assumptions on the quality indexes $(\\theta_{1},\\ldots,\\theta_{J})$ ,hereafter referred to as types. H3. There exists $(\\underline{{\\theta}},\\bar{\\theta})\\in\\mathbb{R}_{+}^{2}$ suchthat $\\underline{{\\theta}}\\leqslant\\theta_{1}<\\theta_{2}<.\\,.\\,.<\\theta_{J}\\leqslant\\bar{\\theta}$ ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "H3 The ordering assumption is just for ease of exposition but is neither used by the aggregator nor the agents. Our condition that types are strictly different is for convenience in simplifying the proofs. ", "page_idx": 3}, {"type": "text", "text": "Collaborative learning framework. We further suppose that agent $j\\in[J]$ can fit a model $g\\in{\\mathcal{G}}$ based on i.i.d. samples $\\{(X_{1}^{j},Y_{1}^{j}),\\ldots,(X_{n_{j}}^{j},Y_{n_{j}}^{j})\\}$ of size $n_{j}\\geqslant0$ from $P_{j}\\in\\mathcal{P}$ in one of two ways: they can compute on their own, or collaborate. This is captured by the two following options: ", "page_idx": 3}, {"type": "text", "text": "Option 1\uff09Agent $j$ performs ERM on their own samples: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{g}_{j}=\\operatorname*{argmin}_{g\\in\\mathcal{G}}\\widehat{\\mathcal{R}}_{j}(g)\\;,\\qquad\\widehat{\\mathcal{R}}_{j}(g)=n_{j}^{-1}\\sum_{i=1}^{n_{j}}\\ell(g(X_{i}^{j}),Y_{i}^{j})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This non-collaborative procedure is referred to as the outside option. ", "page_idx": 3}, {"type": "text", "text": "Option 2) Agent $j$ can take part in a coalition orchestrated by a central data aggregator, encoded as $\\mathsf{B}\\,=\\,\\bigl(B_{1},\\ldots,B_{J}\\bigr)\\,\\in\\,\\bigl\\{0,1\\bigr\\}^{J}$ ,where $B_{j}\\,=\\,1$ means that agent $j$ is member of the coalition. We also write $\\mathcal{B}=\\{j\\in[J]:\\,B_{j}=1\\}$ . In exchange for their samples, agent $j$ gains access to the collaborative model trained over the concatenation of samples: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\widehat{g}_{\\mathsf{B}}=\\operatorname{argmin}_{g\\in\\mathcal{G}}\\widehat{\\mathcal{R}}_{\\mathsf{B}}(g)\\:,}\\\\ {\\mathsf{s}}&{\\displaystyle\\widehat{\\mathcal{R}}_{\\mathsf{B}}(g)=N^{-1}\\sum_{j\\in[J]}B_{j}\\sum_{i=1}^{n_{j}}\\ell(g(X_{i}^{j}),Y_{i}^{j})\\quad\\mathrm{~and~}\\quad N=\\sum_{j\\in[J]}B_{j}n_{j}\\:.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Agent utilities.  We assume that agents incur a unitary cost for sampling from their distribution and dislike statistical risk. Therefore, a baseline model for measuring the preferences associated with a model $g\\in{\\mathcal{G}}$ and a number of samples $n$ is based on a linear map: $(\\bar{g_{,}}n)\\mapsto-a\\mathcal{R}_{0}(g)-c n$ for $a,c>0$ . In practice, however, $\\mathcal{R}_{0}(\\bar{g})$ is typically unknown so agents instead can use a PAC bound of the form $\\mathbb{P}(\\mathcal{R}_{0}(\\hat{g}_{n})\\leqslant\\mathcal{R}_{0}^{\\star}+\\varepsilon)\\geqslant1-\\delta$ to a assess a model $\\hat{g}_{n}\\in\\mathcal{G}$ trained over their samples, where $\\varepsilon>0$ $\\delta\\in(0,1)$ and $\\mathcal{R}_{0}^{\\star}=\\operatorname*{inf}_{g\\in\\mathcal{G}}\\mathcal{R}_{0}(g)$ . Our next result shows that $\\textbf{H}1$ and $\\mathbf{H}\\,2$ allow each agent to pin down such an $\\varepsilon>0$ , under either Option 1) or Option 2). Assuming $\\mathbf{H}1$ \uff0c define the function $\\varepsilon$ for any $(\\theta,n)\\in\\Theta\\times\\mathbb{R}_{+}$ by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\varepsilon(\\theta,n)=2\\big[\\alpha_{\\delta}(1+n)^{-\\gamma}+\\beta+\\theta\\big]\\ .\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Lemma 1. Assume $H I$ and $_{H2}$ ", "page_idx": 3}, {"type": "text", "text": "(i) Any agent $j\\in[J]$ picking the outside Option 1) obtains a model $\\widehat{g}_{j}\\in\\mathcal{G}$ achieving ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{R}_{0}(\\widehat{g}_{j})\\leqslant\\mathcal{R}_{0}^{\\star}+\\varepsilon(\\theta_{j},n_{j})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "(i) Any coalition ${\\mathsf{B}}\\in\\left\\{0,1\\right\\}^{J}$ drawing $\\mathbf{n}=(n_{1},\\ldots,n_{J})\\in\\mathbb{R}_{+}^{J}$ samples obtains amodel $\\widehat{g}_{\\mathsf{B}}\\in\\mathcal{G}$ achieving ", "page_idx": 3}, {"type": "equation", "text": "$\\mathcal{R}_{0}(\\widehat{g}_{\\mathtt{B}})\\leqslant\\mathcal{R}_{0}^{\\star}+\\varepsilon(\\vartheta,N)$ ", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $N$ is the total of samples and $\\vartheta$ is the weighted average type within the coalition: ", "page_idx": 3}, {"type": "equation", "text": "$$\nN(B,{\\mathbf{n}})=\\sum_{j\\in[J]}B_{j}n_{j}\\ ,\\qquad\\vartheta({\\mathsf{B}},{\\mathbf{n}})=N^{-1}\\sum_{j\\in[J]}B_{j}n_{j}\\theta_{j}\\ .\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "When the context is clear, we write $\\varepsilon(\\mathrm{B},\\mathbf{n})=\\varepsilon(\\vartheta(\\mathrm{B},\\mathbf{n}),N(\\mathrm{B},\\mathbf{n}))$ to lighten notation. Based on Lemma 1, we define the utility of agent $j\\in[J]$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\nu_{j}:({\\bf B},{\\bf n})\\mapsto-a[\\mathcal{R}_{0}^{\\star}+(1-B_{j})\\varepsilon(\\theta_{j},n_{j})+B_{j}\\varepsilon({\\bf B},{\\bf n})]-c n_{j}\\ .\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that for any $j\\in[J]$ wetake $n_{j}\\geqslant0$ to be a real number for ease of presentation. In (5), $a/c>0$ captures the extent to which individuals are willing to trade off model quality against sampling cost. ", "page_idx": 4}, {"type": "text", "text": "For any $j\\in[J]$ and ${\\mathsf{B}}_{-j}=\\left(B_{1},\\ldots,B_{j-1},B_{j+1},\\ldots,B_{J}\\right)\\in\\left\\{0,1\\right\\}^{J-1}$ wedenotebya slight abuse of notation $(B,\\mathsf{B}_{-j})=(B_{1},\\dotsc,B_{j-1},B,B_{j+1},\\dotsc,B_{J})$ for any ${\\cal B}\\in\\{0,1\\}$ .Similarly for $\\mathbf{n}_{-j}=$ $(n_{1},\\ldots,n_{j-1},n_{j+1},\\ldots,n_{j})\\,\\in\\,\\mathbb{R}_{+}^{J-1}$ , we write $(n,\\mathbf{n}_{-j})\\,=\\,\\left(n_{1},\\ldots,n_{j-1},n,n_{j+1},\\ldots,n_{j}\\right)$ for any $n\\geqslant0$ . We can then characterize the optimal behavior of any agent picking the outside option as follows. ", "page_idx": 4}, {"type": "text", "text": "Proposition 1. Assume $H I$ and $_{H2}$ For any $j\\,\\in\\,[J],\\,{\\sf B}_{-j}\\,\\in\\,\\{0,1\\}^{J-1}$ and $\\mathbf{n}_{-j}\\,\\in\\,\\mathbb{R}_{+}^{J-1}$ the optimal number of samples to draw under Option $^{\\,l}$ )is $\\mathrm{argmax}_{n\\geq0}\\,u_{j}((0,{\\mathsf{B}}_{-j}),(n,\\mathbf{\\bar{n}}_{-j})\\,;\\,\\theta_{j})=\\underline{{n}}$ where ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\underline{{{n}}}=(2a c^{-1}\\gamma\\alpha_{\\delta})^{1/(\\gamma+1)}-1\\ .\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In what follows, we assume $2a/c>(\\gamma\\alpha_{\\delta})^{-1}$ to exclude the pathological case where no agent is willing to sample data points. From now on, we denote by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\underline{{u}}_{j}=u_{j}\\big((0,\\mathsf{B}_{-j}),(\\underline{{n}},\\mathbf{n}_{-j})\\big)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "the best achievable utility under the outside option. Note that $\\underline{n}$ does not depend on $\\theta_{j}\\in\\Theta$ (but $\\underline{{u}}_{j}$ does) so all agents outside of the coalition draw a same number of data points $\\underline{{n}}\\,>\\,0$ .This result, which may be surprising at first glance, comes from the fact that all agents have the same accuracy-to-sampling-cost ratio $a/c$ in their utility. ", "page_idx": 4}, {"type": "text", "text": "Aggregator. We finally assume that the aggregator acts benevolently to set up a Pareto-optimal collaboration, by maximizing the total welfare under individual rationality. In other words, they solve: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{maximize}}&{\\boldsymbol{W}:\\;(\\mathbf{B},\\mathbf{n})\\in\\{0,1\\}^{J}\\times\\mathbb{R}_{+}^{J}\\mapsto\\displaystyle\\sum_{j\\in[J]}u_{j}(\\mathbf{B},\\mathbf{n})}\\\\ {\\mathrm{subject~to}}&{\\displaystyle\\operatorname*{min}_{j\\in[J]}u_{j}(\\mathbf{B},\\mathbf{n})-\\underline{{u}}_{j}\\geqslant0\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In the Social Choice literature, $W$ is referred to as the utilitarian social welfare function. The participation constraint ensures that no agent within the coalition finds it beneficial to switch to their outside option. Note that $\\mathbf{n}\\in\\mathbb{R}_{+}^{J}$ is required to have non-negative entries, which prevents the aggregator from giving away data points to agents. ", "page_idx": 4}, {"type": "text", "text": "3  Full-Information Benchmark: First-Best Collaboration ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we assume that the profile of types $(\\theta_{1},\\dots,\\theta_{J})\\in\\Theta^{J}$ is public, and study how the aggregator can implement an optimal collaboration among agents under this most-favorable scenario. ", "page_idx": 4}, {"type": "text", "text": "Exact solution. We are looking for a solution to the aggregator's problem (8). For any $\\mathsf{B}\\in\\{0,1\\}^{J}$ and $\\mathbf{n}\\in\\mathbb{R}_{+}^{J}$ denoteby ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{n}}_{j}(\\mathtt{B},\\mathbf{n})=\\operatorname*{max}\\{n\\geqslant0:\\,u_{j}((1,\\mathtt{B}_{-j}),(n,\\mathbf{n}_{-j})\\,;\\,\\theta_{j})\\geqslant\\underline{{u}}_{j}\\}}\\\\ &{\\qquad\\qquad=\\underline{{n}}-(a/c)[\\varepsilon(\\mathtt{B},\\mathbf{n})-\\varepsilon(\\theta_{j},\\underline{{n}})]\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "the maximum number of samples that agent $j$ can be asked to provide within the coalition under its participation constraint, where $\\underline{n}$ is defined as in (6), and $\\underline{{u}}_{j}$ as in (7). With this notation, problem (8) rewrites ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{maximize}\\quad W({\\mathsf{B}},{\\boldsymbol{\\mathbf{n}}})\\quad{\\mathrm{subject~to}}\\quad\\operatorname*{min}_{j\\in B}{\\overline{{n}}}_{j}({\\mathsf{B}},{\\boldsymbol{\\mathbf{n}}})-n_{j}\\geqslant0\\;.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Assume $\\pmb{H}\\allowbreak,\\pmb{H}\\!2,\\pmb{H}\\!3$ Problem (8) admits a unique solution $(\\mathsf{B}^{\\mathrm{opt}},\\mathbf{n}^{\\mathrm{opt}}(\\pmb{\\theta}))\\in\\left\\{0,1\\right\\}^{J}\\times$ $\\mathbb{R}_{+}^{J}$ Moreover, ", "page_idx": 4}, {"type": "equation", "text": "$$\n(i)\\;\\;\\mathrm{B}^{\\mathrm{opt}}={\\bf1}=(1,\\ldots,1),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "(i) Denoting $\\mathbf{n}^{\\mathrm{opt}}(\\pmb{\\theta})=(n_{1}^{\\mathrm{opt}}(\\pmb{\\theta}),\\dots,n_{J}^{\\mathrm{opt}}(\\pmb{\\theta}))$ , there exists $L^{\\mathrm{opt}}\\in[J]$ such that for any $j\\in[J]$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{n_{j}^{\\mathrm{opt}}(\\pmb{\\theta})\\left\\{\\begin{array}{l l}{=\\overline{{n}}_{j}(\\mathbf{1},\\mathbf{n}^{\\mathrm{opt}}(\\pmb{\\theta}))}&{i f j<L^{\\mathrm{opt}},}\\\\ {\\in\\left[\\,0\\,,\\,\\overline{{n}}_{j}(\\mathbf{1},\\mathbf{n}^{\\mathrm{opt}}(\\pmb{\\theta}))\\,\\right]}&{i f j=L^{\\mathrm{opt}},}\\\\ {=0}&{o t h e r w i s e.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The couple $(\\mathsf{B}^{\\mathrm{opt}},\\mathbf{n}^{\\mathrm{opt}})$ is referred to as the optimal contribution scheme. Although implicit, the condition (11) provides insights about the optimal scheme. The aggregator makes everyone enter the coalition, but only asks the $\\mathrm{\\Delta}L^{\\mathrm{opt}}>0$ first-best agents to contribute. This allows to obtain the best possible collaborative model while sparing any sampling cost to other agents. Moreover, the number of required samples $n_{j}^{\\mathrm{opt}}(\\theta)$ slightly differs from $\\underline{n}$ according to the relative performance of the collaborative model with respect to agent $j$ 's one: if the agent gets a better accuracy by collaborating, the aggregator can ask them for more data; if on the other hand the agent gets a worse model by collaborating (i.e., they are a contributor with very high quality data), the aggregator can only ask less data because of the participation constraint. ", "page_idx": 5}, {"type": "text", "text": "Relaxed solution.  Working with the optimal scheme $(\\mathsf{B}^{\\mathrm{opt}},\\mathbf{n}^{\\mathrm{opt}})$ is  difficult  because $\\overline{{n}}_{j}(\\mathbf{1},\\mathbf{n}^{\\mathrm{opt}}(\\pmb{\\theta}))$ has no explicit expression. To make the analysis tractable, we slightly simply the optimal contribution scheme in Theorem 1 and consider the simplified optimal contribution scheme $(\\mathbf{B}^{\\star},\\mathbf{n}^{\\star})$ where $\\mathsf{B}^{\\star}=\\mathsf{B}^{\\mathrm{opt}}=(1,\\ldots,1)$ and for any $j\\in[J]$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{n_{j}^{\\star}(\\theta)=\\mathbb{1}_{\\{j\\leqslant L^{\\star}\\}}\\overline{{n}}_{j}(\\mathbf{1},\\mathbf{n}^{\\star}(\\theta))\\,}&{\\mathrm{and}\\quad L^{\\star}=\\operatorname*{min}\\{j\\in[J]:\\,\\displaystyle\\sum_{k\\leqslant j}\\overline{{n}}_{k}(\\mathbf{1},\\mathbf{n}^{\\star}(\\theta))\\geqslant\\bar{N}\\}~,}\\\\ &{\\mathrm{with}\\quad\\bar{N}=({\\underline{{n}}}+1)J^{\\frac{1}{1+\\gamma}}-1~.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that $(\\mathbf{B}^{\\star},\\mathbf{n}^{\\star})$ only differs from $(\\mathsf{B}^{\\mathrm{opt}},\\mathbf{n}^{\\mathrm{opt}})$ in two ways. First, in $(\\mathsf{B}^{\\star},\\mathbf{n}^{\\star})$ all contributors' participation constraint bind, while in $(\\mathsf{B}^{\\mathrm{opt}},\\mathbf{n}^{\\mathrm{opt}})$ the $L^{\\mathrm{opt}}$ -the one could be slack. Second, the total number of data points required from the coalition is fixed and equal to $\\bar{N}=\\Theta(J^{\\frac{1}{1+\\gamma}})$ . The quantity $\\bar{N}$ comes from a natural relaxation of the original problem Equation (10) where we leave aside an intricate term of the objective function. This relaxation, which is formally described in Appendix A, provides a good approximation of the exact solution in reasonable settings. Indeed, the following result establishes that applying $(\\mathbf{B}^{\\star},\\mathbf{n}^{\\star})$ instead of $(\\mathsf{B}^{\\mathrm{opt}},\\mathbf{n}^{\\mathrm{opt}})$ comes at a negligible welfare cost when types are sufficiently evenly spaced. ", "page_idx": 5}, {"type": "text", "text": "Lemma 2. Assume $H I$ \uff0c $_{H2}$ and $\\theta_{j}-\\theta_{j-1}=\\mathcal{O}(1/J)$ for any $j\\in\\{2,\\dots,J\\}$ . Then, ", "page_idx": 5}, {"type": "equation", "text": "$$\nW({\\sf B}^{\\mathrm{opt}},{\\bf n}^{\\mathrm{opt}}(\\pmb\\theta))=W({\\sf B}^{\\star},{\\bf n}^{\\star}(\\pmb\\theta))+{\\mathcal O}(J^{\\frac{1}{1+\\gamma}})\\;.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Moreover, the following proposition shows that $\\mathbf{n}^{\\star}(\\pmb\\theta)$ admits a workable expression. ", "page_idx": 5}, {"type": "text", "text": "Corollary 1. Assume $H l,H2$ and $H3$ Then $L^{\\star}=\\Theta(J^{\\frac{1}{1+\\gamma}})$ and for any $j\\in[J]$ ", "page_idx": 5}, {"type": "equation", "text": "$$\nn_{j}^{\\star}(\\pmb\\theta)=\\mathbb{1}_{\\{j\\leq L^{\\star}\\}}\\left[\\frac{\\bar{N}}{L^{\\star}}+\\frac{2a}{c}\\left(\\theta_{j}-\\frac{1}{L^{\\star}}\\sum_{\\ell=1}^{L^{\\star}}\\theta_{\\ell}\\right)\\right]\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Since $(\\mathsf{B}^{\\star},\\mathbf{n}^{\\star})$ correctly approximates the optimal scheme while being more tractable, we work with it in the remainder to lighten proofs. ", "page_idx": 5}, {"type": "text", "text": "H4. The aggregator applies the simplified contribution scheme $(\\mathbf{B}^{\\star},\\mathbf{n}^{\\star})$ ", "page_idx": 5}, {"type": "text", "text": "4 Hidden information ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The welfare-maximizing contribution scheme described in (12) depends explicitly on $\\pmb{\\theta}\\in\\Theta^{J}$ ,So it is implementable only if types are public. This often unrealistic, either for legal or competitive reasons. We therefore turn to the problem of setting up a collaboration when types are private. ", "page_idx": 5}, {"type": "text", "text": "4.1  Naive aggregation and unravelling ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "A naive solution to coping with the private nature of $\\pmb{\\theta}\\in\\Theta^{J}$ is for the aggregator to ask agents to disclose their types, and apply the simplified optimal contribution scheme defined in (12). In this setting, however, agents may declare a type $\\tilde{\\theta}_{j}$ different from their true type $\\theta_{j}$ ", "page_idx": 6}, {"type": "text", "text": "This approach corresponds to a direct-revelation mechanism $\\Gamma:(\\mathsf{B},\\widetilde{\\pmb{\\theta}})\\mapsto\\mathbf{n}^{\\star}(\\widetilde{\\pmb{\\theta}})$ which unfolds as follows. ", "page_idx": 6}, {"type": "text", "text": "1. Any agent $j\\in[J]$ declares a tuple $(B_{j},\\widetilde{\\theta}_{j})\\in\\{0,1\\}\\times\\Theta\\cup\\{\\dagger\\}$ .If $B_{j}=1$ , then agent $j$ picks Option 2), and enters the coalition with type ${\\widetilde{\\theta}}_{j}$ .If $B_{j}=0$ , then agent $j$ picks Option 1), their declared type ${\\widetilde{\\theta}}_{j}$ is $\\dagger$ byconvention.   \n2. Setting $\\mathsf{B}=(B_{1},\\ldots,B_{J})$ and $\\tilde{\\theta}\\in(\\Theta\\cup\\{\\dag\\})^{J}$ , then the aggregator applies the contribution scheme defined in (12), so the vector of number of contributions within the coalition is $\\mathbf{n}^{\\star}(\\widetilde{\\pmb{\\theta}})$ ", "page_idx": 6}, {"type": "text", "text": "$\\Gamma$ induces direct revelation game $([J],\\mathcal{S}^{J},(v_{j})_{j\\in[J]})$ where the action space is $\\mathcal{S}=\\{(1,\\widetilde{\\theta})~\\colon\\widetilde{\\theta}\\in$ $\\Theta\\}\\cup\\{(0,\\dag)\\}$ and payoffs are for any $j\\in[J]$ and $\\mathbf{s}\\in\\mathcal{S}^{J}$ \uff0c ", "page_idx": 6}, {"type": "equation", "text": "$$\nv_{j}(s_{j},\\mathbf{s}_{-j})=u_{j}(\\mathbf{B},\\mathbf{n}^{\\star}(\\widetilde{\\theta}))=B_{j}\\Big[{-a\\Big(\\mathcal{R}_{0}^{\\star}+\\varepsilon(\\mathbf{B},\\mathbf{n}^{\\star}(\\widetilde{\\theta}))\\Big)-c n_{j}^{\\star}(\\widetilde{\\theta})\\Big]+(1-B_{j})\\underline{{u}}_{j}\\;.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This mechanism is obviously vulnerable to strategic manipulation, since it disregards incentive compatibility. This has severe consequences for the coalition, as shown by the following proposition. Theorem 2 (Unravelling). Assume $H l$ H2, $H3$ and $H4.$ Let $\\mathcal{E}\\subset\\mathcal{S}^{J}$ be the set of pure-strategy Nashequilibria of thegameinducedby $\\Gamma$ We have ", "page_idx": 6}, {"type": "equation", "text": "$$\n(i)\\ {\\mathcal{E}}\\neq\\emptyset\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Theorem 2 shows that under $\\Gamma$ , the coalition undergoes a full unravelling: it is either empty or comprised solely of the worst agent in any Nash equilibrium. Thus, collaborative learning is not immune to adverse selection, and may suffer from unravelling as any market characterized by information asymmetry. ", "page_idx": 6}, {"type": "text", "text": "Sketch of proof. The profile of actions $((0,\\dag),\\dots,(0,\\dag))$ corresponding to $\\mathsf{B}=(0,\\ldots,0)$ is a pure Nash equilibrium, since forming a lone coalition cannot bring more utility than picking the outside option. Conversely, consider a pure-strategy Nash equilibrium $\\mathbf{s}\\in\\mathcal{E}$ such that $\\textsf{B}\\check{\\neq}\\;(0,\\ldots,0)$ Denote by $\\mathcal{C}=\\{j\\in[J]:\\,B_{j}=1\\}$ and $n_{j}^{\\star}(\\widetilde{\\pmb{\\theta}})>0\\}$ the set of contributors under this equilibrium. It can be shown that (i) for any $(j,k)\\in\\mathcal{C}^{2},\\theta_{j}-\\widetilde{\\theta}_{j}=\\theta_{k}-\\widetilde{\\theta}_{k}$ and (i) For any $j\\in\\mathcal{C}$ $(1,\\widetilde{\\theta}_{j})$ with $\\widetilde{\\theta}_{j}>\\underline{{{\\theta}}}$ is strictly dominated by $(1,\\underline{{\\theta}})$ ${\\widetilde{\\theta\\,}}_{j}\\,=\\,\\underline{{\\theta}}$ at the equilibrium. As a consequence, $\\theta_{j}\\,=\\,\\theta_{k}$ for any $(j,k)\\,\\in\\,\\mathcal{C}^{2}$ , which implies by $\\mathbf{H}3$ that $|{\\mathcal{C}}|=1$ . From the definition of the contribution scheme (2), we can deduce that $\\begin{array}{r}{\\sum_{j\\in[J]}B_{j}\\,=\\,\\ '|\\dot{\\boldsymbol{B}}|\\,=\\,1,}\\end{array}$ beause $|\\beta|>1$ would entail $|{\\mathcal{C}}|>1$ Finally, $B_{J}=1$ because $v_{J}((1,\\underline{{\\theta}}),{\\bf s}_{-J})\\stackrel{}{>}v_{J}((0,\\underline{{\\dagger}}),{\\bf s}_{-J})$ and s is a Nash equilibrium. This leads to $\\mathsf{B}=(0,\\ldots,0,1)$ \u53e3 ", "page_idx": 6}, {"type": "text", "text": "4.2  Breaking unravelling ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The previous results motivate the design of a more sophisticated aggregation scheme that addresses adverse selection. In this section, we discuss how to design such a procedure. ", "page_idx": 6}, {"type": "text", "text": "Is VCG available in our framework? Unravelling occurs under $\\Gamma$ because agents do not find it beneficial to declare their true type and eventually opt for their outside option. This could be avoided bymodifying $\\Gamma$ to make it ", "page_idx": 6}, {"type": "text", "text": "(i) individually rational, that is $v_{j}((1,\\theta_{j}),{\\bf s}_{-j})\\geqslant v_{j}((0,\\dagger),{\\bf s}_{-j})$ for any $j\\in[J]$ \uff0c $\\mathbf{s}_{-j}\\in\\mathcal{S}^{J-1}$ \uff0c (i) and incentive compatible, that is $v_{j}((1,\\theta_{j}),{\\bf s}_{-j})\\geqslant v_{j}((1,\\widetilde{\\theta}_{j}),{\\bf s}_{-j})$ for any $\\widetilde{\\theta}_{j}\\in\\Theta$ ", "page_idx": 6}, {"type": "text", "text": "Under these conditions, the truthful, optimal profile of actions $((1,\\theta_{1}),\\ldots,(1,\\theta_{J}))$ would emerge as a Nash equilibrium. Since the aggregator seeks to minimize the utilitarian function $W$ , one option would be to rely on the VCG mechanism [Vickrey, 1961, Clarke, 1971, Groves, 1973], which is the direct-revelation mechanisms fulfilling these desiderata [Green and Laffont, 1977, Holmstrom, 1979]. Formally, the VCG mechanism writes $\\Gamma^{\\mathrm{vcG}}:\\widetilde{\\pmb{\\theta}}\\mapsto(\\mathbf{n}^{\\star}(\\widetilde{\\pmb{\\theta}}),\\mathbf{t}(\\widetilde{\\pmb{\\theta}}))$ where $\\mathbf{t}(\\widetilde{\\pmb{\\theta}})=(t_{1}(\\pmb{\\theta}),\\dots,t_{J}(\\pmb{\\theta}))\\in$ $\\mathbb{R}^{J}$ is a set of transfers satisfying for any $j\\in[J]$ ", "page_idx": 7}, {"type": "equation", "text": "$$\nt_{j}(\\widetilde{\\pmb{\\theta}})=\\sum_{k\\neq j}u_{k}\\big((0,\\mathbf{1}_{-j}),\\mathbf{n}^{\\star}(\\widetilde{\\pmb{\\theta}})\\big)-\\sum_{k\\neq j}u_{k}\\big(\\mathbf{1},\\mathbf{n}^{\\star}(\\widetilde{\\pmb{\\theta}})\\big)\\ .\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "It re-establishes truthfulness as a dominant strategy by aligning individual payoffs $v_{j}^{\\mathrm{VCG}}(\\widetilde{\\pmb{\\theta}})\\,=$ $u_{j}(\\mathbf{1},\\mathbf{n}^{\\star}(\\widetilde{\\pmb{\\theta}}))-t_{j}(\\widetilde{\\pmb{\\theta}})$ with total social welfare. Unfortunately, the VCG approach is unavailable in our framework, because of the following observation. ", "page_idx": 7}, {"type": "text", "text": "Lemma 3.Thereexists $j\\in[J]$ Suchthat $-t_{j}({\\widetilde{\\pmb{\\theta}}})>0$ ", "page_idx": 7}, {"type": "text", "text": "Lemma 3 shows that some agent would need to receive a strictly positive transfer. This is impossible without a monetary payment-utility can only be decreased by the aggregator, for instance through accuracy shaping [Karimireddy et al., 2022]-, which we exclude here. ", "page_idx": 7}, {"type": "text", "text": "A probabilistic verification-based mechanism.  We now show how to design a mechanism that recovers the optimal collaboration as a Nash equilibrium in high probability without the need for transfers. Inspired by the probabilistic verification approach [Caragiannis et al., 2012, Ferraioli and Ventre, 2018, Ball and Kattwinkel, 2019], we assume that the aggregator can approximately estimate $\\theta_{j}$ with few samples from $P_{j}$ forany $j\\in[J]$ ", "page_idx": 7}, {"type": "text", "text": "H5. There exists a decreasing function $\\eta_{\\delta}:\\;\\mathbb{R}_{+}^{\\star}\\rightarrow\\mathbb{R}_{+}^{\\star}$ ,with $\\delta\\in(0,1)$ defined in $H I$ such that for any $j~\\in[J]$ and i.d samples $(X_{1}^{j},Y_{1}^{j}),\\dots,(X_{q}^{j},Y_{q}^{j})$ of size $q\\,>\\,0$ from $P_{j}$ , there exis $a$ $(X_{1}^{j},Y_{1}^{j}),\\dots,(X_{q}^{j},Y_{q}^{j})$ -measurable estimator ${\\widehat{\\theta}}_{j}$ satisfying ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{P}(|\\widehat{\\theta}_{j}-\\theta_{j}|\\leqslant\\eta_{\\delta}(q))\\geqslant1-\\delta\\;.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In Section 4.2, we show how the aggregator can design such estimators. $\\mathbf{H}5$ allows us to consider a newmechanism $\\widehat{\\Gamma}:\\,\\mathsf{B}\\mapsto\\mathbf{m}(\\mathsf{B})$ as follows: ", "page_idx": 7}, {"type": "text", "text": "1. any agent $j~\\in~[J]$ declares $B_{j}\\;\\in\\;\\{0,1\\}$ . If $B_{j}\\ =\\ 1$ , the principal asks for $\\underline{{q}}\\,\\leqslant\\,\\underline{{n}}\\,-$ $2(a/c)({\\bar{\\theta}}-\\underline{{{\\theta}}})$ i.i.d samples from $P_{j}$ and estimates types as ${\\widehat{\\pmb{\\theta}}}=({\\widehat{\\theta}}_{j})_{j\\in B}$ following H5. ", "page_idx": 7}, {"type": "text", "text": "2. Based on the estimated types ${\\widehat{\\pmb{\\theta}}}\\,=\\,({\\widehat{\\theta}}_{j})_{j\\in B}$ , the aggregator asks for $[{n}_{j}^{\\star}(\\ensuremath{\\widehat{\\pmb{\\theta}}}+{\\pmb{\\eta}}_{j})-\\underline{{q}}]_{+}$ additional samples from $P_{j}$ , where $\\mathbf{n}^{\\star}(\\,\\cdot\\,)$ is defined as in (12) and ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\eta_{j}=\\eta_{\\delta/J}(\\underline{{q}})\\mathbf{1}-2\\delta_{j}\\eta_{\\delta/J}(\\underline{{q}}),\\quad\\mathrm{with}\\quad\\delta_{j}=(0,\\dots,0,1,0,\\dots,0)^{\\mathrm{T}}\\ .\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Thus, the number of draws required from agent $j$ is max[q , $n_{j}^{\\star}(\\widehat{\\pmb{\\theta}}+\\pmb{\\eta}_{j})]$ ", "page_idx": 7}, {"type": "text", "text": "3. The aggregator keeps $m_{j}(\\widehat{\\pmb{\\theta}})\\,=\\,\\Im\\{n_{j_{\\cdot}}^{\\star}(\\widehat{\\pmb{\\theta}}+\\pmb{\\eta}_{j})\\,>\\,0\\}\\,\\mathrm{max}[\\underline{{{q}}}\\,,\\,n_{j}^{\\star}(\\widehat{\\pmb{\\theta}}+\\pmb{\\eta}_{j})]$ samples from agent $j$ , and trains a collaborative model with these pooled samples. ", "page_idx": 7}, {"type": "text", "text": "$\\hat{\\Gamma}$ induces a game $([J],\\{0,1\\}^{J},(\\hat{v}_{j})_{j\\in[J]})$ where any agent $j\\in[J]$ has a payoff function ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{v}_{j}:(B_{j},\\mathbb{B}_{-j})\\mapsto B_{j}\\Big[{-a}\\Big(\\mathcal{R}_{0}^{\\star}+\\varepsilon(\\mathbb{B},\\mathbf{m}(\\widehat{\\theta}))\\Big)-c\\operatorname*{max}[\\underline{{q}},\\,n_{j}^{\\star}(\\widehat{\\theta}+\\eta_{j})]\\Big]+(1-B_{j})\\underline{{u}}_{j}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The rationale behind this mechanism is fairly intuitive: since $\\widehat{\\pmb{\\theta}}$ is a correct estimate of \u03b8 $\\mathbf{\\omega},n_{j}^{\\star}(\\widehat{\\pmb{\\theta}}{+}\\pmb{\\eta}_{j})\\approx$ $n_{j}^{\\star}({\\widehat{\\pmb{\\theta}}})$ correctly approximates the optimal contribution $n_{j}^{\\star}(\\theta)$ for any contributor $j\\in\\mathcal B$ . Note that type estimates are purposely biased by $\\eta_{j}$ when asking for samples. This is a safeguard against over-estimated types, which would lead to asking to many data points and could deter agents from participating in the coalition. ", "page_idx": 7}, {"type": "text", "text": "Critically, $\\mathbf{m}({\\widehat{\\theta}})$ does not depend on declared type, so agents are no longer able to strategically manipulate the mechanism. Moreover, $\\hat{\\Gamma}$ does not require agents to know their own types, which would be an unrealistic assumption. Finally, observe that the number of data points asked to estimate types $\\underline{{q}}$ is low enough to never deter agents from participating in the coalition. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3. Assume Hl, H2, $H3$ $H4$ and $H5$ $\\mathsf{B}^{\\star}=(1,\\ldots,1)^{\\mathrm{T}}$ is a Nash equilibrium under $\\hat{\\Gamma}$ with probability $1-\\delta$ ", "page_idx": 8}, {"type": "text", "text": "Theorem 3 shows that the optimal coalition is a sustainable equilibrium under $\\hat{\\Gamma}$ whicheffectively breaks unravelling: the set of (approximate) Nash equilibria is no more reduced to profiles of actions where the coalition is empty, or reduced to the worst agent. ", "page_idx": 8}, {"type": "text", "text": "Practical implementation.  We now explain how to practically implement F in a collaborative learning setting. This requires defining a collection of estimators $(\\widehat{\\theta}_{j})_{j}$ satisfying H5. To this end, we assume that few samples from the target distribution are available. ", "page_idx": 8}, {"type": "text", "text": "H6. There are $q^{\\prime}>0$ i.i.d samples $\\left\\{(X_{1}^{0},Y_{1}^{0}),\\ldots,(X_{q^{\\prime}}^{0},Y_{q^{\\prime}}^{0})\\right\\}$ from $P_{0}$ available to the aggregator and agents. ", "page_idx": 8}, {"type": "text", "text": "Under $\\mathbf{H}6$ define $\\begin{array}{r}{\\widehat{\\mathcal{R}}_{0}(g)=q^{\\prime-1}\\sum_{i=1}^{q^{\\prime}}\\ell(g(X_{i}^{0}),Y_{i}^{0})}\\end{array}$ for $g\\in{\\mathcal{G}}$ . This allows us to devise suitable estimators ${\\widehat{\\theta\\,}}_{j}$ as follows. ", "page_idx": 8}, {"type": "text", "text": "Proposition 2. Assume $H l$ \uff0c $_{H2}$ and $H6$ For any $j\\in[J]$ the estimator ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\widehat{\\theta}_{0,j}^{\\mathrm{ERM}}=\\operatorname*{sup}_{g\\in\\mathcal{G}}\\left\\vert\\widehat{\\mathcal{R}}_{j}(g)-\\widehat{\\mathcal{R}}_{0}(g)\\right\\vert\\,,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "satisfies $H5$ with ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\eta_{\\delta}(q)=\\alpha_{\\delta/4}\\big[(q+1)^{-\\gamma}+(q^{\\prime}+1)^{-\\gamma}\\big]+2\\beta\\;.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Proposition 2 shows that the empirical version of the $\\mathcal{G}$ -divergence defined in $\\mathbf{H}2$ correctly estimate types. Note that the tighter the PAC bound in $\\mathbf{H}1$ , the better the approximation term in (13). The type etimator $\\widehat{\\theta}_{0,j}^{\\mathrm{ERM}}$ defined in $\\mathbf{H}5$ caneasiltiawi following example. ", "page_idx": 8}, {"type": "text", "text": "H7 (Classfication setting). ${\\boldsymbol{\\mathcal{V}}}=\\{-1,1\\}$ \uff0c\uff0c $\\ell=\\ell_{0,1}:(y,y^{\\prime})\\in\\mathcal{Y}\\times\\mathcal{Y}\\mapsto\\mathbb{1}_{\\{y y^{\\prime}<0\\}}.$ and $\\mathcal{G}$ is $a$ symmetric $\\boldsymbol{g}\\in\\mathcal{G}$ if and only $i f\\!-\\!g\\in\\!\\mathcal{G}.$ ) class of classifiers. ", "page_idx": 8}, {"type": "text", "text": "Example 4. Assume $H l$ $_{H2}$ ,H6and $H7$ (i) Denoting $\\begin{array}{r}{\\widehat{\\mathcal{R}}_{j^{-}}(g)=n_{j}^{-1}\\sum_{i=1}^{n_{j}}\\ell_{0,1}(g(X_{i}^{j}),-Y_{i}^{j})}\\end{array}$ we have ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "equation", "text": "$$\n\\widehat{\\theta}_{0,j}^{\\mathtt{E R M}}=1-\\operatorname*{inf}_{g\\in\\mathcal{G}}\\left\\{\\widehat{\\mathcal{R}}_{0}(g)+\\widehat{\\mathcal{R}}_{j^{-}}(g)\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "(i) In $H\\!I$ assume $\\alpha_{\\delta}=\\ln(1/\\delta)^{1/2}$ \uff0c $\\beta=2\\mathsf{R A D}(\\mathcal{G})$ and $\\gamma=1$ [Bousquet et al., 2003]. With $\\widehat{\\theta}_{0,j}^{\\tt E R M}$ defined in Proposition 2, we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\eta_{\\delta/J}(q)=\\ln(4J/\\delta)^{1/2}[(1+q)^{-\\gamma}+(1+q^{\\prime})^{-\\gamma}]+2\\mathrm{Rad}(\\mathcal{G})\\;.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Example 4 shows that in the classification case, it is sufficient to flip the labels of the data received from each contributor, merge these samples with those from $P_{0}$ , and perform an empirical risk minimization to compute OERM The approximation error grows no more than logarithmically with the number of agents, while decreasing at rate $\\gamma$ with the number of samples used in the estimation. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we show that information asymmetry has deleterious consequences when strategic agents try to learn a collaborative model. More precisely, under a naive aggregation procedure, the ignorance of others? data quality leads the coalition of learners to be either empty or reduced to the lowest-quality agent. We introduce a transfer-free mechanism based on estimation of types. This effectively counteracts unravelling by letting the grand coalition ranks among the approximate Nash equilibria with high probability. ", "page_idx": 8}, {"type": "text", "text": "Several possible extensions can be considered. First, it would be interesting to relax the assumption that all agents have the same ratio $a/c$ in their utility, and see how heterogeneity affects the results. ", "page_idx": 8}, {"type": "text", "text": "Second, the mechanism presented in Section 4 aims for individual rationality. A more desirable, yet difficult to achieve, property would be core stability, to ensure that no group of agents would benefit from a coordinated deviation, i.e., forming an alternative coalition. Finally, it would be interesting to check whether there exist mechanisms where the optimal collaboration not only emerges as a Nash equilibrium, but as a dominant equilibrium under imperfect information. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Funded by the European Union (ERC, Ocean, 101071601). Views and opinions expressed are however those of the author(s) only and do not necessarily refect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "George A. Akerlof. The market for lemons: Quality uncertainty and the market mechanism. The Quarterly Journal of Economics, 84(3):488-500, 1970. URL https : //ideas .repec . org/a/ oup/qjecon/v84y1970i3p488-500..html.   \nNivasini Ananthakrishnan, Stephen Bates, Michael Jordan, and Nika Haghtalab. Delegating data collection in decentralized machine learning, 2023.   \nIan Ball and Deniz Kattwinkel. Probabilistic verification in mechanism design. September 2019.   \nShai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Vaughan. A theory of learning from different domains. Machine Learning, 79:151-175, 05 2010. doi: 10.1007/s10994-009-5152-4.   \nAvrim Blum, Nika Haghtalab, Ariel Procaccia, and Mingda Qiao. Collaborative PAC learning. In 1. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems. Curran Associates, 2017.   \nAvrim Blum, Nika Haghtalab, Richard Lanas Phillips, and Han Shao. One for one, or all for all: Equilibria and optimality of collaboration in federated learning. In Marina Meila and Tong Zhang. editors, Proceedings of the 38th International Conference on Machine Learning,volume 139 of Proceedings of Machine Learning Research, pages 1005-1014. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/blum21a.html.   \nOlivier Bousquet, St\u00e9phane Boucheron, Gabor Lugosi, Ulrike Luxburg, and Gunnar Ratsch. Introduction to statistical learning theory. Advanced Lectures on Machine Learning, 169-207 (2004), 01 2003. doi: 10.1007/978-3-540-28650-9_8.   \nIoannis Caragiannis, Edith Elkind, Mario Szegedy, and Lan Yu. Mechanism design: From partial to probabilistic verification. Proceedings of the ACM Conference on Electronic Commerce, 06 2012. doi: 10.1145/2229012.2229035.   \nEdward Clarke. Multipart pricing of public goods. Public Choice, 11(1):17-33, September 1971. doi: 10.1007/BF01726210. URL https : //ideas.repec .org/a/kap/pubcho/ V11y1971i1p17-33.html.   \nKate Donahue and Jon Kleinberg. Optimality and stability in federated learning: A game-theoretic approach, 06 2021a.   \nKate Donahue and Jon Kleinberg. Model-sharing games: Analyzing federated learning under voluntary participation. Proceedings of the AAAI Conference on Artificial Intelligence, 35:5303- 5311, 05 2021b. doi: 10.1609/aaai. v35i6.16669.   \nFlorian E. Dorner, Nikola Konstantinov, Georgi Pashaliev, and Martin T. Vechev. Incentivizing honesty among competitors in collaborative learning and optimization. ArXiv, abs/2305.16272, 2023. URL https: //api.semanticscholar.org/CorpusID:258887502.   \nLiran Einav and Amy Finkelstein. Selection in insurance markets: Theory and empirics in pictures. Journal of Economic Perspectives, 25(1):115-38, March 2011. doi: 10.1257/jep.25.1.115. URL https://www.aeaweb.org/articles?id=10.1257/jep.25.1.115.   \nDiodato Ferraioli and Carmine Ventre. Probabilistic verification for bviously strategyproof mechanisms, 2018.   \nLei Fu, Huanle Zhang, Ge Gao, Mi Zhang, and Xin Liu. Client selection in federated learning: Principles, challenges, and opportunities, 2023.   \nDashan Gao,Xin Yao, and Qiang Yang. A survey onheterogeneous federated learning. arXiv preprint arXiv:2210.04505, 2022.   \nJerry Green and Jean-Jacques Laffont. Characterization of satisfactory mechanisms for the revelation of preferences for public goods. Econometrica, 45(2):427-38, 1977. URL https : //EconPapers . repec.org/RePEc:ecm: emetrp:v:45:y:1977:i:2:p:427-38.   \nTheodore Groves. Incentives in teams. Econometrica, 41:617-631, 1973. URL https ://api . semanticscholar.org/CorpusID:264740987.   \nNathaniel Hendren. Private Information and Insurance Rejections. Econometrica, 81(5):1713- 1762, September 2013. doi: ECTA10931. URL https : //ideas.repec . org/a/ecm/emetrp/ v81y2013i5p1713-1762.html.   \nBengt Holmstrom. Groves\u2019 scheme on restricted domains. Econometrica, 47(5):1137-44, 1979. URL https://EconPapers.repec.org/RePEc:ecm:emetrp:v:47:y:1979:i:5:p:1137-44.   \nBaihe Huang, Sai Praneeth Karimireddy, and Michael I. Jordan. Evaluating and Incentivizing Diverse Data Contributions in Collaborative Learning. Papers 2306.05592, arXiv.org, June 2023. URL https://ideas.repec.org/p/arx/papers/2306.05592.html.   \nChao Huang, Shuqi Ke, Charles Kamhoua, Prasant Mohapatra, and Xin Liu. Incentivizing data contribution in cross-silo federated learning, 2022.   \nPeter Kairouz, H. Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L. D'Oliveira, Hubert Eichner, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adria Gascon, Badih Ghazi, Phillip B.Gibbons, Marco Gruteser, Zaid Harchaoui, Chayang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Konevcny, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancrede Lepoint, Yang Liu, Pratk Mittal, Mryar Mori, Richard Nock, Ayfer Ozgur, Raus Pagh, Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tramer, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances and open problems in federated learning, 2021.   \nSai Pranth Karimireddy, Wenshuo Guo, and Michael I. Jordan. Mechanisms that incentivize data sharing in federated learning, 2022.   \nDaniel Kifer, Shai Ben-David, and Johannes Gehrke. Detecting change in data streams. pages 180-191, 04 2004.   \nNikola Konstantinov and Christoph Lampert. Robust learning from untrusted sources, 2019.   \nJean-Jacques Lafont and David Martimort. The Theory of Incentives: The Principal-Agent Model. Princeton University Press, Princeton, NJ, USA, 2001.   \nShutian Liu, Tao Li, and Quanyan Zhu. Game-theoretic distributed empirical risk minimization with strategic network design. IEEE Transactions on Signal and Information Processing over Networks, 9:542-556, 2023. doi: 10.1109/TSIPN.2023.3306106.   \nAndreu Mas-Colell, Michael D. Whinston, and Jerry R. Green. Microeconomic Theory. Number 9780195102680 in OUP Catalogue. Oxford University Press, 1995. ISBN ARRAY(0x516030e0). URL https: //ideas.repec.org/b/oxp/obooks/9780195102680.html. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "Michael Rothschild and Joseph Stiglitz. Equilibrium in competitive insurance markets: An essay on the economics of imperfect information. The Quarterly Journal of Economics, 90(4):629- 649, 1976. URL https://EconPapers.repec.org/RePEc:oup:qjecon:v:90:y:1976:i: 4:p:629-649. ", "page_idx": 11}, {"type": "text", "text": "Eden Saig, Inbal Talgam-Cohen, and Nir Rosenfeld. Delegated classification, 2023 ", "page_idx": 11}, {"type": "text", "text": "Vera Shalaeva, Alireza Fakhrizadeh Esfahani, Pascal Germain, and Mihaly Petreczky. Improved pac-bayesian bounds for linear regression. In AAAI Conference on Artificial Intelligence, 2019. URL https: //api.semanticscholar.org/CorpusID:208857400. ", "page_idx": 11}, {"type": "text", "text": "Nikita Tsoy and Nikola Konstantinov. Strategic data sharing between competitors. Advances in Neural InformationProcessing Systems,36,2024. ", "page_idx": 11}, {"type": "text", "text": "Xuezhen Tu, Kun Zhu, Nguyen Cong Luong, Dusit Niyato, Yang Zhang, and Juan Li. Incentive mechanisms for federated learning: From economic and game theoretic perspective. IEEE Transactions on Cognitive Communications and Networking, 8(3):1566-1593, 2022. doi: 10.1109/ TCCN.2022.3177522. ", "page_idx": 11}, {"type": "text", "text": "William Vickrey. Counterspeculation, auctions, and competitive sealed tenders. Journal of Finance, 16 (1):8-37, 1961. URL https: //EconPapers.repec.org/RePEc :bla: jfinan:v:16 :y:1961: i :1 :p : 8-37. ", "page_idx": 11}, {"type": "text", "text": "Zhepei Wei, Chuanhao Li, Tianze Ren, Haifeng Xu, and Hongning Wang. Incentivized truthful communication for federated bandits, 2024. ", "page_idx": 11}, {"type": "text", "text": "Mariel Werner, Sai Praneeth Karimireddy, and Michael I. Jordan. Defection-free collaboration between competitors in a learning system, 2024. URL https : //arxiv . org/abs/2406 . 15898. ", "page_idx": 11}, {"type": "text", "text": "Yizhou Yan, Xinyu Tang, Chao Huang, and Ming Tang. Price of stability in quality-aware federated learning. In GLOBECOM 2023 - 2023 IEEE Global Communications Conference, pages 734-739, 2023. d0i: 10.1109/GLOBECOM54140.2023.10437743. ", "page_idx": 11}, {"type": "text", "text": "A  Relaxation of the optimal aggregation problem. ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Let $\\mathsf{B}\\in\\{0,1\\}^{J}$ and the associated coalition $\\mathcal{B}=\\{j\\in[J]:\\,B_{j}=1\\}$ be fixed. We present in this appendix a natural relaxation of problem Equation (10) which motivates the choice $\\dot{\\sum}_{j\\in B}n_{j}^{\\star}(\\pmb{\\theta})=$ $\\bar{N}=(\\underline{{n}}+1)|\\mathcal{B}|^{\\frac{1}{1+\\gamma}}-1$ in the simplified contribution scheme $(\\mathsf{B}^{\\star},\\mathbf{n}^{\\star})$ . The optimal aggregation problem with $\\textsf{B}$ fixed is ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathrm{maximize~~~}{\\bf n}\\in\\mathbb{R}^{J}\\,\\mapsto\\,\\sum_{j\\in[J]}u_{j}({\\bf B},{\\bf n})\\quad\\mathrm{subject}\\,{\\bf t o}\\quad\\left\\{\\begin{array}{l l}{\\mathrm{max}_{j\\in B}\\,n_{j}-\\overline{n}({\\bf B},{\\bf n})\\leqslant0}\\\\ {\\mathrm{max}_{j\\in[J]}-n_{j}\\leqslant0\\;,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Since agents outside of the coalition maximize their utility, the problem can equivalently be stated as ", "page_idx": 11}, {"type": "equation", "text": "$$\n({\\mathcal{T}}_{\\mathsf{B}}):\\quad{\\mathrm{maximize}}\\quad\\mathbf{n}\\in\\mathbb{R}^{|\\mathcal{B}|}\\mapsto\\sum_{j\\in\\mathcal{B}}u_{j}(\\mathbf{B},\\mathbf{n})+\\zeta_{\\mathsf{B}}\\quad{\\mathrm{subject}}\\ \\mathbf{0}\\quad\\mathbf{n}\\in\\Xi_{\\mathsf{B}}\\ ,\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "where ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\Xi_{\\mathsf{B}}=\\left\\{\\mathbf{n}\\in\\mathbb{R}_{+}^{\\mathcal{B}}:\\begin{array}{l}{\\operatorname*{max}_{j\\in\\mathcal{B}}\\{n_{j}-\\overline{{n}}_{j}(\\mathsf{B},\\mathbf{n})\\}\\leqslant0}\\\\ {\\operatorname*{max}_{j\\in\\mathcal{B}}-n_{j}\\leqslant0}\\end{array}\\right\\}\\quad\\mathrm{and}\\quad\\zeta_{\\mathsf{B}}=\\sum_{j\\notin\\mathcal{B}}\\underline{{u}}_{j}\\;.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Westart byrewriting $(\\mathcal{T}_{\\mathsf{B}})$ in a more convenient way. Instead of working with $\\mathbf{n}\\in\\mathbb{R}_{+}^{|\\mathcal{B}|}$ directly, we make appear (i) the total number of samples and (i) the sharing out of samples between agents. Formally, for any n E RI-il consider $N={\\bf n}^{\\mathrm{T}}{\\sf B}$ and $\\pmb{\\lambda}=N^{-1}\\mathbf{n}\\in\\Delta_{|B|}$ where $\\Delta_{|B|}$ is the simplex of dimension $|\\beta|$ . Observe that the average type in the coalition reads $\\vartheta(\\mathbf{B},\\mathbf{n})=\\lambda^{\\mathrm{T}}\\theta$ . Moreover, ", "page_idx": 11}, {"type": "text", "text": "the welfare evaluated in $(\\mathsf{B},\\mathbf{n})$ rewrites ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{j\\in[J]}u_{j}(\\ B,\\mathbf{n})=\\displaystyle\\sum_{j\\in[J]}B_{j}[-a(\\mathcal{R}_{0}^{\\star}+\\varepsilon(\\ B,\\lambda N))-c n_{j}]+\\displaystyle\\sum_{j\\in[J]}(1-B_{j})\\underline{{u}}_{j}}&{}\\\\ {=-a|B|(\\mathcal{R}_{0}^{\\star}+\\varepsilon(\\mathsf{B},\\lambda N))-c N+\\zeta_{\\mathsf{B}}\\ ,}&{}\\\\ {=-a|B|\\left[\\mathcal{R}_{0}^{\\star}+2\\big(\\alpha_{\\delta}(1+N)^{-\\gamma}+\\beta+\\lambda^{\\mathsf{T}}\\theta\\big)\\right]-c N+\\zeta_{\\mathsf{B}}\\ }&{}\\\\ {=\\widetilde W_{\\mathsf{B}}(\\lambda,N)\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We denote by $\\widetilde{\\Xi}_{\\mathsf{B}}=\\left\\{(\\pmb{\\lambda},N)\\in\\Delta_{|\\mathcal{B}|}\\times\\mathbb{R}_{+}:\\lambda N\\in\\Xi_{\\mathsf{B}}\\right\\}$ and $\\widetilde{\\mathcal{T}}_{\\mathtt{B}}$ the problem: ", "page_idx": 12}, {"type": "equation", "text": "$$\n(\\widetilde{\\mathcal{T}}_{\\mathtt{B}}):\\quad\\mathrm{minimize}\\quad(\\lambda,N)\\in\\Delta_{|\\mathcal{B}|}\\times\\mathbb{R}_{+}\\mapsto-\\widetilde{W}_{\\mathtt{B}}(\\lambda,N)\\quad\\mathrm{subject}\\ \\mathrm{to}\\quad(\\lambda,N)\\in\\widetilde{\\Xi}_{\\mathtt{B}}\\ .\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "By definition, if $(\\lambda,N)$ is a solution to $\\widetilde{\\mathcal{T}}_{\\mathtt{B}}$ , then $\\mathbf{n}\\,=\\,\\lambda N$ is a solution to $\\mathcal{T}_{\\mathtt{B}}$ .We can further decompose $\\widetilde{\\mathcal{T}}_{\\mathtt{B}}$ by observing that ", "page_idx": 12}, {"type": "equation", "text": "$$\n-\\widetilde{W}_{\\mathsf{B}}(\\lambda,N)=f(N)+g(\\lambda)\\;,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "with ", "page_idx": 12}, {"type": "equation", "text": "$$\nf(N)=a|\\mathcal{B}|(\\mathcal{R}_{0}^{\\star}+2\\alpha_{\\delta}(1+N)^{-\\gamma}+2\\beta)+c N+\\zeta_{\\mathsf{B}}\\quad\\mathrm{and}\\quad g(\\lambda)=2a|\\mathcal{B}|\\lambda^{\\mathrm{T}}\\theta\\ .\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Finally, we denote a slice of $\\widetilde{\\Xi}_{\\sf B}$ along $N\\geqslant0$ as $\\widetilde{\\Xi}_{\\mathsf{B}}(N)\\,=\\,\\{\\lambda\\,\\in\\,\\Delta_{|B|}\\,:\\,(\\lambda,N)\\,\\in\\,\\widetilde{\\Xi}_{\\mathsf{B}}\\}$ and $\\begin{array}{r}{{\\mathcal N}=\\{N\\geqslant0:\\,\\widetilde{\\Xi}_{\\mathtt{B}}(N)\\not=\\emptyset\\}.}\\end{array}$ $\\widetilde{\\mathcal{T}}_{\\mathtt{B}}$ comes down to ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{(\\lambda,N)\\in\\tilde{\\Xi}_{8}}\\left\\{f(N)+g(\\lambda)\\right\\}=\\operatorname*{min}_{N\\in{\\cal N}}\\left\\{f(N)+\\operatorname*{min}_{\\lambda\\in\\tilde{\\Xi}_{8}(N)}g(\\lambda)\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "A strategy to solve Equation (16) s to (i) address the inermost problem $\\mathrm{min}_{\\lambda\\in\\widetilde{\\Xi}_{\\mathtt{B}}(N)}\\,g(\\lambda)$ Wwih $N\\in\\mathcal N$ fixed, and denoting $\\pmb{\\lambda}^{(N)}\\in\\widetilde{\\Xi}_{\\mathsf{B}}(N)$ its solution, (i) solve the outermost problem: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{N\\in\\mathcal{N}}\\{f(N)+g(\\mathsf{\\pmb{\\lambda}}^{(N)})\\}\\;.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Point (i) is done in the proof of Theorem 1. However, the resulting problem in (i) is hard to tackle because $\\pmb{\\lambda}^{(N)}$ has no simple form. Therefore, we leave aside the term $g(\\pmb{\\lambda}^{(N)})$ (which can be easily controlled, as shown in Lemma 2) to determine $N$ and only consider the problem ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{N\\geq0}\\;f(N)\\;.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Since $f$ is differentiable and strictly convex, its minimizer is uniquely defined by $f^{\\prime}(\\bar{N})=0$ ,which gives ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\bar{N}=(\\underline{{{n}}}+1)|\\mathcal{B}|^{\\frac{1}{1+\\gamma}}-1~,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\underline{n}$ is defined in Proposition 1. As the solution of the relaxed problem (18), we take $\\begin{array}{r}{\\sum_{j\\in[J]}n_{j}^{\\star}(\\pmb{\\theta})=\\bar{N}}\\end{array}$ is the simplified contribution scheme $(\\mathbf{B}^{\\star},\\mathbf{n}^{\\star})$ . In many reasonable settings, this approximation is very satisfactory as shown by Lemma 2. ", "page_idx": 12}, {"type": "text", "text": "B Proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Lemma 1. Assume $H I$ and $_{H2}$ ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "(i) Any agent $j\\in[J]$ picking the outside Option 1) obtains a model $\\widehat{g}_{j}\\in\\mathcal{G}$ achieving ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{R}_{0}(\\widehat{g}_{j})\\leqslant\\mathcal{R}_{0}^{\\star}+\\varepsilon(\\theta_{j},n_{j})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "(i) Any coalition ${\\mathsf{B}}\\in\\left\\{0,1\\right\\}^{J}$ drawing $\\mathbf{n}=(n_{1},\\ldots,n_{J})\\in\\mathbb{R}_{+}^{J}$ samples obtains a model $\\widehat{g}_{\\mathsf{B}}\\in\\mathcal{G}$ achieving ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{R}_{0}(\\widehat{g}_{\\mathsf{B}})\\leqslant\\mathcal{R}_{0}^{\\star}+\\varepsilon(\\vartheta,N)\\quad w i t h\\,p r o b a b i l i t y\\;1-\\delta\\;,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $N$ is the total of samples and $\\vartheta$ is the weighted average type within the coalition: ", "page_idx": 12}, {"type": "equation", "text": "$$\nN(B,{\\mathbf{n}})=\\sum_{j\\in[J]}B_{j}n_{j}\\ ,\\qquad\\vartheta({\\mathsf{B}},{\\mathbf{n}})=N^{-1}\\sum_{j\\in[J]}B_{j}n_{j}\\theta_{j}\\ .\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof i) Let $\\{(X_{1}^{j},Y_{1}^{i}),\\cdot\\cdot\\cdot,(X_{n_{j}}^{j},Y_{n_{j}}^{j})\\}$ be $n_{j}~>~0$ i.d samples from $P_{j}~\\in~\\mathcal{P}$ and $\\widehat{g}_{j}\\;=$ $\\operatorname{inf}_{g\\in{\\mathcal{G}}}\\widehat{\\mathcal{R}}_{j}(g)$ . First, observe that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{R}_{0}(\\widehat{g}_{j})\\leqslant\\mathcal{R}_{j}(\\widehat{g}_{j})+\\operatorname*{sup}_{g\\in\\mathcal{G}}\\lvert\\mathcal{R}_{0}(g)-\\mathcal{R}_{j}(g)\\rvert=\\mathcal{R}_{j}(\\widehat{g}_{j})+\\theta_{j}\\ .\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Let $\\upsilon>0$ and $g_{j}^{v}\\in\\mathcal{G}$ such that $\\begin{array}{r}{\\mathcal{R}_{j}\\left(g_{j}^{v}\\right)\\leqslant\\operatorname*{inf}_{g\\in\\mathcal{G}}\\mathcal{R}_{j}(g)+\\upsilon}\\end{array}$ Then with probability at least $1-\\delta$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{j}(\\widehat{g}_{j})=\\mathcal{R}_{j}(\\widehat{g}_{j})-\\mathcal{R}_{j}\\bigl(g_{j}^{v}\\bigr)+\\mathcal{R}_{j}\\bigl(g_{j}^{v}\\bigr)}\\\\ &{\\qquad\\qquad\\leqslant\\Big(\\widehat{\\mathcal{R}}_{j}(g_{j}^{v})-\\widehat{\\mathcal{R}}_{j}(\\widehat{g}_{j})\\Big)+\\mathcal{R}_{j}(\\widehat{g}_{j})-\\mathcal{R}_{j}\\bigl(g_{j}^{v}\\bigr)+\\mathcal{R}_{j}\\bigl(g_{j}^{v}\\bigr)}\\\\ &{\\qquad\\leqslant2\\operatorname*{sup}_{g\\in\\mathcal{G}}\\Big|\\mathcal{R}_{j}(g)-\\widehat{\\mathcal{R}}_{j}(g)\\Big|+\\operatorname*{inf}_{g\\in\\mathcal{G}}\\mathcal{R}_{j}(g)+v}\\\\ &{\\qquad\\leqslant2\\biggl(\\frac{\\alpha_{\\delta}}{(n+1)^{\\gamma}}+\\beta\\biggr)+\\operatorname*{inf}_{g\\in\\mathcal{G}}\\mathcal{R}_{j}(g)\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the last inequality holds taking the limit $\\upsilon\\,\\rightarrow\\,0$ and using $\\textbf{H}1$ .Finally, observing that $\\begin{array}{r}{\\operatorname*{inf}_{g\\in\\mathcal{G}}\\mathcal{R}_{j}(g)\\ -\\ \\bar{\\mathcal{R}}_{0}^{\\star}\\ =\\ \\operatorname*{inf}_{g\\in\\mathcal{G}}\\mathcal{R}_{j}(\\bar{g})\\,-\\,\\operatorname*{inf}_{g\\in\\mathcal{G}}\\mathcal{R}_{0}(g)\\ \\leqslant\\ \\operatorname*{sup}_{g\\in\\mathcal{G}}\\lvert\\mathcal{R}_{j}(g)-\\mathcal{R}_{0}(g)\\rvert\\ =\\ \\theta_{j}^{\\star}}\\end{array}$ , and combining (20) as well as (21) yields ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{R}_{0}(\\widehat{g}_{j})\\leqslant\\mathcal{R}_{0}^{\\star}+2\\theta_{j}+2\\bigg(\\frac{\\alpha_{\\delta}}{(n+1)^{\\gamma}}+\\beta\\bigg)=\\mathcal{R}_{0}^{\\star}+\\varepsilon(\\theta_{j},n)\\;,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "with probability at least $1-\\delta$ ", "page_idx": 13}, {"type": "text", "text": "(i) Let ${\\mathsf{B}}\\in\\left\\{0,1\\right\\}^{J}$ and $\\mathbf{n}=(n_{1},\\ldots,n_{j})\\in\\mathbb{R}_{+}^{J}$ . For any $j\\in\\mathsf{B}$ let $\\{(X_{1}^{j},Y_{1}^{i}),\\ldots,(X_{n_{j}}^{j},Y_{n_{j}}^{j})\\}$ be a colletion of $n_{j}>0$ i.idsamples from $P_{j}\\in\\mathcal{P}$ . Denote by $\\begin{array}{r}{N=\\sum_{j\\in[J]}B_{j}n_{j}}\\end{array}$ and consider $\\pmb{\\lambda}=(\\lambda_{1},\\dots,\\lambda_{J})$ such that $\\lambda_{j}=B_{j}n_{j}/N$ . Note that $\\lambda$ belongs to the $J$ -dimensional simplex. For any $g\\in{\\mathcal{G}}$ , the empirical risk over contributions is ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{R}}_{\\mathsf{B}}(g)=\\frac{1}{N}\\sum_{j\\in[J]}B_{j}\\sum_{i=1}^{n_{j}}\\ell(g(X_{i}^{j}),Y_{i}^{j})=\\sum_{j\\in[J]}B_{j}\\lambda_{j}\\widehat{\\mathcal{R}}_{j}(g)\\;,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and the population risk is ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\mathsf{B}}(g)=\\mathbb{E}\\Big[\\widehat{\\mathcal{R}}_{\\mathsf{B}}(g)\\Big]=\\sum_{j\\in[J]}B_{j}\\lambda_{j}\\mathbb{E}\\Big[\\widehat{\\mathcal{R}}_{j}(g)\\Big]=\\sum_{j\\in[J]}B_{j}\\lambda_{j}\\mathcal{R}_{j}(g)\\;.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "One the one hand, the collaborative model $\\widehat{g}_{\\mathsf{B}}\\in\\mathcal{G}$ satisfies: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{R}_{0}(\\widehat{g}_{\\mathtt{B}})\\leqslant\\mathcal{R}_{\\mathtt{B}}(\\widehat{g}_{\\mathtt{B}})+\\underset{g\\in\\mathcal{G}}{\\operatorname*{sup}}|\\mathcal{R}_{0}(g)-\\mathcal{R}_{\\mathtt{B}}(g)|\\leqslant\\mathcal{R}_{\\mathtt{B}}(\\widehat{g}_{\\mathtt{B}})+\\underset{g\\in\\mathcal{G}}{\\operatorname*{sup}}\\Bigg\\{\\sum_{j\\in[J]}\\lambda_{j}|\\mathcal{R}_{j}(g)-\\mathcal{R}_{0}(g)|\\Bigg\\}}\\\\ {\\leqslant\\mathcal{R}_{\\mathtt{B}}(\\widehat{g}_{\\mathtt{B}})+\\sum_{j\\in[J]}\\lambda_{j}\\underset{g\\in\\mathcal{G}}{\\operatorname*{sup}}|\\mathcal{R}_{j}(g)-\\mathcal{R}_{0}(g)|}\\\\ {=\\mathcal{R}_{\\mathtt{B}}(\\widehat{g}_{\\mathtt{B}})+\\vartheta(\\mathsf{B},\\mathbf{n})\\;,}&{\\mathrm{ot~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "With $\\begin{array}{r}{\\vartheta(\\mathsf{B},\\mathbf{n})\\,=\\,N^{-1}\\sum_{j\\in[J]}B_{j}n_{j}\\theta_{j}\\,=\\,\\sum_{j\\in[J]}B_{j}\\lambda_{j}\\theta_{j}}\\end{array}$ Now, let $\\upsilon\\,>\\,0$ and $g^{v}\\in\\mathcal{G}$ such that $\\begin{array}{r}{\\mathcal{R}_{\\mathsf{B}}(g^{v})\\leqslant\\operatorname*{inf}_{g\\in\\mathcal{G}}\\mathcal{R}_{\\mathsf{B}}(g)\\overset{\\cdot}{+}\\overset{\\cdot}{\\upsilon}}\\end{array}$ . We also have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{\\mathsf{B}}(\\widehat{g}_{\\mathsf{B}})=\\mathcal{R}_{\\mathsf{B}}(\\widehat{g}_{\\mathsf{B}})-\\mathcal{R}_{\\mathsf{B}}(g^{v})+\\mathcal{R}_{\\mathsf{B}}(g^{v})}\\\\ &{\\qquad\\qquad\\leqslant\\Big(\\widehat{\\mathcal{R}}_{\\mathsf{B}}(g^{v})-\\widehat{\\mathcal{R}}_{\\mathsf{B}}(\\widehat{g}_{\\mathsf{B}})\\Big)+\\mathcal{R}_{\\mathsf{B}}(\\widehat{g}_{\\mathsf{B}})-\\mathcal{R}_{\\mathsf{B}}(g^{v})+\\mathcal{R}_{\\mathsf{B}}(g^{v})}\\\\ &{\\qquad\\leqslant2\\displaystyle\\operatorname*{sup}_{g\\in\\mathcal{G}}\\Big|\\mathcal{R}_{\\mathsf{B}}(g)-\\widehat{\\mathcal{R}}_{\\mathsf{B}}(g)\\Big|+\\displaystyle\\operatorname*{inf}_{g\\in\\mathcal{G}}\\mathcal{R}_{\\mathsf{B}}(g)+v}\\\\ &{\\qquad\\leqslant2\\bigg(\\displaystyle\\frac{\\alpha_{\\delta}}{(N+1)^{\\gamma}}+\\beta\\bigg)+\\displaystyle\\operatorname*{inf}_{g\\in\\mathcal{G}}\\mathcal{R}_{\\mathsf{B}}(g)\\quad\\mathrm{with~probability~at~least~}1-\\delta\\mathrm{~,~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the first inequality comes from $\\begin{array}{r l r}{\\widehat{\\mathcal{R}}_{\\mathsf{B}}(\\widehat{g}_{\\mathsf{B}})}&{=}&{\\operatorname*{inf}_{g\\in{\\mathcal{G}}}\\widehat{\\mathcal{R}}_{\\mathsf{B}}(g)\\quad\\leqslant\\quad\\widehat{\\mathcal{R}}_{\\mathsf{B}}(g^{v})}\\end{array}$ \uff0cand the last is obtained by taking. $\\ensuremath{\\upsilon}\\ \\ \\to\\ \\ 0$ and applying_ assumption $\\textbf{H}\\,1$ . Now, observe that $\\begin{array}{r l r}{\\mathrm{nf}_{g\\in\\mathcal{G}}\\,\\mathcal{R}_{\\mathtt{B}}(g)\\ -\\ \\bar{\\mathcal{R}}_{0}^{\\star}\\ }&{=}&{\\mathrm{inf}_{g\\in\\mathcal{G}}\\,\\mathcal{R}_{\\mathtt{B}}(g)\\ -\\ \\mathrm{inf}_{g\\in\\mathcal{G}}^{\\star}\\,\\mathcal{R}_{0}(g)\\ ^{\\star}\\leqslant\\ }&{\\operatorname*{sup}_{g\\in\\mathcal{G}}|\\mathcal{R}_{\\mathtt{B}}(g)-\\mathcal{R}_{0}(g)|\\quad\\leqslant}\\end{array}$ $\\begin{array}{r}{\\sum_{j}B_{j}\\lambda_{j}\\operatorname*{sup}_{g\\in\\mathcal{G}}|\\mathcal{R}_{j}(g)-\\mathcal{R}_{0}(g)|\\;=\\;\\vartheta(\\mathsf{B},\\mathbf{n})}\\end{array}$ .Combining this observation with Equation (25) and Equation (26) gives with probability $1-\\delta$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{R}_{0}(\\widehat{g}_{\\mathsf{B}})\\leqslant\\mathcal{R}_{0}^{\\star}+2\\bigg(\\frac{\\alpha_{\\delta}}{(N+1)^{\\gamma}}+\\beta\\bigg)+2\\vartheta(\\mathsf{B},\\mathbf{n})=\\mathcal{R}_{0}^{\\star}+\\varepsilon\\big(\\vartheta(\\mathsf{B},\\mathbf{n}),N\\big)\\;.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proposition 1. Assume $H I$ and $_{H2}$ For any $j\\;\\in\\;[J]_{:}$ $\\mathsf{B}_{-j}\\,\\in\\,\\{0,1\\}^{J-1}$ and $\\mathbf{n}_{-j}\\,\\in\\,\\mathbb{R}_{+}^{J-1}$ the optimal number of samples to draw under Option $^{\\,l}$ )is $\\operatorname{argmax}_{n\\geq0}u_{j}((0,\\mathbf{B}_{-j}),(n,\\mathbf{n}_{-j})\\,;\\,\\theta_{j})=\\underline{{n}}$ where ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\underline{{{n}}}=(2a c^{-1}\\gamma\\alpha_{\\delta})^{1/(\\gamma+1)}-1\\ .\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. For any $j\\in[J]$ and $n\\geq0$ , Lemma 1 gives that ", "page_idx": 14}, {"type": "equation", "text": "$$\n-u_{j}\\big((0,{\\bf B}_{-j}),(n,{\\bf n}_{-j}),\\theta_{j}\\big)=a[{\\mathcal R}_{0}^{\\star}+2(\\alpha_{\\delta}(n+1)^{-\\gamma}+\\beta+\\theta_{j})]+c n=f(n)\\;.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since $f$ is strictly convex and differentiable, it admits a unique maximizer $\\underline{n}\\geqslant0$ determined by $f^{\\prime}({\\underline{{n}}})=0$ Simple algebra leads to $\\underline{{n}}=(2a\\gamma c^{-1}\\alpha_{\\delta})^{1/\\gamma+1}-\\bar{1}$ \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Theorem1l.Assume $\\pmb{H}\\allowbreak,\\pmb{H}\\!2,\\pmb{H}\\!3$ Problem(8)admits a uniquesolution $(\\mathsf{B}^{\\mathrm{opt}},\\mathbf{n}^{\\mathrm{opt}}(\\pmb{\\theta}))\\in\\left\\{0,1\\right\\}^{J}\\times$ $\\mathbb{R}_{+}^{J}$ .Moreover, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathsf{B}^{\\mathrm{opt}}=\\mathbf{1}=(1,\\ldots,1),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "(i) Denoting $\\mathbf{n}^{\\mathrm{opt}}(\\pmb{\\theta})=(n_{1}^{\\mathrm{opt}}(\\pmb{\\theta}),\\dots,n_{J}^{\\mathrm{opt}}(\\pmb{\\theta}))$ , there exists $L^{\\mathrm{opt}}\\in[J]$ such that for any $j\\in[J]$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{n_{j}^{\\mathrm{opt}}(\\pmb{\\theta})\\left\\{\\begin{array}{l l}{=\\overline{{n}}_{j}(\\mathbf{1},\\mathbf{n}^{\\mathrm{opt}}(\\pmb{\\theta}))}&{i f j<L^{\\mathrm{opt}},}\\\\ {\\in\\left[\\,0\\,,\\,\\overline{{n}}_{j}(\\mathbf{1},\\mathbf{n}^{\\mathrm{opt}}(\\pmb{\\theta}))\\,\\right]}&{i f j=L^{\\mathrm{opt}},}\\\\ {=0}&{o t h e r w i s e.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. Recall that the optimal aggregation problem reads ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{maximize}}&{{}W({\\boldsymbol{\\mathsf{B}}},\\mathbf{n})\\in\\{0,1\\}^{J}\\times{\\mathbb{R}}_{+}^{J}\\mapsto\\displaystyle\\sum_{j\\in[J]}u_{j}({\\boldsymbol{\\mathsf{B}}},\\mathbf{n})}\\\\ {\\mathrm{subject~to}}&{{}\\displaystyle\\operatorname*{min}_{j\\in[J]}u_{j}({\\boldsymbol{\\mathsf{B}}},\\mathbf{n})-\\underline{{u}}_{j}\\geqslant0~.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Define for any $j\\in[J],\\mathsf{B}_{-j}\\in\\{0,1\\}^{J-1}$ and $\\mathbf{n}_{-j}\\in\\mathbb{R}_{+}^{J-1}$ , the maximum number of samples agent $j\\in[J]$ having $B_{j}=1$ may be asked given their participation constraint: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\overline{{n}}_{j}(\\mathtt{B},\\mathbf{n})=\\operatorname*{max}\\{n\\geqslant0:\\,u_{j}((1,\\mathtt{B}_{-j}),(n,\\mathbf{n}_{-j}))\\geqslant\\underline{{u}}_{j}\\}\\;.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Given Equation (5) and Proposition 1, we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\overline{{{n}}}_{j}(\\mathsf{B},\\mathbf{n})=\\underline{{{n}}}-a c^{-1}(\\varepsilon(\\mathsf{B},\\mathbf{n})-\\varepsilon(\\theta_{j},\\underline{{{n}}}))\\ ,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\begin{array}{r}{N=\\sum_{j\\in[J]}B_{j}n_{j}=\\mathbf{n}^{\\mathrm{T}}\\mathbf{B}}\\end{array}$ . Problem (28)rewrites in canonical form ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{minimize}}&{{}-W({\\mathsf{B}},{\\mathbf{n}})\\quad\\mathrm{subject}\\;{\\mathsf{t o}}\\quad\\left\\{\\begin{array}{l l}{\\operatorname*{max}_{j\\in{\\mathcal{B}}}n_{j}-\\overline{{n}}_{j}({\\mathsf{B}},{\\mathbf{n}})\\leqslant0}\\\\ {\\operatorname*{max}_{j\\in[J]}-n_{j}\\leqslant0\\;,}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\mathcal{B}=\\{j\\in[J]:\\,B_{j}=1\\}$ . We first show that it admits a unique minimum. ", "page_idx": 14}, {"type": "text", "text": "Fix a ${\\mathsf{B}}\\in\\left\\{0,1\\right\\}^{J}$ and consider problem (30) with respect to $\\mathbf{n}\\in\\mathbb{R}_{+}^{J}$ only. We call this subproblem $\\mathcal{T}_{\\mathtt{B}}$ . First, we show that it is enough to focus on $(n_{j})_{j\\in B}$ rather than $(n_{j})_{j\\in[J]}$ in $\\mathcal{T}_{\\mathtt{B}}$ . With $\\mathbf{n}_{B}=$ $(\\mathrm{n}_{j})_{j\\in B}$ and $\\mathbf{n}_{B^{c}}=(\\mathbf{n}_{j})_{j\\notin\\mathcal{B}}$ , note that $W({\\boldsymbol{\\mathsf{B}}},\\mathbf{n})$ is decomposable in $\\mathbf{n}_{B}$ and $\\mathbf{n}_{B^{c}}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\nW({\\bf B},{\\bf n})=W({\\bf B},{\\bf n}_{B})+W({\\bf B},{\\bf n}_{B^{c}})~,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Wwhere, by a slight abuse of notation $\\begin{array}{r l r}{W({\\tt B},{\\bf n}_{B})}&{{}=}&{\\sum_{j\\in{\\tt B}}u_{j}({\\tt B},{\\bf n})}\\end{array}$ aind $\\begin{array}{r l}{W(\\mathsf{B},\\mathbf{n}_{B^{c}})}&{{}=}\\end{array}$ $\\textstyle\\sum_{j\\in B^{c}}u_{j}(\\mathbf{B},\\mathbf{n})$ .With ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Xi_{\\mathsf{B}}=\\left\\{\\mathbf{n}_{B}\\in\\mathbb{R}_{+}^{8}:\\begin{array}{c}{\\operatorname*{max}_{j\\in{\\boldsymbol{B}}}\\{n_{j}-\\overline{{n}}_{j}(\\mathsf{B},\\mathbf{n})\\}\\leqslant0}\\\\ {\\operatorname*{max}_{j\\in{\\boldsymbol{B}}}-n_{j}\\leqslant0}\\end{array}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "problem $\\mathcal{T}_{\\mathtt{B}}$ is equivalent to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{n}s\\in\\Xi_{\\mathbb{B}}}-W(\\mathbf{B},\\mathbf{n}_{\\mathcal{B}})+\\operatorname*{min}_{\\mathbf{n}_{\\mathbf{B}^{c}}\\in\\mathbb{R}_{+}^{J-|\\mathcal{B}|}}-W(\\mathbf{B},\\mathbf{n}_{\\mathcal{B}^{c}})=\\operatorname*{min}_{\\mathbf{n}s\\in\\Xi_{\\mathbb{B}}}-W(\\mathbf{B},\\mathbf{n}_{\\mathcal{B}})+\\sum_{j\\in[J]}-(1-B_{j})_{\\mathbb{Z}_{j}}\\,,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "by Proposition 1. Since $\\begin{array}{r}{\\sum_{j\\in[J]}-(1-B_{j}){u}_{j}}\\end{array}$ is constant,bEquaton 2iisnughton the existence of $\\mathrm{min}_{{\\bf n}_{B}\\in\\Xi_{8}}-W({\\bf B},{\\bf n}_{B})$ . On the one hand, $\\Xi_{\\mathsf{B}}$ is bounded. Indeed for any $j\\in\\mathcal{B}$ by Equation (29) and Lemma 1 ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{n}}_{j}(\\mathbf{B},\\mathbf{n}_{B})=\\underline{{n}}-\\frac{a}{c}[\\varepsilon(\\mathbf{B},\\mathbf{n}_{B})-\\varepsilon(\\theta_{j},\\underline{{n}})]}\\\\ &{\\qquad\\qquad=\\underline{{n}}-\\frac{a}{c}\\bigg[\\mathcal{R}_{0}^{\\star}+2\\bigg[\\frac{\\alpha_{\\delta}}{(1+\\mathbf{n}_{B}^{\\top}\\mathbf{B})^{\\gamma}}+\\beta+\\vartheta(\\mathbf{B},\\mathbf{n})\\bigg]-\\mathcal{R}_{0}^{\\star}-2\\bigg[\\frac{\\alpha_{\\delta}}{(1+\\underline{{n}})^{\\gamma}}-\\beta-\\theta_{j}\\bigg]\\bigg]}\\\\ &{\\qquad\\qquad=\\underline{{n}}-\\frac{2a}{c}\\big[\\alpha_{\\delta}\\big((1+\\mathbf{n}_{B}^{\\top}\\mathbf{B})^{-\\gamma}-(1+\\underline{{n}})^{-\\gamma}\\big)+(\\vartheta(\\mathbf{B},\\mathbf{n})-\\theta_{j})\\big]}\\\\ &{\\qquad\\qquad\\leqslant\\underline{{n}}-\\frac{2a}{c}\\bigg[\\alpha_{\\delta}\\big(1-(1+\\underline{{n}})^{-\\gamma}\\big)+\\underset{j\\in B}{\\operatorname*{max}}\\theta_{j}\\bigg]}\\\\ &{\\qquad\\qquad=M}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, $\\Xi_{\\mathsf{B}}\\subset[0,M]^{|\\mathcal{B}|}$ . Moreover, $\\Xi_{\\mathsf{B}}$ is closed and convex. For any $j\\in\\mathcal{B}$ , rewrite ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\overline{{n}}_{j}({\\bf B},{\\bf n}_{B})=\\underline{{n}}-\\frac{2a}{c}\\Big[\\alpha_{\\delta}\\big((1+{\\bf n}_{B}^{\\mathrm{T}}{\\bf B})^{-\\gamma}-(1+\\underline{{n}})^{-\\gamma}\\big)+\\Big(\\big({\\bf n}_{B}^{\\mathrm{T}}{\\bf B}\\big)^{-1}{\\bf n}_{B}^{\\mathrm{T}}\\theta-\\theta_{j}\\Big)\\Big]}~}\\\\ {{\\displaystyle=g_{j}\\big({\\bf n}_{B}\\big)~,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and define $h_{j}\\ :{\\bf\\.n}_{B}\\mapsto\\,{\\bf n}_{B,j}\\,-\\,g_{j}({\\bf n}_{B})$ Observe that $\\Xi_{\\mathsf{B}}\\;=\\;\\big\\{\\mathbf{n}_{B}\\;\\in\\;\\mathbb{R}_{+}^{|B|}\\;:\\;\\;\\forall j\\;\\in\\;\\mathcal{B},\\,n_{B,j}\\;\\geqslant$ 0 and $h_{j}(\\mathbf{n}_{B})\\ \\leqslant\\ 0\\}$ , that is $\\Xi_{\\mathsf{B}}\\ =\\ A\\ \\cap\\ B$ with $A~=~\\mathbb{R}_{+}^{|B|}$ and $B~=~h_{1}^{-1}((-\\infty,0])~\\cap$ $\\cdots\\cap h_{|B|}^{-1}((-\\infty,0])$ . For any $j\\ \\in\\ B$ $h_{j}^{-1}((-\\infty,0])$ is convex, because so is $h_{j}$ .Additionally, $h_{j}^{-1}((-\\infty,0])$ is closed as the inverse image of a closed set by a continuous function. It follows that $\\Xi_{\\mathsf{B}}$ is convex and closed as the intersection of convex and closed sets. ", "page_idx": 15}, {"type": "text", "text": "As a consequence $\\Xi_{\\mathsf{B}}$ is compact and convex, and $-W({\\sf B},\\cdot)$ is strictly convex so $\\mathcal{T}_{\\mathtt{B}}$ admits a unique minimizer ng $\\mathbf{n}_{B}^{\\mathrm{opt}}\\in\\Xi_{\\mathsf{B}}$ ", "page_idx": 15}, {"type": "text", "text": "Since there are finitely many ${\\sf B}\\,\\in\\,\\left\\{0,1\\right\\}^{J}$ $\\begin{array}{r}{\\operatorname*{min}\\{-W({\\mathsf{B}},\\mathbf{n}_{B}^{\\mathrm{opt}})+\\sum_{j\\in{\\mathcal{B}}^{c}}{\\underline{{u}}}_{j}}\\end{array}$ $\\mathsf{B}\\,\\in\\,\\{0,1\\}^{J}\\}>0$ exists and coincide with the minimum of problem (30). We show later in the proof that the optimal ${\\sf B}^{\\mathrm{opt}}\\in\\left\\{0,1\\right\\}^{J}$ is unique (see part $^{\\,I}$ ), so the minimizer of Equation (30) is unique. This establishes the point (i) of the result. ", "page_idx": 15}, {"type": "text", "text": "The remainder of the proof proceeds as follows: we first characterize the optimal ${\\sf B}^{\\mathrm{opt}}\\in\\left\\{0,1\\right\\}^{J}$ ,and then provethathesltion $\\mathbf{n}_{B^{\\mathrm{opt}}}^{\\mathrm{opt}}\\in\\mathbb{R}_{+}^{|B|}$ $\\mathcal{T}_{\\mathrm{B^{opt}}}$ ", "page_idx": 15}, {"type": "text", "text": "Part I:Optimal $\\mathsf{B}\\in\\{0,1\\}^{[J]}$ ", "page_idx": 15}, {"type": "text", "text": "Weshowthat $\\mathsf{B}^{\\mathrm{opt}}\\,=\\,(1,\\ldots,1)^{\\mathrm{T}}$ . By contradiction, assume there exists $\\mathsf{B}^{\\prime}\\neq(1,\\ldots,1)^{\\mathrm{T}}$ and $\\mathbf{n}^{\\prime}\\in\\mathbb{R}_{+}^{J}$ such that for any $(\\mathsf{B},\\mathbf{n})\\in\\left\\{0,1\\right\\}^{J}\\times\\mathbb{R}_{+}^{J}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n-W({\\sf B}^{\\prime},{\\bf n}^{\\prime})\\leqslant-W({\\sf B},{\\bf n})\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Denote by $j~\\in~[J]$ an agent such that $B_{j}^{\\prime}\\,=\\,0$ , and first assume $\\theta_{j}\\;\\geqslant\\;\\vartheta({\\sf B}^{\\prime},{\\bf n}^{\\prime})$ . Consider the alternative allocation $(\\mathsf{B}^{\\prime\\prime},\\mathbf{n}^{\\prime\\prime})=((1,\\mathsf{B}_{-j}^{\\prime}),(0,\\mathbf{n}_{-j}^{\\prime}))$ . Observe that $B_{j}^{\\prime}n_{j}^{\\prime}=B_{j}^{\\prime\\prime}n_{j}^{\\prime\\prime}=0$ ", "page_idx": 15}, {"type": "equation", "text": "$$\nN^{\\prime}=\\mathsf{B}^{\\prime\\,\\mathrm{T}}\\mathbf{n}^{\\prime}=\\mathsf{B}^{\\prime\\prime\\,\\mathrm{T}}\\mathbf{n}^{\\prime\\prime}=N^{\\prime\\prime}\\quad\\mathrm{and}\\quad\\vartheta(\\mathsf{B}^{\\prime},\\mathbf{n}^{\\prime})=\\vartheta(\\mathsf{B}^{\\prime\\prime},\\mathbf{n}^{\\prime\\prime})\\ .\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This in particular implies ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\varepsilon({\\boldsymbol{\\mathsf{B}}}^{\\prime},\\mathbf{n}^{\\prime})=2\\bigl[\\alpha_{\\delta}(1+N^{\\prime})^{-\\gamma}+\\beta+\\vartheta({\\boldsymbol{\\mathsf{B}}}^{\\prime},N^{\\prime})\\bigr]}\\\\ &{\\qquad\\qquad=2\\bigl[\\alpha_{\\delta}(1+N^{\\prime\\prime})^{-\\gamma}+\\beta+\\vartheta({\\boldsymbol{\\mathsf{B}}}^{\\prime\\prime},N^{\\prime\\prime})\\bigr]=\\varepsilon({\\boldsymbol{\\mathsf{B}}}^{\\prime\\prime},\\mathbf{n}^{\\prime\\prime})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus by (36) ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{-u_{j}(\\mathbf{B}^{\\prime},\\mathbf{n}^{\\prime})=\\underline{{u}}_{j}=a(\\mathcal{R}_{0}^{\\star}+\\varepsilon(\\theta_{j},\\underline{{n}}))+c\\underline{{n}}>a(\\mathcal{R}_{0}^{\\star}+\\varepsilon(\\theta_{j},\\underline{{n}}))}&{}&\\\\ {\\geqslant a(\\mathcal{R}_{0}^{\\star}+\\varepsilon(\\mathbf{B}^{\\prime},\\mathbf{n}^{\\prime}))=a(\\mathcal{R}_{0}^{\\star}+\\varepsilon(\\mathbf{B}^{\\prime\\prime},\\mathbf{n}^{\\prime\\prime}))=-u_{j}(\\mathbf{B}^{\\prime\\prime},\\mathbf{n}^{\\prime\\prime})\\;,}&{}&\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the second inequality results from $\\theta_{j}\\;\\geqslant\\;\\vartheta({\\sf B}^{\\prime},{\\bf n}^{\\prime})$ and $N^{\\prime}\\geqslant\\;\\underline{{n}}$ .Finally by definition of $(\\mathbf{B}^{\\prime\\prime},\\mathbf{n}^{\\prime\\prime})$ , (35) and (36), for any $k\\neq j$ such that $B_{k}^{\\prime\\prime}=1$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-u_{k}({\\bf B}^{\\prime\\prime},{\\bf n}^{\\prime\\prime})=a\\big[\\mathcal{R}_{0}^{\\star}+2\\big(\\alpha_{\\delta}(1+N^{\\prime\\prime})^{-\\gamma}+\\beta+\\vartheta({\\bf B}^{\\prime\\prime},{\\bf n}^{\\prime\\prime})\\big)\\big]+c n_{k}^{\\prime\\prime}}\\\\ &{\\qquad\\qquad\\qquad=a\\big[\\mathcal{R}_{0}^{\\star}+2\\big(\\alpha_{\\delta}(1+N^{\\prime})^{-\\gamma}+\\beta+\\vartheta({\\bf B}^{\\prime},{\\bf n}^{\\prime})\\big)\\big]+c n_{k}^{\\prime}=-u_{k}({\\bf B}^{\\prime},{\\bf n}^{\\prime})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence, (38) together with (37) give ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle-W({\\bf B}^{\\prime\\prime},{\\bf n}^{\\prime\\prime})=-\\sum_{k\\neq j}u_{k}({\\bf B}^{\\prime\\prime},{\\bf n}^{\\prime\\prime})-u_{j}({\\bf B}^{\\prime\\prime},{\\bf n}^{\\prime\\prime})=-\\sum_{k\\neq j}u_{i}({\\bf B}^{\\prime},{\\bf n}^{\\prime})-u_{j}({\\bf B}^{\\prime\\prime},{\\bf n}^{\\prime\\prime})}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~<-\\sum_{k\\neq j}u_{i}({\\bf B}^{\\prime},{\\bf n}^{\\prime})-u_{j}({\\bf B}^{\\prime},{\\bf n}^{\\prime})=-W({\\bf B}^{\\prime},{\\bf n}^{\\prime})~.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This contradicts (34). Now assume $\\theta_{j}<\\vartheta(\\mathbf{B}^{\\prime},\\mathbf{n}^{\\prime})$ , and let $R\\in[J]$ be such that $\\theta_{R}=\\operatorname*{max}\\{\\theta_{k},\\,k\\in$ $B\\}$ . Consider $\\left(\\mathsf{B}^{\\prime\\prime},\\mathbf{n}^{\\prime\\prime}\\right)\\in\\left\\{0,1\\right\\}^{J}\\times\\mathbb{R}_{+}^{J}$ where ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{B}^{\\prime\\prime}=(B_{1}^{\\prime},\\ldots,B_{j-1}^{\\prime},1,B_{j+1}^{\\prime},\\ldots,B_{J}^{\\prime})^{\\mathrm{T}}}\\\\ &{\\mathsf{l}\\,\\mathbf{n}^{\\prime\\prime}=(n_{1}^{\\prime},\\ldots,n_{j-1}^{\\prime},\\,\\overline{{n}}_{j}\\,,n_{j+1}^{\\prime},\\ldots,n_{R-1}^{\\prime},\\,\\operatorname*{max}(0,n_{R}^{\\prime}-\\overline{{n}}_{j})\\,,n_{R+1}^{\\prime},\\ldots,n_{J}^{\\prime})^{\\mathrm{T}}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which is feasible. Observe on the one hand that ", "page_idx": 16}, {"type": "equation", "text": "$$\nN^{\\prime\\prime}=N^{\\prime}\\quad\\mathrm{and}\\quad\\vartheta(\\mathbf{B}^{\\prime\\prime},\\mathbf{n}^{\\prime\\prime})\\leqslant\\vartheta(\\mathbf{B}^{\\prime},\\mathbf{n}^{\\prime})+\\frac{\\overline{{n}}_{j}}{N^{\\prime}}(\\theta_{j}-\\theta_{R})<\\vartheta(\\mathbf{B}^{\\prime},\\mathbf{n}^{\\prime})\\;,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "because $\\theta_{j}<\\vartheta(\\mathsf{B}^{\\prime},\\mathbf{n}^{\\prime})\\leqslant\\theta_{R}$ . In particular ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\varepsilon({\\mathsf{B}}^{\\prime\\prime},{\\mathbf{n}}^{\\prime\\prime})={\\mathcal{R}}_{0}^{\\star}+2\\bigl[\\alpha_{\\delta}(N^{\\prime\\prime}+1)^{-\\gamma}+\\beta+\\vartheta({\\mathsf{B}}^{\\prime\\prime},{\\mathbf{n}}^{\\prime\\prime})\\bigr]}\\\\ &{\\qquad\\qquad<{\\mathcal{R}}_{0}^{\\star}+2\\bigl[\\alpha_{\\delta}(N^{\\prime}+1)^{-\\gamma}+\\beta+\\vartheta({\\mathsf{B}}^{\\prime},{\\mathbf{n}}^{\\prime})\\bigr]=\\varepsilon({\\mathsf{B}}^{\\prime},{\\mathbf{n}}^{\\prime})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "(40) in turn implies that for any $k\\neq j$ such that $B_{k}^{\\prime\\prime}=1$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n-u_{k}(\\mathsf{B}^{\\prime\\prime},\\mathbf{n}^{\\prime\\prime})=a[\\mathcal{R}_{0}^{\\star}+\\varepsilon(\\mathsf{B}^{\\prime\\prime},\\mathbf{n}^{\\prime\\prime})]+c n_{k}^{\\prime\\prime}<a[\\mathcal{R}_{0}^{\\star}+\\varepsilon(\\mathsf{B}^{\\prime},\\mathbf{n}^{\\prime})]+c n_{k}^{\\prime}=-u_{k}^{\\prime}(\\mathsf{B}^{\\prime},\\mathbf{n}^{\\prime})\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "because $n_{k}^{\\prime}=n_{k}^{\\prime\\prime}$ for $k\\in B\\setminus\\{j,R\\}$ and $n_{R}^{\\prime\\prime}<n_{R}^{\\prime}$ . On the other hand, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{u_{j}\\big(\\mathrm{B}^{\\prime\\prime},\\mathbf{n}^{\\prime\\prime}\\big)=\\underline{{u}}_{j}=u_{j}\\big(\\mathrm{B}^{\\prime},\\mathbf{n}^{\\prime}\\big)\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "by definition (29) of $\\overline{{n}}_{j}(\\mathbf{B}^{\\prime\\prime},\\mathbf{n}^{\\prime\\prime})$ . (41) and (42) together yield ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle-W({\\bf B}^{\\prime\\prime},{\\bf n}^{\\prime\\prime})=-\\sum_{k\\neq j}u_{k}({\\bf B}^{\\prime\\prime},{\\bf n}^{\\prime\\prime})-u_{j}({\\bf B}^{\\prime\\prime},{\\bf n}^{\\prime\\prime})=-\\sum_{k\\neq j}u_{k}({\\bf B}^{\\prime\\prime},{\\bf n}^{\\prime\\prime})-u_{j}({\\bf B}^{\\prime},{\\bf n}^{\\prime})}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~<-\\sum_{k\\neq j}u_{k}({\\bf B}^{\\prime},{\\bf n}^{\\prime})-u_{j}({\\bf B}^{\\prime},{\\bf n}^{\\prime})=-W({\\bf B}^{\\prime},{\\bf n}^{\\prime})~,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which once again violates (34). Thus, we obtain $\\mathsf{B}^{\\mathrm{opt}}=(1,\\ldots,1)^{\\mathrm{T}}$ , that is $B^{\\mathrm{opt}}=[J]$ ", "page_idx": 16}, {"type": "text", "text": "Part 2: Characterization of $\\mathbf{n}_{[J]}^{\\mathrm{opt}}\\in\\mathbb{R}_{+}^{J}$ ", "page_idx": 16}, {"type": "text", "text": "We now foousonth problem $\\mathcal{T}_{\\mathrm{Bopt}}$ To lighten notatons, we write $\\mathbf{n}^{\\mathrm{opt}}\\in\\mathbb{R}_{+}^{J}$ instead of ${\\bf n}_{B^{\\mathrm{opt}}}^{\\mathrm{opt}}$ and B instead of $\\mathsf{B}^{\\mathrm{opt}}=(1,\\ldots,1)$ Wefirst reformulate $\\mathcal{T}_{\\mathtt{B}}$ in a more convenient way. Recall that $\\mathcal{T}_{B}$ reads ", "page_idx": 16}, {"type": "equation", "text": "$$\n({\\mathcal T}_{8}):\\quad\\mathrm{minimize}\\quad-{\\cal W}({\\mathsf{B}},{\\mathsf{n}})\\quad\\mathrm{subject}\\;{\\mathsf{t o}}\\quad{\\mathsf{n}}\\in\\Xi_{8}\\;,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\Xi_{\\mathsf{B}}$ is defined in (31). As explained in Appendix A, this problem can equivalently be stated as ", "page_idx": 17}, {"type": "equation", "text": "$$\n(\\widetilde{\\mathcal{T}}_{8}):\\quad\\mathrm{minimize}\\quad-\\widetilde{W}_{\\mathrm{B}}(\\lambda,N)\\quad\\mathrm{subject}\\;\\mathrm{to}\\quad(\\lambda,N)\\in\\widetilde{\\Xi}_{8}\\ ,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n-\\widetilde{W}_{\\mathsf{B}}(\\lambda,N)=f(N)+g(\\lambda)\\;,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with ", "page_idx": 17}, {"type": "equation", "text": "$$\nf(N)=a J(\\mathcal{R}_{0}^{\\star}+2\\alpha_{\\delta}(1+N)^{-\\gamma})+c N\\quad\\mathrm{and}\\quad g(\\lambda)=2a J\\lambda^{\\mathrm{T}}\\theta\\;,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\widetilde{\\Xi_{\\mathsf{B}}}=\\left\\{(\\pmb{\\lambda},N)\\in\\Delta_{|\\pmb{\\mathscr{B}}|}\\times\\mathbb{R}_{+}:\\,\\pmb{\\lambda}N\\in\\Xi_{\\mathsf{B}}\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Writing $\\widetilde{\\Xi}_{\\mathsf{B}}(N)=\\{\\pmb{\\lambda}\\in\\Delta_{J}:\\,(\\pmb{\\lambda},N)\\in\\widetilde{\\Xi}_{\\mathsf{B}}\\}$ for $N\\geqslant0$ and ${\\mathcal{N}}=\\{N\\geqslant0:\\,\\widetilde{\\Xi}_{\\mathtt{B}}(N)\\neq\\varnothing\\},\\,\\widetilde{\\mathcal{T}}_{\\mathtt{E}}$ comes down to ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{(\\lambda,N)\\in\\tilde{\\Xi}_{8}}\\left\\{f(N)+g(\\lambda)\\right\\}=\\operatorname*{min}_{N\\in{\\cal N}}\\left\\{f(N)+\\operatorname*{min}_{\\lambda\\in\\tilde{\\Xi}_{8}(N)}g(\\lambda)\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In this proof, we address the innermost problem, which is enough to show that ${\\bf n}^{\\mathrm{opt}}$ satisfies the point (ii) of the result. Let $N\\geqslant0$ such that $\\widetilde{\\Xi}_{\\mathsf{B}}(N)\\neq\\emptyset$ which existsbecause $\\Xi_{\\mathsf{B}}\\neq\\emptyset$ (for instance, $((1,0,\\dots,0),(\\underline{{n}},0,\\dots,0))\\in\\Xi_{\\mathsf{B}})$ . By definition of $\\widetilde{\\Xi}_{\\sf B}$ , it reads ", "page_idx": 17}, {"type": "equation", "text": "$$\n(\\widetilde{\\mathcal{T}}^{(N)}):\\quad\\mathrm{minimize}\\quad\\lambda\\in\\Delta_{J}\\mapsto2a J\\lambda^{\\mathrm{T}}\\theta\\quad\\mathrm{subject}\\ 0\\quad\\left\\{\\begin{array}{l l}{\\forall j\\in B:\\lambda_{j}\\geqslant0}\\\\ {\\forall j\\in\\mathcal{B}:\\lambda_{j}N\\leqslant\\overline{{n}}_{j}(\\mathsf{B},\\lambda N)}\\\\ {\\sum_{j\\in\\mathcal{B}}\\lambda_{j}=1\\,,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The following lemma provides a necessary condition for any solution to problem (45). ", "page_idx": 17}, {"type": "text", "text": "Lemma 4. Let $\\pmb{\\lambda}^{(N)}=(\\lambda_{1}^{(N)},...\\,,\\lambda_{J}^{(N)})\\in\\Delta_{J}$ be a solution to (45). If there exists $r\\in[J]$ such that \u5165(N) > O, then \u5165K x(N)= N-1nk(B,NA(N)) forany k <r. ", "page_idx": 17}, {"type": "text", "text": "Proof. We denote by $\\bar{\\mathbf{n}}(\\mathbf{B},\\mathbf{n})\\,=\\,(\\overline{{n}}_{1}(\\mathbf{B},\\mathbf{n}),\\dots,\\overline{{n}}_{J}(\\mathbf{B},\\mathbf{n}))^{\\mathrm{T}}$ for any $\\textbf{n}\\in\\mathbb{R}_{+}^{J}$ . The Lagrangian associated to problem (45), with $\\pmb{\\mu},\\pmb{\\rho}$ and $\\nu$ the associated dual variables, reads: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(\\lambda,\\mu,\\rho,\\nu)=2a J\\lambda^{\\mathrm{T}}\\theta-\\mu^{\\mathrm{T}}\\lambda+\\nu\\big(1-\\lambda^{\\mathrm{T}}\\mathbf{1}\\big)+\\rho^{\\mathrm{T}}[N\\lambda-\\bar{\\mathbf{n}}(\\mathrm{B},N\\lambda)]}\\\\ &{\\quad\\quad\\quad\\quad=2a J\\lambda^{\\mathrm{T}}\\theta+c N-\\mu^{\\mathrm{T}}\\lambda+\\nu\\big(1-\\lambda^{\\mathrm{T}}\\mathbf{1}\\big)}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\,\\rho^{\\mathrm{T}}(N\\lambda-\\underline{{n}}\\mathbf{1}+\\frac{2a}{c}\\big[\\alpha_{\\delta}\\big((N+1)^{-\\gamma}-(\\underline{{n}}+1)^{-\\gamma}\\big)\\mathbf{1}+\\big((\\lambda^{\\mathrm{T}}\\theta)\\mathbf{1}-\\theta\\big)\\big]\\big)\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since the objective and the constraints are convex, $\\mathcal{L}$ admits a saddlepoint $(\\pmb{\\lambda}^{(N)},\\pmb{\\mu},\\pmb{\\rho},\\nu)\\ \\in$ $\\Delta\\times\\mathbb{R}^{J}\\times\\mathbb{R}^{\\mathcal{I}}\\times\\mathbb{R}$ which is solution to problem (45) and verifies the following KKT conditions: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\begin{array}{l l}{\\forall k\\leqslant J:\\;2a J\\theta_{k}-\\mu_{k}+\\rho_{k}\\big(N+2a c^{-1}\\theta_{k}\\big)=\\nu}\\\\ {\\forall k\\leqslant J:\\;\\mu_{k}\\lambda_{k}^{(N)}=0,\\quad\\mu_{k}\\geqslant0}\\\\ {\\forall k\\leqslant J:\\;\\rho_{k}\\Big(\\lambda_{k}^{(N)}\\,N-\\overline{n}_{k}\\big(\\mathsf{B},\\lambda^{(N)}N\\big)\\Big)=0,\\quad\\rho_{k}\\geqslant0}\\\\ {\\displaystyle\\sum_{k\\in\\mathcal{B}}\\lambda_{k}^{(N)}=1\\;.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Assume there exists $r\\in\\mathcal{B}$ such that $\\lambda_{r}^{(N)}>0$ By complementary slackness 47), $\\mu_{r}=0$ and (46) gives for any $k<r$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n2a J\\theta_{k}-\\mu_{k}+\\rho_{k}(N+2a c^{-1}\\theta_{k})=2a J\\theta_{r}+\\rho_{r}(N+2a c^{-1}\\theta_{r})\\;,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "That is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\rho_{k}=\\frac{2a J(\\theta_{r}-\\theta_{k})+\\mu_{k}}{N+2a c^{-1}\\theta_{k}}+\\rho_{r}\\frac{N+2a c^{-1}\\theta_{r}}{N+2a c^{-1}\\theta_{k}}\\ .\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By assumption $\\theta_{r}>\\theta_{k}$ , and since $\\mu_{k}\\geqslant0$ as well as $\\rho_{r}\\geqslant0$ , we have $\\rho_{k}>0$ . It follows from (48) for agent $k$ that $\\lambda_{k}^{\\mathrm{opt}}N=\\overline{{n}}(\\mathsf{B},N\\lambda^{(N)})$ ", "page_idx": 17}, {"type": "text", "text": "Now, define ", "page_idx": 18}, {"type": "equation", "text": "$$\nM_{j}=\\sum_{k\\leqslant j}\\overline{{n}}_{k}\\big({\\sf B},\\lambda^{(N)}N\\big)\\quad\\mathrm{and}\\quad L=\\operatorname*{min}\\{j\\in\\mathcal{B}\\,:\\;M_{j}\\geqslant N\\}\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We show that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\lambda_{k}^{(N)}=\\overline{{n}}_{k}\\big({\\mathtt B},N\\lambda^{(N)}\\big)N^{-1}\\;\\mathrm{for}\\;\\mathrm{any}\\;k<L\\quad\\mathrm{~and~}\\quad\\lambda_{k}^{(N)}=0\\;\\mathrm{for}\\;\\mathrm{any}\\;k>L\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To prove the first point, assume by contradiction that there exists $\\ell~<~L$ such that $\\lambda_{\\ell}^{(N)}~<$ $\\overline{{n}}_{\\ell}(\\mathsf{B},\\lambda^{(N)}N)N^{-1}$ . By the contrapositive of Lemma 4, $\\lambda_{\\ell+1}^{\\mathrm{opt}}=.\\dots=\\lambda_{J}^{\\mathrm{opt}}=0.$ Thus ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{k\\leqslant J}\\lambda_{k}^{(N)}=\\sum_{k\\leqslant\\ell}\\lambda_{k}^{(N)}\\leqslant\\sum_{k\\leqslant\\ell}\\overline{{n}}_{k}({\\mathsf{B}},\\lambda^{(N)}N)N^{-1}=M_{\\ell}N^{-1}<1\\;,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "$L$ $\\ell<L$ $\\ell\\,>\\,L$ $\\lambda_{\\ell}^{(N)}\\,>\\,0$ $\\lambda_{k}^{(N)}\\,=$ $\\overline{{n}}_{k}(\\mathsf{B},\\lambda^{(N)}N)N^{-1}$ for any $k\\leqslant L<\\ell$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\sum_{k\\leqslant J}\\lambda_{k}^{(N)}\\geqslant\\sum_{k\\leqslant L}\\lambda_{k}^{(N)}+\\lambda_{\\ell}^{(N)}=\\sum_{k\\leqslant L}\\overline{{n}}_{k}({\\texttt B},{\\texttt A}^{(N)}N)N^{-1}+\\lambda_{\\ell}^{(N)}}}\\\\ {{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=M_{L}N^{-1}+\\lambda_{\\ell}^{(N)}}}\\\\ {{\\qquad\\qquad\\qquad\\qquad\\qquad\\geqslant1+\\lambda_{\\ell}^{(N)}>1\\;,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which once again violates the constraints of the problem. Finally by (52), ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\lambda_{i_{L}}^{(N)}=1-\\sum_{k<L}\\lambda_{k}^{(N)}-\\sum_{k>L}\\lambda_{k}^{(N)}=1-M_{L-1}N^{-1}\\;,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "by (52). By (52) and (53) we obtain for any $j\\in[J]$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\lambda_{j}^{(N)}=N^{-1}\\overline{{{n}}}_{j}({\\tt B},N{\\tt A}^{(N)})\\mathbb1_{\\{j<L\\}}+(1-N^{-1}M_{j-1})\\mathbb1_{\\{j=L\\}}\\;,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now, consider the solution ${\\bf n}^{\\mathrm{opt}}(\\pmb{\\theta})\\in\\mathbb{R}^{J}$ to the initial problem (45), and define ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{N^{\\mathrm{opt}}}&{=\\sum_{j\\in[J]}n_{j}^{\\mathrm{opt}}(\\pmb{\\theta})}\\\\ {\\lambda^{\\mathrm{opt}}(\\pmb{\\theta})}&{=\\frac{1}{N^{\\mathrm{opt}}}\\mathbf{n}^{\\mathrm{opt}}(\\pmb{\\theta})}\\\\ {L^{\\mathrm{opt}}}&{=\\operatorname*{min}\\{j\\in[J]:\\;\\sum_{k\\leqslant j}\\overline{{n}}_{k}(\\mathsf{B},\\mathbf{n}^{\\mathrm{opt}}(\\pmb{\\theta}))\\geqslant N^{\\mathrm{opt}}\\}\\;.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "$\\lambda^{\\mathrm{opt}}(\\theta)$ satisfies (54), and multiplying by $N^{\\mathrm{opt}}$ yields for any $j\\in[J]$ ", "page_idx": 18}, {"type": "equation", "text": "$$\nn_{j}^{\\mathrm{opt}}(\\pmb{\\theta})=\\overline{{n}}_{j}(\\mathbf{B},\\mathbf{n}^{\\mathrm{opt}}(\\pmb{\\theta}))\\mathbb{1}_{\\{j<L^{\\mathrm{opt}}\\}}+(N^{\\mathrm{opt}}-\\sum_{k\\leqslant j}n_{j}^{\\mathrm{opt}}(\\pmb{\\theta}))\\mathbb{1}_{\\{j=L^{\\mathrm{opt}}\\}}\\;,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which establishes point (i), and concludes the proof. ", "page_idx": 18}, {"type": "text", "text": "Corollary 1. Assume $H l,H2$ and $H3$ Then $L^{\\star}=\\Theta(J^{\\frac{1}{1+\\gamma}})$ and for any $j\\in[J]$ ", "page_idx": 18}, {"type": "equation", "text": "$$\nn_{j}^{\\star}(\\pmb\\theta)=\\mathbb{1}_{\\{j\\leq L^{\\star}\\}}\\left[\\frac{\\bar{N}}{L^{\\star}}+\\frac{2a}{c}\\left(\\theta_{j}-\\frac{1}{L^{\\star}}\\sum_{\\ell=1}^{L^{\\star}}\\theta_{\\ell}\\right)\\right]\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. By defnition of the simplified scheme (12), $n_{j}^{\\star}(\\pmb\\theta)=\\mathbb{1}_{\\{j\\leqslant L^{\\star}\\}}\\overline{{n}}_{j}(\\mathbf{1},\\mathbf{n}^{\\star}(\\pmb\\theta))=\\mathbb{1}_{\\{j\\leqslant L^{\\star}\\}}[\\underline{{n}}-$ $(2a/c)(\\varepsilon(\\mathbf{1},\\mathbf{n}^{\\star}(\\pmb\\theta))-\\varepsilon(\\theta_{j},\\underline{{n}}))]$ for any $j\\leqslant L^{\\star}$ .Since $\\begin{array}{r}{\\sum_{j\\in[J]}n_{j}^{\\star}(\\pmb{\\theta})=\\bar{N}}\\end{array}$ ,we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\bar{N}=L^{\\star}\\underline{{n}}-\\frac{a}{c}\\left[L^{\\star}\\varepsilon({\\bf1},{\\bf n}^{\\star}(\\theta))-\\sum_{j\\leqslant L^{\\star}}\\varepsilon(\\theta_{j},\\underline{{n}})\\right]}}\\\\ {{\\displaystyle\\quad=L^{\\star}\\left[\\underline{{n}}-\\frac{2a}{c}\\big[\\alpha_{\\delta}\\big((1+\\bar{N})^{-\\gamma}-(1+\\underline{{n}})^{-\\gamma}\\big)+(\\vartheta({\\bf1},{\\bf n}^{\\star}(\\theta))-\\bar{\\theta}_{L^{\\star}})\\big]\\right]\\,,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By Lemma 1 and denoting $\\begin{array}{r}{\\bar{\\theta}_{L^{\\star}}=L^{\\star-1}\\sum_{j\\leqslant L^{\\star}}\\theta_{j}}\\end{array}$ . This yelds ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\vartheta(1,\\mathbf{n}^{\\star}(\\pmb\\theta))=\\bar{\\theta}_{L^{\\star}}-\\alpha_{\\delta}((1+\\bar{N})^{-\\gamma}-(1+\\underline{n})^{-\\gamma})+(c/2a)(\\underline{n}-\\bar{N}/L^{\\star})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Plugging back this value in $\\begin{array}{r l r}{\\overline{{n}}_{j}^{\\star}(1,{\\bf n}^{\\star}(\\theta))}&{=}&{\\underline{{n}}\\;-\\;(2a/c)[\\alpha_{\\delta}\\big((1+\\bar{N})^{-\\gamma}-(1+\\underline{{n}})^{-\\gamma}\\big)\\;+}\\end{array}$ $\\left(\\vartheta(\\mathbf{1},\\mathbf{n}^{\\star}(\\pmb\\theta))-\\theta_{j}\\right)]$ gives for any $j\\in[J]$ ", "page_idx": 19}, {"type": "equation", "text": "$$\nn_{j}^{\\star}(\\pmb\\theta)=\\mathbb{1}_{\\{j\\leqslant L^{\\star}\\}}\\left[\\frac{\\bar{N}}{L^{\\star}}-\\frac{2a}{c}(\\bar{\\theta}_{L^{\\star}}-\\theta_{j})\\right]\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We now determine the order of magnitude of $L^{\\star}\\,\\in\\,\\{1,\\dots,J\\}$ . Recall on the one hand that by (33), there exists $M>0$ such that $0\\,\\leqslant n_{j}^{\\star}(\\pmb\\theta)\\,\\leqslant\\,M$ for any $j\\in[J]$ , and on the other hand that $\\bar{\\theta}_{L^{\\star}}-\\theta_{j}\\leqslant\\bar{\\theta}-\\underline{{\\theta}}=\\mathrm{diam}(\\Theta)<\\infty$ Therefore by (56), ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{2a}{c}\\mathrm{diam}(\\Theta)\\leqslant\\frac{\\bar{N}}{L^{\\star}}\\leqslant M+\\frac{2a}{c}\\mathrm{diam}(\\Theta)\\quad\\mathrm{so}\\quad\\frac{\\bar{N}}{L^{\\star}}=\\Theta(1)\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $\\bar{N}=(\\underline{{n}}+1)|\\mathcal{B}^{\\star}|^{\\frac{1}{1+\\gamma}}-1$ and $B^{\\star}=[J]$ , (57) results in $L=\\Theta(J^{\\frac{1}{1+\\gamma}})$ ", "page_idx": 19}, {"type": "text", "text": "Lemma 2. Assume $H I$ \uff0c $_{H2}$ and $\\theta_{j}-\\theta_{j-1}=\\mathcal{O}(1/J)$ for any $j\\in\\{2,\\dots,J\\}$ . Then, ", "page_idx": 19}, {"type": "equation", "text": "$$\nW({\\sf B}^{\\mathrm{opt}},{\\bf n}^{\\mathrm{opt}}(\\pmb\\theta))=W({\\sf B}^{\\star},{\\bf n}^{\\star}(\\pmb\\theta))+{\\mathcal O}(J^{\\frac{1}{1+\\gamma}})\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Recall that for any admissible $\\mathbf n\\in\\Xi_{\\mathsf{B}}$ , we can define $N=\\mathbf{1}^{\\mathrm{T}}\\mathbf{n}$ and $\\pmb{\\lambda}=N^{-1}\\mathbf{n}$ so that the social cost rewrites ", "page_idx": 19}, {"type": "equation", "text": "$$\n-W({\\bf B},{\\bf n})=-\\widetilde{W}_{\\bf B}(\\pmb{\\lambda},N)=f(N)+g(\\pmb{\\lambda})\\;,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with ", "page_idx": 19}, {"type": "equation", "text": "$$\nf(N)=a J(\\mathcal{R}_{0}^{\\star}+2\\alpha_{\\delta}(1+N)^{-\\gamma})+c N\\quad\\mathrm{and}\\quad g(\\lambda)=2a J\\lambda^{\\mathrm{T}}\\theta\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By definition, $\\begin{array}{r}{\\sum_{j\\in[J]}n_{j}^{\\star}(\\pmb{\\theta})=\\bar{N}}\\end{array}$ where $\\bar{N}=({\\underline{{n}}}+1)J^{\\frac{1}{1+\\gamma}}-1=\\mathrm{argmin}_{N\\geqslant0}\\,f(N).$ Hence, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W(B^{\\mathrm{opt}},\\mathbf{n}^{\\mathrm{opt}}(\\theta))-W(\\mathsf{B}^{\\star},\\mathbf{n}^{\\star}(\\theta))=\\widetilde{W}_{\\mathsf{B}}(\\lambda^{\\mathrm{opt}},N^{\\mathrm{opt}})-\\widetilde{W}_{\\mathsf{B}}(\\lambda^{\\star},\\bar{N})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=f(\\bar{N})+g(\\lambda^{\\star})-f(N^{\\mathrm{opt}})-g(\\lambda^{\\mathrm{opt}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leqslant g(\\lambda^{\\star})-g(\\lambda^{\\mathrm{opt}})=2a J(\\lambda^{\\star}-\\lambda^{\\mathrm{opt}})^{\\mathrm{T}}\\theta\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $\\theta_{j}\\leqslant\\theta_{L^{\\star}}$ for any $j\\in[J]$ such that $\\lambda_{j}^{\\star}>0$ , and $\\theta_{j}\\geqslant\\theta_{1}$ for any $j\\in[J]$ such that $\\lambda_{j}^{\\mathrm{opt}}>0$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{W(B^{\\mathrm{opt}},\\mathbf{n}^{\\mathrm{opt}}(\\pmb{\\theta}))-W(\\mathsf{B}^{\\star},\\mathbf{n}^{\\star}(\\pmb{\\theta}))\\leqslant2a J(\\theta_{L^{\\star}}-\\theta_{1})\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By assumption, for any $j\\,\\in\\,\\{2,\\ldots,J\\}\\;\\theta_{j}-\\theta_{j-1}=\\mathcal{O}(1/J)$ so there exists $k_{j}>0$ and $R_{j}\\,\\geqslant\\,0$ such that $\\theta_{j}-\\theta_{j-1}\\leqslant k_{j}J^{-1}$ for $J\\geqslant R_{j}$ . Denoting $R=\\operatorname*{max}_{j\\in[J]}R_{j}$ and $k=\\operatorname*{max}_{j\\in[J]}k_{j}$ , for any $J\\geqslant R$ we have $\\theta_{L^{\\star}}-\\theta_{1}\\leqslant k L^{\\star}$ , SO: ", "page_idx": 19}, {"type": "equation", "text": "$$\nW(B^{\\mathrm{opt}},\\mathbf{n}^{\\mathrm{opt}}(\\pmb{\\theta}))-W(\\mathbf{B}^{\\star},\\mathbf{n}^{\\star}(\\pmb{\\theta}))\\leqslant2a J\\frac{k L^{\\star}}{J}=2a k L^{\\star}\\;,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "$W(B^{\\mathrm{opt}},\\mathbf{n}^{\\mathrm{opt}}(\\pmb{\\theta}))-W(\\mathbf{B}^{\\star},\\mathbf{n}^{\\star}(\\pmb{\\theta}))=\\mathcal{O}(L^{\\star})=\\mathcal{O}(J^{\\frac{1}{1+\\gamma}})$ by Corollary 1. ", "page_idx": 19}, {"type": "text", "text": "Theorem 2 (Unravelling). Assume ${\\cal H}l,\\,{\\cal H}2,\\,{\\cal H}3,$ and $H4.$ Let $\\mathcal{E}\\subset\\mathcal{S}^{J}$ be the set of pure-strategy Nash equilibria of the game induced by $\\Gamma$ Wehave ", "page_idx": 19}, {"type": "equation", "text": "$$\n(i)\\ {\\mathcal{E}}\\neq\\emptyset\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "(i at any $s^{\\star}\\in{\\mathcal{E}}$ \uff0c $\\mathsf{B}=(0,\\ldots,0)$ or $\\mathsf{B}=(0,\\ldots,0,1)$ ", "page_idx": 19}, {"type": "text", "text": "Proof. First, we show that the situation where the coalition is empty is a Nash equilibrium. Consider $\\mathbf{s}=((0,\\dag),\\ldots,(0,\\dag))\\,\\in\\,\\mathcal{S}^{N}$ . For any $j\\,\\in\\,[J]$ and deviation $\\bar{(1,\\tilde{\\theta}_{j})}$ \uff0c $B\\,=\\,\\{j\\}$ $\\widehat{g}_{\\mathtt{B}}=\\widehat{g}_{j}$ and $v_{j}((0,\\dag),{\\bf s}_{-j}^{\\star})=\\underline{{u}}_{j}\\geqslant v_{j}((1,\\widetilde{\\theta}_{j}),{\\bf s}_{-j})$ by Proposition 1. Thus, $s^{\\star}\\in{\\mathcal{E}}$ ", "page_idx": 20}, {"type": "text", "text": "Now, consider a pure-strategy Nash equilibrium $s^{\\star}\\in{\\mathcal{E}}$ such that ${\\mathcal{B}}=\\{j\\in[J]:\\;B_{j}=1\\}\\neq\\emptyset$ Denoteby $\\mathcal{C}=\\{j\\in\\mathcal{B}:\\,n_{j}^{\\star}(\\widetilde{\\pmb{\\theta}})>0\\}$ the set of contributors. We start with two technical lemmas: ", "page_idx": 20}, {"type": "text", "text": "Lemma 5. There exists $\\Delta\\in\\mathbb R$ such that for any $(j,k)\\in\\mathcal{C}^{2}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\theta_{j}-\\widetilde{\\theta}_{j}=\\theta_{k}-\\widetilde{\\theta}_{k}=\\Delta\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. To lighten notation, we write in this proof ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\vartheta(\\widetilde{\\pmb{\\theta}})=N^{-1}\\sum_{j\\in\\mathcal{C}}n_{j}^{\\star}(\\widetilde{\\pmb{\\theta}})\\,\\theta_{j}\\quad\\mathrm{and}\\quad\\widetilde{\\vartheta}(\\widetilde{\\pmb{\\theta}})=N^{-1}\\sum_{j\\in\\mathcal{C}}n_{j}^{\\star}(\\widetilde{\\pmb{\\theta}})\\,\\widetilde{\\theta}_{j},\\quad\\mathrm{where}\\quad N=\\sum_{k\\in\\mathcal{C}}n_{k}^{\\star}(\\widetilde{\\pmb{\\theta}})\\ .\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By Equation (12) and $\\mathbf{H}4$ , for any $\\begin{array}{r}{j\\in\\mathcal{C},\\,n_{j}^{\\star}(\\widetilde{\\pmb{\\theta}})=\\overline{{n}}_{j}(\\mathsf{B},\\mathbf{n}^{\\star}(\\widetilde{\\pmb{\\theta}}))=\\underline{{n}}-\\frac{a}{c}(\\varepsilon(\\widetilde{\\vartheta}(\\widetilde{\\pmb{\\theta}}),N)-\\varepsilon(\\widetilde{\\theta}_{j},\\underline{{n}})),}\\end{array}$ so their payoff reads ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v_{j}((1,\\widetilde{\\theta}_{j}),\\mathbf{s}_{-j})=-a(\\mathcal{R}_{0}^{\\star}+\\varepsilon(\\vartheta(\\widetilde{\\theta}),N)-c\\Big[\\underline{{n}}-\\frac{a}{c}\\Big(\\varepsilon(\\widetilde{\\vartheta}(\\widetilde{\\theta}),N)-\\varepsilon(\\widetilde{\\theta}_{j},\\underline{{n}})\\Big)\\Big]}\\\\ &{\\quad\\quad\\quad\\quad\\quad=-a(\\mathcal{R}_{0}^{\\star}+\\varepsilon(\\theta_{j},\\underline{{n}}))-c\\underline{{n}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\left[\\Big(\\varepsilon(\\theta_{j},\\underline{{n}})-\\varepsilon(\\widetilde{\\theta}_{j},\\underline{{n}})\\Big)-\\Big(\\varepsilon(\\vartheta(\\widetilde{\\theta}),N)-\\varepsilon(\\widetilde{\\vartheta}(\\widetilde{\\theta}),N)\\Big)\\right]}\\\\ &{\\quad\\quad\\quad\\quad=\\underline{{u}}_{j}+2a\\Big[\\Big(\\theta_{j}-\\widetilde{\\theta}_{j}\\Big)-\\Big(\\vartheta(\\widetilde{\\theta})-\\widetilde{\\vartheta}(\\widetilde{\\theta})\\Big)\\Big]\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since s is a Nash equilibrium and $\\underline{{u}}_{j}=v_{j}((0,\\dag),\\mathbf{s}_{-j})$ , we have in particular that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{2a\\Big[\\Big(\\theta_{j}-\\widetilde{\\theta}_{j}\\Big)-\\Big(\\vartheta(\\widetilde{\\pmb{\\theta}})-\\widetilde{\\vartheta}(\\widetilde{\\pmb{\\theta}})\\Big)\\Big]\\geqslant0\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "that is $\\theta_{j}-\\widetilde{\\theta}_{j}=\\Delta_{j}\\geqslant\\Delta=\\vartheta(\\widetilde{\\pmb{\\theta}})-\\widetilde{\\vartheta}(\\widetilde{\\pmb{\\theta}})$ for any $j\\in\\mathcal{C}$ We now show that this holds with strict equality for any $j\\in\\mathcal{C}$ . By contradiction, assume there exists $r\\in\\mathcal{C}$ such that $\\Delta_{r}=\\Delta+\\chi$ with $\\chi>0$ . Then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\Delta=\\vartheta(\\widetilde{\\pmb{\\theta}})-\\widetilde{\\vartheta}(\\widetilde{\\pmb{\\theta}})=\\displaystyle\\sum_{j\\in\\mathcal{C}}\\lambda_{j}^{\\star}(\\widetilde{\\pmb{\\theta}})\\Delta_{j}=\\displaystyle\\sum_{j\\in\\mathcal{C}\\backslash\\{r\\}}\\lambda_{j}^{\\star}(\\widetilde{\\pmb{\\theta}})\\Delta_{j}+\\lambda_{r}^{\\star}(\\widetilde{\\pmb{\\theta}})(\\Delta+\\chi)}}\\\\ {{\\geqslant\\Delta+\\lambda_{r}^{\\star}(\\widetilde{\\pmb{\\theta}})\\chi>\\Delta\\;,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "because $\\lambda_{r}^{\\star}({\\widetilde{\\pmb{\\theta}}})>0$ as $r\\in\\mathcal{C}$ . This is a contradiction, and establishes the result ", "page_idx": 20}, {"type": "text", "text": "Lemma 6. For any $j\\in\\mathcal{C}$ ${\\widetilde{\\theta}}_{j}=\\underline{{\\theta}}$ ", "page_idx": 20}, {"type": "text", "text": "Proof. Let $j\\in\\mathcal{C}$ , by (58): ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v_{j}\\big((1,\\widetilde{\\theta}_{j}),\\mathbf{s}_{-j}\\big)=\\underline{{u}}_{j}+2a\\Big[\\Big(\\theta_{j}-\\widetilde{\\theta}_{j}\\Big)-\\Big(\\vartheta(\\widetilde{\\theta})-\\widetilde{\\vartheta}(\\widetilde{\\theta})\\Big)\\Big]}\\\\ &{\\quad\\quad\\quad\\quad=\\underline{{u}}_{j}+2a\\Bigg[\\Big(1-\\lambda_{j}^{\\star}(\\widetilde{\\theta})\\Big)\\Big(\\theta_{j}-\\widetilde{\\theta}_{j}\\Big)-\\displaystyle\\sum_{k\\in\\mathcal{C}\\setminus\\{j\\}}\\lambda_{k}^{\\star}(\\widetilde{\\theta})\\Big(\\theta_{k}-\\widetilde{\\theta}_{k}\\Big)\\Bigg]}\\\\ &{\\quad\\quad\\quad=\\underline{{u}}_{j}+2a\\Bigg[\\Bigg(\\displaystyle\\sum_{k\\in\\mathcal{C}\\setminus\\{j\\}}\\lambda_{k}^{\\star}(\\widetilde{\\theta})\\Bigg)\\Big(\\theta_{j}-\\widetilde{\\theta}_{j}\\Big)-\\displaystyle\\sum_{k\\in\\mathcal{C}\\setminus\\{j\\}}\\lambda_{k}^{\\star}(\\widetilde{\\theta})\\Big(\\theta_{k}-\\widetilde{\\theta}_{k}\\Big)\\Bigg]}\\\\ &{\\quad\\quad\\quad=\\underline{{u}}_{j}+2a q_{j}(\\widetilde{\\theta}_{j},\\widetilde{\\theta}_{-j})\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where ", "page_idx": 20}, {"type": "equation", "text": "$$\nq_{j}(\\widetilde{\\theta}_{j},\\widetilde{\\theta}_{-j})=\\sum_{k\\in\\mathcal{C}\\backslash\\{j\\}}\\lambda_{k}^{\\star}(\\widetilde{\\theta})[(\\theta_{j}-\\widetilde{\\theta}_{j})-(\\theta_{k}-\\widetilde{\\theta}_{k})]\\;.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We prove that $v_{j}((1,\\,\\cdot\\,),{\\bf s}_{-j})$ is strictly decreasing in ${\\widetilde{\\theta}}_{j}$ for $\\textit{j}\\in\\textit{c}$ \uff0cby showing that $\\partial{\\hat{v}_{j}}((1,\\widetilde{\\theta_{j}}),{\\bf s}_{-i_{j}})\\big/\\partial{\\widetilde{\\theta}_{j}}\\,<\\,0$ For any $j\\,\\in\\,{\\mathcal{C}}$ . For $v_{j}$ to be differentiable, we need $q_{j}$ to be differentiable, that is $\\begin{array}{r}{\\lambda_{j}^{\\star}(\\widetilde{\\pmb{\\theta}})=(\\sum_{k\\in\\mathcal{C}}n_{k}^{\\star}(\\widetilde{\\pmb{\\theta}}))^{-1}n_{j}^{\\star}(\\widetilde{\\pmb{\\theta}})}\\end{array}$ to be differentiable for any $j\\in\\mathcal{C}$ . We re-index $\\mathcal{C}$ as $\\{i_{1},\\ldots,i_{|\\mathcal{C}|}\\}$ so that $\\widetilde{\\theta}_{i_{1}}<\\ldots<\\widetilde{\\theta}_{i_{|c|}}$ . Observe that for $0\\,<\\,h\\,<\\,\\delta$ for $\\delta\\,>\\,0$ sufficiently small, $\\widetilde{\\theta}_{i_{j-1}}<\\widetilde{\\theta}_{j}+h<\\widetilde{\\theta}_{i_{j+1}}$ because $\\widetilde{\\theta}_{i_{j-1}}<\\widetilde{\\theta}_{j}<\\widetilde{\\theta}_{i_{j+1}}$ by Lemma 5 and $\\mathbf{H}3$ . Hence, the bid ordering does not change for any infinitesimal variation $\\mathrm{d}\\widetilde{\\theta}_{j}>0$ , nor does the indicator $\\mathbb{1}\\{i_{j}\\leqslant i_{|c|}\\}$ Consequently by Corollary 1, $n_{j}^{\\star}(\\widetilde{\\pmb{\\theta}})$ is differentiable in ${\\widetilde{\\theta}}_{j}$ and so is $\\lambda^{\\star}(\\widetilde{\\pmb\\theta})$ . We have for any $j\\in\\mathcal{C}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial v_{j}((1,\\widetilde{\\theta}_{j}),\\mathbf{s}_{k})}{\\partial\\widetilde{\\theta}_{j}}=2a\\frac{\\partial q_{j}(\\widetilde{\\theta}_{j},\\widetilde{\\theta}_{-j})}{\\partial\\widetilde{\\theta}_{j}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad=2a\\displaystyle\\sum_{k\\in\\mathcal{C}\\setminus\\{j\\}}\\left[\\frac{\\partial\\lambda_{k}^{k}(\\widetilde{\\theta})}{\\partial\\widetilde{\\theta}_{j}}\\Big(\\Big(\\theta_{j}-\\widetilde{\\theta}_{j}\\Big)-\\Big(\\theta_{k}-\\widetilde{\\theta}_{k}\\Big)\\Big)-\\lambda_{k}^{\\star}(\\widetilde{\\theta})\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad=2a\\displaystyle\\sum_{k\\in\\mathcal{C}\\setminus\\{j\\}}\\frac{\\partial\\lambda_{k}^{\\star}(\\widetilde{\\theta})}{\\partial\\widetilde{\\theta}_{j}}(\\Delta-\\Delta)-2a\\displaystyle\\sum_{k\\in\\mathcal{C}\\setminus\\{j\\}}\\lambda_{k}^{\\star}(\\widetilde{\\theta})}\\\\ &{\\quad\\quad\\quad\\quad\\quad<0\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we have used Lemma 5 at the third line. This implies ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\widetilde{\\theta}_{j}=\\underline{{\\theta}}\\qquad\\mathrm{for\\;any}\\;j\\in\\mathcal{C}\\;.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To see why, assume by contradiction that there exists $j\\in\\mathcal{C}$ such that $s_{j}=(1,\\widetilde{\\theta}_{j})$ with $\\widetilde{\\theta}_{j}>\\underline{{{\\theta}}}$ Then for any $h\\in(0,\\widetilde{\\theta}_{j}-\\underline{{\\theta}}]$ ,by (61): ", "page_idx": 21}, {"type": "equation", "text": "$$\nv_{j}((1,\\widetilde{\\theta}_{j}-h),\\mathbf{s}_{-j})=v_{j}((1,\\widetilde{\\theta}_{j}),\\mathbf{s}_{-j})-\\int_{\\widetilde{\\theta}_{j}-h}^{\\widetilde{\\theta}_{j}}\\frac{\\partial v_{j}((1,t),\\mathbf{s}_{k})}{\\partial t}d t>v_{j}((1,\\widetilde{\\theta}_{j}),\\mathbf{s}_{k})\\;,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which contradicts s being a Nash equilibrium. ", "page_idx": 21}, {"type": "text", "text": "Combining Lemma 5 and Lemma 6 gives for any $(j,k)\\in\\mathcal{C}^{2}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\theta_{j}=\\theta_{k}\\ .\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Recall that by $\\mathbf{H}3$ $\\theta_{m}=\\theta_{n}$ if and only if $m\\,=\\,n$ , so (63) implies $|{\\mathcal{C}}|=1$ . This in turn implies $|\\beta|=1$ . Indeed, assume by contradiction $|{\\mathcal{C}}|=1$ and $|\\beta|\\geqslant2$ . Denote $r\\in\\mathcal{B}$ the only contributing agent. They are asked $n_{r}^{\\star}(\\widetilde{\\pmb{\\theta}})=\\overline{{n}}_{r}(\\widetilde{\\pmb{\\theta}})=\\underline{{n}}$ . Moreover by (19) $\\bar{N}=|\\beta|^{1/1+\\gamma}(\\underline{{n}}+1)-1>\\underline{{n}}$ by definition of the contribution scheme (12), there exists $k\\in B\\setminus\\{r\\}$ such that $n_{k}^{\\star}(\\widetilde{\\pmb{\\theta}})>0$ . This contradicts $|\\mathcal{C}|=1$ ", "page_idx": 21}, {"type": "text", "text": "Wenowshowthat $B\\;=\\;\\{J\\}$ .By contradiction, assume $B\\,=\\,\\{j\\}$ with $j~<J$ . In particular, $s_{J}=(0,\\dag)$ Consider adeviation $s_{J}^{\\prime}\\,=\\,(1,\\widetilde{\\theta}_{J})$ with $\\widetilde{\\theta}_{J}\\in\\Theta$ $B=\\{j,J\\}$ under $(s_{J}^{\\prime},{\\bf s}_{-J})$ .By Lemma 6, $\\widetilde{\\theta}_{j}=\\widetilde{\\theta}_{J}=\\theta$ so(58)rewrites: ", "page_idx": 21}, {"type": "equation", "text": "$$\nv_{k}((1,\\widetilde{\\theta}_{k}),\\mathbf{s}_{-k})=\\underline{{u}}_{k}+2a(\\theta_{k}-\\vartheta(\\mathsf{B},\\mathbf{n}^{\\star}(\\widetilde{\\theta}))\\;,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for any $k\\in\\{j,J\\}$ . Since $\\theta_{j}<\\vartheta\\big(\\mathsf{B},\\mathbf{n}^{\\star}(\\widetilde{\\pmb{\\theta}})\\big)<\\theta_{J}$ by $\\mathbf{H}3$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v_{J}((1,\\widetilde{\\theta}_{J}),{\\bf s}_{-J})>u_{J}=v_{J}((0,\\dagger),{\\bf s}_{-J})\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which contradicts s begin a Nash equilibrium. Hence, $B=\\{J\\}$ . This concludes the proof. ", "page_idx": 21}, {"type": "text", "text": "Lemma 3. There exists $j\\in[J]$ suchthat $-t_{j}({\\widetilde{\\pmb{\\theta}}})>0$ ", "page_idx": 21}, {"type": "text", "text": "Proof. Let $(\\mathsf{B},\\widetilde{\\pmb{\\theta}})\\in\\left\\{0,1\\right\\}^{J}\\times\\Theta^{N}$ be an equilibrium of the game induced by $\\Gamma^{\\mathrm{VCG}}$ .Since theVCG mechanism is strategyproof in dominant strategy, 0 = 0. For any $j\\in[J]$ , the VCG payment is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{t_{j}(\\widetilde{\\theta})=\\displaystyle\\sum_{k\\neq j}u_{k}((0,\\mathbf{1}_{-j}),\\mathbf{n}^{\\star}(\\theta))-\\displaystyle\\sum_{k\\neq j}u_{k}(\\mathbf{1},\\mathbf{n}^{\\star}(\\theta))}\\\\ &{\\qquad=\\underbrace{W((0,\\mathbf{1}_{-j}),\\mathbf{n}^{\\star}(\\theta))-W(\\mathbf{1},\\mathbf{n}^{\\star}(\\theta))}_{\\mathrm{(I)}}-\\underbrace{[u_{j}((0,\\mathbf{1}_{-j}),\\mathbf{n}^{\\star}(\\theta))-u_{j}(\\mathbf{1},\\mathbf{n}^{\\star}(\\theta))]}_{\\mathrm{(II)}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Consider $j\\in\\{1,\\ldots,L^{\\star}\\}$ . By the contribution scheme Corollary 1, ", "page_idx": 22}, {"type": "equation", "text": "$$\nu_{j}(\\mathbf{1},\\mathbf{n}^{\\star}(\\pmb\\theta))=-a(\\mathcal{R}_{0}^{\\star}+\\varepsilon(\\mathbf{1},\\mathbf{n}^{\\star}(\\pmb\\theta)))-c n_{j}^{\\star}(\\pmb\\theta)=\\underline{{u}}_{j}=u_{j}((0,\\mathbf{1}_{-j}),\\mathbf{n}^{\\star}(\\pmb\\theta))\\;,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we used in the third inequality that $n_{j}^{\\star}(\\pmb\\theta)=\\overline{{n}}_{j}(\\mathsf{B},\\mathbf{n}^{\\star}(\\pmb\\theta))$ . Thus, $\\left(\\mathbf{II}\\right)=0$ Moreover by Theorem 1, $W((0,{\\bf1}_{-j}),{\\bf n}^{\\star}(\\pmb\\theta))<W({\\bf1},{\\bf n}^{\\star}(\\pmb\\theta))$ $(\\mathbf{I})<0$ . Thus, by (65) we have $t_{j}({\\widetilde{\\pmb{\\theta}}})<0$ \uff1a\u53e3 ", "page_idx": 22}, {"type": "text", "text": "Theorem 3. Assume ${\\cal H}l,{\\cal H}2,{\\cal H}3,$ $H4$ and $H5$ $\\mathsf{B}^{\\star}=(1,\\ldots,1)^{\\mathrm{T}}$ isaNashequilibriumunder $\\hat{\\Gamma}$ with probability $1-\\delta$ ", "page_idx": 22}, {"type": "text", "text": "Proof. Let $\\mathsf{B}^{\\star}=(1,\\ldots,1)^{\\mathrm{T}}$ , so the aggregator has at her disposal $\\widehat{\\theta}_{1},\\ldots,\\widehat{\\theta}_{J}$ ", "page_idx": 22}, {"type": "text", "text": "We first focus on the event under which ${\\widehat{\\theta\\,}}_{j}$ correctly approximates $\\theta_{j}$ for any $j\\ \\leqslant J$ .Define $\\mathsf E_{j}(\\delta)=\\{\\omega\\in\\Omega:\\,|\\widehat\\theta_{j}(\\omega)-\\theta_{j}|\\leqslant\\eta_{\\delta}\\}$ and $\\mathsf E(\\delta)=\\bigcap_{j\\leqslant J}\\mathsf E_{j}(\\delta)$ We have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(\\mathsf{E}(\\delta/J))\\geqslant1-\\delta\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "because $\\begin{array}{r}{\\mathbb{P}(\\mathsf{E}(\\delta))\\;=\\;\\mathbb{P}(\\cap_{j\\leqslant J}\\mathsf{E}_{j}(\\delta))\\;=\\;1-\\mathbb{P}(\\cup_{j\\leqslant J}\\mathsf{E}_{j}(\\delta))\\;\\geqslant\\;1-\\sum_{j\\leqslant J}\\mathbb{P}(\\mathsf{E}_{j}(\\delta))\\;=\\;1-\\,J\\delta,}\\end{array}$ and setting $\\delta^{\\prime}\\,=\\,J\\delta$ leads to Equation (66). In what follows, we assume that $E(\\delta/J)$ is true so $|\\widehat{\\theta}_{j}-\\theta_{j}|\\leqslant\\eta_{\\delta/J}(\\underline{{q}})$ for any $j\\in[J]$ We denote by $\\mathcal{C}=\\{j\\in[J]:\\,m_{j}(\\widehat{\\pmb{\\theta}})=n_{j}^{\\star}(\\widehat{\\pmb{\\theta}}+\\pmb{\\eta}_{j})>0\\}$ the set of contributors, and tolighten notations we write ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\vartheta(\\widehat{\\theta})=\\bar{N}^{-1}\\mathbf{n}^{\\star}(\\widehat{\\theta}+\\eta_{j})^{\\mathrm{T}}\\theta=\\lambda^{\\star}(\\widehat{\\theta}+\\eta_{j})^{\\mathrm{T}}\\theta}\\\\ {\\mathrm{and}\\quad}&{\\widehat{\\vartheta}_{j}(\\widehat{\\theta})=\\bar{N}^{-1}\\mathbf{n}^{\\star}(\\widehat{\\theta}+\\eta_{j})^{\\mathrm{T}}(\\widehat{\\theta}+\\eta_{j})=\\lambda^{\\star}(\\widehat{\\theta}+\\eta_{j})^{\\mathrm{T}}(\\widehat{\\theta}+\\eta_{j})\\ ,}\\\\ {\\mathrm{where}\\quad\\eta_{j}=\\eta_{\\delta/J}(\\underline{{q}})\\mathbf{1}-2\\delta_{j}\\eta_{\\delta/J}(\\underline{{q}}),\\quad\\mathrm{with}\\quad\\delta_{j}=(0,\\ldots,0,1,0,\\ldots,0)\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for any $j\\in[J]$ . Note that $\\vartheta({\\widehat{\\theta}})$ is the actual weighted type within the coalition, whereas $\\hat{\\vartheta}(\\widehat{\\pmb\\theta})$ is the weighted type estimated by the aggregator. Consider first a contributor $j\\,\\in\\,{\\mathcal{C}}$ who is asked $n_{j}^{\\star}(\\widehat{\\pmb{\\theta}}+\\bar{\\pmb{\\eta}}_{j})>\\dot{0}$ samples. Their payoff under $\\mathsf{B}^{\\star}$ is ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{v}_{j}(1,\\mathbf{1}_{-j})=-a(\\mathcal{R}_{0}^{*}+\\varepsilon(\\vartheta(\\widehat{\\theta}),N))-c n_{j}^{*}(\\widehat{\\theta}+\\eta_{j})}\\\\ &{\\quad\\quad\\quad=-a(\\mathcal{R}_{0}^{*}+\\varepsilon(\\vartheta(\\widehat{\\theta}),N))-c\\Big[\\underline{{n}}-\\frac{a}{c}\\Big(\\varepsilon(\\widehat{\\vartheta}_{j}(\\widehat{\\theta}),N)-\\varepsilon(\\widehat{\\theta}_{j}-\\eta_{\\delta/J}(\\underline{{q}}),\\underline{{n}})\\Big)\\Big]}\\\\ &{\\quad\\quad\\quad=-a(\\mathcal{R}_{0}^{*}+\\varepsilon(\\theta_{j},\\underline{{n}}))-c\\underline{{n}}}\\\\ &{\\quad\\quad\\quad\\quad+a\\Big[\\Big(\\varepsilon(\\theta_{j},\\underline{{n}})-\\varepsilon(\\widehat{\\theta}_{j}-\\eta_{\\delta/J}(\\underline{{q}}))\\Big)-\\Big(\\varepsilon(\\vartheta(\\widehat{\\theta}),N)-\\varepsilon(\\widehat{\\vartheta}_{j}(\\widehat{\\theta}),N)\\Big)\\Big]}\\\\ &{\\quad\\quad\\quad=\\underline{{u}}_{j}+2a\\Big[\\Big(\\theta_{j}-(\\widehat{\\theta}_{j}-\\eta_{\\delta/J}(\\underline{{q}}))\\Big)-\\Big(\\vartheta(\\widehat{\\theta})-\\widehat{\\vartheta}_{j}(\\theta)\\Big)\\Big]}\\\\ &{\\quad\\quad\\quad=\\hat{v}_{j}(0,\\mathbf{1}_{-j})+2a(1-\\lambda_{j}^{*}(\\widehat{\\theta}+\\eta_{j}))(\\theta_{j}-(\\widehat{\\theta}_{j}-\\eta_{\\delta/J}(\\underline{{q}})))}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad-2a\\sum_{j}\\lambda_{j}^{*}(\\widehat{\\theta}+\\eta_{j})(\\theta_{\\ell}-(\\widehat{\\theta}_{\\ell}+\\eta_{\\delta/J}(\\underline{{q}})))\\ ,}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and since $E(\\delta/J)$ is true, $\\theta_{j}\\geqslant\\widehat{\\theta}_{j}-\\eta_{\\delta/J}(\\underline{{q}})$ and $\\theta_{\\ell}\\leqslant\\widehat{\\theta}_{\\ell}+\\eta_{\\delta/J}(\\underline{{q}})$ for any $\\ell\\in\\mathcal{C}\\setminus\\{j\\}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{v}_{j}(1,{\\bf1}_{-j})\\geqslant\\hat{v}_{j}(0,{\\bf1}_{-j})\\;.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Second, consider $j\\not\\in{\\mathcal{C}}$ who is asked $\\underline{{q}}\\leqslant\\underline{{n}}-2(a/c)(\\bar{\\theta}-\\underline{{\\theta}})$ samples, and denote by $r\\in[J]$ the agent such that $\\theta_{r}=\\operatorname*{max}_{k\\in C}\\theta_{k}$ . Observe that $\\underline{{q}}\\leqslant n_{r}^{\\star}(\\widehat{\\pmb{\\theta}}+\\eta_{r})$ ,sO ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{v}_{j}(1,1_{-j})=-a(\\mathcal{R}_{0}^{\\star}+\\varepsilon(\\vartheta(\\widehat{\\pmb{\\theta}}),N))-c\\underline{{q}}}\\\\ &{\\quad\\quad\\quad\\quad\\geqslant-a(\\mathcal{R}_{0}^{\\star}+\\varepsilon(\\vartheta(\\widehat{\\pmb{\\theta}}),N))-c n_{r}^{\\star}(\\widehat{\\pmb{\\theta}}+\\eta_{r})}\\\\ &{\\quad\\quad\\quad\\geqslant\\underline{{u}}_{r}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last inequalitycomes from $r\\in\\mathcal{C}$ and (67). Since $j\\not\\in{\\mathcal{C}}$ we have $\\theta_{j}\\geqslant\\widehat{\\theta}_{j}-\\eta_{\\delta/J}(\\underline{{q}})\\geqslant$ $\\widehat{\\theta}_{r}+\\eta_{\\delta/J}(\\underline{{q}})\\geqslant\\theta_{r}$ $\\underline{{u}}_{r}\\geqslant\\underline{{u}}_{j}$ ,andit folows that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{v}_{j}(1,{\\bf1}_{-j})\\geqslant\\underline{{u}}_{j}=\\hat{v}_{j}(0,{\\bf1}_{-j})\\;.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proposition 2. Assume $H l$ \uff0c $_{H2}$ and $H6$ For any $j\\in[J]$ the estimator ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\widehat{\\theta}_{0,j}^{\\mathrm{ERM}}=\\operatorname*{sup}_{g\\in\\mathcal{G}}\\left\\vert\\widehat{\\mathcal{R}}_{j}(g)-\\widehat{\\mathcal{R}}_{0}(g)\\right\\vert\\,,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "satisfies $H5$ with ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\eta_{\\delta}(q)=\\alpha_{\\delta/4}\\big[(q+1)^{-\\gamma}+(q^{\\prime}+1)^{-\\gamma}\\big]+2\\beta\\;.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Let $j\\in[J]$ and $g\\in{\\mathcal{G}}$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\theta}_{0,j}^{\\mathtt{E R M}}=\\underset{g\\in\\mathcal{G}}{\\operatorname*{sup}}\\Big|\\widehat{\\mathcal{R}}_{j}(g)-\\widehat{\\mathcal{R}}_{0}(g)\\Big|}\\\\ &{\\qquad\\leqslant\\underset{g\\in\\mathcal{G}}{\\operatorname*{sup}}\\Big|\\widehat{\\mathcal{R}}_{j}(g)-\\mathcal{R}_{j}(g)\\Big|+\\underset{g\\in\\mathcal{G}}{\\operatorname*{sup}}|\\mathcal{R}_{j}(g)-\\mathcal{R}_{0}(g)|+\\underset{g\\in\\mathcal{G}}{\\operatorname*{sup}}\\Big|\\widehat{\\mathcal{R}}_{0}(g)-\\mathcal{R}_{0}(g)\\Big|}\\\\ &{\\qquad\\leqslant\\alpha_{\\delta/4}\\big[(q+1)^{-\\gamma}+(q^{\\prime}+1)^{-\\gamma}\\big]+2\\beta+\\theta_{j}\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "with probability $1-\\delta/2$ by $\\mathbf{H}2$ , H1, H6, and a union bound. Similarly, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta_{j}=\\underset{g\\in\\mathcal{G}}{\\operatorname*{sup}}|\\mathcal{R}_{j}(g)-\\mathcal{R}_{0}(g)|}\\\\ &{\\quad\\leqslant\\underset{g\\in\\mathcal{G}}{\\operatorname*{sup}}\\Big|\\mathcal{R}_{j}(g)-\\widehat{\\mathcal{R}}_{j}(g)\\Big|+\\underset{g\\in\\mathcal{G}}{\\operatorname*{sup}}\\Big|\\widehat{\\mathcal{R}}_{j}(g)-\\widehat{\\mathcal{R}}_{0}(g)\\Big|+\\underset{g\\in\\mathcal{G}}{\\operatorname*{sup}}\\Big|\\widehat{\\mathcal{R}}_{0}(g)-\\mathcal{R}_{0}(g)\\Big|}\\\\ &{\\quad\\leqslant\\alpha_{\\delta/4}\\big[(q+1)^{-\\gamma}+(q^{\\prime}+1)^{-\\gamma}\\big]+2\\beta+\\widehat{\\theta}_{0,j}^{\\mathtt{E R M}}\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "with probability $1-\\delta/2$ . Combining (68) and (70) along with an union bound yields ", "page_idx": 23}, {"type": "equation", "text": "$$\n|\\widehat{\\theta}_{0,j}^{\\mathtt{E R M}}-\\theta_{j}|\\leqslant\\alpha_{\\delta/4}\\big[(q+1)^{-\\gamma}+(q^{\\prime}+1)^{-\\gamma}\\big]+2\\beta\\;,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "with probability $1-\\delta$ ", "page_idx": 23}, {"type": "text", "text": "Example 4. Assume $H l$ $_{H2}$ $H6$ and $H7$ ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "(i) Denoting $\\begin{array}{r}{\\widehat{\\mathcal{R}}_{j^{-}}(g)=n_{j}^{-1}\\sum_{i=1}^{n_{j}}\\ell_{0,1}(g(X_{i}^{j}),-Y_{i}^{j})}\\end{array}$ we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\widehat{\\theta}_{0,j}^{\\mathtt{E R M}}=1-\\operatorname*{inf}_{g\\in\\mathcal{G}}\\left\\{\\widehat{\\mathcal{R}}_{0}(g)+\\widehat{\\mathcal{R}}_{j^{-}}(g)\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "(i) In $H\\!I$ assume $\\alpha_{\\delta}=\\ln(1/\\delta)^{1/2}$ $\\beta=2\\mathsf{R A D}(\\mathcal{G})$ and $\\gamma=1$ [Bousquet et al., 2003]. With $\\widehat{\\theta}_{0,j}^{\\tt E R M}$ defined in Proposition 2, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\eta_{\\delta/J}(q)=\\ln(4J/\\delta)^{1/2}[(1+q)^{-\\gamma}+(1+q^{\\prime})^{-\\gamma}]+2\\mathrm{Rad}(\\mathcal{G})\\;.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. (i) Let $j\\in\\mathcal B$ . Observe that under $\\mathbf{H}7$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{R}}_{j}(-g)=n_{j}^{-1}\\sum_{i=1}^{n_{j}}\\mathbb{1}\\{-g(X_{i}^{j})Y_{i}^{j}<0\\}=n_{j}^{-1}\\sum_{i=1}^{n_{j}}(1-\\mathbb{1}\\{g(X_{i}^{j})Y_{i}^{j}<0\\})=1-\\widehat{\\mathcal{R}}_{j}(g)\\;.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since the hypothesis class $\\mathcal{G}$ is symmetric, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\theta}_{j}^{\\mathtt{E R M}}=\\underset{g\\in\\mathcal{G}}{\\operatorname*{sup}}\\Big|\\widehat{\\mathcal{R}}_{j}(g)-\\widehat{\\mathcal{R}}_{0}(g)\\Big|=\\underset{g\\in\\mathcal{G}}{\\operatorname*{sup}}\\Big(\\widehat{\\mathcal{R}}_{j}(g)-\\widehat{\\mathcal{R}}_{0}(g)\\Big)}\\\\ &{\\qquad=\\underset{g\\in\\mathcal{G}}{\\operatorname*{sup}}\\Big(1-\\Big(\\widehat{\\mathcal{R}}_{j}(-g)+\\widehat{\\mathcal{R}}_{0}(g)\\Big)\\Big)=1-\\underset{g\\in\\mathcal{G}}{\\operatorname*{inf}}\\Big(\\widehat{\\mathcal{R}}_{j^{-}}(g)+\\widehat{\\mathcal{R}}_{0}(g)\\Big)\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "(i) Let $j\\in[J]$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta_{\\delta/J}(q)=\\alpha_{\\delta/4J}[(q+1)^{-\\gamma}+(1+q^{\\prime})^{-\\gamma}]+2\\beta}\\\\ &{\\qquad\\qquad=\\ln(4J/\\delta)^{1/2}[(1+q)^{-\\gamma}+(1+q^{\\prime})^{-\\gamma}]+2\\mathrm{Rad}(\\mathcal{G})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: the claimed contributions are supported by proven theorems. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: all the made assumptions are clearly highlighted and discussed. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Assumptions are made clear and given for each theorem individually. Full proofs are given in the Appendix. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: no experimental result in the paper ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: no experimental result in the paper Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https : //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: no experimental result in the paper Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: no experimental result in the paper ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: no experimental result in the paper ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: no data used for this work ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: no societal impact of the work performed ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 27}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: no data Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: no existing assets Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: no new asset Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset isused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: no crowdsourcing nor research with human subjects ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with humansubjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: no crowdsourcing nor research with human subjects ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]