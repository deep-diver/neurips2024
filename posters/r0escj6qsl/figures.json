[{"figure_path": "r0eSCJ6qsL/figures/figures_0_1.jpg", "caption": "Figure 1: Example images generated by our efficient text-to-image generation model based on an asymmetric architecture. It generates photo-realistic images while following long prompts.", "description": "This figure showcases example images generated by the AsCAN model.  The images demonstrate the model's capability to produce photorealistic results from detailed and lengthy text prompts, highlighting its effectiveness in text-to-image generation.", "section": "Abstract"}, {"figure_path": "r0eSCJ6qsL/figures/figures_3_1.jpg", "caption": "Figure 2: Example AsCAN architectures for Image Classification & Text-to-Image Generation. (a): The architecture for the image classification and details of the convolutional (C) and transformer blocks (T). AsCAN includes Stem (consisting of convolutional layers) and four stages followed by pooling and classifier. (b): The UNet architecture for the image generation. The Down blocks (the first three blocks starting from left) have the reverted reflection as the Up blocks (the first three blocks starting from right). (c): The details for C and T used in UNet. For the T that performs the cross attention between latent image features and textural embedding, the Q matrix comes from the textural embedding. Note that, compared to image classification, the C and T blocks for image generation only adds extra components to incorporate the input time-step and textual embeddings.", "description": "This figure showcases two different AsCAN architectures. The first is for image classification and illustrates the arrangement of convolutional (C) and transformer (T) blocks in a stem and four stages followed by a pooling and classifier layer. The second is designed for text-to-image generation, which employs a UNet architecture.  This variation highlights the use of down and up blocks (mirrored structures), illustrating how the convolutional and transformer modules adapt to handle text embeddings and time-step components, making it different from the classification version.", "section": "3.1 Architecture Design: Asymmetric Convolution-Attention Networks (ASCAN)"}, {"figure_path": "r0eSCJ6qsL/figures/figures_6_1.jpg", "caption": "Figure 3: Top-1 Accuracy vs Inference Latency on ImageNet-1K Classification. We plot the latency measured as images inferred per second on a single V100 GPU (Left)/A100 GPU (Right) with batch-size 16 with 224 \u00d7 224 resolution. The plot compares state-of-the-art models (convolutional, transformer, hybrid architectures) against the proposed AsCAN architecture. The area of each circle is proportional to the model size. Our model consistently achieves better accuracy vs latency trade-offs. While some models regress between two hardware (e.g., MaxViT-S vs SMT-B), our model consistently achieves better accuracy vs latency trade-offs. We report additional baselines along with multiply-add operations count and different batch sizes in Appendix Tab. 7.", "description": "This figure compares the performance of AsCAN against other state-of-the-art models for ImageNet-1K classification task.  It shows that AsCAN achieves a superior trade-off between top-1 accuracy and inference latency (throughput) on both NVIDIA V100 and A100 GPUs. The size of each data point is proportional to the model's size, providing a visual representation of the model's complexity relative to its performance.", "section": "4 Experiments"}, {"figure_path": "r0eSCJ6qsL/figures/figures_8_1.jpg", "caption": "Figure 1: Example images generated by our efficient text-to-image generation model based on an asymmetric architecture. It generates photo-realistic images while following long prompts.", "description": "This figure showcases example images produced by the authors' text-to-image generation model. The model is designed for efficiency and utilizes an asymmetric architecture (combining convolutional and transformer blocks).  The examples highlight the model's ability to create photorealistic images from lengthy and detailed text prompts.", "section": "Abstract"}, {"figure_path": "r0eSCJ6qsL/figures/figures_9_1.jpg", "caption": "Figure 3: Top-1 Accuracy vs Inference Latency on ImageNet-1K Classification. We plot the latency measured as images inferred per second on a single V100 GPU (Left)/A100 GPU (Right) with batch-size 16 with 224 \u00d7 224 resolution. The plot compares state-of-the-art models (convolutional, transformer, hybrid architectures) against the proposed AsCAN architecture. The area of each circle is proportional to the model size. Our model consistently achieves better accuracy vs latency trade-offs. While some models regress between two hardware (e.g., MaxViT-S vs SMT-B), our model consistently achieves better accuracy vs latency trade-offs. We report additional baselines along with multiply-add operations count and different batch sizes in Appendix Tab. 7.", "description": "This figure presents a comparison of the accuracy and inference latency of various state-of-the-art image classification models on the ImageNet-1K dataset. The models include convolutional neural networks (CNNs), vision transformers (ViTs), and hybrid architectures. The proposed AsCAN model demonstrates superior performance in terms of accuracy vs. latency trade-off.", "section": "4 Experiments"}, {"figure_path": "r0eSCJ6qsL/figures/figures_18_1.jpg", "caption": "Figure 6: Curation Pipeline for the T2I Dataset. We show various stages involved in the creation of our T2I dataset.", "description": "The figure illustrates the detailed steps in creating the text-to-image (T2I) dataset used in the paper.  It starts with collecting data from image and video sources. Then, multiple filtering stages are applied to enhance data quality and remove unwanted content.  These stages include checks for resolution and aspect ratio, NSFW content, blurriness, watermarks, and whether the image includes text, violence, or other inappropriate elements.  After filtering, the images undergo annotation, adding detailed descriptions using an object detection model, captioning model, and OCR (Optical Character Recognition). The final dataset consists of image-caption pairs.", "section": "A.4 T2I Dataset Details"}, {"figure_path": "r0eSCJ6qsL/figures/figures_20_1.jpg", "caption": "Figure 3: Top-1 Accuracy vs Inference Latency on ImageNet-1K Classification. We plot the latency measured as images inferred per second on a single V100 GPU (Left)/A100 GPU (Right) with batch-size 16 with 224 \u00d7 224 resolution. The plot compares state-of-the-art models (convolutional, transformer, hybrid architectures) against the proposed AsCAN architecture. The area of each circle is proportional to the model size. Our model consistently achieves better accuracy vs latency trade-offs. While some models regress between two hardware (e.g., MaxViT-S vs SMT-B), our model consistently achieves better accuracy vs latency trade-offs. We report additional baselines along with multiply-add operations count and different batch sizes in Appendix Tab. 7.", "description": "This figure compares the performance of AsCAN against other state-of-the-art image classification models using two metrics: top-1 accuracy and inference latency.  It shows that AsCAN consistently achieves a better balance between high accuracy and fast inference speed across different hardware (V100 and A100 GPUs). The size of each point is proportional to the model's size, offering a visual representation of the performance-to-complexity trade-off.", "section": "4 Experiments"}, {"figure_path": "r0eSCJ6qsL/figures/figures_20_2.jpg", "caption": "Figure 8: Comparing T2I 256 \u00d7 256 Resolution Training with and w/o Pre-training. We report the FID score on Set-B-10K for 256 \u00d7 256 resolution training, with and without pre-training from the ImageNet-1k T2I model.", "description": "The figure shows the FID scores for 256x256 resolution training of a text-to-image model with and without pre-training on ImageNet-1K.  The results indicate that pre-training improves the model's performance, as measured by FID, especially during the early training iterations.", "section": "A.6 Hyper-parameter Setup for Text-to-Image Generation"}, {"figure_path": "r0eSCJ6qsL/figures/figures_22_1.jpg", "caption": "Figure 1: Example images generated by our efficient text-to-image generation model based on an asymmetric architecture. It generates photo-realistic images while following long prompts.", "description": "This figure showcases the capabilities of the AsCAN-based text-to-image generation model.  It demonstrates the model's ability to produce photorealistic images from detailed and lengthy text prompts, highlighting the model's efficiency and effectiveness in handling complex textual instructions.", "section": "Abstract"}, {"figure_path": "r0eSCJ6qsL/figures/figures_22_2.jpg", "caption": "Figure 1: Example images generated by our efficient text-to-image generation model based on an asymmetric architecture. It generates photo-realistic images while following long prompts.", "description": "This figure showcases example images produced by the AsCAN model, highlighting its ability to create photorealistic images from detailed text descriptions, even lengthy ones.", "section": "Abstract"}, {"figure_path": "r0eSCJ6qsL/figures/figures_26_1.jpg", "caption": "Figure 1: Example images generated by our efficient text-to-image generation model based on an asymmetric architecture. It generates photo-realistic images while following long prompts.", "description": "This figure showcases example images generated by the AsCAN text-to-image model. The model is designed to efficiently produce photorealistic images from detailed, lengthy text descriptions (prompts).  The images demonstrate the model's capacity to capture intricate details and adhere closely to the input prompt.", "section": "Abstract"}, {"figure_path": "r0eSCJ6qsL/figures/figures_26_2.jpg", "caption": "Figure 1: Example images generated by our efficient text-to-image generation model based on an asymmetric architecture. It generates photo-realistic images while following long prompts.", "description": "This figure showcases the capabilities of the AsCAN model in generating photorealistic images from detailed text prompts.  The examples illustrate the model's ability to create images of diverse subjects and scenes that are visually accurate and closely match the textual description. This demonstrates its strength in text-to-image generation.", "section": "Abstract"}, {"figure_path": "r0eSCJ6qsL/figures/figures_26_3.jpg", "caption": "Figure 1: Example images generated by our efficient text-to-image generation model based on an asymmetric architecture. It generates photo-realistic images while following long prompts.", "description": "This figure showcases the capabilities of the AsCAN-based text-to-image model.  It highlights the model's ability to generate photorealistic images from detailed and lengthy text prompts, demonstrating the effectiveness of the asymmetric architecture in achieving high-quality image generation.", "section": "Abstract"}, {"figure_path": "r0eSCJ6qsL/figures/figures_26_4.jpg", "caption": "Figure 2: Example AsCAN architectures for Image Classification & Text-to-Image Generation. (a): The architecture for the image classification and details of the convolutional (C) and transformer blocks (T). AsCAN includes Stem (consisting of convolutional layers) and four stages followed by pooling and classifier. (b): The UNet architecture for the image generation. The Down blocks (the first three blocks starting from left) have the reverted reflection as the Up blocks (the first three blocks starting from right). (c): The details for C and T used in UNet. For the T that performs the cross-attention between latent image features and textural embedding, the Q matrix comes from the textural embedding. Note that, compared to image classification, the C and T blocks for image generation only adds extra components to incorporate the input time-step and textual embeddings.", "description": "This figure illustrates the AsCAN architecture used for both image classification and text-to-image generation.  Part (a) shows the basic convolutional and transformer blocks, as well as the four-stage architecture for image classification.  Part (b) shows how this is adapted into a U-Net architecture for image generation.  Finally, part (c) details the specific blocks used within the U-Net, highlighting the differences from the classification architecture.", "section": "3.1 Architecture Design: Asymmetric Convolution-Attention Networks (ASCAN)"}, {"figure_path": "r0eSCJ6qsL/figures/figures_27_1.jpg", "caption": "Figure 1: Example images generated by our efficient text-to-image generation model based on an asymmetric architecture. It generates photo-realistic images while following long prompts.", "description": "This figure showcases example images generated by the AsCAN model, highlighting its ability to produce photorealistic images from detailed text prompts.  The image generation is efficient due to the model's asymmetric architecture, which is a key contribution of the paper.", "section": "Abstract"}]