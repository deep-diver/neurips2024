[{"type": "text", "text": "Distributional regression: CRPS-error bounds for model fitting, model selection and convex aggregation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Cl\u00e9ment Dombry\u2217 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ahmed Zaoui ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Universit\u00e9 de Franche-Comt\u00e9, CNRS, LmB (UMR 6623), F-25000 Besan\u00e7on, France. clement.dombry@univ-fcomte.fr ", "page_idx": 0}, {"type": "text", "text": "Universit\u00e9 de Franche-Comt\u00e9, CNRS, LmB (UMR 6623), F-25000 Besan\u00e7on, France. ahmed.zaoui@univ-fcomte.fr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Distributional regression aims at estimating the conditional distribution of a target variable given explanatory co-variates. It is a crucial tool for forecasting when a precise uncertainty quantification is required. A popular methodology consists in fitting a parametric model via empirical risk minimization where the risk is measured by the Continuous Rank Probability Score (CRPS). For independent and identically distributed observations, we provide a concentration result for the estimation error and an upper bound for its expectation. Furthermore, we consider model selection performed by minimization of the validation error and provide a concentration bound for the regret. A similar result is proved for convex aggregation of models. Finally, we show that our results may be applied to various models such as Ensemble Model Output Statistics (EMOS), distributional regression networks, distributional nearest neighbors or distributional random forests and we illustrate our findings on two data sets (QSAR aquatic toxicity and Airfoil self-noise). ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Motivation and related literature. We consider in this paper the distributional regression problem, where we want to estimate the conditional distribution of a target random variable $Y$ given explanatory co-variates $X$ . We assume $(X,Y)\\in\\mathbb{R}^{d}\\times\\mathbb{R}$ and we let ", "page_idx": 0}, {"type": "equation", "text": "$$\nF_{x}^{*}(y)=\\mathbb{P}\\left(Y\\leq y\\,|\\,X=x\\right).\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "be the conditional cumulative distribution functions (c.d.f.). Estimation is built on a training sample $\\mathcal{D}_{n}=\\{(X_{i},Y_{i}),\\,1\\leq i\\leq n\\}$ of independent identically distributed (i.i.d.) copies of $(X,Y)$ . ", "page_idx": 0}, {"type": "text", "text": "Let us emphasize that distributional regression is much more challenging than standard regression where only a point prediction for $Y$ given $X=x$ is provided which typically reduces to an estimation of the conditional expectation. The conditional distribution provides a full account for the variability of $Y$ given $X=x$ and distributional regression is therefore a crucial tool for forecasting when a precise uncertainty quantification is required. ", "page_idx": 0}, {"type": "text", "text": "Distributional regression is of relevance in various applied fields. To name a few, let us mention statistical post-processing of weather forecast (Matheson and Winkler, 1976; Gneiting et al., 2005), forecasting of wind gusts (Baran and Lerch, 2015), of solar irradiance (Schulz et al., 2021), of ICU length stays during COVID-19 pandemy (Henzi et al., 2021a), of breast cancer ODX score in oncology (Al Masry et al., 2024)... ", "page_idx": 0}, {"type": "text", "text": "The methodology relies on probabilistic forecast where the forecaster produces a predictive distribution for the quantity of interest (Gneiting and Katzfuss, 2014). Fitting a distributional regression model typically involves the minimization of a proper scoring rule, the most popular one being the Continuous Ranked Probability Score (CRPS, Matheson and Winkler, 1976; Gneiting and Raftery, 2007). It compares the actual observation with the predictive distribution in a comprehensive manner. Many different models have been proposed for distributional regression among which: Analog similar to a nearest neighbor method (KNN, Toth, 1989), Ensemble Model Output Statistics similar to Gaussian heteroscedastic regression (EMOS, Gneiting and Raftery, 2007), Isotonic Distributional Regression (Henzi et al., 2021b) and more recent machine learning methods such as Distributional Regression Network (DRN, Rasp and Lerch, 2018) or Distributional Random Forest (DRF, \u00b4Cevid et al., 2022). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Despite these successful achievements in terms of methods and applications, a sound theory for distributional regression via scoring rule minimization is still missing. Recently, minimax rates of convergence with CRPS-error have been considered for distributional regression (Pic et al., 2023). The aim of this paper is to provide statistical learning guarantees for model fitting, model selection and convex aggregation based on CRPS minimization. ", "page_idx": 1}, {"type": "text", "text": "Model selection and convex aggregation are very important techniques in the field of statistics and machine learning and have been used very successfully for regression function estimation, see Tsybakov (2003); Bunea et al. (2007). The methods use two independent samples; the first sample, called training sample, is used to construct the initial estimators which are constituted as a dictionary (a collection of candidates). The second sample, called the validation sample, is used to aggregate them. Model selection enables the selection of the best candidate in the dictionary, while convex aggregation provides the optimal convex combination from these candidates. ", "page_idx": 1}, {"type": "text", "text": "Contributions. Our main results include a concentration bound for the theoretical risk when a parametric model is fitted via CRPS empirical risk minimization. We also consider model selection and model aggregation via CRPS minimization on a validation set and provide concentration bounds for the regret. Our results are first derived under sub-Gaussianity assumptions and then extended under weaker moment assumptions. We show that they apply to several popular models such as EMOS, DRN, KNN or DRF and provide a short illustration on two different datasets. ", "page_idx": 1}, {"type": "text", "text": "Structure of the paper. Section 2 first provides some background on distributional regression and proper scoring rules and then presents precisely the main methods and goals. The main results are stated in Section 3: an oracle inequality for the estimation error in model fitting (Theorem 1), and concentration bounds for the regret in model selection (Theorem 2) and model aggregation (Theorem 3). In Section 4, some specific models are introduced and the assumptions for our results to hold are checked. A short illustration on two different data set is provided in Section 5. Finally, an appendix contains all the proofs as well as some additional results. ", "page_idx": 1}, {"type": "text", "text": "2 Background on distributional regression and main goals ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Probabilistic forecast and its evaluation with scoring rules ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We first consider the simple setting of probabilistic forecast without co-variate where a future observation $Y$ is predicted by a probability distribution $F$ , called predictive distribution. Proper scoring rules are used in order to compare the predictive distribution $F$ and the materializing observation $y$ which are objects of different nature. Let $\\mathcal{P}_{0}\\subset\\mathcal{P}(\\mathbb{R})$ denote a subset of the set of all probability measures on $\\mathbb{R}$ , often identified with their c.d.f. A scoring rule2 on $\\mathcal{P}_{0}$ is a function $S\\colon\\mathcal{P}_{0}\\times\\mathbb{R}\\to[0,+\\infty)$ . The quantity $S(F,y)$ is interpreted as the error between the predictive distribution $F$ and the materializing observation $y$ . The mean error when $Y$ has \"true\" distribution $G$ is denoted by ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\bar{S}(F,G)=\\mathbb{E}_{Y\\sim G}[S(F,Y)].\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The following notion of proper and strictly proper scoring rule is central in the theory. ", "page_idx": 1}, {"type": "text", "text": "Definition 1. The scoring rule $S$ is said proper on $\\mathcal{P}_{0}$ when ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\bar{S}(F,G)\\ge\\bar{S}(G,G),\\quad f o r\\,a l l\\,F,G\\in\\mathcal{P}_{0}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "It is said strictly proper when equality in Eq. (2) implies $F=G$ . ", "page_idx": 1}, {"type": "text", "text": "Stated differently, the scoring rule $S$ is strictly proper on ${\\mathcal P}_{0}$ when ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\underset{F\\in\\mathcal{P}_{0}}{\\arg\\operatorname*{min}}\\,\\bar{S}(F,G)=\\{G\\},\\quad\\mathrm{for\\,all}\\;G\\in\\mathcal{P}_{0}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The interpretation is that, in order to minimize its mean error, the forecaster has to predict the \"true\" observation distribution $G$ . ", "page_idx": 2}, {"type": "text", "text": "In this paper, we consider the Continuous Ranked Probability Score (CRPS, Matheson and Winkler 1976). This scoring rule is defined by the formula ", "page_idx": 2}, {"type": "equation", "text": "$$\nS(F,y)=\\int_{\\mathbb{R}}\\left(\\mathbb{1}_{\\{y\\leq z\\}}-F(z)\\right)^{2}\\mathrm{d}z\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for all $F$ finite absolute moment. Here the subset ${\\mathcal{P}}_{0}\\subset{\\mathcal{P}}(\\mathbb{R})$ is the Wasserstein space ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal P_{1}(\\mathbb R)=\\left\\{F\\in\\mathcal P(\\mathbb R)\\colon m_{1}(F)=\\int_{\\mathbb R}|y|F(\\mathrm{d}y)<\\infty\\right\\}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "of probability measures on $\\mathbb{R}$ with finite first moment. One can easily check from this definition that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\bar{S}(F,G)=\\int_{\\mathbb{R}}G(z)\\left(1-G(z)\\right)\\mathrm{d}z+\\int_{\\mathbb{R}}\\left(F(z)-G(z)\\right)^{2}\\mathrm{d}z,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "which implies ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\bar{S}(F,G)-\\bar{S}(G,G)=\\int_{\\mathbb{R}}\\left(F(z)-G(z)\\right)^{2}\\mathrm{d}z.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This quantity is nonnegative and vanishes if and only if $F=G$ , ensuring that the CRPS is a strictly proper scoring rule. For discrete predictive distributions of the form $\\begin{array}{r}{\\check{F^{}}\\!=\\!\\sum_{i=1}^{n}w_{i}\\delta_{y_{i}}}\\end{array}$ (with $\\delta_{y}$ the Dirac mass at $y$ ), the CRPS can be computed simply (Gneiting and Rafter y, 2007) by ", "page_idx": 2}, {"type": "equation", "text": "$$\nS(F,y)=\\sum_{i=1}^{n}w_{i}|y_{i}-y|-\\frac{1}{2}\\sum_{i\\neq j}w_{i}w_{j}|y_{i}-y_{j}|.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "2.2 Model fitting, model selection and convex aggregation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We briefly present the methods and objectives that we address in this paper. ", "page_idx": 2}, {"type": "text", "text": "2.2.1 Theoretical risk in distributional regression ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In a regression framework, we observe a sample $\\boldsymbol{{\\mathcal D}}_{n}\\,=\\,\\{(\\boldsymbol{X}_{i},Y_{i})$ , $1\\,\\leq\\,i\\,\\leq\\,n\\}$ of independent copies of $(X,Y)\\in\\mathbb{R}^{d}\\times\\mathbb{R}$ . Distributional regression aims at estimating the conditional distribution $Y|X=x$ characterized by its c.d.f. $F_{x}^{*}$ defined in Eq. (1). The marginal distribution of $X$ is denoted by $P_{X}$ . The forecaster uses the training sample $\\mathcal{D}_{n}$ and some algorithm to build a functional estimator $\\hat{F}_{n}\\colon x\\mapsto\\hat{F}_{n,x}$ of the map $F^{*}\\colon x\\mapsto F_{x}^{*}$ . The accuracy of this estimator is here measured by its theoretical risk ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\hat{F}_{n})=\\mathbb{E}\\left[S(\\hat{F}_{n,X},Y)\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where expectation is taken with respect to the joint law of $(X,Y)$ . This quantity can be seen as the counterpart of the mean squared error in point regression. The excess risk of $\\hat{F}_{n}$ is defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\hat{F}_{n})-\\mathcal{R}(F^{*})=\\mathbb{E}\\left[S(\\hat{F}_{n,X},Y)-S(F_{X}^{*},Y)\\right]=\\mathbb{E}\\left[\\bar{S}(\\hat{F}_{n,X},F_{X}^{*})-\\bar{S}(F_{X}^{*},F_{X}^{*})\\right]\\geq0.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The nonnegativity is ensured by the fact that $S$ is a proper scoring rule according to Definition 1. If $S$ is strictly proper, the excess risk is equal to 0 if and only if $\\hat{F}_{n,x}=F_{x}^{*}$ almost everywhere (with respect to $P_{X}$ ). For the CRPS, Eq. (4) implies that the excess risk can be rewritten as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\hat{F}_{n})-\\mathcal{R}(F^{*})=\\mathbb{E}\\left[\\int_{\\mathbb{R}}\\left|\\hat{F}_{n,X}(u)-F_{X}^{*}(u)\\right|^{2}d u\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "2.2.2 Model fitting ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our first interest lies in model ftiting by empirical risk minimization. Here we consider a parametric family $(F_{\\theta})_{\\theta\\in\\Theta}$ , $\\Theta\\subset\\mathbb{R}^{K}$ , where $\\bar{F_{\\theta}}\\colon\\dot{x}\\in\\bar{\\mathbb{R}}^{d}\\mapsto F_{\\theta,x}\\in\\mathcal{P}_{0}\\subset\\mathcal{P}(\\mathbb{R})$ . The empirical risk associated with $F_{\\theta}$ is computed on the training sample $\\mathcal{D}_{n}$ by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{R}}_{n}(F_{\\theta})=\\frac{1}{n}\\sum_{i=1}^{n}S(F_{\\theta,X_{i}},Y_{i})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and is an empirical counterpart of the theoretical risk $\\mathcal{R}(F_{\\theta})$ . Empirical risk minimization consists in finding ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\theta}_{n}=\\underset{\\theta\\in\\Theta}{\\arg\\operatorname*{min}}\\,\\mathcal{\\hat{R}}_{n}(F_{\\theta})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and proposing the estimator $F_{\\hat{\\theta}_{n}}$ which is thought as almost optimal within the family $(F_{\\theta})_{\\theta\\in\\Theta}$ . A classical decomposition of the excess risk of the corresponding estimator is given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{R}(F_{\\hat{\\theta}_{n}})-\\mathcal{R}(F^{*})=\\Big(\\mathcal{R}(F_{\\hat{\\theta}_{n}})-\\operatorname*{inf}_{\\theta\\in\\Theta}\\mathcal{R}(F_{\\theta})\\Big)+\\Big(\\operatorname*{inf}_{\\theta\\in\\Theta}\\mathcal{R}(F_{\\theta})-\\mathcal{R}(F^{*})\\Big).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the two terms are called the estimation error and the approximation error respectively. The approximation error is deterministic and depends on the ability of the family $(F_{\\theta})_{\\theta\\in\\Theta}$ to approximate $F^{*}$ . The estimation error is random as it depends on the training sample $\\mathcal{D}_{n}$ . Our first goal is the following: ", "page_idx": 3}, {"type": "text", "text": "Goal 1: provide non asymptotic estimates for the estimation error $\\mathcal{R}(F_{\\hat{\\theta}_{n}})-\\operatorname*{inf}_{\\theta\\in\\Theta}\\mathcal{R}(F_{\\theta})$ . ", "page_idx": 3}, {"type": "text", "text": "2.2.3 Model selection and convex aggregation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our second interest lies in model selection and convex aggregation via validation error minimization. Here we suppose that a validation sample $\\mathcal{D}_{N}^{\\prime}=\\{(X_{i}^{\\prime},Y_{i}^{\\prime}),\\quad1\\leq i\\leq N\\}$ is available, which is assumed independent of the training sample $\\mathcal{D}_{n}$ . ", "page_idx": 3}, {"type": "text", "text": "Model selection. A common situation in machine learning is that we have $M$ algorithms at hand that are trained on $\\mathcal{D}_{n}$ , resulting in models $\\hat{F}_{n}^{1},\\hdots,\\hat{F}_{n}^{M}$ . In order to select the best model, we compute the empirical risks on the validation sample ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{R}}_{N}^{\\prime}(\\hat{F}_{n}^{m})=\\frac{1}{N}\\sum_{i=1}^{N}S(\\hat{F}_{n,X_{i}^{\\prime}}^{m},Y_{i}^{\\prime})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and select the model ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{m}=\\arg\\operatorname*{min}_{1\\leq m\\leq M}\\hat{\\mathcal{R}}_{N}^{\\prime}(\\hat{F}_{n}^{m}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "An oracle having access to the theoretical risk would have selected ", "page_idx": 3}, {"type": "equation", "text": "$$\nm^{*}=\\arg\\operatorname*{min}_{1\\leq m\\leq M}\\mathcal{R}(\\hat{F}_{n}^{m}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "leading to the definition of the regret ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{R}({\\hat{F}_{n}^{\\hat{m}}})-\\mathcal{R}({\\hat{F}_{n}^{m^{*}}})=\\mathcal{R}({\\hat{F}_{n}^{\\hat{m}}})-\\operatorname*{min}_{1\\leq m\\leq M}\\mathcal{R}({\\hat{F}_{n}^{m}}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Goal 2: provide non asymptotic estimates for the regret $\\begin{array}{r}{\\mathcal{R}(\\hat{F}_{n}^{\\hat{m}})-\\operatorname*{min}_{1\\leq m\\leq M}\\mathcal{R}(\\hat{F}_{n}^{m}).}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "Convex aggregation. We define the convex aggregation of models $\\hat{F}_{n}^{1},\\ldots,\\hat{F}_{n}^{M}$ with weights $\\lambda_{1},\\ldots,\\lambda_{M}$ by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{F}_{n,x}^{\\lambda}=\\sum_{m=1}^{M}\\lambda_{m}\\hat{F}_{n,x}^{m}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here $\\lambda=(\\lambda_{1},\\ldots,\\lambda_{M})$ is an element of the simplex $\\begin{array}{r}{\\Lambda_{M}=\\{\\lambda\\colon\\lambda_{m}\\geq0,\\sum_{1\\leq m\\leq M}\\lambda_{m}=1\\}}\\end{array}$ . The best weights for convex aggregation are obtained by minimization of the validation error, i.e. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\lambda}=\\underset{\\lambda\\in\\Lambda_{M}}{\\arg\\operatorname*{min}}\\,\\hat{\\mathcal{R}}_{N}^{\\prime}(\\hat{F}_{n}^{\\lambda}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Goal 3: provide non asymptotic estimates for the regret $\\mathcal{R}(\\hat{F}_{n}^{\\hat{\\lambda}})-\\operatorname*{inf}_{\\lambda\\in\\Lambda_{M}}\\mathcal{R}(\\hat{F}_{n}^{\\lambda})$ . ", "page_idx": 3}, {"type": "text", "text": "3 Main results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We present our main results for model ftiting, model selection and convex aggregation in distributional regression. We first focus on concentration bound under sub-Gaussianity assumptions. In a second step, we extend our results under weaker moment assumptions. The definition of sub-Gaussian random variables and distributions is given in Appendix A as well as some useful concentration inequalities. All the proofs are postponed to Appendices B, C and D. ", "page_idx": 4}, {"type": "text", "text": "3.1 Estimation error in model fitting ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We provide concentration results for the estimation error when ftiting a parametric model via empirical CRPS-error minimization according to the framework described in Section 2.2.2. Our working assumptions are the following. ", "page_idx": 4}, {"type": "text", "text": "Assumption 1. (sub-gaussianity) ", "page_idx": 4}, {"type": "text", "text": "$i$ ) the variable $Y$ is $\\beta_{1}$ -sub-Gaussian;   \n$i i)$ ) there exists $\\beta_{2}>0$ such that $m_{1}(F_{\\theta,X})$ is $\\beta_{2}$ -sub-Gaussian for all $\\theta\\in\\Theta$ . ", "page_idx": 4}, {"type": "text", "text": "Assumption $1{-}i$ ) is classical in a regression setting (Gy\u00f6rfi et al., 2002; Biau and Devroye, 2015) while Assumption ${1-i i}$ ) characterizes the sub-Gaussian behavior of the absolute moment of $F_{\\theta,X}$ . Importantly, Assumption 1 implies that the variable $Z_{\\theta}=S(F_{\\theta,X})$ is $\\sqrt{2(\\beta_{1}^{2}+\\beta_{2}^{2})}$ -sub-Gaussian for all $\\theta\\in\\Theta$ , see Proposition 5 in the appendix. ", "page_idx": 4}, {"type": "text", "text": "Our second assumption requires compactness of the parameter space and Lipschitz continuity of the model $(F_{\\theta})_{\\theta\\in\\Theta}$ . We denote by $W_{1}$ the Wasserstein distance of order 1 on the space ${\\mathcal{P}}_{1}(\\mathbb{R})$ . ", "page_idx": 4}, {"type": "text", "text": "Assumption 2. (regularity) The parameter space $\\Theta\\subset\\mathbb{R}^{K}$ is compact and there exists a constant $L>0$ such that $\\bar{W_{1}}(F_{\\theta_{1},x},F_{\\theta_{2},x})\\leq L\\|\\theta_{1}-\\bar{\\theta}_{2}\\|$ for all $\\theta_{1},\\theta_{2}\\in\\Theta$ , $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ . ", "page_idx": 4}, {"type": "text", "text": "This assumption ensures the Lipschitz continuity of the empirical risk $\\theta\\mapsto\\mathcal{R}_{n}(F_{\\theta})$ , see Proposition 6 and, by compactness, the existence of $\\hat{\\theta}_{n}$ , the ERM estimator defined in Eq. (5). We provide in Section 4 examples of popular models verifying the two assumptions. ", "page_idx": 4}, {"type": "text", "text": "Our main result provides a concentration bound on the estimation error. We let $R>0$ be such that $\\Theta$ is included in the ball centered at 0 and with radius $R$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Under Assumptions $I\\!-\\!2$ , for all $\\delta\\in(0,1)$ , the estimation error satisfies, with probability at least $1-\\delta$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{R}(F_{\\hat{\\theta}_{n}})-\\operatorname*{inf}_{\\theta\\in\\Theta}\\mathcal{R}(F_{\\theta})\\leq\\sqrt{\\frac{c_{\\beta}\\log(2n^{K}/\\delta)}{n}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with $c_{\\beta}=64(\\beta_{1}^{2}+\\beta_{2}^{2})$ and provided that $n$ is large enough so that $n\\log(2n^{K}/\\delta)\\geq(48L R)^{2}/c_{\\beta}$ . ", "page_idx": 4}, {"type": "text", "text": "The inequality (6) is commonly known as an oracle inequality. The convergence rate depends on $\\sqrt{\\log(2n^{K}/\\delta)/n}$ with $K$ the dimension of the parameter space and $n$ the sample size. A key point in the proof is the the combinatorial complexity of $\\Theta$ in terms of $\\epsilon$ -net, see Devroye et al. (1996). A bound in expectation can easily be deduced from Theorem 1 and its proof. ", "page_idx": 4}, {"type": "text", "text": "Corollary 1. Under Assumptions 1- 2, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big[\\mathcal{R}(F_{\\hat{\\theta}_{n}})-\\operatorname*{inf}_{\\theta\\in\\Theta}\\mathcal{R}(F_{\\theta})\\Big]\\leq2\\sqrt{\\frac{c_{\\beta}\\log(2n^{K})}{n}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "provided $n$ is large enough so that $n\\log(2n^{K})\\geq(48L R)^{2}/c_{\\beta}$ . ", "page_idx": 4}, {"type": "text", "text": "In particular, the ERM approach is weakly consistent when $n$ goes to infinity. ", "page_idx": 4}, {"type": "text", "text": "3.2 Estimation error in model selection and convex aggregation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We provide concentration results for the regret in model selection and convex aggregation. Recall that the methodology is based on minimization of the test error on a validation sample, see the precise framework in Section 2.2.3. We need the following assumption. ", "page_idx": 4}, {"type": "text", "text": "Assumption 3. (sub-Gaussianity) ", "page_idx": 4}, {"type": "text", "text": "$i$ ) the variable $Y$ is $\\beta_{1}$ -sub-Gaussian; ", "page_idx": 5}, {"type": "text", "text": "$i i)$ ) conditionally on $\\mathcal{D}_{n}$ , there exists $\\beta_{n}=\\beta(D_{n})>0$ such that $m_{1}(\\hat{F}_{n,X}^{m})$ is $\\beta_{n}$ -sub-Gaussian for all $m=1,\\dotsc,M$ . ", "page_idx": 5}, {"type": "text", "text": "We will see in Section 4 that for several popular models such as distributional nearest neighbors or distributi\u221aonal random forest, Assumption $_{3-i i}$ ) is satisfied with $\\beta_{n}\\,=\\,\\mathrm{max}_{1}{\\le}i{\\le}n\\,|Y_{i}|$ which is of order $\\beta_{1}{\\sqrt{\\log n}}$ (Vershynin, 2018, Exercise 2.5.8 p.25). ", "page_idx": 5}, {"type": "text", "text": "Under this assumption, a control of the regret in model selection is provided by the following theorem.   \nOur results hold conditionally on the training set $\\mathcal{D}_{n}$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. Under Assumption $^3$ , for all $\\delta\\in(0,1)$ , the regret in model selection satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\mathcal{R}(\\hat{F}_{n}^{\\hat{m}})-\\operatorname*{min}_{1\\le m\\le M}\\mathcal{R}(\\hat{F}_{n}^{m})\\le4\\sqrt{c_{n}\\log(2M/\\delta)/N}\\;\\Big|\\;\\mathcal{D}_{n}\\right)\\ge1-\\delta\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with $c_{n}=\\beta_{1}^{2}+\\beta_{n}^{2}$ . Furthermore, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big[\\mathcal{R}(\\hat{F}_{n}^{\\hat{m}})-\\operatorname*{min}_{1\\leq m\\leq M}\\mathcal{R}(\\hat{F}_{n}^{m})\\;\\Big|\\;\\mathcal{D}_{n}\\Big]\\leq8\\sqrt{\\frac{c_{n}\\log(2M)}{N}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "When selecting the hyperparameter $k$ in nearest neighbor distributional regression or mtry in distributional random forest, one has respectively $M=n$ and $M=d$ if all possible values are considered \u2013 see Section 4 for more details. Note that when the response variable $Y$ is bounded, then $c_{n}$ does not depend on $n$ and is constant. We now state a bound for the regret in convex aggregation. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3. Under Assumption $^3$ , for all $\\delta\\in(0,1)$ , the regret in convex aggregation satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\mathcal{R}(\\hat{F}_{n}^{\\hat{\\lambda}})-\\operatorname*{inf}_{\\lambda\\in\\Lambda}\\mathcal{R}(\\hat{F}_{n}^{\\lambda})\\leq8\\sqrt{c_{n}\\log(2N^{M}/\\delta)/N}\\;\\Big|\\;\\mathcal{D}_{n}\\Big)\\geq1-\\delta,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with $c_{n}=\\beta_{1}^{2}+\\beta_{n}^{2}$ provided $N$ is large enough so that $N\\log(2N^{M}/\\delta)\\geq48^{2}/c_{n}$ . Furthermore, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big[\\mathcal{R}(\\hat{F}_{n}^{\\hat{\\lambda}})-\\operatorname*{inf}_{\\lambda\\in\\Lambda}\\mathcal{R}(\\hat{F}_{n}^{\\lambda})\\;\\Big|\\;\\mathcal{D}_{n}\\Big]\\leq2\\sqrt{\\frac{c_{n}\\log(2N^{M})}{N}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "provided $N\\log(2N^{M})\\geq48^{2}/c_{n}$ ", "page_idx": 5}, {"type": "text", "text": "3.3 Beyond sub-gaussianity ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The preceding results hold under a strong sub-Gaussianity. We next adapt our results to the following weaker moment condition. ", "page_idx": 5}, {"type": "text", "text": "Assumption 4. There is $p\\geq2$ and $D>0$ such that $\\mathbb{E}[|Y|^{p}]\\le D$ and $\\mathbb{E}[|m_{1}(F_{\\theta,X})|^{p}]\\leq D$ for all $\\theta\\in\\Theta$ ", "page_idx": 5}, {"type": "text", "text": "We recall that, for $p\\geq1$ , the $L^{p}$ -norm of a random variable $Z$ is defined by $\\|Z\\|_{L^{p}}=\\mathbb{E}[|Z^{p}|]^{1/p}$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 4. Under Assumptions 2 and $^{4}$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left\\|\\mathcal{R}(F_{\\hat{\\theta}_{n}})-\\operatorname*{inf}_{\\theta\\in\\Theta}\\mathcal{R}(F_{\\theta})\\right\\|_{L^{p}}\\leq C n^{-p/(2(p+K))}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with constant $C>0$ depending only on $K,p,L,D,R$ and possibly made explicit from the proof. This implies the bound for the expected estimation error ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big[\\mathcal{R}(F_{\\hat{\\theta}_{n}})-\\operatorname*{inf}_{\\theta\\in\\Theta}\\mathcal{R}(F_{\\theta})\\Big]\\leq C n^{-p/(2(p+K))}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For $p$ large, the rate of convergence tends to the parametric rate $n^{-1/2}$ obtained (up to a logarithmic factor) in the sub-Gaussian case . We propose additional results for model selection (Theorem 5) and convex aggregation (Theorem 6) which are, for the sake of brevity, postponed to Appendix D.2. ", "page_idx": 5}, {"type": "text", "text": "4 Examples and popular models for distributional regression ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We present the most popular models for distributional regression for which we want to apply our results. The first two (EMOS and DRN) are parametric, while the last two (distributional $k$ -NN and DRF) are fully non-parametric. ", "page_idx": 5}, {"type": "text", "text": "4.1 EMOS and distributional regression networks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "EMOS. The EMOS model was designed by Gneiting et al. (2005) for the purpose of statistical post-processing of ensemble weather forecast. In this framework, the predictive distribution takes the form of a discrete distributions $m^{-1}\\sum_{l=1}^{m}\\delta_{y_{l}}$ with members $y_{1},\\ldots,y_{m}$ corresponding to different scenarios obtained from numerical weather predictions. Such forecast typically suffer from bias and underdispersion so that statistical post-processing is needed. The explanatory variable for distributional regression are Ensemble Member Output Statistics such as the ensemble mean $\\bar{y}$ and ensemble variance $v_{y}^{2}$ . In its simplest version, EMOS models the predictive distribution as a Gaussian distribution with parameters $m=\\beta_{0}+\\beta_{1}\\bar{y}$ and $\\sigma^{2}=\\beta_{0}^{\\prime}+\\beta_{1}^{\\prime}v_{y}^{2}$ . This is a parametric model with $\\theta\\,=\\,(\\beta_{0},\\beta_{1},\\beta_{0}^{\\prime},\\beta_{1}^{\\prime})$ in $\\Theta\\,=\\,\\mathbb{R}^{2}\\,\\times\\,(0,\\infty)^{2}$ . Minimum CRPS estimation is used for model fitting as described in 2.2.2. This simple yet successful method has encountered many generalizations (Scheuerer 2013; Scheuerer and Hamill 2015; Baran and Nemoda 2016) and we shall consider the following general setting. Given $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , the predictive distribution takes the form $F(\\cdot;\\theta)=F((\\cdot-m)/\\sigma)$ with $\\theta=(\\alpha,\\beta,\\bar{\\alpha}^{\\prime},\\beta^{\\prime})\\in\\mathbb{R}^{1+d}\\times\\mathbb{R}^{1\\bar{+}d}$ and ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\displaystyle\\{m(\\boldsymbol{x};\\boldsymbol{\\theta})=\\alpha+\\beta^{\\top}\\boldsymbol{x}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with ${\\mathrm{softplus}}(u)=\\log(1+\\mathrm{e}^{u})$ . The predictive distribution belongs to a location/scale family and the location and log-scale parameters are linear in the covariate $x$ . The softplus link function ensures positivity of the scale. ", "page_idx": 6}, {"type": "text", "text": "Distributional Regression Networks (DRN). In order to consider higher dimension co-variates together with non-linear dependence, Rasp and Lerch (2018) introduce distributional regression networks. In a setting similar to EMOS, neural networks are used to model complex response functions $m(x;{\\boldsymbol{\\theta}})$ and $\\log\\sigma^{2}(x;\\theta)$ . In the case of a single hidden layer with $H$ units, the equations write ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\{m(x;\\theta)=\\alpha+\\beta^{\\top}g(\\gamma+\\delta^{\\top}x)\\right.}\\\\ &{\\left.\\left(\\sigma^{2}(x;\\theta)=\\mathrm{softplus}(\\alpha^{\\prime}+\\beta^{'\\top}g(\\gamma+\\delta^{'\\top}x))\\ \\ \\ ,\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $g$ denotes the activation function acting componentwise, $(\\alpha,\\beta,\\alpha^{\\prime},\\beta^{\\prime})\\in\\mathbb{R}^{1+H}\\times\\mathbb{R}^{1+H}$ the parameters for the output layers and $(\\gamma,\\delta)\\in\\mathbb{R}^{H+H d}$ the biases and weights of the hidden layer. The global parameter $\\bar{\\theta^{\\prime}}\\!=\\!\\alpha,\\!\\stackrel{\\cdot}{\\beta},\\!\\alpha^{\\prime},\\beta^{\\prime},\\gamma,\\delta)$ lies in dimension $(d+3)H+2$ . Note that in absence of hidden layers, DRN reduces to EMOS. Extension to a MLP structure with multiple hidden layers is straightforward, see Schulz and Lerch (2022) for a review of DRNs and their applications. ", "page_idx": 6}, {"type": "text", "text": "We next provide conditions ensuring that our results hold for the EMOS and DRN models. ", "page_idx": 6}, {"type": "text", "text": "Proposition 1. Let $(F_{\\theta})_{\\theta\\in\\Theta}$ be a EMOS or DRN model with parameters restricted to a compact subset $\\Theta$ . If $Y$ is sub-Gaussian, $X$ is bounded and the activation function $g$ is Lipschitz continuous, then Assumptions $^{\\,l}$ , 2 and 3 are satisfied. ", "page_idx": 6}, {"type": "text", "text": "4.2 Distributional k-Nearest Neighbors and Random Forests ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Distributional $\\mathbf{k}$ -Nearest Neighbors (KNN). The predictive distribution is built on a straightforward extension of $k$ -nearest neighbor regression: we set ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{F}_{n,x}(y)=\\frac1k\\sum_{i=1}^{k}\\mathbb{1}_{\\{X_{i}\\in\\ker(x)\\}}\\mathbb{1}_{\\{Y_{i}\\leq y\\}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\operatorname{knn}(x)$ denotes the set of $k$ nearest neighbors of $x$ in the training set $(X_{i})_{1\\leq i\\leq n}$ . The main hyperparameter is the number $k$ of neighbors, leading to a model selection problem as considered in 2.2.3. This simple method is known as the Analog method in the framework of statistical postprocessing of weather forecast (Toth, 1989). ", "page_idx": 6}, {"type": "text", "text": "Distributional Random Forests (DRF). Random forests for distributional regression have been introduced by $\\acute{\\mathbf{C}}$ evid et al. (2022) and are a powerful nonparametric method. We describe only the main lines of the method. Recall that in standard regression, Breiman\u2019s Random Forest (Breiman, ", "page_idx": 6}, {"type": "text", "text": "2001) estimates the regression function by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\hat{\\mu}(x)=\\frac{1}{B}\\sum_{b=1}^{B}T^{(b)}(x)=\\frac{1}{B}\\sum_{b=1}^{B}\\frac{1}{|L^{b}(x)|}\\sum_{i=1}^{n}Y_{i}\\mathbb{1}_{\\{X_{i}\\in L^{b}(x)\\}},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $T_{1},\\ldots,T_{B}$ denote randomized regression trees built on bootstrap samples of the original data and $L^{b}(x)$ the leaf containing $x$ in the tree $T^{b}$ . Interverting the two sums yields ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\hat{\\mu}(x)=\\sum_{i=1}^{n}\\left(\\frac{1}{B}\\sum_{b=1}^{B}\\frac{\\mathbb{1}_{\\{X_{i}\\in L^{b}(x)\\}}}{|L^{b}(x)|}\\right)Y_{i}=\\sum_{i=1}^{n}w_{n i}(x)Y_{i}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Using these Random Forest weights, the predictive distribution writes ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\hat{F}_{n,x}(y)=\\sum_{i=1}^{n}w_{n i}(x)\\mathbb{1}_{\\{Y_{i}\\leq y\\}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "This strategy based on Breiman\u2019s regression tree is used by Meinshausen (2006) to construct the quantile regression forest. Different splitting strategies in the tree construction have been considered to better detect changes in distribution rather than changes in mean (Taillardat et al. 2016; Athey et al. 2019). Note that C\u00b4evid et al. (2022) minimizes a scoring rule to construct the splits of the trees. The main hyperparameter of this nonparametric method is the so-called mtry parameter that controls the number of co-variates tested at each split in the trees. ", "page_idx": 7}, {"type": "text", "text": "For the distributional KNN and DRF models, model fitting with ERM is irrelevant and we only consider model selection and convex aggregation. ", "page_idx": 7}, {"type": "text", "text": "Proposition 2. Let $\\hat{F}_{n}$ be a KNN or DRF model ftited on a training set $\\mathcal{D}_{n}$ . Then Assumption 3-ii) is satisfied with $\\beta_{n}=\\operatorname*{max}_{1\\leq i\\leq n}|Y_{i}|$ . ", "page_idx": 7}, {"type": "text", "text": "5 Numerical results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we illustrate how model selection and model aggregation work on real data sets. The cross validation methodology, widely used in practice, is justified by the theoretical results obtained in Section 3. The source code for these experiments can be found at https://github. com/ZaouiAmed/Neurips2024_DistributionalRegression. ", "page_idx": 7}, {"type": "text", "text": "Datasets. We consider two datasets used in the framework of heteroscedastic regression with reject option as detailed in Zaoui et al. (2020). The first dataset, QSAR aquatic toxicity (Ballabio et al., 2019) is referred to as qsar and comprises 546 observations with 8 numerical features used to predict acute toxicity in Pimephales promelas. The toxicity output ranges from 0.12 to 10.05 with evidence of a low heteroscedasticity. The second dataset, Airfoil Self-Noise (Brooks et al., 2014) is referred to as airfoil and consists of 1503 observations with 5 measured features from aerodynamic and acoustic tests. The output represents the scaled sound pressure level in decibels, ranging from 103 to 140 with evidence of a strong heteroscedasticity.. ", "page_idx": 7}, {"type": "text", "text": "Models. We consider the KNN and DRF models to predict the conditional distribution of the output variable. Our focus lies on hyperparameters selection via minimization of the validation error, where the main hyperparameter is the number k of neighbors and the number mtry of variables considered at each split for KNN and DRF respectively. We utilize the implementation of these methods from the R packages KernelKnn and DRF . For DRF, a preliminary exploration shows that sensible choices for the other parameters are num. $\\mathtt{t r e e s=1000}$ , sample.fraction $=\\!0.9$ , min.node. ${\\tt s i z e}{=}1$ and default values for others parameters. Finally, the R package ScoringRule is used for CRPS computation and the optim function based on the Nelder-Mead method is used for parameter optimization in convex aggregation. ", "page_idx": 7}, {"type": "text", "text": "Methodology. We use the same methodology for the two datasets. We divide the data into three parts: $50\\%$ for training, $20\\%$ for validation and $30\\%$ for testing. In a first stage, the training set is used to train the model (KNN or DRF) for the various hyperparameters (k or mtry) and the validation set is used to select best hyperparameters $\\widehat{\\mathtt{k}}$ or $\\widehat{\\mathrm{mtry}}$ . Furthermore, the validation set also used to choose for model selection (MS), choosing between KNN and RF, and the best convex aggregation (CA). In a second stage, the different models (KNN, RF, MS and CA) are refitted on the union of training and validation sets ( $70\\%$ of data) and evaluated on the test set $(30\\%)$ by computing the test CRPS-error. This process is repeated 100 times for different random splits of the data into training, validation and testing sets. The distributions of the test error for KNN, DRF, MS and CA is then analyzed in terms of mean, standard error and boxplot. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Results. For the sake of brevity, only the results for the qsar dataset are presented, while the results for the airfoil dataset are postponed to Appendix F. The first stage of the procedure where the hyperparameter (k or mtry) is selected by minimization of the validation error is shown on the left plot of Fig. 1 for KNN and on the middle plot for DRF. In both cases, the validation error curve shows a clear minimum allowing to select the hyperparameter $\\widehat{\\mathtt{k}}=8$ and $\\widehat{\\mathrm{mtry}}=4$ corresponding to CRPS-error of 0.696 and 0.678 on the validation set respectively. In the second stage, KNN, DRF, MS and CA are evaluated on the test set and the right plot of Fig. 1 shows the distribution of the test error over 100 repetitions. We can see from the boxplots that the distribution of the test error is slightly larger for KNN than for DRF, that MS achieves almost the same performance as RF and that CA achieves slightly better performance than DRF. Hence model selection and convex aggregation accomplish their goal. The numerical values of the means together with standard errors are summarized in Table 1, confirming our analysis from the boxplots. ", "page_idx": 8}, {"type": "image", "img_path": "cSfxzCozPU/tmp/36c50ed6e0f5594a60f7888aaae1229df7d4a66b36d36c6058c40586529574aa.jpg", "img_caption": ["Figure 1: qsar data. Left and middle: selection of $\\mathtt{k}$ for the KNN algorithm and of mtry for the DRF algorithm by minimization of the validation error. Right: test error evaluated with 100 repetitions for KNN, DRF, model selection (MS) and convex aggregation (CA). ", "Table 1: qsar data. Mean of the test CRPS and its standard error (in parenthesis) over 100 repetitions. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we considered distributional regression with error assessed using the CRPS and investigated model fitting via empirical risk minimization. Additionally, we explored classical aggregation procedures: model selection and convex aggregation where two independent samples are used, the first for model construction, the second for model selection or convex aggregation. We derived oracle concentration inequalities for the estimation error and established an upper bound for its expectation within the sub-Gaussian framework and beyond, considering weaker moment assumptions. These new theoretical results are solid mathematical justifications for common practices in the framework of distributional regression. In future work, we will study the minimax convergence rates for our approaches and consider the use of empirical process theory to strengthen our results. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Al Masry, Z., Pic, R., Domby, C., and Devalland, C. (2024). A new methodology to predict the oncotype scores based on clinico-pathological data with similar tumor proflies. Breast Cancer Res Treat, 203:587\u2013598.   \nAthey, S., Tibshirani, J., and Wager, S. (2019). Generalized random forests. Ann. Statist., 47(2):1148\u2013 1178.   \nBallabio, D., Cassotti, M., Consonni, V., and Todeschini, R. (2019). QSAR aquatic toxicity. UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C5SG7H.   \nBaran, S. and Lerch, S. (2015). Log-normal distribution based ensemble model output statistics models for probabilistic wind-speed forecasting. Quarterly Journal of the Royal Meteorological Society, 141(691):2289\u20132299.   \nBaran, S. and Nemoda, D. (2016). Censored and shifted gamma distribution based emos model for probabilistic quantitative precipitation forecasting. Environmetrics, 27.   \nBiau, G. and Devroye, L. (2015). Lectures on the Nearest Neighbor Method. Springer Series in the Data Sciences. Springer New York.   \nBobkov, S. and Ledoux, M. (2019). One-dimensional empirical measures, order statistics and Kantorovich transport distances. Memoirs of the Amer. Math. Soc., 261(1259):v+126, 2019. ISSN 0065-9266.   \nBoucheron, S., Lugosi, G., and Massart, P. (2013). Concentration inequalities. Oxford University Press, Oxford. A nonasymptotic theory of independence, With a foreword by Michel Ledoux.   \nBreiman, L. (2001). Random forests. Machine Learning, 45.   \nBrooks, T., Pope, D., and Marcolini, M. (2014). Airfoil Self-Noise. UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C5VW2C.   \nBunea, F., Tsybakov, A., and Wegkamp, M. (2007). Aggregation for gaussian regression. Annals of Statistics, 35(4):1674\u20131697.   \nChhachhi, S. and Teng, F. (2023). On the 1-wasserstein distance between location-scale distributions and the effect of differential privacy. Preprint.   \nDevroye, L., Gy\u00f6rf,i L., and Lugosi, G. (1996). A Probabilistic Theory of Pattern Recognition. Springer, New York.   \nGneiting, T. and Katzfuss, M. (2014). Probabilistic forecasting. Annual Review of Statistics and Its Application, 1(1):125\u2013151.   \nGneiting, T. and Raftery, A. (2007). Strictly proper scoring rules, prediction, and estimation. Journal of the American Statistical Association, 102(477):359\u2013378.   \nGneiting, T., Raftery, A., A.H., W., and Goldman, T. (2005). Calibrated probabilistic forecasting using ensemble model output statistics and minimum crps estimation. Monthly Weather Review, 133(5):1098\u20131118.   \nGy\u00f6rf,i L., Kohler, M., Krzyz\u02d9ak, A., and Walk, H. (2002). A distribution-free theory of nonparametric regression. Springer Series in Statistics. Springer-Verlag, New York.   \nHenzi, A., Kleger, G.-R., Hilty, M. P., Wendel Garcia, P. D., Ziegel, J. F., and on behalf of RISC-19- ICU Investigators for Switzerland (2021a). Probabilistic analysis of covid-19 patients\u2019 individual length of stay in swiss intensive care units. PLOS ONE, 16:1\u201314.   \nHenzi, A., Ziegel, J. F., and Gneiting, T. (2021b). Isotonic Distributional Regression. Journal of the Royal Statistical Society Series B: Statistical Methodology, 83(5):963\u2013993.   \nHoeffding, W. (1963). Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58:13\u201330.   \nMatheson, J. E. and Winkler, R. L. (1976). Scoring rules for continuous probability distributions. Management Science, 22(10):1087\u20131096.   \nMeinshausen, N. (2006). Quantile regression forests. The Journal of Machine Learning Research, 7:983\u2013999.   \nPetrov, V. (1995). Limit Theorems of Probability Theory: Sequences of Independent Random Variables. Oxford science publications. Clarendon Press.   \nPic, R., Dombry, C., Naveau, P., and Taillardat, M. (2023). Distributional regression and its evaluation with the crps: Bounds and convergence of the minimax risk. Journal of the American Statistical Association, 39(4):1564\u20131572.   \nRasp, S. and Lerch, S. (2018). Neural networks for post-processing ensemble weather forecasts. Monthly Weather Review, 146:3885\u20133900.   \nScheuerer, M. (2013). Probabilistic quantitative precipitation forecasting using ensemble model output statistics. Quarterly Journal of the Royal Meteorological Society, 140.   \nScheuerer, M. and Hamill, T. M. (2015). Statistical post-processing of ensemble precipitation forecasts by fitting censored, shifted gamma distributions. Monthly Weather Review, 143.   \nSchulz, B., El Ayari, M., Lerch, S., and Baran, S. (2021). Post-processing numerical weather prediction ensembles for probabilistic solar irradiance forecasting. Solar Energy, 220:1016\u20131031.   \nSchulz, B. and Lerch, S. (2022). Machine learning methods for postprocessing ensemble forecasts of wind gusts: A systematic comparison. Monthly Weather Review, 150(1):235\u2013257.   \nTaillardat, M., Mestre, O., Zamo, M., and Naveau, P. (2016). Calibrated ensemble forecasts using quantile regression forests and ensemble model output statistics. Monthly Weather Review, 144(6):2375\u20132393.   \nToth, Z. (1989). Long-range weather forecasting using an analog approach. Journal of Climate, 2(6):594 \u2013 607.   \nTsybakov, A. (2003). Optimal rates of aggregation. Learning Theory and Kernel Machines, 2777:303\u2013 313.   \nVershynin, R. (2018). High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press.   \nZaoui, A., Denis, C., and Hebiri, M. (2020). Regression with reject option and application to knn. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H., editors, Advances in Neural Information Processing Systems, volume 33, pages 20073\u201320082. Curran Associates, Inc.   \n\u00b4Cevid, D., Michel, L., N\u00e4f, J., B\u00fchlmann, P., and Meinshausen, N. (2022). Distributional random forests: Heterogeneity adjustment and multivariate distributional regression. Journal of Machine Learning Research, 23(333):1\u201379. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "A Sub-gaussian distributions and concentration inequalities ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "The concept of sub-Gaussianity has been extensively studied and is a key tool in deriving concentration inequalities, as documented in the monographs by Vershynin (2018) and Boucheron et al. (2013). For the convenience of the reader, we recall the definition and basic properties of sub-Gaussian random variables as well as the useful Hoeffding inequality. ", "page_idx": 11}, {"type": "text", "text": "A.1 Sub-Gaussian random variables ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Definition 2 (Sub-Gaussian random variable). Let $\\beta>0$ . The random variable $X$ is said to be $\\beta$ -sub-Gaussian if ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[e^{\\lambda(X-\\mathbb{E}[X])}\\right]\\le e^{\\frac{1}{2}\\lambda^{2}\\beta^{2}}}&{{}f o r\\,a l l\\,\\lambda\\in\\mathbb{R}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Proposition 3. Let $\\beta,\\beta_{1},\\beta_{2}>0$ . ", "page_idx": 11}, {"type": "text", "text": "$i$ ) If $X$ is $\\beta$ -sub-Gaussian, then $|X|$ is also $\\beta$ -sub-Gaussian.   \nii) If $X$ is a bounded random variable such that $X\\,\\in\\,[a,b]$ almost surely, then $X$ is $\\beta$ -subGaussian with $\\beta=(b-a)/2$ .   \niii) If $X,Y$ are random variables such that $X$ is $\\beta$ -sub-Gaussian and $0\\leq Y\\leq X$ , then $Y$ is also $\\beta$ -sub-Gaussian.   \n$i v)$ If $X_{i}$ is $\\beta_{i}$ -sub-Gaussian for $i\\,=\\,1,2,$ , the sum $Y\\,=\\,X_{1}\\,+\\,X_{2}$ is $\\beta$ -sub-Gaussian with $\\beta=\\sqrt{2(\\beta_{1}^{2}+\\beta_{2}^{2})}.$ . ", "page_idx": 11}, {"type": "text", "text": "A.2 Concentration inequalities ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "In this section we gather several technical results which are used to derive the contributions of this work. We start with the general Hoeffding\u2019s inequality (Vershynin, 2018, Theorem 2.6.2 p. 28). ", "page_idx": 11}, {"type": "text", "text": "Proposition 4 ( Hoeffding (1963)). Let $Z_{1},\\ldots,Z_{n}$ be independent random variables such that $Z_{i}$ is $\\beta$ -sub-Gaussian for all $i=1\\ldots,n$ . Then, for all $t\\geq0$ we have ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\frac{1}{n}\\sum_{i=1}^{n}\\left(Z_{i}-\\mathbb{E}[Z_{i}]\\right)\\right|\\geq t\\right)\\leq2\\exp\\left(-\\frac{n t^{2}}{2\\beta^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "The following Lemma is often useful to derive upper bound in expectation from concentration inequalities. ", "page_idx": 11}, {"type": "text", "text": "Lemma 1. Let $a\\ge1$ , $b>0$ and $Z$ a positive random variable such that ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(Z\\geq t\\right)\\leq\\exp(a-b t^{2})\\quad f o r\\,a l l\\,t\\geq0.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Then we have ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathbb{E}[Z]\\leq\\left(1+\\frac{\\sqrt{\\pi}}{2}\\right)\\sqrt{\\frac{a}{b}}\\leq2\\sqrt{\\frac{a}{b}}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Proof. The assumption implies that $\\mathbb{P}\\left(Z\\ge t\\right)\\le\\operatorname*{min}\\left(1,\\exp(a-b t^{2})\\right)$ for all $t\\geq0$ . Then, ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathbb{E}[Z]=\\int_{0}^{+\\infty}\\mathbb{P}(Z\\geq t)d t=\\sqrt{\\frac{a}{b}}+\\int_{\\sqrt{{a}/b}}^{+\\infty}\\exp\\left(-(b t^{2}-a)\\right)d t.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Since $(u-v)^{2}\\leq u^{2}-v^{2}$ for $0\\leq v\\leq u$ , we have ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\int_{\\sqrt{a/b}}^{+\\infty}\\exp\\left(-(b t^{2}-a)\\right)d t\\leq\\int_{\\sqrt{a/b}}^{+\\infty}\\exp\\left(-b\\left(t-\\sqrt{a/b}\\right)^{2}\\right)d t=\\frac{\\sqrt{\\pi}}{2\\sqrt{b}}\\leq\\frac{\\sqrt{\\pi}}{2}\\sqrt{\\frac{a}{b}},\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "where the last inequality uses $a\\geq1$ . Combining Eq. (8) and Eq. (9) yields the result. ", "page_idx": 11}, {"type": "text", "text": "A.3 Application to the empirical risk ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The results from the last two sections are applied to the CRPS and the empirical risk. ", "page_idx": 12}, {"type": "text", "text": "Proposition 5. Under Assumption $^{\\,l}$ , for all $\\theta\\:\\in\\:\\Theta$ , the random variable $S(F_{\\theta,X},Y)$ is $\\beta$ -subGaussian with $\\beta=\\sqrt{2(\\beta_{1}^{2}+\\beta_{2}^{2})}$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. We use following alternative representation of the CRPS, see Gneiting and Raftery (2007): for fixed $F\\in{\\mathcal{P}}_{1}(\\mathbb{R})$ and $y\\in\\mathbb R$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\nS(F,y)=\\mathbb{E}[|Z-y|]-\\frac{1}{2}\\mathbb{E}[|Z^{\\prime}-Z|]\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $Z,Z^{\\prime}$ denote independent random variables with distribution $F$ . The triangle inequality then implies ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S(F,y)=\\mathbb{E}[|Z-y|]-\\frac{1}{2}\\mathbb{E}[|Z^{\\prime}-Z|]\\leq|y|+\\frac{1}{2}\\mathbb{E}[|Z+Z|-|Z^{\\prime}-Z|]}\\\\ {\\leq|y|+\\mathbb{E}[|Z|]=|y|+m_{1}(F),~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "with $m_{1}(F)$ the absolute moment of $F$ . We deduce that, for all $\\theta\\in\\Theta$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\nS(F_{\\theta,X},Y)\\le|Y|+m_{1}(F_{\\theta,X})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where, by Assumption 1, $|Y|$ and $m_{1}(F_{\\theta,X})$ are sub-Gaussian with parameter $\\beta_{1}$ and $\\beta_{2}$ respectively. Then, Proposition 3 implies that $S(F_{\\theta,X},Y)$ is $\\beta$ -sub-Gaussian with $\\beta=\\sqrt{2(\\beta_{1}^{2}+\\beta_{2}^{2})}$ . \u53e3 ", "page_idx": 12}, {"type": "text", "text": "Corollary 2. Under Assumption $^{\\,l}$ , for all $\\theta\\in\\Theta$ , the empirical risk $\\hat{\\mathcal{R}}_{n}(F_{\\theta})$ satisfies the concentration inequality ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\hat{\\mathcal{R}}_{n}(F_{\\theta})-\\mathcal{R}(F_{\\theta})\\right|\\geq t\\right)\\leq2\\exp\\left(-\\frac{n t^{2}}{2\\beta^{2}}\\right),\\quad f o r\\,a l l\\,t\\geq0\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "with $\\beta=\\sqrt{2(\\beta_{1}^{2}+\\beta_{2}^{2})}$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. The random variables $Z_{\\theta,i}=S(F_{\\theta,X_{i}},Y_{i})$ , $1\\leq i\\leq n$ , are i.i.d. with expectation ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}[Z_{\\theta,i}]=\\mathbb{E}[S(F_{\\theta,X_{i}},Y_{i})]=\\mathcal{R}(F_{\\theta})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and empirical mean ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}Z_{\\theta,i}=\\frac{1}{n}\\sum_{i=1}^{n}S(F_{\\theta,X_{i}},Y_{i})=\\hat{\\mathcal{R}}_{n}(F_{\\theta}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "By Proposition 5, $Z_{\\theta,i}$ is $\\beta$ -sub-Gaussian with $\\beta=\\sqrt{2(\\beta_{1}^{2}+\\beta_{2}^{2})}$ . We can then apply the general Hoeffding\u2019s inequality (Proposition 4) and deduce the result. ", "page_idx": 12}, {"type": "text", "text": "B Proof of the results of Section 3.1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Before proving Theorem 1, we introduce preliminary results about the Lipschitz regularity of the CRPS and the empirical risk. ", "page_idx": 12}, {"type": "text", "text": "Lemma 2. For all $F_{1},F_{2}\\in\\mathcal{P}_{1}(\\mathbb{R})$ and $y\\in\\mathbb{R}$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n|S(F_{1},y)-S(F_{2},y)|\\le2W_{1}(F_{1},F_{2})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "This Lemma states that the CRPS is 2-Lipschitz in the first variable with respect to the Wasserstein distance $W_{1}$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. By the definition (3) of the CRPS, ", "page_idx": 12}, {"type": "equation", "text": "$$\nS(F_{1},y)-S(F_{2},y)=\\int_{\\mathbb{R}}\\Big((\\mathbb{1}_{\\{y\\leq z\\}}-F_{1}(z))^{2}-(\\mathbb{1}_{\\{y\\leq z\\}}-F_{2}(z))^{2}\\Big)d z.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Using $a^{2}-b^{2}=(a-b)(a+b)$ , we get ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\vert S(F_{1},y)-S(F_{2},y)\\vert\\le\\displaystyle\\int_{\\mathbb{R}}\\vert F_{1}(z)-F_{2}(z)\\vert\\vert F_{1}(z)+F_{2}(z)-2\\mathbb{1}_{\\{z\\le y\\}}\\vert d z}&{}\\\\ {\\le2\\displaystyle\\int_{\\mathbb{R}}\\vert F_{1}(z)-F_{2}(z)\\vert d z.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We recognize the expression of the Wasserstein distance of order 1 for probability measures on $\\mathbb{R}$ as the $L^{1}$ -distance between their cdf, i.e. the formula ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}}|F_{1}(z)-F_{2}(z)|d z=W_{1}(F_{1},F_{2}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "see Bobkov and Ledoux (2019). The result follows. ", "page_idx": 13}, {"type": "text", "text": "We can deduce that, under Assumption 2, the theoretical and empirical risks are also Lipschitz continuous. ", "page_idx": 13}, {"type": "text", "text": "Proposition 6. Under Assumption 2, for all $\\theta_{1},\\theta_{2}\\in\\Theta$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n|\\mathcal{R}(F_{\\theta_{1}})-\\mathcal{R}(F_{\\theta_{2}})|\\leq2L\\|\\theta_{1}-\\theta_{2}\\|\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\hat{\\mathcal{R}}_{n}(F_{\\theta_{1}})-\\hat{\\mathcal{R}}_{n}(F_{\\theta_{2}})|\\le2L\\|\\theta_{1}-\\theta_{2}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. Using the definition of the theoretical risk and the Lipschitz continuity of the CRPS, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mathcal{R}(F_{\\theta_{1}})-\\mathcal{R}(F_{\\theta_{2}})|=|\\mathbb{E}[S(F_{\\theta_{1},X},Y)-S(F_{\\theta_{2},X},Y)]|}\\\\ &{\\phantom{\\quad\\quad}\\leq\\mathbb{E}[|S(F_{\\theta_{1},X},Y)-S(F_{\\theta_{2},X},Y)|]}\\\\ &{\\phantom{\\quad\\quad=}\\leq2\\mathbb{E}[W_{1}(F_{\\theta_{1},X},F_{\\theta_{2},X})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then, Assumption 2 implies ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathcal{R}(F_{\\theta_{1}})-\\mathcal{R}(F_{\\theta_{2}})||\\leq2L\\|\\theta_{1}-\\theta_{2}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Similarly for the empirical risk, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\hat{\\mathcal{R}}_{n}(F_{\\theta_{1}})-\\hat{\\mathcal{R}}_{n}(F_{\\theta_{2}})|\\leq\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\Big|S(F_{\\theta_{1},X_{i}},Y_{i})-S(F_{\\theta_{2},X_{i}},Y_{i})\\Big|}\\\\ {\\displaystyle\\qquad\\qquad\\leq\\frac{2}{n}\\displaystyle\\sum_{i=1}^{n}W_{1}(F_{\\theta_{1},X_{i}},F_{\\theta_{2},X_{i}})}\\\\ {\\displaystyle\\qquad\\leq2L\\|\\theta_{1}-\\theta_{2}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We are now ready for the proof of Theorem 1. ", "page_idx": 13}, {"type": "text", "text": "Proof of Theorem $^{\\,l}$ . By compactness of $\\Theta$ and continuity of $\\theta\\mapsto\\mathcal{R}(\\theta)$ , the theoretical risk reaches a minimum on $\\Theta$ and we can define $\\begin{array}{r}{\\theta^{*}=\\arg\\operatorname*{min}_{\\theta\\in\\Theta}\\mathcal{R}(F_{\\theta})}\\end{array}$ . Similarly, $\\begin{array}{r}{\\hat{\\theta}_{n}=\\arg\\operatorname*{min}_{\\theta\\in\\Theta}\\mathcal{\\hat{R}}_{n}(F_{\\theta})}\\end{array}$ is well defined. The estimation error can then be decomposed into three terms ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}(F_{\\hat{\\theta}_{n}})-\\operatorname*{inf}_{\\theta\\in\\Theta}\\mathcal{R}(F_{\\theta})}\\\\ &{=\\mathcal{R}(F_{\\hat{\\theta}_{n}})-\\mathcal{R}(F_{\\theta^{*}})}\\\\ &{=\\Big(\\mathcal{R}(F_{\\hat{\\theta}_{n}})-\\hat{\\mathcal{R}}_{n}(F_{\\hat{\\theta}_{n}})\\Big)+\\Big(\\hat{\\mathcal{R}}_{n}(F_{\\hat{\\theta}_{n}})-\\hat{\\mathcal{R}}_{n}(F_{\\theta^{*}})\\Big)+\\Big(\\hat{\\mathcal{R}}_{n}(F_{\\theta^{*}})-\\mathcal{R}(F_{\\theta^{*}})\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By definition of $\\widehat{\\theta}_{n}$ , the second term is non-positive and we deduce ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left|\\mathcal{R}(F_{\\hat{\\theta}_{n}})-\\operatorname*{inf}_{\\theta\\in\\Theta}\\mathcal{R}(F_{\\theta})\\right|\\leq2\\operatorname*{sup}_{\\theta\\in\\Theta}\\left|\\hat{\\mathcal{R}}_{n}\\left(F_{\\theta}\\right)-\\mathcal{R}\\left(F_{\\theta}\\right)\\right|.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since $\\Theta$ is compact, it can be included in some centered closed Euclidean ball $\\bar{B}_{{R}}$ with radius $R>0$ . Therefore, for all $\\epsilon\\le R$ , there exists an $\\epsilon$ -net $\\Theta_{\\epsilon}$ of $\\Theta$ such that $\\mathrm{card}(\\Theta_{\\epsilon})\\leq\\left(\\frac{3R}{\\epsilon}\\right)^{K}$ , see Devroye et al. (1996). The term $\\epsilon_{}$ -net means that, for all $\\theta\\in\\Theta$ , there exists $\\theta_{\\epsilon}\\in\\Theta_{\\epsilon}$ such that $\\lVert{\\boldsymbol{\\theta}}-{\\boldsymbol{\\theta}}_{\\epsilon}\\rVert\\leq\\epsilon$ . Now, for all $\\theta\\in\\Theta$ , we introduce the decomposition ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\hat{\\mathcal{R}}_{n}\\left(F_{\\theta}\\right)-\\mathcal{R}\\left(F_{\\theta}\\right)\\right|\\leq\\left|\\hat{\\mathcal{R}}_{n}\\left(F_{\\theta}\\right)-\\hat{\\mathcal{R}}_{n}\\left(F_{\\theta_{\\epsilon}}\\right)\\right|+\\left|\\hat{\\mathcal{R}}_{n}\\left(F_{\\theta_{\\epsilon}}\\right)-\\mathcal{R}\\left(F_{\\theta_{\\epsilon}}\\right)\\right|+\\left|\\mathcal{R}\\left(F_{\\theta_{\\epsilon}}\\right)-\\mathcal{R}\\left(F_{\\theta}\\right)\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By the Lipschitz properties stated in Proposition 6, the first and third terms are bounded from above by $2L\\epsilon$ . Therefore, we deduce ", "page_idx": 14}, {"type": "equation", "text": "$$\n2\\operatorname*{sup}_{\\theta\\in\\Theta}\\left\\vert\\hat{\\mathcal{R}}_{n}\\left(F_{\\theta}\\right)-\\mathcal{R}\\left(F_{\\theta}\\right)\\right\\vert\\leq8L\\epsilon+2\\operatorname*{sup}_{\\theta\\in\\Theta_{\\epsilon}}\\left\\vert\\hat{\\mathcal{R}}_{n}\\left(F_{\\theta}\\right)-\\mathcal{R}\\left(F_{\\theta}\\right)\\right\\vert,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which implies, for $t\\geq16L\\epsilon$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(2\\operatorname*{sup}_{\\theta\\in\\Theta}\\left|\\hat{\\mathcal{R}}_{n}\\left(F_{\\theta}\\right)-\\mathcal{R}\\left(F_{\\theta}\\right)\\right|\\geq t\\right)\\leq\\mathbb{P}\\left(\\operatorname*{sup}_{\\theta\\in\\Theta_{\\epsilon}}\\left|\\hat{\\mathcal{R}}_{n}\\left(F_{\\theta}\\right)-\\mathcal{R}\\left(F_{\\theta}\\right)\\right|\\geq t/4\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We then use Corollary 2 stating that, for all $\\theta\\in\\Theta$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\hat{\\mathcal{R}}_{n}(F_{\\theta})-\\mathcal{R}(F_{\\theta})\\right|\\ge t\\right)\\le2\\exp\\left(-\\frac{n t^{2}}{2\\beta^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "with $\\beta\\;=\\;\\sqrt{2(\\beta_{1}^{2}+\\beta_{2}^{2})}$ . The union bound and the inequality $\\begin{array}{r}{\\mathrm{card}(\\Theta_{\\epsilon})\\ \\leq\\ \\left(\\frac{3R}{\\epsilon}\\right)^{K}}\\end{array}$ imply, for $t\\geq16L\\epsilon$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\displaystyle\\left(2\\operatorname*{sup}_{\\theta\\in\\Theta}\\left|\\hat{\\mathcal{R}}_{n}\\left(F_{\\theta}\\right)-\\mathcal{R}\\left(F_{\\theta}\\right)\\right|\\geq t\\right)\\leq\\displaystyle\\sum_{\\theta\\in\\Theta_{\\epsilon}}\\mathbb{P}\\left(\\left|\\hat{\\mathcal{R}}_{n}\\left(F_{\\theta}\\right)-\\mathcal{R}\\left(F_{\\theta}\\right)\\right|\\geq t/4\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq2\\,\\left(\\frac{3R}{\\epsilon}\\right)^{K}\\,\\exp\\left(-\\frac{n t^{2}}{32\\beta^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Setting $\\epsilon=3R/n$ in this inequality and using Eq. (12), we deduce ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\mathcal{R}(F_{\\hat{\\theta}_{n}})-\\operatorname*{inf}_{\\theta\\in\\Theta}\\mathcal{R}(F_{\\theta})\\right|\\geq t\\right)\\leq2n^{K}\\,\\exp\\left(-\\frac{n t^{2}}{32\\beta^{2}}\\right),\\quad t\\geq\\frac{48L R}{n}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The right hand side is equal to $\\delta$ for $t=\\sqrt{32\\beta^{2}\\log(2n^{K}/\\delta)/n}$ , yielding the result. Note that we need this specific choice of $t$ to satisfy $t\\geq48L R/n$ , or equivalently $n\\log(2n^{K}/\\delta)\\geq(48L R)^{2}/c_{\\beta}$ with $c_{\\beta}=64(\\beta_{1}^{2}+\\beta_{2}^{2})$ . \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Proof of Corollary $^{\\,I}$ . Setting $a=\\log(2n^{K})$ and $b=n/c_{\\beta}$ , Eq. (14) can be rewritten as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\mathcal{R}(F_{\\hat{\\theta}_{n}})-\\operatorname*{inf}_{\\theta\\in\\Theta}\\mathcal{R}(F_{\\theta})\\right|\\geq t\\right)\\leq\\exp\\left(a-b t^{2}\\right),\\quad t\\geq48L R/n.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For $t\\leq\\sqrt{a/b}$ , the inequality is trivial because the right hand side is larger than 1. We deduce that the inequality holds for all $t\\geq0$ as soon as $48L R/n\\leq\\sqrt{a/b}$ which is equivalent to $n\\log(2n^{K})\\geq$ $(48L R)^{2}/c_{\\beta}$ , which holds for $n$ large enough. Then we can apply Lemma 1 and deduce the upper bound $2{\\sqrt{a/b}}$ for the expectation of the estimation error. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "C Proof of the results of Section 3.2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof of Theorem 2. A straightforward adaptation of the proof of Proposition 5 shows that Assumption 3 implies that the random variables $S(\\hat{F}_{n,X}^{m},Y)$ , $m\\,=\\,1,\\ldots,M$ , are $\\beta.$ -sub-Gaussian conditionally on $\\mathcal{D}_{n}$ with $\\beta=\\sqrt{2(\\beta_{1}^{2}+\\beta_{n}^{2})}$ . Then, similarly as in Corollary 2, Hoeffding inequality implies ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\hat{\\mathcal{R}}_{N}^{\\prime}(\\hat{F}_{n}^{m})-\\mathcal{R}(\\hat{F}_{n}^{m})\\right|\\geq t\\;\\Big|\\;\\mathcal{D}_{n}\\right)\\leq2\\exp\\left(-\\frac{N t^{2}}{4(\\beta_{1}^{2}+\\beta_{n}^{2})}\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for all $t\\geq0$ and $m=1,\\dotsc,M$ . With a similar reasoning as in Eq. (11), the regret in model selection is bounded from above by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}(\\hat{F}_{n}^{\\hat{m}})-\\underset{1\\leq m\\leq M}{\\operatorname*{min}}\\mathcal{R}(\\hat{F}_{n}^{m})}\\\\ &{=\\mathcal{R}(\\hat{F}_{n}^{\\hat{m}})-\\mathcal{R}(\\hat{F}_{n}^{m^{*}})}\\\\ &{=\\bigg(\\mathcal{R}(\\hat{F}_{n}^{\\hat{m}})-\\hat{\\mathcal{R}}_{N}^{\\prime}(\\hat{F}_{n}^{\\hat{m}})\\bigg)+\\bigg(\\hat{\\mathcal{R}}_{N}^{\\prime}(\\hat{F}_{n}^{\\hat{m}})-\\hat{\\mathcal{R}}_{N}^{\\prime}(\\hat{F}_{n}^{m^{*}})\\bigg)+\\bigg(\\hat{\\mathcal{R}}_{N}^{\\prime}(\\hat{F}_{n}^{m^{*}})-\\mathcal{R}(\\hat{F}_{n}^{m^{*}})\\bigg)}\\\\ &{\\leq2\\underset{1\\leq m\\leq M}{\\operatorname*{max}}\\left|\\hat{\\mathcal{R}}_{N}^{\\prime}(\\hat{F}_{n}^{m})-\\mathcal{R}(\\hat{F}_{n}^{m})\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This together with the union bound and Eq. (15) implies ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\mathcal{R}(\\hat{F}_{n}^{\\hat{m}})-\\operatorname*{min}_{1\\leq m\\leq M}\\mathcal{R}(\\hat{F}_{n}^{m})\\geq t\\;\\Big|\\;\\mathcal{D}_{n}\\right)\\leq\\mathbb{P}\\left(\\operatorname*{max}_{1\\leq m\\leq M}\\Big|\\hat{\\mathcal{R}}_{N}^{\\prime}(\\hat{F}_{n}^{m})-\\mathcal{R}(\\hat{F}_{n}^{m})\\Big|\\geq\\frac{t}{2}\\;\\Big|\\;\\mathcal{D}_{n}\\right)}\\\\ {\\leq\\sum_{m=1}^{M}\\mathbb{P}\\left(\\Big|\\hat{\\mathcal{R}}_{N}^{\\prime}(\\hat{F}_{n}^{m})-\\mathcal{R}(\\hat{F}_{n}^{m})\\Big|\\geq\\frac{t}{2}\\;\\Big|\\;\\mathcal{D}_{n}\\right)}\\\\ {\\leq2M\\exp\\left(-\\frac{N t^{2}}{16(\\beta_{1}^{2}+\\beta_{n}^{2})}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The right hand side is equal to $\\delta$ for $t=4\\sqrt{c_{n}\\log(2M/\\delta)/N}$ , which proves the first claim. The second claim follows by an application of Lemma 1 with $a=\\log(2M)$ and $b=N/(16c_{n})$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "For the proof of Theorem 3, we need the following Lemma. ", "page_idx": 15}, {"type": "text", "text": "Lemma 3. Consider cumulative distribution functions $(G^{m})_{1\\leq m\\leq M}$ and, for $\\lambda\\in\\Lambda$ , the convex aggregation $\\begin{array}{r}{G^{\\lambda}=\\sum_{m=1}^{M}\\lambda_{m}G^{m}}\\end{array}$ . Then the following Lipschitz property is satisfied: for all $\\lambda_{1},\\lambda_{2}\\in$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\nW_{1}(G^{\\lambda_{1}},G^{\\lambda_{2}})\\leq\\operatorname*{max}_{1\\leq m\\leq M}m_{1}(G^{m})\\,\\|\\lambda_{1}-\\lambda_{2}\\|_{1}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. In dimension 1, the Wasserstein distance is given by ", "page_idx": 15}, {"type": "equation", "text": "$$\nW_{1}(G^{\\lambda_{1}},G^{\\lambda_{2}})=\\int_{\\mathbb{R}}|G^{\\lambda_{1}}(y)-G^{\\lambda_{2}}(y)|\\mathrm{d}y.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By definition of $G^{\\lambda}$ , we deduce ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{1}(G^{\\lambda_{1}},G^{\\lambda_{2}})=\\displaystyle\\int_{\\mathbb{R}}|\\sum_{m=1}^{M}(\\lambda_{1,m}-\\lambda_{2,m})(G^{m}(y)-\\mathbb{1}_{\\{y\\geq0\\}})|\\mathrm{d}y}\\\\ &{\\phantom{\\sum_{m=1}^{M}}\\displaystyle\\sum_{m=1}^{M}|\\lambda_{1,m}-\\lambda_{2,m}|\\int_{\\mathbb{R}}|G^{m}(y)-\\mathbb{1}_{\\{y\\geq0\\}}|\\mathrm{d}y}\\\\ &{\\phantom{\\sum_{m=1}^{M}}=\\displaystyle\\sum_{m=1}^{M}|\\lambda_{1,m}-\\lambda_{2,m}|\\,m_{1}(G^{m})}\\\\ &{\\phantom{\\sum_{m=1}^{M}}\\sum_{\\kappa\\geq M}m_{1}(G^{m})\\,\\|\\lambda_{1}-\\lambda_{2}\\|_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In the first line, we use that $\\begin{array}{r}{\\sum_{m=1}^{M}(\\lambda_{1,m}-\\lambda_{2,m})=0}\\end{array}$ so that we can introduce the indicator function $\\mathbb{1}_{\\{y\\geq0\\}}$ . In the third line, we use $\\begin{array}{r}{\\int_{\\mathbb{R}}|G^{m}(y)-\\mathbb{1}_{\\{y\\geq0\\}}|\\mathrm{d}y=W_{1}(G^{m},\\delta_{0})=m_{1}(G^{m})}\\end{array}$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Proof of Theorem 3. We work conditionally on $\\mathcal{D}_{n}$ so that $\\hat{F}_{n}^{m},\\,1\\;\\le\\;m\\;\\le\\;M$ , can be seen as deterministic. The empirical risk is computed on the validation set $\\mathcal{D}_{N}^{\\prime}$ with cardinal $N$ . Furthermore, Assumption 3 implies that, conditionally on $\\ensuremath{\\mathcal \u1e0a D \u1e0c }_{n},Y$ and $\\hat{F}_{n}^{\\lambda}$ satisfy Assumption 1 with $\\Theta$ replaced by $\\Lambda$ and constant $\\beta_{2}$ replaced by $\\beta_{n}$ . Lemma 3 implies that, co\u221anditionally on $\\mathcal{D}_{n}$ , $F_{x}^{\\lambda}$ satisfy Assumption 2 with $\\Theta$ replaced by $\\Lambda_{M}$ and constant $L$ replaced by $\\sqrt{M}\\operatorname*{max}_{1\\leq m\\leq M}m_{1}(\\hat{F}_{n,x}^{m})$ . As a consequence, we can apply Theorem 1 and deduce the result. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "D Proofs and additional results for Section 3.3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 Proof of Theorem 4 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Without sub-Gaussian assumptions, we cannot use Hoeffding inequality. Alternatively, we use a consequence of Rosenthal inequality providing a simple control of moments. For a reference, see Petrov (1995, Theorems 2.9 and 2.10). ", "page_idx": 16}, {"type": "text", "text": "Proposition 7. Let $Z$ $;Z_{1},\\ldots,Z_{n}$ be i.i.d. random variables such that $m_{p}(Z)=\\mathbb{E}[|Z\\mathrm{-}\\mathbb{E}[Z]|^{p}]<\\infty$ for some $p\\geq2$ . Then, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\!\\left[\\Big\\lvert\\frac{1}{n}\\sum_{i=1}^{n}\\big(Z_{i}-\\mathbb{E}[Z_{i}]\\big)\\Big\\rvert^{p}\\right]\\leq c(p)m_{p}(Z)n^{-p/2},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with $c(p)>0$ depending only on $p$ . ", "page_idx": 16}, {"type": "text", "text": "We deduce a simple moment bound for the empirical risk. ", "page_idx": 16}, {"type": "text", "text": "Proposition 8. Under Assumption $^{4}$ , we have, for all $\\theta\\in\\Theta$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left|\\hat{\\mathcal{R}}_{n}(F_{\\theta})-\\mathcal{R}(F_{\\theta})\\right|^{p}\\right]\\leq c^{\\prime}(p)D n^{-p/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with $c^{\\prime}(p)=4^{p}c(p)$ depending only on $p$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. Eq. (10) together with the upper bound $|a+b|^{p}\\leq2^{p-1}(|a|^{p}+|b|^{p})$ imply ", "page_idx": 16}, {"type": "equation", "text": "$$\n|S(F_{\\theta,X},Y)|^{p}\\leq\\left(|Y|+m_{1}(F_{\\theta,X})\\right)^{p}\\leq2^{p-1}\\big(|Y|^{p}+m_{1}(F_{\\theta,X})^{p}\\big).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Taking the expectation and using Assumption 4, we deduce, with the notation $Z_{\\theta}=S(F_{\\theta,X},Y)$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[|Z_{\\theta}|^{p}]\\le2^{p-1}\\big(|Y|^{p}+m_{1}(F_{\\theta,X})^{p}\\big)\\le2^{p}D.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By Jensen inequality, this implies the upper bound ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\!\\left[|Z_{\\theta}-\\mathbb{E}[Z_{\\theta}]|^{p}\\right]\\leq4^{p}D.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The centered empirical risk can be written as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{\\hat{R}}_{n}(F_{\\theta})-\\mathcal{R}(F_{\\theta})=\\frac{1}{n}\\sum_{i=1}^{n}\\left(Z_{\\theta,i}-\\mathbb{E}[Z_{\\theta,i}]\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with $Z_{\\theta,i}\\,=\\,S(F_{\\theta,X_{i}},Y_{i})$ , $1\\leq\\,i\\leq\\,n$ , i.i.d. random variables. Then the result follows from an application of Proposition 7. ", "page_idx": 16}, {"type": "text", "text": "Proof of Theorem 4. We use the same notation as in the proof of Theorem 1. The beginning of the proof follows exactly the same lines: according to Eq. 12-13, for all $\\epsilon$ -net $\\Theta_{\\epsilon}\\subset\\Theta$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|\\mathcal{R}(F_{\\hat{\\theta}_{n}})-\\operatorname*{inf}_{\\theta\\in\\Theta}\\mathcal{R}(F_{\\theta})\\right|\\leq8L\\epsilon+2\\operatorname*{sup}_{\\theta\\in\\Theta_{\\epsilon}}\\left|\\hat{\\mathcal{R}}_{n}\\left(F_{\\theta}\\right)-\\mathcal{R}\\left(F_{\\theta}\\right)\\right|.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This implies ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big[\\Big|\\mathcal{R}(F_{\\hat{\\theta}_{n}})-\\operatorname*{inf}_{\\theta\\in\\Theta}\\mathcal{R}(F_{\\theta})\\Big|^{p}\\Big]\\leq2^{p-1}\\Big((8L\\epsilon)^{p}+2^{p}\\mathbb{E}\\Big[\\operatorname*{sup}_{\\theta\\in\\Theta_{\\epsilon}}\\Big|\\hat{\\mathcal{R}}_{n}\\left(F_{\\theta}\\right)-\\mathcal{R}\\left(F_{\\theta}\\right)\\Big|^{p}\\Big]\\Big).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The expectation in the right hand side is bounded from above by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\Big[\\underset{\\theta\\in\\Theta_{\\epsilon}}{\\operatorname*{sup}}\\Big|\\hat{\\mathcal{R}}_{n}\\left(F_{\\theta}\\right)-\\mathcal{R}\\left(F_{\\theta}\\right)\\Big|^{p}\\Big]\\leq\\underset{\\theta\\in\\Theta_{\\epsilon}}{\\sum}\\mathbb{E}\\Big[\\Big|\\hat{\\mathcal{R}}_{n}\\left(F_{\\theta}\\right)-\\mathcal{R}\\left(F_{\\theta}\\right)\\Big|^{p}\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq(3R)^{K}\\epsilon^{-K}c^{\\prime}(p)D n^{-p/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last inequality follows from Proposition 8 and the bound $\\mathrm{card}(\\Theta_{\\epsilon})\\le(3R/\\epsilon)^{K}$ . Hence we deduce ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big[\\Big|\\mathcal{R}(F_{\\hat{\\theta}_{n}})-\\operatorname*{inf}_{\\theta\\in\\Theta}\\mathcal{R}(F_{\\theta})\\Big|^{p}\\Big]\\leq2^{p-1}\\Big((8L\\epsilon)^{p}+2^{p}(3R)^{K}\\epsilon^{-K}c^{\\prime}(p)D n^{-p/2}\\Big).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Minimizing the right hand side with respect to $\\epsilon$ according to Lemma 4 below and taking the $p$ -th root, we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big[\\Big|\\mathcal{R}(F_{\\hat{\\theta}_{n}})-\\operatorname*{inf}_{\\theta\\in\\Theta}\\mathcal{R}(F_{\\theta})\\Big|^{p}\\Big]^{1/p}\\leq C n^{-p/(2(p+K))}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the constant $C=C(K,p,L,D,R)$ does not depend on $n$ and can be made explicit thanks to Lemma 4. Finally, Jensen\u2019s inequality yields the result. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Lemma 4. For all $a,b>0$ and $p,q>0$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\epsilon>0}\\left(a\\epsilon^{p}+b\\epsilon^{-q}\\right)=C_{p,q}a^{\\frac q{p+q}}b^{\\frac p{p+q}}\\,,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. A straightforward analysis of the function $\\epsilon\\mapsto a\\epsilon^{p}+b\\epsilon^{-q}$ shows that its derivative vanishes at $\\begin{array}{r}{\\epsilon=\\left(\\frac{b q}{a p}\\right)^{\\frac{\\tilde{1}}{p+q}}}\\end{array}$ where the minimum is reached. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "D.2 Additional results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We provide some additional results to Theorem 4 from Section 3.3 regarding model selection and convex aggregation. Our working assumption in this framework is the following. ", "page_idx": 17}, {"type": "text", "text": "Assumption 5. For $p\\geq2$ , ", "page_idx": 17}, {"type": "equation", "text": "$\\|Y\\|_{L^{p}}^{p}\\le D_{;}$ ", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|m_{1}(\\hat{F}_{n,X}^{m})\\|_{L^{p}}^{p}\\leq D_{n}=D(\\mathcal{D}_{n})\\,f o r\\,a l l\\,m=1,\\dots,M.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For instance, one can show with a reasoning similar to Proposition 1 that for the EMOS and DRN models, Assumption 5 holds as soon as $Y$ and $F$ have a finite moment of order $p$ and $X$ remains bounded. For the KNN and DRF models, a reasoning similar to Proposition 2 shows that the assumption holds with $D_{n}=\\operatorname*{max}_{1\\leq i\\leq n}|Y_{i}|^{p}$ . ", "page_idx": 17}, {"type": "text", "text": "Model selection. Here is our result on model selection under moment assumption only. ", "page_idx": 17}, {"type": "text", "text": "Theorem 5. Under Assumption $^{5}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big[\\mathcal{R}(\\hat{F}_{n}^{\\hat{m}})-\\operatorname*{min}_{1\\leq m\\leq M}\\mathcal{R}(\\hat{F}_{n}^{m})\\Big|\\mathcal{D}_{n}\\Big]\\leq2\\big(c^{\\prime}(p)\\operatorname*{max}(D,D_{n})M\\big)^{1/p}N^{-1/2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with $c^{\\prime}(p)$ the constant from Proposition 8. ", "page_idx": 17}, {"type": "text", "text": "Before proving the Theorem, we establish the following result. ", "page_idx": 17}, {"type": "text", "text": "Proposition 9. Under Assumption 5, we have, for all $1\\leq m\\leq M$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left|\\hat{\\mathcal{R}}_{N}^{'}(\\hat{F}_{n}^{m})-\\mathcal{R}(\\hat{F}_{n}^{m})\\right|^{p}\\,\\Big|\\,\\mathcal{D}_{n}\\right]\\leq c^{\\prime}(p)\\operatorname*{max}(D,D_{n})N^{-p/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. We work conditionally on $\\mathcal{D}_{n}$ so that $\\hat{F}_{n}^{m}$ can be seen as a deterministic quantity. The empirical risk is computed on the validation set $\\mathcal{D}_{N}^{\\prime}$ with cardinal $N$ . Furthermore, Assumption 5 implies that, conditionally on $\\mathcal{D}_{n}$ , $Y$ and $\\hat{F}_{n}^{m}$ satisfy 4 with constant $D$ replaced by $\\operatorname*{max}(D,D_{n})$ . Given these remarks, Proposition 9 follows by an application of Proposition 8 where $F_{\\theta}$ is replaced by $\\hat{F}_{n}^{m}$ , $\\hat{\\mathscr{R}}_{n}$ is replaced by $\\hat{\\mathcal{R}}_{N}^{\\prime}$ and the expectation by the conditional expectation given $\\mathcal{D}_{n}$ . \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Proof of Theorem 5. Thanks to Eq. (16), ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\hat{F}_{n}^{\\hat{m}})-\\operatorname*{min}_{1\\leq m\\leq M}\\mathcal{R}(\\hat{F}_{n}^{m})\\leq2\\operatorname*{max}_{1\\leq m\\leq M}\\left|\\hat{\\mathcal{R}}_{N}^{\\prime}(\\hat{F}_{n}^{m})-\\mathcal{R}(\\hat{F}_{n}^{m})\\right|,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "whence we deduce ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\Big[\\Big|\\mathcal{R}(\\hat{F}_{n}^{\\hat{m}})-\\operatorname*{min}_{1\\leq m\\leq M}\\mathcal{R}(\\hat{F}_{n}^{m})\\Big|^{p}\\Big|\\mathcal{D}_{n}\\Big]\\leq2^{p}\\mathbb{E}\\Big[\\operatorname*{max}_{1\\leq m\\leq M}\\Big|\\hat{\\mathcal{R}}_{N}^{\\prime}(\\hat{F}_{n}^{m})-\\mathcal{R}(\\hat{F}_{n}^{m})\\Big|^{p}\\Big|\\mathcal{D}_{n}\\Big]}&{}\\\\ {\\leq2^{p}\\displaystyle\\sum_{m=1}^{M}\\mathbb{E}\\Big[\\Big|\\hat{\\mathcal{R}}_{N}^{\\prime}(\\hat{F}_{n}^{m})-\\mathcal{R}(\\hat{F}_{n}^{m})\\Big|^{p}\\Big|\\mathcal{D}_{n}\\Big]}&{}\\\\ {\\leq2^{p}M c^{\\prime}(p)\\operatorname*{max}(D,D_{n})N^{-p/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The result follows by Jensen\u2019s inequality. ", "page_idx": 18}, {"type": "text", "text": "Convex aggregation. Here is our main result on convex aggregation under moment assumption only. ", "page_idx": 18}, {"type": "text", "text": "Theorem 6. Under Assumption $^{5}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathcal{R}(\\hat{F}_{n}^{\\hat{\\lambda}})-\\operatorname*{inf}_{\\lambda\\in\\Lambda_{M}}\\mathcal{R}(\\hat{F}_{n}^{\\lambda})~\\Big|~\\mathcal{D}_{n}\\right]\\leq C\\left(L^{K}\\operatorname*{max}(D,D_{n})N^{-p/2}\\right)^{1/p+K}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with $L=\\sqrt{M}\\operatorname*{max}_{1\\leq m\\leq M}m_{1}(\\hat{F}_{n}^{m})$ and constant $C>0$ depending only on p and $K$ . ", "page_idx": 18}, {"type": "text", "text": "Proof of Theorem $6$ . The proof follows by an application of Theorem 4 exactly in the same way that Theorem 3 was derived from Theorem 1. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "E Proofs for Section 4 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The following Definition and Lemma about location/scale families are useful for the proof of Section 4. ", "page_idx": 18}, {"type": "text", "text": "Definition 3. Let $F$ be a probability distribution on $\\mathbb{R}$ with zero mean and unit variance. The associated location-scale family is defined as $\\{F_{m,\\sigma},\\ m\\in\\mathbb{R},\\ \\sigma>0\\}$ where $F_{m,\\sigma}$ is the law of $m+\\sigma Z$ with $Z\\sim F$ . ", "page_idx": 18}, {"type": "text", "text": "Proposition 10 (Chhachhi and Teng 2023). For all $m_{1}$ $,m_{2}\\in\\mathbb{R}$ and $\\sigma_{1},\\sigma_{2}>0,$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nW_{1}(F_{m_{1},\\sigma_{1}},F_{m_{2},\\sigma_{2}})\\leq|m_{1}-m_{2}|+m_{1}(F)\\,|\\sigma_{1}-\\sigma_{2}|.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof of Proposition $^{\\,l}$ . Denote by $F_{m(x;\\theta),\\sigma(x;\\theta)}$ the output distribution of the DRN with parameter $\\theta\\:\\in\\:\\Theta$ and input $x$ , where $m(x;{\\theta})$ and $\\sigma(x;\\theta)$ are the location and scale parameters given by Eq. (7). Because the activation function $g$ is assumed Lipschitz continuous, when $x$ and $\\theta$ remain in bounded sets, the functions $\\theta\\mapsto m(x;\\theta)$ and $\\theta\\mapsto\\sigma(x;\\theta)$ are Lipschitz continuous with a Lipschitz constant bounded by some constant $C$ uniformly in $x$ . Then, Proposition 10 implies that the output distributions satisfy ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{1}(F_{m(x;\\theta_{1}),\\sigma(x;\\theta_{1})},F_{m(x;\\theta_{2}),\\sigma(x;\\theta_{2})})\\leq|m(x;\\theta_{1})-m(x;\\theta_{2})|+m_{1}(F)\\,|\\sigma(x;\\theta_{1})-\\sigma(x;\\theta_{2})|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq C(1+m_{1}(F))\\|\\theta_{1}-\\theta_{2}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for all $\\theta$ in $\\Theta$ compact and $x$ in the support of $X$ bounded. It follows that Assumption 2 is satisfied. Furthermore, this estimation of the Wasserstein distance implies that the absolute moment $m_{1}(F_{\\theta,X})$ remains bounded for all $\\theta\\:\\in\\:\\Theta$ and $x$ in the support of $X$ . Hence $m_{1}(F_{\\theta,X})$ is sub-Gaussian (uniformly in $\\theta$ ) and this proves Assumption 1. For the same reason, the absolute moment $m_{1}(\\hat{F}_{n})=$ $m_{1}(F_{\\hat{\\theta}_{n},X})$ remains bounded and this implies Assumption 3 with $\\beta(\\mathcal{D}_{n})$ constant not depending on the training set. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Proof of Proposition 2. The two models KNN and DRF can be written in the form ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{F}_{n,x}(y)=\\sum_{i=1}^{n}w_{n i}(x)\\mathbb{1}_{\\{Y_{i}\\leq y\\}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for suitable probability weights $(w_{n i}(x))_{1\\leq i\\leq n}$ . Hence the absolute moment of the predictive distribution satisfies ", "page_idx": 18}, {"type": "equation", "text": "$$\nm_{1}(\\hat{F}_{n,x})=\\sum_{i=1}^{n}w_{n i}(x)|Y_{i}|\\leq\\operatorname*{max}_{1\\leq i\\leq n}|Y_{i}|.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We can see that the random variable $m_{1}(\\hat{F}_{n,X})$ remains bounded and is hence $\\beta_{n}$ -sub-Gaussian with $\\beta_{n}=\\operatorname*{max}_{1\\leq i\\leq n}|Y_{i}|$ , which proves Assumption 3. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "F Additional numerical results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The numerical results for the airfoil data is presented in Figure 2 and Table 2. The left plot of Figure 2 corresponds to KNN, and the middle plot to DRF. In both instances, the validation curve exhibits a clear minimum, enabling the selection parameter values $\\widehat{\\mathtt{k}}=3$ and $\\widehat{\\mathrm{mtry}}=5$ associated with CRPS-error of 2.16 and 1.47 on the validation set respectively. Then the models KNN, DRF, MS and CA are tested on the test set, and the right plot of Figure 2 shows the test error evaluated with 100 repetitions. The boxplots reveal a clear difference between the test errors for KNN and DRF in favor of DRF. In this case, MS achieves the same performance as DRF (in fact DRF is systematically selected), and a slight improve is achieved with CA. These results are confirmed by the numerical values in Table 2. ", "page_idx": 19}, {"type": "image", "img_path": "cSfxzCozPU/tmp/03284547c3b80c3f01f9b15717bd8a0ac94f78eba3ad76a3b52053d7d1a4ceeb.jpg", "img_caption": ["Figure 2: airfoil data. Left and middle: selection of k for the KNN algorithm and of mtry for the DRF algorithm by minimization of the validation error. Right: test error evaluated with 100 repetitions for KNN, DRF, model selection (MS) and convex aggregation (CA). "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 2: airfoil data. Mean of the test CRPS and its standard error (in parenthesis) over 100 repetitions. ", "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The main claims presented in the abstract and introduction accurately reflect the contributions and scope of the paper. The abstract outlines the study\u2019s focus on distributional regression and its importance for uncertainty quantification in forecasting. It mentions key methodologies such as fitting parametric models using empirical risk minimization with CRPS. The claims about providing concentration results for estimation error, bounds on expectation, model selection, and convex aggregation via validation error minimization, as well as application to various regression models, are all consistent with the paper\u2019s contributions and scope as described. Additionally, the examples cited (QSAR aquatic toxicity and Airfoil self-noise datasets) demonstrate practical applications of the discussed methodologies. Therefore, the abstract and introduction effectively summarize the paper\u2019s objectives, findings, and relevance. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: The approach is very general and may apply to parametric and non-parametric methods and does not assume any model assumption. Observations are assumed independent and identically distributed which is a very standard assumption in statistical learning. The results are derived under a sub-Gaussian assumption and a weaker moment condition is also considered. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. ", "page_idx": 20}, {"type": "text", "text": "\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. \u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: the different assumptions are clearly stated in the suitable Assumption environment. At the beginning of each Theorem, the relevant assumptions are recalled. Furthermore, the assumptions are discussed and checked on a variety of different examples. All the proofs are provided and carefully explained in the supplementary material. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: All the details of the numerical experiment can be found in the Models and Methodology parts of Section 5. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Data and code are available on the Github repository https://github.com/ ZaouiAmed/Neurips2024_DistributionalRegression. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: All the details of the numerical experiment can be found in the Models and Methodology parts of Section 5 (data description, methods and algorithms used, hyperparameters, optimizers ...) ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: standard error are provided in parenthesis in Table 1 and reveal the statistical significance of the results (e.g. the convex aggregation is significantly better than the other models). ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The main purpose of the paper is theoretical and the numerical experiment is very quick to run so that it is not useful to discuss the computer resource. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Nothing specific to declare. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. ", "page_idx": 23}, {"type": "text", "text": "\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The introduction remind the reader that uncertainty quantification is important for decision making. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: proper citations are provided for data sets and algorithms. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]