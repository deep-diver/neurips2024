[{"type": "text", "text": "Community Detection Guarantees using Embeddings Learned by Node2Vec ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Andrew Davison ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "S. Carlyle Morgan ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Department of Statistics Columbia University New York, NY 10027 ad3395@columbia.edu ", "page_idx": 0}, {"type": "text", "text": "Department of Statistics University of Michigan Ann Arbor, MA 48109 scmorgan@umich.edu ", "page_idx": 0}, {"type": "text", "text": "Owen G. Ward Department of Statistics and Actuarial Science Simon Fraser University Burnaby, British Columbia owen_ward@sfu.ca ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Embedding the nodes of a large network into an Euclidean space is a common objective in modern machine learning, with a variety of tools available. These embeddings can then be used as features for tasks such as community detection/node clustering or link prediction, where they achieve state of the art performance. With the exception of spectral clustering methods, there is little theoretical understanding for commonly used approaches to learning embeddings. In this work we examine the theoretical properties of the embeddings learned by node2vec. Our main result shows that the use of $\\boldsymbol{\\mathrm{k}}$ -means clustering on the embedding vectors produced by node2vec gives weakly consistent community recovery for the nodes in (degree corrected) stochastic block models. We demonstrate this result empirically for both real and simulated networks, and examine how this relates to other embedding tools and machine learning procedures for network data. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Within network science, a widely applicable and important inference task is to understand how the behavior of interactions between different units (nodes) within the network depend on their latent characteristics. This occurs within a wide array of disciplines, from sociological [14] to biological [33] networks. ", "page_idx": 0}, {"type": "text", "text": "One simple and interpretable model for such a task is the stochastic block model (SBM) [20], which assumes that nodes within the network are assigned a discrete community label. Edges between nodes in the network are then formed independently across all pairs of edges, conditional on these community assignments. While such a model is simplistic, various extensions have been proposed. These include the degree corrected SBM (DCSBM), used to handle degree heterogenity [23], and mixed-membership SBMs, used to allow for more complex community structures [4]. These extensions have seen a wide degree of empirical success [26, 28, 3]. ", "page_idx": 0}, {"type": "text", "text": "A restriction of the stochastic block model and its generalizations is the requirement for a discrete community assignment as a latent representation of the units within the network. While the statistical community has previously considered more flexible latent representations [19], over the past decade, there have been significant advancements in general embedding methods for networks. These produce general vector representations of units within a network, and can achieve start-of-the-art performance in downstream tasks for node classification and link prediction. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "An early example of such a method is spectral clustering [37], which constructs an embedding of the nodes in the network from an eigendecomposition of the graph Laplacian. The $k$ smallest non zero eigenvectors provides a $k$ dimensional representation of each of the nodes in the network. This has been shown to allow consistent community recovery [30], however it may not be computationally feasible on the large networks which are now common. More recently, machine learning methods for producing vector representations have sought inspiration from NLP methods and the broader machine learning literature, such as the node2vec algorithm [16], graph convolutional networks [51], graph attention networks [46] and others. There are now a wide class of embedding methods which are available to practitioners which can be applied across a mixture of unsupervised and supervised settings. [8] provides a survey of relatively recent developments and [49] reviews the connection between the embedding procedure and the potential downstream task. ", "page_idx": 1}, {"type": "text", "text": "Embedding methods such as Deepwalk [38] and node2vec [16] consider random walks on the graph, where the probability of such a walk is a function of the embedding of the associated nodes. Given embedding vectors $\\dot{\\varpi}_{u},\\omega_{v}\\in\\mathbb{R}^{d}$ of nodes $u$ and $v$ respectively, from graph $\\mathcal{G}$ with vertex set $\\mathcal{V}$ , the probability of a ran d om walk from node $u$ to node $v$ is modeled as ", "page_idx": 1}, {"type": "equation", "text": "$$\nP(v|u)=\\frac{\\exp(\\langle\\omega_{v},\\widehat{\\omega}_{u}\\rangle)}{\\sum_{l\\in\\mathcal{V}}\\exp(\\langle\\omega_{l},\\widehat{\\omega}_{u}\\rangle)},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\langle x,y\\rangle$ is the inner product of $x$ and $y$ . This leads to a representation of each of the nodes in the network as a vector in $d$ dimensional Euclidean space. This representation is then amenable to potential downstream tasks about the network. For example, if we wish to cluster the nodes in the network we can simply cluster their embedding vectors. Or, if we wish to classify the nodes in the network, we can use these embeddings to construct a multinomial classifier. We note that the sampling schemes introduced by DeepWalk and node2vec motivate more complex models such as GraphSAGE [17] and Deep Graph Infomax [47], which utilise similar node sampling schemes for learning embeddings of networks. ", "page_idx": 1}, {"type": "text", "text": "As such, one of the key goals of learning vector representations of the units within networks is to allow for easy use for a multitude of downstream tasks. However, there is little theoretical understanding to what information is carried within these representations, and whether they can be applied successfully and efficiently to downstream tasks. This paper aims to address this gap by examining whether learned embeddings can facilitate community detection tasks in an unsupervised setting. ", "page_idx": 1}, {"type": "text", "text": "1.1 Summary of main results ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our main contribution is to describe the asymptotic distribution of the embeddings learned by the node2vec procedure, and to then use this to give consistency guarantees when these embeddings are used for community detection. A simple and informal form of our results, in the scenario of a balanced two block stochastic block model (SBM), is given below: ", "page_idx": 1}, {"type": "text", "text": "Theorem 1. (Informal) Suppose we observe a sequence of graphs $\\mathcal{G}_{n}$ on n vertices arising from a two-dimensional stochastic block model: for each vertex $u\\in[n]$ we assign a community label $c(u)\\;\\in\\;\\{0,1\\}$ with equal probability, and then we form edges in the graph independently with probability ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(u\\;a n d\\;v\\;a r e\\;c o n n e c t e d\\big)=\\Big\\{\\tilde{p}\\quad\\mathsf{}i f c(u)=c(v)\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where ${\\tilde{p}}\\neq{\\tilde{q}}$ . Suppose that $\\left(\\widehat{\\omega}_{u}\\right)$ are two-dimensional embeddings learned by node2vec on the above graph (where we hide the dependence on $n$ ). Then there exists some distinct vectors $\\eta_{c(u)}\\in\\mathbb{R}^{2}$ such that ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{u}\\|\\widehat{\\omega}_{u}-\\eta_{c(u)}\\|_{2}^{2}\\to0\\;i n\\,p r o b a b i l i t y\\;a s\\;n\\to\\infty.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Consequently, if we apply a $k$ -means algorithm to the embeddings learned via node2vec, as $n\\to\\infty$ we will classify at least $100(1-\\epsilon)\\%$ of vertices to the correct community (up to permutation) with asymptotic probability 1, for any $\\epsilon>0$ . ", "page_idx": 1}, {"type": "text", "text": "We give formal theorem statements, complete with full conditions, in Section 3; we note that our results extend to graph models beyond SBMs and are not limited to the dense regime. To give some brief intuition for the method of proof, we show that the probability that a pair $(u,v)$ is positively or negatively sampled within node2vec concentrates around a function which depends only on the underlying communities $c(u)$ and $c(v)$ of $u$ and $v$ . With this, we are able to argue that the node2vec loss concentrates uniformly (in a neighborhood of their minima) around a function whose minima $M^{*}$ is such that $M_{u,v}^{*}=\\widetilde{M}_{c(u),c(v)}$ for some matrix $\\widetilde{M}$ . This allows us to show that any set of embeddings which minimize the  node2vec loss will conver ge (up to rotation) to vectors which depend only on the community label, which consequently allows us to give consistency guarantees for clustering algorithms such as $\\boldsymbol{\\mathrm{k}}$ -means. ", "page_idx": 2}, {"type": "text", "text": "We highlight that while the theoretical properties of spectral clustering are well studied in the literature, there are relatively few theoretical guarantees provided for more modern embedding procedures such as node2vec. Our work provides some of the first theoretical results for models of this form. Our main contributions are the following: ", "page_idx": 2}, {"type": "text", "text": "i) We give convergence guarantees for embeddings learned via node2vec, under various sparsity regimes of (degree corrected) stochastic block models. We then use this to give weak consistency guarantees for community detection, when using the embeddings as features within a k-means clustering algorithm.   \nii) We verify the theoretical guarantees for simulated networks and examine the the performance of this procedure on real networks. We also empirically investigate important extensions of these theoretical results, relating to rates of recovery for community detection between node2vec and spectral clustering methods. We identify that as these networks grow the sampling parameters in node2vec have little impact on the performance of the proposed procedure. ", "page_idx": 2}, {"type": "text", "text": "The layout of the paper is as follows. In Section 2 we formulate the problem of constructing an embedding of the nodes in a network and state the criterion under which we consider community detection. In Section 3 we give the main result of this paper, the conditions under which $\\mathbf{k}$ -means clustering of the node2vec embedding of a network gives consistent community recovery. In Section 4 we verify these theoretical results empirically and investigate potential further results. In Section 5 we summarize our contributions and consider potential extensions. ", "page_idx": 2}, {"type": "text", "text": "1.2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Community detection for networks is a widely studied area with a large literature of existing work. Several notions of theoretical guarantees for community recovery are provided in [1], along with a survey of many existing approaches. There are many existing works which consider the embeddings obtained from the eigenvectors of the adjacency matrix of Laplacian of a network. For example, [30] considers spectral clustering using the eigenvectors of the adjacency matrix for a stochastic block model. Spectral clustering has provided such guarantees for a wide variety of network models, including [35, 12, 42, 32, 29]. ", "page_idx": 2}, {"type": "text", "text": "With the more recent development of random walk based embeddings, several recent works have begun to examine the theoretical properties of such embeddings, however the treatment is limited compared to spectral embeddings. [40] study the global minimizers of the node2vec loss in the setting where $d=n$ , viewing the problem as a matrix factorization problem. If $M^{*}$ is the global minimizing matrix, we highlight that their results apply for any $d\\geq\\operatorname{rank}(M^{*})$ . That said, this minimizer equals the entrywise logarithm of functions of the adjacency matrix $A$ ; we note that entrywise logarithms of matrices typically blow up their rank, and that even when \"in expectation\" the adjacency matrix is of low rank, the actual adjacency matrix is of full rank with high probability [7]. This means that it is unlikely when $d\\ll n$ that the global minimizer is the actual minimizer, which is the regime where embedding dimensions are considered in practice. We contrast that with our results, where we can take $d=\\Omega(\\bar{\\kappa})$ where $\\kappa$ is the number of communities, and obtain rigorous guarantees for the embeddings. ", "page_idx": 2}, {"type": "text", "text": "[52] then studies the concentration of the best rank $d$ approximation (with respect to the Frobenius norm) of the matrix $M^{*}$ about it\u2019s expected value under SBM and DCSBM models for node2vec with $p=q=1$ only, to argue that the best rank $d$ approximation can be used for strongly consistent community detection. We note that our results can be applied to node2vec without this restriction on the hyperparameters. Otherwise, they give similar types of guarantees as our paper in similar sparsity regimes and with similar rates, but in stronger norms. The key difference between our work and that of [52] is that we are able to give guarantees for the the actual minimizers of the node2vec loss as soon as $d=\\Omega(\\kappa)$ , whereas [52] use an approximation to the global minimizer, without studying the gap between this matrix and any minimizer of the node2vec loss (which is a cross-entropy loss, and therefore difficult to relate to a Frobenius norm approximation). [10] and [11] study node2vec with in the constrained setting (where $U=V$ ), and focus on giving more abstract guarantees for the gram matrix in the setting of graphons. In [11] the norm guarantees extend only to the $L_{1}$ norm between the gram matrix of the embeddings and the minimizer, which is not sufficient to give guarantees on the individual embeddings. In [10] the norm guarantees are upgraded to the $L_{2}$ norm, albeit with less optimal rates of convergence than what we show here. Our results also give guarantees for node2vec in full generality (no restriction on $p$ and $q$ ) and give the calculation details for SBMs and DCSBMs to explicitly describe the asymptotic distribution in certain regimes. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "2 Framework ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We consider a network $\\mathcal{G}$ consisting of a vertex set $\\nu$ of size $n$ and edge set $\\mathcal{E}$ . We can express this also using an $n\\times n$ symmetric adjacency matrix $A$ , where $A_{u v}=1$ indicates there is an undirected edge between node $u$ and node $v$ , with $A_{u v}=0$ otherwise, where $u,v\\in\\mathcal{V}$ . Given a realisation of such a network, we wish to examine models for community structure of the nodes in the network. We then examine the embeddings which can be obtained from node2vec and examine how they can be used for community detection. ", "page_idx": 3}, {"type": "text", "text": "2.1 Probabilistic models for community detection ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The most widely studied statistical model for community detection is the Stochastic Block Model (SBM) [20]. The SBM specifies a distribution for the communities, placing each of the $n$ nodes into one of $\\kappa$ communities, where these community assignments are drawn from some categorical distribution Categorical $(\\pi)$ . Writing $c(u)\\in[\\kappa]$ for the community of $u$ , the connection probabilities between edges are independent, conditional on these community assignments, with probability ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{P}(A_{u v}=1|c(u),c(v))=\\rho_{n}P_{c(u),c(v)},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $P$ is a $\\kappa\\times\\kappa$ matrix of probabilities, and $\\rho_{n}$ is the overall network sparsity (so that the network has $O(\\rho_{n}n^{2})$ edges on average). As a special case, the $p$ lanted-partition model considers $P$ as being a matrix with $\\tilde{p}$ along its diagonal and the value $\\tilde{q}$ elsewhere, with $\\kappa$ equally balanced communities, so $\\pi=(\\kappa^{-1},\\cdot\\cdot\\cdot,\\kappa^{\\bar{-}1})$ . We will denote such a model by $\\mathrm{SBM}(n,\\kappa,\\tilde{p},\\tilde{q},\\rho_{n})$ . ", "page_idx": 3}, {"type": "text", "text": "The most widely studied extension of the SBM is to incorporate a degree correction, equipping each node with a non negative degree parameter $\\theta_{u}$ drawn from some distribution independently of the community assignments [4]. This alters the previous model, instead giving ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(A_{u v}=1|c(u),c(v),\\theta_{u},\\theta_{v})=\\rho_{n}\\theta_{u}\\theta_{v}P_{c(u),c(v)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Degree corrected SBM models can be more appropriate for modeling the degree heterogeneity seen within communities in real world network data [23]. ", "page_idx": 3}, {"type": "text", "text": "Performance of stochastic block models is assessed in terms of their ability to recover the true community assignments of the nodes in a network, from the observed adjacency matrix $A$ . Given an estimated community assignment vector $\\hat{\\mathbf{c}}\\in[\\kappa]^{n}$ and the true communities ${\\bf z}$ then we can compute the agreement between these two assignment vectors, up to a relabeling of c, as ", "page_idx": 3}, {"type": "equation", "text": "$$\nL(\\widehat{\\mathbf{c}},\\mathbf{c})=\\operatorname*{min}_{\\sigma\\in\\mathrm{S}_{\\kappa}}\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{1}\\left[\\widehat{c}(i)\\neq\\sigma(c(i))\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $S_{\\kappa}$ denotes the symmetric group of permutations $\\sigma:[\\kappa]\\,\\rightarrow\\,[\\kappa]$ . We can also control the worst-case misclassification rate across all the different communities. If $\\mathcal{C}_{k}$ is the set of nodes belonging to community $k$ , then this is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widetilde{L}(\\widehat{\\mathbf{c}},\\mathbf{c}):=\\operatorname*{max}_{k\\in[\\kappa]}\\operatorname*{min}_{\\sigma\\in S_{\\kappa}}\\frac{1}{|{\\mathcal C}_{k}|}\\sum_{i\\in{\\mathcal C}_{k}}\\mathbb{1}\\big[\\widehat{c}(i)\\neq\\sigma(k)\\big].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Guarantees of the form $L(\\widehat{\\mathbf{c}},\\mathbf{c})=o_{p}(1)$ as $n\\to\\infty$ are known as weak consistency guarantees in the community detection litera ture. Strong consistency considers the stronger setting where $L(\\widehat{\\mathbf{c}},\\mathbf{c})=0$ with asymptotic probability 1. [1] provides a review of results for guarantees of these forms. In this work we consider only the weak consistency setting; we highlight that stricter assumptions are necessary in order to give these type of guarantees. ", "page_idx": 4}, {"type": "text", "text": "2.2 Obtaining embeddings from node2vec ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Machine learning methods such as node2vec aim to obtain an embedding of each node in a network. In general, for each node $u$ two $d_{\\cdot}$ -dimensional embedding vectors are learned, a centered representation $\\overline{{\\omega_{i}}}~\\in~\\mathbb{R}^{d}$ and a context representation $\\widehat{\\omega}_{i}\\ \\in\\ \\mathbb R^{d}$ . node2vec modifies the simple random walk considered in DeepWalk [38], incorpora t ing tuning parameters $p,q$ which encourage the walk to return to previously sampled nodes or transition to new nodes. Formally, this is defined by sampling concurrent pairs of vertices in the second-order random walk $(X_{n})_{n\\geq1}$ defined via ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(X_{n}=u\\,|\\,X_{n-1}=s,X_{n-2}=v\\big)\\propto\\left\\{\\begin{array}{l l}{0}&{\\mathrm{if~}(u,s)\\not\\in\\mathcal{E},}\\\\ {1/p}&{\\mathrm{if~}d_{u,v}=0\\mathrm{~and~}(u,s)\\in\\mathcal{E},}\\\\ {1}&{\\mathrm{if~}d_{u,v}=1\\mathrm{~and~}(u,s)\\in\\mathcal{E},}\\\\ {1/q}&{\\mathrm{if~}d_{u,v}=2\\mathrm{~and~}(u,s)\\in\\mathcal{E}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $d_{u,s}$ denotes the length of the shortest path between $u$ and $s$ , after selecting some initial two vertices. Here we consider the case where $(X_{0},X_{1})$ is drawn uniformly from the set of edges in order to initialize the walk. We note that when $p=q=1$ , corresponding to DeepWalk, this reduces down to a simple random walk, in which case the initial distribution samples a vertex proportionally to their degree. ", "page_idx": 4}, {"type": "text", "text": "A negative sampling approach is also used to approximate the computationally intractable loss function, replacing $\\bar{-}\\log(P(v|u))$ in (1) with ", "page_idx": 4}, {"type": "equation", "text": "$$\n-\\log\\sigma(\\langle\\omega_{u},\\widehat{\\omega}_{v}\\rangle)-\\sum_{l=1}^{L}\\log\\sigma(-\\langle\\omega_{u},\\widehat{\\omega}_{n_{l}}\\rangle),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\sigma(x)=(1+e^{-x})^{-1}$ , the sigmoid function. The vertices $n_{1},...,n_{L}$ are sampled according to a negative sampling distribution, which we denote as $P_{n s}(\\cdot|u)$ . This is usually chosen as the unigram distribution, ", "page_idx": 4}, {"type": "equation", "text": "$$\nP(v|u)=\\frac{\\deg(v)^{\\alpha}}{\\sum_{v^{\\prime}\\in\\mathcal{V}}\\deg(v^{\\prime})^{\\alpha}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which does not depend on the current location of the random walk, $u$ . This unigram distribution has parameter $\\alpha$ , which is commonly chosen as $\\alpha=3/4$ , as was used by word2vec [36]. Given this, and using (9), the loss considered by node2vec for a random walk of length $k$ can be written as ", "page_idx": 4}, {"type": "equation", "text": "$$\n=\\sum_{j=1}^{k+1}\\sum_{\\substack{i:0<|j-i|<W}}\\bigg[-\\log\\sigma\\big(\\langle\\omega_{v_{j}},\\widehat\\omega_{v_{i}}\\rangle\\big)-\\sum_{l=1}^{L}\\mathbb{E}_{n_{l}\\sim P_{n s}(\\cdot|v_{i})}\\log\\sigma\\big(-\\langle\\omega_{v_{j}},\\widehat\\omega_{n_{l}}\\rangle\\big)\\bigg].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here we use $\\mathbb{E}_{n_{l}\\sim P_{n s}(\\cdot|v_{i})}$ to denote the procedure to sample a draw from the negative sampling distribution, with $W=\\dot{1}$ commonly chosen. Given this loss function, stochastic gradient updates are used to estimate the embedding vector for each node. This amounts to minimizing an empirical risk function (e.g [41, 45]), which we can write as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{n}(U,V):=\\sum_{i\\neq j}\\Big\\{-\\mathbb{P}_{n}((i,j)\\in\\mathcal{P})\\log(\\sigma(\\langle u_{i},v_{j}\\rangle))-\\mathbb{P}_{n}((i,j)\\in\\mathcal{N})\\log(1-\\sigma(\\langle u_{i},v_{j}\\rangle))\\Big\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbb{P}_{n}(\\cdot):=\\mathbb{P}(\\cdot\\,|\\,\\mathcal{G}_{n})$ , and ${\\mathcal{P}}={\\mathcal{P}}(\\mathscr{G}_{n})$ and ${\\mathcal{N}}={\\mathcal{N}}({\\mathcal{G}}_{n})$ are sets of positive and negative samples respectively. We consider a sequence of graphs $\\mathcal{G}_{n}$ with $|\\nu|=n$ and study the behavior of this loss function when $n$ is large. To be explicit, $\\mathbb{P}_{n}((i,j)\\in\\mathcal{P})$ denotes the probability (conditional on a realization of the graph) that the vertices $(i,j)$ appear concurrently within a random walk of length $k$ , and $\\mathbb{P}_{n}((i,j)\\in\\mathcal{N})$ denotes the probability that $(i,j)$ is selected as a pair of edges through the negative sampling scheme (conditional on the random walk process in the first stage). ", "page_idx": 4}, {"type": "text", "text": "The loss depends on two matrices $U,V\\in\\mathbb{R}^{n\\times d}$ , with $u_{i},v_{j}\\in\\mathbb{R}^{d}$ denoting the $i$ -th and $j$ -th rows of $U$ and $V$ respectively. The rows of $U$ correspond to the \"centered representations\" of each node, while the rows of $V$ correspond to the \"context representation\" (borrowing the terminology used by e.g Word2Vec). In practice we can constrain the embedding vectors $u_{i}$ and $v_{i}$ to be equal if we wish; we will consider both approaches in this paper. (If these are not constrained to be equal, the centered representation is commonly used for downstream tasks.) We highlight Equation (12) is defined only as a function of $U V^{T}$ . There are two potential approaches to deal with this. We can regularize the objective function to enforce $U^{T}U^{'}=V^{T}V$ , which does not change the matrix $U V^{T}$ that we recover [53]. Alternatively, if these matrices are initialized to be balanced then they will remain balanced during the gradient descent procedure [34]. Either procedure can be used to implicitly enforce $U^{T}U\\ {\\bar{=}}\\ V^{T}{\\bar{V}}$ , which reduces the symmetry group of $(U,V)\\to U V^{T}$ to the orthogonal group. Similarly, if we constrain $U=V$ then we obtain the same reduction. ", "page_idx": 5}, {"type": "text", "text": "2.3 Using embeddings for community detection ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Having learned embedding vectors $\\omega_{i}$ for each node, we seek to use them for a further task, such as node clustering or classification. For community detection a natural procedure is to perform $\\mathbf{k}$ -means clustering on the embedding vectors, using the estimated cluster assignments as inferred communities. $\\mathbf{k}$ -means clustering [18] aims to find $k$ vectors $x_{1},\\ldots,x_{k}\\in\\mathbb{R}^{d}$ which minimize the within cluster sum of squares. This can be formulated in terms of a matrix $X\\in\\mathbb{R}^{k\\times d}$ and a membership matrix $\\Theta\\in\\{0,\\dot{1}\\}^{n\\times k}$ where each row of $\\Theta$ has exactly $k-1$ zero entries. Then the $\\mathbf{k}$ -means clustering objective can be written as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{k-means}}(\\Theta,X)=\\frac{1}{n}||\\widehat{\\Omega}-\\Theta X||_{F}^{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\widehat{\\Omega}\\in\\mathbb{R}^{n\\times d}$ is the matrix whose rows are the ${\\widehat{\\omega}}_{i}$ . The non-zero entries in each row of $\\Theta$ gives the estima ted community assignments. Finding exact  m inima to this minimization problem is NP-hard in general [5]. For theoretical purposes, we will give guarantees for any $(1+\\epsilon)$ -minimizer to the above problem, which returns any pair $(\\widehat\\Theta,\\widehat X)$ for which $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{k-means}}(\\widehat{\\Theta},\\widehat{X})\\leq(1+\\epsilon)\\operatorname*{min}_{\\Theta,X}\\mathcal{L}_{\\mathrm{k-means}}(\\Theta,X),}\\end{array}$ , and can be solved efficiently [25]. ", "page_idx": 5}, {"type": "text", "text": "3 Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Within this section, we give theoretical results which allow us to describe what happens when we use node2vec to learn embedding vectors for each node in the network, and then use these as features for a $\\mathbf{k}$ -means clustering algorithm to perform community detection. Throughout, we assume that we observe a sequence of graphs $(\\mathscr{G}_{n})_{n\\geq1}$ on $n$ vertices drawn from a probabilistic model and fit a node2vec model, according to one of the three scenarios below: ", "page_idx": 5}, {"type": "text", "text": "(i) We use DeepWalk $p=q=1$ in node2vec), and the graph is drawn according to a SBM with $\\rho_{n}\\gg\\log(n)/n$ ;   \n(ii) We use node2vec, and the graph is drawn according to a SBM with $\\rho_{n}=n^{-\\alpha}$ for some $\\alpha<\\alpha^{\\prime}$ , where $\\alpha^{\\prime}$ depends on node2vec\u2019s hyperparameters;   \n(iii) We use DeepWalk and a unigram parameter of $\\alpha=1$ , and the graph is drawn according to a DCSBM with $\\rho_{n}\\gg\\log(n)/n$ where the degree heterogeneity parameters $\\theta_{u}\\in[C^{-1},C]$ for some $C<\\infty$ . ", "page_idx": 5}, {"type": "text", "text": "All probabilistic statements below are with respect to the joint law of $\\mathcal{G}_{n}$ and the sampling which occurs to form the node2vec loss. All proofs are deferred to the Appendix. There we also provide extensions for the tasks of node classification and link prediction. ", "page_idx": 5}, {"type": "text", "text": "3.1 Asymptotic distribution of the embeddings ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We begin with a result which describes the asymptotic distribution of the gram matrices formed by the embeddings which minimize the loss $\\mathcal{L}_{n}(\\dot{U},\\dot{V})$ over matrices $U,V\\in\\check{\\mathbb{R}}^{n\\times d}$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. There exist constants $\\tilde{A}_{\\infty}$ and $\\tilde{A}_{2,\\infty}$ (depending on $\\pi$ , $P$ and the sampling scheme) and $a$ matrix $M^{\\ast}\\in\\mathbb{R}^{\\kappa\\times\\kappa}$ (also depending on $\\pi,P$ and the sampling scheme) such that when $d\\geq\\mathrm{rk}(M^{*})$ , ", "page_idx": 5}, {"type": "text", "text": "for any minimizer $(U^{*},V^{*})$ of ${\\mathcal{L}}(U,V)$ over $X\\times X$ where ", "page_idx": 6}, {"type": "equation", "text": "$$\nX=\\{U\\in\\mathbb{R}^{n\\times d}:\\|U\\|_{\\infty}\\leq\\tilde{A}_{\\infty},\\|U\\|_{2,\\infty}\\leq\\tilde{A}_{2,\\infty}\\},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "we have that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{1}{n^{2}}\\sum_{\\substack{i,j\\in[n]}}\\left(\\langle u_{i}^{*},v_{j}^{*}\\rangle-M_{c(i),c(j)}^{*}\\right)^{2}=C\\cdot\\left\\{O_{p}((\\frac{\\operatorname*{max}\\{\\log n,d\\}}{n\\rho_{n}})^{1/2})\\quad u n d e r\\;s c e n a r i o s\\;(i)\\;a n d\\;(i i i);\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $C$ is a constant depending on the $(D C)S B M$ parameters, the node2vec hyperparameters, $\\tilde{A}_{\\infty}$ and $\\tilde{A}_{2,\\infty}$ . In the case where we constrain $U=V$ within node2vec, the same result holds under scenarios $i$ ) and $i i$ ). Moreover, under all scenarios we can allow the number of communities \u03ba to grow with $n$ - provided $\\kappa=o(n\\rho_{n})$ - and still maintain consistency as $n\\to\\infty$ . ", "page_idx": 6}, {"type": "text", "text": "To give some intuition, we describe the form of $M^{*}$ when the graph arises from a $\\mathrm{SBM}(n,\\kappa,\\tilde{p},\\tilde{q},\\rho_{n})$ model when using DeepWalk. In this case, we show in the Appendix that ", "page_idx": 6}, {"type": "equation", "text": "$$\nM_{l m}^{*}=\\alpha^{*}\\delta_{l m}+\\beta^{*}(1-\\delta_{l m})\\;\\mathrm{for}\\;l,m\\in[\\kappa]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for some constants $\\alpha$ and $\\beta$ and $\\delta_{l m}$ is the Kronecker delta. In the unconstrained case we have that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\alpha^{*}=\\log\\Big(\\frac{1}{1+k^{-1}}\\cdot\\frac{\\kappa\\tilde{p}}{\\tilde{p}+(\\kappa-1)\\tilde{q}}\\Big),\\quad\\beta^{*}=\\log\\Big(\\frac{1}{1+k^{-1}}\\cdot\\frac{\\kappa\\tilde{q}}{\\tilde{p}+(\\kappa-1)\\tilde{q}}\\Big).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In the constrained case we instead have that $\\beta^{*}=-\\alpha^{*}/(\\kappa-1)$ , and that $\\alpha^{*}$ is a function of $p/q$ which is non-negative iff $p>q$ , and equals zero when $p\\leq q$ . With regards to the constants $\\tilde{A}_{\\infty}$ and $\\tilde{A}_{2,\\infty}$ , we have that $\\|M^{*}\\|_{\\infty}\\leq O(|\\log(p/q)|)$ . Additionally, it is possible to write $M^{*}=U_{M}^{*}(V_{M}^{*})^{T}$ where $\\|U_{M}^{*}\\|_{2,\\infty}$ and $\\|V_{M}^{*}\\|_{2,\\infty}$ are upper bounded by $O(|\\log(p/q)|^{1/2})$ . In particular, this means that $\\tilde{A}_{\\infty}$ and $\\tilde{A}_{2,\\infty}$ do not have any implicit dependence on $n$ or $\\kappa$ , and so the constant in Theorem 2 is not affecting the rate here. ", "page_idx": 6}, {"type": "text", "text": "While Theorem 2 gives guarantees from the gram matrices formed by the embeddings, in practice we want guarantees for the actual embedding vectors themselves. For convenience we suppose that the embedding dimension $d$ is chosen exactly to be the rank of $M^{*}$ ; upon doing so, we can then obtain guarantees for the embedding vectors themselves. We recall that in the unconstrained case, we implicitly suppose that we find embedding matrices $U^{*}$ and $V^{*}$ which are balanced in that $(U^{*})^{T}U^{*}=\\bar{(V^{*})^{T}}V^{*}$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 3. Suppose that the conclusion of Theorem 2 holds, and further suppose that d equals the rank of the matrix $M^{*}$ . Then there exists a matrix $\\widetilde U^{*}\\in\\mathbb R^{\\kappa\\times d}$ such that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{Q\\in O(d)}\\frac{1}{n}\\sum_{i=1}^{n}\\|u_{i}^{*}-\\widetilde{u}_{c(i)}^{*}Q\\|_{2}^{2}=C\\cdot\\left\\{O_{p}\\big((\\frac{\\operatorname*{max}\\{\\log n,d\\}}{n\\rho_{n}})^{1/2}\\big)\\right.\\ \\left.u n d e r\\ s c e n a r i o s\\ (i)\\ a n d\\ (i i i);\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "3.2 Guarantees for community detection ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "With Theorem 3, we are now in a position to give guarantees for machine learning methods which use the embeddings as features for a downstream task. We only discuss using the embeddings for clustering; in Appendix D.2 we discuss what can be said for other downstream tasks. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4. Suppose that we have embedding vectors $u_{i}^{*}\\in\\mathbb{R}^{d}$ for $i\\in[n]$ such that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{Q\\in O(d)}\\frac{1}{n}\\sum_{i=1}^{n}\\|u_{i}^{*}-\\widetilde{u}_{c(i)}^{*}Q\\|_{2}^{2}=O_{p}(r_{n})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for some rate function $r_{n}\\to0$ as $n\\to\\infty$ and vectors $\\eta_{l}\\in\\mathbb{R}^{d}$ for $l\\in[\\kappa]$ . Moreover suppose that $\\delta:=\\,\\mathrm{min}_{l\\neq k}\\,\\|\\widetilde{u}_{l}^{*}-\\widetilde{u}_{k}^{*}\\|_{2}>0$ . Then if $\\hat{\\mathbf{c}}(i)$ is the community assignment of node i produced by applying $a$ $(1+\\epsilon)$ -approximate $k$ -means clustering with $k=\\kappa$ to the matrix whose columns are the $u_{i}^{*}$ , we have that $L(\\mathbf{c},{\\hat{\\mathbf{c}}})=O_{p}(\\delta^{-2}r_{n})$ and $\\widetilde{L}(\\mathbf{c},\\hat{\\mathbf{c}})=O_{p}(\\delta^{-2}r_{n})$ . In the case where the RHS of (16) is only $o_{p}(1)$ instead, then instead $L(\\mathbf{c},{\\hat{\\mathbf{c}}})$ and $\\widetilde{L}(\\mathbf{c},\\hat{\\mathbf{c}})$ are $\\delta^{-2}o_{p}(1)$ . ", "page_idx": 6}, {"type": "text", "text": "Within the $\\mathrm{SBM}(n,\\kappa,\\tilde{p},\\tilde{q},\\rho_{n})$ model, we can show in the unconstrained case that $\\delta^{2}{\\bf\\Delta}={\\bf\\Delta}$ $\\Theta(|\\log(\\tilde{p}/\\tilde{q})|)$ , and in the constrained case that $\\delta^{2}\\,=\\,\\Theta((\\tilde{p}/\\tilde{q}))$ . As a result, this suggests that as $\\tilde{p}/\\tilde{q}$ approaches 1, the task of distinguishing the communities becomes more difficult. This is inline with basic intuition, along with our experimental results in Section 4. We note that, due to the nature of the embedding vectors, for any proportion of vertices arbitrarily close to 1, the nodes will, with high probability for sufficiently large $n$ , be separated in the embedding space according to their community assignments. This separation allows clustering methods, such as DBSCAN, to accurately recover the communities of these nodes also. ", "page_idx": 7}, {"type": "text", "text": "Recall that from the discussion before, we know that $M^{*}$ equals the zero matrix in the constrained regime when $\\tilde{p}\\leq\\tilde{q}$ (and therefore the embeddings asymptotically contain no information about the network). As in the case where $\\tilde{p}>\\tilde{q}$ we can show that $\\delta>0$ , we get the immediate corollary. ", "page_idx": 7}, {"type": "text", "text": "Corollary 5. Under scenario $(i)$ , suppose the embedding vectors learned through the node2vec loss are obtained by constraining the embedding matrices $U=V$ . Then the embeddings can be used for weakly consistent recovery of the communities if and only if $\\tilde{p}>\\tilde{q}$ . ", "page_idx": 7}, {"type": "text", "text": "As a result, the constrained model can be disadvantageous if used without a-priori knowledge of the network beforehand (in that within-community connections outnumber between-community connections), even though it avoids interpretability issues about which embedding vector should be used as single representation for the node. ", "page_idx": 7}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section we provide simulation and real data experiments to empirically validate the previous theoretical results. We demonstrate the performance, in terms of community detection, of $\\boldsymbol{\\mathrm{k}}$ -means clustering of the embedding vectors learned by node2vec, for both the regular and degree corrected stochastic block model. We also investigate the role of the negative sampling parameter $\\alpha$ and the node2vec tuning parameters $p$ and $q$ , before examining performance on a real network with known community structure. ", "page_idx": 7}, {"type": "text", "text": "We first simulate data from the planted partition stochastic block model, $\\mathrm{SBM}(n/\\kappa,\\kappa,\\tilde{p},\\tilde{q},\\rho_{n})$ . We consider $\\tilde{q}=\\tilde{p}\\beta$ for a range of values of $\\beta\\ll1$ , giving varying strengths of associative community structure. In each setting we vary both the number of true communities present and the number of nodes in each community, considering $n=200$ to $n=5000$ and $K=2,3,4,5$ . We use node2vec to construct an embedding of the nodes in the network. 1 We use an embedding dimension of 64 and do not modify other default tuning parameters for the embedding procedure unless specified, so that $p=q=1$ . We investigate the role of these tuning parameters below, allowing them to vary as is considered in node2vec. We pass these embedding vectors into $\\mathbf{k}$ -means clustering, where $k=\\kappa$ , the true number of communities present in the network. This estimates a community assignment for each of the nodes in the network. ", "page_idx": 7}, {"type": "text", "text": "To evaluate the performance of our procedure, we compute the proportion of nodes correctly classified, up to permutation of the community assignments. For each simulation setting we perform 10 replications. We show the resulting estimates in Figure 1(a), for the relatively sparse setting where $\\bar{\\rho_{n}}=\\log(n)/n$ . For all settings, the proportion of nodes assigned to the correct community by $\\mathbf{k}$ -means clustering of the node2vec embeddings is high, particularly when the ratio of the between to within community edge probabilities, $\\beta$ , is small. As expected, as we increase the number of nodes in the network, a larger proportion of nodes are correctly recovered. We examine the empirical rate of convergence of this procedure in the Appendix. This appears to be approximately super-linear for dense networks and sub-linear for relatively sparse networks. Compared to the results of [50], this indicates that node2vec may be supoptimal. In the Appendix we also show community recovery using normalized mutual information (NMI) [9]. We also see good performance. ", "page_idx": 7}, {"type": "text", "text": "We can similarly evaluate the performance of node2vec for data generated from a degree corrected SBM (DC-SBM). To generate such networks we modify the simulation setting used by [15]. We generate the degree correction parameters $\\theta_{u}=|Z_{u}|+1-(2\\pi)^{-1/2}$ where $Z_{u}\\sim N(0,\\sigma=0.25)$ and incorporate these into the $\\mathrm{SBM}(n/\\kappa,\\kappa,\\tilde{p},\\tilde{q},\\dot{\\rho}_{n})$ considered previously. Two nodes $u$ and $v$ in the same community will have connection probability $\\theta_{u}\\theta_{v}\\rho_{n}\\tilde{p}$ while for nodes in different communities it will be $\\theta_{u}\\theta_{v}\\rho_{n}\\tilde{q}$ . We again learn an embedding of the nodes using a default implementation of node2vec and cluster these embedding vectors using k-means clustering. We show the corresponding results, in terms of the proportion of the nodes assigned to their correct communities under this setting in Figure 1(b). As expected, the degree corrections make community recovery somewhat more challenging however as we increase the number of nodes in the network, we are able to correctly recover a high proportion of nodes. ", "page_idx": 7}, {"type": "image", "img_path": "cnpR4e2HCQ/tmp/6687ac90296870f92d96db8b2e1238b50d8e1d44e4584b22a11332f75cfc5561.jpg", "img_caption": ["Figure 1: Proportion of nodes correctly recovered for both the regular and degree corrected relatively sparse SBM. ", ""], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We next wish to examine empirically the role of the unigram parameter $\\alpha$ of Equation (10), and how this affects community detection. While the previous theoretical results require $\\alpha\\,=\\,1$ for weak consistency of community recovery in the DC-SBM, we investigate if good empirical performance is possible with other choices of this parameter. We consider the DC-SBM simulation described previously, where we now vary $\\alpha\\in\\{-1,0,0.25,0.5,0.75,1\\}$ when learning the node embeddings. For each of these settings (with all other parameters as before) we consider the proportion of nodes correctly recovered. We show this result for networks with $\\kappa=2$ communities in Figure 2. These experiments indicate similar performance for a range of values of $\\alpha$ . Further work is needed to confirm the guarantees do indeed extend to these alternative choices of $\\alpha$ , and we investigate this for real networks in Section A of the appendix. ", "page_idx": 8}, {"type": "text", "text": "We also investigate the role of the node2vec tuning parameters $p$ and $q$ on performance. For $\\kappa=2$ we consider $\\beta=0.01$ and $\\beta=0.2$ , giving networks with strong and weak associative community structure respectively. We simulate from the previous relatively sparse DC-SBM with varying numbers of nodes and fti node2vec, using $p,q\\in\\{0.5,1,2\\}$ . As the number of nodes in the network increases all choices of $p$ and $q$ give similar good performance for both choices of $\\beta$ . This indicates that the impact of these sampling parameters becomes limited as the networks become sufficiently large. We provide further discussion and a visualization of this result in Appendix A. ", "page_idx": 8}, {"type": "text", "text": "Finally, we briefly examine the performance of our community detection procedure on the political blog data collected by [2]. As highlighted by [23], degree heterogeneity makes community recovery challenging for methods which do not account for this. We see similar performance if we cluster using a Gaussian mixture model rather than $\\mathbf{k}$ -means clustering. In particular, spectral clustering struggles regardless of the graph Laplacian used. Our procedure shows excellent community recovery (average NMI of 0.75) for a range of embedding dimensions and unigram parameter settings as shown in Figure 3, with further details and an additional real network example in Appendix A. ", "page_idx": 8}, {"type": "image", "img_path": "cnpR4e2HCQ/tmp/7ec11dc70384898dbed94694b6ec1c647a1e47bb58f46b382571117a9ac2b9dd.jpg", "img_caption": ["Figure 2: Proportion of nodes correctly recovered as we vary the negative sampling parameter in node2vec with mean and one standard error for each setting. We see similar performance for each choice of $\\alpha$ . "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "cnpR4e2HCQ/tmp/4dec742863d0d83da2a78d03d10b05d35f7ca60acb3cc9c8a924ced45e9e8422.jpg", "img_caption": ["Figure 3: Node2vec with $\\mathbf{k}\\cdot$ - means clustering can recover the communities in the political blog data while spectral clustering fails. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work we consider the theoretical properties of node embeddings learned from node2vec. We show, when the network is generated from a (degree corrected) stochastic block model, that the embeddings learned from DeepWalk and node2vec converge asymptotically to vectors depending only on their community assignment. As a result, we show that K-means clustering of the node2vec embedding vectors can provide weakly consistent estimates of the true community assignments of the nodes in the network. We verify these results empirically using simulated networks. ", "page_idx": 9}, {"type": "text", "text": "There are several important future directions which can extend this work. One direction is in extending the recovery results within the degree corrected SBM to the full range of hyperparmaeters for node2vec, as our simulation studies indicate that a more general result may hold. There is also the matter of increasing the strength of our results to give better rates and strongly consistent community detection; one possible avenue of exploration would be to see whether our results and the results of [52] could be combined to achieve this. Another improvement would be to study the behavior of the random walk on the graph in the sparse regime, although this would require a generalization of e.g the result of [13]. We have also not considered the task of estimating $\\kappa$ , the number of communities in a SBM model, using the embeddings obtained by node2vec. This has been considered for alternative approaches to community detection, ([22, 27] are some recent results) but not in the context of a general embedding of the nodes. Finally, there is a desire to obtain consistency results for more recent and complex network embedding methods, such as [17] and [47]. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Emmanuel Abbe. Community detection and stochastic block models: recent developments. The Journal of Machine Learning Research, 18(1):6446\u20136531, 2017.   \n[2] Lada A. Adamic and Natalie Glance. The political blogosphere and the 2004 u.s. election: divided they blog. In Proceedings of the 3rd International Workshop on Link Discovery, LinkKDD \u201905, page 36\u201343, New York, NY, USA, 2005. Association for Computing Machinery. ISBN 1595932151. doi: 10.1145/1134271.1134277. URL https://doi.org/10.1145/1134271.1134277.   \n[3] Edoardo M Airoldi, David M. Blei, Stephen E. Fienberg, Eric P. Xing, and Tommi Jaakkola. Mixed membership stochastic block models for relational data with application to protein-protein interactions. In Proceedings of the international biometrics society annual meeting, volume 15, page 1, 2006.   \n[4] Edoardo M Airoldi, David M. Blei, Stephen E. Fienberg, and Eric P. Xing. Mixed membership stochastic blockmodels. Advances in neural information processing systems, 21, 2008.   \n[5] Daniel Aloise, Amit Deshpande, Pierre Hansen, and Preyas Popat. NP-hardness of Euclidean sum-ofsquares clustering. Machine Learning, 75(2):245\u2013248, May 2009. ISSN 1573-0565. doi: 10.1007/ s10994-009-5103-0. URL https://doi.org/10.1007/s10994-009-5103-0.   \n[6] B\u00e9la Bollob\u00e1s. Threshold functions for small subgraphs. In Mathematical Proceedings of the Cambridge Philosophical Society, volume 90, pages 197\u2013206. Cambridge University Press, 1981.   \n[7] Kevin P. Costello and Van H. Vu. The rank of random graphs. Random Structures & Algorithms, 33(3): 269\u2013285, 2008. doi: https://doi.org/10.1002/rsa.20219. URL https://onlinelibrary.wiley.com/ doi/abs/10.1002/rsa.20219.   \n[8] Peng Cui, Xiao Wang, Jian Pei, and Wenwu Zhu. A survey on network embedding. IEEE transactions on knowledge and data engineering, 31(5):833\u2013852, 2018.   \n[9] Leon Danon, Albert Diaz-Guilera, Jordi Duch, and Alex Arenas. Comparing community structure identification. Journal of statistical mechanics: Theory and experiment, 2005(09):P09008, 2005.   \n[10] Andrew Davison. Asymptotics of $\\ell\\_2$ regularized network embeddings. Advances in Neural Information Processing Systems, 35:24960\u201324974, 2022.   \n[11] Andrew Davison and Morgane Austern. Asymptotics of network embeddings learned via subsampling. Journal of Machine Learning Research, 24(138):1\u2013120, 2023.   \n[12] Shaofeng Deng, Shuyang Ling, and Thomas Strohmer. Strong consistency, graph laplacians, and the stochastic block model. The Journal of Machine Learning Research, 22(1):5210\u20135253, 2021.   \n[13] Jian Ding, Eyal Lubetzky, and Yuval Peres. Anatomy of the giant component: The strictly supercritical regime. Eur. J. Comb., 35:155\u2013168, January 2014. ISSN 0195-6698. doi: 10.1016/j.ejc.2013.06.004. URL https://doi.org/10.1016/j.ejc.2013.06.004.   \n[14] Linton Freeman. The development of social network analysis. A Study in the Sociology of Science, 1(687): 159\u2013167, 2004.   \n[15] Chao Gao, Zongming Ma, Anderson Y. Zhang, and Harrison H. Zhou. Community detection in degreecorrected block models. The Annals of Statistics, 46(5):2153 \u2013 2185, 2018. doi: 10.1214/17-AOS1615. URL https://doi.org/10.1214/17-AOS1615.   \n[16] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pages 855\u2013864, 2016.   \n[17] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Advances in neural information processing systems, 30, 2017.   \n[18] John A Hartigan and Manchek A Wong. Algorithm as 136: A k-means clustering algorithm. Journal of the royal statistical society. series c (applied statistics), 28(1):100\u2013108, 1979.   \n[19] Peter D Hoff, Adrian E Raftery, and Mark S Handcock. Latent space approaches to social network analysis. Journal of the american Statistical association, 97(460):1090\u20131098, 2002.   \n[20] Paul W Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. Stochastic blockmodels: First steps. Social networks, 5(2):109\u2013137, 1983.   \n[21] Roger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge University Press, New York, NY, USA, 2nd edition, 2012. ISBN 978-0-521-54823-6.   \n[22] Jiashun Jin, Zheng Tracy Ke, Shengming Luo, and Minzhe Wang. Optimal estimation of the number of network communities. Journal of the American Statistical Association, pages 1\u201316, 2022.   \n[23] Brian Karrer and Mark EJ Newman. Stochastic blockmodels and community structure in networks. Physical review E, 83(1):016107, 2011.   \n[24] Vladimir Koltchinskii and Evarist Gin\u00e9. Random Matrix Approximation of Spectra of Integral Operators. Bernoulli, 6(1):113\u2013167, 2000. ISSN 1350-7265. doi: 10.2307/3318636. URL http://www.jstor.org/ stable/3318636. Number: 1 Publisher: International Statistical Institute (ISI) and Bernoulli Society for Mathematical Statistics and Probability.   \n[25] Amit Kumar, Yogish Sabharwal, and Sandeep Sen. Linear Time Algorithms for Clustering Problems in Any Dimensions. In Lu\u00eds Caires, Giuseppe F. Italiano, Lu\u00eds Monteiro, Catuscia Palamidessi, and Moti Yung, editors, Automata, Languages and Programming, Lecture Notes in Computer Science, pages 1374\u20131385, Berlin, Heidelberg, 2005. Springer. ISBN 978-3-540-31691-6. doi: 10.1007/11523468_111.   \n[26] Pierre Latouche, Etienne Birmel\u00e9, and Christophe Ambroise. Overlapping stochastic block models with application to the French political blogosphere. The Annals of Applied Statistics, 5(1):309 \u2013 336, 2011. doi: 10.1214/10-AOAS382. URL https://doi.org/10.1214/10-AOAS382.   \n[27] Can M Le and Elizaveta Levina. Estimating the number of communities by spectral methods. Electronic Journal of Statistics, 16(1):3315\u20133342, 2022.   \n[28] Sirio Legramanti, Tommaso Rigon, Daniele Durante, and David B Dunson. Extended stochastic block models with application to criminal networks. The Annals of Applied Statistics, 16(4):2369, 2022.   \n[29] Jing Lei. Network representation using graph root distributions. The Annals of Statistics, 49(2):745 \u2013 768, 2021.   \n[30] Jing Lei and Alessandro Rinaldo. Consistency of spectral clustering in stochastic block models. The Annals of Statistics, 43(1):215 \u2013 237, 2015. doi: 10.1214/14-AOS1274. URL https://doi.org/10. 1214/14-AOS1274.   \n[31] Jure Leskovec and Andrej Krevl. SNAP Datasets: Stanford large network dataset collection. http: //snap.stanford.edu/data, June 2014.   \n[32] Keith D Levin, Fred Roosta, Minh Tang, Michael W Mahoney, and Carey E Priebe. Limit theorems for out-of-sample extensions of the adjacency and laplacian spectral embeddings. The Journal of Machine Learning Research, 22(1):8707\u20138765, 2021.   \n[33] Feng Luo, Yunfeng Yang, Chin-Fu Chen, Roger Chang, Jizhong Zhou, and Richard H Scheuermann. Modular organization of protein interaction networks. Bioinformatics, 23(2):207\u2013214, 2007.   \n[34] Cong Ma, Yuanxin Li, and Yuejie Chi. Beyond procrustes: Balancing-free gradient descent for asymmetric low-rank matrix sensing. IEEE Transactions on Signal Processing, 69:867\u2013877, 2021.   \n[35] Shujie Ma, Liangjun Su, and Yichong Zhang. Determining the number of communities in degree-corrected stochastic block models. The Journal of Machine Learning Research, 22(1):3217\u20133279, 2021.   \n[36] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems, 26, 2013.   \n[37] Andrew Ng, Michael Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm. Advances in neural information processing systems, 14, 2001.   \n[38] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701\u2013710, 2014.   \n[39] Yannik Pitcan. A note on concentration inequalities for u-statistics, 2019.   \n[40] Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, and Jie Tang. Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec. In Proceedings of the eleventh ACM international conference on web search and data mining, pages 459\u2013467, 2018.   \n[41] Herbert Robbins and Sutton Monro. A Stochastic Approximation Method. The Annals of Mathematical Statistics, 22(3):400\u2013407, September 1951. ISSN 0003-4851, 2168-8990. doi: 10.1214/aoms/1177729586. URL https://projecteuclid.org/journals/annals-of-mathematical-statistics/ volume-22/issue-3/A-Stochastic-Approximation-Method/10.1214/aoms/1177729586. full. Number: 3 Publisher: Institute of Mathematical Statistics.   \n[42] P Rubin-Delanchy, CE Priebe, M Tang, and J Cape. A statistical interpretation of spectral embedding: the generalised random dot product graph. arxiv e-prints. arXiv preprint arXiv:1709.05506, 2017.   \n[43] Michel Talagrand. Upper and Lower Bounds for Stochastic Processes: Modern Methods and Classical Problems. Ergebnisse der Mathematik und ihrer Grenzgebiete. 3. Folge / A Series of Modern Surveys in Mathematics. Springer-Verlag, Berlin Heidelberg, 2014. ISBN 978-3-642-54074-5. doi: 10.1007/ 978-3-642-54075-2. URL https://www.springer.com/gp/book/9783642540745.   \n[44] Stephen Tu, Ross Boczar, Max Simchowitz, Mahdi Soltanolkotabi, and Ben Recht. Low-rank Solutions of Linear Matrix Equations via Procrustes Flow. In Proceedings of The 33rd International Conference on Machine Learning, pages 964\u2013973. PMLR, June 2016. URL https://proceedings.mlr.press/v48/ tu16.html. ISSN: 1938-7228.   \n[45] Victor Veitch, Morgane Austern, Wenda Zhou, David M Blei, and Peter Orbanz. Empirical risk minimization and stochastic gradient descent for relational data. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1733\u20131742. PMLR, 2019.   \n[46] Petar Velic\u02c7kovic\u00b4, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.   \n[47] Petar Velic\u02c7kovic\u00b4, William Fedus, William L. Hamilton, Pietro Li\u00f2, Yoshua Bengio, and R. Devon Hjelm. Deep Graph Infomax. arXiv:1809.10341 [cs, math, stat], December 2018. URL http://arxiv.org/ abs/1809.10341. arXiv: 1809.10341.   \n[48] V. H. Vu. Concentration of non-lipschitz functions and applications. Random Structures & Algorithms, 20(3):262\u2013316, 2002. doi: https://doi.org/10.1002/rsa.10032. URL https://onlinelibrary.wiley. com/doi/abs/10.1002/rsa.10032.   \n[49] Owen G Ward, Zhen Huang, Andrew Davison, and Tian Zheng. Next waves in veridical network embedding. Statistical Analysis and Data Mining: The ASA Data Science Journal, 14(1):5\u201317, 2021.   \n[50] Anderson Ye Zhang. Fundamental limits of spectral clustering in stochastic block models. arXiv preprint arXiv:2301.09289, 2023.   \n[51] Si Zhang, Hanghang Tong, Jiejun Xu, and Ross Maciejewski. Graph convolutional networks: a comprehensive review. Computational Social Networks, 6(1):1\u201323, 2019.   \n[52] Yichi Zhang and Minh Tang. A theoretical analysis of deepwalk and node2vec for exact recovery of community structures in stochastic blockmodels. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(2):1065\u20131078, 2024. doi: 10.1109/TPAMI.2023.3327631.   \n[53] Zhihui Zhu, Qiuwei Li, Gongguo Tang, and Michael B Wakin. The global optimization geometry of low-rank matrix optimization. IEEE Transactions on Information Theory, 67(2):1308\u20131331, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix - Community Detection Guarantees using Embeddings Learned by Node2Vec ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The Appendix consists of the proofs of the results stated within the paper, along with some extra discussions which would detract from the flow of the main paper. We also provide some additional simulation results relating to node classification, and further simulated and real data experiments examining community detection. ", "page_idx": 13}, {"type": "text", "text": "A Additional Experimental Results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Here we provide additional details describing the experimental results presented in the main paper. We also describe additional experiments. All experiments were run on a computing cluster utilising 4 cores of an Intel E5-2683 v4 Broadwell 2.1GHz CPU or similar with 2 GB of memory per core. Each individual experimental run required at most 2 hours of computing time. All experiments, including initial preliminary experiments, required approximately $25\\mathrm{k}$ CPU hours. All code required to reproduce all results is included in the code repository in the supplemental files. ", "page_idx": 13}, {"type": "text", "text": "Additional Simulation, Node Classification We provide a simple experiment to support the theoretical results on node classification demonstrated in Section D of the appendix. We simulate data from a $\\mathrm{SBM}(n/\\kappa,\\kappa,\\tilde{p},\\tilde{q},\\rho_{n})$ as before with $\\tilde{q}=\\tilde{p}\\beta$ as in the main text. We learn an embedding of each node using node2vec with embedding dimension of 64 and all other parameters set at their default values. We then use the true community labels of $10\\%$ of these nodes to train a (multinomial) logistic regression classifier, and predict the class label for the remaining $90\\%$ of nodes in the network. We examine the performance of this classification tool using the node2vec embeddings in terms of classification accuracy. We show these results in Figure S1 for $\\rho_{n}=\\log(n)/n$ , with 10 simulations for each setting, with the mean across these simulations and error bars indicating one standard error. This classifier has excellent accuracy at predicting the labels of other nodes. ", "page_idx": 13}, {"type": "image", "img_path": "cnpR4e2HCQ/tmp/332bf52e0772f3d29ddf734899015fb8eb9ee981c9e5b3a0bc60edb90871a9f3.jpg", "img_caption": ["Figure S1: Classification accuracy using $10\\%$ of the node embeddings to learn a multinomial logistic regression classifier. Mean and one standard error shown. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Additional Results, Community Detection Here we include additional simulation results which were omitted from the main text. In particular, for the simulations considered in the main manuscript we now examine the community recovery performance in terms of the normalized mutual information [9]. We show the average NMI score across these simulations, along with error bars corresponding to one standard error. In each case, the NMI metric is similar to the proportion of nodes correctly recovered. As we increase the number of nodes this performance improves. ", "page_idx": 13}, {"type": "image", "img_path": "cnpR4e2HCQ/tmp/fbe16f1b3e277e5317317c8a99c64b484e0414327c86aeadb5078ea6010ea251.jpg", "img_caption": ["Figure S2: NMI for relatively sparse SBM. Mean and one standard error shown. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "cnpR4e2HCQ/tmp/237a18051e0677f0c8a9329569b4f9f75ffe782680a32177283940b3e6636d7e.jpg", "img_caption": ["Figure S3: NMI for relatively sparse DC-SBM. Mean and one standard error shown. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Rates of Convergence We can also investigate the empirical convergence of these methods. Here, we consider the same simulated SBM data as above, and examine the convergence in the proportion of nodes correctly recovered, as we increase the number of nodes in the network, for $\\kappa=2,3,4,5$ . We empirically investigate this convergence using a log-log plot, which is shown in Figure S5 for a relatively sparse SBM. Our node2vec procedures demonstrates empirical convergence which is super-linear for dense networks while being sub-linear for relatively sparse networks. ", "page_idx": 14}, {"type": "text", "text": "Varying the node2vec walk parameters We also wish to examine the performance of our proposed clustering procedure when the parameters of the random walk are varied. While $p$ and $q$ are both commonly chosen to be 1, resulting in a simple random walk, other values are possible. We consider data simulated from the relatively sparse DC-SBM considered previously with $\\kappa=2$ communities and consider the within between community probability ratio $\\beta=.01$ and $\\beta=0.2$ , corresponding to an easier and harder setting to recover the communities respectively. We then consider $p,q\\in\\{0.5,1,2\\}$ , the common possible values and vary the number of nodes in each community as before. For each of these settings we perform community detection using node2vec and spectral clustering. When $\\beta=0.01$ weobtain excellent community recovery for all values of $p$ and $q$ , as shown in Figure S6(a). When $\\beta=0.2$ community recovery is more challenging for small networks for all values of $p$ and $q$ . As the number of nodes increases, Figure S6(b) shows that all choices of $p$ and $q$ result in good performance. ", "page_idx": 14}, {"type": "image", "img_path": "cnpR4e2HCQ/tmp/3135abf4ebf85c33002684da723e8a9fbf6191b05fc51f65ebf633d2b04a0ac6.jpg", "img_caption": ["Figure S4: NMI varying $\\alpha$ for relatively sparse DC-SBM. Mean and one standard error shown. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "cnpR4e2HCQ/tmp/d1553c6e80988c3cefb7fe593492cea13d0572c2c6b17ffb9d730aa6ea3fcd5d.jpg", "img_caption": ["Figure S5: Log-Log plot showing the rate of convergence as we increase the number of nodes in the network. We show a fitted regression for each of the values of $\\beta$ , showing better convergence when the difference between the within and between community edge probabilities is higher. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A.1 Performance on Real Networks ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We wish to further examine the performance of this community detection procedure for real networks, with known community structure. We also wish to compare this procedure to spectral clustering, which is widely used in practice for community detection. We use two publicly available networks containing known community structure. We first consider a network of emails between 1005 members of a large research institution, available as part of the Stanford Network Analysis Project [31]. There are 25571 directed edges between the nodes in this network, with known ground truth communities consisting of 42 departments present in this institution. We also consider a widely used dataset of directed edges between 1490 U.S political blogs, collected before the 2004 elections [2]. Here the directed edges correspond to hyperlinks, with ground truth communities corresponding to whether the blogs has been identified as liberal or conservative. ", "page_idx": 15}, {"type": "image", "img_path": "cnpR4e2HCQ/tmp/39cb3832a1d44142e0ed2d3b37dfb0c9a11342459bf289ccc6c25b711910e099.jpg", "img_caption": ["Figure S6: Varying the node2vec sampling parameters for DC-SBMs with $\\beta\\,=\\,0.01$ (left) and $\\beta=0.2$ (right). Community recovery is harder when $\\beta$ is larger and this is seen for all values of $p$ and $q$ for small networks. As the number of nodes increases we get good community recovery for all choices of $p$ and $q$ . "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "For each of these datasets we compare the community recovery of Node2Vec and traditional spectral clustering, using the normalized graph Laplacian. As is common in the literature, we remove the direction from these edges and take the largest connected component, forming symmetric adjacency matrices with 986 and 1222 nodes respectively. We then use the previously described procedure to perform community detection using Node2Vec. We consider a range of embedding dimensions $d=16$ , 32, 64, 128, 256) and unigram sampling parameter $(\\alpha=-1,0.0,0.25,0.5,0.75,1.0)$ , while keeping all other parameters fixed at the defaults considered before. With the true number of communities known, we then compare the estimated communities from 10 simulations for each of these parameter settings, along with performing 10 simulations of spectral clustering for each of these settings. ", "page_idx": 16}, {"type": "text", "text": "In Figure S7 we compare the performance of Node2Vec and spectral clustering for the Email network and in Figure S8 we use the Political Blogs network. We measure community recovery in terms of the normalized mutual information (NMI) between the estimated and true communities. Other metrics such as the adjusted rand index (ARI) showing similar results. In each case the communities estimated by Node2Vec are substantially closer to the true communities than those estimated by spectral clustering. As highlighted by Karrer and Newman [23] for the political blog data, models which do not account for degree heterogeneity can struggle to recover the underlying community structure. As shown in Figure S8, spectral clustering is unable to recover the communities due to this heterogeneity, while clustering using the Node2Vec embedding shows strong performance at community recovery. ", "page_idx": 16}, {"type": "text", "text": "We also further expand on the role of the embedding parameters in the performance of Node2Vec on these real networks. In Figure S9 we examine community recovery for the Email data as we vary the embedding dimension $d$ and the unigram sampling parameter $\\alpha$ . As we vary each of these parameters we see good community recovery in all settings. For this dataset all choices of embedding dimension and unigram parameter give good NMI scores. ", "page_idx": 16}, {"type": "text", "text": "B Additional Notation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We give a brief recap of some of the notation introduced in the main paper, along with some more notation which is used purely within the Supplemntary Material. Throughout, we will suppose that the graph $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$ is drawn according to the following generative model: each vertex $u\\in\\mathcal V$ have latent variables ${\\boldsymbol\\lambda}_{u}\\,=\\,(c(u),\\theta_{u})$ where $c(u)\\,\\in\\,[\\kappa]$ is a community assignment, and $\\theta_{u}$ is a degree-heterogenity correction factor. We then suppose that the edges $\\bar{a_{u v}}\\in\\bar{\\{0,1\\}}$ in the graph $\\mathcal{G}_{n}$ on $n$ vertices arise independently with probability ", "page_idx": 16}, {"type": "image", "img_path": "cnpR4e2HCQ/tmp/5bda766eb8f2c21e59eef73cd7c145a57563409bc5064a05e3ccceae71b43e19.jpg", "img_caption": ["Figure S7: Community recovery for the Email data, using both Node2Vec and Spectral Clustering. Node2Vec can better recover the true communities. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "cnpR4e2HCQ/tmp/868bdd66f1a0455e08090451bfaa0c69eacc90372ebe4d256cdbf0bf4079e627.jpg", "img_caption": ["Figure S8: Community recovery for the Political Blog data, using both Node2Vec and Spectral Clustering. Node2Vec can better recover the true communities. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(a_{u v}=1\\,|\\,\\lambda_{u},\\lambda_{v})=\\rho_{n}\\theta_{u}\\theta_{v}P_{c(u),c(v)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for $u<v$ , with $a_{u v}\\,=\\,a_{v u}$ by symmetry for $u\\,>\\,v^{2}$ . The factor $\\rho_{n}$ accounts for sparsity in the network. The above model corresponds to a degree corrected stochastic block model [23]; we ", "page_idx": 17}, {"type": "image", "img_path": "cnpR4e2HCQ/tmp/9bb7a9891c26b20e68148515b282c1cad35f601317b7bdc9f2c2afa96cc275c2.jpg", "img_caption": ["(a) Varying the embedding dimension used. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "cnpR4e2HCQ/tmp/7bfa032deb085ff0353edbc98485f967cc19f31772b28eec4f5416b620ad5981.jpg", "img_caption": ["(b) Varying the Unigram Parameter $\\alpha$ . "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure S9: The effect of different Node2Vec parameters on community recovery, measure in terms of Normalized Mutual Information (NMI), for the Email Data. ", "page_idx": 18}, {"type": "text", "text": "highlight that the case where $\\theta_{u}$ is constant across all $u\\in\\mathcal{V}$ corresponds to the original stochastic block model [20]. For convenience, we will write ", "page_idx": 18}, {"type": "equation", "text": "$$\nW(\\lambda_{u},\\lambda_{v})=\\theta_{u}\\theta_{v}P_{c(u),c(v)}\\qquad\\mathrm{so}\\qquad\\mathbb{P}(A_{u v}=1\\,|\\,\\lambda_{u},\\lambda_{v})=\\rho_{n}W(\\lambda_{u},\\lambda_{v}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We then introduce the notation ", "page_idx": 18}, {"type": "equation", "text": "$$\nW(\\lambda_{i},\\cdot):=\\mathbb{E}[W(\\lambda_{i},\\lambda_{j})\\,|\\,\\lambda_{i}],\\qquad\\mathcal{E}_{W}(\\alpha):=\\mathbb{E}[W(\\lambda_{i},\\cdot)^{\\alpha}]\\mathrm{~for~}\\alpha>0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that under the assumptions that the community assignments are drawn i.i.d from a Categorical $(\\pi)$ random variable, and the degree correction factors are drawn i.i.d from a distribution $\\vartheta$ independently of the community assignments, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle W(\\lambda_{i},\\cdot)=\\theta_{i}\\cdot\\mathbb{E}[\\theta]\\cdot\\mathbb{E}_{j\\sim\\mathrm{Cat}(\\pi)}[P_{c(i),j}\\mid c(i)]=\\theta_{i}\\cdot\\mathbb{E}[\\theta]\\cdot\\sum_{j=1}^{\\kappa}\\pi_{j}P_{c(i),j}\\mathrm{,}}}\\\\ {{\\displaystyle\\mathcal{E}_{W}(\\alpha)=\\mathbb{E}[\\theta^{\\alpha}]\\cdot\\mathbb{E}[\\theta]^{\\alpha}\\cdot\\sum_{i=1}^{\\kappa}\\pi_{i}\\Big(\\sum_{j=1}^{\\kappa}\\pi_{j}P_{i,j}\\Big)^{\\alpha}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For convenience, we will write $\\begin{array}{r}{\\widetilde P_{c(i)}=\\sum_{j=1}^{{\\kappa}}{{\\pi}_{j}}P_{c(i),j}}\\end{array}$ . ", "page_idx": 18}, {"type": "text", "text": "Recall that node2vec attempts to minimize the objective ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{n}(U,V):=\\displaystyle\\sum_{i\\neq j}\\Big\\{-\\mathbb{P}\\big((i,j)\\in\\mathcal{P}(\\mathcal{G}_{n})\\,|\\,\\mathcal{G}_{n}\\big)\\log\\big(\\sigma(\\langle u_{i},v_{j}\\rangle)\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\,\\mathbb{P}\\big((i,j)\\in\\mathcal{N}(\\mathcal{G}_{n})\\,|\\,\\mathcal{G}_{n}\\big)\\log(1-\\sigma(\\langle u_{i},v_{j}\\rangle))\\Big\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $U,V\\in\\mathbb{R}^{n\\times d}$ , with $u_{i},v_{j}\\in\\mathbb{R}^{d}$ denoting the $i$ -th and $j$ -th rows of $U$ and $V$ respectively, and $\\sigma(x):=(1+e^{-x})^{-1}$ denoting the sigmoid function. Here $\\mathcal{P}$ and $\\mathcal{N}$ correspond to the positive and negative sampling schemes induced by the random walk and unigram mechanisms respectively. ", "page_idx": 18}, {"type": "text", "text": "C Proof of Theorems 2 and 3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1 Proof overview ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To give an overview of the proof approach, we work by forming successive approximations to the function $\\mathcal{L}_{n}(U,V)$ where we have uniform convergence of the approximation error as $n\\to\\infty$ over either level sets of the function considered, or the overall domain of optimization of the embedding matrices $U$ and $V$ . We break these approximations up into multiple steps: ", "page_idx": 18}, {"type": "text", "text": "1. Theorems S1, S2, S3 and Proposition S4 - We begin by working with an approximation $\\widehat{\\mathcal{L}}_{n}(U,V)$ of $\\mathcal{L}_{n}(U,V)$ , where the sampling weights $\\mathbb{P}\\big((i,j)\\in\\mathcal{P}(\\mathcal{G}_{n})\\,|\\,\\mathcal{G}_{n}\\big)$ and $\\mathbb{P}((i,j)\\in$ ${\\mathcal{N}}({\\mathcal{G}}_{n})\\mid{\\mathcal{G}}_{n})$ are replaced by functions of the latent variables $(\\lambda_{i},\\lambda_{j})$ of the vertices $i$ and $j$ , along with $a_{i j}$ in the case of $f_{\\mathcal P}(\\lambda_{i},\\lambda_{j})$ . ", "page_idx": 18}, {"type": "text", "text": "2. The resulting approximation $\\widehat{\\mathcal{L}}_{n}(U,V)$ has a dependence on the adjacency matrix of the network. We argue that this  loss function converges uniformly to its average over the adjacency matrix when the vertex latent variables remain fixed; this is the contents of Theorem S5.   \n3. So far, the loss function only looks between interactions of $u_{i}$ and $v_{j}$ for $i\\ne j$ . For theoretical purposes, it is more convenient to work with a loss function where the term with $i=j$ is included. This is handled within Lemma S6.   \n4. Now that we have an averaged version of the loss function to work with, we are able to examine the minima of this loss function, and find that there is a unique minima (in the sense that for any pair of optima matrices $U^{*}$ and $V^{*}$ , the matrix $U^{*}(V^{*})^{T}$ is unique). Moreover, in certain circumstances we can give closed forms for these minima. This is the contents of Section C.6.   \n5. This is then all combined together in order to give Theorems S13 and S14, which correspond to Theorems 1 and 2 of the main text. ", "page_idx": 19}, {"type": "text", "text": "We recap that we consider three scenarios - referred to as Scenario (i), (ii) and (iii) throughout - when proving the following result: ", "page_idx": 19}, {"type": "text", "text": "(i) We use DeepWalk $p=q=1$ in node2vec), and the graph is drawn according to a SBM with $\\rho_{n}\\gg\\log(n)/n$ ;   \n(ii) We use node2vec, and the graph is drawn according to a SBM with $\\rho_{n}=n^{-\\alpha}$ for some $\\alpha<\\alpha^{\\prime}$ , where $\\alpha^{\\prime}$ depends on node2vec\u2019s hyperparameters;   \n(iii) We use DeepWalk and a unigram parameter of $\\alpha=1$ , and the graph is drawn according to a DCSBM with $\\rho_{n}\\gg\\log(n)/n$ where the degree heterogeneity parameters $\\theta_{u}\\in[C^{-1},C]$ for some $C>\\infty$ . ", "page_idx": 19}, {"type": "text", "text": "Generally speaking, the approach is the exact same for all three scenarios. As we have a closed formula in the case where we examine DeepWalk, we will consistently provide the details for the DeepWalk case first, and then discuss afterwards how the results and proofs change (if at all) when considering node2vec in generality. Throughout, we also contextualize the proof by examining what it says for a $\\mathrm{SBM}(n,\\kappa,\\tilde{p},\\tilde{q},\\rho_{n})$ model. This corresponds to a balanced network with $\\pi=(\\bar{\\kappa^{-1}},\\cdot\\cdot\\cdot,\\kappa^{-\\bar{1}})$ . ", "page_idx": 19}, {"type": "text", "text": "C.2 Replacing the sampling weights ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Before giving an approximation to $\\mathcal{L}_{n}(U,V)$ , we need to first come up with approximate forms of $\\mathbb{P}\\big((i,j)\\,\\in\\,\\mathcal{P}(\\mathcal{G}_{n})\\,|\\,\\mathcal{G}_{n}\\big)$ and $\\mathbb{P}\\big((i,j)\\in\\mathcal{N}(\\mathcal{G}_{n})\\,|\\,\\mathcal{G}_{n}\\big)$ . The next three results give examples of this. In this section we prove three main results. The first two give us guarantees for the sampling probabilities of vertex pairs $(u,v)$ for node2vec for any choice of the hyperparameters $(p,q)$ . In particular they will allow us to argue that when the underlying graph arises from a SBM, the sampling probabilities asymptotically depend only on the underlying communities. The last specializes this to the case of DeepWalk (where $p=q=1)$ ), which has enough structure to allow us to get some additional information, such as closed formula for these sampling probabilities, which can be used in the case where the graph arises through a DCSBM. ", "page_idx": 19}, {"type": "text", "text": "Theorem S1. There exists $\\alpha$ sufficiently small, depending on the walk length $k$ , such that if $\\rho_{n}=n^{-\\alpha}$ then there exists a symmetric measurable (with respect to the sigma field generated by $W$ ) function $f_{\\mathcal P}(\\lambda,\\lambda^{\\prime})$ which is bounded below away from zero, and bounded above by $C\\rho_{n}^{-1}$ for some constant $C<\\infty$ , such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\neq j}\\left\\vert\\frac{n^{2}\\mathbb{P}\\big((i,j)\\in\\mathcal{P}(\\mathcal{G}_{n})\\,|\\,\\mathcal{G}_{n}\\big)}{a_{i j}f_{\\mathcal{P}}(\\lambda_{i},\\lambda_{j})}-1\\right\\vert=o_{p}(1).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Theorem S2. There exists $\\alpha$ sufficiently small, depending on the walk length $k$ , such that if $\\rho_{n}=n^{-\\alpha}$ then there exists a symmetric measurable (with respect to the sigma field generated by $W$ ) function $f_{\\mathcal P}(\\lambda,\\lambda^{\\prime})$ which is bounded below away from zero, and bounded above by some constant $C<\\infty$ , such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\neq j}\\left\\vert\\frac{n^{2}\\mathbb{P}\\big((i,j)\\in\\mathcal{P}(\\mathcal{G}_{n})\\,|\\,\\mathcal{G}_{n}\\big)}{f_{\\mathcal{N}}(\\lambda_{i},\\lambda_{j})}-1\\right\\vert=o_{p}(1).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The proof of these two results are given in Appendix E.1.1 and E.1.2 respectively. We note that while in principle we could give a closed formula for $f_{\\mathcal{P}}$ and $f_{\\mathcal{N}}$ in this scenario, they are sufficiently intractable to inspection that doing so would not provide any benefit. ", "page_idx": 20}, {"type": "text", "text": "In the case of DeepWalk where $p=q=1$ , the calculations involved are tractable enough such that we can improve the sparsity constraints, give closed forms for the measurable functions discussed above, and also provide rates of convergence. ", "page_idx": 20}, {"type": "text", "text": "Theorem S3. Denote ", "text_level": 1, "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{f_{\\mathcal{P}}(\\lambda_{i},\\lambda_{j}):=\\displaystyle\\frac{2k}{\\rho_{n}\\mathcal{E}_{W}(1)},}}\\\\ {{f_{N}(\\lambda_{i},\\lambda_{j}):=\\displaystyle\\frac{l(k+1)}{\\mathcal{E}_{W}(1)\\mathcal{E}_{W}(\\alpha)}\\big(W(\\lambda_{i},\\cdot)W(\\lambda_{j},\\cdot)^{\\alpha}+W(\\lambda_{i},\\cdot)^{\\alpha}W(\\lambda_{j},\\cdot)\\big).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{i\\neq j}{\\operatorname*{max}}\\left\\vert\\frac{n^{2}\\mathbb{P}\\left(\\left(i,j\\right)\\in\\mathcal{P}\\left(\\mathcal{G}_{n}\\right)\\vert\\,\\mathcal{G}_{n}\\right)}{a_{i j}f_{\\mathcal{P}}\\left(\\lambda_{i},\\lambda_{j}\\right)}-1\\right\\vert=O_{p}\\Big(\\Big(\\frac{\\log n}{n\\rho_{n}}\\Big)^{1/2}\\Big),}\\\\ &{\\underset{i\\neq j}{\\operatorname*{max}}\\left\\vert\\frac{n^{2}\\mathbb{P}\\left(\\left(i,j\\right)\\in\\mathcal{N}\\left(\\mathcal{G}_{n}\\right)\\vert\\,\\mathcal{G}_{n}\\right)}{f_{\\mathcal{N}}\\left(\\lambda_{i},\\lambda_{j}\\right)}-1\\right\\vert=O_{p}\\Big(\\Big(\\frac{\\log n}{n\\rho_{n}}\\Big)^{1/2}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. This is a consequence of [11, Proposition 26]. We highlight the referenced result supposes that for the negative sampling scheme, vertices for which $a_{i j}=0$ are rejected, whereas this does not happen here. Other than for the factor of $(1-a_{i j})$ in the quoted result, the proof is otherwise unchanged, which gives the statement above for $\\mathbb{P}\\big((i,j)\\in\\mathcal{N}(\\mathcal{G}_{n})\\,|\\,\\mathcal{G}_{n}\\big)$ . \u53e3 ", "page_idx": 20}, {"type": "text", "text": "With this, we then get the following result: ", "page_idx": 20}, {"type": "text", "text": "Proposition S4. Denote ", "text_level": 1, "page_idx": 20}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{L}}_{n}(U,V):=\\frac{1}{n^{2}}\\sum_{i\\neq j}\\Big\\{-f_{\\mathcal{P}}(\\lambda_{i},\\lambda_{j})a_{i j}\\log\\big(\\sigma(\\langle u_{i},v_{j}\\rangle)\\big)-f_{\\mathcal{N}}(\\lambda_{i},\\lambda_{j})\\log\\(1-\\sigma(\\langle u_{i},v_{j}\\rangle))\\Big\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and define the set ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Psi_{\\tilde{A}}:=\\Big\\{U,V\\in\\mathbb{R}^{n\\times d}\\,\\vert\\,\\,\\mathcal{L}_{n}(U,V)\\leq\\tilde{A}\\mathcal{L}_{n}(\\mathbb{0}_{n\\times d},\\mathbb{0}_{n\\times d})\\Big\\}\\subseteq\\mathbb{R}^{n\\times d}\\times\\mathbb{R}^{n\\times d}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for any constant $\\tilde{A}>1$ , where $0_{n\\times d}$ denotes the zero matrix in $\\mathbb{R}^{n\\times d}$ . Then for any set $X\\subseteq$ $\\mathbb{R}^{n\\times d}\\stackrel{\\cdot}{\\times}\\mathbb{R}^{n\\times d}$ containing the pair of zero matrices $O_{n\\times d},$ , we have under Scenario $i$ ) and iii) that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{(U,V)\\in\\Psi_{A}\\cap X}{\\operatorname*{sup}}\\,\\big|\\mathcal{L}_{n}(U,V)-\\widehat{\\mathcal{L}}_{n}(U,V)\\big|=O_{p}\\Big(\\tilde{A}\\cdot\\Big(\\frac{\\log n}{n\\rho_{n}}\\Big)^{1/2}\\Big),}\\\\ {\\mathbb{P}\\Big(\\underset{(U,V)\\in X}{\\operatorname*{sup}}\\,\\mathcal{L}_{n}(U,V)\\cup\\underset{(U,V)\\in X}{\\operatorname{arg\\,min}}\\,\\widehat{\\mathcal{L}}_{n}(U,V)\\subseteq\\Psi_{\\tilde{A}}\\cap X\\Big)=1-o(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In Scenario $(i i),$ the $O_{p}(\\cdot)$ bound is replaced by an $o_{p}(1)$ bound. ", "page_idx": 20}, {"type": "text", "text": "Proof. The proof is essentially equivalent to Lemma 32 of [11] up to changes in notation, and so we do not repeat the details. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Note that in practice we can choose $A$ to be any constant greater than 1 but fixed with n - e.g $A=10$ , and have the result hold. We will do so going forward. ", "page_idx": 20}, {"type": "text", "text": "C.3 Averaging over the adjacency matrix of the graph ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Following the proof outline, the next step is to argue that $\\mathcal{L}_{n}(U,V)$ is close to its expectation when we average over the adjacency matrix of the graph $\\mathcal{G}_{n}$ . We begin with showing what occurs in the ", "page_idx": 20}, {"type": "text", "text": "DeepWalk case (Scenarios (i) and (iii)), and at the end of the section we discuss how the proof changes for the more general node2vec case. Note that we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Im[\\widehat{Z}_{n}(U,V)\\,|\\,\\lambda]=\\frac{1}{n^{2}}\\sum_{i\\neq j}\\Big\\{-f_{\\mathcal{P}}(\\lambda_{i},\\lambda_{j})\\rho_{n}W(\\lambda_{u},\\lambda_{v})\\log(\\sigma(\\langle u_{i},v_{j}\\rangle))-f_{\\mathcal{N}}(\\lambda_{i},\\lambda_{j})\\log(1-\\sigma(\\langle u_{i},v_{j}\\rangle))\\Big\\},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and so ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{n}(U,V):=\\displaystyle\\frac{\\mathcal{E}_{W}(1)}{2k}\\Big(\\widehat{\\mathcal{L}}_{n}(U,V)-\\mathbb{E}[\\widehat{\\mathcal{L}}_{n}(U,V)\\mid\\lambda]\\Big)}\\\\ &{\\qquad\\qquad=\\displaystyle\\frac{1}{n^{2}}\\sum_{i\\neq j}\\Big(\\rho_{n}^{-1}a_{i j}-W(\\lambda_{i},\\lambda_{j})\\Big)\\cdot(-\\log\\sigma(\\langle u_{i},v_{j}\\rangle)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that $\\mathbb{E}[E_{n}(U,V)\\,|\\,\\lambda]=0$ , and so it therefore suffices to control $E_{n}(U,V)-\\mathbb{E}[E_{n}(U,V)\\,|\\,\\lambda]$ uniformly over embedding matrices $U,V\\in\\mathbb{R}^{n\\times d}$ . This is the contents of the next theorem. ", "page_idx": 21}, {"type": "text", "text": "Theorem S5. Begin by defining the set ", "page_idx": 21}, {"type": "equation", "text": "$$\nB_{2,\\infty}(\\tilde{A}_{2,\\infty}):=\\{U\\in\\mathbb{R}^{n\\times d}:\\|U\\|_{2,\\infty}\\leq\\tilde{A}_{2,\\infty}\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then we have the bound ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{U,V\\in B_{2,\\infty}(\\tilde{A}_{2,\\infty})}\\big|E_{n}(U,V)\\big|=O_{p}\\Big(\\tilde{A}_{2,\\infty}^{2}\\Big(\\frac{d}{n\\rho_{n}}\\Big)^{1/2}\\Big).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In particular, we also have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{U,V\\in B_{2,\\infty}(\\tilde{A}_{2,\\infty})}\\big|\\widehat{Z}_{n}(U,V)-\\mathbb{E}[\\widehat{\\mathcal{L}}_{n}(U,V)\\,|\\,\\lambda]\\big|=O_{p}\\Big(\\frac{\\tilde{A}_{2,\\infty}^{2}k}{\\mathcal{E}_{W}(1)}\\Big(\\frac{d}{n\\rho_{n}}\\Big)^{1/2}\\Big).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Begin by noting that for any set $C\\subseteq\\mathbb{R}^{n\\times d}\\times\\mathbb{R}^{n\\times d}$ for which $0_{n\\times d}\\times0_{n\\times d}\\in C$ , we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\operatorname*{sup}_{(U,V)\\in C}|E_{n}(U,V)|\\leq\\displaystyle\\operatorname*{sup}_{(U,V)\\in C}\\left|E_{n}(U,V)-E_{n}(0_{n\\times d},0_{n\\times d})\\right|+\\left|E_{n}(0_{n\\times d},0_{n\\times d})\\right|}\\\\ {\\leq\\displaystyle\\operatorname*{sup}_{(U,V),(\\tilde{U},\\tilde{V})\\in C}\\left|E_{n}(U,V)-E_{n}(\\tilde{U},\\tilde{V})\\right|+\\left|E_{n}(0_{n\\times d},0_{n\\times d})\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We therefore need to control these two terms. We begin with the second; note that as ", "page_idx": 21}, {"type": "equation", "text": "$$\nE_{n}(0_{n\\times d},0_{n\\times d})=\\frac{1}{n^{2}}\\sum_{i\\ne j}\\left(\\rho_{n}^{-1}a_{i j}-W(\\lambda_{i},\\lambda_{j})\\right)\\cdot\\frac{1}{n^{2}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "it follows by Lemma S30 that this term is $O_{p}((n^{2}\\rho_{n})^{-1/2})$ . For the first term, we make use of a chaining bound. Note that if we write $T_{i j}\\,=\\,-\\log\\sigma(\\langle u_{i},v_{j}\\rangle)$ and $S_{i j}\\,=\\,-\\log\\sigma(\\langle\\tilde{u}_{i},\\tilde{v}_{j}\\rangle)$ for $i,j\\in[n]$ , then we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\nE_{n}(U,V)-E_{n}(\\tilde{U},\\tilde{V})=\\frac{1}{n^{2}}\\sum_{i\\neq j}\\Big(\\rho_{n}^{-1}a_{i j}-W(\\lambda_{i},\\lambda_{j})\\Big)\\cdot(T_{i j}-S_{i j}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Because the function $x\\mapsto-\\log\\sigma(x)$ is 1-Lipschitz, it follows that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|T-S\\|_{F}^{2}\\leq\\|U V^{T}-\\tilde{U}\\tilde{V}^{T}\\|_{F}^{2},\\qquad\\|T-S\\|_{\\infty}\\leq\\|U V^{T}-\\tilde{U}\\tilde{V}^{T}\\|_{\\infty}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and consequently we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{P}\\big(|E_{n}(U,V)-E_{n}(\\tilde{U},\\tilde{V})|\\ge u\\big)}}\\\\ &{}&{\\le2\\exp\\Big(-\\operatorname*{min}\\Big\\{\\frac{u^{2}}{128\\rho_{n}^{-1}n^{-4}\\|U V^{T}-\\tilde{U}\\tilde{V}^{T}\\|_{F}^{2}},\\frac{u}{16\\rho_{n}^{-1}n^{-2}\\|U V^{T}-\\tilde{U}\\tilde{V}^{T}\\|_{\\infty}}\\Big\\}\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "as a result of Lemma S30. Now, as $U,V\\in B_{F}(A_{F})\\cap B_{2,\\infty}(\\tilde{A}_{2,\\infty})$ , by Lemma S19 if we define the metrics ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad d_{F}((U_{1},V_{1}),(U_{2},V_{2})):=\\|U_{1}-U_{2}\\|_{F}+\\|V_{1}-V_{2}\\|_{F},}\\\\ &{d_{2,\\infty}((U_{1},V_{1}),(U_{2},V_{2})):=\\|U_{1}-U_{2}\\|_{2,\\infty}+\\|V_{1}-V_{2}\\|_{2,\\infty},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "then we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathsf{\\Pi}^{\\mathsf{P}}\\big(|E_{n}(U,V)-E_{n}(\\tilde{U},\\tilde{V})|\\ge u\\big)}}\\\\ &{}&{\\le2\\exp\\Big(-\\operatorname*{min}\\Big\\{\\frac{u^{2}}{128\\rho_{n}^{-1}n^{-4}A_{F}^{2}d_{F}((U,V),(\\tilde{U},\\tilde{V}))^{2}},\\frac{u}{16\\rho_{n}^{-1}n^{-2}\\tilde{A}_{2,\\infty}d_{2,\\infty}((U,V),(\\tilde{U},\\tilde{V}))}\\Big\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "As a result of Corollary S22, it therefore follows that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{(U,V),(\\tilde{U},\\tilde{V})\\in T\\times T}\\big|E_{n}(U,V)-E_{n}(\\tilde{U},\\tilde{V})\\big|=O_{p}\\Big(\\tilde{A}_{2,\\infty}^{2}\\Big(\\frac{d}{n\\rho_{n}}\\Big)^{1/2}+\\tilde{A}_{2,\\infty}^{2}\\frac{d}{n\\rho_{n}}\\Big)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The desired conclusion follows by combining the bounds (S24) and (S33). ", "page_idx": 22}, {"type": "text", "text": "For the more abstract node2vec case under Scenario (ii), we highlight that we can take ", "page_idx": 22}, {"type": "equation", "text": "$$\nE_{n}(U,V)=\\frac{1}{n^{2}}\\sum_{i\\neq j}\\rho_{n}f_{\\mathcal{P}}(\\lambda_{i},\\lambda_{j})\\Big(\\rho_{n}^{-1}a_{i j}-W(\\lambda_{i},\\lambda_{j})\\Big)\\cdot(-\\log\\sigma(\\langle u_{i},v_{j}\\rangle)).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now, as $f_{\\mathcal P}(\\lambda_{u},\\lambda_{v})$ is a function of the community assignments only within the SBM case, we can replace this by a matrix of constants $f_{\\mathcal P,c,c^{\\prime}}$ for $\\overline{{c}},c^{\\prime}\\in[\\kappa]$ , and therefore the error term can be decomposed into a sum ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{c_{1},c_{2}}\\left(\\rho_{n}f_{\\mathcal P,c_{1},c_{2}}\\right)\\sum_{\\stackrel{i\\neq j}{i:c(u)=c_{1}}}\\left(\\rho_{n}^{-1}a_{i j}-W(\\lambda_{i},\\lambda_{j})\\right)\\cdot(-\\log\\sigma(\\langle u_{i},v_{j}\\rangle)),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we recall that $\\mathrm{max}_{c_{1},c_{2}}(\\rho_{n}f_{\\mathcal{P},c_{1},c_{2}})<\\infty$ as guaranteed by Theorem S1. Each of these terms (of which there are finitely many) can be controlled using the exact same argument as in Theorem S5, and so the conclusion of the Theorem also holds with the same overall rate of convergence in Scenario (ii). ", "page_idx": 22}, {"type": "text", "text": "C.4 Adding in a diagonal term ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Currently the sum in $\\mathbb{E}[\\widehat{\\mathcal{L}}_{n}(U,V)\\,|\\,\\lambda]$ is defined only terms $i,j$ with $i\\neq j$ - it is more convenient to work with the version where the diagonal term is added in: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathcal{R}_{n}(U,V):=\\frac{1}{n^{2}}\\sum_{i,j\\in[n]}\\Big\\{-f_{\\mathcal{P}}(\\lambda_{i},\\lambda_{j})\\rho_{n}W(\\lambda_{u},\\lambda_{v})\\log(\\sigma(\\langle u_{i},v_{j}\\rangle))}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad-f_{\\mathcal{N}}(\\lambda_{i},\\lambda_{j})\\log(1-\\sigma(\\langle u_{i},v_{j}\\rangle))\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We show that this does not significantly change the size of the loss function. ", "page_idx": 22}, {"type": "text", "text": "Lemma S6. With the same notation as in Theorem $S5$ , we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{U,V\\in B_{2,\\infty}(\\tilde{A}_{2,\\infty})}{\\operatorname*{sup}}\\left|\\mathcal{R}_{n}(U,V)-\\mathbb{E}[\\widehat{\\mathcal{L}}_{n}(U,V)\\,|\\,\\lambda]\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=O_{p}\\Big(\\frac{1}{n}\\tilde{A}_{2,\\infty}^{2}\\Big(\\|\\rho_{n}f_{\\mathcal{P}}(\\lambda,\\lambda^{\\prime})W(\\lambda,\\lambda^{\\prime})\\|_{\\infty}+\\|f_{\\mathcal{N}}(\\lambda,\\lambda^{\\prime})\\|_{\\infty}\\Big)\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In particular, in the case of DeepWalk we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{J,V\\in B_{2,\\infty}(\\tilde{A}_{2,\\infty})}\\big|\\mathcal{R}_{n}(U,V)-\\mathbb{E}[\\widehat{\\mathcal{L}}_{n}(U,V)\\,|\\,\\lambda]\\big|=O_{p}\\Big(\\frac{1}{n}\\tilde{A}_{2,\\infty}^{2}\\Big(\\frac{2k\\|W\\|_{\\infty}}{\\mathcal{E}_{W}(1)}+\\frac{2l(k+1)\\|W\\|_{\\infty}^{2}}{\\mathcal{E}_{W}(1)\\mathcal{E}_{W}(\\alpha)}\\Big)\\Big).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Begin by noting that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0\\le\\mathcal{R}_{n}(U,V)-\\mathbb{E}[\\widehat{\\mathcal{L}}_{n}(U,V)\\,|\\,\\lambda]}\\\\ &{\\quad=\\displaystyle\\frac{1}{n^{2}}\\sum_{i=1}^{n}\\big\\{-\\,f_{\\mathcal{P}}(\\lambda_{i},\\lambda_{j})\\rho_{n}W(\\lambda_{u},\\lambda_{v})\\log(\\sigma(\\langle u_{i},v_{i}\\rangle))-f_{\\mathcal{N}}(\\lambda_{i},\\lambda_{j})\\log(1-\\sigma(\\langle u_{i},v_{i}\\rangle))\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that we can bound ", "page_idx": 23}, {"type": "equation", "text": "$$\n-\\log(\\sigma(\\langle u_{i},v_{j}\\rangle)\\leq|\\langle u_{i},v_{i}\\rangle\\leq\\|u_{i}\\|_{2}\\|v_{i}\\|_{2}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and similarly $-\\log(1-\\sigma(\\langle u_{i},v_{i}\\rangle))\\leq|\\langle u_{i},v_{i}\\rangle|\\leq\\|u_{i}\\|_{2}\\|v_{i}\\|_{2}$ . Moreover, we have the bounds ", "page_idx": 23}, {"type": "equation", "text": "$$\nf p(\\lambda_{i},\\lambda_{j})\\rho_{n}W(\\lambda_{i},\\lambda_{j})\\leq\\|\\rho_{n}f p(\\lambda,\\lambda^{\\prime})W(\\lambda,\\lambda^{\\prime})\\|_{\\infty}<\\infty,f_{\\cal N}(\\lambda_{i},\\lambda_{j})\\leq\\|f_{\\cal N}(\\lambda,\\lambda^{\\prime})\\|_{\\infty}<\\infty\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "under our assumptions. As a result, because $U,V\\in B_{2,\\infty}(\\tilde{A}_{2,\\infty})$ , we end up with the final bound ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\big|\\mathcal{R}_{n}(U,V)-\\mathbb{E}[\\widehat{\\mathcal{L}}_{n}(U,V)\\,|\\,\\lambda]\\big|\\le\\frac{1}{n}\\tilde{A}_{2,\\infty}^{2}\\Big(\\|\\rho_{n}f_{\\mathcal{P}}(\\lambda,\\lambda^{\\prime})W(\\lambda,\\lambda^{\\prime})\\|_{\\infty}+\\|f_{\\mathcal{N}}(\\lambda,\\lambda^{\\prime})\\|_{\\infty}\\Big)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which gives the stated result as the RHS is free of $U$ and $V$ . ", "page_idx": 23}, {"type": "text", "text": "C.5 Chaining up the loss function approximations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "By chaining up the prior results, we end up with the following result: ", "page_idx": 23}, {"type": "text", "text": "Proposition S7. There exists a non-empty set $\\Psi_{n}$ for each n such that, for any set $X\\subseteq\\mathbb{R}^{n\\times d}\\times\\mathbb{R}^{n\\times d}$ containing $0_{n\\times d}\\times0_{n\\times d},$ , we have for DeepWalk that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{(U,V)\\in\\Psi_{n}\\cap B_{2,\\infty}(\\tilde{A}_{2,\\infty})}\\big|\\mathcal{L}_{n}(U,V)-\\mathcal{R}_{n}(U,V)\\big|=O_{p}\\Big(\\Big(\\frac{\\log n}{n\\rho_{n}}\\Big)^{1/2}+\\tilde{A}_{2,\\infty}^{2}\\Big(\\frac{d}{n\\rho_{n}}\\Big)^{1/2}\\Big)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\bigg(\\underset{(U,V)\\in B_{2,\\infty}(\\tilde{A}_{2,\\infty})\\cap X}{\\arg\\operatorname*{min}}\\mathcal{L}_{n}(U,V)\\cup\\underset{(U,V)\\in B_{2,\\infty}(\\tilde{A}_{2,\\infty})\\cap X}{\\arg\\operatorname*{min}}\\mathcal{R}_{n}(U,V)\\subseteq\\Psi_{A}\\cap B_{2,\\infty}(\\tilde{A}_{2,\\infty})\\cap X\\bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=1-o(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For node2vec, the same result holds when we replace the $(\\log n/n\\rho_{n})^{1/2}$ term with an $o_{p}(1)$ term and add the constraint that $d\\ll n\\rho_{n}$ . The same result also holds when we constrain $U=V$ , but otherwise keep everything else unchanged. ", "page_idx": 23}, {"type": "text", "text": "C.6 Minimizers of $\\mathcal{R}_{n}(U,V)$ ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Recall that we have earlier defined ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathcal{R}_{n}(U,V):=\\frac{1}{n^{2}}\\sum_{i,j\\in[n]}\\Big\\{-f_{\\mathcal{P}}(\\lambda_{i},\\lambda_{j})\\rho_{n}W(\\lambda_{u},\\lambda_{v})\\log(\\sigma(\\langle u_{i},v_{j}\\rangle))}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad-f_{\\mathcal{N}}(\\lambda_{i},\\lambda_{j})\\log(1-\\sigma(\\langle u_{i},v_{j}\\rangle))\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We now want to reason about the minima of these functions. To do so, note that the optimization domain is non-convex - firstly due to the rank constraints on the matrix $U V^{T}$ , and secondly due to the fact that the loss function is invariant to any mapping $(U,V)\\rightarrow(U M,V M^{-1})$ for any invertible $d\\times d$ matrix $M$ . To handle the second part, we consider the global minima of this function when parameterized only in term of the matrix $U V^{T}$ . We will then see that the minima matrix is already low rank. ", "page_idx": 23}, {"type": "text", "text": "We first begin by giving some basic facts about the function $\\mathcal{R}_{n}(U,V)$ when parameterized as a function of $\\overbar{U}V^{\\tilde{T}}$ ", "page_idx": 23}, {"type": "text", "text": "Lemma S8. Define the modified function ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathsf{R}_{n}(M):=\\frac{1}{n^{2}}\\sum_{i,j\\in[n]}\\Big\\{-f_{\\mathcal{P}}(\\lambda_{i},\\lambda_{j})\\rho_{n}W(\\lambda_{u},\\lambda_{v})\\log(\\sigma(M_{i j}))-f_{N}(\\lambda_{i},\\lambda_{j})\\log(1-\\sigma(M_{i j}))\\Big\\}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "over all matrices $M\\in\\mathbb{R}^{n\\times n}$ . Then we have the following: ", "page_idx": 23}, {"type": "text", "text": "a) The function $\\mathcal{R}_{n}(M)$ is strictly convex in $M$ . ", "page_idx": 24}, {"type": "text", "text": "$b$ ) The global minimizer of $\\mathcal{R}_{n}(M)$ is given by ", "page_idx": 24}, {"type": "equation", "text": "$$\nM_{i j}^{*}=\\log\\Big(\\frac{f_{\\mathcal{P}}(\\lambda_{i},\\lambda_{j})\\rho_{n}W(\\lambda_{i},\\lambda_{j})}{f_{\\mathcal{N}}(\\lambda_{i},\\lambda_{j})}\\Big)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and satisfies $\\nabla_{M}\\mathcal{R}_{n}(M)=0$ . ", "page_idx": 24}, {"type": "text", "text": "c) When restricted to a cone of semi-positive definite matrices $M\\in\\mathcal{M}_{n}^{\\succ0}$ , there exists a unique minimizer to $\\mathcal{R}_{n}(M)$ over this set, which we call $M^{\\succcurlyeq0}$ . Moreover, $\\stackrel{.}{M}\\succeq0$ has the property that $\\langle\\nabla_{M}\\mathcal{R}_{n}(M^{\\ast0}),M^{\\ast0}-M\\rangle\\leq0$ for all $M\\in\\mathcal{M}_{n}^{\\succ0}$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. For part a), this follows by the fact that the functions $-\\log(\\sigma(x))$ and $-\\log(1-\\sigma(x))$ are positive and strictly convex functions of $x\\in\\mathbb R$ , the fact that $f_{\\mathcal{P}}(\\lambda_{i},\\lambda_{j})\\rho_{n}W(\\lambda_{i},\\lambda_{j})$ and $f_{\\mathcal{N}}(\\lambda_{i},\\lambda_{j})$ are positive quantities which are bounded above (see e.g Lemma S6), and the fact that the sum of strictly convex functions is strictly convex. For part b), this follows by noting that each of the $M_{i j}^{*}$ are pointwise minima of the functions ", "page_idx": 24}, {"type": "equation", "text": "$$\nr_{i j}(x)=-f_{\\mathcal{P}}(\\lambda_{i},\\lambda_{j})\\rho_{n}W(\\lambda_{u},\\lambda_{v})\\log(\\sigma(x)))-f_{\\mathcal{N}}(\\lambda_{i},\\lambda_{j})\\log(1-\\sigma(x))\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "defined over $x\\in\\mathbb R$ . Indeed, note that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{d r_{i j}}{d x}=(-1+\\sigma(x))f_{\\mathcal{P}}(\\lambda_{i},\\lambda_{j})\\rho_{n}W(\\lambda_{u},\\lambda_{v})+\\sigma(x)f_{\\mathcal{N}}(\\lambda_{i},\\lambda_{j}),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "so setting this equal to zero, rearranging and making use of the equality $\\sigma^{-1}(a/(a+b))=\\log(a/b)$ gives the stated result. Part c) is a consequence of strong convexity, the optimization domain being convex and self dual, and the KKT conditions. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "To understand the form of the the global minimizer of $\\mathcal{R}_{n}(M)$ in the DeepWalk case, by substituting in the values for $f_{\\mathcal P}(\\lambda_{i},\\lambda_{j})$ and $f_{\\mathcal{N}}(\\lambda_{i},\\lambda_{j})$ we end up with ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M_{i j}^{*}=\\log\\Big(\\frac{2P_{c(i),c(j)}\\mathcal{E}_{W}(\\alpha)}{\\big(1+k^{-1}\\big)\\mathbb{E}[\\theta]\\mathbb{E}[\\theta]^{\\alpha}\\big(\\theta_{j}^{\\alpha-1}\\tilde{P}_{c(i)}\\tilde{P}_{c(j)}^{\\alpha}+\\theta_{i}^{\\alpha-1}\\tilde{P}_{c(i)}^{\\alpha}\\tilde{P}_{c(j)}\\big)}\\Big)}\\\\ &{\\quad\\quad=\\log\\Big(\\frac{2\\mathcal{E}_{W}(\\alpha)}{\\big(1+k^{-1}\\big)\\mathbb{E}[\\theta]\\mathbb{E}[\\theta]^{\\alpha}}\\cdot\\frac{P_{c(i),c(j)}}{\\tilde{P}_{c(i)}\\tilde{P}_{c(j)}\\cdot\\big(\\theta_{i}^{\\alpha-1}\\tilde{P}_{c(i)}^{\\alpha-1}+\\theta_{j}^{\\alpha-1}\\tilde{P}_{c(j)}^{\\alpha-1}\\big)}\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In particular, from the above formula we get the following lemma as a consequence: ", "page_idx": 24}, {"type": "text", "text": "Lemma S9. Suppose that Scenarios $(i)$ or (iii) holds, so that either a) $\\theta_{i}$ is constant for all $i,$ or $^b$ ) $\\alpha=1$ . Then if we write $\\Pi_{C}\\in\\mathbb{R}^{n\\times\\kappa}$ for the matrix where $(\\Pi_{C})_{i l}=1[c(i)=l]$ , and define the matrix ", "page_idx": 24}, {"type": "equation", "text": "$$\n(\\widetilde{M}_{\\alpha}^{*})_{l m}=\\log\\Big(\\frac{2\\mathcal{E}_{W}(\\alpha)}{(1+k^{-1})\\mathbb{E}[\\theta]\\mathbb{E}[\\theta]^{\\alpha}}\\cdot\\frac{P_{l m}}{\\widetilde{P}_{m}\\widetilde{P}_{l}^{\\alpha}+\\widetilde{P}_{m}^{\\alpha}\\widetilde{P}_{l}}\\Big)\\ f o r\\ l,m\\in[\\kappa],\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "then we have that $M^{*}=\\Pi_{C}\\widetilde{M}_{\\alpha}^{*}\\Pi_{C}^{T}$ . In particular, as soon as the matrix $\\Pi_{C}$ is of full rank (which occurs with asymptotic probability $^{\\,l}$ ), then the rank of $M^{*}$ equals the rank of $\\widetilde{M}_{\\alpha}^{*}$ . Moreover, as soon as $d$ is greater than or equal to the rank of $\\widetilde{M}_{\\alpha}^{*}$ , $(U,V)$ is a minimizer of $\\mathcal{R}_{n}(U,V)$ if and only $i f\\,U V^{T}=\\bar{M}^{*}$ . ", "page_idx": 24}, {"type": "text", "text": "Under Scenario $(i i)$ , the same result applies noting that $f_{\\mathcal{P}}$ and $f_{\\mathcal{N}}$ are functions only of the underling communities, and so if we abuse notation and write e.g $f_{\\mathcal P}(l,m)$ to indicate the value of $f_{\\mathcal P}(\\lambda_{i},\\lambda_{j})$ when $c(i)=l$ and $c(j)=m$ , one can take ", "page_idx": 24}, {"type": "equation", "text": "$$\n(\\widetilde M^{*})_{l m}=\\log\\Big(\\frac{f_{\\mathcal{P}}(l,m)\\rho_{n}P_{l,m}}{f_{\\mathcal{N}}(l,m)}\\Big)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and have the above result hold. ", "page_idx": 24}, {"type": "text", "text": "We discuss in Appendix F what happens when we apply DeepWalk in the DCSBM regime when $\\alpha\\neq1$ . To give an example of what $M^{*}$ looks like, we write it down in the case of a $\\mathrm{SBM}(n,\\kappa,\\tilde{p},\\tilde{q},\\rho_{n})$ model, which is frequently used to illustrate the behavior of various community detection algorithms. Such a model assumes that the community assignments $\\pi_{l}=1/\\kappa$ for all $l\\in[\\kappa]$ , and that ", "page_idx": 25}, {"type": "equation", "text": "$$\nP_{k l}={\\binom{\\tilde{p}}{\\tilde{q}}}\\quad\\mathrm{if~}k=l,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In this case, we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widetilde{P}_{l}=\\frac{\\widetilde{p}+\\kappa(\\widetilde{q}-1)}{\\kappa}\\;\\mathrm{for}\\;l\\in[\\kappa],\\qquad\\mathcal{E}_{W}(\\alpha)=\\mathbb{E}[\\theta]^{\\alpha}\\mathbb{E}[\\theta^{\\alpha}]\\cdot\\Big(\\frac{\\widetilde{p}+(\\kappa-1)\\widetilde{q}}{\\kappa}\\Big)^{\\alpha}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Substituting these values into the matrix $\\widetilde{M}_{\\alpha}^{*}$ gives ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widetilde{M}_{\\alpha}^{*})_{l m}=\\log\\Big(\\frac{\\mathbb{E}[\\theta^{\\alpha}]}{\\mathbb{E}[\\theta](1+k^{-1})}\\cdot\\frac{\\kappa\\tilde{p}}{\\tilde{p}+(\\kappa-1)\\tilde{q}}\\Big)\\delta_{l m}+\\log\\Big(\\frac{\\mathbb{E}[\\theta^{\\alpha}]}{\\mathbb{E}[\\theta](1+k^{-1})}\\cdot\\frac{\\kappa\\tilde{q}}{\\tilde{p}+(\\kappa-1)\\tilde{q}}\\Big)(1-\\delta_{l m}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We highlight this is a matrix of the form $\\alpha\\delta_{l m}+\\beta(1-\\delta_{l m})$ , and so it is straightforward to describe the spectral behavior of the matrix (see Lemma S31). ", "page_idx": 25}, {"type": "text", "text": "C.6.1 Minimizers in the constrained regime $U=V$ ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In the case where we have constrained $U=V$ , it is not possible in general to write down the closed form of the minimizer of ${\\mathcal{R}}_{n}(M)$ over $\\mathcal{M}_{n}^{\\succ0}$ . However, it is still possible to draw enough conclusions about the form of the minimizer in order to give guarantees for community detection. We begin with the proposition below. We state the next two results for DeepWalk only, but note that the first generalizes to the node2vec case immediately. ", "page_idx": 25}, {"type": "text", "text": "Proposition S10. Suppose that $\\theta_{i}$ is constant across all i. Supposing that $\\widetilde{M}\\in\\mathbb{R}^{\\kappa\\times\\kappa}$ is of the form $\\widetilde{M}=\\widetilde{U}\\widetilde{U}^{T}$ for matrices $\\widetilde{U}\\in\\mathbb{R}^{\\kappa\\times d}$ , define the function ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{R}}_{n}(\\widetilde{M})=\\sum_{l,m\\in[\\kappa]}\\hat{p}_{n}(l)\\hat{p}_{n}(m)\\Big\\{-2k P_{l m}\\log\\sigma(\\langle u_{l},u_{m}\\rangle)-\\{\\widetilde{P}_{l}\\widetilde{P}_{m}^{\\alpha}+\\widetilde{P}_{m}\\widetilde{P}_{l}^{\\alpha}\\}\\log(1-\\sigma(\\langle u_{l},u_{m}\\rangle))\\Big\\}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we define $\\hat{p}_{n}(l):=n^{-1}|\\{i\\ :\\ c(i)=l\\}|\\,f o r\\,l\\in\\,[\\kappa]$ . Then ${\\widetilde{\\mathcal{R}}}_{n}({\\widetilde{M}})$ is strongly convex, and moreover has a unique minimizer as soon as $d\\geq\\kappa$ . ", "page_idx": 25}, {"type": "text", "text": "Moreover, any minimizer of ${\\mathcal{R}}_{n}(M)$ over matrices $M$ of the form $M=U U^{T}$ where $U\\in\\mathbb{R}^{n\\times d}$ must take the form $M=\\Pi_{C}M^{*}\\Pi_{C}^{T}$ where $(\\Pi_{C})_{i l}=1[c(i)=l]$ where $M^{*}$ is a minimizer of ${\\widetilde{\\mathcal{R}}}_{n}({\\widetilde{M}})$ . $I n$ particular, once $d\\geq\\kappa,$ , there is a unique minimizer to $\\mathcal{R}_{n}(M)$ . ", "page_idx": 25}, {"type": "text", "text": "Proof. The properties of ${\\widetilde{\\mathcal{R}}}_{n}({\\widetilde{M}})$ are immediate by similar arguments to Lemma S8 and standard facts in convex analysis. We begin by noting that if we substitute in the values ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\rho_{n}W(\\lambda_{i},\\lambda_{j})f_{\\mathcal{P}}(\\lambda_{i},\\lambda_{j})=\\frac{2k P_{c(i),c(j)}}{\\mathcal{E}_{W}(1)},}}\\\\ &{}&{f_{\\mathcal{N}}(\\lambda_{i},\\lambda_{j})=\\frac{l(k+1)}{\\mathcal{E}_{W}(1)\\mathcal{E}_{W}(\\alpha)}\\big(\\widetilde{P}_{c(i)}\\widetilde{P}_{c(j)}^{\\alpha}+\\widetilde{P}_{c(j)}\\widetilde{P}_{c(i)}^{\\alpha}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for $f_{\\mathcal P}(\\lambda_{i},\\lambda_{j})$ and $f_{\\mathcal{N}}(\\lambda_{i},\\lambda_{j})$ , then we can write that (recalling that $M_{i j}=\\left\\langle{u_{i},u_{j}}\\right\\rangle$ ) ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}_{n}(M):=\\displaystyle\\frac{1}{n^{2}}\\sum_{i,j\\in[n]}\\Big\\{-2k P_{c(i),c(j)}\\log\\sigma(\\langle u_{i},u_{j}\\rangle)}&{{\\scriptstyle(\\mathrm{S})}}\\\\ {\\displaystyle~~~~~~~~~~~~~~~~~-\\frac{l(k+1)}{\\mathcal{E}_{W}(1)\\mathcal{E}_{W}(\\alpha)}\\big(\\widetilde{P}_{c(i)}\\widetilde{P}_{c(j)}^{\\alpha}+\\widetilde{P}_{c(j)}\\widetilde{P}_{c(i)}^{\\alpha}\\big)\\log(1-\\sigma(\\langle u_{i},u_{j}\\rangle))\\Big\\}}&{{\\scriptstyle(\\mathrm{S})}}\\\\ {:=\\displaystyle\\sum_{l,m\\in[\\kappa]}\\hat{p}_{n}(l)\\hat{p}_{n}(m)\\Big\\{-2k P_{l m}\\frac{1}{|C_{l}||C_{m}|}\\displaystyle\\sum_{i\\in C_{l},j\\in C_{m}}\\log\\sigma(\\langle u_{i},u_{j}\\rangle)}&{{\\scriptstyle(\\mathrm{S})}}\\\\ {\\displaystyle~~~~~~~~~~~~~~~~~~-\\{\\widetilde{P}_{c(i)}\\widetilde{P}_{c(j)}^{\\alpha}+\\widetilde{P}_{c(j)}\\widetilde{P}_{c(i)}^{\\alpha}\\}\\displaystyle\\frac{1}{|C_{l}||C_{m}|}\\displaystyle\\sum_{i\\in C_{l},j\\in C_{m}}\\log(1-\\sigma(\\langle u_{i},u_{j}\\rangle))\\Big\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where for $l\\in[\\kappa]$ we define $\\hat{p}_{n}(l):=n^{-1}|\\{i\\,:\\,c(i)=l\\}|$ , along with the sets $\\mathcal{C}_{l}=\\{i\\,:\\,c(i)=l\\}$ . Now, note that as the functions $-\\log(\\sigma(x))$ and $-\\log(1-\\sigma(x))$ are strictly convex, by Jensen\u2019s inequality we have that e.g ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{1}{|\\mathcal{C}_{l}||\\mathcal{C}_{m}|}\\sum_{\\substack{i\\in\\mathcal{C}_{l},\\,j\\in\\mathcal{C}_{m}}}-\\log\\sigma(\\langle u_{i},u_{j}\\rangle)\\ge-\\log\\sigma\\Big(\\Big\\langle\\frac{1}{|\\mathcal{C}_{l}|}\\sum_{\\substack{i\\in\\mathcal{C}_{l}}}u_{i},\\frac{1}{|\\mathcal{C}_{m}|}\\sum_{\\substack{j\\in\\mathcal{C}_{m}}}u_{j}\\Big\\rangle\\Big)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "(where we also used bilinearity of the inner product) where equality holds above if and only if the $u_{i}$ are constant are across all indices $i$ . In particular, any minimizer of $\\mathcal{R}_{n}(M)$ must have the $u_{i}$ constant across $i\\in\\mathcal{C}_{l}$ for each $l\\in[\\kappa]$ , which defines the function $\\tilde{\\mathcal{R}}_{n}(\\widetilde{M})$ . This gives the claimed statement. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "In certain cases, we are able to give a closed form to the minimizer. We illustrate this for the case of the $\\mathrm{SBM}(n,\\kappa,\\tilde{p},\\tilde{q},\\rho_{n})$ model. ", "page_idx": 26}, {"type": "text", "text": "Proposition S11. Let $\\widetilde{M}^{*}$ be the unique minimizer of $\\tilde{\\mathcal{R}}_{n}(\\widetilde{M})$ as introduced in Proposition S10. In the case of a $S B M(n,\\kappa,\\tilde{p},\\tilde{q},\\rho_{n})$ model, we have that $\\kappa^{-2}\\|\\widetilde{M}^{*}-M^{*}\\|_{1}=O_{p}((\\kappa\\log\\kappa/n)^{1/4})$ , where $M^{*}$ is of the form ", "page_idx": 26}, {"type": "equation", "text": "$$\n(M^{*})_{i j}=\\alpha^{*}\\delta_{i j}-\\frac{\\alpha^{*}}{\\kappa-1}(1-\\delta_{i j})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for some $\\alpha^{*}=\\alpha^{*}(\\tilde{p},\\tilde{q})\\geq0$ . Moreover, $\\alpha^{*}>0$ iff $\\tilde{p}>\\tilde{q}$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. We begin by arguing that the objective function ${\\widetilde{\\mathcal{R}}}_{n}({\\widetilde{M}})$ converges uniformly to the objective ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\bar{\\mathcal{R}}_{n}(\\widetilde{M}):=\\frac{1}{\\kappa^{2}}\\sum_{\\substack{l,m\\in[\\kappa]}}\\bigg\\{-2k P_{l m}\\log\\sigma(\\langle u_{l},u_{m}\\rangle)-\\{\\widetilde{P}_{m}\\widetilde{P}_{l}^{\\alpha}+\\widetilde{P}_{l}\\widetilde{P}_{m}^{\\alpha}\\}\\log(1-\\sigma(\\langle u_{l},u_{m}\\rangle))\\bigg\\}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "over a set containing the minimizers of both functions. Note that this function is also strictly convex, and has a unique minimizer as soon as $d\\geq\\kappa$ . To do so, we highlight that as we have that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{k\\neq l}\\Big|\\frac{\\hat{p}_{n}(l)\\hat{p}_{n}(k)-\\kappa^{-2}}{\\kappa^{-2}}\\Big|=O_{p}\\Big(\\Big(\\frac{\\kappa\\log\\kappa}{n}\\Big)^{1/2}\\Big)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "by standard concentration results for Binomial random variables (e.g Proposition 47 of [11]), it follows that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\big|\\bar{\\mathcal{R}}_{n}(\\widetilde{M})-\\widetilde{\\mathcal{R}}_{n}(\\widetilde{M})\\big|\\leq\\bar{\\mathcal{R}}_{n}(\\widetilde{M})\\cdot O_{p}\\Big(\\Big(\\frac{\\kappa\\log\\kappa}{n}\\Big)^{1/2}\\Big).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Consequently, ${\\widetilde{\\mathcal{R}}}_{n}({\\widetilde{M}})$ converges to $\\bar{\\mathcal{R}}_{n}(\\widetilde{M})$ uniformly over any level set of $\\bar{\\mathcal{R}}_{n}(\\widetilde{M})$ , which necessarily contains the minima of $\\bar{\\mathcal{R}}_{n}(\\widetilde{M})$ . If one does so over the set (for example) ", "page_idx": 26}, {"type": "equation", "text": "$$\nA=\\{\\widetilde M\\,:\\,\\bar{\\mathcal{R}}_{n}(\\widetilde M)\\leq10\\bar{\\mathcal{R}}_{n}(0)\\}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "(for example), then as $\\bar{\\mathcal{R}}_{n}(0)$ is constant across $n$ , we have uniform convergence of (S67) over the set $A$ at a rate of $O_{p}\\big((\\log\\kappa/n p)^{1/2}\\big)$ . This argument can be reversed, which therefore ensures uniform convergence (over the same set) which contains the minimizers (with the minimizer of $\\tilde{\\mathcal{R}}_{n}(M)$ being contained within this set with asymptotic probability 1) at a rate of $O_{p}((\\kappa\\log\\kappa/n)^{1/2})$ . ", "page_idx": 26}, {"type": "text", "text": "With this, we note that an application of Lemma S33 gives that for any matrices $\\widetilde{M}_{1}$ and $\\widetilde{M}_{2}$ we have that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\mathcal{R}}_{n}(\\widetilde{M}_{1})\\geq\\bar{\\mathcal{R}}_{n}(\\widetilde{M}_{2})+\\langle\\Delta\\bar{\\mathcal{R}}_{n}(\\widetilde{M}_{2}),\\widetilde{M}_{1}-\\widetilde{M}_{2}\\rangle}\\\\ &{\\qquad\\qquad\\qquad+\\,\\displaystyle\\frac{C}{\\kappa^{2}}\\sum_{i,j\\in[\\kappa]}\\operatorname*{min}\\{|(\\widetilde{M}_{2})_{i j}-(\\widetilde{M}_{1})_{i j}|^{2},2|(\\widetilde{M}_{2})_{i j}-(\\widetilde{M}_{1})_{i j}|\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where to save on notation, we define ", "page_idx": 26}, {"type": "equation", "text": "$$\nC:=\\frac{1}{4}e^{-\\|\\widetilde{M}_{2}\\|_{\\infty}}\\operatorname*{min}_{l,m}\\{2k P_{l m},\\widetilde{P}_{m}\\widetilde{P}_{l}^{\\alpha}\\}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "In particular, if $\\widetilde{M}_{2}=\\bar{M}^{*}$ is an optimum of $\\bar{\\mathcal{R}}_{n}(\\widetilde{M})$ , then by the KKT conditions (similarly as in Lemma S8) we  have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\bar{\\mathcal{R}}_{n}(\\widetilde{M}_{1})-\\bar{\\mathcal{R}}_{n}(\\bar{M}^{*})\\geq\\frac{C}{\\kappa^{2}}\\sum_{i,j\\in[\\kappa]}\\operatorname*{min}\\{|(\\bar{M}^{*})_{i j}-(\\widetilde{M}_{1})_{i j}|^{2},2|(\\bar{M}^{*})_{i j}-(\\widetilde{M}_{1})_{i j}|\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "In particular, if we then let $\\widetilde{M}^{*}$ be any minimizer of ${\\widetilde{\\mathcal{R}}}_{n}({\\widetilde{M}})$ , then we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{C}{\\kappa^{2}}\\sum_{i,j\\in[\\kappa]}\\operatorname*{min}\\{|(\\bar{M}^{*})_{i j}-(\\widetilde{M}_{1})_{i j}|^{2},2|(\\bar{M}^{*})_{i j}-(\\widetilde{M}_{1})_{i j}|\\}}\\quad}&{}\\\\ &{\\leq\\bar{\\mathcal{R}}_{n}(\\widetilde{M}_{1})-\\bar{\\mathcal{R}}_{n}(\\bar{M}^{*})\\leq\\bar{\\mathcal{R}}_{n}(\\widetilde{M}_{1})-\\tilde{\\mathcal{R}}_{n}(\\bar{M}^{*})+\\tilde{\\mathcal{R}}_{n}(\\widetilde{M}^{*})-\\bar{\\mathcal{R}}_{n}(\\bar{M}^{*})}\\\\ &{\\leq2\\ \\underset{M\\in{\\cal A}}{\\operatorname*{sup}}\\left|\\tilde{\\mathcal{R}}_{n}(M)-\\bar{\\mathcal{R}}_{n}(M)\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "on an event of asymptotic probability 1. Consequently, it follows by Lemma S34 that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{1}{\\kappa^{2}}\\|\\bar{M}^{*}-\\widetilde{M}^{*}\\|_{1}=O_{p}\\big((\\kappa\\log\\kappa/n)^{1/4}\\big).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We now need to find the minimizing positive semi-definite matrix which optimizes $\\bar{\\mathcal{R}}_{n}(\\widetilde{M})$ . To do so, we will argue that one can find $\\alpha$ for which ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\widehat{M}_{i j}=\\alpha\\delta_{i j}-\\frac{\\alpha}{\\kappa-1}(1-\\delta_{i j}),\\quad\\nabla\\bar{\\mathcal{R}}_{n}(\\widehat{M})=C1_{\\kappa}1_{\\kappa}^{T},\\quad1_{\\kappa}=(1,\\cdots,1)^{T}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for some positive constant $C$ , as then the KKT conditions for the constrained optimization problem will hold. Indeed, for any positive definite matrix $M$ , as by definition ofM we have that $\\langle\\nabla\\bar{\\mathcal{R}}_{n}(\\widehat{M}),\\widehat{M}\\rangle\\;=\\;0$ as all of the eigenvectors ofM are orthogonal to the unit vector $1_{\\kappa}$ (Lemma S31). It consequently follows that as $\\nabla\\bar{\\mathcal{R}}_{n}(\\widehat{M})$ is itself positive definite, we get that $\\langle-\\nabla\\bar{\\mathcal{R}}_{n}(\\widehat{M}),\\widehat{M}-M\\rangle=\\langle\\nabla\\bar{\\mathcal{R}}_{n}(\\widehat{M}),M\\rangle\\geq0.$ We now need to verify the existence of a constant $\\alpha$ for which this condition holds. We note that as $\\widehat{M}_{i j}$ is constant across $i=j$ , and also constant across $i\\neq j$ , to verify the condition that $\\nabla\\bar{\\mathcal{R}}_{n}(\\widehat{M})$ is proportional to $1_{\\kappa}1_{\\kappa}^{T}$ , it suffices to check whether the on and off diagonal terms of $\\nabla\\bar{\\mathcal{R}}_{n}(\\widehat{M})$ are equal to each other. This gives the equation ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma(\\alpha)\\cdot\\Bigl(k\\tilde{p}+l(k+1)\\frac{\\tilde{p}+(\\kappa-1)\\tilde{q}}{\\kappa}\\Bigr)}\\\\ &{\\qquad\\qquad=k(\\tilde{p}-\\tilde{q})+\\sigma(-\\alpha/(\\kappa-1))\\Bigl(k\\tilde{q}+l(k+1)\\frac{\\tilde{p}+(\\kappa-1)\\tilde{q}}{\\kappa}\\Bigr)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By applying Lemma S32, this has a singular positive solution in $\\alpha$ if and only if $k(\\tilde{p}{-}\\tilde{q})\\geq k(\\tilde{p}{-}\\tilde{q})/2$ , which holds iff $\\tilde{p}\\geq\\tilde{q}$ . In the case where $\\tilde{p}<\\tilde{q}$ , it follows that the solution has $\\alpha=0$ . \u5382 ", "page_idx": 27}, {"type": "text", "text": "C.7 Strong convexity properties of the minima matrix ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Proposition S12. Define the modified function ", "text_level": 1, "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathsf{R}_{n}(M):=\\frac{1}{n^{2}}\\sum_{i,j\\in[n]}\\Big\\{-f_{\\mathcal{P}}(\\lambda_{i},\\lambda_{j})\\rho_{n}W(\\lambda_{u},\\lambda_{v})\\log(\\sigma(M_{i j}))-f_{\\mathcal{N}}(\\lambda_{i},\\lambda_{j})\\log(1-\\sigma(M_{i j}))\\Big\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "over all matrices $\\boldsymbol{M}\\,\\,\\,\\in\\,\\,\\,\\mathbb{R}^{n\\times n}$ . Then we have for any matrices $M_{1},M_{2}\\mathrm{\\boldmath~\\Omega~}\\in\\mathrm{\\boldmath~\\mathbb{R}^{n\\times n}~}$ with $\\|M_{1}\\|_{\\infty},\\|M_{2}\\|_{\\infty}\\leq\\tilde{A}_{\\infty}$ that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{R}_{n}(M_{1})\\geq\\mathcal{R}_{n}(M_{2})+\\langle\\nabla\\mathcal{R}_{n}(M_{2}),M_{1}-M_{2}\\rangle+\\frac{\\widetilde{C}e^{-\\tilde{A}_{\\infty}}}{2}\\cdot\\frac{1}{n^{2}}\\|M_{1}-M_{2}\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\begin{array}{r l r}{\\tilde{C}}&{{}=}&{\\operatorname*{min}_{l,m}\\{2k P_{l,m},\\tilde{P}_{l}^{\\alpha}\\tilde{P}_{m}\\}}\\end{array}$ for Scenarios (i) and (iii), and $\\begin{array}{r l}{\\widetilde{C}}&{{}=}\\end{array}$ $\\operatorname*{min}\\{\\|\\rho_{n}f_{\\mathcal{P}}(\\lambda,\\lambda^{\\prime})\\|_{-\\infty},\\|f_{\\mathcal{N}}(\\lambda,\\lambda^{\\prime})\\|_{-\\infty}\\}>0$ for Scenario (ii). Moreover, ", "page_idx": 27}, {"type": "text", "text": "i) If $\\mathcal{R}_{n}(M)$ is constrained over a set $\\begin{array}{r}{\\mathcal{X}=\\{M=U V^{T}\\,:\\,U,V\\in\\mathbb{R}^{n\\times d},\\|M\\|_{\\infty}\\leq\\tilde{A}_{\\infty}\\},}\\end{array}$ , and there exists $M^{*}$ in $\\mathcal{X}$ such that $\\nabla\\mathcal{R}_{n}(M^{*})=0$ , then we have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{1}{n^{2}}\\|M^{*}-M\\|_{F}^{2}\\leq2\\widetilde{C}^{-1}e^{\\tilde{A}_{\\infty}}\\cdot\\left(\\mathcal{R}_{n}(M)-\\mathcal{R}_{n}(M^{*})\\right)f o r\\,a l l\\;M\\in\\mathcal{X}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "ii) If ${\\mathcal{R}}_{n}(M)$ is constrained over a set $\\mathcal{X}^{\\geq0}=\\{M=U U^{T}\\,:\\,U\\in\\mathbb{R}^{n\\times d},\\|M\\|_{\\infty}\\leq\\tilde{A}_{\\infty}\\,j,$ , and there exists $M^{*}$ in $\\chi{\\geq}0$ such that $\\langle\\nabla\\mathcal{R}_{n}(M^{*}),M-M^{*}\\rangle\\geq0$ for all $M\\in\\mathcal{X}^{\\geq0}$ , then we get the same inequality as in part $i,$ ) above. ", "page_idx": 28}, {"type": "text", "text": "Proof. The first inequality follows by an application of Lemma S33, with the second and third parts following by applying the conditions stated and rearranging. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "C.8 Convergence of the gram matrices of the embeddings ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "By combining together Proposition S12 and Proposition S7 we end up with the following result: ", "page_idx": 28}, {"type": "text", "text": "Theorem S13. Suppose that the conditions of Lemma $_{S9}$ hold. (In particular, recall that $d\\geq\\kappa.$ .) Then there exist constants $\\tilde{A}_{\\infty}$ and $\\tilde{A}_{2,\\infty}$ (depending on the parameters of the model and sampling scheme) and a matrix $M^{\\ast}\\in\\mathbb{R}^{\\kappa\\times\\kappa}$ (also depending on the parameters of the model and the sampling scheme) such that for any minimizer $(U^{*},V^{*})$ of ${\\mathcal{L}}(U,V)$ over the set ", "page_idx": 28}, {"type": "equation", "text": "$$\nX=\\{(U,V)\\,:\\,\\|U\\|_{\\infty},\\|V\\|_{\\infty}\\leq\\tilde{A}_{\\infty},\\|U\\|_{2,\\infty},\\|V\\|_{2,\\infty}\\leq\\tilde{A}_{2,\\infty}\\},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "we have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{1}{n^{2}}\\sum_{\\substack{i,j\\in[n]}}\\left(\\langle u_{i}^{*},v_{j}^{*}\\rangle-M_{c(i),c(j)}^{*}\\right)^{2}=C\\cdot\\left\\{O_{p}((\\frac{\\operatorname*{max}\\{\\log n,d\\}}{n\\rho_{n}})^{1/2})\\quad u n d e r\\ S c e n a r i o s\\ (i)\\ a n d\\ (i i i);\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for some constant $C$ depending on the model, the node2vec hyperparameters, $\\tilde{A}_{\\infty}$ and $\\tilde{A}_{2,\\infty}$ . In the case where we constrain $U=V$ , the same result holds provided the conditions of Proposition $S I O$ hold. ", "page_idx": 28}, {"type": "text", "text": "Proof. We note that by Lemma S9, there exists a minimizer $\\widetilde{M}^{*}$ for ${\\mathcal{R}}_{n}(M)$ of the form $\\widetilde{M}^{*}\\,=$ $\\Pi M^{*}\\Pi^{T}$ for a matrix $M^{\\ast}\\in\\mathbb{R}^{\\kappa\\times\\kappa}$ . We can then take $\\tilde{A}_{\\infty}$ and $\\tilde{A}_{2,\\infty}$ as $2\\|M^{\\ast}\\|_{\\infty}$ and $2\\Vert M^{*}\\Vert_{2,\\infty}$ . We highlight that we can do this even when $d>\\kappa$ , as we can embed $M^{*}$ into the block diagonal matrix $\\mathrm{diag}(M^{*},{\\cal O}_{d-\\kappa,d-\\kappa})$ , which preserves both the norms above. Lemma S8 and Proposition S12 then guarantee that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{1}{n^{2}}\\|U^{*}(V^{*})^{T}-\\widetilde{M}^{*}\\|_{F}^{2}\\le\\tilde{C}\\cdot\\big(\\mathcal{R}_{n}(U V^{T})-\\mathcal{R}_{n}(\\widetilde{M}^{*})\\big)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for some constant $\\tilde{C}$ depending only on the quantities mentioned in the theorem statement. As $\\mathcal{X}$ is a subset of $B_{2,\\infty}(\\tilde{A}_{2,\\infty})$ , and $(U^{*},V^{*})$ is a minimizer of ${\\mathcal{L}}(U,V)$ , we end up getting that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\mathcal{R}_{n}(U V^{T})-\\mathcal{R}_{n}(\\widetilde{M}^{*})\\right)}\\\\ &{\\qquad\\qquad\\leq\\mathcal{R}_{n}(U V^{T})-\\mathcal{L}_{n}(U^{*},V^{*})+\\mathcal{L}_{n}(M^{*})-\\mathcal{R}_{n}(\\widetilde{M}^{*})}\\\\ &{\\qquad\\qquad\\leq2\\underset{(U,V)\\in\\mathcal{X}}{\\operatorname*{sup}}\\left|\\mathcal{R}_{n}(U,V)-\\mathcal{L}_{n}(U,V)\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "from which we can apply Proposition S7 to then give the claimed result. ", "page_idx": 28}, {"type": "text", "text": "We give some brief intuition as to the size of the constants involved here, to understand any potential hidden dependencies involved in them. Of greatest concern are the constants $\\tilde{A}_{\\infty}$ and $\\bar{A}_{2,\\infty}$ (as the remaining constants are explicit throughout the proof, and depend only on the hyperparameters of the sampling schema and the model in a polynomial fashion). Note that in the case where $k$ is large and we have a $\\mathrm{SBM}(n,\\kappa,\\tilde{p},\\tilde{q},\\rho_{n})$ model and we apply the DeepWalk scheme, from the discussion after Lemma S9, the minimizing matrix $M^{*}$ takes the form ", "page_idx": 28}, {"type": "equation", "text": "$$\n(M^{*})_{l m}\\approx\\log\\Big(\\frac{\\kappa\\widetilde{p}}{\\widetilde{p}+(\\kappa-1)\\widetilde{q}}\\Big)\\delta_{l m}+\\log\\Big(\\frac{\\kappa\\widetilde{q}}{\\widetilde{p}+(\\kappa-1)\\widetilde{q}}\\Big)(1-\\delta_{l m}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Supposing for simplicity that $\\tilde{p}\\,>\\,\\tilde{q}$ , it follows that we can take can take $\\tilde{A}_{\\infty}$ to be of the order $O(\\mathrm{log}(\\tilde{p}/\\bar{\\tilde{q}}))$ when $\\kappa$ is large. In the rate from Proposition S12, this gives a rate of $O(\\tilde{p}/\\tilde{q})$ from the $e^{\\tilde{A}_{\\infty}}$ factor; note that the dependence on the parameters of the models here are not unreasonable. As for $\\tilde{A}_{2,\\infty}$ , we first highlight the fact that ", "page_idx": 29}, {"type": "equation", "text": "$$\n(\\kappa-1)\\log\\Big(\\frac{\\kappa\\tilde{q}}{\\tilde{p}+(\\kappa-1)\\tilde{q}}\\Big)\\to\\frac{\\tilde{p}-\\tilde{q}}{\\tilde{q}}\\;\\mathrm{as}\\;\\kappa\\to\\infty.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By Lemma S31 we can therefore take $\\tilde{A}_{2,\\infty}$ to be a scalar multiple of $|\\log(\\tilde{p}/\\tilde{q})|^{1/2}$ , avoiding any implicit dependence on $\\kappa$ or the embedding dimension $d$ . ", "page_idx": 29}, {"type": "text", "text": "C.9 Convergence of the embedding vectors ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We can then get results guaranteeing the convergence of the individual embedding vectors (rather than their gram matrix) up to rotations, as stated by the following theorem. ", "page_idx": 29}, {"type": "text", "text": "Theorem S14. Suppose that the conclusion of Theorem S13 holds, and further suppose that d equals the rank of the matrix $M^{*}$ . Then there exists a matrix $\\tilde{U}^{*}\\in\\mathbb{R}^{\\kappa\\times d}$ such that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{Q\\in O(d)}\\frac{1}{n}\\sum_{i=1}^{n}\\|u_{i}^{*}-\\tilde{u}_{c(i)}^{*}Q\\|_{2}^{2}=C\\cdot\\left\\{\\mathop{O_{p}}_{p}\\bigl((\\frac{\\operatorname*{max}\\{\\log n,d\\}}{n\\rho_{n}})^{1/2}\\bigr)\\quad u n d e r\\;S c e n a r i o s\\;(i)\\;a n d\\;(i i i);\\right.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. We handle the cases where $U\\neq V$ and $U=V$ separately. For the case where $U\\neq V$ , we note that without loss of generality we can suppose that $U\\dot{U}^{T}=\\dot{V}V^{T}$ , in which case we can apply Lemma S23 and Theorem S13 to give the stated result. To do so, we note that by Lemma S25 we have that $n^{-1}\\sigma_{d}(\\Pi M^{*}\\Pi^{T})\\geq c\\sigma_{d}(\\bar{M^{*}})$ for some constant $c$ with asymptotic probability 1, as a result of the fact that $n_{k}(\\Pi)\\ge1/2n\\pi_{k}$ with asymptotic probability 1 uniformly across all communities $k\\in[\\kappa]$ . As moreover we have that $n^{-1}\\|\\dot{U}V^{\\hat{T}}-\\Pi\\dot{M}^{*}\\Pi^{T}\\|_{\\mathrm{op}}\\dot{\\le}\\;n^{-1}\\|U V^{\\bar{T}}-\\Pi M^{*}\\Pi^{T}\\|_{F}=o_{p}(1),$ , the condition that $\\|U V^{T}-\\Pi M^{*}\\Pi^{T}\\|_{\\mathrm{op}}\\leq1/2\\sigma_{d}(\\Pi M^{*}\\Pi^{T})$ holds with asymptotic probability 1, we have verified the conditions in Lemma S23, giving the desired result. In the case where we constrain $U=V$ , the same argument holds, except we no longer need to verify the condition that $\\lVert U U^{*}-M^{*}\\rVert_{\\mathrm{op}}$ is sufficiently small, and so we have concluded in this case also. \u53e3 ", "page_idx": 29}, {"type": "text", "text": "In the case of a $\\mathrm{SBM}(n,\\kappa,\\tilde{p},\\tilde{q},\\rho_{n})$ model it is actually able to give closed form expressions for the embedding vectors which are converged to by factorizing the minima matrix $M^{*}$ in the way described by the above proof. These details are given in Lemma S31. ", "page_idx": 29}, {"type": "text", "text": "D Proof of Theorem 4 and Corollary 5 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "D.1 Guarantees for community detection ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We begin with a discussion of how we can get guarantees for community detection via approximate $\\boldsymbol{\\mathrm{k}}$ -means clustering method, using the convergence criteria for embeddings we have derived already. To do so, suppose we have a matrix $U\\in\\mathbb{R}^{n\\times\\overline{{d}}}$ corresponding of $n$ columns of $d$ -dimensional vectors. Defining the set ", "page_idx": 29}, {"type": "equation", "text": "$$\nM_{n,K}:=\\{\\Pi\\in\\{0,1\\}^{n\\times K}\\,:\\,\\mathrm{each\\,row\\,\\,of\\,\\,II\\,has\\,\\,exactly\\,\\,}K-1\\,\\,\\mathrm{zero\\,\\,entries}\\},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "we seek to find a factorization $U\\approx\\Pi X$ for matrices $\\Pi\\in M_{n,K}$ and $X\\in\\mathbb{R}^{K\\times d}$ . To do so, we minimize the objective ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k}(\\Pi,X)=\\frac{1}{n}\\|U-\\Pi X\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "In practice, this minimization problem is NP-hard [5], but we can find $(1+\\epsilon)$ -approximate solutions in polynomial time [25]. As a result, we consider any minimizers \u02c6\u03a0 and X\u02c6 such that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k}(\\hat{\\Pi},\\hat{X})\\leq(1+\\epsilon)\\operatorname*{min}_{\\Pi,X}\\mathcal{L}_{k}(\\Pi,X).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We want to examine the behavior of $\\boldsymbol{\\mathrm{k}}$ -means clustering on the matrix $U$ , when it is close to a matrix $U^{*}$ which has an exact factorization $U^{*}=\\Pi^{*}X^{*}$ for some matrices $\\Pi^{*}\\in{\\cal M}_{n,K}$ and $X^{*}\\in\\mathbb{R}^{K\\times d}$ . We introduce the notation ", "page_idx": 30}, {"type": "equation", "text": "$$\nG_{k}(\\Pi):=\\{i\\in[n]:\\Pi_{i k}=1\\},\\qquad n_{k}(\\Pi):=|G_{k}(\\pi)|\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for the columns of $U$ which are assigned as closest to the $k$ -th column of $X$ as according to the matrix $\\Pi$ . ", "page_idx": 30}, {"type": "text", "text": "We make use of the following theorem from Lei and Rinaldo [30], which we restate for ease of use. ", "page_idx": 30}, {"type": "text", "text": "Proposition S15 (Lemma 5.3 of Lei and Rinaldo [30]). Let $(\\hat{\\Pi},\\hat{X})$ be any $(1+\\epsilon)$ -approximate minimizer to the $k$ -means problem given a matrix $U\\in\\mathbb{R}^{n\\times d}$ . Suppose that $U^{*}=\\Pi^{*}X^{*}$ for some matrices $\\Pi^{*}\\in{\\cal M}_{n,\\kappa}$ and $\\bar{X}^{\\ast}\\in\\mathbb{R}^{\\breve{\\kappa}\\times d}$ . Fix any $\\begin{array}{r}{\\delta_{k}\\leq\\operatorname*{min}_{l\\neq k}\\|X_{l\\cdot}^{*}-X_{k\\cdot}^{*}\\|_{2},}\\end{array}$ , and suppose that the condition ", "page_idx": 30}, {"type": "equation", "text": "$$\n(16+8\\epsilon)\\|U-U^{*}\\|_{F}^{2}/\\delta_{k}^{2}<n_{k}(\\Pi^{*})\\,f o r\\,a l l\\,k\\in[\\kappa]\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "holds. Then there exist subsets $S_{k}\\subseteq G_{k}(\\Pi^{*})$ and a permutation matrix $\\sigma\\in\\mathbb{R}^{\\kappa\\times\\kappa}$ such that the following holds: ", "page_idx": 30}, {"type": "text", "text": "i) For $G=\\bigcup_{k}(G_{k}(\\Pi^{*})\\setminus S_{k}),$ , we have that $(\\Pi^{*})_{G}.=\\sigma\\Pi_{G}$ \u00b7. In words, outside of the sets $S_{k}$ we recover the assignments given by $\\Pi^{*}$ up to a re-labelling of the clusters. ", "page_idx": 30}, {"type": "equation", "text": "$\\begin{array}{r}{\\sum_{k=1}^{\\kappa}|S_{k}|\\delta_{k}^{2}\\le(16+8\\epsilon)\\|U-U^{*}\\|_{F}^{2}\\,h o l d s.}\\end{array}$ ", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "In particular, we can then apply this to our consistency results with the embeddings learned by node2vec. Recall that we are interested in the following metrics measuring recovery of communities by any given procedure: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\cal L}(c,\\hat{c}):=\\operatorname*{min}_{\\sigma\\in\\mathrm{Sym}(\\kappa)}\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}1[\\hat{c}(i)\\neq\\sigma(c(i))],}\\\\ {\\displaystyle\\widetilde{\\cal L}(c,\\hat{c}):=\\operatorname*{max}_{k\\in[\\kappa]}\\operatorname*{min}_{\\sigma\\in\\mathrm{Sym}(\\kappa)}\\displaystyle\\frac{1}{|\\mathcal{C}_{k}|}\\sum_{i\\in\\mathcal{C}_{k}}1[\\hat{c}(i)\\neq\\sigma(k)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "These measure the overall misclassification rate and worst-case class misclassification rate respectively. ", "page_idx": 30}, {"type": "text", "text": "Corollary S16. Suppose that we have embedding vectors $\\omega_{i}\\in\\mathbb{R}^{d}$ for $i\\in[n]$ such that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{Q\\in O(d)}\\frac{1}{n}\\sum_{i=1}^{n}\\|\\omega_{i}-\\eta_{C(i)}Q\\|_{2}^{2}=O_{p}(r_{n})\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for some rate function $r_{n}\\to0$ as $n\\to\\infty$ and vectors $\\eta_{l}\\in\\mathbb{R}^{d}$ for $l\\in[\\kappa]$ . Moreover suppose that $\\begin{array}{r}{\\delta:=\\operatorname*{min}_{l\\neq k}\\|\\eta_{l}-\\eta_{k}\\|_{2}>0}\\end{array}$ . Then if $\\hat{c}(i)$ are the community assignments produced by applying a $(1+\\epsilon)$ -approximate $k$ -means clustering to the matrix whose columns are the $\\omega_{i}$ , we have that $L(c,\\hat{c})\\,=\\,O_{p}(\\delta^{-2}r_{n})$ and $\\widetilde{L}(c,\\hat{c})\\,=\\,{\\cal O}_{p}(\\delta^{-2}r_{n})$ . If the RHS of (S96) is instead $o_{p}(1)$ , then we replace $O_{p}(r_{n})$ by $o_{p}(1)$ in the statements for $L(c,{\\hat{c}})$ and $\\widetilde{L}(\\boldsymbol{c},\\hat{\\boldsymbol{c}})$ . ", "page_idx": 30}, {"type": "text", "text": "Proof. We apply Proposition S15 with $\\Pi^{*}$ corresponding to the matrix of community assignments according to $c(\\cdot)$ , and $X^{*}$ the matrix whose columns are the $Q\\eta_{l}$ for $l\\in[\\kappa]$ where $Q\\in O(d)$ attains the minimizer in (S96). Letting $U$ be the matrix whose columns are the $\\omega_{i}$ and taking $\\delta_{k}=\\delta$ , the condition (S93) to verify becomes ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{16+8\\epsilon}{\\delta^{2}}\\frac{1}{n}\\sum_{i=1}^{n}\\|\\omega_{i}-Q\\eta_{c(i)}\\|_{2}^{2}<\\frac{|\\mathcal{C}_{k}|}{n}\\mathrm{~for~all~}k\\in[\\kappa].\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "As $r_{n}\\to0$ and $|{\\mathcal{C}}_{l}|/n>c>0$ for some constant $c$ uniformly across vertices $l\\in[\\kappa]$ with asymptotic probability 1 (as a result of the community generation mechanism, the communities are balanced), the above event will be satisfied with asymptotic probability 1. The desired conclusion follows by making use of the inequalities ", "page_idx": 30}, {"type": "equation", "text": "$$\nL(c,\\hat{c})\\leq\\frac{1}{n}\\sum_{k\\in[\\kappa]}|S_{k}|,\\qquad\\widetilde{L}(c,\\hat{c})\\leq\\operatorname*{max}_{k\\in[\\kappa]}\\frac{1}{|\\mathcal{C}_{k}|}|S_{k}|\\leq\\left(\\operatorname*{max}_{k\\in[\\kappa]}\\frac{n}{|\\mathcal{C}_{k}|}\\right)\\cdot\\frac{1}{n}\\sum_{l\\in[\\kappa]}|S_{l}|\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which hold by the first consequence in Proposition S15, and then applying the bound ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{k\\in[\\kappa]}|S_{k}|\\leq\\frac{16+8\\epsilon}{\\delta^{2}}\\cdot\\frac{1}{n}\\sum_{i=1}^{n}\\|\\omega_{i}-Q\\eta_{c(i)}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We note that in order to apply this theorem, we require the further separation criterion of $\\delta>0$ . As a result of Lemma S31, we can guarantee this for the $\\mathrm{SBM}(n,\\kappa,\\tilde{p},\\tilde{q},\\rho_{n})$ model when either a) DeepWalk is trained in the unconstrained setting, or b) we are in the constrained setting with $\\tilde{p}>\\tilde{q}$ . As we know that the embedding vectors converge to the zero vector on average when we are in the constrained setting with $\\tilde{p}\\leq\\tilde{q}$ , as a result we know that community detection is possible in the constrained setting iff $\\tilde{p}>\\tilde{q}$ , which gives Corollary 5 of the main paper. ", "page_idx": 31}, {"type": "text", "text": "D.2 Guarantees for node classification and link prediction ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We now discuss what guarantees we can make when using the embedding vectors for classification. In this section, we suppose that we have a guarantee ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\operatorname*{min}_{Q\\in O(d)}\\sum_{i=1}^{n}\\|u_{i}-\\eta_{C(i)}Q\\|_{2}^{2}\\leq C(\\tau)r_{n}\\qquad\\mathrm{~holds~with~probability~}\\geq1-\\tau\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for some constant $C(\\tau)$ and rate function $r_{n}\\to0$ as $n\\to\\infty$ . This is the same as saying that the LHS is $O_{p}(r_{n})$ - it will happen to be more convenient to use this formulation. We also suppose that there exists a positive constant $\\delta>0$ for which ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\delta\\leq\\operatorname*{min}_{k\\neq l}\\|\\eta_{k}-\\eta_{l}\\|_{2}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We begin with a lemma which discusses the underlying geometry when we take a small sample of the embedding vectors. ", "page_idx": 31}, {"type": "text", "text": "Lemma S17. Suppose we sample $K$ embeddings from the set $(u_{i})_{i\\in[n]}$ , which we denote as $u_{i_{1}},\\ldots,u_{i_{K}}$ . Define the sets ", "page_idx": 31}, {"type": "equation", "text": "$$\nS_{l}=\\{i\\in\\mathcal{C}_{l}\\,:\\,\\|u_{i}-\\eta_{C(i)}\\|_{2}<\\delta/4\\}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then there exists $n_{0}(K,\\delta,\\tau^{\\prime})$ such that if $n\\geq n_{0}$ , with probability $1-\\tau^{\\prime}$ we have that $u_{i_{j}}\\in S_{c(i_{j})}$ for all $j\\in[K]$ . ", "page_idx": 31}, {"type": "text", "text": "Proof. Without loss of generality, we will suppose that $Q=I$ . For each $l\\in[\\kappa]$ , define the sets $S_{l}=\\{i\\in\\mathcal{C}_{l}:\\|u_{i}-\\eta_{l}\\|_{2}\\leq\\delta/\\bar{4}\\}$ . Then by the condition (S100), by Markov\u2019s inequality we know that with probability $1-\\tau$ we have that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac1n\\sum_{l\\in[\\kappa]}|\\mathcal{C}_{l}\\setminus S_{l}|\\le4\\delta^{-2}C(\\tau/2)r_{n}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We now suppose that we sample $K$ embeddings uniformly at random; for convenience, we suppose that they are done so with replacement. Then the probability that all of the embeddings are outside the set $\\bar{\\bigcup_{l}}(\\mathcal{C}_{l}\\setminus S_{l})$ is given by $\\begin{array}{r}{(1-\\frac{1}{n}\\sum_{l}|\\mathcal{C}_{l}\\setminus S_{l}|)^{\\ast}\\geq1-\\frac{\\'\\kappa}{n}\\sum_{l}|\\mathcal{C}_{l}\\setminus S_{l}|}\\end{array}$ . In particular, this means with probability no less than $1-\\tau-4K\\delta^{-1}C(\\tau)r_{n}$ , if we sample $K$ embeddings with indices $i_{1},\\ldots,i_{K}$ at random from the set of $n$ embeddings, they lie within the sets $S_{C(i_{1})},\\ldots,S_{C(i_{K})}$ respectively. The desired result then follows by noting that we take $\\tau=\\tau^{\\prime}/2$ , and choose $n$ such that $4\\delta^{\\stackrel{\\cdot}{-2}}C(\\tau/2)r_{n}<\\tau^{\\prime}/2$ . \u53e3 ", "page_idx": 31}, {"type": "text", "text": "To understand how this lemma can give insights into the downstream use of embeddings, suppose that we have access to an oracle which provides the community assignments of a vertex when requested, but otherwise the community assignments are unseen. ", "page_idx": 31}, {"type": "text", "text": "We note that in practice, only a small number of labels are needed to be provided to embedding vectors in order to achieve good classification results (see e.g the experiments in Hamilton et al. [17], Veli\u02c7ckovi\u00b4c et al. [47]). As a result, we can imagine keeping $K$ fixed in the regime where $n$ is large. Moreover, the constant $\\delta$ simply reflects the underlying geometry of the learned embeddings, and $\\tau^{\\prime}$ is a tolerance we can choose such that the stated result is very likely to hold (by e.g choosing $\\tau^{\\prime}=10^{-2}$ or $10^{-3}$ ). As a consequence, the above lemma tells us with high probability, we can ", "page_idx": 31}, {"type": "text", "text": "i) learn a classifier which is able to distinguish between the sets $S_{l}$ given use of the sampled embeddings $u_{i_{1}},\\ldots,u_{i_{K}}$ and the labels $c(i_{1}),\\dots,c(i_{K})$ , provided the classifier is flexible enough to separate $\\kappa$ disjoint convex sets; and   \nii) as a consequence of (S103), this classifier will correctly classify a large proportion of vertices within the correct sets $S_{l}$ . ", "page_idx": 32}, {"type": "text", "text": "The same argument applies if instead we have classes assigned to embedding vectors which form a coarser partitioning of the underlying community assignments. The importance of the above result is that in order to understand the behavior of embedding methods for classification, it suffices to understand which geometries particular classifiers are able to separate - for example, when the number of classes equals 2, this reduces down to the classic concept of linear separability, in which case a logistic classifier would suffice. ", "page_idx": 32}, {"type": "text", "text": "We end with a discussion as to the task of link prediction, which asks to predict whether two vertices are connected or not given a partial observation of the network. To do so, we suppose that from the observed network, we delete half of the edges in the network, and then train node2vec on the resulting network. Note that the node2vec mechanism only makes explicit use of known edges within the network. This corresponds to training the node2vec model on the data with sparsity factor $\\rho_{n}\\to\\rho_{n}/2$ ; in particular, this leaves the underlying asymptotic representations unchanged and slows the rate of convergence by a factor of 2. With this, a link prediction classifier is formed by the following process: ", "page_idx": 32}, {"type": "text", "text": "1. Take a set of edges $J\\,\\subseteq\\,\\{(i,j)\\ :\\ a_{i j}\\,=\\,1\\}$ for which the node2vec algorithm was not trained on, and a set of non-edges $\\tilde{J}\\,\\subseteq\\,\\{(i,j)\\,:\\,a_{i j}\\,=\\,0\\}$ . As in practice networks are sparse, these sets are not sampled randomly from the network, but are assumed to be sampled in a balanced fashion so that the sets $J$ and $\\tilde{J}$ are roughly balanced in size. One way of doing so is to pick a number of edges in advance, say $E$ , and then sample $E$ elements from the set of edges and non-edges in order to form $J$ and $\\tilde{J}$ respectively.   \n2. Form edge embeddings $e_{i j}=f(u_{i},u_{j})$ given some symmetric function $f(x,y)$ and node embeddings $u_{i}$ . Two popular choices of functions are the average function $f(x,y)\\;=$ $(x+y)/2$ and the Hadamard product $f(x,y)=(x_{i}y_{i})_{i\\in[d]}$ .   \n3. Using the features $e_{i j}$ and the labels provided by the sets $J$ and $\\tilde{J}$ , build a classifier using your favorite ML algorithm. ", "page_idx": 32}, {"type": "text", "text": "By our convergence guarantees, we know that the asymptotic distribution of the edge embeddings $e_{i j}$ will approach some vectors $\\eta_{c(i),c(j)}\\in\\mathbb{R}^{d}$ , giving at most $\\kappa^{2}$ distinct vectors overall. Note that these embedding vectors in of themselves contain little information about whether the edges are connected; that said, even given perfect information of the communities and the connectivity matrix $P$ , one can only form probabilistic guesses as to whether two vertices are connected. That said, by clustering together the link embeddings we can identify together edges as having vertices belonging to a particular pair of communities. With knowledge of the sampling mechanism, it is then possible to backout estimates for $p$ and $q$ by counting the overlap of the sets $J$ and $\\tilde{J}$ in the neighbourhoods of the clustered node embeddings. ", "page_idx": 32}, {"type": "text", "text": "We note that in practice, ML classification algorithms such as logistic regression are used instead. This instead depends on the typical geometry of the sets $J$ and $\\widetilde{J}$ . Suppose we have a $\\mathrm{SBM}(n,2,\\tilde{p},\\tilde{q},\\rho_{n})$ model. In this case, the set $J$ will approximately cons ist of $\\tilde{p}/2(\\tilde{p}+\\tilde{q})\\times E$ vectors from $\\eta_{11}$ , $\\tilde{p}/2(\\tilde{p}+\\tilde{q})\\times E$ vectors from $\\eta_{22}$ , $\\tilde{q}/\\bar{2}\\bar{(p+\\tilde{q})}\\times\\dot{E}$ vectors from $\\eta_{12}$ and $\\tilde{q}/2(\\tilde{p}+\\tilde{q})\\times{\\cal E}$ vectors from $\\eta_{21}$ . In contrast, the set $\\tilde{J}$ will approximately have $E/4$ of each of $\\eta_{11},\\,\\eta_{12},\\,\\eta_{21}$ and $\\eta_{22}$ . As a result, in the case where $\\tilde{p}\\gg\\tilde{q}$ , a linear classifier (for example) will be biased towards classifying more frequently vectors with $\\bar{c(i)}=c(j)$ , which is at least directionally correct. ", "page_idx": 32}, {"type": "text", "text": "So far, we have not talked about the particular mechanism used to form link embeddings from the node embeddings. The Hadamard product is popular, but particularly difficult to analyze given our results, as it does not remain invariant to an orthogonal rotation of the embedding vectors. In contrast, the average link function retains this information. In the $\\mathrm{SBM}(n,2,\\tilde{p},\\tilde{q},\\rho_{n})$ , it ends up giving embeddings which will asymptotically depend on only whether $c(i)=c(j)$ or not (i.e, whether the vertices belong to the same community or not). ", "page_idx": 32}, {"type": "text", "text": "E Intermediate results ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "E.1 Sampling probabilities for node2vec ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In this section, we derive asymptotic results for the sampling probabilities of edges within node2vec. We begin by recapping the second-order random walk defined for node2vec. To do so, we define a random process $(X_{n})_{n\\geq1}$ via the second-order Markov property ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(X_{n}=u\\,|\\,X_{n-1}=s,X_{n-2}=v\\big)\\propto\\left\\{\\begin{array}{l l}{0}&{\\mathrm{if~}(u,s)\\not\\in\\mathcal{E},}\\\\ {1/p}&{\\mathrm{if~}d_{u,v}=0\\mathrm{~and~}(u,s)\\in\\mathcal{E},}\\\\ {1}&{\\mathrm{if~}d_{u,v}=1\\mathrm{~and~}(u,s)\\in\\mathcal{E},}\\\\ {1/q}&{\\mathrm{if~}d_{u,v}=2\\mathrm{~and~}(u,s)\\in\\mathcal{E}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $d_{u,s}$ denotes the length of the shortest path between $u$ and $s$ . Given the extra information that $(u,s)$ is an edge, $d_{u,v}=0$ occurs iff $u=v$ , $d_{u,v}=1$ occurs iff $(u,v)$ is an edge, and $d_{u,v}=2$ occurs iff $(u,v)$ is not an edge (as given that $(v,s)$ is an edge, the shortest path must be $v\\rightarrow s\\rightarrow u_{s}^{\\prime}$ ). With this, we select positive samples by selecting $k$ concurrent edges within the walk (via taking a walk of length $k+1$ ). ", "page_idx": 33}, {"type": "text", "text": "To initialize the random walk, we note that for the second order walk we need to specify a distribution on the first two vertices; for DeepWalk where this collapses down to a first order walk, we only need to specify a distribution on ther first vertex. To do so generally, we consider an initial distribution of selecting the first vertex via $\\begin{array}{r}{\\pi(u)=\\deg(u)/\\sum_{v}\\deg(v)=\\deg(u)/2E_{n}}\\end{array}$ with $E_{n}$ being the number of edges in the graph (single counting $(u,v)\\,\\in\\,\\mathcal{E}$ and $(v,u)\\in\\mathcal{E})$ ), and select the second vertex uniformly at random from those connected to the first. (Note that this is the transition kernel used for DeepWalk, and so we handle both cases via this argument.) One can show this is equivalent to selecting an edge uniformly at random. ", "page_idx": 33}, {"type": "text", "text": "For the negative sampling mechanism, we consider the vertices which arose as part of the positive sampling process - which we denote $V(\\mathcal{P})$ - and then sample $l$ vertices independently according to the unigram distribution ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname{Ug}_{\\alpha}(v\\mid u,{\\mathcal G}_{n})={\\frac{\\deg(v)^{\\alpha}}{\\sum_{v^{\\prime}\\neq u}\\deg(v)^{\\alpha}}}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $u\\in V({\\mathcal{P}})$ . We note that the case where $\\alpha\\rightarrow0$ corresponds to the uniform distribution on vertices not equal to $u$ . ", "page_idx": 33}, {"type": "text", "text": "E.1.1 Proof of Theorem S1 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In this section and the next, it will be convenient to use the notation $\\sim_{p}$ to indicate that two positive random variables $X_{n}$ and $Y_{n}$ are asymptotic in the sense that $|X_{n}/\\dot{Y}_{n}-1|=o_{p}(1)$ when $n\\to\\infty$ . If we say such a bound happens uniformly over some free variables - say $X_{n,k}\\sim_{p}Y_{n,k}$ uniformly over $k$ - then this means $\\operatorname*{max}_{k}|X_{n,k}/Y_{n,k}\\,-\\,1|\\ =\\ o_{p}(1)$ . We also make extensive use of the result that if X(ni) $X_{n}^{(i)}\\sim_{p}\\ r_{n}Y_{n}^{(i)}$ for $i\\,\\in\\,\\{0,1\\}$ and $Y_{n}^{(i)}\\ \\in\\ [C^{-1},C]$ for $C\\,>\\,1$ , then $X_{n}^{(0)}+X_{n}^{(1)}\\sim_{p}r_{n}(Y_{n}^{(0)}+Y_{n}^{(1)})$ . Indeed, if we write $X_{n}^{(i)}=Y_{n}^{(i)}r_{n}(1+\\epsilon_{n}^{(i)}$ where \u03f5n $\\epsilon_{n}^{(1)}=o_{p}(1)$ , then ", "page_idx": 33}, {"type": "equation", "text": "$$\nX_{n}^{(0)}+X_{n}^{(1)}=r_{n}(Y_{n}^{(0)}+Y_{n}^{(1)})\\cdot\\Big(1+\\frac{Y_{n}^{(0)}}{Y_{n}^{(0)}+Y_{n}^{(1)}}\\epsilon_{n}^{(0)}+\\frac{Y_{n}^{(1)}}{Y_{n}^{(0)}+Y_{n}^{(1)}}\\epsilon_{n}^{(1)}\\Big)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "from which the claimed result follows as the terms weighting the $\\epsilon_{n}^{(1)}$ can be bounded below away from zero, and are bounded above by 1. We also note that $X_{n}^{(0)}-X_{n}^{(1)}=O_{p}(r_{n})$ , meaning that the order of magnitude of terms cannot increase (only decrease) by subtracting them. ", "page_idx": 33}, {"type": "text", "text": "As we are interested in the sampling probability of edges within node2vec, it will be convenient to instead study the first order Markov process $Y_{n}\\,=\\,{\\dot{\\left(X_{n},X_{n-1}\\right)}}$ , as then we instead study the sampling probability of individual states in a regular Markov chain. We note that normally we use the notation $(u,v)$ to refer an unordered pair belonging to an edge in a graph, but for the Markov process $(Y_{n})_{n\\geq1}$ the order matters, we will write $Y_{n}=e_{v\\rightarrow u}$ whenever $X_{n}=u$ and $X_{n-1}=v$ . In such a scenario, the random walk is therefore defined on the state space ", "page_idx": 33}, {"type": "equation", "text": "$$\nS=\\bigcup_{(u,v)\\in\\mathcal{E}}\\left\\{e_{u\\rightarrow v},e_{v\\rightarrow u}\\right\\}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "with the law of $Y$ given by ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\big(Y_{n}=e_{t\\to u}\\,|\\,Y_{n-1}=e_{v\\to s}\\big)=0\\mathrm{~if~}t\\neq s,}\\\\ &{\\mathbb{P}\\big(Y_{n}=e_{s\\to u}\\,|\\,Y_{n-1}=e_{v\\to s}\\big)\\propto\\left\\{\\frac{0}{\\frac{1[u=v]}{p}+1[u\\neq v](a_{u v}+\\frac{1-a_{u v}}{q})}\\right.\\quad\\mathrm{otherwise}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "One can calculate the normalizing factor for the probability distribution as being ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left(\\frac{1}{p}-\\frac{1}{q}\\right)+\\frac{1}{q}\\deg(s)+\\left(1-\\frac{1}{q}\\right)\\sum_{u\\in\\mathcal{V}\\setminus\\{v\\}}a_{s u}a_{u v},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "from which we observe that when $p\\,=\\,q\\,=\\,1$ we recover the simple random walk defined by DeepWalk, as then the probability an edge is selected with source node $u$ is uniform over edges $(u,v)$ where $v$ is a neighbour of $u$ . ", "page_idx": 34}, {"type": "text", "text": "With this in mind, we define the transition matrix ", "page_idx": 34}, {"type": "equation", "text": "$$\nP_{v\\rightarrow s,s\\rightarrow u}=\\frac{a_{s u}\\cdot\\{1[u=v]\\cdot1/p+1[u\\neq v](a_{u v}+1/q\\cdot(1-a_{u v})\\}}{\\left(\\frac{1}{p}-\\frac{1}{q}\\right)+\\frac{1}{q}\\deg(s)+\\left(1-\\frac{1}{q}\\right)\\sum_{u\\in\\mathcal{V}\\backslash\\{v\\}}a_{s u}a_{u v}}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "governing the transition probabilities on the above chain. We note that by [11, Proposition 72] and Theorem S26 respectively that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\sum_{\\substack{u\\leq w\\,0\\,u\\cup w}}\\alpha_{n}\\alpha_{n}W(\\lambda_{s},\\cdot),\\;\\;\\mathrm{~o~n~}\\;\\Gamma(\\lambda_{s},\\cdot),}\\\\ {\\sum_{\\substack{u\\leq w\\,0\\leq w}}\\alpha_{s u}a_{u v}\\sim_{p}n\\rho_{n}^{2}T(\\lambda_{s},\\lambda_{v})\\mathrm{~where~}T(\\lambda_{s},\\lambda_{v}):=\\mathbb{E}_{\\lambda\\sim\\mathrm{Unif}[0,1]}[W(\\lambda_{u},\\lambda)W(\\lambda,\\lambda_{v})\\mid\\lambda_{u},\\lambda_{v}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "uniformly over all $s,u,v$ . As a result, we define ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\widetilde{P}_{v\\to s,s\\to u}=\\frac{a_{s u}\\cdot\\{q^{-1}+(1-q^{-1})a_{v u}+\\delta_{u v}(p^{-1}-q^{-1})\\}}{\\Big(\\frac{1}{p}-\\frac{1}{q}\\Big)+\\frac{1}{q}n\\rho_{n}W(\\lambda_{s},\\cdot)+\\Big(1-\\frac{1}{q}\\Big)n\\rho_{n}^{2}T(\\lambda_{s},\\lambda_{v})}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\delta_{u v}\\,:=\\,1[u\\,=\\,v]$ and the numerator is the same as in $P_{v\\to s,s\\to u}$ (only written in a more convenient to use fashion), and the denominator makes use of the asymptotic statements (S111) and (S112). As a result, we have that $P_{v\\rightarrow s,s\\rightarrow u}\\sim_{p}\\widetilde{P}_{v\\rightarrow s,s\\rightarrow u}$ uniformly over $v,s,u$ . In particular, we have that $\\widetilde{P}_{v\\rightarrow s,s\\rightarrow u}=\\Theta_{p}(a_{s u}(n\\rho_{n})^{-1})$ uniformly over all triples of indices $(v,s,u)$ . ", "page_idx": 34}, {"type": "text", "text": "Let $A_{j}(u\\to v)=\\{Y_{j}=e_{u\\to v}\\}$ . We then note that the sampling probability of $(u,v)$ being sampled within the first $k+1$ steps of the second order random walk is given by ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\bigcup_{j\\leq k}A_{j}(u\\rightarrow v)\\cup A_{j}(v\\rightarrow u)\\,|\\,\\mathcal{G}_{n}\\Big).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "To ease on the notation going forward, we write $\\mathbb{P}_{n}(\\cdot)\\,:=\\,\\mathbb{P}(\\cdot\\,|\\,\\mathcal{G}_{n})$ . By the inclusion-exclusion principle, we can write this probability as equalling ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\sum_{\\stackrel{l,m\\geq1}{l+m\\leq k}}(-1)^{k+m+1}\\sum_{\\stackrel{1\\leq i_{1}<i_{2}<\\cdots<i_{l}\\leq k}{1\\leq j_{1}<j_{2}<\\cdots<j_{m}\\leq m}}{\\mathbb{P}}_{n}\\Big(\\bigcap_{k\\leq l}A_{i_{k}}(u\\rightarrow v)\\cap\\bigcap_{k\\leq m}A_{j_{k}}(v\\rightarrow u)\\Big).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We note that the number of terms in this sum is bounded above by $(2k)!$ ! (some terms will be zero, as we cannot select $e_{u\\to v}$ two times in a row), and so for asymptotic purposes we can focus on the individual terms. ", "page_idx": 34}, {"type": "text", "text": "We now address the individual probabilities making up this sum. Intuitively, we want to show the following: that the terms for which $(l,m)\\neq(1,0)$ or $(0,1)$ are asymptotically negligible, and that asymptotically these terms are functions only of $\\left({\\lambda_{u}},{\\lambda_{v}}\\right)$ . We fix a particular instance of the $i_{1},\\ldots,i_{l}$ and $j_{1},\\ldots,j_{m}$ , and denote $\\beta_{1}<\\beta_{2}<\\cdot\\cdot<\\beta_{l+m}$ for the ordering of these indices. As we use indices $i_{k}$ to denote the direction $u\\rightarrow v$ and $j_{k}$ for the direction $v\\rightarrow u$ , we write ", "page_idx": 34}, {"type": "equation", "text": "$$\nA_{i}(u\\to v)=:A_{\\beta}(u,v,0),\\qquad A_{j}(v\\to u)=:A_{\\beta}(u,v,1)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the third argument (which we refer to as the orientation herein) indicates which of the first two arguments are used as the source node for the edge. For each $\\beta_{k}$ for $k\\leq l+m$ , we write $o_{k}$ to denote this orientation. As a result, it suffices for us to analyze ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}_{n}\\Big(\\bigcap_{k\\leq l+m}A_{\\beta_{k}}(u,v,o_{k})\\Big)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "over all sequences $1\\leq\\beta_{1}<\\beta_{2}<\\cdots<\\beta_{l+m}\\leq k$ and orientations $(o_{k})_{k=1}^{l+m}$ . For this, we then note that by the Markov property of the random walk, we are able to write this probability as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left[\\displaystyle\\prod_{k\\leq l+m-1}\\mathbb{P}_{n}\\Big(A_{\\beta_{k+1}}(u,v,o_{k+1})\\,|\\,A_{\\beta_{k}}(u,v,o_{k})\\Big)\\right]\\cdot\\mathbb{P}_{n}\\big(A_{\\beta_{1}}(u,v,o_{1})\\big)}\\\\ &{\\qquad\\qquad=\\left[\\displaystyle\\prod_{k\\leq l+m-1}\\mathbb{P}_{n}\\Big(A_{\\beta_{k+1}-\\beta_{k}+1}(u,v,o_{k+1})\\,|\\,A_{1}(u,v,o_{k})\\Big)\\right]\\cdot\\mathbb{P}_{n}\\big(A_{\\beta_{1}}(u,v,o_{1})\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Focusing now on the terms in the product, if $\\beta_{k+1}-\\beta_{k}=1$ , then this term equals zero if $o_{k}=o_{k=1}$ , or otherwise equals e.g $P_{u\\to v,v\\to u}$ which is $O_{p}((n\\rho_{n})^{-1})$ as discussed above. If the walk is longer, then by the same argument as in [11, Proposition 73], by conditioning on the second step in the walk one can show this probability is asymptotically of the same order of a walk of length $\\beta_{k+1}-\\beta_{k}-1$ initialized from the uniform distribution on the edges of $\\mathcal{G}_{n}$ . As a result, we therefore only need to analyze events of the form ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}_{n}\\big(A_{\\beta}(u,v,o)\\big)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "which will allow us to then show that the events of the form $(l,m)=(1,0)$ or $(0,1)$ are the only ones we need to consider in the asymptotic expansion. Going forward, we assume that $o=0$ , as the sum (S115) is symmetric in the orientation $o$ and the arguments are unchanged. ", "page_idx": 35}, {"type": "text", "text": "To do so, we begin by writing writing $\\pi^{\\prime}=(a_{u v}/|\\mathcal{E}|)_{u,v}$ for the initial distribution provided to $Y_{1}$ . To analyze $p_{n}(u,v,\\beta):=\\mathbb{P}_{n}\\big(A_{\\beta}(u,v,0)\\big)$ , note that when $\\beta=1$ we trivially have that this probability equals $a_{u v}/|\\mathcal{E}|$ and we know that $|\\mathcal{E}|\\sim_{p}n^{2}\\rho_{n}\\mathcal{E}_{W}(1)$ . In the case where $\\beta\\geq2$ , we consider the set of sequences $\\alpha=(\\alpha_{0},\\ldots,\\alpha_{\\beta-2})\\in\\mathcal{V}^{\\beta-1}$ , where we then have that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle p_{n}(u,v,2)=\\frac{1}{|\\mathcal{E}|}\\sum_{\\alpha_{0}}a_{\\alpha_{0},u}P_{\\alpha_{0}\\rightarrow u,u\\rightarrow v}}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle p_{n}(u,v,\\beta)=\\frac{1}{|\\mathcal{E}|}\\sum_{\\alpha}a_{\\alpha_{0},\\alpha_{1}}\\cdot\\prod_{j=1}^{\\beta}P_{\\alpha_{j-1}\\rightarrow\\alpha_{j},\\alpha_{j}\\rightarrow\\alpha_{j+1}}\\cdot P_{\\alpha_{\\beta-2}\\rightarrow\\alpha_{\\beta-1},\\alpha_{\\beta-1}\\rightarrow u}P_{\\alpha_{\\beta-1}\\rightarrow u,u\\rightarrow v}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "for $\\beta\\geq3$ . ", "page_idx": 35}, {"type": "text", "text": "To study these sums, we begin by noting that they are asymptotic to their versions where we replace P \u2192P . Indeed, we note that if we have positive sequences $\\left(a_{i}\\right)$ and $\\left(b_{i}\\right)$ , then ", "page_idx": 35}, {"type": "equation", "text": "$$\n{\\Big|}{\\frac{\\sum_{j}a_{j}}{\\sum_{j}b_{j}}}-1{\\Big|}={\\frac{|\\sum_{j}b_{j}(a_{j}/b_{j}-1)|}{\\sum_{j}b_{j}}}\\leq\\operatorname*{max}_{j}{\\Big|}{\\frac{a_{j}}{b_{j}}}-1{\\Big|},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and so the fact that we know $P\\sim_{p}\\widetilde{P}$ uniformly, means that we can apply this to obtain asymptotic formulae for their sums also. With  this, if we write $N(\\lambda_{s},\\lambda_{t})$ for the denominator of $\\widetilde{P}_{t\\rightarrow s,s\\rightarrow u}$ , $p_{n}(u,v,\\beta)$ can be asymptotically be decomposed into a linear combination of terms (bounded in number by a function of $k$ independent of $n$ ) of the form ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{\\d(p,q)a_{u v}}{\\d|\\mathcal{E}|}\\sum_{\\alpha\\in\\mathcal{V}^{\\beta-1}}\\left\\{\\Big(\\prod_{2\\leq i\\leq\\beta}N\\big(\\lambda_{\\tilde{\\alpha}_{i-1}},\\lambda_{\\tilde{\\alpha}_{i}}\\big)\\Big)^{-1}\\cdot\\prod_{i\\leq\\beta-1}a_{\\tilde{\\alpha}_{i-1},\\tilde{\\alpha}_{i}}\\cdot\\prod_{j\\in J}a_{\\tilde{\\alpha}_{j-1},\\tilde{\\alpha}_{j+1}}\\cdot\\prod_{k\\in K}\\delta_{\\tilde{\\alpha}_{k-1},\\tilde{\\alpha}_{k+1}}\\right\\}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where: ", "page_idx": 35}, {"type": "text", "text": "\u2022 we write $\\tilde{\\alpha}$ for the concatenation $(\\alpha,u,v)$ , meaning $\\tilde{\\alpha}$ is of length $\\beta+1$ , with $\\tilde{\\alpha}_{k}=\\alpha_{k}$ for $k\\le\\beta-1,\\tilde{\\alpha}_{\\beta}=u$ and $\\tilde{\\alpha}_{\\beta+1}=v$ ; $c(p,q)=(q^{-1})^{\\beta-|J|-|K|}(1-q^{-1})^{|J|}(p^{-1}-q^{-1})^{|K|}$ is a polynomial in $p^{-1}$ and $q^{-1}$ ; \u2022 $J$ and $K$ are possibly empty subsets of $\\{1,\\ldots,\\beta\\}$ which are disjoint. ", "page_idx": 36}, {"type": "text", "text": "The more tedious part to handle is when the set $K$ is non-empty; as each delta function acts to contract the sum along one variable, doing so allows us to rewrite (S124) as ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\frac{\\iota_{u v}}{|\\mathcal{E}|}c(p,q)\\sum_{\\alpha\\in\\mathcal{V}^{\\beta-1-|K|}}\\left\\{\\Big(\\prod_{2\\leq i\\leq\\beta-|K|}N(\\lambda_{\\tilde{\\alpha}_{i-1}},\\lambda_{\\tilde{\\alpha}_{i}})^{n_{i}}\\Big)^{-1}\\cdot\\prod_{i\\leq\\beta-1-|K|}a_{\\tilde{\\alpha}_{i-1},\\tilde{\\alpha}_{i}}\\cdot\\prod_{j\\in\\tilde{J}}a_{\\tilde{\\alpha}_{j-1},\\tilde{\\alpha}_{j+1}}\\right\\}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "after a) performing some relabeling of the indices and modification to the set $J$ , to give a new set $\\tilde{J}$ which is a subset of $\\{1,\\ldots,\\beta-|K|\\}$ and b) introducing some multiplicities $n_{i}$ which sum to $\\beta-1$ . By Theorem S26 we uniformly have that this quantity is asymptotic, uniformly over all the free variables in the expression, to ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\frac{\\rho_{n}^{|\\mathcal{J}|}}{(n\\rho_{n})^{|K|}}\\cdot\\frac{a_{u v}c(p,q)\\rho_{n}^{-1}}{n^{2}\\mathcal{E}_{W}(1)}\\cdot\\mathbb{E}\\left[\\frac{\\prod_{i\\leq\\beta-1-|K|}W(\\lambda_{i-1}^{\\prime},\\lambda_{i}^{\\prime})\\prod_{j\\in\\mathcal{J}}W(\\lambda_{j-1}^{\\prime},\\lambda_{j+1}^{\\prime})}{\\prod_{2\\leq i\\leq\\beta-|K|}N^{\\prime}(\\lambda_{i-1}^{\\prime},\\lambda_{i}^{\\prime})^{n_{i}}}\\mid\\lambda_{u},\\lambda_{v}\\right]\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where we write $\\lambda^{\\prime}\\ =\\ (\\widetilde{\\lambda}_{0},\\ldots,\\widetilde{\\lambda}_{\\beta-2-|K|},\\lambda_{u},\\lambda_{v})$ and $\\widetilde{\\lambda}$ is an independent copy of $\\lambda$ , and $N^{\\prime}(\\lambda_{u},\\lambda_{v}):=(n\\rho_{n})^{-1}N(\\lambda_{u},\\lambda_{v})$ . As $n\\rho_{n}\\rightarrow\\infty$ under the prescribed conditions, we only need to consider leading terms of the order $\\rho_{n}^{-1}\\,n^{2}$ , which shows that the sampling probability is asymptotic (uniformly over all vertices) to $\\rho_{n}^{-1}\\,\\ddot{n}^{2}$ for some function $g_{\\mathcal{P}}(\\lambda_{u},\\lambda_{v})$ . To argue that this function is bounded above away from zero, we note that the terms where $|J|+|K|>0$ will be asymptotically negligible, and the remainder of the terms give a positive weighted sum. ", "page_idx": 36}, {"type": "text", "text": "E.1.2 Proof of Theorem S2 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "To understand the selection probability for the vertex pair $(u,v)$ to be selected via negative sampling, define the events ", "page_idx": 36}, {"type": "equation", "text": "$$\nA_{i}(u)=\\{X_{i}=u\\},\\qquad B_{i}(v|u)=\\{v{\\mathrm{~selected~via~negative~sampling~from~u}}\\}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "so then ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{P}((u,v)\\in\\mathcal{N}(\\mathcal{G}_{n})\\,|\\,\\mathcal{G}_{n})=\\mathbb{P}\\Big(\\bigcup_{i=0}^{k}(A_{i}(u)\\cap B_{i}(v|u))\\cup(A_{i}(v)\\cap B_{i}(u|v))\\,|\\,\\mathcal{G}_{n}\\Big).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We note that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{P}\\!\\left(A_{i}(u)\\cap B_{i}(v|u)\\,|\\,\\mathcal{G}_{n}\\right)=\\mathbb{P}\\!\\left(A_{i}(u)\\,|\\,\\mathcal{G}_{n}\\right)\\cdot\\mathbb{P}\\!\\left(\\mathrm{Binomial}(l,\\mathrm{Ug}_{\\alpha}(v|u))\\geq1\\,|\\,\\mathcal{G}_{n}\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "As a result, we need to begin by understanding the asymptotic probabilities of $\\mathbb{P}(A_{i}(v)\\,|\\,\\mathcal{G}_{n})$ and the unigram sampling probability. We begin with understanding the first probability. If $i\\in\\{0,1\\}$ , then we have that $\\mathbb{P}(\\tilde{A}_{i}(\\boldsymbol{\\breve{v}})\\vert\\,\\mathcal{G}_{n})=\\mathrm{deg}(\\boldsymbol{v})/2\\tilde{E}_{n}\\sim_{p}W(\\lambda_{v},\\cdot)/n\\tilde{\\mathcal{E}_{W}}(1)$ uniformly in $v$ [11, Proposition 72]. For $i\\geq2$ , we have that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{P}(A_{i}(v)\\,|\\,\\mathcal{G}_{n})=\\sum_{u}\\mathbb{P}(A_{i}(u\\rightarrow v)\\,|\\,\\mathcal{G}_{n})\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "using the same notation as in Appendix E.1.1. Consequently, via the same arguments as in Appendix E.1.1, it will be asymptotic to a positive linear combination of statistics of the form ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\frac{c(p,q)}{|\\mathcal{E}|}\\sum_{\\alpha\\in\\mathcal{V}^{\\beta}}\\left\\{\\Big(\\prod_{2\\leq i\\leq\\beta-|K|}N(\\lambda_{\\tilde{\\alpha}_{i-1}},\\lambda_{\\tilde{\\alpha}_{i}})^{n_{i}}\\Big)^{-1}\\cdot\\prod_{i\\leq\\beta-|K|}a_{\\tilde{\\alpha}_{i-1},\\tilde{\\alpha}_{i}}\\cdot\\prod_{j\\in\\tilde{J}}a_{\\tilde{\\alpha}_{j-1},\\tilde{\\alpha}_{j+1}}\\right\\}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where we write $\\boldsymbol{\\Tilde{\\alpha}}\\,=\\,(\\alpha,v)$ for $\\alpha\\,\\in\\,\\mathcal{V}^{\\beta}$ . Using the same relabeling and arguments as given in Appendix E.1.1 will be asymptotic to ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\frac{\\rho_{n}^{|\\tilde{J}|}}{(n\\rho_{n})^{|K|}}\\cdot\\frac{c(p,q)}{n\\mathscr{E}_{W}(1)}\\cdot\\mathbb{E}\\left[\\frac{\\prod_{i\\leq\\beta-|K|}W(\\lambda_{i-1}^{\\prime},\\lambda_{i}^{\\prime})\\prod_{j\\in\\tilde{J}}W(\\lambda_{j-1}^{\\prime},\\lambda_{j+1}^{\\prime})}{\\prod_{2\\leq i\\leq\\beta-|K|}N^{\\prime}(\\lambda_{i-1}^{\\prime},\\lambda_{i}^{\\prime})^{n_{i}}}\\,|\\,\\lambda_{v}\\right]\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "uniformly in all the free variables involved, where $\\lambda^{\\prime}=(\\widetilde{\\lambda}_{0},\\ldots,\\widetilde{\\lambda}_{\\beta-1-|K|},\\lambda_{v})$ and $\\widetilde{\\lambda}$ is an independent copy of $\\lambda$ . (We note that while Theorem S26 is expressed in  terms of concentrat ion of quantities around functions which depend on both $\\lambda_{u}$ and $\\lambda_{v}$ , the exact same reasoning will apply for statistics which only end up depending on $\\lambda_{v}$ .) In particular by taking the highest order terms of this expansion, we have that there exists some measurable function $g_{i}(\\cdot)$ which is bounded below and above, for each $i$ , such that $\\mathbb{P}(A_{i}(u)\\,|\\,\\mathcal{G}_{n})\\sim_{p}n^{-1}g_{i}(\\lambda_{u})$ uniformly in $u$ . ", "page_idx": 37}, {"type": "text", "text": "As for the unigram sampling term, we note that by [11, Proposition 77] we have that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathrm{Binomial}(l,\\mathrm{Ug}_{\\alpha}(v|u))\\sim_{p}\\frac{l W(\\lambda_{u},\\cdot)^{\\alpha}}{n\\mathcal{E}_{W}(\\alpha)}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "uniformly in the vertices $v,u$ . With this, we note that the same arguments via self-intersection allow us to argue that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{P}((u,v)\\in\\mathcal{N}(\\mathcal{G}_{n})\\,|\\,\\mathcal{G}_{n})\\sim_{p}\\frac{l}{n^{2}}\\sum_{i=0}^{k}\\frac{l}{\\mathcal{E}_{W}(\\alpha)}(g_{i}(\\lambda_{u})W(\\lambda_{v},\\cdot)^{\\alpha}+g_{i}(\\lambda_{v})W(\\lambda_{u},\\cdot)^{\\alpha})\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "which gives the claimed result. ", "page_idx": 37}, {"type": "text", "text": "E.2 Chaining and bounds on Talagrand functionals ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "In this section, let $L>0$ denote a universal constant (which may differ across occurrences) and $K(\\alpha)$ a universal constant which depends on a variable $\\alpha$ (but for fixed $\\alpha$ also differs across occurrences). For a metric space $(T,d)$ , we define the diameter of $T$ as ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\Delta(T):=\\operatorname*{sup}_{t_{1},t_{2}\\in T}d(t_{1},t_{2}).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We also define the entropy and covering numbers respectively by ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{N(T,d,\\epsilon):=\\operatorname*{min}\\big\\{n\\in\\mathbb N\\,|\\,F\\subseteq T,|F|\\leq n,d(t,F)\\le\\epsilon\\,\\mathrm{for~all}\\,t\\in T\\big\\},\\qquad}\\\\ &{}&{e_{n}(T):=\\operatorname*{inf}\\big\\{\\displaystyle\\operatorname*{sup}_{t\\in T}d(t,T_{n})\\,|\\,T_{n}\\subseteq T,|T_{n}|\\le2^{2^{n}}\\big\\}=\\operatorname*{inf}\\big\\{\\epsilon>0\\,|\\,N(t,d,\\epsilon)\\le2^{2^{n}}\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We then define the Talagrand $\\gamma_{\\alpha}$ functional [43] of the metric space $(T,d)$ by ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\gamma_{\\alpha}(T,d)=\\operatorname*{inf}_{t\\in T}\\sum_{n\\geq0}2^{n/\\alpha}\\Delta\\big(A_{n}(t)\\big)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the infimum is taking over all admissable sequences; these are increasing sequences $({\\mathcal{A}}_{n})_{n\\geq0}$ of $T$ such that $|\\mathcal{A}_{0}|=1$ and $|{\\mathcal{A}}_{n}|\\leq2^{2^{n}}$ for all $n$ , with $A_{n}(t)$ being the unique element of $A_{n}$ which contains $t$ . We will shortly see that this quantity helps to control the supremum of empirical processes on the metric space $(T,{\\dot{d}})$ . We first give some generic properties for the above functional. ", "page_idx": 37}, {"type": "text", "text": "Lemma S18. a) Suppose that $d$ is a metric on $T_{i}$ , and $M~>~0$ is a constant. Then $\\gamma_{\\alpha}(T,M d)=M\\gamma_{\\alpha}(T,d)$ . If $U\\subseteq T$ , then $\\gamma_{\\alpha}(U,d)\\le\\gamma_{\\alpha}(T,d)$ . ", "page_idx": 37}, {"type": "text", "text": "$b$ ) Suppose that $(T_{1},d_{1})$ and $(T_{2},d_{2})$ are metric spaces, so $d=d_{1}+d_{2}$ is a metric on the product space $T=T_{1}\\times T_{2}$ . Then $\\gamma_{\\alpha}(T,d)\\leq\\bar{K}(\\alpha)(\\gamma_{\\alpha}(T_{1},d_{1})+\\gamma_{\\alpha}(T_{2},d_{2}))$ . ", "page_idx": 37}, {"type": "text", "text": "$c$ ) We have the upper bounds ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\gamma_{\\alpha}(T,d)\\leq K(\\alpha)\\sum_{n\\geq0}2^{n/\\alpha}e_{n}(T)\\leq K(\\alpha)\\int_{0}^{\\infty}\\left(\\log N(T,d,\\epsilon)\\right)^{1/\\alpha}d\\epsilon.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "d) Suppose that $\\|\\cdot\\|$ is a norm on $\\mathbb{R}^{m}$ , $d$ is the metric induced by $\\|\\cdot\\|$ , and ${\\cal B}_{\\cal A}\\;=\\;\\{x\\;:$ $\\|x\\|\\leq A\\}$ . Then one has the bound $N(B_{A},d,\\epsilon)\\leq\\operatorname*{max}\\{(3A/\\epsilon)^{m},1\\}$ , and consequently $\\gamma_{\\alpha}(B_{A},d)\\leq K(\\alpha)A m^{1/\\alpha}$ . ", "page_idx": 37}, {"type": "text", "text": "Proof. The first statement in a) is immediate, and the second part is Theorem 2.7.5 a) of Talagrand [43]. ", "page_idx": 37}, {"type": "text", "text": "For part b), suppose that $\\mathcal{A}_{n}^{i}$ are admissable sequences for $(T_{i},d_{i})$ such that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t_{i}\\in T_{i}}\\sum_{n\\geq0}2^{n/\\alpha}\\Delta(A_{n}^{i}(t))\\leq2\\gamma_{\\alpha}(T_{i},d_{i})\\;{\\mathrm{for}}\\;i=1,2.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "If we then form the sequence of sets $\\mathcal{B}_{n}:=\\{A_{1}\\times A_{2}\\,:\\,A_{i}\\in\\mathcal{A}_{n-1}^{i}\\}$ for $n\\geq1$ and $B_{0}=T_{1}\\times T_{2}$ , we have that $B_{n}$ is a partition of $T$ for each $n$ , $|\\beta_{0}|=1$ and $|\\mathcal{B}_{n}|=|\\mathcal{A}_{n-1}^{1}|\\cdot|\\mathcal{A}_{n-1}^{2}|\\leq2^{2^{n}}$ for each $n$ , meaning that $B_{n}$ is an admissable sequence for the metric space $(T,d)$ . Moreover, note that we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\Delta((A_{1}\\times A_{2})(t_{1},t_{2}))=\\Delta(A_{1}(t_{1}))+\\Delta(A_{2}(t_{2}))\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "for all sets $A_{1}\\subseteq T_{1}$ , $A_{2}\\subseteq T_{2}$ and $t_{1}\\in T_{1},t_{2}\\in T_{2}$ . As a result, if write $B_{n}(t_{1},t_{2})=A_{n-1}^{1}(t_{1})\\times$ $A_{n-1}^{2}(t_{2})$ for the unique set in $B_{n}$ for which the point $(t_{1},t_{2})$ lies within it, then we have that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\sum_{n\\geq0}2^{n/\\alpha}\\Delta(B_{n}(t_{1},t_{2}))\\leq2^{\\alpha}\\Big(\\sum_{n\\geq1}2^{(n-1)/\\alpha}\\Delta(A_{n-1}^{i}(t_{1}))+\\sum_{n\\geq1}2^{(n-1)/\\alpha}\\Delta(A_{n-1}^{i}(t_{2}))\\Big).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "In particular, taking supremum over all $t\\,\\in\\,T$ then gives the result, as the resuling LHS is lower bounded by $\\gamma_{\\alpha}(T,d)$ , and the resulting RHS is upper bounded by $2(\\gamma_{\\alpha}(T_{1},d_{1})+\\gamma_{\\alpha}^{-}(T_{2},d_{2}))$ . ", "page_idx": 38}, {"type": "text", "text": "For part c), the first inequality is Corollary 2.3.2 in Talagrand [43]. As for the second inequality, note that if $\\epsilon\\leq e_{n}(T)$ , then $N(\\dot{T},d,\\epsilon)>2^{2^{\\bar{n}}}$ and consequently $N(T,d,\\epsilon)\\geq2^{2^{n}}+1$ (recall that both quantities are integers). Writing $N_{n}=2^{2^{n}}$ , this implies that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\big(\\log(1+N_{n})\\big)^{1/\\alpha}(e_{n}(T)-e_{n+1}(T))\\leq\\int_{e_{n+1}(T)}^{e_{n}(T)}\\big(\\log N(T,d,\\epsilon)\\big)^{\\alpha}\\,d\\epsilon.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "As $\\log(1+N_{n})\\leq2^{n}\\log(2)$ for all $n\\geq0$ , summation over all $n\\geq0$ implies that ", "page_idx": 38}, {"type": "equation", "text": "$$\n(\\log2)^{1/\\alpha}\\sum_{n\\geq0}2^{n/\\alpha}(e_{n}(T)-e_{n+1}(T))\\leq\\int_{0}^{e_{0}(T)}{\\bigl(}\\log N(T,d,\\epsilon){\\bigr)}^{\\alpha}\\,d\\epsilon.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "As we have that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\sum_{n\\geq0}2^{n/\\alpha}{\\big(}e_{n}(T)-e_{n+1}(T){\\big)}\\geq(1-2^{1/\\alpha})\\sum_{n\\geq0}2^{n/\\alpha}e_{n}(T),\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "combining this and the prior inequality gives the stated result. ", "page_idx": 38}, {"type": "text", "text": "For part d), we can calculate that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\int_{0}^{\\infty}\\big(\\log N(B_{A},d,\\epsilon)\\big)^{1/\\alpha}\\,d\\epsilon\\leq\\int_{0}^{3A}m^{1/\\alpha}\\big(\\log(3A/\\epsilon)\\big)^{1\\alpha}\\,d\\epsilon\\leq3A m^{1/\\alpha}\\int_{0}^{1}(\\log(1/y))^{1/\\alpha}\\,d y.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "For the remaining integral, note that if we make the substitution $\\begin{array}{r}{y=\\exp(-t^{\\alpha})}\\end{array}$ , then the integral equals ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\int_{0}^{1}(\\log(1/y))^{1/\\alpha}\\,d y=\\alpha\\int_{0}^{\\infty}t^{\\alpha}e^{-t^{\\alpha}}\\,d t,\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "which we recognize as the mean of an $\\operatorname{Exp}(1)$ random variabe in the case where $\\alpha=1$ , and the variance of an unnormalized $\\mathrm{N}(0,2)$ density in the case where $\\alpha=2$ , and so in both cases the integral is finite. The desired conclusion follows. \u53e3 ", "page_idx": 38}, {"type": "text", "text": "Before stating a corollary of this result involving bounds on the $\\gamma$ -functional of some of the sets introduced in Theorem S5, we discuss some of the properties of these sets. ", "page_idx": 38}, {"type": "text", "text": "Lemma S19. Define the sets ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{B}_{F}(A):=\\big\\{U\\in\\mathbb{R}^{n\\times d}\\,|\\,\\|U\\|_{F}\\leq A\\big\\},}\\\\ &{\\mathcal{B}_{2,\\infty}(A):=\\big\\{U\\in\\mathbb{R}^{n\\times d}\\,|\\,\\|U\\|_{2,\\infty}\\leq A\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Moreover, define the metrics ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad d_{F}((U_{1},V_{1}),(U_{2},V_{2})):=\\|U_{1}-U_{2}\\|_{F}+\\|V_{1}-V_{2}\\|_{F}}\\\\ &{d_{2,\\infty}((U_{1},V_{1}),(U_{2},V_{2})):=\\|U_{1}-U_{2}\\|_{2,\\infty}+\\|V_{1}-V_{2}\\|_{2,\\infty}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "defined on the space $\\mathbb{R}^{n\\times d}\\times\\mathbb{R}^{n\\times d}$ of pairs of $n\\times d$ matrices. Then we have that for $U_{1},U_{2},V_{1},V_{2}\\in$ $\\mathcal{B}_{F}(A_{F})\\cap B_{2,\\infty}(\\tilde{A}_{2,\\infty})$ that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{U_{1}V_{1}^{T}-U_{2}V_{2}^{T}\\|_{F}\\le A_{F}d_{F}\\big((U_{1},V_{1}),(U_{2},V_{2})\\big),\\qquad\\|U_{1}V_{1}^{T}-U_{2}V_{2}^{T}\\|_{\\infty}\\le\\tilde{A}_{2,\\infty}d_{2,\\infty}((U_{1},V_{1}),(U_{2},V_{2})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Moreover, if $U\\in B_{2,\\infty}(A)$ , then $U\\in B_{F}({\\sqrt{n}}A)$ also, and consequently if $U\\in B_{2,\\infty}(\\tilde{A}_{2,\\infty})$ then we have that $U\\in\\mathcal{B}_{2,\\infty}(\\tilde{A}_{2,\\infty})\\cap\\mathcal{B}_{F}(\\sqrt{n}\\tilde{A}_{2,\\infty})$ . ", "page_idx": 39}, {"type": "text", "text": "Proof. Begin by noting that, if $U_{1},V_{1},U_{2},V_{2}\\in\\mathbb{R}^{n\\times d}$ are matrices, then we have that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{U_{1}V_{1}^{T}-U_{2}V_{2}^{T}\\|_{F}=\\|U_{1}(V_{1}-V_{2})^{T}+(U_{1}-U_{2})V_{2}^{T}\\|_{F}\\le\\|U_{1}\\|_{F}\\|V_{1}-V_{2}\\|_{F}+\\|U_{1}-U_{2}\\|_{F}\\|V_{2}\\|_{F},}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and similarly ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{U_{1}V_{1}^{T}-U_{2}V_{2}^{T}\\|_{\\infty}=\\|U_{1}(V_{1}-V_{2})^{T}+(U_{1}-U_{2})V_{2}^{T}\\|_{\\infty}\\le\\|U_{1}\\|_{2,\\infty}\\|V_{1}-V_{2}\\|_{2,\\infty}+\\|U_{1}-U_{2}\\|_{2,\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "As a result, we therefore have that in the case where $U_{1},V_{1},U_{2},V_{2}$ all have $\\|\\cdot\\|_{F}\\leq A_{F}$ , then ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\|U_{1}V_{1}^{T}-U_{2}V_{2}^{T}\\|_{F}\\leq A_{F}\\big(\\|U_{1}-U_{2}\\|_{F}+\\|V_{1}-V_{2}\\|_{F}\\big)\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and similarly if each of $U_{1},V_{1},U_{2},V_{2}$ have $\\|\\cdot\\|_{2,\\infty}\\leq\\tilde{A}_{2,\\infty}$ then ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\|U_{1}V_{1}^{T}-U_{2}V_{2}^{T}\\|\\le\\tilde{A}_{2,\\infty}\\big(\\|U_{1}-U_{2}\\|_{2,\\infty}+\\|V_{1}-V_{2}\\|_{2,\\infty}\\big),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "giving the first result of the lemma. The second part follows by noting that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\sum_{j=1}^{d}|u_{i j}|^{2}\\leq n\\operatorname*{max}_{i\\in[n]}\\sum_{j=1}^{d}|u_{i j}|^{2}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and taking square roots. ", "page_idx": 39}, {"type": "text", "text": "Corollary S20. With the same notation as in Lemma S19, and writing $T=\\mathcal{B}_{F}(A_{F})\\cap\\mathcal{B}_{2,\\infty}(\\tilde{A}_{2,\\infty})$ , we have that for any constant $C>0$ that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\gamma_{\\alpha}(T\\times T,C d_{F})\\leq\\gamma_{\\alpha}(B_{F}(A_{F}),C d_{F})\\leq K(\\alpha)\\cdot C A_{F}(n d)^{1/\\alpha}\\leq K(\\alpha)\\cdot C\\tilde{A}_{2,\\infty}n^{1/2+1/\\alpha}d^{1/\\alpha},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "equation", "text": "$$\n\\gamma_{\\alpha}(T\\times T,C d_{2,\\infty})\\leq\\gamma_{\\alpha}(B_{2,\\infty}(\\tilde{A}_{2,\\infty}),C d_{F})\\leq K(\\alpha)\\cdot C\\tilde{A}_{2,\\infty}(n d)^{1/\\alpha}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Proof. This is a combination of Lemma S18 and Lemma S19 ", "page_idx": 39}, {"type": "text", "text": "We now state a result which illustrates the usefulness of the above quantity when trying to control the supremum of empirical processes on a metric space $(T,d)$ . ", "page_idx": 39}, {"type": "text", "text": "Theorem S21. Suppose $(X_{t})t\\in T$ is a mean-zero stochastic process, where $d_{1}$ and $d_{2}$ are two metrics on $T$ . Suppose for all $s,t\\in T$ we have the inequality ", "page_idx": 39}, {"type": "equation", "text": "$$\n{\\mathbb P}\\big(|X_{s}-X_{t}|\\ge u\\big)\\le2\\exp\\Big(-\\operatorname*{min}\\big\\{\\frac{u^{2}}{d_{2}(s,t)^{2}},\\frac{u}{d_{1}(s,t)}\\big\\}\\Big).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Then we have that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(\\operatorname*{sup}_{s,t\\in T}|X_{s}-X_{t}|\\geq L u\\big(\\gamma_{2}(T,d_{2})+\\gamma_{1}(T,d_{1})\\big)\\big)\\leq L\\exp(-u).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Proof. This can be found within the proof of Theorem 2.2.23 in Talagrand [43]. ", "page_idx": 39}, {"type": "text", "text": "Corollary S22. With the notation of Theorem S5, Lemma S19 and Corollary S20, if we have the bound ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathsf{\\Upsilon}^{\\mathsf{\\beta}}\\big(\\vert E_{n}(U,V)-E_{n}(\\tilde{U},\\tilde{V})\\vert\\ge u\\big)}}\\\\ &{}&{\\le2\\exp\\Big(-\\operatorname*{min}\\Big\\{\\frac{u^{2}}{128\\rho_{n}^{-1}n^{-4}A_{F}^{2}d_{F}\\big((U,V),(\\tilde{U},\\tilde{V})\\big)^{2}},\\frac{u}{16\\rho_{n}^{-1}n^{-2}\\tilde{A}_{2,\\infty}d_{2,\\infty}\\big((U,V),(\\tilde{U},\\tilde{V})\\big)}\\Big\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "then as a consequence we can deduce that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{(U,V),(\\tilde{U},\\tilde{V})\\in T\\times T}\\big|E_{n}(U,V)-E_{n}(\\tilde{U},\\tilde{V})\\big|=O_{p}\\Big(\\tilde{A}_{2,\\infty}^{2}\\Big(\\frac{d}{n\\rho_{n}}\\Big)^{1/2}+\\tilde{A}_{2,\\infty}^{2}\\frac{d}{n\\rho_{n}}\\Big)\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proof. This is a consequence of Corollary S20 and Theorem S21. ", "page_idx": 40}, {"type": "text", "text": "E.3 Matrix Algebra ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Proposition S23. Suppose that we have matrices U ${\\cal I},X\\in\\mathbb{R}^{n\\times d}$ with $n\\geq d$ , and suppose that $X$ is $a$ full rank matrix so $\\hat{\\sigma_{d}}\\hat{(}X X^{T})>0$ . Then we have that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{Q\\in O(d)}\\frac{1}{n}\\|U-X Q\\|_{F}^{2}\\leq\\frac{n^{-2}\\|U U^{T}-X X^{T}\\|_{F}^{2}}{\\sqrt{2}(\\sqrt{2}-1)n^{-1}\\sigma_{d}(X X^{T})}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Now instead suppose we have matrices $U,V\\in\\mathbb{R}^{n\\times d}$ and a matrix $M\\in\\mathbb{R}^{n\\times d}$ of rank d. Let $M=$ $U_{M}\\Sigma V_{M}^{T}$ be a SVD of $M$ . Moreover suppose that $U^{T}U=V^{T}V$ , and $\\|U V^{T}-\\dot{M}\\|_{o p}\\le\\sigma_{d}(M)/2$ . Then we have that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{Q\\in O(d)}\\frac{1}{n}\\|U-U_{M}\\Sigma^{1/2}Q\\|_{F}^{2}\\leq\\frac{2n^{-2}\\|U V^{T}-M\\|_{F}^{2}}{(\\sqrt{2}-1)n^{-1}\\sigma_{d}(M)}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proof. The first part of the theorem statement is Lemma 5.4 of Tu et al. [44]. For the second part, we note that by Proposition S24, we can let $U=U_{M}\\Sigma^{1/2}Q$ and $V=V_{M}\\Sigma^{1/2}Q$ for some orthonormal matrix $Q$ , where $\\tilde{U}\\tilde{\\Sigma}\\tilde{V}^{T}$ is the SVD of $U V^{T}$ . As a result, we can therefore apply without loss of generality Lemma 5.14 of Tu et al. [44], which then gives the desired statement. \u53e3 ", "page_idx": 40}, {"type": "text", "text": "Proposition S24. Suppose that $U,V\\in\\mathbb{R}^{n\\times d}$ are matrices such that $U V^{T}=M$ for some rank $d$ matrix $M\\in\\mathbb{R}^{n\\times n}$ . Moreover suppose that $U^{T}U=V^{T}V$ . Let $M=U_{M}\\Sigma V_{M}^{T}$ be the SVD of $M$ . Then there exists an orthonormal matrix $Q\\,\\in\\,O(d)$ such that $V=V_{M}\\Sigma^{1/2}Q_{\\mathrm{.}}$ . In particular, the symmetry group of the mapping $(U,V)\\to U V^{T}$ under the constraint $U^{T}U=V^{T}{\\bar{V}}$ is exactly the orthogonal group $O(d)$ . ", "page_idx": 40}, {"type": "text", "text": "Proof. Begin by noting that the condition $U^{T}U=V^{T}V$ forces there to exist an orthonormal matrix $R\\in O(n)$ such that $R U=V$ (e.g by Theorem 7.3.11 of Horn and Johnson [21]). As a consequence, we therefore have that $M\\,=\\,R^{-1}\\dot{V}V^{T}$ . This is a polar decomposition of $M$ , and therefore as the semi-positive definite factor is unique, we have that $V V^{T}\\,\\stackrel{\\textstyle>}{=}\\,(V_{M}\\Sigma^{1/2})(V_{M}\\Sigma^{1/2})^{T}$ , where $M=U_{M}\\dot{\\Sigma}V_{M}^{T}$ is the SVD of $M$ , and we highlight that the polar decomposition of $M$ is usually represented by $M\\,=\\,(U_{M}V_{M}^{-1})\\cdot(V_{M}\\Sigma V_{M}^{T})$ . As $V V^{T}\\,=\\,(V_{M}\\Sigma^{1/2})(V_{M}\\Sigma^{1/2})^{T}$ , again by e.g Theorem 7.3.11 of Horn and Johnson [21] we have that there exists an orthonormal matrix $Q\\in O(d)$ such that $V=V_{M}\\Sigma^{1/2}Q$ , giving the desired result. \u53e3 ", "page_idx": 40}, {"type": "text", "text": "Lemma S25. Suppose $X\\in\\mathbb{R}^{n\\times n}$ is a symmetric matrix such that $X=\\Pi A\\Pi^{T}$ where $A\\in\\mathbb{R}^{d\\times d}$ is of full rank, and $\\Pi\\in\\mathbb{R}^{n\\times d}$ is the assignment matrix for a partition of $[n]$ ; that is, there exists $a$ partition of $[n]$ into $d$ sets $B(1),\\ldots,B(d)$ such that $\\Pi_{i l}=1[i\\in B(l)]$ . Suppose further that $\\Pi$ is of full rank. Then we have that $\\sigma_{d}(X)\\geq\\sigma_{d}(A)\\times\\operatorname*{min}_{l}|B(l)|$ . ", "page_idx": 40}, {"type": "text", "text": "Proof. Let $\\Delta=\\mathrm{diag}(|B(1)|^{1/2},\\ldots,|B(d)|^{1/2})$ . Then note that we can write ", "page_idx": 40}, {"type": "equation", "text": "$$\nX=(\\Pi\\Delta^{-1})\\cdot\\Delta A\\Delta\\cdot(\\Pi\\Delta)^{-1}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where $(\\Pi\\Delta^{-1})$ is an orthonormal matrix. As a result, we can simply concentrate on the spectrum of the matrix $\\Delta A\\Delta$ . As the smallest singular value of a matrix product is less than the product of the smallest singular values, the stated result follows. \u53e3 ", "page_idx": 40}, {"type": "text", "text": "E.4 Concentration inequalities ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Theorem S26. Suppose that $H$ is a graph on a vertex set $\\left\\{{{r}_{1}},\\ldots,{{r}_{l}},{{v}_{1}},\\ldots,{{v}_{m}}\\right\\}$ where the vertices $r_{i}$ are referred to as root vertices, and the remaining vertices as free vertices. We refer to such $a$ graph as a rooted graph. Suppose that all the edges in $H$ have at least one free vertex as an endpoint. Write $\\mathbf{x}=(x_{1},\\ldots,x_{m})$ for the collection of $m$ variables $x_{i}$ , and let $Y$ be a statistic of the form ", "page_idx": 41}, {"type": "equation", "text": "$$\nY=\\sum_{x_{1},...,x_{m}\\in[n]}g_{\\mathbf{x}}\\prod_{i\\sim_{H}j}t_{x_{i},x_{j}}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where the random variables $t_{x_{i},x_{j}}$ are independent and $\\{0,1\\}$ valued with $c_{p}\\leq\\mathbb{P}(t_{x_{i},x_{j}}=1)\\leq$ $1-c_{p}$ for all $x_{i},x_{j}$ ; the coefficients $c_{g}\\,\\leq\\,g_{x}\\,\\leq\\,\\|g\\|_{\\infty}\\,<\\,\\infty$ for some $c_{g}\\;>\\;0$ ; and $i\\sim_{H}$ j iff $(i,j)$ is an edge within the graph $H$ . Suppose that $\\rho_{n}\\:=\\:n^{-\\alpha}$ for some $\\alpha\\,<\\,1/m^{\\prime}(H)$ where $\\begin{array}{r}{m^{\\prime}(H)=\\operatorname*{max}_{2\\leq j\\leq k}(j-1)/(\\bar{v}(j)-2),\\ \\bar{v}(j)=\\operatorname*{min}_{|A|\\geq j}v(A)}\\end{array}$ and $v(A)$ for a set of edges $A$ indicates the number of vertices in $A$ . Then there exist constants $c,\\delta,\\Delta$ which depend only on $c_{g},\\,c_{p}$ , $\\|g\\|_{\\infty}$ , $H$ and $\\alpha$ such that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(|Y-\\mathbb{E}[Y]|\\ge\\mathbb{E}[Y]\\sqrt{\\lambda(n^{2}\\rho_{n})^{-1}}\\big)\\le\\exp(-c\\lambda)\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "for all $\\Delta\\le\\lambda\\le n^{\\delta}$ . ", "page_idx": 41}, {"type": "text", "text": "Proof. Without loss of generality suppose that $\\|g\\|_{\\infty}=1$ . The proof is essentially the same as $\\mathrm{Vu}$ [48, Corollary 6.4], where we extend the result derived for the asymptotics of subgraph counts to that of a weighted count of rooted subgraph counts. To do so, we introduce some notation introduced within [48]. If $H$ has $k$ edges, and $A$ is a set of pairs $\\{x_{i},x_{j}\\}$ , we write $\\partial_{A}T$ for the polynomial $\\textstyle\\prod_{x\\in A}\\partial_{x}T$ when interpreting $T$ as a formal sum in the variables $a_{x_{i},x_{j}}$ (which we recall are $\\{0,1\\}$ valued. We then define for $1\\le j\\le k$ the quantities ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{E}_{j}[Y]=\\operatorname*{max}_{|A|\\geq j}\\mathbb{E}[\\partial_{A}Y],M_{j}(Y)=\\operatorname*{max}_{t,|A|\\geq j}\\partial_{A}Y(t).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Let $v(A)$ denote the number of vertices specified within the set $A$ , and let $v(j)\\mathrm{~-~}\\operatorname*{min}_{|A|\\geq j}v(A)$ . With this, we note that $\\mathbb{E}[Y]=\\Theta(n^{m}\\rho_{n}^{k})$ and $\\mathbb{E}[\\partial_{A}Y)=\\Theta(n^{m-v(A)}\\rho_{n}^{k-|A|})$ . Consequently, we have that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{E}_{j}[Y]=\\operatorname*{max}_{h\\ge j}\\Theta(n^{m-v(h)}\\rho_{n}^{k-h}),\\mathbb{E}[Y]/\\mathbb{E}_{j}[Y]=\\Theta(\\operatorname*{min}_{h\\ge j}n^{v(h)}\\rho_{n}^{h})\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where the implied constants depend only on $k$ , $c_{g}$ and $c_{p}$ . The same arguments as given in Claim 6.2 and Corollary 6.4 in [48] can then be applied verbatim to give the claimed result. \u53e3 ", "page_idx": 41}, {"type": "text", "text": "Lemma S27. Let $T$ be a statistic of the form ", "page_idx": 41}, {"type": "equation", "text": "$$\nT^{\\prime}=\\sum_{x_{1}\\neq x_{2}\\neq\\cdots\\neq x_{m}}g(\\lambda_{x_{1}},\\ldots,\\lambda_{x_{m}})\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $c_{g}\\leq g(\\cdot)\\leq\\|g\\|_{\\infty}<\\infty$ . Then we have that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(|T^{\\prime}-\\mathbb{E}[T^{\\prime}]|\\ge\\epsilon\\mathbb{E}[T^{\\prime}]\\big)\\le2\\exp\\Big(\\frac{-\\epsilon^{2}c_{g}^{2}\\lfloor n/m\\rfloor}{2\\|g\\|_{\\infty}^{2}}\\Big).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Consequently, if we define ", "page_idx": 41}, {"type": "equation", "text": "$$\nT_{l,k}=\\sum_{x_{1},x_{2},\\ldots,x_{m}}g(\\lambda_{x_{1}},\\ldots,\\lambda_{x_{m}},\\lambda_{l},\\lambda_{k}),\\quad T_{l,k}^{\\prime}=\\sum_{x_{1}\\neq x_{2}\\neq\\cdots\\neq x_{m}}g(\\lambda_{x_{1}},\\ldots,\\lambda_{x_{m}},\\lambda_{l},\\lambda_{k})\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $c_{g}\\leq g(\\cdot)\\leq\\|g\\|_{\\infty}<\\infty$ as above, then we have that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{l,k}\\Big|\\frac{T_{l,k}}{\\mathbb{E}[T_{l,k}^{\\prime}\\,|\\,\\lambda_{l},\\lambda_{k}]}-1\\Big|=O_{p}\\Big(\\Big(\\frac{\\log n}{n}\\Big)^{1/2}\\Big)\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where the implied constant depends only on m and $c_{g}$ ", "page_idx": 41}, {"type": "text", "text": "Proof. The first part is an immediate consequence of Hoeffding\u2019s inequality for U-statistics [39], which states that for $U=((n-m)!/n!)\\cdot T$ that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(|U-\\mathbb{E}[U]|\\geq t\\Big)\\leq2\\exp\\Big(\\frac{-t^{2}\\lfloor n/m\\rfloor}{2\\|g\\|_{\\infty}^{2}}\\Big),\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "by substituting in $t\\mapsto t\\mathbb{E}[U]$ and making use of the bound $\\mathbb{E}[U]\\geq c_{g}$ ", "page_idx": 42}, {"type": "text", "text": "For the second part, we work conditionally on $\\lambda_{l},\\lambda_{k}$ and note we can decompose $T_{l,m}$ for each $l,m$ into a sum of statistics of the form $T^{\\prime}$ , one of order $\\Theta_{p}(n^{m})$ and $\\binom{m}{k}$ of order $\\Theta_{p}(n^{m-k})$ (corresponding to when some of the indices $x_{i}$ are equal) for $1\\leq k\\leq\\dot{m}$ . By applying the first concentration inequality to these $m!\\cdot n^{2}$ random variables, conditional on the $(\\lambda_{l},\\lambda_{k})$ , we note the RHS is independent of these quantities, and so the probability bounds hold unconditionally. Consequently, we know that asymptotically $T_{l,k}$ is asymptotic to $T_{l,k}^{\\prime}$ , from which we can then apply the resulting concentration bound for this term. \u53e3 ", "page_idx": 42}, {"type": "text", "text": "Theorem S28. Suppose we have a statistic of the form ", "page_idx": 42}, {"type": "equation", "text": "$$\nT_{n,\\beta,J}(\\lambda_{u},\\lambda_{v})=\\rho_{n}^{-\\beta-|J|}\\sum_{\\alpha\\in\\mathcal{V}^{\\beta-1}}g(\\lambda_{\\tilde{\\alpha}_{0}},\\ldots,\\lambda_{\\tilde{\\alpha}_{\\beta-1}},\\lambda_{u},\\lambda_{v})\\prod_{i\\leq\\beta}a_{\\tilde{\\alpha}_{i-1},\\tilde{\\alpha}_{i}}\\cdot\\prod_{j\\in J}a_{\\tilde{\\alpha}_{j-1},\\tilde{\\alpha}_{j+1}}\\cdot\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $\\boldsymbol{\\Tilde{\\alpha}}=\\left(\\alpha,u,v\\right)$ is a concatenation of $\\alpha$ , u and $v$ in order, $g:\\mathbb{R}^{\\beta+1}\\rightarrow\\mathbb{R}$ is a positive function which satisfies $c_{g}\\leq g\\leq\\|g\\|_{\\infty}<\\infty$ for some constant $c_{g}$ , and $J$ is a possibly empty set of indices. Define $\\lambda^{\\prime}=(\\widetilde{\\lambda}_{0},\\ldots,\\widetilde{\\lambda}_{\\beta-1},\\lambda_{u},\\lambda_{v})$ where $\\widetilde{\\lambda}$ is an independent copy of $\\lambda$ . Further define the statistic ", "page_idx": 42}, {"type": "equation", "text": "$$\nT_{n,\\beta,J}^{\\prime}(\\lambda_{u},\\lambda_{v}):=\\frac{(n-\\beta)!}{n!}\\cdot\\mathbb{E}\\Big[g(\\lambda^{\\prime})\\prod_{i\\leq\\beta}W(\\lambda_{i-1}^{\\prime},\\lambda_{i}^{\\prime})\\prod_{j\\in J}W(\\lambda_{j-1}^{\\prime},\\lambda_{j+1}^{\\prime})\\,|\\,\\lambda_{u},\\lambda_{v}\\Big].\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Then for any $\\rho_{n}=n^{-\\alpha}$ for $\\alpha$ sufficiently small, we have that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\beta,J,u,v}\\Big|\\frac{T_{n,\\beta,J}(\\lambda_{u},\\lambda_{v})}{T_{n,\\beta,J}^{\\prime}(\\lambda_{u},\\lambda_{v})}-1\\Big|=O_{p}\\Big(\\Big(\\frac{(\\log n)^{k}}{n\\cdot(n\\rho_{n})}\\Big)^{1/2}\\Big).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof. For this, we apply the above results. We begin by working conditionally on all of the $\\lambda$ , whose collection we denote $\\lambda$ , and note that by Theorem S26 by taking $\\lambda=(\\log n)^{\\bar{k}}$ for some $k>1$ and a union bound, we have that ", "page_idx": 42}, {"type": "equation", "text": "$$\nT_{n,\\beta,J}(\\lambda_{u},\\lambda_{v})=\\mathbb{E}[T_{n,\\beta,J}(\\lambda_{u},\\lambda_{v})\\,|\\,\\lambda]\\cdot(1+E_{n}^{(1)})\\mathrm{~where~}E_{n}^{(1)}=O\\Big(\\Big(\\frac{(\\log n)^{k}}{n\\cdot(n\\rho_{n})}\\Big)^{1/2}\\Big)\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "uniformly over all $O(m^{2}m!\\cdot n^{2})$ random variables with probability $1-\\exp(O((\\log n)^{k}))$ . As we have that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left\\{\\left[T_{n,\\beta,J}(\\lambda_{u},\\lambda_{v})\\;|\\;\\lambda\\right]=\\sum_{\\alpha\\in\\mathcal{V}^{\\beta-1}}g(\\lambda_{\\tilde{\\alpha}_{0}},\\ldots,\\lambda_{\\tilde{\\alpha}_{\\beta-1}},\\lambda_{u},\\lambda_{v})\\prod_{i\\leq\\beta}W(\\lambda_{\\tilde{\\alpha}_{i-1}},\\lambda_{\\tilde{\\alpha}_{i}})\\cdot\\prod_{j\\in J}W(\\lambda_{\\tilde{\\alpha}_{j-1}},\\lambda_{\\tilde{\\alpha}_{j+1}})\\right\\}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where the function is bounded below by cp\u03b2+|J|and is bounded above by \u2225g\u2225\u221e, we can make use of Lemma S27 to show that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\beta,J,u,v}\\Big|\\frac{\\mathbb{E}[T_{n,\\beta,J}(\\lambda_{u},\\lambda_{v})\\,|\\,\\lambda]}{T_{n,\\beta,J}^{\\prime}(\\lambda_{u},\\lambda_{v})}-1\\Big|=O_{p}\\Big(\\Big(\\frac{\\log n}{n}\\Big)^{1/2}\\Big)\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "from which the claimed result follows. ", "page_idx": 42}, {"type": "text", "text": "Remark 1. One natural question to ask about the necessity of the range of values of $\\rho_{n}$ specified above. Generally speaking, one can show for Erdos-Renyi graphs $G(n,p)$ that the number of subgraphs $Y_{H}$ of $H$ in $\\mathcal{G}_{n}$ satisfy a zero-one law, where ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{P}(Y_{H}=0)=\\left\\{1-o(1)\\;i f p\\ll n^{-c(H)},\\right.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "for some constant $c(H)$ which relates to the geometry of the graph $G$ [6]. In the latter regime, one can then show that $Y_{H}\\sim E[Y_{H}]$ asymptotically again, and in the former this shows that the term is asymptotically negligible. As the purpose of this result is to derive an asymptotic expansion for the sum of various statistics of the form of $T$ to the highest order, provided $\\rho_{n}$ is of an order which avoids any of the \"phase transition\" stages of the form above we could eventually generalize our results further. As this involves even more additional book-keeping, we do not do so here. ", "page_idx": 42}, {"type": "text", "text": "Lemma S29. Let $I$ be a finite index set of size $|I|=m$ . Suppose that there exist constants $\\tau>0$ , a bounded non-negative sequence $(p_{i})_{i\\in I}$ such that $p_{i}\\leq\\tau^{-1}$ for all $i$ , and a real sequence $(t_{i})_{i\\in I}$ . Define the random variable ", "page_idx": 43}, {"type": "equation", "text": "$$\nX=\\frac{1}{m}\\sum_{i\\in I}\\big(\\tau^{-1}a_{i}-p_{i}\\big)t_{i}\\qquad w h e r e\\qquad a_{i}\\stackrel{i n d e p}{\\sim}\\mathrm{Bernoulli}(\\tau p_{i})\\,f o r\\,i\\in I.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Then for all $u>0$ , we have that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(|X|\\geq u\\big)\\leq2\\exp\\Big(-\\operatorname*{min}\\Big\\{\\frac{u^{2}}{4\\tau^{-1}m^{-2}\\|t\\|_{2}^{2}},\\frac{u}{2\\tau^{-1}m^{-1}\\|t\\|_{\\infty}}\\Big\\}\\Big).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Proof. This follows by an application of Bernstein\u2019s inequality, by noting that $X$ is a sum of independent mean zero random variables $X_{i}=m^{-1}(\\tau^{-1}{a_{i}}^{\\circ}-p_{i}\\dot{)}t_{i}$ which satisfy ", "page_idx": 43}, {"type": "equation", "text": "$$\n|X_{i}|\\leq\\tau^{-1}m^{-1}|t_{i}|\\leq\\tau^{-1}m^{-1}\\|t\\|_{\\infty}\\;\\mathrm{for~all~}i,\\qquad\\mathbb{E}[X_{i}^{2}]\\leq m^{-2}\\tau^{-1}t_{i}^{2}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Lemma S30. Define the random variable ", "page_idx": 43}, {"type": "equation", "text": "$$\nY=\\frac{1}{n(n-1)}\\sum_{i\\neq j}\\big(\\rho_{n}^{-1}a_{i j}-W(\\lambda_{i},\\lambda_{j})\\big)T_{i j}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "for some constants $(T_{i j})$ . Write $\\begin{array}{r}{\\|T\\|_{2}^{2}=\\sum_{i\\neq j}T_{i j}^{2}}\\end{array}$ and $\\|T\\|_{\\infty}=\\operatorname*{max}_{i\\neq j}|T_{i j}|$ . Then we have that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(|Y|\\geq u\\Big)\\leq2\\exp\\Big(-\\operatorname*{min}\\Big\\{\\frac{u^{2}}{128\\rho_{n}^{-1}n^{-4}\\|T\\|_{2}^{2}},\\frac{u}{16\\rho_{n}^{-1}n^{-2}\\|T\\|_{\\infty}}\\Big\\}\\Big)\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "In particular, when $T_{i j}=1$ for all $i\\neq j$ , we have that $Y=O_{p}((n^{2}\\rho_{n})^{-1/2})$ . ", "page_idx": 43}, {"type": "text", "text": "Proof. Note that under the assumptions on the model (where we have that $a_{i j}=a_{j i}$ and ${\\cal W}(\\lambda_{i},\\lambda_{j})=$ $W(\\lambda_{j},\\lambda_{i})$ for all $i\\neq j$ ), we can write ", "page_idx": 43}, {"type": "equation", "text": "$$\nY=\\frac{2}{n(n-1)/2}\\sum_{i<j}\\big(\\rho_{n}^{-1}a_{i j}-W(\\lambda_{i},\\lambda_{j})\\big)(T_{i j}+T_{j i}).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Note that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i<j}(T_{i j}+T_{j i})^{2}\\leq2\\sum_{i<j}\\left(T_{i j}^{2}+T_{j i}^{2}\\right)\\leq2\\|T\\|_{2}^{2},}\\\\ &{\\displaystyle\\operatorname*{max}_{i<j}|T_{i j}+T_{j i}|\\leq\\operatorname*{max}_{i<j}|T_{i j}|+\\displaystyle\\operatorname*{max}_{i<j}|T_{j i}|\\leq2\\|T\\|_{\\infty},}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where we have used the inequality $(a+b)^{2}\\leq2(a^{2}+b^{2})$ which holds for all $a,b\\in\\mathbb{R}$ . Consequently, as a result of Lemma S29, we have conditional on $\\lambda$ that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(|Y|\\geq u\\,|\\,\\lambda\\Big)\\leq2\\exp\\Big(-\\operatorname*{min}\\Big\\{\\frac{u^{2}}{128\\rho_{n}^{-1}n^{-4}\\|T\\|_{2}^{2}},\\frac{u}{16\\rho_{n}^{-1}n^{-2}\\|T\\|_{\\infty}}\\Big\\}\\Big)\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "As the right hand side has no dependence on $\\lambda$ , taking expectations gives the first part of the lemma statement. For the second part, note that if $T_{i j}=1$ for all $i\\neq j$ , then we have that $\\|T\\|_{2}^{2}\\leq n^{2}$ and $\\|T\\|_{\\infty}=1$ , and consequently ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(|Y|\\ge u\\big)\\le2\\exp\\Big(-\\operatorname*{min}\\Big\\{\\frac{u^{2}}{128\\rho_{n}^{-1}n^{-2}},\\frac{u}{16\\rho_{n}^{-1}n^{-2}}\\Big\\}\\Big)\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "In particular, this implies that $Y=O_{p}((n^{2}\\rho_{n})^{-1/2})$ . ", "page_idx": 43}, {"type": "text", "text": "E.5 Miscellaneous results ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Lemma S31. Suppose that $A\\in\\mathbb{R}^{m\\times m}$ is a matrix whose diagonal entries are $\\alpha_{:}$ , and off-diagonal entries are $\\beta$ , so $A_{i j}=\\alpha\\delta_{i j}+\\beta(1-\\delta_{i j})$ , where $\\delta_{i j}$ is the Kronecker delta. Then $A$ has an eigenvalue $\\alpha+(m-1)\\beta$ of multiplicity one with eigenvector $1_{m}$ , and an eigenvalue $\\alpha-\\beta$ of multiplicity $m-1$ , whose eigenvectors form an orthonormal basis of the subspace $\\{v\\,:\\,\\langle v,1_{m}\\rangle=0\\}$ . For the subspace $\\{v\\,:\\,\\langle v,1_{m}\\rangle=0\\}$ , we can take the eigenvectors to be ", "page_idx": 44}, {"type": "equation", "text": "$$\nv_{i}=\\frac{1}{\\sqrt{2}}(e_{m,1}-e_{m,i+1})\\,f o r\\,i\\in[m-1]\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where $e_{m,i}$ are the unit column vectors in $\\mathbb{R}^{m}$ , The singular values of $A$ are $|\\alpha-\\beta|$ and $|\\alpha\\!+\\!(\\kappa\\!-\\!1)\\beta|$ . Consequently, we can write $A=U V^{T}$ for matrices $U,V\\in\\mathbb{R}^{m\\times m}$ with $U U^{T}=V V^{T}$ , where the rows of $U$ satisfy ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{U_{1:}=\\displaystyle\\frac{|\\alpha+\\beta(m-1)|^{1/2}}{\\sqrt{m}}e_{m,1}+\\frac{|\\alpha-\\beta|^{1/2}}{\\sqrt{2}}e_{m,2}}}\\\\ {{U_{i:}=\\displaystyle\\frac{|\\alpha+\\beta(m-1)|^{1/2}}{\\sqrt{m}}e_{m,1}-\\frac{|\\alpha-\\beta|^{1/2}}{\\sqrt{2}}e_{m,i}f o r\\,i\\in\\{2,\\ldots,m\\}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Consequently, we then have that $\\lVert U_{i\\cdot}\\rVert_{2}\\,\\le\\,\\bigl(2|\\alpha+\\beta(m-1)|/m+|\\alpha-\\beta|/2\\bigr)^{1/2}$ for all $i$ , and $\\begin{array}{r}{\\operatorname*{min}_{i\\neq j}\\|U_{i\\cdot}-U_{j\\cdot}\\|_{2}=(|\\alpha-\\beta|)^{1/2}}\\end{array}$ . ", "page_idx": 44}, {"type": "text", "text": "Further suppose that $\\beta=-\\alpha/(m-1)$ . Then provided $\\alpha>0$ , $A$ is positive semi-definite, is of rank $m-1$ , with a singular non-zero eigenvalue \u03b1m/ $[m-1)$ of multiplicity $m-1$ . Consequently one can write $A=U U^{T}$ where $U\\,\\in\\,\\mathbb{R}^{m\\times(m-1)}$ and whose columns equal the $\\sqrt{\\alpha m/(m-1)}v_{i}$ . $I n$ particular, the rows of $U$ equal ", "page_idx": 44}, {"type": "equation", "text": "$$\nU_{1}.=\\big(\\frac{\\alpha m}{2(m-1)}\\big)^{1/2}e_{m-1,1}^{T},\\ \\ \\ U_{i\\cdot}=-\\big(\\frac{\\alpha m}{2(m-1)}\\big)^{1/2}e_{m-1,i-1}^{T}f o r\\,i\\in[2,m].\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Consequently, one has that $\\lVert U_{i\\cdot}\\rVert_{2}=\\sqrt{\\alpha m/(m-1)}$ for all $i_{\\cdot}$ , and moreover we have the separability condition $\\mathrm{min}_{1\\le i<j\\le m}\\,\\|U_{i\\cdot}-U_{j\\cdot}\\|_{2}=(\\alpha m/(m-1))^{1/2}$ . ", "page_idx": 44}, {"type": "text", "text": "Proof. It is straightforward to verify that $A$ has an eigenvalue of $\\alpha+(n-1)\\beta$ with the claimed eigenvector. For the second part, we note that the characteristic polynomial of $A$ is ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\operatorname*{det}(A-t I)=(\\alpha-\\beta-t)^{n-1}\\cdot(\\alpha+(n-1)\\beta-t)\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and so $A$ has $m-1$ eigenvalues equal to $\\alpha-\\beta$ ; as $A$ is symmetric, we know that we can always take eigenvectors to be orthogonal to each other, and consequently the eigenspace associated with such an eigenvalue must be a subspace of $\\{v\\,:\\,\\langle v,1_{m}\\rangle=0\\}$ . As both of these subspaces are of dimension $m-1$ , it consequently follows that they are equal. We then highlight that if $A$ is a symmetric matrix with eigendecomposition $A=Q\\dot{\\Lambda}Q^{T}$ for an orthogonal matrix $Q$ , then the SVD is given by $Q|\\Lambda|\\mathrm{sgn}(\\Lambda)Q^{T}$ , and we can write $A=U V^{T}$ with $U=Q\\bar{|}\\Lambda|^{1/2}$ and $V=Q\\mathrm{sgn}(\\Lambda)|\\Lambda|^{1/2}$ such that $U\\dot{U}^{T}=\\dot{V}\\dot{V}^{T}$ . This allows us to derive the remaining statements about the matrix $A$ which hold in generality. The remaining parts discussing what occurs when $\\beta=-\\alpha/(m-1)$ follow by routine calculation. \u53e3 ", "page_idx": 44}, {"type": "text", "text": "Lemma S32. Let $\\sigma(x)=(1+\\exp(-x))^{-1}$ be the sigmoid function. Then there exists a unique $y\\in\\mathbb R$ which solves the equation ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\alpha\\sigma(y)=\\beta+\\gamma\\sigma(-y/s)\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "for $\\alpha,\\gamma,s\\,>\\,0$ and $\\beta\\in\\mathbb R$ if and only if $\\beta<\\alpha$ and $\\beta+\\gamma>0$ . Moreover, $y>0$ if and only $i f$ $\\beta+\\gamma/2>\\alpha/2$ . ", "page_idx": 44}, {"type": "text", "text": "Proof. Note that $\\alpha\\sigma(x)$ is a function whose range is $(0,\\alpha)$ on $x\\in(-\\infty,\\infty)$ , and is strictly monotone increasing on the domain. Similarly, $\\beta+\\gamma\\sigma(-y/s)$ is strictly monotone decreasing with range $(\\beta,\\beta+\\gamma)$ , and so simple geometric considerations of the graphs of the two functions gives the existence result. For the second part, note that the ranges of the functions on the LHS and the RHS on the range $y>0$ are $[\\alpha/2,\\alpha)$ and $(\\beta,\\beta+\\gamma/2]$ respectively, and so the same considerations as above give the second claim. \u53e3 ", "page_idx": 44}, {"type": "text", "text": "Lemma S33. Let $\\sigma(x)=(e^{x})/(1+e^{x})$ be the sigmoid function. Then for any $x,y\\in\\mathbb{R},$ , we have that ", "page_idx": 45}, {"type": "text", "text": "where ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\log(1-\\sigma(x))\\geq-\\log(1-\\sigma(y))+\\sigma(y)(x-y)+E(x-y)}\\\\ &{\\quad E(z)=\\bigg\\{\\frac{1}{2}e^{-A}z^{2}\\quad}&{i f|x|,|y|\\leq A,}\\\\ &{\\quad E(z)=\\bigg\\{\\frac{1}{4}e^{-A}\\operatorname*{min}\\{z^{2},2|z|\\}\\quad}&{i f e i t h e r\\,|x|\\leq A\\mathrm{~}o r\\,|y|\\leq A.}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Proof. Note that by the integral version of Taylor\u2019s theorem, for a twice differentiable function $f$ one has for all $x,y\\in\\mathbb{R}$ that ", "page_idx": 45}, {"type": "equation", "text": "$$\nf(x)=f(y)+f^{\\prime}(y)(x-y)+\\int_{0}^{1}(1-t)f^{\\prime\\prime}(t x+(1-t)y)(x-y)^{2}\\,d t.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Applying this to $f(x)=-\\log\\sigma(x)$ gives ", "page_idx": 45}, {"type": "equation", "text": "$$\n-\\log\\sigma(x)=-\\log\\sigma(y)+(-1+\\sigma(y))(x-y)+\\int_{0}^{1}(1-t)(x-y)^{2}\\sigma^{\\prime}(t x+(1-t)y)\\,d t\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where $\\sigma^{\\prime}(x)=e^{x}/(1+e^{x})^{2}$ . Applying this to $f(x)=\\log(1-\\sigma(x))$ gives ", "page_idx": 45}, {"type": "equation", "text": "$$\n-\\log(1-\\sigma(x))=-\\log(1-\\sigma(y))+\\sigma(y)(x-y)+\\int_{0}^{1}(1-t)(x-y)^{2}\\sigma^{\\prime}(t x+(1-t)y)\\,d t\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "As the integral terms are the same, we concentrate on lower bounding this quantity. To do so, we make use of the lower bound $\\sigma^{\\prime}(x)\\geq e^{-|x|}/4$ (Lemma 68 of Davison and Austern [11]) which holds for all $x\\in\\mathbb R$ . We then note that if $|x|,|y|\\leq A$ , then we have that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle-\\log(1-\\sigma(x))=-\\log(1-\\sigma(y))+\\sigma(y)(x-y)+\\int_{0}^{1}(1-t)(x-y)^{2}\\sigma^{\\prime}(t x+(1-t)y)\\,d t}\\\\ {\\displaystyle-\\log(1-\\sigma(y))+\\sigma(y)(x-y)+\\displaystyle\\frac{e^{-|A|}}{2}(x-y)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Alternatively, if we only make use of the fact that $|x|\\leq A$ (without loss of generality - the argument is essentially equivalent if we only assume that $\\vert y\\vert\\le A$ ), then we have that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int_{0}^{1}(1-t)\\sigma^{\\prime}(t x+(1-t)y)(x-y)^{2}\\,d t\\ge\\int_{0}^{1}(1-t)e^{-|t x+(1-t)y|}(x-y)^{2}\\,d t}}\\\\ &{\\ge\\int_{0}^{1}(1-t)e^{-|x|}e^{-(1-t)|x-y|}(x-y)^{2}\\,d t}\\\\ &{=e^{-|x|}\\big\\{|x-y|+e^{-|x-y|}-1\\big\\}}\\\\ &{\\ge\\frac14e^{-A}\\operatorname*{min}\\{(x-y)^{2},2|x-y|\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "and consequently we get that ", "page_idx": 45}, {"type": "equation", "text": "$$\n-\\log(1-\\sigma(x))\\geq-\\log(1-\\sigma(y))+\\sigma(y)(x-y)+\\frac{1}{4}e^{-A}\\operatorname*{min}\\{|x-y|^{2},2|x-y|\\}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Lemma S34. Suppose that we have a function ", "page_idx": 45}, {"type": "equation", "text": "$$\nf(X)={\\frac{1}{m^{2}}}\\sum_{i,j=1}^{m}\\operatorname*{min}\\{X_{i j}^{2},2|X_{i j}|\\}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Then if $f(X)\\leq r$ , we have that $\\begin{array}{r}{m^{-2}\\sum_{i,j=1}^{m}|X_{i j}|\\leq r+r^{1/2}}\\end{array}$ . ", "page_idx": 45}, {"type": "text", "text": "Proof. To proceed, note that if we have that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\operatorname*{min}\\{X^{2},2X\\}]\\leq r\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "for a non-negative random variable $X$ , then by Jensen\u2019s inequality we get that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\left(\\mathbb{E}[X1[X<2]]\\right)^{2}+\\mathbb{E}[X1[X\\geq2]]\\leq\\mathbb{E}[\\operatorname*{min}\\{X^{2},2X\\}]\\leq r\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "and consequently $\\mathbb{E}[X]\\leq r+r^{1/2}$ by decomposing $\\mathbb{E}[X]$ into the parts where $X\\geq2$ and $X<2$ Applying this result to the empirical measure on the $|X_{i j}|$ across indices $i,j\\in[m]$ gives the desired result. \u53e3 ", "page_idx": 45}, {"type": "text", "text": "F Minimizers for degree corrected SBMs when $\\alpha\\neq1$ ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "In this section, we give an informal discussion of how to study the minimizers of $\\mathcal{R}_{n}(M)$ for degree corrected SBMs when the unigram parameter $\\alpha\\neq1$ . We begin by highlighting that ${\\mathcal{R}}_{n}(M)$ does not concentrate around its expectation when averaging over only the degree heterogenity parameters $\\theta_{i}$ , which rules out using a similar proof approach as to what was carried out earlier in Appendix 1. ", "page_idx": 46}, {"type": "text", "text": "Recall that we were able to derive that the global minima of ${\\mathcal{R}}_{n}(M)$ was the matrix ", "page_idx": 46}, {"type": "equation", "text": "$$\nM_{i j}^{*}=\\log\\Big(\\frac{2\\mathcal{E}_{W}(\\alpha)}{(1+k^{-1})\\mathbb{E}[\\theta]\\mathbb{E}[\\theta]^{\\alpha}}\\cdot\\frac{P_{c(i),c(j)}}{\\widetilde{P}_{c(i)}\\widetilde{P}_{c(j)}\\cdot\\big(\\theta_{i}^{\\alpha-1}\\widetilde{P}_{c(i)}^{\\alpha-1}+\\theta_{j}^{\\alpha-1}\\widetilde{P}_{c(j)}^{\\alpha-1}\\big)}\\Big).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "When $\\alpha=1$ or the $\\theta_{i}$ are constant, this allows us to write $M^{*}=\\Pi M\\Pi^{T}$ where $\\Pi$ is the matrix of community assignments for the network and $M$ is some matrix, which allows us to simplify the problem. If we supposed that the $\\theta$ actually had some dependence on the $c(i)$ and were discrete - in that $\\theta_{i}|c(i)=l\\sim\\^{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}Q_{l}$ for some discrete distributions $Q_{l}$ for $l\\in[\\kappa]$ , then we could in fact employ the same type of argument as done throughout the paper. The major change is that then the embedding vectors would each concentrate around a vector decided by both a) their community assignment, and b) the particular degree correction parameter they were assigned. This would then potentially effect our ability to perform community detection depending on the underlying geometry of these vectors. One possible idea would be to explore ${\\mathcal{R}}_{n}(M)$ partially averaged over the $\\theta_{i}$ - we divide the $\\theta_{i}$ into $B$ bins where $B=n^{\\beta}$ for some $\\beta\\in(0,1)$ , and average over only over the refinement of the $\\theta_{i}$ as belonging to the different bins. This would be similar to the argument employed in Davison and Austern [11]. ", "page_idx": 46}, {"type": "text", "text": "An alternative perspective to give some type of guarantee on the concentration of the embedding vectors is to study the rank of the matrix $M^{*}$ . If we are able to prove that is of finite rank $r$ even as $n$ grows large, then we are able to give a convergence result for the embeddings as soon as the embedding dimension $d$ is greater than or equal to $r$ . To study this, it suffices to look at the matrix ", "page_idx": 46}, {"type": "equation", "text": "$$\n(M_{E}^{*})_{i j}=\\log\\big(\\theta_{i}^{\\alpha-1}\\widetilde{P}_{c(i)}^{\\alpha-1}+\\theta_{j}^{\\alpha-1}\\widetilde{P}_{c(j)}^{\\alpha-1}\\big)\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "and argue that this is low rank (due to the logarithm, we can write $M^{*}$ as the difference between this matrix and a matrix of rank $\\kappa$ , which is therefore also low rank). The entry-wise logarithm is a complicating factor here, as otherwise it is straightforward to argue that the entry-wise exponential of this matrix is of rank 2. One can reduce studying the rank of the matrix $M_{E}^{*}$ to studying the rank of the kernel ", "page_idx": 46}, {"type": "equation", "text": "$$\nK_{M}\\big((x,c_{x}),(y,c_{y})\\big)=\\log\\big(x^{\\alpha-1}\\tilde{P}_{c_{x}}^{\\alpha-1}+y^{\\alpha-1}\\tilde{P}_{c_{y}}^{\\alpha-1}\\big)\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "of an operator $L^{2}(P)\\rightarrow L^{2}(P)$ , where $P$ is the product measure induced by $\\theta$ and the community assignment mechanism $c$ . As $K_{M}$ is of finite rank $r$ if and only if it can be written as ", "page_idx": 46}, {"type": "equation", "text": "$$\nK_{M}\\big((x,c_{x}),(y,c_{y})\\big)=\\sum_{i=1}^{r}\\phi_{i}(x,c_{x})\\psi_{i}(y,c_{y})\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "for some functions $\\phi_{i},\\psi_{i}$ , it follows that the matrix $(M_{E}^{*})_{i j}$ will be of finite rank $r$ also. Indeed, this representation forces that $M_{E}^{*}=\\Phi\\Psi^{T}$ for some matrices $\\bar{\\Phi},\\Psi\\in\\mathbb{R}^{n\\times r}$ , meaning that $M_{E}^{*}$ is of rank $\\leq r$ ; Corollary 5.5 of Koltchinskii and Gin\u00e9 [24] then guarantees convergence of the eigenvalues of the matrix $M_{E}^{*}$ to the operator $K_{M}$ so that $M_{E}^{*}$ is actually of full rank. ", "page_idx": 46}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: The claims in the abstract and introduction match the theoretical contributions of the paper. These are supported by experimental verification. Similarly, where theoretical results are not obtained we investigate these scenarios using simulation. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 47}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: We highlight and discuss the assumptions required for the theoretical results presented within the paper and in detail in the appendix. We demonstrate empirically the performance of our procedure when these assumptions are relaxed, if theoretical results were not obtained. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 47}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Justification: Due to space constraints all proofs appear in the supplemental material. We provide intuition for these proofs in the paper where space allows. Complete proofs are included in the supplemental material, along with all required Lemmas and exact assumptions. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 48}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: We detail the experimental setup used in this work in the supplemental procedure, along with providing all code required to run and replicate these experiments also. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 48}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 49}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: We have included an anonymized version of the code repository used to create all experimental results in this paper. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 49}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Justification: We provide sufficient detail in the main text to understand the experimental results presented. In the appendix, we completely detail all experimental details, along with providing the exact code used as supplemental material. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 49}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: For all experimental results we either show error bars corresponding to one standard error or all simulation results (in the case of box plots). ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 50}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: The computation required for individual experiments was relatively small (in terms of both memory and time) and is detailed in the appendix. These were run on a computing cluster. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 50}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: We have ensured the research conforms with the code of ethics. Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 50}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: This paper provides theoretical guarantees for community detection in a specific class of statistical network models. Any potential societal impacts, positive or negative, will be ancillary from the theoretical focus of this paper. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 51}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Justification: As this work theoretical guarantees for community detection in a specific class of statistical network models, such safeguards are not applicable. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 51}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: We credit the original owners of code and data used. Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 51}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 52}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: We provide an anonymized zip flie which details the code used to generate all results. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 52}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: Crowdsourcing or human subjects were not used in this research. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 52}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 52}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] Justification: Crowdsourcing or human subjects were not used in this research. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 53}]