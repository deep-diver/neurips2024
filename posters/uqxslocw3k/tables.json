[{"figure_path": "uqxSLoCw3K/tables/tables_5_1.jpg", "caption": "Table 1: The datasets used in experiments and their corresponding tasks. # Train and # Validation denote the numbers of samples during training and validation, respectively. # Demo denotes the average number of demonstrations used in each task. # Expert represents the number of experts used in each task.", "description": "This table presents the datasets used in the experiments, categorized by type (classification or generation) and task.  For each dataset, it provides the number of training samples, validation samples, and the average number of demonstrations used per task during validation. It also indicates the number of experts used in the MoD framework for each task.", "section": "4.1 Experimental Settings"}, {"figure_path": "uqxSLoCw3K/tables/tables_6_1.jpg", "caption": "Table 2: The comparative results of our method and other baselines on various datasets. We present the absolute performance gain over CEIL, and the best results are shown in bold.", "description": "This table presents a comparison of the proposed MoD method against several baselines across various NLP datasets.  It shows the absolute performance improvement of MoD over the CEIL baseline for both classification and generation tasks.  The best results for each dataset are highlighted in bold.", "section": "4.2 Comparative Results"}, {"figure_path": "uqxSLoCw3K/tables/tables_7_1.jpg", "caption": "Table 3: Performance of our framework and various baselines on processed compositional datasets GeoQuery and SMCalFlow-CS. S refers to a non-compositional test set and C refers to a compositional set with additional cross-domain examples as demonstrations.", "description": "This table presents the performance comparison of MoD and other baselines (TopK-BERT, EPR, CEIL) on two compositional datasets: GeoQuery and SMCalFlow-CS.  Each dataset has two subsets: a standard non-compositional set (S) and a compositional set (C) that includes cross-domain examples. The results are reported in terms of absolute performance gains of MoD compared to CEIL, showing MoD's superior performance across all datasets, particularly in the compositional subsets.", "section": "4.3 Results on Compositional Datasets"}, {"figure_path": "uqxSLoCw3K/tables/tables_8_1.jpg", "caption": "Table 4: Performance improvements over TopK-BERT when transferring learned retriever models in MoD to other LLMs on four datasets.", "description": "This table presents the results of a robustness study evaluating the transferability of the MoD framework across different LLMs.  Retriever models trained on one LLM (GPT-Neo and LLAMA-7B) were used to select demonstrations for other LLMs (GPT-Neo, GPT2-XL, LLAMA-7B, and GPT3.5) on four different datasets (SST-5, CMSQA, GeoQ, and MTOP). The table shows the absolute performance gain of MoD over TopK-BERT for each LLM-dataset combination, demonstrating the model's robustness and transferability.", "section": "4.5 Robustness Study"}, {"figure_path": "uqxSLoCw3K/tables/tables_15_1.jpg", "caption": "Table 2: The comparative results of our method and other baselines on various datasets. We present the absolute performance gain over CEIL, and the best results are shown in bold.", "description": "This table compares the performance of the proposed MoD framework against several other baselines (both learning-free and learning-based methods) across various datasets encompassing classification and generation tasks.  The table shows the absolute performance improvement of MoD over the CEIL baseline for each dataset and task.  The best performance is highlighted in bold.", "section": "4.2 Comparative Results"}, {"figure_path": "uqxSLoCw3K/tables/tables_16_1.jpg", "caption": "Table 6: Performance under various numbers of in-context examples.", "description": "This table compares the performance of CEIL and MoD models with different numbers of in-context examples (4 and 16) across three datasets (MRPC, SST-5, and MTOP). It demonstrates that MoD consistently outperforms CEIL, even when using fewer examples.", "section": "E.4 Effect of Retriever Models"}, {"figure_path": "uqxSLoCw3K/tables/tables_17_1.jpg", "caption": "Table 7: Ablation study results of specific designs in the expert-wise training.", "description": "This table presents the ablation study results of the MoD framework. By removing the few-shot scoring, the random selection of demonstrations, and the hard negative samples in contrastive learning, the impact of each component in expert-wise training is evaluated. The results show that all three components contribute positively to the overall performance, with the few-shot scoring being the most critical.", "section": "E.1 Impact of Designs in Expert-wise Training"}, {"figure_path": "uqxSLoCw3K/tables/tables_17_2.jpg", "caption": "Table 8: Results of transferring a retriever learned on one dataset (row) to others (column). We report the absolute improvement over the baseline TopK-BERT.", "description": "This table presents the results of an experiment evaluating the transferability of the retriever model trained in the MoD framework.  A retriever model was trained on each of four datasets (SST-5, MNLI, GeoQ, MTOP), and then used to select demonstrations for each of those four datasets. The table shows the absolute performance gain over a baseline model (TopK-BERT) for each dataset-to-dataset transfer.  Positive values indicate improved performance, while negative values show a decline in performance when the model is transferred.", "section": "E.2 Transferability of MoD Retriever"}, {"figure_path": "uqxSLoCw3K/tables/tables_18_1.jpg", "caption": "Table 9: Impact of different embedding models on clustering performance on dataset MRPC.", "description": "This table presents the results of clustering performance using three different embedding models (Sentence-BERT, Arctic-xs, and Arctic-m) on the MRPC dataset. The metrics used for evaluation are Silhouette Score, Davies-Bouldin Index, Dunn Index, and Accuracy.  The table helps to assess the impact of the choice of embedding model on the quality of the resulting clusters, and ultimately on the overall classification accuracy.", "section": "E.3 Effect of Embedding Models"}, {"figure_path": "uqxSLoCw3K/tables/tables_18_2.jpg", "caption": "Table 10: Impact of different retriever backbone models.", "description": "This table presents the results of experiments comparing the performance of EPR and MoD using different retriever models (BERT-base, RoBERTa, and DeBERTa) on four datasets: SST-5, CMSQA, GeoQ, and MTOP. The results show that MoD consistently outperforms EPR across all models, highlighting the benefit of the MoD framework.", "section": "E.4 Effect of Retriever Models"}, {"figure_path": "uqxSLoCw3K/tables/tables_18_3.jpg", "caption": "Table 11: Effect of K and K.", "description": "This table presents the results of experiments conducted to evaluate the effect of varying the number of top-K demonstrations retrieved by each expert (K) and the number of candidate demonstrations considered for scoring (K) within the MoD framework. The results demonstrate the impact of these hyperparameters on model performance across different datasets.", "section": "E.5 Effect of K and K"}, {"figure_path": "uqxSLoCw3K/tables/tables_19_1.jpg", "caption": "Table 12: Effect of the number of hard negatives.", "description": "This table presents the ablation study results on the effect of varying the number of hard negative samples in the contrastive learning loss during the expert-wise training of the MoD framework.  The results show that using a moderate number of hard negative samples, around 5, yields the best performance across four datasets (SST-5, CMSQA, GeoQ, and MTOP). Increasing the number of hard negatives beyond this optimal point leads to a decline in performance.", "section": "E.6 Effect of Hard Negative Sampling"}]