[{"type": "text", "text": "Handling Learnwares from Heterogeneous Feature Spaces with Explicit Label Exploitation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Peng Tan, Hai-Tian Liu, Zhi-Hao Tan, Zhi-Hua Zhou National Key Laboratory for Novel Software Technology, Nanjing University, China School of Artificial Intelligence, Nanjing University, China {tanp,liuht,tanzh,zhouzh}@lamda.nju.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The learnware paradigm aims to help users leverage numerous existing highperforming models instead of starting from scratch, where a learnware consists of a well-trained model and the specification describing its capability. Numerous learnwares are accommodated by a learnware dock system. When users solve tasks with the system, models that fully match the task feature space are often rare or even unavailable. However, models with heterogeneous feature space can still be helpful. This paper finds that label information, particularly model outputs, is helpful yet previously less exploited in the accommodation of heterogeneous learnwares. We extend the specification to better leverage model pseudo-labels and subsequently enrich the unified embedding space for better specification evolvement. With label information, the learnware identification can also be improved by additionally comparing conditional distributions. Experiments demonstrate that, even without a model explicitly tailored to user tasks, the system can effectively handle tasks by leveraging models from diverse feature spaces. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The current machine learning paradigm has achieved remarkable success across various domains. This success, however, hinges on several critical factors: access to abundant high-quality labeled data, expensive computational resources, and deep expertise in feature engineering and algorithm design. These requirements pose a significant challenge for ordinary individuals aiming to build high-quality models from scratch. Moreover, issues such as data privacy, the difficulty of model adaptation, and catastrophic forgetting complicate the reuse or adaptation of trained models across different users. ", "page_idx": 0}, {"type": "text", "text": "Indeed, most efforts have focused on these issues separately, paying less attention to the fact that these problems are entangled. To address these challenges simultaneously, the learnware paradigm was proposed by Zhou [2016]. The learnware paradigm [Zhou and Tan, 2024] aims to assist users in solving their tasks by leveraging existing high-performing models, through the establishment of a learnware dock system. One important purpose of learnware paradigm is to enable high-performing models, submitted by developers, to be used \"beyond-what-was-submitted.\" This means that models can be repurposed to assist with tasks not originally targeted by developers. To achieve this, learnware is designed as a high-performing model with a specification describing its capability and utility. The specification, a central component for learnware management and identification, can be implemented by sketching the data distribution in which the model is proficient [Zhou and Tan, 2024]. Recently, to facilitate research on the learnware paradigm, the learnware dock system, Beimingwu, has been released [Tan et al., 2024a]. ", "page_idx": 0}, {"type": "text", "text": "Previous research [Liu et al., 2024, Xie et al., 2023, Zhang et al., 2021] focuses on the homogeneous case where models and user tasks share the same feature space. However, in real-world scenarios, the feature spaces of models often differ due to varied feature engineering. As an example, we consider the widely used clinical database, the OMOP Common Data Model [Biedermann et al., 2021], as illustrated in Figure 1. This model manages healthcare data from various sources through several standardized tables, such as demographic information, diagnoses, laboratory results, and medications. Experts across different hospitals often use different tables for feature engineering, even when working on the same clinical task, leading to the development of heterogeneous models. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In order to manage and exploit models developed from heterogeneous feature spaces, it is essential to build connections between these different spaces. Existing related techniques for exploiting relationships between feature spaces either rely on raw data [Wang and Sun, 2022, Zhu et al., 2023] of the model or utilize additional co-occurrence data [Xu et al., 2013, Huang et al., 2023]. However, with model specifications, the learnware dock system can determine the relationships through subspace learning without the need for raw data or extra auxiliary data [Tan et al., 2023]. To effectively accommodate heterogeneous learnwares, a unified subspace is constructed based on specifications of all submitted models, which helps to evolve the specification to have the capabilities of meeting requirements across different feature spaces. This paper finds that, without label information, subspace learning tends to yield suboptimal results, causing embeddings with entangled class representations in the subspace, or even rendering them meaningless when feature spaces are only weakly correlated. Additionally, without exploiting label information, the system can only identify models with marginal distributions similar to the user\u2019s task, ignoring models\u2019 capabilities. ", "page_idx": 1}, {"type": "image", "img_path": "3YIyB82rjX/tmp/e61fff228c5e953fd4a4667979874746330f5f529845e43dc939c3a2cc1188fb.jpg", "img_caption": ["Figure 1: Heterogeneous feature space models in real-world scenario. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "This paper explicitly leverages label information for managing and utilizing heterogeneous models. We extend the specification to better incorporate model pseudo-labels, enabling the transition from unsupervised to supervised subspace learning for better specification evolvement. The extended specification also allows for additional comparison of conditional distributions using label information, thereby improving the learnware identification. The contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 This paper proposes to exploit the model outputs to evolve specifications into a unified space during heterogeneous learnware accommodation. Specifically, the unified space is constructed based on the specifications of all models. By exploiting model outputs encoded in the specification, the resulting subspace exhibits improved properties, with less entangled class representations and more coherent embeddings.   \n\u2022 This paper extends the specification implementation to more effectively leverage label information by encoding both marginal and conditional distributions. This extended specification provides more accurate label information during subspace learning to better evolve specifications. Additionally, it also allows for additional comparison of conditional distributions, thereby improving the learnware identification.   \n\u2022 Experiments demonstrate that, even without a model explicitly tailored to the user\u2019s task, the system can effectively handle the task by leveraging models from diverse feature spaces. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminary ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Specification is the central part of the learnware, capturing the model ability. This section briefly introduces the Reduced Kernel Mean Embedding (RKME) specification [Zhou and Tan, 2024], which sketches the joint distribution of task features and model outputs with kernel methods. ", "page_idx": 1}, {"type": "text", "text": "We start by introducing the Kernel Mean Embedding (KME) [Sch\u00f6lkopf and Smola, 2002], which offers a novel representation for distributions. KME transforms a distribution into a reproducing kernel Hilbert space (RKHS). Given a distribution $\\mathcal{D}$ defined over a space $\\mathcal{X}$ , the KME is defined as $\\begin{array}{r}{\\mu_{k}(\\mathcal{D}):=\\int_{\\mathcal{X}}\\tilde{k}(\\pmb{x},\\cdot)\\mathrm{d}\\mathcal{D}(\\pmb{x})}\\end{array}$ , where $k:\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}$ is a symmetric and positive definite kernel function, and its associated RKHS is $\\mathcal{H}$ . For a data set $\\{{\\pmb x}_{i}\\}_{i=1}^{m}$ sampled from $\\mathcal{D}$ , the empirical estimate of KME is given by $\\begin{array}{r}{\\hat{\\mu}_{k}(\\mathcal{D}):=\\frac{1}{m}\\sum_{i=1}^{m}k\\left(\\pmb{x}_{i},\\cdot\\right)}\\end{array}$ . ", "page_idx": 1}, {"type": "text", "text": "KME is considered as a potential specification due to several favorable properties. Accessing the raw data, however, compromises the necessary privacy concerns of the specification. Based on KME, the RKME specification is proposed to use a reduced set of minor weighted samples $\\{(\\beta_{j},t_{j})\\}_{j=1}^{n}\\,,n\\ \\ll\\,\\stackrel{\\cdot}{m}$ to approximate the empirical KME of the original dataset with model pseudo-outputs $\\{\\pmb{q}_{i}\\}_{i=1}^{m}=\\{(\\pmb{x}_{i},\\hat{y}_{i})\\}_{i=1}^{m}$ , where ${\\hat{y}}_{i}=f(\\pmb{x}_{i})$ is the model prediction. The reduced set is generated by: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\beta,t}\\left\\|\\frac{1}{m}\\sum_{i=1}^{m}k\\left(\\pmb{q}_{i},\\cdot\\right)-\\sum_{j=1}^{n}\\beta_{j}k\\left(\\pmb{t}_{j},\\cdot\\right)\\right\\|_{\\mathcal{H}}^{2},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with the non-negative coefficients $\\{\\beta_{j}\\}_{j=1}^{n}$ . The RKME $\\begin{array}{r}{\\Phi(\\cdot)=\\sum_{j=1}^{n}\\beta_{j}k\\left(\\pmb{t}_{j},\\cdot\\right)\\in\\mathcal{H}}\\end{array}$ acts as the specification, and the RKHS $\\mathcal{H}$ is referred to as the specification space. This specification effectively captures the major information of the distribution $\\mathcal{D}$ without exposing raw data and explicitly encodes the model capability based on its outputs. Notably, in simple cases where the features are sufficient to represent the model capability, the sketch solely on the features $\\{{\\pmb x}_{i}\\}_{i=1}^{m}$ can also be used as the model specification [Wu et al., 2023]. In this paper, we further extend the specification generation process to more effectively encode the model\u2019s outputs. ", "page_idx": 2}, {"type": "text", "text": "3 Problem setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This paper addresses the challenge of constructing a heterogeneous learnware dock system and leveraging it to assist users who have only limited labeled data such that training a model by themselves will lead to poor performance. Without loss of generality, we consider the underlying full feature space, denoted as ${\\mathcal X}_{\\mathrm{all}}$ , as a composite of $Q$ distinct blocks, i.e., $\\mathcal{X}_{\\mathrm{all}}\\,=\\,\\mathcal{X}_{1}\\,\\times\\,\\cdot\\,\\cdot\\,\\times\\,\\mathcal{X}_{Q}$ . The feature spaces for developers, ${\\mathcal{X}}^{\\mathrm{dev}}$ , and for users, $\\chi^{\\mathrm{user}}$ , are represented as Cartesian products of specific blocks ${\\times}_{i\\in C}{\\mathcal{X}}_{i}$ , where $C$ refers to block indices. ", "page_idx": 2}, {"type": "text", "text": "The overall procedure consists of two stages: the submission stage and the deployment stage. In the submission stage, the developer trains a well-performing model $f_{i}$ on the dataset ${\\cal D}_{i}\\;:=\\;$ $\\{(\\overline{{\\mathbf{x}_{i j}}},\\overline{{y_{i j}}})\\}_{j=1}^{n_{i}}$ and generates a developer-level specification $\\pmb{s}_{i}^{\\mathrm{dev}}$ , which captures the model\u2019s performance without exposing raw data. After receiving all heterogeneous models and their developerlevel specifications, the learnware dock system assigns a system-level specification $s_{i}$ to each model $f_{i}$ , based on all submitted specifications $\\bar{\\{\\mathbf{s}_{i}^{\\mathrm{dev}}\\}}_{i=1}^{N}$ . The heterogeneous learnware dock system is then constructed as $\\{(f_{i},\\pmb{s}_{i})\\}_{i=1}^{N}$ . In the deployment stage, the user has unlabeled data $D_{0}^{u}=\\{{\\pmb x}_{0i}\\}_{i=1}^{n_{u}}$ and a limited amount of labeled data $D_{0}^{l}=\\{(\\tilde{{\\pmb x}}_{0i},y_{0i})\\}_{i=1}^{n_{l}}$ (the unlabeled data cover labeled data features, i.e., $\\{\\tilde{x}_{i}\\}_{i}\\subseteq\\{x_{i}\\}_{i})$ . The user generates a user-level task requirement $\\pmb{s}_{0}^{\\mathrm{user}}$ and submits it to the learnware dock system. The system then identifies the most helpful model(s) for reuse to tackle the user task. ", "page_idx": 2}, {"type": "image", "img_path": "3YIyB82rjX/tmp/2f62946b705b4bdddd94d51cdbf2fb34b5c308858e6b31a625f495a0108ba18d.jpg", "img_caption": ["Figure 2: An illustration of the learnware paradigm with heterogeneous feature spaces "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "4 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section outlines our methodology for accommodating heterogeneous models under the learnware paradigm and assisting user tasks, emphasizing the importance and utilization of label information, which remains unexplored in learnware paradigm when dealing with heterogeneous feature spaces. ", "page_idx": 2}, {"type": "text", "text": "4.1 Improve managing heterogeneous models with label information ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To handle learnwares with heterogeneous feature spaces, it is helpful to exploit the relationships between these spaces. A common approach is to learn a unified subspace. However, without label information, the resulting subspace may produce entangled embeddings of samples from different classes, and when feature blocks are weakly correlated, the subspace may become meaningless (see Section B.2 for detailed discussion). Since subspace learning is based on all learnware specifications, incorporating label information into the model specification is highly beneficial. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "We rewrite the RKME specification represented by $R=(\\beta,T)=\\left\\{(\\beta_{j},t_{j})\\right\\}_{j=1}^{m}$ to $\\mathtt{R K M E_{L}}$ represented by $R_{L}\\,=\\,(\\beta,Z,Y)\\,=\\,\\{(\\beta_{j},z_{j},y_{j})\\}_{j=1}^{m}$ by splitting the sample $\\pmb{t}_{j}$ into the feature $z_{j}$ and the pseudo label $y_{j}$ , emphasizing label information. Given existing model specifications $\\left\\{s_{i}^{\\mathrm{dev}}\\right.:=$ $\\{(\\beta_{i j},z_{i j},y_{i j})\\}_{j=1}^{m_{i}}\\}_{i=1}^{N}$ , the learnware dock system can learn a unified subspace $\\chi_{\\mathrm{sub}}$ with encoding functions $\\{h_{k}\\,:\\,\\mathcal{X}_{k}\\,\\mapsto\\,\\mathcal{X}_{\\mathrm{sub}}\\}_{k=1}^{Q}$ and decoding functions $\\{g_{k}\\,:\\,\\mathcal{X}_{\\mathrm{sub}}\\,\\mapsto\\,\\mathcal{X}_{k}\\}_{k=1}^{Q}$ by optimizing $L=\\alpha_{1}L_{\\mathrm{reconstruction}}+\\alpha_{2}L_{\\mathrm{similar}}+\\alpha_{3}L_{\\mathrm{supervised}}$ over mapping functions $\\{h_{k},g_{k}\\}_{k=1}^{Q}$ . The objective function has three components: the reconstruction loss, which trains mapping functions $(h_{k},g_{k})$ to map and reconstruct data in $\\scriptstyle{\\mathcal{X}}_{k}$ ; the similarity loss, which makes embeddings of different slices of $z_{j}$ similar; and the supervised loss, which uses label information to improve subspace learning by making class embeddings more separable or aligning samples within the same class. After subspace learning, the mapping functions $\\{h_{k},g_{k}\\}_{k=1}^{Q}$ can project data from any combination of feature space blocks to the subspace. When reusing heterogeneous models, these functions can also flil in missing parts needed for model predictions. Such a framework can be implemented by existing subspace learning methods, such as self-supervised learning [Ucar et al., 2021, Bahri et al., 2022], matrix factorization $[\\mathrm{Xu}$ and Gong, 2004, Wang et al., 2016]. When the system receives learnwares from unseen feature spaces after subspace generation, the system can update the subspace during idle time. ", "page_idx": 3}, {"type": "text", "text": "Matching with only marginal distribution $P_{X}$ is not enough. We first review previous methodologies for matching a user\u2019s task with a model in the homogeneous case, where all models and user tasks share the same feature space [Wu et al., 2023, Zhang et al., 2021]. These methods recommend the ", "page_idx": 3}, {"type": "image", "img_path": "3YIyB82rjX/tmp/ff59531577d49ab6ba06530a15901438b579d2f8876863978927718495c0c1bb.jpg", "img_caption": ["4.2 Improve matching model and user task with label information ", "Figure 3: Label information is beneficial for matching. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "model with the most similar marginal distribution $P_{X}$ . To avoid exposing raw data, they use RKME to sketch the marginal distributions of the model task and user task, serving as the model specifications and user requirements. To illustrate the deficiency, we refer to Figure 3, which presents five tasks with uniform distributions. Among these tasks, four have circular support sets and one has a square support set. The two problems are: 1) Models with the same $P_{X}$ but different $P_{X\\mid Y}$ are indistinguishable. In Figure 3, Models 1, 2 and 4 are all recommended, but model 2 is unsuitable for the user task. 2) Models with different $P_{X}$ are rarely considered, despite their potential usefulness. Model 3 in Figure 3, though suitable, are excluded because its $P_{X}$ is square instead of circle. ", "page_idx": 3}, {"type": "text", "text": "Enhance matching by incorporating the conditional distribution $P_{X\\mid Y}$ . Matching solely on the marginal distribution is insufficient for model identification. To better recommend models to user tasks, we propose additionally considering the conditional distribution $P_{X\\mid Y}$ , which helps exclude the model with dissimilar conditional distributions (Model 2) and include the model with similar ones (Model 3). While the user\u2019s task can estimate the conditional distribution from labeled data, a key question arises for the model task: should we use true labels or model-generated pseudo labels? Using True labels results in comparing the user\u2019s task distribution $P(X,Y)$ with the model\u2019s original task distribution, while pseudo labels results in comparing the model-generated joint distribution $P(X,{\\hat{Y}})$ with new tasks, allowing the model to be reused beyond its original purpose. As shown in Figure 3, Model 4 would be recommended using pseudo labels but not using true labels. In conclusion, considering both marginal and conditional distributions improves model identification, with model-generated pseudo labels being helpful for encoding model capabilities. ", "page_idx": 3}, {"type": "text", "text": "4.3 Summary ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To accommodate and identify models developed from heterogeneous feature spaces, it is advantageous to utilize pseudo-label information generated by the models. To better incorporate this information, we propose to integrate both marginal and conditional distributions into the model specification and user requirements, represented as $\\{(\\beta_{j},z_{j},y_{j})\\}_{j=1}^{m}$ . By comparing these distributions, we improve learnware identification. The inclusion of label information enhances subspace learning, and the framework can be applied across various subspace learning methods. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "5 Detailed solution ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we provide the detailed procedure for the heterogeneous learnware problem based on the aforementioned methodology, which consists of model specification generation, heterogeneous learnwares accommodation by the system, and system exploitation for solving new user tasks. ", "page_idx": 4}, {"type": "text", "text": "5.1 The developer generates the model specification ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The model specification sketches task distribution and model capabilities with a reduced set. Instead of sketching the joint distribution of task features and outputs (Eq. (1)), we propose to generate feature and label part separately to balance label and feature information. This includes a unified mechanism for classification and regression, and a specialized mechanism for classification. ", "page_idx": 4}, {"type": "text", "text": "Unified mechanism for classification and regression tasks. Given a dataset $D=\\{(\\pmb{x}_{i},y_{i})\\}_{i=1}^{n}$ and a model $f$ trained on it, we first generate a reduced set $\\{(\\beta_{j},z_{j})\\}_{j=1}^{m}$ solely on $\\{{\\pmb x}_{i}\\}_{i=1}^{n}$ based on RKME via Eq. (1) with $\\mathbf{q}_{i}=\\mathbf{x}_{i}$ , which sketches the marginal distribution of the task feature. To encode the model\u2019s ability, pseudo labels can be assigned to the reduced set using $y_{j}=f(z_{j})$ , resulting in the labeled reduced set $R_{L}=(\\beta,Z,Y)=\\{(\\beta_{j},z_{j},y_{j})\\}_{j=1}^{m}$ , serving as the developer-level model specification $s^{\\mathrm{dev}}$ . ", "page_idx": 4}, {"type": "text", "text": "Specialized mechanism for classification tasks. For classification problems, given the finite label space $\\boldsymbol{\\wp}$ , we propose the mechanism to directly sketch the model\u2019s capacity, characterized by the conditional distribution $P(X|Y)$ of the model $f$ . We first obtain the model predictions $\\{\\hat{y}_{i}\\}_{i=1}^{n}$ on its \"skilled\" marginal distribution, i.e., its training data $\\{{\\pmb x}_{i}\\}_{i=1}^{n}$ . Then, the pseudo-labeled dataset $\\{(\\pmb{x}_{i},\\hat{y}_{i})\\}_{i=1}^{n}$ , which encodes the model\u2019s conditional distribution, can be sketched by a labeled reduced set $\\dot{R}_{L}=(\\beta,Z,Y)=\\left\\{(\\beta_{j},z_{j},y_{j})\\right\\}_{j=1}^{m}$ using the following objective: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{i=1}^{n}{\\frac{1}{n}}k\\left({\\pmb x}_{i},\\cdot\\right)-\\sum_{j=1}^{m}\\beta_{j}k\\left({\\pmb z}_{j},\\cdot\\right)\\right\\|_{{\\mathcal{H}}_{k}}^{2}+\\theta\\sum_{c=1}^{C}\\left\\|\\sum_{i\\in{\\mathcal{I}}_{c}}{\\frac{1}{n}}k\\left({\\pmb x}_{i},\\cdot\\right)-\\sum_{j\\in{\\mathcal{I}}_{c}^{\\prime}}\\beta_{j}k\\left({\\pmb z}_{j},\\cdot\\right)\\right\\|_{{\\mathcal{H}}_{k}}^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{Z}_{c}$ and $\\mathcal{Z}_{c}^{'}$ represent the indices of samples $\\pmb{x}_{i}$ and $z_{j}$ belonging to class $c$ , respectively. $\\theta$ idsi stthaen cpe.a rTahmee ltaebr eulesde dr etdou cbeadla snect $R_{L}$ es hmoaurlgdi anpalp rdoixsitrmibatuet iboont hd itshtea nmcaer gainnda l cdoisntdriitbiuotnioaln $\\textstyle\\sum_{i=1}^{n}{\\frac{1}{n}}\\delta_{{\\pmb x}_{i}}$ with $\\textstyle\\sum_{j=1}^{m}\\beta_{j}\\delta_{z_{j}}$ and the conditional distribution given the $c$ class $\\textstyle\\sum_{i\\in{\\mathbb{Z}}_{c}}{\\frac{1}{n}}\\delta_{{\\pmb x}_{i}}$ with $\\sum_{j\\in\\mathbb{Z}_{c}^{\\prime}}\\beta_{j}\\delta_{z_{j}}$ simultaneously. Here, $\\delta(\\cdot)$ is the Dirichlet function, which describes the probability mass at a single point. The objective Eq. (2) can be optimized by alternating optimization, the details are showed in E.1. The optimized reduced set $R_{L}$ is served as the developer-level model specification $s^{\\mathrm{dev}}$ . ", "page_idx": 4}, {"type": "text", "text": "The first unified mechanism sketches the marginal distribution and then encodes the model information, while the second specialized mechanism directly sketches the model\u2019s conditional distribution. ", "page_idx": 4}, {"type": "text", "text": "5.2 The system accommodates heterogeneous learnwares ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "After the developer-level specification $s^{\\mathrm{dev}}$ is generated, the developer submits the model $f$ with specification $s^{\\mathrm{de^{-}}}$ to the learnware dock system. The system exploits the relationship of different feature spaces and manages heterogeneous models by assigning system-level specification $s^{\\mathrm{sys}}$ . ", "page_idx": 4}, {"type": "text", "text": "Subspace learning After the learnware dock system receives heterogeneous models with their developer-level specifications, it generates a unified subspace $\\chi_{\\mathrm{sub}}$ to connect different feature blocks $\\{\\mathcal{X}_{i}\\}_{i=1}^{Q}$ based on all developer-level specifications $\\{s_{i}^{\\mathrm{dev}}:=\\{(\\beta_{i j},z_{i j},y_{i j})\\}_{j=1}^{m_{i}}\\}_{i=1}^{N}$ . During subspace learning, the learnware dock system learns $2Q$ mapping functions: $\\{h_{k}:\\mathcal{X}_{k}\\mapsto\\mathcal{X}_{\\mathrm{sub}}\\}_{k=1}^{Q}$ and $\\{g_{k}:\\mathcal{X}_{\\mathrm{sub}}\\mapsto\\mathcal{X}_{k}\\}_{k=1}^{Q}$ . For a particular sample $z_{i j}$ , it can be split into several blocks $\\{z_{i j}^{(k)}\\}_{k\\in C_{i}}$ according to the feature split $X_{\\mathrm{all}}\\;=\\;\\chi_{1}\\;\\times\\;\\cdot\\;\\cdot\\;\\times\\;\\chi_{Q}$ . The encoding function $h_{k}$ produces the embedding of sample slice zi(jk) as $\\pmb{v}_{i j}^{(k)}$ , and the decoding function $g_{k}$ reconstructs it to $\\scriptstyle{\\mathcal{X}}_{k}$ as $\\tilde{z}_{i j}^{(k)}$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "The loss for subspace learning is implemented as follows: 1) The reconstruction loss, $\\mathcal{L}_{\\mathrm{reconstruct}}=$ $\\begin{array}{r}{\\sum_{i=1}^{N}\\sum_{j=1}^{m_{i}}\\beta_{i j}\\lVert\\tilde{{\\boldsymbol z}}_{i j}^{(k)}-{\\boldsymbol z}_{i j}^{(k)}\\rVert_{\\mathrm{F}}^{2}}\\end{array}$ , penalizes the difference between the original sample $z_{i j}^{(k)}$ and the reconstructed sample $\\tilde{z}_{i j}^{(k)}$ , weighted by the sample importance $\\beta_{i j}$ . 2) The supervised loss involves building a simple classifier on $\\{\\{(\\pmb{v}_{i j},y_{i j})\\}_{j=1}^{m_{i}}\\}_{i=1}^{N}$ , where $\\pmb{v}_{i j}=\\mathrm{mean}(\\{\\pmb{v}_{i j}^{(k)}\\}_{k\\in C_{i}})$ , and calculating the prediction loss to make the embeddings of different classes more separable. 3) The contrastive loss aims to make the embeddings {vi(jk )}k\u2208Ci of a single sample zij similar, while ensuring that embeddings of different samples are dissimilar. The contrastive loss $\\mathcal{L}_{\\mathrm{contrastive}}\\,=$ $\\textstyle\\sum_{i=1}^{N}l_{i}$ includes $N$ terms, each term being a weighted loss extended from the Self-VPCL loss [Wang and Sun, 2022], calculated on the embeddings $\\{\\{\\pmb{v}_{i j}^{(k)}\\}_{k\\in C_{i}}\\}_{i=1}^{m_{i}}$ of one specification $\\pmb{s}_{i}^{\\mathrm{dev}}$ : ${l}_{i}\\,=$ $\\begin{array}{r}{\\sum_{j=1}^{m_{i}}\\beta_{i j}\\sum_{k\\in C_{i}}\\sum_{{k^{\\prime}}\\in C_{i},{k^{\\prime}}\\neq k}\\log\\frac{\\exp\\psi\\Big({\\pmb v}_{i j}^{(k)},{\\pmb v}_{i j}^{(k^{\\prime})}\\Big)}{\\sum_{t=1}^{m_{i}}\\sum_{{k^{\\dagger}}\\in C_{i}}\\exp\\psi\\Big({\\pmb v}_{i j}^{(k)},{\\pmb v}_{i t}^{(k^{\\dagger})}\\Big)}}\\end{array}$ , where $\\psi$ is the cosine similarity function. In the logarithm term, the numerator represents the similarity of the positive pair, while the denominator is the sum of all pairs. The loss $\\mathcal{L}$ is optimized with gradient descent. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Heterogeneous learnware accommodation. After subspace learning, the learnware dock system builds a unified subspace and corresponding mapping functions $\\{g_{i},h_{i}\\}_{i=1}^{Q}$ . The dock system then assigns a system-level specification $\\pmb{s}_{i}=\\{(\\beta_{i j},\\pmb{v}_{i j},y_{i j})\\}_{j=1}^{m_{i}}$ for each model based on its developerlevel specification $s_{i}^{\\mathrm{dev}}:=\\{(\\beta_{i j},z_{i j},y_{i j})\\}_{j=1}^{m_{i}}$ . During the system-level specification generation, the sample $z_{i j}$ is projected to the unified subspace as $\\pmb{v}_{i j}$ , while the coefficient $\\beta_{i j}$ and the label $y_{i j}$ remain unchanged. The projection $\\pmb{v}_{i j}$ is calculated as follows: $\\begin{array}{r}{\\pmb{v}_{i j}=\\frac{1}{|C_{i}|}\\sum_{k\\in C_{i}}h_{k}(\\pmb{z}_{i j}^{(k)})}\\end{array}$ . The whole procedure of learnware dock system construction is described in Algorithm 3. ", "page_idx": 5}, {"type": "text", "text": "5.3 The user exploits the learnware dock system ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "After the learnware dock system accommodates heterogeneous learnwares, users can submit their task requirements to receive recommended models and the toolkit used for feature transformation. They can then directly reuse the model or combine it with a self-training model. ", "page_idx": 5}, {"type": "text", "text": "User requirement generation. As described in Section 4.2, comparing both the marginal distribution $P_{X}$ and conditional distribution $P_{X\\mid Y}$ based on the model specification and user requirement helps better learnware identification. Similar to $\\mathtt{R K M E_{L}}$ specification encoding both distributions, the $\\mathtt{R K M E_{L}}$ requirement of the user is generated similarly to reflect both. In details, given the unlabeled data $D^{u}\\,\\bar{=}\\,\\{{\\pmb x}_{i}\\}_{i=1}^{n_{u}}$ and some labeled data $D^{l}=\\{(\\tilde{{\\pmb{x}}}_{i},y_{i})\\}_{i=1}^{n_{l}}$ (the unlabeled data cover labeled data features, i.e., $\\{\\widetilde{\\pmb{x}}_{i}\\}_{i}\\subseteq\\{\\pmb{x}_{i}\\}_{i})$ , the user can generate $\\mathtt{R K M E_{L}}$ requirement presented by labeled reduced set $R_{L}=\\left\\{(\\beta_{j},z_{j},y_{j})\\right\\}_{j=1}^{m}$ to sketch the task distribution. ", "page_idx": 5}, {"type": "text", "text": "For the classification case, the reduced set $R_{L}$ can be generated by minimizing the following distance: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{i=1}^{n_{u}}\\frac{1}{n_{u}}k\\left(\\pmb{x}_{i},\\cdot\\right)-\\sum_{j=1}^{m}\\beta_{j}k\\left(\\pmb{z}_{j},\\cdot\\right)\\right\\|_{\\mathcal{H}_{k}}^{2}+\\theta\\sum_{c=1}^{C}\\left\\|\\sum_{i\\in\\mathbb{Z}_{c}}\\frac{1}{n_{l}}k\\left(\\pmb{\\tilde{x}}_{i},\\cdot\\right)-\\sum_{j\\in\\mathcal{X}_{c}^{\\prime}}\\beta_{j}k\\left(\\pmb{z}_{j},\\cdot\\right)\\right\\|_{\\mathcal{H}_{k}}^{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This equation is similar to specification generation in Eq. (2), where the specification sketches the tphsee uddisotm-alnacbee lbeedt dwaeteans etth, ew mhialreg tihnea lr edqisutirriebumteiontn ss koeft tchhee su snelambie-lseudp edravtiasseedt $\\textstyle\\sum_{i=1}^{n_{u}}{\\frac{1}{n_{u}}}\\delta_{{\\pmb{x}}_{i}}$ t atnedr mth ce arlecduluacteeds set $\\textstyle\\sum_{j=1}^{m}\\beta_{j}\\delta_{z_{j}}$ . The second term calculates the distance between the conditional distributions of the labeled dataset i\u2208Icn1l \u03b4 and the reduced set $\\sum_{j\\in\\mathbb{Z}_{c}^{\\prime}}\\beta_{j}\\delta_{z_{j}}$ , where $\\mathcal{Z}_{c}$ and $\\mathcal{I}_{c}^{'}$ denote the sample indices of the labeled dataset and the reduced set with label $c$ , respectively. The optimized reduced set $R_{L}$ becomes the user-level requirement $\\boldsymbol{s}_{0}^{\\mathrm{user}}$ . The optimization is described in E.2 ", "page_idx": 5}, {"type": "text", "text": "For regression, the requirement is generated by sketching the marginal distribution and applying a self-trained model for pseudo-labeling, similar to unified specification generation in Section 5.1. ", "page_idx": 5}, {"type": "text", "text": "Learnware identification. After the user submits the user-level task requirement $\\begin{array}{r l}{s_{0}^{\\mathrm{user}}}&{{}=}\\end{array}$ $\\{(\\beta_{0k},z_{0k},y_{0k})\\}_{k=1}^{m_{0}}$ to the dock symstem, the dock system transforms it into the system-level task requirement $s_{0}=\\stackrel{\\cdot}{\\{}(\\beta_{0k},{\\pmb v}_{0k},y_{0k})\\}_{k=1}^{m_{0}}$ by projecting $z_{0k}$ into the subspace as . The dock system then calculates the distance between the system-level specification $\\pmb{s}_{i}=\\{(\\beta_{i j},\\pmb{v}_{i j},y_{i j})\\}_{j=1}^{m_{i}}$ and the system-level task requirement $\\ s_{\\mathrm{0}}$ as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{k}^{m_{0}}\\beta_{0k}k(v_{0k},\\cdot)-\\sum_{j}^{m_{i}}\\beta_{i j}k(v_{i j},\\cdot)\\right\\|+\\alpha\\sum_{C}\\left\\|\\sum_{k\\in\\mathbb{Z}_{0,C}}\\beta_{0k}k(v_{0k},\\cdot)-\\sum_{j\\in\\mathbb{Z}_{i,C}}\\beta_{i j}k(v_{i j},\\cdot)\\right\\|,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "which measures both the conditional distribution distances and marginal distribution distances. Where $\\mathcal{T}_{i,C}$ represents the indices of $\\pmb{v}_{i j}$ with the class c. The learnware dock system then recommends the learnware with a minimal distance. ", "page_idx": 6}, {"type": "text", "text": "Learnware reuse. Once the user receives the recommended model $f_{i}$ and the dock system toolkit $\\{h_{k},g_{k}\\}_{k=1}^{Q}$ , they can apply the model to their task. The toolkit helps bridge the gap between different feature spaces. For example, if the user\u2019s task is on $\\mathcal{X}_{1}\\times\\mathcal{X}_{2}$ and the model is on $\\mathcal{X}_{2}\\times\\mathcal{X}_{3}\\times\\mathcal{X}_{4}$ , the user can project their data using $h_{1}$ and $h_{2}$ , then decode it to $\\chi_{3}$ and $\\chi_{4}$ with $g_{3}$ and $g_{4}$ . The user can use the recommended model\u2019s predictions directly or ensemble them with a self-trained model. ", "page_idx": 6}, {"type": "text", "text": "5.4 Overall procedure ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In the submission stage, the dock system receives models with developer-level specifications that sketch model capabilities and assigns system-level specifications by a learned unified subspace. In the deployment stage, users submit task requirements detailing marginal and conditional distributions to receive recommended learnware. This learnware can be integrated with their self-trained models to significantly enhance performance. The overall process is summarized in Appendix D.1. ", "page_idx": 6}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "6.1 Experiment setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. We tested our methods on 30 datasets from the Tabzilla benchmark [McElfresh et al., 2023], excluding tiny datasets. These include 23 classification tasks and 7 regression tasks. For classification tasks, the sample sizes range from 1,000 to 58,310, feature space dimensions from 7 to 7,200, and the number of classes from 2 to 10. For regression tasks, the sample sizes range from 418 to 108,000, and feature space dimensions from 8 to 128. ", "page_idx": 6}, {"type": "text", "text": "Compared methods. As the heterogeneous learnware problem, where the user has some labeled data, is a new problem, we first compare our approach with two basic methods that train models from scratch: lightgbm [Ke et al., 2017], a widely used tree-based method for tabular datasets, and TabPFN [Hollmann et al., 2023], a recently proposed prior-data ftited network capable of training and inference on small classification datasets in less than one second. When seeking assistance from the model repository, one simple but inefficient approach is to fetch all models, conduct heterogeneous transfer learning, and select the best one. ${\\tt A l i g n}_{\\tt u n l a b e1}$ [Tan et al., 2024a] aligns the feature space and uses the aligned model directly, while ${\\tt A l i g n}_{\\tt l a b e1}$ [Tan et al., 2024a] goes a step further by finetuning through training a new model with augmented features that include aligned model predictions. Another method for reusing knowledge from heterogeneous tasks involves pre-training a unified tabular network on different tables and fine-tuning on the downstream user tasks: Transtab [Wang and Sun, 2022] and Xtab [Zhu et al., 2023]. However, these methods require access to raw task data, whereas our method protects user privacy. Next, we compare with Hetero [Tan et al., 2023], an initial attempt to address the heterogeneous learnware problem without using label information. Finally, we substitute the specification in our method with the RKME specification from [Zhou and Tan, 2024] as $\\mathsf{O u r}_{\\mathsf{b a s i c}}$ and conduct a comparison with the proposed method. ", "page_idx": 6}, {"type": "text", "text": "Experiment configuration. The feature space is randomly divided into four equal blocks, creating feature spaces from three-block combinations for developer tasks and two-block combinations for user tasks. For user tasks, 100 labeled data points are sampled from the training set. All experiments are repeated five times. For more details, please see Appendix F.1. ", "page_idx": 6}, {"type": "table", "img_path": "3YIyB82rjX/tmp/681f74d145c6f33af36bad0967ca85bffc589be82d9383c7cb11958127a8c675.jpg", "table_caption": ["Table 1: Accuracy $(\\%)$ (mean $\\pm$ std) on user data true labels. The best performance is in bold. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "3YIyB82rjX/tmp/a6b6293bec82300a18de1aa0dd541700741c3187478037fd4b00f5bf37303a30.jpg", "table_caption": ["Table 2: RMSE (mean $\\pm$ std) on user data true labels. The best performance is emphasized in bold. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "6.2 Performance on user tasks ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Tables 1 and 2 compare the performance of our proposed methods with other contenders on classification and regression tasks. $\\mathtt{O u r_{u n i f y}}$ refers to the performance of the overall procedure with the unified specification, while $\\tt{O u r}_{c l s}$ refers to the specialized specification designed for classification tasks. Our approach, $\\mathtt{O u r}_{\\mathtt{u n i f y}}$ , outperforms the competitors in most cases. While Lightgbm and TabPFN use self-training, their performance is limited by the small amount of labeled data. It is showed that TabPFN performs better than Lightgbm under these conditions. This highlights the importance of leveraging well-trained models, even with heterogeneous feature spaces, to improve performance. ", "page_idx": 7}, {"type": "text", "text": "Examining ${\\tt A l i g n}_{\\tt u n l a b e1}$ shows that heterogeneous transfer learning with only aligning feature spaces without labels is less effective than self-training. However, further fine-tuning enables Alignlabel to outperform self-training methods. Nevertheless, without leveraging knowledge across different tasks, ${\\tt A l i g n}_{\\tt L a b e1}$ still performs worse than our approach. Transtab and Xtab attempt to create a unified backbone across different tables to leverage cross-task knowledge, but they fail to reuse the high-performing model on each developer task, leading to worse performance than ours. These methods also require training on raw developer data, whereas our method only accesses model specifications without exposing raw data. ", "page_idx": 7}, {"type": "text", "text": "Hetero performs worse than our methods due to its lack of modeling the conditional distribution of submitted models and its reliance on unsupervised subspace learning. Compared to $\\mathsf{O u r}_{\\mathsf{b a s i c}}$ , our proposed specification outperforms the RKME specification, as it alleviates the issue of label information being overshadowed by feature information during specification generation and comparison. Notably, for classification tasks, our specialized model $\\tt{O u r}_{\\tt{c l s}}$ outperforms $\\mathtt{O u r_{u n i f y}}$ due to its ability to encode conditional distribution of the model more effectively. ", "page_idx": 7}, {"type": "image", "img_path": "3YIyB82rjX/tmp/5cd95097f687be29d799713b71c5ed04a41a4c09fc6482f69927812f742638e9.jpg", "img_caption": ["Figure 4: User performance curve for classification tasks. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "3YIyB82rjX/tmp/eb5af428652cf1793696c235915eef77a45dd0663fb973cf959214a035b45575.jpg", "img_caption": ["Figure 5: User performance curve for regression tasks. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "6.3 Evaluation on users with different size of labeled data ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In the previous section, we showed that using a single learnware with heterogeneous feature spaces outperforms training models from scratch when labeled data is limited. Now, we analyze how performance changes as users train models and ensemble their predictions with learnware across different amounts of labeled data. Figures 4 and 5 display these trends for classification and regression tasks. These figures indicate that ensemble methods consistently outperform self-training with 100 labeled data points. With 500 labeled data points, the ensemble method still performs better in nearly $80\\%$ of cases. Additionally, learnware continues to enhance performance even with 5000 labeled samples, improving $21\\%$ of classification cases and $50\\%$ of regression cases. For certain datasets like $\\mathrm{kin}8\\mathrm{nm}$ in regression tasks, even when users use their entire training dataset, the recommended heterogeneous learnware can still significantly boost performance. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This paper evolves specifications to a unified subspace with explicit exploitation of model outputs under the heterogeneous learnware scenario. The specification is extended by additionally encoding conditional distribution to better encode the model capability, which can be further evolved by more effective subspace learning enriched by label information. The extended specification also improves learnware identification by additionally matching conditional distributions. We present the complete workflow of the learnware dock system accommodating heterogeneous learnwares and validate the effectiveness of the proposed methods through extensive experiments. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was supported by NSFC (62250069) and the Program for Outstanding PhD Candidates of Nanjing University (202401B07). The authors would like to thank Jian-Dong Liu and Jia-Wei Shan for helpful discussions. We are also grateful for the anonymous reviewers for their valuable comments. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Zhi-Hua Zhou. Learnware: on the future of machine learning. Frontiers of Computer Science, 10: 589\u2013590, 2016. ", "page_idx": 9}, {"type": "text", "text": "Zhi-Hua Zhou and Zhi-Hao Tan. Learnware: small models do big. Science China Information Sciences, 67(1):112102, 2024.   \nZhi-Hao Tan, Jian-Dong Liu, Xiao-Dong Bi, Peng Tan, Qin-Cheng Zheng, Hai-Tian Liu, Yi Xie, Xiao-Chuan Zou, Yang Yu, and Zhi-Hua Zhou. Beimingwu: A learnware dock system. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 5773\u20135782, 2024a.   \nJian-Dong Liu, Zhi-Hao Tan, and Zhi-Hua Zhou. Towards making learnware specification and market evolvable. In Proceedings of the 38th AAAI Conference on Artificial Intelligence, pages 13909\u201313917, 2024.   \nYi Xie, Zhi-Hao Tan, Yuan Jiang, and Zhi-Hua Zhou. Identifying helpful learnwares without examining the whole market. In Proceedings of the 26th European Conference on Artificial Intelligence, pages 2752\u20132759, 2023.   \nYu-Jie Zhang, Yu-Hu Yan, Peng Zhao, and Zhi-Hua Zhou. Towards enabling learnware to handle unseen jobs. In Proceedings of the 35th AAAI Conference on Artificial Intelligence, pages 10964\u2013 10972, 2021.   \nPatricia Biedermann, Rose Ong, Alexander Davydov, Alexandra Orlova, Philip Solovyev, Hong Sun, Graham Wetherill, Monika Brand, and Eva-Maria Didden. Standardizing registry data to the omop common data model: experience from three pulmonary hypertension databases. BMC Medical Research Methodology, 21:1\u201316, 2021.   \nZifeng Wang and Jimeng Sun. Transtab: Learning transferable tabular transformers across tables. In Advances in Neural Information Processing Systems 35, pages 2902\u20132915, 2022.   \nBingzhao Zhu, Xingjian Shi, Nick Erickson, Mu Li, George Karypis, and Mahsa Shoaran. Xtab: Cross-table pretraining for tabular transformers. In Proceedings of 40th International Conference on Machine Learning, pages 43181\u201343204, 2023.   \nChang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv:1304.5634, 2013.   \nWeitian Huang, Sirui Yang, and Hongmin Cai. Generalized information-theoretic multi-view clustering. In Advances in Neural Information Processing Systems 36, pages 58752\u201358764, 2023.   \nPeng Tan, Zhi-Hao Tan, Yuan Jiang, and Zhi-Hua Zhou. Handling learnwares developed from heterogeneous feature spaces without auxiliary data. In Proceedings of the 32nd International Joint Conference on Artificial Intelligence, pages 4235\u20134243, 2023.   \nBernhard Sch\u00f6lkopf and Alexander Johannes Smola. Learning with kernels: Support vector machines, regularization, optimization, and beyond. MIT press, 2002.   \nXi-Zhu Wu, Wenkai Xu, Song Liu, and Zhi-Hua Zhou. Model reuse with reduced kernel mean embedding specification. IEEE Transactions on Knowledge and Data Engineering, 35(1):699\u2013710, 2023.   \nTalip Ucar, Ehsan Hajiramezanali, and Lindsay Edwards. Subtab: Subsetting features of tabular data for self-supervised representation learning. In Advances in Neural Information Processing Systems 34, pages 18853\u201318865, 2021.   \nDara Bahri, Heinrich Jiang, Yi Tay, and Donald Metzler. Scarf: Self-supervised contrastive learning using random feature corruption. In The 10th International Conference on Learning Representations, 2022.   \nWei Xu and Yihong Gong. Document clustering by concept factorization. In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 202\u2013209, 2004.   \nHao Wang, Yan Yang, and Tianrui Li. Multi-view clustering via concept factorization with local manifold regularization. In Proceedings of the 16th International Conference on Data Mining, pages 1245\u20131250, 2016.   \nDuncan McElfresh, Sujay Khandagale, Jonathan Valverde, Vishak Prasad C, Ganesh Ramakrishnan, Micah Goldblum, and Colin White. When do neural nets outperform boosted trees on tabular data? In Advances in Neural Information Processing Systems 36, pages 76336\u201376369, 2023.   \nGuolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and TieYan Liu. Lightgbm: A highly efficient gradient boosting decision tree. In Advances in Neural Information Processing Systems 30, pages 3146\u20133154, 2017.   \nNoah Hollmann, Samuel M\u00fcller, Katharina Eggensperger, and Frank Hutter. Tabpfn: A transformer that solves small tabular classification problems in a second. In The 11th International Conference on Learning Representations, 2023.   \nYazheng Yang, Yuqi Wang, Guang Liu, Ledell Wu, and Qi Liu. Unitabe: Pretraining a unified tabular encoder for heterogeneous tabular data. arXiv:2307.09249, 2023.   \nSiyuan Guo, Jonas Bernhard Wildberger, and Bernhard Sch\u00f6lkopf. Out-of-variable generalisation for discriminative models. In The 12th International Conference on Learning Representations, 2024.   \nPeng Tan, Zhi-Hao Tan, Yuan Jiang, and Zhi-Hua Zhou. Towards enabling learnware to handle heterogeneous feature spaces. Machine Learning, 113(4):1839\u20131860, 2024b.   \nLan-Zhe Guo, Zhi Zhou, Yu-Feng Li, and Zhi-Hua Zhou. Identifying useful learnwares for heterogeneous label spaces. In Proceedings of the 40th International Conference on Machine Learning, pages 12122\u201312131, 2023.   \nDavid Alvarez-Melis and Nicol\u00f2 Fusi. Geometric dataset distances via optimal transport. In Advances in Neural Information Processing Systems 33, 2020.   \nFacundo M\u00e9moli. Gromov-wasserstein distances and the metric approach to object matching. Foundations of Computational Mathematics, 11(4):417\u2013487, 2011.   \nLixin Duan, Dong Xu, and Ivor W. Tsang. Learning with augmented features for heterogeneous domain adaptation. In Proceedings of the 29th International Conference on Machine Learning, 2012.   \nChang Wang and Sridhar Mahadevan. Heterogeneous domain adaptation using manifold alignment. In Proceedings of the 22th International Joint Conference on Artificial Intelligence, 2011.   \nOscar Day and Taghi M Khoshgoftaar. A survey on heterogeneous transfer learning. Journal of Big Data, 4:1\u201342, 2017.   \nHan-Jia Ye, De-Chuan Zhan, Yuan Jiang, and Zhi-Hua Zhou. Rectify heterogeneous models with semantic mapping. In Proceedings of the 37th International Conference on Machine Learning, pages 5630\u20135639, 2018.   \nHan-Jia Ye, De-Chuan Zhan, Yuan Jiang, and Zhi-Hua Zhou. Heterogeneous few-shot model rectification with semantic mapping. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(11):3878\u20133891, 2020.   \nYao-Xiang Ding and Zhi-Hua Zhou. Boosting-based reliable model reuse. In Proceedings of the 12th Asian Conference on Machine Learning, pages 145\u2013160, 2020.   \nYao-Xiang Ding, Xi-Zhu Wu, Kun Zhou, and Zhi-Hua Zhou. Pre-trained model reusability evaluation for small-data transfer learning. In Advances in Neural Information Processing Systems 35, pages 37389\u201337400, 2022.   \nYi-Kai Zhang, Ting-Ji Huang, Yao-Xiang Ding, De-Chuan Zhan, and Han-Jia Ye. Model spider: Learning to rank pre-trained models efficiently. In Advances in Neural Information Processing Systems 36, pages 13692\u201313719, 2023.   \nChao Yi, De-Chuan Zhan, and Han-Jia Ye. Bridge the modality and capacity gaps in vision-language model selection. arXiv:2403.13797, 2024.   \nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016.   \nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, The 3rd International Conference on Learning Representations, 2015. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Table of contents ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "\u2022 Section A presents the key notations used throughout this work.   \n\u2022 Section B offers an in-depth discussion on the proposed methods.   \n\u2022 Section C reviews the related literature and techniques.   \n\u2022 Section D describes the detailed algorithm of the overall procedure.   \n\u2022 Section E explains the optimization during specification generation and user requirement generation.   \n\u2022 Section F provides additional information on the experiments. ", "page_idx": 12}, {"type": "text", "text": "A Notations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The major notations of this paper are summarized in Table 3. ", "page_idx": 12}, {"type": "table", "img_path": "3YIyB82rjX/tmp/a63c97357341481a8d256eeb0350cedf853b1ca2775be4a30e07e748837d1647.jpg", "table_caption": ["Table 3: Major notations of this work. "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "B More discussion ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "B.1 Superiority of the learnware paradigm for handling heterogeneous models ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Difficulties of managing models with heterogeneous feature spaces. To manage models decvoelrroepsepdo nfrdoinmg  hfeetateurroeg esnpaecoeuss $\\{\\mathcal{X}_{i}^{\\mathrm{dev}}\\}_{i=1}^{N}$ .a cWesh,i liet  ims ueltsis-evinetiwa ll etoar enixnpgl $[\\mathrm{Xu}$ heet  rael.l,a t2i0o1n3s]h icpa na bmeo bneg ntehfeicial if co-occurrence data across the entire feature space ${\\mathcal X}_{\\mathrm{all}}$ is available, obtaining such data is nearly impossible in real-world scenarios. Alternatively, if the raw data of the model task is accessible, training a unified tabular network on heterogeneous tables [Wang and Sun, 2022, Zhu et al., 2023, Yang et al., 2023] offers another approach to reusing knowledge from heterogeneous tasks. However, a unified model often struggles to perform well across all source tasks due to complex and sometimes confilcting internal patterns. Additionally, in sensitive areas like medicine, data sharing is restricted, and privacy concerns prevent access to raw data. In our problem, raw data is inaccessible to protect the model provider\u2019s privacy, and we do not use hard-to-collect auxiliary data. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Exploit feature space relationship with model specifications. Under the learnware paradigm, each model is submitted with a specification that describes its abilities. This specification can be naturally used to explore the relationships between feature spaces. [Tan et al., 2023] generates a unified subspace $\\chi_{\\mathrm{sub}}$ and linear projection functions linking it to all feature blocks $\\bar{\\{X_{i}\\}}_{i=1}^{Q}$ by leveraging model RKME specifications generated solely on features. However, the specification lack of label information often leads to unsatisfactory performance of subspace learning, like entangled classes of embeddings, it also performs poorly when feature blocks are weakly dependent. This paper better incorporates label information into the specification to improve subspace learning for better heterogeneous learnware accommodation. ", "page_idx": 13}, {"type": "text", "text": "B.2 Exploit label information to handle models with heterogeneous feature spaces ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To handle learnwares with heterogeneous feature spaces, it\u2019s crucial to exploit the relationship between different feature spaces. When the overall feature space is divided into disjoint blocks and each task data is an arbitrary Cartesian product of several blocks, this exploitation can be divided into two parts: learning relationships between co-occurring feature spaces using data of specific specification, and learning relationships between non-co-occurring feature spaces across all specifications. The first part involves subspace learning to identify a unified subspace, while the second ensures that embeddings of heterogeneous data with intersecting features are closely aligned in the subspace. The first part lays foundation for the second, which is key to managing models with heterogeneous feature spaces. ", "page_idx": 13}, {"type": "text", "text": "For the foundational step of subspace learning, if label information is not available, the embeddings of different slices of the same data may not align correctly within the subspace. This misalignment can result in embeddings from different slices having entangled or mixed classes. Furthermore, in extreme cases where feature blocks are jointly independent, subspace learning becomes meaningless due to the irrelevance of features. However, when label information is available, the feature blocks are no longer independent, as all information is used to generate the labels. More discussion that the label information is useful for building connections between independent feature blocks can be found in [Guo et al., 2024]. ", "page_idx": 13}, {"type": "text", "text": "In summary, label information is essential for subspace learning, as it mitigate entangled classes of embeddings, which critically affects the learnware identification and reuse. It also help ensure performance even when feature blocks are weakly dependent. ", "page_idx": 13}, {"type": "text", "text": "C Related work ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The learnware paradigm. The learnware paradigm [Zhou, 2016, Zhou and Tan, 2024] offers a systematic approach to managing well-trained models and leveraging their capabilities to assist users in solving their tasks, rather than training a model from scratch. A learnware consists of a well-trained model accompanied by a specification that describes its capabilities, with this specification being the central component of the learnware. Wu et al. [2023] proposed the RKME specification, which uses a reduced set to sketch the distribution of the task data. Based on the RKME specification, Wu et al. [2023] proposed to match the data distribution for learnware identification, while Zhang et al. [2021] extended it to handle user tasks with unseen parts. To efficiently recommend learnwares among numerous learnwares, Liu et al. [2024] suggested evolving the specification with other learnwares for more accurate identifications and construct the specification index for managing learnwares for efficient learnware search, Xie et al. [2023] proposed using minor representative learnwares as anchors to speed up learnware identification without traversing the whole system. ", "page_idx": 13}, {"type": "text", "text": "Previous research has primarily focused on the homogeneous case, where all models and user tasks share the same feature space. However, in real-world applications, the feature spaces of developer models and user tasks often differ. Tan et al. [2024b] was the first to consider the heterogeneous feature space scenario, but it assumes that the original training data is accessible, and auxiliary data across the entire feature space is collected. To relax this strong assumption of data accessibility, Tan et al. [2023] investigated the organization and utilization of a heterogeneous learnware dock system without requiring access to the original data or auxiliary data across the feature space. While this approach is more realistic, its lack of effective use of label information leads to unsatisfactory performance. This paper examines the importance of label information and integrates it throughout the entire process of the heterogeneous learnware dock system. As a broader impact, the detailed implementation of incorporating label information into the learnware specification can help enhance various aspects of the learnware paradigm. In addition to research on heterogeneous feature spaces, Guo et al. [2023] considered scenarios involving heterogeneous labels. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Based on above research, the first learnware docking system, Beimingwu [Tan et al., 2024a], was recently released. The system streamlines the entire learnware process and provides a highly scalable architecture, facilitating future algorithm implementation and experimental research. ", "page_idx": 14}, {"type": "text", "text": "Related techniques. To measure the distance between two labeled datasets in the same feature space, Alvarez-Melis and Fusi [2020] proposed the Optimal Transport Dataset Distance (OTDD). This approach separately calculates feature and label distances using optimal transport and then combines them. The label distance is derived from the feature distances of partial samples with specific labels. This method aligns with the proposed loss for sketching the labeled dataset to generate specifications and requirements, where the loss consists of feature and label components based on MMD, with the label loss defined by the conditional distributions $P(X|Y)$ . Comparing simply concatenating feature and label, separately tackling feature and label can better measure the distance without label information overwhelmed by the longer feature information. For measuring the distance between two distributions in different feature spaces, M\u00e9moli [2011] proposed the Gromov\u2013Wasserstein distance, which aggregates all distances of tetrads to measure the distances between two points. In our work, we introduce a method to measure the distance between two labeled datasets in heterogeneous feature spaces using subspace learning and maximum mean discrepancy (MMD) over marginal and conditional distributions. ", "page_idx": 14}, {"type": "text", "text": "Existing studies on heterogeneous feature spaces, such as heterogeneous domain adaptation [Duan et al., 2012, Wang and Mahadevan, 2011], heterogeneous transfer learning [Day and Khoshgoftaar, 2017], and heterogeneous model reuse [Ye et al., 2018, 2020], generally map different feature spaces to an intermediate subspace. This process typically requires original data from both domains or co-occurrence data to establish the relationship between different spaces. However, in the learnware paradigm, managing models developed from different feature spaces without auxiliary data becomes feasible due to the existence of RKME specifications associated with each model. Based on this paradigm, we can accommodate, identify, and reuse heterogeneous models of any type without accessing original data or additional co-occurrence auxiliary data. Recently, Guo et al. [2024] explored the relationship between two intersecting feature spaces from a causal perspective, showing that residuals from model predictions can provide information into unobserved variables, specifically, the partial derivative of the true generating function with respect to these unobserved variables. This finding aligns with our approach, where we leverage label information to explore the relationship of different feature spaces. ", "page_idx": 14}, {"type": "text", "text": "Recently, some works have focused on identifying and reusing models from a model hub. Ding and Zhou [2020] select models based on an anomaly detector associated with each model, which helps determine whether a feature sample is appropriate for prediction with that model. Ding et al. [2022] propose selecting models using a task-model metric that requires only minimal interaction with data providers. Zhang et al. [2023] propose selecting pre-trained models by calculating the similarity between learned model embeddings and task embeddings, both of which are obtained through the ranking loss. Yi et al. [2024] explore the model selection specifically for visual language models. However, it is important to note that none of these approaches can directly address scenarios involving heterogeneous feature spaces. ", "page_idx": 14}, {"type": "text", "text": "D Algorithm details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D.1 Summary of overall procedure ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The overall procedure of the heterogeneous learnware dock system consists of two stages. In the submission stage, the dock system receives models with developer-level specifications sketching model capabilities and assigns system-level specifications using a learned unified subspace. In the deployment stage, users submit task requirements detailing marginal and conditional distributions to receive the recommended learnware. This learnware can be integrated with their self-trained models to significantly enhance performance. The detailed procedures for each stage are outlined in Algorithm 1 and 2, respectively. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Algorithm 1 Submitting stage (learnware accommodation by the system)   \n1: Each developer trains a model $f_{i}$ and generates the developer-level specification $\\pmb{s}_{i}^{\\mathrm{dev}}$ on the dataset $D_{i}$ defined on $\\mathcal{X}_{i}^{\\mathrm{dev}}\\times\\mathcal{Y}$ .   \n2: Each developer submits both the model and the developer-level specification $(f_{i},s_{i}^{\\mathrm{dev}})$ to the learnware dock system.   \n3: The learnware dock system generates the projection function $h_{i}:\\mathcal{X}_{i}\\mapsto\\mathcal{X}_{\\mathrm{sub}}$ , reconstruction function $g_{i}:\\mathcal{X}_{\\mathrm{sub}}\\mapsto\\mathcal{X}_{i}$ for each feature block $\\mathcal{X}_{i}$ and the system-level specification $s_{i}$ for each model $f_{i}$ based on all developer-level specifications $\\{s_{i}^{\\mathrm{dev}}\\}_{i=1}^{N}$   \n4: The model $f_{i}$ is accommodated by the learnware dock system with system-level specifications as learnware $\\bar{l_{i}}:=\\{f_{i},s_{i}\\}$ .   \n5: The heterogeneous learnware dock system is established as $\\{l_{i}\\}_{i=1}^{N}$ . ", "page_idx": 15}, {"type": "text", "text": "Algorithm 2 Deploying stage (system exploitation by the user) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1: The user generates the user-level task requirement $\\pmb{s}_{0}^{\\mathrm{user}}$ based on the unlabeled data $D_{0}^{u}$ and limited labeled data $D_{0}^{l}$ defined on $\\chi_{0}^{\\mathrm{user}}\\times\\mathcal{Y}$ .   \n2: The user submits the user-level requirement $\\pmb{s}_{0}^{\\mathrm{user}}$ to the learnware dock system.   \n3: The learnware dock system uses the projection functions $\\{h_{i}\\}_{i}^{Q}$ to generate the system-level requirement $\\mathbf{\\boldsymbol{s}}_{0}$ .   \n4: The learnware dock system recommends one learnware $l_{i}$ based on the system-level requirement $\\ s_{\\mathrm{0}}$ and the system-level specifications $\\{s_{i}\\}_{i=1}^{N}$ and provides the toolkit $\\{g_{i},h_{i}\\}_{i}^{Q}$ to the user.   \n5: The user reuses recommended learnware $l_{i}$ with the toolkit $\\{g_{i},h_{i}\\}_{i}^{Q}$ on the task. ", "page_idx": 15}, {"type": "text", "text": "D.2 Subspace learning and system-level specification generation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "After the developer-level specification $s^{\\mathrm{dev}}$ generation, the developer submits the model $f$ with specification $s^{\\mathrm{de{\\bar{v}}}}$ to the learnware dock system. The system exploits the relationship of different feature spaces and manages heterogeneous models by assigning system-level specification $s^{\\mathrm{sys}}$ . The details of subspace learning and system-level specification generation are described in Algorithm 3. ", "page_idx": 15}, {"type": "text", "text": "E Optimization ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "E.1 Optimization of specification generation specialized for classification tasks ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The $\\mathtt{R K M E_{L}}$ specificati epresented by $R_{L}=(\\beta,Z,Y)=\\left\\{(\\beta_{m},z_{m},y_{m})\\right\\}_{m=1}^{M}$ sketches both the marginal distribution $P_{X}$ of the training data and the conditional distribution $P_{X\\mid Y}$ of the model prediction, which is obtained through minimizing the following objective over $R_{L}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{n=1}^{N}{\\frac{1}{N}}k\\left({\\pmb x}_{n},\\cdot\\right)-\\sum_{m=1}^{M}\\beta_{m}k\\left({\\pmb z}_{m},\\cdot\\right)\\right\\|_{\\mathcal{H}_{k}}^{2}+\\theta\\sum_{c=1}^{C}\\left\\|\\sum_{n\\in\\mathbb{Z}_{c}}{\\frac{1}{N}}k\\left({\\pmb x}_{n},\\cdot\\right)-\\sum_{m\\in\\mathbb{Z}_{c}^{\\prime}}\\beta_{m}k\\left({\\pmb z}_{m},\\cdot\\right)\\right\\|_{\\mathcal{H}_{k}}^{2},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This objective consists of two parts: the first term sketches the marginal distribution $P_{X}$ of the training data, while the second term sketches the conditional distribution $P_{X\\mid Y}$ of the model prediction. This objective can be optimized by iterative optimization, which is detailed in the following. ", "page_idx": 15}, {"type": "text", "text": "Algorithm 3 System-level specification generation ", "page_idx": 16}, {"type": "text", "text": "Input: All developer-level specifications $\\{s_{i}^{\\mathrm{dev}}\\}_{i=1}^{N}$ ", "page_idx": 16}, {"type": "text", "text": "Hyper-Parameters: batch size $B$ , temperature $\\tau$ , corruption rate $c$ , max iteration $T$ , trade-off parameters \u03b11, \u03b12, \u03b13, \u03b14   \nOutput: System-level specifications $\\{\\pmb{s}_{i}^{\\mathrm{sys}}\\}_{i=1}^{N}$ , learnware dock system toolkit $\\{t_{i}:=(h_{i},g_{i})\\}_{i=1}^{Q}$ , system engine $F(\\cdot)$   \n1: while Max epoch is not achieved do   \nInitialize the system engine $F(\\cdot)$ and dock system toolkit $\\{t_{i}:=(h_{i},g_{i})\\}_{i=1}^{Q}$ .   \n3: for each specification $\\pmb{s}_{i}^{\\mathrm{dev}}$ in all developer-level specifications $\\{s_{i}^{\\mathrm{dev}}\\}_{i=1}^{N}$ do   \n4: for sampled mini-batch $\\{\\beta_{j},z_{j}:=(x_{j},y_{j})\\}_{j=1}^{B}$ do   \n5: split $\\pmb{x}_{j}$ according to the feature blocks $\\{\\bar{\\chi_{i}}\\}_{i=1}^{Q};\\{\\pmb{x}_{j}^{(k)}\\}_{k\\in C}$ .   \n6: let vj ${\\pmb v}_{j}^{(k)}=h_{i}({\\pmb x}_{j}^{(k)})$ . # embeddings of each blocks.   \n7: $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{cont}}\\;:=-\\sum_{i=1}^{B}\\beta_{i}\\sum_{k=1}^{K}\\sum_{k^{\\prime}\\neq k}^{K}\\log\\frac{\\exp\\psi\\left({\\mathbf{v}_{i}^{k}},{\\mathbf{v}_{i}^{k^{\\prime}}}\\right)}{\\sum_{j=1}^{B}\\sum_{k^{\\prime}=1}^{K}\\exp\\psi\\left({\\mathbf{v}_{i}^{k}},{\\mathbf{v}_{j}^{k^{\\dagger}}}\\right)}}\\end{array}$ where $\\psi$ is the cosine similarity function.   \n8: let $\\hat{\\pmb{x}}_{j}^{(k)}=g_{i}(\\pmb{v}_{j}^{\\dot{(k)}})$ and concatenate them as $\\hat{\\pmb{x}}_{j}$ . # reconstructed samples.   \n9: define Lreco $\\begin{array}{r}{\\widetilde{\\mathrm{nstructed}}=\\sum_{J=1}^{B}\\sum_{k\\in C}\\beta_{j}\\|\\pmb{x}_{j}^{(k)}-\\hat{\\pmb{x}}_{j}^{(k)}\\|^{2}}\\end{array}$   \n10: let $\\hat{y}_{j}=F(\\pmb{v}_{j})$ where $v_{j}=\\mathrm{mean}(\\{v_{j}^{(k)}\\}_{k\\in C}):$ # simple classifier prediction.   \n11: define $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{supervised}}=\\sum_{j=1}^{B}\\beta_{j}l(y_{j},\\hat{y}_{j})}\\end{array}$ where $l$ is cross entropy for classification problem and mean squared error for regression problem.   \n12: $\\mathcal{L}=\\mathcal{L}+\\alpha_{1}\\mathcal{L}_{\\mathrm{reconstruct}}+\\alpha_{2}\\mathcal{L}_{\\mathrm{contrastive}}+\\alpha_{3}\\mathcal{L}_{\\mathrm{super}}$ vised   \n13: update the encoder networks $\\{h_{i}\\}_{i=1}^{Q}$ , decoder networks $\\{g_{i}\\}_{i=1}^{Q}$ and system engine $F(\\cdot)$ according to $\\mathcal{L}$ using gradient descent.   \n14: end for ", "page_idx": 16}, {"type": "text", "text": "15: end for ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "16: end while ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "17: generate system-level specification $\\pmb{s}_{i}^{\\mathrm{sys}}:=\\{\\beta_{j},(\\pmb{v}_{j},y_{j})\\}_{j=1}^{m_{i}}$ based on the developer-level specification $s_{i}^{\\mathrm{dev}}\\,:=\\,\\{\\beta_{j},({\\pmb x}_{j},y_{j})\\}_{j=1}^{m_{i}}$ by replacing the raw sample $\\pmb{x}_{j}$ with the embedding $\\pmb{v}_{j}$ . ", "page_idx": 16}, {"type": "text", "text": "18: return System-level specifications $\\{s_{i}\\}_{i=1}^{N}$ with corresponding toolkit $\\{\\pmb{t}_{i}\\,:=\\,(h_{i},g_{i})\\}_{i=1}^{Q}$ consists of encoder networks and decoder networks, system engine $F(\\cdot)$ . ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle^\\pi(\\beta,Z,Y)=\\sum_{n,m=1}^{N}\\frac{1}{N^{2}}k\\left(\\pmb{x}_{n},\\pmb{x}_{m}\\right)+\\sum_{n,m=1}^{M}\\beta_{n}\\beta_{m}k\\left(\\pmb{z}_{n},\\pmb{z}_{m}\\right)-2\\sum_{n=1}^{N}\\sum_{m=1}^{M}\\frac{\\beta_{m}}{N}k\\left(\\pmb{x}_{n},\\pmb{z}_{m}\\right)}\\\\ &{\\displaystyle\\qquad+\\theta\\sum_{c=1}^{C}\\left(\\sum_{n,m\\in\\mathbb{Z}_{c}}\\frac{1}{N^{2}}k\\left(\\pmb{x}_{n},\\pmb{x}_{m}\\right)+\\sum_{n,m\\in\\mathbb{Z}_{c}^{\\prime}}\\beta_{n}\\beta_{m}k\\left(\\pmb{z}_{n},\\pmb{z}_{m}\\right)-2\\sum_{n\\in\\mathbb{Z}_{c}}\\sum_{m\\in\\mathbb{Z}_{c}^{\\prime}}\\frac{\\beta_{m}}{N}k\\left(\\pmb{x}_{n},\\pmb{z}_{m}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The distance $F(\\beta,Z,Y)$ can also be rewritten as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\alpha^{T}K_{x x}\\alpha+\\beta^{T}K_{z z}\\beta-2\\alpha^{T}K_{x z}\\beta+\\theta\\displaystyle\\sum_{c=1}^{C}\\left(\\alpha_{c}^{T}K_{x_{c}x_{c}}\\alpha_{c}+\\beta_{c}^{T}K_{z_{c}z_{c}}\\beta_{c}-2\\alpha_{c}^{T}K_{x_{c}z_{c}}\\beta_{c}\\right)}\\\\ &{=\\alpha^{T}(K_{x x}+\\theta K_{x x}^{b})\\alpha+\\beta^{T}(K_{z z}+\\theta K_{z z}^{b})\\beta-2\\alpha^{T}(K_{x z}+\\theta K_{x z}^{b})\\beta}\\\\ &{=\\alpha^{T}K_{x x}^{'}\\alpha+\\beta^{T}K_{z z}^{'}\\beta-2\\alpha^{T}K_{x z}^{'}\\beta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\alpha$ is the vector containing $N$ elements of 1. ", "page_idx": 16}, {"type": "text", "text": "Next, we address the optimization of Eq. (5). First, we generate $Y$ while preserving the class ratio of the original labels $\\{y_{n}\\}_{n=1}^{N}$ . Then, we proceed to optimize $\\beta$ and $Z$ . ", "page_idx": 16}, {"type": "text", "text": "Fix $Z$ and update $\\beta$ . Suppose vectors in $Z$ are fixed, setting $\\begin{array}{r}{\\frac{\\partial F(\\beta,Z)}{\\partial\\beta}=0}\\end{array}$ obtains the closed-form solution of $\\beta$ using pseudo-inverse of $K_{z z}^{'}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\beta=(K_{z z}^{'})^{\\dagger}K_{z x}^{'}\\alpha.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Fix $\\beta$ and update $Z$ . When $\\beta$ is fixed, $\\{z_{1},\\cdot\\cdot\\cdot,z_{M}\\}$ in $Z$ are independent in Eq. (5), therefore we can iteratively run gradient descent on each $z_{m}$ as ", "page_idx": 17}, {"type": "equation", "text": "$$\nz_{m}^{(t)}=z_{m}^{(t-1)}-\\eta\\frac{\\partial F(\\beta,Z)}{\\partial z_{m}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We first review the gradient $\\frac{\\partial G(\\beta,Z)}{\\partial z_{m}}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\frac{\\partial G(\\beta,Z)}{\\partial z_{m}}=2\\sum_{n=1}^{M}\\beta_{n}\\beta_{m}\\frac{\\partial k(z_{n},z_{m})}{\\partial z_{m}}-2\\sum_{n=1}^{N}\\frac{\\beta_{m}}{N}\\frac{\\partial k(x_{n},z_{m})}{\\partial z_{m}}}\\\\ {\\displaystyle}&{\\displaystyle=2\\sum_{n=1}^{M}\\beta_{n}\\beta_{m}(-2\\gamma k(z_{n},z_{m})(z_{m}-z_{n}))-2\\sum_{n=1}^{N}\\frac{\\beta_{m}}{N}(-2\\gamma k(x_{n},z_{m})(z_{m}-x_{n}))}\\\\ {\\displaystyle}&{\\displaystyle=-4\\gamma\\beta_{m}(\\sum_{n=1}^{M}\\beta_{n}k(z_{n},z_{m})(z_{m}-z_{n})-\\frac{1}{N}\\sum_{n=1}^{N}k(x_{n},z_{m})(z_{m}-x_{n}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "then, we consider the gradient in our problem $\\frac{\\partial F(\\beta,\\!Z)}{\\partial z_{m}}$ , which is calculated as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad_{2}\\frac{\\mathcal{E}_{x}}{w_{1}}\\delta_{x}\\delta_{x}(\\xi_{1},\\xi_{2},\\ldots)=-\\frac{\\mathcal{E}_{x}}{w_{2}}\\frac{\\delta_{x}(\\xi_{1},\\xi_{2})}{|\\mathcal{E}_{x}|}}\\\\ &{\\quad+\\varepsilon\\left(\\sum_{s\\geq1,\\,s}\\delta_{x}\\frac{\\delta_{x}(\\xi_{1},\\xi_{2})}{|\\mathcal{E}_{x}|}(\\xi_{1},\\ldots)-2\\sum_{s\\geq1}^{\\infty}\\delta_{x}\\frac{\\delta_{x}(\\xi_{2},\\ldots)}{|\\mathcal{E}_{x}|}\\right)}\\\\ &{\\quad}\\\\ &{=2\\frac{\\mathcal{E}_{x}}{w_{2}}\\delta_{x}(\\xi_{1},\\ldots)\\!\\left(\\delta_{x},\\xi_{2}\\right)\\!\\left(\\delta_{x}-\\varepsilon_{3}\\right)\\!-2\\sum_{s\\geq1}^{\\infty}\\delta_{x}(\\xi_{2},\\ldots)}\\\\ &{\\quad+\\varepsilon\\left(\\sum_{s\\geq1,\\,s}\\delta_{x}(\\xi_{2},\\ldots)(\\xi_{1},\\xi_{2})\\ldots)-2\\sum_{s\\geq1}^{\\infty}\\delta_{x}(\\xi_{2},\\ldots)(\\xi_{1},\\xi_{2})\\!\\left(\\delta_{x}-\\varepsilon_{3}\\right)\\right)}\\\\ &{\\quad}\\\\ &{=-\\delta\\left(\\sum_{s\\geq1}^{\\infty}\\delta_{x}(\\xi_{1},\\ldots)\\!\\left(\\delta_{x}-\\varepsilon_{3}\\right)-\\frac{1}{\\delta_{x}}\\sum_{s\\geq1}^{\\infty}\\delta_{x}(\\xi_{2},\\ldots,\\xi_{3})\\right)}\\\\ &{\\quad+\\varepsilon\\left(\\sum_{s\\geq1}^{\\infty}\\delta_{x}(\\xi_{1},\\ldots)(\\xi_{1},\\ldots)-\\frac{1}{\\delta_{x}}\\sum_{s\\geq1}^{\\infty}(\\xi_{2},\\ldots,\\xi_{3})(\\xi_{1},\\ldots)\\right)}\\\\ &{\\quad+\\varepsilon\\left(\\sum_{s\\geq1}^{\\infty}\\delta_{x}(\\xi_{1},\\ldots)(\\xi_{1},\\ldots)-\\frac{1}{\\delta_{x}}\\sum_{s\\geq1}^{\\infty}\\delta_{x}(\\xi_{2},\\ldots,\\xi_{3})\\right)}\\\\ &{\\quad=-\\delta\\left( \n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathcal{Z}_{c}$ is the sample indices of class c of sample $z_{m}$ . ", "page_idx": 17}, {"type": "text", "text": "E.2 Optimization of user requirement generation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "When the user has major unlabeled data $D^{u}=\\{{\\pmb x}_{i}\\}_{i=1}^{N_{l}}$ and limited labeled data $D^{l}=\\{(\\tilde{{\\pmb{x}}}_{i},y_{i})\\}_{i=1}^{N_{u}}$ (We adjust the notation slightly for a clearer description of the optimization process), we can also ", "page_idx": 17}, {"type": "text", "text": "find a weighted labeled set of points $R=(\\beta,Z,Y)=\\{(\\beta_{m},z_{m},y_{m})\\}_{m=1}^{M}$ to sketch the user data as task requirement. ", "page_idx": 18}, {"type": "text", "text": "The weighted labeled set can be generated by minimizing the following distance: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{n=1}^{N_{u}}\\frac{1}{N_{u}}k\\left(x_{n},\\cdot\\right)-\\sum_{m=1}^{M}\\beta_{m}k\\left(z_{m},\\cdot\\right)\\right\\|_{\\mathcal{H}_{k}}^{2}+\\theta\\sum_{c=1}^{C}\\left\\|\\sum_{n\\in\\mathbb{Z}_{c}}\\frac{1}{N_{l}}k\\left(\\tilde{x}_{n},\\cdot\\right)-\\sum_{m\\in\\mathcal{T}_{c}^{'}}\\beta_{m}k\\left(z_{m},\\cdot\\right)\\right\\|_{\\mathcal{H}_{k}}^{2},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which can be optimized by iterative optimization. The details are described as follows. ", "page_idx": 18}, {"type": "text", "text": "Denote $\\beta=(\\beta_{1},\\cdot\\cdot\\cdot,\\beta_{M})$ , $Z=\\{z_{1},\\cdots,z_{M}\\}$ and $Y=\\{y_{1},\\cdot\\cdot\\cdot,y_{M}\\}$ , expanding Eq. (6) gives ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle^\\circ(\\beta,Z,Y)=\\sum_{n,m=1}^{N_{u}}\\frac1{N_{u}^{2}}k\\left({\\pmb x}_{n},{\\pmb x}_{m}\\right)+\\sum_{n,m=1}^{M}\\beta_{n}\\beta_{m}k\\left({\\pmb z}_{n},{\\pmb z}_{m}\\right)-2\\sum_{n=1}^{N_{u}}\\sum_{m=1}^{M}\\frac{\\beta_{m}}{N_{u}}k\\left({\\pmb x}_{n},{\\pmb z}_{m}\\right)}}\\\\ {{\\displaystyle+\\,\\theta\\sum_{c=1}^{C}\\left(\\sum_{n,m\\in{\\mathbb{Z}}_{c}}\\frac1{N_{l}^{2}}k\\left(\\tilde{{\\pmb x}}_{n},\\tilde{{\\pmb x}}_{m}\\right)+\\sum_{n,m\\in{\\mathbb{Z}}_{c}^{\\prime}}\\beta_{n}\\beta_{m}k\\left({\\pmb z}_{n},{\\pmb z}_{m}\\right)-2\\sum_{n\\in{\\mathbb{Z}}_{c}}\\sum_{m\\in{\\mathbb{Z}}_{c}^{\\prime}}\\frac{\\beta_{m}}{N_{l}}k\\left(\\tilde{{\\pmb x}}_{n},{\\pmb z}_{m}\\right)\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The distance $F(\\beta,Z,Y)$ can also be rewritten as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\alpha^{T}K_{x x}\\alpha+\\beta^{T}K_{z z}\\beta-2\\alpha^{T}K_{x z}\\beta+\\theta\\sum_{c=1}^{C}\\left(\\tilde{\\alpha}_{c}^{T}K_{\\tilde{x}_{c}\\tilde{x}_{c}}\\tilde{\\alpha}_{c}+\\beta_{c}^{T}K_{z_{c}z_{c}}\\beta_{c}-2\\tilde{\\alpha}_{c}^{T}K_{\\tilde{x}_{c}z_{c}}\\beta_{c}\\right)}}\\\\ {{\\displaystyle=\\left(\\alpha^{T}K_{x x}\\alpha+\\theta\\tilde{\\alpha}^{T}K_{\\tilde{x}\\tilde{x}}^{b}\\tilde{\\alpha}\\right)+\\beta^{T}(K_{z z}+\\theta K_{z z}^{b})\\beta-2(\\alpha^{T}K_{x z}+\\theta\\tilde{\\alpha}^{T}K_{\\tilde{x}z}^{b})\\beta}}\\\\ {{\\displaystyle=\\left(\\alpha^{T}K_{x x}\\alpha+\\theta\\tilde{\\alpha}^{T}K_{\\tilde{x}\\tilde{x}}^{b}\\tilde{\\alpha}\\right)+\\beta^{T}(K_{z z}^{'})\\beta-2(\\alpha^{T}K_{x z}+\\theta\\tilde{\\alpha}^{T}K_{\\tilde{x}z}^{b})\\beta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\alpha$ is the vector containing $N_{u}$ elements of 1, $\\tilde{\\alpha}$ is the vector containing $N_{l}$ elements of 1. ", "page_idx": 18}, {"type": "text", "text": "Then, we discuss the optimization of Eq. (6). We first generate $Y$ , which will keep the class ratio of $\\{y_{n}\\}_{n=1}^{N}$ . Then we optimize $\\beta,Z$ . ", "page_idx": 18}, {"type": "text", "text": "Fix $Z$ and update $\\beta$ . Suppose vectors in $Z$ are fixed, setting $\\begin{array}{r}{\\frac{\\partial F(\\beta,Z)}{\\partial\\beta}=0}\\end{array}$ obtains the closed-form solution of $\\beta$ using pseudo-inverse of $K_{z z}^{'}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\beta=(K_{z z}^{'})^{\\dagger}(K_{z x}\\alpha+\\theta K_{z\\tilde{\\pm}}^{b}\\tilde{\\alpha}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Fix $\\beta$ and update $Z$ . When $\\beta$ is fixed, $\\{z_{1},\\cdot\\cdot\\cdot,z_{M}\\}$ in $Z$ are independent in Eq. (6), therefore we can iteratively run gradient descent on each $z_{m}$ as ", "page_idx": 18}, {"type": "equation", "text": "$$\nz_{m}^{(t)}=z_{m}^{(t-1)}-\\eta\\frac{\\partial F(\\beta,Z)}{\\partial z_{m}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The gradient in our problem $\\frac{\\partial F(\\beta,Z)}{\\partial z_{m}}$ can be calculated as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial{\\cal F}(\\beta,Z)}{\\partial z_{m}}=2\\sum_{n=1}^{M}\\beta_{n}\\beta_{m}\\frac{\\partial k(z_{n},z_{m})}{\\partial z_{m}}-2\\sum_{n=1}^{N_{l}}\\frac{\\beta_{m}}{N_{l}}\\frac{\\partial k(x_{n},z_{m})}{\\partial z_{m}}}\\\\ {\\displaystyle\\quad\\quad\\quad\\quad\\quad=-4\\gamma\\beta_{m}\\left(\\sum_{n=1}^{M}\\beta_{n}(1+\\theta\\mathbb{I}(y_{z_{n}}=c))k(z_{n},z_{m})(z_{m}-z_{n})\\right.}\\\\ {\\displaystyle\\left.\\quad\\quad\\quad-\\frac{1}{N_{u}}\\sum_{n=1}^{N_{u}}k(x_{n},z_{m})(z_{m}-x_{n})-\\frac{\\theta}{N_{l}}\\sum_{n\\in\\mathbb{Z}_{c}}k(\\tilde{x}_{n},z_{m})(z_{m}-\\tilde{x}_{n})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathcal{Z}_{c}$ represents the indices of labeled samples $\\tilde{\\pmb{x}}_{n}$ belonging to the same class as $z_{m}$ . ", "page_idx": 18}, {"type": "text", "text": "E.3 Summary ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The generation mechanism for model specification and user requirement is summarized in Figure 6. ", "page_idx": 18}, {"type": "image", "img_path": "3YIyB82rjX/tmp/32fadf0c66fc7f347dbadfc1e14edd8ebecf912a2c2c009b07c9ca928a186b82.jpg", "img_caption": ["Figure 6: summarized mechanisms for $\\mathtt{R K M E_{L}}$ generation. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "F Experiments ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "F.1 More details for basic information ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Dataset details. The basic information of used datasets is summarized in Table 4 and Table 5. ", "page_idx": 19}, {"type": "table", "img_path": "3YIyB82rjX/tmp/2ec9dd2a7f23c75b25ad8fc58f67591fc150d05016a4ce3db768aea72ae729fe.jpg", "table_caption": ["Table 4: Details for classification tasks "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Experiment configuration. Each dataset is split into training and test sets with a 4:1 ratio [McElfresh et al., 2023]. The output of regression tasks is scaled to [0,1]. The feature space is randomly divided into four equal blocks. We create four feature spaces for developer tasks from all three-block combinations and six feature spaces for user tasks from all two-block combinations. Our encoder, decoder, and system classifier are two-layer ResNets [He et al., 2016] for tabular data, with subspace and hidden layer dimensions set to 16 and 32, respectively. We optimize using Adam [Kingma and Ba, 2015]. For user tasks, we sample 100 labeled data points from the training set, using stratified sampling for classification and binning for regression. The coefficients for contrastive, reconstruction and supervised losses are set to 100, 1, and 1, respectively. Developers or users train the model using LightGBM [Ke et al., 2017] with grid search. All experiments are repeated five times. ", "page_idx": 19}, {"type": "table", "img_path": "3YIyB82rjX/tmp/48fcbd0c1e2e8d16bdf569974fc6895b4fd073d743c73b35390380e557eadbc2.jpg", "table_caption": ["Table 5: Details for regression tasks "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Model training details. Tree-based models provide strong performance and high efficiency for supervised tabular tasks, so we use the popular efficient tree ensemble method LightGBM [Ke et al., 2017] to train developer models and include it as a comparative method. The hyper-parameter search space used for the developer and user LightGBM models consists of a list of specific combinations over parameters learning_rate, num_leaves and max_depth: (0.015, 224, 66), (0.005, 300, 50), (0.01, 128, 80), (0.15, 224, 80), and (0.01, 300, 66). ", "page_idx": 20}, {"type": "text", "text": "Baseline details. More details of deep tabular network are described as follows: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 TabPFN: For TabPFN, we use the official checkpoint as a pre-trained model and further fine-tune it on downstream datasets. When testing TabPFN on datasets with more than 1,000 instances or 100 features, we randomly sample up to 1,000 instances and 100 features from the full training set, repeating this process three times. The final output is obtained by averaging the predictions from these trials.   \n\u2022 Xtab: We utilize the pre-trained backbone with the most iterations from XTab\u2019s official implementation. Since this model is based on the FT-Transformer from AutoGluon, which currently does not support fine-tuning on tasks with more than 300 features, we generate random subsets of up to 300 features from the overall training dataset, repeating this procedure three times. The final prediction is derived by averaging the results across three evaluations on the target datasets, using XTab\u2019s lightweight fine-tuning approach over 15 epochs.   \n\u2022 Transtab: We employ the official version of TransTab v0.0.5, testing it in a contrastive learning setting. This involves initial contrastive pre-training on the developers\u2019 training data, followed by fine-tuning on user tasks. We adopt a supervised contrastive learning objective during pre-training, which has been shown to outperform the unsupervised version as noted in [Wang and Sun, 2022]. For regression tasks, a linear regressor is integrated during the fine-tuning phase. To manage memory constraints with large datasets, we subsample the developers\u2019 training sets to 20,000 data points and 100 features. ", "page_idx": 20}, {"type": "text", "text": "Computation resources. Experiments were conducted using a Tesla A100 80GB GPU, two Intel Xeon Platinum 8358 CPUs with 32 cores each (base clock $2.6\\:\\mathrm{GHz}$ , turbo boost $3.4\\:\\mathrm{GHz}$ ), and 512 GB of RAM. ", "page_idx": 20}, {"type": "text", "text": "F.2 All user curves for classification tasks ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The all user performance curves for classification tasks are shown in Figure 7. ", "page_idx": 20}, {"type": "text", "text": "F.3 Ablation study ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Ablation study for subspace learning loss by adding loss in sequence. We undertake an ablation study to evaluate the efficacy of our subspace learning procedure, as depicted in Tables 6 and 7. Initially, we utilize only the contrastive loss as the baseline. This helps in learning encoding functions for different feature blocks, ensuring consistency in embeddings across all sample slices. However, the reconstruction functions learned in this manner are subpar. By adding a reconstruction loss, the performance of decoding functions is matched with the encoding functions. Despite the initial effectiveness of unsupervised subspace learning, further enhancements are achieved by incorporating label information through our proposed specification. With the addition of supervised loss, we ultimately attain optimal performance for both classification and regression tasks. ", "page_idx": 20}, {"type": "image", "img_path": "3YIyB82rjX/tmp/acd4adebe2f1ae1c4f6d9a8873caebc5875940019241e194eb44289af6f66629.jpg", "img_caption": ["Figure 7: All user performance curve for classification tasks. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Table 6: Ablation study for subspace learning loss Table 7: Ablation study for subspace learning loss by adding loss in sequence for classification tasks. by adding loss in sequence for regression tasks. ", "page_idx": 21}, {"type": "table", "img_path": "3YIyB82rjX/tmp/263925eb79820cbd04c71c4ca6be3dd16198778a62ab7d60fea371c62c3eeec5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "3YIyB82rjX/tmp/a55b5045c62f1882f9b542b0c78e02c4ee02c94a761e1648efc9cacba521e5f8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "F.4 Code availability ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The code can be found at https://github.com/LAMDA-TP/Hetero-Learnware-Label-Info. ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions supported by the remaining sections. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: This paper assumes that all models with heterogeneous feature spaces share the same label space. However, this assumption can be further extended to include heterogeneous label spaces as well. For example, a basic way is to recommend multiple learnwares by distribution matching and reuse learnwares by dynamic classifier selection proposed in previous work [Wu et al., 2023]. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not include theoretical results. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide a detailed description of the experimental setup in Appendix F.1.   \nThe specifics of our algorithms are thoroughly explained in Appendix D. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The link to the code can be found in Appendix F.4. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: A detailed description of the experimental setup is provided in Appendix F.1. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We report the standard deviations for performance comparison in our experiments. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We report our compute resources in Appendix F.1. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper clearly cites the original paper that produced the code package or dataset. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}]