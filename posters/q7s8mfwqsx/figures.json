[{"figure_path": "Q7s8mFWqsx/figures/figures_1_1.jpg", "caption": "Figure 1: Overall framework of VPDD.", "description": "The figure illustrates the overall framework of the Video-based Policy learning framework via Discrete Diffusion (VPDD). It shows how the model leverages large-scale actionless human videos and a small number of action-labeled robot videos to learn an actionable discrete diffusion policy.  The process involves three stages: 1) Mixed video data (human and robot videos) undergoes a Vector Quantized Variational Autoencoder to create unified latent representations. 2) Self-supervised pre-training using a discrete diffusion model predicts future videos and actions, learning shared knowledge between humans and robots. 3) Fine-tuning on limited robot data guides low-level action learning using predicted future videos to enhance the learned policies.", "section": "1 Introduction"}, {"figure_path": "Q7s8mFWqsx/figures/figures_3_1.jpg", "caption": "Figure 2: Overall pipeline of VPDD. A video-based VQ-VAE is leveraged to encode both human and robot videos into discrete latent codes. Subsequently, a unified discrete diffusion is firstly pre-trained on these video latent codes via a self-supervised objective, predicting future videos conditioning on language instructions and historical videos. The pre-trained video prediction model p\u03b81 can capture temporal dynamics and task-specific representations. Lastly, we fine-tune our diffusion model on a limited number of robot data. In each diffusion step of the fine-tuning stage, we leverage p\u03b81 to provide hidden representations zx to benefit downstream action learning with video foresight. This integration of video prediction and action learning is achieved through our unified discrete diffusion.", "description": "This figure illustrates the overall framework of the Video-based Policy learning framework via Discrete Diffusion (VPDD) model.  It shows three stages: 1) Video Tokenization using VQ-VAE to encode human and robot videos into a unified latent space; 2) Pre-training with mixed data using a discrete diffusion model to predict future videos based on historical videos and language instructions; and 3) Fine-tuning with robot data to learn an actionable policy by using the predicted future videos to guide action learning.", "section": "3 Methodology"}, {"figure_path": "Q7s8mFWqsx/figures/figures_7_1.jpg", "caption": "Figure 3: Single-view and multi-view images from Meta-World button-press and RLBench drug-stick tasks, sampled from videos predicted by p\u03b81.", "description": "This figure shows sample images from videos generated by the model's video prediction component (p\u03b81).  It demonstrates the model's ability to generate both single-view (Meta-World button-press) and multi-view (RLBench drug-stick) images that capture the temporal dynamics of the tasks. This is crucial for the model's downstream action prediction capabilities. The images are taken from different viewpoints, including front and various shoulder and wrist camera angles for the RLBench task. These illustrate the model's capacity to learn general dynamic patterns for planning and control.", "section": "5.2 Results Analysis"}, {"figure_path": "Q7s8mFWqsx/figures/figures_7_2.jpg", "caption": "Figure 4: Average success rate across 3 seeds on MT50-rand. Each task is evaluated for 50 episodes.", "description": "This figure shows the average success rate across three different random seeds for 50 distinct manipulation tasks in the Meta-World benchmark (MT50-rand).  Each task was evaluated using 50 episodes. The performance of VPDD (ours), VPDD without human data, MTDIFF-P, R3M-Diffusion, VC-1-Diffusion, Video-MTDT, and SODA are compared to highlight the effectiveness of VPDD's approach.", "section": "5.2 Results Analysis"}, {"figure_path": "Q7s8mFWqsx/figures/figures_8_1.jpg", "caption": "Figure 4: Average success rate across 3 seeds on MT50-rand. Each task is evaluated for 50 episodes.", "description": "This figure presents a bar chart comparing the average success rates of different methods on the MT50-rand benchmark of Meta-World.  The success rate, expressed as a percentage, represents the proportion of times each method successfully completed a task. The chart showcases the performance of VPDD (Ours), VPDD without human data, R3M-Diffusion, VC-1-Diffusion, and Video-MTDT across multiple tasks. Error bars indicate the standard deviation across three independent runs. The figure demonstrates VPDD's superior performance compared to the baselines on Meta-World tasks.", "section": "5.2 Results Analysis"}, {"figure_path": "Q7s8mFWqsx/figures/figures_8_2.jpg", "caption": "Figure 6: Average success rate across 3 seeds on MT50-rand, where VPDD is trained on a different number of demonstrations.", "description": "This figure shows the ablation study on the number of demonstrations used in the fine-tuning stage.  The results indicate that the performance of VPDD exhibits linear growth after training on 5 or more demonstrations, suggesting good sample efficiency.  Even with only 1 demonstration, VPDD maintains a comparable success rate, highlighting its ability to learn effectively from limited data due to the strong pre-training.", "section": "5.2 Results Analysis"}, {"figure_path": "Q7s8mFWqsx/figures/figures_8_3.jpg", "caption": "Figure 4: Average success rate across 3 seeds on MT50-rand. Each task is evaluated for 50 episodes.", "description": "This figure shows the average success rate across three different random seeds for 50 distinct manipulation tasks in the Meta-World benchmark (MT50-rand).  Each task was evaluated over 50 episodes. The figure compares the performance of the proposed method (VPDD) against several baseline approaches.", "section": "5.2 Results Analysis"}, {"figure_path": "Q7s8mFWqsx/figures/figures_16_1.jpg", "caption": "Figure 2: Overall pipeline of VPDD. A video-based VQ-VAE is leveraged to encode both human and robot videos into discrete latent codes. Subsequently, a unified discrete diffusion is firstly pre-trained on these video latent codes via a self-supervised objective, predicting future videos conditioning on language instructions and historical videos. The pre-trained video prediction model p\u03b81 can capture temporal dynamics and task-specific representations. Lastly, we fine-tune our diffusion model on a limited number of robot data. In each diffusion step of the fine-tuning stage, we leverage p\u03b81 to provide hidden representations zx to benefit downstream action learning with video foresight. This integration of video prediction and action learning is achieved through our unified discrete diffusion.", "description": "This figure shows the overall pipeline of the proposed Video-based Policy learning framework via Discrete Diffusion (VPDD). It illustrates the process of encoding human and robot videos using VQ-VAE, pre-training a unified discrete diffusion model for video prediction using a self-supervised objective, and fine-tuning the model for action learning using a limited set of robot data. The figure highlights the integration of video prediction and action learning through the unified discrete diffusion model.", "section": "3 Methodology"}, {"figure_path": "Q7s8mFWqsx/figures/figures_20_1.jpg", "caption": "Figure 2: Overall pipeline of VPDD. A video-based VQ-VAE is leveraged to encode both human and robot videos into discrete latent codes. Subsequently, a unified discrete diffusion is firstly pre-trained on these video latent codes via a self-supervised objective, predicting future videos conditioning on language instructions and historical videos. The pre-trained video prediction model p\u03b81 can capture temporal dynamics and task-specific representations. Lastly, we fine-tune our diffusion model on a limited number of robot data. In each diffusion step of the fine-tuning stage, we leverage p\u03b81 to provide hidden representations zx to benefit downstream action learning with video foresight. This integration of video prediction and action learning is achieved through our unified discrete diffusion.", "description": "This figure illustrates the overall pipeline of the Video-based Policy learning framework via Discrete Diffusion (VPDD).  It shows how human and robot videos are encoded into discrete latent codes using a VQ-VAE.  The model is then pre-trained using a self-supervised approach to predict future videos, conditioned on language instructions and past video frames. Finally, it's fine-tuned on robot data, using the pre-trained video prediction to guide action learning.  The process combines generative video prediction and action prediction within a unified discrete diffusion framework.", "section": "3 Methodology"}, {"figure_path": "Q7s8mFWqsx/figures/figures_21_1.jpg", "caption": "Figure 2: Overall pipeline of VPDD. A video-based VQ-VAE is leveraged to encode both human and robot videos into discrete latent codes. Subsequently, a unified discrete diffusion is firstly pre-trained on these video latent codes via a self-supervised objective, predicting future videos conditioning on language instructions and historical videos. The pre-trained video prediction model po\u2081 can capture temporal dynamics and task-specific representations. Lastly, we fine-tune our diffusion model on a limited number of robot data. In each diffusion step of the fine-tuning stage, we leverage po\u2081 to provide hidden representations zx to benefit downstream action learning with video foresight. This integration of video prediction and action learning is achieved through our unified discrete diffusion.", "description": "This figure illustrates the overall framework of the Video-based Policy learning framework via Discrete Diffusion (VPDD).  It shows the three main stages: video tokenization using a VQ-VAE, pre-training with mixed human and robot data using a discrete diffusion model to predict future videos, and fine-tuning with limited robot data to learn an actionable policy by leveraging video foresight.", "section": "3 Methodology"}, {"figure_path": "Q7s8mFWqsx/figures/figures_22_1.jpg", "caption": "Figure 2: Overall pipeline of VPDD. A video-based VQ-VAE is leveraged to encode both human and robot videos into discrete latent codes. Subsequently, a unified discrete diffusion is firstly pre-trained on these video latent codes via a self-supervised objective, predicting future videos conditioning on language instructions and historical videos. The pre-trained video prediction model p\u03b8\u2081 can capture temporal dynamics and task-specific representations. Lastly, we fine-tune our diffusion model on a limited number of robot data. In each diffusion step of the fine-tuning stage, we leverage p\u03b8\u2081 to provide hidden representations zx to benefit downstream action learning with video foresight. This integration of video prediction and action learning is achieved through our unified discrete diffusion.", "description": "This figure illustrates the overall framework of the Video-based Policy learning framework via Discrete Diffusion (VPDD). It shows how human and robot videos are encoded into discrete latent codes using a VQ-VAE.  The framework then uses a unified discrete diffusion model for pre-training (predicting future videos using language instructions and historical videos) and fine-tuning (predicting actions from future video predictions using limited robot data).", "section": "3 Methodology"}, {"figure_path": "Q7s8mFWqsx/figures/figures_23_1.jpg", "caption": "Figure 2: Overall pipeline of VPDD. A video-based VQ-VAE is leveraged to encode both human and robot videos into discrete latent codes. Subsequently, a unified discrete diffusion is firstly pre-trained on these video latent codes via a self-supervised objective, predicting future videos conditioning on language instructions and historical videos. The pre-trained video prediction model p\u03b8\u2081 can capture temporal dynamics and task-specific representations. Lastly, we fine-tune our diffusion model on a limited number of robot data. In each diffusion step of the fine-tuning stage, we leverage p\u03b8\u2081 to provide hidden representations zx to benefit downstream action learning with video foresight. This integration of video prediction and action learning is achieved through our unified discrete diffusion.", "description": "This figure illustrates the overall framework of the Video-based Policy learning framework via Discrete Diffusion (VPDD) model.  It details the three stages: 1) Video Tokenizing using VQ-VAE to encode human and robot videos into a unified latent space; 2) Pre-training with mixed data using a discrete diffusion model for future video prediction, conditioned on language instructions and historical frames; 3) Fine-tuning with robot data, incorporating predicted future videos to guide action learning. The model combines generative pre-training and policy fine-tuning for multi-task robotic learning.", "section": "3 Methodology"}]