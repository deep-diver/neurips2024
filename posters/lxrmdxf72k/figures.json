[{"figure_path": "LxRmdXf72k/figures/figures_1_1.jpg", "caption": "Figure 1: Visualizations of (a) Attention Map and (b) Gradient Map of different models, including Non-causal ViT, Causal ViT, Causal Mamba and our De-focus Attention Network (Mamba-based). The results are from the 11th layer of ViT (12 in total) and 22nd layer of Mamba (24 in total). (a) The approximated attention maps of all image tokens: The row and column axes represent the query and key token index respectively. Brighter color indicates larger attention values. (b) The gradient maps of each image token input after back-propagation: Redder colors indicate larger gradient norms. See Appendix A for more visualizations on different layers.", "description": "This figure visualizes attention maps and gradient maps for four different models: Non-causal ViT, Causal ViT, Causal Mamba, and the proposed De-focus Attention Network.  The attention maps show where the model focuses its attention, highlighting the \"over-focus\" issue in 1D causal models. The gradient maps show how gradients flow during backpropagation, illustrating the impact of over-focus on model optimization.", "section": "Abstract"}, {"figure_path": "LxRmdXf72k/figures/figures_5_1.jpg", "caption": "Figure 2: Architecture of our De-focus Attention Network. Left: Detailed architecture of De-focus Attention Block: The input tokens are projected to Q, K, and other parameters required by certain causal attention layer (e.g. Transformer or Mamba). A is data-dependent in De-focus Mamba, while  is set to 1 in De-focus ViT. Learnable decay and learnable relative position embeddings form a learnable bandpass filter and are calculated before being fed into the causal attention layer. Parameter A in De-focus ViT corresponds to A in this figure. Right: Overall architecture of De-focus Attention Network: Drop paths are incorporated after each De-focus Attention Block. All output image tokens are passed through Average Pooling and a fully connected layer to produce the auxiliary loss.", "description": "The figure illustrates the architecture of the De-focus Attention Network. The left part shows the detailed architecture of a De-focus Causal Attention Block, highlighting the learnable bandpass filter and its components (learnable decay, learnable RoPE, and projection layer). The right part shows the overall network architecture, indicating the sequence of De-focus Causal Attention Blocks, drop path mechanism, average pooling, and the auxiliary loss used for optimization. It explains how the learnable bandpass filter, drop path, and auxiliary loss work together in the 1D causal visual representation learning.", "section": "4 Method"}, {"figure_path": "LxRmdXf72k/figures/figures_14_1.jpg", "caption": "Figure 1: Visualizations of (a) Attention Map and (b) Gradient Map of different models, including Non-causal ViT, Causal ViT, Causal Mamba and our De-focus Attention Network (Mamba-based). The results are from the 11th layer of ViT (12 in total) and 22nd layer of Mamba (24 in total). (a) The approximated attention maps of all image tokens: The row and column axes represent the query and key token index respectively. Brighter color indicates larger attention values. (b) The gradient maps of each image token input after back-propagation: Redder colors indicate larger gradient norms. See Appendix A for more visualizations on different layers.", "description": "This figure visualizes the attention maps and gradient maps from the 11th layer of ViT and the 22nd layer of Mamba models.  It compares the attention and gradient patterns of a non-causal ViT, a causal ViT, a causal Mamba, and the proposed De-focus Attention Network. Brighter colors in the attention maps indicate stronger attention weights, while redder colors in the gradient maps represent larger gradient norms. The visualization highlights the \"over-focus\" issue in 1D causal models, where attention concentrates on a small subset of visual tokens.", "section": "Abstract"}, {"figure_path": "LxRmdXf72k/figures/figures_15_1.jpg", "caption": "Figure 1: Visualizations of (a) Attention Map and (b) Gradient Map of different models, including Non-causal ViT, Causal ViT, Causal Mamba and our De-focus Attention Network (Mamba-based). The results are from the 11th layer of ViT (12 in total) and 22nd layer of Mamba (24 in total). (a) The approximated attention maps of all image tokens: The row and column axes represent the query and key token index respectively. Brighter color indicates larger attention values. (b) The gradient maps of each image token input after back-propagation: Redder colors indicate larger gradient norms. See Appendix A for more visualizations on different layers.", "description": "The figure visualizes the attention maps and gradient maps of four different models: non-causal ViT, causal ViT, causal Mamba, and the proposed De-focus Attention Network.  It highlights the \"over-focus\" issue in 1D causal models, where attention concentrates on a small portion of visual tokens, hindering feature extraction and gradient optimization. The De-focus Network addresses this by creating diverse attention patterns. The visualizations show that the proposed model has a more balanced distribution of attention and gradients across image tokens.", "section": "Abstract"}, {"figure_path": "LxRmdXf72k/figures/figures_15_2.jpg", "caption": "Figure 1: Visualizations of (a) Attention Map and (b) Gradient Map of different models, including Non-causal ViT, Causal ViT, Causal Mamba and our De-focus Attention Network (Mamba-based). The results are from the 11th layer of ViT (12 in total) and 22nd layer of Mamba (24 in total). (a) The approximated attention maps of all image tokens: The row and column axes represent the query and key token index respectively. Brighter color indicates larger attention values. (b) The gradient maps of each image token input after back-propagation: Redder colors indicate larger gradient norms. See Appendix A for more visualizations on different layers.", "description": "This figure visualizes the attention maps and gradient maps of different vision models, including non-causal and causal versions of Vision Transformers (ViTs) and Mamba models.  It highlights the \"over-focus\" issue in 1D causal models, where attention concentrates on a small portion of visual tokens, and gradients are not effectively distributed. The De-focus Attention Network is shown to have a more balanced attention and gradient distribution.", "section": "1 Introduction"}, {"figure_path": "LxRmdXf72k/figures/figures_17_1.jpg", "caption": "Figure 7: Rearranging Procedure in Object Detection. This illustration presents a image divided into several 2x2 sections.", "description": "This figure illustrates how image patches are divided into sections and then rearranged before being fed into the model. The original image is divided into smaller 2x2 sections.  These sections are then scanned in a specific order (shown in the figure) and concatenated into a single sequence for processing by the 1D causal model.  This rearrangement is done to address the challenges of processing 2D images with a 1D causal model and improves performance on object detection tasks.", "section": "5.1 Experiment Setup"}]