{"references": [{"fullname_first_author": "A. Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-10-27", "reason": "This paper is foundational for the use of transformers in image recognition, providing the basis for many of the causal visual modeling approaches discussed in the target paper."}, {"fullname_first_author": "A. Gu", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "publication_date": "2023-12-06", "reason": "The Mamba model is central to the target paper's experimental setup, and its properties are leveraged extensively in the proposed de-focus attention network."}, {"fullname_first_author": "T. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This work highlights the power of causal language modeling and has influenced the exploration of causal modeling for visual data, a core theme in the target paper."}, {"fullname_first_author": "M. Chen", "paper_title": "Generative pretraining from pixels", "publication_date": "2020-06-01", "reason": "This paper explores the potential of autoregressive modeling for images, providing a direct comparison point for the 1D causal visual representation explored in the target paper."}, {"fullname_first_author": "A. Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "CLIP, introduced in this paper, is a benchmark for multi-modal understanding and is used in the target paper to evaluate the effectiveness of 1D causal visual representation."}]}