[{"type": "text", "text": "Carefully Blending Adversarial Training and Purification Improves Adversarial Robustness ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 In this work, we propose a novel adversarial defence mechanism for image classi  \n2 fication \u2013 CARSO \u2013 blending the paradigms of adversarial training and adversarial   \n3 purification in a synergistic robustness-enhancing way. The method builds upon   \n4 an adversarially-trained classifier, and learns to map its internal representation   \n5 associated with a potentially perturbed input onto a distribution of tentative clean   \n6 reconstructions. Multiple samples from such distribution are classified by the same   \n7 adversarially-trained model, and an aggregation of its outputs finally constitutes the   \n8 robust prediction of interest. Experimental evaluation by a well-established bench  \n9 mark of strong adaptive attacks, across different image datasets, shows that CARSO   \n0 is able to defend itself against adaptive end-to-end white-box attacks devised for   \n11 stochastic defences. Paying a modest clean accuracy toll, our method improves   \n2 by a significant margin the state-of-the-art for CIFAR-10, CIFAR-100, and   \n13 TINYIMAGENET- $200\\;\\ell_{\\infty}$ robust classification accuracy against AUTOATTACK. ", "page_idx": 0}, {"type": "text", "text": "14 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "15 Vulnerability to adversarial attacks [8, 57] \u2013 i.e. the presence of inputs, usually crafted on purpose,   \n16 capable of catastrophically altering the behaviour of high-dimensional models [9] \u2013 constitutes a major   \n17 hurdle towards ensuring the compliance of deep learning systems with the behaviour expected by   \n18 modellers and users, and their adoption in safety-critical scenarios or tightly-regulated environments.   \n19 This is particularly true for adversarially-perturbed inputs, where a norm-constrained perturbation \u2013   \n20 often hardly detectable by human inspection [48, 5] \u2013 is added to an otherwise legitimate input, with   \n21 the intention of eliciting an anomalous response [34].   \n22 Given the widespread nature of the issue [30], and the serious concerns raised about the safety and   \n23 reliability of data-learnt models in the lack of an appropriate mitigation [7], adversarial attacks have   \n24 been extensively studied. Yet, obtaining generally robust machine learning (ML) systems remains a   \n25 longstanding issue, and a major open challenge.   \n26 Research in the field has been driven by two opposing, yet complementary, efforts. On the one   \n27 hand, the study of failure modes in existing models and defences, with the goal of understanding   \n28 their origin and developing stronger attacks with varying degrees of knowledge and control over the   \n29 target system [57, 21, 44, 60]. On the other hand, the construction of increasingly capable defence   \n30 mechanisms. Although alternatives have been explored [15, 59, 11, 68], most of the latter is based on   \n31 adequately leveraging adversarial training [21, 42, 58, 49, 23, 31, 54, 62, 18, 47], i.e. training a ML   \n32 model on a dataset composed of (or enriched with) adversarially-perturbed inputs associated with   \n33 their correct, pre-perturbation labels. In fact, adversarial training has been the only technique capable   \n34 of consistently providing an acceptable level of defence [24], while still incrementally improving up   \n35 to the current state-of-the-art [18, 47].   \n36 Another defensive approach is that of adversarial purification [53, 66], where a generative model is   \n37 used \u2013 similarly to denoising \u2013 to recover a perturbation-free version of the input before classification   \n38 is performed. Nonetheless, such attempts have generally fallen short of expectations due to inherent   \n39 limitations of the generative models used in early attempts [45], or due to decreases in robust   \n40 accuracy1 when attacked end-to-end [25] \u2013 resulting in subpar robustness if the defensive structure is   \n41 known to the adversary [60]. More recently, the rise of diffusion-based generative models [28] and   \n42 their use for purification have enabled more successful results of this kind [45, 13] \u2013 although at the   \n43 cost of much longer training and inference times, and a much brittler robustness evaluation [13, 38].   \n44 In this work, we design a novel adversarial defence for supervised image classification, dubbed   \n45 CARSO (i.e., Counter-Adversarial Recall of Synthetic Observations). The approach relies on an   \n46 adversarially-trained classifier (called hereinafter simply the classifier), endowed with a stochastic   \n47 generative model (called hereinafter the purifier). Upon classification of a potentially-perturbed input,   \n48 the latter learns to generate \u2013 from the tensor2 of (pre)activations registered at neuron level in the   \n49 former \u2013 samples from a distribution of plausible, perturbation-free reconstructions. At inference   \n50 time, some of these samples are classified by the very same classifier, and the original input is robustly   \n51 labelled by aggregating its many outputs. This method \u2013 to the best of our knowledge the first attempt   \n52 to organically merge the adversarial training and purification paradigms \u2013 avoids the vulnerability   \n53 pitfalls typical of the mere stacking of a purifier and a classifier [25], while still being able to take   \n54 advantage of independent incremental improvements to adversarial training or generative modelling.   \n55 An empirical assessment3 of the defence in the $\\ell_{\\infty}$ white-box setting is provided, using a conditional   \n56 [56, 64] variational autoencoder [32, 50] as the purifier and existing state-of-the-art adversarially   \n57 pre-trained models as classifiers. Such choices are meant to give existing approaches \u2013 and the   \n58 adversary attacking our architecture end-to-end as part of the assessment \u2013 the strongest advantage   \n59 possible. Yet, in all scenarios considered, CARSO improves significantly the robustness of the   \n60 pre-trained classifier \u2013 even against attacks specifically devised to fool stochastic defences like ours.   \n61 Remarkably, with a modest clean accuracy toll, our method improves by a significant margin the   \n62 current state-of-the-art for CIFAR-10 [33], CIFAR-100 [33], and TINYIMAGENET-200 [14] $\\ell_{\\infty}$   \n63 robust classification accuracy against AUTOATTACK [17]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "64 In summary, the paper makes the following contributions: ", "page_idx": 1}, {"type": "text", "text": "65 \u2022 The description of CARSO, a novel adversarial defence method synergistically blending   \n66 adversarial training and adversarial purification;   \n67 \u2022 A collection of relevant technical details fundamental to its successful training and use,   \n68 originally developed for the purifier being a conditional variational autoencoder \u2013 but   \n69 applicable to more general scenarios as well;   \n70 \u2022 Experimental assessment of the method, against standardised benchmark adversarial attacks   \n71 \u2013 showing higher robust accuracy w.r.t. to existing state-of-the-art adversarial training and   \n72 purification approaches.   \n73 The rest of the manuscript is structured as follows. In section 2 we provide an overview of selected   \n74 contributions in the fields of adversarial training and purification-based defences \u2013 with focus on   \n75 image classification. In section 3, a deeper analysis is given of two integral parts of our experimental   \n76 assessment: PGD adversarial training and conditional variational autoencoders. Section 4 is devoted   \n77 to the intuition behind CARSO, its architectural description, and the relevant technical details that   \n78 allow it to work. Section 5 contains details about the experimental setup, results, comments, and   \n79 limitations. Section 6 concludes the paper and outlines directions of future development. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "80 2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "81 Adversarial training as a defence The idea of training a model on adversarially-generated examples   \n82 as a way to make it more robust can be traced back to the very beginning of research in the area.   \n83 The seminal [57] proposes to perform training on a mixed collection of clean and adversarial data,   \n84 generated beforehand.   \n85 The introduction of FGSM [21] enables the efficient generation of adversarial examples along the   \n86 training, with a single normalised gradient step. Its iterative counterpart PGD [42] \u2013 discussed   \n87 in section 3 and Appendix A \u2013 significantly improves the effectiveness of adversarial examples   \n88 produced, making it still the de facto standard for the synthesis of adversarial training inputs [24].   \n89 Further incremental improvements have also been developed, some focused specifically on robustness   \n90 assessment (e.g. adaptive-stepsize variants, as in [17]).   \n91 The most recent adversarial training protocols further rely on synthetic data to increase the numerosity   \n92 of training datapoints [23, 49, 62, 18, 47], and adopt adjusted loss functions to balance robustness and   \n93 accuracy [67] or generally foster the learning process [18]. The entire model architecture may also be   \n94 tuned specifically for the sake of robustness enhancement [47]. At least some of such ingredients are   \n95 often required to reach the current state-of-the-art in robust accuracy via adversarial training.   \n96 Purification as a defence Amongst the first attempts of purification-based adversarial defence,   \n97 [25] investigates the use of denoising autoencoders [61] to recover examples free from adversarial   \n98 perturbations. Despite its effectiveness in the denoising task, the method may indeed increase   \n99 the vulnerability of the system when attacks are generated against it end-to-end. The contextually   \n100 proposed improvement adds a smoothness penalty to the reconstruction loss, partially mitigating such   \n101 downside [25]. Similar in spirit, [39] tackles the issue by computing the reconstruction loss between   \n102 the last-layers representations of the frozen-weights attacked classifier, respectively receiving, as   \n103 input, the clean and the tentatively denoised example.   \n104 In [52], Generative Adversarial Networks (GANs) [22] learnt on clean data are used at inference time   \n105 to find a plausible synthetic example \u2013 close to the perturbed input \u2013 belonging to the unperturbed   \n106 data manifold. Despite encouraging results, the delicate training process of GANs and the existence   \n107 of known failure modes [70] limit the applicability of the method. More recently, a similar approach   \n108 [27] employing energy-based models [37] suffered from poor sample quality [45].   \n109 Purification approaches based on (conditional) variational autoencoders include [29] and [53]. Very   \n110 recently, a technique combining variational manifold learning with a test-time iterative purification   \n111 procedure has also been proposed [65].   \n112 Finally, already-mentioned techniques relying on score- [66] and diffusion- based [45, 13] models   \n113 have also been developed, with generally favourable results \u2013 often balanced in practice by longer   \n114 training and inference times, and a much more fragile robustness assessment [13, 38]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "115 3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "116 PGD adversarial training The task of finding model parameters robust to adversarial perturbations   \n117 is framed by [42] as a min-max optimisation problem seeking to minimise adversarial risk. The   \n118 inner optimisation (i.e., the generation of worst-case adversarial examples) is solved by an iterative   \n119 algorithm \u2013 Projected Gradient Descent \u2013 interleaving gradient ascent steps in input space with   \n120 the eventual projection on the shell of an $\\epsilon_{}$ -ball centred around an input datapoint, thus imposing a   \n121 perturbation strength constraint.   \n122 In this manuscript, we will use the shorthand notation $\\epsilon_{p}$ to denote $\\ell_{p}$ norm-bound perturbations of   \n123 maximum magnitude $\\epsilon$ .   \n124 The formal details of such method are provided in Appendix A.   \n125 (Conditional) variational autoencoders Variational autoencoders (VAEs) [32, 50] allow the learn  \n126 ing from data of approximate generative latent-variable models of the form $p(\\pmb{x},z)=p(\\pmb{x}\\,|\\,\\pmb{z})p(\\pmb{z})$ ,   \n127 whose likelihood and posterior are approximately parameterised by deep artificial neural networks   \n128 (ANNs). The problem is cast as the maximisation of a variational lower bound.   \n129 In practice, optimisation is performed iteratively \u2013 on a loss function given by the linear mixture of   \n130 data-reconstruction loss and empirical $K L$ divergence $w.r t$ . a chosen prior, computed on mini-batches   \n131 of data.   \n132 Conditional Variational Autoencoders [56, 64] extend VAEs by attaching a conditioning tensor c \u2013   \n133 expressing specific characteristics of each example \u2013 to both $\\textbf{\\em x}$ and $_{z}$ during training. This allows the   \n134 learning of a decoder model capable of conditional data generation. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "136 4 Structure of CARSO ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "137 The core ideas informing the design of our method are driven more by first principles rather than   \n138 arising from specific contingent requirements. This section discusses such ideas, the architectural   \n139 details of CARSO, and a group of technical aspects fundamental to its training and inference processes. ", "page_idx": 3}, {"type": "text", "text": "140 4.1 Architectural overview and principle of operation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "141 From an architectural point of view, CARSO is essentially composed of two ANN models \u2013 a classifier   \n142 and a purifier \u2013 operating in close synergy. The former is trained on a given classification task,   \n143 whose inputs might be adversarially corrupted at inference time. The latter learns to generate samples   \nfrom a distribution of potential input reconstructions, tentatively free from adversarial perturbations.   \nCrucially, the purifier has only access to the internal representation of the classifier \u2013 and not even   \n146 directly to the perturbed input \u2013 to perform its task.   \n147 During inference, for each input, the internal representation of the classifier is used by the purifier to   \n148 synthesise a collection of tentatively unperturbed input reconstructions. Those are classified by the   \n149 same classifier, and the resulting outputs are aggregated into a final robust prediction.   \n150 There are no specific requirements for the classifier, whose training is completely independent of   \n151 the use of the model as part of CARSO. However, training it adversarially improves significantly   \n152 the clean accuracy of the overall system, allowing it to benefit from established adversarial training   \n153 techniques.   \n154 The purifier is also independent of specific architectural choices, provided it is capable of stochastic   \n155 conditional data generation at inference time, with the internal representation of the classifier used as   \n156 the conditioning set.   \n157 In the rest of the paper, we employ a state-of-the-art adversarially pre-trained WIDERESNET model   \n158 as the classifier, and a purpose-built conditional variational autoencoder as the purifier, the latter   \n159 operating decoder-only during inference. Such choice was driven by the deliberate intention to assess   \n160 the adversarial robustness of our method in its worst-case scenario against a white-box attacker, and   \n161 with the least advantage compared to existing approaches based solely on adversarial training.   \n162 In fact, the decoder of a conditional VAE allows for exact algorithmic differentiability [6] w.r.t. its   \n163 conditioning set, thus averting the need for backward-pass approximation [2] in generating end-to-end   \n164 adversarial attacks against the entire system, and preventing (un)intentional robustness by gradient   \n165 obfuscation [2]. The same cannot be said [13] for more capable and modern purification models,   \n166 such as those based e.g. on diffusive processes, whose robustness assessment is still in the process of   \n167 being understood [38].   \n168 A downside of such choice is represented by the reduced effectiveness of the decoder in the synthesis   \n169 of complex data, due to well-known model limitations. In fact, we experimentally observe a modest   \n170 increase in reconstruction cost for non-perturbed inputs, which in turn may limit the clean accuracy of   \n171 the entire system. Nevertheless, we defend the need for a fair and transparent robustness evaluation,   \n172 such as the one provided by the use of a VAE-based purifier, in the evaluation of any novel architecture  \n173 agnostic adversarial defence technique.   \n174 A diagram of the whole architecture is shown in Figure 1, and its detailed principles of operation are   \n175 recapped below.   \n176 Training At training time, adversarially-perturbed examples are generated against the classifier,   \n177 and fed to it. The tensors containing the classifier (pre)activations across the network are then   \n178 extracted. Finally, the conditional $V\\!A E$ serving as the purifier is trained on perturbation-free input   \n179 reconstruction, conditional on the corresponding previously extracted internal representations, and   \n180 using pre-perturbation examples as targets.   \n181 Upon completion of the training process, the encoder network may be discarded as it will not be used   \n182 for inference. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "image", "img_path": "s5Y3M5l1qg/tmp/d7a325312ad3d67662b6e65f06bf1b38764c2d82fbecc7a08fc8264c0a739428.jpg", "img_caption": ["Figure 1: Schematic representation of the CARSO architecture used in the experimental phase of this work. The subnetwork bordered by the red dashed line is used only during the training of the purifier. The subnetwork bordered by the blue dashed line is re-evaluated on different random samples $z_{i}$ and the resulting individual $\\hat{y}_{i}$ are aggregated into $\\hat{y}_{\\mathrm{rob}}$ . The classifier $f(\\cdot;\\pmb\\theta)$ is always kept frozen; the remaining network is trained on $\\mathcal{L}_{\\mathrm{VAE}}(\\pmb{x},\\hat{\\pmb{x}})$ . More precise details on the functioning of the networks are provided in subsection 4.1. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "183 Inference The example requiring classification is fed to the classifier. Its corresponding internal   \n184 representation is extracted and used to condition the generative process described by the decoder   \n185 of the VAE. Stochastic latent variables are repeatedly sampled from the original priors, which are   \n186 given by an i.i.d. multivariate Standard Normal distribution. Each element in the resulting set of   \n187 reconstructed inputs is classified by the same classifier, and the individually predicted class logits are   \n188 aggregated. The result of such aggregation constitutes the robust prediction of the input class.   \n189 Remarkably, the only link between the initial potentially-perturbed input and the resulting purified reconstructions (and thus the predicted class) is through the internal representation of the classifier, which serves as a featurisation of the original input. The whole process is exactly differentiable endto-end, and the only potential hurdle to the generation of adversarial attacks against the entire system   \n193 is the stochastic nature of the decoding \u2013 which is easily tackled by Expectation over Transformation   \n194 [3]. ", "page_idx": 4}, {"type": "text", "text": "195 4.2 A first-principles justification ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "196 If we consider a trained ANN classifier, subject to a successful adversarial attack by means of a   \n197 slightly perturbed example, we observe that \u2013 both in terms of $\\ell_{p}$ magnitude and human perception   \n198 \u2013 a small variation on the input side of the network is amplified to a significant amount on the   \n199 output side, thanks to the layerwise processing by the model. Given the deterministic nature of such   \n200 processing at inference time, we speculate that the trace obtained by sequentially collecting the   \n201 (pre)activation values within the network, along the forward pass, constitutes a richer characterisation   \n202 of such an amplification process compared to the knowledge of the input alone. Indeed, as we do, it   \n203 is possible to learn a direct mapping from such featurisation of the input, to a distribution of possible   \n204 perturbation-free input reconstructions \u2013 taking advantage of such characterisation. ", "page_idx": 4}, {"type": "text", "text": "205 4.3 Hierarchical input and internal representation encoding ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "206 Training a conditional VAE requires [56] that the conditioning set $^c$ is concatenated to the input $\\textbf{\\em x}$   \n207 before encoding occurs, and to the sample of latent variables $_{\\textit{z}}$ right before decoding. The same is   \n208 also true, with the suitable adjustments, for any conditional generative approach where the target and   \n209 the conditioning set must be processed jointly.   \n210 In order to ensure the usability and scalability of CARSO across the widest range of input data and   \n211 classifier models, we propose to perform such processing in a hierarchical and partially disjoint   \n212 fashion between the input and the conditioning set. In principle, the encoding of $\\textbf{\\em x}$ and $^c$ can be   \n213 performed by two different and independent subnetworks, until some form of joint processing must   \n214 occur. This allows to retain the overall architectural structure of the purifier, while having finer-grained   \n215 control over the inductive biases [43] deemed the most suitable for the respective variables.   \n216 In the experimental phase of our work, we encode the two variables independently. The input   \n217 is compressed by a multilayer convolutional neural network (CNN). The internal representation \u2013   \n218 which in our case is composed of differently sized multi-channel images \u2013 is processed layer by   \n219 layer by independent multilayer CNNs (responsible for encoding local information), whose flattened   \n220 outputs are finally concatenated and compressed by a fully-connected layer (modelling inter-layer   \n221 correlations in the representation). The resulting compressed input and conditioning set are then   \n222 further concatenated and jointly encoded by a fully-connected network (FCN).   \n223 In order to use the VAE decoder at inference time, the entire compression machinery for the condi  \n224 tioning set must be preserved after training, and used to encode the internal representations extracted.   \n225 The equivalent input encoder may be discarded instead. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "226 4.4 Adversarially-balanced batches ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "227 Training the purifier in representation-conditional input reconstruction requires having access to   \nadversarially-perturbed examples generated against the classifier, and to the corresponding clean data.   \n229 Specifically, we use as input a mixture of clean and adversarially perturbed examples, and the clean   \n230 input as the target.   \n231 Within each epoch, the training set of interest is shuffled [51, 10], and only a fixed fraction of each   \n232 resulting batch is adversarially perturbed. Calling $\\epsilon$ the maximum $\\ell_{p}$ perturbation norm bound for   \n233 the threat model against which the classifier was adversarially pre-trained, the portion of perturbed   \n234 examples is generated by an even split of $\\mathrm{FGSM_{\\epsilon/2}}$ , $\\mathrm{PGD}_{\\epsilon/2}$ , $\\mathrm{FGSM}_{\\epsilon}$ , and $\\mathrm{PGD}_{\\epsilon}$ attacks.   \n235 Any smaller subset of attack types and strengths, or a detailedly unbalanced batch composition,   \n236 always experimentally results in a worse performing purification model. More details justifying such   \n237 choice are provided in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "238 4.5 Robust aggregation strategy ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "239 At inference time, many different input reconstructions are classified by the classifier, and the   \n240 respective outputs concur to the settlement of a robust prediction.   \n241 Calling $l_{i}^{\\alpha}$ the output logit associated with class $i\\in\\{1,\\ldots,C\\}$ in the prediction by the classifier on   \n242 sample $\\dot{\\alpha}\\in\\{1,\\ldots,N\\}$ , we adopt the following aggregation strategy: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\nP_{i}:=\\frac{1}{Z}\\prod_{\\alpha=1}^{N}e^{e^{l_{i}^{\\alpha}}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "243 with $P_{i}$ being the aggregated probability of membership in class $i,Z$ a normalisation constant such   \n244 that $\\textstyle\\sum_{i=1}^{C}P_{i}=1$ , and $e$ Euler\u2019s number.   \n245 Such choice produces a robust prediction much harder to take over in the event that an adversary   \n246 selectively targets a specific input reconstruction. A heuristic justification for this property is given in   \n247 Appendix D. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "248 5 Experimental assessment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "249 Experimental evaluation of our method is carried out in terms of robust and clean image classification   \n250 accuracy within three different scenarios $a,b$ and $c$ ), determined by the specific classification task.   \n251 The white-box threat model with a fixed $\\ell_{\\infty}$ norm bound is assumed throughout, as it generally   \n252 constitutes the most demanding setup for adversarial defences. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "253 5.1 Setup", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "254 Data The CIFAR-10 [33] dataset is used in scenario (a), the CIFAR-100 [33] dataset is used in   \n255 scenario (b), whereas the TINYIMAGENET-200 [14] dataset is used in scenario (c).   \n256 Architectures A WIDERESNET-28-10 model is used as the classifier, adversarially pre-trained on   \n257 the respective dataset \u2013 the only difference between scenarios being the number of output logits: 10   \n258 in scenario (a), 100 in scenario $(b)$ , and 200 in scenario (c).   \n259 The purifier is composed of a conditional VAE, processing inputs and internal representations in a   \n260 partially disjoint fashion, as explained in subsection 4.3. The input is compressed by a two-layer   \n261 CNN; the internal representation is instead processed layerwise by independent CNNs (three-layered   \n262 in scenarios (a) and $(b)$ , four-layered in scenario (c)) whose outputs are then concatenated and   \n263 compressed by a fully-connected layer. A final two-layer FCN jointly encodes the compressed input   \n264 and conditioning set, after the concatenation of the two. A six-layer deconvolutional network is used   \n265 as the decoder. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "266 More precise details on all architectures are given in Appendix E. ", "page_idx": 6}, {"type": "text", "text": "267 Outer minimisation In scenarios (a) and $(b)$ , the classifier is trained according to [18]; in scenario   \n268 (c), according to [62]. Classifiers were always acquired as pre-trained models, using publicly available   \n269 weights provided by the respective authors.   \n270 The purifier is trained on the $V\\!A E$ loss, using summed pixel-wise channel-wise binary cross-entropy   \n271 as the reconstruction cost. Optimisation is performed by RADAM+LOOKAHEAD [41, 69] with a   \n272 learning rate schedule that presents a linear warm-up, a plateau phase, and a linear annealing [55].   \n273 To promote the learning of meaningful reconstructions during the initial phases of training, the $K L$   \n274 divergence term in the VAE loss is suppressed for an initial number of epochs. Afterwards, it is   \n275 linearly modulated up to its actual value, during a fixed number of epochs ( $\\beta$ increase) [26]. The   \n276 initial and final epochs of such modulation are reported in Table 14. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "277 Additional scenario-specific details are provided in Appendix E. ", "page_idx": 6}, {"type": "text", "text": "278 Inner minimisation $\\epsilon_{\\infty}={}^{8}/255$ is set as the perturbation norm bound. ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "279 Adversarial examples against the purifier are obtained, as explained in subsection 4.4, by $\\mathrm{FGSM_{\\epsilon/2}}$ ,   \n280 $\\mathrm{PGD_{\\epsilon/2}}$ , $\\mathrm{FGSM_{\\epsilon}}$ , and $\\mathrm{PGD}_{\\epsilon}$ , in a class-untargeted fashion on the cross-entropy loss. In the case of   \n281 PGD, gradient ascent with a step size of $\\alpha=0.01$ is used.   \n282 The complete details and hyperparameters of the attacks are described in Appendix E.   \n283 Evaluation In each scenario, we report the clean and robust test-set accuracy \u2013 the latter by means   \n284 of AUTOATTACK [17] \u2013 of the classifier and the corresponding CARSO architecture.   \n285 For the classifier alone, the standard version of AUTOATTACK (AA) is used: i.e., the worst-case   \n286 accuracy on a mixture of AUTOPGD on the cross-entropy loss [17] with 100 steps, AUTOPGD on   \n287 the difference of logits ratio loss [17] with 100 steps, FAB [16] with 100 steps, and the black-box   \n288 SQUARE attack [1] with 5000 queries.   \n289 In the evaluation of the CARSO architecture, the number of reconstructed samples per input is set to 8,   \n290 the logits are aggregated as explained in subsection 4.5, and the output class is finally selected as the   \n291 arg max of the aggregation. Due to the stochastic nature of the purifier, robust accuracy is assessed   \n292 by a version of AUTOATTACK suitable for stochastic defences (randAA) \u2013 composed of AUTOPGD   \n293 on the cross-entropy and difference of logits ratio losses, across 20 Expectation over Transformation   \n294 (EOT) [3] iterations with 100 gradient ascent steps each.   \n295 Computational infrastructure All experiments were performed on an NVIDIA DGX A100 system.   \n296 Training in scenarios (a) and (c) was run on 8 NVIDIA A100 GPUs with 40 GB of dedicated memory   \n297 each; in scenario (b) 4 of such devices were used. Elapsed real training time for the purifier in all   \n298 scenarios is reported in Table 1. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "s5Y3M5l1qg/tmp/c237f793b7e01ac6c0b642ce6ce5b2972489d6a964045e5f8ae550042aa31735.jpg", "table_caption": ["Table 1: Elapsed real running time for training the purifier in the different scenarios considered. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "299 5.2 Results and discussion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "300 An analysis of the experimental results is provided in the subsection that follows, whereas their   \n301 systematic exposition is given in Table 2. ", "page_idx": 7}, {"type": "text", "text": "Table 2: Clean (results in italic) and adversarial (results in upright) accuracy for the different models and datasets used in the respective scenarios. The following abbreviations are used: Scen: scenario considered; AT/Cl: clean accuracy for the adversarially-pretrained model used as the classifier, when considered alone; C/Cl: clean accuracy for the CARSO architecture; AT/AA: robust accuracy (by the means of AUTOATTACK) for the adversarially-pretrained model used as the classifier, when considered alone; C/randAA: robust accuracy for the CARSO architecture, when attacked end-to-end by AUTOATTACK for randomised defences; Best AT/AA: best robust accuracy result for the respective dataset (by the means of AUTOATTACK), obtained by adversarial training alone (any model); Best P/AA: best robust accuracy result for the respective dataset (by the means of AUTOATTACK), obtained by adversarial purification (any model). Robust accuracies in round brackets are obtained using the PGD $^+$ EOT [38] pipeline, developed for diffusion-based purifiers. The best clean and robust accuracies per dataset are shown in bold. The clean accuracies for the models referred to in the Best columns are shown in Table 15 (in Appendix F). ", "page_idx": 7}, {"type": "table", "img_path": "s5Y3M5l1qg/tmp/52c498513dd3b926b9c23bdc23f16b1886e2010520829b8576ddd723953397fb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "302 Scenario (a) Comparing the robust accuracy of the classifier model used in scenario (a) [18]   \n303 with that resulting from the inclusion of the same model in the CARSO architecture, we observe   \n304 a $+8.4\\%$ increase. This is counterbalanced by a $-5.6\\%$ clean accuracy toll. The same version of   \n305 CARSO further provides a $+5.03$ robustness increase $w.r t$ . the current best AT-trained model [47]   \n306 that employs $a\\sim3\\times$ larger RAWIDERESNET-70-16 model.   \n307 In addition, our method provides a remarkable $+9.72\\%$ increase in robust accuracy w.r.t. to the best   \n308 adversarial purification approach [40], a diffusion-based purifier. However, the comparison is not as   \n309 straightforward. In fact, the paper [40] reports a robust accuracy of $78.12\\%$ using AUTOATTACK on   \n310 the gradients obtained via the adjoint method [45]. As noted in [38], such evaluation (which uses the   \n311 version of AUTOATTACK that is unsuitable for stochastic defences) leads to a large overestimation of   \n312 the robustness of diffusive purifiers. As suggested in [38], the authors of [40] re-evaluate the robust   \n313 accuracy according to a more suitable pipeline $\\scriptstyle\\mathrm{PGD+EOT}$ , whose hyperparameters are shown in   \n314 Table 12), obtaining a much lower robust accuracy of $66.41\\%$ . Consequently, we repeat the same   \n315 evaluation for CARSO and compare the worst-case robustness amongst the two. In line with typical   \n316 AT methods, and unlike diffusive purification, the robustness of CARSO assessed by means of randAA   \n317 is still lower w.r.t. than achieved by $\\scriptstyle\\mathrm{PGD+EOT}$ .   \n318 Scenario (b) Moving to scenario $(b)$ , CARSO achieves a robust accuracy increase of $+27.47\\%$ w.r.t.   \n319 the classifier alone [18], balanced by a $5.79\\%$ decrease in clean accuracy. Our approach also improves   \n320 upon the robust accuracy of the best AT-trained model [62] (WIDERESNET-70-16) by $23.98\\%$ . In   \n321 the absence of a reliable robustness evaluation by means of $\\mathrm{PGD+EOT}$ for the best purification-based   \n322 method [40], we still obtain a $+20.25\\%$ increase in robust accuracy upon its (largely overestimated)   \n323 AA result.   \n324 Scenario (c) In scenario (c), CARSO improves upon the classifier alone [62] (which is also the   \n325 best AT-based approach for TINYIMAGENET-200) by $+22.26\\%$ . A significant clean accuracy toll is   \n326 imposed by the relative complexity of the dataset, i.e. $-8.87\\%$ . In this setting, we lack any additional   \n327 purification-based methods.   \n328 Assessing the impact of gradient obfuscation Although the architecture of CARSO is algorith  \n329 mically differentiable end-to-end \u2013 and the integrated diagnostics of the randAA routines raised no   \n330 warnings during the assessment \u2013 we additionally guard against the eventual gradient obfuscation [2]   \n331 induced by our method by repeating the evaluation at $\\epsilon_{\\infty}=0.95$ , verifying that the resulting robust   \n332 accuracy stays below random chance [12]. Results are shown in Table 3. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Table 3: Robust classification accuracy against AUTOATTACK, for $\\epsilon_{\\infty}=0.95$ , as a way to assess the (lack of) impact of gradient obfuscation on robust accuracy evaluation. ", "page_idx": 8}, {"type": "table", "img_path": "s5Y3M5l1qg/tmp/2ee711002d92851d2813671d83b77d2f65c19c5e01d6d0b9109355e002c9c593.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "333 5.3 Limitations and open problems ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "334 In line with recent research aiming at the development of robust defences against multiple perturb  \n335 ations [20, 35], our method determines a decrease in clean accuracy w.r.t. the original model on   \n336 which it is built upon \u2013 especially in scenario (c) as the complexity of the dataset increases. This   \n337 phenomenon is partly dependent on the choice of a VAE as the generative purification model, a   \n338 requirement for the fairest evaluation possible in terms of robustness.   \n339 Yet, the issue remains open: is it possible to devise a CARSO-like architecture capable of the same   \n340 \u2013 if not better \u2013 robust behaviour, which is also competitively accurate on clean inputs? Potential   \n341 avenues for future research may involve the development of CARSO-like architectures in which   \n342 representation-conditional data generation is obtained by means of diffusion or score-based models.   \n343 Alternatively, incremental developments aimed at improving the cross-talk between the purifier and   \n344 the final classifier may be pursued.   \n345 Lastly, the scalability of CARSO could be strongly improved by determining whether the internal   \n346 representation used in conditional data generation may be restricted to a smaller subset of layers,   \n347 while still maintaining the general robustness of the method. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "348 6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "349 In this work, we presented a novel adversarial defence mechanism tightly integrating input purification,   \n350 and classification by an adversarially-trained model \u2013 in the form of representation-conditional data   \n351 purification. Our method is able to improve upon the current state-of-the-art in CIFAR-10, CIFAR  \n352 100, and TINYIMAGENET $\\ell_{\\infty}$ robust classification, w.r.t. both adversarial training and purification   \n353 approaches alone.   \n354 Such results suggest a new synergistic strategy to achieve adversarial robustness in visual tasks and   \n355 motivate future research on the application of the same design principles to different models and   \n356 types of data. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "357 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "358 [1] Maksym Andriushchenko et al. \u2018Square Attack: a query-efficient black-box adversarial attack via random   \n359 search\u2019. In: 16th European Conference on Computer Vision. 2020.   \n360 [2] Anish Athalye, Nicholas Carlini and David Wagner. \u2018Obfuscated Gradients Give a False Sense of Security:   \n361 Circumventing Defenses to Adversarial Examples\u2019. In: Proceedings of the International Conference on   \n362 Machine Learning. 2018.   \n363 [3] Anish Athalye et al. \u2018Synthesizing Robust Adversarial Examples\u2019. In: Proceedings of the International   \n364 Conference on Machine Learning. 2018.   \n365 [4] Emanuele Ballarin. ebtorch: Collection of PyTorch additions, extensions, utilities, uses and abuses.   \n366 2024. URL: https://github.com/emaballarin/ebtorch.   \n367 [5] Vincent Ballet et al. \u2018Imperceptible Adversarial Attacks on Tabular Data\u2019. In: Thirty-third Conference on   \n368 Neural Information Processing Systems, Workshop on Robust AI in Financial Services: Data, Fairness,   \n369 Explainability, Trustworthiness, and Privacy (Robust AI in FS). 2019.   \n370 [6] Atilim Gunes Baydin et al. \u2018Automatic differentiation in machine learning: a survey\u2019. In: The Journal of   \n371 Machine Learning Research 18.153 (2018), pp. 1\u201343.   \n372 [7] Battista Biggio and Fabio Roli. \u2018Wild patterns: Ten years after the rise of adversarial machine learning\u2019.   \n373 In: Pattern Recognition 84 (2018), pp. 317\u2013331.   \n374 [8] Battista Biggio et al. \u2018Evasion Attacks against Machine Learning at Test Time\u2019. In: Proceedings of the   \n375 2013th European Conference on Machine Learning and Knowledge Discovery in Databases - Volume   \n376 Part III. 2013.   \n377 [9] Luca Bortolussi and Guido Sanguinetti. Intrinsic Geometric Vulnerability of High-Dimensional Artificial   \n378 Intelligence. 2018. arXiv: 1811.03571.   \n379 [10] L\u00e9on Bottou. \u2018On-Line Algorithms and Stochastic Approximations\u2019. In: On-Line Learning in Neural   \n380 Networks. Cambridge University Press, 1999. Chap. 2.   \n381 [11] Ginevra Carbone et al. \u2018Robustness of Bayesian Neural Networks to Gradient-Based Attacks\u2019. In:   \n382 Advances in Neural Information Processing Systems. 2020.   \n383 [12] Nicholas Carlini et al. On Evaluating Adversarial Robustness. 2019. arXiv: 1902.06705.   \n384 [13] Huanran Chen et al. Robust Classification via a Single Diffusion Model. 2023. arXiv: 2305.15241.   \n385 [14] Patryk Chrabaszcz, Ilya Loshchilov and Frank Hutter. A Downsampled Variant of ImageNet as an   \n386 Alternative to the CIFAR datasets. 2017. arXiv: 1707.08819.   \n387 [15] Moustapha Cisse et al. \u2018Parseval Networks: Improving Robustness to Adversarial Examples\u2019. In: Pro  \n388 ceedings of the International Conference on Machine Learning. 2017.   \n389 [16] Francesco Croce and Matthias Hein. Minimally distorted Adversarial Examples with a Fast Adaptive   \n390 Boundary Attack. 2020. arXiv: 1907.02044.   \n391 [17] Francesco Croce and Matthias Hein. \u2018Reliable Evaluation of Adversarial Robustness with an Ensemble of   \n392 Diverse Parameter-free Attacks\u2019. In: Proceedings of the International Conference on Machine Learning.   \n393 2020.   \n394 [18] Jiequan Cui et al. Decoupled Kullback-Leibler Divergence Loss. 2023. arXiv: 2305.13948.   \n395 [19] Gavin Weiguang Ding, Luyu Wang and Xiaomeng Jin. AdverTorch v0.1: An Adversarial Robustness   \n396 Toolbox based on PyTorch. 2019. arXiv: 1902.07623.   \n397 [20] Hadi M. Dolatabadi, Sarah Erfani and Christopher Leckie. $\\cdot\\ell_{\\infty}$ -Robustness and Beyond: Unleashing   \n398 Efficient Adversarial Training\u2019. In: 18th European Conference on Computer Vision. 2022.   \n399 [21] Ian Goodfellow, Jonathon Shlens and Christian Szegedy. \u2018Explaining and Harnessing Adversarial Ex  \n400 amples\u2019. In: International Conference on Learning Representations. 2015.   \n401 [22] Ian Goodfellow et al. \u2018Generative Adversarial Nets\u2019. In: Advances in Neural Information Processing   \n402 Systems. 2014.   \n403 [23] Sven Gowal et al. \u2018Improving Robustness using Generated Data\u2019. In: Advances in Neural Information   \n404 Processing Systems. 2021.   \n405 [24] Sven Gowal et al. Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial   \n406 Examples. 2020. arXiv: 2010.03593.   \n407 [25] Shixiang Gu and Luca Rigazio. \u2018Towards Deep Neural Network Architectures Robust to Adversarial   \n408 Examples\u2019. In: Workshop Track of the International Conference on Learning Representations. 2015.   \n409 [26] Irina Higgins et al. \u2018beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Frame  \n410 work\u2019. In: International Conference on Learning Representations. 2017.   \n411 [27] Mitch Hill, Jonathan Mitchell and Song-Chun Zhu. \u2018Stochastic Security: Adversarial Defense Using Long  \n412 Run Dynamics of Energy-Based Models\u2019. In: International Conference on Learning Representations.   \n413 2021.   \n414 [28] Chin-Wei Huang, Jae Hyun Lim and Aaron C Courville. \u2018A Variational Perspective on Diffusion-Based   \n415 Generative Models and Score Matching\u2019. In: Advances in Neural Information Processing Systems. 2021.   \n416 [29] Uiwon Hwang et al. \u2018PuVAE: A Variational Autoencoder to Purify Adversarial Examples\u2019. In: IEEE   \n417 Access. 2019.   \n418 [30] Andrew Ilyas et al. \u2018Adversarial Examples Are Not Bugs, They Are Features\u2019. In: Advances in Neural   \n419 Information Processing Systems. 2019.   \n420 [31] Xiaojun Jia et al. \u2018LAS-AT: Adversarial Training With Learnable Attack Strategy\u2019. In: Proceedings of the   \n421 IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.   \n422 [32] Diederik P. Kingma and Max Welling. \u2018Auto-Encoding Variational Bayes\u2019. In: International Conference   \n423 on Learning Representations. 2014.   \n424 [33] Alex Krizhevsky. \u2018Learning Multiple Layers of Features from Tiny Images\u2019. In: 2009.   \n425 [34] Alexey Kurakin, Ian J. Goodfellow and Samy Bengio. \u2018Adversarial Examples in the Physical World\u2019. In:   \n426 Artificial Intelligence Safety and Security (2018).   \n427 [35] Cassidy Laidlaw, Sahil Singla and Soheil Feizi. \u2018Perceptual Adversarial Robustness: Defense Against   \n428 Unseen Threat Models\u2019. In: International Conference on Learning Representations. 2021.   \n429 [36] Yann LeCun and Corinna Cortes. The MNIST handwritten digit database. 2010.   \n430 [37] Yann LeCun et al. \u2018A tutorial on energy-based learning\u2019. In: Predicting structured data. MIT Press, 2006.   \n431 Chap. 1.   \n432 [38] Minjong Lee and Dongwoo Kim. \u2018Robust Evaluation of Diffusion-Based Adversarial Purification\u2019. In:   \n433 International Conference on Computer Vision. 2024.   \n434 [39] Fangzhou Liao et al. \u2018Defense Against Adversarial Attacks Using High-Level Representation Guided   \n435 Denoiser\u2019. In: IEEE Conference on Computer Vision and Pattern Recognition. 2018.   \n436 [40] Guang Lin et al. Robust Diffusion Models for Adversarial Purification. 2024. arXiv: 2403.16067.   \n437 [41] Liyuan Liu et al. \u2018On the Variance of the Adaptive Learning Rate and Beyond\u2019. In: International   \n438 Conference on Learning Representations. 2020.   \n439 [42] Aleksander Madry et al. \u2018Towards Deep Learning Models Resistant to Adversarial Attacks\u2019. In: Interna  \n440 tional Conference on Learning Representations. 2018.   \n441 [43] Tom M. Mitchell. The Need for Biases in Learning Generalizations. Tech. rep. New Brunswick, NJ:   \n442 Rutgers University, 1980.   \n443 [44] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi and Pascal Frossard. \u2018DeepFool: A Simple and   \n444 Accurate Method to Fool Deep Neural Networks\u2019. In: IEEE Conference on Computer Vision and Pattern   \n445 Recognition. 2016.   \n446 [45] Weili Nie et al. \u2018Diffusion Models for Adversarial Purification\u2019. In: Proceedings of the International   \n447 Conference on Machine Learning. 2022.   \n448 [46] Adam Paszke et al. \u2018PyTorch: An Imperative Style, High-Performance Deep Learning Library\u2019. In:   \n449 Advances in Neural Information Processing Systems. 2019.   \n450 [47] ShengYun Peng et al. Robust Principles: Architectural Design Principles for Adversarially Robust CNNs.   \n451 2023.   \n452 [48] Yao Qin et al. \u2018Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recog  \n453 nition\u2019. In: Proceedings of the International Conference on Machine Learning. 2019.   \n454 [49] Sylvestre-Alvise Rebuff iet al. \u2018Data Augmentation Can Improve Robustness\u2019. In: Advances in Neural   \n455 Information Processing Systems. 2021.   \n456 [50] Danilo Jimenez Rezende, Shakir Mohamed and Daan Wierstra. \u2018Stochastic Backpropagation and Ap  \n457 proximate Inference in Deep Generative Models\u2019. In: Proceedings of the International Conference on   \n458 Machine Learning. 2014.   \n459 [51] Herbert Robbins and Sutton Monro. \u2018A Stochastic Approximation Method\u2019. In: The Annals of Mathemat  \n460 ical Statistics 22.3 (1951), pp. 400\u2013407.   \n461 [52] Pouya Samangouei, Maya Kabkab and Rama Chellappa. \u2018Defense-GAN: Protecting Classifiers Against   \n462 Adversarial Attacks Using Generative Models\u2019. In: International Conference on Learning Representations.   \n463 2018.   \n464 [53] Changhao Shi, Chester Holtz and Gal Mishne. \u2018Online Adversarial Purification based on Self-supervised   \n465 Learning\u2019. In: International Conference on Learning Representations. 2021.   \n466 [54] Naman D Singh, Francesco Croce and Matthias Hein. Revisiting Adversarial Training for ImageNet:   \n467 Architectures, Training and Generalization across Threat Models. 2023. arXiv: 2303.01870.   \n468 [55] Leslie N. Smith. \u2018Cyclical Learning Rates for Training Neural Networks\u2019. In: IEEE Winter Conference   \n469 on Applications of Computer Vision. 2017.   \n470 [56] Kihyuk Sohn, Honglak Lee and Xinchen Yan. \u2018Learning Structured Output Representation using Deep   \n471 Conditional Generative Models\u2019. In: Advances in Neural Information Processing Systems. 2015.   \n472 [57] Christian Szegedy et al. \u2018Intriguing properties of neural networks\u2019. In: International Conference on   \n473 Learning Representations. 2014.   \n474 [58] Florian Tram\u00e8r and Dan Boneh. \u2018Adversarial Training and Robustness for Multiple Perturbations\u2019. In:   \n475 Advances in Neural Information Processing Systems. 2019.   \n476 [59] Florian Tram\u00e8r et al. \u2018Ensemble Adversarial Training: Attacks and Defenses\u2019. In: International Conference   \n477 on Learning Representations. 2018.   \n478 [60] Florian Tram\u00e8r et al. \u2018On Adaptive Attacks to Adversarial Example Defenses\u2019. In: Advances in Neural   \n479 Information Processing Systems. 2020.   \n480 [61] Pascal Vincent et al. \u2018Extracting and composing robust features with denoising autoencoders\u2019. In: Inter  \n481 national Conference on Machine Learning. 2008.   \n482 [62] Zekai Wang et al. Better Diffusion Models Further Improve Adversarial Training. 2023. arXiv: 2303.   \n483 10130.   \n484 [63] Han Xiao, Kashif Rasul and Roland Vollgraf. Fashion-MNIST: a Novel Image Dataset for Benchmarking   \n485 Machine Learning Algorithms. 2017. arXiv: 1708.07747.   \n486 [64] Xinchen Yan et al. \u2018Attribute2Image: Conditional Image Generation from Visual Attributes\u2019. In: Proceed  \n487 ings of the European Conference on Computer Vision. 2016.   \n488 [65] Zhaoyuan Yang et al. \u2018Adversarial Purification with the Manifold Hypothesis\u2019. In: AAAI Conference on   \n489 Artificial Intelligence. 2024.   \n490 [66] Jongmin Yoon, Sung Ju Hwang and Juho Lee. \u2018Adversarial Purification with Score-based Generative   \n491 Models\u2019. In: Proceedings of the International Conference on Machine Learning. 2021.   \n492 [67] Hongyang Zhang et al. \u2018Theoretically Principled Trade-off between Robustness and Accuracy\u2019. In:   \n493 Proceedings of the International Conference on Machine Learning. 2019.   \n494 [68] M. Zhang, S. Levine and C. Finn. \u2018MEMO: Test Time Robustness via Adaptation and Augmentation\u2019. In:   \n495 Advances in Neural Information Processing Systems. 2022.   \n496 [69] Michael Zhang et al. \u2018Lookahead Optimizer: k steps forward, 1 step back\u2019. In: Advances in Neural   \n497 Information Processing Systems. 2019.   \n498 [70] Zhaoyu Zhang, Mengyan Li and Jun Yu. \u2018On the Convergence and Mode Collapse of GAN\u2019. In: SIG  \n499 GRAPH Asia 2018 Technical Briefs. 2018. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "500 A On Projected Gradient Descent adversarial training ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "501 The task of determining model parameters $\\theta^{\\star}$ that are robust to adversarial perturbations is cast in   \n502 [42] as a min-max optimisation problem seeking to minimise adversarial risk, i.e.: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\pmb{\\theta}^{\\star}\\approx\\hat{\\pmb{\\theta}}^{\\star}:=\\arg\\operatorname*{min}_{\\pmb{\\theta}}\\mathbb{E}_{(\\pmb{x},\\pmb{y})\\sim\\mathcal{D}}\\left[\\operatorname*{max}_{\\pmb{\\delta}\\in\\mathbb{S}}\\mathcal{L}\\left(f\\left(\\pmb{x}+\\pmb{\\delta};\\pmb{\\theta}\\right),\\pmb{y}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "503 where $\\mathcal{D}$ is the distribution on the examples $\\textbf{\\em x}$ and the corresponding labels $y$ , $f(\\cdot;\\pmb\\theta)$ is a model   \n504 with learnable parameters $\\theta,\\mathcal{L}$ is a suitable loss function, and $\\mathbb{S}$ is the set of allowed constrained   \n505 perturbations. In the case of $\\ell_{p}$ norm-bound perturbations of maximum magnitude $\\epsilon$ , we can further   \n506 specify $\\mathbb{S}:=\\{\\pmb{\\delta}\\,|\\,\\|\\pmb{\\delta}\\|_{p}\\leq\\epsilon\\}$ .   \n507 The inner optimisation problem is solved, in [42], by Projected Gradient Descent (PGD), an iterative   \n508 algorithm whose goal is the synthesis of an adversarial perturbation ${\\hat{\\pmb\\delta}}={\\pmb\\delta}^{(K)}$ after $K$ gradient ascent   \n509 and projection steps defined as: ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\n\\delta^{(k+1)}\\gets\\mathfrak{P}_{\\mathbb{S}}\\Big(\\delta^{(k)}+\\alpha\\,\\mathrm{sign}\\,\\Big(\\nabla_{\\delta^{(k)}}\\mathcal{L}_{c e}\\big(f(\\boldsymbol{x}+\\delta^{(k)};\\boldsymbol{\\theta}),\\boldsymbol{y}\\big)\\Big)\\Big)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "510 where $\\pmb{\\delta}^{(0)}$ is randomly sampled within $\\mathbb{S}$ , $\\alpha$ is a hyperparameter (step size), $\\mathcal{L}_{c e}$ is the cross-entropy   \n511 function, and $\\mathfrak{P}_{\\mathbb{A}}$ is the Euclidean projection operator onto set A, i.e.: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Re\\boldsymbol{\\mathrm{a}}(\\mathbf{a}):=\\underset{\\mathbf{a}^{\\prime}\\in\\mathbb{A}}{\\arg\\operatorname*{min}}\\,||\\mathbf{a}-\\mathbf{a}^{\\prime}||_{2}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "512 The outer optimisation is carried out by simply training $f(\\cdot;\\pmb\\theta)$ on the examples found by PGD against   \n513 the current model parameters \u2013 and their original pre-perturbation labels. The overall procedure just   \n514 described constitutes PGD adversarial training. ", "page_idx": 12}, {"type": "text", "text": "515 B On the functioning of (conditional) Variational Autoencoders ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "516 Variational autoencoders (VAEs) [32, 50] learn from data a generative distribution of the form   \n517 $p(\\pmb{x},z)=p(\\pmb{x}\\,|\\,\\pmb{z})p(\\pmb{z})$ , where the probability density $p(z)$ represents a prior over latent variable $_{\\textit{z}}$ ,   \n518 and $p(x\\mid z)$ is the likelihood function, which can be used to sample data of interest $\\textbf{\\em x}$ , given $_{\\textit{z}}$ .   \n519 Training is carried out by maximising a variational lower bound, $-{\\mathcal{L}}_{\\mathrm{VAE}}(x)$ , on the log-likelihood   \n520 $\\log p(x)$ \u2013 which is a proxy for the Evidence Lower Bound $\\left(E L B O\\right)-i.e.$ .: ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\n-{\\mathcal{L}}_{\\mathrm{VAE}}(x):=\\mathbb{E}_{q(z\\,|\\,x)}[\\log p(x\\,|\\,z)]-\\mathrm{KL}(q(z\\,|\\,x)||p(z))\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "521 where $q(z\\,|\\,\\pmb{x})\\approx p(z\\,|\\,\\pmb{x})$ is an approximate posterior and $\\operatorname{KL}(\\cdot\\|\\cdot)$ is the Kullback-Leibler divergence. ", "page_idx": 12}, {"type": "text", "text": "522 By parameterising the likelihood with a decoder ANN $p_{\\pmb\\theta_{\\mathrm{D}}}(\\pmb x\\,|\\,\\pmb z;\\pmb\\theta_{\\mathrm{D}})\\,\\approx\\,p(\\pmb x\\,|\\,\\pmb z)$ , and a possible   \n523 variational posterior with an encoder ANN $q_{\\pmb{\\theta}_{\\mathrm{E}}}(z\\,|\\,\\pmb{x};\\pmb{\\theta}_{\\mathrm{E}})\\;\\approx\\;q(z\\,|\\,\\pmb{x})$ , the parameters $\\theta_{\\mathrm{D}}^{\\star}$ of the   \n524 generative model that best reproduces the data can be learnt \u2013 jointly with $\\theta_{\\mathrm{E}}^{\\star}-\\mathrm{as}$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta_{\\mathrm{E}}^{\\star},\\theta_{\\mathrm{D}}^{\\star}:=}\\\\ &{\\quad\\quad\\underset{(\\theta_{\\mathrm{E}},\\theta_{\\mathrm{D}})}{\\mathrm{arg}\\,\\mathrm{min}}\\,\\mathcal{L}_{\\mathrm{vaE}}({\\boldsymbol{x}})=}\\\\ &{\\quad\\quad\\underset{(\\theta_{\\mathrm{E}},\\theta_{\\mathrm{D}})}{\\mathrm{arg}\\,\\mathrm{min}}\\,\\mathbb{E}_{{\\boldsymbol{x}}\\sim\\mathcal{D}}\\left[-\\mathbb{E}_{z\\sim{q}_{\\theta_{\\mathrm{E}}}(z\\mid{\\boldsymbol{x}};{\\boldsymbol{\\theta}}_{\\mathrm{E}})}\\left[\\log p_{\\theta_{\\mathrm{D}}}({\\boldsymbol{x}}\\mid{\\boldsymbol{z}};{\\boldsymbol{\\theta}}_{\\mathrm{D}})\\right]+\\mathrm{KL}(q_{\\theta_{\\mathrm{E}}}(z\\mid{\\boldsymbol{x}};{\\boldsymbol{\\theta}}_{\\mathrm{E}})\\|p({\\boldsymbol{z}}))\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "525 where $\\mathcal{D}$ is the distribution over the (training) examples $\\textbf{\\em x}$ . ", "page_idx": 12}, {"type": "text", "text": "526 From a practical point of view, optimisation is based on the empirical evaluation of $\\mathcal{L}_{\\mathrm{VAE}}(\\boldsymbol{x};\\boldsymbol{\\theta})$ on   \n527 mini-batches of data, with the term $-\\mathbb{E}_{z\\sim q_{\\theta_{\\mathrm{E}}}(z\\mid x;\\pmb{\\theta}_{\\mathrm{E}})}$ $[\\log p_{\\theta_{\\mathrm{D}}}(x\\,|\\,z;\\theta_{\\mathrm{D}})]$ replaced by a reconstruction   \n528 cost ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{Reco}}(x,x^{\\prime})\\ge0\\,|\\,\\mathcal{L}_{\\mathrm{Reco}}(x,x^{\\prime})=0\\iff x=x^{\\prime}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "529 The generation of new data according to the fitted model is achieved by sampling from ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left.p_{\\pmb{\\theta}_{\\mathrm{D}}^{\\star}}(\\pmb{x}\\,|\\,z;\\pmb{\\theta}_{\\mathrm{D}}^{\\star})\\right|_{z\\sim p(z)}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "530 i.e. decoding samples from $p(z)$ . ", "page_idx": 13}, {"type": "text", "text": "531 The setting is analogous in the case of conditional Variational Autoencoders [56, 64] (see section 3),   \n532 where conditional sampling is achieved by ", "page_idx": 13}, {"type": "equation", "text": "$$\nx_{c_{j}}\\sim p_{\\theta_{\\mathrm{D}}^{\\star}}(x\\,|\\,z,c;\\theta_{\\mathrm{D}}^{\\star})\\bigg|_{z\\sim p(z);\\;c=c_{j}}\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "533 C Justification of Adversarially-balanced batches ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "534 During the incipient phases of experimentation, preliminary tests were performed with the MNIST   \n535 [36] and Fashion-MNIST [63] datasets \u2013 using a conditional VAE as the purifier, and small FCNs or   \n536 convolutional ANNs as the classifiers. Adversarial examples were generated against the adversarially   \n537 pre-trained classifier, and tentatively denoised by the purifier with one sample only. The resulting   \n538 recovered inputs were classified by the classifier and the overall accuracy was recorded.   \n539 Importantly, such tests were not meant to assess the end-to-end adversarial robustness of the whole   \n540 architecture, but only to tune the training protocol of the purifier.   \n541 Generating adversarial training examples by means of PGD is considered the gold standard [24]   \n542 and was first attempted as a natural choice to train the purifier. However, in this case, the following   \n543 phenomena were observed:   \n544 \u2022 Unsatisfactory clean accuracy was reached upon convergence, speculatively a consequence   \n545 of the $V\\!A E$ having never been trained on clean-to-clean example reconstruction;   \n546 \u2022 Persistent vulnerability to same norm-bound FGSM perturbations was noticed;   \n547 \u2022 Persistent vulnerability to smaller norm-bound FGSM and PGD perturbations was noticed.   \n548 In an attempt to mitigate such issues, the composition of adversarial examples was adjusted to   \n549 specifically counteract each of the issues uncovered. The adoption of any smaller subset of attack   \n550 types or strength, compared to that described in subsection 4.4, resulted in unsatisfactory mitigation.   \n551 At that point, another problem emerged: if such an adversarial training protocol was carried out in   \n552 homogeneous batches, each containing the same type and strength of attack (or none at all), the   \n553 resulting robust accuracy was still partially compromised due to the homogeneous ordering of attack   \n554 types and strengths across batches.   \n555 Such observations lead to the final formulation of the training protocol, detailed in subsection 4.4,   \n556 which mitigates to the best the issues described so far. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "557 D Heuristic justification of the robust aggregation strategy ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "558 The rationale leading to the choice of the specific robust aggregation strategy described in subsec  \n559 tion 4.5 was an attempt to answer the following question: \u2018How is it possible to aggregate the results   \n560 of an ensemble of classifiers in a way such that it is hard to tilt the balance of the ensemble by   \n561 attacking only a few of its members?\u2019. The same reasoning can be extended to the reciprocal problem   \n562 we are trying to solve here, where different input reconstructions obtained from the same potentially   \n563 perturbed input are classified by the same model (the classifier).   \n564 Far from providing a satisfactory answer, we can analyse the behaviour of our aggregation strategy   \n565 as the logit associated with a given model and class varies across its domain, under the effect of   \n566 adversarial intervention. Comparison with existing (and more popular) probability averaging and   \n567 logit averaging aggregation strategies should provide a heuristic justification of our choice. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "568 We recall our aggregation strategy: ", "page_idx": 14}, {"type": "equation", "text": "$$\nP_{i}:=\\frac{1}{Z}\\prod_{\\alpha=1}^{N}e^{e^{l_{i}^{\\alpha}}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "569 Additionally, we recall logit averaging aggregation ", "page_idx": 14}, {"type": "equation", "text": "$$\nP_{i}:=\\frac{1}{Z}e^{\\frac{1}{N}\\sum_{\\alpha=1}^{N}l_{i}^{\\alpha}}=\\frac{1}{Z}\\prod_{\\alpha=1}^{N}e^{\\frac{1}{N}l_{i}^{\\alpha}}=\\frac{1}{Z}\\left(\\prod_{\\alpha=1}^{N}e^{l_{i}^{\\alpha}}\\right)^{\\frac{1}{N}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "570 and probability averaging aggregation ", "page_idx": 14}, {"type": "equation", "text": "$$\nP_{i}:=\\frac{1}{Z}{\\sum_{\\alpha=1}^{N}\\frac{e^{l_{i}^{\\alpha}}}{\\sum_{j=1}^{C}e^{l_{j}^{\\alpha}}}}=\\sum_{\\alpha=1}^{N}e^{l_{i}^{\\alpha}}{\\frac{1}{Q^{\\alpha}}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "571 where $\\begin{array}{r}{Q^{\\alpha}=\\sum_{j=1}^{C}e^{l_{j}^{\\alpha}}}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "572 Finally, since $l_{i}^{\\alpha}\\;\\in\\;\\mathbb{R},\\forall l_{i}^{\\alpha}$ , $\\scriptstyle\\operatorname*{lim}_{x\\to-\\infty}e^{x}\\ =\\ 0$ and $e^{0}\\,=\\,1$ , we can observe that $e^{l_{i}^{\\alpha}}\\ >\\ 0$ and   \n573 $e^{e^{l_{i}^{\\alpha}}}>1,\\forall l_{i}^{\\alpha}$ .   \n574 Now, we consider a given class $i^{\\star}$ and the classifier prediction on a given input reconstruction $\\alpha^{\\star}$ , and   \n575 study the potential effect of an adversary acting on $l_{i^{\\star}}^{\\alpha^{\\star}}$ . This adversarial intervention can be framed   \n576 in two complementary scenarios: either the class $i^{\\star}$ is correct and the adversary aims to decrease its   \n577 membership probability, or the class $i^{\\star}$ is incorrect and the adversary aims to increase its membership   \n578 probability. In any case, the adversary should comply with the $\\epsilon_{\\infty}$ -boundedness of its perturbation on   \n579 the input.   \n580 Logit averaging In the former scenario, the product of $e^{l_{i}^{\\alpha}}$ terms can be arbitrarily deflated (up to   \n581 zero) by lowering the $l_{i^{\\star}}^{\\alpha^{\\star}}$ logit only. In the latter scenario, the logit can be arbitrarily inflated, and   \n582 such effect is only partially suppressed by normalisation by $Z$ (a sum of $^1/N$ -exponentiated terms).   \n583 Probability averaging In the former scenario, although the effect of the deflation of a single logit   \n584 is bounded by $e^{\\boldsymbol{l}_{i^{\\star}}^{\\alpha^{\\star}}}>0$ , two attack strategies are possible: either decreasing the value of $l_{i^{\\star}}^{\\alpha^{\\star}}$ or   \n585 increasing the value of $Q^{\\alpha^{\\star}}$ , giving rise to complex combined effects. In the latter scenario, the   \n586 reciprocal is possible, i.e. either inflating $l_{i^{\\star}}^{\\alpha^{\\star}}$ or deflating $Q^{\\alpha^{\\star}}$ . Normalisation has no effect in both   \n587 cases.   \n588 Ours In the former scenario, the effect of logit deflation on a single product term is bounded by   \n589 $e^{e^{l_{i^{\\star}}^{\\alpha^{\\star}}}}>1$ , thus exerting only a minimal collateral effect on the product, through a decrease of $Z$ .   \n590 This effectively prevents aggregation takeover by logit deflation. Similarly to logit averaging, in the   \n591 latter scenario, the logit can be arbitrarily inflated. However, in this case, the effect of normalisation   \n592 by $Z$ is much stronger, given its increased magnitude.   \n593 From such a comparison, our aggregation strategy is the only one that strongly prevents adversarial   \n594 takeover by logit deflation, while still defending well against perturbations targeting logit inflation. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "595 E Architectural details and hyperparameters ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "596 In the following section, we provide more precise details about the architectures (subsection E.1) and   \n597 hyperparameters (subsection E.2) used in the experimental phase of our work.   \n599 In the following subsection, we describe the specific structure of the individual parts composing the   \n600 purifier \u2013 in the three scenarios considered. As far as the classifier architectures are concerned, we   \n601 redirect the reader to the original articles introducing those models (i.e.: [18] for scenarios (a) and   \n602 (b), [62] for scenario (c)).   \n603 During training, before being processed by the purifier encoder, input examples are standardised   \n604 according to the statistics of the respective training dataset.   \n605 Afterwards, they are fed to the disjoint input encoder (see subsection 4.3), whose architecture is   \n606 shown in Table 4. The same architecture is used in all scenarios considered. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Table 4: Architecture for the disjoint input encoder of the purifier. The same architecture is used in all scenarios considered. The architecture is represented layer by layer, from input to output, in a PyTorch-like syntax. The following abbreviations are used: Conv2D: 2-dimensional convolutional layer; ch_in: number of input channels; ch_out: number of output channels; ks: kernel size; s: stride; p: padding; b: presence of a learnable bias term; BatchNorm2D: 2-dimensional batch normalisation layer; affine: presence of learnable affine transform coefficients; slope: slope for the activation function in the negative semi-domain. ", "page_idx": 15}, {"type": "table", "img_path": "s5Y3M5l1qg/tmp/b19a6dd5ab422a054b3dedaef6143251f6eeeed25db4da7fefdf5a1bd107bd7c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "607 The original input is also fed to the classifier. The corresponding internal representation is extracted,   \n608 preserving its layered structure. In order to improve the scalability of the method, only a subset of   \n609 classifier layers is used instead of the whole internal representation. Specifically, for each block of   \n610 the WIDERESNET architecture, only the first layers have been considered; two shortcut layers have   \n611 also been added for good measure. The exact list of those layers is reported in Table 5.   \n612 Each extracted layerwise (pre)activation tensor has the shape of a multi-channel image, which is   \n613 processed \u2013 independently for each layer \u2013 by a different CNN whose individual architecture is shown   \n614 in Table 6 (scenarios (a) and $(b)$ ) and Table 7 (scenario (c)).   \n615 The resulting tensors (still having the shape of multi-channel images) are then jointly processed by a   \n616 fully-connected subnetwork whose architecture is shown in Table 8. The value of fcrepr for the   \n617 different scenarios considered is shown in Table 13.   \n618 The compressed input and compressed internal representation so obtained are finally jointly encoded   \n619 by an additional fully-connected subnetwork whose architecture is shown in Table 9. The output is a   \n620 tuple of means and standard deviations to be used to sample the stochastic latent code $_{z}$ .   \n621 The sampler used for the generation of such latent variables $_{z}$ , during the training of the purifier,   \n622 is a reparameterised [32] Normal sampler $z\\,\\sim\\mathcal{N}(\\mu,\\sigma)$ . During inference, $_{\\textit{z}}$ is sampled by re  \n623 parameterisation from the i.i.d Standard Normal distribution $z\\sim\\mathcal{N}(0,1)$ (i.e. from its original   \n624 prior).   \n625 The architectures for the decoder of the purifier are shown in Table 10 (scenarios $(a)$ and $(b)$ ) and   \n626 Table 11 (scenario (c)). ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "627 E.2 Hyperparameters ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "628 In the following section, we provide the hyperparameters used for adversarial example generation and   \n629 optimisation during the training of the purifier, and those related to the purifier model architectures.   \n630 We also provide the hyperparameters for the PGD $^+$ EOT attack, which is used as a complementary   \n631 tool for the evaluation of adversarial robustness. ", "page_idx": 15}, {"type": "text", "text": "Table 5: Classifier model (WIDERESNET-28-10) layer names used as (a subset of) the internal representation fed to the layerwise convolutional encoder of the purifier. The names reflect those used in the model implementation. ", "page_idx": 16}, {"type": "table", "img_path": "s5Y3M5l1qg/tmp/2d528b731985d832378b76606f2da5ee2b14d0076de46fe5a9a22f3bb1c4f657.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "s5Y3M5l1qg/tmp/a5aceb0aab7241983844ac0633b2ef40bc8cc3116fa59f57333f6258e279feca.jpg", "table_caption": ["Table 6: Architecture for the layerwise internal representation encoder of the purifier. The architecture shown in this table is used in scenarios (a) and (b). The architecture is represented layer by layer, from input to output, in a PyTorch-like syntax. The following abbreviations are used: Conv2D: 2-dimensional convolutional layer; ch_in: number of input channels; ch_out: number of output channels; ks: kernel size; s: stride; p: padding; b: presence of a learnable bias term; BatchNorm2D: 2-dimensional batch normalisation layer; affine: presence of learnable affine transform coefficients; slope: slope for the activation function in the negative semi-domain. The abbreviation [ci] indicates the number of input channels for the (pre)activation tensor of each extracted layer. The abbreviation ceil indicates the ceiling integer rounding function. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 7: Architecture for the layerwise internal representation encoder of the purifier. The architecture shown in this table is used in scenario (c). The architecture is represented layer by layer, from input to output, in a PyTorch-like syntax. The following abbreviations are used: Conv2D: 2-dimensional convolutional layer; ch_in: number of input channels; ch_out: number of output channels; ks: kernel size; s: stride; p: padding; b: presence of a learnable bias term; BatchNorm2D: 2-dimensional batch normalisation layer; affine: presence of learnable affine transform coefficients; slope: slope for the activation function in the negative semi-domain. The abbreviation [ci] indicates the number of input channels for the (pre)activation tensor of each extracted layer. The abbreviation ceil indicates the ceiling integer rounding function. ", "page_idx": 17}, {"type": "table", "img_path": "s5Y3M5l1qg/tmp/aa8b14b4b420059a5fbf397aa61b288e4d9a0a93740bec84a662f21155928891.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 8: Architecture for the fully-connected representation encoder of the purifier. The architecture shown in this table is used in all scenarios considered. The architecture is represented layer by layer, from input to output, in a PyTorch-like syntax. The following abbreviations are used: Concatenate: layer concatenating its input features; flatten_features: whether the input features are to be flattened before concatenation; feats_in, feats_out: number of input and output features of a linear layer; b: presence of a learnable bias term; BatchNorm1D: 1-dimensional batch normalisation layer; affine: presence of learnable affine transform coefficients; slope: slope for the activation function in the negative semi-domain. The abbreviation [computed] indicates that the number of features is computed according to the shape of the concatenated input tensors. The value of fcrepr for the different scenarios considered is shown in Table 13. ", "page_idx": 17}, {"type": "table", "img_path": "s5Y3M5l1qg/tmp/c8e35bdff50da264f610550ab558b168d3f7f4b71ea80025241679cd270d13be.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 9: Architecture for the fully-connected joint encoder of the purifier. The architecture shown in this table is used in all scenarios considered. The architecture is represented layer by layer, from input to output, in a PyTorch-like syntax. The following abbreviations are used: Concatenate: layer concatenating its input features; flatten_features: whether the input features are to be flattened before concatenation; feats_in, feats_out: number of input and output features of a linear layer; b: presence of a learnable bias term; BatchNorm1D: 1-dimensional batch normalisation layer; affine: presence of learnable affine transform coefficients; slope: slope for the activation function in the negative semi-domain. The abbreviation [computed] indicates that the number of features is computed according to the shape of the concatenated input tensors. The value of fjoint for the different scenarios considered is shown in Table 13. The last layer of the network returns a tuple of 2 tensors, each independently processed \u2013 from the output of the previous layer \u2013 by the two comma-separated sub-layers. ", "page_idx": 17}, {"type": "table", "img_path": "s5Y3M5l1qg/tmp/41458b9b0d33e4bd7ce5b435405e494434000a4c1cd6aacf065975d03715790f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 10: Architecture for the decoder of the purifier. The architecture shown in this table is used in scenarios (a) and (b). The architecture is represented layer by layer, from input to output, in a PyTorch-like syntax. The following abbreviations are used: Concatenate: layer concatenating its input features; flatten_features: whether the input features are to be flattened before concatenation; feats_in, feats_out: number of input and output features of a linear layer; b: presence of a learnable bias term; ConvTranspose2D: 2-dimensional transposed convolutional layer; ch_in: number of input channels; ch_out: number of output channels; ks: kernel size; s: stride; p: padding; op: PyTorch parameter \u2018output padding\u2019, used to disambiguate the number of spatial dimensions of the resulting output; b: presence of a learnable bias term; BatchNorm2D: 2-dimensional batch normalisation layer; affine: presence of learnable affine transform coefficients; slope: slope for the activation function in the negative semi-domain. The values of fjoint and fcrepr for the different scenarios considered are shown in Table 13. ", "page_idx": 18}, {"type": "table", "img_path": "s5Y3M5l1qg/tmp/5f6eca4922627a5e089a4506628714fc56ccc45e867d15c08aa2c7d8fb1f8c0d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 11: Architecture for the decoder of the purifier. The architecture shown in this table is used in scenario (c). The architecture is represented layer by layer, from input to output, in a PyTorch-like syntax. The following abbreviations are used: Concatenate: layer concatenating its input features; flatten_features: whether the input features are to be flattened before concatenation; feats_in, feats_out: number of input and output features of a linear layer; b: presence of a learnable bias term; ConvTranspose2D: 2-dimensional transposed convolutional layer; ch_in: number of input channels; ch_out: number of output channels; ks: kernel size; s: stride; p: padding; op: PyTorch parameter \u2018output padding\u2019, used to disambiguate the number of spatial dimensions of the resulting output; b: presence of a learnable bias term; BatchNorm2D: 2-dimensional batch normalisation layer; affine: presence of learnable affine transform coefficients; slope: slope for the activation function in the negative semi-domain. The values of fjoint and fcrepr for the different scenarios considered are shown in Table 13. ", "page_idx": 18}, {"type": "table", "img_path": "s5Y3M5l1qg/tmp/f72f7ab5665b8cac78761eec991daf3cc5ca5593b83cc7d37ab9a5e52f1384ab.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "632 Attacks The hyperparameters used for the adversarial attacks described in subsection 4.4 are shown   \n633 in Table 12. The value of $\\epsilon_{\\infty}$ is fixed to $\\epsilon_{\\infty}={}^{8}/255$ . With the only exception of $\\epsilon_{\\infty}$ , AUTOATTACK   \n634 is to be considered a hyperparameter-free adversarial example generator.   \n635 Architectures Table 13 contains the hyperparameters that define the model architectures used as   \n636 part of the purifier, in the different scenarios considered. ", "page_idx": 19}, {"type": "table", "img_path": "s5Y3M5l1qg/tmp/9abecd2213c60406ca994afa83dc72ba68c21003e9e03bd43e2ce293cef1d6d1.jpg", "table_caption": ["Table 12: Hyperparameters for the attacks used for training and testing the purifier The FGSM and PDG attacks refer to the training phase (see subsection 4.4), whereas the $\\scriptstyle\\mathrm{PGD+EOT}$ attack [38] refers to the robustness assessment pipeline. The entry CCE denotes the Categorical CrossEntropy loss function. The $\\ell_{\\infty}$ threat model is assumed, and all inputs are linearly rescaled within [0.0, 1.0] before the attack. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Table 13: Scenario-specific architectural hyperparameters for the purifier, as referred to in Table 8, Table 9, Table 10, and Table 11. ", "page_idx": 19}, {"type": "table", "img_path": "s5Y3M5l1qg/tmp/6eba0356a357853d087efbca9825e2a92ca3ad93a12422eb49818c7e65c8bf85.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "637 Training Table 14 collects the hyperparameters governing the training of the purifier in the different   \n638 scenarios considered. ", "page_idx": 19}, {"type": "table", "img_path": "s5Y3M5l1qg/tmp/108909fe91aa541e71923c47f84d79705d179c668bd9595f2dd1bf36c315de5a.jpg", "table_caption": ["Table 14: Hyperparameters used for training the purifier, grouped by scenario. The entry CCE denotes the Categorical CrossEntropy loss function. The $L R$ scheduler is stepped after each epoch. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "639 F Additional tables ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "640 The following section contains additional tabular data that may be of interest to the reader. ", "page_idx": 19}, {"type": "text", "text": "641 Table 15 reports the respective clean accuracies for the best models available in terms of AUTOAT  \n642 TACK robust accuracy, in scenarios (a) and $(b)$ . Models are further divided in AT-based and   \n643 purification-based, so as to match the corresponding columns for robust accuracy shown in Table 2.   \n644 The best AT-based model for CIFAR-10 is taken from [18], whereas that for CIFAR-100 from [62].   \n645 Both best purification-based models are taken from [40].   \n646 The clean and robust accuracies for the best AT-based model on TINYIMAGENET-200 (scenario (c))   \n647 are already part of Table 2 and we redirect the reader there for such information. We are not aware of   \n648 any published state-of-the-art adversarial purification-based model for TINYIMAGENET-200. ", "page_idx": 20}, {"type": "text", "text": "Table 15: Clean accuracy for the best models (by robust accuracy) on the datasets considered in scenarios (a) and $(b)$ , mentioned in Table 2. The following abbreviations are used: Scen: scenario considered; Best AT/Cl: clean accuracy for the most robust model (by the means of AUTOATTACK) on the respective dataset, obtained by adversarial training alone; Best P/Cl: clean accuracy for the most robust model (by the means of AUTOATTACK) on the respective dataset, obtained by adversarial purification alone. ", "page_idx": 20}, {"type": "table", "img_path": "s5Y3M5l1qg/tmp/908ca766b1c2c1158f9f8e622df708368975b7542b170255f91c68c63ba9a736.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "649 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "650 1. Claims   \n651 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n652 paper\u2019s contributions and scope?   \n653 Answer: [Yes]   \n654 Justification: Claims made in the abstract and introduction of the paper accurately reflect   \n655 its contributions, and those are directly corroborated by experimental analysis. Results and   \n656 their discussion is available in subsection 5.2 and subsection 5.3.   \n657 2. Limitations   \n658 Question: Does the paper discuss the limitations of the work performed by the authors?   \n659 Answer: [Yes]   \n660 Justification: subsection 5.2 and subsection 5.3 also contain the discussion of potential   \n661 limitations of the method, and open problems it introduces.   \n662 3. Theory Assumptions and Proofs   \n663 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n664 a complete (and correct) proof?   \n665 Answer: [NA]   \n666 Justification: The paper does not contribute novel theoretical results. Assumptions anyway   \n667 related to the contribution are clearly stated throughout the paper.   \n668 4. Experimental Result Reproducibility   \n669 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n670 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n671 of the paper (regardless of whether the code and data are provided or not)?   \n672 Answer: [Yes]   \n673 Justification: Training and evaluation details required to reproduce the experimental results   \n674 of the paper are reported in section 5 and Appendix E. Code and data for the reproduction   \n675 of all experiments are additionally released as part of Supplementary Material.   \n676 5. Open access to data and code   \n677 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n678 tions to faithfully reproduce the main experimental results, as described in supplemental   \n679 material?   \n680 Answer: [Yes]   \n681 Justification: Code and data for the reproduction of all experiments are released to reviewers   \n682 as part of Supplementary Material. The public version of the paper will include instructions   \n683 to obtain the same material from a dedicated publicly accessible source.   \n684 6. Experimental Setting/Details   \n685 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n686 parameters, how they were chosen, type of optimiser, etc.) necessary to understand the   \n687 results?   \n688 Answer: [Yes]   \n689 Justification: Training and evaluation details, including hyperparameters, required to re  \n690 produce the experimental results of the paper are reported in section 5 and Appendix E.   \n691 Code and data for the reproduction of all experiments are additionally released as part of   \n692 Supplementary Material.   \n693 7. Experiment Statistical Significance   \n694 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n695 information about the statistical significance of the experiments?   \n696 Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "697 Justification: As doing adversarial training with 40-steps PGD is roughly 40 times more   \n698 computationally demanding than nominal training, unfortunately, we are unable to show   \n699 error bars or otherwise quantify statistical errors. In any case, the improvement induced   \n700 by our method w.r.t. their state-of-the-art counterparts is well clear of the threshold for   \n701 statistical significance.   \n702 8. Experiments Compute Resources   \n703 Question: For each experiment, does the paper provide sufficient information on the com  \n704 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n705 the experiments?   \n706 Answer: [Yes]   \n707 Justification: Information about computational resources and required time is contained in   \n708 subsection 5.1 as well as in the supplementary materials.   \n709 9. Code Of Ethics   \n710 Question: Does the research conducted in the paper conform, in every respect, with the   \n711 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n712 Answer: [Yes]   \n713 Justification: The research conducted in the paper does conform, in every respect, with the   \n714 NeurIPS Code of Ethics.   \n715 10. Broader Impacts   \n716 Question: Does the paper discuss both potential positive societal impacts and negative   \n717 societal impacts of the work performed?   \n718 Answer: [Yes]   \n719 Justification: The paper proposes a novel technique to mitigate the problem of adversarial   \n720 vulnerability of high-dimensional classifiers. Such vulnerability may pose potential societal   \n721 impacts, as discussed in section 1.   \n722 11. Safeguards   \n723 Question: Does the paper describe safeguards that have been put in place for responsible   \n724 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n725 image generators, or scraped datasets)?   \n726 Answer: [NA]   \n727 Justification: We do not plan to release models or data with a high risk for misuse. Models   \n728 to be released do not reasonably carry risk for misuse.   \n729 12. Licenses for existing assets   \n730 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n731 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n732 properly respected?   \n733 Answer: [Yes]   \n734 Justification: The creators and/or owners of assets used in the paper are either credited by   \n735 reference to their original research work, or directly with a link to the preferred landing page   \n736 for such assets. The use of licensed material is compliant with the respective licenses.   \n737 13. New Assets   \n738 Question: Are new assets introduced in the paper well documented and is the documentation   \n739 provided alongside the assets?   \n740 Answer: [Yes]   \n741 Justification: Trained model weights are provided alongside the paper, and their details are   \n742 provided as part of supplementary materials. In case of release to the public, such details   \n743 will be provided contextually to the models.   \n744 14. Crowdsourcing and Research with Human Subjects   \n745 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n746 include the full text of instructions given to participants and screenshots, if applicable, as   \n747 well as details about compensation (if any)?   \n748 Answer: [NA]   \n749 Justification: No crowdsourcing or research with human subjects has been performed as part   \n750 of the work of, or leading to, this paper.   \n751 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n752 Subjects   \n753 Question: Does the paper describe potential risks incurred by study participants, whether   \n754 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n755 approvals (or an equivalent approval/review based on the requirements of your country or   \n756 institution) were obtained?   \n757 Answer: [NA]   \n758 Justification: No crowdsourcing or research with human subjects has been performed as part   \n759 of the work of or leading to this paper - including that potentially requiring IRB approval or   \n760 equivalent authorisation. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}]