[{"figure_path": "SRWs2wxNs7/tables/tables_3_1.jpg", "caption": "Table 1: Toy experiments on U-Net-style DiTs. The naive DiT-UNet performs slightly better than the isotropic DiT-S/4; but interestingly, when we apply token downsampling for self-attention, the DiT-UNet performs better with fewer costs.", "description": "This table presents the results of a toy experiment comparing different variations of U-Net-style diffusion transformers (DiTs). It shows that a naive U-Net-style DiT (DiT-UNet) performs only slightly better than a standard isotropic DiT. However, incorporating token downsampling into the self-attention mechanism significantly improves performance while reducing computational cost.", "section": "3 Investigating U-Net DiTs in Latent"}, {"figure_path": "SRWs2wxNs7/tables/tables_5_1.jpg", "caption": "Table 2: Comparing U-DiTs against DiTs on ImageNet 256\u00d7256 generation. Experiments with supermarks * are replicated according to the official code of DiT. We compare models trained for 400K iterations with the standard training hyperparameters of DiT. The performance of U-DiTs is outstanding: U-DiT-B could beat DiT-XL/2 with only 1/6 of inference FLOPs; U-DiT-L could outcompete DiT-XL/2 by 10 FIDs.", "description": "This table compares the performance of U-DiTs and DiTs on ImageNet 256x256 image generation.  It shows that U-DiTs achieve significantly better results (lower FID, higher IS, etc.) than DiTs, even with fewer computational resources (GFLOPs).  The results are particularly striking for U-DiT-B, which surpasses DiT-XL/2 despite using only 1/6th of the computation.", "section": "4.1 U-DiT at Larger Scales"}, {"figure_path": "SRWs2wxNs7/tables/tables_5_2.jpg", "caption": "Table 3: Comparing U-DiTs against competitive diffusion architectures on ImageNet 256\u00d7256 generation. Since different architectures use different training settings, we align them under the official 400K-iteration setting of DiT for a fair comparison. The proposed U-DiT series could outperform these models by large margins at fewer FLOPs. Experiments with supermarks * include necessary modifications of the original work (detailed in the appendix).", "description": "This table compares the performance of U-DiTs with other competitive diffusion architectures on the ImageNet 256x256 dataset.  The comparison is done using several evaluation metrics including FLOPs, FID, sFID, IS, Precision, and Recall.  The key takeaway is that U-DiTs achieve better performance with significantly lower computational costs.", "section": "4.1 U-DiT at Larger Scales"}, {"figure_path": "SRWs2wxNs7/tables/tables_5_3.jpg", "caption": "Table 2: Comparing U-DiTs against DiTs on ImageNet 256\u00d7256 generation. Experiments with supermarks * are replicated according to the official code of DiT. We compare models trained for 400K iterations with the standard training hyperparameters of DiT. The performance of U-DiTs is outstanding: U-DiT-B could beat DiT-XL/2 with only 1/6 of inference FLOPs; U-DiT-L could outcompete DiT-XL/2 by 10 FIDs.", "description": "This table compares the performance of U-DiTs and DiTs on the ImageNet 256x256 dataset.  It shows that U-DiTs achieve better FID (Fr\u00e9chet Inception Distance), IS (Inception Score), precision, and recall scores compared to DiTs, even with significantly fewer FLOPs (floating point operations). This highlights the efficiency and performance advantages of U-DiTs.", "section": "4.1 U-DiT at Larger Scales"}, {"figure_path": "SRWs2wxNs7/tables/tables_6_1.jpg", "caption": "Table 5: The performance of U-DiT-B and U-DiT-L models with respect to training iterations. The unconditional generation performance of both models on ImageNet 256\u00d7256 consistently improves as training goes on, where U-DiT-L at 600K steps strikingly beats DiT-XL/2 at 7M steps.", "description": "This table presents the FID, sFID, IS, precision, and recall scores for the U-DiT-B and U-DiT-L models trained for different numbers of iterations (200K, 400K, 600K, 800K, and 1M) on the ImageNet 256x256 dataset.  It shows the improvement in the model's performance (lower FID and sFID, higher IS, precision and recall) as the training progresses, demonstrating the models' ability to learn better image generation with more training.  U-DiT-L at 600k steps surpasses the performance of DiT-XL/2 trained for 7 million steps.", "section": "4 Scaling the Model Up"}, {"figure_path": "SRWs2wxNs7/tables/tables_7_1.jpg", "caption": "Table 6: Comparing U-DiTs against DiTs on ImageNet 512\u00d7512 generation. Experiments with supermarks * are replicated according to the official code of DiT. We compare models trained for 400K iterations with the standard training hyperparameters of DiT.", "description": "This table compares the performance of U-DiTs and DiTs on ImageNet 512x512 image generation.  It shows that U-DiT-B significantly outperforms DiT-XL/2 in terms of FID and IS scores, while having a much lower computational cost (FLOPs). The asterisk (*) indicates experiments that replicated the original DiT code for fair comparison.", "section": "4.1 U-DiT at Larger Scales"}, {"figure_path": "SRWs2wxNs7/tables/tables_7_2.jpg", "caption": "Table 7: Ablations on the choice of downsampler. We have tried several downsampler designs, and it turns out that the parallel connection of a shortcut and a depthwise convolution is the best fit. We avoid using ordinary convolution (i.e. Conv.+PS) because channel-mixing is costly: conventional convolution-based downsamplers could double the amount of computation. The U-DiT with a conventional downsampler costs as many as 2.22G FLOPs in total.", "description": "This table presents ablation studies on different downsampling methods used in the U-DiT architecture.  It compares the performance (FID, SFID, IS, Precision, Recall) of three different downsamplers: Pixel Shuffle (PS), Depthwise Convolution (DW) Conv + PS, and DW Conv. || Shortcut + PS. The results show that combining depthwise convolution with a shortcut and then using pixel shuffling achieves the best performance with a relatively low computational cost.", "section": "4.2 Ablations"}, {"figure_path": "SRWs2wxNs7/tables/tables_8_1.jpg", "caption": "Table 8: Configurations of U-DiTs architecture with different model sizes. Channel represents the initial output channel number of first layer. Encoder-Decoder denotes the transformer block number of encoder and decoder module.", "description": "This table presents the configurations for three different sizes of the U-DiT model: U-DiT-S, U-DiT-B, and U-DiT-L.  It lists the number of parameters (in millions), the number of floating-point operations (GFLOPs), the initial channel number, the number of attention heads, and the number of encoder and decoder blocks in each stage. The table provides essential details about the model's architecture and computational complexity.", "section": "4.1 U-DiT at Larger Scales"}, {"figure_path": "SRWs2wxNs7/tables/tables_8_2.jpg", "caption": "Table 9: Ablations on U-DiT components. Apart from the toy example in Sec. 3, we further validate the effectiveness of downsampled by comparing the U-DiT with a slimmed version of DiT-UNet at equal FLOPs. Results reveal that downsampling could bring ~18FIDs on DiT-UNet. Further modifications on top of the U-DiT architecture could improve 2 to 5 FIDs each.", "description": "This table presents ablation studies on different components of the U-DiT model. It compares a baseline model (DiT-UNet (Slim)) with several variations of the U-DiT, each incorporating a different modification.  The modifications evaluated are downsampling, cosine similarity, RoPE2D, depthwise convolution FFN, and re-parameterization.  The table shows the impact of each modification on FID, SFID, IS, Precision, and Recall, demonstrating the contribution of each component to the model's overall performance.", "section": "4.2 Ablations"}, {"figure_path": "SRWs2wxNs7/tables/tables_8_3.jpg", "caption": "Table 10: Comparison between vanilla U-DiTs and improved U-DiTs with all modifications. With negligible extra computational overhead, the proposed modifications could improve the performance of U-DiT; but it is worth noting that vanilla U-DiTs are powerful enough against DiTs.", "description": "This table compares the performance of vanilla U-DiT models (without any modifications) to the performance of U-DiT models that include all proposed modifications.  The results show that while the modifications improve performance metrics (FID, sFID, IS, Precision, Recall), the vanilla U-DiTs already perform competitively against DiTs, indicating the effectiveness of the core U-DiT architecture.", "section": "4.1 U-DiT at Larger Scales"}, {"figure_path": "SRWs2wxNs7/tables/tables_13_1.jpg", "caption": "Table 11: The training overhead of DiT-XL/2 and U-DiTs. \"TS\" stands for training speed, measured in steps per second on 8 NVIDIA A100 (80G).", "description": "This table compares the training speed (steps per second) of the DiT-XL/2 model and different variants of the U-DiT model.  It shows that the training speed of the vanilla U-DiT-L model is comparable to that of DiT-XL/2.  Adding all modifications to the U-DiT-L model slightly decreases the training speed.", "section": "A.2 Additional Experiment Details"}]