[{"type": "text", "text": "Stability and Sharper Risk Bounds with Convergence Rate $O(1/n^{2})$ ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The sharpest known high probability excess risk bounds are up to ${\\cal O}(1/n)$ for   \n\uff1f empirical risk minimization and projected gradient descent via algorithmic stability   \n3 [Klochkov and Zhivotovskiy, 2021]. In this paper, we show that high probability excess risk bounds of order up to $O(1/n^{2})$ are possible. We discuss how high probability excess risk bounds reach $O(1/n^{2})$ under strongly convexity, smoothness and Lipschitz continuity assumptions for empirical risk minimization, projected gradient descent and stochastic gradient descent. Besides, to the best of our knowl  \n3 edge, our high probability results on the generalization gap measured by gradients for nonconvex problems are also the sharpest. ", "page_idx": 0}, {"type": "text", "text": "101 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "11 Algorithmic stability is a fundamental concept in learning theory [Bousquet and Elisseeff, 2002],   \n12   which can be traced back to the foundational works of Vapnik and Chervonenkis [1974] and has   \n13  a deep connection with learnability [Rakhlin et al., 2005, Shalev-Shwartz et al., 2010, Shalev  \n14  Shwartz and Ben-David, 2014]. It is not difficult for only providing in-expectation error bounds via   \n15 stability arguments. However, high probability bounds are beneficial to understand the robustness of   \n16 optimization algorithms [Bousquet et al., 2020, Klochkov and Zhivotovskiy, 2021] and are much   \n17 more challenging [Feldman and Vondrak, 2019, Bousquet et al., 2020, Lv et al., 2021]. In this paper,   \n18 our goal is to improve the high probability risk bounds via algorithmic stability.   \n19 Let us start with some standard notations. We have a set of independent and identically distributed   \n20  observations $S=\\{z_{1},\\ldots,z_{n}\\}$ sampled from a probability measure $\\rho$ defined on a sample space   \n21 $\\mathcal{Z}:=\\mathcal{X}\\times\\mathcal{Y}$ . Based on the training set $S$ , our goal is to build a model $h:\\mathcal{X}\\mapsto\\mathcal{Y}$ for prediction,   \n22 where the model is determined by parameter w from parameter space $\\mathcal{W}\\subset\\mathbb{R}^{d}$ . The performance of   \n23  a model w on an example $z$ can be quantified by a loss function $f(\\mathbf{w};z)$ , where $f:\\mathcal{W}\\times\\mathcal{Z}\\mapsto\\mathbb{R}_{+}$   \n24  Then the population risk and the empirical risk of $\\mathbf{w}\\in\\mathcal{W}$ , respectively as ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "equation", "text": "$$\nF(\\mathbf{w}):=\\mathbb{E}_{z}\\left[f(\\mathbf{w};z)\\right],\\quad F_{S}(\\mathbf{w}):=\\frac{1}{n}\\sum_{i=1}^{n}f(\\mathbf{w};z_{i}),\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "25  where $\\mathbb{E}_{z}$ denotes the expectation w.r.t. $z$ ", "page_idx": 0}, {"type": "text", "text": "26 Let $\\mathbf{w}^{*}\\in\\arg\\operatorname*{min}_{\\mathbf{w}\\in\\mathcal{W}}F(\\mathbf{w})$ be the model with the minimal population risk in $\\mathcal{W}$ and $\\mathbf{w}^{*}(S)\\in$   \n27arg $\\mathrm{min}_{\\mathbf{w}\\in\\mathcal{W}}\\,F_{S}(\\mathbf{w})$ be the model with the minimal empirical risk w.r.t. dataset $S$ . Let $A(S)$ be the   \n28 output of a (possibly randomized) algorithm $A$ on the dataset $S$ . Let $\\Vert\\cdot\\Vert_{2}$ denote the Euclidean norm   \n29  and $\\nabla g(\\mathbf{w})$ denote a subgradient of $g$ at w.   \n30 Traditional generalization analysis aims to bound the generalization error $F(A(S))-F_{S}(A(S))$ W.r.t   \n31 the algorithm $A$ and the dataset $S$ . Based on the technique developed by Feldman and Vondrak [2018,   \n32  2019], Bousquet et al. [2020] provide the sharpest high probability bounds of $O\\left(L/{\\sqrt{n}}\\right)$ , where the   \n33 loss function $f(\\cdot,\\cdot)$ is bounded by $M$ . No matter how stable the algorithm is, the high probability   \n34 generalization bound will not be smaller than $O(L/\\sqrt{n})$ . This is sampling error term scaling as   \n35 $O\\left(1/{\\sqrt{n}}\\right)$ that controls the generalization error [Klochkov and Zhivotovskiy, 2021].   \n36 A frequently used alternative to generalization bounds, that can avoid the sampling error, are the   \n37 excess risk bounds. The excess risk of algorithm $A$ w.r.t. the dataset $S$ is $F(A(S{\\bar{)}})-{\\bar{F}}(\\mathbf{w}^{*})$ , which   \n38 is more essential because it considers both generalization error and optimization error. Recently,   \n39 Klochkov and Zhivotovskiy [2021] provided the best high probability excess risk bounds of order   \n40upto $O(\\log n/n)$ for empirical risk minimization (ERM) and projected gradient descent (PGD)   \n41 algorithms via algorithmic stability.   \n42 On the other hand, Zhang et al. [2017], Li and Liu [2021], Xu and Zeevi [2024] derived high   \n43 probability excess risk bounds with $O\\left(1/n^{2}\\right)$ for ERM and stochastic gradient descent (SGD) via   \n44 uniform convergence when the sample number satisfies $n\\,=\\,\\Omega(d)$ , which implied that the rate   \n45 $O\\left(1/n^{2}\\right)$ is possible. However, the results obtained by the uniform convergence technique are related   \n46 to the dimension $d$ . which is unacceptable in high-dimensional learning problems. Since stability   \nanalysis can yield dimension-free bounds, we naturally have the following question:   \n48 Can algorithmic stability provide high probability excess risk bounds with the rate beyond $O(1/n)$ \uff1f   \nThe main results of this paper answers this question positively. We provides the first high probability   \n50 bounds that are dimension-free with the rate $O(1/n^{2})$ for ERM, PGD and SGD. Our framework can   \n51 also be used to solve other stable algorithms.   \nTo this end, we develop the generalization gap measured by gradients. Our bounds under nonconvex   \nsetting are tighter than existing works based on both algorithmic stability [Fan and Lei, 2024] and   \n54 uniform convergence $[\\mathrm{Xu}$ and Zeevi, 2024]. This is why we can achieve dimension-free excess risk   \n55 bounds of order $O(1/n^{2})$ . In fact, in nonconvex problems, optimization algorithms can only find   \n56  a local minimizer and we can only obtain optimization error bounds for $\\|\\nabla\\bar{F}_{S}(A(S))\\|_{2}$ [Ghadimi   \n57 and Lan, 2013]. Therefore, it is important to study the generalization behavior of $A(S)$ measuredby   \n58 gradients. Under Polyak-Lojasiewicz condition, we also obtain sharper results for both generalization   \n59 bounds of gradients and excess risk bounds. Our route to excess risk bounds can also be applied   \n60 to various stable algorithms and complex learning scenarios. In this paper, we take ERM, PGD,   \n61 and SGD as examples to explore the stability of stochastic convex optimization algorithms with   \n62 strongly convex losses. We provide tighter high probability dimension-free excess risk bounds of   \n63 up to $\\breve{O}(1/n^{2})$ comapring with existing works based on both algorithmic stability [Klochkov and   \n64 Zhivotovskiy, 2021, Fan and Lei, 2024] and uniform convergence [Zhang et al., 2017, Li and Liu,   \n652021, Xu and Zeevi, 2024].   \n66 Besides, to obtain tighter bounds, we obtain a tighter $p$ -moment bound for sums of vector-valued   \n67 functions by introducing the optimal Marcinkiewicz-Zygmund's inequality for random variables   \n68  taking values in a Hilbert space in the proof, which has more potential applications in vector-valued   \n69 functional data.   \n70This paper is organized as follows. The related work is reviewed in Section 2. In Section 3, we   \n71  present our main results for stability and generalization. We give applications to ERM, PGD and   \n72 SGD in Section 4. The conclusion is given in Section 5. All the proofs and additional lemmata are   \n73 deferred to the Appendix. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "742Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "75 Algorithmic stability. Algorithmic stability is a classical approach in generalization analysis, which   \n76  can be traced back to the foundational works of [Vapnik and Chervonenkis, 1974]. It gave the   \n77 generalization bound by analyzing the sensitivity of a particular learning algorithm when changing   \n78 one data point in the dataset. Modern framework of stability analysis was established by Bousquet   \n79 and Elisseeff [2002], where they presented an important concept called uniform stability. Since   \n80 then, a lot of works based on uniform stability have emerged. On one hand, generalization bounds   \n81 with algorithmic stability have been significantly improved by Feldman and Vondrak [2018, 2019],   \n82 Bousquet et al. [2020], Klochkov and Zhivotovskiy [2021]. On the other hand, different algorithmic   \n83  stability measures such as uniform argument stability [Liu et al., 2017, Bassily et al., 2020], on   \n84 average stability [Shalev-Shwartz et al., 2010, Kuzborskij and Lampert, 2018], hypothesis stability   \n85 [Bousquet and Elisseeff, 2002, Charles and Papailiopoulos, 2018], hypothesis set stability [Foster   \n86 et al., 2019], pointwise uniform stability [Fan and Lei, 2024], PAC-Bayesian stability [Li et al., 2020],   \n87 locally elastic stability [Deng et al., 2021], collective stability [London et al., 2016] and uniform   \n88 stability in gradients [Lei, 2023, Fan and Lei, 2024] have been developed. Most of them provided the   \n89 connection on stability and generalization in expectation. Bousquet and Elisseeff [2002], Elisseeff   \n90 et al. [2005], Feldman and Vondrak [2018, 2019], Bousquet et al. [2020], Klochkov and Zhivotovskiy   \n91 [2021], Fan and Lei [2024] considered high probability bounds. However, only Fan and Lei [2024]   \n92 developed vector-valued bounds (eg: generalization bounds of gradients), which can be the order at   \n93most $\\bar{O}\\left(M/\\sqrt{n}\\right)$ and remain improvement.   \n94  Uniform convergence. Uniform convergence is another popular approach in statistical learning   \n95 theory to study generalization bounds [Fisher, 1922, Vapnik, 1999, Van der Vaart, 2000]. The main   \n96 idea is to bound the generalization gap by its supremum over the whole (or a subset) of the hypothesis   \n97 space via some space complexity measures, such as VC dimension, covering number and Rademacher   \n98 complexity. For finite-dimensional problem, Kleywegt et al. [2002] provided that the generalization   \n99  eror is $O\\left({\\sqrt{d/n}}\\right)$ depended on the sample size $n$ and the dimension of parameters $d$ in high   \n100 probability. In nonconvex settings, Mei et al. [2018] showed that the empirical of generalization error   \n101 is $O({\\sqrt{d/n}})$ . Xu and Zeevi [2024] developed a novel \u201cuniform localized convergence\u201d framework   \n102 using generic chaining for the minimization problem and provided the localized generalization bounds   \n103  in gradients $O\\left(\\operatorname*{max}\\left\\{\\|\\mathbf{w}-\\mathbf{w}^{*}\\|_{2},{\\frac{1}{n}}\\right\\}\\left({\\sqrt{\\frac{d}{n}}}+{\\frac{d}{n}}\\right)\\right)$ , which is the optimal result when we only   \n104consider the order of $n$ . However, uniform convergence results are related to the dimension $d$ , which   \n105  is unacceptable in high-dimensional learning problems. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "106 3 Stability and Generalization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "107 To derive sharper generalization bounds of gradients, we need to develop a novel concentration   \n108 inequality which provide $p$ -moment bound for sums of vector-valued functions. For a real-valued   \n109 random variable $Y$ , the $L_{p}$ -norm of $Y$ is defined by $\\|Y\\|_{p}:=(\\mathbb{E}[|Y|^{p}])^{\\frac{1}{p}}$ . Similarly, let $\\Vert\\cdot\\Vert$ denote   \n110 the norm in a Hilbert space $\\mathcal{H}$ . Then for a random variable $X$ taking values in a Hilbert space, the   \n111 $L_{p}$ -norm of $X$ is defined by $\\|\\|\\mathbf{X}\\|\\|_{p}:=(\\mathbb{E}\\left[\\|\\mathbf{X}\\|^{p}\\right])^{\\frac{1}{p}}$ ", "page_idx": 2}, {"type": "text", "text": "1123.1 A Moment Bound for Sums of Vector-valued Functions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "113 Here we present our sharper moment bound for sums of vector-valued functions of $n$ independent   \n114variables.   \n115 Theorem 1. Let $\\mathbf{Z}=(Z_{1},\\ldots,Z_{n})$ be a vector of independent random variables each taking values   \n116 in $\\mathcal{Z}$ and let $\\mathbf{g}_{1},\\ldots,\\mathbf{g}_{n}$ be some functions: $\\mathbf{g}_{i}:\\mathcal{Z}^{n}\\mapsto\\mathcal{H}$ such that the following holds for any   \n117 $i\\in[n]$ ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\ \\lVert\\mathbb{E}[\\mathbf{g}_{i}(\\mathbf{Z})\\vert Z_{i}]\\rVert\\le M\\,a.s.,}\\\\ &{\\bullet\\ \\mathbb{E}\\left[\\mathbf{g}_{i}(\\mathbf{Z})\\vert Z_{[n]\\backslash\\{i\\}}\\right]=0\\,a.s.,}\\end{array}\n$$118 119 ", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "120\u00b7 $\\mathbf{g}_{i}$ satisfies the bounded difference property with $\\beta$ namely, for any $i=1,\\dots,n$ , the following   \n121 inequalityholds ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{z_{1},\\ldots,z_{n},z_{j}^{\\prime}}\\big\\|\\mathbf{g}_{i}(z_{1},\\ldots,z_{j-1},z_{j},z_{j+1},\\ldots,z_{n})-\\mathbf{g}_{i}(z_{1},\\ldots,z_{j-1},z_{j}^{\\prime},z_{j+1},\\ldots,z_{n})\\big\\|\\leq\\beta.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "122 Then, for any $p\\geq2$ we have ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\left\\|\\left\\Vert\\sum_{i=1}^{n}\\mathbf{g}_{i}\\right\\Vert\\right\\Vert_{p}\\leq2(\\sqrt{2p}+1)\\sqrt{n}M+4\\times2^{\\frac{1}{2p}}\\left(\\sqrt{\\frac{p}{e}}\\right)(\\sqrt{2p}+1)n\\beta\\left[\\log_{2}n\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "123  Remark 1. The proof is motivated by Bousquet et al. [2020]. Under the same assumptions, Fan and   \n124 Lei [2024] also established the following inequality ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left\\|\\left\\|\\sum_{i=1}^{n}\\mathbf{g}_{i}\\right\\|\\right\\|_{p}\\leq2(\\sqrt{2}+1)\\sqrt{n p}M+4(\\sqrt{2}+1)n p\\beta\\left\\lceil\\log_{2}n\\right\\rceil.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "125 It is easy to verify that our result is tighter than result provided by Fan and Lei [2024] for both the   \n126  first and second term. Comparing Theorem 1 with (2), the larger $p$ is, the tighter our result is relative   \n127to (2). In the worst case, when $p=2$ , the constant of our first term is 0.879 times tighter than (2),   \n128  and the constant of our second term is 0.634 times tighter than (2). This is because we derive the   \n129 optimal Marcinkiewicz-Zygmund's inequality for random variables taking values in a Hilbert space   \n130 in the proof.   \n131  The improvement of this concentration inequality is meaningful. On one hand, we derive the optimal   \n132 Marcinkiewicz-Zygmund's inequality for random variables taking values in a Hilbert space. On the   \n133 other hand, in Section 3.2, we will carefully construct vector-valued functions which satisfies all   \n134 the assumptions in Theorem 1 and ensures $M=0$ at the same time. Under this condition, we can   \n135  eliminate the first term. When we use Theorem 1 instead of (2) in the whole proofs, at least 0.634   \n136  times tighter bound can be obtained strictly. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "1373.2 Sharper Generalization Bounds in Gradients ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "138 Let $S=\\{z_{1},\\ldots,z_{n}\\}$ be a set of independent random variables each taking values in $\\mathcal{Z}$ and $S^{\\prime}=$   \n139 $\\{z_{1}^{\\prime},\\ldots,z_{n}^{\\prime}\\}$ be its independent copy. For any $i\\in[n]$ , define ${\\cal{S}}^{(i)}=\\{z_{i},\\ldots,z_{i-1},z_{i}^{\\prime},z_{i+1},\\ldots,z_{n}\\}$   \n140 be a dataset replacing the $i$ -th sample in $S$ with another i.i.d. sample $z_{i}^{\\prime}$ . We introduce some basic   \n141 definitions here and we want to emphasize that our main Theorem 2 and Theorem 3 do not need   \n142 smoothness assumption and PL condition. ", "page_idx": 3}, {"type": "text", "text": "143Definition 1. Let $g:\\mathcal{W}\\mapsto\\mathbb{R}$ Let $\\gamma,\\mu<0$ ", "page_idx": 3}, {"type": "text", "text": "144 ", "page_idx": 3}, {"type": "text", "text": "\u00b7 We say $g$ is $\\gamma$ -smoothif ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\|\\nabla g(\\mathbf{w})-\\nabla g(\\mathbf{w}^{\\prime})\\|_{2}\\leq\\gamma\\|\\mathbf{w}-\\mathbf{w}^{\\prime}\\|_{2},\\quad\\forall\\mathbf{w},\\mathbf{w}^{\\prime}\\in\\mathcal{W}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "145 ", "page_idx": 3}, {"type": "text", "text": "146 ", "page_idx": 3}, {"type": "text", "text": "\u00b7 Let $\\begin{array}{r}{g\\ast=\\operatorname*{min}_{\\mathbf{w}\\in\\mathcal{W}}g(\\mathbf{w}).}\\end{array}$ We say $g$ satisfies the Polyak-Lojasiewicz $(P L)$ condition with parameter $\\mu>0$ on $\\mathcal{W}\\,i f$ ", "page_idx": 3}, {"type": "equation", "text": "$$\ng(\\mathbf{w})-g\\ast\\leq\\frac{1}{2\\mu}\\|\\nabla g(\\mathbf{w})\\|_{2}^{2},\\quad\\forall\\mathbf{w}\\in\\mathcal{W}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "147 Then we define uniform stability in gradients. ", "page_idx": 3}, {"type": "text", "text": "148 Definition 2 (Uniform Stability in Gradients). Let $A$ be a randomized algorithm.We say $A$ is   \n149 $\\beta$ -uniformly-stable in gradients if for all neighboring datasets $S,S^{(i)}$ wehave ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\underset{z}{\\operatorname*{sup}}\\left\\|\\nabla f(A(S);z)-\\nabla f(A(S^{(i)});z)\\right\\|_{2}\\le\\beta.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "150  Remark 2. Gradient-based stability was firstly introduced by Lei [2023], Fan and Lei [2024] to   \n151 describe the generalization performance for nonconvex problems. In nonconvex problems, we can   \n152 only find a local minimizer by optimization algorithms which may be far away from the global   \n153  minimizer. Thus the convergence does not make much sense in function values.  Instead, the   \n154 convergence of $\\|\\nabla F_{S}(A(S))\\|_{2}$ was often studied in the optimization community [Ghadimi and Lan,   \n155 2013]. Since the population risk of gradients $\\|\\nabla F(A(S))\\|_{2}$ can be decomposed as the convergence   \n156 of $\\big\\|\\nabla F_{S}\\big(A(S)\\big)\\big\\|_{2}$ and the_ generalization gap $\\|\\dot{\\nabla}F(\\dot{A}(\\dot{S}))-\\nabla F_{S}(A(\\bar{S}))\\|_{2}$ , the generalization   \n157   analysis of $\\|\\nabla{\\dot{F}}{\\(\\boldsymbol{A}(S))}-{\\bar{\\nabla}}F_{S}(\\boldsymbol{A}(S))\\|_{2}$ is important, which can be achieved by uniform stability   \n158  in gradients.   \n159 Theorem 2 (Generalization via Stability in Gradients). Assume for any $S$ andany $z$ \uff0c   \n160 $\\|\\nabla f(A(S);z)\\|_{2}\\;\\leq\\;M$ f $A$ is $\\beta$ -uniformly-stable in gradients, then for any $\\delta~\\in~(0,1)$ \uff0cthe ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "161followinginequality holds with probability at least $1-\\delta$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\nabla F(A(S))-\\nabla F_{S}(A(S))\\|_{2}}\\\\ &{\\le2\\beta+\\displaystyle\\frac{4M\\left(1+e\\sqrt{2\\log\\left(e/\\delta\\right)}\\right)}{\\sqrt{n}}+8\\times2^{\\frac{1}{4}}(\\sqrt{2}+1)\\sqrt{e}\\beta\\left\\lceil\\log_{2}n\\right\\rceil\\log\\left(e/\\delta\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "162  Remark 3. Theorem 2 is a direct application via Theorem 1 where we denote $\\begin{array}{r l}{\\mathbf{g}_{i}(S)}&{{}=}\\end{array}$   \n163 $\\mathbb{E}_{z_{i}^{\\prime}}\\left[\\mathbb{E}_{Z}\\left[\\nabla f(A(S^{(i)}),Z)\\right]-\\nabla f(A(S^{(i)}),z_{i})\\right]$ and find that $\\mathbf{g}_{i}(S)$ satisfies all the assumptions   \n164 in Theorem 1. As a comparison, Fan and Lei [2024] also developed high probability bounds under   \n165 same assumptions, but our bounds are sharper since our moment inequality for sums of vector-valued   \n166 functions are tighter as we have discussed in Remark 1. Next, we derive sharper generalization bound   \n167 of gradients under same assumptions.   \n168Theorem 3 (Sharper Generalization via Stability in Gradients). Assume for any $S$ and any $z$ \uff0c   \n169 $\\|\\nabla f(A(S);z)\\|_{2}\\le M.$ If $A$ is $\\beta$ -uniformly-stableingradients,thenforany $\\delta\\in(0,1)$ , the following   \n170inequalityholdswith probability atleast $1-\\delta$ ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\nabla F(A(S))-\\nabla F_{S}(A(S))\\right\\|_{2}}\\\\ &{\\leq\\sqrt{\\frac{4\\mathbb{E}_{Z}\\left[\\|\\nabla f(A(S);Z)\\|_{2}^{2}\\right]\\log\\frac{6}{\\delta}}{n}}+\\sqrt{\\frac{\\left(\\frac{1}{2}\\beta^{2}+32n\\beta^{2}\\log(3/\\delta)\\right)\\log\\frac{6}{\\delta}}{n}}+\\frac{M\\log\\frac{6}{\\delta}}{n}}\\\\ &{\\qquad+\\,16\\times2^{\\frac{3}{4}}\\sqrt{e}\\beta\\left[\\log_{2}n\\right]\\log\\left(3e/\\delta\\right)+32\\sqrt{e}\\beta\\left[\\log_{2}n\\right]\\sqrt{\\log{3e/\\delta}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "171 Remark 4. Note that the factor in Theorem 2 before $1/\\sqrt{n}$ is $O\\left(M\\sqrt{\\log\\left(e/\\delta\\right)}\\right)$ ,which   \n172  depends on the bound of $\\|\\nabla f(\\cdot,\\cdot)\\|_{2}$ .However, the factor in Theorem 3 before $1/\\sqrt{n}$ is   \n173 $O\\left(\\sqrt{\\mathbb{E}_{Z}\\left[\\|\\nabla f(A(S);Z)\\|_{2}^{2}\\right]\\log1/\\delta}+\\beta\\log(1/\\delta)\\right)$ , not involving the possibly large term $M$ .As is   \n174 known, optimization algorithms often provide parameters approaching the optimal solution, which   \n175  make the term $\\mathbb{E}_{Z}[\\|\\nabla\\bar{f}(A(S);Z)\\|_{2}^{2}]$ much more smaller than $M$ . We will give further reasonable   \n176  results under more assumptions such as smoothness in Lemma 1 and Lemma 2.   \n177 On the other hand, best high probability bounds based on uniform convergence [Xu and Zeevi, 2024]   \n178is ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\nabla F(A(S))-\\nabla F_{S}(A(S))\\|_{2}}\\\\ &{\\lesssim\\!\\sqrt{\\frac{\\mathbb{E}_{Z}\\left[\\nabla\\|f(\\mathbf{w}^{*};Z)\\|_{2}^{2}\\right]\\log(1/\\delta)}{n}}+\\frac{\\log(1/\\delta)}{n}+\\operatorname*{max}\\left\\{\\|\\mathbf{w}-\\mathbf{w}^{*}\\|_{2},\\frac{1}{n}\\right\\}\\left(\\sqrt{\\frac{d}{n}}+\\frac{d}{n}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "179 which is the optimal result when we only consider the order of $n$ . However, uniform convergence   \n180 results are related to the dimension $d$ , which is unacceptable in high-dimensional learning problems.   \n181  Note that (4) requires an additional smoothness-type assumption. As a comparison, when $f$ is   \n182 $\\gamma$ -smoothness, our result in Theorem 3 can be easily derived as ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\|\\nabla F(A(S))-\\nabla F_{S}(A(S))\\|_{2}}\\\\ &{\\lesssim\\beta\\log n\\log(1/\\delta)+\\frac{\\log(1/\\delta)}{n}+\\sqrt{\\frac{\\mathbb{E}_{Z}\\left[\\nabla\\|f(\\mathbf{w}^{*};Z)\\|_{2}^{2}\\right]\\log(1/\\delta)}{n}}+\\|A(S)-\\mathbf{w}^{*}\\|\\sqrt{\\frac{\\log(1/\\delta)}{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "183 This result implies that when the uniformly stable in gradients parameter $\\beta$ is smaller than $1/\\sqrt{n}$ ,our   \n184  bound is tighter than (4) and is dimension independent. Note that Theorem 3 holds in nonconvex   \n185 problems, to the best of our knowledge, this is the sharpest upper bound in both uniform convergence   \n186 and algorithmic stability analysis.   \n187  Here we give the proof sketch of Theorem 3, which is motivated by the analysis in Klochkov and   \n188 Zhivotovskiy [2021]. The key idea is to build vector functions $\\dot{\\mathbf{q}_{i}(S)}=\\mathbf{h}_{i}(S)-\\mathbb{E}_{S\\{z_{i}\\}}[\\mathbf{h}_{i}(S)]$   \n189  where we define $\\mathbf{h}_{i}(S)=\\mathbb{E}_{z_{i}^{\\prime}}\\left[\\mathbb{E}_{Z}\\left[\\nabla f(A(S^{(i)}),Z)\\right]-\\nabla f(A(S^{(i)}),z_{i})\\right]$ These functions satisfy   \n190 all the assumptions in Theorem 1 and ensure the factor $M$ in Theorem 1 to 0. Then the term $O(1/{\\sqrt{n}})$   \n191 can be eliminated.   \n192 Lemma 1. Let assumptions in Theorem 3 hold. Suppose the function $f$ .s $\\gamma$ -smoothandthepopulation   \n193 risk $F$ satisfies the $P L$ conditionwith parameter $\\mu$ Then for any $\\delta\\in(0,1)$ when $n\\geq\\frac{16\\gamma^{2}\\log\\frac{6}{\\delta}}{\\mu^{2}}$   \n194with probability at least $1-\\delta$ we have ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\nabla F(A(S))-\\nabla F_{S}(A(S))\\right\\|_{2}}\\\\ &{\\le\\!\\|\\nabla F_{S}(A(S))\\|_{2}+4\\sqrt{\\frac{\\mathbb{E}_{Z}\\,[\\|\\nabla f(\\mathbf{w}^{*};Z)\\|_{2}^{2}]\\log\\frac{6}{\\delta}}{n}}+2\\sqrt{\\frac{\\left(\\frac{1}{2}\\beta^{2}+32n\\beta^{2}\\log(3/\\delta)\\right)\\log\\frac{6}{\\delta}}{n}}}\\\\ &{\\qquad+\\,\\frac{2M\\log\\frac{6}{\\delta}}{n}+32\\times2^{\\frac{3}{4}}\\sqrt{e}\\beta\\left[\\log_{2}n\\right]\\log\\left(3e/\\delta\\right)+64\\sqrt{e}\\beta\\left[\\log_{2}n\\right]\\sqrt{\\log{3e/\\delta}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "195  Remark 5. The following inequality can be easily derived using triangle inequality and Cauchy  \n196 Bunyakovsky-Schwarz inequality: ", "page_idx": 5}, {"type": "equation", "text": "$$\nF(A(S))-F(\\mathbf{w}^{*})\\lesssim\\|\\nabla F_{S}(A(S))\\|_{2}+\\frac{F(\\mathbf{w}^{*})\\log{(1/\\delta)}}{n}+\\frac{\\log^{2}(1/\\delta)}{n^{2}}+\\beta^{2}\\log^{2}n\\log^{2}(1/\\delta).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "197 Above inequality implies that excess risk can be bound by the optimization gradient error   \n198 $\\|\\nabla F_{S}(A(S)\\bar{)}\\|_{2}$ and uniform stability in gradients $\\beta$ . Note that the assumption $F(\\bar{\\mathbf{w}}^{*})=O(1/n)$   \n199 is common and can be found in Srebro et al. [2010], Lei and Ying [2020], Liu et al. [2018], Zhang   \n200 et al. [2017], Zhang and Zhou [2019]. This is natural since $F(\\mathbf{w}^{*})$ is the minimal population risk.   \n201 On the other hand, we can derive that under $\\mu$ -strongly convex and $\\gamma$ -smooth assumptions for the   \n202 objective function $f$ , uniform stability in gradients can be bounded of order $O(1/n)$ for ERM and   \n203 PGD. Thus high probability excess risk can be bounded of order up to $O\\left(1/n^{2}\\right)$ under these common   \n204 assumptions via algorithmic stability. Comparing with current best related work [Klochkov and   \n205 Zhivotovskiy, 2021], they are insensitive to the stability parameter being smaller than ${\\cal O}(1/n)$ and   \n206 their best rates can only up to ${\\cal O}(1/n)$ . Although we involve extra smoothness and PL condition   \n207 assumptions, these assumptions are also common in optimization community and our work can fully   \n208utilize these assumptions.   \n209  Besides, we discuss uniform stability in gradients for common algorithms such as ERM, PGD, and   \n210SGD in Section 4. Our results can be easily extended to other stable algorithms. Due to smoothness's   \n211 property to link the uniform stability in gradients with uniform argument stability, many works   \n212 [Bassily et al., 2020, Feldman and Vondrak, 2019, Hardt et al., 2016] exploring uniform argument   \n213stability can also use our framework.   \n214  Finally, the population risk of gradients $\\|\\nabla F(A(S))\\|_{2}$ can be gracefully bounded by the empirical   \n215 risk of gradients $\\|\\nabla F_{S}(A(S)\\bar{)}\\|_{2}$ under strong growth condition (SGC), that connects the rates at   \n216 which the stochastic gradients shrink relative to the full gradient Vaswani et al. [2019]. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "217 Definition 3 (Strong Growth Condition). We say SGC holds if ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{Z}\\left[\\|\\nabla f(\\mathbf{w};Z)\\|_{2}^{2}\\right]\\le\\rho\\|\\nabla F(\\mathbf{w})\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "218  Remark 6. There has been some related work that takes SGC into assumption Solodov [1998],   \n219 Vaswani et al. [2019], Lei [2023]. Vaswani et al. [2019] has proved that the squared-hinge loss with   \n220 linearly separable data and finite support features satisfies the SGC. Note that we only suppose this   \n221condition holds in Lemma 2.   \n222Lemma 2 (SGC case). Let assumptions in Theorem 3 hold and suppose SGC holds. Then for any   \n223 $\\delta>0$ withprobabilityatleast $1-\\delta$ wehave ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|\\nabla F(A(S))\\|\\lesssim(1+\\eta)\\|\\nabla F_{S}(A(S))\\|+\\frac{1+\\eta}{\\eta}\\left(\\frac{M}{n}\\log\\frac{6}{\\delta}+\\beta\\log n\\log\\frac{1}{\\delta}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "224 Remark 7. Lemma 2 build a connection between the population gradient error and the empirical   \n225 gradient error under Lipschitz, nonconvex, nonsmooth and SGC case and elucidate that the population   \n226 gradient error can be bounded of up to $O(1/n)$ under nonconvex problems. ", "page_idx": 5}, {"type": "text", "text": "2274Application ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "228In this section, we analysis stochastic convex optimization with strongly convex losses. The most   \n229 common setting is where at each round, the learner gets information on $f$ through astochastic   \n230 gradient oracle [Rakhlin et al., 2012]. To derive uniform stability in gradients for algorithms, we   \n231 firstly introduce the strongly convex assumption. ", "page_idx": 5}, {"type": "text", "text": "232Definition 4. We say $g$ is $\\mu$ -strongly convex if ", "page_idx": 6}, {"type": "equation", "text": "$$\ng(\\mathbf{w})\\geq g(\\mathbf{w}^{\\prime})+\\langle\\mathbf{w}-\\mathbf{w}^{\\prime},\\nabla g(\\mathbf{w}^{\\prime})\\rangle+\\frac{\\mu}{2}\\|\\mathbf{w}-\\mathbf{w}^{\\prime}\\|_{2}^{2},\\quad\\forall\\mathbf{w},\\mathbf{w}^{\\prime}\\in\\mathcal{W}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "233  4.1 Empirical Risk Minimizer ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "234 Empirical risk minimizer is one of the classical approaches for solving stochastic optimization (also   \n235 referred to as sample average approximation (SAA) in machine learning community. The following   \n236 lemma shows the uniform stability in gradient for ERM under $\\mu$ -strongly convexity and $\\gamma$ -smoothness   \n237 assumptions.   \n238 Lemma 3 (Stability of ERM). Suppose the objective function $f$ is $\\mu$ -strongly-convex and $\\gamma$ -smooth.   \n239  For any $\\mathbf{w}\\in\\mathcal{W}$ and any $z$ ,suppose that $\\|\\nabla f(\\mathbf{w};z)\\le M\\|$ . Let $\\hat{\\mathbf{w}}^{*}(S^{(i)})$ be the ERM of $F_{S^{(i)}}(\\mathbf{w})$   \n240that denotes the empiricalrisk on the samples ${\\cal S}^{(i)}=\\{z_{1},...,z_{i}^{\\prime},...,z_{n}\\}$ and $\\mathrm{i}\\tau^{*}(S)$ be the ERM of   \n241 $F_{S}(\\mathbf{w})$ on the samples $S=\\{z_{1},...,z_{i},...,z_{n}\\}$ . For any $S^{(i)}$ and $S$ , there holds the following uniform   \n242 stability bound of ERM: ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\forall z\\in\\mathcal{Z},\\quad\\left\\|\\nabla f(\\hat{\\mathbf{w}}^{*}(S^{(i)});z)-\\nabla f(\\hat{\\mathbf{w}}^{*}(S);z)\\right\\|_{2}\\leq\\frac{4M\\gamma}{n\\mu}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "243 Then, we present the application of our main sharper Theorem 3. In the strongly convex and smooth   \n244 case, we provide a up to $O\\left(1/n^{2}\\right)$ high probability excess risk guarantee valid for any algorithms   \n245 depending on the optimal population error $F(\\mathbf{w}^{*})$   \n246 Theorem 4. Let assumptions in Theorem 3 and Lemma 3 hold. Suppose the function $f$ isnonnegative.   \n247Thenfor any $\\delta\\in(0,1)$ when $n\\geq\\frac{16\\gamma^{2}\\log\\frac{6}{\\delta}}{\\mu^{2}}$ with probabilityat least $1-\\delta$ we have ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\nF(\\hat{\\mathbf{w}})-F(\\mathbf{w}^{*})\\lesssim\\frac{F(\\mathbf{w}^{*})\\log\\left(1/\\delta\\right)}{n}+\\frac{\\log^{2}n\\log^{2}(1/\\delta)}{n^{2}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "248 Furthermore, assume $\\begin{array}{r}{F(\\mathbf{w}^{*})=O(\\frac{1}{n})}\\end{array}$ we have ", "page_idx": 6}, {"type": "equation", "text": "$$\nF(\\hat{\\mathbf{w}})-F(\\mathbf{w}^{*})\\lesssim\\frac{\\log^{2}n\\log^{2}(1/\\delta)}{n^{2}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "249 Remark 8. Theorem 4 shows that when the objective function $f$ is $\\mu$ -strongly convex, $\\gamma.$ -smooth   \n250and nonnegative, high probability risk bounds can even up to $O\\left(1/n^{2}\\right)$ for ERM. The most related   \n251 work to ours is Zhang et al. [2017]. They also obtain the $O\\left(1/n^{2}\\right)$ -type bounds for ERM by uniform   \n252 convergence of gradients approach. However, they need the sample number $n=\\Omega(\\gamma d/\\mu)$ , which   \n253 is related to the dimension $d$ Our risk bounds are dimension independent and only require the   \n254 sample number $n=\\Omega(\\gamma^{2}/\\mu^{2})$ . Comparing with Klochkov and Zhivotovskiy [2021], we add two   \n255 assumptions, smoothness and $F(\\mathbf{w}^{*})\\,=\\,O(1/n)$ , but our bounds also tighter, from $O(1/n)$ to   \n256 $O\\left(1/n^{2}\\right)$ ", "page_idx": 6}, {"type": "text", "text": "257   4.2 Projected Gradient Descent ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "258 Note that when the objective function $f$ is strongly convex and smooth, the optimization error can be   \n259 ignored. However, the generalization analysis framework proposed by Klochkov and Zhivotovskiy   \n260 [2021] does not use smoothness assumption, which only derive high probability excess risk bound   \n261 of order $O(1/n)$ after $T=O(\\log n)$ steps under strongly convex and smooth assumptions. In this   \n262  subsection, we provide sharper risk bound under the same iteration steps, which is because our   \n263 generalization analysis also fully utilized the smooth assumptions. Here we give the definition of   \n264PGD.   \n265 Definition 5 (Projected Gradient Descent). Let $\\mathbf{w}_{1}=o\\in\\mathbb{R}^{d}$ be an initial point and $\\{\\eta_{t}\\}_{t}$ be $a$   \n266 sequence of positive step sizes. PGD updates parameters by ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{w}_{t+1}={\\Pi_{\\mathcal{W}}}\\left({\\mathbf{w}_{t}}-{\\eta_{t}}\\nabla{F_{S}}\\left({\\mathbf{w}_{t}}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "267 where $\\nabla F_{S}(\\mathbf{w}_{t})$ denotes a subgradient of $F$ w.r.t. ${\\bf w}_{t}$ and $\\Pi_{\\mathcal{W}}$ is the projection operator onto $\\mathcal{W}$ ", "page_idx": 6}, {"type": "text", "text": "268 Lemma 4 (Stability of Gradient Descent). Suppose the objective function $f$ is $\\mu$ -strongly-convex   \n269 and $\\gamma$ -smooth.For any $\\mathbf{w}\\in\\mathcal{W}$ and any $z$ ,suppose that $\\|\\nabla f(\\mathbf{w};z)\\|_{2}\\leq M$ Let $\\mathbf{w}_{t}^{i}$ be the output of   \n270 $F_{S^{(i)}}(\\mathbf{w})$ on $t$ -thiteration on the samples $S^{(i)}=\\left\\{z_{1},...,z_{i}^{\\prime},...,z_{n}\\right\\}$ in running $P G D$ and ${\\bf w}_{t}$ be the   \n271 output of $F_{S}(\\mathbf{w})$ on $t$ -thiterationonthesamples $S=\\{z_{1},...,z_{i},...,z_{n}\\}$ in running $P G D$ Let the   \n272  constant step size $\\eta_{t}=1/\\gamma$ . For any $S^{(i)}$ and $S$ there holds the following uniform stability bound of   \n273 $P G D$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\forall z\\in\\mathcal{Z},\\quad\\left\\|\\nabla f(\\hat{\\mathbf{w}}^{*}(S^{(i)});z)-\\nabla f(\\hat{\\mathbf{w}}^{*}(S);z)\\right\\|_{2}\\leq\\frac{4M\\gamma}{n\\mu}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "274 Remark 9. The derivations of Feldman and Vondrak [2019] in Section 4.1.2 (See also Hardt et al.   \n275 [2016] in Section 3.4) imply that if the objective function $f$ is $\\gamma.$ -smooth in addition to $\\mu$ -strongly   \n2 convxity and $M$ Lipschitz propety thenPGD wit the constantste ie $\\eta\\,=\\,1/\\gamma$ $\\left({\\frac{2M}{n\\mu}}\\right)$   \n277 unifomly argument sable fr any number of steps, which means that PGD is (2M) uniformly  \n278  stable in gradients regardless of iteration steps.   \n279Theorem 5. Let assumptions in Theorem 3 and Lemma 3 hold. Suppose the function $f$ is nonnegative.   \n280 Let $\\{\\mathbf{w}_{t}\\}_{t}$ be the sequence produced by PGD with $\\eta_{t}\\,=\\,1/\\gamma$ Then for any $\\delta\\ \\in\\ (0,1)$ when   \n281 $\\begin{array}{r}{n\\geq\\frac{16\\gamma^{2}\\log\\,\\frac{6}{\\delta}}{\\mu^{2}}}\\end{array}$ ,with probability at least $1-\\delta$ we have ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\nF(\\mathbf{w})-F(\\mathbf{w}^{*})\\lesssim\\left(1-\\frac{\\mu}{\\gamma}\\right)^{2T}+\\frac{F(\\mathbf{w}^{*})\\log{(1/\\delta)}}{n}+\\frac{\\log^{2}{n\\log^{2}(1/\\delta)}}{n^{2}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "282 Furthermore, assume $\\begin{array}{r}{F(\\mathbf{w}^{*})=O(\\frac{1}{n})}\\end{array}$ and let $T\\asymp\\log n$ we have ", "page_idx": 7}, {"type": "equation", "text": "$$\nF(\\hat{\\mathbf{w}})-F(\\mathbf{w}^{*})\\lesssim\\frac{\\log^{2}n\\log^{2}(1/\\delta)}{n^{2}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "283 Remark 10. Theorem 5 shows that under the same assumptions as Klochkov and Zhivotovskiy [2021],   \n28  our bound is $\\begin{array}{r}{O\\left(\\frac{F(\\mathbf{w}^{*})\\log(1/\\delta)}{n}+\\frac{\\log^{2}n\\log^{2}(1/\\delta)}{n^{2}}\\right)}\\end{array}$ . Comparing with their bound $O\\left(\\frac{\\log n\\log(1/\\delta)}{n}\\right)$   \n285  we are sharper because $F(\\mathbf{w}^{*})$ is the minimal population risk, which is a common assumption   \n286 towards sharper risk bounds Srebro et al. [2010], Lei and Ying [2020], Liu et al. [2018], Zhang et al.   \n287 [2017], Zhang and Zhou [2019]. ", "page_idx": 7}, {"type": "text", "text": "288 4.3 Stochastic Gradient Descent ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "289 Stochastic gradient descent optimization algorithm has been widely used in machine learning due to   \n290 its simplicity in implementation, low memory requirement and low computational complexity per   \n291 iteration, as well as good practical behavior. Here we give the definition of standard SGD.   \n292  Definition 6 (Stochastic Gradient Descent). Let $\\mathbf{w}_{1}=o\\in\\mathbb{R}^{d}$ be an initial point and $\\{\\eta_{t}\\}_{t}$ be $a$   \n293 sequence of positive step sizes. SGD updates parameters by ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{w}_{t+1}=\\Pi_{\\mathcal{W}}\\left(\\mathbf{w}_{t}-\\eta_{t}\\nabla{f}\\left(\\mathbf{w}_{t};z_{i_{t}}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "294  where $\\nabla f(\\mathbf{w}_{t};z_{i_{t}})$ denotes a subgradient of $f\\ w.r.t$ ${\\bf w}_{t}$ and $i_{t}$ is independently drawn from the   \n295  uniform distribution over $[n]:=\\{1,2,\\dots,n\\}$   \n296  Lemma 5 (Stability of SGD). Suppose the objective function $f$ is $\\mu$ -strongly-convex and $\\gamma$ -smooth.   \n297  For any $\\mathbf{w}\\in\\mathcal{W}$ and any $z$ ,suppose that $\\|\\nabla f(\\mathbf{w};z)\\|_{2}\\leq M$ Let $\\mathbf{\\dot{w}}_{t}^{i}$ be the output of $F_{S^{(i)}}$ (w) on   \n298 $t$ th iteration on theamples ${\\cal S}^{(i)}=\\{z_{1},...,z_{i}^{\\prime},...,z_{n}\\}$ in running $P G D$ and and ${\\bf w}_{t}$ be the output of   \n299 $F_{S}(\\mathbf{w})$ on $t$ -th iteration on the samples $S=\\{z_{1},...,z_{i},...,z_{n}\\}$ in running SGD. For any $S^{(i)}$ and $S$   \n300 there holds the following uniform stability bound of $S G D$ ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\big\\|\\nabla f(\\mathbf{w}_{t};z)-\\nabla f(\\mathbf{w}_{t}^{i};z)\\big\\|_{2}\\leq2\\gamma\\sqrt{\\frac{2\\epsilon_{o p t}(\\mathbf{w}_{t})}{\\mu}}+\\frac{4M\\gamma}{n\\mu},\\quad\\forall z\\in\\mathcal{Z},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "301 where $\\epsilon_{o p t}(\\mathbf{w}_{t})=F_{S}(\\mathbf{w}_{t})-F_{S}(\\hat{\\mathbf{w}}^{*}(S))$ and $\\hat{\\mathbf{w}}^{*}(S)$ is the ERM of $F_{S}(\\mathbf{w})$ ", "page_idx": 7}, {"type": "text", "text": "302 Next, we introduce a necessary assumption in stochastic optimization theory. ", "page_idx": 7}, {"type": "text", "text": "303 Assumption 1. Assume the existence of $\\sigma>0$ satisfying ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{i_{t}}[\\|\\nabla f(\\mathbf{w}_{t};z_{i_{t}})-\\nabla F_{S}(\\mathbf{w}_{t})\\|_{2}^{2}]\\leq\\sigma^{2},\\quad\\forall t\\in\\mathbb{N},}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "304  where $\\mathbb{E}_{i_{t}}$ denotes the expectation w.r.t. $i_{t}$ ", "page_idx": 8}, {"type": "text", "text": "305 Remark 11. Assumption 1 is a standard assumption from the stochastic optimization theory [Ne  \n306 mirovski et al., 2009, Ghadimi and Lan, 2013, Ghadimi et al., 2016, Kuzborskij and Lampert, 2018,   \n307  Zhou et al., 2018, Bottou et al., 2018, Lei and Tang, 2021], which essentially bounds the variance of   \n308 the stochastic gradients for dataset $S$   \n309Theorem 6. Let assumptions in Theorem $^3$ and Lemma 5 hold. Suppose Assumption 1 holds and the   \n310 function $f$ is nonnegative.Let $\\{\\mathbf{w}_{t}\\}_{t}$ be the sequence produced by SGD with $\\dot{\\eta_{t}}=\\eta_{1}t^{-\\theta},\\theta\\in\\left(0,1\\right)$   \n311 and $\\begin{array}{r}{\\eta_{1}<\\frac{1}{2\\gamma}}\\end{array}$ . Then for any $\\delta\\in(0,1)$ when $\\begin{array}{r}{n\\geq\\frac{16\\gamma^{2}\\log\\frac{6}{\\delta}}{\\mu^{2}}}\\end{array}$ , with probabiliy at least $1-\\delta$ we have ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\displaystyle\\sum_{t=1}^{T}\\eta_{t}\\right)^{-1}\\displaystyle\\sum_{t=1}^{T}\\eta_{t}\\|\\nabla F(\\mathbf{w}_{t})\\|_{2}^{2}}\\\\ &{=\\left\\{O\\left(\\frac{\\log^{2}n\\log^{3}(1/\\delta)}{T^{-\\theta}}\\right)+O\\left(\\frac{\\log^{2}n\\log^{2}(1/\\delta)}{n^{2}}+\\frac{F(\\mathbf{w}^{*})\\log^{2}(1/\\delta)}{n}\\right),\\;\\;\\;i f\\theta<1/2\\right.}\\\\ &{=\\left\\{O\\left(\\frac{\\log^{2}n\\log^{3}(1/\\delta)}{T^{-\\frac{1}{2}}}\\right)+O\\left(\\frac{\\log^{2}n\\log^{2}(1/\\delta)}{n^{2}}+\\frac{F(\\mathbf{w}^{*})\\log^{2}(1/\\delta)}{n}\\right),\\;\\;\\;i f\\theta=1/2\\right.}\\\\ &{\\left.O\\left(\\frac{\\log^{2}n\\log^{3}(1/\\delta)}{T^{\\theta-1}}\\right)+O\\left(\\frac{\\log^{2}n\\log^{2}(1/\\delta)}{n^{2}}+\\frac{F(\\mathbf{w}^{*})\\log^{2}(1/\\delta)}{n}\\right),\\;\\;\\;i f\\theta>1/2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "312 Remark 12. When $\\theta<1/2$ , we take $T\\asymp n^{2/\\theta}$ . When $\\theta=1/2$ , we take $T\\asymp n^{4}$ and when $\\theta>1/2$   \n313  we set $T\\asymp n^{2/(1-\\theta)}$ . Then according to Theorem 6, the population risk of gradient is bounded by   \n314 $\\begin{array}{r}{O\\left(\\frac{\\log^{2}n\\log^{3}(1/\\delta)}{n^{2}}+\\frac{F(\\mathbf{w}^{*})\\log^{2}(1/\\delta)}{n}\\right)}\\end{array}$   \n315   population gradient bound $\\|\\nabla F_{(}\\mathbf{w}_{t})\\|_{2}$ for SGD via algorithmic stability.   \n316 Theorem 7. Let Assumptions in Theorem 3 and Lemma 5 hold. Suppose Assumption 1 holds and the   \n317 function f is nonnegative. Let wt)t be the sequence produced by SGD with n = \u03bc(t+to) suchthat   \n318 $t_{0}\\ge\\operatorname*{max}\\left\\{\\frac{4\\gamma}{\\mu},1\\right\\}$ Then for any $\\delta>0$ when $\\begin{array}{r}{n\\geq\\frac{16\\gamma^{2}\\log\\frac{6}{\\delta}}{\\mu^{2}}}\\end{array}$ and $T\\asymp n^{2}$ withprobablityat least   \n319 $1-\\delta$ we have ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "equation", "text": "$$\nF(\\mathbf{w}_{T+1})-F(\\mathbf{w}^{*})=O\\left(\\frac{\\log^{4}n\\log^{5}(1/\\delta)}{n^{2}}+\\frac{F(\\mathbf{w}^{*})\\log(1/\\delta))}{n}\\right).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "320 Furthermore, assume $\\begin{array}{r}{F(\\mathbf{w}^{*})=O(\\frac{1}{n})}\\end{array}$ we have ", "page_idx": 8}, {"type": "equation", "text": "$$\nF(\\mathbf{w}_{T+1})-F(\\mathbf{w}^{*})=O\\left(\\frac{\\log^{4}n\\log^{5}(1/\\delta)}{n^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "321Remark 13. Theorem 7 implies that high probability risk bounds for SGD optimization algorithm   \n322  can be up to $O(1/n^{2})$ and the rate is dimension-free in high-dimensional learning problems. We   \n323 compare Theorem 7 with most related work. For algorithmic stability, high probability risk bounds in   \n324 Fan and Lei [2024] is up to ${\\cal O}(1/n)$ when choosing optimal iterate number $T$ for SGD optimization   \n325 algorithm. To the best of knowledge, we are faster than all the existing bounds. The best high   \n326 probability risk bounds of order $O(\\Bar{1}/n^{2})$ are given by Li and Liu [2021] via uniform convergence,   \n327  which require the sample number $n=\\Omega(\\gamma d/\\mu)$ depending on dimension $d$ ", "page_idx": 8}, {"type": "text", "text": "328 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "329  In this paper, we improve a $p$ -moment concentration inequality for sums of vector-valued functions.   \n330 By carefully constructing functions, we apply this moment concentration to derive sharper gener  \n331 alization bounds in gradients in nonconvex problems, which can further be used to obtain sharper   \n332 high probability excess risk bounds for stable optimization algorithms. In application, we study three   \n333 common algorithms: ERM, PGD, SGD. To the best of our knowledge, we provide the sharpest high   \n334probability dimension independent $O(1/n^{2})$ -type for these algorithms.   \n336R.Bassily, V Feldman, C. Guman, and K. Talwar. Stability of stochastic gradient descent on nonsmooth convex   \n337 losses. In Proceedings of the 34th International Conference on Neural Information Processing Systems   \n338 (NeurIPS), volume 33, pages 4381-4391, 2020.   \n339 L. Bottou, F E. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning. SIAM review,   \n340 60(2):223-311, 2018.   \n341 O. Bousquet and A. Elisseeff. Stability and generalization. The Journal of Machine Learning Research, 2:   \n342 499-526, 2002.   \n343 O. Bousquet, Y. Klochkov, and N.Zhivotovskiy. Sharper bounds for uniformly stable algorithms. In Conference   \n344 on Learning Theory, pages 610-626. PMLR, 2020.   \n345 Z. Charles and D.Papailopoulos. Stability and generalization of learming algorithms that converge to global   \n346 optima. In International conference on machine learning, pages 745-754. PMLR, 2018.   \n347P. J. Davis. Gamma function and related functions. Handbook of mathematical functions, 256, 1972.   \n348 V. De la Pena and E. Gine. Decoupling: from dependence to independence. Springer Science & Business Media,   \n349 2012.   \n350 Z. Deng, H. He, and W. Su. Toward better generalization bounds with locally elastic stability. In International   \n351 Conference on Machine Learning, pages 2590-2600. PMLR, 2021.   \n352 A. Eliseeff T Evgeniou,MPontil andL P Kalbing. Stability of randmized learing algoritms. Jl f   \n353 Machine Learning Research, 6(1), 2005.   \n354 J. Fan and Y Lei High-probability generalization bounds for pointwise uniformly stable algorithm. Applid   \n355 and Computational Harmonic Analysis, 70:101632, 2024.   \n356 V. Feldman and J. Vondrak. Generalization bounds for uniformly stable algorithms. Advances in Neural   \n357 Information Processing Systems, 31, 2018.   \n358VFelman andJVndra.Hig probability geeralizationbous frnifomly stable algorithms with nealy   \n359 optimal rate. In Conference on Learning Theory, pages 1270-1279. PMLR, 2019.   \n360 R. A. Fisher. On the mathematical foundations of theoretical statistics. Philosophical transactions of the Royal   \n361 Society of London. Series A, containing papers of a mathematical or physical character, 222(594-604):   \n362 309-368, 1922.   \n363 D. J. Foster, S. Greenberg, S. Kale, H. Luo, M. Mohri, and K. Sridharan. Hypothesis set stability and   \n364 generalization. Advances in Neural Information Processing Systems, 32, 2019.   \n365 S. Ghadimi and G. Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM   \n366 journal on optimization, 23(4):2341-2368, 2013.   \n367 S. Ghadimi, G. Lan, and H. Zhang. Mini-batch stochastic approximation methods for nonconvex stochastic   \n368 composite optimization. Mathematical Programming, 155(1):267-305, 2016.   \n369M.Hardt, B.Recht, and Y Singer Train faster, generalize better: Stability of stochastic gradient descent. In   \n370 International conference on machine learning, pages 1225-1234. PMLR, 2016.   \n371 H. Karimi, J. Nutini, and M. Schmidt Linear convergence of gradient and proximal-gradient mthods ude th   \n372 polyak-lojasiewicz condition. In ECML, pages 795-811. Springer, 2016.   \n373 A. J. Kleywegt, A. Shapir, and T. mem-de Mell. The sample averae approximation mthd for stochastic   \n374 discrete optimization. SIAM Journal on optimization, 12(2):479-502, 2002.   \n375 Y. Klochkov and N. Zhivotovskiy. Stability and deviation optimal risk bounds with convergence rate $o(1/n)$   \n376 Advances in Neural Information Processing Systems, 34:5065-5076, 2021.   \n377 1. Kuzborskij and C. Lampert. Data-deendent stabilityof stochastic gradient descent. In Proceedings of th   \n378 35th International Conference on Machine Learning (ICML), pages 2815-2824. PMLR, 2018.   \n379 R. Latata and K. Oleszkiewicz. On the best constant in the khinchin-kahane inequality. Studia Mathematica,   \n380 109(1):101-104, 1994.   \n381 Y. Lei. Stability and generalization of stochastic optimization with nonconvex and nonsmooth problems. In The   \n382 Thirty Sixth Annual Conference on Learning Theory, pages 191-227. PMLR, 2023.   \n383 Y. Lei and K. Tang. Learning rates for stochastic gradient descent with nonconvex objectives. IEEE Transactions   \n384 on Pattern Analysis and Machine Intelligence, 43(12):4505-4511, 2021.   \n385 Y. Lei and Y. Ying. Fine-grained analysis of stability and generalization for stochastic gradient descent. In   \n386 International Conference on Machine Learning, pages 5809-5819. PMLR, 2020.   \n387 J. Li, X. Luo, and M Qiao. On generalization eror bounds of noisy gradient methods for non-convex learning.   \n388 In International Conference on Learning Representations, 2020.   \n389 S. Li and Y. Liu Improved learning rates for stochastic optimization: Two theoretical viewpoints. arXiv preprint   \n390 arXiv:2107.08686, 2021.   \n391 M. Liu, X. Zhang, L. Zhang, R. Jin, and T. Yang. Fast rates of erm and stochastic approximation: Adaptive to   \n392 error bound conditions. Advances in Neural Information Processing Systems, 31, 2018.   \n393T. Liu, G. Lugosi, G. Neu, and D. Tao. Algorithmic stability and hypothesis complexity. In International   \n394 Conference on Machine Learning, pages 2159-2167. PMLR, 2017.   \n395 B. London, B. Huang, and L. Getoor. Stability and generalization in structured prediction. The Journal of   \n396 Machine Learning Research, 17(1):7808-7859, 2016.   \n397 X. Luo and D. Zhang. Khintchine inequality on normed spaces and the application to banach-mazur distance.   \n398 arXiv preprint arXiv:2005.03728, 2020.   \n399 s. Lv, J. Wang, J. Liu, and Y. Liu. Improved learning rates of a functional lasso-type svm with sparse multi-kernel   \n400 representation. Advances in Neural Information Processing Systems, 34:21467-21479, 2021.   \n401 S. Mei, Y. Bai, and A. Montanari. The landscape of empirical risk for nonconvex losses. The Annals o Statistics,   \n402 46(6A):2747-2774, 2018.   \n403A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic   \n404 programming. SIAM Journal on optimization, 19(4):1574-1609, 2009.   \n405 I. Pinelis. Optimum bounds for the distributions of martingales in banach spaces. The Annals of Probability,   \n406 pages 1679-1706, 1994.   \n407 A. Rakhlin, S. Mukherjee, and T. Poggio. Stability results in learning theory. Analysis and Applications, 3(04):   \n408 397-417, 2005.   \n409  A. Rakhlin, O. Shamir, and K. Sridharan. Making gradient descent optimal for strongly convex stochastic   \n410 optimization. In Proceedings of the 29th International Coference on International Conference on Machine   \n411 Learning, pages 1571-1578, 2012.   \n412 O. Rivasplata, E. Parrado-Hernandez, J. S. Shawe-Taylor, S. Sun, and C. Szepesvari. Pac-bayes bounds for stable   \n413 algorithms with instance-dependent priors. Advances in Neural Information Processing Systems, 31, 2018.   \n414  S. Shalev-Shwartz and S. Ben-David. Understanding machine learning: From theory to algorithms. Cambridge   \n415 university press, 2014.   \n416 S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan. Learnability, stability and uniform convergence.   \n417 The Journal of Machine Learning Research, 11:2635-2670, 2010.   \n418 S. Smale and D-X. Zhou. Learning theory estimates viaintegral operators and their approximations. Constructive   \n419 approximation, 26(2):153-172, 2007.   \n420 M. V. Solodov. Incremental gradient algorithms with stepsizes bounded away from zero. Computational   \n421 Optimization and Applications, 11:23-35, 1998.   \n422 N. Srebro, K. Sridharan, and A. Tewari. Optimistic rates for learning with a smooth loss. arXiv preprint   \n423 arXiv:1009.3896, 2010.   \n424  A. W. Van der Vart. Asymptotic statistics, volume 3. Cambridge university press, 2000.   \n425V. Vapnik and A. Chervonenkis. Theory of Pattern Recognition. 1974.   \n426V. N. Vapnik. An overview of statistical learning theory. IEEE transactions on neural networks, 10(5):988-999,   \n427 1999.   \n428  S. Vaswani, F. Bach, and M. Schmidt. Fast and faster convergence of sgd for over-parameterized models and an   \n429 accelerated perceptron.In The 22ndinternational conference onartifcial intelligenceand statistics, pages   \n430 1195-1204. PMLR, 2019.   \n431 R. Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47.   \n432 Cambridge university press, 2018.   \n433 Y. Xu and A. Zeevi. Towards optimal problem dependent generalization error bounds in statistical learning   \n434 theory. Mathematics of Operations Research, 2024.   \n435 L. Zhang and Z.-H. Zhou. Stochastic approximation of smooth and strongly convex functions: Beyond the $\\mathrm{o}(1/\\mathrm{t})$   \n436 convergence rate. In Conference on Learning Theory, pages 3160-3179. PMLR, 2019.   \n437 L. Zhang, T. Yang, and R. Jin. Empirical risk minimization for stochastic convex optimization: ${\\mathrm{O}}(1/{\\mathrm{n}})$ -and   \n438 $\\mathrm{o}(1/\\mathfrak{n}^{**}\\!2)$ -type of risk bounds. In Conference on Learning Theory, pages 1954-1979. PMLR, 2017.   \n439 Y. Zhou, Y. Liang, and H. Zhang. Generalization error bounds with probabilistic guarantee for sgd in nonconvex   \n440 optimization. arXiv preprint arXiv: 1802.06903, 2018. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "441 A Additional definitions and lemmata ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "442Lemma 6 (Equivalence of tails and moments for random vectors [Bassily et al., 2020]). Let $X$ bea   \n443randomvariablewith ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\|X\\|_{p}\\leq\\sqrt{p}a+p b\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "444  for some $a,b\\ge0$ and for any $p\\geq2$ Then for any $\\delta\\in(0,1)$ we have, with probability at least $1-\\delta$ ", "page_idx": 11}, {"type": "equation", "text": "$$\n|X|\\leq e\\left(a{\\sqrt{\\log\\left({\\frac{e}{\\delta}}\\right)}}+b\\log{\\frac{e}{\\delta}}\\right).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "445 Lemma 7 (Vector Bernstein's inequality [Pinelis, 1994, Smale and Zhou, 2007]). Let $\\{X_{i}\\}_{i=1}^{n}$ be   \n446a sequence of i.i.d. random variables taking values in a real separable Hilbert space. Assume   \n447  that $\\mathbf{\\dot{\\mathbb{E}}}[X_{i}]=\\mu,\\,\\mathbb{E}[\\|X_{i}-\\mu\\|^{2}]=\\sigma^{2}$ ,and $\\|\\bar{X}_{i}\\|\\leq M,\\,\\forall1\\leq i\\leq n,$ then for all $\\delta\\,\\in\\,(0,1)$ with   \n448probabilityatleast $1-\\delta$ we have ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\left\\|{\\frac{1}{n}}\\sum_{i=1}^{n}X_{i}-\\mu\\right\\|\\leq{\\sqrt{\\frac{2\\sigma^{2}\\log({\\frac{2}{\\delta}})}{n}}}+{\\frac{M\\log{\\frac{2}{\\delta}}}{n}}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "449Definition 7 (Weakly self-Bounded Function). Assume that $a,b>0.$ Afunction $f:{\\mathcal{Z}}^{n}\\mapsto[0,+\\infty)$   \n450 is said to be $(a,b)$ -weakly self-bounded if there exist functions $f_{i}:\\mathcal{Z}^{n-1}\\mapsto[0,+\\infty)$ that satisfies   \n451 for all $Z^{n}\\in{\\mathcal{Z}}^{n}$ \uff0c ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}(f_{i}(Z^{n})-f(Z^{n}))^{2}\\leq a f(Z^{n})+b.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "452  Lemma 8 ([Klochkov and Zhivotovskiy, 2021]). Suppose that $z_{1},\\ldots,z_{n}$ are independentrandom   \n453 variables and thefunction $f:{\\mathcal{Z}}^{n}\\mapsto[0,+\\infty)$ is $(a,b)$ -weakly self-bounded and the corresponding   \n454  function $f_{i}$ satisfy $f_{i}(Z^{n})\\geq f(Z^{n})\\,f o r\\,\\forall i\\in[n]$ and any $Z^{n}\\in{\\mathcal{Z}}^{n}$ . Then, for any $t>0$ ", "page_idx": 11}, {"type": "equation", "text": "$$\nP r(\\mathbb{E}f(z_{1},\\dots,z_{n})\\geq f(z_{1},\\dots,z_{n})+t)\\leq\\exp\\left(-{\\frac{t^{2}}{2a\\mathbb{E}f(z_{1},\\dots,z_{n})+2b}}\\right).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "455Definition 8. A Rademacher random variable is a Bernoulli variable that takes values $\\pm1$ With   \n456probability $\\frac{1}{2}$ each. ", "page_idx": 11}, {"type": "text", "text": "457B  Proofs of Section 3.1 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "458  The proof of Theorem 1 is motivated by Bousquet et al. [2020], which need the Marcinkiewicz  \n459 Zygmund's inequality for random variables taking values in a Hilbert space and the McDiarmid's   \n460 inequality for vector-valued functions.   \n461 Firstly, we derive the optimal constants in the Marcinkiewicz-Zygmund's inequality for random   \n462 variables taking values in a Hilbert space.   \n463 Lemma 9 (Marcinkiewicz-Zygmund's Inequality for Random Variables Taking Values in a Hilbert   \n464 Space). Let $\\mathbf{X}_{1},\\ldots,\\mathbf{X}_{n}$ be random variables taking values in a Hilbert space with $\\mathbb{E}[\\mathbf{X}_{i}]=0$ for   \n465all $i\\in[n]$ Thenfor $p\\geq2$ wehave ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "equation", "text": "$$\n\\left\\|\\left\\Vert\\sum_{i=1}^{n}\\mathbf{X}_{i}\\right\\Vert\\right\\Vert_{p}\\leq2\\cdot2^{\\frac{1}{2p}}\\sqrt{\\frac{n p}{e}}\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\left\\Vert\\left\\Vert\\mathbf{X}_{i}\\right\\Vert\\right\\Vert_{p}^{p}\\right)^{\\frac{1}{p}}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "466 Remark 14. Comparing with Marcinkiewicz-Zygmund's inequality given by Fan and Lei [2024],   \n467we provide best constants. Next, we give the proof of Lemma 9.   \n468 The Marcinkiewicz-Zygmund's inequality can be proved by using its connection to Khintchine  \n469 Kahane's inequality. Thus, we introduce the best constants in Khintchine-Kahane's inequality for   \n470 random variables taking values from a Hilbert space here.   \n471 Lemma 10 (Best constants in Khintchine-Kahane's inequality in Hilbert space [Latala and   \n472Oleszkiewicz, 1994, Luo and Zhang, 2020]). For all $p\\,\\in\\,[2,\\infty)$ and for all choices of Hilbert   \n473space $\\mathcal{H}$ ,finite sets of vectors $\\mathbf{X}_{i},\\ldots,\\mathbf{X}_{n}\\,\\in\\,{\\mathcal{X}}\\,\\in\\,{\\mathcal{H}}_{n}$ .and independent Rademacher variables   \n474 $r_{1},\\ldots,r_{n}$ ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left[\\mathbb{E}\\left\\|\\sum_{i=1}^{n}r_{i}\\mathbf{X}_{i}\\right\\|^{p}\\right]^{\\frac{1}{p}}\\leq C_{p}\\cdot\\left[\\sum_{i=1}^{n}\\left\\|\\mathbf{X}_{i}\\right\\|^{2}\\right]^{\\frac{1}{2}},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "475 where $\\begin{array}{r}{C_{p}=2^{\\frac{1}{2}}\\left\\{\\frac{\\Gamma\\left(\\frac{p+1}{2}\\right)}{\\sqrt{\\pi}}\\right\\}^{\\frac{1}{p}}}\\end{array}$ ", "page_idx": 12}, {"type": "text", "text": "476 Proof of Lemma 9. The symmetrization argument goes as follows: Let $(r_{1},\\ldots,r_{n})$ be i.i.d. with   \n477 $\\mathbb{P}(r_{i}=1)=\\mathbb{P}(r_{i}=-1)=1/2$ and besides such that $r_{1},\\ldots,r_{n}$ and $(\\mathbf{X}_{1},\\ldots,\\mathbf{X}_{n})$ are independent.   \n478 Then by independence and symmetry, according to Lemma 1.2.6 of De la Pena and Gine [2012],   \n479conditioning on $(\\mathbf{X}_{1},\\ldots,\\mathbf{X}_{n})$ yields ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\sum_{i=1}^{n}\\mathbf{X}_{i}\\right\\Vert^{p}\\right]=2^{p}\\mathbb{E}\\left[\\left\\Vert\\sum_{i=1}^{n}r_{i}\\mathbf{X}_{i}\\right\\Vert^{p}\\right]\\leq2^{p}\\mathbb{E}\\left[\\mathbb{E}\\left[\\left\\Vert\\sum_{i=1}^{n}r_{i}\\mathbf{X}_{i}\\right\\Vert^{p}\\bigg|\\mathbf{X}_{1},\\ldots,\\mathbf{X}_{n}\\right]\\right].\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "480 As for the conditional expectation in (7), notice that by independence ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\sum_{i=1}^{n}r_{i}\\mathbf{X}_{i}\\right\\Vert^{p}\\bigg|\\mathbf{X}_{1}=\\mathbf{x}_{1},\\ldots,\\mathbf{X}_{n}=\\mathbf{x}_{n}\\right]=\\mathbb{E}\\left[\\left\\Vert\\sum_{i=1}^{n}r_{i}\\mathbf{X}_{i}\\right\\Vert^{p}\\right]\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "481 According to Lemma 10, for $v_{n}$ -almost every $\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n}\\in\\mathbb{R}^{n}$ , where $v_{n}:=\\mathbb{P}\\circ(\\mathbf{X}_{1},\\ldots,\\mathbf{X}_{n})^{-1}$   \n482  denotes the distribution of $(\\mathbf{X}_{1},\\ldots,\\mathbf{X}_{n})$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left[\\mathbb{E}\\left\\|\\sum_{i=1}^{n}r_{i}\\mathbf{x}_{i}\\right\\|^{p}\\right]\\leq C\\cdot\\left[\\sum_{i=1}^{n}\\left\\|\\mathbf{x}_{i}\\right\\|^{2}\\right]^{\\frac{p}{2}},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "483 where C = 2 () and $C$ is optimal. This means that for any constant $C^{\\prime}$ such that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left[\\mathbb{E}\\left\\|\\sum_{i=1}^{n}r_{i}\\mathbf{x}_{i}\\right\\|^{p}\\right]\\leq C^{\\prime}\\cdot\\left[\\sum_{i=1}^{n}\\left\\|\\mathbf{x}_{i}\\right\\|^{2}\\right]^{\\frac{p}{2}},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "484  for all $n\\in\\mathbb{N}$ and for each collection of vectors $\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n}$ , it follows that $C^{\\prime}\\geq C$ ", "page_idx": 12}, {"type": "text", "text": "485  From (8) and (9), we can infer that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\lVert\\sum_{i=1}^{n}r_{i}\\mathbf{X}_{i}\\right\\rVert^{p}\\left|\\mathbf{X}_{1}=\\mathbf{x}_{1},\\ldots,\\mathbf{X}_{n}=\\mathbf{x}_{n}\\right]\\le C\\cdot\\left[\\sum_{i=1}^{n}\\left\\lVert\\mathbf{X}_{i}\\right\\rVert^{2}\\right]^{\\frac{p}{2}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "486 Taking expectations in the above inequalities and (7) yield that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\sum_{i=1}^{n}\\mathbf{X}_{i}\\right\\Vert^{p}\\right]\\leq C\\cdot\\mathbb{E}\\left[\\sum_{i=1}^{n}\\left\\Vert\\mathbf{X}_{i}\\right\\Vert^{2}\\right]^{\\frac{p}{2}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "487 To see optimality let the above statement hold for some constants $C^{\\prime}$ in place of $C$ .Then if we choose   \n488 $\\mathbf{X}_{i}:=\\mathbf{x}_{i}r_{i},1\\le i\\le n$ with arbitrary reals vectors $\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n}$ , it follows that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\sum_{i=1}^{n}r_{i}\\mathbf{x}_{i}\\right\\Vert^{p}\\right]\\leq C^{\\prime}\\cdot\\mathbb{E}\\left[\\sum_{i=1}^{n}\\left\\Vert\\mathbf{x}_{i}\\right\\Vert^{2}\\right]^{\\frac{p}{2}},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "489  whence we can conclude from (10) that $C^{\\prime}\\geq C$ . Thus we obtain that $C^{\\prime}=C$ ", "page_idx": 12}, {"type": "text", "text": "490 Notice that by Holder's inequality ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left[\\sum_{i=1}^{n}\\left\\|\\mathbf{X}_{i}\\right\\|^{2}\\right]^{\\frac{p}{2}}\\leq n^{p/2-1}\\sum_{i=1}^{n}\\|\\mathbf{X}_{i}\\|^{p}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "491Plugging (12) into (11), we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\sum_{i=1}^{n}\\mathbf{X}_{i}\\right\\Vert^{p}\\right]\\leq C\\cdot2^{p}n^{p/2-1}\\cdot\\mathbb{E}\\left[\\sum_{i=1}^{n}\\left\\Vert\\mathbf{X}_{i}\\right\\Vert^{p}\\right],\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "492 where C = 2 F() is a constant. ", "page_idx": 13}, {"type": "text", "text": "493  Next, we use the following form of Stirling's formula for the Gamma-function, which follows from   \n494  (6.1.5), (6.1.15) and (6.1.38) in Davis [1972] to bound the constant $C$ .Forevery $x>0$ ,there exists a   \n495 $\\mu(x)\\in(0,1/(12x))$ suchthat ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\Gamma(x)=\\sqrt{2\\pi}x^{x-1/2}e^{-x}e^{\\mu(x)}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "496 Thus ", "page_idx": 13}, {"type": "equation", "text": "$$\nC=2^{\\frac{p}{2}}{\\frac{\\Gamma\\left({\\frac{p+1}{2}}\\right)}{\\sqrt{\\pi}}}=g(p){\\sqrt{2}}e^{-p/2}p^{p/2},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "47 winhg(g)= (1+)/2e where $0<v(p)<1/(6(p+1))$ By Taylor's formula we have   \n498that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\log(1+x)=\\sum_{m=1}^{\\infty}{\\frac{1}{m}}(-1)^{m-1}x^{m},\\quad\\forall x\\in(-1,1],\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "499  and that for every $k\\in\\ensuremath{\\mathbb{N}}_{0}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{m=1}^{2k}\\frac{1}{m}(-1)^{m-1}x^{m}\\leq\\log(1+x)\\leq\\sum_{m=1}^{2k+1}\\frac{1}{m}(-1)^{m-1}x^{m},\\forall x\\geq0.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "500Therefor we obtain with $k=1$ that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\log g(p)={\\frac{p}{2}}\\log(1+{\\frac{1}{p}})+v(p)-{\\frac{1}{2}}\\leq-{\\frac{1}{4p}}+{\\frac{1}{6p^{2}}}+{\\frac{1}{6(p+1)}}\\leq-{\\frac{1}{18p}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "501 where the last equality follows from elementary calculus. Similarly, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\log g(p)=\\frac{p}{2}\\log(1+\\frac{1}{p})+v(p)-\\frac{1}{2}\\geq-\\frac{1}{4p}+v(p)\\geq-\\frac{1}{4p},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "502 Thus, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\ne^{-\\frac{1}{4p}\\sqrt{2}e^{-p/2}p^{p/2}<C<e^{-\\frac{1}{18p}\\sqrt{2}}e^{-p/2}p^{p/2},}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "503  which implies that $C$ is strictly smaller than $\\sqrt{2}e^{-p/2}p^{p/2}$ for all $p\\geq2$ ", "page_idx": 13}, {"type": "text", "text": "504  Since $\\begin{array}{r}{C=\\frac{1}{g(p)}\\sqrt{2}e^{-p/2}p^{p/2}}\\end{array}$ and $g(p)\\geq e^{-\\frac{1}{4p}}$ ,we can obtain that the relative errorbetween $C$ and   \n505 $\\sqrt{2}e^{-p/2}p^{p/2}$ is equal to ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{1}{g(p)}-1\\leq e^{-\\frac{1}{4p}}-1\\leq\\frac{1}{4p}e^{\\frac{1}{4p}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "506  using Mean Value Theorem. This implies that the corresponding relative errors between $C$ and   \n507 $\\bar{\\sqrt{2}}e^{-p/2}p^{p/2}$ converge to zero as $p$ tendsto infinity. ", "page_idx": 13}, {"type": "text", "text": "508  The proof is complete. ", "page_idx": 13}, {"type": "text", "text": "510Then we introduce the McDiarmid's inequality for vector-valued functions. We firstly consider   \n511 real-valued functions, which follows from the standard tail-bound of McDiarmid's inequality and   \n512Proposition 2.5.2 in Vershynin [2018].   \n513  Lemma 11 (McDiarmid's Inequality for real-valued functions). Let $Z_{i},\\ldots,Z_{n}$ beindepen  \n514 dent random variables, and $f\\ :\\ {\\mathcal{Z}}^{n}\\ \\mapsto\\ \\mathbb{R}$ such that the following inequality holds for any   \n515 $z_{i},\\dotsc,z_{i-1},z_{i+1},\\dotsc,z_{n}$ ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{z_{i},z_{i}^{\\prime}}\\left|f(z_{1},\\dots,z_{i-1},z_{i},z_{i+1},\\dots,z_{n})-f(z_{1},\\dots,z_{i-1},z_{i}^{\\prime},z_{i+1},\\dots,z_{n})\\right|\\le\\beta,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "516 Then for any $p>1$ we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|f(Z_{1},\\dots,Z_{n})-\\mathbb{E}f(Z_{1},\\dots,Z_{n})\\|_{p}\\leq{\\sqrt{2p n}}\\beta.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "517  To derive the McDiarmid's inequality for vector-valued functions, we need the expected distance   \n518between $\\mathbf{f}(Z_{1},\\ldots,Z_{n})$ and its expectation.   \n519 Lemma 12 ([Rivasplata et al., 2018]). Let $Z_{i},\\ldots,Z_{n}$ be independent random variables, and   \n520 $\\mathbf{f}\\,:\\,\\mathcal{Z}^{n}\\,\\mapsto\\,\\mathcal{H}$ is a function into a Hilbert space $\\mathcal{H}$ such that the following inequality holds for   \n521 any $z_{i},\\dotsc,z_{i-1},z_{i+1},\\dotsc,z_{n}$ ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{z_{i},z_{i}^{\\prime}}\\left\\|\\mathbf{f}\\left(z_{1},\\ldots,z_{i-1},z_{i},z_{i+1},\\ldots,z_{n}\\right)-\\mathbf{f}\\left(z_{1},\\ldots,z_{i-1},z_{i}^{\\prime},z_{i+1},\\ldots,z_{n}\\right)\\right\\|\\le\\beta,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "522 Thenwehave ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert\\mathbf{f}\\left(Z_{1},\\ldots,Z_{n}\\right)-\\mathbb{E}\\mathbf{f}\\left(Z_{1},\\ldots,Z_{n}\\right)\\Vert\\right]\\leq\\sqrt{n}\\beta.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "523  Now, we can easily derive the $p$ -norm McDiarmid's inequality for vector-valued functions which   \n524  refines from Fan and Lei [2024] with better constants.   \n525 Lemma 13 (McDiarmid's inequality for vector-valued functions). Let $Z_{i},\\ldots,Z_{n}$ be independent   \n526 random variables, and f $:\\mathcal{Z}^{n}\\mapsto\\mathcal{H}$ is a function into a Hilbert space $\\mathcal{H}$ such that the following   \n527 inequality holds for any $z_{i},\\dotsc,z_{i-1},z_{i+1},\\dotsc,z_{n}$ ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{z_{i},z_{i}^{\\prime}}\\left\\|\\mathbf{f}\\left(z_{1},\\ldots,z_{i-1},z_{i},z_{i+1},\\ldots,z_{n}\\right)-\\mathbf{f}\\left(z_{1},\\ldots,z_{i-1},z_{i}^{\\prime},z_{i+1},\\ldots,z_{n}\\right)\\right\\|\\le\\beta,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "528 Then for any $p>1$ we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\|\\mathbf{f}(Z_{1},\\ldots,Z_{n})-\\mathbb{E}\\mathbf{f}(Z_{1},\\ldots,Z_{n})\\|\\|_{p}\\leq({\\sqrt{2p}}+1){\\sqrt{n}}\\beta.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proofof Lemma $^{l3}$ . Define a real-valued function $h:\\mathcal{Z}^{n}\\mapsto\\mathbb{R}$ as ", "page_idx": 14}, {"type": "equation", "text": "$$\nh(z_{1},\\ldots,z_{n})=\\|\\mathbf{f}(z_{1},\\ldots,z_{n})-\\mathbb{E}[\\mathbf{f}(Z_{1},\\ldots,Z_{n})]\\|.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "529We notice that this function satisfies the increment condition. For any $i$ and $z_{1},\\dotsc,z_{i-1},z_{i+1},\\dotsc,z_{n}$   \n530wehave ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{z_{i},z_{i}^{\\prime}}{\\operatorname*{sup}}\\,|h(z_{1},\\ldots,z_{i-1},z_{i},z_{i+1},\\ldots,z_{n})-h(z_{1},\\ldots,z_{i-1},z_{i}^{\\prime},z_{i+1},\\ldots,z_{n})|}\\\\ &{=\\underset{z_{i},z_{i}^{\\prime}}{\\operatorname*{sup}}\\,|\\|\\mathbf{f}(z_{1},\\ldots,z_{n})-\\mathbb{E}[\\mathbf{f}(Z_{1},\\ldots,Z_{n})]\\|-\\|\\mathbf{f}(z_{1},\\ldots,z_{i-1},z_{i}^{\\prime},z_{i+1},\\ldots,z_{n})-\\mathbb{E}[\\mathbf{f}(Z_{1},\\ldots,Z_{n})]\\||}\\\\ &{\\le\\underset{z_{i},z_{i}^{\\prime}}{\\operatorname*{sup}}\\,|\\|\\mathbf{f}(z_{1},\\ldots,z_{n})-\\mathbf{f}(z_{1},\\ldots,z_{i-1},z_{i}^{\\prime},z_{i+1},\\ldots,z_{n})\\|\\le\\beta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "531 Therefore, we can apply Lemma 11 to the real-valued function $h$ and derive the following inequality ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|h(Z_{1},\\dots,Z_{n})-\\mathbb{E}[h(Z_{1},\\dots,Z_{n})]\\|_{p}\\leq{\\sqrt{2p n}}\\beta.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "532  According to Lemma 12, we know the following inequality $\\mathbb{E}[h(Z_{1},\\ldots,Z_{n})]\\leq\\sqrt{n}\\beta$ .Combing the   \n533 above two inequalities together and we can derive the following inequality ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~\\|\\|\\mathbf{f}(Z_{1},\\ldots,Z_{n})-\\mathbb{E}\\mathbf{f}(Z_{1},\\ldots,Z_{n})\\|\\|_{p}}\\\\ &{\\le\\!\\|h(Z_{1},\\ldots,Z_{n})-\\mathbb{E}[h(Z_{1},\\ldots,Z_{n})]\\|_{p}+\\|\\mathbb{E}[h(Z_{1},\\ldots,Z_{n})]\\|_{p}}\\\\ &{\\le\\!(\\sqrt{2p}+1)\\sqrt{n}\\beta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "534 The proof is complete. ", "page_idx": 14}, {"type": "text", "text": "535 ", "page_idx": 14}, {"type": "text", "text": "536 Proof of Theorem 1. For $\\mathbf{g}(Z_{1},\\ldots,Z_{n})$ and $A\\subset[n]$ ,wewrite $\\|\\|\\mathbf{g}\\|\\|_{p}(Z_{A})\\,=\\,(\\mathbb{E}\\,[\\|f\\|^{p}\\,Z_{A}])^{\\frac{1}{p}}$   \n537  Without loss of generality, we suppose that $n=2^{k}$ . Otherwise, we can add extra functions equal to   \n538 zero, increasing the number of therms by at most two times.   \n539  Consider a sequence of partitions $\\mathcal{P}_{0},\\ldots,\\mathcal{P}_{k}$ with $\\mathcal{P}_{0}=\\{\\{i\\}:i\\in[n]\\},\\mathcal{P}_{k}$ with $\\mathcal{P}_{n}=\\{[n]\\}$ , and   \n540 to get $\\mathcal{P}_{l}$ from $\\mathcal{P}_{l+1}$ we split each subset in $\\mathcal{P}_{l+1}$ into two equal parts. We have ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{P}_{0}=\\{\\{1\\},\\ldots,\\{2^{k}\\}\\},\\quad\\mathcal{P}_{1}=\\{\\{1,2\\},\\{3,4\\},\\ldots,\\{2^{k}-1,2^{k}\\}\\},\\quad\\mathcal{P}_{k}=\\{\\{1,\\ldots,2^{k}\\}\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "541   We have $|\\mathcal{P}_{l}|=2^{k-l}$ and $|P|=2^{l}$ for each $P\\in\\mathcal{P}_{l}$ . For each $i\\in[n]$ and $l=0,\\ldots,k$ , denote by   \n542 $P^{l}(i)\\in\\mathcal{P}_{l}$ the only set from $\\mathcal{P}_{l}$ that contains $i$ . In particular, $P^{0}(i)=\\{i\\}$ and $\\begin{array}{r}{P^{K}(i)=[n]}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "543 For each $i\\in[n]$ and every $l=0,\\ldots,k$ consider the random variables ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\bf g}_{i}^{l}={\\bf g}_{i}^{l}(Z_{i},Z_{[n]\\backslash P^{l}(i)})=\\mathbb{E}[{\\bf g}_{i}|Z_{i},Z_{[n]\\backslash P^{l}(i)}],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "544  i.e. conditioned on $z_{i}$ and all the variables that are not in the same set as $Z_{i}$ in the partition $\\mathcal{P}_{l}$ . In   \n545  particular, $\\mathbf{g}_{i}^{0}=\\mathbf{g}_{i}$ and $\\mathbf{g}_{i}^{k}=\\mathbb{E}[\\mathbf{g}_{i}\\vert Z_{i}]$ . We can write a telescopic sum for each $i\\in[n]$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{g}_{i}-\\mathbb{E}[\\mathbf{g}_{i}|Z_{i}]=\\sum_{l=1}^{k-1}\\mathbf{g}_{i}^{l}-\\mathbf{g}_{i}^{l+1}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "546  Then, by the triangle inequality ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|\\left\\|\\sum_{i=1}^{n}\\mathbf{g}_{i}\\right\\|\\right\\|_{p}\\leq\\left\\|\\left\\|\\sum_{i=1}^{n}\\mathbb{E}[\\mathbf{g}_{i}|Z_{i}]\\right\\|\\right\\|_{p}+\\sum_{l=0}^{k-1}\\left\\|\\left\\|\\sum_{i=1}^{n}\\mathbf{g}_{i}^{l}-\\mathbf{g}_{i}^{l+1}\\right\\|\\right\\|_{p}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "547 To bound the first term, since $\\|\\mathbb{E}[\\mathbf{g}_{i}|Z_{i}]\\|\\,\\le\\,M$ , we can check that the vector-valued function   \n548 $\\begin{array}{r}{\\mathbf{f}\\left(Z_{1},\\ldots,Z_{n}\\right)=\\sum_{i=1}^{n}\\mathbb{E}[\\mathbf{g}_{i}|Z_{i}]}\\end{array}$ satisfies (13) with $\\beta=2M$ , and $\\mathbb{E}[\\mathbb{E}[\\mathbf{g}_{i}|Z_{i}]]=0$ applying Lemma   \n549 13 with $\\beta=2M$ ,we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|\\left\\vert\\sum_{i=1}^{n}\\mathbb{E}[\\mathbf{g}_{i}\\vert Z_{i}]\\right\\Vert\\right\\Vert_{p}\\leq2(\\sqrt{2p}+1)\\sqrt{n}M.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "550 Then we start to bound the second term of the right hand side of (14). Observe that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{g}_{i}^{l+1}(Z_{i},Z_{[n]\\backslash P^{l+1}(i)})=\\mathbb{E}\\left[\\mathbf{g}_{i}^{l}(Z_{i},Z_{[n]\\backslash P^{l}(i)})\\big|Z_{i},Z_{[n]\\backslash P^{l+1}(i)}\\right],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "551 where the expectation is taken with respect to the variables $Z_{j},j\\in P^{l+1}(i)\\backslash P^{l}(i)$ Changing any   \n552 $Z_{j}$ would change $\\mathbf{g}_{i}^{l}$ by $\\beta$ . Therefore, we apply Lemma 13 with $\\mathbf{f}=\\mathbf{g}_{i}^{l}$ where there are $2^{l}$ random   \n53  variables and obtain a uniform bound ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\left\\|\\mathbf{g}_{i}^{l}-\\mathbf{g}_{i}^{l+1}\\right\\|\\right\\|_{p}(Z_{i},Z_{[n]\\setminus P^{l+1}(i)})\\leq(\\sqrt{2p}+1)\\sqrt{2^{l}}\\beta,\\quad\\forall p\\geq2,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "554 Taking integration over $(Z_{i},Z_{[n]\\backslash P^{l+1}(i)})$ , we have $\\left\\|\\left\\|\\mathbf{g}_{i}^{l}-\\mathbf{g}_{i}^{l+1}\\right\\|\\right\\|_{p}\\leq(\\sqrt{2p}+1)\\sqrt{2^{l}}\\beta$ as well. ", "page_idx": 15}, {"type": "text", "text": "5  Next, we turn to the sum $\\textstyle\\sum_{i\\in P^{l}}\\mathbf{g}_{i}^{l}-\\mathbf{g}_{i}^{l+1}$ for any $P^{l}\\in\\mathcal{P}_{l}$ . Since $\\mathbf{g}_{i}^{l}-\\mathbf{g}_{i}^{l+1}$ for $i\\in P^{l}$ depends   \n556 Only on $Z_{i},Z_{[n]\\backslash P^{l}}$ the terms are independent and zero mean conditioned on $Z_{[n]\\setminus P^{l}}$ . Applying   \n557  Lemma 9, we have for any $p\\geq2$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|\\left\\|\\sum_{i\\in P^{l}}\\mathbf{g}_{i}^{l}-\\mathbf{g}_{i}^{l+1}\\right\\|\\right\\|_{p}^{p}(Z_{[n]\\setminus P^{l}})\\leq\\left(2\\cdot2^{\\frac{1}{2p}}\\sqrt{\\frac{2^{l}p}{e}}\\right)^{p}\\frac{1}{2^{l}}\\sum_{i\\in P^{l}}\\left\\|\\left\\|\\mathbf{g}_{i}^{l}-\\mathbf{g}_{i}^{l+1}\\right\\|\\right\\|_{p}^{p}(Z_{[n]\\setminus P^{l}}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "558  Integrating with respect to $(Z_{[n]\\setminus P^{l}})$ and using $\\|\\left\\|\\mathbf{g}_{i}^{l}-\\mathbf{g}_{i}^{l+1}\\right\\|\\|_{p}\\leq(\\sqrt{2p}+1)\\sqrt{2^{l}}\\beta$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\displaystyle\\left\\|\\sum_{i\\in P^{l}}g_{i}^{l}-g_{i}^{l+1}\\right\\|\\right\\|_{p}\\leq\\left(2\\cdot2^{\\frac{1}{2p}}\\sqrt{\\frac{2^{l}p}{e}}\\right)\\frac{1}{2^{l}}\\times2^{l}(\\sqrt{2p}+1)\\sqrt{2^{l}}\\beta}\\\\ &{\\qquad\\qquad\\qquad\\qquad=2^{1+\\frac{1}{2p}}\\left(\\sqrt{\\frac{p}{e}}\\right)(\\sqrt{2p}+1)2^{l}\\beta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "559   Then using triangle inequality over all sets $P^{l}\\in\\mathcal{P}_{l}$ ,wehave ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\displaystyle\\left\\|\\sum_{i\\in[n]}\\mathbf{g}_{i}^{l}-\\mathbf{g}_{i}^{l+1}\\right\\|\\right\\|_{p}\\leq\\displaystyle\\sum_{P^{l}\\in\\mathcal{P}_{l}}\\left\\|\\left\\|\\sum_{i\\in P^{l}}\\mathbf{g}_{i}^{l}-\\mathbf{g}_{i}^{l+1}\\right\\|\\right\\|_{p}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2^{k-l}\\times2^{1+\\frac{1}{2p}}\\left(\\sqrt{\\frac{p}{e}}\\right)(\\sqrt{2p}+1)2^{l}\\beta}\\\\ &{\\qquad\\qquad\\qquad\\leq2^{1+\\frac{1}{2p}}\\left(\\sqrt{\\frac{p}{e}}\\right)(\\sqrt{2p}+1)2^{k}\\beta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "560 Recall that $2^{k}\\leq n$ due to the possible extension of the sample. Then we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{k-1}\\left\\|\\left\\|\\sum_{i=1}^{n}\\mathbf{g}_{i}^{l}-\\mathbf{g}_{i}^{i+1}\\right\\|\\right\\|_{p}\\leq2^{2+\\frac{1}{2p}}\\left(\\sqrt{\\frac{p}{e}}\\right)(\\sqrt{2p}+1)n\\beta\\left\\lceil\\log_{2}n\\right\\rceil.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "561 We can plug the above bound together with (15) into (14), to derive the following inequality ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\|\\left\\Vert\\sum_{i=1}^{n}\\mathbf{g}_{i}\\right\\Vert\\right\\Vert_{p}\\leq2(\\sqrt{2p}+1)\\sqrt{n}M+2^{2+\\frac{1}{2p}}\\left(\\sqrt{\\frac{p}{e}}\\right)(\\sqrt{2p}+1)n\\beta\\left\\lceil\\log_{2}n\\right\\rceil.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "562  The proof is completed. ", "page_idx": 16}, {"type": "text", "text": "563 ", "page_idx": 16}, {"type": "text", "text": "564 C Proofs of Section 3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "565 Proof of Theorem 2. Let $S=\\{z_{1},\\ldots,z_{n}\\}$ be a set of independent random variables each taking   \n566  values in $\\mathcal{Z}$ and $S^{\\prime}\\,=\\,\\{z_{1}^{\\prime},\\cdot\\cdot\\cdot,z_{n}^{\\prime}\\}$ be its independent copy. For any $i~\\in~[n]$ . define ${\\cal S}^{(i)}\\;=\\;$   \n567 $\\{z_{i},\\ldots,z_{i-1},z_{i}^{\\prime},z_{i+1},\\ldots,\\bar{z}_{n}\\}$ be a dataset replacing the $i$ -th sample in $S$ with another i.i.d. sample   \n568 $z_{i}^{\\prime}$ . Then we can firstly write the following decomposition ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle n\\nabla F(A(S))-n\\nabla F_{S}(A(S))}\\\\ &{\\displaystyle=\\sum_{i=1}^{n}\\mathbb{E}_{Z}\\left[\\nabla f(A(S);Z)\\right]-\\mathbb{E}_{z_{i}^{\\prime}}\\left[\\nabla f(A(S^{(i)}),Z)\\right]\\right]}\\\\ &{\\displaystyle~~+\\sum_{i=1}^{n}\\mathbb{E}_{z_{i}^{\\prime}}\\left[\\mathbb{E}_{Z}\\left[\\nabla f(A(S^{(i)}),Z)\\right]-\\nabla f(A(S^{(i)}),z_{i})\\right]}\\\\ &{\\displaystyle~~~+\\sum_{i=1}^{n}\\mathbb{E}_{z_{i}^{\\prime}}\\left[\\nabla f(A(S^{(i)}),z_{i})\\right]-\\sum_{i=1}^{n}\\nabla f(A(S),z_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "569  We denote that $\\mathbf{g}_{i}(S)=\\mathbb{E}_{z_{i}^{\\prime}}\\left[\\mathbb{E}_{Z}\\left[\\nabla f(A(S^{(i)}),Z)\\right]-\\nabla f(A(S^{(i)}),z_{i})\\right]$ , thus we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|{n\\nabla F(A(S))-n\\nabla F_{S}(A(S))}\\right\\|_{2}}\\\\ &{=\\left\\|\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{Z}\\left[\\nabla f(A(S);Z)\\right]-\\mathbb{E}_{z_{i}^{\\prime}}\\left[\\nabla f(A(S^{(i)}),Z)\\right]\\right\\|}\\\\ &{\\quad+\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{z_{i}^{\\prime}}\\left[\\mathbb{E}_{Z}\\left[\\nabla f(A(S^{(i)}),Z)\\right]-\\nabla f(A(S^{(i)}),z_{i})\\right]}\\\\ &{\\quad\\quad+\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{z_{i}^{\\prime}}\\left[\\nabla f(A(S^{(i)}),z_{i})\\right]-\\displaystyle\\sum_{i=1}^{n}\\nabla f(A(S),z_{i})\\right\\|_{2}}\\\\ &{\\leq2n\\beta+\\left\\|\\displaystyle\\sum_{i=1}^{n}\\mathbf{g}_{i}(S)\\right\\|_{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "570 where the inequality holds from the definition of uniform stability in gradients. ", "page_idx": 17}, {"type": "text", "text": "571 According to our assumptions, we get $\\|\\mathbf{g}_{i}(S)\\|_{2}\\leq2M$ and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{z_{i}}[\\mathbf{g}_{i}(S)]=\\mathbb{E}_{z_{i}}\\mathbb{E}_{z_{i}^{\\prime}}\\left[\\mathbb{E}_{Z}\\left[\\nabla f(A(S^{(i)});Z)\\right]-\\nabla f(A(S^{(i)});z_{i})\\right]}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{z_{i}^{\\prime}}\\left[\\mathbb{E}_{Z}\\left[\\nabla f(A(S^{(i)});Z)\\right]-\\mathbb{E}_{z_{i}}\\left[\\nabla f(A(S^{(i)});z_{i})\\right]\\right]=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "572 where this equality holds from the fact that $z_{i}$ and $Z$ follow from the same distribution. For any   \n573 $i\\in[n]$ ,any $j^{\\ne}i$ and any $z_{j}^{\\prime\\prime}$ ,wehave ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbf{g}_{i}(z_{1},\\dots,z_{j-1},z_{j},z_{j+1},\\dots,z_{n})-\\mathbf{g}_{i}(z_{1},\\dots,z_{j-1},z_{j}^{\\prime\\prime},z_{j+1},\\dots,z_{n})\\right\\|_{2}}\\\\ &{\\leq\\left\\|\\mathbb{E}_{z_{i}^{\\prime}}\\left[\\mathbb{E}_{Z}\\left[\\nabla f(A(S^{(i)});Z)\\right]-\\nabla f(A(S^{(i)});z_{i})\\right]-\\mathbb{E}_{z_{i}^{\\prime}}\\left[\\mathbb{E}_{Z}\\left[\\nabla f(A(S_{j}^{(i)});Z)\\right]-\\nabla f(A(S_{j}^{(i)});z_{i})\\right]\\right\\|_{2}}\\\\ &{\\leq\\left\\|\\mathbb{E}_{z_{i}^{\\prime}}\\left[\\mathbb{E}_{Z}\\left[\\nabla f(A(S^{(i)});Z)-\\nabla f(A(S_{j}^{(i)});Z)\\right]\\right]\\right\\|_{2}+\\left\\|\\mathbb{E}_{z_{i}^{\\prime}}\\left[\\mathbb{E}_{Z}\\left[\\nabla f(A(S^{(i)});Z)\\right]-\\nabla f(A(S_{j}^{(i)});z_{i})\\right]\\right\\|_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "574where $S^{(i)}\\,=\\,\\left\\{z_{i},\\dots,z_{i-1},z_{i}^{\\prime},z_{i+1},\\dots,z_{n}\\right\\}$ Thus, we have verifed that three conditions in   \n575  Theorem 1 are satisfied for $\\mathbf{g}_{i}(S)$ . We have the following result for any $p>2$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\|\\left\\Vert\\sum_{i=1}^{n}\\mathbf{g}_{i}(S)\\right\\Vert\\right\\|_{p}\\leq4(\\sqrt{2p}+1)\\sqrt{n}M+8\\times2^{\\frac{1}{4}}\\left(\\sqrt{\\frac{p}{e}}\\right)(\\sqrt{2p}+1)n\\beta\\left\\lceil\\log_{2}n\\right\\rceil.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "576 We can combine the above inequality and (16) to derive the following inequality ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad n\\left\\|\\left\\|\\nabla F(A(S))-n\\nabla F_{S}(A(S))\\right\\|\\right\\|_{p}}\\\\ &{\\leq\\!2n\\beta+4(\\sqrt{2p}+1)\\sqrt{n}M+8\\times2^{\\frac{1}{4}}\\left(\\sqrt{\\frac{p}{e}}\\right)(\\sqrt{2p}+1)n\\beta\\left\\lceil\\log_{2}n\\right\\rceil.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "577 According to Lemma 6 for any $\\delta\\in(0,1)$ , with probability at least $1-\\delta$ ,we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad n\\|\\nabla F(A(S))-\\nabla F_{S}(A(S))\\|_{2}}\\\\ &{\\le2n\\beta+4\\sqrt{n}M+8\\times2^{\\frac{3}{4}}\\sqrt{e}n\\beta\\left[\\log_{2}n\\right]\\log{(e/\\delta)}+\\left(4e\\sqrt{2n}M+8\\times2^{\\frac{1}{4}}\\sqrt{e}n\\beta\\left[\\log_{2}n\\right]\\right)\\sqrt{\\log{e/\\delta}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "578 This implies that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\nabla F(A(S))-\\nabla F_{S}(A(S))\\|_{2}}\\\\ &{\\le2\\beta+\\displaystyle\\frac{4M\\left(1+e\\sqrt{2\\log\\left(e/\\delta\\right)}\\right)}{\\sqrt{n}}+8\\times2^{\\frac{1}{4}}(\\sqrt{2}+1)\\sqrt{e}\\beta\\left\\lceil\\log_{2}n\\right\\rceil\\log\\left(e/\\delta\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "579  The proof is completed. ", "page_idx": 17}, {"type": "text", "text": "580 Proof of Theorem 3. We can firstly write the following decomposition ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle n\\nabla F(A(S))-n\\nabla F_{S}(A(S))}\\\\ &{\\displaystyle=\\sum_{i=1}^{n}\\mathbb{E}_{Z}\\left[\\nabla f(A(S);Z)\\right]-\\mathbb{E}_{z_{i}^{\\prime}}\\left[\\nabla f(A(S^{(i)}),Z)\\right]\\right]}\\\\ &{\\displaystyle~~+\\sum_{i=1}^{n}\\mathbb{E}_{z_{i}^{\\prime}}\\left[\\mathbb{E}_{Z}\\left[\\nabla f(A(S^{(i)}),Z)\\right]-\\nabla f(A(S^{(i)}),z_{i})\\right]}\\\\ &{\\displaystyle~~~+\\sum_{i=1}^{n}\\mathbb{E}_{z_{i}^{\\prime}}\\left[\\nabla f(A(S^{(i)}),z_{i})\\right]-\\sum_{i=1}^{n}\\nabla f(A(S),z_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "581  We denote that $\\mathbf{h}_{i}(S)=\\mathbb{E}_{z_{i}^{\\prime}}\\left[\\mathbb{E}_{Z}\\left[\\nabla f(A(S^{(i)}),Z)\\right]-\\nabla f(A(S^{(i)}),z_{i})\\right]\\!.$ we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{\\displaystyle n\\nabla F(A(S))-n\\nabla F_{S}(A(S))-\\displaystyle\\sum_{i=1}^{n}\\mathbf{h}_{i}(S)}\\\\ {\\displaystyle=\\sum_{i=1}^{n}\\mathbb{E}_{Z}\\left[\\nabla f(A(S);Z)\\right]-\\mathbb{E}_{z_{i}^{\\prime}}\\left[\\nabla f(A(S^{(i)}),Z)\\right]\\right]}\\\\ {\\displaystyle\\qquad+\\sum_{i=1}^{n}\\mathbb{E}_{z_{i}^{\\prime}}\\left[\\nabla f(A(S^{(i)}),z_{i})\\right]-\\displaystyle\\sum_{i=1}^{n}\\nabla f(A(S),z_{i}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "582 which implies that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left\\|n\\nabla F(A(S))-n\\nabla F_{S}(A(S))-\\displaystyle\\sum_{i=1}^{n}\\mathbf{h}_{i}(S)\\right\\|_{2}}\\\\ &{\\displaystyle=\\left\\|\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{Z}\\left[\\nabla f(A(S);Z)\\right]-\\mathbb{E}_{z_{i}^{\\prime}}\\left[\\nabla f(A(S^{(i)}),Z)\\right]\\right\\|}\\\\ &{\\displaystyle\\qquad+\\sum_{i=1}^{n}\\mathbb{E}_{z_{i}^{\\prime}}\\left[\\nabla f(A(S^{(i)}),z_{i})\\right]-\\displaystyle\\sum_{i=1}^{n}\\nabla f(A(S),z_{i})\\right\\|_{2}}\\\\ &{\\displaystyle\\leq2n\\beta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "583 where the inequality holds from the definition of uniform stability in gradients. ", "page_idx": 18}, {"type": "text", "text": "584  Then, for any $i\\,=\\,1,\\,.\\,.\\,,n$ we define ${\\bf q}_{i}(S)={\\bf h}_{i}(S)-\\mathbb{E}_{S\\{z_{i}\\}}[{\\bf h}_{i}(S)]$ . It is easy to verify that   \n585 $\\mathbb{E}_{S\\backslash\\{z_{i}\\}}[{\\bf q}_{i}(S)]={\\bf0}$ and $\\mathbb{E}_{z_{i}}\\big[\\mathbf{h}_{i}(S)\\big]=\\mathbb{E}_{z_{i}}\\big[\\mathbf{q}_{i}(S)\\big]-\\mathbb{E}_{z_{i}}\\mathbb{E}_{S\\backslash\\{z_{i}\\}}\\big[\\mathbf{q}_{i}(S)\\big]=\\mathbf{0}-\\mathbf{0}=\\mathbf{0},$ Also, for any   \n586 $j\\in[n]$ with $j\\neq i$ and $z_{j}^{\\prime\\prime}\\in\\mathcal{Z}$ , we have the following inequality ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\mathbf q_{i}(S)-\\mathbf q_{i}(z_{1},\\ldots,z_{j-1},z_{j}^{\\prime\\prime},z_{j+1},\\ldots,z_{n})\\|_{2}}\\\\ &{\\le\\!\\|\\mathbf h_{i}(S)-\\mathbf h_{i}(z_{1},\\ldots,z_{j-1},z_{j}^{\\prime\\prime},z_{j+1},\\ldots,z_{n})\\|_{2}}\\\\ &{\\quad+\\,\\|\\mathbb{E}_{S\\setminus\\{z_{i}\\}}[\\mathbf h_{i}(S)]-\\mathbb{E}_{S\\setminus\\{z_{i}\\}}[\\mathbf h_{i}(1,\\ldots,z_{j-1},z_{j}^{\\prime\\prime},z_{j+1},\\ldots,z_{n})]\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "587  For the first term $\\|\\mathbf{h}_{i}(S)-\\mathbf{h}_{i}(z_{1},\\ldots,z_{j-1},z_{j}^{\\prime\\prime},z_{j+1},\\ldots,z_{n})\\|_{2}$ , it can be bounded by $2\\beta$ according   \n58 to the definition of uniform stability. Similar result holds for the second term $\\|\\mathbb{E}_{S\\setminus\\{z_{i}\\}}[\\mathbf{h}_{i}(S)]-$   \n589 $\\mathbb{E}_{S\\backslash\\{z_{i}\\}}[\\mathbf{h}_{i}(1,\\dots,z_{j-1},z_{j}^{\\prime\\prime},z_{j+1},\\dots,z_{n})]||_{2}$ according to the uniform stability. By a combina  \n590 tion of the above analysis, we get $\\|\\mathbf{q}_{i}(S)\\,-\\,\\mathbf{q}_{i}(1,\\dots,z_{j-1},z_{j}^{\\prime\\prime},z_{j+1},\\dots,z_{n})\\|_{2}\\ \\leq\\ \\|\\mathbf{h}_{i}(S)\\,-\\,$   \n591 $\\mathbf{h}_{i}(1,\\dots,z_{j-1},z_{j}^{\\prime\\prime},z_{j+1},\\dots,z_{n})\\|_{2}\\le4\\beta$   \n592  Thus, we have verified that three conditions in Theorem 1 are satisfied for $\\mathbf{q}_{i}(S)$ .Wehavethe   \n593 following result for any $p\\geq2$ ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|\\left\\Vert\\sum_{i=1}^{n}\\mathbf{q}_{i}(S)\\right\\Vert\\right\\|_{p}\\leq2^{4+\\frac{1}{4}}\\left(\\sqrt{\\frac{p}{e}}\\right)(\\sqrt{2p}+1)n\\beta\\left\\lceil\\log_{2}n\\right\\rceil.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "594  Furthermore, we can derive that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~n\\nabla F(A(S))-n\\nabla F_{S}(A(S))-\\displaystyle\\sum_{i=1}^{n}\\mathbf{h}_{i}(S)+\\displaystyle\\sum_{i=1}^{n}\\mathbf{q}_{i}(S)}\\\\ &{{=}n\\nabla F(A(S))-n\\nabla F_{S}(A(S))-\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{S\\setminus\\{z_{i}\\}}[\\mathbf{h}_{i}(S)]}\\\\ &{{=}n\\nabla F(A(S))-n\\nabla F_{S}(A(S))-n\\mathbb{E}_{S^{\\prime}}[\\nabla F(A(S^{\\prime}))]+n\\mathbb{E}_{S}[\\nabla F(A(S))].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "595 Due to the i.i.d. property between $S$ and $S^{\\prime}$ ,we know that $\\mathbb{E}_{S^{\\prime}}[\\nabla F(A(S^{\\prime}))]=\\mathbb{E}_{S}[\\nabla F(A(S))]$   \n596 Thus, combined above equality, (17) and (18), we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\|n\\nabla F(A(S))-n\\nabla F_{S}(A(S))-n\\mathbb{E}_{S}[\\nabla F(A(S))]+n\\mathbb{E}_{S^{\\prime}}[\\nabla F_{S}(A(S^{\\prime}))]\\|_{p}\\right\\|_{p}}\\\\ &{\\le\\left\\|\\left\\|n\\nabla F(A(S))-n\\nabla F_{S}(A(S))-\\displaystyle\\sum_{i=1}^{n}\\mathbf{h}_{i}(S)\\right\\|\\right\\|_{p}}\\\\ &{+\\left\\|\\left\\|\\displaystyle\\sum_{i=1}^{n}\\mathbf{h}_{i}(S)-n\\mathbb{E}_{S}[\\nabla F(A(S))]+n\\mathbb{E}_{S^{\\prime}}F_{S}[A(S^{\\prime})]\\right\\|_{p}}\\\\ &{=\\left\\|\\left\\|n\\nabla F(A(S))-n\\nabla F_{S}(A(S))-\\displaystyle\\sum_{i=1}^{n}\\mathbf{h}_{i}(S)\\right\\|\\right\\|_{p}+\\left\\|\\displaystyle\\sum_{i=1}^{n}\\mathbf{q}_{i}(S)\\right\\|\\right\\|_{p}}\\\\ &{\\le2n\\beta+2^{4+4}\\left(\\sqrt{\\displaystyle\\frac{p}{\\epsilon}}\\right)(\\sqrt{2p}+1)n\\beta\\left[\\log_{2}n\\right]}\\\\ &{\\le16\\times2^{4}\\left(\\sqrt{\\displaystyle\\frac{1}{\\epsilon}}\\right)p n\\beta\\left[\\log_{2}n\\right]+32\\left(\\sqrt{\\displaystyle\\frac{1}{\\epsilon}}\\right)\\sqrt{p n}\\beta\\left[\\log_{2}n\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "597  According to Lemma 6 for any $\\delta\\in(0,1)$ , with probability at least $1-\\delta/3$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\nabla F(A(S))-\\nabla F_{S}(A(S))\\|_{2}}\\\\ &{\\le\\!\\|{\\mathbb{E}}_{S^{\\prime}}[\\nabla F_{S}(A(S^{\\prime}))]-{\\mathbb{E}}_{S}[\\nabla F(A(S))]\\|_{2}}\\\\ &{\\qquad+\\,16\\times2^{\\frac{3}{4}}\\sqrt{e}\\beta\\,\\lceil\\log_{2}n\\rceil\\log\\left(3e/\\delta\\right)+32\\sqrt{e}\\beta\\,\\lceil\\log_{2}n\\rceil\\,\\sqrt{\\log{3e/\\delta}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "598  Next, we need to bound the term $\\|\\mathbb{E}_{S^{\\prime}}[\\nabla F_{S}(A(S^{\\prime}))]\\,-\\,\\mathbb{E}_{S}[\\nabla F(A(S))]\\|_{2}$ . There holds that   \n599 $\\|\\mathbb{E}_{S}\\mathbb{E}_{S^{\\prime}}[\\nabla F_{S}(A(S^{\\prime}))]\\|_{2}=_{\\mathcal{A}}\\|\\mathbb{E}_{S}[\\nabla F(A(\\bar{S}))]\\|_{2}$ . Then, by the Bernstein inequality in Lemma 7, we   \n600 obtain the following inequality with probability at least $\\dot{1}-\\delta/3$ \uff0c ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\big\\|\\mathbb{E}_{S^{\\prime}}[\\nabla F_{S}(A(S^{\\prime}))]-\\mathbb{E}_{S}[\\nabla F(A(S))]\\big\\|_{2}\\leq\\sqrt{\\frac{2\\mathbb{E}_{z_{i}}[\\|\\mathbb{E}_{S^{\\prime}}\\nabla f(A(S^{\\prime});z_{i})\\|_{2}^{2}]\\log\\frac{6}{\\delta}}{n}}+\\frac{M\\log\\frac{6}{\\delta}}{n}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "601 Then using Jensen's inequality, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{z_{i}}[\\|\\mathbb{E}_{S^{\\prime}}\\nabla f(A(S^{\\prime});z_{i})\\|_{2}^{2}]\\leq\\mathbb{E}_{z_{i}}\\mathbb{E}_{S^{\\prime}}\\|\\nabla f(A(S^{\\prime});z_{i})\\|_{2}^{2}}\\\\ {=\\mathbb{E}_{Z}\\mathbb{E}_{S^{\\prime}}\\|\\nabla f(A(S^{\\prime});Z)\\|_{2}^{2}}\\\\ {=\\mathbb{E}_{Z}\\mathbb{E}_{S}\\|\\nabla f(A(S);Z)\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "602 Combing (19), (20) with (21), we finally obtain that with probability at least $1-2\\delta/3$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\nabla F(A(S))-\\nabla F_{S}(A(S))\\|_{2}}\\\\ &{\\le\\!\\sqrt{\\frac{2\\mathbb{E}_{Z}\\mathbb{E}_{S}\\|\\nabla f(A(S);Z)\\|_{2}^{2}\\log\\frac{6}{\\delta}}{n}}+\\frac{M\\log\\frac{6}{\\delta}}{n}}\\\\ &{\\qquad+\\,16\\times2^{\\frac{3}{4}}\\sqrt{e}\\beta\\,\\lceil\\log_{2}n\\rceil\\log\\left(3e/\\delta\\right)+32\\sqrt{e}\\beta\\,\\lceil\\log_{2}n\\rceil\\,\\sqrt{\\log{3e/\\delta}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "603  Next, since $S\\,=\\,\\{z_{i},\\dots,z_{n}\\}$ , we define $p\\,=\\,p(z_{1},\\dots,z_{n})\\,=\\,\\mathbb{E}_{Z}[\\|\\nabla f(A(S);Z)\\|_{2}^{2}]$ and $p_{i}=$   \n604 $\\begin{array}{r}{p_{i}(z_{1},\\dots,z_{n})\\,=\\,\\operatorname*{sup}_{z_{i}\\in{\\mathcal{Z}}}{p(z_{i},\\dots,z_{n})}}\\end{array}$ . So there holds $p_{i}~\\geq~p$ for any $i\\,=\\,1,\\ldots,n$ and any ", "page_idx": 19}, {"type": "text", "text": "605 $\\{z_{1},\\ldots,z_{n}\\}\\in{\\mathcal{Z}}^{n}$ . Also, there holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{i=1}{\\overset{\\sum}{\\sum}}(p_{i}-p)^{2}}\\\\ &{=\\underset{i=1}{\\overset{\\sum}{\\sum}}\\left(\\underset{s\\in\\mathbb{Z}}{\\operatorname*{sup}}\\ \\mathbb{E}_{Z}[\\|\\nabla f(A(S^{\\prime});Z)\\|_{2}^{2}]-\\mathbb{E}_{Z}[\\|\\nabla f(A(S);Z)\\|_{2}^{2}]\\right)^{2}}\\\\ &{\\leq\\underset{i=1}{\\overset{\\sum}{\\sum}}\\left(\\underset{s\\in\\mathbb{Z}}{\\operatorname*{sup}}\\ \\mathbb{E}_{Z}\\left[\\underset{s\\in\\mathbb{Z}}{\\operatorname*{sup}}\\ |\\nabla f(A(S^{\\prime});Z)\\|_{2}^{2}-\\|\\nabla f(A(S);Z)\\|_{2}^{2}\\right]\\right)^{2}}\\\\ &{=\\underset{i=1}{\\overset{\\sum}{\\sum}}\\left(\\mathbb{E}_{Z}\\left[\\underset{s\\in\\mathbb{Z}}{\\operatorname*{sup}}\\ |\\nabla f(A(S^{\\prime});Z)\\|_{2}-\\|\\nabla f(A(S);Z)\\|_{2}\\right)\\left(\\underset{s\\in\\mathbb{Z}}{\\operatorname*{sup}}\\ |\\nabla f(A(S^{\\prime});Z)\\|_{2}+\\|\\nabla f(A(S);Z)\\|_{2}\\right)}\\\\ &{\\leq\\underset{i=1}{\\overset{\\sum}{\\sum}}p^{2}\\left(\\mathbb{E}_{Z}\\left[\\|\\nabla f(A(S);Z)\\|_{2}+\\underset{s\\in\\mathbb{Z}}{\\operatorname*{sup}}\\ |\\nabla f(A(S);Z)\\|_{2}\\right]\\right)^{2}}\\\\ &{\\leq\\underset{j=1}{\\overset{\\sum}{\\sum}}\\left(\\underset{s\\in\\mathbb{Z}}{\\operatorname*{sup}}^{2}\\left(2\\mathbb{E}_{Z}[\\|\\nabla f(A(S);Z)\\|_{2}+\\underset{s\\in\\mathbb{Z}}{\\operatorname*{sup}}]\\ |\\nabla f(A(S);Z)\\|_{2}\\right)^{2}\\right)}\\\\ &{\\leq\\underset{j=1}{\\overset{\\sum}{\\operatorname*{sup}}}(2\\mathbb{E}_{Z}[\\|\\nabla f(A(S);Z)\\|_{2}+\\beta])^{2}}\\\\ &{\\leq\\underset{j=1}{\\overset{\\sum}{\\operatorname*{sup}}}\\beta^{2}+2n\\beta^{4},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "606 where the first inequality follows from the Jensen's inequality. The second and third inequalities   \n607 follow from the definition of uniform stability in gradients. The last inequality holds from that   \n608 $(a+b)^{2}\\leq2a^{2}+2b^{2}$   \n609From (23), we know that $p$ is $(8n\\beta^{2},2n\\beta^{4})$ weakly self-bounded. Thus, by Lemma 8, we obtain that   \n610 with probability at least $1-\\delta/3$ ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad{\\mathbb E}_{Z}{\\mathbb E}_{S}[\\|\\nabla f(A(S);Z)\\|_{2}^{2}]-{\\mathbb E}_{Z}[\\|\\nabla f(A(S);Z)\\|_{2}^{2}]}\\\\ &{\\le\\!\\sqrt{(16n\\beta^{2}{\\mathbb E}_{S}{\\mathbb E}_{Z}[\\|\\nabla f(A(S);Z)\\|_{2}^{2}]+4n\\beta^{4})\\log(3/\\delta)}}\\\\ &{=\\!\\sqrt{({\\mathbb E}_{S}{\\mathbb E}_{Z}[\\|\\nabla f(A(S);Z)\\|_{2}^{2}]+\\frac{1}{4}\\beta^{2})16n\\beta^{2}\\log(3/\\delta)}}\\\\ &{\\le\\!\\frac{1}{2}({\\mathbb E}_{S}{\\mathbb E}_{Z}[\\|\\nabla f(A(S);Z)\\|_{2}^{2}]+\\frac{1}{4}\\beta^{2})+8n\\beta^{2}\\log(3/\\delta),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "611 where the last inequality follows from that ${\\sqrt{a b}}\\leq{\\frac{a+b}{2}}$ for all $a,b>0$ . Thus, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{Z}\\mathbb{E}_{S}[\\|\\nabla f(A(S);Z)\\|_{2}^{2}]\\le2\\mathbb{E}_{Z}[\\|\\nabla f(A(S);Z)\\|_{2}^{2}]+\\frac{1}{4}\\beta^{2}+16n\\beta^{2}\\log(3/\\delta).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "612 Substituting (24) into (22), we finally obtain that with probability at least $1-\\delta$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\lVert\\nabla F(A(S))-\\nabla F_{S}(A(S))\\right\\rVert_{2}}\\\\ &{\\le\\sqrt{\\frac{2\\left(2\\mathbb{E}_{Z}[\\left\\lVert\\nabla f(A(S);Z)\\right\\rVert_{2}^{2}]+\\frac{1}{4}\\beta^{2}+16n\\beta^{2}\\log(3/\\delta)\\right)\\log\\frac{6}{\\delta}}{n}}+\\frac{M\\log\\frac{6}{\\delta}}{n}}\\\\ &{\\qquad+\\,16\\times2^{\\frac{3}{4}}\\sqrt{e}\\beta\\left\\lceil\\log_{2}n\\right\\rceil\\log\\left(3e/\\delta\\right)+32\\sqrt{e}\\beta\\left\\lceil\\log_{2}n\\right\\rceil\\sqrt{\\log3e/\\delta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "613  According to inequality ${\\sqrt{a+b}}={\\sqrt{a}}+{\\sqrt{b}}$ for any $a,b>0$ with probability at least $1-\\delta$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\nabla F(A(S))-\\nabla F_{S}(A(S))\\right\\|_{2}}\\\\ &{\\leq\\sqrt{\\frac{4\\mathbb{E}_{Z}[\\|\\nabla f(A(S);Z)\\|_{2}^{2}]\\log\\frac{6}{\\delta}}{n}}+\\sqrt{\\frac{\\left(\\frac{1}{2}\\beta^{2}+32n\\beta^{2}\\log(3/\\delta)\\right)\\log\\frac{6}{\\delta}}{n}}+\\frac{M\\log\\frac{6}{\\delta}}{n}}\\\\ &{\\qquad+\\,16\\times2^{\\frac{3}{4}}\\sqrt{e}\\beta\\left[\\log_{2}n\\right]\\log\\left(3e/\\delta\\right)+32\\sqrt{e}\\beta\\left[\\log_{2}n\\right]\\sqrt{\\log{3e/\\delta}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "614  The proof is complete. ", "page_idx": 20}, {"type": "text", "text": "615 ", "page_idx": 20}, {"type": "text", "text": "616Proof of Remark 4. According to the proof in Theorem 3, we have the following inequality that with   \n617probability at least $1-\\delta$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\nabla F(A(S))-\\nabla F_{S}(A(S))\\right\\|_{2}}\\\\ &{\\leq\\sqrt{\\frac{4\\mathbb{E}_{Z}[\\|\\nabla f(A(S);Z)\\|_{2}^{2}]\\log\\frac{6}{\\delta}}{n}}+\\sqrt{\\frac{\\left(\\frac{1}{2}\\beta^{2}+32n\\beta^{2}\\log(3/\\delta)\\right)\\log\\frac{6}{\\delta}}{n}}+\\frac{M\\log\\frac{6}{\\delta}}{n}}\\\\ &{\\qquad+\\,16\\times2^{\\frac{3}{4}}\\sqrt{e}\\beta\\left\\lceil\\log_{2}n\\right\\rceil\\log\\left(3e/\\delta\\right)+32\\sqrt{e}\\beta\\left\\lceil\\log_{2}n\\right\\rceil\\sqrt{\\log3e/\\delta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "618 Since $f(\\mathbf{w})$ is $\\gamma$ -smooth, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{Z}[\\|\\nabla f(A(S);Z)\\|_{2}^{2}]}\\\\ &{\\le\\!\\mathbb{E}_{Z}[\\|\\nabla f(A(S);Z)-\\nabla f(\\mathbf{w}^{*};Z)\\|_{2}^{2}+\\|\\nabla f(\\mathbf{w}^{*};Z)\\|_{2}^{2}]}\\\\ &{\\le\\!\\gamma^{2}\\|A(S)-\\mathbf{w}^{*}\\|_{2}^{2}+\\mathbb{E}_{Z}[\\|\\nabla f(\\mathbf{w}^{*};Z)\\|_{2}^{2}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "619  Plugging (27) into (26), we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla F(A(S))-\\nabla F_{S}(A(S))\\|_{2}}\\\\ &{\\le\\sqrt{\\frac{4(\\gamma^{2}\\|A(S)-\\mathbf{w}^{*}\\|_{2}^{2}+\\mathbb{E}_{Z}[\\|\\nabla f(\\mathbf{w}^{*};Z)\\|_{2}^{2}])\\log\\frac{6}{\\delta}}}+\\sqrt{\\frac{\\left(\\frac{1}{2}\\beta^{2}+32n\\beta^{2}\\log(3/\\delta)\\right)\\log\\frac{6}{\\delta}}{n}}}\\\\ &{\\qquad+\\frac{M\\log\\frac{6}{\\delta}}{n}+16\\times2^{\\frac{3}{4}}\\sqrt{e\\beta}\\left[\\log_{2}n\\right]\\log\\left(3e/\\delta\\right)+32\\sqrt{e\\beta}\\left[\\log_{2}n\\right]\\sqrt{\\log3e/\\delta}}\\\\ &{\\le2\\gamma\\|A(S)-\\mathbf{w}^{*}\\|_{2}\\sqrt{\\frac{\\log\\frac{6}{\\delta}}{n}}+\\sqrt{\\frac{4\\mathbb{E}_{Z}[\\|\\nabla f(\\mathbf{w}^{*};Z)\\|_{2}^{2}]\\log\\frac{6}{\\delta}}{n}}}\\\\ &{\\qquad+\\sqrt{\\frac{\\left(\\frac{1}{2}\\beta^{2}+32n\\beta^{2}\\log(3/\\delta)\\right)\\log\\frac{6}{\\delta}}{n}}+\\frac{M\\log\\frac{6}{\\delta}}{n}}\\\\ &{\\qquad\\qquad+16\\times2^{\\frac{3}{4}}\\sqrt{e\\beta\\log_{2}n}\\log(3e/\\delta)+32\\sqrt{e\\beta}\\left[\\log_{2}n\\right]\\sqrt{\\log3e/\\delta},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "620  where the second inequality holds because ${\\sqrt{a+b}}+{\\sqrt{a}}+{\\sqrt{b}}$ for any $a,b>0$ which means that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\|\\nabla F(A(S))-\\nabla F_{S}(A(S))\\|_{2}}\\\\ &{\\lesssim\\beta\\log n\\log(1/\\delta)+\\frac{\\log(1/\\delta)}{n}+\\sqrt{\\frac{\\mathbb{E}_{Z}\\left[\\nabla\\|f(\\mathbf{w}^{*};Z)\\|_{2}^{2}\\right]\\log(1/\\delta)}{n}}+\\|A(S)-\\mathbf{w}^{*}\\|\\sqrt{\\frac{\\log(1/\\delta)}{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "621  The proof is complete. ", "page_idx": 21}, {"type": "text", "text": "622 ", "page_idx": 21}, {"type": "text", "text": "623 Proof of Lemma $^{\\,l}$ . Inequality (28) implies that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\nabla F(A(S))\\|_{2}-\\|\\nabla F_{S}(A(S))\\|_{2}}\\\\ &{\\le\\sqrt{\\frac{4(\\gamma^{2}\\|A(S)-\\mathbf{w}^{*}\\|_{2}^{2}+\\mathbb{E}_{Z}\\|\\|\\nabla f(\\mathbf{w}^{*};Z)\\|_{2}^{2})\\log\\frac{6}{\\delta}}{n}}+\\sqrt{\\frac{\\left(\\frac{1}{2}\\beta^{2}+32n\\beta^{2}\\log(3/\\delta)\\right)\\log\\frac{6}{\\delta}}{n}}}\\\\ &{\\qquad+\\frac{M\\log\\frac{6}{\\delta}}{n}+16\\times2^{\\frac{3}{4}}\\sqrt{e\\beta\\left[\\log_{2}n\\right]\\log(3e/\\delta)}+32\\sqrt{e\\beta}\\left[\\log_{2}n\\right]\\sqrt{\\log{3e/\\delta}}}\\\\ &{\\le2\\gamma\\|A(S)-\\mathbf{w}^{*}\\|_{2}\\sqrt{\\frac{\\log\\frac{6}{\\delta}}{n}}+\\sqrt{\\frac{4\\mathbb{E}_{Z}\\left[\\|\\nabla f(\\mathbf{w}^{*};Z)\\|_{2}^{2}\\right]\\log\\frac{6}{\\delta}}{n}}+\\sqrt{\\frac{\\left(\\frac{1}{2}\\beta^{2}+32n\\beta^{2}\\log(3/\\delta)\\right)\\log\\frac{6}{\\delta}}{n}}}\\\\ &{\\qquad+\\frac{M\\log\\frac{6}{\\delta}}{n}+16\\times2^{\\frac{3}{4}}\\sqrt{e\\beta\\left[\\log_{2}n\\right]\\log(3e/\\delta)+32\\sqrt{e\\beta}\\left[\\log_{2}n\\right]\\sqrt{\\log{3e/\\delta}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "624When $F(\\mathbf{w})$ satisfies the $\\mathrm{PL}$ condition, there holds the following error bound property (refer to   \n625 Theorem 2 in Karimi et al. [2016]) ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\nabla F(A(S))\\|_{2}\\geq\\mu\\|A(S)-\\mathbf{w}^{*}\\|_{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "626 Thus, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mu\\|A(S)-\\mathbf{w}^{*}\\|_{2}\\leq\\|\\nabla F(A(S))\\|_{2}}\\\\ &{\\leq\\|\\nabla F_{S}(A(S))\\|_{2}+2\\gamma\\|A(S)-\\mathbf{w}^{*}\\|_{2}\\sqrt{\\frac{\\log\\frac{6}{\\delta}}{n}}+\\sqrt{\\frac{4\\mathbb{E}_{Z}[\\|\\nabla f(\\mathbf{w}^{*};Z)\\|_{2}^{2}]\\log\\frac{6}{\\delta}}{n}}}\\\\ &{\\quad\\quad+\\sqrt{\\frac{\\left(\\frac{1}{2}\\beta^{2}+32n\\beta^{2}\\log(3/\\delta)\\right)\\log\\frac{6}{\\delta}}{n}}+\\frac{M\\log\\frac{6}{\\delta}}{n}}\\\\ &{\\quad\\quad\\quad+16\\times2^{\\frac{3}{4}}\\sqrt{e}\\beta\\left[\\log_{2}n\\right]\\log\\left(3e/\\delta\\right)+32\\sqrt{e}\\beta\\left[\\log_{2}n\\right]\\sqrt{\\log3e/\\delta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "627 When $\\begin{array}{r}{n\\geq\\frac{16\\gamma^{2}\\log\\frac{6}{\\delta}}{\\mu^{2}}}\\end{array}$ , we have $\\begin{array}{r}{2\\gamma\\sqrt{\\frac{\\log\\frac{6}{\\delta}}{n}}\\leq\\frac{\\mu}{2}}\\end{array}$ , then we can derive that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~\\mu\\|A(S)-\\mathbf{w}^{*}\\|_{2}\\leq\\|\\nabla F(A(S))\\|_{2}}\\\\ &{\\leq\\|\\nabla F_{S}(A(S))\\|_{2}+\\frac{\\mu}{2}\\|A(S)-\\mathbf{w}^{*}\\|_{2}+\\sqrt{\\frac{4\\mathbb{E}_{Z}[\\|\\nabla f(\\mathbf{w}^{*};Z)\\|_{2}^{2}]\\log\\frac{6}{\\delta}}{n}}}\\\\ &{\\qquad+\\,\\sqrt{\\frac{\\left(\\frac{1}{2}\\beta^{2}+32n\\beta^{2}\\log(3/\\delta)\\right)\\log\\frac{6}{\\delta}}{n}}+\\frac{M\\log\\frac{6}{\\delta}}{n}}\\\\ &{\\qquad\\qquad+\\,16\\times2^{\\frac{3}{4}}\\sqrt{e}\\beta\\left[\\log_{2}n\\right]\\log\\left(3e/\\delta\\right)+32\\sqrt{e}\\beta\\left[\\log_{2}n\\right]\\sqrt{\\log3e/\\delta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "628 This implies that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|A(S)-\\mathbf{w}^{*}\\|_{2}}\\\\ &{\\leq\\!\\frac{2}{\\mu}\\Big(\\|\\nabla F_{S}(A(S))\\|_{2}+\\sqrt{\\frac{4\\mathbb{E}_{Z}[\\|\\nabla f(\\mathbf{w}^{*};Z)\\|_{2}^{2}]\\log\\frac{6}{\\delta}}{n}}}\\\\ &{\\quad\\quad+\\sqrt{\\frac{\\left(\\frac{1}{2}\\beta^{2}+32n\\beta^{2}\\log(3/\\delta)\\right)\\log\\frac{6}{\\delta}}{n}}+\\frac{M\\log\\frac{6}{\\delta}}{n}}\\\\ &{\\qquad\\quad+\\ 16\\times2^{\\frac{3}{4}}\\sqrt{e}\\beta\\left[\\log_{2}n\\right]\\log\\left(3e/\\delta\\right)+32\\sqrt{e}\\beta\\left[\\log_{2}n\\right]\\sqrt{\\log{3e/\\delta}}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "629Then, ubstng29it28, $\\begin{array}{r}{n\\geq\\frac{16\\gamma^{2}\\log\\frac{6}{\\delta}}{\\mu^{2}}}\\end{array}$ with probabity a east $1-\\delta$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\lVert\\nabla F(A(S))-\\nabla F_{S}(A(S))\\right\\rVert}\\\\ &{\\le\\lVert\\nabla F_{S}(A(S))\\rVert+4\\sqrt{\\frac{\\mathbb{E}_{Z}[\\left\\lVert\\nabla f(\\mathbf{w}^{*};Z)\\right\\rVert^{2}]\\log\\frac{6}{\\delta}}{n}}+2\\sqrt{\\frac{\\left(\\frac{1}{2}\\beta^{2}+32n\\beta^{2}\\log(3/\\delta)\\right)\\log\\frac{6}{\\delta}}{n}}}\\\\ &{\\qquad+\\,\\frac{2M\\log\\frac{6}{\\delta}}{n}+32\\times2^{\\frac{3}{4}}\\sqrt{e}\\beta\\left[\\log_{2}n\\right]\\log\\left(3e/\\delta\\right)+64\\sqrt{e}\\beta\\left[\\log_{2}n\\right]\\sqrt{\\log3e/\\delta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "630 The proof is complete. ", "page_idx": 22}, {"type": "text", "text": "631 Proof of Remark 5. Here we briefy prove the results given in Remark 5. Since $F$ satisfies the PL   \n632condition with $\\mu$ ,wehave ", "page_idx": 22}, {"type": "equation", "text": "$$\nF(A(S))-F(\\mathbf{w}^{*})\\leq\\frac{\\left\\|\\nabla F(A(S))\\right\\|^{2}}{2\\mu},\\quad\\forall\\mathbf{w}\\in\\mathcal{W}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "633  So to bound $F(A(S))-F(A(S))$ , we need to bound the term $\\big\\|\\nabla F\\big(A(S)\\big)\\big\\|^{2}$ . And there holds ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\|\\nabla F(A(S))\\|_{2}^{2}}=2\\,{\\|\\nabla F(A(S))-\\nabla F_{S}(A(S))\\|^{2}}+2\\|\\nabla F_{S}(A(S))\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "634  From Lemma 1, if $f$ is $M$ -Lipschitz and $\\gamma.$ smooth and $F$ satisfies PL condition with $\\mu$ , for any $\\delta>0$   \n635 when $\\begin{array}{r}{n\\geq\\frac{16\\gamma^{2}\\log\\frac{6}{\\delta}}{\\mu^{2}}}\\end{array}$ , with probability at least $1-\\delta$ , there holds ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\lVert\\nabla F(A(S))-\\nabla F_{S}(A(S))\\right\\rVert_{2}}\\\\ &{\\le\\left\\lVert\\nabla F_{S}(A(S))\\right\\rVert_{2}+C\\left(\\sqrt{\\frac{2\\mathbb{E}_{Z}\\left[\\left\\lVert\\nabla f(\\mathbf{w}^{*};Z)\\right\\rVert_{2}^{2}\\right]\\log\\frac{6}{\\delta}}{n}}+\\frac{M\\log\\frac{6}{\\delta}}{n}+e\\beta\\left\\lceil\\log_{2}n\\right\\rceil\\log\\left(3e/\\delta\\right)\\right)}\\\\ &{\\le\\left\\lVert\\nabla F_{S}(A(S))\\right\\rVert_{2}+C\\left(\\sqrt{\\frac{8\\gamma F(\\mathbf{w}^{*})\\log\\frac{6}{\\delta}}{n}}+\\frac{M\\log\\frac{6}{\\delta}}{n}+e\\beta\\left\\lceil\\log_{2}n\\right\\rceil\\log\\left(3e/\\delta\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "636where $C$ is a positive constant and the last inequality follows from Lemma 4.1 of Srebro et al. [2010]   \n637when $f$ is nonegative and $\\gamma$ -smooth (see (44)). ", "page_idx": 23}, {"type": "text", "text": "638 Combing above inequality with (30), (31), we can derive that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\overset{\\vartriangle}{\\boldsymbol{\\tau}}(A(S))-\\boldsymbol{F}(\\mathbf{w}^{*})\\lesssim\\|\\nabla F_{S}(A(S))\\|_{2}+\\frac{\\boldsymbol{F}(\\mathbf{w}^{*})\\log\\left(1/\\delta\\right)}{n}+\\frac{M\\log^{2}(1/\\delta)}{n^{2}}+\\beta^{2}\\log^{2}n\\log^{2}(1/\\delta).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "639  The proof is complete. ", "page_idx": 23}, {"type": "text", "text": "640 ", "page_idx": 23}, {"type": "text", "text": "641  Proof of Lemma 2. According to the proof in Theorem 3, we have the following inequality with   \n642probability at least $1-\\delta$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\nabla F(A(S))-\\nabla F_{S}(A(S))\\right\\|_{2}}\\\\ &{\\le\\sqrt{\\frac{2\\left(2\\mathbb{E}_{Z}[\\left\\|\\nabla f(A(S);Z)\\right\\|_{2}^{2}]+\\frac{1}{4}\\beta^{2}+16n\\beta^{2}\\log(3/\\delta)\\right)\\log\\frac{6}{\\delta}}{n}}}\\\\ &{\\qquad+\\,\\frac{M\\log\\frac{6}{\\delta}}{n}+16\\times2^{\\frac{3}{4}}\\sqrt{e}\\beta\\left\\lceil\\log_{2}n\\right\\rceil\\log\\left(3e/\\delta\\right)+32\\sqrt{e}\\beta\\left\\lceil\\log_{2}n\\right\\rceil\\sqrt{\\log{3e/\\delta}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "643   Since SGC implies that $\\mathbb{E}_{Z}[\\|\\nabla f(\\mathbf{w};Z)\\|_{2}^{2}]\\le\\rho\\|\\nabla F(\\mathbf{w})\\|_{2}^{2}$ according to inequalities ${\\sqrt{a b}}\\leq\\eta a\\!+\\!{\\frac{1}{\\eta}}b$   \n64   and ${\\sqrt{a+b}}\\leq{\\sqrt{a}}+{\\sqrt{b}}$ for any $a,b,\\eta>0$ , we have the following inequality with probability at   \n645least $1-\\delta$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla F(A(S))-\\nabla F_{S}(A(S))\\|_{2}}\\\\ &{\\le\\sqrt{\\frac{2\\left(2\\rho\\|\\nabla F(A(S))\\|_{2}^{2}+\\frac{1}{4}\\beta^{2}+16n\\beta^{2}\\log(3/\\delta)\\right)\\log\\frac{6}{\\delta}}{n}}}\\\\ &{\\qquad+\\,\\frac{M\\log\\frac{6}{\\delta}}{n}+16\\times2^{\\frac{3}{4}}\\sqrt{e}\\beta\\left[\\log_{2}n\\right]\\log\\left(3e/\\delta\\right)+32\\sqrt{e}\\beta\\left[\\log_{2}n\\right]\\sqrt{\\log{3e/\\delta}}}\\\\ &{\\le\\sqrt{\\frac{(\\frac{1}{2}\\beta^{2}+32n\\beta^{2}\\log(3/\\delta))\\log\\frac{6}{\\delta}}{n}}+\\frac{\\eta}{1+\\eta}\\|\\nabla F(A(S))\\|+\\frac{1+\\eta}{\\eta}\\frac{4\\rho M\\log\\frac{6}{\\delta}}{n}}\\\\ &{\\qquad+\\,\\frac{M\\log\\frac{6}{\\delta}}{n}+16\\times2^{\\frac{3}{4}}\\sqrt{e}\\beta\\left[\\log_{2}n\\right]\\log\\left(3e/\\delta\\right)+32\\sqrt{e}\\beta\\left[\\log_{2}n\\right]\\sqrt{\\log{3e/\\delta}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "646 which implies that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\nabla F(A(S))\\|_{2}\\leq(1+\\eta)\\|\\nabla F_{S}(A(S))\\|_{2}+C\\frac{1+\\eta}{\\eta}\\left(\\frac{M}{n}\\log\\frac{6}{\\delta}+\\beta\\log n\\log\\frac{1}{\\delta}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "647 The proof is complete. ", "page_idx": 23}, {"type": "text", "text": "648 D Proofs of ERM ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "649  Proof of Lemma 3. Since $\\begin{array}{r}{F_{S^{(i)}}(\\mathbf{w})=\\frac{1}{n}\\left(f(\\mathbf{w};z_{i}^{\\prime})+\\sum_{j\\neq i}f(\\mathbf{w},z_{j})\\right)}\\end{array}$ we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad F_{S}(\\widehat{\\mathbf{w}}^{*}(S^{(i)}))-F_{S}(\\widehat{\\mathbf{w}}^{*}(S))}\\\\ &{=\\frac{f(\\widehat{\\mathbf{w}}^{*}(S^{(i)});\\,z_{i})-f(\\widehat{\\mathbf{w}}^{*}(S);\\,z_{i})}{n}+\\frac{\\sum_{j\\neq i}(f(\\widehat{\\mathbf{w}}^{*}(S^{(i)});\\,z_{j})-f(\\widehat{\\mathbf{w}}^{*}(S);\\,z_{j}))}{n}}\\\\ &{=\\frac{f(\\widehat{\\mathbf{w}}^{*}(S^{(i)});\\,z_{i})-f(\\widehat{\\mathbf{w}}^{*}(S);\\,z_{i})}{n}+\\frac{f(\\widehat{\\mathbf{w}}^{*}(S);\\,z_{i}^{\\prime})-f(\\widehat{\\mathbf{w}}^{*}(S^{(i)});\\,z_{i}^{\\prime})}{n}}\\\\ &{\\quad+\\left(F_{S^{(i)}}(\\widehat{\\mathbf{w}}^{*}(S^{(i)}))-F_{S^{(i)}}(\\widehat{\\mathbf{w}}^{*}(S))\\right)}\\\\ &{\\leq\\frac{f(\\widehat{\\mathbf{w}}^{*}(S^{(i)});\\,z_{i})-f(\\widehat{\\mathbf{w}}^{*}(S);\\,z_{i})}{n}+\\frac{f(\\widehat{\\mathbf{w}}^{*}(S);\\,z_{i}^{\\prime})-f(\\widehat{\\mathbf{w}}^{*}(S^{(i)});\\,z_{i}^{\\prime})}{n}}\\\\ &{\\leq\\frac{2M}{n}\\Vert\\widehat{\\mathbf{w}}^{*}(S^{(i)})-\\widehat{\\mathbf{w}}^{*}(S)\\Vert_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "650 where the first inequality follows from the fact that $\\hat{\\mathbf{w}}^{*}(S^{(i)})$ is the ERM of $F_{S^{(i)}}$ and the second   \n651 inequality follows from the Lipschitz property. Furthermore, for $\\hat{\\mathbf{w}}^{*}(S^{(i)})$ , the convexity of $f$ and   \n652 the strongly-convex property of $F_{S}$ imply that its closest optima point of $F_{S}$ is $\\hat{\\mathbf{w}}^{*}(S)$ (the global   \n653 minimizer of $F_{S}$ is unique). Then, there holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\nF_{S}\\big(\\hat{\\mathbf{w}}^{*}(S^{(i)})\\big)-F_{S}\\big(\\hat{\\mathbf{w}}^{*}(S)\\big)\\geq\\frac{\\mu}{2}\\|\\hat{\\mathbf{w}}^{*}(S^{(i)})-\\hat{\\mathbf{w}}^{*}(S)\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "654 Then we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{\\mu}{2}\\|\\hat{\\mathbf{w}}^{*}(S^{(i)})-\\hat{\\mathbf{w}}^{*}(S)\\|_{2}^{2}\\leq F_{S}(\\hat{\\mathbf{w}}^{*}(S^{(i)}))-F_{S}(\\hat{\\mathbf{w}}^{*}(S))\\leq\\frac{2M}{n}\\|\\hat{\\mathbf{w}}^{*}(S^{(i)})-\\hat{\\mathbf{w}}^{*}(S)\\|_{2},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "65 hich imples that $\\begin{array}{r}{\\|\\hat{\\mathbf{w}}^{*}(S^{(i)})-\\hat{\\mathbf{w}}^{*}(S)\\|_{2}\\le\\frac{4M}{n\\mu}}\\end{array}$ Combined with the smoothness property of $f$ we   \n656  obtain that for any $S^{(i)}$ and $S$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\forall z\\in\\mathcal{Z},\\quad\\left\\|\\nabla f(\\hat{\\mathbf{w}}^{*}(S^{(i)});z)-\\nabla f(\\hat{\\mathbf{w}}^{*}(S);z)\\right\\|_{2}\\leq\\frac{4M\\gamma}{n\\mu}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "657 The proof is complete. ", "page_idx": 24}, {"type": "text", "text": "658 Proof of Theorem 4. Since $F$ is $\\mu$ -strongly convex, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\nF(\\mathbf{w})-F(\\mathbf{w}^{*})\\leq\\frac{\\|\\nabla F(\\mathbf{w})\\|_{2}^{2}}{2\\mu},\\quad\\forall\\mathbf{w}\\in\\mathcal{W}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "659  So to bound $F(\\hat{\\mathbf{w}}^{*})-F(\\mathbf{w}^{*})$ , we need to bound the term $\\big\\|\\nabla F\\big(\\hat{\\mathbf{w}}^{*}\\big)\\big\\|_{2}^{2}$ . And there holds ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|\\nabla F(\\hat{\\mathbf{w}}^{*})\\|_{2}^{2}=2\\,\\|\\nabla F(\\hat{\\mathbf{w}}^{*})-\\nabla F_{S}(\\hat{\\mathbf{w}}^{*})\\|_{2}^{2}+2\\|\\nabla F_{S}(\\hat{\\mathbf{w}}^{*})\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "660 From Lemma 1, if $f$ .s $M$ -Lipschitz and $\\gamma$ smooth and $F_{S}$ .s $\\mu$ -strongly convex, for any $\\delta>0$ , when   \n061 $\\begin{array}{r}{n\\geq\\frac{16\\gamma^{2}\\log\\,\\frac{6}{\\delta}}{\\mu^{2}}}\\end{array}$ $1-\\delta$ there hols ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\nabla F(\\hat{\\mathbf{w}}^{*})-\\nabla F_{S}(\\hat{\\mathbf{w}}^{*})\\|_{2}}\\\\ &{\\le\\|\\nabla F_{S}(\\hat{\\mathbf{w}}^{*})\\|_{2}+C\\left(\\sqrt{\\frac{2\\mathbb{E}_{Z}[\\|\\nabla f(\\mathbf{w}^{*};Z)\\|_{2}^{2}]\\log\\frac{6}{\\delta}}{n}}+\\frac{M\\log\\frac{6}{\\delta}}{n}+e\\hat{\\beta}\\left[\\log_{2}n\\right]\\log\\left(3e/\\delta\\right)\\right)}\\\\ &{\\le\\|\\nabla F_{S}(\\hat{\\mathbf{w}}^{*})\\|_{2}+C\\left(\\sqrt{\\frac{8\\gamma F(\\mathbf{w}^{*})\\log\\frac{6}{\\delta}}{n}}+\\frac{M\\log\\frac{6}{\\delta}}{n}+e\\hat{\\beta}\\left[\\log_{2}n\\right]\\log\\left(3e/\\delta\\right)\\right),}\\\\ &{\\le\\|\\nabla F_{S}(\\hat{\\mathbf{w}}^{*})\\|_{2}+C\\left(\\sqrt{\\frac{8\\gamma F(\\mathbf{w}^{*})\\log\\frac{6}{\\delta}}{n}}+\\frac{M\\log\\frac{6}{\\delta}}{n}+e\\hat{\\beta}\\left[\\log_{2}n\\right]\\log\\left(3e/\\delta\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "662 where the last inequality follows from Lemma 4.1 of Srebro et al. [2010] when $f$ is nonegative (see   \n663(44)) and $\\gamma$ -smooth and $\\hat{\\beta}=\\|\\nabla f(\\hat{\\mathbf{w}}^{*}(S);z)-\\nabla f(\\hat{\\mathbf{w}}^{*}(S^{\\prime});z)\\|_{2}$ $C$ is a positive constant.   \n664  From Lemma 3, we have $\\begin{array}{r}{\\|\\nabla f(\\hat{\\mathbf{w}}^{*}(S);z)-\\nabla f(\\hat{\\mathbf{w}}^{*}(S^{\\prime});z)\\|_{2}\\,\\le\\frac{4M\\gamma}{n\\mu}}\\end{array}$ 4M. Since VFs(w\\*) = 0, we   \n665 have $\\|\\nabla F_{S}(\\hat{\\mathbf{w}}^{*})\\|_{2}=0$ , then we can derive that ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "equation", "text": "$$\nF(\\mathbf{w})-F(\\mathbf{w}^{*})\\lesssim\\frac{F(\\mathbf{w}^{*})\\log\\left(1/\\delta\\right)}{n}+\\frac{\\log^{2}n\\log^{2}(1/\\delta)}{n^{2}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "666 ", "page_idx": 25}, {"type": "text", "text": "667 E Proofs of PGD ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "668 Proof of Theorem 5. According to smoothness assumption and $\\eta=1/\\gamma$ , we can derive that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad F_{S}(\\mathbf w_{t+1})-F_{S}(\\mathbf w_{t})}\\\\ &{\\le\\langle\\mathbf w_{t+1}-\\mathbf w_{t},\\nabla F_{S}(\\mathbf w_{t})\\rangle+\\frac{\\gamma}{2}\\|\\mathbf w_{t+1}-\\mathbf w_{t}\\|_{2}^{2}}\\\\ &{=-\\eta_{t}\\|\\nabla F_{S}(\\mathbf w_{t})\\|_{2}^{2}+\\frac{\\gamma}{2}\\eta_{t}^{2}\\|\\nabla F_{S}(\\mathbf w_{t})\\|_{2}^{2}}\\\\ &{=\\left(\\frac{\\gamma}{2}\\eta_{t}^{2}-\\eta_{t}\\right)\\|\\nabla F_{S}(\\mathbf w_{t})\\|_{2}^{2}}\\\\ &{\\le-\\frac{1}{2}\\eta_{t}\\|\\nabla F_{S}(\\mathbf w_{t})\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "669 According to above inequality and the assumptions that $F_{S}$ is $\\mu$ -strongly convex, we can prove that ", "page_idx": 25}, {"type": "equation", "text": "$$\nF_{S}(\\mathbf{w}_{t+1})-F_{S}(\\mathbf{w}_{t})\\leq-\\frac{1}{2}\\eta_{t}\\|\\nabla F_{S}(\\mathbf{w}_{t})\\|_{2}^{2}\\leq-\\mu\\eta_{t}(F_{S}(\\mathbf{w}_{t})-F_{S}(\\hat{\\mathbf{w}}^{*})),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "670 which implies that ", "page_idx": 25}, {"type": "equation", "text": "$$\nF_{S}(\\mathbf{w}_{t+1})-F_{S}(\\hat{\\mathbf{w}}^{*})\\leq(1-\\mu\\eta_{t})(F_{S}(\\mathbf{w}_{t})-F_{S}(\\hat{\\mathbf{w}}^{*})).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "671 According to the property for $\\gamma$ -smoothfor $F_{S}$ and the property for $\\mu$ -strongly convex for $F_{S}$ \uff0cwe   \n672have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{1}{2\\gamma}\\|\\nabla F_{S}(\\mathbf{w})\\|_{2}^{2}\\leq F_{S}(\\mathbf{w})-F_{S}(\\hat{\\mathbf{w}}^{*})\\leq\\frac{1}{2\\mu}\\|\\nabla F_{S}(\\mathbf{w})\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "673 which means that $\\textstyle{\\frac{\\mu}{\\gamma}}\\leq1$ ", "page_idx": 25}, {"type": "text", "text": "674  Then If $\\eta_{t}=1/\\gamma,0\\leq1-\\mu\\eta_{t}<1$ , taking over $T$ iterations, we get ", "page_idx": 25}, {"type": "equation", "text": "$$\nF_{S}(\\mathbf{w}_{t+1})-F_{S}(\\hat{\\mathbf{w}}^{*})\\leq(1-\\mu\\eta_{t})^{T}(F_{S}(\\mathbf{w}_{t})-F_{S}(\\hat{\\mathbf{w}}^{*})).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "675  Combined (36), the smoothness of $F_{S}$ and the nonnegative property of $f$ , it can be derive that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\nabla F_{S}(\\mathbf{w}_{T+1}))\\|_{2}^{2}=O\\left((1-\\frac{\\mu}{\\gamma})^{T}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "676  Furthermore, since $F$ is $\\mu$ -strongly convex, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\nF(\\mathbf{w})-F(\\mathbf{w}^{*})\\leq\\frac{\\|\\nabla F(\\mathbf{w})\\|_{2}^{2}}{2\\mu},\\quad\\forall\\mathbf{w}\\in\\mathcal{W}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "677 So to bound $F(\\mathbf{w}_{T+1})-F(\\mathbf{w}^{*})$ , we need to bound the term $\\lVert\\nabla F(\\mathbf{w}_{T+1})\\rVert_{2}^{2}$ . And there holds ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla F(\\mathbf{w}_{T+1})\\|_{2}^{2}=2\\,\\|\\nabla F(\\mathbf{w}_{T+1})-\\nabla F_{S}(\\mathbf{w}_{T+1})\\|_{2}^{2}+2\\|\\nabla F_{S}(\\mathbf{w}_{T+1})\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "678  From Lemma 1, if $f$ .s $M$ -Lipschitz and $\\gamma$ smooth and $F_{S}$ .s $\\mu$ -strongly convex, for any $\\delta>0$ , when   \n679 $\\begin{array}{r}{n\\geq\\frac{16\\gamma^{2}\\log\\,\\frac{6}{\\delta}}{\\mu^{2}}}\\end{array}$ $1-\\delta$ there olds ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\nabla F(\\mathbf{w}_{T+1})-\\nabla F_{S}(\\mathbf{w}_{T+1})\\right\\|_{2}}\\\\ &{\\le\\|\\nabla F_{S}(\\mathbf{w}_{T+1})\\|_{2}+C\\left(\\sqrt{\\frac{2\\mathbb{E}_{Z}[\\|\\nabla f(\\mathbf{w}^{*};Z)\\|_{2}^{2}]\\log\\frac{6}{\\delta}}{n}}+\\frac{M\\log\\frac{6}{\\delta}}{n}+e\\beta\\left\\lceil\\log_{2}n\\right\\rceil\\log\\left(3e/\\delta\\right)\\right)}\\\\ &{\\le\\|\\nabla F_{S}(\\mathbf{w}_{T+1})\\|_{2}+C\\left(\\sqrt{\\frac{8\\gamma F(\\mathbf{w}^{*})\\log\\frac{6}{\\delta}}{n}}+\\frac{M\\log\\frac{6}{\\delta}}{n}+e\\beta\\left\\lceil\\log_{2}n\\right\\rceil\\log\\left(3e/\\delta\\right)\\right),}\\\\ &{\\le\\|\\nabla F_{S}(\\mathbf{w}_{T+1})\\|_{2}+\\cdots\\left(\\sqrt{\\frac{8\\gamma F(\\mathbf{w}^{*})\\log\\frac{6}{\\delta}}{n}}+\\frac{M\\log\\frac{6}{\\delta}}{n}+e\\beta\\left\\lceil\\log_{2}n\\right\\rceil\\log\\left(3e/\\delta\\right)\\right),}\\\\ &{\\le\\|\\nabla F_{S}(\\mathbf{w}_{T+1})\\|_{2}+C\\left(\\sqrt{\\frac{8\\gamma F(\\mathbf{w}^{*})\\log\\frac{6}{\\delta}}{n}}+\\frac{M\\log\\frac{6}{\\delta}}{n}+e\\beta\\left\\lceil\\log_{2}n\\right\\rceil\\log\\left(3e/\\delta\\right)\\right),}\\\\ &{\\le\\|\\nabla F_{S}(\\mathbf{w}_{T+1})\\|_{2}+C\\left(\\sqrt{\\frac{8\\gamma F(\\mathbf{w}^{*})\\log\\frac{6}{\\delta}}{n}}+\\frac{M\\log\\frac{6}{\\delta}}{n}+e\\beta\\left\\lceil\\log_{2}n\\right)\\log(3e/\\delta\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "680 where the last inequality follows from Lemma 4.1 of Srebro et al. [2010] when $f$ is nonegative and   \n681 $\\gamma$ -smooth (see (44)) and $\\beta=\\|\\nabla f(\\mathbf{w}_{T+1}(S);z)-\\nabla f(\\mathbf{w}_{T+1}(S^{\\prime});z)\\|_{2}$ $C$ is a positive constant.   \n682 From Lemma 4, we have $\\begin{array}{r l r}{\\beta}&{=}&{\\|\\nabla f(\\mathbf{w}_{T+1}(S);z)-\\nabla f(\\mathbf{w}_{T+1}(S^{\\prime});z)\\|_{2}}&{\\leq}&{\\frac{2M\\gamma}{n\\mu}.}\\\\ &{}&\\end{array}$ 2M. Since   \n683 $\\begin{array}{r}{\\|\\nabla F_{S}(\\mathbf{w}_{T+1})\\|_{2}=O\\left((1-\\frac{\\mu}{\\gamma})^{T}\\right)}\\end{array}$ , then we can derive that ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "equation", "text": "$$\nF(\\mathbf{w})-F(\\mathbf{w}^{*})\\lesssim\\left(1-\\frac{\\mu}{\\gamma}\\right)^{2T}+\\frac{F(\\mathbf{w}^{*})\\log{(1/\\delta)}}{n}+\\frac{\\log^{2}{n\\log^{2}(1/\\delta)}}{n^{2}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "684 Let $T\\asymp\\log n$ ,we have ", "page_idx": 26}, {"type": "equation", "text": "$$\nF(\\mathbf{w})-F(\\mathbf{w}^{*})\\lesssim\\frac{F(\\mathbf{w}^{*})\\log\\left(1/\\delta\\right)}{n}+\\frac{\\log^{2}n\\log^{2}(1/\\delta)}{n^{2}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "685  The proof is complete. ", "page_idx": 26}, {"type": "text", "text": "686 F  Proofs of SGD ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "687 We frst introduce some necessary lemmata on the empirical risk. ", "page_idx": 26}, {"type": "text", "text": "688 Lemma 14 ([Lei and Tang, 2021]). Let $\\{\\mathbf{w}_{t}\\}_{t}$ be the sequence produced by SGD with $\\begin{array}{r}{\\eta_{t}\\leq\\frac{1}{2\\gamma}}\\end{array}$ for   \n689   all $t\\in\\mathbb{N}$ .Suppose Assumption 1 hold. Assume for all $z$ the function $\\mathbf{w}\\mapsto f(\\mathbf{w};z)$ is $M$ -Lipschitz   \n690and $\\gamma$ -smooth.Then,for any $\\delta\\in(0,1)$ ,with probability at least $1-\\delta$ thereholdsthat ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{t}\\eta_{k}\\|\\nabla F_{S}(\\mathbf{w}_{k})\\|_{2}^{2}=O\\left(\\log\\frac{1}{\\delta}+\\sum_{k=1}^{t}\\eta_{k}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "691 Lemma 15(Leiand Tang, 2021]). Let {wt}t b the sequence produced by SGD with m = utto)   \n692  such that $t_{0}\\geq\\operatorname*{max}\\{\\frac{4\\gamma}{\\mu},1\\}$ for all $t\\in\\mathbb{N}$ Suppose Assumption $^{\\,l}$ hold. Assume for all $z$ the function   \n693 $\\mathbf{w}\\mapsto f(\\mathbf{w};z)$ $M$ -Lipschitz, and $\\gamma$ smooth and assume $F_{S}$ satisfies $P L$ condition with parameter $\\mu$   \n694Then, for any $\\delta\\in(0,1)$ ,withprobability atleast $1-\\delta$ there holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\nF_{S}(\\mathbf{w}_{T+1})-F_{S}(\\hat{\\mathbf{w}}^{*})=O\\left(\\frac{\\log(T)\\log^{3}(1/\\delta)}{T}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "695 Lemma 16 ([Lei and Tang, 2021]). Let e be the base of the natural logarithm. There holds the   \n696followingelementary inequalities. ", "page_idx": 26}, {"type": "text", "text": "697 ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bullet\\ I f\\theta\\in(0,1),\\,t h e n\\sum_{k=1}^{t}k^{-\\theta}\\leq t^{1-\\theta}/(1-\\theta);}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "698 ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bullet\\;I f\\theta=1,\\,t h e n\\,\\sum_{k=1}^{t}k^{-\\theta}\\leq\\log(e t);}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "699 ", "page_idx": 26}, {"type": "text", "text": "700 Proof of Lemma 5. We have known that $\\begin{array}{r}{F_{S^{(i)}}(\\mathbf{w})\\,=\\,\\frac{1}{n}\\left(f(\\mathbf{w};z_{i}^{\\prime})+\\sum_{j\\neq i}f(\\mathbf{w};z_{j})\\right)}\\end{array}$ . We denote   \n701 $\\hat{\\mathbf{w}}^{*}(S^{(i)})$ be the ERM of $F_{S^{(i)}}(\\mathbf{w})$ and $\\hat{\\mathbf{w}}_{S}^{*}$ be the ERM of $F_{S}(\\mathbf{w})$ . From Lemma 3, we know that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\forall z\\in\\mathcal{Z},\\quad\\left\\|\\nabla f(\\hat{\\mathbf{w}}^{*}(S^{(i)});z)-f(\\hat{\\mathbf{w}}^{*}(S);z)\\right\\|_{2}\\leq\\frac{4M\\gamma}{n\\mu}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "702Also, for ${\\bf w}_{t}$ , the convexity of $f$ and the strongly-convex property implies that its closest optima point   \n703 of $F_{S}$ is $\\hat{\\mathbf{w}}^{*}(S)$ (the global minimizer of $F_{S}$ is unique). Then, there holds that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{\\mu}{2}\\|\\mathbf{w}_{t}-\\hat{\\mathbf{w}}^{*}(S)\\|_{2}^{2}\\leq F_{S}(\\mathbf{w}_{t})-F_{S}(\\hat{\\mathbf{w}}^{*}(S))=\\epsilon_{o p t}(\\mathbf{w}_{t}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "704  Thus we have $\\begin{array}{r}{\\|\\mathbf{w}_{t}-\\hat{\\mathbf{w}}^{*}(S)\\|_{2}\\leq\\sqrt{\\frac{2\\epsilon_{o p t}(\\mathbf{w}_{t})}{\\mu}}}\\end{array}$ Asimilarlationholds bween $\\hat{\\mathbf{w}}^{*}(S^{(i)})$ and $\\mathbf{w}_{t}^{i}$   \n705 Combined with the Lipschitz property of $f$ we obtain that for $\\forall z\\in{\\mathcal{Z}}$ , there holds that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\nabla f(\\mathbf{w}_{t};z)-\\nabla f(\\mathbf{w}_{t}^{i};z)\\right\\|_{2}}\\\\ &{\\le\\left\\|\\nabla f(\\mathbf{w}_{t};z)-\\nabla f(\\hat{\\mathbf{w}}^{*}(S);z)\\right\\|_{2}+\\left\\|\\nabla f(\\hat{\\mathbf{w}}^{*}(S);z)-\\nabla f(\\hat{\\mathbf{w}}^{*}(S^{(i)});z)\\right\\|_{2}}\\\\ &{\\quad+\\left\\|\\nabla f(\\hat{\\mathbf{w}}^{*}(S^{(i)});z)-\\nabla f(\\mathbf{w}_{t}^{i};z)\\right\\|_{2}}\\\\ &{\\le\\gamma\\|\\mathbf{w}_{t}-\\hat{\\mathbf{w}}^{*}(S)\\|_{2}+\\displaystyle\\frac{4M\\gamma}{n\\mu}+\\gamma\\|\\hat{\\mathbf{w}}^{*}(S^{(i)})-\\mathbf{w}_{t}^{i}\\|_{2}}\\\\ &{\\le\\gamma\\sqrt{\\frac{2\\epsilon_{o p t}(\\mathbf{w}_{t})}{\\mu}}+\\displaystyle\\frac{4M\\gamma}{n\\mu}+\\gamma\\sqrt{\\frac{2\\epsilon_{o p t}(\\mathbf{w}_{t}^{i})}{\\mu}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "706 According to Lemma 15, for any dataset $S$ , the optimization error $\\epsilon_{o p t}(\\mathbf{w}_{t})$ is uniformly bounded by   \n707 the same upper bound. Therefore, we write $\\begin{array}{r}{\\left\\|\\nabla f(\\mathbf{w}_{t};z)-\\nabla f(\\mathbf{w}_{t}^{i};z)\\right\\|_{2}\\leq2\\gamma\\sqrt{\\frac{2\\epsilon_{o p t}(\\mathbf{w}_{t})}{\\mu}}+\\frac{4M\\gamma}{n\\mu}}\\end{array}$   \n708here. ", "page_idx": 27}, {"type": "text", "text": "709 The proof is complete. ", "page_idx": 27}, {"type": "text", "text": "710 Now We begin to prove Lemma 6. ", "page_idx": 27}, {"type": "text", "text": "711 Proof of Lemma 6. If $f$ is $L$ -Lipschitz and $\\gamma.$ -smooth and $F_{S}$ is $\\mu$ -strongly convex. According to   \n712 Lemma 1, we know that for all $\\mathbf{w}\\in\\mathcal{W}$ and any $\\delta\\in(0,1)$ , with probability at least $1-\\delta/2$ , when   \n713 $n>\\frac{16\\gamma^{2}\\log\\,\\frac{6}{\\delta}}{\\mu^{2}}$ ,we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\left(\\displaystyle\\sum_{t=1}^{T}\\eta_{t}\\right)^{-1}\\sum_{t=1}^{T}\\eta_{t}\\|\\nabla F(\\mathbf{w}_{t})\\|_{2}^{2}}\\\\ &{\\le\\!16\\left(\\displaystyle\\sum_{t=1}^{T}\\eta_{t}\\right)^{-1}\\sum_{t=1}^{T}\\eta_{t}\\|\\nabla F_{S}(\\mathbf{w}_{t})\\|_{2}^{2}+\\frac{4C^{2}L^{2}\\log^{2}\\frac{6}{\\delta}}{n^{2}}+\\frac{8C^{2}\\mathbb{E}_{Z}[\\|\\nabla f(\\mathbf{w}^{*};Z)\\|_{2}^{2}]\\log^{2}\\frac{6}{\\delta}}{n}}\\\\ &{\\quad+\\left(\\displaystyle\\sum_{t=1}^{T}\\eta_{t}\\right)^{-1}\\sum_{t=1}^{T}\\eta_{t}C^{2}e^{2}\\beta_{t}^{2}\\left[\\log_{2}n\\right]^{2}\\log^{2}\\left(3e/\\delta\\right)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "714  where $\\beta_{t}=\\left\\|\\nabla f(\\mathbf{w}_{t};z)-\\nabla f(\\mathbf{w}_{t}^{i};z)\\right\\|_{2}$ and $C$ is a positive constant. ", "page_idx": 27}, {"type": "text", "text": "715 From Lemma 5, we have $\\begin{array}{r}{\\left\\|\\nabla f(\\mathbf{w}_{t};z)-\\nabla f(\\mathbf{w}_{t}^{i};z)\\right\\|_{2}\\leq2\\gamma\\sqrt{\\frac{2\\epsilon_{o p t}(\\mathbf{w}_{t})}{\\mu}}+\\frac{4M\\gamma}{n\\mu}}\\end{array}$ thus ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta_{t}^{2}=\\left\\|\\nabla f(\\mathbf{w}_{t};z)-\\nabla f(\\mathbf{w}_{t}^{i};z)\\right\\|_{2}^{2}}\\\\ &{\\quad\\leq\\left(2\\gamma\\sqrt{\\frac{2\\epsilon_{o p t}\\left(\\mathbf{w}_{t}\\right)}{\\mu}}+\\frac{4M\\gamma}{n\\mu}\\right)^{2}}\\\\ &{\\quad\\leq\\frac{16\\gamma^{2}\\left(F_{S}\\left(\\mathbf{w}_{t}\\right)-F_{S}\\left(\\hat{\\mathbf{w}}^{*}\\left(S\\right)\\right)\\right)}{\\mu}+\\frac{32M^{2}\\gamma^{2}}{n^{2}\\mu^{2}}}\\\\ &{\\quad\\leq\\frac{8\\gamma^{2}\\left\\|\\nabla F_{S}\\left(\\mathbf{w}_{t}\\right)\\right\\|_{2}^{2}}{\\mu^{2}}+\\frac{32M^{2}\\gamma^{2}}{n^{2}\\mu^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "716 where the second inequality holds from Cauchy-Bunyakovsky-Schwarz inequality and the second   \n717 inequality satisfies because $F_{S}$ is $\\mu$ -stronglyconvex. ", "page_idx": 27}, {"type": "text", "text": "78 Pluging 41)it(40),withprobabilty a least 1 - /2, whenn>16 ,wehave ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left(\\displaystyle\\sum_{t=1}^{T}\\eta_{t}\\right)^{-1}\\displaystyle\\sum_{t=1}^{T}\\eta_{t}\\|\\nabla F(\\mathbf{w}_{t})\\|_{2}^{2}}\\\\ &{\\le\\left(16+\\frac{8\\gamma^{2}C^{2}e^{2}\\int\\log_{2}n\\|^{2}\\log^{2}\\left(6e/\\delta\\right)}{\\mu^{2}}\\right)\\left(\\displaystyle\\sum_{t=1}^{T}\\eta_{t}\\right)^{-1}\\displaystyle\\sum_{t=1}^{T}\\eta_{t}\\|\\nabla F_{S}(\\mathbf{w}_{t})\\|_{2}^{2}}\\\\ &{\\quad+\\,\\frac{4C^{2}L^{2}\\log^{2}\\frac{12}{\\delta}}{n^{2}}+\\frac{8C^{2}\\mathbb{E}_{Z}[\\|\\nabla f(\\mathbf{w}^{*};Z)\\|_{2}^{2}]\\log^{2}\\frac{12}{\\delta}}{n}+\\frac{32L^{2}\\gamma^{2}C^{2}e^{2}\\left[\\log_{2}n\\right]^{2}\\log^{2}\\left(6e/\\delta\\right)}{n^{2}\\mu^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "719When $\\eta_{t}=\\eta_{1}t^{-\\theta},\\theta\\in(0,1)$ With $\\begin{array}{r}{\\eta_{1}\\leq\\frac{1}{2\\beta}}\\end{array}$ and Assumption 1, according to Lemma 14 and Lemma   \n720 16, we obtain the following inequality with probability at least $1-\\delta/2$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left(\\sum_{t=1}^{T}\\eta_{t}\\right)^{-1}\\sum_{t=1}^{T}\\eta_{t}\\Vert\\nabla F_{S}(\\mathbf{w}_{t})\\Vert^{2}=\\left\\{O\\left(\\frac{\\log(1/\\delta)}{T^{-\\theta}}\\right),\\quad\\mathrm{if}\\ \\theta<1/2\\right.}\\\\ {O\\left(\\frac{\\log(1/\\delta)}{T^{-\\frac{1}{2}}}\\right),\\quad\\mathrm{if}\\ \\theta=1/2}\\\\ {O\\left(\\frac{\\log(1/\\delta)}{T^{\\theta-1}}\\right),\\quad\\mathrm{if}\\ \\theta>1/2.}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "721 On the other hand, when $f$ is nonegative and $\\gamma.$ -smooth, from Lemma 4.1 of Srebro et al. [2010], we   \n722have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|\\nabla f(\\mathbf{w}^{*};z)\\|_{2}^{2}\\leq4\\gamma f(\\mathbf{w}^{*};z),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "723 which implies that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{Z}[\\|\\nabla f(\\mathbf{w}^{*};Z)\\|_{2}^{2}]\\leq4\\gamma\\mathbb{E}_{Z}f(\\mathbf{w}^{*};Z)=4\\gamma F(\\mathbf{w}^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "724 Plugging (44), (43) into (42), with probability at least $1-\\delta$ , we derive that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\displaystyle\\sum_{t=1}^{T}\\eta_{t}\\right)^{-1}\\sum_{t=1}^{T}\\eta_{t}\\|\\nabla F(\\mathbf{w}_{t})\\|_{2}^{2}}\\\\ &{=\\left\\{O\\left(\\frac{\\log^{2}n\\log^{3}(1/\\delta)}{T^{-\\theta}}\\right)+O\\left(\\frac{\\log^{2}n\\log^{2}(1/\\delta)}{n^{2}}+\\frac{F(\\mathbf{w}^{*})\\log^{2}(1/\\delta)}{n}\\right),\\;\\;\\mathrm{if}\\;\\theta<1/2\\right.}\\\\ &{=\\left\\{O\\left(\\frac{\\log^{2}n\\log^{3}(1/\\delta)}{T^{-\\frac{1}{2}}}\\right)+O\\left(\\frac{\\log^{2}n\\log^{2}(1/\\delta)}{n^{2}}+\\frac{F(\\mathbf{w}^{*})\\log^{2}(1/\\delta)}{n}\\right),\\;\\;\\mathrm{if}\\;\\theta=1/2\\right.}\\\\ &{\\left.O\\left(\\frac{\\log^{2}n\\log^{3}(1/\\delta)}{T^{\\theta-1}}\\right)+O\\left(\\frac{\\log^{2}n\\log^{2}(1/\\delta)}{n^{2}}+\\frac{F(\\mathbf{w}^{*})\\log^{2}(1/\\delta)}{n}\\right),\\;\\;\\mathrm{if}\\;\\theta>1/2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "725 When $\\theta<1/2$ ,we set $T\\asymp n^{\\frac{2}{\\theta}}$ and assume $\\begin{array}{r}{F(\\mathbf{w}^{*})=O(\\frac{1}{n})}\\end{array}$ , then we obtain the following result   \n726  with probability at least $1-\\delta$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left(\\sum_{t=1}^{T}\\eta_{t}\\right)^{-1}\\sum_{t=1}^{T}\\eta_{t}\\|\\nabla F(\\mathbf{w}_{t})\\|_{2}^{2}=O\\left(\\frac{\\log^{2}n\\log^{3}(1/\\delta)}{n^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "727When $\\theta=1/2$ ,we set $T\\asymp n^{4}$ and assume $\\begin{array}{r}{F(\\mathbf{w}^{*})=O(\\frac{1}{n})}\\end{array}$ , then we obtain the following result with   \n728  probability at least $1-\\delta$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left(\\sum_{t=1}^{T}\\eta_{t}\\right)^{-1}\\sum_{t=1}^{T}\\eta_{t}\\|\\nabla F(\\mathbf{w}_{t})\\|_{2}^{2}=O\\left(\\frac{\\log^{2}n\\log^{3}(1/\\delta)}{n^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "729When $\\theta>1/2$ weset $T\\asymp n^{\\frac{2}{1-\\theta}}$ and assume $\\begin{array}{r}{F(\\mathbf{w}^{*})=O(\\frac{1}{n})}\\end{array}$ , then we obtain the following result   \n730 with probability at least $1-\\delta$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left(\\sum_{t=1}^{T}\\eta_{t}\\right)^{-1}\\sum_{t=1}^{T}\\eta_{t}\\|\\nabla F(\\mathbf{w}_{t})\\|_{2}^{2}=O\\left(\\frac{\\log^{2}n\\log^{3}(1/\\delta)}{n^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "731 The proof is complete. ", "page_idx": 28}, {"type": "text", "text": "733Proof of Theorem 7. Since $F$ is $\\mu$ -strongly convex, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\nF(\\mathbf{w})-F(\\mathbf{w}^{*})\\leq\\frac{\\|\\nabla F(\\mathbf{w})\\|_{2}^{2}}{2\\mu},\\quad\\forall\\mathbf{w}\\in\\mathcal{W}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "734  So to bound $F(\\mathbf{w}_{T+1})-F(\\mathbf{w}^{*})$ , we need to bound the term $\\lVert\\nabla F(\\mathbf{w}_{T+1})\\rVert_{2}^{2}$ . And there holds ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla F(\\mathbf{w}_{T+1})\\|_{2}^{2}=2\\,\\|\\nabla F(\\mathbf{w}_{T+1})-\\nabla F_{S}(\\mathbf{w}_{T+1})\\|^{2}+2\\|\\nabla F_{S}(\\mathbf{w}_{T+1})\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "735 From Lemma 1, if $f$ is $L$ -Lipschitz and $\\gamma$ -smooth and $F_{S}$ is $\\mu$ strongly convex, for all $\\mathbf{w}\\in\\mathcal{W}$ and   \n736 any $\\delta>0$ when $\\begin{array}{r}{n\\geq\\frac{16\\gamma^{2}\\log\\frac{6}{\\delta}}{\\mu^{2}}}\\end{array}$ with probability a east $1-\\delta/2$ there hods ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\nabla F(\\mathbf{w}_{T+1})-\\nabla F_{S}(\\mathbf{w}_{T+1})\\right\\|_{2}}\\\\ &{\\le\\|\\nabla F_{S}(\\mathbf{w}_{T+1})\\|_{2}+C\\left(\\sqrt{\\frac{2\\mathbb{E}_{Z}\\left[\\|\\nabla f(\\mathbf{w}^{*};Z)\\|_{2}^{2}\\right]\\log\\frac{12}{\\delta}}{n}}+\\frac{M\\log\\frac{12}{\\delta}}{n}+e\\beta\\left\\lceil\\log_{2}n\\right\\rceil\\log\\left(6e/\\delta\\right)\\right)}\\\\ &{\\le\\|\\nabla F_{S}(\\mathbf{w}_{T+1})\\|_{2}+C\\left(\\sqrt{\\frac{8\\gamma F(\\mathbf{w}^{*})\\log\\frac{12}{\\delta}}{n}}+\\frac{M\\log\\frac{12}{\\delta}}{n}+e\\beta\\left\\lceil\\log_{2}n\\right\\rceil\\log\\left(6e/\\delta\\right)\\right),}\\\\ &{\\le\\|\\nabla F_{S}(\\mathbf{w}_{T+1})\\|_{2}+\\underbrace{\\left(\\sqrt{\\frac{8\\gamma F(\\mathbf{w}^{*})\\log\\frac{12}{\\delta}}{n}}+\\frac{M\\log\\frac{12}{\\delta}}{n}+e\\beta\\left\\lceil\\log_{2}n\\right\\rceil\\log\\left(6e/\\delta\\right)\\right)}_{\\sim_{0},}\\quad\\ldots.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "737 where the last inequality follows from Lemma 4.1 of Srebro et al. [2010] when $f$ is nonegative and   \n738 $\\gamma$ -smooth (see (44) and $C$ is a positive constant. Then we can derive that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\,\\|\\nabla F(\\mathbf{w}_{T+1})-\\nabla F_{S}(\\mathbf{w}_{T+1})\\|_{2}^{2}}\\\\ &{\\le4\\|\\nabla F_{S}(\\mathbf{w}_{T+1})\\|_{2}^{2}+\\frac{32C^{2}\\gamma F(\\mathbf{w}^{*})\\log\\frac{12}{\\delta}}{n}+\\frac{4M^{2}C^{2}\\log^{2}\\frac{12}{\\delta}}{n^{2}}+4e^{2}\\beta_{T+1}^{2}\\left[\\log_{2}n\\right]^{2}\\log^{2}{(6e/\\delta)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "739 From Lemma 5, we have $\\begin{array}{r}{\\left\\|\\nabla f(\\mathbf{w}_{t};z)-\\nabla f(\\mathbf{w}_{t}^{i};z)\\right\\|_{2}\\leq2\\gamma\\sqrt{\\frac{2\\epsilon_{o p t}(\\mathbf{w}_{t})}{\\mu}}+\\frac{4M\\gamma}{n\\mu}}\\end{array}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta_{t}^{2}=\\left\\|\\nabla f(\\mathbf w_{t};z)-\\nabla f(\\mathbf w_{t}^{i};z)\\right\\|_{2}^{2}}\\\\ &{\\quad\\leq\\left(2\\gamma\\sqrt{\\frac{2\\epsilon_{o p t}\\left(\\mathbf w_{t}\\right)}{\\mu}}+\\frac{4M\\gamma}{n\\mu}\\right)^{2}}\\\\ &{\\quad\\leq\\frac{16\\gamma^{2}\\left(F_{S}\\left(\\mathbf w_{t}\\right)-F_{S}\\left(\\hat{\\mathbf w}^{*}\\left(S\\right)\\right)\\right)}{\\mu}+\\frac{32M^{2}\\gamma^{2}}{n^{2}\\mu^{2}}}\\\\ &{\\quad\\leq\\frac{8\\gamma^{2}\\left\\|\\nabla F_{S}\\left(\\mathbf w_{t}\\right)\\right\\|_{2}^{2}}{\\mu^{2}}+\\frac{32M^{2}\\gamma^{2}}{n^{2}\\mu^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "740 where the second inequality holds from Cauchy-Bunyakovsky-Schwarz inequality and the second   \n741 inequality satisfies because $F_{S}$ is $\\mu$ -stronglyconvex. ", "page_idx": 29}, {"type": "text", "text": "742 Plugging (49) into (48), with probability at least $1-\\delta/2$ , when , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\nabla F(\\mathbf{w}_{T+1})-\\nabla F_{S}(\\mathbf{w}_{T+1})\\|_{2}^{2}}\\\\ &{\\leq\\left(4+32e^{2}\\left[\\log_{2}n\\right]^{2}\\log^{2}\\left(6e/\\delta\\right)\\right)\\|\\nabla F_{S}(\\mathbf{w}_{T+1})\\|_{2}^{2}+\\frac{32C^{2}\\gamma F(\\mathbf{w}^{*})\\log\\frac{6}{\\delta}}{n}}\\\\ &{\\qquad+\\,\\frac{4L^{2}C^{2}\\log^{2}\\frac{12}{\\delta}}{n^{2}}+\\frac{128M^{2}\\gamma^{2}e^{2}\\left[\\log_{2}n\\right]^{2}\\log^{2}\\left(6e/\\delta\\right)}{n^{2}\\mu^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "743 According to the smoothness property of $F_{S}$ and Lemma 15, it can be derived that with propability at   \n744least $1-\\bar{\\delta}/2$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\|\\nabla F_{S}(\\mathbf{w}_{T+1})\\right\\|_{2}^{2}=O\\left(\\frac{\\log T\\log^{3}(1/\\delta)}{T}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "745  Substituting (51), (50) into (46), we derive that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\nabla F(\\mathbf w_{T+1})\\|_{2}^{2}}\\\\ &{=O\\left(\\frac{\\left[\\log_{2}n\\right]^{2}\\log T\\log^{5}(1/\\delta)}{T}\\right)+O\\left(\\frac{\\left[\\log_{2}n\\right]^{2}\\log^{2}(1/\\delta)}{n^{2}}+\\frac{F(\\mathbf w^{*})\\log(1/\\delta)}{n}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "746Further substituting (52) into (45) and choosing $T\\,\\asymp\\,n^{2}$ , we finally obtain that when $n$ ,With   \n747probability at least $1-\\delta$ ", "page_idx": 30}, {"type": "equation", "text": "$$\nF(\\mathbf{w}_{T+1})-F(\\mathbf{w}^{*})=O\\left(\\frac{\\log^{4}n\\log^{5}(1/\\delta)}{n^{2}}+\\frac{F(\\mathbf{w}^{*})\\log(1/\\delta)}{n}\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "748 ", "page_idx": 30}, {"type": "text", "text": "749 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "'50 1.Claims   \n'51 Question: Do the main claims made in the abstract and introduction accurately refect the   \n'52 paper's contributions and scope?   \n\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "766 2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have clearly stated the required assumptions for each theorem and lemma, and the conditions for the assumptions to hold are also stated in the main text. ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "798 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "799 Question: For each theoretical result, does the paper provide the full set of assumptions and B300 a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer:[Yes]   \nJustification: We have clearly stated the required assumptions for each theorem and lemma, and all proofs are provided in the appendix.   \n\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "315 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "316 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n317 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n318 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper focuses on learning theory. ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "853 5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "854 Question: Does the paper provide open accesto the data and code, with sufficient instruc   \n855 tions to faithfully reproduce the main experimental results, as described in supplemental   \n856 material?   \n857 Answer: [NA]   \n858 Justification: This paper focuses on learning theory and does not include experiments.   \n859 Guidelines:   \n860 \u00b7 The answer NA means that paper does not include experiments requiring code.   \n861 \u00b7 Please see the NeurlPS code and data submission guidelines (https://nips.cc/   \n862 public/guides/CodeSubmissionPolicy) for more details.   \n863 \u00b7 While we encourage the release of code and data, we understand that this might not be   \n864 possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not   \n865 including code, unles this is central to the contribution (e.g., for a new open-source   \n866 benchmark).   \n867 \u00b7 The instructions should contain the exact command and environment needed to run to   \n868 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n869 //nips. cc/public/guides/CodeSubmissionPolicy) for more details.   \n870 \u00b7The authors should provide instructions on data access and preparation, including how   \n871 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n872 \u00b0 The authors should provide scripts to reproduce all experimental results for the new   \n873 proposed method and baselines. If only a subset of experiments are reproducible, they   \n874 should state which ones are omitted from the script and why.   \n875 \u00b7\u00b0 At submission time, to preserve anonymity,the authors should release anonymized   \n876 versions (if applicable).   \n877 \u00b7 Providing as much information as possible in supplemental material (appended to the   \n878 paper) is recommended, but including URLs to data and code is permitted.   \n879 6. Experimental Setting/Details   \n880 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n881 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n882 results?   \n883 Answer: [NA]   \n884 Justification: This paper focuses on learning theory and does not include experiments.   \n885 Guidelines:   \n886 \u00b7 The answer NA means that the paper does not include experiments.   \n887 \u00b7The experimental setting should be presented in the core of the paper to a level of detail   \n888 that is necessary to appreciate the results and make sense of them.   \n889 \u00b7The full details can be provided either with the code, in appendix, or as supplemental   \n890 material.   \n891 7. Experiment Statistical Significance   \n892 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n893 information about the statistical significance of the experiments?   \n894 Answer: [NA]   \n895 Justification: This paper focuses on learming theory and does not include experiments.   \n896 Guidelines:   \n897 \u00b7 The answer NA means that the paper does not include experiments.   \n898 \u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, conf  \n899 denc ntervals, orstatistical signifcane tsts, at last frth exprimnts that supt   \n900 the main claims of the paper.   \n901 \u00b0 The factors of variability that the eror bars are capturing should be clearly stated (for   \n902 example, train/test split, initialization, random drawing of some parameter, or overall   \n903 run with given experimental conditions).   \n904 \u00b7 The method for calculating the error bars should be explained (closed form formula,   \n905 call to a library function, bootstrap, etc.)   \n906 \u00b7 The assumptions made should be given (e.g., Normally distributed erors).   \n907 \u00b7 It should be clear whether the error bar is the standard deviation or the standard error   \n908 of the mean.   \n909 \u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n910 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n911 of Normality of errors is not verified.   \n912 \u00b7 For asymmetric distributions, the authors should be careful not to show in tables or   \n913 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n914 error rates).   \n915 \u00b7If errr bars are reported in tables or plots, The athors should explain in the text how   \n916 they were calculated and reference the corresponding figures or tables in the text.   \n917 8. Experiments Compute Resources   \n918 Question: For each experiment, does the paper provide suffcient information on the com  \n919 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n920 the experiments?   \n921 Answer: [NA]   \n922 Justification: This paper focuses on learning theory and does not include experiments.   \n923 Guidelines:   \n924 \u00b7 The answer NA means that the paper does not include experiments.   \n925 \u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n926 or cloud provider, including relevant memory and storage.   \n927 \u00b7 The paper should provide the amount of compute required for each of the individual   \n928 experimental runs as well as estimate the total compute.   \n929 \u00b7 The paper should disclose whether the full research project required more compute   \n930 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n931 didn't make it into the paper).   \n932 9. Code Of Ethics   \n933 Question: Does the research conducted in the paper conform, in every respect, with the   \n934 NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines?   \n935 Answer: [Yes]   \n936 Justification: We have reviewed the NeurIPS Code of Ethics.   \n937 Guidelines:   \n938 \u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n939 \u00b7 f the authors answer No, they should explain the special circumstances that require a   \n940 deviation from the Code of Ethics.   \n941 \u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n942 eration due to laws or regulations in their jurisdiction).   \n943 10. Broader Impacts   \n944 Question: Does the paper discuss both potential positive societal impacts and negative   \n945 societal impacts of the work performed?   \n946 Answer: [NA]   \n947 Justification: This paper focuses on learming theory and there is no societal impact of the   \n948 work performed.   \n949 Guidelines:   \n950 \u00b7 The answer NA means that there is no societal impact of the work performed.   \n951 \u00b7 If the authors answer NA or No, they should explain why their work has no societal   \n952 impact or why the paper does not address societal impact.   \n953 \u00b7 Examples of negative societal impacts include potential malicious or unintended uses   \n954 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n955 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n956 groups), privacy considerations, and security considerations.   \n957 \u00b7 The conference expects that many papers will be foundational research and not tied   \n958 to particular applications, let alone deployments. However, if there is a direct path to   \n959 any negative applications, the authors should point it out. For example, it is legitimate   \n960 to point out that an improvement in the quality of generative models could be used to   \n961 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n962 that a generic algorithm for optimizing neural networks could enable people to train   \n963 models that generate Deepfakes faster.   \n964 \u00b7 The authors should consider possible harms that could arise when the technology is   \n965 being used as intended and functioning correctly, harms that could arise when the   \n966 technology is being used as intended but gives incorrect results, and harms following   \n967 from (intentional or unintentional) misuse of the technology.   \n968 \u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation   \n969 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n970 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n971 feedback over time, improving the efficiency and accessibility of ML).   \n972 11. Safeguards   \n973 Question: Does the paper describe safeguards that have been put in place for responsible   \n974 release of data ormdels that haveahigh risk formisuse eg, preraned language mode,   \n975 image generators, or scraped datasets)?   \n976 Answer: [NA]   \n977 Justification: This paper focuses on learning theory and poses no such risks.   \n978 Guidelines:   \n979 \u00b7 The answer NA means that the paper poses no such risks.   \n980 \u00b7 Released models that have a high risk for misuse or dual-use should be released with   \n981 necessary safeguards to allow for controlled use of the model, for example by requiring   \n982 that users adhere to usage guidelines or restrictions to access the model or implementing   \n983 safety filters.   \n984 \u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n985 should describe how they avoided releasing unsafe images.   \n986 We recognize that providing effective safeguards is challenging, and many papers o   \n987 not require this, but we encourage authors to take this into account and make a best   \n988 faith effort.   \n989 12. Licenses for existing assets   \n990 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n991 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n992 properly respected?   \n993 Answer: [NA]   \n994 Justification: This paper focuses on learning theory and does not use existing assets.   \n995 Guidelines:   \n996 \u00b7 The answer NA means that the paper does not use existing assets.   \n997 \u00b7 The authors should cite the original paper that produced the code package or dataset.   \n998 \u00b7 The authors should state which version of the asset is used and, if possible, include a   \n999 URL.   \n1000 \u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n1001 \u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of   \n1002 service of that source should be provided.   \n1003 \u00b7 If assets are released, the license, copyright information, and terms of use in the   \n1004 package should be provided. For popular datasets, paperswithcode . com/datasets   \n1005 has curated licenses for some datasets. Their licensing guide can help determine the   \n1006 license of a dataset.   \n1007 \u00b7 For existing datasets that are re-packaged, both the original license and the license of   \n1008 the derived asset (if it has changed) should be provided.   \n010 the asset's creators.   \n011 13. New Assets   \n012 Question: Are new assets introduced in the paper well documented and is the documentation   \n013 provided alongside the assets?   \n014 Answer: [NA]   \n015 Justification: The paper does not release new assets.   \n016 Guidelines:   \n017 \u00b7 The answer NA means that the paper does not release new assets.   \n018 \u00b7 Researchers should communicate the details of the dataset/code/model as part of their   \n019 submissions via structured templates. This includes details about training, license,   \n020 limitations, etc.   \n021 \u00b7 The paper should discuss whether and how consent was obtained from people whose   \n022 asset is used.   \n023 \u00b7 At submission time, remember to anonymize your assets (if applicable). You can either   \n024 create an anonymized URL or include an anonymized zip file.   \n025 14. Crowdsourcing and Research with Human Subjects   \n026 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n027 include the full text of instructions given to participants and screenshots, if applicable, as   \n028 well as details about compensation (if any)?   \n029 Answer: [NA]   \n030 Justification: This paper does not involve crowdsourcing nor research with human subjects.   \n031 Guidelines:   \n032 \u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with   \n033 human subjects.   \n034 \u00b7 Including this information in the supplemental material is fne, but if the main contribu  \n035 tion of the paper involves human subjects, then as much detail as possible should be   \n036 included in the main paper.   \n037 \u00b7 According to the NeurlPs Code of Ethics, workers involved in data collection, curation,   \n038 or other labor should be paid at least the minimum wage in the country of the data   \n039 collector.   \n040 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n041 Subjects   \n042 Question: Does the paper describe potential risks incurred by study participants, whether   \n043 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n044 approvals (or an equivalent approval/review based on the requirements of your country or   \n045 institution) were obtained?   \n046 Answer: [NA]   \n047 Justification: This paper does not involve crowdsourcing nor research with human subjects.   \n048 Guidelines:   \n049 \u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with   \n050 human subjects. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]