{"references": [{"fullname_first_author": "R. Bassily", "paper_title": "Stability of stochastic gradient descent on nonsmooth convex losses", "publication_date": "2020-12-01", "reason": "This paper provides foundational work on the stability of SGD for non-smooth convex losses, which is relevant to the current paper's analysis of SGD under various conditions."}, {"fullname_first_author": "O. Bousquet", "paper_title": "Sharper bounds for uniformly stable algorithms", "publication_date": "2020-06-01", "reason": "This paper provides sharper generalization bounds for uniformly stable algorithms, which serves as a starting point for the current paper's efforts to improve these bounds."}, {"fullname_first_author": "V. Feldman", "paper_title": "High probability generalization bounds for uniformly stable algorithms with nearly optimal rate", "publication_date": "2019-06-01", "reason": "This paper establishes high probability generalization bounds with nearly optimal rates for uniformly stable algorithms, directly influencing the current paper's approach."}, {"fullname_first_author": "Y. Klochkov", "paper_title": "Stability and deviation optimal risk bounds with convergence rate o(1/n)", "publication_date": "2021-12-01", "reason": "This paper provides the sharpest known high-probability excess risk bounds using algorithmic stability, which the current paper aims to improve upon."}, {"fullname_first_author": "L. Zhang", "paper_title": "Empirical risk minimization for stochastic convex optimization: O(1/n)-and o(1/n2)-type of risk bounds", "publication_date": "2017-06-01", "reason": "This paper derives O(1/n2) excess risk bounds for ERM under certain conditions, which motivates the current paper's investigation into achieving similar rates through algorithmic stability."}]}