[{"figure_path": "LtS7pP8rEn/figures/figures_1_1.jpg", "caption": "Figure 1: Our sensing framework (PiMForce) leverages 3D hand posture information along with sEMG data to enable a whole-hand pressure estimation during various hand-object interactions. We support real-time pressure estimation on the fingertips and palm regions based on RGB image and sEMG inputs. The intensity of each node's color indicates the pressure level.", "description": "This figure illustrates the PiMForce framework, showing the data acquisition and model training/testing phases.  During training, a pressure glove, sEMG armband, and hand tracking module provide multimodal data to a deep neural network. This network learns to estimate hand pressure from the combined input.  During testing, a camera and sEMG armband provide inputs to the trained model, allowing real-time pressure estimation on fingertips and palm regions without the pressure glove.", "section": "1 Introduction"}, {"figure_path": "LtS7pP8rEn/figures/figures_4_1.jpg", "caption": "Figure 2: Our multimodal hand pressure estimation architecture enhances sEMG data by embedding 3D hand pose information. We train the model using a classification-regression joint loss to improve hand pressure estimation.", "description": "This figure illustrates the architecture of the PiMForce model, which estimates hand pressure using both sEMG and 3D hand pose data.  The model consists of two branches: one for processing sEMG signals (using STFT, CNN encoder-decoder, and an FC layer) and another for processing 3D hand pose data (using a ResNet and FC layer).  The outputs from these branches are concatenated and fed into further FC layers to predict both the classification of pressure (presence/absence) and the regression of pressure value in different hand regions.  The training process uses a joint loss function combining classification and regression losses to optimize the model for accurate and comprehensive hand pressure estimation.", "section": "4 Method"}, {"figure_path": "LtS7pP8rEn/figures/figures_8_1.jpg", "caption": "Figure 3: Quantitative evaluation of the user-independent model, showing the posture-wise performance on the estimation of hand pressure. The error bars indicate standard error.", "description": "This figure presents a quantitative analysis of the model's performance across different hand postures.  It uses three key metrics: Coefficient of Determination (R2), Normalized Root Mean Squared Error (NRMSE), and Accuracy. The x-axis shows various hand postures (including different types of presses and grasps), and the y-axis displays the values for each metric.  Error bars illustrate the standard error for each data point, indicating the variability in performance. The overall trend shows higher accuracy and lower NRMSE for simpler actions (like presses), while more complex grasps exhibit slightly lower accuracy.", "section": "5.2 Results"}, {"figure_path": "LtS7pP8rEn/figures/figures_9_1.jpg", "caption": "Figure 4: (a) Qualitative results in the absence of a pressure glove. The 3D Hand Pose Estimation [69] represents 3D hand posture, including hand occlusion, using the 3D hand detector. The Pressure-Vision++ [28] column shows the pressure estimation of fingertips. The red rectangles indicate the instances of pressure estimation failure due to hand occlusion. The proposed multimodal framework shows robust whole-hand pressure estimation for diverse hand-object interactions. (b) Illustration of the demo video footage showing robust hand pressure estimation with varying hand postures, pressure levels, and interacting objects.", "description": "This figure presents a qualitative comparison between the proposed PiMForce model and the PressureVision++ model for hand pressure estimation.  Subfigure (a) shows a table comparing the results from both methods across various hand interactions. The PressureVision++ method struggles with hand occlusions, while the PiMForce model consistently predicts pressures accurately. Subfigure (b) shows a video demonstrating the real-time performance of PiMForce, handling various poses, pressures, and objects, showcasing its robustness.", "section": "Experiments"}, {"figure_path": "LtS7pP8rEn/figures/figures_15_1.jpg", "caption": "Figure 5: We added position tracking sensors at the knuckles and 5 fingertip pressure sensors. Our glove records occlusion-free hand-tracking data with exerted hand pressures at 9 regions (5 fingertips and 4 palm regions).", "description": "The figure shows a customized pressure glove used for data collection.  The glove incorporates a position tracking module (Manus Quantum Mocap Metaglove) and pressure sensors on fingertips and palm. The pressure sensors cover nine regions of the hand: five fingertips and four sections of the palm. This integrated system allows simultaneous collection of accurate hand pose information and hand pressure data.", "section": "B.1 Data Collection Hardware"}, {"figure_path": "LtS7pP8rEn/figures/figures_16_1.jpg", "caption": "Figure 6: FSR sensor calibration and response curves. (a) We calibrated each FSR pressure sensor with the calibration setup using the precise push-pull gauge. (b) & (c) The FSR's resistance and conductance characteristics as a function of applied force, respectively, illustrating the sensor's calibration curve derived from multiple trials for data collection.", "description": "This figure shows the calibration and response curves for the Force-Sensing Resistor (FSR) pressure sensor used in the pressure glove.  (a) shows the experimental setup for calibration. (b) and (c) display the sensor's resistance and conductance as functions of the applied force.  Multiple trials are shown, illustrating the sensor's repeatability and stability.", "section": "B.1 Data Collection Hardware"}, {"figure_path": "LtS7pP8rEn/figures/figures_17_1.jpg", "caption": "Figure 7: (a) Data collection setup to capture time-synchronized hand pressure, 3D hand pose, and sEMG data. We asked participants to interact with 22 action sets while wearing customized pressure gloves integrated with a finger tracking module and an 8-channel EMG armband. (b) We captured 9-nodes hand pressure values, 8-channel sEMG signals, 20 joint angles, and 21 joint coordinates (computed from joint angles).", "description": "This figure shows the experimental setup for capturing multimodal data (hand pressure, 3D hand pose, and sEMG signals).  It highlights the key components: a customized pressure glove with embedded sensors, an 8-channel sEMG armband, and a markerless finger-tracking module.  The image also depicts a graphical representation of the collected data streams showing how hand pressure is distributed across nine different regions of the hand, the sEMG signals across 8 channels, and the 3D hand pose data which includes 20 joint angles and 21 joint coordinates.", "section": "3 Building Multimodal Dataset: Posture, Electromyography, and Pressure"}, {"figure_path": "LtS7pP8rEn/figures/figures_17_2.jpg", "caption": "Figure 8: 22 actions executed by the hand while collecting multimodal sensing data. It includes 7 hand-plane interactions, 5 pinch actions, and 10 hand-object interactions selected from hand grasp taxonomy. Capital letters before the hyphen, namely T, I, M, R, and P, stand for thumb, index finger, middle finger, ring finger, and pinky finger, respectively.", "description": "This figure shows the 22 different hand postures used in the multimodal dataset collection.  These postures are categorized into three types: Plane, Pinch, and Grasp.  Each category contains several variations of hand interactions, with clear visual representations for each. The labeling system in the figure also includes abbreviations to clearly identify each posture.", "section": "B.3 Data Collection Protocol"}, {"figure_path": "LtS7pP8rEn/figures/figures_18_1.jpg", "caption": "Figure 9: Representative grasp postures selected for data collection. This illustrates the ten grasp postures chosen for the study, reflecting a wide range of hand interactions. The postures are categorized by opposition type, virtual finger usage, grip type, and thumb position. The color-coded diagrams above the images indicate the pressure points for each grasp, corresponding to the regions of the hand engaged during the posture.", "description": "This figure shows ten representative grasp postures used in the data collection process.  Each grasp is categorized by its type (power, intermediate, precision), and thumb position.  The color-coding on the hand diagrams indicates which pressure sensors were used in each grasp.", "section": "B.2 Defined Hand Postures"}, {"figure_path": "LtS7pP8rEn/figures/figures_19_1.jpg", "caption": "Figure 10: Data collection interface displaying real-time feedback for hand posture, pressure, and EMG signals, along with a timer for data acquisition sessions.", "description": "The figure shows a computer screen displaying real-time data during a hand pressure data collection session.  The screen is divided into sections showing the hand posture (video and a 3D hand model), the pressure readings from the pressure glove sensors (represented as colored intensity on a diagram of a hand), and the EMG signals from the armband (as waveforms). A timer shows the elapsed time of the recording session. This interface provides visual feedback to the participant and the researcher during data collection, ensuring synchronized and accurate data acquisition.", "section": "3 Building Multimodal Dataset: Posture, Electromyography, and Pressure"}, {"figure_path": "LtS7pP8rEn/figures/figures_19_2.jpg", "caption": "Figure 11: Hand skeleton model.", "description": "This figure shows a diagram of a hand skeleton model used in the paper for representing hand poses.  The image displays the different joints of the fingers and thumb, including the carpometacarpal (CMC) joint, metacarpophalangeal (MCP) joint, proximal interphalangeal (PIP) joint, and distal interphalangeal (DIP) joint.  This model is important because it helps the researchers' 3D hand pose feature extractor fhand process the 3D joint angle information for the model to estimate pressure.", "section": "B.4.2 3D Hand Pose"}, {"figure_path": "LtS7pP8rEn/figures/figures_21_1.jpg", "caption": "Figure 12: Visualization of patterns with similar EMG footprint on different postures.", "description": "This figure visualizes the sEMG signal patterns obtained from eight channels during four different hand postures: I-Press, M-Press, TI-Pinch, and TM-Pinch.  The purpose is to demonstrate that similar sEMG patterns can be observed for different hand actions, highlighting the importance of incorporating 3D hand posture information to enhance the accuracy of hand pressure estimation.  Despite different hand actions that involve different fingers, there is significant similarity in EMG patterns, indicating that sEMG alone may not be sufficient for accurate and precise hand pressure estimation.", "section": "C.1 Empirical Motivation of Our Framework"}, {"figure_path": "LtS7pP8rEn/figures/figures_25_1.jpg", "caption": "Figure 13: Visualization of ground truth pressure and predicted pressure for the same posture using the existing PressureVision++ hand pressure prediction framework. (a) Original image from camera. (b) Input image for the PressureVision++ model. (c) Overlaied predicted pressure by PressureVision++. (d) Original predicted pressure. (e) Overlaied predicted pressure by PressureVision++, projected onto Sensel pressure array. (f) Ground truth pressure, projected onto Sensel pressure array.", "description": "This figure compares the ground truth pressure values with the pressure values predicted by the PressureVision++ model for the same posture. The figure shows that the PressureVision++ model struggles to accurately predict pressure in some scenarios.", "section": "D.4 Additional Qualitative Results"}, {"figure_path": "LtS7pP8rEn/figures/figures_26_1.jpg", "caption": "Figure 10: Data collection interface displaying real-time feedback for hand posture, pressure, and EMG signals, along with a timer for data acquisition sessions.", "description": "The figure shows a screenshot of the data collection interface used in the study.  It provides real-time feedback to the participant regarding their hand posture, the pressure exerted by their hand (captured by the pressure glove), and the electromyography (EMG) signals (captured by the EMG armband). The interface also features a timer to help track data acquisition sessions.", "section": "3 Building Multimodal Dataset: Posture, Electromyography, and Pressure"}, {"figure_path": "LtS7pP8rEn/figures/figures_27_1.jpg", "caption": "Figure 15: Qualitative visual analysis among comparative models.", "description": "This figure provides a qualitative comparison of hand pressure estimation results from five different methods: SEMG Only, 3D Hand Posture Only, SEMG + Hand Angles, SEMG + 3D Hand Posture (the proposed PiMForce method), and Ground Truth.  The image shows several different hand-object interaction types (I-Press, M-Press, R-Press, P-Press, IM-Press, and MR-Press). For each interaction type, the estimated pressure at different fingertip and palm locations is visualized with a color map for each of the five methods. The intensity of the color represents the pressure level, with darker shades indicating higher pressure. The rendered images of the \"Ours\" (PiMForce) column show the 3D hand pose overlay, illustrating the system's ability to estimate hand pressure accurately and robustly across varied hand poses and interactions. The ground truth is shown as a reference point.", "section": "D.4 Additional Qualitative Results"}, {"figure_path": "LtS7pP8rEn/figures/figures_28_1.jpg", "caption": "Figure 16: Qualitative visual analysis among comparative models.", "description": "This figure provides a visual comparison of hand pressure estimation results obtained using different methods: SEMG Only, 3D Hand Posture Only, SEMG + Hand Angles, SEMG + 3D Hand Posture, and the proposed PiMForce model.  Each row represents a different hand-object interaction type, displaying the predicted pressure distribution across the hand's fingertips and palm region using each method.  The intensity of the color in each node represents the pressure exerted in the corresponding region.  The \"Ground Truth\" column depicts the actual hand pressure distribution measured by a pressure-sensitive glove, providing a benchmark for evaluating the accuracy of the other methods.  The \"Ours (Rendered)\" column shows the hand pressure estimation from the PiMForce model.", "section": "Additional Qualitative Results"}, {"figure_path": "LtS7pP8rEn/figures/figures_29_1.jpg", "caption": "Figure 19: Pressure estimation results for the vision-aided hand in the test set.", "description": "This figure shows a qualitative comparison of pressure estimation results between the proposed PiMForce model and the existing PressureVision++ model for the vision-aided hand on a test set. The comparison includes the ground truth pressure and the pressure estimation for each model (rendered). The hand-object interaction types vary, and for some of them the PressureVision++ model is \"Undetectable\". This visualization demonstrates the performance and robustness of PiMForce for various hand postures and hand-object interactions.", "section": "D.4 Additional Qualitative Results"}, {"figure_path": "LtS7pP8rEn/figures/figures_30_1.jpg", "caption": "Figure 18: Qualitative visual analysis among comparative models.", "description": "This figure provides a qualitative comparison of hand pressure estimation results across various comparative models for different hand-object interaction types.  It visually demonstrates the performance of the SEMG Only, 3D Hand Posture Only, SEMG + Hand Angles, and SEMG + 3D Hand Posture models against the ground truth using a rendered visualization.  Each row represents a unique hand-object interaction, while the columns depict each method's estimated hand pressure distribution.  The color intensity represents pressure level, allowing for a visual comparison of the accuracy and effectiveness of each approach.", "section": "D.4 Additional Qualitative Results"}, {"figure_path": "LtS7pP8rEn/figures/figures_31_1.jpg", "caption": "Figure 19: Pressure estimation results for the vision-aided hand in the test set.", "description": "This figure shows a qualitative comparison of pressure estimation results between the proposed method (PiMForce) and PressureVision++ for various hand-object interaction types in the test set, where hand pose is estimated from vision. The intensity of each node's color indicates the pressure level, and the results demonstrate the effectiveness of PiMForce compared to PressureVision++, particularly in complex interactions where occlusion is present.", "section": "5 Experiments"}, {"figure_path": "LtS7pP8rEn/figures/figures_32_1.jpg", "caption": "Figure 4: (a) Qualitative results in the absence of a pressure glove. The 3D Hand Pose Estimation [69] represents 3D hand posture, including hand occlusion, using the 3D hand detector. The Pressure-Vision++ [28] column shows the pressure estimation of fingertips. The red rectangles indicate the instances of pressure estimation failure due to hand occlusion. The proposed multimodal framework shows robust whole-hand pressure estimation for diverse hand-object interactions. (b) Illustration of the demo video footage showing robust hand pressure estimation with varying hand postures, pressure levels, and interacting objects.", "description": "This figure demonstrates the qualitative results of the proposed PiMForce model in the absence of a pressure glove.  It compares the performance of PiMForce to a vision-only approach (PressureVision++) for various hand postures and object interactions. The top panel shows that PressureVision++ fails when the hand is occluded, while PiMForce performs well regardless of occlusion. The bottom panel displays images from a video showcasing the robustness and accuracy of PiMForce for various pressure levels and interactions.", "section": "Experiments"}, {"figure_path": "LtS7pP8rEn/figures/figures_33_1.jpg", "caption": "Figure 4: (a) Qualitative results in the absence of a pressure glove. The 3D Hand Pose Estimation [69] represents 3D hand posture, including hand occlusion, using the 3D hand detector. The Pressure-Vision++ [28] column shows the pressure estimation of fingertips. The red rectangles indicate the instances of pressure estimation failure due to hand occlusion. The proposed multimodal framework shows robust whole-hand pressure estimation for diverse hand-object interactions. (b) Illustration of the demo video footage showing robust hand pressure estimation with varying hand postures, pressure levels, and interacting objects.", "description": "This figure demonstrates the qualitative results of the proposed PiMForce framework for hand pressure estimation, both with and without a pressure glove.  (a) shows a comparison across different methods (PiMForce, PressureVision++, and baselines) for various hand-object interactions, highlighting PiMForce's ability to handle occlusions. (b) points to a supplementary video demonstrating robust real-time estimation across diverse scenarios.", "section": "5 Experiments"}, {"figure_path": "LtS7pP8rEn/figures/figures_34_1.jpg", "caption": "Figure 4: (a) Qualitative results in the absence of a pressure glove. The 3D Hand Pose Estimation [69] represents 3D hand posture, including hand occlusion, using the 3D hand detector. The Pressure-Vision++ [28] column shows the pressure estimation of fingertips. The red rectangles indicate the instances of pressure estimation failure due to hand occlusion. The proposed multimodal framework shows robust whole-hand pressure estimation for diverse hand-object interactions. (b) Illustration of the demo video footage showing robust hand pressure estimation with varying hand postures, pressure levels, and interacting objects.", "description": "This figure shows a qualitative comparison of hand pressure estimation results between the proposed method (PiMForce) and PressureVision++, a vision-based method. (a) shows that PiMForce accurately estimates the pressure across the whole hand, even in cases with hand occlusion. In contrast, PressureVision++ fails to estimate pressure in the presence of occlusion. (b) displays a video showing robust pressure estimation by PiMForce across different hand postures and objects.", "section": "Experiments"}, {"figure_path": "LtS7pP8rEn/figures/figures_35_1.jpg", "caption": "Figure 2: Our multimodal hand pressure estimation architecture enhances sEMG data by embedding 3D hand pose information. We train the model using a classification-regression joint loss to improve hand pressure estimation.", "description": "The figure illustrates the architecture of the PiMForce model, highlighting the integration of sEMG and 3D hand pose data to improve hand pressure estimation. It depicts the model's training process, utilizing a combination of classification and regression losses, and its inference process to provide real-time pressure readings from RGB images and sEMG input, thereby addressing limitations of traditional sEMG or vision-based methods.", "section": "4 Method"}]