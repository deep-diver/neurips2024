[{"type": "text", "text": "Differentiable Quantum Computing for Large-scale Linear Control ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Connor Clayton $^{*,1,2}$ Jiaqi Leng\u2217,1,3,5 Gengzhi Yang\u2217,1,3 ", "page_idx": 0}, {"type": "text", "text": "Yi-Ling Qiao $^{2,4}$ Ming C. Lin $^{2,4}$ Xiaodi Wu\u2020,1,2 ", "page_idx": 0}, {"type": "text", "text": "$^{\\mathrm{~1~}}{}$ Joint Center for Quantum Information and Computer Science, University of Maryland $^2$ Department of Computer Science, University of Maryland $^3$ Department of Mathematics, University of Maryland $^4$ Center for Machine Learning, University of Maryland $^5$ Department of Mathematics and Simons Institute for the Theory of Computing, UC Berkeley \u2217Equal Contribution \u2020xiaodiwu@umd.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "As industrial models and designs grow increasingly complex, the demand for optimal control of large-scale dynamical systems has significantly increased. However, traditional methods for optimal control incur significant overhead as problem dimensions grow. In this paper, we introduce an end-to-end quantum algorithm for linear-quadratic control with provable speedups. Our algorithm, based on a policy gradient method, incorporates a novel quantum subroutine for solving the matrix Lyapunov equation. Specifically, we build a quantum-assisted differentiable simulator for efficient gradient estimation that is more accurate and robust than classical methods relying on stochastic approximation. Compared to the classical approaches, our method achieves a super-quadratic speedup. To the best of our knowledge, this is the first end-to-end quantum application to linear control problems with provable quantum advantage. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Over the past few decades, the growing complexity of modern engineering designs has made the control of large-scale dynamical systems a crucial task across various application fields, such as power grid management [56], swarm robotics [16, 18], sensor networks [21], and airline scheduling [57]. These challenges often involve high-dimensional solution spaces with tens of thousands of degrees of freedom, presenting a significant obstacle for traditional optimal control methods. ", "page_idx": 0}, {"type": "text", "text": "The emergence of quantum computing has expanded the potential for designing efficient algorithms in numerical optimization and machine learning [1, 36, 62]. By leveraging the principles of quantum mechanics, such as superposition and entanglement, quantum computers excel at efficient data processing, making them promising for accelerating solutions to large-scale computational challenges [31]. ", "page_idx": 0}, {"type": "text", "text": "Although there has been some progress in quantum algorithms for some specific optimal control problems arising in quantum sciences [37, 39], a viable pathway for accelerating general large-scale optimal control problems remains unclear. A conventional approach to optimal control involves solving the Algebraic Riccati Equation (ARE, see Section 1.1 for details), which is a nonlinear matrix equation. This problem has been less explored in the field of quantum computing for two reasons. First, most proposed quantum algorithms for algebraic and differential equations focus on linear and vector-valued problems, and extending them to nonlinear matrix equations is highly challenging. Second, while efficient quantum algorithms exist for certain weakly nonlinear problems [41], they are not powerful enough to handle the nonlinearity present in the Algebraic Riccati Equation. A breakthrough in this direction calls for novel ideas in algorithm design. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Inspired by the recent advances in differentiable physics [37, 46, 53] and reinforcement learning [19, 45], we develop an end-to-end quantum algorithm that solves a fundamental optimal control problem called the linear-quadratic regulator (LQR). Given its widely applicable mathematical formulation, LQR has been extensively researched and serves as a standard case study for various computing and learning algorithms [25]; moreover, LQR is of significant practical relevance as many real-world optimal control problems can be formulated to address through linearization techniques. Our quantum algorithm is proven to output an $\\varepsilon$ -approximate optimal solution in time $\\widetilde{\\mathcal{O}}\\left(n\\varepsilon^{-1.5}\\right)^{1}$ , where $n$ is the dimension of the state vector and $\\varepsilon$ is an error tolerance parameter. The algorithm involves two major components: a quantum differentiable simulator and a quantum-accessible classical data structure. This hybrid quantum-classical framework enables us to employ a policy gradient method that exhibits a fast convergence rate for the LQR problem. Since almost all known classical methods for the LQR problem heavily rely on subroutines such as matrix factorization and matrix inversion [6, 32, 34, 35], which require at least $\\mathcal{O}(n^{3})$ overhead in the problem dimension $n$ , our new linear-time quantum algorithm, with super-quadratic speedup, offers significant promise for large-scale applications. ", "page_idx": 1}, {"type": "text", "text": "Notation. We use $\\mathbb{R}$ and $\\mathbb{C}$ to denote the set of real and complex numbers, respectively. $\\parallel$ denotes an identity operator with an appropriate dimension. For two real vectors $u,v\\in$ $\\mathbb{R}^{n}$ , the Euclidean inner product $\\langle u,v\\rangle\\;=\\;u^{T}v$ , and the norm of a vector $u$ is $\\|u\\|\\;=$ $\\sqrt{u^{T}u}$ . Given a symmetric/Hermitian matrix $M$ , we denote $\\lambda_{\\operatorname*{max}}(M)$ (or $\\lambda_{\\operatorname*{min}}(M))$ as the maximal/minimal eigenvalue of $M$ . The spectral norm of a matrix $M\\,\\in\\,\\mathbb{R}^{m\\times n}$ is denoted by $\\|M\\|=\\operatorname*{sup}_{\\|v\\|=1}\\|M v\\|$ . The Frobenius norm of a matrix $M\\in\\mathbb{R}^{m\\times n}$ is denoted by $\\begin{array}{r}{\\|M\\|_{F}=\\sum_{i,j}|M_{i,j}|^{2}=\\mathrm{Tr}\\big[M^{T}M\\big]}\\end{array}$ . We say $\\xi\\sim\\mathcal{D}$ if the random variable $\\xi\\,\\in\\,\\mathbb{R}^{n}$ is distributed according to $\\mathcal{D}$ . ", "page_idx": 1}, {"type": "text", "text": "1.1 Problem Formulation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We focus on the infinite-horizon continuous-time linear-quadratic regulator (LQR) problem: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x,u}J=\\mathbb{E}\\left[\\int_{0}^{\\infty}\\left({x}^{\\top}(t)Q x(t)+{u}^{\\top}(t)R u(t)\\right)\\mathrm{d}t\\right]\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $x(t)\\colon[0,\\infty]\\to\\mathbb{R}^{n}$ is the state vector, $u(t)\\colon[0,\\infty]\\to\\mathbb{R}^{m}$ is the control input. $A$ and $B$ are constant matrices of appropriate dimensions; $Q$ and $R$ are positive definite matrices. ", "page_idx": 1}, {"type": "text", "text": "Definition 1. For a square matrix $M\\in\\mathbb{R}^{n\\times n}$ , we say $M$ is Hurwitz if every eigenvalue of $M$ has a strictly negative real part. ", "page_idx": 1}, {"type": "text", "text": "Definition 2. For a controllable pair $(A,B)$ , the set of stabilizing feedback gains is given by ", "page_idx": 1}, {"type": "equation", "text": "$$\nS_{K}:=\\{K\\in\\mathbb{R}^{m\\times n}\\colon A-B K{\\mathrm{~is~Hurwitz}}\\}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Given a controllable pair $(A,B)$ , the optimal controller $u(t)$ of problem (1) can be expressed as a linear function of the state vector $x(t)$ , namely ", "page_idx": 1}, {"type": "equation", "text": "$$\nu(t)=-K^{*}x(t),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where the matrix $K^{*}$ is the optimal linear feedback gain. An analytical form of the optimal feedback gain is given by $K^{*}=R^{-1}B^{\\top}P^{*}$ , where $P^{*}$ is the unique positive solution to the Algebraic Riccati Equation (ARE), ", "page_idx": 1}, {"type": "equation", "text": "$$\nA^{\\top}P+P A+Q-P B R^{-1}B^{\\top}P=0.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "For large-scale control problems where the control input is much smaller than the state vector (i.e., $m\\ll n$ ), it is often desired to compute the optimal feedback gain matrix $K^{*}$ without explicitly solving for $P^{*}$ . To this end, we can rewrite the LQR objective function $J(x,u)$ as a function solely depending on $K$ by leveraging the linearity of the optimal controller $u(t)=-K x(t)$ . With standard algebraic manipulation, it turns out that ", "page_idx": 2}, {"type": "equation", "text": "$$\nJ(x,u)=f(K)=\\left\\{\\!\\!\\begin{array}{l l}{\\mathrm{Tr}[P(K)\\Sigma_{0}]}&{K\\in S_{K},\\ }\\\\ {\\infty}&{\\mathrm{otherwise},}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ", "page_idx": 2}, {"type": "equation", "text": "$$\nP(K)=\\int_{0}^{\\infty}e^{(A-B K)^{\\top}t}\\left(Q+K^{\\top}R K\\right)e^{(A-B K)t}\\;\\mathrm{d}t,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and $\\Sigma_{0}:=\\mathbb{E}_{\\xi\\sim\\mathcal{D}}[\\xi\\xi^{\\top}]$ . Given this reformulation, the search for the optimal feedback gain $K^{*}$ reduces to minimizing the unconstrained objective function $f(K)$ . ", "page_idx": 2}, {"type": "text", "text": "In practice, the matrices $A,B,Q,R$ often possess sparsity structures that can be leveraged by quantum computers. We make the assumptions on the efficient quantum access model. ", "page_idx": 2}, {"type": "text", "text": "Assumption 1 (Sparse-access matrices). We assume $A$ , $B$ , $Q$ , and $R$ are $s$ -sparse, i.e., there are at most $s$ non-zero entries in each row/column. For $M\\in\\{A,B,Q,R\\}$ , we assume access to an efficient procedure2 that loads the matrix into quantum data: ", "page_idx": 2}, {"type": "equation", "text": "$$\n|i\\rangle|k\\rangle\\mapsto|i\\rangle|r_{i,k}\\rangle,\\quad|\\ell\\rangle|j\\rangle\\mapsto|c_{\\ell,j}\\rangle|j\\rangle,\\quad|i\\rangle|j\\rangle|0\\rangle\\mapsto|i\\rangle|j\\rangle|M_{i j}\\rangle,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where is the index of the $k$ -th non-zero entry of the $i$ -th row of $M$ , is the index of $\\boldsymbol{r}_{i,k}$ $c\\ell,j$ the $\\ell$ -th non-zero entry of the $j$ -th column of $M$ , and $M_{i j}$ is a fixed-length binary description of the $(i,j)$ -th entry of $M$ . ", "page_idx": 2}, {"type": "text", "text": "With quantum access to the problem data, we aim to determine the optimal linear feedback gain $K^{*}$ so that the objective function $f(K)$ is minimized, as summarized in the following problem statement: ", "page_idx": 2}, {"type": "text", "text": "Problem 1. Assume $(A,B)$ is a controllable pair and $Q$ , $R$ are positive-definite. Given quantum access to $A,B,Q,R$ in the sense of Assumption 1, we want to compute an $\\varepsilon$ - approximate solution $K$ such that $\\|K-K^{*}\\|_{F}\\le\\varepsilon$ , where $\\varepsilon>0$ is prefixed. ", "page_idx": 2}, {"type": "text", "text": "1.2 Main Contributions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this paper, we propose an end-to-end quantum algorithm for solving LQR problems that exhibits the desired quantum advantage in the large-scale setting (i.e., in the parameter regime $m\\ll n$ ). A detailed comparison between ours and various other methods is given in Table 1. Compared with state-of-the-art classical methods, our algorithm achieves a super-quadratic speedup in terms of the state vector dimension $n$ . To the best of our knowledge, this is the first end-to-end quantum application to linear control problems with provable speedup. ", "page_idx": 2}, {"type": "table", "img_path": "GHqw3xLAvd/tmp/f8432a836fa3ef57edfa76c3b22963790aaa3eee0e3a2ee630943ebc7cebe02f.jpg", "table_caption": [], "table_footnote": ["Table 1: Asymptotic cost of different methods for LQR. "], "page_idx": 2}, {"type": "text", "text": "Our algorithm is based on a novel policy gradient strategy to find globally optimal solutions for linear-quadratic control problems. A brief overview of the policy gradient method for LQR is available in Section 3.2. In each iteration cycle, our algorithm executes a fast, quantum-assisted differentiable simulator to obtain robust gradient estimates, as detailed in Theorem 31. The gradient estimates are then utilized by a classical computer to update the control policy $K$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "As illustrated in Figure 1, with the back-and-forth iterations between the quantum simulator and a classical computer, the control policy $K$ converges to the optimal policy $K^{*}$ at a linear rate, leading to an end-to-end resolution of the LQR problem. ", "page_idx": 3}, {"type": "text", "text": "Our quantum algorithm design can be regarded as a novel realization of the hybrid quantum-classical computing paradigm, where collaboration between classical and quantum computers significantly reduces the burden on the quantum side. Moreover, we provide explicit constructions of the quantum simulator and analyze the convergence rate of the policy gradient based on our end-to-end model. These desirable attributes make our proposed design more practical and relevant in the early fault-tolerant era [29]. ", "page_idx": 3}, {"type": "text", "text": "Notably, we propose a new quantum algorithm for solving the Lyapunov equation, a fundamental task in optimal control theory [25]. Based on an integral representation of the solution and a rich toolbox of methods for quantum numerical linear algebra, our algorithm can produce a quantum representation of the solution matrix in a cost that ", "page_idx": 3}, {"type": "image", "img_path": "GHqw3xLAvd/tmp/aea71beb0d62d3e55103b24a4f4d4d436ac319c87a9c5083431f3b0a6023edce.jpg", "img_caption": ["Figure 1: Differentiable quantum computing for linear control. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "is polylogarithmic in the matrix dimension $n$ (see Theorem 4), leading to an exponential speedup over existing classical methods [51]. Moreover, since the matrix Lyapunov equation is fundamental to many control problems, we foresee that our quantum algorithm may play an important role in finding speedups for other tasks. ", "page_idx": 3}, {"type": "text", "text": "The fast quantum algorithm for the Lyapunov equation enables us to develop a quantum gradient estimation subroutine in near-optimal cost, as detailed in Theorem 31. Compared with the conventional gradient estimation techniques based on stochastic approximation (e.g., one- and two-point gradient estimators [45]), our quantum gradient estimation benefits from the explicit exploitation of the analytical form of the gradient. Numerical experiments suggest that our gradient estimation is robust and often leads to faster convergence in practice, as demonstrated in Section 5.3. ", "page_idx": 3}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We survey related work on model-based and model-free linear-quadratic control, differentiable physics, and quantum reinforcement learning in this section. ", "page_idx": 3}, {"type": "text", "text": "Model-based linear-quadratic control. Model-based optimal control [17, 50] refers to the scenario where historical measurement data explicitly gives (or estimates) the problem description. In this case, the optimal linear feedback gain $K^{*}$ can be computed by solving the algebraic Riccati equation (ARE), as detailed in Section 1.1. Commonly used numerical methods for ARE include factorization methods (e.g., Schur method [35, 48]) and iterative methods (e.g., Newton-Kleinman method [20, 32, 47]). These methods require computing matrix factorization or matrix inverse, which in general leads to a $O(n^{3})$ run time (assuming $m\\leq n$ ). Some methods for ARE can achieve $\\mathcal{O}(n)$ runtime under strong assumptions such as the solution $P^{*}$ is of low rank [8]. It is also possible to solve LQR by reformulating it as a semidefinite programming (SDP) problem [14]. We do not dive into the SPD approach as it does not demonstrate superior asymptotic scaling compared to other direct methods. ", "page_idx": 3}, {"type": "text", "text": "Model-free linear-quadratic control. LQR can be regarded as a continuous-time analog of the discrete Markov Decision Process (MDP) model and many techniques from reinforcement learning (RL) can be introduced to learn the optimal feedback gain (i.e., control policy), such as policy gradient [19, 45], natural gradient [25], and policy iteration [10]. ", "page_idx": 3}, {"type": "text", "text": "These RL-based methods are particularly useful when we have access to the observed costs but the system model can not be directly constructed. ", "page_idx": 4}, {"type": "text", "text": "Differentiable physics and quantum computing. The differentiable programming paradigm has been applied to many dynamical systems for learning [46] and control [53]. Those gradient-based methods can be used in reinforcement learning [61], inverse problems [27], optimization [5], design [60], etc. People have developed differentiable pipelines for various dynamics including fluids [59], rigid body [49], soft body [26], and other hybrid systems [52]. Recently, [37] have derived a differentiable analog quantum computing pipeline for quantum optimization and control. In this work, we will focus on using differentiable quantum computing to accelerate a widely studied classical problem - linear control synthesis. ", "page_idx": 4}, {"type": "text", "text": "Quantum reinforcement learning. Recently, quantum-accelerated reinforcement learning has attracted significant attention as it demonstrates the potential for computational speedup [43]. It has been shown that quantum computers can be used to compute policy gradients given coherent access to a Markov Decision tree model [15, 28]. Some works also discuss the quantum policy iteration method for RL, see [12, 58]. It is worth noting that the existing works are usually based on a strong quantum access model and it remains unclear what the cost of constructing such models is in an end-to-end sense. ", "page_idx": 4}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.1 Introduction to Quantum Computing ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "All quantum states of a quantum system form a Hilbert space, which is isomorphic to $\\mathbb{C}^{N}$ . We may assume $N=2^{n}$ and $n$ is a non-negative integer. An element $|\\psi\\rangle$ in this Hilbert space is then noted as a $N$ -dimensional quantum state, where ", "page_idx": 4}, {"type": "equation", "text": "$$\n|\\psi\\rangle=\\left[\\begin{array}{c}{{v_{0}}}\\\\ {{v_{1}}}\\\\ {{\\vdots}}\\\\ {{v_{N-1}}}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $v_{i}\\in\\mathbb{C},i\\in\\{0,1,\\cdots,N-1\\}$ . Also, we often use $\\langle\\psi|$ to denote the conjugate transpose of $|\\psi\\rangle$ . For any $c\\neq0$ , $c|\\psi\\rangle$ and $|\\psi\\rangle$ refer to the same state, thus without loss of generality, $\\||\\psi\\rangle\\|=1$ always holds. Specifically, a one qubit system corresponds to the aforementioned Hilbert space with $n=1$ . ", "page_idx": 4}, {"type": "text", "text": "Given ${\\boldsymbol{r}}n$ quantum states $|\\psi_{1}\\rangle,|\\psi_{2}\\rangle,\\cdots,|\\psi_{m}\\rangle$ from $m$ quantum systems, then ", "page_idx": 4}, {"type": "equation", "text": "$$\n|\\psi\\rangle=|\\psi_{1}\\rangle\\otimes|\\psi_{2}\\rangle\\otimes\\cdots\\otimes|\\psi_{m}\\rangle\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "is a quantum state in the space that consists $m$ subspaces. ", "page_idx": 4}, {"type": "text", "text": "The evolution of a quantum state can be described by a unitary operator $U$ , meaning ", "page_idx": 4}, {"type": "equation", "text": "$$\nU^{\\dagger}U=U U^{\\dagger}=I.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We often note these operations as gates on the quantum circuit. One important type of unitary operators are the Pauli operators, namely ", "page_idx": 4}, {"type": "equation", "text": "$$\nX={\\left[\\!\\!\\begin{array}{l l}{0}&{1}\\\\ {1}&{0}\\end{array}\\!\\!\\right]}\\,,\\quad Y={\\left[\\!\\!\\begin{array}{l l}{0}&{-i}\\\\ {i}&{0}\\end{array}\\!\\!\\right]}\\,,\\quad Z={\\left[\\!\\!\\begin{array}{l l}{1}&{0}\\\\ {0}&{-1}\\end{array}\\!\\!\\right]}\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "They form a basis of all the linear operators acting on $\\mathbb{C}^{2}$ . ", "page_idx": 4}, {"type": "text", "text": "For the quantum measurement, given a quantum observable $H$ , we can do the measurement of a quantum state $|\\psi\\rangle$ . Specifically, after the measurement on $|\\psi\\rangle$ , the state collapses to $\\frac{P_{m}|\\psi\\rangle}{\\sqrt{p_{m}}}$ , and we get an outcome $\\lambda_{m}$ with probability $p_{m}=\\langle\\psi|P_{m}|\\psi\\rangle$ , where ", "page_idx": 4}, {"type": "equation", "text": "$$\nH=\\sum_{m}\\lambda_{m}P_{m}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "is the spectral decomposition of $H$ . ", "page_idx": 4}, {"type": "text", "text": "3.2 Policy Gradient for LQR ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "For all stabilizing feedback gains $K\\in S_{K}$ , the gradient of the objective function $f(K)$ as defined in (5) has the following closed-form expression [38, 40]: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla f(K)=2(R K-B^{\\top}P(K))X(K),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $P(K)$ is given in (6), and $X(K)$ is determined by ", "page_idx": 5}, {"type": "equation", "text": "$$\nX(K)=\\int_{0}^{\\infty}e^{(A-B K)t}\\Sigma_{0}e^{(A-B K)^{\\top}t}\\ \\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The (direct) policy gradient method for LQR minimizes the objective $f(K)$ via the vanilla gradient update rule $K\\leftarrow K-s\\nabla f(K)$ , where $s>0$ is a fixed step size. Given sufficiently small $\\boldsymbol{s}$ , it has been shown that the policy gradient method converges at a linear rate [45, Theorem 2]. In practice, however, the policy gradient is often estimated through stochastic approximation, such as one- and two-point estimation [45]. While these zeroth-order gradient estimation methods are less demanding in terms of computational cost, they tend to be sensitive to random perturbations and slow to converge. ", "page_idx": 5}, {"type": "text", "text": "In this paper, we propose a fast quantum algorithm that outputs a robust estimate of the gradient in $\\tilde{\\mathcal{O}}(n)$ time (assuming $m\\ll n$ , see Theorem 31). Leveraging the quantum gradient estimation  subroutine, we recover the linear convergence rate using robust gradient descent, as detailed in Proposition 34. ", "page_idx": 5}, {"type": "text", "text": "3.3 Quantum Data Structure ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To perform policy gradient in the training process, the linear feedback gain $K$ is stored in a quantum-accessible data structure as proposed in [30]. This data structure allows intermediate updates on $K$ and efficient quantum queries to $K$ as a block-encoded matrix. This data structure is a purely classical representation of $K$ , and quantum access to this data structure (e.g., through qRAM [23]) is required to build the block-encoding of $K$ . In the literature, this data structure is also known as classical-write, quantum-read qRAM [7, 54]. ", "page_idx": 5}, {"type": "text", "text": "Definition 3 (Block-encoding). Suppose that $M$ is an $p$ -qubit operator, $\\alpha,\\varepsilon\\in\\mathbb{R}^{+}$ and $r\\in\\mathbb N$ , then we say that the $(p+r)$ -qubit unitary $U_{M}$ is an $(\\alpha,r,\\varepsilon)$ -block-encoding of $M$ , if ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\lVert M-\\alpha(\\langle0|^{r}\\otimes\\mathbb{I})U_{M}(\\lvert0\\rangle^{r}\\otimes\\mathbb{I})\\rVert\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In this paper, the growth of ancilla qubits (space complexity) is dominated by the number of elementary gates (gate complexity). Therefore, when referring to a specific block-encoding, we often omit the number of ancilla qubits (i.e., the parameter $r$ ) for simplicity. ", "page_idx": 5}, {"type": "text", "text": "Lemma 1. Let $K\\in\\mathbb{R}^{m\\times n}$ . There exists a data structure to store $K$ with the following properties: (1) the size of the data structure is ${\\mathcal{O}}\\left(m n\\log^{2}(m n)\\right)$ , (2) the time to store a new entry $(i,j,\\hat{K}_{i,j})$ is ${\\mathcal{O}}\\left(\\log^{2}(m n)\\right)$ , and (3) for any $\\varepsilon>0$ , a quantum algorithm can implement $a$ $(\\|K\\|_{F},\\lceil\\log_{2}n\\rceil+2,\\varepsilon)$ -block-encoding of $K$ in time $\\mathcal{O}\\left(\\mathrm{poly}\\log(n,1/\\varepsilon)\\right)$ . There also exists an analogous data structure for $\\hat{K}^{\\top}$ . ", "page_idx": 5}, {"type": "text", "text": "Proof. We use the data structure as described in [30, Theorem 5.1]. To construct the block-encoding, we utilize [22, Lemma 50]. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "3.4 Quantum Simulation of Linear Dynamics ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Quantum computers can simulate certain linear ordinary differential equations (ODEs) exponentially faster than classical computers [3, 4, 9, 22, 33]. In this paper, we present a quantum simulation subroutine based on quantum linear system solvers (QLSS) [9, 33], as described below. While this approach may not be optimal in terms of state preparation cost compared to quantum singular value transformation [22] or linear combination of Hamiltonian simulation [3, 4], it allows us to incorporate the Hurwitz stability of the system. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2 (Informal version of Theorem 17). Suppose that $A\\in\\mathbb{R}^{n\\times n}$ is a Hurwitz matrix, and $O_{A}$ is an $(\\alpha,0)$ -block-encoding of $\\mathcal{A}$ . For an arbitrary $t>0$ , we can implement $a$ $(\\zeta t,\\varepsilon)$ - block-encoding of $e^{\\mathcal{A}t}$ using $\\tilde{\\mathcal{O}}\\left(\\alpha\\rho t\\cdot\\mathrm{poly}\\log(1/\\varepsilon)\\right)$ queries to $O_{A}$ , and $\\tilde{\\mathcal{O}}\\left(\\alpha\\rho t\\cdot\\mathrm{poly}\\log(1/\\varepsilon)\\right)$ queries to additional gates. Here, the normalization factor $\\zeta=\\mathcal{O}(\\alpha\\rho)$ , and the constant $\\rho$ is solely determined by $\\mathcal{A}$ . ", "page_idx": 6}, {"type": "text", "text": "More details and the proof of Theorem 2 can be found in Appendix C. Note that the dependence on $t$ in the above result can be further improved using a standard padding technique, but for simplicity, we do not discuss this minor improvement, as it does not affect our main end-to-end result. We also notice a technique called quantum eigenvalue transformation (QEVT), recently proposed by Low and Su [42]. While this method cannot be directly applied to Hurwitz-stable systems, it may be enhanced to provide a simulation algorithm with a similar cost, as discussed in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "4 Quantum Algorithm for the Lyapunov Equation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The (continuous-time) Lyapunov equation is a linear matrix equation of the following form, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{A}\\boldsymbol{X}+\\boldsymbol{X}\\boldsymbol{\\mathcal{A}}^{\\top}+\\boldsymbol{\\Omega}=\\boldsymbol{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Since this equation is linear in terms of the matrix $X$ , it is possible to derive a vectorization form of (16) and solve it using a quantum linear system algorithm [11, 24]. In this paper, we propose a new quantum algorithm for solving the Lyapunov equation based on an integral representation formula. Our algorithm directly prepares a block-encoded solution matrix $X^{*}$ . Compared to the previous approach, our method leads to an exponentially faster quantum objective function evaluation algorithm (see Theorem 5) and a new quantum gradient estimation subroutine (see Theorem 6). ", "page_idx": 6}, {"type": "text", "text": "4.1 Representation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Given a positive-definite $Q$ , there exists a unique positive-definite $X^{*}$ satisfying (16) if and only if $\\mathcal{A}$ is Hurwitz. The unique positive solution is given by ", "page_idx": 6}, {"type": "equation", "text": "$$\nX^{*}=\\int_{0}^{\\infty}e^{\\mathcal{A}t}\\Omega e^{\\boldsymbol{A}^{\\top}t}\\mathrm{~d}t.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The integral formula (17) suggests that the solution to the Lyapunov equation can be computed using a numerical integration technique. For a finite $\\tau>0$ , we define ", "page_idx": 6}, {"type": "equation", "text": "$$\nX_{\\tau}:=\\int_{0}^{\\tau}e^{\\mathcal{A}t}\\Omega e^{\\boldsymbol{A}^{\\top}t}\\mathrm{~d}t.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "For any arbitrary $\\varepsilon>0$ , we find that a $\\tau=\\widetilde{\\mathcal{O}}(\\log(1/\\varepsilon))$ is sufficient to ensure an $\\varepsilon$ -approximate solution. We denote $\\kappa:=\\|X^{\\ast}\\|/\\lambda_{\\mathrm{min}}(\\Omega)$ . ", "page_idx": 6}, {"type": "text", "text": "Lemma 3 (Numerical integration). For any $\\varepsilon>0$ , we have $\\|X^{*}-X_{\\tau}\\|\\leq\\varepsilon$ , provided that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\tau=\\kappa\\log\\!\\left(\\frac{||\\Omega|||X^{*}||\\kappa}{\\varepsilon\\lambda_{\\operatorname*{min}}(X^{*})}\\right)\\!.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Proof. Note that $\\begin{array}{r}{\\|X^{*}-X_{\\tau}\\|=\\left\\|\\int_{\\tau}^{\\infty}e^{\\mathcal{A}t}\\Omega e^{\\mathcal{A}^{\\top}t}\\;\\mathrm{d}t\\right\\|\\leq\\int_{\\tau}^{\\infty}\\|\\Omega\\|\\|e^{\\mathcal{A}t}\\|\\|e^{\\mathcal{A}^{\\top}t}\\|\\;\\mathrm{d}t}\\end{array}$ , where $\\|e^{\\b{A}t}\\|$ (or \u2225eA\u22a4t\u2225) is upper bounded by \u03bbm\u2225inX(X\u2225\u2217)e\u2212 (see [44, Lemma 12]). It follows that $\\begin{array}{r}{\\|X^{*}-X_{\\tau}\\|\\leq\\frac{\\|\\Omega\\|\\|X^{*}\\|\\kappa}{\\lambda_{\\operatorname*{min}}(X^{*})}e^{-\\tau/\\kappa}}\\end{array}$ . Therefore, an integration time as given in (19) guarantees that $\\|X^{*}-X_{\\tau}\\|\\leq\\varepsilon$ . \u53e3 ", "page_idx": 6}, {"type": "text", "text": "4.2 Algorithm Complexity Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The matrix $X_{\\tau}$ can be approximated by a trapezoidal rule with $(K+1)$ quadrature node points, namely, ", "page_idx": 7}, {"type": "equation", "text": "$$\nX_{\\tau}\\approx\\sum_{k=0}^{K}w_{k}e^{A t_{k}}\\Omega e^{A^{\\top}t_{k}}=\\sum_{k=0}^{K}w_{k}\\mathcal{F}(t_{k}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "cwohmerpeu $\\begin{array}{r}{w_{k}=\\frac{(2-1_{k=0,K})\\tau}{2K}}\\end{array}$ ,m $\\begin{array}{r}{t_{k}=\\frac{k\\tau}{K}}\\end{array}$ ,t earn bd $\\mathcal{F}(t):=e^{\\mathcal{A}t}\\Omega e^{\\boldsymbol{A}^{\\top}t}$ r.  cTohme bsiunamtimoantsi oonf  ibnl o(c2k0-)e nccaond ebde ted on a quantu  compu y performing linea matrices, which we will explain shortly. ", "page_idx": 7}, {"type": "text", "text": "Definition 4 (Select oracle). Let $A\\in\\mathbb{R}^{n\\times n}$ be a Hurwitz matrix. Given an integer $K>0$ and two positive scalars $\\tau,\\varepsilon>0$ , we define the following unitary (named as the select oracle): ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname{select}(A,\\varepsilon):=\\sum_{k=0}^{K}\\left|k\\right>\\!\\!\\left<k\\right|\\otimes U_{k},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where for each $k=0,\\ldots,K$ , $U_{k}$ is a $(\\zeta t_{k},\\varepsilon)$ -block-encoding of $e^{\\mathcal{A}t_{k}}$ with $t_{k}=k\\tau/K$ . Here, $\\zeta$ denotes some parameter that only depends on $\\mathcal{A}$ . ", "page_idx": 7}, {"type": "text", "text": "Now, we consider two select oracles: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname{select}(A,\\varepsilon):=\\sum_{k=0}^{K}|k\\rangle\\langle k|\\otimes U_{k},\\quad\\operatorname{select}(A^{\\top},\\varepsilon):=\\sum_{k=0}^{K}|k\\rangle\\langle k|\\otimes V_{k},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where for each $k=0,\\ldots,K$ , $U_{k}$ (or $V_{k}$ ) denotes a block-encoding of $e^{\\b{A}t_{k}}$ (or $e^{\\boldsymbol{A}^{\\intercal}t_{k}}$ ) with normalization factor $\\zeta t_{k}$ . Let $O_{\\Omega}$ be a $(\\eta,0)$ -block-encoding of $\\Omega$ , and we find that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname{select}(A,\\varepsilon)(I\\otimes O_{\\Omega})\\mathrm{select}(A^{\\top},\\varepsilon)=\\sum_{k=0}^{K}|k\\rangle\\!\\langle k|\\otimes W_{k},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $W_{k}:=U_{k}O_{\\Omega}V_{k}$ is a $(\\eta\\zeta^{2}t_{k}^{2},2\\zeta\\eta\\varepsilon)$ -block-encoding of the matrix $\\mathcal{F}(t_{k})$ . Denoting $\\lambda_{k}:=$ $w_{k}k^{2}$ , then it is clear that $\\sum_{k=0}^{K}\\lambda_{k}W_{k}$ is a block-encoding of $X_{\\tau}$ . Thus we can implement a block-encoded $X_{\\tau}$ on a qu antum computer using a technique known as linear combination of unitaries (LCU) [22, Lemma 52]. The rigorous complexity of this quantum algorithm is given in the following theorem, for which a complete proof is provided in Appendix $\\mathrm{E}$ . ", "page_idx": 7}, {"type": "text", "text": "Theorem 4. Suppose that $A\\in\\mathbb{R}^{n\\times n}$ is Hurwitz and $\\Omega\\in\\mathbb{R}^{n\\times n}$ is positive-definite. Let $O_{A}$ be an $(\\alpha,0)$ -block-encoding of $\\mathcal{A}$ and $O_{\\Omega}$ be an $(\\eta,0)$ -block-encoding of $\\Omega$ . Then, we can implement a $(\\gamma,\\varepsilon)$ -block-encoding of $X^{*}$ , the unique solution to the Lyapunov equation $(\\mathit{16})$ , using a single query to $O_{\\Omega}$ , $\\widetilde{\\mathcal{O}}\\left(\\alpha^{2}\\sqrt{\\frac{\\eta}{\\varepsilon}}\\right)$ queries to controlled $O_{A}$ and its inverse, and $\\widetilde{\\mathcal{O}}\\left(\\alpha^{2}\\sqrt{\\frac{\\eta}{\\varepsilon}}\\right)$ queries to other additional elementary gates. Here $\\gamma=\\widetilde{\\mathcal{O}}(\\alpha^{2}\\eta)$ . ", "page_idx": 7}, {"type": "text", "text": "4.3 Objective Function Evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As a direct consequence of Theorem 4, we can evaluate the objective function value $f(K)$ for a given $K\\in S_{K}$ in a cost that is logarithmic in the dimension parameter $n$ . This result demonstrates an exponential quantum advantage for the objective function evaluation task, as any known classical algorithm for this task requires at least matrix multiplication time. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5. Assume that we have efficient procedures (as described in Assumption 1) to access the problem data $A,B,Q,R$ in $\\mathcal{O}(\\mathrm{poly}\\log(n))$ time. Let $K\\in S_{K}$ be a stabilizing policy stored in a quantum-accessible data structure. We can estimate the objective function $f(K)$ up to multiplicative error $\\theta$ in cost $\\begin{array}{r}{\\widetilde{\\mathcal{O}}\\left(\\frac{1}{\\theta^{2}}\\right)}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "Proof. Given a $K\\ \\in\\ S_{K}$ , we can evaluate the objective function $f(K)$ via the formula $f(K)\\,=\\,\\mathrm{Tr}[P(K)\\Sigma_{0}]$ . Here, without loss of generality, we assume $\\Sigma_{0}\\,=\\,\\mathbb{I}$ . $P(K)$ has a closed-form representation as in (6), which corresponds to the unique positive solution to the Lyapunov equation ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(A-B K)^{\\top}P+P(A-B K)+\\left(Q+K^{\\top}R K\\right)=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We denote $A=A-B K$ , and $\\Omega={Q}+{K}^{\\top}{R}{K}$ . By Lemma 11 and Remark 1, we can block-encode $\\mathcal{A}$ with normalization factor $s(\\|K\\|_{F}+1)$ and $\\Omega$ with normalization factor $s(\\|K\\|_{F}^{2}\\!+\\!1)$ , both in cost $\\mathcal{O}(\\mathrm{poly}\\log(n,s))$ . Suppose that $f(K)=a$ , it follows from Lemma 8 that $\\|K\\|_{F}\\leq{\\mathcal{O}}(a)$ . Therefore, by Theorem 4, we can implement a $(\\gamma,\\varepsilon)$ -block-encoding of $P(K)$ in cost $\\begin{array}{r}{\\tilde{\\mathcal{O}}\\left(a^{3}\\rho\\sqrt{\\frac{\\kappa^{5}}{\\varepsilon}}\\right)}\\end{array}$ , where $\\gamma\\le\\widetilde O(a^{4}\\rho^{2}\\kappa^{3})$ . Note that $P(K)$ is a Hermitian matrix, and $\\lambda_{\\operatorname*{min}}(P(K))\\geq\\lambda_{\\operatorname*{min}}(Q)$ , by invoking Theorem 25, we can estimate $f(K)=\\mathrm{Tr}[P(K)]$ up to a multiplicative error $\\theta$ in cost $\\begin{array}{r}{\\widetilde{\\mathcal{O}}\\left(\\frac{a^{3}\\rho}{\\theta}\\sqrt{\\frac{\\gamma^{3}\\kappa^{5}}{\\varepsilon}}\\right)\\leq\\widetilde{\\mathcal{O}}\\left(\\frac{a^{13}\\rho^{6}\\kappa^{10}}{\\theta^{2}}\\right)}\\end{array}$ . Here, the error parameter must be chosen so that $\\varepsilon\\leq\\widetilde{\\mathcal{O}}(\\theta^{2}/\\gamma^{2})$ . \u53e3 ", "page_idx": 8}, {"type": "text", "text": "5 Quantum Policy Gradient for Large-Scale Control ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "5.1 Quantum Gradient Estimation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Definition 5. Given any $K\\in S_{K}$ , we call $G\\in\\mathbb{R}^{m\\times n}$ a $\\theta$ -robust estimate of $\\nabla f(K)$ if it approximates the gradient $\\nabla f(K)$ up to a multiplicative error $\\theta$ , i.e., ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\|G-\\nabla f(K)\\|_{F}\\leq\\theta\\|\\nabla f(K)\\|_{F}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Here, we utilize the close-form expression of the policy gradient $\\nabla f(K)$ , as shown in (13), to construct a quantum algorithm for gradient estimation. The complete theorem statement and the proof are in Appendix G. ", "page_idx": 8}, {"type": "text", "text": "Theorem 6 (Informal version of Theorem 31). Assume we have efficient procedures (as described in Assumption 1) to access $A,B,Q,R$ in $\\mathcal{O}(\\mathrm{poly}\\log(n))$ time. Let $K\\in S_{K}$ be $a$ stabilizing policy stored in a quantum-accessible data structure. Provided that $\\|K-K^{*}\\|>\\varepsilon$ and $m\\ll n$ , we can compute a $\\theta$ -robust estimate of $\\nabla f(K)$ in cost $\\widetilde{\\mathcal{O}}\\left(\\frac{n}{\\theta^{1.5}\\varepsilon^{1.5}}\\right)$ . ", "page_idx": 8}, {"type": "text", "text": "5.2 Quantum Policy Gradient ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our main quantum algorithm for LQR is summarized in Algorithm 1. ", "page_idx": 8}, {"type": "table", "img_path": "GHqw3xLAvd/tmp/8c93a5793904bc68c3fb59eaa69ca8f1ec09d582ee5a6d3761b529f3dcda82f7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "We can prove that the iterates in Algorithm 1 converges to the optimal control policy $K^{*}$ at a linear rate (Proposition 34). It follows that our algorithm can find an $\\varepsilon$ -approximate optimal policy with an end-to-end cost $\\widetilde{\\mathcal{O}}\\left(\\frac{n}{\\varepsilon^{1.5}}\\right)$ . ", "page_idx": 8}, {"type": "text", "text": "Theorem 7 (Informal version of Theorem 35). Assume that we have efficient procedures (as described in Assumption 1) to access the problem data $A,B,Q,R$ in $\\mathcal{O}(\\mathrm{poly}\\log(n))$ time. Let $K_{0}\\in S_{K}$ be a stabilizing policy and assume that $m\\ll n$ . Then, Algorithm $\\mathit{1}$ outputs an $\\varepsilon$ -approximate solution to Problem 1 in cost $\\widetilde{\\mathcal{O}}\\left(\\frac{n}{\\varepsilon^{1.5}}\\right)$ . ", "page_idx": 8}, {"type": "text", "text": "5.3 Numerical Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Correctness. We conduct a numerical experiment to showcase the correctness of our quantum policy gradient algorithm. Following a similar setup as in [45], a mass-springdamper system with $g=4$ masses is used for constructing our LQR problem. The state ", "page_idx": 8}, {"type": "image", "img_path": "GHqw3xLAvd/tmp/b07fcf6b58bf853c422e8ca19f4b14fce771e49dec6439c430be7688c41328ac.jpg", "img_caption": ["Figure 2: Numerical Results on Convergence. Following the mass-spring-damper setup in [45], our policy gradient descent algorithm converges much faster than [45]. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "$\\boldsymbol{x}=[\\boldsymbol{p}^{\\top},\\boldsymbol{v}^{\\top}]^{\\top}\\in\\mathbb{R}^{2g}$ contains positions and velocities, with dynamic and input matrices, ", "page_idx": 9}, {"type": "equation", "text": "$$\nA=\\left[\\!\\!\\begin{array}{c c}{0}&{I}\\\\ {-T}&{-T}\\end{array}\\!\\!\\right],B=\\left[\\!\\!\\begin{array}{c}{0}\\\\ {I}\\end{array}\\!\\!\\right],Q=I+100e_{1}e_{1}^{\\top},R=I+4e_{2}e_{2}^{\\top},\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $0,I$ are $g\\times g$ zero and identity matrices, $e_{i}$ is the $i$ th unit vector, and matrix $T$ has 2 on the main diagonal and -1 on the first super- and sub-diagonal. In Figure 2, we run our method against the classical model-free gradient-based method [45]. It shows that our model-based policy gradient converges much faster to the ground truth ARE solution $K^{*}$ . In the benchmark example [45], ours converges within 750 iterations, while the classical method takes $2\\times10^{4}$ iterations (orders of magnitude longer). In Figure 2 (c), we increase the system size by scaling $g$ from 4 to 64. Both methods run on a classical simulator with Intel i9- 10980XE CPU. Our method runs much faster than [45] by nearly 3 orders of magnitude. The code for both methods can be seen at https://github.com/YilingQiao/diff_lqr. Additional numerical results can be found in Appendix I. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose the first quantum algorithm for solving linear-quadratic control problems that achieves end-to-end quantum speedups. Our quantum algorithm utilizes an exponentially faster quantum linear dynamics simulator combined with a policy gradient method. Compared to classical approaches relying on matrix factorization and iterations, our method achieves super-quadratic speedup in the large-scale regime (i.e., $m\\ll n$ ). Moreover, the hybrid quantum-classical algorithm design makes our algorithm a promising candidate for practical quantum advantage in the near horizon. We also provide numerical evidence to demonstrate the robustness and favorable convergence behavior of our method. ", "page_idx": 9}, {"type": "text", "text": "Limitations and Future Work. Accelerating optimal control and reinforcement learning using quantum computers remains an emerging research topic. Our work has focused on the theoretical aspects of quantum advantage for LQR, a classic optimal control problem of fundamental importance in both theory and practice. However, for special cases [17], we have no guarantee that our quantum algorithm still applies, since Lemma 33 may not hold. In the future, we aim to explore both the practical utility of quantum computing for such tasks and its potential for handling more complex optimal control scenarios, such as non-quadratic and nonlinear problems. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Kaiqing Zhang for the helpful discussions and the anonymous reviewers for their helpful feedback. This work was supported in part by the U.S. Department of Energy Office of Science, National Quantum Information Science Research Centers, Quantum Systems Accelerator, Air Force Office of Scientific Research (Award#FA9550-21-1-0209), the U.S. National Science Foundation Career Grant (CCF-1942837), a Sloan research fellowship, Meta Research PhD Fellowship, National Science Foundation Graduate Research Fellowship (Grant No. DGE 2236417), the Simons Quantum Postdoctoral Fellowship, a Simons Investigator award (Grant No. 825053), and Dr. Barry Mersky & Captial One E-Nnovate Endowed Professorships. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Amira Abbas, Andris Ambainis, Brandon Augustino, Andreas B\u00e4rtschi, Harry Buhrman, Carleton Coffrin, Giorgio Cortiana, Vedran Dunjko, Daniel J Egger, Bruce G Elmegreen, et al. Quantum optimization: Potential, challenges, and the path forward, 2023. arXiv:2312.02279.   \n[2] Dorit Aharonov and Amnon Ta-Shma. Adiabatic quantum state generation and statistical zero knowledge. In Proceedings of the thirty-fifth annual ACM symposium on Theory of computing, pages 20\u201329, 2003. [3] Dong An, Andrew M Childs, and Lin Lin. Quantum algorithm for linear non-unitary dynamics with near-optimal dependence on all parameters, 2023. arXiv:2312.03916. [4] Dong An, Jin-Peng Liu, and Lin Lin. Linear combination of Hamiltonian simulation for nonunitary dynamics with optimal state preparation cost. Physical Review Letters, 131(15):150603, 2023.   \n[5] Rika Antonova, Jingyun Yang, Krishna Murthy Jatavallabhula, and Jeannette Bohg. Rethinking optimization with differentiable simulation from a global perspective. In Proceedings of The 6th Conference on Robot Learning. PMLR, 2023.   \n[6] William F Arnold and Alan J Laub. Generalized eigenproblem algorithms and software for algebraic Riccati equations. Proceedings of the IEEE, 72(12):1746\u20131754, 1984.   \n[7] Brandon Augustino, Giacomo Nannicini, Tam\u00e1s Terlaky, and Luis F Zuluaga. Quantum interior point methods for semidefinite optimization. Quantum, 7:1110, 2023.   \n[8] Peter Benner, Zvonimir Bujanovic, Patrick Kurschner, and Jens Saak. A numerical comparison of different solvers for large-scale, continuous-time algebraic Riccati equations and lqr problems. SIAM Journal on Scientific Computing, 42(2):A957\u2013A996, 2020. [9] Dominic W Berry. High-order quantum algorithm for solving linear differential equations. Journal of Physics A: Mathematical and Theoretical, 47(10):105301, 2014.   \n[10] Steven J Bradtke, B Erik Ydstie, and Andrew G Barto. Adaptive linear quadratic control using policy iteration. In Proceedings of 1994 American Control Conference-ACC\u201994, volume 3, pages 3475\u20133479. IEEE, 1994.   \n[11] Shantanav Chakraborty, Andr\u00e1s Gily\u00e9n, and Stacey Jeffery. The power of block-encoded matrix powers: improved regression techniques via faster Hamiltonian simulation, 2018. arXiv:1804.01973.   \n[12] El Amine Cherrat, Iordanis Kerenidis, and Anupam Prakash. Quantum reinforcement learning via policy iteration. Quantum Machine Intelligence, 5(2):30, 2023.   \n[13] Andrew M Childs, Robin Kothari, and Rolando D Somma. Quantum algorithm for systems of linear equations with exponentially improved dependence on precision. SIAM Journal on Computing, 46(6):1920\u20131950, 2017.   \n[14] Alon Cohen, Avinatan Hasidim, Tomer Koren, Nevena Lazic, Yishay Mansour, and Kunal Talwar. Online linear quadratic control. In International Conference on Machine Learning, pages 1029\u20131038. PMLR, 2018.   \n[15] Arjan Cornelissen, Yassine Hamoudi, and Sofiene Jerbi. Near-optimal quantum algorithms for multivariate mean estimation. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages 33\u201343, 2022.   \n[16] Nikolaus Correll. Parameter estimation and optimal control of swarm-robotic systems: A case study in distributed task allocation. In 2008 IEEE International Conference on Robotics and Automation, pages 3302\u20133307. IEEE, 2008.   \n[17] Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. On the sample complexity of the linear quadratic regulator. Foundations of Computational Mathematics, 20(4):633\u2013679, 2020.   \n[18] Karthik Elamvazhuthi and Spring Berman. Optimal control of stochastic coverage strategies for robotic swarms. In 2015 ieee international conference on robotics and automation (icra), pages 1822\u20131829. IEEE, 2015.   \n[19] Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi. Global convergence of policy gradient methods for the linear quadratic regulator. In International conference on machine learning, pages 1467\u20131476. PMLR, 2018.   \n[20] Franziska Feitzinger, Timo Hylla, and Ekkehard W Sachs. Inexact Kleinman-Newton method for Riccati equations. SIAM Journal on Matrix Analysis and Applications, 31(2):272\u2013288, 2009.   \n[21] Greg Foderaro, Pingping Zhu, Hongchuan Wei, Thomas A Wettergren, and Silvia Ferrari. Distributed optimal control of sensor networks for dynamic target tracking. IEEE Transactions on Control of Network Systems, 5(1):142\u2013153, 2016.   \n[22] Andr\u00e1s Gily\u00e9n, Yuan Su, Guang Hao Low, and Nathan Wiebe. Quantum singular value transformation and beyond: exponential improvements for quantum matrix arithmetics. In Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, pages 193\u2013204, 2019.   \n[23] Vittorio Giovannetti, Seth Lloyd, and Lorenzo Maccone. Quantum random access memory. Physical review letters, 100(16):160501, 2008.   \n[24] Aram W Harrow, Avinatan Hassidim, and Seth Lloyd. Quantum algorithm for linear systems of equations. Physical review letters, 103(15):150502, 2009.   \n[25] Bin Hu, Kaiqing Zhang, Na Li, Mehran Mesbahi, Maryam Fazel, and Tamer Ba\u015far. Toward a theoretical foundation of policy optimization for learning control policies. Annual Review of Control, Robotics, and Autonomous Systems, 6:123\u2013158, 2023.   \n[26] Yuanming Hu, Luke Anderson, Tzu-Mao Li, Qi Sun, Nathan Carr, Jonathan RaganKelley, and Fr\u00e9do Durand. DiffTaichi: Differentiable programming for physical simulation. In ICLR, 2020.   \n[27] Krishna Murthy Jatavallabhula, Miles Macklin, Florian Golemo, Vikram Voleti, Linda Petrini, Martin Weiss, Breandan Considine, Jerome Parent-Levesque, Kevin Xie, Kenny Erleben, Liam Paull, Florian Shkurti, Derek Nowrouzezahrai, and Sanja Fidler. gradsim: Differentiable simulation for system identification and visuomotor control. International Conference on Learning Representations (ICLR), 2021.   \n[28] Sofiene Jerbi, Arjan Cornelissen, M\u00afaris Ozols, and Vedran Dunjko. Quantum policy gradient algorithms, 2022. arXiv:2212.09328.   \n[29] Amara Katabarwa, Katerina Gratsea, Athena Caesura, and Peter D Johnson. Early fault-tolerant quantum computing, 2023. arXiv:2311.14814.   \n[30] Iordanis Kerenidis and Anupam Prakash. Quantum recommendation systems, 2016. arXiv:1603.08675.   \n[31] Youngseok Kim, Andrew Eddins, Sajant Anand, Ken Xuan Wei, Ewout Van Den Berg, Sami Rosenblatt, Hasan Nayfeh, Yantao Wu, Michael Zaletel, Kristan Temme, et al. Evidence for the utility of quantum computing before fault tolerance. Nature, 618(7965):500\u2013 505, 2023.   \n[32] David Kleinman. On an iterative technique for Riccati equation computations. IEEE Transactions on Automatic Control, 13(1):114\u2013115, 1968.   \n[33] Hari Krovi. Improved quantum algorithms for linear and nonlinear differential equations. Quantum, 7:913, 2023.   \n[34] Peter Lancaster and Leiba Rodman. Algebraic Riccati equations. Clarendon press, 1995.   \n[35] Alan Laub. A Schur method for solving algebraic Riccati equations. IEEE Transactions on automatic control, 24(6):913\u2013921, 1979.   \n[36] Jiaqi Leng, Ethan Hickman, Joseph Li, and Xiaodi Wu. Quantum Hamiltonian Descent, 2023. arXiv:2303.01471.   \n[37] Jiaqi Leng, Yuxiang Peng, Yi-Ling Qiao, Ming Lin, and Xiaodi Wu. Differentiable analog quantum computing for optimization and control. Advances in Neural Information Processing Systems, 35:4707\u20134721, 2022.   \n[38] William Levine and Michael Athans. On the determination of the optimal constant output feedback gains for linear multivariable systems. IEEE Transactions on Automatic control, 15(1):44\u201348, 1970.   \n[39] Xiantao Li and Chunhao Wang. Efficient quantum algorithms for quantum optimal control. In International Conference on Machine Learning, pages 19982\u201319994. PMLR, 2023.   \n[40] Fu Lin, Makan Fardad, and Mihailo R Jovanovic. Augmented lagrangian approach to design of structured optimal state feedback gains. IEEE Transactions on Automatic Control, 56(12):2923\u20132929, 2011.   \n[41] Jin-Peng Liu, Herman \u00d8ie Kolden, Hari K Krovi, Nuno F Loureiro, Konstantina Trivisa, and Andrew M Childs. Efficient quantum algorithm for dissipative nonlinear differential equations. Proceedings of the National Academy of Sciences, 118(35):e2026805118, 2021.   \n[42] Guang Hao Low and Yuan Su. Quantum eigenvalue processing, 2024. arXiv:2401.06240.   \n[43] Nico Meyer, Christian Ufrecht, Maniraman Periyasamy, Daniel D Scherer, Axel Plinge, and Christopher Mutschler. A survey on quantum reinforcement learning, 2022. arXiv:2211.03464.   \n[44] Hesameddin Mohammadi, Armin Zare, Mahdi Soltanolkotabi, and Mihailo R Jovanovi\u0107. Convergence and sample complexity of gradient methods for the model-free linear\u2013 quadratic regulator problem, 2019. arXiv:1912.11899.   \n[45] Hesameddin Mohammadi, Armin Zare, Mahdi Soltanolkotabi, and Mihailo R Jovanovi\u0107. Convergence and sample complexity of gradient methods for the model-free linear\u2013 quadratic regulator problem. IEEE Transactions on Automatic Control, 67(5):2435\u20132450, 2021.   \n[46] Miguel Angel Zamora Mora, Momchil Peychev, Sehoon Ha, Martin Vechev, and Stelian Coros. Pods: Policy optimization via differentiable simulation. In International Conference on Machine Learning, pages 7805\u20137817. PMLR, 2021.   \n[47] Kirsten Morris and Carmeliza Navasca. Iterative solution of algebraic Riccati equations for damped systems. In Proceedings of the 45th IEEE Conference on Decision and Control, pages 2436\u20132440. IEEE, 2006.   \n[48] Kirsten A Morris. Introduction to feedback control. Academic Press, Inc., 2000.   \n[49] Yi-Ling Qiao, Junbang Liang, Vladlen Koltun, and Ming C. Lin. Efficient differentiable simulation of articulated bodies. In ICML, 2021.   \n[50] Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing: Towards a sharp analysis of linear system identification. In Conference On Learning Theory, pages 439\u2013473. PMLR, 2018.   \n[51] Valeria Simoncini. Computational methods for linear matrix equations. SIAM Review, 58(3):377\u2013441, 2016.   \n[52] Sanghyun Son, Yi-Ling Qiao, Jason Sewall, and Ming C. Lin. Differentiable hybrid traffic simulation. ACM Transactions on Graphics (TOG), 2022.   \n[53] Hyung Ju Suh, Max Simchowitz, Kaiqing Zhang, and Russ Tedrake. Do differentiable simulators give better policy gradients? In Proceedings of the 39th International Conference on Machine Learning, volume 162, pages 20668\u201320696. PMLR, 2022.   \n[54] Joran van Apeldoorn and Andr\u00e1s Gily\u00e9n. Quantum algorithms for zero-sum games, 2019. arXiv:1904.03180.   \n[55] Joran Van Apeldoorn, Andr\u00e1s Gily\u00e9n, Sander Gribling, and Ronald de Wolf. Quantum SDP-solvers: Better upper and lower bounds. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pages 403\u2013414. IEEE, 2017.   \n[56] R Vinifa and A Kavitha. Linear quadratic regulator based current control of grid connected inverter for renewable energy applications. In 2016 International Conference on Energy Efficient Technologies for Sustainability (ICEETS), pages 106\u2013111. IEEE, 2016.   \n[57] Xin Wen, Xuting Sun, Yige Sun, and Xiaohang Yue. Airline crew scheduling: Models and algorithms. Transportation research part E: logistics and transportation review, 149:102304, 2021.   \n[58] Simon Wiedemann, Daniel Hein, Steffen Udluft, and Christian Mendl. Quantum policy iteration via amplitude estimation and grover search\u2013towards quantum advantage for reinforcement learning, 2022. arXiv:2206.04741.   \n[59] Zhou Xian, Bo Zhu, Zhenjia Xu, Hsiao-Yu Tung, Antonio Torralba, Katerina Fragkiadaki, and Chuang Gan. Fluidlab: A differentiable environment for benchmarking complex fluid manipulation. In International Conference on Learning Representations, 2023.   \n[60] Jie Xu, Tao Chen, Lara Zlokapa, Michael Foshey, Wojciech Matusik, Shinjiro Sueda, and Pulkit Agrawal. An End-to-End Differentiable Framework for Contact-Aware Robot Design. In Proceedings of Robotics: Science and Systems, Virtual, July 2021.   \n[61] Jie Xu, Viktor Makoviychuk, Yashraj Narang, Fabio Ramos, Wojciech Matusik, Animesh Garg, and Miles Macklin. Accelerated policy learning with parallel differentiable simulation. In International Conference on Learning Representations, 2021.   \n[62] Yao Zhang and Qiang Ni. Recent advances in quantum machine learning. Quantum Engineering, 2(1):e34, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A LQR theory ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Definition 6. Given a positive number $a>0$ , we define the sublevel set $S_{K}(a):=\\{K\\in$ $\\mathbb{R}^{m\\times n}\\colon f(K)\\leq a\\right\\}$ . We have the following useful bound on $K$ . ", "page_idx": 14}, {"type": "text", "text": "Lemma 8. Over the sublevel set $S_{K}(a)$ of the LQR objective function $f(K)$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname{Tr}[X(K)]\\leq a/\\lambda_{\\operatorname*{min}}(Q),}\\\\ &{~~~~~~~~~~\\nu/a\\leq\\lambda_{\\operatorname*{min}}(X(K)),}\\\\ &{~~~~~~~~~\\|K\\|_{F}\\leq a/\\sqrt{\\nu\\lambda_{\\operatorname*{min}}(R)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the constant ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nu=\\frac{1}{4}\\left(\\frac{\\|A\\|_{2}}{\\sqrt{\\lambda_{\\operatorname*{min}}(Q)}}+\\frac{\\|B\\|_{2}}{\\sqrt{\\lambda_{\\operatorname*{min}}(R)}}\\right)^{-2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. See [45, Lemma 16]. ", "page_idx": 14}, {"type": "text", "text": "Lemma 9. For any $K\\in{\\cal S}_{K}(a)$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|K-K^{*}\\|_{F}^{2}\\leq\\frac{a}{\\nu\\lambda_{\\operatorname*{min}}(R)}\\left(f(K)-f(K^{*})\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\nu$ is the same as in (28). ", "page_idx": 14}, {"type": "text", "text": "Proof. By [45, Lemma 2], we have ", "page_idx": 14}, {"type": "text", "text": "$\\begin{array}{r}{f(K)-f(K^{*})=\\operatorname{Tr}\\left[(K-K^{*})^{\\top}R(K-K^{*})X(K)\\right]\\geq\\lambda_{\\operatorname*{min}}(R)\\lambda_{\\operatorname*{min}}(X(K))\\|K-K^{*}\\|_{F}^{2}.}\\end{array}$ Combining the above result with Lemma 8, we end up with (29). ", "page_idx": 14}, {"type": "text", "text": "Lemma 10 (PL condition). Fix $a>0$ . For any $K\\in{\\cal S}_{K}(a)$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\nabla f(K)\\|_{F}^{2}\\geq2\\mu_{f}(f(K)-f(K^{*})),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\mu_{f}>0$ is a constant that only depends on the problem data and $a$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. See [45, Remark 2]. ", "page_idx": 14}, {"type": "text", "text": "B Implementation of block-encoded matrices ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Lemma 11. Assume that we have efficient procedures (as described in Assumption 1) to access the problem data $A,B,Q,R$ in $\\mathcal{O}(\\mathrm{poly}\\log(n))$ time. For a fixed $a>0$ , suppose that $K\\in{\\cal S}_{K}$ is a stabilizing policy stored in a quantum-accessible data structure. Then, we can implement ", "page_idx": 14}, {"type": "text", "text": "Proof. By [22, Lemma 48], we can implement a $(s,\\varepsilon)$ -block-encoding of $A$ (or $B,Q,R)$ in cost ${\\mathcal{O}}(\\operatorname{poly}\\log(n,s/\\varepsilon))$ . Also, due to [22, Lemma 50], we can implement a $\\big(\\|K\\|_{F},\\varepsilon\\big)$ - block-encoding of $K$ in cost ${\\mathcal{O}}(\\mathrm{poly}\\log(n,1/\\varepsilon))$ . Therefore, by [22, Lemma 52, 53], a $(s(\\|K\\|_{F}+1),\\varepsilon)$ -block-encoding of $A-B K$ can be implement in cost ${\\mathcal{O}}(\\mathrm{poly}\\log(n,s/\\varepsilon))$ . Similarly, we can implement a $(s(\\|K\\|_{F}^{2}+1),\\varepsilon)$ -block-encoding of $Q+K^{\\top}R K$ in cost ${\\mathcal{O}}(\\mathrm{poly}\\log(n,s/\\varepsilon))$ . \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Remark 1. We observe that the block-encodings of $A\\mathrm{~-~}B K$ and $Q+K^{\\top}R K$ can be implemented with high precisions, i.e., in cost ${\\mathcal{O}}(\\operatorname{poly}\\log(1/\\varepsilon))$ . To simplify our technical arguments, we assume these block-encodings can be implemented with no error, i.e., we can implement a $(s(\\|K\\|_{F}{+}1),0)$ -block-encoding of $A\\!-\\!B K$ (or a $(s(\\|K\\|_{F}^{:2}{+}1),0)$ -block-encoding of $Q+K^{\\top}R K$ in cost $\\mathcal{O}(\\mathrm{poly}\\log(n,s))$ . ", "page_idx": 14}, {"type": "text", "text": "C Matrix exponential based on quantum linear system solver ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Block-encoding for matrix inverse ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma 12 (Modified from [42], Lemma 11). Let $C$ be a matrix such that $C/\\alpha_{C}$ is blockencoded by $O_{C}$ with some normalization factor $\\alpha{_C}$ . Then we can implement $a$ $(O(\\alpha_{C^{-1}}),\\varepsilon)$ - block-encoding of $C^{-1}$ using ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\mathcal{O}}(\\kappa_{C}\\log(1/\\varepsilon))\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "queries to $O_{C}$ . Here $\\kappa_{C}$ is the condition number of $C$ . ", "page_idx": 15}, {"type": "text", "text": "Suppose we have an $(\\alpha,\\varepsilon)$ -block-encoding $U_{M}$ that block-encodes $M$ . Denote ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\alpha\\langle0|U_{M}|0\\rangle-M=\\Lambda,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "we have $\\|\\Lambda\\|\\leq\\varepsilon$ . Then we can see $U_{M}$ as an $(\\alpha,0)$ -block-encoding of $M+\\Lambda/\\alpha$ . So with the lemma above, we have the theorem below: ", "page_idx": 15}, {"type": "text", "text": "Theorem 13. Suppose we have an $(\\alpha,\\varepsilon_{1})$ -block-encoding $U_{M}$ that block-encodes $M$ , then we can implement a $\\begin{array}{r l r}{\\lefteqn{(\\mathcal{O}(\\alpha_{M^{-1}}),\\varepsilon_{2}+\\frac{\\alpha_{M^{-1}}^{2}}{\\alpha-\\varepsilon_{1}\\alpha_{M^{-1}}}\\varepsilon_{1})}}\\end{array}$ )-block-encoding for $M^{-1}$ using ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\mathcal{O}}(\\kappa_{M}\\log(1/\\varepsilon_{2}))\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "queries to $U_{M}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. From the lemma above we know we can implement a $\\left(\\tilde{\\alpha}_{M^{-1}},\\varepsilon_{2}\\right)$ -block-encoding as $\\tilde{U}_{M^{-1}}$ for $(M+\\Lambda/\\alpha)$ , where $\\tilde{\\alpha}_{M^{-1}}=\\mathcal{O}(\\alpha_{M^{-1}})$ . To analyze the error, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{t^{-1}\\tilde{U}_{M^{-1}}-M^{-1}\\|\\leq\\varepsilon_{2}+\\|(M+\\Lambda/\\alpha)^{-1}-M^{-1}\\|=\\varepsilon_{2}+\\|(I+M^{-1}\\Lambda/\\alpha)^{-1}M^{-1}-M^{-1}\\|}\\\\ &{\\qquad\\qquad\\qquad=\\varepsilon_{2}+\\|(I+M^{-1}\\Lambda/\\alpha)^{-1}-I\\|\\|M^{-1}\\|=\\varepsilon_{2}+\\left\\|\\displaystyle\\sum_{n=1}^{\\infty}((-M^{-1}\\Lambda)/\\alpha)^{n}\\right\\|\\|M^{-1}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\varepsilon_{2}+\\displaystyle\\sum_{n=1}^{\\infty}\\|((-M^{-1}\\Lambda)/\\alpha)^{n}\\|\\,\\|M^{-1}\\|\\leq\\varepsilon_{2}+\\displaystyle\\frac{\\alpha_{M^{-1}}^{2}}{\\alpha-\\varepsilon_{1}\\alpha_{M^{-1}}}\\varepsilon_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "C.2 Introduction to quantum linear system solver ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The first quantum algorithm for solving linear differential equations was proposed by [9]. Since this work was done, several refinements have been made. Among these, [33] significantly loosen the requirement of performing the algorithm. We notice that this whole algorithm can be written in the form of block-encoding. Basically, for a linear differential equation ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}u}{\\mathrm{d}t}=A u,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "upon appropriate discretization method and the method of line, one may construct a big matrix A s.t. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{A}\\left[{\\begin{array}{c}{u(0)}\\\\ {u(h)}\\\\ {u(2h)}\\\\ {\\vdots}\\\\ {u(T)}\\end{array}}\\right]=\\left[{\\begin{array}{c}{u(0)}\\\\ {0}\\\\ {0}\\\\ {\\vdots}\\\\ {0}\\end{array}}\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then it is easy to see that the matrix inverse ${\\bf A}^{-1}$ satisfies ", "page_idx": 15}, {"type": "equation", "text": "$$\n(\\langle k|\\otimes I){\\bf A}^{-1}(|0\\rangle\\otimes u(0))=u(k h)=e^{k h A}u(0).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Because $u(0)$ is arbitrarily chosen, we conclude ", "page_idx": 15}, {"type": "equation", "text": "$$\n(\\langle k|\\otimes I){\\bf A}^{-1}(|0\\rangle\\otimes I)=e^{k h A}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Suppose ${\\bf A}^{-1}$ is block-encoded in some $U_{\\mathbf{A}^{-1}}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n(\\langle k|\\otimes I)((\\langle0^{a}|\\otimes I)U_{\\mathbf{A}^{-1}}(|0^{a}\\rangle\\otimes I))(|0\\rangle\\otimes I)=(\\langle k0^{a}|\\otimes I)U_{\\mathbf{A}^{-1}}(|0^{a+\\ell}\\rangle\\otimes I).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here $\\ell$ satisfy $2^{\\ell}=T/h$ , where $T$ is the simulation time and $h$ is the step size. Then it is obvious that $U_{\\mathbf{A}^{-1}}$ is also a block-encoding of $e^{k h A}$ . ", "page_idx": 15}, {"type": "text", "text": "C.3 Matrix exponential construction ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma 14 (Modified from [33]). If we have a block-encoding $U_{L}$ that block-encodes $L$ defined as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal L}={\\cal I}-{\\cal N},}}\\\\ {{\\displaystyle{\\cal N}=\\sum_{i=0}^{m}\\vert i+1\\rangle\\langle i\\vert\\otimes{\\cal M}_{2}({\\cal I}-{\\cal M}_{1})^{-1}+\\sum_{i=m+1}^{2m}\\vert i+1\\rangle\\langle i\\vert\\otimes{\\cal I},}}\\\\ {{\\displaystyle{\\cal M}_{1}=\\sum_{j=0}^{k-1}\\vert j+1\\rangle\\langle j\\vert\\otimes\\frac{{\\cal A}h}{j+1},}}\\\\ {{\\displaystyle M_{2}=\\sum_{j=0}^{k}\\vert0\\rangle\\langle j\\vert\\otimes{\\cal I},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "then the $\\left(\\alpha_{L^{-1}},\\varepsilon\\right)$ -block-encoding $U_{L^{-1}}$ that block-encodes $L^{-1}$ satisfies ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\alpha_{L^{-1}}(\\langle r0^{a}|\\otimes I)U_{L^{-1}}(|0^{a+s}\\rangle\\otimes I)-e^{T A}\\right\\|\\leq\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for any $r\\geq m+1$ . Let $2^{s-1}=m$ , thus ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\alpha_{L^{-1}}(\\left<0^{a+s}\\right|\\otimes I)\\left((X\\otimes I_{s-1}\\otimes I_{a}\\otimes I)U_{L^{-1}}\\right)(|0^{a+s}\\rangle\\otimes I)\\approx e^{T A},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which means that ( $(X\\otimes I_{s-1}\\otimes I_{a}\\otimes I)U_{L^{-1}})$ is a block-encoding of $e^{T A}$ with its normalization factor as $\\alpha_{L}\\!-\\!1$ and error being at most $\\varepsilon$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. The proof of the correctness of $L$ can be found in [33]. ", "page_idx": 16}, {"type": "text", "text": "Now we turn to construct the block-encoding for $L$ . ", "page_idx": 16}, {"type": "text", "text": "Lemma 15. Assume we can query the $(\\alpha_{A},0)$ -block-encoding $U_{A}$ that encodes $A$ and $h\\alpha_{A}=\\mathcal{O}(1)$ , then we can construct $a$ $(\\mathcal{O}(k^{1.5}),k^{1/2}\\epsilon)$ -block-encoding for $L$ defined in (40) using ${\\mathcal{O}}(k\\log(1/\\epsilon))$ queries of $U_{A}$ and same queries to additional elementary gates. ", "page_idx": 16}, {"type": "text", "text": "Proof. First we need to block-encode $M_{1}$ . We will use the ADD operator defined as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{ADD}:=\\sum_{j}|(j+1)\\quad\\mathrm{mod}\\quad k\\rangle\\langle j|,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and the controlled-rotation $\\;U_{R}$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\nU_{R}|j+1\\rangle|0\\rangle=|j+1\\rangle\\left(\\frac{1}{j+1}|0\\rangle+\\sqrt{1-\\frac{1}{(j+1)^{2}}}|1\\rangle\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Notice that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(U_{R}\\otimes I)(\\mathrm{ADD}\\otimes I\\otimes I)\\left(\\displaystyle\\sum_{j=0}^{k-1}|j\\rangle\\!\\langle j|\\otimes I\\otimes U_{A}\\right)}\\\\ &{=\\displaystyle\\sum_{j=0}^{k-1}U_{R}\\big(|j+1\\!\\mod\\ k\\rangle\\!\\langle j|\\otimes I\\big)\\otimes U_{A}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Post-select on the second register and the ancilla qubits of $U_{A}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(I\\otimes\\langle0|\\otimes\\langle0|)\\left(\\displaystyle\\sum_{j=0}^{k-1}U_{R}\\big(|j+1\\mod\\ k\\rangle\\langle j|\\otimes I\\big)\\otimes U_{A}\\right)(I\\otimes|0\\rangle\\otimes|0\\rangle)}\\\\ &{=\\displaystyle\\sum_{j=0}^{k-1}|(j+1)\\mod\\ k\\rangle\\langle j|\\otimes\\displaystyle\\frac{A/\\alpha_{A}}{j+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "If we apply the operator on a state, namely ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left(\\sum_{j=0}^{k-1}U_{R}\\big(|(j+1)\\pmod{k}\\langle j|\\otimes I\\big)\\otimes U_{A}\\right)|t\\rangle|0\\rangle|\\psi\\rangle=\\frac{1}{\\alpha_{A}(t+1)}|(t+1)\\pmod{k}|0\\rangle A|\\psi\\rangle+|0\\rangle+\\frac{1}{\\alpha_{A}(t+1)}|0\\rangle+|0\\rangle+|0\\rangle=0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "So we can apply one more multi-controlled- $X$ gate to flip the flag register for the control register being $|0\\rangle$ , then the whole thing becomes a $(\\alpha_{A}h,0)$ -block-encoding for $M_{1}$ , using just one query to $U_{A}$ . We can then easily generate the block-encoding of $I-M_{1}$ by LCU, with a $1+\\alpha_{A}h$ normalization factor. By the analysis in [33], we know $\\begin{array}{r}{\\|I-M_{1}\\|\\|(I-M_{1})^{-1}\\|\\le}\\end{array}$ $2k$ . Using Lemma 12, we can implement a $(O(k),\\epsilon_{1})$ -block-encoding of $(I-M_{1})^{-1}$ using ${\\mathcal{O}}(k\\log(1/\\varepsilon_{1}))$ queries to the block-encoding of $I-M_{1}$ thus the same queries to $U_{A}$ . ", "page_idx": 17}, {"type": "text", "text": "For the block-encoding of $M_{2}$ , since it is sparse, we may directly implement\u221a it through the sparse input model. We may just assume the\u221a normalization factor to be $\\sqrt{k}$ and there is no error. Then we can implement a $(\\mathcal{O}(k^{3/2}),\\sqrt{k}\\varepsilon_{1})$ -block-encoding of $(I-M_{1})^{-1}M_{2}$ using ${\\mathcal{O}}(k\\log(1/\\varepsilon_{1}))$ queries to $U_{A}$ and other additional gates. ", "page_idx": 17}, {"type": "text", "text": "For the block-encoding for $N$ , we can see the normalizati\u221aon factor would be the same scaling as $M_{2}(I-M_{1})^{-1}$ , thus we implemented a $(\\mathcal{O}(k^{3/2}),\\mathcal{O}(\\sqrt{k}\\varepsilon_{1}))$ -block-encoding of $N$ . $\\sqsubset$ ", "page_idx": 17}, {"type": "text", "text": "Now if we use QSVT to perform the matrix inverse, just as described in Theorem 13, we can implement an (\u03b1L\u22121, \u03b52 +\u03b1L\u2212\u03b5L1\u2212\u03b11L\u22121 $\\begin{array}{r l}{\\!\\!\\!}&{{}(\\alpha_{L^{-1}},\\varepsilon_{2}+\\frac{\\alpha_{L^{-1}}^{2}}{\\alpha_{L}-\\varepsilon_{1}\\alpha_{L^{-1}}}\\sqrt{k}\\varepsilon_{1})}\\end{array}$ -block-encoding $U_{L}\\!-\\!1$ that encodes $L^{-1}$ in cost ${\\mathcal{O}}(\\kappa_{L}\\log(1/\\varepsilon_{2}))$ queries to $U_{L}$ , i.e. ${\\mathcal{O}}(\\kappa_{L}\\log(1/\\varepsilon_{2})k\\log(1/\\varepsilon_{1}))$ queries to $U_{A}$ . ", "page_idx": 17}, {"type": "text", "text": "In order to perform the inverse of $L$ , we need to know the condition number of $L$ . ", "page_idx": 17}, {"type": "text", "text": "Lemma 16 (Modified from [33], Theorem $3\\ \\&\\ 4$ ). Suppose $E$ is the solution operator block-encoded by $U_{L}\\!-\\!1$ that approximates $e^{A T}$ and m is the number of steps. Let ", "page_idx": 17}, {"type": "equation", "text": "$$\n(k+1)!\\geq\\frac{m e^{3}}{\\delta}C_{A},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t}\\|\\exp(A t)\\|\\leq C_{A},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|E-e^{A T}\\|\\leq\\delta,\\quad\\|L^{-1}\\|=\\mathcal{O}(m C_{A}(1+\\delta))\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\kappa_{L}\\leq\\mathcal{O}(m\\sqrt{k}C_{A}(1+\\delta)).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. The proof can be seen at [33, Theorem 4]. ", "page_idx": 17}, {"type": "text", "text": "Theorem 17. Let matrix $A$ be a Hurwitz matrix with $\\operatorname*{sup}_{t}\\|\\exp(t A)\\|$ bounded by some constant $\\rho$ , and $O_{A}$ is a $(\\alpha_{A},0)$ -block-encoding of $A$ . Then we can construct $a$ $(\\zeta T,\\epsilon)$ -blockencoding of $e^{A T}$ using ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\widetilde{\\mathcal{O}}}\\left(\\alpha_{A}\\rho T\\cdot\\mathrm{poly}\\log\\left({\\frac{1}{\\epsilon}}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "queries to $O_{A}$ and same queries to other additional elementary gates. Here $\\zeta=\\mathcal{O}(\\alpha_{A}\\rho)$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. Firstly notice that $m=\\mathcal{O}(\\alpha_{A}T)$ and lolgo lgo(gT( T\u03b1 A\u03b1\u03c1A/\u03c1\u03b4/)\u03b4) . For a given accuracy \u03b5, we need $\\delta\\leq\\varepsilon$ , which could be given by letting ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\varepsilon_{2}+\\frac{\\alpha_{L^{-1}}^{2}}{\\alpha_{L}-\\varepsilon_{1}\\alpha_{L^{-1}}}\\sqrt{k}\\varepsilon_{1}\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Choose $\\varepsilon_{1}=\\widetilde{\\mathcal{O}}(\\varepsilon/T^{2})$ and $\\varepsilon_{2}=\\varepsilon/2$ , from the discussion above we know we can implement an $\\left(\\alpha_{L^{-1}},\\varepsilon\\right)$ - block-encoding of $e^{T A}$ . Since $\\alpha_{L^{-1}}=\\mathcal{O}(m\\rho(1+\\delta))$ , the queries we need is $\\tilde{\\mathcal{O}}(\\alpha_{A}\\rho T\\cdot\\mathrm{poly}\\log(1/\\varepsilon))$ . \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Remark 2. If we directly use the theorem above to prepare the state $|e^{A T}|\\psi\\rangle\\rangle$ for some input state $|\\psi\\rangle$ , the dependence on $T$ would be $T^{2}$ , which does not match the optimal scaling. This is actually due to our usage of QSVT to block-encode matrices inverses. If we adopt the common technique of padding in the quantum linear system solver, we can easily improve the current scaling into $T^{3/2}$ . However, for simplicity, we do not implement these standard improvements in this work, as they does not affect our end-to-end complexity result. ", "page_idx": 18}, {"type": "text", "text": "D Quantum eigenvalue transformation for linear differential equations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Here we exploit a technique known as quantum eigenvalue transformation (QEVT) by Low and Su [42] to simulate linear dynamics ${\\dot{x}}=A x$ . The algorithm (QEVT) requires that the dynamics is stable under $A+A^{\\top}\\leq0$ , which is stronger than $\\mathcal{A}$ is Hurwitz stable. ", "page_idx": 18}, {"type": "text", "text": "Notice that $A+A^{\\top}\\leq0$ is the stability under the common 2-norm, and $\\mathcal{A}$ satisfying the Lyapunov equation $\\boldsymbol{\\mathcal{A}}\\boldsymbol{X}+\\boldsymbol{X}\\boldsymbol{\\mathcal{A}}^{\\intercal}\\leq\\boldsymbol{0}$ (16) indicates the stability under the inner product induced by the matrix $X$ . We hence follow [42], use a sequence of polynomial functions (known as the Faber polynomials) to approximate the time-evolution operator $e^{\\mathcal{A}t}$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma 18 (Faber truncation of matrix exponentials, [42, Lemma 27]). Suppose we have a matrix $A$ where its numerical range $\\mathcal{W}(A):=\\{\\langle\\psi|A|\\psi\\rangle|\\||\\psi\\rangle\\|=1\\}$ is enclosed by a Faber region $\\boldsymbol{\\varepsilon}$ with associated conformal maps $\\Phi:\\mathcal{E}^{c}\\rightarrow\\mathcal{D}^{c}$ , $\\Psi:{\\mathcal{D}}^{c}\\rightarrow{\\mathcal{E}}^{c}$ and Faber polynomials $F_{s}(z)$ . Given $t>0$ , let $\\begin{array}{r}{e^{t z}=\\sum_{j=0}^{\\infty}\\beta_{j}F_{j}(z)}\\end{array}$ be the Faber expansion of the complex exponential function $e^{t z}$ . Assume that $\\boldsymbol{\\xi}$ is convex and symmetric with respect to the real axis, lying on the left half of the complex plane, then for sufficiently large $s$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|e^{A t}-\\sum_{k=0}^{s-1}\\beta_{j}F_{j}(A)\\right\\|=\\mathcal{O}\\left(\\left(\\frac{c t}{s}\\right)^{s}\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where c is a constant determined by the conformal maps. ", "page_idx": 18}, {"type": "text", "text": "By Lemma 18, given any $\\varepsilon>0$ , it suffices to choose $s\\sim\\mathcal{O}(t\\log(\\frac{1}{\\varepsilon}))$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|e^{A t}-\\sum_{k=0}^{s-1}\\beta_{j}F_{j}(A)\\right\\|\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Definition 7. For some matrix $A$ of size $N\\times N$ with its normalization factor $\\alpha_{A}$ , denote the identity matrix of size $N\\times N$ as $I_{N}$ , and ", "page_idx": 18}, {"type": "equation", "text": "$$\nL_{N}=\\left[\\begin{array}{c c c c c c}{{0}}&{{}}&{{}}&{{}}&{{}}&{{}}\\\\ {{1}}&{{0}}&{{}}&{{}}&{{}}&{{}}\\\\ {{}}&{{1}}&{{0}}&{{}}&{{}}&{{}}\\\\ {{}}&{{}}&{{1}}&{{0}}&{{}}&{{}}\\\\ {{}}&{{}}&{{\\ddots}}&{{\\ddots}}&{{}}&{{}}\\\\ {{}}&{{}}&{{}}&{{}}&{{1}}&{{0}}\\end{array}\\right]_{N\\times N}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "as the lower shift matrix of size $N\\times N$ . We then define ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{PAD}(A):=\\!|0\\rangle\\!\\langle0|\\otimes\\left(L_{N}\\Psi(L_{N}^{-1})\\otimes I_{N}-L_{N}\\otimes\\frac{A}{\\alpha_{A}}\\right)}\\\\ &{~~~~~~~~~~~~~~~+|1\\rangle\\!\\langle0|\\otimes|0\\rangle\\!\\langle N-1|\\otimes(-I_{N})+|1\\rangle\\!\\langle1|\\otimes(I_{N}-L_{N})\\otimes I_{N},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{PAD}(B):=|0\\rangle\\langle0|\\otimes\\Psi^{\\prime}(L_{N}^{-1})\\otimes I_{N}+|1\\rangle\\langle1|\\otimes I_{N}\\otimes I_{N}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that the conformal map $\\Psi$ has its Laurent expansion, and so are $\\Psi^{\\prime}(\\omega)$ and $\\omega\\Psi(\\omega^{-1})$ . Furthermore, $\\Psi^{\\prime}(\\omega)$ and $\\omega\\Psi(\\omega^{-1})$ only contains terms with non-negative exponents. Thus $\\Psi\\big(L_{N}^{-1}\\big)$ and $\\Psi^{\\prime}(L_{N}^{-1})$ are well-defined even though $L_{N}^{-1}$ is not invertible itself. ", "page_idx": 18}, {"type": "text", "text": "With the matrices $\\mathrm{PAD}(A)$ and $\\mathrm{PAD}(B)$ , we can compute the matrix $\\operatorname{PAD}(A)^{-1}\\operatorname{PAD}(B)$ , say ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{PAD}(A)^{-1}\\mathrm{PAD}(B)=\\left[\\begin{array}{c c c c c c c c}{F_{0}(\\frac{A}{\\alpha A})}&{0}&{\\cdots}&{0}&{\\vdots}&{0}&{\\cdots}&{0}\\\\ {F_{1}(\\frac{A}{\\alpha A})}&{F_{0}(\\frac{A}{\\alpha A})}&{\\ddots}&{\\vdots}&{\\vdots}&{0}&{\\cdots}&{0}\\\\ {\\vdots}&{\\ddots}&{\\ddots}&{0}&{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {F_{s-1}(\\frac{A}{\\alpha A})}&{F_{n-2}(\\frac{A}{\\alpha A})}&{\\cdots}&{F_{0}(\\frac{A}{\\alpha A})}&{0}&{\\cdots}&{0}\\\\ {\\cdots}&{\\cdots}&{\\cdots}&{\\cdots}&{\\cdots}&{\\vdots}&{0}&{\\cdots}&{\\cdots}\\\\ {\\frac{A}{F_{s-1}}\\mathrm{-}\\mathrm{\\alpha}(\\frac{A}{\\alpha A})^{-\\cdots}\\bar{F}_{s-2}\\mathrm{\\alpha}(\\frac{A}{\\alpha A})}&{\\cdots}&{\\cdots}&{\\bar{F}_{0}(\\frac{A}{\\alpha A})^{-1}\\bar{F}^{-1}0}&{\\cdots}&{\\cdots}&{0}\\\\ {F_{s-1}(\\frac{A}{\\alpha A})}&{F_{s-2}(\\frac{A}{\\alpha A})}&{\\cdots}&{F_{0}(\\frac{A}{\\alpha A})}&{\\vdots}&{I}&{\\cdots}&{0}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\ddots}&{0}\\\\ {F_{s-1}(\\frac{A}{\\alpha A})}&{F_{s-2}(\\frac{A}{\\alpha A})}&{\\cdots}&{F_{0}(\\frac{A}{\\alpha A})}&{I}&{I}&{\\cdots}&{I}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that this matrix has $(2s)\\times(2s)$ blocks, and each block of size $N\\times N$ . ", "page_idx": 19}, {"type": "text", "text": "Definition 8. For a matrix $A$ whose numerical range is enclosed by a Faber region $\\boldsymbol{\\varepsilon}$ with associated conformal maps $\\Phi:\\mathcal{E}^{c}\\rightarrow\\mathcal{D}^{c},\\Psi:\\mathcal{D}^{c}\\rightarrow\\mathcal{E}^{c}$ and Faber polynomials $F_{s}(z)$ , consider some Faber expansion till $\\boldsymbol{s}$ -th order, we then define ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\rho\\geq\\operatorname*{max}_{j=1,\\cdots,s}\\left\\|{\\frac{F_{j}^{\\prime}({\\frac{A}{\\alpha_{A}}})}{j}}\\right\\|\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "as an upper bound on the derivative of Faber polynomials. ", "page_idx": 19}, {"type": "text", "text": "Lemma 19. We are able to have a $(\\xi,\\varepsilon)$ -block-encoding to $(P A D(A))^{-1}$ using $O(\\xi\\log{\\left(\\frac{1}{\\varepsilon}\\right)})$ queries to the block-encoding of $A$ and $\\widetilde O(1)$ additional elementary gates. Here $\\xi\\sim\\mathcal{O}(\\rho s)$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Since we know that $\\mathrm{PAD}(A)$ can be block-encoded with an $\\mathcal{O}(1)$ normalization factor with arbitrary precision using a single query to the block-encoding of $A$ and a polylogarithmic number of elementary gates, and $\\|(\\operatorname{PAD}(A))^{-1}\\|={\\mathcal{O}}(s\\rho)$ , the rest of the proof is done by Lemma 12. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Theorem 20 (Modified version of Theorem 10. [42]). Let matrix $A$ have its numerical range $\\mathcal{W}(A):=\\{\\langle\\psi|A|\\psi\\rangle|\\||\\psi\\rangle\\|=1\\}$ enclosed by a Faber region $\\mathcal{E}$ with associated conformal maps $\\Phi:\\mathcal{E}^{c}\\rightarrow\\mathcal{D}^{c},\\Psi:\\mathcal{D}^{c}\\rightarrow\\mathcal{E}^{c}$ and Faber polynomials $F_{s}(z)$ . Let $\\begin{array}{r}{p(z)=\\sum_{k=0}^{s-1}\\beta_{k}F_{k}(z)}\\end{array}$ be the Faber expansion of a degree- $\\left(s-1\\right)$ polynomial $p$ and $\\begin{array}{r}{O_{\\beta}|0\\rangle=\\sum_{k=0}^{s-1}\\beta_{k}|n-1-k\\rangle/\\|\\beta\\|}\\end{array}$ be the oracle preparing the coefficients. Then, we can construct $a$ $\\left(\\xi^{\\prime}\\|\\beta\\|,\\|\\beta\\|\\varepsilon\\right)$ -block-encoding of $\\textstyle\\sum_{k=0}^{s-1}\\beta_{k}F_{k}(A/\\alpha_{A})$ using ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\rho s\\log\\left(\\frac{1}{\\varepsilon}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "queries to $O_{A}$ , one query to $O_{\\beta}$ and $\\widetilde O(1)$ additional gate. Here, $\\xi^{\\prime}\\le\\mathcal{O}(\\rho s)$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. First, we observe that ", "page_idx": 19}, {"type": "equation", "text": "$$\n(\\langle0|\\otimes\\langle0|\\otimes I)\\left(\\mathrm{PAD}(A)^{-1}\\mathrm{PAD}(B)(X\\otimes O_{\\beta}\\otimes I)\\right)(|0\\rangle\\otimes|0\\rangle\\otimes I)=\\sum_{k=0}^{s-1}\\frac{\\beta_{k}}{||\\beta||}F_{k}(A/\\alpha_{A}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The $X$ is the quantum Pauli-X gate. Here the circuit has 3 registers, the first one only has one qubit, the second has $\\log_{2}s$ qubits, and the third one matches the size of $A$ . ", "page_idx": 19}, {"type": "text", "text": "By [42, Theorem 9], we can implement a block-encoding of PAD $(A)$ using a single query to the block-encoded matrix $A/\\alpha_{A}$ . Also, given a constant ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\rho\\geq\\operatorname*{max}_{j=1,\\cdots,s}\\left\\|\\frac{F_{j}^{\\prime}(\\frac{A}{\\alpha_{A}})}{j}\\right\\|,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "we know that $\\|\\operatorname{PAD}(A)^{-1}\\|={\\mathcal{O}}(\\rho s)$ . By Lemma 19, we can construct a $(\\xi,\\varepsilon)$ -block-encoding of $\\operatorname{PAD}(A)^{-1}$ using ${\\mathcal{O}}(\\xi\\log(1/\\epsilon))$ queries to the block-encoding of $A/\\alpha_{A}$ . ", "page_idx": 19}, {"type": "text", "text": "Also, assume we can implement a block-encoding of $\\mathrm{PAD}(B)$ with a $O(1)$ normalization factor. Combining these two block-encodings together, we obtain a $(\\xi^{\\prime},\\varepsilon)$ -block-encoding of $\\operatorname{PAD}(A)^{-1}\\operatorname{PAD}(B)$ , where $\\xi^{\\prime}\\le\\mathcal{O}(\\rho s)$ . Then, by (62), the block-encoding we implemented turns out to be a $(\\xi^{\\prime}\\|\\beta\\|,\\|\\beta\\|\\varepsilon)$ -block-encoding of $\\textstyle\\sum_{k=0}^{s-1}\\beta_{k}F_{k}(A/\\alpha_{A})$ . \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Theorem 21 (Block-encoding of matrix exponential). Suppose that $A\\in\\mathbb{R}^{m\\times n}$ is a Hurwitz matrix, and $O_{A}$ is a $(\\alpha_{A},0)$ -block-encoding of $A$ . Then we can implement $a$ $(\\zeta t,\\varepsilon)$ -blockencoding of $e^{\\b{A}t}$ using ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\alpha_{A}t\\,\\mathrm{poly}\\log\\left(\\frac{1}{\\varepsilon}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "queries to $O_{A}$ , one query to a state preparation oracle $O_{\\beta}$ and $\\widetilde O(1)$ additional gate. Here $\\zeta=\\mathcal{O}(\\alpha_{A}\\,\\mathrm{poly}\\log(1/\\epsilon))$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. First we use Lemma 18 to $e^{\\alpha_{A}(A/\\alpha_{A})t}$ , then we know the polynomial should be of degree $\\begin{array}{r}{\\mathcal{O}(\\alpha_{A}t\\log\\left(\\frac{1}{\\varepsilon_{1}}\\right))}\\end{array}$ in order to have a $\\varepsilon_{1}$ accuracy. Then leverage Theorem 20, we need $\\begin{array}{r}{\\mathcal{O}\\left(\\rho\\left(\\alpha_{A}t\\log\\left(\\frac{1}{\\varepsilon_{1}}\\right)\\right)\\log\\left(\\frac{1}{\\varepsilon_{2}}\\right)\\right)}\\end{array}$ queries to $O_{A}$ to construct a block-encoding of the polynomial. Since the Faber polynomial expansion of $e^{\\b{A}t}$ converges absolutely, without loss of generality, .assume $\\|\\beta\\|$ is $O(1)$ . Choosing $\\begin{array}{r}{\\varepsilon_{1}=\\frac{\\varepsilon}{2}}\\end{array}$ and $\\begin{array}{r}{\\varepsilon_{2}=\\frac{\\varepsilon}{2\\left\\|\\beta\\right\\|}}\\end{array}$ , we have a $(\\zeta t,\\varepsilon)$ -block-encoding of $e^{\\b{A}t}$ ", "page_idx": 20}, {"type": "text", "text": "E Proof of Theorem 4 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Lemma 22 (Select oracle). Suppose that $\\mathcal{A}$ is Hurwitz, and $O_{A}$ is an $(\\alpha,0)$ -block-encoding of $\\mathcal{A}$ . Let $K$ be a positive integer and $\\tau,\\varepsilon>0$ are two real-valued scalars. The select oracle as defined in Definition 4 can be implemented using ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{O}}\\left(\\alpha\\rho\\tau K\\log(K)\\cdot\\mathrm{poly}\\log\\left(\\frac{1}{\\varepsilon}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "queries to controlled $O_{A}$ , and $\\tilde{\\mathcal{O}}\\left(\\alpha\\rho\\tau K\\log(K)\\cdot\\mathrm{poly}\\log\\left(\\frac{1}{\\varepsilon}\\right)\\right)$ queries to other additional elementary gates. ", "page_idx": 20}, {"type": "text", "text": "Proof. Firstly, $t_{k}$ here attains $\\begin{array}{r}{t_{k}\\,=\\,\\frac{k\\tau}{K}}\\end{array}$ . By Theorem 17, for each $k\\,=\\,0,\\ldots,K$ , we can implement a $(\\zeta t_{k},\\varepsilon)$ -block-encoding of $e^{A t_{k}}$ using $\\tilde{\\mathcal{O}}(\\alpha\\rho t_{k}\\cdot\\mathrm{poly}\\log(1/\\varepsilon))$ queries to $O_{A}$ . We denote this block-encoding as $U_{k}$ . Notice that the  select oracle $\\mathrm{select}({\\mathcal{A}},{\\varepsilon})$ can be represented by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname{select}(A,\\varepsilon)=\\prod_{k=0}^{K}{\\Big[}(\\mathbb{I}-|k\\rangle\\langle k|)\\otimes\\mathbb{I}+|k\\rangle\\langle k|\\otimes U_{k}{\\Big]},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where each unitary operator $[(\\mathbb{I}-|k\\rangle\\langle k|)\\otimes\\mathbb{I}+|k\\rangle\\langle k|\\otimes U_{k}]$ in this product is a $k$ -controlled version of $U_{k}$ that can be implemented with ${\\tilde{\\mathcal{O}}}(\\alpha\\rho t_{k}\\log(K)\\cdot\\mathrm{poly}\\log(1/\\varepsilon))$ queries to the controlled $O_{A}$ and other additional gates. Th erefore, the overall query complexity is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{O}}\\left(\\alpha\\rho\\tau\\left(\\sum_{k=0}^{K}t_{k}\\right)\\log(K)\\cdot\\mathrm{poly}\\log\\left(\\frac{1}{\\varepsilon}\\right)\\right)\\le\\tilde{\\mathcal{O}}\\left(\\alpha\\rho\\tau K\\log(K)\\cdot\\mathrm{poly}\\log\\left(\\frac{1}{\\varepsilon}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last step follows from $\\begin{array}{r}{\\sum_{k=0}^{K}t_{k}=\\sum_{k=0}^{K}\\frac{k\\tau}{K}=\\frac{(K+1)\\tau}{2}}\\end{array}$ . ", "page_idx": 20}, {"type": "text", "text": "In the following proof, we will need a state preparation oracle ", "page_idx": 20}, {"type": "equation", "text": "$$\nO_{\\lambda}|0\\rangle=\\frac{1}{\\sqrt{\\|\\lambda\\|_{1}}}\\sum_{k=0}^{K}\\sqrt{\\lambda_{k}}|k\\rangle,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where \u03bbk = wkk2, \u2225\u03bb\u22251 = kK=0 |\u03bbk| and wk = ( 2\u22121k2=K0,K)\u03c4. Since \u03bbk can be expressed as a smooth function of $k$ for $k\\in\\{0,1,\\cdot\\cdot\\cdot,K\\}$ , this state preparation oracle can be implemented in $\\mathcal{O}(\\mathrm{poly}\\log(K))$ cost. ", "page_idx": 21}, {"type": "text", "text": "Now, we are ready to prove Theorem 4. ", "page_idx": 21}, {"type": "text", "text": "Proof. The global error of the trapezoidal rule (see (20)) is proportional to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\xi\\in[0,\\tau]}\\frac{\\tau^{3}}{K^{2}}|F^{\\prime\\prime}(\\xi)|,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where ", "page_idx": 21}, {"type": "equation", "text": "$\\begin{array}{r}{|F^{\\prime\\prime}(\\xi)|=\\left|A^{2}F(\\xi)+2A F(\\xi)A^{\\top}+F(\\xi)(A^{\\top})^{2}\\right|\\leq4\\|A\\|^{2}\\|\\Omega\\|}\\end{array}$ ", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, to achieve precision $\\varepsilon_{1}$ , the total number of quadrature points is ", "page_idx": 21}, {"type": "equation", "text": "$$\nK=\\mathcal{O}\\left(\\frac{\\tau^{3/2}\\alpha\\eta^{1/2}}{\\varepsilon_{1}^{1/2}}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now, we consider the following two select oracles, as defined in Definition 4: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname{select}(\\mathcal A,\\varepsilon_{2}):=\\sum_{k=0}^{K}|k\\rangle\\langle k|\\otimes U_{k},\\quad\\mathrm{select}(\\mathcal A^{\\top},\\varepsilon_{2}):=\\sum_{k=0}^{K}|k\\rangle\\langle k|\\otimes V_{k},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $U_{k}$ (or $V_{k}$ ) is a $(\\zeta t_{k},\\varepsilon_{2})$ -block-encoding of $e^{\\mathcal{A}t_{k}}$ (or $\\boldsymbol{e}^{\\boldsymbol{A}^{\\intercal}t_{k}}$ ) for $0\\le k\\le K$ . Recall that $O_{\\Omega}$ is a $(\\eta,0)$ -block-encoding of $\\Omega$ , it turns out that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname{select}(A,\\varepsilon)(I\\otimes O_{\\Omega})\\mathrm{select}(A^{\\top},\\varepsilon)=\\sum_{k=0}^{K}|k\\rangle\\!\\langle k|\\otimes W_{k},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $W_{k}:=U_{k}U_{\\Omega}V_{k}$ is a $(\\eta\\zeta^{2}t_{k}^{2},2\\zeta t_{k}\\eta\\varepsilon_{2})$ -block-encoding of the matrix $\\mathcal{F}(t_{k})$ . ", "page_idx": 21}, {"type": "text", "text": "Let $O_{\\lambda}$ be the state preparation oracle defined in (68). By the LCU technique [22, Lemma 52], we can implement a $\\begin{array}{r}{(\\gamma,\\sum_{k=0}^{K}2|w_{k}|\\eta\\zeta t_{k}\\varepsilon_{2})}\\end{array}$ -block-encoding (where $\\begin{array}{r}{\\gamma:=\\sum_{k=0}^{K}|w_{k}|\\zeta^{2}t_{k}^{2}\\eta)}\\end{array}$ ionf $\\begin{array}{r}{\\sum_{k=0}^{K}w_{k}e^{A t_{k}}\\Omega e^{A^{\\top}t_{k}}}\\end{array}$ usin g a single query to select $(A,\\varepsilon_{2})$ , $\\operatorname{select}(A^{\\top},\\varepsilon_{2})$ , $U_{\\Omega}$ , $O_{\\lambda}$ and its ", "page_idx": 21}, {"type": "text", "text": "Notice that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{k=0}^{K}|w_{k}|t_{k}\\le\\sum_{k=0}^{K}\\frac{\\tau}{K}\\frac{k\\tau}{K}=\\mathcal{O}(\\tau^{2}),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "thus ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{k=0}^{K}2|w_{k}|\\eta\\zeta t_{k}\\varepsilon_{2}=\\widetilde{\\mathcal{O}}\\left(\\tau^{2}\\eta\\zeta\\varepsilon_{2}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Given a positive scalar $\\varepsilon>0$ , and set the integration time ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\tau=\\kappa\\log\\biggl(\\frac{3\\|X^{*}\\|\\kappa}{\\varepsilon\\|w\\|_{1}\\lambda_{\\operatorname*{min}}(X^{*})}\\biggr),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\varepsilon^{\\delta}\\mathrm{poly}\\left(\\log\\left(\\frac{1}{\\varepsilon}\\right)\\right)\\rightarrow0,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "then if we choose ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\varepsilon_{1}=\\frac{\\varepsilon}{3},\\quad\\varepsilon_{2}=\\mathcal{O}(\\varepsilon^{1+\\delta}),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\delta>0$ is a constant positive number. By Lemma 3 and the error analysis above, we implement a $(\\gamma,\\varepsilon)$ -block-encoding of $X^{*}$ , with ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\gamma=\\sum_{k=0}^{K}|w_{k}|\\zeta^{2}t_{k}^{2}\\eta=\\mathcal{O}(\\zeta^{2}\\eta\\tau^{3})=\\widetilde{\\mathcal{O}}(\\alpha^{2}\\rho^{2}\\kappa^{3}\\eta).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "It follows that the overall cost of this block-encoding is ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{O}}\\left(\\frac{\\alpha^{2}\\rho\\kappa^{5/2}\\eta^{1/2}}{\\varepsilon^{1/2}}\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "queries to $O_{A}$ and same queries to other additional elementary gates. ", "page_idx": 22}, {"type": "text", "text": "F Quantum trace estimation ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Here we show how to estimate the trace of a positive definite block-encoded matrix. The main result is in Theorem 25. ", "page_idx": 22}, {"type": "text", "text": "Lemma 23. Let $U_{H}$ be an $(\\alpha,m,\\varepsilon_{H})$ -block-encoding of a positive-definite Hermitian matrix $H\\in\\mathbb{C}^{n\\times n}$ and let $\\varepsilon\\in\\left(0,\\frac{1}{3}\\right]$ . Let $\\lambda_{\\operatorname*{min}}>0$ be a known lower bound on the smallest eigenvalue of $H$ . Provided that $\\begin{array}{r}{\\varepsilon_{H}\\leq\\mathcal{O}\\left(\\frac{\\lambda_{\\operatorname*{min}}\\varepsilon^{2}}{\\log\\left(1/\\varepsilon\\right)}\\right)}\\end{array}$ is sufficiently small (where $\\varepsilon>0$ is a given number), we can construct a (1, m+2, \u03b5)-block-encoding of H3/6\u03b1 using $\\begin{array}{r}{\\mathcal{O}\\left(\\frac{\\alpha}{\\lambda_{\\mathrm{min}}}\\log(1/\\varepsilon)\\right)}\\end{array}$ applications of $U_{H}$ and $U_{H}^{\\dag}$ , a single application of controlled- $U_{H}$ , and \u03b1(\u03bbmmi+n1)log(1/\u03b5) other oneand two-qubit gates. The description of this block-encoding circuit can be computed classically in time $\\begin{array}{r}{\\mathcal{O}\\left(\\mathrm{poly}\\left(\\frac{\\alpha}{\\lambda_{\\mathrm{min}}}\\log(1/\\varepsilon)\\right)\\right)}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. Define $\\widetilde{\\lambda}_{\\mathrm{min}}:=\\,\\lambda_{\\mathrm{min}}/\\alpha$ , such that $\\widetilde\\lambda_{\\mathrm{min}}I\\,\\preceq\\,H/\\alpha\\,\\preceq\\,I$ . We first find a polynomial approximation of $f(x):=\\sqrt{\\frac{x}{36}}$ over $x\\in[\\widetilde{\\lambda}_{\\operatorname*{min}},1]$ using [22, Corollary 66]. To use this corollary, let $x_{0}:=1$ , $r:=1-\\widetilde{\\lambda}_{\\operatorname*{min}}$ , $\\delta:=\\widetilde{\\lambda}_{\\mathrm{min}}$ , and observe that $\\begin{array}{r}{f(x_{0}\\!+\\!x)=\\frac{1}{6}\\sqrt{1+x}=\\frac{1}{6}\\sum_{\\ell=0}^{\\infty}\\binom{1/2}{\\ell}x^{\\ell}}\\end{array}$ whenever $|x|\\,\\le\\,r+\\delta\\,=\\,1$ . Also note that $\\begin{array}{r}{\\frac16\\sum_{\\ell=0}^{\\infty}\\left|\\binom{1/2}{\\ell}\\right|\\,\\le\\,\\frac13\\,=:\\,B}\\end{array}$ . Then there is an efficiently computable polynomial $P_{0}\\,\\in\\,\\mathbb{C}[x]$ of degree $\\begin{array}{r}{d\\,=\\,\\mathcal{O}\\left(\\frac{1}{\\widetilde{\\lambda}_{\\operatorname*{min}}}\\log(1/\\varepsilon)\\right)}\\end{array}$ such that $\\|f(x)-P_{0}(x)\\|\\le\\frac{\\varepsilon}{2}$ for $x\\in[\\widetilde{\\lambda}_{\\operatorname*{min}},1]$ and $\\begin{array}{r}{\\|P_{0}(x)\\|\\le\\frac{\\varepsilon}{2}+B\\le\\frac{1}{2}}\\end{array}$ for $x\\in[-1,1]$ . It follows that defining $P(x):=\\mathrm{Re}\\left(P_{0}(x)\\right)$ gives $|f(x)-P(x)|_{[\\widetilde{\\lambda}_{\\operatorname*{min}},1]}\\le\\frac{\\varepsilon}{2}$ and $|P(x)|_{[-1,1]}\\leq\\frac{1}{2}$ . ", "page_idx": 22}, {"type": "text", "text": "Next, we use [22, Theorem 56] to construct construct a unitary $V$ that is a $\\left(1,m+2,\\varepsilon/2\\right)$ - encoding of $P(H/\\alpha)$ with the desired complexity, using the promise that $\\varepsilon H$ is sufficiently small to satisfy $4d\\sqrt{\\varepsilon_{H}/\\alpha}\\leq\\varepsilon/4$ . $V$ is then a $(1,m+2,\\varepsilon)$ -block-encoding of $\\sqrt{\\frac{H/\\alpha}{36}}$ since ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\sqrt{\\cfrac{H/\\alpha}{36}}-(\\mathbf{\\Delta}\\langle\\boldsymbol{0}|^{\\otimes m+2}\\otimes I)V(|\\boldsymbol{0}\\rangle^{\\otimes m+2}\\otimes I)\\right\\|}\\\\ &{\\leq\\left\\|\\sqrt{\\cfrac{H/\\alpha}{36}}-P(H/\\alpha)\\right\\|+\\left\\|P(H/\\alpha)-(\\langle\\boldsymbol{0}|^{\\otimes m+2}\\otimes I)V(|\\boldsymbol{0}\\rangle^{\\otimes m+2}\\otimes I)\\right\\|}\\\\ &{\\leq\\|g(H/\\alpha)-P(H/\\alpha)\\|+\\frac{\\mathcal{E}}{2}=\\|\\mathrm{diag}(g(\\lambda_{H}/\\alpha))-\\mathrm{diag}(P(\\lambda_{H}/\\alpha))\\|+\\frac{\\mathcal{E}}{2}}\\\\ &{=\\underset{\\lambda_{i}\\in\\lambda_{H}}{\\operatorname*{max}}|g(\\lambda_{i}/\\alpha)-P(\\lambda_{i}/\\alpha)|+\\frac{\\mathcal{E}}{2}\\leq\\frac{\\mathcal{E}}{2}+\\frac{\\mathcal{E}}{2}=\\varepsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\lambda_{H}\\in\\mathbb{R}^{n}$ is the vector of eigenvalues of $H$ . ", "page_idx": 22}, {"type": "text", "text": "Lemma 24. Let $\\mu~>~0$ . Let $A$ be an $n$ -by-n Hermitian matrix such that $0\\ \\preceq\\ A\\ \\preceq$ $I$ and let $\\widetilde{V}$ be a $(1,m,\\mu/3)$ -block-encoding of $\\sqrt{A}$ . Then there exists $\\widetilde{U}_{A}$ such that $\\begin{array}{r}{\\left|\\left\\|(\\langle0|\\otimes I)\\widetilde{U}_{A}|0\\dots0\\rangle\\right\\|^{2}-\\frac{\\mathrm{Tr}(A)}{n}\\right|\\leq\\mu}\\end{array}$ that uses $\\mathit{1}$ query to $\\Tilde{V}$ and ${\\mathcal{O}}(\\log n)$ additional gates. ", "page_idx": 22}, {"type": "text", "text": "Proof. Our proof is similar to the proof \u221aof [55, Lemma 13]. The idea is to first prepare the maximally\u221a entangled state $\\textstyle\\sum_{i=1}^{n}|i\\rangle|i\\rangle/{\\sqrt{n}}$ (which requires ${\\mathcal{O}}(\\log n)$ gates) and then apply the map $\\sqrt{A}$ to the first register. We can assume without loss of generality that $\\mu\\leq1$ , otherwise the statement is trivial. ", "page_idx": 22}, {"type": "text", "text": "Note that $\\widetilde{V}_{0}:=(\\langle0|^{\\otimes m}\\otimes I)\\widetilde{V}(|0\\rangle^{\\otimes m}\\otimes I)$ is a $\\mu/3$ -approximation of $\\sqrt{A}$ by definition. We are interested in the probability $p$ of measuring $0^{\\otimes m}$ in the first register after applying $\\Tilde{V}$ . ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle p:=\\left\\|(\\langle0|^{\\otimes m}\\otimes I)\\widetilde{V}(|0\\rangle^{\\otimes m}\\otimes I)\\sum_{i=1}^{n}\\frac{|i\\rangle|i\\rangle}{\\sqrt{n}}\\right\\|^{2}=\\left\\|\\widetilde{V}_{0}\\sum_{i=1}^{n}\\frac{|i\\rangle|i\\rangle}{\\sqrt{n}}\\right\\|^{2}}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle=\\frac{1}{n}\\sum_{i=1}^{n}\\langle i|\\widetilde{V}_{0}^{\\dagger}\\widetilde{V}_{0}|i\\rangle=\\frac{1}{n}\\,\\mathrm{Tr}\\Big(\\widetilde{V}_{0}^{\\dagger}\\widetilde{V}_{0}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "It remains to show that $p$ is a good approximation of $\\operatorname{Tr}(A)/n$ . For this, we show $\\widetilde{V}_{0}^{\\dagger}\\widetilde{V}_{0}\\approx A$ . Note that for all matrices $B,\\widetilde{B}$ with $\\|B\\|\\leq1$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|B^{\\dagger}B-\\widetilde{B}^{\\dagger}\\widetilde{B}\\right\\|=\\left\\|(B^{\\dagger}-\\widetilde{B}^{\\dagger})B+B^{\\dagger}(B-\\widetilde{B})-(B^{\\dagger}-\\widetilde{B}^{\\dagger})(B-\\widetilde{B})\\right\\|}\\\\ &{\\qquad\\qquad\\leq\\left\\|(B^{\\dagger}-\\widetilde{B}^{\\dagger})B\\right\\|+\\left\\|B^{\\dagger}(B-\\widetilde{B})\\right\\|+\\left\\|(B^{\\dagger}-\\widetilde{B}^{\\dagger})(B-\\widetilde{B})\\right\\|}\\\\ &{\\qquad\\qquad\\leq\\left\\|B^{\\dagger}-\\widetilde{B}^{\\dagger}\\right\\|\\left\\|B\\right\\|+\\left\\|B^{\\dagger}\\right\\|\\left\\|B-\\widetilde{B}\\right\\|+\\left\\|B^{\\dagger}-\\widetilde{B}^{\\dagger}\\right\\|\\left\\|B-\\widetilde{B}\\right\\|}\\\\ &{\\qquad\\qquad\\leq2\\left\\|B-\\widetilde{B}\\right\\|+\\left\\|B-\\widetilde{B}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Using equation (81) along with equation (82) (letting $B={\\sqrt{A}}$ and $\\widetilde{B}=\\widetilde{V_{0}}$ ), we see ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|\\frac{\\mathrm{Tr}(A)}{n}-p\\right|=\\frac{1}{n}\\left|\\mathrm{Tr}\\Big(A-\\widetilde{V}_{0}^{\\dagger}\\widetilde{V}_{0}\\Big)\\right|\\le\\left\\|A-\\widetilde{V}_{0}^{\\dagger}\\widetilde{V}_{0}\\right\\|\\le2\\left(\\frac{\\mu}{3}\\right)+\\left(\\frac{\\mu}{3}\\right)^{2}\\le\\mu.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Theorem 25. Let $U_{H}$ be an $(\\alpha,m,\\varepsilon)$ -block-encoding of a Hermitian matrix $H\\,\\in\\,\\mathbb{C}^{n\\times n}$ , where $U_{H}$ can be implemented using $T_{H}$ elementary gates. Suppose $H\\succ0$ and let $\\lambda_{\\operatorname*{min}}>0$ be a known lower bound on the smallest eigenvalue of $H$ . Then, with probability at least $4/5$ , we can estimate $\\operatorname{Tr}(H)$ to within multiplicative error $\\theta\\in(0,1]$ in cost ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\frac{\\alpha^{3/2}T_{H}}{\\theta\\lambda_{\\mathrm{min}}^{3/2}}\\log\\!\\left(\\frac{\\alpha}{\\theta\\lambda_{\\mathrm{min}}}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "provided that $\\begin{array}{r}{\\varepsilon\\leq{\\mathcal O}\\left(\\frac{\\lambda_{\\operatorname*{min}}^{3}\\theta^{2}}{\\alpha^{2}\\log\\left(\\alpha/\\theta\\lambda_{\\operatorname*{min}}\\right)}\\right)}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "Proof. Our proof is similar to the proof of [55, Corollary 10]. Let $A:=\\frac{H/\\alpha}{36}$ and note that since $H/\\alpha\\succeq\\lambda_{\\mathrm{min}}I/\\alpha$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\frac{\\operatorname{Tr}(A)}{n}}:={\\frac{\\operatorname{Tr}(H/\\alpha)}{36n}}\\geq{\\frac{\\lambda_{\\operatorname*{min}}}{36\\alpha}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let $\\widetilde{U}_{A}$ be a $(1,m+2,\\theta\\lambda_{\\mathrm{min}}/216\\alpha)$ -block-encoding of $\\sqrt{A}$ constructed via Lemma 23. Using Lemma 24, we can construct a unitary circuit $\\Tilde{V}$ such that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|\\left\\|(\\langle0|\\otimes I)\\widetilde{V}|0\\dots0\\rangle\\right\\|^{2}-\\frac{\\operatorname{Tr}(A)}{n}\\right|\\leq\\frac{\\theta\\lambda_{\\operatorname*{min}}}{72\\alpha}\\leq\\frac{\\theta}{2}\\cdot\\frac{\\operatorname{Tr}(A)}{n}\\leq\\frac{\\operatorname{Tr}(A)}{2n}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|(\\langle0|\\otimes I\\rangle\\widetilde V|0\\ldots0\\rangle\\right\\|^{2}\\geq\\frac{\\mathrm{Tr}(A)}{n}-\\frac{\\mathrm{Tr}(A)}{2n}=\\frac{\\mathrm{Tr}(A)}{2n}\\geq\\frac{\\lambda_{\\mathrm{min}}}{72\\alpha}=:p_{\\mathrm{min}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "as well as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|(\\langle0|\\otimes I){\\widetilde{V}}|0\\dots0\\rangle\\right\\|^{2}\\leq{\\frac{\\mathrm{Tr}(A)}{n}}+{\\frac{\\mathrm{Tr}(A)}{2n}}={\\frac{3\\,\\mathrm{Tr}(A)}{2n}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Using the amplitude estimation technique of [55, Lemma 9] with $p_{\\mathrm{min}}\\,=\\,\\lambda_{\\mathrm{min}}/72\\alpha$ and $\\mu=\\theta/3$ gives us a $\\tilde{p}$ that with probability at least $4/5$ satisfies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|\\widetilde{p}-\\left\\|(\\langle0|\\otimes I)\\widetilde{V}|0\\dots0\\rangle\\right\\|^{2}\\right|\\leq\\frac{\\theta}{3}\\left\\|(\\langle0|\\otimes I)\\widetilde{V}|0\\dots0\\rangle\\right\\|^{2}\\leq\\frac{\\theta}{2}\\cdot\\frac{\\mathrm{Tr}(A)}{n}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By combining (86) and (89) and using the triangle inequality, we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left|{\\frac{\\operatorname{Tr}(A)}{n}}-\\widetilde{p}\\right|\\leq\\theta\\cdot{\\frac{\\operatorname{Tr}(A)}{n}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, 36\u03b1np is a multiplicative $\\theta$ -approximation of $\\operatorname{Tr}(H)$ . ", "page_idx": 24}, {"type": "text", "text": "It remains to show that the complexity statement holds. The construction of $\\widetilde{U}_{A}$ via Lemma 23 uses O(TH \u00b7\u03bbm\u03b1in $\\mathcal{O}(T_{H}\\cdot\\,\\frac{\\alpha}{\\lambda_{\\mathrm{min}}}\\log(\\alpha/\\theta\\lambda_{\\mathrm{min}}))$ elementary gates from the applications of $U_{H}$ , $U_{H}^{\\dag}$ , and controlled- $U_{H}$ , and $\\mathcal{O}((m+1)\\frac{\\alpha}{\\lambda_{\\mathrm{min}}}\\log(\\alpha/\\theta\\lambda_{\\mathrm{min}}))$ other one- and two-qubit gates. The construction of $\\tilde{V}$ requires an additional ${\\mathcal{O}}(\\log n)$ gates. The amplitude estimation step requires $\\mathcal{O}(\\sqrt{\\alpha/\\lambda_{\\mathrm{min}}}/\\theta)$ uses of $\\Tilde{V}$ and ${\\widetilde{V}}^{\\dagger}$ , and $\\mathcal{O}(\\sqrt{\\alpha/\\lambda_{\\mathrm{min}}}\\log n/\\theta)$ additional gates. So the overall complexity is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\frac{\\sqrt{\\alpha/\\lambda_{\\mathrm{min}}}}{\\theta}\\left(\\frac{\\alpha T_{H}}{\\lambda_{\\mathrm{min}}}\\log\\!\\left(\\frac{\\alpha}{\\theta\\lambda_{\\mathrm{min}}}\\right)+\\frac{(m+1)\\alpha}{\\lambda_{\\mathrm{min}}}\\log\\!\\left(\\frac{\\alpha}{\\theta\\lambda_{\\mathrm{min}}}\\right)+\\log n\\right)\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Noting that $T_{H}\\geq m$ and $T_{H}\\geq\\log n$ for any non-trivial $U_{H}$ gives the stated bound. ", "page_idx": 24}, {"type": "text", "text": "G Quantum gradient estimation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Lemma 26. Given $K\\in{\\cal S}_{K}(a)$ and $\\|K-K^{*}\\|_{F}>\\varepsilon$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|\\nabla f\\|\\geq c\\varepsilon,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the constant $c=\\sqrt{2\\mu_{f}\\nu\\lambda_{\\mathrm{min}}(R)/a}$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. This lemma follows directly from Lemma 9 and Lemma 10. ", "page_idx": 24}, {"type": "text", "text": "Lemma 27 (Block-encoded $\\nabla f(K))$ . Assume that we have efficient procedures (as described in Assumption 1) to access the problem data $A,B,Q,R$ in $\\mathcal{O}(\\mathrm{poly}\\log(n))$ time. Let $K\\in$ $\\mathcal{S}_{K}(a)$ be a stabilizing policy stored in a quantum-accessible data structure. Given any $\\varepsilon_{b}>0$ , we can implement a $(\\gamma_{\\nabla},\\varepsilon_{b})$ -block-encoding of $\\nabla f(K)$ in cost $\\begin{array}{r}{\\widetilde{\\mathcal{O}}\\left(a^{6}\\rho^{3}\\sqrt{\\frac{\\kappa^{11}}{\\varepsilon_{b}}}\\right)}\\end{array}$ , where $\\gamma_{\\nabla}\\le\\widetilde O(a^{6}\\rho^{4}\\kappa^{6})$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. We know that for the function $f(K)$ defined in (5), the gradient has a closed-form expression: $\\nabla f(K)=2(R K-B^{\\top}P(K))X(K)$ . As shown in the proof of Theorem 5, we can implement a $(\\gamma_{P},\\varepsilon)$ -block-encoding of $P(K)$ in cost $\\begin{array}{r}{\\widetilde{\\mathcal{O}}\\left(a^{3}\\rho\\sqrt{\\frac{\\kappa^{5}}{\\varepsilon}}\\right)}\\end{array}$ , where $\\gamma_{P}\\leq\\tilde{O}(a^{4}\\rho^{2}\\kappa^{3})$ . Similarly, since $X(K)$ is the solution to the Lyapunov equation (assuming $\\Sigma_{0}=\\mathbb{I}$ ) ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\big(A-B K\\big)X+X(A-B K)^{\\top}+\\mathbb I=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "we can implement a $(\\gamma_{X},\\varepsilon)$ -block-encoding of $X(K)$ in cost $\\begin{array}{r}{\\widetilde{\\mathcal{O}}\\left(a^{2}\\rho\\sqrt{\\frac{\\kappa^{5}}{\\varepsilon}}\\right)}\\end{array}$ , where $\\gamma_{X}\\ \\leq$ $\\widetilde{\\mathcal{O}}(a^{2}\\rho^{2}\\kappa^{3})$ . Based on our assumptions on the input procedures and the usage of quantum data structure, we can implement a $\\left(s\\|K\\|_{F},0\\right)$ -block-encoding of $R K$ and a $\\left(s\\gamma_{P},s\\varepsilon\\right)$ -blockencoding of $B^{\\top}P(K)$ . It follows that a $(\\gamma_{\\nabla},\\varepsilon_{b})$ -block-encoding of $\\nabla f(K)$ can be implemented in cost $\\begin{array}{r}{\\widetilde{\\mathcal{O}}\\left(a^{3}\\rho\\sqrt{\\frac{\\kappa^{5}}{\\varepsilon}}\\right)}\\end{array}$ , where ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\gamma_{\\nabla}:=\\frac12s\\gamma_{X}\\bigl(\\gamma_{P}+\\|K\\|_{F}\\bigr)\\leq\\widetilde{O}\\left(a^{6}\\rho^{4}\\kappa^{6}\\right),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and $\\varepsilon_{b}\\leq\\mathcal{O}((a+\\gamma_{P}+\\gamma_{X}\\gamma_{P})\\varepsilon)$ . To achieve the desired precision, we pass the error parameter $\\varepsilon\\rightarrow\\varepsilon/(2(a+\\gamma_{P}+\\gamma_{X}\\gamma_{P}))$ ) to the asymptotic cost. $\\sqsubset$ ", "page_idx": 24}, {"type": "text", "text": "Definition 9. For a matrix $G$ of size $m\\times n$ , we could always write is as ", "page_idx": 25}, {"type": "equation", "text": "$$\nG=[G_{1}\\quad G_{2}\\quad\\cdots\\quad G_{n}]\\,,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $G_{i}$ is a column vector of size $m\\times1$ . Define $|G_{\\mathrm{vec}}\\rangle$ as ", "page_idx": 25}, {"type": "equation", "text": "$$\n|G_{\\mathrm{vec}}\\rangle=\\left[\\begin{array}{c}{G_{1}}\\\\ {G_{2}}\\\\ {\\vdots}\\\\ {G_{n}}\\end{array}\\right]/\\|G\\|_{F}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma 28 (Matrix Vectorization). Let $U_{G}$ be a $(\\gamma_{\\nabla},\\varepsilon)$ -block-encoding of $\\left(\\nabla f(K)\\right)^{\\top}$ , and we denote ", "page_idx": 25}, {"type": "equation", "text": "$$\nG:=\\gamma_{\\nabla}(\\langle0^{r}|\\otimes I)U_{G}(|0^{r}\\rangle\\otimes I),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $r$ is the number of ancilla qubits required. Then, we can prepare a quantum state $|G_{\\mathrm{vec}}\\rangle$ with $\\Omega(1)$ success probability using $\\widetilde{\\mathcal{O}}\\left(\\frac{\\gamma_{\\nabla}\\sqrt{m}}{\\|G\\|_{F}}\\right)$ queries to $U_{G}$ and its inverse. ", "page_idx": 25}, {"type": "text", "text": "Proof. First, from Lemma 27 we know that we have $U_{G}^{\\top}$ that encodes $\\nabla f(K)^{\\top}$ . Notice that $\\nabla f(K)$ is of size $m\\times n$ , so ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{I^{r_{1}}\\otimes(U_{G}^{r_{2},r_{3}})^{\\top}\\left(\\displaystyle\\frac1{\\sqrt{m}}\\displaystyle\\sum_{i=0}^{m-1}|i\\rangle^{r_{1}}\\otimes(|0^{a}\\rangle)^{r_{2}}\\otimes|i\\rangle^{r_{3}}\\right)}\\\\ {\\ \\ =\\displaystyle\\frac1{\\gamma\\nabla\\sqrt{m}}\\displaystyle\\sum_{i=0}^{m-1}|i\\rangle^{r_{1}}\\otimes(|0^{a}\\rangle)^{r_{2}}\\otimes(G^{\\top}|i\\rangle)^{r_{3}}+|\\bot\\rangle}\\\\ {\\ \\ =\\displaystyle\\frac{\\|G\\|_{F}}{\\gamma\\sqrt{m}}\\displaystyle\\sum_{i=0}^{m-1}|i\\rangle^{r_{1}}\\otimes(|0^{a}\\rangle)^{r_{2}}\\otimes(G^{\\top}|i\\rangle)^{r_{3}}+|\\bot\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $r_{1},r_{2},r_{3}$ are the indexes for registers, where the $r_{1}$ register has $\\log_{2}(m)$ qubits and the $r_{3}$ register has $\\log_{2}(n)$ qubits(We can always assume ${\\boldsymbol{r}}n$ and $n$ are the power of 2). The state $|\\bot\\rangle$ contains the state satisfy ", "page_idx": 25}, {"type": "equation", "text": "$$\n(I^{r_{1}}\\otimes\\langle0^{a}|^{r_{2}}\\otimes I^{r_{3}})|{\\bot}\\rangle=0.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By leveraging amplitude amplification, we can get the state $|G_{\\mathrm{vec}}\\rangle$ using $\\mathcal{O}\\big(\\frac{\\gamma_{\\nabla}\\sqrt{m}}{\\|G\\|_{F}}\\big)$ queries to the block-encoding $U_{G}^{\\top}$ . \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Lemma 29 (Gradient entry estimation). Denote the state preparation in Lemma 28 for $|G_{v e c}\\rangle$ as $U_{G_{v e c}}$ , we are able to get the classical $\\mathcal{G}$ with ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\mathcal{G}-|G_{\\mathrm{vec}}\\rangle\\|\\leq\\varepsilon_{r}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "using ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{O}}\\left(\\frac{\\gamma_{\\nabla}}{\\|G\\|_{F}}\\frac{m^{3/2}n}{\\varepsilon_{r}}\\right)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "queries to UG andO $\\widetilde{\\mathcal{O}}\\left(\\frac{\\gamma_{\\nabla}}{\\|G\\|_{F}}\\frac{m^{3/2}n}{\\varepsilon_{r}}\\right)$ additional elementary gates. ", "page_idx": 25}, {"type": "text", "text": "Proof. [7, Theorem 2] indicates that we can get $\\mathcal{G}$ using $\\widetilde{\\mathcal{O}}(\\frac{m n}{\\epsilon_{r}})$ queries to the oracle that prepares $|G_{\\mathrm{vec}}\\rangle$ and $\\begin{array}{r}{\\widetilde O\\left(\\frac{m n}{\\varepsilon_{r}}\\right)}\\end{array}$ additional elementary gates. Since each $U_{G_{\\mathrm{vec}}}$ requires ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{O}}\\left(\\frac{\\gamma_{\\nabla}\\sqrt{m}}{\\Vert G\\Vert_{F}}\\right)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "queries, we know the total cost should be ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{O}}\\left(\\frac{\\gamma_{\\nabla}}{\\|G\\|_{F}}\\frac{m^{3/2}n}{\\varepsilon_{r}}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma 30 (Gradient norm estimation). Suppose we have the block-encoding $U_{G}$ defined in Lemma 27, where ", "page_idx": 26}, {"type": "equation", "text": "$$\nG=\\gamma_{\\nabla}(\\langle0|\\otimes I)U_{G}(|0\\rangle\\otimes I),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "then we are able to estimate $\\|G\\|_{F}$ up an additive error $\\varepsilon_{a}$ using ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\frac{\\gamma_{\\nabla}\\sqrt{m}\\|G\\|_{F}}{\\varepsilon_{a}^{2}+2\\|G\\|_{F}\\varepsilon_{a}}\\right)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "queries to $U_{G}$ and its inverse and also $\\widetilde{\\mathcal{O}}\\left(\\frac{\\gamma_{\\nabla}\\sqrt{m}\\|G\\|_{F}}{\\varepsilon_{a}^{2}+2\\|G\\|_{F}\\varepsilon_{a}}\\right)$ elementary gates. ", "page_idx": 26}, {"type": "text", "text": "Proof. We still need the computation we did in equation 98, say ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{I^{r_{1}}\\otimes(U_{G}^{r_{2},r_{3}})^{\\top}\\left(\\frac{1}{\\sqrt{m}}\\sum_{i=0}^{m-1}|i\\rangle^{r_{1}}\\otimes(|0^{a}\\rangle)^{r_{2}}\\otimes|i\\rangle^{r_{3}}\\right)}}}\\\\ {{\\displaystyle=\\frac{\\|G\\|_{F}}{\\gamma\\nabla}\\frac{\\sum_{i=0}^{m-1}|i\\rangle^{r_{1}}\\otimes(|0^{a}\\rangle)^{r_{2}}\\otimes(G^{\\top}|i\\rangle)^{r_{3}}}{\\|G\\|_{F}}+|\\bot\\rangle.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "According to [55, Lemma 9], we can get the estimation to $\\left(\\frac{\\|G\\|_{F}}{\\gamma\\nabla\\sqrt{m}}\\right)^{2}$ with multiplicative error $\\mu$ using $\\mathcal{O}\\big(\\frac{\\gamma_{\\nabla}\\sqrt{m}}{\\mu\\|G\\|_{F}}\\big)$ queries to $U_{G}^{\\top}$ . Note our estimator to $\\|G\\|_{F}$ as $a_{\\mathrm{{est}}}$ , the goal we want to achieve is ", "page_idx": 26}, {"type": "equation", "text": "$$\n|a_{\\mathrm{est}}-\\|G\\|_{F}|\\leq\\varepsilon_{a}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Consider our estimation to the amplitude as $\\left(\\frac{a_{\\mathrm{est}}}{\\gamma_{\\nabla}\\sqrt{m}}\\right)^{2}$ , set $\\begin{array}{r}{\\mu=\\frac{\\epsilon_{a}^{2}+2\\epsilon_{a}\\|G\\|_{F}}{\\|G\\|_{F}^{2}}}\\end{array}$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\displaystyle\\frac{1}{\\gamma_{\\nabla}\\sqrt{m}}\\right)^{2}\\lvert a_{\\mathrm{est}}^{2}-\\|G\\|_{F}^{2}\\rvert=\\left(\\displaystyle\\frac{1}{\\gamma_{\\nabla}\\sqrt{m}}\\right)^{2}\\left(\\left\\lvert a_{\\mathrm{est}}-\\|G\\|_{F}\\right\\rvert\\cdot\\left\\lvert a_{\\mathrm{est}}+\\|G\\|_{F}\\right\\rvert\\right)}\\\\ &{\\leq\\left(\\displaystyle\\frac{1}{\\gamma_{\\nabla}\\sqrt{m}}\\right)^{2}\\left(\\varepsilon_{a}\\left(\\lvert a_{\\mathrm{est}}-\\|G\\|_{F}\\rvert+2\\|G\\|_{F}\\right)\\right)\\leq\\left(\\displaystyle\\frac{1}{\\gamma_{\\nabla}\\sqrt{m}}\\right)^{2}\\left(\\varepsilon_{a}^{2}+2\\varepsilon_{a}\\|G\\|_{F}\\right)}\\\\ &{\\leq\\mu\\left(\\displaystyle\\frac{\\|G\\|_{F}}{\\gamma_{\\nabla}\\sqrt{m}}\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This in turn gives us the estimation in 107. And this estimation requires ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\frac{\\gamma_{\\nabla}\\sqrt{m}\\|G\\|_{F}}{\\varepsilon_{a}^{2}+2\\|G\\|_{F}\\varepsilon_{a}}\\right)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "queries to $U_{G}$ and its inverse. The gate complexity also follows from [55, Lemma 9]. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "Theorem 31 (Quantum gradient estimation). Assume that we have efficient procedures (as described in Assumption $\\mathit{1}$ ) to access the problem data $A,B,Q,R$ in $\\mathcal{O}(\\mathrm{poly}\\log(n))$ time. Let $K\\in{\\cal S}_{K}(a)$ be a stabilizing policy stored in a quantum-accessible data structure. Provided that $\\|K-K^{*}\\|>\\varepsilon$ , we can compute a $\\theta$ -robust estimate of $\\nabla f(K)$ in cost ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{O}}\\left(\\frac{m^{1.5}n}{\\theta^{1.5}\\varepsilon^{1.5}}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. We need an estimation to $\\nabla f(K)$ up to a multiplicative error $\\theta$ . This requires a few steps as below: ", "page_idx": 26}, {"type": "text", "text": "Block-Encoding. From Lemma 27, we know we are able to construct a $(\\gamma_{\\nabla},\\varepsilon_{b})$ -blockencoding to $\\nabla f(K)$ using ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{O}}\\left(a^{6}\\rho^{3}\\sqrt{\\frac{\\kappa^{11}}{\\varepsilon_{b}}}\\right)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "elementary gates. And we also have the estimation that $\\gamma_{\\nabla}\\le\\widetilde O(a^{6}\\rho^{4}\\kappa^{6})$ . ", "page_idx": 26}, {"type": "text", "text": "Entry Retrieving. The next step is to retrieve the entries in the estimation to $\\nabla f(K)$ . We firstly use the technique introduced in Lemma 28 to get a state of the estimation to $|G_{\\mathrm{vec}}\\rangle$ , and this step requires ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\frac{\\gamma_{\\nabla}\\sqrt{m}}{\\|G\\|_{F}}\\right)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "queries. Now we can use Lemma 29 to get the entries in $|G_{\\mathrm{vec}}\\rangle$ up to an additive error $\\epsilon_{r}$ , using ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{O}}\\left(\\frac{\\gamma_{\\nabla}}{\\|G\\|_{F}}\\frac{m^{3/2}n}{\\varepsilon_{r}}\\right)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "queries to $U_{G}$ . ", "page_idx": 27}, {"type": "text", "text": "Norm Estimation. The final step is to estimate the norm of $G$ . Lemma 30 tells us that we can achieve an additive error $\\varepsilon_{a}$ to $\\|G\\|_{F}$ using ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\frac{\\gamma_{\\nabla}\\sqrt{m}\\|G\\|_{F}}{\\varepsilon_{a}^{2}+2\\|G\\|_{F}\\varepsilon_{a}}\\right)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "queries to $U_{G}^{\\top}$ . ", "page_idx": 27}, {"type": "text", "text": "The rest of the theorem is just to assemble all these steps. Note the requirement is ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|G-\\nabla f(K)\\|_{F}\\leq\\theta\\|f(K)\\|_{F},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and according to Lemma 26, $\\|\\nabla f(K)\\|_{F}\\geq c\\epsilon$ before convergence. ", "page_idx": 27}, {"type": "text", "text": "Denote the entries retrieved in the Entry Retrieving step form a matrix $\\mathcal{G}$ where $\\|\\mathcal G\\|_{F}=1$ , the norm estimated in the Norm Estimation step as $a_{\\mathrm{{est}}}$ , our estimator is then $a_{\\mathrm{est}}\\xi$ . Choose ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\varepsilon_{b}=\\frac{c\\theta\\varepsilon}{3},\\quad\\varepsilon_{a}=\\frac{c\\theta\\varepsilon}{3},\\quad\\varepsilon_{r}=\\frac{1}{3+c\\theta},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "we then know that $\\begin{array}{r}{\\|G-\\nabla f(K)\\|\\le\\frac{c\\theta\\varepsilon}{3}}\\end{array}$ , $\\nabla f(K)\\geq c\\varepsilon$ , then ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{O}(\\varepsilon)\\leq\\left|\\|\\nabla f(K)\\|_{F}-\\frac{c\\theta\\varepsilon}{3}\\right|\\leq\\|G\\|_{F}\\leq\\|\\nabla f(K)\\|_{F}+\\frac{c\\theta\\varepsilon}{3}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus it is clear to see ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|a_{\\mathrm{est}}\\mathcal{G}-\\nabla f(K)\\|_{F}}\\\\ &{=\\bigg\\|a_{\\mathrm{est}}\\mathcal{G}-\\|G\\|_{F}\\mathcal{G}+\\|G\\|_{F}\\mathcal{G}-G+G-\\nabla f(K)\\bigg\\|_{F}}\\\\ &{\\le\\bigg\\|a_{\\mathrm{est}}\\mathcal{G}-\\|G\\|_{F}\\mathcal{G}\\bigg\\|_{F}+\\bigg\\|\\|G\\|_{F}\\mathcal{G}-G\\bigg\\|_{F}+\\bigg\\|G-\\nabla f(K)\\bigg\\|_{F}}\\\\ &{\\le\\varepsilon_{a}+\\|G\\|_{F}\\varepsilon_{r}+\\varepsilon_{b}\\le\\varepsilon_{a}+(\\|\\nabla f(K)\\|_{F}+\\frac{c\\theta\\varepsilon}{3})\\varepsilon_{r}+\\varepsilon_{b}}\\\\ &{\\le\\theta\\|f(K)\\|_{F}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "And the total gate complexity should be ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{O}}\\left(\\frac{a^{12}\\rho^{7}\\kappa^{11.5}m^{1.5}n}{\\theta^{1.5}\\varepsilon^{1.5}}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "H Convergence analysis ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Lemma 32. Given any $K\\in S_{K}$ , let $G$ be a $\\theta$ -robust estimate of $\\nabla f(K)$ . Then, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\langle G,\\nabla f(K)\\rangle\\geq(1-\\theta)\\|\\nabla f(K)\\|_{F}^{2},}&{}\\\\ {\\|G\\|_{F}^{2}\\leq(1+\\theta)^{2}\\|\\nabla f(K)\\|_{F}^{2}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. It is straightforward to verify that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle G,\\nabla f(K)\\rangle=\\langle G-\\nabla f(K)+\\nabla f(K),\\nabla f(K)\\rangle=\\langle G-\\nabla f(K),\\nabla f(K)\\rangle+\\|\\nabla f(K)\\|_{F}^{2}}\\\\ &{\\qquad\\qquad\\geq-\\|G-\\nabla f(K)\\|_{F}\\|\\nabla f(K)\\|_{F}+\\|\\nabla f(K)\\|_{F}^{2}\\geq(1-\\theta)\\|\\nabla f(K)\\|_{F}^{2}.\\qquad\\mathscr{A}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|G\\|_{F}^{2}=\\|G-\\nabla f(K)+\\nabla f(K)\\|_{F}^{2}\\leq\\big(\\|G-\\nabla f(K)\\|_{F}+\\|\\nabla f(K)\\|_{F}\\big)^{2}}\\\\ &{\\qquad\\leq(1+\\theta)^{2}\\|\\nabla f(K)\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Lemma 33 (Descent lemma). Given $K\\in{\\cal S}_{K}(a)$ , and let $G$ be a $\\theta$ -robust estimate of $\\nabla f(K)$ . Then, there exists a positive $\\sigma_{m}$ such that for all $\\sigma\\in[0,\\sigma_{m}]$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\nf(K-\\sigma G)-f(K^{*})\\le\\left(1-\\sigma/\\mu\\right)\\left(f(K)-f(K^{*})\\right),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\mu$ , $\\sigma_{m}$ only depends on $A,B,R,Q$ , and $a$ , and $\\mu>\\sigma_{m}$ . ", "page_idx": 28}, {"type": "text", "text": "Proof. This lemma is a direct consequence of Lemma 32 (with $\\theta<1/2$ ) and [45, Proposition 6]. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "Proposition 34. For any initial stabilizing feedback gain $K_{0}\\in{\\cal S}_{K}$ , we denote $a:=f(K_{0})$ . Then, there exists constants $\\mu>\\sigma_{m}>0$ , depending only on $A,B,Q,R$ , and $a$ , such that for any fixed $\\sigma\\in[0,\\sigma_{m}]$ , the iterates of Algorithm 1 satisfy ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f(K_{k})-f(K^{*})\\leq(1-\\sigma/\\mu)^{k}\\left(f(K_{0})-f(K^{*})\\right),}\\\\ {\\|K_{k}-K^{*}\\|^{2}\\leq b(1-\\sigma/\\mu)^{k}\\|K_{0}-K^{*}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $b$ is an absolute constant that is independent of $k$ . ", "page_idx": 28}, {"type": "text", "text": "Proof. The first part (124a) is a direct corollary of Lemma 33. By Lemma 9 and (124a), we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|K_{k}-K^{*}\\|_{F}^{2}\\leq\\frac{a}{\\nu\\lambda_{\\operatorname*{min}}(R)}\\left(f(K_{k})-f(K^{*})\\right)}\\\\ {\\leq\\displaystyle\\left(1-\\sigma/\\mu\\right)^{k}\\left(f(K_{0})-f(K^{*})\\right)\\leq b(1-\\sigma/\\mu)^{k}\\|K_{0}-K^{*}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the last step follows from [45, Lamma 2] and $b=\\lambda_{\\operatorname*{max}}(R X(K_{0}))$ . ", "page_idx": 28}, {"type": "text", "text": "Theorem 35 (Quantum policy gradient for LQR). Assume that we have efficient procedures (as described in Assumption 1) to access the problem data $A,B,Q,R$ in $\\mathcal{O}(\\mathrm{poly}\\log(n))$ time. Let $K_{0}\\in S_{K}$ be a stabilizing policy. Then, Algorithm $\\mathit{1}$ outputs an $\\varepsilon$ -approximate solution to Problem 1 in cost ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{O}}\\left(\\frac{m^{1.5}n}{\\varepsilon^{1.5}}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. Notice that with lemma 33, we need $\\log\\left({\\frac{1}{\\varepsilon}}\\right)$ iterations. By Theorem 31, the quantum gradient estimation subroutine requires $\\begin{array}{r}{\\widetilde{O}\\left(\\frac{m^{1.5}n}{\\varepsilon^{1.5}}\\right)}\\end{array}$ elementary gates. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "I Extended numerical experiments ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "I.1 Aircraft Control Problem ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We perform another experiment on a practical problem that can be formulated as LQR. Here, we consider the aircraft flight control problem, specifically for pitch angle control. We adopt a linearized model of the aircraft around a steady flight condition. For a small aircraft, the pitch dynamics can be represented by the following state variables: pitch angle $\\theta$ (rad) and pitch rate $q$ ( $\\mathrm{rad/s}$ ). The control input is elevator deflection angle $\\delta$ (rad). The state-space model can be represented as ${\\dot{x}}=A x+B u$ , where $x=[\\theta,q]^{\\top}$ , $u=[\\delta]$ . We set $A=[[0,1],[0,-0.5]]^{\\top}$ , $B=[0,1]^{\\top}$ , $Q\\,=\\,[[10,0],[0,1]]\\,^{\\prime}$ , and $R=[0.1]$ . The plot of our optimization curve is available in the Figure Figure 3. Our method converges faster than the classical method. ", "page_idx": 28}, {"type": "image", "img_path": "GHqw3xLAvd/tmp/b6f617801880f53d2f141458b43cfd4cfbc74dd0ec5ada80ac34534bab8a7d0d.jpg", "img_caption": ["Figure 3: Numerical Results on Convergence. In the aircraft control problem, our policy gradient descent algorithm converges much faster than classic method [45]. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "I.2 Relative Errors ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We conducted further numerical experiments to understand how the optimality scales with problem size in Figure Figure 4. Here, we scale the number of masses in a spring-mass system from 2 to 4, and the problem dimension scales accordingly from 2 to 8. We measure the optimality by the relative error found in both our method and the classical method. The relative errors are $(J-J^{*})/j^{*}$ and $(f(K)-f(K^{*}))/f(K^{*})$ . As the dimension scales up, the relative errors increase, while our method consistently outperforms the classical optimization method. ", "page_idx": 29}, {"type": "image", "img_path": "GHqw3xLAvd/tmp/bfcae21f72ccddbf05396f71885837465f618771c8515c8cd3b5e2382eaf3100.jpg", "img_caption": ["Figure 4: Relative Error. We scale the size of a mass-spring system and our method consistently gets smaller relative error compared to [45]. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The main claims made in the abstract accurately reflect the paper\u2019s technical contributions and scope. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The limitations and future work of this work are discussed in the last section of the main paper. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The full set of technical assumptions is provided in the introduction section (see \u201cProblem Formulation\u201d). A complete proof of our main theoretical results can be found in the main paper and the appendices. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: All information needed to reproduce the main experiment results of the paper is disclosed in the \u201cNumerical Experiments\" section. The code for the experiment is attached in the supplementary material. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 31}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The code for the experiment is attached in the supplementary material. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips. cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \nThe authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: All the training and test details are provided in the \u201cNumerical Experiments\u201d section. The source code for the experiment is attached in the supplementary material. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper reports error bars to reflect the statistical significance of the experiments, see the \u201cNumerical Experiments\u201d section and Figure 2. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper provides sufficient descriptive information on the computer resources, see the \u201cNumerical Experiments\u201d section. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. \u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. \u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The research conducted in the paper conforms with the NeurIPS Code of Ethics. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 33}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper discusses the societal impacts of the work performed in the \u201cConclusion and Future Work\u201d section. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper mainly focuses on the theoretical foundation of computer algorithms and poses no such risks. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/ datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The source code of the numerical experiments is provided in the supplementary material with appropriate documentation. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. ", "page_idx": 35}, {"type": "text", "text": "\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]