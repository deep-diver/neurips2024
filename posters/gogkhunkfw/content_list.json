[{"type": "text", "text": "Simulation-Free Training of Neural ODEs on Paired Data ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Semin Kim1\u2217 Jaehoon Yoo1\u2217 Jinwoo Kim1 Yeonwoo Cha1 Saehoon Kim2 Seunghoon Hong1 1KAIST 2Kakao Brain ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this work, we investigate a method for simulation-free training of Neural Ordinary Differential Equations (NODEs) for learning deterministic mappings between paired data. Despite the analogy of NODEs as continuous-depth residual networks, their application in typical supervised learning tasks has not been popular, mainly due to the large number of function evaluations required by ODE solvers and numerical instability in gradient estimation. To alleviate this problem, we employ the flow matching framework for simulation-free training of NODEs, which directly regresses the parameterized dynamics function to a predefined target velocity field. Contrary to generative tasks, however, we show that applying flow matching directly between paired data can often lead to an ill-defined flow that breaks the coupling of the data pairs (e.g., due to crossing trajectories). We propose a simple extension that applies flow matching in the embedding space of data pairs, where the embeddings are learned jointly with the dynamic function to ensure the validity of the flow which is also easier to learn. We demonstrate the effectiveness of our method on both regression and classification tasks, where our method outperforms existing NODEs with a significantly lower number of function evaluations. The code is available at https://github.com/seminkim/simulation-free-node. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Continuous-depth models [3, 8] have received growing attention as an alternative to deep feedforward networks composed of a stack of discrete layers. As they approximate continuous, infinite-depth models with a constant set of model parameters, they are parameter-efficient models that can reuse their parameters across depth. Additionally, by controlling the number of function evaluations (NFEs) during inference, we can choose the optimal trade-off between performance and computational cost. This trade-off can be adjusted without retraining the model, unlike conventional neural networks that require separately trained models of different capacities to achieve a similar flexibility. ", "page_idx": 0}, {"type": "text", "text": "A prominent class of continuous-depth models is Neural Ordinary Differential Equations (NODEs) [8], which utilize continuous-time differential equations to describe evolutions of intermediate states. In NODEs, a parameterized dynamics function learns the time derivative of the continuous transformation of a state. NODEs can be interpreted as a continuous limit of residual networks [8, 15], and hence they offer a versatile framework similarly to ResNet [17]. As a result, NODEs have been successfully applied to tasks such as physically informed modeling [37, 41], time series modeling [5, 23, 36], and generative modeling with normalizing flows [12, 14]. ", "page_idx": 0}, {"type": "text", "text": "However, despite their success in various tasks, the application of NODEs to learn deterministic mappings between paired data, such as regression or classification, remains under-explored. This is largely due to the substantial computational burden of training NODEs, since numerical ODE solving during training is inherently serial, slow, and requires large NFEs. ", "page_idx": 0}, {"type": "text", "text": "Recently, an alternative, simulation-free training method has been introduced for ODE-based generative models [1, 28, 29]. Known as flow matching, this approach eliminates the need for the expensive ODE solving during training by directly regressing the model on a presumed velocity field. While this method significantly improves training efficiency by requiring only one function evaluation per training step, its effectiveness on deterministic tasks has not been well explored. ", "page_idx": 1}, {"type": "text", "text": "In this work, we investigate a simulation-free training method for continuous-depth models in learning deterministic mapping between paired data. We first identify potential problems that occur when applying flow matching objective to NODEs, and then propose a method to mitigate them. With our simulation-free training scheme, we can significantly reduce the computational demands of NODE training while maintaining competitive performance in deterministic tasks. We demonstrate the advantages of our method on both regression and classification problems, providing empirical evidence of its effectiveness and versatility in common supervised learning settings. Furthermore, by leveraging simple flows, our method can achieve superior performance compared to NODEs in low-NFE regime. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminary ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "NODEs as Continuous-Depth Models We aim to learn a deterministic mapping between paired data $\\mathcal{D}=\\{x^{(i)},y^{(i)}\\}_{i=1}^{N}$ with a continuous-depth model. To this end, we consider Neural Ordinary Differential Equations (NODEs) [8], which can be regarded as a continuous limit of residual networks. Formally, a single layer of a residual network transforms a hidden state $z$ with a residual connection, i.e. $z_{t+1}=z_{t}\\!+\\!h_{\\theta}(z_{t})$ . This update resembles a single step of Euler solver, which uses parameterized dynamics function $h_{\\theta}$ to approximate the derivative of $z$ with respective to $t$ . Taking this discretization to the continuous limit, residual networks are equivalent to the following ODE: ", "page_idx": 1}, {"type": "equation", "text": "$$\nv_{t}={\\frac{\\mathrm{d}z_{t}}{\\mathrm{d}t}}=h_{\\theta}(z_{t},t).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "To train a NODE with a loss defined on the state at time $t_{2}$ , we first solve an initial value problem starting from a known initial state $z_{t_{1}}$ : ", "page_idx": 1}, {"type": "equation", "text": "$$\nz_{t_{2}}=z_{t_{1}}+\\int_{t_{1}}^{t_{2}}h_{\\theta}(z_{t},t)d t=0{\\tt D E S o l v e}(z_{t_{1}},h_{\\theta},t_{1},t_{2}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "For the task of fitting a paired dataset, each data-label pair $(x,y)$ is placed on the state-space of the ODE as endpoints $(z_{0},z_{1})$ for time interval $[0,1]$ . This requires matching the dimensions of data $x$ and label $y$ to states $z$ , which is done by projecting data $x$ to initial state $\\bar{z_{0}}=f_{\\phi}(x)$ ,2and the solved final state $z_{1}$ to predicted label $\\hat{y}=d\\bar{\\psi}(z_{1})$ . Then, NODEs are trained end-to-end to minimize the loss $\\mathcal{L}(\\hat{y},y)$ . However, this approach is inherently slow and computationally intensive, as it requires a large number of sequential function evaluations by the ODE solver [8, 22, 34]. There is also a hidden cost in integrating the ODE backward in time for gradient estimation (i.e., adjoint method [8]), which necessitates additional serial function evaluations and is numerically unstable [43]. Also, common choices of adaptive-step ODE solvers tend to make NODEs learn arbitrarily complex trajectories [42], making both training and inference computationally expensive. ", "page_idx": 1}, {"type": "text", "text": "Flow Matching for Simulation-Free Training Instead of optimizing Eq. (1) via the expensive initial value problem of Eq. (2), we can alternatively employ the flow matching framework [1, 28, 29] to directly regress the dynamics function $h_{\\theta}$ to the velocity field $v_{t}$ by: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbb{E}_{t}[||h_{\\theta}(z_{t},t)-v_{t}||_{2}^{2}].\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Since the intermediate state $z_{t}$ and its velocity $v_{t}$ are generally unknown and intractable to compute, existing works [1, 28, 29] employ predefined, tractable conditional velocity fields defined per sample. Specifically, a closed form expression of $z_{t}$ is given as an interpolant of two endpoints and time (i.e. $z_{t}=\\alpha_{t}z_{0}\\!+\\!\\beta_{t}z_{1})$ , and a target velocity field is constructed as a time derivative of the interpolant. One simple instantiation of an interpolant and its corresponding target velocity field is linear dynamics with a constant speed [28, 29]: ", "page_idx": 1}, {"type": "equation", "text": "$$\nz_{t}=(1-t)z_{0}+t z_{1},\\quad v_{t}=z_{1}-z_{0}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "image", "img_path": "GOgKhunkfw/tmp/4c67b69d90a424aa5921f65e05b5565e9a92769a24fc54b7c4d863d93bba9853.jpg", "img_caption": ["Figure 1: Comparison of the learned trajectories. The final train loss (MSE) and training NFE are shown above each plot. (a) We consider deterministic regression task of four data pairs, each of which is represented by two circles (fliled and empty circles) connected by dotted lines. (b) NODEs can correctly associate the pairs but through complex paths (solid lines) that require large NFEs. (c) Flow matching with linear velocity can greatly reduce the training NFEs by simulation-free training, but fails to associate the correct pairs due to the crossing trajectories induced by predefined dynamics. (d) The proposed method can alleviate the problems by learning the embeddings for data jointly with the flow matching. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Since we can obtain $v_{t}$ at arbitrary time $t$ , this allows the dynamics function to be trained in a simulation-free manner, where the regression of the vector field is performed parallel in time. Simulation-free training in the flow matching models is intriguing, since there is no need for serialized function evaluations during training as well as backpropagation through time. Employing a simple velocity field such as Eq. (4) also greatly simplifies the trajectory of the dynamics function, reducing the number of function evaluations for inference. However, flow matching has been mainly studied for generative modeling, which aims to find a transportation map between two marginal distributions $p(z_{0})$ and $p(z_{1})$ while learning arbitrary per-sample couplings between $z_{0}\\sim p(z_{0})$ and $z_{1}\\sim p(z_{1})$ . It makes it difficult to be applied in deterministic regression tasks where the per-sample coupling is defined by data pairs $(z_{0},\\bar{z_{1}})\\sim(x,y)$ . We elaborate on this issue in the next section. ", "page_idx": 2}, {"type": "text", "text": "3 Challenges in Flow Matching for Paired Data ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We seek to adopt the simulation-free training objective in Eq. (3) to learn a continuous-depth model of Eq. (1). However, we find that careful consideration is required when employing the flow-matching objective to model a deterministic mapping between paired data. ", "page_idx": 2}, {"type": "text", "text": "To illustrate this point, we provide a toy example in Fig. 1. Here, we consider a simple regression task with two-dimensional inputs and outputs, translating four points at $x=-1$ to the ones at $x=1$ but in a different order in the $y$ axis. For ease of analysis, we consider simple linear velocity field in Eq. (4) for flow matching. ", "page_idx": 2}, {"type": "text", "text": "First, we observe that NODE successfully learns to associate inputs and outputs by solving Eq. (2) (Fig. 1 (b)). However, the learned trajectories are highly non-linear and complex which leads to a large number of function evaluations for training and inference. ", "page_idx": 2}, {"type": "text", "text": "The flow matching can greatly improve the computation cost by optimizing the simulation-free objective in Eq. (3). However, we observe that it fails to infer the correct input-output correspondence according to the learned trajectories (Fig. 1 (c)). This is because some target trajectories induced by the predefined velocity field are crossing each other, which cannot be modeled by ODEs [40]. As a result, the learned dynamics function at the intersection of crossing trajectories produces their mean velocity, which makes the inferred trajectory fail to arrive at the correct output. ", "page_idx": 2}, {"type": "text", "text": "Note that the crossing trajectories induced by the predefined flow are less problematic in generative tasks. It is because their objective is to transport between two marginal distributions $p(z_{0})$ and $p(z_{1})$ while allowing arbitrary association between samples $z_{0}\\sim p(z_{0})$ and $z_{1}\\sim p(z_{1})$ . It is evident from Fig. 1 (c), where the model fails to preserve the initial coupling in the dataset but still yields valid marginal distribution at the output. However, it is not desirable for deterministic regression, since the goal is to preserve per-sample correspondence between data and label. ", "page_idx": 2}, {"type": "image", "img_path": "GOgKhunkfw/tmp/ef540d5a63bb20690a7719880a5e72989912431e2a00de8030dfcf317bfc5c42.jpg", "img_caption": ["Figure 2: An overview of our framework. We avoid the crossing trajectory problem in data space by introducing learnable encoders that project data and label to embedding space. In the learned embedding space, the presumed dynamics induce valid target velocity field. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "4 End-to-End Latent Flow Matching ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Previous discussions suggest that predefined flow between paired data can yield crossing trajectories that cannot be modeled by the well-defined ODE, which leads to invalid paths by the dynamics function that break the coupling of data and label. As a simple solution, we propose to learn the embeddings of data and label jointly with dynamics function, in such a way that the predefined flow induces non-crossing trajectories in the embedding space (Fig. 1 (d)). ", "page_idx": 3}, {"type": "text", "text": "Fig. 2 illustrates the overall framework. The proposed framework comprises the data encoder $f_{\\phi}$ , label encoder $g_{\\varphi}$ , label decoder $d_{\\psi}\\,\\approx\\,g_{\\varphi}^{-1}$ , and dynamics function $h_{\\theta}$ . Given a data pair $(x,y)$ , our model first projects the data and label using respective encoders by $\\left(z_{0},z_{1}\\right)=\\left(f_{\\phi}(x),g_{\\varphi}(y)\\right)$ . Then, for all $t\\in[0,1]$ , the state $z_{t}$ and the corresponding target velocity $v_{t}$ are obtained by some predefined dynamics (e.g., Eq. (4)). We train the encoders and dynamics function jointly using the flow matching loss, while an additional label autoencoding objective is applied to the label encoder and decoder. The inference procedure remains similar to that of NODEs: given the input embedding $z_{0}=f_{\\phi}(x)$ , we obtain the state $\\hat{z}_{1}$ by solving ODE in Eq. (2) and decode it to a label by the label decoder $\\hat{y}=d_{\\psi}(z_{1})$ . Below, we describe the learning objective and optimization of our method in detail. ", "page_idx": 3}, {"type": "text", "text": "Flow Loss With the learnable projections $z_{0}\\,=\\,f_{\\phi}(x),z_{1}\\,=\\,g_{\\varphi}(y)$ and any simple dynamics assumption $z_{t}=\\alpha_{t}z_{0}+\\beta_{t}z_{1}$ , our flow matching objective is defined by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{f l o w}(\\theta,\\phi,\\varphi)=\\mathbb{E}_{t,(x,y)\\sim\\mathcal{D}}[||h_{\\theta}(z_{t},t)-v_{t}||_{2}^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In contrast to conventional flow matching (Eq. (3)) that optimizes the dynamics function given the fixed endpoints $z_{0}$ and $z_{1}$ , the loss in Eq. (5) is optimized jointly with two encoders $f_{\\phi}$ and $g_{\\varphi}$ . Since the loss has the optimum at $\\mathcal{L}_{f l o w}(\\theta^{*},\\phi^{*},\\varphi^{*})=0$ when the dynamics function $h_{\\theta^{*}}$ perfectly fits to the velocity field $v_{t}$ , optimizing Eq. (5) guarantees that the optimal encoders $f_{\\phi^{*}}$ and $g_{\\varphi^{*}}$ induce non-crossing target trajectories in the embedding space. ", "page_idx": 3}, {"type": "text", "text": "Conceptually, our approach resembles the reflow procedure in Rectified Flow [29], which straightens the target trajectories by recursively rewiring $z_{0}$ and $z_{1}$ according to the trajectory obtained by solving an ODE. In contrast, our approach straightens the trajectory by learning the encoders such that the trajectories defined by the predefined flow do not cross in the embedding space, which does not involve ODE solving and, most importantly, preserves the initial coupling $(z_{0},z_{1})$ . ", "page_idx": 3}, {"type": "text", "text": "However, contrary to Eq. (3), we observe that optimizing the flow matching objective with learnable encoders in Eq. (5) can lead to trivial degenerate solutions. One such example is when both encoders collapse to output constant ignoring data, which produces trivial targets for the dynamic model (e.g., $z_{0}=z_{1}=h_{\\theta}(\\cdot,\\cdot)=0)$ . To avoid trivial solutions, both encoders require strong regularization to be relevant to the inputs. Fortunately, such regularization is naturally provided by the learning objective for the label decoder $d_{\\psi}$ , which is described below. ", "page_idx": 3}, {"type": "text", "text": "Label Autoencoding Loss We define the label autoencoding loss as a simple mean squared error (MSE) between the label $y$ and its reconstruction obtained by label encoder and decoder: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{l a b e l\\_a e}(\\psi,\\varphi)=\\mathbb{E}[||d_{\\psi}(g_{\\varphi}(y))-y||_{2}^{2}].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The primary purpose of Eq. (6) is to provide learning signals for the label decoder $d_{\\psi}$ , which is used to decode the predicted $z_{1}$ to the label $\\hat{y}=d_{\\psi}(z_{1})$ at inference. However, Eq. (6) also functions as a regularization to avoid trivial solutions of Eq. (5) by preventing the label encoder $g_{\\varphi}$ from ignoring the input label $y$ . Regularizing the label encoder is sufficient to prevent trivial solutions in principle, since it eliminates the trivial targets for both data encoders $f_{\\phi}$ and the dynamics function $h_{\\theta}$ . ", "page_idx": 4}, {"type": "text", "text": "Objective Function The overall objective function for the proposed framework is given by combining the two losses introduced above: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta,\\phi,\\varphi,\\psi}\\mathcal{L}_{f l o w}(\\theta,\\phi,\\varphi)+\\mathcal{L}_{l a b e l_{-}a e}(\\psi,\\varphi)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Theoretically, we can show that optimizing the combination of flow loss and autoencoding loss in Eq. 7 can indeed mitigate the problem of target trajectory crossing: ", "page_idx": 4}, {"type": "text", "text": "Proposition 1. There exist $f_{\\phi}$ and $g_{\\varphi}$ that induce non-crossing target trajectory for any data pair in D while minimizing Llabel_ae. ", "page_idx": 4}, {"type": "text", "text": "Proof. The proof can be found in App. A.1. ", "page_idx": 4}, {"type": "text", "text": "Proposition 2. If $g_{\\varphi}$ is injective, the following equivalence holds: $f_{\\phi},g_{\\varphi}$ and $h_{\\theta}$ minimize $\\mathcal{L}_{f l o w}$ to zero for all $t\\in[0,1)$ if and only if $f_{\\phi}$ and $g_{\\varphi}$ induce non-crossing target trajectory and $h_{\\theta}$ perfectly fits the induced target velocity, for any data pair in $\\mathcal{D}$ . ", "page_idx": 4}, {"type": "text", "text": "Proof. The proof can be found in App. A.2. ", "page_idx": 4}, {"type": "text", "text": "Proposition 1 ensures the existence of encoders that do not induce crossing in the target trajectory, while Proposition 2 suggests that the encoders are optimal when flow loss is minimized, assuming the label encoder is injective. Since the label autoencoding task enforces $g_{\\varphi}$ to be injective, combining the two propositions implies that optimizing the objective function in Eq. (7) prevents the encoders from inducing target trajectory crossing and enables the dynamics function to accurately fit the induced trajectories. ", "page_idx": 4}, {"type": "text", "text": "Optimization During the optimization of Eq. (7), we find that two additional regularizations on the encoders are useful in further preventing suboptimal solutions and stabilizing training and inference. ", "page_idx": 4}, {"type": "text", "text": "First, although the autoencoding loss in Eq. (6) prevents degenerate encoders in principle, we find that flow matching loss in Eq. (5) can still induce an ill-behaved local minima for the data encoder $f_{\\phi}$ . For instance, under linear velocity field (Eq. (4)), a collapsed, constant data encoder, e.g., $z_{0}=0$ , makes both intermediate state and target velocity depend only on $z_{1}$ at $t\\in(0,1]$ , e.g., $z_{t}=t z_{1}$ and $v_{t}=z_{1}$ . In this case, a dynamics function that scales with time, $h_{\\theta}(z,t)=z/t$ , can fit the target velocity field for all $t\\in(0,1]$ although it does not yield meaningful mapping between $(x,y)$ . We find that explicitly sampling $t=0$ with a certain probability during training effectively resolves the issue. ", "page_idx": 4}, {"type": "text", "text": "Second, we empirically find that the label encoder tends to reduce the scale of the output embeddings to optimize the flow matching loss. Although this is not a fundamental problem in principle, we observe that it often affects the generalization performance by making the model prone to small numerical errors at inference, such as prediction errors in the dynamics function or discretization errors in the ODE solver. To address this, we encourage the label embeddings to repel each other, so that they construct a robust destination point for ODE solving during inference. Specifically, during training we add random noise $\\epsilon\\sim\\bar{\\mathcal{N}}(0,\\sigma^{2})$ to label embedding and let the label decoder to reconstruct original label from it, i.e., we minimize $\\mathbb{E}[||d_{\\psi}(g_{\\varphi}(y)+\\epsilon)-y||_{2}^{2}]$ instead of Eq. (6). We empirically find these techniques helpful for better optimization, as shown in Sec. 6.3. ", "page_idx": 4}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Enhancing Efficiency of NODEs Several previous works have addressed the problem of high numbers of function evaluations (NFEs) in the forward process of Neural Ordinary Differential Equations (NODEs). Most approaches involve imposing regularization on the learned trajectory, such as penalizing higher-order derivatives [20] or incorporating kinetic regularization [12]. Similar effects can be achieved through weight decay [14], augmenting dimensions [11], or using internal solver heuristics [34]. Additionally, sampling the end time of the integration interval [13] has also been explored as a simple solution. These methods encourage the model to learn simpler trajectories, thereby effectively reducing the NFE required to solve the ODE. However, with the common choice of adaptive-step solvers, there often remain dozens of sequential function evaluations during a single training step, making the training of NODEs slow and computationally intensive. ", "page_idx": 5}, {"type": "text", "text": "Simulation-Free Training on Paired Dataset Some studies have explored simulation-free training methods for fitting dynamics functions on paired datasets, primarily within the context of diffusion probabilistic models (DPMs) [18]. DPMs achieve simulation-free training by learning to denoise data at multiple noise levels in parallel [30]. During inference, DPMs generate data from standard Gaussian noise through iterative denoising. These models have been applied to various vision tasks aimed at learning deterministic mappings, including segmentation [2, 4, 9], object detection [7], and image restoration tasks [27, 35, 39], among others. While these applications are impressive, they are tailored specifically to their target tasks and do not represent general methods for paired data with diverse label structures. ", "page_idx": 5}, {"type": "text", "text": "One notable work in this area is CARD [16], which introduced a new conditional diffusion process for classification and regression tasks, making it applicable to arbitrary regression and classification data. Although these works, based on diffusion models trained with denoising objectives, align more closely with neural SDEs rather than ODEs, we include a comparison with CARD in our main experiments to provide a comprehensive evaluation. ", "page_idx": 5}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "6.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Baselines and Datasets We validate the effectiveness of our method on various datasets for both regression and classification tasks. For baselines, we compare our method with the standard NODE [8] and previous works that utilize regularization to reduce NFEs. Specifically, we compare against STEER [13], which introduces stochasticity in the integration interval, and RNODE [12], which regularizes the norm of the velocity field. Additionally, we include a comparison with CARD [16], a classification and regression model based on diffusion. Following prior work [11, 13], we use MNIST [26], SVHN [33], and CIFAR10 [25] for image classification experiments. For regression tasks, we use UCI regression datasets [21], adhering to the protocol used by CARD. ", "page_idx": 5}, {"type": "text", "text": "Evaluation We report classification accuracy and root mean square error (RMSE) as the main performance metrics. To quantify computational costs, we report average per-sample NFEs along with training throughput measured by the total training iterations divided by training time. For all NODE baselines, we use the dopri5 [10] adaptive-step solver implemented in torchdiffeq [6] package for both training and inference. Additionally, we report few-step inference results using the Euler solver. For CARD, we perform few-step inference by periodically skipping intermediate steps, similar to DDPM in few-step inference [18, 38], and report the metric of full-step inference in the place of adaptive-step solver in NODEs. ", "page_idx": 5}, {"type": "text", "text": "Implementation Details We use the same network architecture across baselines, employing an MLP-based architecture for MNIST and UCI and a convolutional architecture for SVHN and CIFAR10. For classification tasks, we use one-hot encoded labels for $y$ and assign the predicted label $\\hat{y}$ by applying argmax on the channel dimension. To handle the significant memory requirements of training NODE-based baselines, we use the adjoint sensitivity method [8] for SVHN and CIFAR10 experiments. When using the adjoint method, we report the total NFE by summing the number of function evaluations in both the forward and backward passes. For our model, we primarily use a simple linear dynamics assumption, which is shown to be effective according to the analysis in Sec. 6.3. Further experimental details can be found in App. B. ", "page_idx": 5}, {"type": "table", "img_path": "GOgKhunkfw/tmp/9d9582fa463f5f81ef55e28eb66147a52779f0917fe3b82498b2a1e9ea696f69.jpg", "table_caption": ["Table 1: Experiment results on image classification. Training cost and few-/full-step performances are reported in three datasets. For classification accuracy, numbers indicate the number of function evaluations with Euler solver, where $\\infty$ denotes the result of dopri5 adaptive-step solver. For CARD, we report the 1000-step decoding results instead of using the adaptive solver, as the model was trained on discrete timesteps. CARD\u2020 is trained with 4 times longer steps. "], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "GOgKhunkfw/tmp/84584d6b0785d48b4cd6e329a03d35b33ae400f48b64deb7513335b0b30c0b3a.jpg", "img_caption": ["Figure 3: RMSE over NFEs on UCI regression tasks. To control the NFE, we use Euler solver for the evaluation. By assuming linear dynamics, our model shows better performance in low NFE regime. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "6.2 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Training Cost and Performance In Tab. 1, we report the training cost and classification accuracy on MNIST, SVHN, and CIFAR10. NODE-based baselines suffer from large NFEs\u2014ranging from tens to hundreds\u2014during training, resulting in a low throughput. In contrast, simulation-free training method (i.e., ours and CARD) require only a single function evaluation per training step, significantly boosting training speed. ", "page_idx": 6}, {"type": "text", "text": "The trade-off for faster training in simulation-free methods is a constraint on the dynamics that can be learned. However, we observe that this reduced flexibility does not lead to significant performance degradation. Compared to NODE-based baselines, our method shows only minor degradation on MNIST and even improvements on SVHN and CIFAR10 in terms of final classification accuracy. On SVHN and CIFAR10, NODE-based baselines are trained with the adjoint sensitivity method [8] to meet memory requirements, which is known to suffer from inaccurate gradient estimation [42, 43]. ", "page_idx": 6}, {"type": "text", "text": "Thus we hypothesize that the performance gains on SVHN and CIFAR10 may be due to the accurate gradient calculation achieved through direct backpropagation in our model, which is a positive byproduct of utilizing simulation-free training. ", "page_idx": 7}, {"type": "text", "text": "When compared to the diffusion-based baseline, our model tends to show better performance and faster convergence. Specifically, our model outperforms CARD on all three datasets, even compared to CARD variants trained for longer iterations. While diffusion models also benefti from simulationfree training, they are based on stochastic differential equations that induce stochastic and non-linear trajectories. We conjecture that this characteristic is not ideal for few-step inference and also contributes to slower convergence. ", "page_idx": 7}, {"type": "text", "text": "Few-Step Inference Ideally, our model trained with linear dynamics will yield a perfectly straight solution trajectory, which can be accurately estimated even with a one-step Euler solver. Consequently, with our linear dynamics assumption, we can significantly enhance inference speed by utilizing fewstep inference while maintaining competitive performance compared to many-step solving. To demonstrate this, we report few-step inference results with the Euler solver in Tab. 1. Our model, by avoiding crossing points in the learnable embedding space, produces a linear trajectory and thus exhibits superior few-step performance. This observation aligns with findings in flow matching models [28, 29], which highlight the advantage of linear dynamics for generating high quality samples with low inference cost. ", "page_idx": 7}, {"type": "text", "text": "Regression We further analyze the effectiveness of our method in regression tasks, as shown in Fig. 3. See App. D.4 for full results. Despite differences in label structure, we observe similar trends for both classification and regression tasks: our method significantly reduces computational burden during training and demonstrates superior performance in few-step inference with linear dynamics. Similar to the original NODE, our model can be effectively applied to a wide range of common supervised learning settings, regardless of whether the labels are categorical or continuous. ", "page_idx": 7}, {"type": "text", "text": "6.3 Analysis and Discussion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Learning Encoders with Flow Loss To support our key claim that learning encoders with flow loss allows our model to avoid crossing trajectories, we compare our model with two variants that do not utilize flow loss for learning encoders. Specifically, we consider: (1) $A N O D E{+}F M$ which augments the data and label to same dimensionality by zero-padding and learns dynamics function with flow matching; and (2) Autoencoder+FM which employs a two-stage approach by first learning the embedding space using an independent autoencoding objective for both data and labels, and then learning the dynamics function on the fixed embedding space3. To quantify the crossing points in trajectories, we train all models with the linear dynamics assumption and measure the proportion of samples where the predicted labels from a one-step Euler solver and an adaptive-step ", "page_idx": 7}, {"type": "text", "text": "Table 2: The effectiveness of learning encoders with flow loss. Training accuracy and the proportion of disagreement in prediction between a one-step Euler solver and an adaptive-step solver are shown. Simply augmenting dimensions (AN$\\mathrm{ODE+FM}_{\\phantom{\\rule{0.16em}{0ex}}}$ ) does not effectively prevent trajectory crossing. Furthermore, learning encoders without flow loss (Autoencoder+FM) also fails to preserve the original coupling due to crossing trajectories. ", "page_idx": 7}, {"type": "table", "img_path": "GOgKhunkfw/tmp/c2551d6e5cce502b4fb488a3cacbe3a4ef53b679dc50dd7113d1963e8928778b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "solver disagree4. We report the disagreement ratio and training accuracy in Tab. 2. ", "page_idx": 7}, {"type": "text", "text": "As discussed in Sec. 3, our results indicate that merely augmenting the dimension $(\\mathbf{A}\\mathbf{N}\\mathbf{O}\\mathbf{D}\\mathbf{E}{+}\\mathbf{F}\\mathbf{M})$ does not resolve the issue of crossing trajectories induced by the predefined dynamics, resulting in a poor performance even on training data. Although employing more sophisticated encoders (Autoencoder $\\mathbf{\\cdot+FM}_{\\mathbf{\\cdot}}$ ) partially reduces disagreements, it still fails to fti the training data properly. Such crossing trajectories can be eliminated by learning encoders with the flow matching loss, allowing our model to fit successfully to training data with high accuracy. ", "page_idx": 7}, {"type": "image", "img_path": "GOgKhunkfw/tmp/fa5a5250ef736bd42be03122c2c1912d7f459fd3233f02ee799f11fcca7dd425.jpg", "img_caption": ["Figure 5: Ablation study of optimization techniques on CIFAR10. Explicitly sampling $t=0$ in training prevents suboptimal solutions while adding noise to label autoencoding improves generalization. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Role of the Learned Dynamics Function A potential concern is that learning a nonlinear transformation to embed data might lead the data encoder to perfectly predict the label. This issue, noted in previous work [32], warns that learning a complex input transformation could result in a collapse where the dynamics function becomes a simple identity map. To investigate whether this issue occurs in our case, we conduct a 1-NN classification using the learned data encoder on CIFAR10 image classification. The 1-NN classification accuracy with the learned data embedding $z_{0}$ is $65.66\\%$ , which is significantly lower than the accuracy of $88.47\\%$ when we utilize the learned dynamics function to obtain the predicted label embedding $\\hat{z}_{1}$ . This result indicates that the learned data embedding is not yet linearly separable enough, and the learned flow can further enhance accuracy. Based on this observation, we conclude that the collapse scenario, where the velocity field becomes an identity map, does not occur in our model. Instead, we find that the learned dynamics function clearly plays a role in processing the data, thereby avoiding the aforementioned pitfall. ", "page_idx": 8}, {"type": "text", "text": "Nonlinear Predefined Dynamics Our method embraces not only the linear dynamics but also dynamics having fixed form of $z_{t}\\,=\\,\\alpha_{t}z_{0}+\\beta_{t}z_{1}$ in general. Here, we investigate the effect of utilizing different dynamics assumptions, including nonlinear ones. To be specific, we choose three different dynamics that is easy to implement: ", "page_idx": 8}, {"type": "text", "text": "\u2022 Concave: $\\begin{array}{r}{\\alpha_{t}=\\cos(\\frac{\\pi}{2}t),\\beta_{t}=\\sin(\\frac{\\pi}{2}t)}\\end{array}$ \u2022 Linear: $\\alpha_{t}=(1-t),\\beta_{t}=t$ \u2022 Convex: $\\begin{array}{r}{\\alpha_{t}=1-\\sin(\\frac{\\pi}{2}t),\\beta_{t}=1-\\cos(\\frac{\\pi}{2}t)}\\end{array}$ . ", "page_idx": 8}, {"type": "image", "img_path": "GOgKhunkfw/tmp/f4e948e907a803a093674d1ab81306b32bb4551cb067262db70aad4b9ca6ef38.jpg", "img_caption": ["Figure 4: Analysis on predefined dynamics. (Left) Change of coefficients in interpolant with respective to time. (Right) Prediction RMSE over NFE on UCI Boston dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "We visualize the difference of dynamics and their effect on final performance in Fig. 4. ", "page_idx": 8}, {"type": "text", "text": "As it shows, the performance of the nonlinear variants (i.e., convex and concave) clearly improves with an increased number of function evaluations. For these variants, the many-step inference performance increases as we invest more NFEs and then later saturates, indicating that the model\u2019s approximation of an infinite-depth model becomes sufficiently accurate as discretization error diminishes. In contrast, our model trained on the linear dynamics shows consistently good performance, even with a few function evaluations. This behavior is somewhat expected, as the ideal linear trajectory can be already accurately inferred using a one-step Euler solver. Surprisingly, we also empirically observe that the choice of linear dynamics leads to better performance, compared to more complex choice of dynamics assumption. Thus, similar to the claims of Liu et al.(2022) [29], we believe that linear dynamics should be considered as a default choice unless specific constraints on hidden states are required. ", "page_idx": 8}, {"type": "text", "text": "Ablation on Optimization Techniques We conduct an ablation study on CIFAR10 dataset to investigate the effects of optimization techniques introduced in Sec. 4, and report the results in Fig. 5. Similar ablation studies were also conducted on other datasets, as detailed in App. D.3. The variant without explicitly sampling $t=0$ fails to fit on training data, despite the convergence of the flow loss. Furthermore, the variant without adding noise to the label autoencoding objective succeeds in fitting the training set, but its test accuracy significantly degrades compared to the version with noise in label autoencoding. Thus, as discussed in Sec. 4, we conclude that explicitly sampling $t=0$ and introducing noise in label autoencoding effectively regularize both encoders and produces label embedding that is robust to test-time errors. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "7 Limitations and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we primarily study the problem of adopting flow matching by imposing a fixed and simple dynamics assumption, observing that a fixed, closed-form equation for the intermediate state is already sufficient to bring the advantages of flow matching. Although employing simple dynamics (e.g. linear) may seem overly restrictive, this approach can be justified in the context of Koopman operator theory, which aims to find embeddings that globally linearize the dynamics. Since our method of flow matching with linear dynamics shares a high-level concept with Koopman operator theory, exploring the relationship between the two could be a promising direction for future research. We leave further discussions in App. C. ", "page_idx": 9}, {"type": "text", "text": "On the other hand, while we studied simple and predefined dynamics, the form of dynamics assumption could be further generalized to be a learnable component. To be specific, introducing a learnable target dynamics that determines per-sample dynamics would be a promising future direction to study. This would be advantageous for handling inputs with varying complexity, by efficiently and adaptively allocating more computation on hard samples. ", "page_idx": 9}, {"type": "text", "text": "Furthermore, extending our method to a broader range of paired data applications might be a useful future direction. The reduced computational burden achieved through simulation-free training could offer several beneftis of continuous-depth models to diverse applications. For example, applying our method to model compression or knowledge distillation could be particularly promising, leveraging the parameter efficiency of continuous-depth models. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we adopted a flow matching objective to achieve simulation-free training of continuousdepth models for learning deterministic mappings between paired data. We proposed learning an embedding space where flow matching occurs, which we identified as a crucial component for ensuring the validity of the target velocity field. Our proposed method significantly reduces the computational burden of training NODEs while maintaining competitive performance. Additionally, we found that our method, leveraging simple linear dynamics, demonstrates impressive performance on low-NFE regime. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was in part supported by the National Research Foundation of Korea (RS-2024-00351212 and RS-2024-00436165), and Institute of Information and communications Technology Planning and Evaluation (IITP) grant (RS-2022-II220926, RS-2024-00509279, RS-2021-II212068, and RS-2019- II190075) funded by the Korean government (MSIT). We thank Youngmin Ryou (KAIST) for his valuable input in formulating the theoretical analysis presented in this work. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Michael S. Albergo and Eric Vanden-Eijnden. 2023. Building Normalizing Flows with Stochastic Interpolants. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.   \n[2] Tomer Amit, Eliya Nachmani, Tal Shaharabany, and Lior Wolf. 2021. SegDiff: Image Segmentation with Diffusion Probabilistic Models. CoRR, abs/2112.00390.   \n[3] Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. 2019. Deep Equilibrium Models. In NeurIPS, pages 688\u2013699. [4] Dmitry Baranchuk, Andrey Voynov, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. 2022. Label-Efficient Semantic Segmentation with Diffusion Models. In ICLR.   \n[5] Edward De Brouwer, Jaak Simm, Adam Arany, and Yves Moreau. 2019. GRU-ODE-Bayes: Continuous Modeling of Sporadically-Observed Time Series. In NeurIPS.   \n[6] Ricky T. Q. Chen. 2018. torchdiffeq.   \n[7] Shoufa Chen, Peize Sun, Yibing Song, and Ping Luo. 2023. DiffusionDet: Diffusion Model for Object Detection. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 19773\u201319786. IEEE. [8] Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. 2018. Neural Ordinary Differential Equations. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, pages 6572\u20136583.   \n[9] Ting Chen, Lala Li, Saurabh Saxena, Geoffrey E. Hinton, and David J. Fleet. 2023. A Generalist Framework for Panoptic Segmentation of Images and Videos. In ICCV.   \n[10] J.R. Dormand and P.J. Prince. 1980. A family of embedded Runge-Kutta formulae. Journal of Computational and Applied Mathematics, 6(1):19\u201326.   \n[11] Emilien Dupont, Arnaud Doucet, and Yee Whye Teh. 2019. Augmented Neural ODEs. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 3134\u20133144.   \n[12] Chris Finlay, J\u00f6rn-Henrik Jacobsen, Levon Nurbekyan, and Adam M. Oberman. 2020. How to Train Your Neural ODE: the World of Jacobian and Kinetic Regularization. In ICML, volume 119 of Proceedings of Machine Learning Research, pages 3154\u20133164. PMLR.   \n[13] Arnab Ghosh, Harkirat Behl, Emilien Dupont, Philip Torr, and Vinay Namboodiri. 2020. STEER : Simple Temporal Regularization For Neural ODE. In Advances in Neural Information Processing Systems, volume 33, pages 14831\u201314843. Curran Associates, Inc.   \n[14] Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. 2019. FFJORD: Free-Form Continuous Dynamics for Scalable Reversible Generative Models. In ICLR. OpenReview.net.   \n[15] Eldad Haber and Lars Ruthotto. 2017. Stable architectures for deep neural networks. Inverse Problems, 34(1):014004.   \n[16] Xizewen Han, Huangjie Zheng, and Mingyuan Zhou. 2022. CARD: Classification and Regression Diffusion Models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022.   \n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In CVPR, pages 770\u2013778. IEEE Computer Society.   \n[18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising Diffusion Probabilistic Models. In NeurIPS.   \n[19] Sergey Ioffe and Christian Szegedy. 2015. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In ICML.   \n[20] Jacob Kelly, Jesse Bettencourt, Matthew J. Johnson, and David Duvenaud. 2020. Learning Differential Equations that are Easy to Solve. In NeurIPS.   \n[21] Markelle Kelly, Rachel Longjohn, and Kolby Nottingham. 2023. The UCI machine learning repository.   \n[22] Patrick Kidger. 2022. On Neural Differential Equations. CoRR, abs/2202.02435.   \n[23] Patrick Kidger, James Morrill, James Foster, and Terry J. Lyons. 2020. Neural Controlled Differential Equations for Irregular Time Series. In NeurIPS.   \n[24] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In ICLR (Poster).   \n[25] Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features from tiny images.   \n[26] Yann LeCun, Bernhard E. Boser, John S. Denker, Donnie Henderson, Richard E. Howard, Wayne E. Hubbard, and Lawrence D. Jackel. 1989. Handwritten Digit Recognition with a Back-Propagation Network. In NIPS, pages 396\u2013404. Morgan Kaufmann.   \n[27] Haoying Li, Yifan Yang, Meng Chang, Shiqi Chen, Huajun Feng, Zhihai Xu, Qi Li, and Yueting Chen. 2022. SRDiff: Single image super-resolution with diffusion probabilistic models. Neurocomputing, 479:47\u201359.   \n[28] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. 2023. Flow Matching for Generative Modeling. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.   \n[29] Xingchao Liu, Chengyue Gong, and Qiang Liu. 2023. Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.   \n[30] Calvin Luo. 2022. Understanding Diffusion Models: A Unified Perspective. CoRR, abs/2208.11970.   \n[31] Bethany Lusch, J Nathan Kutz, and Steven L Brunton. 2018. Deep learning for universal linear embeddings of nonlinear dynamics. Nature communications, 9(1):4950.   \n[32] Stefano Massaroli, Michael Poli, Jinkyoo Park, Atsushi Yamashita, and Hajime Asama. 2020. Dissecting Neural ODEs. In NeurIPS.   \n[33] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al. 2011. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning. Granada, Spain.   \n[34] Avik Pal, Yingbo Ma, Viral B. Shah, and Christopher Vincent Rackauckas. 2021. Opening the Blackbox: Accelerating Neural Differential Equations by Regularizing Internal Solver Heuristics. In ICML.   \n[35] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. 2022. High-Resolution Image Synthesis with Latent Diffusion Models. In CVPR.   \n[36] Yulia Rubanova, Tian Qi Chen, and David Duvenaud. 2019. Latent Ordinary Differential Equations for Irregularly-Sampled Time Series. In NeurIPS.   \n[37] Alvaro Sanchez-Gonzalez, Victor Bapst, Kyle Cranmer, and Peter W. Battaglia. 2019. Hamiltonian Graph Networks with ODE Integrators. CoRR, abs/1909.12790.   \n[38] Jiaming Song, Chenlin Meng, and Stefano Ermon. 2021. Denoising Diffusion Implicit Models. In ICLR.   \n[39] Jay Whang, Mauricio Delbracio, Hossein Talebi, Chitwan Saharia, Alexandros G. Dimakis, and Peyman Milanfar. 2022. Deblurring via Stochastic Refinement. In CVPR.   \n[40] Laurent Younes. 2010. Shapes and Diffeomorphisms, volume 171. Springer Science & Business Media.   \n[41] Yaofeng Desmond Zhong, Biswadip Dey, and Amit Chakraborty. 2020. Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control. In ICLR.   \n[42] Juntang Zhuang, Nicha C. Dvornek, Xiaoxiao Li, Sekhar Tatikonda, Xenophon Papademetris, and James S. Duncan. 2020. Adaptive Checkpoint Adjoint Method for Gradient Estimation in Neural ODE. In ICML.   \n[43] Juntang Zhuang, Nicha C. Dvornek, Sekhar Tatikonda, and James S. Duncan. 2021. MALI: A memory efficient and reverse accurate integrator for Neural ODEs. In ICLR. OpenReview.net. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we provide full proofs for the propositions in Sec. 4. We first start with formal definition of target trajectory crossing. Assume that we have a data encoder $f_{\\phi}$ and label encoder $g_{\\varphi}$ that transform data $x\\in\\mathbb{R}^{d_{x}}$ and labels $\\boldsymbol{y}\\in\\mathbb{R}^{d_{y}}$ to latent $z_{0},z_{1}\\in\\mathbb{R}^{d}$ , where $z_{0}=f_{\\phi}(x)$ and $z_{1}=g_{\\varphi}(y)$ , respectively. We also choose a predefined dynamics $F(z_{0},z_{1},t)=\\alpha_{t}z_{0}+\\beta_{t}z_{1}=z_{t}$ . Under the assumption of $d>d_{x},d_{y}^{\\;\\;5}$ and both $\\alpha_{t}$ , $\\beta_{t}$ being smooth and nonzero except for $t=0$ and $t=1$ , we define the target trajectory crossing as follows: ", "page_idx": 13}, {"type": "text", "text": "Definition 1 (Target Trajectory Crossing). The encoders $(f_{\\phi},g_{\\varphi})$ are said to induce a target trajectory crossing if there exists a tuple $(t,x,y,x^{\\prime},y^{\\prime})$ such that $\\acute{\\alpha_{t}}f_{\\phi}(\\acute{x})+\\beta_{t}g_{\\varphi}(y)=\\alpha_{t}f_{\\phi}(\\acute{x^{\\prime}})+\\acute{\\beta_{t}}g_{\\varphi}(y^{\\prime})$ for $x\\neq x^{\\prime}$ and $y\\ne y^{\\prime}$ . ", "page_idx": 13}, {"type": "text", "text": "We now provide the proofs for each proposition in the following subsections. ", "page_idx": 13}, {"type": "text", "text": "A.1 Proof of Proposition 1 (Section 4) ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proposition 1. There exist $(f_{\\phi},g_{\\varphi})$ that induces non-crossing target trajectory for any data pair in D while minimizing Llabel_ae. ", "page_idx": 13}, {"type": "text", "text": "Proof. Let the latent space constructed by a set of basis $\\mathbb{I}=\\{e_{1},e_{2},...,e_{d}\\}$ . Since $d>d_{y}$ , we can find a label encoder $g_{\\varphi}$ such that utilizes $k$ basis $\\mathbb{J}=\\{e_{1},e_{2},...,e_{k}\\}$ $(d>k\\geq d_{y}$ ) and minimizes the autoencoding loss (i.e., $g_{\\varphi}(y)=g_{\\varphi}(y^{\\prime})$ if and only if $y=y^{\\prime},$ ). Also, we can find a data encoder $f_{\\phi}$ such that p $\\cdot{\\mathrm{oj}}_{\\mathrm{span}(\\mathbb{K})}f_{\\phi}(x)\\,{\\overset{\\cdot}{=}}\\,{\\mathrm{proj}}_{\\mathrm{span}(\\mathbb{K})}\\,f_{\\phi}(x^{\\prime})$ if and only if $x=x^{\\prime}$ , where $\\mathbb{K}=\\{e_{k+1},...,e_{d}\\}$ . ", "page_idx": 13}, {"type": "text", "text": "Then, suppose that there exists a tuple $(t,x,y,x^{\\prime},y^{\\prime})$ such that $\\alpha_{t}f_{\\phi}(x)+\\beta_{t}g_{\\varphi}(y)=\\alpha_{t}f_{\\phi}(x^{\\prime})+$ $\\beta_{t}g_{\\varphi}(y^{\\prime})$ , i.e., $\\alpha_{t}(f_{\\phi}(x)-f_{\\phi}(x^{\\prime}))\\stackrel{!}{+}\\beta_{t}(g_{\\varphi}(y)-g_{\\varphi}(y^{\\prime}))=0$ . ", "page_idx": 13}, {"type": "text", "text": "Since $g_{\\varphi}(y)-g_{\\varphi}(y^{\\prime})=0$ if and only if $y=y^{\\prime}$ and ${\\mathrm{proj}}_{\\mathrm{span}(\\mathbb{K})}(f_{\\phi}(x)-f_{\\phi}(x^{\\prime}))=0$ if and only if $x=x^{\\prime}$ by construction, such a tuple does not exist. Therefore, there exists $f_{\\phi},g_{\\varphi}$ such that does not induce target trajectory crossing, while minimizing the autoencoding loss. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "A.2 Proof of Proposition 2 (Section 4) ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proposition 2. If $g_{\\varphi}$ is injective, the following equivalence holds: $(f_{\\phi},g_{\\varphi},h_{\\theta})$ minimizes $\\mathcal{L}_{f l o w}$ to zero for all $t\\in[0,1)$ if and only i $f(f_{\\phi},g_{\\varphi})$ induces non-crossing target trajectory and $h_{\\theta}$ perfectly fits the induced target velocity, for any data pair in $\\mathcal{D}$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. $(\\Longleftarrow)$ If $(f_{\\phi},g_{\\varphi})$ induces non-crossing target trajectory for any data pair in $\\mathcal{D}$ , there is a well-defined target velocity $\\frac{d}{d t}z_{t}$ at every $z_{t}$ which is continuous on $t$ . If $h_{\\theta}$ perfectly fits this target velocity for all $(z_{t},t)$ , the flow loss is zero. ", "page_idx": 13}, {"type": "text", "text": "$(\\Longrightarrow)$ We prove by contradiction. Suppose the flow loss is zero but there is a crossing trajectory, i.e., there exists a tuple $(t,x,y,x^{\\prime},y^{\\prime})$ that $z_{t}=z_{t}^{\\prime}$ for $x\\ne x^{\\prime}$ and $y\\ne y^{\\prime}$ . Since the loss is zero for all $t\\in[0,1)$ , the dynamics function $h_{\\theta}$ must output $\\textstyle{\\frac{d}{d t}}F(z_{0},z_{1},t)$ at $z_{t}$ , and $\\textstyle{\\frac{d}{d t}}F(z_{0}^{\\prime},z_{1}^{\\prime},t)$ at $z_{t}^{\\prime}$ . This is a contradiction since at the point of crossing we have $z_{t}=z_{t}^{\\prime}$ but $\\begin{array}{r}{\\frac{d}{d t}F(z_{0},z_{1},t)\\neq\\frac{d}{d t}F(z_{0}^{\\prime},z_{1}^{\\prime},t)}\\end{array}$ . ", "page_idx": 13}, {"type": "text", "text": "B Experiment Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This section describes the implementation detail in our experiments (Sec. 6). ", "page_idx": 13}, {"type": "text", "text": "B.1 Experiment Details for Classification Tasks ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Network Architecture (MNIST) We employ an MLP-based architecture for the MNIST dataset. The data encoder maps each image to a 784-dimensional embedding, using a three-layer MLP with a hidden dimension of 784 and BatchNorm [19]. The dynamics function consists of a three-layer MLP with 2048 hidden dimension. Following NODE [8], we concatenate the time variable to the input of each layer. Consistent with common practices for NODEs [22], normalization layers are not included in the dynamics function. For the label autoencoder, class labels are converted into one-hot vectors and encoded with a single linear layer, while the label decoder utilizes a two-layer MLP with BatchNorm. ", "page_idx": 14}, {"type": "text", "text": "Network Architecture (SVHN, CIFAR10) We adopt a CNN-based architecture for both the SVHN and CIFAR10 datasets. The data encoder utilizes 7 convolutional layers with a hidden dimension of 64. By default, we employ 3x3 convolution kernels, while the final two layers utilize $4\\mathrm{x4}$ kernels with a stride of 2 for downsampling, resulting in the data being encoded into states with dimensions of $7\\mathrm{x}7\\mathrm{x}64$ . The dynamics function consists of 6 convolutional layers with 3x3 kernels and a hidden dimension of 256. At each layer, the time variable is concatenated to the input. We utilize a single linear layer for label encoding, reshaping the output to match the size of embedding. In the label decoding process, we average the feature map over the spatial dimension and apply a single linear layer. ", "page_idx": 14}, {"type": "text", "text": "Training We train all models for 100,000 iterations using the Adam optimizer [24] with a cosine learning rate scheduler. For all classification experiments, we utilize a batch size of 1024. Additionally, we set a maximum training time of 48 hours for the feasibility of experiments. By default, we set the learning rate to 1e-3 for MNIST and 3e-4 for CIFAR10 and SVHN. For our method, we set the ratio of explicitly sampling $t=0$ to $10\\%$ for all datasets. Regarding the noise introduced to the label autoencoder, we set the standard deviation $\\sigma$ to 3 for MNIST, 7 for SVHN, and 10 for CIFAR10. In cases where training of NODE baselines fails, we adjust the learning rate accordingly. The failure cases of NODE baselines are illustrated in Fig. 6. ", "page_idx": 14}, {"type": "image", "img_path": "GOgKhunkfw/tmp/891432f0bdf04cd80f01eae55adc0649118d4bad24139bfe7be247004a7bdf96.jpg", "img_caption": ["Figure 6: Failure cases of NODEs. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "B.2 Experiment Details for Regression Tasks ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Network Architecture We utilize a 2-layer MLP with a hidden dimension of 64 for the data encoder, and a 7-layer MLP with the same hidden dimension for the dynamics function. In the dynamics function, we concatenate the time variable to the input of each layer. On the label side, we employ a single linear layer for encoding and another single linear layer for decoding. ", "page_idx": 14}, {"type": "text", "text": "Training We trained all models for 100,000 iterations using the Adam optimizer with a constant learning rate of 3e-3. Additionally, we split the training set into a train-validation split with a ratio of 6:4, and utilized the validation metric for early-stopping. Early stopping was implemented by measuring the validation metric every 1,000 iterations and setting the patience level to 10. For our model, we sample the noise added to the label autoencoding from $\\mathcal{N}(0,3^{2})$ and the proportion of explicitly sampling $t=0$ as $10\\%$ . ", "page_idx": 14}, {"type": "text", "text": "B.3 Experiment Details about Baselines ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "NODE [8] Following the tolerance values used in NODE [8] and ANODE [11], we used dopri5 [10] solver with the absolute and relative tolerance of 1e-3 for both training and inference. ", "page_idx": 14}, {"type": "text", "text": "STEER [13] STEER introduces a new hyperparameter $b$ that controls the integration interval. Instead of integrating the dynamics function from 0 to 1, STEER integrates from 0 to $t\\,{\\overset{\\,^{\\cdot}}{\\sim}}\\,\\mathrm{Uniform}(1-$ $b,1+b)$ during training. We follow the default configuration of using $b~=~0.99$ for MNIST classification. For other tasks, we set $b$ to 0.1 since higher values resulted in training failures. ", "page_idx": 14}, {"type": "text", "text": "RNODE [12] RNODE introduces two hyperparameters used for the coefficients of regularization terms. Specifically, it regularizes the Jacobian norm and the kinetic energy of the dynamics function to encourage the model to learn straight and constant-speed dynamics. The coefficient of 0.01 was generally used throughout the experiments in the original paper. Therefore, we set both coefficients for the Jacobian norm and kinetic energy as 0.01 for our experiments. ", "page_idx": 15}, {"type": "text", "text": "CARD [16] CARD utilizes discrete timesteps and a hyperparameter $\\beta_{t}$ to schedule the noise level for diffusion modeling. Following the paper\u2019s approach, we use 1000 discrete timesteps and set a linear noise schedule from $\\beta_{1}=1\\mathrm{e}{-4}$ to $\\beta_{1000}=0.02$ . ", "page_idx": 15}, {"type": "text", "text": "B.4 Computation Resources ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We conducted experiments on our internal cluster with two types of machine. We list their specifications below. ", "page_idx": 15}, {"type": "text", "text": "1. Intel Xeon Gold 6330 CPU and NVIDIA RTX A6000 GPU (with 48GB VRAM)   \n2. Intel Xeon Gold 6230 CPU and NVIDIA RTX 3090 GPU (with 24GB VRAM) ", "page_idx": 15}, {"type": "text", "text": "We utilized the first machine for image classification experiments, and latter one for regression experiments. We expect training our model will take about $90\\;\\mathrm{min}$ ., 270 min, $230\\;\\mathrm{min}$ . for classification experiments on MNIST, SVHN and CIFAR10, respectively. For regression tasks, we estimate training cost, summing up for all splits and datasets, would cost about 288 GPU hours. ", "page_idx": 15}, {"type": "text", "text": "C Relation to Koopman Autoencoder ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Our model with linear dynamics shares the high-level motivation with Lusch et al. (2018) [31], which aims to find an embedding space that yields linear dynamics between source and target. Regardless of the theoretical background, both flow matching and Koopman operator theory are promising approaches that seek to interpret a nonlinear system within a well-studied linear framework. ", "page_idx": 15}, {"type": "text", "text": "At the same time, we identify several differences between our work and the line of research based on Koopman operator theory. While those works mainly focus on a systematic way to obtain a linearized representation of the underlying nonlinear dynamics (with eigenfunctions), our work aims to find a way to learn it in a simulation-free manner, avoiding the heavy computation of forward simulation (e.g., which appears in $\\mathscr{L}_{l i n}$ of Lusch et al. (2018) [31]) from an initial state to an end state. Additionally, compared to the discrete depth neural networks that have a single linear layer processor, our proposed method is generally applicable to any nonlinear dynamics that connects two endpoints $z_{0}$ and $z_{1}$ , exemplified as convex or concave as discussed in Sec. 6.3. This implies that in our case, it is possible to have a latent trajectory as a curve in non-Euclidean geometry whenever the interpolated state $z_{t}$ is tractable. ", "page_idx": 15}, {"type": "text", "text": "D Additional Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we extend the discussions in Sec. 6 with additional results. ", "page_idx": 15}, {"type": "text", "text": "D.1 Additional Result of Learning Encoders with Flow Loss ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We present the reconstructed images from the trained data autoencoder in Fig. 7, which are used to analyze the effectiveness of the flow loss as reported in Tab. 2. While the pretrained autoencoder reconstructs the images holistically with minimal information loss (Tab. 2), the embedding space learned without the flow loss fails to maintain the coupling with the predefined dynamics. ", "page_idx": 15}, {"type": "image", "img_path": "GOgKhunkfw/tmp/b662cd685293db1ef14e7bc7bccda4bc675af411b578915095cb694c720aa9e1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 7: Reconstruction from the autoencoder. ", "page_idx": 15}, {"type": "image", "img_path": "GOgKhunkfw/tmp/e724ae2f892bf4fe8d98a4f79176bd76cd275d069ca9dd4c65da35399c981b4e.jpg", "img_caption": ["Figure 8: Cosine similarity between target velocity and predicted velocity over time $t$ . Similar to the flow loss, we measure the cosine similarity between target velocity and predicted velocity. Low cosine similarity near $t=0$ and $t=1$ indicates the occurrence of target trajectory intersection near the endpoints. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "GOgKhunkfw/tmp/f387303f7be085db482a6342168d713970dc3fc8134cd10b0aa9fcaea2d99bb8.jpg", "img_caption": ["Figure 9: Ablation study of optimization techniques on MNIST and SVHN datasets, corresponding to Fig. 5. Explicitly sampling $t\\,=\\,0$ prevents suboptimal solutions, while adding noise to label autoencoding improves generalization performance. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "D.2 Location of Crossing Points ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Fig. 8, we extend the analysis in Sec. 6.3 to identify where target trajectory intersections occur. Specifically, we measure the cosine similarity between target velocity and predicted velocity across t. By measuring cosine similarity instead of MSE, we can ignore the effect of absolute scale in the embedding space and accurately compare ANODE $^+.$ FM, Autoencoder+FM, and our method. Model variants that do not learn encoders with flow loss particularly suffer from trajectory crossing near both endpoints, showing low prediction accuracy for the direction of target velocity. In contrast, our proposed method shows consistently high cosine similarity, mitigating the issue of target trajectory intersections near these regions. Since both endpoints are constructed from encoders, the result also supports our claim that the encoders should be trained with flow loss to effectively penalize such intersections. ", "page_idx": 16}, {"type": "text", "text": "D.3 Additional Results of Ablation on Optimization Techniques ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We provide additional results on the optimization techniques in Fig. 9. Consistent with the findings on CIFAR10 (Fig. 5), the model without explicitly sampling at $t=0$ converges to suboptimal solutions, while introducing noise during label autoencoding improves test accuracy. ", "page_idx": 16}, {"type": "text", "text": "D.4 Additional Experiment Result for UCI Regression Tasks ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The full results on UCI regression tasks with various solvers are shown in from Tab. 3 to 9. As discussed in the main text, our method with linear dynamics assumption shows superior performance compared to baselines in low-NFE (1-2 steps) regime, which aligns with our observations in image classification experiments. ", "page_idx": 16}, {"type": "table", "img_path": "GOgKhunkfw/tmp/f169efa290dd62dc7f4419958b3ae6c7a24364b48d29c7b441c16f3df07b1390.jpg", "table_caption": ["Table 3: Experiment results on UCI regression tasks with Euler 1-step solver. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "GOgKhunkfw/tmp/ce523ab7ffecc14d3cb58381f79aeba4a2c0b5d1bae29cea99f9e40884fae4b6.jpg", "table_caption": ["Table 4: Experiment results on UCI regression tasks with Euler 2-step solver. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "GOgKhunkfw/tmp/62a479d4d270533ddea2e5d1b01f629e7c06f04fbfb6c260e46f0506f9fa2925.jpg", "table_caption": ["Table 5: Experiment results on UCI regression tasks with Euler 10-step solver. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "GOgKhunkfw/tmp/fbbd413388262e66283d883cec8ba1cd657c8ec5732057ea22939dc9dcd8e0e3.jpg", "table_caption": ["Table 6: Experiment results on UCI regression tasks with Euler 20-step solver. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "GOgKhunkfw/tmp/315ec94248cedf65e0d94b81ca16f8fc20ca22aabf9b4f6d74204d7555f799bb.jpg", "table_caption": ["Table 7: Experiment results on UCI regression tasks with Euler 100-step solver. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "GOgKhunkfw/tmp/26120ce888c2b7d0b984469433e15350175a1c5b9635404105b0e5f1ded67dde.jpg", "table_caption": ["Table 8: Experiment results on UCI regression tasks with Euler 1000-step solver. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "GOgKhunkfw/tmp/c4538c6c0c009bdd2d0c62b2265f1962ff45f275ab0b77d2be2ed019442e7012.jpg", "table_caption": ["Table 9: Experiment results on UCI regression tasks with dopri5 solver. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Our main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We discussed the limitations of our work in Sec. 7. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We provide the full set of assumptions and proof for theoretical analysis. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide all the information needed to reproduce our work. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We release the code with instructions at https://github.com/seminkim/ simulation-free-node. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We specified all the experiment details either in the main text or in the appendix. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: We reported either $95\\%$ CI or standard deviation for UCI regression tasks. However, we were not able to repeat image classification experiment due to the limited computational resources. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provided sufficient information on the computer resources in the appendix. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Our research conforms, in every respect, with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: We observed no particular societal impact to address. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: We do not have experiments with dataset having such risk. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We properly cited all the datasets and assets that we used. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 23}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provided the code with documentation in our submission. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]