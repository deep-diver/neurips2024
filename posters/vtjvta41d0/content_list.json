[{"type": "text", "text": "Stability and Generalizability in SDE Diffusion Models with Measure-Preserving Dynamics ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Weitong Zhang1 Chengqi Zang2 Liu Li1 Sarah Cechnicka1 ", "page_idx": 0}, {"type": "text", "text": "Cheng Ouyang1,3 Bernhard Kainz1,4 ", "page_idx": 0}, {"type": "text", "text": "1 Imperial College London, UK, 2 University of Tokyo, JP, 3 University of Oxford, UK 4Friedrich-Alexander University Erlangen-N\u00fcrnberg, GER weitong.zhang20@imperial.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Inverse problems describe the process of estimating the causal factors from a set of measurements or data. Mapping of often incomplete or degraded data to parameters is ill-posed, thus data-driven iterative solutions are required, for example when reconstructing clean images from poor signals. Diffusion models have shown promise as potent generative tools for solving inverse problems due to their superior reconstruction quality and their compatibility with iterative solvers. However, most existing approaches are limited to linear inverse problems represented as Stochastic Differential Equations (SDEs). This simplification falls short of addressing the challenging nature of real-world problems, leading to amplified cumulative errors and biases. We provide an explanation for this gap through the lens of measure-preserving dynamics of Random Dynamical Systems (RDS) with which we analyse Temporal Distribution Discrepancy and thus introduce a theoretical framework based on RDS for SDE diffusion models. We uncover several strategies that inherently enhance the stability and generalizability of diffusion models for inverse problems and introduce a novel score-based diffusion framework, the Dynamics-aware SDE Diffusion Generative Model $(\\mathrm{D^{3}G M})$ . The Measure-preserving property can return the degraded measurement to the original state despite complex degradation with the RDS concept of stability. Our extensive experimental results corroborate the effectiveness of $\\bar{\\mathrm{D^{3}G M}}$ across multiple benchmarks including a prominent application for inverse problems, magnetic resonance imaging. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion probabilistic models [53, 54, 52] have demonstrated impressive performance across various image generation tasks, primarily by modeling a diffusion process and then learning an associated reverse process. Among the many commonly used approaches [63], diffusion models that incorporate the concept of score functions [28, 55] can capture the intrinsic random fluctuations of the forward diffusion process, positioning them as a good choice for in-depth analysis. Score-based generative models (SGMs) entail gradually diffusing images towards a noise distribution, and then generating samples by chaining the score functions at decreasing noise levels with score-based sampling approaches. One such example of an SGM with a score-based sampling technique, known as score matching [54], has gained popularity for density estimation [16]. It employs methods such as ", "page_idx": 0}, {"type": "text", "text": "Langevin dynamics [21, 43, 30] and SDEs [29, 37, 31, 71] to simulate the underlying probability distribution of training samples. ", "page_idx": 1}, {"type": "text", "text": "However, vanilla unconditional SGMs can be extended to inverse problems by leveraging an implicit prior distribution, based on the available counterpart measurement, subjected to corruption and/or noise. To this end transitionary SGMs enable an iterative recovery of the data from this noisy counterpart, instead of relying on Gaussian white noise as a starting point [66, 37, 39, 12, 20, 34]. ", "page_idx": 1}, {"type": "text", "text": "Intuitively, leveraging priors and a generative capacity into transitionary SGMs offers the possibility of exploring high quality reconstruction and restoration and gaining better performance. However, current transitionary SGMs that incorporate priors have largely overlooked the unreliable quality of the prior and its measurement. Empirically, transitionary SGMs cannot always be trusted in terms of stability and efficiency, especially in a regime of non-uniformly distributed noise or corrupted signal quality [9, 66, 37]. Hence, the exploitation of transitional learning within SGMs does not come without costs as their advantages vanish in limited data quality settings. Theoretical understanding is notably lacking in this field with the following fundamental open problem: Can we realize reliable transitionary diffusion processes in practice for inverse problems with a theoretical guarantee? ", "page_idx": 1}, {"type": "text", "text": "While recent works have started to lay down a theoretical foundation for these models, a detailed understanding is still lacking. Current best practice advocates for smaller initialisation values (e.g., noise schedule [66, 20], instead of large values [15, 61] to ensure that the forward dynamics brings the diffusion sufficiently close to a known prior and simple noise distribution. However, a proper choice of the values conditioned on the prior within a theoretical framework should be preferred for a better approximation of the score-matching objective and higher computational efficiency. To fully facilitate the power of reversion and generation of transitionary SGMs and to mitigate the influence of low-quality measurements for solving inverse problems, this paper provides a measurepreserving dynamics of random dynamical system (RDS) perspective as a promising way to obtain reliable reversion and generation. Notably, our \u2018measure\u2019 is not only the observations (e.g., degraded images), but also represents the invariant probability measure (distribution) of the RDS. This allows to consider the concept of an RDS stability and to frame challenging degradation learning within a measure-preserving dynamical system. Thus, we can start from a transitionary SGM interpretation of diffusion models and connect RDS to the SDE in transitionary SGMs. The pitfalls (e.g., Instability) are discussed in Sect. 3 and further implications can be found in the Appendix. Transitionary SGMs have not been fully explored before, and we provide a theoretical interpretation of a stationary process as a possible solution. ", "page_idx": 1}, {"type": "text", "text": "Our $\\mathrm{D^{3}G M}$ framework is abstracted from transitionary SGMs. The key to our framework is a stationary process following measure-preserving dynamics to ensure the stability and generalizability of the diffusion, as well as reducing the influence of accumulated error, distribution bias and degradation discrepancy. Our contributions can be summarised as follows: ", "page_idx": 1}, {"type": "text", "text": "1. Temporal Distribution Discrepancy: We conduct a rigorous theoretical examination of the instability issue of transitionary SGMs, measured as Temporal Distribution Discrepancy (i.e., lower bound of modeling error). This analysis sheds light on critical aspects related to stability and generalizability1, effectively addressing an unexplored fundamental gap in the understanding of solving challenging inverse problems with SDEs. ", "page_idx": 1}, {"type": "text", "text": "2. $\\mathbf{D}^{3}\\mathbf{GM}$ Framework: We propose a solution, $\\mathrm{D^{3}G M}$ , and an explanation from measure-preserving dynamics of Random Dynamical Systems (RDS). \u2018Measure\u2019 includes both measurements (degraded image) and invariant measures (distribution) of RDS, which allows complex degradation learning and enhances restoration and reconstruction accuracy. ", "page_idx": 1}, {"type": "text", "text": "3. Thorough Evaluation: Our contributions are substantiated by extensive validation. We demonstrate the practical beneftis of our $\\mathrm{D^{3}G M}$ framework across various benchmarks, including challenging tasks such the reconstruction of Magnetic Resonance Imaging (MRI) data. ", "page_idx": 1}, {"type": "text", "text": "We address the instability of diffusion models for inverse problems under domain shift and concept drift (unknown and heterogeneous degradation). This leads to what we believe is a completely novel view on the theoretical foundation of how the degradation process is modelled. The result is an approach that is more in line with the original intention of the theory of diffusion. We chose inverse problems as a relevant application area to demonstrate our ideas but also included a variety of challenging problem settings to explore the generalizability of $D^{3}G M$ . To the best of our knowledge, no other method can handle a diverse range of challenging tasks like real-world dehazing, compressed MRI reconstruction, blind MRI super-resolution, etc. with a unified underlying theoretical framework. In Tab. 1 we illustrate the key differences of $D^{3}G M$ compared with SGMs and transitionary SGMs. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "SGMs. We will follow the typical construction of the diffusion process $\\mathbf{\\boldsymbol{x}}(t),\\;t\\;\\in\\;[0,T]$ with $\\pmb{x}(t)\\in\\mathbb{R}^{d}$ . Concretely, we want ${\\pmb x}(0)\\sim p_{0}({\\pmb x})$ , where $p_{0}=p_{\\mathrm{data}}$ , and $\\pmb{x}(T)\\sim p_{T}$ , where $p_{T}$ is a tractable distribution that can be sampled. In this work, we consider the score-based diffusion form of the SDE [55]. Consider the following It\u00f4 diffusion process defined by an SDE: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d\\pmb{x}=\\pmb{f}(\\pmb{x},t)d t+g(t)d\\pmb{W},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\pmb{f}:\\mathbb{R}^{d}\\mapsto\\mathbb{R}^{d}$ is the drift coefficient of ${\\mathbf{}}x(t)$ $,g:\\mathbb{R}\\mapsto\\mathbb{R}$ is the diffusion coefficient coupled with the standard ${\\mathrm{d}}\\cdot$ -dimensional Wiener process $\\pmb{w}\\,\\in\\,\\mathbb{R}^{d}$ . By carefully choosing ${\\bar{f}},g$ , one can achieve a spherical Gaussian distribution as $t\\rightarrow T$ . For the forward SDE in Eq. 1, there exists a corresponding reverse-time SDE [3, 55]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d\\pmb{x}=[\\pmb{f}(\\pmb{x},t)-\\pmb{g}(t)^{2}\\underbrace{\\nabla_{\\pmb{x}}\\log p_{t}(\\pmb{x})}_{\\pmb{x}}]d t+\\pmb{g}(t)d\\pmb{W},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $d t$ is the infinitesimal negative time step, and $\\mathbf{\\nabla}w$ is the Brownian motion running backwards. The score function $\\nabla_{\\pmb{x}}\\log p_{t}(\\pmb{x})$ is in general intractable and thus SDE-based diffusion models approximate it by training a time-dependent neural network under a score function [57, 28]. ", "page_idx": 2}, {"type": "text", "text": "Transitionary SGMs. [17, 37, 66, 34, 20, 12] leverage a transitionary iterative denoising paradigm for the inverse problems. In inverse problems, such as super-resolution, we have an (nonlinear, partial, and noisy) observation $\\textit{\\textbf{y}}$ of the underlying high-quality signal $\\textbf{\\em x}$ . The mapping $\\boldsymbol{x}\\mapsto\\boldsymbol{y}$ is many-to-one, posing an ill-posed problem. In this case, a strong prior on $\\textbf{\\em x}$ is needed for finding a realistic solution. Formally, the general form of the forward (measurement) model is: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{y}=\\pmb{\\mathcal{A}}\\left(\\pmb{x}\\right)+\\pmb{n},\\quad\\pmb{y},\\pmb{n}\\in\\mathbb{R}^{n},\\pmb{x}\\in\\mathbb{R}^{d},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $A(\\cdot)$ : $\\mathbb{R}^{d}\\mapsto\\mathbb{R}^{n2}$ , oftentimes $n\\ll d$ is the forward measurement operator and $\\mathbfit{\\Delta}$ is the measurement noise, assuming $\\boldsymbol{n}\\sim\\mathcal{N}\\left(0,\\sigma^{2}\\boldsymbol{I}\\right)$ . While sharing a similar aim of bridging $\\textit{\\textbf{y}}$ and $\\textbf{\\em x}$ in transitionary SGMs, different mathematical frameworks have been used: [17] employs Inversion by Direct Iteration; [37, 34, 20] model it as a Mean-reverting SDE. ", "page_idx": 2}, {"type": "text", "text": "Transitionary SGM has become an increasingly important line of SDE research due to the applicability on images with theoretical guarantees. However, they often perfrom poorly in real-world scenarios. To provide a theoretical investigation of this gap, we interpret Transitionary SGM as Ornstein-Uhlenbeck $(O U)$ process. This perspective allows us to understand the random fluctuations in image degradation as stochastic processes, providing a foundation to integrate random dynamical systems (RDS) with the diffusion process as a natural extension of the SDE framework involving the OU process. ", "page_idx": 2}, {"type": "text", "text": "The Measure-preserving property is introduced from the perspective of RDS: The distribution can still return to the original state despite severe degradation. Our approach constructs a bridge from measure-preserving dynamical system to transitionary SGM through measure-preserving dynamics, and highlights the Temporal Distribution Discrepancy in Sect. 3. Subsequently, we address this issue of instability: by incorporating a measure-preserving strategy into the solution of inverse problems, which is detailed in Sect. 4. This covers counterpart modeling, bridging a transition from uncertain diffusion modeling to deterministic solutions, yielding significant improvements in both performance and efficiency as demonstrated in Sect. 5. More details can be found in Sect. 6 and Appendix. ", "page_idx": 2}, {"type": "text", "text": "3 Instability Analysis: Transitionary SGMs with Corrupted SDE Diffusion ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Ornstein-Uhlenbeck (OU) process. An OU process is a common case in transitionary SGMs, where $x_{t}$ is defined using an SDE: $d x_{t}=-\\theta_{t}x_{t}d t+\\sigma_{t}d W_{t}$ . $W_{t}$ is standard Brownian motion. A drift term $\\mu$ can be introduced: ", "page_idx": 2}, {"type": "table", "img_path": "VTJvTa41D0/tmp/1759c266ba8abd0f6c48b3dae8c0fa18cd9ff863ed1a19736c650cd536ddb150.jpg", "table_caption": ["Table 1: Differences between state-of-the-art SDE diffusion-based approaches. "], "table_footnote": [], "page_idx": 3}, {"type": "equation", "text": "$$\nd x_{t}=\\theta_{t}(\\mu-x_{t})d t+\\sigma_{t}d W_{t},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mu$ denotes state mean, reflecting the expected state of the measurement (e.g., corrupted image [37], noisy speech [59]) over time. $\\theta_{t},\\,\\sigma_{t}$ are time-dependent parameters. The drift term corrects deviations from the constant $\\mu$ , effectively pulling the process towards $\\mu$ $\\left(t\\rightarrow\\infty\\right)$ ) with Stability (in Appx. G) as opposed to pure noise in Eq. 1. ", "page_idx": 3}, {"type": "text", "text": "Measure-preserving Dynamics in SDE Diffusion. The solution of the above SDE can be represented by a continuous-time random dynamical system $\\varphi$ defined on a complete separable metric space $(X,d)$ . (See precise definition of RDS in Appx. C). More generally, we can extend the RDS to a two-sided solution operator with a flow map. The base flow driven by Brownian motion can be written as $W\\left(t,\\vartheta_{s}(\\bar{\\omega})\\right)=W(t+s,\\omega)-\\bar{W_{\\big(s,\\omega\\big)}}$ . ", "page_idx": 3}, {"type": "text", "text": "Proposition 1 After extending the solution of the OU process to RDS, the measure-preserving RDS $\\varphi$ should meet the property $\\varphi(t,s;\\omega)x=\\varphi\\left(t-s,0;\\vartheta_{s}\\omega\\right)x$ . However, OU processes with timevarying coefficients usually do not satisfy this property. In this situation, the system breaks the forward-reverse processes, making it difficult to maintain stability. ", "page_idx": 3}, {"type": "text", "text": "Intuition 1. A two-sided measure-preserving random dynamical (MP-RDS) system formulation enables us to use the Poincare recurrence theorem [45], (see precise statement in Appx.D), intuitively, with a two-sided MP-RDS $\\varphi_{t}$ , the Poincare recurrence theorem ensures that the system $\\varphi_{t}$ starts from terminal condition $x_{T}$ , run backward in time, will hit a region $(x_{0}-\\epsilon,x_{0}+\\epsilon)$ for small $\\epsilon$ in finite time, where $x_{0}$ is the high-quality image. ", "page_idx": 3}, {"type": "text", "text": "Example 1. Following Intuition 1. and Prop. 1, suppose that the OU\u2019s $\\theta_{t}$ follows a cosine schedule, such that $\\theta_{t}=\\cos(t)$ for $0\\leq t\\leq T$ , then for some $0\\leq s\\leq t\\leq T$ , $\\varphi(t,s;\\omega)x\\neq$ $\\varphi\\left(t-s,0;\\vartheta_{s}\\omega\\right)x$ because the change of $\\theta_{t}$ w.r.t. time is not uniform. The OU-process instability exists due to Temporal Distribution Discrepancy (Prop. 2). ", "page_idx": 3}, {"type": "text", "text": "At a high level, Proposition 1 can be extended to show that there exists a compact attracting set at any $-\\infty<t<\\infty$ , and this convention has allowed us to characterize the attractor $K(\\omega)=\\mathcal{N}\\bar{(}\\mu,\\sigma^{2}/2\\theta\\bar{)}$ The closed-form distribution for y can be complex and may not be tractable depending on the particular scenarios of the actual image degradation process $\\pmb{\\mu}$ . The modification of $\\pmb{\\sigma}$ and $\\pmb{\\theta}$ is used to regularize the perturbation and attempt to close the distribution. However, these injections might bypass the stationary process. More details can be found in Appx. D. ", "page_idx": 3}, {"type": "text", "text": "Instability-Temporal Distribution Discrepancy. Given the process $\\operatorname{OU}\\left(x_{t},\\mu;t,\\theta\\right)$ with Eq. 4, where $x_{T}\\ne x_{\\infty}$ for finite $T$ , indicates that the perturbed state cannot move towards the degraded LQ image and fails in matching the theoretical distribution. This inherent discrepancy further causes bias in the estimation of $\\mu_{t}$ , which gradually accumulates into error in the reverse process. ", "page_idx": 3}, {"type": "text", "text": "Temporal Distribution Discrepancy is illustrated by Proposition 2 (proof can be found in Appx. E): ", "page_idx": 3}, {"type": "text", "text": "Proposition 2 Given Eq. 3 and Eq. 4, and assume that the score function is bounded by $C$ in $L^{2}$ norm, then the discrepancy between the reference and the retrieved data is, with probability $(1-\\delta)$ at least: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|x_{0}-\\operatorname{OU}(x_{0},\\mu;T,\\theta)\\|_{2}^{2}\\geq|\\left((x_{0}-\\mu)^{2}-\\sigma_{T}^{2}/2\\theta_{T}\\right)\\mathrm{e}^{-2\\bar{\\theta}_{T}}+\\sigma_{T}^{2}/2\\theta_{T}}\\\\ &{\\,-\\,\\sigma_{m a x}^{2}\\left(C\\sigma_{m a x}^{2}+d+2\\sqrt{-d\\cdot\\log\\delta}-2\\log\\delta\\right)|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Intuition 2. Intuitively, Proposition 2 provides a theoretical measurement on how the difference between finite iteration distribution and the asymptotic distribution of the OU-process in $L^{2}$ could further enlarge the discrepancy between the retrieved image and the actual HQ image. Discrepancies are typically explained by complex degradation $\\nu s$ . monotonic modeling. ", "page_idx": 4}, {"type": "text", "text": "For a noisy inverse problem, the retrieved data with any finite $T$ depends on $\\sigma_{t},\\mu_{t},\\lambda,T,\\bar{K}$ (Lipschitz constant). This proposition also correlates to [39], where the lower bound of the distance between the high quality image and retrieved image in $L^{2}$ norm in our model is further enlarged by this discrepancy, which correlates to the term $\\bar{\\left((x_{0}-\\mu)^{2}-(\\sigma_{T}^{2}/2\\theta_{T})\\right)}\\,e^{-2\\bar{\\theta}_{T}}+\\sigma_{T}^{2}/2\\theta_{T}$ . ", "page_idx": 4}, {"type": "text", "text": "Another way to further minimize this bound is through the term $e^{-2{\\bar{\\theta}}_{T}}$ with $[0,T]$ normalized to $[0,1]$ . What we refer to as $\\theta$ -schedule corresponds to the exact functional form of $\\theta_{t}$ , several schedules can be set here, e.g., constant, linear, cosine, and log. At a high level, the discrepancy between the reference and the retrieved data stems from the divergence between the forwarded final state and the low quality image. Eq. 5 can be factored into three constituent parts: the data residual, the stationary disturbance, and the random noise. While conditional diffusion generation entails a trade-off between variability and faithfulness [66], the persistent discrepancy within the residual has a significant impact on the generalizability of solving the transitionary tasks. This also establishes a connection with SDEdit [39] and CCDF [12]. When fitting inverse problems involving paired data into diffusion models, while accounting for deviations and degradation, inserting them into Eq. 4 directly may not be the most effective strategy. ", "page_idx": 4}, {"type": "text", "text": "During sampling and inference with the degraded input $y$ , the discrepancy identified in Prop. 2 intensifies. The complex degradations in $y$ exacerbate the divergence from the expected $\\mu$ distribution, significantly impacting the accuracy of the restored data $\\scriptstyle{\\hat{x}}_{0}$ . More details are in Appx. F. ", "page_idx": 4}, {"type": "text", "text": "4 Towards Stability: Measure-preserving Dynamics in SDE Diffusion ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In Sect. 3, we extrapolate and theorize the Temporal Distribution Discrepancy on $\\mu$ and $x$ in the diffusion model for the inverse problem. Our key idea is to combine the stationary process to alleviate the Temporal Distribution Discrepancy problem following the measure-preserving dynamics from RDS. Recall that our \u2018measure\u2019 is not only the measurements (i.e., degraded image), but also represents the invariant measure (distribution) of the RDS. ", "page_idx": 4}, {"type": "text", "text": "We begin by describing the forward and reverse processes of the $\\mathrm{D^{3}G M}$ , which serves as a stable bridge between the quality data and the counterpart measurement. We adapt score-based training methods to estimate this SDE. Following this, we describe the essential constructions for preserving the stationary process in the diffusion model and solving for Temporal Distribution Discrepancy on an orthogonal basis compared to current transitionary diffusion models. ", "page_idx": 4}, {"type": "text", "text": "Measure-preserving Dynamics with the Stationary Process. Following Prop. 1, in a SDE Diffusion from 0 to $T$ , the corresponding \u2018attractors\u2019 (states) can be viewed as $\\mathcal{N}\\left(\\mu+(x_{0}-\\mu)\\mathrm{e}^{-\\bar{\\theta}_{t}},\\sigma_{t}^{2}(1-\\mathrm{e}^{-2\\bar{\\theta}_{t}})/2\\theta_{t}\\right)$ . We can guide SDE Diffusion towards a stable and robust solution based on the properties of Measure-preserving in RDS. It can be extended to impose that for every t, 2\u03c3\u03b8t $\\begin{array}{r}{\\frac{\\sigma_{t}^{2}}{2\\theta_{t}}\\,=\\,\\lambda^{2}\\,}\\end{array}$ , where $\\lambda$ is the variance of the designated stationary measure forward process. This convention allows us to reduce the regularization on two variables $\\sigma_{t},\\theta_{t}$ to just one variable to satisfy the property of the measure-preserving dynamics in the asymptotic sense, i.e., $\\begin{array}{r}{\\operatorname*{lim}_{t\\rightarrow\\infty}\\varphi(t,s;\\omega)x=\\operatorname*{lim}_{t\\rightarrow\\infty}\\varphi\\left(t-s,0;\\vartheta_{s}\\omega\\right)x.}\\end{array}$ . This convention allows us to characterize the attractor of the system as $K(\\omega)=\\mathcal{N}(\\mu,\\lambda^{2})$ . ", "page_idx": 4}, {"type": "text", "text": "The definition and constraint of the attractor are significant; without imposing this constraint, the measure-preserving property cannot be maintained, and the system would degrade into a Coefficient Decoupled SDE (Coe. Dec. SDE), we also analyse this in Fig. 2 and Tab. 11 in Appx. H. ", "page_idx": 4}, {"type": "image", "img_path": "VTJvTa41D0/tmp/2c2ece84446694e407368c1482bf669742d718ec4a4e51bc365fbf69d1f5870c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 1: Dynamics-aware SDE Diffusion Generative Model $(\\mathrm{D^{3}G M})$ . When extending transitionary SDEs to random dynamical systems (RDS), their measure-preserving property should be kept to maintain stability. This corresponds to driving the SDE towards the drift term $\\mu$ (LQ). There is a Temporal Distribution Discrepancy which results from the gap between the forward estimation $x_{T}$ and the low quality image in the SDE. With the distribution aligned between $x_{T}$ and $\\mu$ , the SDE can be made more robust to inverse problems. Reconstruction results for low quality (LQ) images after application of our $\\mathrm{D^{3}G M}$ method, on different tasks, compared to the ground truth (GT) on two domains - The frequency domain: MRI Reconstruction (undersampling factor 8x, 16x, frequency masks are colored red); MRI Super-resolution (up-scaling factor of X4, cross-domain evaluation). The image domain: Real Dense Haze Removal; Rain Removal (light, heavy). ", "page_idx": 5}, {"type": "text", "text": "Example 2. When $\\frac{\\sigma}{\\theta}\\rightarrow\\infty$ , the attractor becomes excessively large, reducing the significance of $\\mu$ . SDEs exhibiting this behavior are defined as Coefficient Decoupled SDEs. In practice, $\\mu$ demonstrates non-infinite properties as an input image, while the corresponding sigma and theta are indeed unconstrained. In such decoupled forms, the coefficients of the attractor size increases $\\frac{\\sigma}{\\mu}$ , diminishing the significance of $\\mu$ . ", "page_idx": 5}, {"type": "text", "text": "Based on Prop. 2, since the temporal distribution discrepancy always exists as long as the running time $T$ is finite, and we want the final state $x_{T}$ to be as close as possible to the distribution of $x_{\\infty}$ . Therefore, we introduce $\\tau$ such that given $T$ , the distribution of $x_{T}$ follows $\\mathcal{N}(\\mu(1-e^{\\bar{\\theta}_{T}})\\:+\\:$ $x_{0}e^{\\bar{\\theta}_{T}},\\tau^{2}\\lambda^{2}(1-e^{2\\bar{\\theta}_{T}}))$ , and $x_{\\infty}$ follows $\\mathcal{N}(\\mu,\\tau^{2}\\lambda^{2})$ , with $\\tau>1$ , we increase the possibility of a sample $\\tilde{x}_{T}$ from $x_{T}$ to become closer to the distribution of $x_{\\infty}$ , and thus serves as a plausible initial state for the reverse process. We can control how much to close the distributions, either by increasing the stiffness $\\tau$ at the cost of potentially destabilizing the reverse process, or by decreasing $\\tau$ to further smooth the density functions of both distributions at the cost of more reverse iterations. ", "page_idx": 5}, {"type": "text", "text": "By connecting the inverse problem with the analysis above, we clarify the discrepancy in the stationary modeling process from measure-preserving dynamics and thereby improve the generalization of diffusion processes and the accuracy of the reverse process. This is particularly important for accommodating the diversity of degradation states and to ensure accurate sampling. ", "page_idx": 5}, {"type": "text", "text": "Forward Process. We describe the forward process as: $d x_{t}=\\theta_{t}(\\mu\\!-\\!x_{t})d t\\!+\\!\\tau\\sigma_{t}d W_{t}$ , parameterized by $\\tau$ to calibrate the SDE modeling, $\\mu$ is the state mean. The parameters $\\theta_{t}$ and $\\sigma_{t}$ , both being timedependent and strictly positive, correspond to the rate of mean reversion and the stochastic volatility, respectively. The selection of $\\theta_{t}$ and $\\sigma_{t}$ offers flexibility in Tab. 2, c.p., Sect. 3. ", "page_idx": 5}, {"type": "text", "text": "Considering the trade-off between complexity and effectiveness, Cos has been chosen for both $\\theta_{t}$ and $\\sigma_{t}$ . This aims at capturing complex temporal dynamics in a computationally tractable manner, thereby optimizing the balance between the performance and calculation convenience. ", "page_idx": 5}, {"type": "table", "img_path": "VTJvTa41D0/tmp/072a8a84c8d3b1c74c7250e97e099ec01e3d5c73a6a1cf6e8e3aaad696d8dc86.jpg", "table_caption": ["Table 2: $\\mu_{t}(\\boldsymbol{x}_{t},t)$ , ${\\boldsymbol{v}}_{t}({\\boldsymbol{x}}_{t},t)$ solutions with various $\\theta_{t}$ , $\\sigma_{t}$ "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "In the forward process, the mean $\\mu_{t}$ approaches the low-quality image with ${\\mathrm{E}}\\left(x_{t}\\right)\\mathrm{~=~}\\mu$ , while the variance tends toward the stationary variance var $\\begin{array}{r}{(x_{t})\\,=\\,\\frac{\\tau^{2}\\sigma^{2}}{2\\theta}\\,}\\end{array}$ \u03c4 22\u03b8\u03c32 . Essentially, the forward SDE transitions the high-quality image to a low-quality counterpart infused with Gaussian noise. The discretized SDE for the forward process is $\\bar{x_{t_{i}}}=\\bar{x_{t_{i-1}}}+\\bar{\\theta_{t_{i-1}}}(\\mu-x_{t_{i-1}})\\Delta t+\\tau\\sigma_{t_{i-1}}\\Delta W_{i}$ . We employ a transition strategy utilizing a varied stationary variance. Additionally, we execute an unconditional update, which operates without the need for matching in the reverse process. These not only allow image corruption but also provides effective adaptability for improvements. ", "page_idx": 6}, {"type": "text", "text": "Reverse Process. The reverse process aims at reconstructing the original image by gradually denoising a low quality image. It utilizes the score of the marginal distribution, denoted as $\\nabla_{x}\\log{\\hat{p}_{t}(x)}$ , and is governed by: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{d}x_{t}=\\left[\\theta_{t}(\\mu-x_{t})-\\tau^{2}\\sigma_{t}^{2}\\nabla_{x}\\log\\hat{p}_{t}(x)\\right]\\mathrm{d}t+\\tau\\sigma_{t}\\mathrm{d}\\widehat{W}_{t}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The reverse-time $\\mathrm{D^{3}G M}$ process of Eq. 6 can be found in Appx. D. This closely mirrors the forward process and incorporates an additional drift term proportional to the score of the marginal distribution. The ground truth score for this process, necessary for training our generative model, is: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\nabla_{x}\\log\\hat{p}_{t}(x\\mid x_{0})=-\\frac{x_{t}-\\mu_{t}(x)}{v_{t}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mu_{t}(x)$ represents the random attractor of the process at time $t$ , and $v_{t}$ is the variance. Our training objective is defined as the minimization of the expected discrepancy between the predicted and true scores over the data distribution: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\theta^{*}=\\underset{\\theta}{\\arg\\operatorname*{min}}\\;\\mathbb{E}_{t,(\\mathbf{x}_{0},\\mathbf{y}),\\mathbf{z},\\mathbf{x}_{t}}\\left[w\\;\\big\\|S_{\\theta}(\\mathbf{x}_{t},\\mathbf{y},t)-\\nabla_{\\mathbf{x}_{t}}\\log p_{0t}(\\mathbf{x}_{t}\\mid\\mathbf{x}_{0},\\mathbf{y})\\big\\|_{2}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $w\\;=\\;-1/\\tau^{2}$ is a time-dependent weighting function, and $S_{\\theta}$ denotes the score network parameterized by $\\theta$ which approximates the score of the marginal distribution. The optimization is conducted over the network parameters $\\theta$ , under the expectation with respect to the time variable $t$ , the initial image $\\mathbf{x}_{\\mathrm{0}}$ , noisy image $\\mathbf{x}_{t}$ , and data $\\mathbf{y}$ . ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Experimental Settings: We evaluate $\\mathrm{D^{3}G M}$ on various challenging restoration and reconstruction problems. We initially analyze our method by examining its performance with closely related diffusion formulation variants. Subsequently, we benchmark $\\dot{\\mathrm{D}}^{3}\\mathrm{GM}$ against the state-of-the-art techniques in these domains. For comprehensive evaluation across all experiments, we report the PSNR [25] and SSIM [58] for pixel- and structural-level alignment, LPIPS [72] for measuring perceptual variance. An in-depth description of our implementation is provided in Appx. G. ", "page_idx": 6}, {"type": "text", "text": "5.1 Stability: Illustrations of the Measure-preserving Dynamics within Diffusion Models ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "SGM, and Transitionary SGMs vs. $\\mathbf{D}^{3}\\mathbf{GM}$ : ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We perform qualitative and quantitative analyses using variants of closely related formulations for Prop. 1 and 2 and evaluate across (A) SGMs and (B) transitionary SGMs. (A) uses a common score-based SDE, (B) uses a Coefficient Decoupled SDE (e.g., variance exploding SDE with the drift term $\\mu$ ) according to Prop. 1 and OU SDE, alongside our $\\mathrm{D^{3}G M}$ . ", "page_idx": 6}, {"type": "table", "img_path": "", "table_caption": ["Table 3: Quantitative results for Rain100H and Table 4: Quantitative results for ORain100L.(best in bold and second best underlined) HAZE and Dense-Haze. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "VTJvTa41D0/tmp/b2c395adcef8800610c9ed70a94c688d8e6f7ffe375e458e429ff2ee92804f00.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "", "img_caption": ["Figure 3: Qualitative results for (a) deraining and (b) dehazing. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Following Tab. 1, VPB [73] can be regarded as a Coefficient Decoupled SDE, and IR-SDE [37] as an OU SDE. Our results in Fig. 2 illustrate that $\\mathrm{D^{3}G M}$ converges stably towards the expected distribution, unlike other methods which exhibit instability or deviation. This highlights the reliance of other techniques, e.g., scorebased SDEs, on retrospective measurement consistency corrections. ", "page_idx": 7}, {"type": "text", "text": "Simulated Deraining: We evaluated $\\mathrm{D^{3}G M}$ together with the state-of-the-art deraining strategies: (1) OU SDE method IR-SDE [37], Coefficient Decoupled (CD) VPB [73] and other CNNs [64, 49, 68, 56]. We use two of the most renowned synthetic raining datasets: Rain100H [65] and Rain100L [65]. Rain100H contains 1800 pairs of images with and without rain, along with 100 test pairs. As for Rain100L, ", "page_idx": 7}, {"type": "image", "img_path": "VTJvTa41D0/tmp/585e1d9f08aa0e6e36297a1eda8d777708c8cf51cce7f28e93b29383fa28afde.jpg", "img_caption": ["Figure 2: Sampling trajectories of SGM, transitionary SGMs: Coef. Dec., OU SDE, and $\\mathrm{D^{3}G M}$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "it consists of 200 pairs for training and 100 pairs for testing. We present results based on the PSNR, SSIM, and LPIPS metrics. ", "page_idx": 7}, {"type": "text", "text": "Quantitative results from the two raining datasets are presented in Tab. 3. Based on both distortion and perceptual metrics, $\\mathrm{D^{3}G M}$ is capable of generating the most realistic and high fidelity results as shown in Fig. 3b. ", "page_idx": 7}, {"type": "text", "text": "5.2 Generalizability: $\\mathbf{D}^{3}\\mathbf{GM}$ for real-world data ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Case Study 1: Dehazing. We utilize the real-world datasets O-HAZE [2], Dense-Haze [1], which contain 45 and 55 paired images, respectively. We use the last 5 images of each dataset as the testing set and the rest as the training set following the common split of other methods. Results are shown in Tab. 4 and Fig. 3a. Our work improves results on O-HAZE both quantitatively and qualitatively. Smaller improvements are observed on Dense-Haze. This can be attributed to the severe signal corruption of the Dense-Haze data. A combination with tailored task-specific, Transformer-based methods [47, 22] might lead to further performance gains for such data. Such extensions are beyond the focus of this paper. Qualitatively $\\mathrm{D^{3}G M}$ achieves excellent visual results (Fig. 1 and Fig. 3a). ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Case Study 2: MRI Reconstruction. MRI data is represented in the complex-valued frequency domain, which is distinctly different from the natural image domain. We utilized the fastMRI dataset [69], containing single-channel, complex-valued MRI samples. Implementation details can be found in Appx. G. For a robust comparison, we benchmarked against a diverse set of deep learningbased state-of-the-art reconstruction methods. Although our method does not have a task-specific design, we still get comparable performance (more details and results are provided in Appx. H). Fig. 1 illustrates our reconstruction results from masked $\\mathbf{k}$ -space data for $8\\mathrm{{x}}$ and 16x acceleration, i.e., under-sampling for faster data acquisition. ", "page_idx": 8}, {"type": "table", "img_path": "VTJvTa41D0/tmp/f379282cbfbb1d9c5f0f36172ac29307ea27b28b308227c8d6532e0f9d8a0510.jpg", "table_caption": ["Table 5: Quantitative results for fastMRI dataset with acceleration rates $\\mathbf{\\times8}$ and $\\mathbf{x}16$ . "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "VTJvTa41D0/tmp/f1dee1c1df71137313c64f7c1472defe3e5dd10bc58c4a9921ab0bbc826917b8.jpg", "table_caption": ["Table 6: Quantitative results for IXI MRI SR on unseen datasets. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Case Study 3: MRI Super-resolution (SR). $\\mathrm{DM}^{3}$ dataset is the largest benchmark considered in our MRI SR evaluation. Clinical MRI T2-weighted (T2w) scans are collected from three hospitals with different imaging protocols: HH, Guys, and IOP. For investigating our cross-domain generalization and robustness, a challenging task for both MRI SR and natural image restoration, we trained on HH data with $\\boldsymbol{\\mathrm{k}}$ -space truncation, and tested on Guys and IOP with kernel degradation with an up-scaling factor of X4. More details can be found in Appx. G. ", "page_idx": 8}, {"type": "text", "text": "The methods are tested under unseen data conditions, including different acquisition parameters, MRI scanners (different vendors and field-strengths) and unseen degradations. With $\\mathrm{D^{3}G\\dot{M}}$ we are able to demonstrate varying degrees of improvement, as well as generalizability to the discrepancy within the training domain and across the test domain as shown in Tab. 6. Qualitative results are shown in Fig. 11 and further results across domains in Appx. H. ", "page_idx": 8}, {"type": "text", "text": "6 Discussion and limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Other works, like VPB (CD) [73], $\\mathrm{I}^{2}\\mathrm{SB}$ [34] are based on diffusion bridges assuming that clean and degraded images are already close. Thus, the tractability of the reverse process heavily relies on the validity of the assumed Dirac delta distribution. IR-SDE [37] employs the mean-reverting SDE theorem based on running the reverse SDE with instability. Since unstable errors accumulate in each step, this model will eventually become unable to learn the transformation, e.g., degradation. DPS [9] and CDDB [10] assume that the degradation process is known, or linear operations are directly used to simulate the degradation process, which limits the generalizability of the method. In contrast, $\\mathrm{D^{3}G M}$ is built on the theorem of Measure-preserving RDS, which bridges clean and degraded image distributions while taking both degradation and measurements into account. Moreover, $\\mathrm{\\bar{D}^{3}G M}$ can be extended to a two-sided solution operator (tractability) with a flow map according to Prop. 1. ", "page_idx": 8}, {"type": "text", "text": "Limitations. Even though our results are better than others when the degradation process is very severly corrupted (e.g., real dehazing), the overall quality of the restored image is still limited, which is consistent with Prop. 2. This might be alleviated via guiding the sampling process with priors and enhanced $\\mu$ , such as posterior sampling or degradation maps on the data manifold, but such approaches are still limited as shown in Tab. 7 and Appx I. ", "page_idx": 8}, {"type": "text", "text": "Computational Complexity vs. Performance. Tab. 7 highlights that prior work is often tailor-made for a specific subset of tasks and thus also generalisation-limited for challenging environments in practice. $\\mathrm{D^{3}G M}$ \u2019s focus on a generic robust solutions from an RDS perspective can mitigate this, while maintaining en-par performance with task-specific approaches in Tab. 8. ", "page_idx": 8}, {"type": "table", "img_path": "VTJvTa41D0/tmp/4d4545afcebc77010f0f7d926472cce6970c650c1126bfb68edb4d1f71f0156b.jpg", "table_caption": ["Table 8: $\\mathrm{D^{3}G M}$ vs. recent deraining works. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "VTJvTa41D0/tmp/da004cf458ede5eccbd3ff90b14921d7dc474447348333c55b66abd43a259263.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The proposed $\\mathrm{D^{3}G M}$ framework enhances the stability and generalizability of SDE-based diffusion methods for challenging inverse problems. Our approach, grounded in measure-preserving dynamics of random dynamical systems, ensures broad applicability and relevance. We demonstrate $\\mathrm{\\dot{D}^{3}G M^{\\prime}}$ s effectiveness across various benchmarks, including challenging tasks like MRI reconstruction. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements: This work was supported by the JADS programme and UK Research and Innovation [UKRI Centre for Doctoral Training in AI for Healthcare grant number EP/S023283/1]. HPC resources were provided by the Erlangen National High Performance Computing Center (NHR $@$ FAU) of the Friedrich-Alexander-Universit\u00e4t Erlangen-N\u00fcrnberg (FAU) under the NHR project b143dc and b180dc. NHR funding is provided by federal and Bavarian state authorities. NHR $@$ FAU hardware is partially funded by the German Research Foundation (DFG) \u2013 440719683. Support was also received by the ERC - projects MIA-NORMAL 101083647 as well as DFG 513220538, 512819079. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] C. O. Ancuti, C. Ancuti, M. Sbert, and R. Timofte. Dense-haze: A benchmark for image dehazing with dense-haze and haze-free images. In ICIP, pages 1014\u20131018. IEEE, 2019.   \n[2] C. O. Ancuti, C. Ancuti, R. Timofte, and C. De Vleeschouwer. O-haze: a dehazing benchmark with real hazy and haze-free outdoor images. In CVPR workshops, pages 754\u2013762, 2018.   \n[3] B. D. Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313\u2013326, 1982.   \n[4] H. Bai, J. Pan, X. Xiang, and J. Tang. Self-guided image dehazing using progressive feature fusion. TIP, 31:1217\u20131229, 2022.   \n[5] S. Bell-Kligler, A. Shocher, and M. Irani. Blind super-resolution kernel estimation using an internal-gan. NeurIPS, 32, 2019.   \n[6] B. Cai, X. Xu, K. Jia, C. Qing, and D. Tao. Dehazenet: An end-to-end system for single image haze removal. IEEE TIP, 25(11):5187\u20135198, 2016.   \n[7] X. Chen, H. Li, M. Li, and J. Pan. Learning a sparse transformer network for effective image deraining. In CVPR, 2023.   \n[8] X. Chu, L. Chen, and W. Yu. Nafssr: Stereo image super-resolution using nafnet. In CVPR, pages 1239\u20131248, 2022.   \n[9] H. Chung, J. Kim, M. T. Mccann, M. L. Klasky, and J. C. Ye. Diffusion posterior sampling for general noisy inverse problems. ICLR, 2023.   \n[10] H. Chung, J. Kim, and J. C. Ye. Direct diffusion bridge using data consistency for inverse problems. NeurIPS, 2023.   \n[11] H. Chung, D. Ryu, M. T. McCann, M. L. Klasky, and J. C. Ye. Solving 3d inverse problems using pre-trained 2d diffusion models. In CVPR, pages 22542\u201322551, 2023.   \n[12] H. Chung, B. Sim, and J. C. Ye. Come-closer-diffuse-faster: Accelerating conditional diffusion models for inverse problems through stochastic contraction. In CVPR, pages 12413\u201312422, 2022.   \n[13] H. Crauel, A. Debussche, and F. Flandoli. Random attractors. Journal of Dynamics and Differential Equations, 9:307\u2013341, 1997.   \n[14] M. Z. Darestani, J. Liu, and R. Heckel. Test-time training can close the natural distribution shift performance gap in deep learning based compressed sensing. In ICML, pages 4754\u20134776. PMLR, 2022.   \n[15] V. De Bortoli, J. Thornton, J. Heng, and A. Doucet. Diffusion schrodinger bridge with applications to score-based generative modeling. NeurIPS, 34:17695\u201317709, 2021.   \n[16] K. Dehnad. Density estimation for statistics and data analysis, 1987.   \n[17] M. Delbracio and P. Milanfar. Inversion by direct iteration: An alternative to denoising diffusion for image restoration. TMLR, 2023.   \n[18] H. Dong, J. Pan, L. Xiang, Z. Hu, X. Zhang, F. Wang, and M.-H. Yang. Multi-scale boosted dehazing network with dense feature fusion. In CVPR, pages 2157\u20132167, 2020.   \n[19] M. El Helou, R. Zhou, and S. S\u00fcsstrunk. Stochastic frequency masking to improve super-resolution and denoising networks. In ECCV, pages 749\u2013766. Springer, 2020.   \n[20] G. Franzese, S. Rossi, L. Yang, A. Finamore, D. Rossi, M. Filippone, and P. Michiardi. How much is enough? a study on diffusion times in score-based generative models. Entropy, 25(4):633, 2023.   \n[21] U. Grenander and M. I. Miller. Representations of knowledge in complex systems. Journal of the Royal Statistical Society: Series B (Methodological), 56(4):549\u2013581, 1994.   \n[22] C.-L. Guo, Q. Yan, S. Anwar, R. Cong, W. Ren, and C. Li. Image dehazing transformer with transmissionaware 3d position embedding. In CVPR, pages 5812\u20135820, 2022.   \n[23] K. He, J. Sun, and X. Tang. Single image haze removal using dark channel prior. IEEE TPAMI, 33(12):2341\u2013 2353, 2010.   \n[24] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33:6840\u20136851, 2020.   \n[25] A. Hore and D. Ziou. Image quality metrics: Psnr vs. ssim. In ICPR, pages 2366\u20132369. IEEE, 2010.   \n[26] J. Huang, A. I. Aviles-Rivero, C.-B. Sch\u00f6nlieb, and G. Yang. Cdiffmr: Can we replace the gaussian noise with k-space undersampling for fast mri? In MICCAI, pages 3\u201312. Springer, 2023.   \n[27] J. Huang, Y. Fang, Y. Wu, H. Wu, Z. Gao, Y. Li, J. Del Ser, J. Xia, and G. Yang. Swin transformer for fast mri. Neurocomputing, 493:281\u2013304, 2022.   \n[28] A. Hyv\u00e4rinen and P. Dayan. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(4), 2005.   \n[29] A. Jolicoeur-Martineau, K. Li, R. Pich\u00e9-Taillefer, T. Kachman, and I. Mitliagkas. Gotta go fast when generating data with score-based models. arXiv preprint arXiv:2105.14080, 2021.   \n[30] A. Jolicoeur-Martineau, R. Pich\u00e9-Taillefer, R. T. d. Combes, and I. Mitliagkas. Adversarial score matching and improved sampling for image generation. arXiv preprint arXiv:2009.05475, 2020.   \n[31] T. Karras, M. Aittala, T. Aila, and S. Laine. Elucidating the design space of diffusion-based generative models. arXiv preprint arXiv:2206.00364, 2022.   \n[32] B. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection. Annals of statistics, pages 1302\u20131338, 2000.   \n[33] B. Lim, S. Son, H. Kim, S. Nah, and K. Mu Lee. Enhanced deep residual networks for single image super-resolution. In CVPR workshops, pages 136\u2013144, 2017.   \n[34] G.-H. Liu, A. Vahdat, D.-A. Huang, E. A. Theodorou, W. Nie, and A. Anandkumar. I2sb: Image-to-image schrbackslash odinger bridge. ICML, 2023.   \n[35] H.-T. D. Liu, F. Williams, A. Jacobson, S. Fidler, and O. Litany. Learning smooth neural functions via lipschitz regularization. In ACM SIGGRAPH 2022 Conference Proceedings, pages 1\u201313, 2022.   \n[36] X. Liu, Y. Ma, Z. Shi, and J. Chen. Griddehazenet: Attention-based multi-scale network for image dehazing. In ICCV, pages 7314\u20137323, 2019.   \n[37] Z. Luo, F. K. Gustafsson, Z. Zhao, J. Sj\u00f6lund, and T. B. Sch\u00f6n. Image restoration with mean-reverting stochastic differential equations. ICML, 2023.   \n[38] K. Mei, A. Jiang, J. Li, J. Ye, and M. Wang. An effective single-image super-resolution model using squeeze-and-excitation networks. In ICONIP, pages 542\u2013553. Springer, 2018.   \n[39] C. Meng, Y. He, Y. Song, J. Song, J. Wu, J.-Y. Zhu, and S. Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. ICLR, 2022.   \n[40] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral normalization for generative adversarial networks. ICLR, 2018.   \n[41] J. C. Nguyen, A. A. De Smet, B. K. Graf, and H. G. Rosas. Mr imaging\u2013based diagnosis and classification of meniscal tears. Radiographics, 34(4):981\u2013999, 2014.   \n[42] A. Q. Nichol and P. Dhariwal. Improved denoising diffusion probabilistic models. In ICML, pages 8162\u20138171. PMLR, 2021.   \n[43] G. Parisi. Correlation functions and computer simulations. Nuclear Physics B, 180(3):378\u2013384, 1981.   \n[44] C. Peng, P. Guo, S. K. Zhou, V. M. Patel, and R. Chellappa. Towards performant and reliable undersampled mr reconstruction via diffusion model sampling. In MICCAI, pages 623\u2013633. Springer, 2022.   \n[45] H. Poincar\u00e9. Sur le probl\u00e8me des trois corps et les \u00e9quations de la dynamique. Acta mathematica, 13(1):A3\u2013A270, 1890.   \n[46] X. Qin, Z. Wang, Y. Bai, X. Xie, and H. Jia. Ffa-net: Feature fusion attention network for single image dehazing. In AAAI, 2020.   \n[47] Y. Qiu, K. Zhang, C. Wang, W. Luo, H. Li, and Z. Jin. Mb-taylorformer: Multi-branch efficient transformer expanded by taylor formula for image dehazing. In CVPR, pages 12802\u201312813, 2023.   \n[48] M. S. Rad, T. Yu, B. Bozorgtabar, and J.-P. Thiran. Test-time adaptation for super-resolution: You only need to overfit on a few more images. In ICCV, pages 1845\u20131854, 2021.   \n[49] D. Ren, W. Zuo, Q. Hu, P. Zhu, and D. Meng. Progressive image deraining networks: A better and simpler baseline. In CVPR, pages 3937\u20133946, 2019.   \n[50] W. Ren, L. Ma, J. Zhang, J. Pan, X. Cao, W. Liu, and M.-H. Yang. Gated fusion network for single image dehazing. In CVPR, pages 3253\u20133261, 2018.   \n[51] J. Schlemper, J. Caballero, J. V. Hajnal, A. Price, and D. Rueckert. A deep cascade of convolutional neural networks for mr image reconstruction. In IPMI, pages 647\u2013658. Springer, 2017.   \n[52] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, pages 2256\u20132265. PMLR, 2015.   \n[53] J. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models. In ICLR.   \n[54] Y. Song and S. Ermon. Generative modeling by estimating gradients of the data distribution. NeurIPS, 32, 2019.   \n[55] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2020.   \n[56] Z. Tu, H. Talebi, H. Zhang, F. Yang, P. Milanfar, A. Bovik, and Y. Li. Maxim: Multi-axis mlp for image processing. In CVPR, pages 5769\u20135780, 2022.   \n[57] P. Vincent. A connection between score matching and denoising autoencoders. Neural computation, 23(7):1661\u20131674, 2011.   \n[58] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE TIP, 13(4):600\u2013612, 2004.   \n[59] S. Welker, J. Richter, and T. Gerkmann. Speech enhancement with score-based generative models in the complex stft domain. arXiv preprint arXiv:2203.17004, 2022.   \n[60] H. Wu, Y. Qu, S. Lin, J. Zhou, R. Qiao, Z. Zhang, Y. Xie, and L. Ma. Contrastive learning for compact single image dehazing. In CVPR, pages 10551\u201310560, 2021.   \n[61] Z. Xiao, K. Kreis, and A. Vahdat. Tackling the generative learning trilemma with denoising diffusion gans. In ICLR, 2021.   \n[62] G. Yang, S. Yu, H. Dong, G. Slabaugh, P. L. Dragotti, X. Ye, F. Liu, S. Arridge, J. Keegan, Y. Guo, et al. Dagan: Deep de-aliasing generative adversarial networks for fast compressed sensing mri reconstruction. TMI, 37(6):1310\u20131321, 2017.   \n[63] L. Yang, Z. Zhang, Y. Song, S. Hong, R. Xu, Y. Zhao, Y. Shao, W. Zhang, B. Cui, and M.-H. Yang. Diffusion models: A comprehensive survey of methods and applications. ACM Computing Surveys, 2022.   \n[64] W. Yang, R. T. Tan, J. Feng, Z. Guo, S. Yan, and J. Liu. Joint rain detection and removal from a single image with contextualized deep networks. IEEE TPAMI, 42(6):1377\u20131393, 2019.   \n[65] W. Yang, R. T. Tan, J. Feng, J. Liu, Z. Guo, and S. Yan. Deep joint rain detection and removal from a single image. In CVPR, pages 1357\u20131366, 2017.   \n[66] Z. Yue, J. Wang, and C. C. Loy. Resshift: Efficient diffusion model for image super-resolution by residual shifting. NeurIPS, 2023.   \n[67] S. W. Zamir, A. Arora, S. Khan, M. Hayat, F. S. Khan, and M.-H. Yang. Restormer: Efficient transformer for high-resolution image restoration. In CVPR, pages 5728\u20135739, 2022.   \n[68] S. W. Zamir, A. Arora, S. Khan, M. Hayat, F. S. Khan, M.-H. Yang, and L. Shao. Multi-stage progressive image restoration. In CVPR, pages 14821\u201314831, 2021.   \n[69] J. Zbontar, F. Knoll, A. Sriram, T. Murrell, Z. Huang, M. J. Muckley, A. Defazio, R. Stern, P. Johnson, M. Bruno, et al. fastmri: An open dataset and benchmarks for accelerated mri. arXiv preprint arXiv:1811.08839, 2018.   \n[70] K. Zhang, J. Liang, L. Van Gool, and R. Timofte. Designing a practical degradation model for deep blind image super-resolution. In CVPR, pages 4791\u20134800, 2021.   \n[71] Q. Zhang and Y. Chen. Fast sampling of diffusion models with exponential integrator. ICLR, 2022.   \n[72] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, pages 586\u2013595, 2018.   \n[73] L. Zhou, A. Lou, S. Khanna, and S. Ermon. Denoising diffusion bridge models. arXiv preprint arXiv:2309.16948, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Broader Impacts ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Our proposed method offers significant advancements in the restoration of degraded images with minimal risk of hallucinations due to stability guarantees and with applications including MRI reconstruction and super-resolution. In the clinical domain, the adoption of our method for MRI reconstruction must adhere to stringent regulatory and approval processes. The results generated by our model should serve as an auxiliary tool to assist healthcare professionals in their diagnostic and treatment decisions, rather than as a standalone diagnostic tool. ", "page_idx": 13}, {"type": "text", "text": "B Intuition of Measure-Preserving Dynamics in SDE: ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Consider the analogy of a stretched rubber band, which naturally seeks to return to its original but does so with a lot of oscillations when released. This elastic behavior parallels the dynamics of the OU process, where deviations from a mean state are counteracted by a restorative force, guiding the system back towards equilibrium (i.e., final state), with random perturbations. ", "page_idx": 13}, {"type": "text", "text": "Our process models the noise as a stochastic component that fluctuates around a stationary process and improve the OU process with RDS. Measure-preserving dynamics ensure that while the image undergoes transformations during the denoising process, the overall statistical properties remain consistent (i.e., invariant image features), which cannot be satisfied by vanilla OU processes or previous approaches (Tab. 1). ", "page_idx": 13}, {"type": "text", "text": "C Mathematical Preliminaries ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Consider a probability space $(\\Omega,{\\mathcal{F}},P)$ accompanied by a standard Brownian motion $W_{t}$ . A stochastic process $x_{t}$ over the interval $0\\leq t\\leq T$ can be formulated by the following stochastic differential equation (SDE): ", "page_idx": 13}, {"type": "equation", "text": "$$\nd x_{t}=b(t,x_{t})d t+\\sigma(t,x_{t})d W_{t}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Definition 1 Filtration A collection of sigma-fields, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{F}:=\\{\\mathcal{F}_{t},0\\leq t\\leq T\\},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "is termed a filtration if: ", "page_idx": 13}, {"type": "text", "text": "Here, $\\mathcal{F}_{t}$ represents the information set at time $t$ . ", "page_idx": 13}, {"type": "text", "text": "Definition 2 Strong Solution $A$ process $x$ , which is $\\mathcal{F}$ -progressively measurable, is considered a strong solution to the SDE given by Eq. 1 if: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\int_{0}^{T}(|b(t,x_{t})|^{2}+|\\sigma(t,x_{t})|^{2})d t<\\infty\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "almost surely. This is captured by: ", "page_idx": 13}, {"type": "equation", "text": "$$\nx_{t}=x_{0}+\\int_{0}^{t}b(s,x_{s})d s+\\int_{0}^{t}\\sigma(s,x_{s})d W_{s},\\forall t\\in[0,T]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Definition 3 Lipschitz Continuity For an $N$ -dimensional stochastic process $x_{t}$ over $t\\,\\in\\,[0,\\infty)$ , adapted to the filtration $\\mathcal{F}$ , function $w(x_{t})$ exhibits Lipschitz continuity in x $i f$ : ", "page_idx": 13}, {"type": "text", "text": "1. $w(x_{t})$ is $\\mathcal{F}$ -measurable with the requisite dimensions. ", "page_idx": 13}, {"type": "text", "text": "2. There exists a non-negative constant $K$ such that, for every $x_{t}$ and $x_{s}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n|w(x_{t})-w(x_{s})|\\leq K|x_{t}-x_{s}|.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In numerous diffusion models, Lipschitz continuity is inherent, ensuring the existence and uniqueness of the solution to the stochastic process. However, for clarity in network design, the emphasis on Lipschitz continuity ensures the foundation of neural networks remains consistent. ", "page_idx": 14}, {"type": "text", "text": "Definition 4 Random Dynamical System A random dynamical system(RDS) consists of a base flow, the \"noise\", and a cocycle dynamical system on the \"physical\" phase space, we first discuss one fundamental element of our RDS, the base flow. ", "page_idx": 14}, {"type": "text", "text": "Let $\\textstyle(\\Omega,{\\mathcal{F}},\\mathbb{P})$ be a probability space, the noise space. Define the base flow $\\vartheta:\\mathbb{R}\\times\\Omega\\,\\rightarrow\\,\\Omega$ as follows: for each \"time\" $s\\in\\mathbb{R},$ , let $\\vartheta_{s}:\\Omega\\,\\rightarrow\\,\\Omega$ be a measure-preserving measurable function: $\\mathbb{P}(E)=\\operatorname{\\dot{\\mathbb{P}}}\\left(\\vartheta_{s}^{-1}(E)\\right)$ for all $E\\in{\\mathcal{F}}$ and $s\\in\\mathbb{R}$ ", "page_idx": 14}, {"type": "text", "text": "Suppose also that ", "page_idx": 14}, {"type": "text", "text": "1. $\\vartheta_{0}=\\mathrm{id}_{\\Omega}:\\Omega\\to\\Omega$ , the identity function on $\\Omega$ ;   \n2. for all $s,t\\in\\mathbb{R},\\vartheta_{s}\\circ\\vartheta_{t}=\\vartheta_{s+t}$ .   \nThat is, $\\vartheta_{s},s\\in\\mathbb{R},$ , forms a group of measure-preserving transformation of the noise $\\textstyle(\\Omega,{\\mathcal{F}},\\mathbb{P})$ ", "page_idx": 14}, {"type": "text", "text": "Now we are ready to define the random dynamical system $(R D S)$ . ", "page_idx": 14}, {"type": "text", "text": "let $(X,d)$ be a complete separable metric space, the phase space. Let $\\varphi:\\mathbb{R}\\times\\Omega\\times X\\to X$ be $a$ $({\\mathcal{B}}(\\mathbb{R}){\\otimes}{\\mathcal{F}}{\\otimes}B(X),{\\mathcal{B}}(X))$ measurable function such that 1. for all $\\omega\\in\\Omega,\\varphi(0,\\omega)=\\mathrm{id}_{X}:X\\to X$ , the identity function on $X;2$ . for (almost) all $\\omega\\in\\Omega,(t,x)\\mapsto\\varphi(t,\\omega,x)$ is continuous; 3. $\\varphi$ satisfies the (crude) cocycle property: for almost all $\\omega\\in{\\Omega}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\varphi\\left(t,\\vartheta_{s}(\\omega)\\right)\\circ\\varphi(s,\\omega)=\\varphi(t+s,\\omega)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In the case of random dynamical systems driven by $a$ Wiener process $W:\\mathbb{R}\\times\\Omega\\rightarrow X$ , the base flow $\\vartheta_{s}:\\Omega\\to\\Omega$ would be given by ", "page_idx": 14}, {"type": "equation", "text": "$$\nW\\left(t,\\vartheta_{s}(\\omega)\\right)=W(t+s,\\omega)-W(s,\\omega).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Theorem 1 Existence and Uniqueness If the initial condition $x_{0}\\in\\mathbb{L}^{2}$ is a random variable that\u2019s independent of $W$ and both $\\mu(0,x_{0})$ and $\\sigma(0,x_{0})\\,\\in\\,\\mathbb{H}^{2}$ , then, provided there exists a constant $K>0$ that satisfies: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|b(t,x)-b(t,y)|+|\\sigma(t,x)-\\sigma(t,y)|}\\\\ &{\\leq K|x-y|,\\quad\\forall t\\in[0,T],x,y\\in{\\mathbb R}^{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "(an attribute also recognized as Lipschitz continuity), $a$ unique strong solution to Eq. 1 exists in $\\mathbb{H}^{2}$ for every $T>0$ . Additionally: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{sup}_{t\\leq T}|x_{t}|^{2}\\right]\\leq C(1+\\mathbb{E}|x_{0}|^{2})e^{C T}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "holds true, where the constant $C$ depends on both $T$ and $K$ . ", "page_idx": 14}, {"type": "text", "text": "D Preliminaries and Proof for Proposition 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D.1 Proof for reverse-time $\\mathbf{D}^{3}\\mathbf{GM}$ process: ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "There is a one-to-one and onto correspondence between the stochastic differential equation and the Kolmogorov equation for $p\\left(x_{t},t\\mid x_{s}^{-},s\\right),t\\geqslant s$ , which describes the evolution of the underlying probability distribution. Consequently, there should be a one-to-one and onto correspondence between a reverse-time equation for $\\tilde{x}_{t}$ and a Kolmogorov equation for $p(x_{t},t|x_{s},s),s\\geqslant t$ ", "page_idx": 14}, {"type": "equation", "text": "$$\nd x_{t}=\\theta_{t}(\\mu-x_{t})d t+\\tau\\sigma d W_{t}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We have the corresponding Kolmogorov backward equation given by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{-\\frac{\\partial p\\left(x_{s},s\\mid x_{t},t\\right)}{\\partial t}=\\!\\!\\theta_{t}(\\mu-x_{t})\\cdot\\frac{\\partial p\\left(x_{s},s\\mid x_{t},t\\right)}{\\partial x_{t}}}\\\\ &{}&{\\qquad\\qquad\\qquad+\\,\\frac{1}{2}\\tau^{2}\\sigma_{t}^{2}\\cdot\\frac{\\partial^{2}p\\left(x_{s},s\\mid x_{t},t\\right)}{\\partial x_{t}^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The unconditioned Kolmogorov forward equation is given by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{-\\frac{\\partial p\\left(x_{s},s\\mid x_{t},t\\right)}{\\partial t}=\\!\\frac{\\partial\\left(\\theta_{t}\\left(\\mu-x_{t}\\right)\\cdot p\\left(x_{s},s\\mid x_{t},t\\right)\\right)}{\\partial x_{t}}}\\\\ &{}&{\\qquad-\\,\\frac{1}{2}\\cdot\\frac{\\partial^{2}\\left(\\tau^{2}\\sigma_{t}^{2}\\cdot p\\left(x_{s},s\\mid x_{t},t\\right)\\right)}{\\partial x_{t}^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "See [3] for more details on Kolmogorov equations. Bayes rule gives ", "page_idx": 15}, {"type": "equation", "text": "$$\np\\left(x_{t},t,x_{s},s\\right)=p\\left(x_{s},s\\mid x_{t},t\\right)p\\left(x_{t},t\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We plug this result into 13, which gives us the Kolmogorov equation ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{-\\frac{\\partial}{\\partial t}p\\left(x_{t},t,x_{s},s\\right)=\\!\\!\\frac{\\partial}{\\partial x_{t}}\\left[\\bar{f}\\left(x_{t},t\\right)p\\left(x_{t},t,x_{s},s\\right)\\right]}}\\\\ &{}&{+\\,\\frac{1}{2}\\frac{\\partial^{2}\\,\\left[p\\left(x_{t},t,x_{s},s\\right)\\cdot\\tau^{2}\\sigma_{t}^{2}\\right]}{\\partial x_{t}^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and the expression for $\\bar{f}$ is given by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\bar{f}\\left(x_{t},t\\right)=\\theta_{t}(\\mu-x_{t})-\\frac{1}{p\\left(x_{t},t\\right)}\\frac{\\partial}{\\partial x_{t}}\\left[p\\left(x_{t},t\\right)\\tau^{2}\\sigma_{t}^{2}\\right]}}\\\\ &{}&{=\\theta_{t}(\\mu-x_{t})-\\tau^{2}\\sigma_{t}^{2}\\log\\frac{\\partial}{\\partial x_{t}}\\left[p(x_{t},t)\\right]\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, we have that the reverse process corresponds to the Kolmogorov equation 16 is given by ", "page_idx": 15}, {"type": "equation", "text": "$$\nd x_{t}=\\theta_{t}(\\mu-x_{t})-\\tau^{2}\\sigma_{t}^{2}\\log\\nabla_{x}p_{t}(x_{t})+\\tau^{2}\\sigma_{t}^{2}d\\bar{W}_{t}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Definition 5 Different from deterministic dynamical systems, random dynamical systems usually consider a pullback attractor rather than a forward attractor due to the non-autonomousness introduced by the random noise. The pullback attractor (or random global attractor) $\\boldsymbol{A}(\\omega)$ for the $R D S\\;\\varphi$ we defined in $^{\\,I}$ is a $\\mathbb{P}$ -almost surely unique random set such that: ", "page_idx": 15}, {"type": "text", "text": "1. $A(\\omega)$ is a random compact set: ${\\mathcal{A}}(\\omega)\\subseteq X$ is almost surely compact and $\\omega\\mapsto\\mathrm{d}(x,\\mathcal{A}(\\omega))$ is $a$ $({\\mathcal{F}},{\\mathcal{B}}(X))$ -measurable function for every $x\\in X$   \n2. $\\mathcal{A}(\\omega)$ is invariant: for all $\\varphi(t,\\omega)(A(\\omega))=A\\left(\\vartheta_{t}\\omega\\right)$ almost surely;   \n3. $A(\\omega)$ is attractive: for any deterministic bounded set $B\\qquad\\subseteq\\qquad X,$ , $\\begin{array}{r}{\\operatorname*{lim}_{t\\rightarrow+\\infty}\\mathrm{d}\\left(\\varphi\\left(t,\\vartheta_{-t}\\omega\\right)(B),A(\\omega)\\right)=0}\\end{array}$ almost surely. ", "page_idx": 15}, {"type": "text", "text": "$B(X)$ denotes the Borel $\\sigma$ -algebra generated by the space $X$ where the RDS is defined. ", "page_idx": 15}, {"type": "text", "text": "Definition 6 Poincare Recurrence Theorem ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Let ", "page_idx": 15}, {"type": "equation", "text": "$$\n(X,\\Sigma,\\mu)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "be a finite measure space and let ", "page_idx": 15}, {"type": "equation", "text": "$$\nf:X\\to X\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "be a measure-preserving transformation. then we have that for any $E\\in\\Sigma$ , the set of those points $x$ of $E$ for which there exists $N\\in\\mathbb{N}$ such that $f^{n}(x)\\notin E$ for all $n>N$ has zero measure. In other words, almost every point of $E$ returns to $E$ . In fact, almost every point returns infinitely often; i.e. $\\mu$ ( $\\{x\\in E$ : there exists $N$ such that $f^{n}(x)\\notin E$ for all $n>N\\})=0$ . ", "page_idx": 15}, {"type": "text", "text": "D.2 Analysis of Proposition 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proposition 1 After extending the solution of the OU process to RDS, the measure-preserving flow map of the solution should meet the property $\\varphi(t,s;\\omega)x=\\varphi\\left(t-s,0;\\vartheta_{s}\\omega\\right)x$ . However, OU processes with time-varying coefficients are usually not satisfied for this property(can be referred to as time-homogeneity) and thus the stability of the system breaks.   \nThe forward process in SDE notation ", "page_idx": 15}, {"type": "equation", "text": "$$\nd x_{t}=\\theta_{t}(\\mu-x_{t})d t+\\sigma_{t}d W_{t}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and the solution to the above SDE is given by ", "page_idx": 16}, {"type": "equation", "text": "$$\nx_{t}=\\mu+(x_{s}-\\mu)\\mathrm{e}^{-\\bar{\\theta}_{s:t}}+\\int_{s}^{t}\\sigma_{z}\\mathrm{e}^{-\\bar{\\theta}_{z:t}}d W_{z}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{\\theta}_{s:t}=\\int_{s}^{t}\\theta_{z}d z}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "This solution above can be represented by a continuous random dynamical system (RDS) $\\varphi$ defined on a complete separable metric space $(X,d)$ , where the noise is chosen from a probability space $\\textstyle(\\Omega,{\\mathcal{F}},\\mathbb{P})$ . More details can be found in [13]. ", "page_idx": 16}, {"type": "text", "text": "More generally, we can extend the RDS to two-sided, infinite time, define a flow map or (solution operator) $\\varphi:\\check{\\mathbb{R}}\\times\\Omega\\times\\mathbb{R}^{d}\\to\\mathbb{R}^{d}$ by $\\varphi\\left(t,s,\\vert\\omega,x_{0}\\right):=x\\left(t,s\\vert\\omega,x_{0}\\right)$ with $\\omega\\in\\Omega,-\\infty<s\\leqslant t<\\infty$ . The base flow driven by Brownian motion can be explicitly written as $W\\left(t,\\vartheta_{s}(\\omega)\\right)=W(t+s,\\omega)-$ $W(s,\\omega)$ . ", "page_idx": 16}, {"type": "text", "text": "Now suppose that: ", "page_idx": 16}, {"type": "text", "text": "1. The flow map $\\vartheta_{t},t\\in\\mathbb{R}$ is a measure-preserving transformations of $(\\Omega,{\\mathcal{F}},P)$ , with the property that for all $s<t$ and $x\\in X$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\varphi(t,s;\\omega)x=\\varphi\\left(t-s,0;\\vartheta_{s}\\omega\\right)x,\\quad P{\\mathrm{-a.s.}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "2. (i) $\\varphi(t,r;\\omega)\\varphi(r,s;\\omega)x=\\varphi(t,s;\\omega)x$ for all $s\\leqslant r\\leqslant t$ and $x\\in X$ ; ", "page_idx": 16}, {"type": "text", "text": "(ii) $\\varphi(t,s;\\omega)$ is continuous in $X$ , for all $s\\leqslant t$ . ", "page_idx": 16}, {"type": "text", "text": "(iii) for all $s<t$ and $x\\in X$ , the mapping ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\omega\\mapsto\\varphi(t,s;\\omega)x\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "is measurable from $(\\Omega,{\\mathcal{F}})$ to $(X,B(X))$ ; and ", "page_idx": 16}, {"type": "text", "text": "(iv) for all $t,x\\in X$ , and $P$ -a.e. $\\omega$ , the mapping $s\\mapsto\\varphi(t,s;\\omega)x$ is right continuous at any point.   \nWhere $B(X)$ denotes the $\\sigma$ -algebra generated by $\\Chi$ . ", "page_idx": 16}, {"type": "text", "text": "Under assumptions (i), (ii), (iii), (iv) and suppose that for $P$ -a.e. $\\omega$ there exists a compact attracting set $K(\\omega)$ at time 0, i.e., such that for all bounded sets $B\\subset X$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\nd(\\varphi(0,s;\\omega)B,K(\\omega))\\to0\\quad\\mathrm{~as~}\\quad s\\to-\\infty\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We can see that the attractor of this system is defined in the pullback sense, such that time is rewind backward before iterating forward. ", "page_idx": 16}, {"type": "text", "text": "Moreover, the reverse process with any starting time $t$ to $s$ is defined as the RDS going backward in time ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\varphi(s,t|\\vartheta_{t}\\omega,x_{t})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "start from the time $t$ realization and run backwards to $s$ . ", "page_idx": 16}, {"type": "text", "text": "the above proposition can be extended to show that there exists a compact attracting set at any $-\\infty<t<\\infty$ . and this convention has allowed us to characterize the attractor $K(\\omega)\\bar{=}\\mathcal{N}(\\mu,\\lambda^{2})$ , when the time becomes finite, for example from 0 to $T$ , the random attractors can be abstractly viewed as $N\\left(\\mu+(x_{s}-\\mu)\\mathrm{e}^{-\\bar{\\theta}_{0:t}},\\lambda(1-\\mathrm{e}^{-\\bar{\\theta}_{0:t}})\\right)$ . ", "page_idx": 16}, {"type": "text", "text": "Moreover, an important assumption in the 17 is usually not satisfied for OU processes with timevarying coefficients, therefore, we impose that for every t, 2\u03c3\u03b8tt $\\begin{array}{r}{\\frac{\\sigma_{t}^{2}}{2\\theta_{t}}\\,=\\,\\lambda^{2}\\,}\\end{array}$ , where $\\lambda$ is a constant, and will be the asymptotic variance of the forward process. This convention has allowed us to reduce the regularization on two variables $\\sigma_{t},\\theta_{t}$ to just one variable to satisfy 17; and this convention has allowed as to characterize the attractor $\\bar{K}(\\omega)=\\mathcal{N}(\\mu,\\lambda^{2})$ , when the time becomes finite, for example from 0 to $T$ , the random attractors can be abstractly viewed as the Gaussian measure $N\\left(\\bar{\\mu}+(x_{s}-\\mu)\\mathrm{e}^{-\\bar{\\theta}_{0:t}},\\lambda(1-\\mathrm{e}^{-\\bar{\\theta}_{0:t}})\\right)\\!.$ ", "page_idx": 16}, {"type": "text", "text": "E Proof for Proposition 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proposition 2 Given Eq. 3 and Eq. 4, and assume that the score function is bounded by $C$ in $L^{2}$ norm, then the discrepancy between the reference and the retrieved data is, with probability at least $(1-\\delta)$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|x_{0}-\\operatorname{OU}(x_{0},\\mu;T,\\theta)\\|_{2}^{2}}\\\\ &{\\ge|\\left((x_{0}-\\mu)^{2}-\\sigma_{T}^{2}/2\\theta_{T}\\right)\\mathrm{e}^{-2\\bar{\\theta}_{T}}+\\sigma_{T}^{2}/2\\theta_{T}}\\\\ &{-\\,\\sigma_{m a x}^{2}\\left(C\\sigma_{m a x}^{2}+d+2\\sqrt{-d\\cdot\\log\\delta}-2\\log\\delta\\right)|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $x_{0}\\ \\hat{x}_{0}$ are the quality reference and sampling data. For a noisy inverse problem scenario, the retrieved data with any finite $T$ always indicates difference depends on $\\sigma_{t},\\mu_{t},\\lambda,T,\\bar{K}$ , where $\\bar{K}$ is the Lipschitz constant for the reverse process. ", "page_idx": 17}, {"type": "text", "text": "we have that the absolute value between the theoretical expectation and actual expectation after $T$ period is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\mu-\\mathbb{E}(\\hat{x}_{T})\\|=\\|(x_{0}-\\mu)e^{-\\bar{\\theta}_{T}}\\|>0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similarly, the difference between theoretical variance and T-period variance also has a strictly positive difference. where $\\begin{array}{r}{\\bar{\\theta}_{t}=\\int_{0}^{t}\\theta_{s}d s}\\end{array}$ . ", "page_idx": 17}, {"type": "text", "text": "Therefore, with finite $T$ , the final state of the forward process can only reach a $\\hat{x}_{T}$ rather than the theoretical stationary distribution, which we denote by $x_{\\infty}$ , we denote the retrieved image after T-periods from theoretical stationary distribution by $\\hat{x}_{0},x_{0}$ by the ground truth HQ image, $\\hat{x}_{T}$ the true distribution after $\\mathrm{T}$ iteration, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|{x_{Q}}-{f_{\\mathrm{OU}}}n(x_{Q},\\mu;t_{0},)\\|=\\|\\hat{x}_{T}-x_{\\infty}-(\\hat{x}_{0})-x_{\\infty}\\|_{2}^{2}\\qquad}\\\\ {\\qquad\\qquad\\geq\\|\\|\\hat{x}_{T}-x_{\\infty}\\|_{2}^{2}-\\|\\hat{x}_{0}-x_{\\infty}\\|_{2}^{2}\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Inside the norm, the first term is bounded below, since both $\\hat{x}_{T}$ and $x_{\\infty}$ both follow a normal distribution and are independent of each other, the difference between those two random variables, we denote by $z_{T}$ , that follows a normal distribution ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left.\\mathcal{N}\\left(\\mu+(x_{0}-\\mu)\\mathrm{e}^{-\\bar{\\theta}_{t}}-\\mu,\\lambda^{2}(1-e^{-2\\bar{\\theta}_{t}})+\\lambda^{2}\\right)\\right.}\\\\ &{\\left.=\\!\\!\\mathcal{N}\\left((x_{0}-\\mu)\\mathrm{e}^{-\\bar{\\theta}_{t}},\\lambda^{2}(2-e^{-2\\bar{\\theta}_{t}})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, we could rewrite the equality as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\left|x_{Q}-f_{\\mathrm{OU}}(x_{Q},\\mu;t_{0})\\right|\\right|}\\\\ &{\\geq\\left|\\left|\\|z_{T}\\|_{2}^{2}-\\left\\|\\int_{T}^{0}-\\frac{d\\sigma_{t}^{2}}{d t}\\nabla_{x}\\log p_{t}(x)\\,d t+d\\overline{{\\mathbf{w}}}_{t}\\right\\|_{2}^{2}\\right|\\right|_{2}^{2}}\\\\ &{=\\left\\|(x_{0}-\\mu)^{2}e^{-2\\bar{\\theta}_{t}}+\\lambda^{2}(2-e^{-2\\bar{\\theta}_{t}})-C\\sigma_{\\operatorname*{max}}^{4}-\\left\\|\\int_{T}^{0}\\sqrt{\\frac{d\\sigma_{t}^{2}}{d t}}\\,d\\overline{{\\mathbf{w}}}_{t}\\right\\|_{2}^{2}\\right\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since we require $\\begin{array}{r}{\\lambda=\\frac{\\sigma_{t}^{2}}{2\\theta_{t}}}\\end{array}$ 2\u03c3\u03b8tt , we can find a \u03c3max such that \u03c3t < \u03c3max for all t. The last term only concerns the random noise, according to [32], we have that the last term is equivalent to the squared $L_{2}$ norm of a random variable from a Wiener process at time $t=0$ , with marginal distribution being $\\boldsymbol{\\epsilon}\\sim\\mathcal{N}\\left(\\mathbf{0},\\sigma_{T}^{2}\\mathbf{I}\\right)$ . The squared $L_{2}$ norm of $\\epsilon$ divided by $\\sigma_{T}^{2}$ is a $\\chi^{2}$ -distribution with $d$ -degrees of freedom, we have the following one-sided tail bound, according to [32] ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\|\\epsilon\\|_{2}^{2}/\\sigma^{2}\\left(t_{0}\\right)\\geq d+2{\\sqrt{d\\cdot-\\log{\\delta}}}-2\\log{\\delta}\\right)\\leq\\exp(\\log{\\delta})=\\delta\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, with probability $1-\\delta$ , the HQ image and retrieved image has a lower bound of,(d is the number of dimension for $x_{0}$ ), ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{|\\left((x_{0}-\\mu)^{2}-\\lambda^{2}\\right)\\mathrm{e}^{-2\\bar{\\theta}_{T}}+2\\lambda^{2}}}\\\\ {{-\\sigma_{m a x}^{2}\\left(C\\sigma_{m a x}^{2}+d+2\\sqrt{-d\\cdot\\log\\delta}-2\\log\\delta\\right)|}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A SDE modeling that has suffered complex degradation is considered \u2019corrupted\u2019: ", "page_idx": 17}, {"type": "text", "text": "Example 1. Based on the Prop. The conditional SDE diffusion is composed of three general stages, forward, backward, and sampling. Both time-reversal processes have been declared unstable under the degradation process $\\pmb{\\mu}$ according to the Prop. 2. ", "page_idx": 18}, {"type": "text", "text": "The following advantages would be offered by our measure-preserving dynamical system when formulated as SDEs: ", "page_idx": 18}, {"type": "text", "text": "Intuition 1. A two-sided measure-preserving random dynamical(MP-RDS) system formulation enables us to use the Poincare recurrence theorem, intuitively, with a two-sided MP-RDS $\\varphi_{t}$ , the Poincare recurrence theorem ensures that the system $\\varphi_{t}$ starts from terminal condition $x_{T}$ , run backwards in time, will hit a region $(x_{0}-\\epsilon,x_{0}+\\epsilon)$ for small $\\epsilon$ in finite time, where $x_{0}$ is the high-quality image. ", "page_idx": 18}, {"type": "text", "text": "F Temporal Distribution Discrepancy during Sampling ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Theorem 2 Suppose that both the drift $b_{t}(x)$ and diffusion $\\sigma_{t}(x)$ term of a stochastic process $x_{t}$ is Lipschitz continuous with some constant $K$ , moreover, $x\\in\\mathbb{L}^{2}\\left(\\vec{\\mathbb{F}},\\mathbb{R}\\right)$ is a solution to the SDE ", "page_idx": 18}, {"type": "equation", "text": "$$\nd x_{t}=b(t,x)d t+\\sigma(t,x)d W_{t}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and initial condition $b_{0},\\sigma_{0}$ then we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\vert x_{T}-x_{0}\\right\\vert^{2}\\right]\\leq C I_{0}^{2},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "such that ", "page_idx": 18}, {"type": "equation", "text": "$$\nI_{0}^{2}:=\\mathbb{E}\\left[\\left(\\int_{0}^{T}|b_{0}|\\,d t\\right)^{2}+\\int_{0}^{T}|\\sigma_{0}|^{2}\\,d t\\right]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and $C$ depends only on $T,K$ , which is the running time and Lipschitz constant ", "page_idx": 18}, {"type": "text", "text": "Firstly, we have the following relationship: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle x_{T}\\leq|x_{0}|+\\int_{0}^{T}|b(t,x)|\\ d t+\\operatorname*{sup}_{0\\leq t\\leq T}\\left|\\int_{0}^{t}\\sigma(s,x)\\,d W_{s}\\right|,}}\\\\ {{\\displaystyle|x_{T}-x_{0}|\\leq\\int_{0}^{T}|b(t,x)|\\ d t+\\operatorname*{sup}_{0\\leq t\\leq T}\\left|\\int_{0}^{t}\\sigma(s,x)\\,d W_{s}\\right|.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Squaring both sides, taking expectations, and applying the Burkholder-Davis-Gundy inequality, we get: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left|x_{T}-x_{0}\\right|^{2}\\right]}\\\\ &{\\leq C\\ensuremath{{\\mathbb E}}\\left[\\left(\\int_{0}^{T}\\left|b(t,x_{t})\\right|\\,d t\\right)^{2}+\\underset{0\\leq t\\leq T}{\\operatorname*{sup}}\\left(\\int_{0}^{t}\\sigma(s,x_{s})\\,d B_{s}\\right)^{2}\\right]}\\\\ &{\\leq C\\ensuremath{{\\mathbb E}}\\left[\\left(\\int_{0}^{T}\\left||b_{0}|+|x_{t}|\\right]\\,d t\\right)^{2}+\\int_{0}^{T}\\left|\\sigma(t,x_{t})\\right|^{2}\\,d t\\right]}\\\\ &{\\leq C\\ensuremath{{\\mathbb E}}\\left[\\left(\\int_{0}^{T}\\left|b_{0}\\right|\\,d t\\right)^{2}+\\int_{0}^{T}\\left[\\left|\\sigma_{0}\\right|^{2}+\\left|x_{t}\\right|^{2}\\right]\\,d t\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Remark: It should be noted that the constant $C$ , which depends on $T$ and $K$ , varies from line to line. Next, we show that for any $\\varepsilon>0$ , there exists a constant $C_{\\varepsilon}>0$ such that: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\leq t\\leq T}\\mathbb{E}\\left[\\left|x_{t}\\right|^{2}\\right]\\leq\\varepsilon\\mathbb{E}\\left[\\left|x_{T}^{*}\\right|^{2}\\right]+C_{\\varepsilon}I_{0}^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Applying It\u00f4\u2019s formula, we get: ", "page_idx": 19}, {"type": "equation", "text": "$$\nd\\left|x_{t}\\right|^{2}=\\left[2x_{t}b(t,x_{t})+\\left|\\sigma(t,x_{t})\\right|^{2}\\right]\\,d t+2x_{t}\\sigma(t,x_{t})\\,d B_{t}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Considering the martingale property of the third term, integrating, and taking expectation on both sides, we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left|x_{t}\\right|^{2}\\right]=\\mathbb{E}\\left[|x_{0}|^{2}+\\int_{0}^{t}\\left[2x_{s}b(s,x_{s})+|\\sigma(s,x_{s})|^{2}\\right]\\,d s\\right]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}\\left[|x_{0}|^{2}+\\int_{0}^{t}\\left[C\\left|x_{s}\\right|^{2}+2\\left|x_{s}\\right||b_{0}\\right|+C\\left|\\sigma_{0}\\right|^{2}\\right]\\,d s\\right]}\\\\ &{\\qquad\\qquad\\leq C\\int_{0}^{t}\\mathbb{E}\\left[\\left|x_{s}\\right|^{2}\\right]\\,d s+2\\mathbb{E}\\left[x_{T}\\int_{0}^{T}\\left|b_{0}\\right|\\,d s\\right]+C I_{0}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using Gronwall\u2019s inequality, we can prove (25) with the result above. Substituting (25) into (24) completes the proof, showing that the distance between $x_{T}$ and $x_{0}$ in the $L^{2}$ -sense is bounded by $C(\\bar{K},T)$ . ", "page_idx": 19}, {"type": "text", "text": "With theorem 1 in hand, since the OU process is Lipschitz continuous, then the reverse process for the OU process is also Lipschitz continuous. ", "page_idx": 19}, {"type": "text", "text": "Now, suppose that the Lipschitz constant for the reverse process is given by $\\bar{K}$ . then we have that in $L^{2}$ -norm, the distance between any final state $x_{T}\\in{\\mathcal{N}}(\\mu,\\lambda)$ and the initial state(HQ image) is bounded by a constant that only depends on time $T$ and Lipschitz constant $\\bar{K}$ , and the initial condition for the drift and diffusion term, where we denote as $C(\\bar{K},T,\\mu,\\lambda)$ , which will be written as $C(\\bar{K},T)$ for short. ", "page_idx": 19}, {"type": "text", "text": "Now, since the time $T$ is finite, where theoretically only when $T\\rightarrow\\infty$ would $x_{T}$ converge to the theoretical stationary distribution, thus, if we denote the sample from $T$ time steps by $\\hat{x}_{T}$ , and $x_{T}$ by the sample from the theoretical stationary distribution, then we have $\\mathbb{E}[x_{T}-\\hat{x}_{T})]>\\epsilon(T,K,\\mu,\\lambda)$ , note that the $K$ here is the Lipschitz constant for the forward process, and this distance strictly decrease in $T$ . ", "page_idx": 19}, {"type": "text", "text": "Suppose that in inference, when the ground truth $x_{0}$ is unknown, the distance between ground truth $x_{0}$ and sample from the theoretical distribution $x_{T}$ is bounded from below by ", "page_idx": 19}, {"type": "equation", "text": "$$\n||x_{0}-x_{\\infty}||>C(\\bar{K},T)I_{0}^{2}+\\epsilon\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, we can see that the gap between $x_{T}$ and $\\hat{x}_{T}$ increases such bound, which is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|x_{0}-\\hat{x}_{T}\\|\\ge||x_{0}-x_{\\infty}-(\\hat{x}_{T}-x_{\\infty})||}\\\\ &{\\quad>C(\\bar{K},T)I_{0}^{2}+\\epsilon(T,K,\\mu,\\lambda)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $T$ is the time step for the forward process, and $\\epsilon$ is some strictly positive constant. Now suppose that $\\hat{x_{t}}$ is the solution to the reverse process, which runs for $T$ periods in total, then we have that for $t\\in[0,T]$ , denote $x_{0},{\\hat{x}}_{0}$ as the original HQ image and the final state of the reverse process, respectively. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|x^{Q u a l i t y}-\\mathrm{OU}\\left(x^{Q u a l i t y},\\mu;t_{0},\\theta\\right)\\right\\|_{2}^{2}=}\\\\ &{\\left\\|x_{0}-\\hat{x}_{0}\\right\\|=\\left\\|x_{0}-\\hat{x}_{T}-\\left(\\hat{x}_{0}-\\hat{x}_{T}\\right)\\right\\|\\geq}\\\\ &{\\left\\|\\left\\|x_{0}-\\hat{x}_{T}\\right\\|-\\left\\|\\left(\\hat{x}_{0}-x_{T}\\right)\\right\\|\\right\\|>}\\\\ &{\\epsilon+C(\\bar{K},T)I_{0}^{2}-C(\\bar{K},T)I_{0}^{2}=\\epsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, the bias created in inference depends on the LQ image $\\mu$ , stationary variance, Lipschitz constant, and the time steps $\\mathrm{T}$ . ", "page_idx": 19}, {"type": "text", "text": "G Stable in Probability ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Definition 7 Stable in Probability ", "page_idx": 20}, {"type": "text", "text": "Given a probability space $(\\Omega,{\\mathcal{F}},P)$ and a standard Brownian motion $W_{t}$ , a general form of SDE for a stochastic process $x_{t},0\\leq t\\leq T$ is given by ", "page_idx": 20}, {"type": "equation", "text": "$$\nd x_{t}=b(t,x)d t+\\sigma(t,x)d W_{t}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Such that the Lipschitz condition is satisfied, for both $b(t,x),\\sigma(t,x)$ , A solution $x(t,\\omega)\\equiv0$ is said to be stable in probability for $t\\geq0$ if for any $s\\geq0$ and $\\varepsilon>0$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{x_{0}\\to0}\\mathbf{P}\\left\\{\\operatorname*{sup}_{t>s}|x^{s,x}(t)|>\\varepsilon\\right\\}=0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "It says that the sample path of the process issuing from a point $x$ at time $s$ will always remain within any prescribed neighbourhood of the origin with probability tending to one as $x\\to0$ . In practice, this property ensures that the perturbation from the initial state caused by a stable process is bounded for all $t$ with probability one. ", "page_idx": 20}, {"type": "text", "text": "For example, the OU process 4 admits a unique unconditional stationary solution provided by theorem 19, however, in this example, without specifying the value that determines the stationary variance, i.e., $\\sigma,\\theta$ . If for large $\\sigma$ and a sample $\\hat{x}$ from the stationary distribution of the OU process, we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\hat{x}>\\mu\\pm\\frac{\\sigma}{2\\theta})>0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This means that a sample from the stationary distribution of the forward process could deviate largely from $\\mu$ , thus making the result no different from the traditional VE(variance exploding) diffusion models that are defined as ", "page_idx": 20}, {"type": "equation", "text": "$$\nd x_{t}=\\sigma_{t}d W_{t}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "because the variance could be set arbitrarily large if no restriction is specified. Therefore, how should such a problem be approached most easily? The Lyapunov theorem for stability has provided an easy way, without explicitly solving the SDEs, to ensure stability just from the coefficients. ", "page_idx": 20}, {"type": "text", "text": "Then we provide the main theorem that ensures the stability of SDE, first, we give the definition of positive definite in the Lyapunov sense ", "page_idx": 20}, {"type": "text", "text": "Definition 8 Let $K$ denote the family of all continuous nondecreasing functions $\\mu:R_{+}\\to R_{+}s u c h$ that $\\mu(0)\\,=\\,0$ and $\\mu(r)\\,>\\,0$ if $r\\,>\\,0$ . For $h\\,>\\,0$ , let $S_{h}\\,=\\,\\{x\\in R^{n}:|x|<h\\}$ . A continuous function $V(x,t)$ defined on $S_{h}\\times[t_{0},\\infty)$ is said to be positive-definite (in the sense of Lyapunov) $i f$ $V(0,t)\\equiv0$ and, for some $\\mu\\in K$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\nV(x,t)\\geq\\mu(|x|)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for all $(x,t)\\in S_{h}\\times[t_{0},\\infty)$ ", "page_idx": 20}, {"type": "text", "text": "Then we will use the convention of the Lyapunov quadratic function, such that ", "page_idx": 20}, {"type": "text", "text": "Definition 9 Lyapunov quadratic function $V$ is given ", "page_idx": 20}, {"type": "equation", "text": "$$\nV\\left(\\boldsymbol{x}_{t}\\right)=x_{t}^{T}Q x_{t},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $Q$ is a symmetric positive-definite matrix. ", "page_idx": 20}, {"type": "text", "text": "Theorem 3 The function $L V$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{L V\\left(x_{t}\\right)=\\!x_{t}^{T}Q b\\left(t,x_{t}\\right)+b\\left(t,x_{t}\\right)^{T}Q x_{t}+}}\\\\ {{\\sigma\\left(t,x_{t}\\right)^{T}Q\\sigma\\left(t,x_{t}\\right),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "is negative-definite in some neighbourhood of $x_{t}=0\\,f o r\\,t\\geq t_{0}$ , with respect to system 7. Then the trivial solution of equation 7 is stochastically asymptotically stable. ", "page_idx": 20}, {"type": "text", "text": "Since this theorem is important and the proof will be intuitive in explaining why such a condition could ensure stability, the proof will be put here. ", "page_idx": 21}, {"type": "text", "text": "Proof: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "First, we compute $d V(x)$ , which is the instantaneous growth of the Lyapunov quadratic function, gives ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d V\\left(x_{t}\\right)=V\\left(x_{t}+d x_{t}\\right)-V\\left(x_{t}\\right)}\\\\ &{\\qquad\\qquad=\\left(x_{t}^{T}+d x_{t}^{T}\\right)Q\\left(x_{t}+d x_{t}\\right)-x_{t}^{T}Q x_{t}}\\\\ &{\\qquad\\quad=x_{t}^{T}Q b\\left(t,x_{t}\\right)d t+x_{t}^{T}Q\\sigma\\left(t,x_{t}\\right)d B_{t}+}\\\\ &{\\qquad\\quad\\,\\,b\\left(t,x_{t}\\right)^{T}d t Q x_{t}+\\sigma\\left(t,x_{t}\\right)^{T}d B_{t}Q x_{t}+}\\\\ &{\\qquad\\quad\\,\\,\\sigma\\left(t,x_{t}\\right)^{T}Q\\sigma\\left(t,x_{t}\\right)d t}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then, take expectation, we can get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E\\left\\{d V\\left(x_{t}\\right)\\right\\}=x_{t}^{T}Q b\\left(t,x_{t}\\right)d t+b\\left(t,x_{t}\\right)^{T}Q x_{t}d t+}\\\\ &{\\quad\\quad\\quad\\quad\\sigma\\left(t,x_{t}\\right)^{T}Q\\sigma\\left(t,x_{t}\\right)d t}\\\\ &{\\quad\\quad\\quad\\quad\\quad=L V\\left(x_{t}\\right)d t}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then, if we assume that $L V(x)$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n-L V\\left(x_{t}\\right)\\geq k V\\left(x_{t}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "such that $k$ is a constant, then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{d}{d t}E\\left\\{V\\left(x_{t}\\right)\\right\\}\\leq-k E\\left\\{V\\left(x_{t}\\right)\\right\\},}\\\\ &{E\\left\\{V\\left(x_{t}\\right)\\right\\}\\leq\\exp(-k t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "As can be seen from the proof, the operator $L V$ as the function of the SDE $x_{t}$ is the expectation of the $d V(x_{t})$ , and the negative semi-definiteness can be regarded as requiring $d V(x_{t})$ to be a contraction. This can be understood from 34 such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\frac{d}{d t}}E(V(x_{t}))/E(V(x_{t}))<-k\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for $k>0$ ", "page_idx": 21}, {"type": "text", "text": "H Implementation details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Model Implementation: Our exploration into mitigating Temporal Distribution Discrepancy in diffusion models employs two neural network architectures, each catering to different dataset complexities. ", "page_idx": 21}, {"type": "text", "text": "We utilize the adopted UNet, a staple in DDPM [24] and DDIM[53] frameworks, chosen for its widespread use and strong benchmarking capabilities. By solving the discrepancy through this established structure, we achieve state-of-the-art results on synthetic data, showcasing the potential of improving transanary SDE diffusion models in terms of Temporal Distribution Discrepancy and stationary process. Additionally, the use of this prevalent architecture allows for comprehensive analysis and discussion. ", "page_idx": 21}, {"type": "text", "text": "Real-world data with its inherent complexity, such as combined degradations, large resolutions, and extensive interdependencies, requires an architecture beyond the conventional UNet. Our improved model incorporates Squeeze-and-Excitation [38] and NAF [8], explicitly designed to capture intricate feature interrelations. While these models do not seek to innovate the architectural paradigm, it provides a solid baseline that provides better feature extraction ability than UNet performance in demanding scenarios. ", "page_idx": 21}, {"type": "text", "text": "Additional details: The U-Net we adopted is similar to DDPM as described in [37, 11, 9], where the improved model incorporates Squeeze-and-Excitation [38] to replace the Attention module within NAF [8]. EDSR [33] is employed as the base model for TTA-based comparison methods in MRI super-resolution. In different downstream tasks, we follow the common setting of the latest compared methods: Deraining [37], Real Dehazing [47], MRI Reconstruction [26], and MRI super-resolution [14] and [33]. Below are more specific details about MRI reconstruction and MRI superresolution. For most of the experiments, the training patch-size is set to $128\\mathrm{x}128$ with a batch size of 16. We utilize the Adam optimizer with $\\beta_{1}=0.9$ , $\\beta_{2}=0.999$ , a learning rate of $10^{-4}$ with a decay strategy. Our models are trained on three RTX 6000 GPUs for about four days, each with 40GB of memory. Random seed is 42. All mathematical variants follow the cosine schedule as per [42]. The variance $\\lambda$ is set to 10 for the OU and stationary processes. In the coef. decoupled SDE, $\\sigma$ is kept decoupled and does not vary with $\\theta$ . We observed that the adaptability of $\\tau$ to tasks is contingent upon the task\u2019s corruption for model stability and generalizability. ", "page_idx": 21}, {"type": "table", "img_path": "VTJvTa41D0/tmp/af855b83815653c60d6c86d15c93516a0a7f38dd4e1582f4ef8bff045749d7ae.jpg", "table_caption": ["Table 9: Datasets parameters and split setting. IOP details of the scan parameters are not available. "], "table_footnote": ["Table 10: Evaluation of the Lipschitz continuity. "], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "$\\tau$ is a hyperparameter as one of the ways to introduce Measure-preserving Dynamics to shape more stable SDE diffusion. It is informed by the deviation between the degraded image and the expected high-quality image. For tasks with moderate intra-domain deviations, $\\tau$ is set to 2, while for tasks with substantial cross-domain and degradation discrepancies, a larger $\\tau$ is used. This metric is based on RDS settings of different degradations, rather than learned. A MLP can be used to learn $\\tau$ as adaptive metric, although it is not comparable with obvious improvements. ", "page_idx": 22}, {"type": "text", "text": "SGM, and Transitionary SGMs vs. $\\mathbf{D}^{3}\\mathbf{GM}$ : We perform qualitative 2 and quantitative 11 analyses using variants of closely related formulations for Prop. 1 and 2 and evaluate across (A) SGMs and (B) transitionary SGMs. (A) uses a common score-based SDE, (B) uses a Coefficient Decoupled SDE (e.g., variance exploding SDE with the drift term $\\mu$ ) according to Prop. 1 and OU SDE, alongside our $\\mathrm{D^{3}G M}$ . ", "page_idx": 22}, {"type": "text", "text": "MRI Reconstruction: We applied 584 proton-density weighted knee MRI scans without fat suppression. These were subsequently partitioned into a training set (420 scans), a validation set (64 scans), and a testing set (100 scans). For each scan, we extracted 20 coronal 2D singlechannel complex-valued slices, predominantly from the central region with the uniform size of 320 $\\textbf{x}320$ . Our test set differs from the 200 reported in [26], possibly as a result of the change in the official versions of FastMRI. We subjected all experiments to Cartesian under-sampling masks with undersampling factor $8\\mathbf{x}$ and $16\\mathrm{x}$ . In the undersampled MRI scans, the acquisition process entails sampling a fractional subset of the Fourier-space (k-space), typically governed by a mask along the dimension of undersampling rate. Undersampling inherently induces aliasing artifacts in the resultant images. Considering the domain deviation, we did not supplement the extra test set into the final 100 samples. Otherwise, we stayed consistent with the details of the paper. For a robust comparison, we benchmarked against a diverse set of deep learning-based state-of-the-art reconstruction methods, including CNN-based approaches such as D5C5 [51] and DAGAN [62], the Transformer-based SwinMR [27], diffusion model-inspired DiffuseRecon [44], and the CDiffMR [26]. Quantitative results in Tab. 5 exhibit a differing trend from the image domain. The task-specific diffusion models achieve better results and are more capable of capturing the complex degradation that occurs in the frequency domain. ", "page_idx": 22}, {"type": "table", "img_path": "VTJvTa41D0/tmp/0cf13f971a0d5303475b6f7a519b0403d522cf7671dddd04b1b8c395c38220d7.jpg", "table_caption": ["Table 11: Exemplary deraining results of (A) SGM, (B) transitionary SGMs: Coefficent Decoupled SDE, OU SDE, and $\\mathrm{D^{3}G M}$ . "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Generally, MRI uses a mask in the phase-encoding direction (the shortest anatomical direction [41]) to model the complex degradation caused by undersampling. In the knee data, based upon the uncertainty of clinical diagnosis (longitudinal artifacts can significantly confuse the diagnosis of meniscal injury), we also masked the frequency direction, which will cause resolution reduction, deterioration of image features, and longitudinal artifacts, more qualitative results can be found in the next section. ", "page_idx": 23}, {"type": "text", "text": "MRI Super-resolution: $\\mathrm{IXI^{4}}$ contains clinical T1- and T2-weighted scans from three hospitals with different imaging protocols: HH, Guys, and IOP. We selected 184 HH T2 subjects as the sourcedomain data [train/val/test ratio: 7:1:2], and 30 subjects each from Guys and IOP as two target-domain datasets without degradation, acquisition parameters, datasets, and patient-wise crossovers. The central 60 slices were selected. ", "page_idx": 23}, {"type": "text", "text": "We consider two benchmark ideas: (1) Blind SR degradation methods in frequency SFM [19] and spatial PDM [70] domain. (2) Source-free SR adaptation ACT [14] using external priors and testtime adaptation proposed initially for MRI CST [14] with data consistency in the source-domain. We are concerned that existing work on robustness and generalization focuses on medical image segmentation and natural images, and rarely on super-resolution and reconstruction of medical images. Employing the available evaluation criteria in this inevitable problem in practice makes it difficult to reflect the performance of $\\mathrm{D^{3}G M}$ . Therefore, domain-aware datasets isolation standard for public datasets is designed for the SRR adaptation to cross-domain data rather than same-domain in a source-free manner. The publicly available data were split into several subsets based on hospitals, scanner, acquisition parameters, modality, and anatomy as illustrated in Table 9. Explicit reference standards implicitly correspond to various degradation patterns, thus enabling the isolation of natural degradation patterns in source training domain and target-testing domain. In addition to this, different artificial degradation patterns for the subsets in the training and testing domains are employed: K-space truncation downsampling was applied to obtain LR data in the source domain and a kernel degradation [5] was applied in target domain. ", "page_idx": 23}, {"type": "text", "text": "We reproduce two types of benchmark ideas on the top of EDSR [33] backbone to achieve the multipurpose goal: (1) Repurposed blind SR (BSR) for cross-domain data: We utilized BSR degradation methods in frequency SFM [19] and spatial PDM [70] domain. (2) Test-time adaptation (TTA): We compare to a source-free TTA method ACT [14] using external priors and a second TTA method proposed initially for MR reconstruction CST [14] with cycle consistency at source-domain. The settings and adaptation strategies of the comparison methods were used directly. ", "page_idx": 23}, {"type": "text", "text": "Lipschitz Continuity for Stationary Process: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We hypothesize that ensuring Lipschitz continuity of the neural network is pivotal for the convergence and stability of the diffusion process, particularly in the context of achieving a stationary process. Theoretically, Lipschitz continuity offers two key benefits: (1) it mitigates the impact of small perturbations in input data or model parameters, thus safeguarding against excessive output variability which could cause numerical instabilities or result in an ill-posed problem, and (2) it guarantees the existence of a unique solution to the diffusion process, underpinning the reliability and convergence of the numerical methods employed to solve these equations. ", "page_idx": 23}, {"type": "text", "text": "To instantiate these theoretical beneftis within our architecture, we integrated spectral normalization (SN) [40] and weight decay (WD) [35] into a U-Net structured score network. In the case of SN, it is achieved by rescaling each layer\u2019s weight matrix $W^{(l)}$ by its spectral norm, $\\sigma_{m a x}(W^{(l)})$ , to obtain a normalized weight $\\begin{array}{r}{\\tilde{W}^{(l)}=\\frac{W^{(l)}}{\\sigma_{m a x}(W^{(l)})}}\\end{array}$ \u03c3maWx( (lW) (l)). By doing so, we intend to control the overall Lipschitz constant of the network for robust score matching within our diffusion model framework. A quantitive result can be seen in Tab. 10. ", "page_idx": 23}, {"type": "image", "img_path": "VTJvTa41D0/tmp/95da77534ad99450b3a4f42700fd3424eecc884760bc199fe477849fbb7887ca.jpg", "img_caption": ["Figure 4: Reverse Initialization with Basin of attraction. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "I Additional Insights ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Basin of Attraction in Reverse Sampling: Within our diffusion framework, we establish a forward process with variety to accommodate a wide range of potential corruptions, ensuring the desired final distribution close to the expected distribution. We consider that our diffusion models inherently encompass the forward operator $\\boldsymbol{\\mathcal{A}}$ within their structure. The transition from high to low-quality images is implicitly encoded in the diffusion pathway, hence the additional forward operator guidance might not contribute supplementary information, which we initially hypothesized would enhance the reverse process. ", "page_idx": 24}, {"type": "text", "text": "Thus, the keypoint transfers from the $\\boldsymbol{\\mathcal{A}}$ to the initial $y^{\\prime}$ (Detailed analysis based on Prop. 2 is provided in Appx. F). Also we found this problem fall into the Basin of Attraction (BA) in dynamical system as shown in Fig. 4. BA can be interpreted as the quality of the attractor of the degraded image in the reverse process here. A BA-guided initialization might be a more effective approach for posterior sampling in transitional SDE diffusion models. By initializing the reverse process closer to the expected solution, we may bypass the initial hurdle of distribution mismatch. ", "page_idx": 24}, {"type": "text", "text": "J More Qualitative Results ", "text_level": 1, "page_idx": 24}, {"type": "image", "img_path": "VTJvTa41D0/tmp/422c45807b31bf8e8278bb281013cc3e783aee2bc45519706ae598d74e5a9b07.jpg", "img_caption": ["Figure 5: Deraining results with light rain images of our method. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "VTJvTa41D0/tmp/32056ac3624d20dc5b5cb64c7203a8c779df4a8d8e435a160ba8851de07b55f0.jpg", "img_caption": ["Figure 6: Deraining results with heavy rain images of our method. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "VTJvTa41D0/tmp/73e96647f6a1de5865370ccbf8612ac58386875482b563d5e833e3d1bc462a7c.jpg", "img_caption": ["Figure 7: Dehazing results with real hazy images of our method. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "VTJvTa41D0/tmp/d08365e0181715db12968440f799507d9a667992eed2802b076b07e60e8edd08.jpg", "img_caption": ["Figure 8: MRI reconstruction results with undersampling rate $\\mathbf{\\nabla}\\times\\mathbf{8}$ and ${\\bf x}16$ , on Frequency-encoding and Phase-encoding directions. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "VTJvTa41D0/tmp/f70c2d87ac46212a09fa7021fe84e84ec1d1604dd029eb592b2e98dada9bb576.jpg", "img_caption": ["Figure 9: MRI super-resolution results with in-domain images of our method. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "VTJvTa41D0/tmp/105455beffbdb6b3f6ab68130fd183973ed27eb38e10be6aba6ec77c0939512a.jpg", "img_caption": ["Figure 10: MRI super-resolution results with cross-domain (different imaging devices and degradation methods) images of our method. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "VTJvTa41D0/tmp/dca34364dd6aba7f97d5bf8dc090cc392b1840d08b2fea6912c3768b4ceb42b2.jpg", "img_caption": ["Figure 11: MRI reconstruction results with undersampling rate $^{\\mathrm{\\scriptsize~x8}}$ and $\\mathbf{x}16$ of our method, on Phaseencoding and Frequency-encoding directions. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. The abstract clearly outlines the claims of existing approaches, introduces the novel score-based diffusion framework $(\\mathrm{D^{3}G M})$ , and highlights the effectiveness of the proposed method through extensive experimental results. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: The paper discusses the limitations of the method under the discussion, and provide more details in the appendix. The main claims made in the paper accurately reflect the paper\u2019s scope. Thus, we also provide more extra findings beyond the limitation. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: The paper provides a full set of assumptions and complete proofs for the theoretical results introduced, including the measure-preserving dynamics of Random Dynamical Systems (RDS) and the Temporal Distribution Discrepancy analysis. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper fully discloses all the information needed to reproduce the main experimental results, including details about the benchmarks used, such as magnetic resonance imaging, and the effectiveness of the $\\mathrm{D^{3}G M}$ framework across multiple benchmarks. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 33}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Answer: [No] ", "page_idx": 34}, {"type": "text", "text": "Justification: We have provide all the details and instructions along with the paper and the appendix for the reproducible results. The datasets involved in the paper are public. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper specifies all necessary training and test details, such as data splits, hyperparameters, and optimizer types, ensuring that the results can be fully understood and appreciated. Full details are provided in the supplemental material to complement the core paper. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper reports evaluation metrics and other appropriate statistical significance information for the experiments, clearly explaining the factors of variability and the methods used for calculating these measures. On different downstream experiments, we adapted appropriate evaluation metrics, following state-of-the-art work. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g., negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 35}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper provides detailed information on the compute resources required for the experiments. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: The research conducted in the paper conforms to the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: The paper discusses both potential positive and negative societal impacts of the work, such as the improvement of diffusion models for medical imaging and the risk of misuse in generating deceptive content. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: Necessary safeguards are described for the responsible release of models medical and image generators. These safeguards help ensure controlled and ethical use. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper properly credits the contributors of existing assets used in the research. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The new assets introduced in the paper, including the $\\mathrm{D^{3}G M}$ framework, are well-documented, with comprehensive documentation provided alongside the assets. This facilitates their use and further development by the research community. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: NA. The paper does not involve crowdsourcing or research with human subjects, so this section is not applicable. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: NA. The paper does not involve crowdsourcing or research with human subjects, so this section is not applicable. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}]