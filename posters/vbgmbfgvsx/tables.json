[{"figure_path": "vBGMbFgvsX/tables/tables_4_1.jpg", "caption": "Table 1: IQM and probability of improvement (PI) over H-only (P(X > H-only)) with 95% confidence intervals across 12 heuristic reward functions (Section 4.2). H-only uses PPO with heuristic rewards. HEPO achieves higher normalized returns and a statistically significant PI greater than 0.5, indicating it significantly outperforms H-only.", "description": "This table presents a comparison of the performance of different algorithms on 12 tasks with heuristic reward functions created by human participants. The algorithms are evaluated using two metrics: the interquartile mean (IQM) of normalized task returns and the probability of improvement (PI) over the heuristic-only baseline (H-only). The results show that HEPO consistently outperforms H-only, demonstrating its robustness and effectiveness in utilizing heuristic rewards.", "section": "4.2 Can HEPO be robust to reward functions designed in the wild?"}, {"figure_path": "vBGMbFgvsX/tables/tables_6_1.jpg", "caption": "Table 1: IQM and probability of improvement (PI) over H-only (P(X > H-only)) with 95% confidence intervals across 12 heuristic reward functions (Section 4.2). H-only uses PPO with heuristic rewards. HEPO achieves higher normalized returns and a statistically significant PI greater than 0.5, indicating it significantly outperforms H-only.", "description": "This table presents a comparison of the performance of different reinforcement learning algorithms on 12 tasks.  The algorithms are evaluated based on two metrics: the interquartile mean (IQM) of normalized task return and the probability of improvement (PI) over a baseline algorithm trained only with heuristic rewards (H-only).  The results demonstrate that the HEPO algorithm significantly outperforms the baseline and other compared methods, showcasing its robustness and effectiveness.", "section": "4.2 Can HEPO be robust to reward functions designed in the wild?"}, {"figure_path": "vBGMbFgvsX/tables/tables_15_1.jpg", "caption": "Table 2: HEPO Hyperparameters", "description": "This table lists the hyperparameters specifically used for the Heuristic-Enhanced Policy Optimization (HEPO) algorithm in the paper's experiments.  It includes the initial value of the Lagrangian multiplier (\u03b1), the learning rate for updating \u03b1, the clipping range applied to the \u03b1 updates to prevent instability, and the allowed range of \u03b1 values.", "section": "3.2 Implementation"}, {"figure_path": "vBGMbFgvsX/tables/tables_23_1.jpg", "caption": "Table 1: IQM and probability of improvement (PI) over H-only (P(X > H-only)) with 95% confidence intervals across 12 heuristic reward functions (Section 4.2). H-only uses PPO with heuristic rewards. HEPO achieves higher normalized returns and a statistically significant PI greater than 0.5, indicating it significantly outperforms H-only.", "description": "This table presents the results of comparing HEPO's performance against a baseline (H-only) across 12 heuristic reward functions.  The interquartile mean (IQM) of normalized task returns and the probability of improvement (PI) are shown, along with 95% confidence intervals. The results demonstrate that HEPO consistently outperforms the H-only baseline.", "section": "4.2 Can HEPO be robust to reward functions designed in the wild?"}, {"figure_path": "vBGMbFgvsX/tables/tables_35_1.jpg", "caption": "Table 1: IQM and probability of improvement (PI) over H-only (P(X > H-only)) with 95% confidence intervals across 12 heuristic reward functions (Section 4.2). H-only uses PPO with heuristic rewards. HEPO achieves higher normalized returns and a statistically significant PI greater than 0.5, indicating it significantly outperforms H-only.", "description": "This table presents a quantitative comparison of HEPO against the H-only baseline and other algorithms.  The comparison uses two metrics: the Interquartile Mean (IQM) of normalized task returns, a robust measure of central tendency, and the probability of improvement (PI) over the H-only baseline, indicating the frequency with which HEPO outperforms the H-only policy. The results show HEPO's statistical significance in improving performance.", "section": "4.2 Can HEPO be robust to reward functions designed in the wild?"}, {"figure_path": "vBGMbFgvsX/tables/tables_41_1.jpg", "caption": "Table 1: IQM and probability of improvement (PI) over H-only (P(X > H-only)) with 95% confidence intervals across 12 heuristic reward functions (Section 4.2). H-only uses PPO with heuristic rewards. HEPO achieves higher normalized returns and a statistically significant PI greater than 0.5, indicating it significantly outperforms H-only.", "description": "This table presents the results of a comparison between HEPO and the H-only baseline across 12 heuristic reward functions. The results are presented in terms of Interquartile Mean (IQM) of normalized task return and the probability of improvement (PI) over H-only, both with 95% confidence intervals.  The results show that HEPO significantly outperforms H-only.", "section": "4.2 Can HEPO be robust to reward functions designed in the wild?"}, {"figure_path": "vBGMbFgvsX/tables/tables_43_1.jpg", "caption": "Table 1: IQM and probability of improvement (PI) over H-only (P(X > H-only)) with 95% confidence intervals across 12 heuristic reward functions (Section 4.2). H-only uses PPO with heuristic rewards. HEPO achieves higher normalized returns and a statistically significant PI greater than 0.5, indicating it significantly outperforms H-only.", "description": "This table presents a quantitative comparison of different reinforcement learning algorithms on 12 heuristic reward functions.  The Interquartile Mean (IQM) of normalized task returns and the probability of improvement over a heuristic-only baseline (H-only) are reported for each algorithm, along with 95% confidence intervals. The results show that HEPO consistently outperforms the heuristic-only baseline, achieving statistically significant improvements.", "section": "4.2 Can HEPO be robust to reward functions designed in the wild?"}, {"figure_path": "vBGMbFgvsX/tables/tables_46_1.jpg", "caption": "Table 1: IQM and probability of improvement (PI) over H-only (P(X > H-only)) with 95% confidence intervals across 12 heuristic reward functions (Section 4.2). H-only uses PPO with heuristic rewards. HEPO achieves higher normalized returns and a statistically significant PI greater than 0.5, indicating it significantly outperforms H-only.", "description": "This table presents the results of an experiment comparing different RL algorithms on 12 heuristic reward functions.  The interquartile mean (IQM) of normalized task return and the probability of improvement over the heuristic-only policy (H-only) are reported for each algorithm, along with 95% confidence intervals.  HEPO consistently outperforms other methods.", "section": "4.2 Can HEPO be robust to reward functions designed in the wild?"}]