[{"type": "text", "text": "Going Beyond Heuristics by Imposing Policy Improvement as a Constraint ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Chi-Chang Lee1\u2217, Zhang-Wei $\\mathbf{Hong^{2*}}$ , Pulkit Agrawal2 Improbable AI Lab Massachusetts Institute of Technology ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In many reinforcement learning (RL) applications, incorporating heuristic rewards alongside the task reward is crucial for achieving desirable performance. Heuristics encode prior human knowledge about how a task should be done, providing valuable hints for RL algorithms. However, such hints may not be optimal, limiting the performance of learned policies. The currently established way of using heuristics is to modify the heuristic reward in a manner that ensures that the optimal policy learned with it remains the same as the optimal policy for the task reward (i.e., optimal policy invariance). However, these methods often fail in practical scenarios with limited training data. We found that while optimal policy invariance ensures convergence to the best policy based on task rewards, it doesn\u2019t guarantee better performance than policies trained with biased heuristics under a finite data regime, which is impractical. In this paper, we introduce a new principle tailored for finite data settings. Instead of enforcing optimal policy invariance, we train a policy that combines task and heuristic rewards and ensures it outperforms the heuristic-trained policy. As such, we prevent policies from merely exploiting heuristic rewards without improving the task reward. Our experiments on robotic locomotion, helicopter control, and manipulation tasks demonstrate that our method consistently outperforms the heuristic policy, regardless of the heuristic rewards\u2019 quality. Code is available at https://github.com/Improbable-AI/hepo. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement learning (RL) [1] is a powerful framework for learning policies that can surpass human performance in complex tasks. However, training RL policies with sparse or delayed rewards is often ineffective. Instead of relying solely on sparse task rewards that indicate an agent\u2019s success or failure, it is common to augment the sparse reward with heuristic reward terms that provide denser reward supervision to speed up and improve the performance of RL policies [2, 3]. Shining examples of the success and necessity of heuristic reward terms are complex robotic object manipulation [4] and locomotion [5\u20138] tasks. However, heuristics impose human assumptions that may limit the RL algorithm. For example, a heuristic reward function may encourage a robot to walk like a human, yet there could be faster walking policies that don\u2019t resemble human gait. ", "page_idx": 0}, {"type": "text", "text": "The key question is how to learn a policy $\\pi$ that outperforms one trained solely on heuristics (i.e., heuristic policy $\\pi_{H}$ ). Practitioners tackle this problem by tuning the balance between the task objective $J(\\pi)$ and the heuristic objective $H(\\pi)$ in the augmented training objective $J(\\pi)+\\lambda H(\\pi)$ , where $\\lambda$ controls the balance between the two objectives for a policy $\\pi$ . However, it requires careful tuning of $\\lambda$ to make the policy $\\pi$ outperform the heuristic policy; otherwise, the algorithm might prioritize heuristic rewards while neglecting the task objective. ", "page_idx": 0}, {"type": "text", "text": "Tuning $\\lambda$ to balance both objectives is time-consuming. We desire an algorithm that finds a policy that outperforms the heuristic policy without requiring such tuning for any given heuristic reward function. Classic methods [2, 9\u201314] modify heuristic rewards to align the augmented objective\u2019s optimal policy with the one for the task objective (i.e., optimal policy invariance), theoretically ensuring that with infinite data the policy outperforms the heuristic. However, in practice, these modified heuristics often fall short on complex robotic tasks compared to policies trained solely on heuristic objectives, as demonstrated in our study (Section 4.1) and prior work [10]. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we challenge the prevailing paradigm by questioning whether optimal policy invariance is the appropriate objective to prevent heuristics from limiting RL agent\u2019s performance in the finite data regime. As optimal policy invariance ensures convergence to the optimal policy with infinite data it may not be practical in many real-world settings. We propose an alternative paradigm that, in every step of training, imposes the constraint of improving task performance beyond a policy trained solely on heuristic rewards (i.e., $J(\\pi)\\,\\ge\\,J(\\pi_{H}))$ . This condition $J(\\pi)\\,\\ge\\,J(\\pi_{H})$ guarantees that the learned policy $\\pi$ outperforms the heuristic policy $\\pi_{H}$ , effectively surpassing human-designed heuristics. Additionally, such policy improvements can be verified and achieved using many existing deep RL algorithms [15\u201318] in finite data settings. ", "page_idx": 1}, {"type": "text", "text": "Therefore, we enforce the policy improvement condition $J(\\pi)\\ge J(\\pi_{H})$ as a constraint, preventing the policy from exploiting heuristic rewards during training. We propose the following constrained optimization objective: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi}J(\\pi)+H(\\pi)\\quad\\mathrm{subject\\,to}\\quad J(\\pi)\\geq J(\\pi_{H})\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Optimizing this objective at each iteration allows learning a policy performing better than or equal to policies trained only on heuristic rewards. It prevents capitalizing on heuristic rewards at the expense of task rewards. Moreover, it enables adaptively balancing both rewards over time instead of using a fixed coefficient $\\lambda$ . Our contribution is an add-on to existing deep RL algorithms to improve RL algorithms trained with heuristic rewards. We evaluated our method on robotic locomotion, helicopter, and manipulation tasks using the IsaacGym simulator [19]. The results show that our method led to superior task rewards and higher task-completion success rates compared to the policies solely trained with heuristic rewards, even when heuristic rewards are ill-designed. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries: Reinforcement Learning with Heuristic ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Reinforcement Learning (RL): RL is a popular paradigm for solving sequential decision-making problems [1] where the problems are modeled as an interaction between an agent and an unknown environment [1]. The agent aims to improve its performance through repeated interactions with the environment. At each round of interaction, the agent starts from the environment\u2019s initial state $s_{0}$ and samples the corresponding trajectory. At each timestep $t$ within that trajectory, the agent perceives the state $s_{t}$ , takes an action $a_{t}\\sim\\pi(.|s_{t})$ according to its policy $\\pi$ , receives a $t a s k$ reward $\\bar{r_{t}}=r(s_{t},a_{t})$ , and transitions to a next state $s_{t+1}$ until reaching terminal states, after which a new trajectory is initialized from $s_{0}$ , and the cycle repeats. The agent\u2019s goal is to learn a policy $\\pi$ that maximizes the expected return $J(\\pi)$ in a trajectory as below: ", "page_idx": 1}, {"type": "equation", "text": "$$\nJ(\\pi)=\\mathbb{E}_{\\pi}[\\sum_{t=0}^{\\infty}\\gamma^{t}r(s_{t},a_{t})],\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\gamma$ denotes a discount factor [1] and $\\mathbb{E}_{\\pi}\\left[.\\right]$ denotes taking expectation over the trajectories sampled by $\\pi$ . In the following, we term $J$ as the true task objective, as it indicates the performance of a policy on the task. ", "page_idx": 1}, {"type": "text", "text": "RL with Heuristic: In many tasks, learning a policy to maximize the true objective $J$ is challenging because rewards may be sparse or delayed. This lack of feedback makes policy optimization difficult for RL algorithms. To address this, practitioners often use a heuristic reward function $h$ with denser reward signals to facilitate optimization, aiming to learn a policy that performs better in $J$ . The policy trained to maximize the expected return of heuristic rewards is called the heuristic policy $\\pi_{H}$ . The expected return of heuristic rewards, termed the heuristic objective $H$ , is defined as: ", "page_idx": 1}, {"type": "equation", "text": "$$\nH(\\pi_{H})=\\mathbb{E}_{\\pi_{H}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}h(s_{t},a_{t})\\right],\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "3 Method: Improving Heuristic Policy via Constrained Optimization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Problem statement: Optimizing both task $J$ and heuristic $H$ objectives jointly could lead to better task performance than training solely with $J$ or $H$ , but needs careful tuning on the weight coefficient $\\lambda$ among both objectives in $\\operatorname*{max}_{\\pi}\\bar{J}(\\pi)+\\lambda H(\\pi)$ . Without careful tuning, the policy $\\pi$ may learn to exploit heuristic rewards $H$ and compromise performance of $J$ . The goal of this paper is to mitigate the requirement of tuning this coefficient to balance them in order to improve task performance. ", "page_idx": 2}, {"type": "text", "text": "Key insight - Leveraging Heuristic with Constraint: We aim to use the heuristic objective $H$ for training only when it improves task performance $J$ and ignore it otherwise. Rather than manually tuning the weight coefficient $\\lambda$ to balance both rewards, we introduce a key insight: impose a policy improvement constraint (i.e., $J(\\pi)\\,\\ge\\,J(\\pi_{H}))$ during training. This prevents RL algorithms from exploiting heuristic rewards $H$ at the expense of task rewards $J$ . To achieve this goal, we introduce the following constrained optimization objective: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi}J(\\pi)+H(\\pi){\\mathrm{~subject~to~}}J(\\pi)\\geq J(\\pi_{H}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This constrained objective (Equation 3) results in an improved policy $\\pi$ over the heuristic policy $\\pi_{H}$ , leading us to call this framework Heuristic-Enhanced Policy Optimization $\\langle H E P O\\rangle$ . A practical algorithm to optimize this objective is presented in Section 3.1, and its implementation on a widelyused RL algorithm [15] in robotics is detailed in Section 3.2. ", "page_idx": 2}, {"type": "text", "text": "3.1 Algorithm: Heuristic-Enhanced Policy Optimization (HEPO) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Finding feasible solutions for the constrained optimization problem in Equation 3 is challenging due to the nonlinearity of the objective function $J$ with respect to $\\pi$ . One practical approach is to convert it into the following unconstrained min-max optimization problem using Lagrangian duality: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\alpha\\geq0}\\operatorname*{max}_{\\pi}\\mathcal{L}(\\pi,\\alpha),\\mathrm{~where~}\\mathcal{L}(\\pi,\\alpha):=J(\\pi)+H(\\pi)+\\alpha\\left(J(\\pi)-J(\\pi_{\\mathrm{H}})\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the Lagrangian multiplier is $\\alpha\\in\\mathbb{R}^{+}$ . We can optimize the policy $\\pi$ and the multiplier $\\alpha$ for this min-max problem by a gradient descent-ascent strategy, alternating between optimizing $\\pi$ and $\\alpha$ . ", "page_idx": 2}, {"type": "text", "text": "Enhanced policy $\\pi$ : The optimization objective for the policy $\\pi$ can be obtained by rearranging Equation 4 as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{\\pi}\\ (1+\\alpha)J(\\pi)+H(\\pi),}\\\\ &{\\mathrm{where}\\ (1+\\alpha)J(\\pi)+H(\\pi)=\\mathbb{E}_{\\pi}\\left[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\big((1+\\alpha)r(s_{t},a_{t})\\big)+h(s_{t},a_{t})\\big)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This represents an unconstrained regular RL objective with the modified reward at each step as $(1+\\alpha)\\bar{r}(s_{t},a_{t})+h(s_{t},a_{t})$ , which can be optimized using any off-the-shelf deep RL algorithm. In this modified reward, the task reward $r(s_{t},a_{t})$ is weighted by the Lagrangian multiplier $\\alpha$ , reflecting the potential variation in the task reward\u2019s importance during training as $\\alpha$ evolves. The interaction between the update of the Lagrangian multiplier and the policy will be elaborated upon next. ", "page_idx": 2}, {"type": "text", "text": "Lagrangian Multiplier $\\alpha$ : The Lagrangian multiplier $\\alpha$ is optimized for Equation 4 by stochastic gradient descent, with the gradient defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\nabla_{\\alpha}\\mathcal{L}(\\pi,\\alpha)=J(\\pi)-J(\\pi_{\\mathrm{H}}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Notably, $\\nabla_{\\alpha}\\mathcal{L}(\\pi,\\alpha)$ is exactly the performance gain of the policy $\\pi$ over the heuristic policy $\\pi_{H}$ on the task objective $J$ . By applying gradient descent with $\\nabla_{\\alpha}\\mathcal{L}(\\pi,\\alpha)$ , when $J(\\pi)>\\bar{J}(\\pi_{H}\\bar{)}$ and thus $\\nabla_{\\alpha}\\mathcal{L}(\\pi,\\bar{\\alpha})>0$ , the Lagrangian multiplier $\\alpha$ decreases. As $\\alpha$ represents the weight of the task reward in Equation 5, it indicates that when $\\pi$ outperforms $\\pi_{H}$ , the importance of the task reward diminishes because $\\pi$ already achieves superior performance compared to the heuristic policy $\\pi_{H}$ regarding the task objective $J$ . Conversely, when $\\bar{J}(\\pi)<J(\\pi_{H})$ , $\\alpha$ increases, thereby emphasizing the importance of task rewards in optimization. The update procedure for the Lagrangian multiplier $\\alpha$ offers an adaptive reconciliation between the heuristic reward $h$ and the task reward $r$ . ", "page_idx": 2}, {"type": "text", "text": "3.2 Implementation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We present a practical approach to optimize the min-max problem in Equation 4 using Proximal Policy Optimization (PPO) [15]. We selected PPO because it is widely used in robotic applications involving heuristic rewards, although our HEPO framework is not restricted to PPO. The standard PPO implementation involves iterative stochastic gradient descent updates over numerous iterations, alternating between collecting trajectories with policies and updating those policies. We outline the optimization process for each iteration and provide a summary of our implementation in Algorithm 1. ", "page_idx": 3}, {"type": "text", "text": "Training policies $\\pi$ and $\\pi_{H}$ : Instead of pre-training the heuristic policy $\\pi_{H}$ , which requires additional data and reduces data efficiency, we concurrently train both the enhanced policy $\\pi$ and the heuristic policy $\\pi_{H}$ , allowing them to share data. For each iteration $i$ , we gather trajectories $\\tau$ and $\\tau_{H}$ using the enhanced policy $\\pi^{i}$ and the heuristic policy $\\pi_{H}^{i}$ , respectively. Following PPO\u2019s implementation, we compute the advantages $A_{r}^{\\pi^{i}}(s_{t},a_{t}),A_{r}^{\\pi_{H}^{i}}(s_{t},a_{t}),A_{h}^{\\pi^{i}}(s_{t},a_{t})$ , and $A_{h}^{\\pi_{H}^{i}}(s_{t},a_{t})$ for the task reward $r$ and heuristic reward $h$ with respect to $\\pi^{i}$ and $\\pi_{H}^{i}$ . We then weight the advantage with the action probability ratio between the new policies being optimized (i.e., $\\pi^{i+1}$ and $\\pi_{H}^{i+1}.$ ) and the policies collecting the trajectories (i.e., $\\pi^{i}$ or $\\pi_{H}^{i}$ ). Finally, we optimize the policies at the next iteration $i+1$ for the objectives in Equations 7 and 8: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi^{i+1}\\leftarrow\\arg\\operatorname*{max}\\mathbb{E}_{\\tau\\sim\\pi^{i}}\\left[\\frac{\\pi\\left(a_{t}|s_{t}\\right)}{\\pi^{i}\\left(a_{t}|s_{t}\\right)}\\left((1+\\alpha)A_{r}^{\\pi^{i}}\\left(s_{t},a_{t}\\right)+A_{h}^{\\pi^{i}}(s_{t},a_{t})\\right)\\right]+}\\\\ &{\\qquad\\qquad\\qquad\\mathbb{E}_{\\tau_{H}\\sim\\pi_{H}^{i}}\\left[\\frac{\\pi\\left(a_{t}|s_{t}\\right)}{\\pi_{H}^{i}\\left(a_{t}|s_{t}\\right)}\\left((1+\\alpha)A_{r}^{\\pi_{H}^{i}}\\left(s_{t},a_{t}\\right)+A_{h}^{\\pi_{H}^{i}}(s_{t},a_{t})+\\right)\\right]}\\\\ &{\\pi_{H}^{i+1}\\leftarrow\\arg\\operatorname*{max}\\mathbb{E}_{\\tau_{H}\\sim\\pi_{H}^{i}}\\left[\\frac{\\pi\\left(a_{t}|s_{t}\\right)}{\\pi_{H}^{i}\\left(a_{t}|s_{t}\\right)}A_{h}^{\\pi^{i}}\\left(s_{t},a_{t}\\right)\\right]+}\\\\ &{\\qquad\\qquad\\qquad\\mathbb{E}_{\\tau\\sim\\pi^{i}}\\left[\\frac{\\pi\\left(a_{t}|s_{t}\\right)}{\\pi^{i}\\left(a_{t}|s_{t}\\right)}A_{h}^{\\pi_{H}^{i}}\\left(s_{t},a_{t}\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Maximizing the advantages will result in a policy that maximizes the expected return for a chosen reward function, as demonstrated in PPO [15]. This enables us to maximize the objective $J$ and $H$ . We estimate the advantages $A_{r}^{\\pi^{i}}(s_{t},a_{t})$ and $A_{h}^{\\pi^{i}}(s_{t},a_{t})$ (or $A_{r}^{\\pi_{H}^{i}}(s_{t},a_{t})$ and $A_{h}^{\\pi_{H}^{i}}\\big(s_{t},a_{t}\\big))$ using the standard PPO implementation with different reward functions. Therefore, we omit the details of the advantage\u2019s clipped surrogate objective in PPO, and leave them in Appendix A.1. ", "page_idx": 3}, {"type": "text", "text": "Although PPO is an on-policy algorithm, the use of off-policy importance ratio correction (i.e., the action probability ratios between two policies) allows us to use states and actions generated by another policy. This enables us to train $\\pi$ using data from $\\pi_{H}$ and vice versa. Both policies $\\pi$ and $\\pi_{H}$ are trained using the same data but with different reward functions. Note that collecting trajectories from both policies does not require more data than the standard PPO implementation. We collect half the trajectories with each policy, $\\pi$ and $\\pi_{H}$ , for a total of $B$ trajectories (see Algorithm 1). Then, we update both $\\pi$ and $\\pi_{H}$ using all $B$ trajectories. ", "page_idx": 3}, {"type": "text", "text": "Optimizing the Lagrangian multiplier $\\alpha$ : To update the Lagrangian multiplier $\\alpha$ , we need to compute the gradient in Equation 6, which corresponds to the performance gain of the enhanced policy $\\pi$ over the heuristic policy $\\pi_{H}$ on the task objective $J$ . Utilizing the performance difference lemma [20, 16], we relate this improvement to the expected advantages over trajectories sampled by the enhanced policy $\\pi$ as $J(\\pi)-J(\\pi_{H})=\\mathbb{E}_{\\pi}\\left[A^{\\pi_{H}}r(s_{t},a_{t})\\right]$ . However, this approach only utilizes half of the trajectories at each iteration since it exclusively relies on trajectories from the enhanced policy $\\pi$ . To leverage trajectories from both policies, we also consider the performance gain in the reverse direction as $-(J(\\pi_{H})-J(\\pi))=-\\mathbb{E}_{\\pi_{H}}\\left[A^{\\pi}r(s_{t},a_{t})\\right]$ . Consequently, we can estimate the gradient of $\\alpha$ using trajectories from both policies, as illustrated below: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\alpha}\\mathcal{L}(\\pi,\\alpha)=J(\\pi)-J(\\pi_{H})=\\mathbb{E}_{\\pi}\\left[A_{r}^{\\pi_{H}}(s_{t},a_{t})\\right]}\\\\ &{\\qquad\\qquad\\qquad=-(J(\\pi_{H})-J(\\pi))=-\\mathbb{E}_{\\pi_{H}}\\left[A_{r}^{\\pi}(s_{t},a_{t})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "At each iteration $i$ , we estimate the gradient of $\\alpha$ using the advantage $A_{r}^{\\pi_{H}}\\left(s_{t},a_{t}\\right)$ and $A_{r}^{\\pi}\\!\\left(s_{t},a_{t}\\right)$ on the trajectories sampled from both $\\pi^{i}$ and $\\pi_{H}^{i}$ , and update $\\alpha$ with stochastic gradient descent as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\alpha\\gets\\alpha-\\frac{\\eta}{2}\\left(\\mathbb{E}_{\\tau\\sim\\pi^{i}}\\left[A_{r}^{\\pi_{H}^{i}}(s_{t},a_{t})\\right]-\\mathbb{E}_{\\tau\\sim\\pi_{H}^{i}}\\left[A_{r}^{\\pi^{i}}(s_{t},a_{t})\\right]\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\eta\\,\\in\\,\\mathbb{R}^{+}$ is the step size. The expected advantage in Equation 11 are estimated using the generalized advantage estimator (GAE) [21]. ", "page_idx": 4}, {"type": "table", "img_path": "vBGMbFgvsX/tmp/78167a9dbcf6dc752bc2294278be39d6ac95a3ba0a7a211b02fe8db3ec5f5da7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.3 Connection to Extrinsic-Intrinsic Policy Optimization [22] ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Closely related to our HEPO framework, Chen et al. [22] proposes Extrinsic-Intrinsic Policy Optimization (EIPO), which trains a policy to maximize both task rewards and exploration bonuses [23] subject to the constraint that the learned policy $\\pi$ must outperform the task policy $\\pi_{J}$ trained solely on task rewards. HEPO and EIPO differ in their objective functions and implementation of the constrained optimization problem. Additional information can be found in the Appendix, covering the objective formulation (Appendix A.1), implementation tricks (Appendix A.2), and detailed pseudocode (Appendix A.3). ", "page_idx": 4}, {"type": "text", "text": "Exploration bonuses [23] can be viewed as heuristic rewards. The main difference between HEPO and EIPO\u2019s optimization objectives lies in constraint design. Both frameworks require the learned policy $\\pi$ to outperform a reference policy $\\pi_{\\mathrm{ref}}$ (i.e., $J(\\pi)\\bar{\\bf\\Xi}\\geq J(\\pi_{\\mathrm{ref}}))$ but use a different reference policy. EIPO uses the task policy $\\pi_{J}$ as the reference policy $\\pi_{\\mathrm{ref}}$ because they aim for asymptotic optimality in task rewards. If the constraint is satisfied with $\\pi_{J}$ being the optimal policy for task rewards, the learned policy $\\pi$ will also be optimal for task rewards. In contrast, HEPO uses the heuristic policy $\\pi_{H}$ trained solely on heuristic rewards since HEPO aims to improve upon it. ", "page_idx": 4}, {"type": "text", "text": "HEPO simplifies the implementation. Both HEPO and EIPO train two policies with shared data, but EIPO alternates the policy used for trajectory collection each iteration and has a complex switching rule, which introduces more hyperparameters. HEPO collects trajectories using both policies together at each iteration, simplifying implementation and avoiding extra hyperparameters. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We evaluate whether HEPO enhances the performance of RL algorithms in maximizing task rewards while training with heuristic rewards. We conduct experiments on 9 tasks from IsaacGym (ISAAC) [19] and 20 tasks from the Bidexterous Manipulation (BI-DEX) benchmark [24]. These tasks rely on heavily engineered reward functions for training RL algorithms. Each task has a task reward function $r$ that defines the task objective $J$ to be maximized, and a heuristic reward function $h$ that defines the heuristic objective $H$ , provided in the benchmarks to facilitate the optimization of task objectives $J$ . We implement HEPO based on PPO [15] and compare it with the following baselines: ", "page_idx": 4}, {"type": "text", "text": "\u2022 H-only (heuristic only): This is the standard PPO baseline provided in ISAAC. The policy is trained solely using the heuristic reward: $\\operatorname*{max}_{\\pi}H(\\pi)$ . The heuristic reward functions in ISAAC and BI-DEX are designed to help RL algorithms maximize the task objective $J$ . This baseline is crucial to determine if an algorithm can surpass a policy trained with highly engineered heuristic rewards. ", "page_idx": 4}, {"type": "text", "text": "\u2022 J-only (task only): The policy is trained using only the task reward: $\\operatorname*{max}_{\\pi}{J(\\pi)}$ . This baseline demonstrates the performance achievable without heuristics. Ideally, algorithms that incorporate heuristics should outperform this baseline. ", "page_idx": 4}, {"type": "text", "text": "\u2022 $\\mathbf{J}\\mathbf{+H}$ (mixture of task and heuristic): The policy is trained using a mixture of task and heuristic rewards: $\\operatorname*{max}_{\\pi}J(\\pi)+\\lambda H(\\pi)$ , with $\\lambda$ balancing the two rewards. As [22] shows, proper tuning of $\\lambda$ can enhance task performance by balancing both training objectives.   \n\u2022 Potential-based Reward Shaping (PBRS) [2]: The policy is trained to maximize $\\mathbb{E}_{\\pi}[\\sum_{t=0}^{\\infty}\\gamma^{t}r_{t}+$ $\\gamma h_{t+1}-h_{t}]$ , where $r_{t}$ and $h_{t}$ are the task and heuristic rewards at timestep $t$ . PBRS guarantees that the optimal policy is invariant to the task reward function. We include it as a baseline to examine if these theoretical guarantees hold in practice.   \n\u2022 HuRL [10]: The policy is trained to maximize $\\begin{array}{r}{\\mathbb{E}_{\\pi}[\\sum_{t=0}^{\\infty}\\gamma^{t}r_{t}+(1-\\beta_{i})\\gamma h_{t+1}]}\\end{array}$ , where $\\beta_{i}$ is a coefficient updated at each iteration to balance heuristic rewards during different training stages. The scheduling mechanism is detailed in [10] and our source code provided in the Supplementary Material.   \n\u2022 EIPO [22]: The policy is trained using the constrained objective: $\\operatorname*{max}_{\\pi}{J(\\pi)}~+$ $H(\\pi)$ s.t. $J(\\pi)~\\ge~J(\\pi_{J})$ , where $\\pi_{J}$ is the policy trained with task rewards only. EIPO is similar to HEPO but differs in formulation and implementation, as detailed in Section 4.3. ", "page_idx": 5}, {"type": "text", "text": "Each method is trained for 5 random seeds and implemented based on the open-sourced implementation [25], where the detailed training hyperparameters can be found in Appendix A.4. ", "page_idx": 5}, {"type": "text", "text": "Metrics: Based on the task success criteria in ISAAC and BI-DEX, we consider two types of task reward functions $r$ : (i) Progressing (for locomotion or helicopter robots) and (ii) Goal-reaching (for manipulation). In progressing tasks, robots aim to maximize their traveling distance or velocity from an initial point to a destination. Thus, movement progress is defined as the task reward. In goal-reaching tasks, robots aim to complete assigned goals by reaching specific goal states. Here, task rewards are binary, with a value of 1 indicating successful attainment of the goal and 0 otherwise. Detailed descriptions of our task objectives and total reward definitions are provided in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "4.1 Benchmark results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Setup: We aim to determine if HEPO achieves higher task returns and improves upon the policy trained with only heuristic rewards (H-only) in the majority of tasks. In this experiment, we use the heuristic reward functions from the ISAAC and BI-DEX benchmarks. To measure performance improvement over the heuristic policy, we normalize the return of each algorithm $X$ using the formula $(\\bar{J_{X}}-\\bar{J_{\\mathrm{random}}})/(\\bar{J_{\\mathrm{H-only}}}-\\bar{J_{\\mathrm{random}}})$ , where $J_{X}$ , $J_{\\mathrm{H-only}}$ , and $J_{\\mathrm{random}}$ denote the task returns of algorithm $X$ , the heuristic policy, and the random policy, respectively. In Figure 1, we present the interquartile mean (IQM) of the normalized return and the probability of improvement for each method across 29 tasks, following [26]. IQM, also known as the $25\\%$ trimmed mean, is a robust estimate against outliers. It discards the bottom and top $25\\%$ of runs and calculates the mean score of the remaining $50\\%$ . The probability of improvement measures whether an algorithm performs better than another, regardless of the margin of improvement. Both approaches prevent outliers from dominating the performance estimate. ", "page_idx": 5}, {"type": "text", "text": "Results: The results in Figure 1 indicate that policies trained with task rewards only (J-only) generally perform worse than those trained with heuristics, both in terms of IQM of normalized return and probability of improvement. PBRS does not improve upon J-only, demonstrating that the optimal policy invariance guarantee rarely holds in practice. Both EIPO and HuRL outperform J-only but do not surpass H-only, demonstrating that neither approach can improve upon the heuristic policy. Policies trained with both task and heuristic rewards $\\scriptstyle(\\mathrm{J}+\\mathrm{H})$ perform slightly worse than those trained with heuristics only (H-only), possibly because the weight coefficient balancing both rewards is too task-sensitive to work across all tasks. HEPO, however, outperforms all other methods in both IQM of normalized returns and shows a probability of improvement over the heuristic policy greater than $50\\%$ , indicating statistically significant improvements as suggested by Agarwal et al. [26]. Complete learning curves are presented in the Appendix B.1. Additional results on various benchmarks and RL algorithms are provided in Appendix B.3, demonstrating that HEPO is effective in hard-exploration tasks using exploration bonuses [23] and with RL algorithms beyond PPO. ", "page_idx": 5}, {"type": "text", "text": "4.2 Can HEPO be robust to reward functions designed in the wild? ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Setup: We envision to develop an RL algorithm that can effectively utilize heuristic reward functions, thereby reducing the time costs associated with reward design. We simulate reward design scenarios ", "page_idx": 5}, {"type": "image", "img_path": "vBGMbFgvsX/tmp/89d0f3caa41f8454b273e54b676fca40710225caf513a9cbf7a34da2edac14c4.jpg", "img_caption": ["(a) IQM over ISAAC $^+$ BI-DEX "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "vBGMbFgvsX/tmp/84040bab476125496ebdf10f3055d95e81ab382b60dd89ffbefa477d14811d5c.jpg", "img_caption": ["(b) Probability of Improvement over ISAAC $^+$ BI-DEX "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 1: (a) The vertical axis represents the interquartile mean (IQM) [26] of normalized task return across 29 tasks. HEPO outperforms the policy trained solely with a heavily engineered heuristic reward function (H-only) and other methods, demonstrating that HEPO makes better use of heuristic rewards for learning. (b) The horizontal axis shows the probability that algorithm X outperforms the policy trained solely with heuristic rewards (H-only). HEPO achieves a $62\\%$ probability of improvement over the heuristic policy on average, with the lower bound of the confidence interval above $50\\%$ , indicating statistically significant improvements over the heuristic policy. ", "page_idx": 6}, {"type": "text", "text": "and evaluate the algorithm\u2019s performance when trained with heuristic reward functions created under real-world conditions. Unlike the highly engineered reward functions in ISAAC, participants were asked to design their heuristic reward functions within a short time frame. This approach assesses the algorithm\u2019s effectiveness when trained with less refined heuristic reward functions. Participants were asked to iterate on their reward design by writing the reward function, training a policy with a given RL algorithm, reviewing the videos and learning curves, refining the reward, and repeating the process. We selected the FrankaCabinet task for this study because its original heuristic reward function is heavily engineered and consisting of many terms. The task of FrankaCabinet is training a robot arm to open a cabinet. We recruited twelve graduate students with varying levels of proficiency in machine learning and robotics and divided them into two groups. One group used HEPO to iterate on heuristic rewards, while the other group used PPO. This approach ensures that the designed heuristic reward functions are not specialized for one algorithm and ineffective for another. Each participant was instructed to edit the heuristic reward function to help RL algorithms maximize the task return. We used the same task reward metric as in Section 4.1. We then used the final versions of their heuristic reward functions to train HEPO and PPO, and reported the normalized return of the learned policies. Note that we adhered to the normalization scheme outlined in Section 4.1, based on the performance of the policy trained with the original heuristic reward function. This approach allows us to observe any performance drop when training policies with less engineered heuristic reward functions. ", "page_idx": 6}, {"type": "text", "text": "Quantitative results: Figure 2 shows that HEPO achieves significantly higher (lower confidence bound above the baselines\u2019 upper confidence bound) average normalized returns than PPO trained only on heuristic rewards (PPO (Honly)) in 9 out of 12 heuristic reward functions. Additionally, Table 1 indicates that across all heuristic functions, HEPO achieves a higher interquartile mean (IQM) of normalized returns and has a statistically significant probability of outperforming PPO (H-only) with lower confidence bound greater than 0.5. This suggests that even when trained with poorly designed heuristic reward functions, HEPO performs better than PPO (H-only). Notably, PPO (H-only) that is trained with $H2$ , $H5$ , and $H6$ achieves normal", "page_idx": 6}, {"type": "table", "img_path": "vBGMbFgvsX/tmp/0a36d81cf7d78a22b36b1121b6669c5341ecafc70bc97fa4da602bf673a7bdac.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 1: IQM and probability of improvement (PI) over H-only ( $\\mathrm{P}(\\mathrm{X}>\\mathrm{H}$ -only)) with $95\\%$ confidence intervals across 12 heuristic reward functions (Section 4.2). H-only uses PPO with heuristic rewards. HEPO achieves higher normalized returns and a statistically significant PI greater than 0.5, indicating it significantly outperforms H-only. ", "page_idx": 6}, {"type": "image", "img_path": "vBGMbFgvsX/tmp/226cd9acb99d0c0c894ce8d42cc8f596e0fb7c8ce7e070d595daaf51cc11d989.jpg", "img_caption": ["Figure 2: Normalized task return of PPO (H-only) and HEPO that are trained with heuristic reward function $H1$ to $H12$ designed by human subjects in the real world reward design condition. HEPO achieves higher task return than PPO (H-only) in 9 out of 12 tasks. This shows HEPO is robust to possibly ill-designed heuristic reward functions and can leverage them to improve performance. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "ized returns below 1, while HEPO achieves returns greater than or close to 1. Since returns are normalized using the performance of the PPO policy trained with the well-designed heuristic reward function in ISAAC, a return below 1.0 indicates a performance drop for PPO (H-only) when using potentially ill-designed heuristic rewards. In contrast, HEPO can improve upon policies trained with carefully engineered heuristic reward functions, even when trained with possibly ill-designed heuristic reward functions. ", "page_idx": 7}, {"type": "text", "text": "Qualitative observation: We aim to understand why PPO\u2019s performance declines when trained with heuristic reward functions $H2$ , $H5$ , and $H6$ . These functions are similar to the original heuristic reward in FrankaCabinet, but with different weights for each term. For example, in $H5$ , the weight of action penalty is 1, whereas in the original heuristic reward function it is 7.5. This suggests that HEPO might handle poorly scaled heuristic reward terms better than PPO, which is sensitive to these weights. The heuristic reward functions $H12$ and $H9$ had an incorrect sign for the distance component, which caused the policy to be rewarded for moving away from the cabinet instead of toward it, making the learning task more challenging. ", "page_idx": 7}, {"type": "text", "text": "4.3 Ablation Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Expanding on the discussion of relation to relevant work EIPO [22] in Section 3.3, our goal is to examine the implementation choices of HEPO and illustrate the efficacy of each modification in this section. HEPO differs from EIPO primarily in two aspects: (1) the selection of a reference policy $\\pi_{\\mathrm{ref}}$ in the constraint $J(\\pi)\\geq J(\\pi_{\\mathrm{ref}})$ , and (2) the strategy for utilizing policies to gather trajectories. Both studies are conducted on standard locomotion and manipulation tasks, such as Ant, FrankaCabinet, and AllegroHand. In addition, we provide further studies on the sensitivity to hyperparameters in Appendix B.2. ", "page_idx": 7}, {"type": "text", "text": "Selection of reference policy in constraint: HEPO and EIPO both enforce a performance improvement constraint $J(\\pi)\\geq J(\\pi_{\\mathrm{ref}})$ during training. HEPO uses a heuristic policy $\\pi_{H}$ as the reference $\\left[\\pi_{\\mathrm{ref}}\\,=\\mathrm{H}\\right.$ -only), while EIPO uses a task-only policy $\\pi_{\\mathrm{ref}}=\\mathbf{J}$ -only). However, relying solely on policies trained with task rewards as references may not suffice for complex robotic tasks, as they often perform much worse than those trained with heuristic rewards. We compared the performance of HEPO with different reference policies in Figure 3a. The result shows that setting $\\pi_{\\mathrm{ref}}=\\mathbf{J}$ -only (EIPO) improves the performance over the task-only policy $J.$ -only while notably degrading performance, sometimes even worse than $H$ -only, suggesting it\u2019s insufficient for surpassing the heuristic policy. ", "page_idx": 7}, {"type": "text", "text": "Strategy of collecting trajectories: We use both the enhanced policy $\\pi$ and the heuristic policy $\\pi_{H}$ simultaneously to sample half of the environment\u2019s trajectories (referred to as Joint). Conversely, EIPO switches between $\\pi$ and $\\pi_{H}$ using a specified mechanism, where only one selected policy samples trajectories for updating both $\\pi$ and $\\pi_{H}$ within the same episode (referred to as Alternating). This study compares the performance of these two trajectory rollout methods. We modify HEPO to gather trajectories using the Alternating strategy and present the results in Figure 3b. The findings indicate that Alternating results in a performance drop during mid-training and fails to match the performance of HEPO(Joint). We hypothesize that this occurs because the batch of trajectories collected solely by one policy deviates significantly from those that another policy can generate (i.e., high off-policy error), leading to less effective PPO policy updates. In contrast, Joint samples trajectories using both policies, preventing the collected trajectories from deviating too much from each other. ", "page_idx": 7}, {"type": "image", "img_path": "vBGMbFgvsX/tmp/9be6fef09f9548b4d6832fbc28ddfb4cb37740c5c4c97c4b5ceaafc4c4067f95.jpg", "img_caption": ["Figure 3: (a) We show that using the policies trained with heuristic rewards (J-only) is better than using the policies trained with task rewards (J-only) when training HEPO. (b) HEPO(Joint) that collects trajectories using both policies leads to better performance than HEPO(Alternating) that alternates between two policies to collect trajectories. See Section 4.3 for details ", "(b) Comparison of trajectory collecting strategies "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5 Related Works ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Reward shaping: Reward shaping has been a significant area, including potential-based reward shaping (PBRS) [2, 27], bilevel optimization approaches [28\u201330] on reward model learning, and heuristic-guided methods (HuRL) [10] that schedule heuristic rewards. Our method differs as it is a policy optimization method agnostic to the heuristic reward function and can be applied to those shaped or learned rewards. ", "page_idx": 8}, {"type": "text", "text": "Constrained policy optimization: Recent work like Extrinsic-Intrinsic Policy Optimization (EIPO) [22] proposes constrained optimization by tuning exploration bonuses to prevent exploiting them at the cost of task rewards. Extensions [31] balance imitating a teacher model and reward maximization. Our work differs in balancing human-designed heuristic rewards and task rewards, improving upon policies trained with engineered heuristic rewards. We also propose implementation enhancements over EIPO [22] (Section 4.3). ", "page_idx": 8}, {"type": "text", "text": "6 Discussion & Limitation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "HEPO for RL practitioners: In this paper, we showed that HEPO is robust to the possibly illdesigned heuristic reward function in Section 4.2 and also exhibit high-probability improvement over PPO when training with heavily engineered heuristic rewards in robotic tasks in Section 4.1. Moving forward, when users need to integrate heuristic reward functions into RL algorithms, HEPO can potentially be a useful tool to reduce users\u2019 time on designing rewards since it can improve performance even with under-engineered heuristic rewards. ", "page_idx": 8}, {"type": "text", "text": "Limitations: While HEPO shows high-probability performance improvement over heuristic policies trained with well-designed heuristic reward, one limitation is that HEPO does not have a guarantee to converge to the optimal policy theoretically. One future work can be incorporating the insight in recent theoretical advances on reward engineering [3] to make a convergence guarantee. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank members of the Improbable AI Lab for helpful discussions and feedback. We are grateful to MIT Supercloud and the Lincoln Laboratory Supercomputing Center for providing HPC resources. This research was supported in part by Hyundai Motor Company, Quanta Computer Inc., an AWS MLRA research grant, ARO MURI under Grant Number W911NF-23-1-0277, DARPA Machine Common Sense Program, ARO MURI under Grant Number W911NF-21-1-0328, and ONR MURI under Grant Number N00014-22-1-2740. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the United States Air Force or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes, notwithstanding any copyright notation herein. ", "page_idx": 9}, {"type": "text", "text": "Author Contributions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "\u2022 Chi-Chang Lee: Co-led the project, led the implementation of the proposed algorithms and baselines, and conducted the experiments.   \n\u2022 Zhang-Wei Hong: Co-led the project, led the writing of the paper, scaled up the experiment infrastructure, and conducted the experiments.   \n\u2022 Pulkit Agrawal: Played a key role in overseeing the project, editing the manuscript, and the presentation of the work. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. 2018.   \n[2] Andrew $\\textrm{Y N g}$ , Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In Icml, volume 99, pages 278\u2013287, 1999.   \n[3] Abhishek Gupta, Aldo Pacchiano, Yuexiang Zhai, Sham Kakade, and Sergey Levine. Unpacking reward shaping: Understanding the benefits of reward engineering on sample complexity. Advances in Neural Information Processing Systems, 35:15281\u201315295, 2022.   \n[4] Tao Chen, Jie Xu, and Pulkit Agrawal. A system for general in-hand object re-orientation. Conference on Robot Learning, 2021.   \n[5] Gabriel B Margolis, Ge Yang, Kartik Paigwar, Tao Chen, and Pulkit Agrawal. Rapid locomotion via reinforcement learning. Robotics: Science and Systems, 2022.   \n[6] Gabriel B Margolis and Pulkit Agrawal. Walk these ways: Tuning robot control for generalization with multiplicity of behavior. In Conference on Robot Learning, pages 22\u201331. PMLR, 2023.   \n[7] Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra Malik. RMA: rapid motor adaptation for legged robots. In Robotics: Science and Systems XVII, Virtual Event, July 12-16, 2021, 2021.   \n[8] Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. Learning quadrupedal locomotion over challenging terrain. Science robotics, 5(47):eabc5986, 2020.   \n[9] Sam Michael Devlin and Daniel Kudenko. Dynamic potential-based reward shaping. In 11th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2012), pages 433\u2013440. IFAAMAS, 2012.   \n[10] Ching-An Cheng, Andrey Kolobov, and Adith Swaminathan. Heuristic-guided reinforcement learning. Advances in Neural Information Processing Systems, 34:13550\u201313563, 2021.   \n[11] Sam Devlin, Logan Yliniemi, Daniel Kudenko, and Kagan Tumer. Potential-based difference rewards for multiagent reinforcement learning. In Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems, pages 165\u2013172, 2014.   \n[12] Adam Eck, Leen-Kiat Soh, Sam Devlin, and Daniel Kudenko. Potential-based reward shaping for finite horizon online pomdp planning. Autonomous Agents and Multi-Agent Systems, 30: 403\u2013445, 2016.   \n[13] Babak Badnava, Mona Esmaeili, Nasser Mozayani, and Payman Zarkesh-Ha. A new potentialbased reward shaping for reinforcement learning agent. In 2023 IEEE 13th Annual Computing and Communication Workshop and Conference (CCWC), pages 01\u201306. IEEE, 2023.   \n[14] Grant C Forbes and David L Roberts. Potential-based reward shaping for intrinsic motivation (student abstract). In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 23488\u201323489, 2024.   \n[15] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n[16] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 1889\u20131897, 2015.   \n[17] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290, 2018.   \n[18] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 2015.   \n[19] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, and Gavriel State. Isaac gym: High performance gpu-based physics simulation for robot learning, 2021.   \n[20] Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In In Proc. 19th International Conference on Machine Learning. Citeseer, 2002.   \n[21] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. International Conference of Representation Learning, 2015.   \n[22] Eric Chen, Zhang-Wei Hong\\*, Joni Pajarinen, and Pulkit ( $^*$ equal contribution) Agrawal. Redeeming intrinsic rewards via constrained optimization. Advances in Neural Information Processing Systems, 35:4996\u20135008, 2022.   \n[23] Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=H1lJJnR5Ym.   \n[24] Yuanpei Chen, Yaodong Yang, Tianhao Wu, Shengjie Wang, Xidong Feng, Jiechuan Jiang, Zongqing Lu, Stephen Marcus McAleer, Hao Dong, and Song-Chun Zhu. Towards human-level bimanual dexterous manipulation with reinforcement learning. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022.   \n[25] Denys Makoviichuk and Viktor Makoviychuk. rl-games: A high-performance framework for reinforcement learning. https://github.com/Denys88/rl_games, May 2021.   \n[26] Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare. Deep reinforcement learning at the edge of the statistical precipice. Advances in Neural Information Processing Systems, 34, 2021.   \n[27] Sam Devlin and Daniel Kudenko. Dynamic potential-based reward shaping. In Adaptive Agents and Multi-Agent Systems, 2012.   \n[28] Yujing Hu, Weixun Wang, Hangtian Jia, Yixiang Wang, Yingfeng Chen, Jianye Hao, Feng Wu, and Changjie Fan. Learning to utilize shaping rewards: A new approach of reward shaping. Advances in Neural Information Processing Systems, 33:15931\u201315941, 2020.   \n[29] Dhawal Gupta, Yash Chandak, Scott Jordan, Philip S Thomas, and Bruno C da Silva. Behavior alignment via reward function optimization. Advances in Neural Information Processing Systems, 36, 2024.   \n[30] Zeyu Zheng, Junhyuk Oh, and Satinder Singh. On learning intrinsic rewards for policy gradient methods. Advances in Neural Information Processing Systems, 31, 2018.   \n[31] Idan Shenfeld, Zhang-Wei Hong, Aviv Tamar, and Pulkit Agrawal. TGRL: An algorithm for teacher guided reinforcement learning. In Proceedings of the 40th International Conference on Machine Learning, 2023.   \n[32] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations, 2014. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Implementation Details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Full Derivation ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We will detailedly describe the update of the enhanced policy ( $\\pi$ in Equation 7) and the heuristic policy ( $\\pi_{H}$ in Equation 8) at each iteration. ", "page_idx": 12}, {"type": "text", "text": "A.1.1 Notations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "$\\begin{array}{r l}{\\bullet}&{V_{r}^{\\pi}(s_{t}):=\\mathbb{E}_{(s_{t},a_{t})\\sim\\pi}\\Big[\\sum_{t=0}^{\\infty}\\gamma^{t}r(s_{t},a_{t})|s_{0}=s_{t}\\Big]}\\\\ &{\\bullet\\;V_{h}^{\\pi}(s_{t}):=\\mathbb{E}_{(s_{t},a_{t})\\sim\\pi}\\Big[\\sum_{t=0}^{\\infty}\\gamma^{t}h(s_{t},a_{t})|s_{0}=s_{t}\\Big]}\\\\ &{\\bullet\\;A_{r}^{\\pi}(s_{t},a_{t}):=r(s_{t},a_{t})+V_{r}^{\\pi}(s_{t+1})-V_{r}^{\\pi}(s_{t})}\\\\ &{\\bullet\\;A_{h}^{\\pi}(s_{t},a_{t}):=h(s_{t},a_{t})+V_{h}^{\\pi}(s_{t+1})-V_{h}^{\\pi}(s_{t})}\\end{array}$ \u2022 $B_{\\mathrm{HEPO}}$ : the buffer to store samples collected by $\\pi^{i}$ \u2022 BH: the buffer to store samples collected \u03c0iH. ", "page_idx": 12}, {"type": "text", "text": "A.1.2 Enhanced Policy $\\pi$ Update ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Given a $\\alpha$ value, $\\pi^{i+1}$ is derived using the arguments of the maxima in Equation 4, which can be re-written as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi^{i+1}=\\underset{\\pi}{\\arg\\operatorname*{max}}\\left\\{J(\\pi)+H(\\pi)-\\alpha\\Big(J(\\pi)-J(\\pi_{H}^{i})\\Big)\\right\\}}\\\\ &{\\qquad=\\underset{\\pi}{\\arg\\operatorname*{max}}\\left\\{(1+\\alpha)J(\\pi)+H(\\pi)\\right\\}}\\\\ &{\\qquad=\\underset{\\pi}{\\arg\\operatorname*{max}}\\left\\{\\left((1+\\alpha)J(\\pi)+H(\\pi)\\right)-\\frac{1}{2}\\Big((1+\\alpha)J(\\pi^{i})+H(\\pi^{i})\\Big)\\right.}\\\\ &{\\qquad\\qquad\\qquad-\\frac{1}{2}\\Big((1+\\alpha)J(\\pi_{H}^{i})+H(\\pi_{H}^{i})\\Big)\\Big\\}}\\\\ &{\\qquad=\\underset{\\pi}{\\arg\\operatorname*{max}}\\left\\{\\frac{1}{2}E_{\\pi}\\Big[(1+\\alpha)A_{\\pi}^{\\pi^{i}}(s_{t},\\alpha_{t})+A_{\\Lambda}^{\\pi^{i}}(s_{t},\\alpha_{t})\\Big]\\right.}\\\\ &{\\qquad\\qquad\\qquad+\\frac{1}{2}E_{\\pi}\\Big[(1+\\alpha)A_{\\pi}^{\\pi^{i}}(s_{t},\\alpha_{t})+A_{\\Lambda}^{\\pi^{i}}(s_{t},\\alpha_{t})\\Big]}\\\\ &{\\qquad=\\underset{\\pi}{\\arg\\operatorname*{max}}\\left\\{\\frac{1}{2}E_{\\pi}\\Big[U_{\\pi}^{\\pi^{i}}(s_{t},\\alpha_{t})\\Big]+\\frac{1}{2}E_{\\pi}\\Big[U_{\\pi}^{\\pi^{i}}(s_{t},\\alpha_{t})\\Big]\\right\\}}\\\\ &{\\qquad=\\underset{\\pi}{\\arg\\operatorname*{max}}\\left\\{\\mathbb{E}_{\\pi}\\Big[U_{\\pi}^{\\pi^{i}}(s_{t},\\alpha_{t})\\Big]+\\mathbb{E}_{\\pi}\\Big[U_{\\pi}^{\\pi^{i}}(s_{t},\\alpha_{t})\\Big]\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $U_{\\alpha}^{\\pi^{i}}$ and $U_{\\alpha}^{\\pi_{H}^{i}}$ are defined as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{U_{\\alpha}^{\\pi^{i}}}(s_{t},a_{t}):=(1+\\alpha){A_{r}^{\\pi^{i}}}(s_{t},a_{t})+{A_{h}^{\\pi^{i}}}(s_{t},a_{t})}\\\\ &{{U_{\\alpha}^{\\pi_{H}^{i}}}(s_{t},a_{t}):=(1+\\alpha){A_{r}^{\\pi_{H}^{i}}}(s_{t},a_{t})+{A_{h}^{\\pi_{H}^{i}}}(s_{t},a_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "To efficiently achieve the update process in Equation 12, we aim to utilize previously collected trajectories for optimization, outlined in Equation 7. Here, we refer to [15], using those previously collected trajectories to form a lower bound surrogate objectives, $\\hat{J}_{\\alpha}^{\\pi^{i}}(\\pi)$ and $\\hat{J}_{\\alpha}^{\\pi_{H}^{i}}(\\pi)$ , as alternatives ", "page_idx": 12}, {"type": "text", "text": "of $\\mathbb{E}_{\\pi}[U_{\\alpha}^{\\pi^{i}}(s_{t},a_{t})]$ and $\\mathbb{E}_{\\pi}[U_{\\alpha}^{\\pi_{H}^{i}}(s_{t},a_{t})]$ to derive $\\pi^{i+1}$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\hat{J}_{\\mathrm{HEPO}}^{\\bar{\\alpha}^{i}}(\\pi):=\\frac{1}{|{\\cal B}_{\\mathrm{HEPO}}|}\\displaystyle\\sum_{(s_{t},a_{t})\\in{\\cal B}_{\\mathrm{HEP}}}\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\operatorname*{min}\\left\\{\\frac{\\pi\\left(a_{t}|s_{t}\\right)}{\\pi^{i}\\left(a_{t}|s_{t}\\right)}U_{\\alpha}^{\\pi^{i}}(s_{t},a_{t}),\\right.}\\\\ &{}&{\\displaystyle}&{\\mathrm{clip}\\left(\\frac{\\pi\\left(a_{t}|s_{t}\\right)}{\\pi^{i}\\left(a_{t}|s_{t}\\right)},1-\\epsilon,1+\\epsilon\\right)U_{\\alpha}^{\\pi^{i}}(s_{t},a_{t})\\right\\}\\right]\\quad\\quad}\\\\ &{}&{\\displaystyle\\hat{J}_{\\mathrm{HEPO}}^{\\bar{\\alpha}_{H}^{i}}(\\pi):=\\frac{1}{|{\\cal B}_{H}|}\\displaystyle\\sum_{(s_{t},a_{t})\\in{\\cal B}_{H}}\\left[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\operatorname*{min}\\left\\{\\frac{\\pi\\left(a_{t}|s_{t}\\right)}{\\pi_{H}^{i}\\left(a_{t}|s_{t}\\right)}U_{\\alpha}^{\\pi^{i}}(s_{t},a_{t}),\\right.\\right.}\\\\ &{}&{\\displaystyle\\left.\\displaystyle}&{\\displaystyle\\left.\\mathrm{clip}\\left(\\frac{\\pi\\left(a_{t}|s_{t}\\right)}{\\pi_{H}^{i}\\left(a_{t}|s_{t}\\right)},1-\\epsilon,1+\\epsilon\\right)U_{\\alpha}^{\\pi_{H}^{i}}(s_{t},a_{t})\\right\\}\\right],\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mathbb{E}_{\\pi}[U_{\\alpha}^{\\pi^{i}}(s_{t},a_{t})]\\geq\\hat{J}_{\\mathrm{HEPO}}^{\\pi^{i}}(\\pi)$ and $\\mathbb{E}_{\\pi}[U_{\\alpha}^{\\pi_{H}^{i}}(s_{t},a_{t})]\\geq\\hat{J}_{\\mathrm{HEPO}}^{\\pi_{H}^{i}}(\\pi)$ alwa hold; $\\epsilon\\in[0,1]$ denotes a threshold. Intuitively, this clipped objective (Eq. 14) penalizes the policy $\\pi$ that behaves differently from $\\pi^{i}$ or $\\pi_{H}^{i}$ because overly large or small the action probability ratios between two policies are clipped. ", "page_idx": 13}, {"type": "text", "text": "A.1.3 Heuristic Policy $\\pi_{H}$ Update ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "$\\pi_{H}^{i+1}$ is derived using the arguments of the maxima of $H(\\pi)$ , which can be re-written as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\pi_{H}^{i+1}=\\underset{\\pi}{\\arg\\operatorname*{max}}\\left\\{H(\\pi)\\right\\}}}\\\\ {{\\displaystyle{\\qquad=\\arg\\operatorname*{max}\\left\\{H(\\pi)-\\frac{1}{2}H(\\pi^{i})-\\frac{1}{2}H(\\pi_{H}^{i})\\right\\}}}}\\\\ {{\\displaystyle{\\qquad=\\arg\\operatorname*{max}\\left\\{\\frac{1}{2}\\mathbb{E}_{\\pi}\\left[A_{h}^{\\pi^{i}}(s_{t},a_{t})\\right]+\\frac{1}{2}\\mathbb{E}_{\\pi}\\left[A_{h}^{\\pi_{H}^{i}}(s_{t},a_{t})\\right]\\right\\}}}}\\\\ {{\\displaystyle{\\qquad=\\arg\\operatorname*{max}\\left\\{\\mathbb{E}_{\\pi}\\left[A_{h}^{\\pi^{i}}(s_{t},a_{t})\\right]+\\mathbb{E}_{\\pi}\\left[A_{h}^{\\pi_{H}^{i}}(s_{t},a_{t})\\right]\\right\\}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Similarly, we again rely on the approximation from [15] to derive a lower bound surrogate objective for both $\\mathbb{E}_{\\pi}[A_{h}^{\\pi^{i}}(s_{t},a_{t})]$ and $\\mathbb{E}_{\\pi}[A_{h}^{\\pi_{H}^{i}}(s_{t},a_{t})]$ as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\hat{H}^{\\pi^{i}}(\\pi):=\\frac{1}{|B_{\\mathrm{HEPO}}|}\\displaystyle\\sum_{(s_{t},a_{t})\\in B_{\\mathrm{HBP}}}\\Big[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\operatorname*{min}\\Big\\{\\frac{\\pi\\big(a_{t}|s_{t}\\big)}{\\pi^{i}(a_{t}|s_{t})}A_{h}^{\\pi^{i}}(s_{t},a_{t}),}\\\\ &{}&{\\qquad\\qquad\\qquad\\mathrm{clip}\\left(\\displaystyle\\frac{\\pi\\big(a_{t}|s_{t}\\big)}{\\pi^{i}(a_{t}|s_{t})},1-\\epsilon,1+\\epsilon\\right)A_{h}^{\\pi^{i}}(s_{t},a_{t})\\Big\\}\\Big],}\\\\ &{}&{\\hat{H}^{\\pi_{H}^{i}}(\\pi):=\\displaystyle\\frac{1}{|B_{H}|}\\sum_{(s_{t},a_{t})\\in B_{H}}\\Big[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\operatorname*{min}\\Big\\{\\displaystyle\\frac{\\pi\\big(a_{t}|s_{t}\\big)}{\\pi_{H}^{i}(a_{t}|s_{t})}A_{h}^{\\pi_{H}^{i}}(s_{t},a_{t}),}\\\\ &{}&{\\qquad\\qquad\\qquad\\mathrm{clip}\\left(\\displaystyle\\frac{\\pi\\big(a_{t}|s_{t}\\big)}{\\pi_{H}^{i}(a_{t}|s_{t})},1-\\epsilon,1+\\epsilon\\right)A_{h}^{\\pi_{H}^{i}}(s_{t},a_{t})\\Big\\}\\Big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mathbb{E}_{\\pi}[A_{h}^{\\pi^{i}}(s_{t},a_{t})]\\,\\ge\\,\\hat{H}^{\\pi^{i}}(\\pi)$ and $\\mathbb{E}_{\\pi}[A_{h}^{\\pi_{H}^{i}}(s_{t},a_{t})]\\,\\ge\\,\\hat{H}^{\\pi_{H}^{i}}(\\pi)$ always hold. Different from vanilla heuristic training, instead of solely collecting trajectories from $\\pi_{H}^{i}$ , we collect trajectories from both $\\pi$ and $\\pi_{H}$ to enrich sample efficiency. ", "page_idx": 13}, {"type": "text", "text": "A.2 Implementation Tricks ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.2.1 Sample Sharing for Value Function Update ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In practice, obtaining real value functions for training is not feasible. We estimate the value function using collected trajectories, but this approach tends to fail because the value function becomes biased toward the policy responsible for trajectory collection. ", "page_idx": 13}, {"type": "text", "text": "To prevent error information from the estimated value function interfering with the training procedure, we share the trajectory samples within the $B_{\\mathrm{HEPO}}$ and $B_{H}$ buffers to update our value functions: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{r}^{\\pi^{i+1}}\\gets\\operatorname{arg\\,min}\\Big\\{\\underset{\\left(s_{i},r(s_{i},a_{i}),s_{i+1}\\right)\\in B_{M\\times}(B_{R})}{\\sum}\\frac{\\left|r\\left(s_{i},a_{i}\\right)+\\gamma V_{r}^{\\pi^{i}}\\left(s_{i+1}\\right)-V\\left(s_{i}\\right)\\right|^{2}}{|B_{\\mathrm{tlg\\,Pol}}|+|B_{H}|}\\Big\\}}\\\\ &{V_{h}^{\\pi^{i+1}}\\gets\\operatorname{arg\\,min}\\Big\\{\\underset{\\left(s_{i},h_{i},a_{i}\\right)\\in B_{M\\times}(B_{R})}{\\sum}\\frac{\\left|h\\left(s_{i},a_{i}\\right)+\\gamma V_{h}^{\\pi^{i}}\\left(s_{i+1}\\right)-V\\left(s_{i}\\right)\\right|^{2}}{|B_{\\mathrm{tlg\\,Pol}}|+|B_{H}|}\\Big\\}}\\\\ &{V_{r}^{\\pi_{H}^{i+1}}\\gets\\operatorname{arg\\,min}\\Big\\{\\underset{\\left(s_{i},r(s_{i},a_{i}),s_{i+1}\\right)\\in B_{M\\times}(B_{R})}{\\sum}\\frac{\\left|r\\left(s_{i},a_{i}\\right)+\\gamma V_{r}^{\\pi_{H}^{i}}\\left(s_{i+1}\\right)-V\\left(s_{i}\\right)\\right|^{2}}{|B_{\\mathrm{HEP}}|+|B_{H}|}\\Big\\}}\\\\ &{V_{h}^{\\pi_{H}^{i+1}}\\gets\\operatorname{arg\\,min}\\Big\\{\\underset{\\left(s_{i},h_{i},a_{i}\\right)\\in B_{M\\times}(B_{R})}{\\sum}\\frac{\\left|h\\left(s_{i},a_{i}\\right)+\\gamma V_{h}^{\\pi_{H}^{i}}\\left(s_{i+1}\\right)-V\\left(s_{i}\\right)\\right|^{2}}{|B_{\\mathrm{HEP}}|+|B_{H}|}\\Big\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.2.2 Smoothing Lagrangian Multiplier $\\alpha$ Update ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The Lagrangian multiplier $\\alpha$ determines the desired constraint information during training. However, in practice the gradient $\\alpha$ tends to become explosive. To stabilize the $\\alpha$ update procedure, we accumulate previous gradients and adopt the Adam optimizer [32] as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle g(\\alpha)\\gets\\mathrm{med}\\Big\\{\\frac{1}{|B_{\\mathrm{HEPO}}|}\\sum_{(s_{t},a_{t})\\in B_{\\mathrm{HEPO}}}\\Big[A_{r}^{\\pi_{H}^{i}}(s_{t},a_{t})\\Big]-\\frac{1}{|B_{H}|}\\sum_{(s_{t},a_{t})\\in B_{H}}\\Big[A_{r}^{\\pi^{i}}(s_{t},a_{t})\\Big]\\Big\\}_{i-K}^{i}}\\\\ {\\alpha\\gets\\mathrm{AdamOpt}\\Big[g(\\alpha)\\Big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $K$ is the number of previous $K$ advantage expectation records that we take into account. To smooth the current $\\alpha$ gradient for each update, we calculate the median of the previous $K$ records. In our experiments, we assigned $K$ a value of 8. ", "page_idx": 14}, {"type": "text", "text": "A.3 Overall Workflow ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Algorithm 2 Detailed Heuristic-Enhanced Policy Optimization (HEPO)   \n1: Initialize policies $(\\pi^{1},\\pi_{H}^{1})$ and values $(V_{r}^{\\pi^{1}},V_{h}^{\\pi^{1}},V_{r}^{\\pi_{H}^{1}},V_{h}^{\\pi_{H}^{1}})$   \n2: for $i=1\\cdot\\cdot\\cdot$ do \u25b7i denotes iteration index   \n3: # ROLLOUT STAGE   \n4: Collect trajectory buffers $(B_{\\mathrm{HEPO}},B_{H})$ using $(\\pi^{i},\\pi_{H}^{i})$   \n5: Compute $\\left(A_{r}^{\\pi^{i}}(s_{t},a_{t}),A_{h}^{\\pi^{i}}(s_{t},a_{t})\\right)$ via GAE with $\\left(V_{r}^{\\pi^{i}},V_{h}^{\\pi^{i}}\\right)\\,\\forall(s_{t},a_{t})\\in B_{\\mathrm{HEPO}}$   \n6: Compute $\\left({\\cal A}_{r}^{\\pi_{H}^{i}}(s_{t},a_{t}),{\\cal A}_{h}^{\\pi_{H}^{i}}(s_{t},a_{t})\\right)$ via GAE with $\\left(V_{r}^{\\pi_{H}^{i}},V_{h}^{\\pi_{H}^{i}}\\right)\\forall(s_{t},a_{t})\\in B_{H}$   \n7: Compute $\\left(\\hat{J}_{\\mathrm{HEPO}}^{\\pi^{i}},\\hat{J}_{\\mathrm{HEPO}}^{\\pi_{H}^{i}}\\right)$ based on Equation 14   \n8: Compute $\\left(\\hat{H}^{\\pi^{i}},\\hat{H}^{\\pi_{H}^{i}}\\right)$ based on Equation 16   \n9:   \n10: # UPDATE STAGE   \n11: $\\begin{array}{r l}&{\\pi_{H}^{i+1}\\leftarrow\\arg\\operatorname*{max}_{\\pi}\\left\\{\\hat{J}_{\\mathrm{HEPO}}^{\\pi^{i}}(\\pi)+\\hat{J}_{\\mathrm{HEPO}}^{\\pi_{H}^{i}}(\\pi)\\right\\}}\\\\ &{\\pi_{H}^{i+1}\\leftarrow\\arg\\operatorname*{max}_{\\pi}\\left\\{\\hat{H}^{\\pi^{i}}(\\pi)+\\hat{H}^{\\pi_{H}^{i}}(\\pi)\\right\\}}\\end{array}$   \n12:   \n13: Update $(V_{r}^{\\pi^{i}},V_{h}^{\\pi^{i}},V_{r}^{\\pi_{H}^{i}},V_{h}^{\\pi_{H}^{i}})$ based on Equation 18   \n14: Update $\\alpha$ based on Equation 22   \n15: end for ", "page_idx": 14}, {"type": "text", "text": "A.4 Training details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Following the PPO framework [15], our experiments are based on a continuous action actor-critic algorithm implemented in rl_games [25], using Generalized Advantage Estimation (GAE) [21] to compute advantages for policy optimization. For PPO, we employed the same policy network and value network architecture, and the same hyperparameters used in IsaacGymEnvs [19]. We also include our source code in the supplementary material. In HEPO, we use two policies for optimization, with each policy maintaining the same model configurations as those used in PPO. Below we introduce HEPO-specific hyperparameters used in our experiments in Section 4.1. The hyperparameters for updating the Lagrangian multiplier $\\alpha$ in HEPO are listed as follows: ", "page_idx": 15}, {"type": "table", "img_path": "vBGMbFgvsX/tmp/e02e34bc948b55aec7e01ab7e94609a60eae515ec55d342dd33abf1cb1b8d958.jpg", "table_caption": ["Table 2: HEPO Hyperparameters "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "For baselines, we search for hyperparamters $\\lambda$ for $J+H$ in Ant, FrankaCabinet, and AllegroHand, as shown in Section B.2. We set $\\lambda=1$ for all the experiments because it shows better performance on the three chosen environments. For HuRL [10], we follow the scheduling setting provided in their paper. ", "page_idx": 15}, {"type": "text", "text": "B Supplementary Experimental Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 All Learning Curves on the task objective $J$ ", "page_idx": 16}, {"type": "text", "text": "We present all the learning curves in Figure 4. ", "page_idx": 16}, {"type": "image", "img_path": "vBGMbFgvsX/tmp/b179324c564b98f8d5de197bda7528cb13c1defeb1a4a98234772f9ef3a47245.jpg", "img_caption": ["Figure 4: All learning curves in Section 4.1 "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.2 Sensitivity to hyperparameters ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we aim to verify HEPO\u2019s sensitivity to two main types of hyperparameters: (1) the weight of the heuristic reward in optimization (denoted as $\\lambda$ ) and (2) the learning rate for updating $\\alpha$ . ", "page_idx": 16}, {"type": "image", "img_path": "vBGMbFgvsX/tmp/d1dbe34d0022ac1bf31b349f0ebee4ada3dd222d3aa582a9b831031cca32e1a9.jpg", "img_caption": ["Figure 6: Sensitivity to alpha learning rate "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Similar to Section 4.3, we conducted our experiments on the Ant, FrankaCabinet, and AllegroHand tasks. ", "page_idx": 17}, {"type": "text", "text": "B.2.1 Sensitivity to the $\\lambda$ Value ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Both HEPO and $\\mathbf{J}\\mathbf{+H}$ can set a scaling coefficient to weight the heuristic reward in optimization, such that the objective becomes $J(\\pi)+\\lambda H(\\pi)$ . This scaling coefficient can be used to balance both objectives. In this study, we compare HEPO and $\\mathbf{J}\\mathbf{+}\\mathbf{H}$ on their performance sensitivity to the choice of $\\lambda$ , exhaustively training both HEPO and $\\mathbf{J}\\mathbf{+H}$ with varying $\\lambda$ values. Note that though the formulation of HEPO does not depend on $\\lambda$ , one can still set a $\\lambda$ coefficient to scale the heuristic reward in HEPO. In our experiments, we did not optimize $\\lambda$ for HEPO but for the baselines trained with both rewards $\\mathrm{(J+H)}$ . In Figure 5, we found that $\\mathbf{J}\\mathbf{+}\\mathbf{H}$ is sensitive to $\\lambda$ in all selected tasks, while HEPO performs well across a wide range of $\\lambda$ values. This indicates that HEPO is robust to the choice of $\\lambda$ . ", "page_idx": 17}, {"type": "text", "text": "B.2.2 Sensitivity to the Learning Rate for Updating $\\alpha$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "HEPO\u2019s robustness relies on the $\\alpha$ update, as it reflects the necessary constraint information at each iteration. Similar to Appendix B.2.1, setting different initial values of $\\alpha$ is equivalent to using different $\\lambda$ values for our estimation, since both can be rewritten as the ratio between $H(\\pi)$ and $J(\\pi)$ . Both of these parameters indicate the necessary constraint information for conducting multi-objective optimization. In this study, we aim to verify whether HEPO can yield comparable improvement gaps under different initial values of $\\alpha$ , thus providing a more robust optimization procedure. ", "page_idx": 17}, {"type": "text", "text": "As shown in Figure 6, we observe that HEPO is also robust to the choice of $\\alpha$ \u2019s initial values, similar to the results in Figure 5. ", "page_idx": 17}, {"type": "text", "text": "B.3 Additional results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Generality of HEPO: We also demonstrated HEPO can be implemented over the other RL algorithms in addition to PPO. We integrated HEPO into HuRL\u2019s SAC codebase. Despite HuRL using tuned hyperparameters as reported in its paper [10], HEPO outperformed SAC and matched HuRL on the most challenging task in Figure 7b using the same hyperparameters from Section 4 of our manuscript, showing the generality of HEPO on different RL algorithms. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Better than EIPO on the most challenging task: Comparing HEPO and EIPO on the most challenging task, Montezuma\u2019s Revenge, reported in EIPO\u2019s paper [22] using RND exploration bonuses [23] (as used in the EIPO paper), Figure 7a shows that HEPO performs better than EIPO. Also, HEPO matched PPO trained with RND bonuses at convergence (2 billion frames) reported in [23] using only $20\\%$ of the training data, demonstrating drastically improved sample efficiency. ", "page_idx": 18}, {"type": "text", "text": "How quality of heuristic reward functions impact HEPO? We believe that Figure 2 reveals the relationship between HEPO\u2019s performance and the quality of the heuristic reward. The policy trained with only heuristic rewards (H-only) represents both the asymptotic performance of in HEPO and the quality of the heuristic itself. We found a positive correlation (Pearson coefficient of 0.9) between the average performances of H-only and HEPO in Figure 2 results, suggesting that better heuristics lead to improved HEPO performance. Figure $\\mathrm{7c}$ provides more details. ", "page_idx": 18}, {"type": "image", "img_path": "vBGMbFgvsX/tmp/a65f83c99fd80dc6d729584cca4179743ed8d9d101e922bb12e41952117720c7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 7: (a) Comparison of HEPO and EIPO [22] on the most challenging Atari task, Montezuma\u2019s Revenge, shown in the EIPO paper [22]. Both are implemented on top of EIPO\u2019s PPO codebase using RND exploration bonuses [23] as heuristic rewards $H$ , as suggested in [22]. HEPO outperforms EIPO, achieving the performance (denoted as dashed line) similar to PPO trained with RND at convergence (2 billion frames) reported in [23] in five times fewer frames. (b) HEPO matches HuRL\u2019s performance on the most challenging Sparse-Reacher task using HuRL\u2019s SAC codebase [10], despite HuRL being tuned for this task and HEPO using the same hyperparameters from our Section 4. This also highlights HEPO\u2019s generality in different RL algorithms. (c) HEPO\u2019s performance is positively correlated with that of the heuristic policy trained with heuristic rewards only (H-only), suggesting that HEPO\u2019s effectiveness will improve as the quality of heuristic rewards increases. ", "page_idx": 18}, {"type": "text", "text": "C Environment Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "As depicted in Section 4, we conducted our experiments based on the Isaac Gym (ISAAC) simulator [19] and the Bi-DexHands (BI-DEX) benchmark [24]. The selected task classes in ISAAC can be partitioned into 4 groups - Locomotion Tracking (Anymal), Locomotion Progressing (Ant and Humanoid), Helicopter Progressing (Ingenuity and Quadcopter), and Manipulation Tasks (FrankaCabinet, FrankaCubeStack, ShadowHand, and AllegroHand). In addition, BI-DEX provides dual dexterous hand manipulation tasks through ISAAC, reaching human-level sophistication of hand dexterity and bimanual coordination. Their tasks include ShadowHandOver, ShadowHandCatchUnderarm, ShadowHandCatchOver2Underarm, ShadowHandCatchAbreast, ShadowHandTwoCatchUnderarm, ShadowHandLiftUnderarm, ShadowHandDoorOpenInward, ShadowHandDoorOpenOutward, ShadowHandDoorCloseInward, ShadowHandDoorCloseOutward, ShadowHandSpin, ShadowHandUpsideDown, ShadowHandBlockStack, ShadowHandBottleCap, ShadowHandGraspAndPlace, ShadowHandKettle, ShadowHandPen, ShadowHandPushBlock, ShadowHandReOrientation, ShadowHandScissors, ShadowHandSwingCup. ", "page_idx": 18}, {"type": "text", "text": "For Locomotion Tracking, our emphasis lies in assessing the precision of velocities in linear and angular motions, ensuring that the robot responds closely to the assigned values. To this end, we define tracking errors as our task rewards. For Locomotion Progressing and Helicopter Progressing, our emphasis lies in evaluating the progress made by the robots in reaching the assigned destination from a given start point. To this end, we define movement progress as our task rewards. For Manipulation tasks and all tasks within the BI-DEX benchmark, our emphasis lies in whether and how quickly the robotic hands can successfully complete the assigned missions, reaching the desired goal states. To this end, we define task rewards using a binary label, assigning a value of 1 to indicate successful attainment of the goal state and 0 otherwise. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "The following are our reward function definitions, which include heuristic and task reward terms in Python style: ", "page_idx": 19}, {"type": "text", "text": "Isaac Gym - Locomotion Tracking Task: Anymal ", "text_level": 1, "page_idx": 19}, {"type": "image", "img_path": "vBGMbFgvsX/tmp/198ab1edc18945eeb2b19cdc99186dd565caec3b7f14255dfbe3eaf2827a06c3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Isaac Gym - Locomotion Progressing Task: Ant ", "text_level": 1, "page_idx": 20}, {"type": "image", "img_path": "vBGMbFgvsX/tmp/56a2d67caa21ff0c176076b5802b988a220f05501bde45538922995e825a5348.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Isaac Gym - Locomotion Progressing Task: Humanoid ", "text_level": 1, "page_idx": 21}, {"type": "image", "img_path": "vBGMbFgvsX/tmp/fc6da732bd4ba05c0bb8189c6fba02687c0f648e74aa4faa1d2a41fba7a71e5e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Isaac Gym - Helicopter Progressing Task: Ingenuity ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "def compute_ingenuity_reward(root_positions, target_root_positions, root_quats, root_linvels,   \n$\\hookrightarrow$ root_angvels, reset_buf, progress_buf, max_episode_length): # type: (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, float) -> Tuple[Tensor, Tensor, $\\hookrightarrow$ Tensor] # distance to target target_dist $=$ torch.sqrt(torch.square(target_root_positions - root_positions).sum(-1)) pos_reward $=$ 1.0 / (1.0 $^+$ target_dist $^*$ target_dist) # uprightness ups $=$ quat_axis(root_quats, 2) tiltage $=$ torch.abs(1 - ups[..., 2]) up_reward $=~5,0$ / (1.0 $^+$ tiltage $^*$ tiltage) # spinning spinnage $=$ torch.abs(root_angvels[..., 2]) spinnage_reward = 1.0 / (1.0 $^+$ spinnage $^*$ spinnage) # combined reward # uprigness and spinning only matter when close to the target reward $=$ pos_reward $^+$ pos_reward $^*$ (up_reward $^+$ spinnage_reward) $\\#$ resets due to misbehavior ones $=$ torch.ones_like(reset_buf) die $=$ torch.zeros_like(reset_buf) die $=$ torch.where(target_dist $>~8~.~0$ , ones, die) die $=$ torch.where(root_positions[..., $2]<\\ 0.5$ , ones, die) # resets due to episode length reset $=$ torch.where(progress_buf $>=$ max_episode_length - 1, ones, die) heuristic_reward, task_reward $=$ reward, pos_reward return heuristic_reward, task_reward, reset ", "page_idx": 22}, {"type": "text", "text": "Isaac Gym - Helicopter Progressing Task: Quadcopter ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "def compute_quadcopter_reward(root_positions, root_quats, root_linvels, root_angvels, reset_buf,   \n$\\hookrightarrow$ progress_buf, max_episode_length): # type: (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, float) -> Tuple[Tensor, Tensor, Tensor] # distance to target target_dist $=$ torch.sqrt(root_positions[..., 0] $^*$ root_positions[..., 0] $^+$ root_positions[..., 1] $^*$ root_positions[..., 1] $^+$ (1 - root_positions[..., 2]) $^*$ (1 - root_positions[..., 2])) pos_reward = 1.0 / (1.0 $^+$ target_dist $^*$ target_dist) # uprightness ups $=$ quat_axis(root_quats, 2) tiltage $=$ torch.abs(1 - ups[..., 2]) up_reward $=~1,0$ / (1.0 $^+$ tiltage $^*$ tiltage) # spinning spinnage $=$ torch.abs(root_angvels[..., 2]) spinnage_reward $=~1.0~\\times$ (1.0 $^+$ spinnage $^*$ spinnage) # combined reward # uprigness and spinning only matter when close to the target reward $=$ pos_reward $^+$ pos_reward $^*$ (up_reward $^+$ spinnage_reward) $\\#$ resets due to misbehavior ones $=$ torch.ones_like(reset_buf) die $=$ torch.zeros_like(reset_buf) die $=$ torch.where(target_dist $>~3\\cdot0$ , ones, die) die $=$ torch.where(root_positions[..., 2] $<~0~,3$ , ones, die) # resets due to episode length reset $=$ torch.where(progress_buf $>=$ max_episode_length - 1, ones, die) heuristic_reward, task_reward $=$ reward, pos_reward return heuristic_reward, task_reward, reset ", "page_idx": 22}, {"type": "text", "text": "Isaac Gym - Manipulation Task: FrankaCabinet ", "text_level": 1, "page_idx": 23}, {"type": "table", "img_path": "vBGMbFgvsX/tmp/4390d615edd1d39c56237a7b5954ac1eae8fc7f90cac571a23a66207078e3b78.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Isaac Gym - Manipulation Task: FrankaCubeStack ", "text_level": 1, "page_idx": 24}, {"type": "image", "img_path": "vBGMbFgvsX/tmp/dc847ee6654e8c4cf68aeef9603836bc30272ecc448cf99746bd8ab6c27766ee.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Isaac Gym - Manipulation Task: ShadowHand ", "text_level": 1, "page_idx": 25}, {"type": "image", "img_path": "vBGMbFgvsX/tmp/c96cd064f460aa90d1c8cd540cb82c575b99a8fe653a9f9d9dbb9b03221b6784.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Isaac Gym - Manipulation Task: AllegroHand ", "text_level": 1, "page_idx": 26}, {"type": "image", "img_path": "vBGMbFgvsX/tmp/d5162e3825065801e59a0fa584aa7c0e31ab93566a46c6ac36a3430a169f3dfa.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Bi-DexHands: ShadowHandOver ", "text_level": 1, "page_idx": 27}, {"type": "image", "img_path": "vBGMbFgvsX/tmp/e6872c941b383a0c817de88ece9dc989ddb4a80e09fee9b302c4d2326e6ea882.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Bi-DexHands: ShadowHandCatchUnderarm ", "text_level": 1, "page_idx": 28}, {"type": "image", "img_path": "vBGMbFgvsX/tmp/5250db511ebe5d22e4850a206b062a4ffe098385856dca12d4dac1eadd5d18a7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Bi-DexHands: ShadowHandCatchOver2Underarm ", "text_level": 1, "page_idx": 29}, {"type": "image", "img_path": "vBGMbFgvsX/tmp/d1c6c90035b200ef8b5a87c7b5b1eed7b5f2242d3c532fde57d6ae9124fa82d1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Bi-DexHands: ShadowHandCatchAbreast ", "text_level": 1, "page_idx": 30}, {"type": "image", "img_path": "vBGMbFgvsX/tmp/b61079867ec90414a36f404edd6de52bdea560ea14830034bca975d7f0f5d40a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Bi-DexHands: ShadowHandTwoCatchUnderarm ", "text_level": 1, "page_idx": 31}, {"type": "image", "img_path": "vBGMbFgvsX/tmp/20fd26c843bff5cdf87eae05e4d8d31354ae3f5b18031220e363af03509b2b00.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Bi-DexHands: ShadowHandLiftUnderarm ", "text_level": 1, "page_idx": 32}, {"type": "image", "img_path": "vBGMbFgvsX/tmp/60b292e0bec0535ad92baeb9e41d4b24e3820ff0d7a4a857189bf414a7cfe975.jpg", "img_caption": [], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "Bi-DexHands: ShadowHandDoorOpenInward ", "text_level": 1, "page_idx": 33}, {"type": "image", "img_path": "vBGMbFgvsX/tmp/37e519013384079ea16f2a8898867ce1e980cc55072300e7e1271642fc306695.jpg", "img_caption": [], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Bi-DexHands: ShadowHandDoorOpenOutward ", "text_level": 1, "page_idx": 34}, {"type": "image", "img_path": "vBGMbFgvsX/tmp/d7e069d37f299fbd9b7741fd6e86cf54ec6d43c129819611d3f4014e800b6cb8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Bi-DexHands: ShadowHandDoorCloseInward ", "text_level": 1, "page_idx": 35}, {"type": "table", "img_path": "vBGMbFgvsX/tmp/260faec92d27509039e7248cb0ad56a3f5fe4787c2a67c79fdcb1bc51684802f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 35}, {"type": "text", "text": "Bi-DexHands: ShadowHandDoorCloseOutward ", "text_level": 1, "page_idx": 36}, {"type": "image", "img_path": "vBGMbFgvsX/tmp/c31954348f42b577b812c64d7eea27422273ba5d8e733744da9ffce1ea45dfef.jpg", "img_caption": [], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "Bi-DexHands: ShadowHandSpin ", "text_level": 1, "page_idx": 37}, {"type": "image", "img_path": "vBGMbFgvsX/tmp/6599fe498e00ed3dd757988c24175bfe0d167f2ee24a31be21742f2283641398.jpg", "img_caption": [], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "Bi-DexHands: ShadowHandUpsideDown ", "text_level": 1, "page_idx": 38}, {"type": "image", "img_path": "vBGMbFgvsX/tmp/f8318d86456967686fafa8a4d602a4a6d4acab7dbaee4e17895a3185bf565687.jpg", "img_caption": [], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "Bi-DexHands: ShadowHandBlockStack ", "text_level": 1, "page_idx": 39}, {"type": "image", "img_path": "vBGMbFgvsX/tmp/0a8bad95e0bcdd4d2dcb81c0d0161ea0835a786f2d1f763eb701fb679798d970.jpg", "img_caption": [], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "Bi-DexHands: ShadowHandBottleCap ", "text_level": 1, "page_idx": 40}, {"type": "image", "img_path": "vBGMbFgvsX/tmp/afac8e38a658c8468d41796d276a31acd78c6155a0dc1fb975763cf9f1877d8e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "Bi-DexHands: ShadowHandGraspAndPlace ", "text_level": 1, "page_idx": 41}, {"type": "table", "img_path": "vBGMbFgvsX/tmp/5214d8fe91a18f273d779fa191319f9bcddbd7f8e58f74c28a54618c8c61190f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 41}, {"type": "text", "text": "Bi-DexHands: ShadowHandKettle ", "text_level": 1, "page_idx": 42}, {"type": "image", "img_path": "vBGMbFgvsX/tmp/1ddd1527b87e90321722d708689b40150230a85d63f2e9846cdb6edd58c560ef.jpg", "img_caption": [], "img_footnote": [], "page_idx": 42}, {"type": "text", "text": "Bi-DexHands: ShadowHandPen ", "text_level": 1, "page_idx": 43}, {"type": "table", "img_path": "vBGMbFgvsX/tmp/d8418d379ab5da93cb1cf78fd2a59701743ab4e1a9c7ab1c4d06c74f8d200ba4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 43}, {"type": "text", "text": "Bi-DexHands: ShadowHandPushBlock ", "text_level": 1, "page_idx": 44}, {"type": "image", "img_path": "vBGMbFgvsX/tmp/e45c43f6d9e575a69178a8d97e7bd984be3d569e96b2d6b1a4a8f53bb4ec04b4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 44}, {"type": "text", "text": "Bi-DexHands: ShadowHandReOrientation ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "def compute_hand_reward( rew_buf, reset_buf, reset_goal_buf, progress_buf, successes, consecutive_successes, max_episode_length: float, object_pos, object_rot, target_pos, target_rot, object_another_pos, $\\hookrightarrow$ object_another_rot, target_another_pos, target_another_rot, dist_reward_scale: float, rot_reward_scale: float, rot_eps: float, actions, action_penalty_scale: float, success_tolerance: float, reach_goal_bonus: float, fall_dist: float, fall_penalty: float, max_consecutive_successes: int, av_factor: float, ignore_z_rot: bool   \n): # Distance from the hand to the object goal_dist $=$ torch.norm(target_pos - object_pos, $\\mathtt{p}{=}2$ , $\\mathsf{d i m}\\!=\\!-1,$ ) if ignore_z_rot: success_tolerance $=~2,0~*$ success_tolerance goal_another_dist $=$ torch.norm(target_another_pos - object_another_pos, $\\scriptstyle\\mathtt{p}=2$ , ${\\tt d i m}{=}-1$ ) if ignore_z_rot: success_tolerance $=~2,0~*$ success_tolerance # Orientation alignment for the cube in hand and goal cube quat_diff $=$ quat_mul(object_rot, quat_conjugate(target_rot)) rot_dist $=~2,0~*$ torch.asin(torch.clamp(torch.norm(quat_diff[:, 0:3], $\\scriptstyle\\mathtt{p}=2$ , dim=-1), $\\mathtt{m a x=1.0})$ ) quat_another_diff $=$ quat_mul(object_another_rot, quat_conjugate(target_another_rot)) rot_another_dist $=~2,0~*$ torch.asin(torch.clamp(torch.norm(quat_another_diff[:, 0:3], $\\scriptstyle\\mathtt{p}=2$ , $\\mathtt{d i m=-1};$ ), $\\hookrightarrow$ max=1.0)) dist_rew $=$ goal_dist $^*$ dist_reward_scale $^+$ goal_another_dist $^*$ dist_reward_scale rot_rew $=~1.0/$ (torch.abs(rot_dist) rot_eps) rot_reward_scale $+\\ \\ 1.\\ 0/$ (torch.abs(rot_another_dist) + $\\hookrightarrow$ rot_eps) $^*$ rot_reward_scale action_penalty $=$ torch.sum(actions $4\\ast~2$ , dim $^{=-1}$ ) # Total reward is: position distance $^+$ orientation alignment $^+$ action regularization $^+$ success bonus $\\hookrightarrow\\;\\;+$ fall penalty reward $=$ dist_rew $^+$ rot_rew $^+$ action_penalty $^*$ action_penalty_scale # Find out which envs hit the goal and update successes count goal_resets $=$ torch.where(torch.abs(rot_dist) $<~0~.~1$ , torch.ones_like(reset_goal_buf), reset_goal_buf) goal_resets $=$ torch.where(torch.abs(rot_another_dist) $<~0~.1$ , torch.ones_like(reset_goal_buf), $\\hookrightarrow$ reset_goal_buf) successes $=$ successes $^+$ goal_resets # Success bonus: orientation is within \\`success_tolerance\\` of goal orientation reward $=$ torch.where(goal_resets $\\circleddash=~1$ , reward $^+$ reach_goal_bonus, reward) # Fall penalty: distance to the goal is larger than a threashold reward $=$ torch.where(object_pos[:, 2] $<=~0~,2$ , reward $^+$ fall_penalty, reward) reward $=$ torch.where(object_another_pos[:, 2] $<=~0,2$ , reward $^+$ fall_penalty, reward) # Check env termination conditions, including maximum success number resets $=$ torch.where(object_pos[:, 2] $<=~0~,2$ , torch.ones_like(reset_buf), reset_buf) resets $=$ torch.where(object_another_pos[:, 2] $<=~0,2$ , torch.ones_like(reset_buf), resets) if max_consecutive_successes $>~0$ : # Reset progress buffer on goal envs if max_consecutive_successes $>\\ 0$ progress_buf $=$ torch.where(torch.abs(rot_dist) $<=$ success_tolerance, $\\hookrightarrow$ torch.zeros_like(progress_buf), progress_buf) resets $=$ torch.where(successes $>=$ max_consecutive_successes, torch.ones_like(resets), resets) resets $=$ torch.where(progress_buf $>=$ max_episode_length, torch.ones_like(resets), resets) # Apply penalty for not reaching the goal if max_consecutive_successes $>~0$ : reward torch.where(progress_buf $>=$ max_episode_length, reward $+\\ 0.5\\ *$ fall_penalty, reward) num_resets $=$ torch.sum(resets) finished_cons_successes $=$ torch.sum(successes $^*$ resets.float()) cons_successes $=$ torch.where(num_resets $>~0$ , av_factor\\*finished_cons_successes/num_resets $^+$ (1.0 - $\\hookrightarrow$ av_factor) $^*$ consecutive_successes, consecutive_successes) goal_reach $={\\phantom{-}0},5*$ (torch.where(torch.abs(rot_dist) $<=~0~.~1$ , torch.ones_like(reset_goal_buf), $\\hookrightarrow$ torch.zeros_like(reset_goal_buf)) \\ $^+$ torch.where(torch.abs(rot_another_dist) $<=~0~.~1$ , torch.ones_like(reset_goal_buf), $\\hookrightarrow$ torch.zeros_like(reset_goal_buf))) heuristic_reward, task_reward $=$ reward, goal_reach return heuristic_reward, task_reward, resets, $\\hookrightarrow$ goal_resets, progress_buf, successes, cons_successes ", "page_idx": 45}, {"type": "text", "text": "Bi-DexHands: ShadowHandScissors ", "text_level": 1, "page_idx": 46}, {"type": "table", "img_path": "vBGMbFgvsX/tmp/79b9dc5fa03ca3d4a960d1b0fe837fb3fcec32f33e38cb6ff218a7dcf70a4a4f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 46}, {"type": "text", "text": "Bi-DexHands: ShadowHandSwingCup ", "text_level": 1, "page_idx": 47}, {"type": "image", "img_path": "vBGMbFgvsX/tmp/d9244f63b86baadabebdc8d6f2a46a8eebd57beadcf0edc734601a782eb2c1ef.jpg", "img_caption": [], "img_footnote": [], "page_idx": 47}, {"type": "text", "text": "Bi-DexHands: ShadowHandSwitch ", "text_level": 1, "page_idx": 48}, {"type": "image", "img_path": "vBGMbFgvsX/tmp/22cf0ebe12233735b3a0169103b938ba600016964a6abe2374a9aea3b17c5372.jpg", "img_caption": [], "img_footnote": [], "page_idx": 48}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Justification: This paper aims to explore alternatives for improving task performance in finite data settings using heuristic signals. Our experiments on robotic locomotion, helicopter, and manipulation tasks demonstrate that this method consistently improves performance, regardless of the general effectiveness of the heuristic signals. We are confident that our abstract and introduction sections accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 49}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Justification: The limitations of our approach are illustrated in Section 6. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 49}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Justification: We have shown our derivation details and limitations in Section A.1 and Section 6. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 50}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: We have provided detailed derivation and implementation descriptions (including the simulation environments, hyperparameters, and reward definitions for training and evaluations) in our Appendix section. Additionally, we have also provided our source code in our Supplementary Materials. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 50}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 51}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: We have provided detailed implementation descriptions (including the simulation environments, hyperparameters, and reward definitions for training and evaluations) in our Appendix section. Additionally, we have also provided our source code in our Supplementary Materials. The adopted simulation environments are all well-known and available online. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 51}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Justification: We have provided detailed implementation descriptions (including the simulation environments, hyperparameters, and reward definitions for training and evaluations) in the experiment and Appendix sections. Additionally, we have also provided our source code in our Supplementary Materials, including all the training and environment configurations. Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 51}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: All of our experimental results were obtained by 5 random seeds. We have provided the mean and standard deviation for our experimental results. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 52}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 52}, {"type": "text", "text": "Answer: [No] ", "page_idx": 52}, {"type": "text", "text": "Justification: But each training procedure can be performed on a single GeForce RTX 2080 Ti device. The required computational resources for all the simulation benchmarks are listed on their respective websites. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 52}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: We have carefully examined the ethical guidelines and verified that our work fully adheres to all the principles and requirements. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 52}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 53}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 53}, {"type": "text", "text": "Justification: The paper does not discuss both potential positive societal impacts and negative societal impacts of the work performed. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 53}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 53}, {"type": "text", "text": "Justification: This paper has no such risks. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 53}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: Throughout this paper, we have provided proper citations and references for all utilized repositories, benchmark simulations, and models/algorithms to uphold transparency and ensure appropriate attribution. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 54}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 54}, {"type": "text", "text": "Justification: We will furnish comprehensive documentation for our released code, elucidating its usage and providing information about the original source. Additionally, we have ensured that any code modified from external sources is subject to licenses that permit modification and redistribution. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 54}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 54}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 54}, {"type": "text", "text": "Justification: No, but we engaged 12 participants in devising reward functions as part of the experiments detailed in Section 4.2. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 54}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 55}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 55}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 55}]