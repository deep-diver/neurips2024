{"references": [{"fullname_first_author": "Agarwal, A.", "paper_title": "On the theory of policy gradient methods: Optimality, approximation, and distribution shift", "publication_date": "2021-00-00", "reason": "This paper provides a comprehensive theoretical foundation for policy gradient methods, which are central to the proposed algorithm in the current paper."}, {"fullname_first_author": "Wei, C.-Y.", "paper_title": "Model-free reinforcement learning in infinite-horizon average-reward Markov decision processes", "publication_date": "2020-00-00", "reason": "This paper addresses the average reward MDP setup without constraints, providing a baseline for the current paper's constrained counterpart."}, {"fullname_first_author": "Ding, D.", "paper_title": "Natural policy gradient primal-dual method for constrained Markov decision processes", "publication_date": "2020-00-00", "reason": "This work tackles constrained reinforcement learning in the discounted reward setting, offering valuable insights for adapting techniques to the average reward scenario."}, {"fullname_first_author": "Chen, L.", "paper_title": "Learning infinite-horizon average-reward Markov decision processes with constraints", "publication_date": "2022-00-00", "reason": "This study directly addresses average-reward CMDPs, laying the groundwork for analyzing the regret and constraint violation which is central to this paper's contribution."}, {"fullname_first_author": "Wei, H.", "paper_title": "A provably-efficient model-free algorithm for infinite-horizon average-reward constrained Markov decision processes", "publication_date": "2022-00-00", "reason": "This paper presents a model-free algorithm for average reward CMDPs in the tabular setting, setting the state-of-the-art for tabular solutions that this paper improves upon with general parameterization."}]}