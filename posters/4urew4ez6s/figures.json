[{"figure_path": "4UReW4Ez6s/figures/figures_8_1.jpg", "caption": "Figure 1: Energy Landscape under Different Iterations of Algorithm 1. Left: M = 2, Right: M = 4. Lighter color represents higher energy. The first row represents the raw energy landscape without applying U-Hop+. The second to last row represents the energy landscape when N = (1, 2, 5). The visualization shows that Algorithm 1 not only separates the local minima better, but also pushes memories closer to the fixed point.", "description": "This figure visualizes the energy landscape of KHMs at different stages of Algorithm 1 for two settings: 2 and 4 memories stored in a 2-dimensional space.  The raw energy landscape (without U-Hop+) is compared against the energy landscape after applying U-Hop+ for different iterations (N=1,2,5). Lighter colors represent higher energy. The visualization demonstrates how Algorithm 1 improves the separation of local minima (valleys) and pushes memories closer to fixed points, suggesting improved memory storage and retrieval capacity. The visualization shows that Algorithm 1 not only separates the local minima better, but also pushes memories closer to the fixed point.", "section": "4.2 Energy Landscape under U-Hop+ Stores More Memories"}, {"figure_path": "4UReW4Ez6s/figures/figures_9_1.jpg", "caption": "Figure 2: Basins of Attraction Comparison of Algorithm 1. The first row represents the raw Basins of Attraction without applying U-Hop+ or KHM. The second to last row shows the basins when N = (1,2,5). Square points are memories. White area is where queries are not able to converge to a single memory. Colored area is where queries converges to the corresponding memory. The result indicates that U-Hop+ is capable of converging to fixed point fast and reduce metastable states. 1 and 2-entmax corresponds to Softmax [Ramsauer et al., 2020] and Sparsemax [Hu et al., 2023].", "description": "This figure visualizes the basins of attraction for different numbers of iterations of Algorithm 1. It compares the raw basins of attraction (without U-Hop+ or KHM) to those after 1, 2, and 5 iterations. Each color represents a different memory, while white indicates queries that don't converge to a single memory. The results show that U-Hop+ improves convergence to fixed points and reduces metastable states.", "section": "4.2 Energy Landscape under U-Hop+ Stores More Memories"}, {"figure_path": "4UReW4Ez6s/figures/figures_29_1.jpg", "caption": "Figure 3: Separation Bound Numerical Simulation We visualize the bound presented in Proposition 3.1 in 3-D dimension. The bound goes tighter as the number of points increases.", "description": "This figure shows a numerical simulation of the bound presented in Proposition 3.1 of the paper, which deals with the minimal separation in 3 dimensions.  The plot compares the theoretical upper and lower bounds with the empirically observed minimal separation (ground truth) as the number of points (M) increases. The results indicate that the bound becomes tighter (more accurate) as the number of points increases.", "section": "D Experimental Details"}, {"figure_path": "4UReW4Ez6s/figures/figures_30_1.jpg", "caption": "Figure 4: Assignment Problem in 2D We observe that the learned feature map consistently put similar pairs closer to each other, leading to preserving some level of semantic information.", "description": "This figure visualizes the results of an experiment on a point assignment problem in 2D space.  The goal was to see if the learned feature map from the U-Hop+ algorithm would place semantically similar items closer together, even without explicitly incorporating semantic information into the algorithm's training.  The plot shows the arrangement of six points (representing different image classes from CIFAR10) on a unit circle after the feature map learned these points via Algorithm 1.  As shown, semantically similar pairs were placed closer together in most trials, suggesting that the algorithm implicitly learns and preserves semantic relationships.", "section": "4.3 Multiple Instance Learning"}, {"figure_path": "4UReW4Ez6s/figures/figures_30_2.jpg", "caption": "Figure 5: Loss Curve of L w.r.t. different memory set size. We run separation maximization for 100 epochs on MNIST under 2 settings, M = 100/200. We set \u03c4 = 0.1, learning rate 1e-3, D\u03c6 = 100. The result shows L converges fast, which echoes our sub-linear time complexity.", "description": "This figure shows the loss curves for the average separation loss (L) during the training process of the U-Hop+ algorithm. Two different memory set sizes (M=100 and M=200) are compared. The x-axis represents the training epoch, and the y-axis represents the value of the loss function L.  The plot demonstrates the convergence of the algorithm, showing that the loss decreases quickly within the first few epochs and then plateaus, indicating the algorithm's fast convergence speed which confirms the sub-linear time complexity of the algorithm, as theoretically analyzed in the paper.", "section": "D.6 Additional Experiments"}]