[{"figure_path": "4UReW4Ez6s/tables/tables_7_1.jpg", "caption": "Table 1: Distribution of Metastable State (in %). For MNIST, we use the training set as memories and test set as queries. ||p|| denotes the size of metastable state, which is the amount of non-zero entries of the probability distribution. For Softmax, we use a threshold of 0.01. For hyperparameter settings, see Table 3.", "description": "This table shows the distribution of metastable states for different models (Softmax, 1.5-entmax, sparsemax) with and without the U-Hop+ algorithm, on both synthetic and MNIST datasets.  A metastable state represents a situation where the model's retrieval process converges to a mixture of memories instead of a single memory. The size of a metastable state (||p||) indicates the number of memories involved in this mixture.  The table illustrates that U-Hop+ significantly reduces the proportion of metastable states, particularly for larger state sizes.", "section": "4 Experimental Studies"}, {"figure_path": "4UReW4Ez6s/tables/tables_8_1.jpg", "caption": "Table 2: Test AUC of Multiple Instance Learning Datasets. We compare the HopfieldPooling-based model with and without U-Hop+. We use the dense [Ramsauer et al., 2020] and sparse [Hu et al., 2023] modern Hopfield models as baselines. We use K-fold cross validation on all 4 datasets, with K = 10. The reported AUC is the average AUC score across 10 folds. For the baselines, we use the results reported in [Hu et al., 2023]. For our method, we directly use the default hyperparameter without grid search instead of using hyperparameter optimization (HPO) in [Hu et al., 2023, Ramsauer et al., 2020]. We exclude the variance as they are all smaller than 0.07. The result shows that even without HPO, U-Hop+ is still able to obtain a performance gain.", "description": "This table presents the test Area Under the Curve (AUC) scores for multiple instance learning on four datasets.  It compares the performance of HopfieldPooling models with and without the U-Hop+ algorithm, using dense and sparse modern Hopfield models as baselines.  The results demonstrate that U-Hop+ improves performance even without hyperparameter optimization.", "section": "4.3 Multiple Instance Learning"}, {"figure_path": "4UReW4Ez6s/tables/tables_28_1.jpg", "caption": "Table 1: Distribution of Metastable State (in %). For MNIST, we use the training set as memories and test set as queries. ||p|| denotes the size of metastable state, which is the amount of non-zero entries of the probability distribution. For Softmax, we use a threshold of 0.01. For hyperparameter settings, see Table 3.", "description": "This table presents the distribution of metastable states for different softmax variants (Softmax, 1.5-entmax, sparsemax) and for both synthetic and MNIST datasets.  The size of a metastable state (||p||) is the number of non-zero entries in the probability distribution.  A size of 1 indicates that the system converges to a single memory, while larger sizes indicate convergence to a mixture of memories. The table demonstrates the impact of the U-Hop+ algorithm on reducing the size of metastable states, indicating improved memory capacity.  The data was obtained under specific experimental conditions detailed in Table 3.", "section": "4.1 U-Hop+ Reduces Metastable States"}, {"figure_path": "4UReW4Ez6s/tables/tables_28_2.jpg", "caption": "Table 1: Distribution of Metastable State (in %). For MNIST, we use the training set as memories and test set as queries. ||p|| denotes the size of metastable state, which is the amount of non-zero entries of the probability distribution. For Softmax, we use a threshold of 0.01. For hyperparameter settings, see Table 3.", "description": "This table shows the distribution of metastable states for different models and datasets (synthetic and MNIST).  A metastable state represents the convergence of the Hopfield model update to a state that is not a single memory but rather a mixture of memories. The size of the metastable state, denoted as ||p||, indicates the number of non-zero entries in the probability distribution. The table compares different softmax variants (softmax, 1.5-entmax, sparsemax) along with the use of the U-Hop+ algorithm.  A threshold of 0.01 is applied to the softmax probability distribution to determine non-zero values. Hyperparameters used are specified in Table 3 of the paper.", "section": "4 Experimental Studies"}, {"figure_path": "4UReW4Ez6s/tables/tables_29_1.jpg", "caption": "Table 5: Hyperparameter used in the basins of attraction experiment.", "description": "This table lists the hyperparameters used in the basins of attraction experiment.  It includes settings for the optimizer, learning iterations, update rule iterations, learning rate, memory set size, pattern and feature dimensions, beta parameter, and query grid resolution.", "section": "D.3 Basins of Attraction"}]