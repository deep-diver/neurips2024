[{"figure_path": "Oq32ylAOu2/figures/figures_1_1.jpg", "caption": "Figure 1: Examples of multilingual mathematical reasoning from the MGSM dataset. LLM can generate correct and incorrect answers when asked in different languages.", "description": "This figure shows two examples of math word problems, one in English and one in Chinese, and how a large language model (LLM) attempted to solve them.  The examples highlight the challenges of multilingual reasoning for LLMs. In both cases, the LLM correctly understands and solves the English version, but its performance varies greatly with the Chinese version. In the first problem, the LLM correctly calculates the answer.  However, in the second problem, the LLM fails to fully grasp the logic of the Chinese version, resulting in an incorrect answer. This illustrates the significant discrepancy in the LLM's ability to handle reasoning tasks in different languages.", "section": "1 Introduction"}, {"figure_path": "Oq32ylAOu2/figures/figures_2_1.jpg", "caption": "Figure 2: Overview of the model structure and training scheme of MindMerger, which consists of an LLM (blue) and a external model (yellow) and is trained by a two-stage scheme.", "description": "This figure illustrates the architecture and training process of the MindMerger model.  The model structure shows how a Large Language Model (LLM) is combined with an external multilingual encoder.  The training scheme details the two-stage process: a mapping stage using general bilingual pairs to integrate the external model's capabilities into the LLM, and an augmentation stage using query translation task data to collaboratively utilize both internal and external capabilities.  The colors (blue and yellow) distinguish between the LLM and the external model, respectively.", "section": "3 Approach"}, {"figure_path": "Oq32ylAOu2/figures/figures_7_1.jpg", "caption": "Figure 3: Ablation experiments of MindMerger-Soft on the MGSM dataset. Lrl., Hrl., and Avg. represent the average accuracy across low-resource languages, high-resource languages, and all languages, respectively. Referring to Shi et al. [2023], we regard Bn, Th, and Sw as low-resourse languages, and regard the remaining languages as high-resource languages.", "description": "This figure presents the ablation study results for MindMerger-Soft on the MGSM dataset.  It shows the impact of removing the mapping stage (a), the augmentation stage (b), and compares the replacement-based and augmentation-based strategies (c).  The results are broken down by average accuracy across low-resource languages (Lrl.), high-resource languages (Hrl.), and all languages (Avg.).  The figure demonstrates the importance of both the mapping and augmentation stages in achieving high performance, particularly for low-resource languages.", "section": "5.2 Ablation Studies"}, {"figure_path": "Oq32ylAOu2/figures/figures_8_1.jpg", "caption": "Figure 4: T-SNE visualization in the spaces of the LLM embeddings and mapping layer outputs.", "description": "This figure visualizes the representation spaces of LLM embeddings and the outputs of the mapping layer using t-SNE.  The left panel (a) shows the LLM embeddings, illustrating how the representations of different languages are mostly separate, especially low-resource languages, from English. In contrast, the right panel (b) shows the mapping layer outputs, where the representations of all languages are clustered together and near to English. This demonstrates how the mapping layer helps to bridge the representation gap between multilingual model and LLM, facilitating the effective utilization of both internal and external capabilities.", "section": "5.4 Representation Space Changes"}, {"figure_path": "Oq32ylAOu2/figures/figures_16_1.jpg", "caption": "Figure 3: Ablation experiments of MindMerger-Soft on the MGSM dataset. Lrl., Hrl., and Avg. represent the average accuracy across low-resource languages, high-resource languages, and all languages, respectively. Referring to Shi et al. [2023], we regard Bn, Th, and Sw as low-resourse languages, and regard the remaining languages as high-resource languages.", "description": "This figure shows the results of ablation studies conducted on the MindMerger-Soft model using the MGSM dataset.  Three ablation experiments were performed: removing the mapping stage, removing the augmentation stage, and replacing the augmentation strategy with a replacement strategy. The results are presented in terms of average accuracy across low-resource, high-resource, and all languages. The figure demonstrates the importance of both the mapping and augmentation stages for achieving optimal performance, particularly in low-resource languages.", "section": "5.2 Ablation Studies"}]