[{"figure_path": "Oq32ylAOu2/tables/tables_5_1.jpg", "caption": "Table 1: Experimental results on MGSM and MSVAMP datasets. Lrl., Hrl., and Avg. represent the average accuracy across low-resource languages, high-resource languages, and all languages, respectively. Referring to Shi et al. [2023], we regard Bn, Th, and Sw as low-resourse languages, and regard the remaining languages as high-resource languages. We used the checkpoints given by Yu et al. [2023] as the MonoReason models.", "description": "This table presents the experimental results of various multilingual reasoning models on two benchmark datasets: MGSM and MSVAMP.  The results are broken down by language, distinguishing between low-resource and high-resource languages. The table compares the performance of MonoReason (a baseline), several relearning-based models (MultiReason-Lora, MultiReason-Full, QAlign), two replacement-based models (Translate-En, LangBridge), and the proposed MindMerger method (both hard and soft versions).  The average accuracy across low-resource languages (Lrl.), high-resource languages (Hrl.), and all languages (Avg.) are reported to provide a comprehensive comparison.", "section": "4.3 Experimental Results"}, {"figure_path": "Oq32ylAOu2/tables/tables_5_2.jpg", "caption": "Table 2: Results on the X-CSQA dataset. Avg. represents the average accuracy across all languages.", "description": "This table presents the results of the X-CSQA (Cross-lingual Commonsense Question Answering) dataset experiment.  It shows the performance of various models across different languages, including low-resource languages. The \"Avg.\" column represents the average accuracy across all languages tested. The table aims to demonstrate the multilingual reasoning capabilities of the models and their effectiveness in handling commonsense questions across various language backgrounds.", "section": "4.3 Experimental Results"}, {"figure_path": "Oq32ylAOu2/tables/tables_6_1.jpg", "caption": "Table 1: Experimental results on MGSM and MSVAMP datasets. Lrl., Hrl., and Avg. represent the average accuracy across low-resource languages, high-resource languages, and all languages, respectively. Referring to Shi et al. [2023], we regard Bn, Th, and Sw as low-resourse languages, and regard the remaining languages as high-resource languages. We used the checkpoints given by Yu et al. [2023] as the MonoReason models.", "description": "This table presents the experimental results of various multilingual reasoning models on the MGSM and MSVAMP datasets.  The results are broken down by language, showing the average accuracy for low-resource languages, high-resource languages, and overall.  It compares the performance of the proposed MindMerger method against several baseline models.", "section": "4.3 Experimental Results"}, {"figure_path": "Oq32ylAOu2/tables/tables_6_2.jpg", "caption": "Table 1: Experimental results on MGSM and MSVAMP datasets. Lrl., Hrl., and Avg. represent the average accuracy across low-resource languages, high-resource languages, and all languages, respectively. Referring to Shi et al. [2023], we regard Bn, Th, and Sw as low-resourse languages, and regard the remaining languages as high-resource languages. We used the checkpoints given by Yu et al. [2023] as the MonoReason models.", "description": "This table presents the experimental results of various multilingual reasoning models on the MGSM and MSVAMP datasets.  It compares the performance of the proposed MindMerger models (MindMerger-Hard and MindMerger-Soft) against several baseline methods, including MonoReason, MultiReason-Lora, MultiReason-Full, QAlign, LangBridge, and Translate-En.  The results are broken down by language, distinguishing between low-resource and high-resource languages, and overall average accuracy.  The table highlights the improvements achieved by MindMerger, particularly in low-resource languages.", "section": "4.3 Experimental Results"}, {"figure_path": "Oq32ylAOu2/tables/tables_7_1.jpg", "caption": "Table 1: Experimental results on MGSM and MSVAMP datasets. Lrl., Hrl., and Avg. represent the average accuracy across low-resource languages, high-resource languages, and all languages, respectively. Referring to Shi et al. [2023], we regard Bn, Th, and Sw as low-resourse languages, and regard the remaining languages as high-resource languages. We used the checkpoints given by Yu et al. [2023] as the MonoReason models.", "description": "This table presents a comparison of the performance of various models on two multilingual mathematical reasoning datasets: MGSM and MSVAMP.  The models compared include MonoReason (a baseline), several relearning-based methods (MultiReason-Lora, MultiReason-Full, QAlign), and two replacement-based methods (Translate-En, LangBridge). The table shows the average accuracy across all languages, as well as the average accuracy specifically for low-resource and high-resource languages. Low-resource languages are identified as Bengali (Bn), Thai (Th), and Swedish (Sw), while high-resource languages include Japanese, Chinese, German, French, Russian, Spanish, and English. The results highlight MindMerger's superior performance, particularly in low-resource languages.", "section": "4.3 Experimental Results"}, {"figure_path": "Oq32ylAOu2/tables/tables_13_1.jpg", "caption": "Table 6: Influence of training set size (per language) used in the augmentation stage. Lrl., Hrl., and Avg. represent the average accuracy across low-resource languages, high-resource languages, and all languages, respectively. Referring to Shi et al. [2023], we regard Bn, Th, and Sw as low-resourse languages, and regard the remaining languages as high-resource languages.", "description": "This table presents the results of an ablation study on the impact of different training set sizes used in the augmentation stage of the MindMerger model on the MGSM dataset.  It shows how the average accuracy across low-resource languages (Lrl.), high-resource languages (Hrl.), and all languages (Avg.) changes with varying training set sizes.  The results highlight the influence of data quantity on the model's performance, particularly for low-resource languages.", "section": "A. Supplementary Experiments"}, {"figure_path": "Oq32ylAOu2/tables/tables_14_1.jpg", "caption": "Table 7: The selection of mapping layers. # Parm represents parameters size of the mapping layers. Lrl., Hrl., and Avg. represent the average accuracy across low-resource languages, high-resource languages, and all languages, respectively. Referring to Shi et al. [2023], we regard Bn, Th, and Sw as low-resourse languages, and regard the remaining languages as high-resource languages.", "description": "This table presents the results of experiments conducted to determine the optimal configuration of the mapping layers within the MindMerger model. Different mapping layer architectures (Linear, 2-layer MLP, 3-layer MLP, and QFormer) were compared, and their performance was evaluated across low-resource, high-resource, and all languages combined.  The results demonstrate that a 2-layer MLP architecture yields the best performance in terms of average accuracy across all languages.", "section": "A.2 The Selection of Mapping Layers"}, {"figure_path": "Oq32ylAOu2/tables/tables_14_2.jpg", "caption": "Table 8: The usage of encoder-decoder model (M2M100-1.2B). Lrl., Hrl., and Avg. represent the average accuracy across low-resource languages, high-resource languages, and all languages, respectively. Referring to Shi et al. [2023], we regard Bn, Th, and Sw as low-resourse languages, and regard the remaining languages as high-resource languages.", "description": "This table presents the results of experiments evaluating the performance of MindMerger-Soft using different components of the encoder-decoder model M2M100-1.2B.  It compares the performance when using only the encoder, only the decoder, and both encoder and decoder components. The results are broken down by language group (low-resource and high-resource) and overall, showing the impact of each model component on multilingual reasoning accuracy.", "section": "A.3 The Usage of Encoder-Decoder Model"}, {"figure_path": "Oq32ylAOu2/tables/tables_14_3.jpg", "caption": "Table 1: Experimental results on MGSM and MSVAMP datasets. Lrl., Hrl., and Avg. represent the average accuracy across low-resource languages, high-resource languages, and all languages, respectively. Referring to Shi et al. [2023], we regard Bn, Th, and Sw as low-resourse languages, and regard the remaining languages as high-resource languages. We used the checkpoints given by Yu et al. [2023] as the MonoReason models.", "description": "This table presents the experimental results of various multilingual reasoning methods on the MGSM and MSVAMP datasets.  It compares the performance of MindMerger against several baseline methods, broken down by language (including low-resource languages like Bengali, Thai, and Swahili, and high-resource languages such as English, French, and German).  The results show the average accuracy for each language and across all languages, highlighting MindMerger's improvements, especially in low-resource scenarios.", "section": "4.3 Experimental Results"}, {"figure_path": "Oq32ylAOu2/tables/tables_15_1.jpg", "caption": "Table 1: Experimental results on MGSM and MSVAMP datasets. Lrl., Hrl., and Avg. represent the average accuracy across low-resource languages, high-resource languages, and all languages, respectively. Referring to Shi et al. [2023], we regard Bn, Th, and Sw as low-resourse languages, and regard the remaining languages as high-resource languages. We used the checkpoints given by Yu et al. [2023] as the MonoReason models.", "description": "This table presents a comparison of the performance of various multilingual reasoning models on two datasets: MGSM and MSVAMP.  The models are categorized into several types: MonoReason (a baseline), relearning-based methods (MultiReason-Lora, MultiReason-Full, QAlign), and replacement-based methods (Translate-En, LangBridge).  The table shows the average accuracy across all languages, as well as broken down by low-resource and high-resource languages.  The low-resource languages are identified as Bengali (Bn), Thai (Th), and Swahili (Sw). The results demonstrate the comparative performance of MindMerger against established methods.", "section": "4.3 Experimental Results"}, {"figure_path": "Oq32ylAOu2/tables/tables_16_1.jpg", "caption": "Table 1: Experimental results on MGSM and MSVAMP datasets. Lrl., Hrl., and Avg. represent the average accuracy across low-resource languages, high-resource languages, and all languages, respectively. Referring to Shi et al. [2023], we regard Bn, Th, and Sw as low-resourse languages, and regard the remaining languages as high-resource languages. We used the checkpoints given by Yu et al. [2023] as the MonoReason models.", "description": "This table presents the experimental results of various multilingual reasoning models on two datasets: MGSM and MSVAMP.  It compares the performance of MindMerger against several baseline models. The results are broken down by language, differentiating between low-resource and high-resource languages, and showing average accuracy across all languages. The table helps demonstrate MindMerger's improved performance, especially in low-resource languages.", "section": "4.3 Experimental Results"}, {"figure_path": "Oq32ylAOu2/tables/tables_16_2.jpg", "caption": "Table 1: Experimental results on MGSM and MSVAMP datasets. Lrl., Hrl., and Avg. represent the average accuracy across low-resource languages, high-resource languages, and all languages, respectively. Referring to Shi et al. [2023], we regard Bn, Th, and Sw as low-resourse languages, and regard the remaining languages as high-resource languages. We used the checkpoints given by Yu et al. [2023] as the MonoReason models.", "description": "This table presents the experimental results of various multilingual reasoning models on two datasets: MGSM and MSVAMP.  The results are broken down by language, distinguishing between low-resource and high-resource languages.  The table compares the performance of MindMerger (in two variants) against several baseline models, showing accuracy scores for each language and averaged across low-resource, high-resource, and all languages. The MonoReason model checkpoints are from Yu et al. (2023).", "section": "4.3 Experimental Results"}, {"figure_path": "Oq32ylAOu2/tables/tables_17_1.jpg", "caption": "Table 1: Experimental results on MGSM and MSVAMP datasets. Lrl., Hrl., and Avg. represent the average accuracy across low-resource languages, high-resource languages, and all languages, respectively. Referring to Shi et al. [2023], we regard Bn, Th, and Sw as low-resourse languages, and regard the remaining languages as high-resource languages. We used the checkpoints given by Yu et al. [2023] as the MonoReason models.", "description": "This table presents the experimental results of MindMerger and its baselines on two multilingual mathematical reasoning datasets: MGSM and MSVAMP.  The results are broken down by language, distinguishing between low-resource and high-resource languages.  The table shows the average accuracy for each model on each dataset and language group, allowing for a comparison of the performance of MindMerger against various methods, and highlights performance differences between resource levels for each language.", "section": "4.3 Experimental Results"}, {"figure_path": "Oq32ylAOu2/tables/tables_17_2.jpg", "caption": "Table 1: Experimental results on MGSM and MSVAMP datasets. Lrl., Hrl., and Avg. represent the average accuracy across low-resource languages, high-resource languages, and all languages, respectively. Referring to Shi et al. [2023], we regard Bn, Th, and Sw as low-resourse languages, and regard the remaining languages as high-resource languages. We used the checkpoints given by Yu et al. [2023] as the MonoReason models.", "description": "This table presents the experimental results of different multilingual reasoning models on the MGSM and MSVAMP datasets.  It compares the performance of MindMerger (with two variants and using two different multilingual encoders) against several baseline methods.  The results are broken down by language (showing low-resource languages separately), highlighting the improvements achieved by MindMerger, particularly in low-resource settings.  MonoReason models from Yu et al. (2023) serve as a baseline for comparison.", "section": "4.3 Experimental Results"}, {"figure_path": "Oq32ylAOu2/tables/tables_18_1.jpg", "caption": "Table 1: Experimental results on MGSM and MSVAMP datasets. Lrl., Hrl., and Avg. represent the average accuracy across low-resource languages, high-resource languages, and all languages, respectively. Referring to Shi et al. [2023], we regard Bn, Th, and Sw as low-resourse languages, and regard the remaining languages as high-resource languages. We used the checkpoints given by Yu et al. [2023] as the MonoReason models.", "description": "This table presents the experimental results of several multilingual reasoning models on the MGSM and MSVAMP datasets.  It compares the performance of MindMerger (in two variants) against several baseline methods. The results are broken down by language, distinguishing between low-resource and high-resource languages, and showing average accuracy across all languages. The baseline models include MonoReason, MultiReason-Lora, MultiReason-Full, QAlign, LangBridge, and Translate-En.  This allows for a comprehensive comparison of the proposed MindMerger approach against existing state-of-the-art methods in multilingual reasoning.", "section": "4.3 Experimental Results"}, {"figure_path": "Oq32ylAOu2/tables/tables_18_2.jpg", "caption": "Table 16: Results on the MGSM dataset. Lrl., Hrl., and Avg. represent the average accuracy across low-resource languages, high-resource languages, and all languages, respectively. We regard Te, Bn, Th, and Sw as low-resourse languages, and regard the remaining languages as high-resource.", "description": "This table presents the results of the MGSM dataset experiment, comparing the performance of MindMerger-Soft against other methods.  It shows the average accuracy for low-resource (Lrl), high-resource (Hrl), and all languages (Avg).  The low-resource languages specified are Telugu, Bengali, Thai, and Swahili.", "section": "B.4 Complete Experiments on the MGSM Dataset"}, {"figure_path": "Oq32ylAOu2/tables/tables_19_1.jpg", "caption": "Table 17: Dataset statistics. # Train, # Test, and # Lang refer to the size of query translation training set per language, the size of multilingual test set per language, and the number of language we evaluated for each dataset, respectively.", "description": "This table presents the statistics of the four datasets used in the paper's experiments.  For each dataset, it shows the number of training examples per language (# Train), the number of test examples per language (# Test), and the total number of languages included (# Lang). The datasets cover three different tasks: mathematical reasoning (MGSM and MSVAMP), commonsense reasoning (X-CSQA), and natural language inference (XNLI).", "section": "4.2 Datasets"}, {"figure_path": "Oq32ylAOu2/tables/tables_19_2.jpg", "caption": "Table 1: Experimental results on MGSM and MSVAMP datasets. Lrl., Hrl., and Avg. represent the average accuracy across low-resource languages, high-resource languages, and all languages, respectively. Referring to Shi et al. [2023], we regard Bn, Th, and Sw as low-resourse languages, and regard the remaining languages as high-resource languages. We used the checkpoints given by Yu et al. [2023] as the MonoReason models.", "description": "This table presents the experimental results of various multilingual reasoning models on two datasets: MGSM and MSVAMP.  The results are broken down by language, distinguishing between low-resource and high-resource languages.  The table compares the performance of MindMerger (in two variants) against several baseline models, showing the average accuracy for each language and across different language categories.  The MonoReason model's checkpoints from Yu et al. (2023) were used as a baseline.", "section": "4.3 Experimental Results"}, {"figure_path": "Oq32ylAOu2/tables/tables_19_3.jpg", "caption": "Table 1: Experimental results on MGSM and MSVAMP datasets. Lrl., Hrl., and Avg. represent the average accuracy across low-resource languages, high-resource languages, and all languages, respectively. Referring to Shi et al. [2023], we regard Bn, Th, and Sw as low-resourse languages, and regard the remaining languages as high-resource languages. We used the checkpoints given by Yu et al. [2023] as the MonoReason models.", "description": "This table presents the experimental results of different multilingual reasoning models on two datasets: MGSM and MSVAMP.  It compares the average accuracy of several models (MonoReason, MultiReason-Lora, MultiReason-Full, QAlign, LangBridge, Translate-En, MindMerger-Hard, and MindMerger-Soft) across various languages, distinguishing between low-resource and high-resource languages. The results showcase the performance improvements achieved by MindMerger compared to the baselines, particularly for low-resource languages.", "section": "4.3 Experimental Results"}]