{"importance": "This paper is crucial for researchers working on multilingual large language models (LLMs) and cross-lingual reasoning.  It offers a novel method to significantly improve LLM reasoning capabilities in non-English languages, particularly low-resource ones, **without requiring extensive parameter updates**.  This addresses a critical limitation in current LLM research and opens up new avenues for enhancing multilingual understanding and reasoning across diverse languages. The proposed method is also computationally efficient, making it practical for real-world applications.", "summary": "MindMerger efficiently boosts LLM reasoning in non-English languages by merging LLMs with external multilingual language understanding capabilities, achieving significant accuracy improvements, especially in low-resource languages, without modifying LLM parameters.", "takeaways": ["MindMerger significantly improves LLM reasoning accuracy in non-English languages, especially those with limited resources.", "The method achieves these improvements without updating the LLM's parameters, making it computationally efficient.", "A two-stage training process effectively integrates external multilingual capabilities with the LLM's inherent strengths."], "tldr": "Large Language Models (LLMs) often exhibit a performance gap between English and other languages, particularly those with limited resources.  Existing methods either retrain LLMs for each language or rely on translation, often underutilizing LLMs' inherent reasoning and language understanding capabilities. This leads to suboptimal performance and high computational costs.\nMindMerger, a novel method, directly addresses these issues. It merges LLMs with external multilingual language understanding models using a two-stage training scheme to integrate external capabilities without modifying the core LLM parameters. Experiments show that MindMerger significantly outperforms existing methods across diverse multilingual datasets, achieving notable improvements in both high- and low-resource languages.  **This approach demonstrates a new paradigm for leveraging existing multilingual resources to enhance LLM performance.**", "affiliation": "Shanghai Artificial Intelligence Laboratory", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "Oq32ylAOu2/podcast.wav"}