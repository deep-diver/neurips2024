[{"figure_path": "iYcY7KAkSy/figures/figures_2_1.jpg", "caption": "Figure 1: The architecture of Spikformer-like network and the issue of spiking matrix multiplication and max pooling layer (A) The overall architecture of STMixer and Spikformer. (B) The issue of Max Pooling layer when there is a spike delay in asynchronous hardware. (C) The spike delay issue of spiking matrix multiplication. When both matrices involved in matrix multiplication are composed of spikes, the imprecise timing of spike arrival can result in inaccuracies in the computed product.", "description": "This figure illustrates the architecture of Spikformer-like networks and highlights the challenges of using spiking matrix multiplication and max-pooling layers in asynchronous hardware. Panel A shows the overall architecture of both Spikformer and the proposed STMixer, emphasizing their shared structure of Spiking Patch Splitting (SPS), encoder blocks, and a classification head. Panel B demonstrates how a delay in spike arrival in an asynchronous max-pooling layer affects the output, while Panel C visualizes how similar inaccuracies arise in spike matrix multiplication due to imprecise spike arrival timings.", "section": "1 Introduction"}, {"figure_path": "iYcY7KAkSy/figures/figures_5_1.jpg", "caption": "Figure 2: The architecture of (A) Spiking token mixing (STM) module and (B) Information protect Spiking Patch Splitting (IPSPS) module.", "description": "This figure shows the architecture of two crucial modules in the proposed STMixer network: the Spiking Token Mixing (STM) module and the Information Protect Spiking Patch Splitting (IPSPS) module.  (A) The STM module details how a weighted matrix is used to mix token information, replacing the original Spiking Self-Attention (SSA) module to be more suitable for asynchronous hardware. (B) The IPSPS module modifies the original Spiking Patch Splitting (SPS) module to reduce information loss during downsampling by adding an additional information protection pathway, using a convolutional layer to generate additional feature maps.", "section": "4.3 STMixer"}, {"figure_path": "iYcY7KAkSy/figures/figures_6_1.jpg", "caption": "Figure 3: Visualization results of random initial weight output feature maps of the SPS module. We present the feature maps of the 100th channel and the average of all channels, and in SNN, the feature map is the mean feature map across all time steps. When using the LIF neuron model as the activation function for SPS, it is obvious that there is a significant information loss in the output feature map compared to the input image. Employing the IPSPS can alleviate this information loss.", "description": "This figure visualizes the information loss in the Spiking Patch Splitting (SPS) module of a Spiking Neural Network (SNN).  It compares feature maps generated using ReLU and LIF activation functions at different time steps (T=1 and T=4) for both the original SPS and the improved IPSPS (Information Protected SPS) module. The results show that the LIF activation function in the standard SPS leads to significant information loss compared to ReLU and to the input image. The IPSPS module mitigates this information loss.", "section": "4.3 STMixer"}, {"figure_path": "iYcY7KAkSy/figures/figures_8_1.jpg", "caption": "Figure 4: Neural architecture search space and search results. Left: Search space of each Cell. Mid: random sample result from supernet on CIFAR-100. Right: random sample result from supernet on CIFAR-10.", "description": "This figure shows the results of a neural architecture search. The search space is shown on the left, which includes different combinations of convolutional layers, SSA (Spiking Self-Attention), and STM (Spiking Token Mixing) modules. The middle and right panels show the results of the search on CIFAR-100 and CIFAR-10 datasets, respectively. Each point represents a randomly sampled architecture from the search space, and its position indicates its accuracy and FLOPs (floating point operations). The figure shows that STMixer achieves a good balance between accuracy and FLOPs.", "section": "5.2 Exploration of the architecture of Encoders"}, {"figure_path": "iYcY7KAkSy/figures/figures_13_1.jpg", "caption": "Figure 3: Visualization results of random initial weight output feature maps of the SPS module. We present the feature maps of the 100th channel and the average of all channels, and in SNN, the feature map is the mean feature map across all time steps. When using the LIF neuron model as the activation function for SPS, it is obvious that there is a significant information loss in the output feature map compared to the input image. Employing the IPSPS can alleviate this information loss.", "description": "This figure visualizes the information loss in the Spiking Patch Splitting (SPS) module of Spiking Neural Networks (SNNs) and demonstrates how the proposed Information Protection Spiking Patch Splitting (IPSPS) module mitigates this issue.  It compares feature maps from SPS using ReLU and LIF activation functions, showing a significant information loss in the LIF version (SNN). The IPSPS module is shown to significantly reduce this information loss.  The visualization uses both individual channel and average across all channels.", "section": "4.3 STMixer"}]