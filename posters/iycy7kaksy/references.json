{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, which is the foundation of many state-of-the-art models in NLP and has significantly influenced the design of the Spikingformer and STMixer architectures."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-01-01", "reason": "This work introduced the Vision Transformer (ViT), which adapted the Transformer architecture to image processing and achieved state-of-the-art results in image classification, inspiring the use of Transformer-like architectures in Spiking Neural Networks."}, {"fullname_first_author": "Mike Davies", "paper_title": "Loihi: A neuromorphic manycore processor with on-chip learning", "publication_date": "2018-01-01", "reason": "This paper describes the Loihi neuromorphic chip, which is a significant advancement in hardware for implementing SNNs and directly influenced the consideration of hardware limitations in this paper's architecture design."}, {"fullname_first_author": "Shikuang Deng", "paper_title": "Surrogate module learning: Reduce the gradient error accumulation in training spiking neural networks", "publication_date": "2023-01-01", "reason": "This work introduces the Surrogate Module Learning (SML) method, which is a key technique used in this paper to address gradient issues during SNN training, improving performance and enabling the use of low time steps."}, {"fullname_first_author": "Zhaokun Zhou", "paper_title": "Spikingformer: Spike-driven residual learning for transformer-based spiking neural network", "publication_date": "2023-01-01", "reason": "This paper introduced the Spikingformer architecture, which is directly compared against and improved upon by the proposed STMixer architecture; it represents the current state-of-the-art in spiking transformer networks."}]}