[{"type": "text", "text": "Covariate Shift Corrected Conditional Randomization Test ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bowen Xu\u02da Harvard University bowenxu@g.harvard.edu ", "page_idx": 0}, {"type": "text", "text": "Yiwen Huang\u02da Department of Statistics Peking University 2000010773@stu.pku.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Chuan Hong Department of Biostatistics and Bioinformatics Duke University chuan.hong@duke.edu ", "page_idx": 0}, {"type": "text", "text": "Shuangning Li Booth School of Business University of Chicago shuangning.li@chicagobooth.edu ", "page_idx": 0}, {"type": "text", "text": "Molei Liu: Department of Biostatistics Columbia Mailman School of Public Health ml4890@cumc.columbia.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Conditional independence tests are crucial across various disciplines in determining   \n2 the independence of an outcome variable $Y$ from a treatment variable $X$ , condition  \n3 ing on a set of confounders $Z$ . The Conditional Randomization Test (CRT) offers   \n4 a powerful framework for such testing by assuming known distributions of $X\\mid Z$ ;   \n5 it controls the Type-I error exactly, allowing for the use of flexible, black-box   \n6 test statistics. In practice, testing for conditional independence often involves   \n7 using data from a source population to draw conclusions about a target population.   \n8 This can be challenging due to covariate shift\u2014differences in the distribution of   \n9 $X,\\,Z.$ , and surrogate variables, which can affect the conditional distribution of $Y\\mid X,Z$ \u2014rendering traditional CRT approaches invalid. To address this issue, we propose a novel Covariate Shift Corrected Pearson Chi-squared Conditional Randomization (csPCR) test. This test adapts to covariate shifts by integrating importance weights and employing the control variates method to reduce variance in the test statistics and thus enhance power. Theoretically, we establish that the csPCR test controls the Type-I error asymptotically. Empirically, through simulation studies, we demonstrate that our method not only maintains control over Type-I errors but also exhibits superior power, confirming its efficacy and practical utility in real-world scenarios where covariate shifts are prevalent. Finally, we apply our methodology to a real-world dataset to assess the impact of a COVID-19 treatment on the 90-day mortality rate among patients. ", "page_idx": 0}, {"type": "text", "text": "10   \n11   \n12   \n13   \n14   \n15   \n16   \n17   \n18   \n19   \n20 ", "page_idx": 0}, {"type": "text", "text": "21 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "22 Conditional independence tests are important across diverse fields for determining whether an   \n23 outcome variable $Y$ is independent of a treatment variable $X$ , conditioning on a potentially high  \n24 dimensional vector of confounding variables $Z$ . This type of testing is critical for understanding the   \n25 complex relationships among variables. For instance, scientists may hope to understand whether   \n26 a specific genetic feature influences disease outcomes, whether a particular treatment effectively   \n27 extends life expectancy, or whether certain demographic factors impact college admissions.   \n28 Traditionally, these conditional testing problems are approached by modeling $Y$ against $X$ and $Z$   \n29 through some parametric or semiparametric model. However, this strategy has been criticized due   \n30 to potential model misspecification and limited observations of $Y$ . As an alternative strategy, the   \n31 model-X framework and Conditional Randomization Test (CRT) propose testing for the general   \n32 conditional independence hypothesis $H_{0}:X\\perp\\!\\!\\!\\!\\perp Y\\mid Z$ , free of any specific effect parameters [2].   \n33 The CRT assumes the distribution of $X\\mid Z$ to be known and can control the type-I error exactly,   \n34 allowing for the choice of any flexible, black-box test statistic. This strategy is particularly useful   \n35 when there is either strong and reliable scientific knowledge of the distribution of $X\\mid Z$ or an   \n36 auxiliary dataset of $(X,Z)$ of large sample size, known as the semi-supervised setting. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "Me5esZTRqW/tmp/88088df5e667dc4935e870221403bfb1295793292365e9cd8ed15a0d260d5a48.jpg", "img_caption": ["47 48 Figure 1: Type-I Error rates of our proposed csPCR and the source-only PCR 49 on a simulated example. The Type-I er50 ror inflation of PCR demonstrates that 51 source analysis is not valid or generaliz52 able on the target due to covariate shift. 53 "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "In practice, testing for conditional independence frequently involves using data from a source population to draw conclusions about a target population. This situation presents challenges due to potential differences in the distribution of variables between the two populations. For example, economists may be interested in whether college admission $(Y)$ is independent of family income $(X)$ , conditioning on variables such as GPA, extracurricular activities, geographic location, and other demographics $(Z)$ . In the source population, the relationship might be influenced by factors like wealthy parents investing in SAT preparation, which boosts admission rates\u2014a relationship that may not exist in a target population where such preparation is less common. Although $Y$ may not appear independent of $X$ given $Z$ in the source population, the conclusion could vary significantly in the target population. This discrepancy underscores the need for a robust and flexible testing procedure that can adapt to shifts in distributions. ", "page_idx": 1}, {"type": "text", "text": "54 More specifically, we address the covariate shift scenario, where the distributions of the treatment   \n55 variables $X$ , the confounding variables $Z$ , and some surrogate or auxiliary variables $V$ (e.g., SAT   \n56 scores) may differ between the source and target populations. However, the conditional distribution   \n57 of $Y$ given $X,Z$ , and $V$ remains the same between them. In such scenarios, our goal is to leverage   \n58 information from the source to accurately test for conditional independence in the target population   \n59 without the observation of $Y$ on target. In the scenario we consider, the presence of $V$ and potential   \n60 differences in $P(V\\mid X,Z)$ between the source and target populations may lead to the conditional   \n61 independence $X\\perp Y\\mid Z$ not holding simultaneously in the two populations. Specifically, because ", "page_idx": 1}, {"type": "equation", "text": "$$\nP(Y\\mid X,Z)=\\int P(Y\\mid X,Z,V)P(V\\mid X,Z)\\,d V,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "62 the conditional distribution of $Y$ given $X$ and $Z$ can vary between populations. This underscores   \n63 why the problem is non-trivial. ", "page_idx": 1}, {"type": "text", "text": "64 See Figure 1 for an example of the consequences of such covariate shift. ", "page_idx": 1}, {"type": "text", "text": "65 In this paper, we propose a novel conditional independence test suitable for covariate shift scenarios.   \n66 Our method builds upon the Pearson Chi-Squared Conditional Randomization (PCR) test, a powerful   \n67 model-X testing procedure that effectively addresses a broader range of alternative $p$ -value distri  \n68 butions than the vanilla CRT [5]. Methodologically, we make two major contributions. First, we   \n69 introduce importance weights into the label counting steps of the original PCR test, making the new   \n70 test valid under covariate shift. These weights adjust the importance of each sample according to   \n71 its density ratio, effectively rebalancing the source data to match the target population\u2019s distribution.   \n72 Second, we introduce a power enhancement method that employs the control variates method to   \n73 reduce variance in the test statistics. Although importance weights can increase the variance in test   \n74 statistics, especially when the density ratio can become extremely high, potentially reducing power,   \n75 our power enhancement method effectively addresses this issue. Together, these innovations enable   \n76 us to develop a PCR test that is both powerful and valid under covariate shifts.   \n77 The rest of the paper is organized as follows: In Section 2, we provide a formal introduction to   \n78 the problem setup. In Section 3, we introduce the proposed Covariate Shift Corrected Pearson   \n79 Chi-squared Conditional Randomization (csPCR) test and establish that the proposed csPCR test   \n80 controls the Type-I error asymptotically. In Section 4, we demonstrate the empirical performance   \n81 of the csPCR test through simulation studies. In Section 5, we apply the proposed csPCR test to a   \n82 real-world dataset to assess the impact of a COVID-19 treatment on the 90-day mortality rate among   \n83 patients. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "84 1.1 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "85 Our work builds upon the model-X framework and the conditional randomization test proposed by   \n86 Candes et al. [2]. The particular method we develop is based on a variant of the vanilla CRT, the   \n87 Pearson Conditional Randomization (PCR) test [5]. Recent advances in the CRT include improving   \n88 computation time [7, 10], studying robustness [6, 11], and examining statistical power [19]. The   \n89 focus of this paper, different from the above, is on how to build a valid CRT procedure when there   \n90 is covariate shift. The paper is also complementary to the above literature: for example, we hope   \n91 that future work can conduct theoretical power analysis for our procedure or develop a double robust   \n92 version of the procedure just like in [6]. Finally, we note that surrogate variables play a crucial role in   \n93 this paper: because the distribution of the surrogate variables is different in the source and the target   \n94 population, naively testing the conditional independence hypothesis in the source population can   \n95 yield invalid conclusions for the target population. A surrogate or silver standard label is a variable   \n96 that is more feasible and accessible than $Y$ in data collection and can be viewed as a noisy measure   \n97 of $Y$ . For example, tumor response rate is often used as an early endpoint surrogate for the long-term   \n98 survival outcome [3], and blood pressure is commonly used as a surrogate for heart attacks. Surrogate   \n99 variables are also commonly used in environmental studies and economics. Surrogate variables also   \n100 play an important role in the paper by [6], albeit in a different way, where the surrogate variables are   \n101 used to learn the distribution of $Y\\mid X,Z$ and to further improve the robustness of the CRT procedure.   \n102 Statistical learning and inference under covariate shift has been extensively studied over the past years.   \n103 As a seminal work in addressing covariate shift bias, [4] proposed a density ratio weighting approach   \n104 using kernel mean matching to characterize the adjusting weights. Their key idea of importance   \n105 (re)weighting is intrinsically connected with early work in broader contexts like importance sampling   \n106 [15, e.g.] and semiparametric inference [13, e.g.]. [8] extended this idea to a doubly robust framework   \n107 accommodating surrogate variables like $V$ and being more robust to the misspecification or poor   \n108 quality of the density ratio models. [18] handled a more challenging scenario with severe shift and   \n109 poor overlap between the source and target populations. Among this track of literature, [17] is the   \n110 most closely related to our work as they also considered conditional independence testing under   \n111 distributional shifts and proposed a general testing procedure base on importance sampling (IS)   \n112 allowing for the use of CRT. Different from us, their work does not accommodate the covariate shifts   \n113 of some surrogate or auxiliary $V$ . Moreover, as will be shown in our numerical studies, their general   \n114 IS testing strategy can encounter the loss of effective sample sizes and be less powerful than ours. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "115 2 Problem Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "116 2.1 Conditional Independence Testing under Covariate Shift ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "117 Let $Y\\,\\in\\,\\mathbb{R}$ denote the outcome variable, $X\\ \\in\\ \\mathbb{R}$ the treatment variable, $Z~\\in~\\mathbb{R}^{p}$ a vector of   \n118 confounding variables, and $V\\,\\in\\,\\mathbb{R}^{d}$ a vector of surrogate variables. To make the problem more   \n119 concrete, consider the following two examples:   \n120 Example 1 (College Admission). $Y$ is college admission, $X$ is family income, $Z$ includes a number   \n121 of factors such as GPA, extracurricular activities, geographic location, and demographic information,   \n122 $V$ is the SAT score. In this case, $V$ is easier to collect compared to $Y$ as the college admission   \n123 requires individual-level surveys.   \n124 Example 2 (Health Outcome). $Y$ is a long-term health outcome, $X$ is a medical treatment, $Z$   \n125 includes factors such as age, gender, and health history, $V$ includes surrogate variables like blood   \n126 pressure, BMI, and duration of hospital stays post the treatment, which can be measured within a   \n127 much shorter term than $Y$ .   \n128 Consider a scenario involving two distinct populations: the source population $\\boldsymbol{S}$ and the target   \n129 population $\\tau$ . We collect data from the source population with the goal of making inferences about   \n130 the target population. The source data contains $n$ independent and identically distributed samples of   \n131 $(Y_{i},X_{i},Z_{i\\cdot},V_{i\\cdot})$ for $i=1,\\hdots,n$ . Let $\\mathbf{y}=(Y_{1},Y_{2},\\ldots,Y_{n})^{\\top}\\in\\mathbb{R}^{n}$ , $\\mathbf{x}=(\\dot{X_{1}},X_{2},\\ldots,X_{n})^{\\top}\\in\\mathbb{R}^{n}$ ,   \n132 $\\mathbf{Z}\\,=\\,(Z_{1}.,Z_{2}.,.\\,.\\,.\\,,Z_{n}.)^{\\top}\\,\\in\\,\\mathbb{R}^{n\\times p}$ , and $\\mathbf{V}\\,=\\,(V_{1}.,V_{2}.,.\\,.\\,.\\,,V_{n}.)^{\\top}\\,\\in\\,\\mathbb{R}^{n\\times d}$ . We are interested in   \n133 testing the following conditional independence hypothesis in the target population: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "image", "img_path": "Me5esZTRqW/tmp/bb376c5d4583c73fa25d52c85917cf420ea562264c58967cf51be8e1c81dcb6c.jpg", "img_caption": ["Figure 2: Direct acyclic graphs illustrating possible differences between the source and the target populations. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{H}_{0}:X\\perp\\!\\!\\!\\perp Y\\mid Z.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "134 We assume that the conditional distribution of $Y\\mid X,Z,V$ is the same in both populations; however,   \n135 the distribution of $(X,Z,V)$ varies between $\\boldsymbol{S}$ and $\\tau$ . More precisely, the joint distribution of   \n136 $Y,X,Z,V$ can be described as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P_{S}(Y,X,Z,V)=P_{S}(X,Z,V)P(Y|X,Z,V)\\quad\\mathrm{on}\\;S,}\\\\ {P_{T}(Y,X,Z,V)=P_{T}(X,Z,V)P(Y|X,Z,V)\\quad\\mathrm{on}\\;T.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "137 This situation is referred to as the covariate shift scenario because the distribution of the covariates   \n138 $X,Z$ , and $V$ in the source population $\\boldsymbol{S}$ does not match that in the target population $\\tau$ .   \n139 Let\u2019s understand the above assumption and its implications through the two examples above. In the   \n140 college admissions example, it is plausible to assume that the rate of college admissions remains   \n141 consistent across the two populations when conditioned on the SAT score, family income, and other   \n142 confounding variables. However, the joint distribution of $X,V$ and $Z$ can differ: in the source   \n143 population, if wealthy parents frequently invest in SAT preparation, boosting admission rates, this   \n144 relationship may not hold in a target population where such preparation is uncommon. In such cases,   \n45 it is thus possible that $X\\underline{{{H Y}}}\\mid\\overline{{{Z}}}$ in the source population but $X\\perp\\!\\!\\!\\!\\perp Y\\mid Z$ in the target population   \n146 (see Figure 2 for such an example). In the health outcomes example, it is again plausible that the   \n147 conditional distribution of long-term health outcomes given the treatment variable, confounding   \n148 variables, and surrogates remains the same across the two populations. However, the assignment of   \n149 the treatment may depend differently on the surrogate variables across the two populations. Therefore,   \n150 it\u2019s possible that $X\\perp\\!\\!\\!\\!\\perp Y\\mid Z$ in one population, but not in the other.   \n151 In both examples, we can see that the result of naively applying a valid conditional independence test   \n152 on the source population cannot guarantee a valid conclusion for testing $\\mathcal{H}_{0}$ in the target population.   \n153 Therefore, we need to develop new tools for addressing covariate shifts in conditional independence   \n154 tests. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "155 2.2 Model-X Framework ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "156 In this paper, we operate within the model-X framework, as described by Candes et al. [2], which   \n157 assumes that the joint distributions of covariates $X,V,Z$ are perfectly known in both the source and   \n158 target populations. This framework is particularly suited for scenarios where: (1) there is substantial   \n159 prior domain knowledge about the covariates $X,V,$ , and $Z$ , or (2) there is a significant amount of   \n160 unsupervised data for these covariates in both populations, in addition to $n$ labeled observations in   \n161 the source population, characterizing a semi-supervised setting.   \n162 An example of the first scenario can be seen in genetics, where researchers have well-established   \n163 models for the joint distributions of single nucleotide polymorphisms (SNPs). For the second scenario,   \n164 consider our earlier example involving health outcomes. Here, the outcome variable $Y$ represents a   \n165 long-term health outcome that is more costly or sensitive to measure compared to the shorter-term   \n166 variables $X,V,$ , and $Z$ . In such cases, the variables $X,V,$ , and $Z$ are typically easier and less costly to   \n167 collect, frequently resulting in a semi-supervised setting in these health-related studies. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "168 3 Method: Covariate Shift Corrected PCR Test ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "169 3.1 Incorporating the Density Ratio into the PCR Test ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "170 In Section 2.1, we discussed how naively applying conditional independence tests to the source   \n171 data cannot guarantee valid conclusions for the target population. To address this issue, we must   \n172 incorporate information about the differences between the two populations into our testing procedure.   \n173 In particular, we will make use of the density ratio defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\ne(X,Z,V)={\\frac{P_{T}(X,Z,V)}{P_{S}(X,Z,V)}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "174 This ratio measures the relative likelihood of observing each combination of variables $(X,Z,V)$   \n175 in the target population compared to the source population. By reweighting the data points in the   \n176 source population using this density ratio, we effectively transform the source distribution to match   \n177 the distribution of the target population, thereby addressing the covariate shift problem.   \n178 More specifically, we build our method upon the recently proposed Pearson Chi-Squared Conditional   \n179 Randomization (PCR) test [5]. Compared to the vanilla CRT, the PCR test is designed to be more   \n180 powerful across a broader range of alternative $p$ -value distributions. At a high level, the PCR test   \n181 assigns a label to each data point following a counterfeit sampling step and a subsequent score   \n182 computation step. Under the null hypothesis that $X\\,\\perp Y\\,\\mid\\,{\\bar{Z}}$ , the distribution of these labels   \n183 should be uniform across all possible labels. The PCR test then rejects the null hypothesis if   \n184 the empirical distribution of the labels deviates significantly from uniformity, as determined by a   \n185 Pearson\u2019s chi-squared test.   \n186 Under distributional shift, if the data points were sampled from the target population, then the   \n187 distribution of the labels would be uniform. However, since the data points are actually sampled from   \n188 the source population, they must be reweighted using the density ratio. More specifically, in the final   \n189 step of the PCR test, where the Pearson\u2019s chi-squared test is applied, we consider not the count of   \n190 data points for each label, but the sum of the density ratios of the data points for each label instead.   \n191 Under the null hypothesis, each sum should approximate $n/L$ , where $L$ is the total number of labels.   \n192 Consequently, we modify the Pearson\u2019s chi-squared test to determine whether these weighted sums   \n193 deviate significantly from $n/L$ .   \n194 Based on the above intuition, we propose the Covariate Shift Corrected PCR (csPCR) Test, as outlined   \n195 in Algorithm 1.   \n196 In Algorithm 1, lines 1-7 correspond to those in the original PCR test. These lines initiate the test by   \n197 gdeern tehrae tinnugll  choyupnottehrfeesiits ,s tahme prlaensd $\\mathring{X}_{j}^{(m)}$ .r iaAbslseus $(X_{j}^{\\checkmark},Y_{j},Z_{j}),(\\tilde{X}_{j}^{(1)},Y_{j}^{\\bullet},Z_{j}),\\dots,(\\tilde{X}_{j}^{(M)},Y_{j},Z_{j})$ c awl,o uulnd  \n199 be exchangeable. Consequently, the rank $R_{j}$ would be uniformly distributed over $\\mathbf{\\dot{\\left\\{}1,\\dots;M+1\\right\\}}$ in   \n200 the absence of ties, leading to a uniform distribution of the labels as well.   \n201 Lines 8-10 in Algorithm 1 address the covariate shift by incorporating density ratios as importance   \n202 weights into $W_{j}$ . Due to this redefinition of $W_{\\ell}$ , the null distribution of the final test statistic   \n203 $U_{n,L}$ is also different. Therefore, we also adjust the rejection threshold from the quantile of a chi  \n204 squared distribution, as in the original PCR test, to the quantile of the weighted sum of chi-squared   \n205 distributions. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "206 3.2 Power Enhancement ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "207 To effectively address covariate shift, incorporating density ratios as importance weights into the   \n208 PCR test is essential. However, when these ratios become large, they can increase the variance of the   \n209 statistics $W_{l}$ . This elevated variance can diminish the test\u2019s power. Therefore, developing methods to   \n210 reduce this variance is crucial for maintaining the power of the test. ", "page_idx": 4}, {"type": "text", "text": "Input: Data $D_{\\mathcal{T}}=(\\mathbf{y},\\mathbf{x},\\mathbf{Z},\\mathbf{V})$ , the density ratio $e$ , the test statistics $T$ , integers $K,L\\geqslant1$ , and the significance level $\\alpha$ .   \n1: Take $M=K L-1$ .   \n2: for each data point $j=1$ to $n$ do   \n3: Draw $M$ i.i.d samples $\\widetilde{X}_{j}^{(1)},\\ldots,\\widetilde{X}_{j}^{(M)}$ XjpMqfrom PT pX | Zq.   \n4: Use $T$ to score the init iarl data poi nrt $(X_{j},Y_{j},Z_{j})$ and its $M$ counterfeits $(\\widetilde{X}_{j}^{(1:M)},Y_{j},Z_{j})$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad T_{j}\\,=\\,T(X_{j},Y_{j},Z_{j})}\\\\ &{\\smash{\\widetilde{T}_{j}^{(i)}=T(\\widetilde{X}_{j}^{(i)},Y_{j},Z_{j}),\\;\\mathrm{for\\,}i\\in\\{1,\\dots,M\\}\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "5: Let $R_{j}$ denote the rank of $T_{j}$ amo n g $\\{T_{j},\\tilde{T}_{j}^{(1)},\\ldots,\\tilde{T}_{j}^{(M)}\\}$ , with ties broken randomly. 6: Partition $\\{1,\\dots,M+1\\}=S_{1}\\bigcup\\dots\\bigcup S_{L}$ with $S_{\\ell}:=\\{(\\ell-1)K+1,\\ldots,\\ell K\\}$ . Assign label $\\ell_{j}\\in\\{1,2,\\ldots,L\\}$ to sample $j$ if $R_{j}\\in S_{\\ell_{j}}$ . ", "page_idx": 5}, {"type": "text", "text": "7: end for ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "8: Let $w_{j}=e(X_{j},Z_{j},V_{j})$ for each $j\\in\\{1,2,\\dots,n\\}$ .   \n9: for each label $\\mathcal{l}\\in\\bar{\\{1,2,\\ldots,L\\}}$ : do ", "page_idx": 5}, {"type": "text", "text": "10: Let $W_{\\ell}$ be the sum of $\\ell$ -labeled importance weights: $W_{\\ell}=\\sum_{j=1}^{n}w_{j}\\cdot\\mathbb{1}\\{\\ell_{j}=\\ell\\}$ . ", "page_idx": 5}, {"type": "text", "text": "11: Let $D_{\\ell}$ be the sum of $\\ell$ -labeled squared importance weights: $D_{\\ell}=\\sum_{j=1}^{n}w_{j}^{2}\\cdot\\mathbb{1}\\{\\ell_{j}=\\ell\\}.$ . ", "page_idx": 5}, {"type": "text", "text": "12: end for ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "13: Let $\\begin{array}{r}{\\hat{\\Omega}_{n}=\\frac{L}{n}\\mathrm{diag}(D_{1},D_{2},\\cdot\\cdot\\cdot,D_{L})-\\frac{1}{L}\\cdot\\mathbf{1}_{L\\times L}.}\\end{array}$   \n14: Calculate the test statistic $U_{n,L}$ as follows $\\begin{array}{r}{U_{n,L}=\\frac{L}{n}\\sum_{\\ell=1}^{L}\\left(W_{\\ell}-\\frac{n}{L}\\right)^{2}}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Output: Reject the null hypothesis if $U_{n,L}\\geqslant\\theta_{\\hat{\\Omega}_{n},\\alpha}$ ; otherwise, accept the null hypothesis. Here, $\\theta_{\\hat{\\Omega}_{n},\\alpha}$ is the $1-\\alpha$ quantile of the distribution $\\chi_{\\hat{\\Omega}_{n}}^{2}$ , where $A\\sim\\chi_{\\Omega}^{2}$ denotes that $A=x^{\\mathsf{T}}x$ for $\\boldsymbol{x}\\sim\\mathcal{N}(\\boldsymbol{0},\\boldsymbol{\\Omega})$ . ", "page_idx": 5}, {"type": "text", "text": "211 To this end, we introduce a control variate function $a$ , allowing $a(X,Z,V)$ to serve as a control   \n212 variate in reducing variance in $W_{l}$ [14]. Specifically, for a chosen $\\gamma_{\\ell}$ , we define ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widetilde{W}_{\\ell}=\\sum_{j=1}^{n}w_{j}\\cdot\\left[\\mathbb{1}\\{\\ell_{j}=\\ell\\}-\\gamma_{\\ell}a(X_{j},Z_{j},V_{j})\\right]+n\\gamma_{\\ell}\\mathbb{E}_{T}\\left[a(X,Z,V)\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "213 We can then use $\\widetilde{W}_{\\ell}$ instead of $W_{\\ell}$ in our algorithm. ", "page_idx": 5}, {"type": "text", "text": "214 We note that for  any arbitrary choice of the function $a$ and the parameter $\\gamma_{\\ell}$ , the expectation of $\\widetilde{W}_{\\ell}$   \n215 would be the same as that of $W_{\\ell}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}\\left[\\widetilde{W}_{\\ell}\\right]=\\sum_{j=1}^{n}\\mathbb{E}\\left[w_{j}\\mathbb{1}\\{\\ell_{j}=\\ell\\}\\right]-\\sum_{j=1}^{n}\\gamma_{\\ell}\\mathbb{E}\\left[w_{j}a(X_{j},Z_{j},V_{j})\\right]+n\\gamma_{\\ell}\\mathbb{E}_{T}\\left[a(X,Z,V)\\right]}}\\\\ &{}&{=\\mathbb{E}\\left[W_{\\ell}\\right]-n\\gamma_{\\ell}\\left(\\mathbb{E}_{S}\\left[e(X,Z,V)a(X,Z,V)\\right]-\\mathbb{E}_{T}\\left[a(X,Z,V)\\right]\\right)=\\mathbb{E}\\left[W_{\\ell}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "216 Therefore, even if we make a sub-optimal choice of the function $a$ and the parameter $\\gamma_{\\ell}$ in practice,   \n217 the resulting test (under certain assumptions) will still remain asymptotically valid (see Section 3.3   \n218 for more details).   \n219 However, for effective variance reduction, it is preferable to have the control covariates $a(X,Z,V)$   \n220 well-correlated with the outcome (See Section 4 for practical discussions on choices of the function   \n221 $a$ ). This is quite feasible, especially since the surrogate variable $V$ is likely to be predictive of $Y$ . ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Input: Data $D_{\\mathcal{T}}=(\\mathbf{y},\\mathbf{x},\\mathbf{Z},\\mathbf{V})$ , the density ratio $e$ , the test statistics $T$ , the control variate function $a$ , integers $K,L\\geqslant1$ , and the significance level $\\alpha$ .   \n1: for each data point $j=1$ to $n$ do   \n2: Compute the labels $\\ell_{j}$ as in Algorithm 1. ", "page_idx": 6}, {"type": "text", "text": "3: end for ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4: Let $w_{j}=e(X_{j},Z_{j},V_{j})$ for each $j\\in\\{1,2,\\dots,n\\}$ .   \n5: for each label $\\mathcal{l}\\in\\bar{\\{1,2,\\ldots,L\\}}$ : do   \n6: Compute $\\hat{\\gamma}\\ell$ , the regression coefficient obtained by a weighted linear regression of the indicator   \nfunction $\\left\\{{\\mathbb{1}}\\{\\ell_{j}=\\bar{\\ell}\\}\\right\\}_{j=1}^{n}$ on the control variate $\\left\\{a(X_{j},\\bar{Z}_{j},V_{j})\\right\\}_{j=1}^{n}$ with weights $\\left\\{w_{j}\\right\\}_{j=1}^{n}$ .   \n7: Compute the augmented version of $W_{\\ell}$ as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widetilde{W}_{\\ell}=\\sum_{j=1}^{n}w_{j}\\cdot\\left[\\mathbb{1}\\{\\ell_{j}=\\ell\\}-\\hat{\\gamma}_{\\ell}a(X_{j},Z_{j},V_{j})\\right]+n\\hat{\\gamma}_{\\ell}\\mathbb{E}_{\\mathcal{T}}\\left[a(X,Z,V)\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "8: end for ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "9: Let $\\stackrel{\\star\\star\\star}{\\mathbf{W}}=\\left(w_{j}\\cdot\\left[\\mathbb{1}\\{\\ell_{j}=\\ell\\}-\\hat{\\gamma}_{\\ell}a(X_{j},Z_{j},V_{j})\\right]+\\hat{\\gamma}_{\\ell}\\mathbb{E}_{\\mathcal{T}}\\left[a(X,Z,V)\\right]\\right)_{\\ell,j}\\mathrm{for~}1\\leqslant\\ell\\leqslant L,\\,1\\leqslant\\mathcal{T}\\ell.$ $j\\leqslant n.$ . 10: Calculate the sample covariance matrix $\\begin{array}{r}{\\widetilde\\Omega_{n}=\\frac{L}{n}(\\mathbf W-\\frac{1}{L}\\cdot\\mathbf{1}_{L\\times n})(\\mathbf W-\\frac{1}{L}\\cdot\\mathbf{1}_{L\\times n})^{\\mathsf{T}}.}\\end{array}$ . 11: Calculate the test statistic $U_{n,L}$ as follo wrs $\\widetilde{U}_{n,L}=\\frac{L}{n}\\sum_{\\ell=1}^{L}\\left(\\widetilde{W}_{\\ell}-\\frac{n}{L}\\right)^{2}$ . ", "page_idx": 6}, {"type": "text", "text": "Output: Reject the null hypothesis if $\\tilde{U}_{n,L}\\geqslant\\theta_{\\tilde{\\Omega}_{n},\\alpha}$ ; otherwise, accept the null hypothesis. Here, $\\theta_{\\widetilde{\\Omega}_{n},\\alpha}$ is the $1-\\alpha$ quantile of the  dristributiorn $\\chi_{\\tilde{\\Omega}_{n}}^{2}$ , where $A\\sim\\chi_{\\Omega}^{2}$ denotes that $A=x^{\\mathsf{T}}x$ for $\\boldsymbol{x}\\sim\\mathcal{N}(\\boldsymbol{0},\\boldsymbol{\\Omega})$ . ", "page_idx": 6}, {"type": "text", "text": "222 We would also like to discuss the choice of $\\gamma_{\\ell}$ . According to the control covariate literature, with a   \n223 fixed function $a$ , the optimal choice of $\\gamma_{\\ell}$ that minimizes variance is given by: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\gamma_{\\ell}=\\frac{\\mathrm{Cov}\\left[w_{j}\\mathbb{1}\\left\\{\\ell_{j}=\\ell\\right\\},w_{j}a(X_{j},Z_{j},W_{j})\\right]}{\\mathrm{Var}\\left[w_{j}a(X_{j},Z_{j},W_{j})\\right]}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "224 This coefficient is also the same as that obtained from a linear regression [14]. Thus, when implement  \n225 ing the algorithm, we take $\\gamma_{\\ell}$ to be the regression coefficient obtained by running a weighted linear   \n226 regression of the indicator function $\\{\\mathbb{1}\\left\\{\\ell_{j}=\\ell\\right\\}\\}_{j=1}^{n}$ on the control variate $\\left\\{a(X_{j}^{\\bar{\\mathbf{\\alpha}}},Z_{j},\\bar{V_{j}})\\right\\}_{j=1}^{n}$ with   \n227 weights wj jn\u201c1. ", "page_idx": 6}, {"type": "text", "text": "228 We have outlined the new csPCR test, including this power enhancement step, in Algorithm 2. ", "page_idx": 6}, {"type": "text", "text": "229 3.3 Theoretical Properties ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "230 In this section, we establish that the proposed tests control the type-I error asymptotically. Further  \n231 more, we show that the power enhancement step effectively reduces the variance of the statistics $W_{\\ell}$ ,   \n232 which can typically improve the power.   \n233 Assumption 1 (Fourth moment). The fourth moment of the density ratio $e(X,Z,V)$ is finite:   \n234 $\\mathbb{E}_{S}\\left[e(X,Z,V)^{4}\\right]\\,<\\,\\infty$ . Furthermore, the fourth moment of product of the density ratio and the   \n235 cont\u201crol variate fu\u2030nction is also finite: $\\mathbb{E}_{S}\\left[e(X,Z,V)^{4}a(X,Z,V)^{4}\\right]<\\infty.$ .   \n236 Theorem 1 (Valid Tests). Under Assumption $^{\\,l}$ , assume that the null hypothesis of $X\\perp Y\\mid Z$ holds   \n237 in the target population, then ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\operatorname*{lim}_{n\\to\\infty}\\mathbb{P}\\left[\\mathrm{Algorithm~1~rejects}\\right]=\\alpha.}\\\\ {\\displaystyle}\\\\ {\\displaystyle\\operatorname*{lim}_{n\\to\\infty}\\mathbb{P}\\left[\\mathrm{Algorithm~2~rejects}\\right]=\\alpha.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "238 ", "page_idx": 6}, {"type": "text", "text": "239 Theorem 2 (Variance Reduction). Let $W_{l}$ be the statistics computed in line 10 in Algorithm 1, and   \n240 $\\widetilde{W}_{l}$ be the statistics computed in line 7 in Algorithm 2. Under Assumption $^{\\,l}$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\left(\\mathrm{Var}\\left[\\widetilde W_{l}\\right]/\\mathrm{Var}\\left[W_{l}\\right]\\right)\\leqslant1.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "241 4 Numerical Simulation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "242 In this section, we present simulation studies to assess the performance of our proposed csPCR   \n243 method and its power enhancement version denoted csPCR(pe), and compare them to a benchmark   \n244 method. The benchmark method adopted is an importance-resampling based method [17], denoted as   \n245 the IS method. For fair comparison, we used the same PCR statistic as our method for the testing   \n246 with IS. We use a significance level of $\\alpha=0.05$ . ", "page_idx": 7}, {"type": "text", "text": "247 4.1 Simulation Setup ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "248 We consider a semi-supervised setting where we have a large volume of unlabeled data of $(X_{j},Z_{j},V_{j})$   \n249 from both the source and target populations. In addition, we have a small number of labeled data of   \n250 $(Y_{j},X_{j},Z_{j},V_{j})$ from the source population.   \n251 We separate confounding variables $Z$ into two sets: $Z=(Z_{\\mathrm{r}},Z_{\\mathrm{null}})$ , where $Z_{\\mathrm{r}}$ is the relevant set and   \n252 $Z_{\\mathrm{null}}$ is the null set. The relevant confounding variables $Z_{\\mathrm{r}}$ are generated as i.i.d. multivariate normal,   \n253 with mean 0 for the source population and 1 for the target population to simulate the distributional   \n254 shift in $Z$ , where $Z_{\\mathrm{r}}\\ \\in\\ \\mathbb{R}^{p}$ and we set $p~=~5$ . Null confounding variables $Z_{\\mathrm{null}}$ are generated   \n255 independently with no correlation to other variables, modeled as $\\bar{\\mathcal{N}}(0.1,I_{q})$ with $q=50$ for sparse   \n256 high-dimensional settings in both populations.   \n257 The treatment variable $X$ and the surrogate variable $V$ are conditionally generated based on $Z$ .   \n258 Specifically, $X$ is modeled identically across both the source and target populations as $\\mathcal{N}(u^{\\top}Z_{\\mathrm{r}},1)$ ,   \n259 where $u$ is a predefined parameter vector that remains the same for both populations.   \n260 For $V$ , it is modeled differently in the two populations, represented as ${\\cal N}(v_{S/T}^{\\top}Z_{\\mathrm{r}}+(1-\\theta)a_{S/T}X+$   \n261 $\\theta a_{S/T}\\sin(X),1)$ . Here, $v_{S}$ and $v\\tau$ are predefined parameter vectors for the source and target   \n262 populations, respectively. The parameter $a$ varies between populations ( $a_{S}$ for the source and $a\\tau$ for   \n263 the target), controlling the effect of $X$ on $V$ , modeling the indirect effect. The factor $\\theta$ modulates the   \n264 nonlinear component of this relationship.   \n265 The outcome variable $Y$ is generated for both populations using the same conditional model over   \n266 $(X,Z,V)$ : ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\nY|(X,Z,V)_{S/T}\\sim{\\mathcal{N}}((v^{\\top}Z_{\\mathrm{r}})^{2}+\\beta V+\\gamma X,1),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "267 where $\\beta$ and $\\gamma$ control the effects of $V$ (indirect) and $X$ (direct) on $Y$ , respectively. ", "page_idx": 7}, {"type": "text", "text": "268 We generate 1000 unlabeled source and target samples to estimate the density ratio and generate 500   \n269 labeled source samples for testing. Moreover, in the simulation, we assume we have full knowledge   \n270 of the joint distribution of $(X,Z)$ and estimate $V|X,Z$ using an Elastic net regression model with   \n271 5-fold cross-validation [20]. For the test statistic $T$ in the algorithm, we choose a simple function   \n272 $T(\\tilde{X},Z,V,Y)=Y\\cdot\\tilde{X}$ . For each parameter iteration, we conduct 1000 Monte Carlo simulations   \n273 to estimate the Type-I error and power. We estimate the covariance matrix of the sequence of $W_{i}$ \u2019s   \n274 using the Monte Carlo method and use the momentchi2 package [1] for calculating the $p$ -value.   \n275 Additionally, we empirically choose the best hyperparameter $L=3$ for all our experiments through   \n276 additional experiments shown in Appendix B.2. ", "page_idx": 7}, {"type": "text", "text": "277 4.2 Simulation Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "278 In Figure 3, we choose $a_{S}\\,=\\,1$ and $a_{7}=0$ to compare the Type-I error control of our methods   \n279 with the benchmark. The left panel shows the Type-I error rate as the sample size of the data used   \n280 to estimate the density ratio, $n_{e}$ , varies from small to large. There appears to be a slight Type-I   \n281 error inflation for all three methods when the sample size $n_{e}$ is small, but the Type-I error quickly   \n282 converges to the ideal level of 0.05 as $n_{e}$ grows larger. Moreover, our methods show more stable   \n283 Type-I error control than the benchmark method when the estimation sample size is low. The right   \n284 panel shows that when the density ratio is well approximated, all three methods attain good Type-I   \n285 error control regardless of the change in $\\beta$ , i.e., the strength of the indirect effect, but the csPCR and   \n286 csPCR(pe) methods have more stable control. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "Me5esZTRqW/tmp/a3f3f97f0923c020f98baf5e9d9258dc9b99360f7c45043cc21af66c02824909.jpg", "img_caption": ["294 Figure 3: Comparison of Type-I error control across three methods. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "To evaluate the statistical power of our csPCR test, we choose $a s\\;=\\;0$ and $a_{\\mathcal{T}}=2$ , so that the null hypothesis holds true in the target population but not in the source population. As Figure ${4\\mathrm{a}}$ shows, both the csPCR and the csPCR(pe) methods have uniformly higher power than the bench", "page_idx": 8}, {"type": "text", "text": "296 effect size $\\beta$ . For example, when $\\beta=1.4$ , the benchmark IS method has a power of 0.33, the csPCR   \n297 method has a power of 0.44, and the csPCR(pe) method can attain a power of 0.8.   \n298 When we fix the indirect effect $\\beta=2$ and vary the direct effect of $X\\left(\\gamma\\right)$ , as shown in Figure 4b, our   \n299 methods still exceed the benchmark, and the power enhancement significantly improves the original   \n300 version of the test. For example, when $\\gamma=1$ , the benchmark IS method has a power of 0.4, the   \n301 csPCR method has a power of 0.62, and the csPCR(pe) method can attain a power of 0.86.   \n302 We also test how adding a nonlinear component to the indirect effect affects the power when we   \n303 assume a linear model of $V\\mid Z,X$ in the estimation stage. This can be helpful in assessing the   \n304 performance of our methods under model misspecification. As Figure $4\\mathrm{c}$ indicates, as the nonlinear   \n305 effect increases, the power of all three methods decreases, though our methods still significantly   \n306 exceed the benchmark. Interestingly, we observe that as $\\theta\\,\\rightarrow\\,1$ , i.e., there is a full nonlinear   \n307 component without a linear component, the advantage of the power-enhanced version over the   \n308 original csPCR test disappears. This occurs because when the $V\\mid X,Z$ model is misspecified and   \n309 the density ratio estimation is inaccurate, the variance reduction in the control variates step reduces   \n310 variance in the \u201cwrong\" direction, and thus does not improve the power of the original method. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "Me5esZTRqW/tmp/88c437be60c1a34d11f6d72c9f7ff89cc4dcce9df600fdd442c954f25b72675e.jpg", "img_caption": ["Figure 4: Comparison of statistical power of the three methods as the effect size varies: (a) indirect effect $\\beta$ , (b) direct effect $\\gamma$ , and (c) nonlinear effect size $\\theta$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "311 4.3 Effective Sample Size ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "331123 aWpep rnootaicche eas  s[e9r]i.e s Aofm wonorgk  tihne mme, aosnueri nogf  tthhee  emffoesctt icvoe smammopnle  msiezae s(uErSe Si)s $\\begin{array}{r}{n_{\\mathrm{eff}}\\,=\\,(\\sum_{i=1}^{n}w_{i})^{2}/\\sum_{i=1}^{n^{\\smash{\\sim}}}w_{i}^{\\widecheck{2}}}\\end{array}$   \n314 When the covariate shift between the source and target becomes stronger, \u0159the varian c\u0159e of the   \n315 importance weight $w_{i}$ tends to be large and $n_{\\mathrm{eff}}$ will become smaller, which could result in lower   \n316 power. We carry out simulation studies on the relationship between the power of csPCR and the ESS   \n317 determined by the degree of covariate shift as discussed in Appendix B.3. ", "page_idx": 8}, {"type": "text", "text": "318 5 Real-World Application ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "319 The COVID-19 pandemic has presented unprecedented challenges to global health systems, with   \n320 high variability in outcomes based on demographic and clinical characteristics. Early identification   \n321 of patients at high risk for severe outcomes, such as mortality within 90 days of hospital admission,   \n322 is crucial for timely and effective treatment interventions. This study leverages extensive hospital   \n323 data to develop models predicting 90-day mortality following hospital admission due to COVID-19.   \n324 For this study, we extract patient data spanning from January 2020 to December 2023 from Duke   \n325 University Health System (DUHS), focusing on individuals admitted with COVID-19. This period   \n326 encompasses multiple waves of the pandemic, influenced by various circulating variants.   \n327 Our dataset comprises patient records for a total of $N=3,057$ individuals admitted with COVID-19.   \n328 The outcome $Y$ is defined as mortality within 90 days since hospital admission due to COVID-19.   \n329 The treatment variable $X$ is defined as binary, where 1 indicates the administration of any COVID-19   \n330 specific medication (explained in Appendix C) and 0 otherwise. The covariates $Z$ include comorbidity   \n331 indices (renal disease, diabetes without complication, diabetes with complication, local tumor, and   \n332 metastatic tumor), age, gender, and race, which are critical for adjusting the risk models due to   \n333 their known influence on COVID-19 outcomes. The length of hospitalization, denoted as $V$ , is   \n334 standardized to follow a standard normal distribution (with a mean of zero and a standard deviation   \n335 of one), facilitating comparisons and integration into predictive models regardless of original scale or   \n336 distribution.   \n337 The dataset is segmented into two distinct groups based on the date of hospital admission to align   \n338 with pivotal changes in virus strain predominance and public health guidelines. The source data   \n339 comprises COVID-19 admissions prior to November 30, 2021, with a sample size of $N_{1}=1$ , 131   \n340 patients. The target data includes admissions from November 30, 2021, through December 2023,   \n341 totaling $N_{2}=792$ patients. This temporal division allows for the analysis of trends and outcomes   \n342 associated with the evolving pandemic landscape. Prevalence of the 90-day mortality outcome within   \n343 the source data is $14.3\\%$ , reflecting the impact of earlier virus strains and treatment protocols, while   \n344 in the target data, the prevalence is substantially lower at $3.7\\%$ , possibly indicating the effect of   \n345 improved treatments and vaccines, as well as the influence of different virus variants over time.   \n346 For the analysis, we divide $50\\%$ of the source data, comprising 565 individuals, alongside the entirety   \n347 of the target data, to estimate the density ratio. Density ratios of $X,Z$ are estimated using probabilistic   \n348 classification method [12], while the density ratio of $V|X,Z$ is determined through Elastic Net   \n349 regression. For all three methods, the test statistic $T$ is chosen to be $T(\\tilde{X},Z,V,Y)=Y\\cdot\\tilde{X}$ . As   \n350 indicated in Table 1, both csPCR and csPCR(pe) give statistically significant results, whereas the   \n351 IS method does not. The statistically significant results are consistent with biomedical literature.   \n352 For example, through systematic review and meta-analysis, [21] reported that Bamlanivimab is   \n353 effective in reducing the mortality rates of COVID patients. In a cohort study, [16] also found similar   \n354 effectiveness for Nirmatrelvir\u2013ritonavir.   \n355 These results align with our findings from the simulation study and demonstrate that our method has   \n356 increased power compared with the benchmark IS method. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "table", "img_path": "Me5esZTRqW/tmp/99eb7b3262eef2d14bf1409a061b883c0ff6db49d14bd641fb250699fd35ce36.jpg", "table_caption": ["Table 1: $p$ -values of different methods on COVID-19 dataset "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "357 References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "358 [1] Dean A Bodenham and Niall M Adams. A comparison of efficient approximations for a   \n359 weighted sum of chi-squared random variables. Statistics and Computing, 26(4):917\u2013928, 2016.   \n360 [2] Emmanuel Candes, Yingying Fan, Lucas Janson, and Jinchi Lv. Panning for gold:\u2018model  \n361 x\u2019knockoffs for high dimensional controlled variable selection. Journal of the Royal Statistical   \n362 Society Series B: Statistical Methodology, 80(3):551\u2013577, 2018.   \n363 [3] Emerson Y Chen, Vikram Raghunathan, and Vinay Prasad. An overview of cancer drugs   \n364 approved by the us food and drug administration based on the surrogate end point of response   \n365 rate. JAMA Internal Medicine, 179(7):915\u2013921, 2019.   \n366 [4] Jiayuan Huang, Arthur Gretton, Karsten Borgwardt, Bernhard Sch\u00f6lkopf, and Alex Smola.   \n367 Correcting sample selection bias by unlabeled data. Advances in neural information processing   \n368 systems, 19, 2006.   \n369 [5] Adel Javanmard and Mohammad Mehrabi. Pearson chi-squared conditional randomization test.   \n370 arXiv preprint arXiv:2111.00027, 2021.   \n371 [6] Shuangning Li and Molei Liu. Maxway crt: improving the robustness of the model- $\\mathbf{\\nabla}\\cdot\\mathbf{X}$ inference.   \n372 Journal of the Royal Statistical Society Series B: Statistical Methodology, 85(5):1441\u20131470,   \n373 2023.   \n374 [7] Molei Liu, Eugene Katsevich, Lucas Janson, and Aaditya Ramdas. Fast and powerful conditional   \n375 randomization testing via distillation. Biometrika, 109(2):277\u2013293, 2022.   \n376 [8] Molei Liu, Yi Zhang, Katherine P Liao, and Tianxi Cai. Augmented transfer regression learning   \n377 with semi-non-parametric nuisance models. Journal of Machine Learning Research, 24(293):   \n378 1\u201350, 2023.   \n379 [9] Luca Martino, V\u00edctor Elvira, and Francisco Louzada. Effective sample size for importance   \n380 sampling based on discrepancy measures. Signal Processing, 131:386\u2013401, 2017.   \n381 [10] Binh T Nguyen, Bertrand Thirion, and Sylvain Arlot. A conditional randomization test for   \n382 sparse logistic regression in high-dimension. Advances in Neural Information Processing   \n383 Systems, 35:13691\u201313703, 2022.   \n384 [11] Ziang Niu, Abhinav Chakraborty, Oliver Dukes, and Eugene Katsevich. Reconciling   \n385 model- $\\mathbf{\\nabla}\\cdot\\mathbf{X}$ and doubly robust approaches to conditional independence testing. arXiv preprint   \n386 arXiv:2211.14698, 2022.   \n387 [12] Jing Qin. Inferences for case-control and semiparametric two-sample density ratio models.   \n388 Biometrika, 85(3):619\u2013630, 1998.   \n389 [13] James M Robins, Andrea Rotnitzky, and Lue Ping Zhao. Estimation of regression coefficients   \n390 when some regressors are not always observed. Journal of the American statistical Association,   \n391 89(427):846\u2013866, 1994.   \n392 [14] Sheldon M Ross. Simulation. academic press, 2022.   \n393 [15] Donald B Rubin. The calculation of posterior distributions by data augmentation: Comment: A   \n394 noniterative sampling/importance resampling alternative to the data augmentation algorithm for   \n395 creating a few imputations when fractions of missing information are modest: The sir algorithm.   \n396 Journal of the American Statistical Association, 82(398):543\u2013546, 1987.   \n397 [16] Kevin L Schwartz, Jun Wang, Mina Tadrous, Bradley J Langford, Nick Daneman, Valerie   \n398 Leung, Tara Gomes, Lindsay Friedman, Peter Daley, and Kevin A Brown. Population-based   \n399 evaluation of the effectiveness of nirmatrelvir\u2013ritonavir for reducing hospital admissions and   \n400 mortality from covid-19. Cmaj, 195(6):E220\u2013E226, 2023.   \n401 [17] Nikolaj Thams, Sorawit Saengkyongam, Niklas Pfister, and Jonas Peters. Statistical testing under   \n402 distributional shifts. Journal of the Royal Statistical Society Series B: Statistical Methodology,   \n403 85(3):597\u2013663, 2023.   \n404 [18] Kaizheng Wang. Pseudo-labeling for kernel ridge regression under covariate shift. arXiv   \n405 preprint arXiv:2302.10160, 2023.   \n406 [19] Wenshuo Wang and Lucas Janson. A high-dimensional power analysis of the conditional   \n407 randomization test and knockoffs. Biometrika, 109(3):631\u2013645, 2022.   \n408 [20] Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal of   \n409 the Royal Statistical Society Series B: Statistical Methodology, 67(2):301\u2013320, 2005.   \n410 [21] Ling Zuo, Guangyu Ao, Yushu Wang, Ming Gao, and Xin Qi. Bamlanivimab improves   \n411 hospitalization and mortality rates in patients with covid-19: a systematic review and meta  \n412 analysis. The Journal of infection, 84(2):248, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "413 A Proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "414 A.1 Preliminaries ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "415 Throughout this section, we write $S(x_{j},z_{j},v_{j})=s_{j}$ as the label assigned to sample $j$ in Algorithms   \n416 1 and 2, instead of using $\\ell_{j}$ . This notation helps avoid confusion between different label choices.   \n417 Proposition 1. Assume that the conditional independence $X\\perp\\!\\!\\!\\!\\perp Y\\mid Z$ holds on the target population   \n418 $\\tau$ . Let $e(x_{j},z_{j},v_{j})$ denote the density ratio. For any integer $\\ell\\in[1,L].$ , the following holds: ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}_{S}[e(x_{j},z_{j},v_{j})\\cdot\\mathbb{1}\\{S_{T}(x_{j},z_{j},v_{j})=\\ell\\}]=\\frac{1}{L}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "419 Proof of Proposition $^{\\,l}$ . For simplicity, denote $w_{j}=e(X_{j},Z_{j},V_{j})$ and $s_{j}=S_{T}(X_{j},Z_{j},V_{j})$ . ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\delta}[c(X_{j},Z_{j},V_{j})]\\,,\\,[\\delta\\,x_{7}(X_{j},Z_{j},V_{j})-\\ell]\\,]}\\\\ &{\\quad=\\,\\mathbb{E}_{\\delta}\\Bigg[\\mathbb{E}\\,[\\approx_{j}\\cdot\\mathsf{P}(s_{j}\\in\\ell\\,\\vert\\,Y_{j},Z_{j},X_{j},V_{j})]\\,\\Bigg]\\Bigg[Z_{j},X_{j},V_{j}\\Bigg]}\\\\ &{\\quad=\\,\\mathbb{E}_{\\delta}\\Bigg[\\nu_{j}\\cdot\\mathbb{E}\\,\\Big[\\mathsf{P}(s_{j}=\\ell\\,\\vert\\,Y_{j},Z_{j},X_{j},V_{j})\\Big]\\,\\Bigg]\\,\\Bigg[Z_{j},X_{j},V_{j}\\Bigg]}\\\\ &{\\quad=\\,\\Bigg[_{Z_{j},X_{j},V_{j}}\\nu_{j}\\cdot p(Z_{j},X_{j},V_{j})\\cdot\\mathbb{E}\\,[\\mathsf{P}(s_{j}=\\ell\\,\\vert\\,Y_{j},Z_{j},X_{j},V_{j})]\\,\\Bigg]\\,d Z_{j}\\,d X_{j}\\,d V_{j}}\\\\ &{\\quad=\\,\\Bigg[_{Z_{j},X_{j},V_{j}}p(Z_{j},X_{j},V_{j})\\cdot\\mathbb{E}\\,[\\mathsf{P}(s_{j}=\\ell\\,\\vert\\,Y_{j},Z_{j},X_{j},V_{j})]\\,\\,\\mathrm{d}Z_{j}\\,d X_{j}\\,d V_{j}}\\\\ &{\\quad=\\,\\mathbb{E}_{T}\\Bigg[\\mathbb{E}\\,[\\mathsf{P}(s_{j}=\\ell\\,\\vert\\,Y_{j},Z_{j},X_{j},V_{j})]\\,\\Bigg]\\,Z_{j},X_{j},V_{j}\\Bigg]}\\\\ &{\\quad=\\,\\mathbb{E}_{T}\\,\\Big[\\mathsf{P}(s_{j}=\\ell\\,\\vert\\,Y_{j},Z_{j},X_{j},V_{j})]\\,\\Bigg]}\\\\ &{\\quad=\\,\\frac{1}{L}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "420 The last equation follows from results in the non-covariate-shift scenario, e.g., from [5]. ", "page_idx": 12}, {"type": "text", "text": "421 A.2 Proof of Theorem 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "422 A.2.1 Results for Algorithm 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "423 Let $(W_{\\ell})_{\\ell=1,\\dots,L}$ be the sum of weights and $\\hat{\\Omega}_{n}$ be the sample covariance matrix in Algorithm 1. By   \n424 Proposition 1, we have that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}(W_{\\ell})=n\\cdot\\mathbb{E}[w_{j}\\cdot\\mathbb{1}\\{\\ell_{j}=\\ell\\}]=\\frac{n}{L}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "425 Note that the $W_{\\ell}$ \u2019s are sums of i.i.d. random variables, and thus by the Central Limit Theorem, as   \n426 $n\\to\\infty$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbf{A}_{n}=\\sqrt{\\frac{L}{n}}\\left(W_{1}-\\frac{n}{L},W_{2}-\\frac{n}{L},\\cdot\\cdot\\cdot,W_{L}-\\frac{n}{L}\\right)\\overset{d}{\\rightarrow}\\mathcal{N}_{L}(0,\\Omega),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "427 where for any $\\ell,\\ell^{*}\\in\\{1,\\ldots,L\\}$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Omega_{\\ell,\\ell^{*}}=L C\\mathrm{ov}(w_{1}\\mathbb{1}\\left\\{s_{1}=\\ell\\right\\},w_{1}\\mathbb{1}\\left\\{s_{1}=\\ell^{*}\\right\\})=L\\mathbb{E}_{S}\\left[w_{1}^{2}\\cdot\\mathbb{1}\\{s_{1}=\\ell\\}\\mathbb{1}\\{s_{1}=\\ell^{*}\\}\\right]-\\frac{1}{L}}\\\\ &{\\quad\\quad=L\\mathbb{E}_{S}\\left[w_{1}^{2}\\cdot\\mathbb{1}\\{s_{1}=\\ell\\}\\right]\\mathbb{1}\\left\\{\\ell=\\ell^{*}\\right\\}-\\frac{1}{L}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "428 Therefore, ", "page_idx": 12}, {"type": "equation", "text": "$$\nU_{n,L}=\\mathbf{A}_{n}^{\\mathsf{T}}\\mathbf{A}_{n}\\xrightarrow{d}\\chi_{\\Omega}^{2}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "429 Next, we will focus on the variance estimation part. We will show that $\\hat{\\Omega}_{n}\\stackrel{p}{\\longrightarrow}\\Omega$ as $n\\to\\infty$ . For any   \n430 $\\ell,\\ell^{*}\\in\\{1,\\ldots,L\\}$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\Omega}_{n,\\ell,\\ell^{*}}=\\mathbb{1}\\left\\{\\ell=\\ell^{*}\\right\\}\\frac{L}{n}D_{l}-\\frac{1}{L}=\\mathbb{1}\\left\\{\\ell=\\ell^{*}\\right\\}\\frac{L}{n}\\displaystyle\\sum_{j=1}^{n}w_{j}^{2}\\cdot\\mathbb{1}\\{\\ell_{j}=\\ell\\}-\\frac{1}{L}}\\\\ &{\\qquad\\textrm{\\tiny\\frac{p}{\\ell}\\mathbb{1}}\\left\\{\\ell=\\ell^{*}\\right\\}L\\mathbb{E}_{S}\\left[w_{1}^{2}\\cdot\\mathbb{1}\\{s_{1}=\\ell\\}\\right]-\\displaystyle\\frac{1}{L}=\\Omega_{\\ell,\\ell^{*}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "431 Up til now, we have that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{U_{n,L}=\\mathbf{A}_{n}^{\\top}\\mathbf{A}_{n}\\xrightarrow{d}\\chi_{\\Omega}^{2},\\quad\\mathrm{and}\\quad\\theta_{\\hat{\\Omega}_{n},\\alpha}\\xrightarrow{p}\\theta_{\\Omega,\\alpha}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "432 Therefore, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathrm{Algorithm~1~rejects})=\\mathbb{P}(U_{n,L}\\geqslant\\theta_{{\\hat{\\Omega}}_{n},\\alpha})\\rightarrow\\mathbb{P}(\\chi_{\\Omega}^{2}\\geqslant\\theta_{\\Omega,\\alpha})=\\alpha.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "433 A.2.2 Results for Algorithm 2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "434 Let $(\\widetilde{W}_{\\ell})_{\\ell=1,\\dots,L}$ and $\\widetilde{\\Omega}_{n}$ be the sum of weights and the sample covariance matrix in Algorithm 2.   \n435 Let $\\hat{\\gamma}_{\\ell}$ \u0102be the estimatedr coefficient in Algorithm 2.   \n436 Recall that in (7), we have identified the optimal choice of $\\gamma_{\\ell}$ . We will start by working with this   \n437 optimal choice and show that the $\\hat{\\gamma}_{\\ell}$ is close to it. Define ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\widetilde{W}_{\\ell}=\\sum_{j=1}^{n}w_{j}\\left(\\mathbb{1}\\{\\ell_{j}=\\ell\\}-\\gamma_{\\ell}a(X_{j},Z_{j},V_{j})\\right)+n\\gamma_{\\ell}\\mathbb{E}_{\\mathcal{T}}[a(X,Z,V)],}}\\\\ &{}&{K_{\\ell,j}(\\gamma)=w_{j}\\left(\\mathbb{1}\\{\\ell_{j}=\\ell\\}-\\gamma a(X_{j},Z_{j},V_{j})\\right)+\\gamma\\mathbb{E}_{\\mathcal{T}}[a(X,Z,V)],\\mathrm{~and~}}\\\\ &{}&{H_{j}=w_{j}a(X_{j},Z_{j},V_{j})-\\mathbb{E}_{\\mathcal{T}}\\left[a(X,Z,V)\\right].\\ \\ \\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "438 Therefore, we have $\\begin{array}{r}{\\widetilde{W}_{\\ell}=\\sum_{j}K_{\\ell,j}(\\gamma_{\\ell})}\\end{array}$ and $\\begin{array}{r}{\\widetilde{W}_{\\ell}=\\sum_{j}K_{\\ell,j}(\\hat{\\gamma}_{\\ell})=\\widecheck{W}_{\\ell}-(\\hat{\\gamma}_{\\ell}-\\gamma_{\\ell})\\sum_{j}H_{j}.}\\end{array}$ ", "page_idx": 13}, {"type": "text", "text": "439 Note that by (6), $\\mathbb{E}(H_{j})=0$ . By Proposition 1 and (6), we have $\\begin{array}{r}{\\mathbb{E}(\\widecheck{W}_{\\ell})=\\frac{n}{L}}\\end{array}$ . Furthermore, because   \n440 $\\widetilde{W}_{\\ell}$ is a sum of i.i.d. random variables, we have that as $n\\to\\infty$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\check{\\mathbf{A}}_{n}=\\sqrt{\\frac{L}{n}}\\left(\\widetilde{W}_{1}-\\frac{n}{L},\\widetilde{W}_{2}-\\frac{n}{L},\\ldots,\\widecheck{W}_{L}-\\frac{n}{L}\\right)\\overset{d}{\\rightarrow}\\mathcal{N}_{L}(0,\\widecheck{\\Omega}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "441 where for each $\\ell,\\ell^{*}\\in\\{1,\\ldots,L\\}$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\breve{\\Omega}_{\\ell,\\ell^{*}}=L\\mathrm{Cov}\\left\\{K_{\\ell,j}(\\gamma_{\\ell}),K_{\\ell^{*},j}(\\gamma_{\\ell^{*}})\\right\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "442 Therefore, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\check{U}_{n,L}=\\check{\\mathbf{A}}_{n}^{\\mathsf{T}}\\check{\\mathbf{A}}_{n}\\;\\xrightarrow{d}\\chi_{\\tilde{\\Omega}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "443 Next, we will show that the actual statistic $\\tilde{U}_{n,L}$ is close to $\\check{U}_{n,L}$ , and that the estimated variance   \n444 matrix is also close to $\\check{\\Omega}$ . We start with notin gr that the estima toqr $\\hat{\\gamma}\\ell$ from linear regress?ion is close to   \n445 the optimal choice $\\gamma_{\\ell}$ dqefined in (7): by the Central Limit Theorem, ${\\hat{\\gamma}}\\ell=\\gamma_{\\ell}+{\\mathcal{O}}_{p}{\\bar{(}}1/{\\sqrt{n}})$ . And thus ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{W}_{\\ell}=\\displaystyle\\sum_{j=1}^{n}w_{j}\\left(1\\{\\ell_{j}=\\ell\\}-\\hat{\\gamma}_{\\ell}a(X_{j},Z_{j},V_{j})\\right)+n\\hat{\\gamma}_{\\ell}\\mathbb{E}_{T}[a(X,Z,V)]}\\\\ &{\\quad=\\displaystyle\\sum_{j=1}^{n}w_{j}\\mathbb{I}\\{\\ell_{j}=\\ell\\}-\\hat{\\gamma}_{\\ell}\\left(\\displaystyle\\sum_{j=1}^{n}w_{j}a(X_{j},Z_{j},V_{j})-n\\mathbb{E}_{T}[a(X,Z,V)]\\right)}\\\\ &{\\quad=\\displaystyle\\sum_{j=1}^{n}w_{j}\\mathbb{I}\\{\\ell_{j}=\\ell\\}-\\gamma_{\\ell}\\left(\\displaystyle\\sum_{j=1}^{n}w_{j}a(X_{j},Z_{j},V_{j})-n\\mathbb{E}_{T}[a(X,Z,V)]\\right)+\\mathcal{O}_{p}(1)}\\\\ &{\\quad=\\displaystyle\\widetilde{W}_{\\ell}+\\mathcal{O}_{p}(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "446 The second-to-last line is because $\\hat{\\gamma}_{\\ell}=\\gamma_{\\ell}+\\mathcal{O}_{p}(1/\\sqrt{n})$ and the terms inside the parenthesis, $\\textstyle\\sum_{j}H_{j}$ ,   \n447 is a sum of $n$ independent mean-zero random variables. ", "page_idx": 14}, {"type": "text", "text": "448 Therefore, together with (12), by Slusky\u2019s Theorem, we have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf{A}}_{n}=\\sqrt{\\frac{L}{n}}\\left(\\widetilde{W}_{1}-\\frac{n}{L},\\widetilde{W}_{2}-\\frac{n}{L},\\ldots,\\widetilde{W}_{L}-\\frac{n}{L}\\right)\\overset{d}{\\rightarrow}\\mathcal{N}_{L}(0,\\breve{\\Omega}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "449 and thus, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\widetilde{U}_{n,L}=\\widetilde{\\mathbf{A}}_{n}^{\\mathsf{T}}\\widetilde{\\mathbf{A}}_{n}\\stackrel{d}{\\to}\\chi_{\\breve{\\Omega}}^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "450 We will work on sample covariance matrix now. Recall that the sample covariance matrix $\\widetilde\\Omega_{n}\\mathrm{~=~}$   \n451 $\\begin{array}{r}{\\frac{L}{n}(\\mathbf{W}\\mathrm{~-~}\\frac{1}{L}\\,\\cdot\\,\\mathbf{1}_{L\\times n})(\\mathbf{\\dot{W}}\\mathrm{~-~}\\frac{1}{L}\\,\\cdot\\,\\mathbf{1}_{L\\times n})^{\\intercal}}\\end{array}$ , where $\\mathbf{W}_{\\ell,j}\\ =\\ w_{j}\\ \\cdot\\left[\\mathbb{1}\\{\\ell_{j}=\\ell\\}-\\ \\hat{\\gamma}_{\\ell}a(X_{j},Z_{j},V_{j})\\right]\\,+$   \n452 $\\begin{array}{r}{\\hat{\\gamma}_{\\ell}\\mathbb{E}_{\\mathcal{T}}\\left[a(X,Z,V)\\right]=K_{\\ell,j}(\\hat{\\gamma}_{\\ell})}\\end{array}$ . Let\u2019s start with WWT. For any $\\ell,\\ell^{*}\\in\\{1,\\ldots,L\\}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbf{W}\\mathbf{W}^{\\dagger}\\}_{\\ell}\\ell^{*}\\boldsymbol{\\epsilon}^{*}=\\sum_{j}K_{\\ell,j}(\\hat{\\gamma}_{\\ell})K_{\\ell^{*},j}(\\hat{\\gamma}_{\\ell^{*}})}\\\\ &{=\\sum_{j}\\big(K_{\\ell,j}(\\gamma_{\\ell})-(\\hat{\\gamma}_{\\ell}-\\gamma_{\\ell})H_{j}\\big)\\,\\big(K_{\\ell^{*},j}(\\gamma_{\\ell})-(\\hat{\\gamma}_{\\ell^{*}}-\\gamma_{\\ell^{*}})H_{j}\\big)}\\\\ &{=\\sum_{j}K_{\\ell,j}(\\gamma_{\\ell})K_{\\ell^{*},j}(\\gamma_{\\ell^{*}})-(\\hat{\\gamma}_{\\ell}-\\gamma_{\\ell})\\displaystyle\\sum_{j}H_{j}K_{\\ell^{*},j}(\\gamma_{\\ell^{*}})-\\big(\\hat{\\gamma}_{\\ell^{*}}-\\gamma_{\\ell^{*}}\\big)\\displaystyle\\sum_{j}H_{j}K_{\\ell,j}(\\gamma_{\\ell})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,(\\hat{\\gamma}_{\\ell}-\\gamma_{\\ell})(\\hat{\\gamma}_{\\ell^{*}}-\\gamma_{\\ell^{*}})\\displaystyle\\sum_{j}H_{j}^{2}}\\\\ &{=\\sum_{j}K_{\\ell,j}(\\gamma_{\\ell})K_{\\ell^{*},j}(\\gamma_{\\ell^{*}})+\\mathcal{O}_{p}(\\sqrt{n})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "453 Therefore, by the law of large numbers, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{L}{\\i}(\\mathbf{W}\\mathbf{W}^{\\mathsf{T}})_{\\ell,\\ell^{*}}=\\frac{L}{\\i}\\sum_{j}K_{\\ell,j}(\\gamma_{\\ell})K_{\\ell^{*},j}(\\gamma_{\\ell^{*}})+\\mathcal{O}_{p}(1/\\sqrt{n})=L\\mathbb{E}\\left[K_{\\ell,1}(\\gamma_{\\ell})K_{\\ell^{*},1}(\\gamma_{\\ell^{*}})\\right]+\\mathcal{O}_{p}(1/\\sqrt{n}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "454 Similarly, for $\\mathbf{W1^{\\top}}$ , we have that for any $\\ell,\\ell^{*}\\in\\{1,\\ldots,L\\}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n(\\mathbf{W1}^{\\mathsf{T}})_{\\ell,\\ell^{*}}=\\sum_{j}K_{\\ell,j}(\\hat{\\gamma}_{\\ell})=\\sum_{j}K_{\\ell,j}(\\gamma_{\\ell})-(\\hat{\\gamma}_{\\ell}-\\gamma_{\\ell})H_{j}=\\sum_{j}K_{\\ell,j}(\\gamma_{\\ell})+\\mathcal{O}_{p}(\\sqrt{n}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "455 Therefore, again by the law of large numbers, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{L}{N}(\\mathbf{W1}^{\\mathsf{T}})_{\\ell,\\ell^{*}}=\\frac{L}{N}\\sum_{j}K_{\\ell,j}(\\gamma_{\\ell})+\\mathcal{O}_{p}(1/\\sqrt{n})=L\\mathbb{E}\\left[K_{\\ell,j}(\\gamma_{\\ell})\\right]+\\mathcal{O}_{p}(1/\\sqrt{n})=1+\\mathcal{O}_{p}(1/\\sqrt{n}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "456 Combining the above results gives, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\widetilde{\\Omega}_{n,\\ell,\\ell^{*}}=\\cfrac{L}{n}\\left[(\\mathbf{W}-\\cfrac{1}{L}\\cdot\\mathbf{1}_{L\\times n})(\\mathbf{W}-\\cfrac{1}{L}\\cdot\\mathbf{1}_{L\\times n})^{\\top}\\right]_{\\ell,\\ell^{*}}}&{}\\\\ {=L\\mathbb{E}\\left[K_{\\ell,1}(\\gamma_{\\ell})K_{\\ell^{*},1}(\\gamma_{\\ell^{*}})\\right]-L\\mathbb{E}\\left[K_{\\ell,j}(\\gamma_{\\ell})\\right]\\mathbb{E}\\left[K_{\\ell^{*},j}(\\gamma_{\\ell^{*}})\\right]+\\mathcal{O}_{p}(1/\\sqrt{n})}\\\\ &{=L\\operatorname{Cov}\\left[K_{\\ell,1}(\\gamma_{\\ell}),K_{\\ell^{*},1}(\\gamma_{\\ell^{*}})\\right]+\\mathcal{O}_{p}(1/\\sqrt{n})}\\\\ &{=\\check{\\Omega}_{\\ell,\\ell^{*}}+\\mathcal{O}_{p}(1/\\sqrt{n}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "457 Therefore, $\\tilde{\\Omega}_{n}\\stackrel{p}{\\cal{S}}\\stackrel{\\vee}{\\Omega}$ . ", "page_idx": 14}, {"type": "text", "text": "458 To summar irze, we  qhave that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\widetilde{U}_{n,L}=\\widetilde{\\mathbf{A}}_{n}^{\\mathsf{T}}\\widetilde{\\mathbf{A}}_{n}\\overset{d}{\\to}\\chi_{\\breve{\\Omega}}^{2},\\quad\\mathrm{and}\\quad\\theta_{\\widetilde{\\Omega}_{n},\\alpha}\\overset{p}{\\to}\\theta_{\\breve{\\Omega},\\alpha}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "459 Therefore, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathrm{Algorithm~2~rejects})=\\mathbb{P}(\\tilde{U}_{n,L}\\geqslant\\theta_{\\tilde{\\Omega}_{n},\\alpha})\\rightarrow\\mathbb{P}(\\chi_{\\tilde{\\Omega}}^{2}\\geqslant\\theta_{\\tilde{\\Omega},\\alpha})=\\alpha.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "460 A.3 Proof of Theorem 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "461 Similar to the proof of Theorem 1, we define ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\widetilde{W}_{\\ell}=\\sum_{j=1}^{n}w_{j}\\left(\\mathbb{1}\\{\\ell_{j}=\\ell\\}-\\gamma_{\\ell}a(X_{j},Z_{j},V_{j})\\right)+n\\gamma_{\\ell}\\mathbb{E}_{\\mathcal{T}}[a(X,Z,V)],}}\\\\ &{}&{K_{\\ell,j}(\\gamma)=w_{j}\\left(\\mathbb{1}\\{\\ell_{j}=\\ell\\}-\\gamma a(X_{j},Z_{j},V_{j})\\right)+\\gamma\\mathbb{E}_{\\mathcal{T}}[a(X,Z,V)],\\mathrm{~and~}}\\\\ &{}&{H_{j}=w_{j}a(X_{j},Z_{j},V_{j})-\\mathbb{E}_{\\mathcal{T}}\\left[a(X,Z,V)\\right].\\ \\ \\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "462 Therefore, we have $\\begin{array}{r}{\\widetilde{W}_{\\ell}=\\sum_{j}K_{\\ell,j}(\\gamma_{\\ell})}\\end{array}$ and $\\begin{array}{r}{\\widetilde{W}_{\\ell}=\\sum_{j}K_{\\ell,j}(\\hat{\\gamma}_{\\ell})=\\widecheck{W}_{\\ell}-(\\hat{\\gamma}_{\\ell}-\\gamma_{\\ell})\\sum_{j}H_{j}.}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "463 We know from the literature that $\\gamma_{\\ell}$ is the optimal choice of $\\gamma$ and thus $\\mathrm{Var}\\left[\\widetilde{W}_{\\ell}\\right]\\leqslant\\mathrm{Var}\\left[W_{\\ell}\\right]$ . We   \n464 will then move on to show that $\\mathrm{Var}\\left[\\widetilde{W}_{\\ell}\\right]$ is close to $\\mathrm{Var}\\left[\\widetilde{W}_{\\ell}\\right]$ and thus asym|ptotically no greater   \n465 than Var $[W_{\\ell}]$ . ", "page_idx": 15}, {"type": "text", "text": "466 To this end, note that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Var}\\left[\\widetilde W_{\\ell}\\right]=\\mathrm{Var}\\left[\\widetilde W_{\\ell}-(\\hat{\\gamma}\\ell-\\gamma_{\\ell})\\underset{-1}{\\sum}H_{j}\\right]}\\\\ &{\\phantom{=}=\\mathrm{Var}\\left[\\widetilde W_{\\ell}\\right]+2\\,\\mathrm{Cov}\\left[\\widetilde W_{\\ell},(\\hat{\\gamma}_{\\ell}-\\gamma_{\\ell})\\underset{j}{\\sum}H_{j}\\right]+\\mathrm{Var}\\left[(\\hat{\\gamma}_{\\ell}-\\gamma_{\\ell})\\underset{j}{\\sum}H_{j}\\right]}\\\\ &{\\phantom{=}\\leqslant\\mathrm{Var}\\left[\\widetilde W_{\\ell}\\right]+2\\sqrt{\\mathrm{Var}\\left[\\widetilde W_{\\ell}\\right]}\\sqrt{\\mathbb{E}\\left[\\left((\\hat{\\gamma}_{\\ell}-\\gamma_{\\ell})\\underset{j}{\\sum}H_{j}\\right)^{2}\\right]}+\\mathbb{E}\\left[\\left((\\hat{\\gamma}_{\\ell}-\\gamma_{\\ell})\\underset{j}{\\sum}H_{j}\\right)^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "467 But we also know from the proof of Theorem 1 that $\\hat{\\gamma}_{\\ell}-\\gamma_{\\ell}\\overset{p}{\\to}0$ . Then, because of the bounded   \n468 fourth moment assumption, by the Dominated Convergence Theorem, we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\mathbb{E}\\left[\\left((\\hat{\\gamma}\\ell-\\gamma_{\\ell})\\underset{j}{\\sum}\\bar{H}_{j}\\right)^{2}\\right]\\rightarrow0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "469 Therefore, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\frac{1}{n}\\left(\\mathrm{Var}\\left[\\widetilde{W}_{\\ell}\\right]-\\mathrm{Var}\\left[\\widetilde{W}_{\\ell}\\right]\\right)\\leqslant0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "470 Finally, we note that Var $[W_{l}]=\\Omega(n)$ , and hence ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}(\\mathrm{Var}\\left[\\widetilde W_{l}\\right]/\\mathrm{Var}\\left[W_{l}\\right])\\leqslant1.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "471 B Additional Simulation Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "472 B.1 Running time ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "473 All experiments run on a Macbook Pro 2022 M2. ", "page_idx": 15}, {"type": "text", "text": "474 Artificial dataset: Regarding running time for one iteration including density ratio estimation and   \n475 $X|Z$ model fitting (on average), csPCR took 5.12s, csPCR(pe) took 14.95s, IS method took 1.5s,   \n476 PCR took 1.25s.   \n477 Real-world application: Regarding running time for one test procedure, csPCR took 3.41s,   \n478 csPCR(pe) took 11.32s, IS method took 0.81s. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "479 B.2 Finding optimal hyperparameter $L$ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "480 We find the optimal $L$ value for the testing algorithm by performing numerical simulations, evaluating   \n481 its Type-I error control and power. We adopt the same numerical simulation setup as in the main text   \n482 Section 4. We first choose $a_{S}=1$ and $\\boldsymbol{a}_{T}=0$ and also fix $\\beta=1$ to compare the Type-I error rate   \n483 for different choice of $L$ of the csPCR method. We perform experiments with both true density ratio   \n484 and estimated density ratio. The results are shown in Table 2.   \n485 We also test the power of the csPCR and csPCR(pe) method with different choices of $L$ value. We   \n486 choose $a_{S}=0$ and $a\\tau=2$ and fix $\\beta=2$ .   \n487 As Table 2 and Figure 5 shows, as $L$ value increases, the csPCR method become more conservative   \n488 with more tight Type-I error control and lower power. We can observe that when we set $L=3$ , the   \n489 csPCR method can achieve most stable Type-I error rate control and also highest power empirically.   \n490 Therefore, in our simulation experiments and real world data experiments, we fix $L=3$ . ", "page_idx": 15}, {"type": "table", "img_path": "Me5esZTRqW/tmp/ff874a889440152a4c2c214b9c6f1cd4e44597388075c9445369d7b1287b5f84.jpg", "table_caption": ["Table 2: Type-I Error Rates at Different Levels of L of csPCR Method "], "table_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "Me5esZTRqW/tmp/c3cd11cd901fa495bbb0a8a932523a137c2406dc697926e67575930f73a5efdb.jpg", "img_caption": ["Figure 5: Comparison of statistical power of the three methods as the the parameter $L$ varies. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "491 B.3 Role of effective sample size ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "492 We notice a series of work in measuring the effective sample size (ESS) of importance weight or   \n493 sampling in the statistical computation literature, e.g., [Martino, et al, 2017] and others. Among them,   \n494 one of the most common ways is to use the ratio $\\begin{array}{r}{n_{e f f}=\\frac{(\\sum_{i=1}w_{i})^{2}}{\\sum_{i=1}w_{i}^{2}}}\\end{array}$ to approximate the ESS. When   \n495 the covariate shift between the source and target becomes s\u0159tronger, the variance of the importance   \n496 weight $w_{i}$ tends to be large and $n_{e f f}$ will become smaller, which can result in lower power. Our   \n497 power enhancement method based on control variate could potentially alleviate this issue with   \n498 properly specified control functions.   \n500 In the simulation study, we varied only $\\mu_{z}$ , the mean of the confounding variables $Z_{T}$ . A higher $\\mu_{z}$   \n501 signifies a stronger covariate shift between the source and target populations. From Figure 6 , it is   \n502 evident that as $\\mu_{z}$ increases, the Effective Sample Size (ESS) required significantly decreases, while   \n503 the power of the csPCR method concurrently declines. These results suggest that increasing covariate   \n504 shift leads to a reduction in ESS and a corresponding decrease in statistical power. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "505 B.4 Instability of the Importance Resampling (IS) method ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "506 In this section, we will use numerical simulations to ilustrate that the performance if the IS method   \n507 is subject to the resample size heavily. IS method performs resampling without replacement and   \n508 typically has to sample a much smaller subset (theoretically, in the order of $o({\\sqrt{n}}))$ ) of the source data   \n509 to approximate the target. Consequently, the power of IS is substantially lower than our approach. If   \n510 the resample size of IS is overly increased, it may fail to control the Type-I error due to excessive   \n511 similarity between the resampled data and the original source data.   \n513 To further illustrate, we conducted additional experiments with varied resample sizes in IS to assess   \n514 its effect on Type-I error control and power. From Figure7. one can observe that IS starts to show   \n515 high Type-I error inflation when its resample size increases to 400 but still shows much lower power ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "image", "img_path": "Me5esZTRqW/tmp/7b24f3a0b41ff30ee4838c5c381718a11d2c5a1aca19634d244c7788c28cbfe9.jpg", "img_caption": ["Figure 6: The left panel shows the comparison of statistical power of csPCR and csPCR(pe) method as the covariate shift gets stronger. The right panel illustrates how the Effective Sample Size(ESS) changes as covariate shift scale becomes larger. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "516 (by around 0.4) than our method with this resample size (or even larger ones). This indicates that our method achieves better statistical efficiency than IS (DRPL). ", "page_idx": 17}, {"type": "image", "img_path": "Me5esZTRqW/tmp/a4a530d100617af6d382ace681889b31cbcc0bda0d9c1450256ee00e0a144ace.jpg", "img_caption": ["Figure 7: Detailed comparison of Type-I error rate and power of csPCR and the IS method. With the source sample size $n_{s}=1000$ , we gradually increase the resample size for the IS method from 100 to 1000. The two horizontal lines represent the Type-I error rate and power, respectively, of the csPCR and csPCR(pe) methods (they do not change with the tuning of IS). "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "517 ", "page_idx": 17}, {"type": "text", "text": "518 B.5 Choice of test statistic ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "519 In this section we explore the effect of test statistics on the algorithm performance. The main   \n520 principle of choosing the test statistic is to characterize the conditional dependency between $X$ and   \n521 $Y$ under the alternative hypothesis. The test statistic $Y X$ may not be the optimal choice and that   \n522 using $(Y-{\\hat{E}}[Y\\mid Z])(X-E[X\\mid Z])$ could remove the confounding effect of $Z$ .   \n523   \n524 Inspired by this, we used $Y(X-E[X|Z])$ as the test statistic to conduct additional simulations. As   \n525 illustrated in Figure8, we find that $\\dot{Y}(\\dot{X}-E[X|Z])$ and $Y X$ produce nearly the same power for   \n526 both csPCR and csPCR(pe) with the change of effect size. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "527 C Real-World Application ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "528 The specific medication indicated by the treatment variable $X$ includes Ritonavir, Bamlanivimab,   \n529 Casirivimab-Imdevimab, Remdesivir, Ritonavir Nirmatrelvir, Sotrovimab, Bamlanivimab Etesevimab.   \n530 For simplicity, $X=1$ indicates any of these specific medication and $X=0$ otherwise. ", "page_idx": 17}, {"type": "image", "img_path": "Me5esZTRqW/tmp/961f488fd745ad6f1e2f6023ec6964e4dcb218abf43e36c939937a1385ed0f85.jpg", "img_caption": ["Figure 8: Power against effect size for csPCR and csPCR(pe) with two different test statistics $X Y$ and $(X-\\operatorname{E}[X\\mid Z])Y$ . We observe that the power is very similar with the two different test statistics. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "531 C.1 Different outcome ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "532 In our real data experiment part, the outcome variable $Y$ is defined as mortality within 90 days since 533 hospital admission due to COVID-19. In addition, we also analyzed mortality within 30 days since 534 hospital admission. As shown in Table 3, both csPCR and csPCR(pe) methods give significant results, aligning with biomedical literature. ", "page_idx": 18}, {"type": "table", "img_path": "Me5esZTRqW/tmp/bcff2de6f17170d37e48526aa6be9744c3c47a94cf53c7941febabd39bd77454.jpg", "table_caption": ["Table 3: $p$ -values of different methods on COVID-19 dataset (mortality 30 "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "535 ", "page_idx": 18}, {"type": "text", "text": "536 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper. \u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. \u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We examine the performance of the algorithms under model misspecification in Section 4. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "6 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "87 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n88 a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "603 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "604 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n605 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n606 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Justification: Details of the simulation studies and real-data application are included in Sections 4, 5 and Appendix B.2. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "642 5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "643 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n644 tions to faithfully reproduce the main experimental results, as described in supplemental   \n645 material?   \n646 Answer: [Yes]   \n647 Justification: Replication code for our simulation studies is submitted as supplementary   \n648 material. It will also be made publicly available on GitHub once our paper is accepted.   \n649 The COVID data set used for the real example in our paper is not publicly available due to   \n650 privacy constraints.   \n651 Guidelines:   \n652 \u2022 The answer NA means that paper does not include experiments requiring code.   \n653 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n654 public/guides/CodeSubmissionPolicy) for more details.   \n655 \u2022 While we encourage the release of code and data, we understand that this might not be   \n656 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n657 including code, unless this is central to the contribution (e.g., for a new open-source   \n658 benchmark).   \n659 \u2022 The instructions should contain the exact command and environment needed to run to   \n660 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n661 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n662 \u2022 The authors should provide instructions on data access and preparation, including how   \n663 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n664 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n665 proposed method and baselines. If only a subset of experiments are reproducible, they   \n666 should state which ones are omitted from the script and why.   \n667 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n668 versions (if applicable).   \n669 \u2022 Providing as much information as possible in supplemental material (appended to the   \n670 paper) is recommended, but including URLs to data and code is permitted.   \n671 6. Experimental Setting/Details   \n672 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n673 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n674 results?   \n675 Answer: [Yes]   \n676 Justification: Details of the simulation studies and real-data application are included in   \n677 Sections 4, 5 and Appendix B.2.   \n678 Guidelines:   \n679 \u2022 The answer NA means that the paper does not include experiments.   \n680 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n681 that is necessary to appreciate the results and make sense of them.   \n682 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n683 material.   \n684 7. Experiment Statistical Significance   \n685 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n686 information about the statistical significance of the experiments?   \n687 Answer: [Yes]   \n688 Justification: Type-I errors, power, and p-values are provided in Sections 4 and 5.   \n689 Guidelines:   \n690 \u2022 The answer NA means that the paper does not include experiments.   \n691 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n692 dence intervals, or statistical significance tests, at least for the experiments that support   \n693 the main claims of the paper.   \n694 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n695 example, train/test split, initialization, random drawing of some parameter, or overall   \n696 run with given experimental conditions).   \n697 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n698 call to a library function, bootstrap, etc.)   \n699 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n700 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n701 of the mean.   \n702 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n703 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n704 of Normality of errors is not verified.   \n705 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n706 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n707 error rates).   \n708 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n709 they were calculated and reference the corresponding figures or tables in the text.   \n710 8. Experiments Compute Resources   \n711 Question: For each experiment, does the paper provide sufficient information on the com  \n712 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n713 the experiments?   \n714 Answer: [Yes]   \n715 Justification: We record relevant information in Appendix B.1.   \n716 Guidelines:   \n717 \u2022 The answer NA means that the paper does not include experiments.   \n718 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n719 or cloud provider, including relevant memory and storage.   \n720 \u2022 The paper should provide the amount of compute required for each of the individual   \n721 experimental runs as well as estimate the total compute.   \n722 \u2022 The paper should disclose whether the full research project required more compute   \n723 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n724 didn\u2019t make it into the paper).   \n725 9. Code Of Ethics   \n726 Question: Does the research conducted in the paper conform, in every respect, with the   \n727 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n728 Answer: [Yes]   \n729 Justification: Yes, our research conforms to the NeurIPS Code of Ethics in every respect,   \n730 including fairness, transparency, privacy, and social responsibility.   \n731 Guidelines:   \n732 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n733 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n734 deviation from the Code of Ethics.   \n735 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n736 eration due to laws or regulations in their jurisdiction).   \n737 10. Broader Impacts   \n738 Question: Does the paper discuss both potential positive societal impacts and negative   \n739 societal impacts of the work performed?   \n740 Answer: [Yes]   \n741 Justification: We have discussed the impact of the paper on the fields of healthcare and   \n742 social sciences.   \n743 Guidelines:   \n744 \u2022 The answer NA means that there is no societal impact of the work performed.   \n745 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n746 impact or why the paper does not address societal impact.   \n747 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n748 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n749 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n750 groups), privacy considerations, and security considerations.   \n751 \u2022 The conference expects that many papers will be foundational research and not tied   \n752 to particular applications, let alone deployments. However, if there is a direct path to   \n753 any negative applications, the authors should point it out. For example, it is legitimate   \n754 to point out that an improvement in the quality of generative models could be used to   \n755 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n756 that a generic algorithm for optimizing neural networks could enable people to train   \n757 models that generate Deepfakes faster.   \n758 \u2022 The authors should consider possible harms that could arise when the technology is   \n759 being used as intended and functioning correctly, harms that could arise when the   \n760 technology is being used as intended but gives incorrect results, and harms following   \n761 from (intentional or unintentional) misuse of the technology.   \n762 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n763 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n764 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n765 feedback over time, improving the efficiency and accessibility of ML).   \n766 11. Safeguards   \n767 Question: Does the paper describe safeguards that have been put in place for responsible   \n768 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n769 image generators, or scraped datasets)?   \n770 Answer: [NA]   \n771 Justification: We believe the paper poses no such risks.   \n772 Guidelines:   \n773 \u2022 The answer NA means that the paper poses no such risks.   \n774 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n775 necessary safeguards to allow for controlled use of the model, for example by requiring   \n776 that users adhere to usage guidelines or restrictions to access the model or implementing   \n777 safety filters.   \n778 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n779 should describe how they avoided releasing unsafe images.   \n780 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n781 not require this, but we encourage authors to take this into account and make a best   \n782 faith effort.   \n783 12. Licenses for existing assets   \n784 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n785 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n786 properly respected?   \n787 Answer: [Yes]   \n788 Justification: We have cited the relevant papers and packages.   \n789 Guidelines:   \n790 \u2022 The answer NA means that the paper does not use existing assets.   \n791 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n792 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n793 URL.   \n794 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n795 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n796 service of that source should be provided.   \n797 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n798 package should be provided. For popular datasets, paperswithcode.com/datasets   \n799 has curated licenses for some datasets. Their licensing guide can help determine the   \n800 license of a dataset.   \n801 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n802 the derived asset (if it has changed) should be provided.   \n803 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n804 the asset\u2019s creators.   \n805 13. New Assets   \n806 Question: Are new assets introduced in the paper well documented and is the documentation   \n807 provided alongside the assets?   \n808 Answer: [Yes]   \n809 Justification: We have provided detailed documentation of the newly proposed algorithm.   \n810 Guidelines:   \n811 \u2022 The answer NA means that the paper does not release new assets.   \n812 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n813 submissions via structured templates. This includes details about training, license,   \n814 limitations, etc.   \n815 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n816 asset is used.   \n817 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n818 create an anonymized URL or include an anonymized zip file.   \n819 14. Crowdsourcing and Research with Human Subjects   \n820 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n821 include the full text of instructions given to participants and screenshots, if applicable, as   \n822 well as details about compensation (if any)?   \n823 Answer: [NA]   \n824 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n825 Guidelines:   \n826 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n827 human subjects.   \n828 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n829 tion of the paper involves human subjects, then as much detail as possible should be   \n830 included in the main paper.   \n831 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n832 or other labor should be paid at least the minimum wage in the country of the data   \n833 collector.   \n834 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n835 Subjects   \n836 Question: Does the paper describe potential risks incurred by study participants, whether   \n837 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n838 approvals (or an equivalent approval/review based on the requirements of your country or   \n839 institution) were obtained?   \n840 Answer: [NA]   \n841 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n842 Guidelines:   \n843 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n844 human subjects.   \n845 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n846 may be required for any human subjects research. If you obtained IRB approval, you   \n847 should clearly state this in the paper. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]