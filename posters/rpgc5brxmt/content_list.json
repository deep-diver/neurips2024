[{"type": "text", "text": "Interaction-Force Transport Gradient Flows ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Egor Gladin   \nHumboldt University of Berlin   \nBerlin, Germany   \n& HSE University   \negorgladin@yandex.ru ", "page_idx": 0}, {"type": "text", "text": "Pavel Dvurechensky Weierstrass Institute for Applied Analysis and Stochastics Berlin, Germany pavel.dvurechensky@wias-berlin.de ", "page_idx": 0}, {"type": "text", "text": "Alexander Mielke Humboldt University of Berlin & WIAS Berlin, Germany alexander.mielke@wias-berlin.de ", "page_idx": 0}, {"type": "text", "text": "Jia-Jie Zhu\u2217 Weierstrass Institute for   \nApplied Analysis and Stochastics Berlin, Germany   \njia-jie.zhu@wias-berlin.de ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper presents a new gradient flow dissipation geometry over non-negative and probability measures. This is motivated by a principled construction that combines the unbalanced optimal transport and interaction forces modeled by reproducing kernels. Using a precise connection between the Hellinger geometry and the maximum mean discrepancy (MMD), we propose the interaction-force transport (IFT) gradient flows and its spherical variant via an infimal convolution of the Wasserstein and spherical MMD tensors. We then develop a particle-based optimization algorithm based on the JKO-splitting scheme of the mass-preserving spherical IFT gradient flows. Finally, we provide both theoretical global exponential convergence guarantees and improved empirical simulation results for applying the IFT gradient flows to the sampling task of MMD-minimization. Furthermore, we prove that the spherical IFT gradient flow enjoys the best of both worlds by providing the global exponential convergence guarantee for both the MMD and KL energy. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Optimal transport (OT) distances between probability measures, including the earth mover\u2019s distance [Werman et al., 1985, Rubner et al., 2000] and Monge-Kantorovich or Wasserstein distance [Villani, 2008], are one of the cornerstones of modern machine learning as they allow performing a variety of machine learning tasks, e.g., unsupervised learning [Arjovsky et al., 2017, Bigot et al., 2017], semi-supervised learning [Solomon et al., 2014], clustering [Ho et al., 2017], text classification [Kusner et al., 2015], image retrieval, clustering and classification [Rubner et al., 2000, Cuturi, 2013, Sandler and Lindenbaum, 2011], and distributionally robust optimization [Sinha et al., 2020, Mohajerin Esfahani and Kuhn, 2018]. Many recent works in machine learning adopted the techniques from PDE gradient flows over optimal transport geometries and interacting particle systems for inference and sampling tasks. Those tools not only add new interpretations to the existing algorithms, but also provide a new perspective on designing new algorithms. ", "page_idx": 0}, {"type": "text", "text": "For example, the classical Bayesian inference framework minimizes the Kulback-Leibler divergence towards a target distribution $\\pi$ . From the optimization perspective, this can be viewed as solving ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mu\\in A\\subset\\mathcal{P}}\\big\\{F(\\mu):=\\mathrm{D}_{\\mathrm{KL}}(\\mu|\\pi)\\big\\},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $A$ is a subset of the space of probability measures $\\mathcal{P}$ , e.g., the Gaussian family. The Wasserstein gradient flow of the KL gives the FokkerPlanck equation, which can be simulated using the Langevin SDE for MCMC. Beyond the KL, many researchers following Arbel et al. [2019] advocated using the squared MMD instead as the driving energy for the Wasserstein gradient flows for sampling. However, in contrast to the KL setting, there is little sound convergence analysis for the MMDminimization scheme like the celebrated Bakry\u00c9mery Theorem. Furthermore, it was shown, e.g., in [Korba et al., 2021], that Arbel et al. [2019]\u2019s algorithm suffers a few practical drawbacks. For example, their particles tend to collapse around the mode or get stuck at local minima, and the algorithm requires a heuristic noise injection strategy that is tuned over the iterations; see Figure 1 and $\\S4$ for illustrations. Subsequently, many such as Carrillo et al. [2019], Chewi et al. [2020], Korba et al. [2021], Glaser et al. [2021], Craig et al. [2023], Hertrich et al. [2023], Neumayer et al. [2024] proposed modified energies to be used in the Wasserstein gradient flows. In contrast, this paper does not propose ", "page_idx": 1}, {"type": "image", "img_path": "rPgc5brxmT/tmp/4da850ed0bbcc7aa5871b5273857d8d4eead227702b3cc52725e09d159eb79e5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: (Left) Wasserstein flow of the MMD energy [Arbel et al., 2019]. Some particles get stuck at points away from the target. (Right) IFT gradient flow (this paper) of the MMD energy. Particle mass is teleported to close to the target, avoiding local minima. Hollow circles indicate particles with zero mass. The red dots are the initial particles, and the green dots are the target distribution. See $\\S4$ for more details. ", "page_idx": 1}, {"type": "text", "text": "new energy objectives. Instead, we propose a new gradient flow geometry \u2013 the IFT gradient flows. To summarize, our main contributions are: ", "page_idx": 1}, {"type": "text", "text": "1. We propose the interaction-force transport (IFT) gradient flow geometry over non-negative measures and spherical IFT over probability measures, constructed from the first principles of the reaction-diffusion type equations, previously studied in the context of the HellingerKantorovich (Wasserstein-Fisher-Rao) distance and gradient flows. It was first studied by three groups including Chizat et al. [2018, 2019], Liero et al. [2018], Kondratyev et al. [2016], Gallou\u00ebt and Monsaingeon [2017]. Our IFT gradient flow is based on the inf-convolution of the Wasserstein and the newly constructed spherical MMD Riemannian metric tensors. This new unbalanced gradient flow geometry allows teleporting particle mass in addition to transportation, which avoids the flow getting stuck at local minima; see Figure 1 for an illustration.   \n2. We provide theoretical analysis such as the global exponential decay of energy functionals via the Polyak-\u0141ojasiewicz type functional inequalities. As an application, we provide the first global exponential convergence analysis of IFT for both the MMD and KL energy functionals. That is, the IFT gradient flow enjoys the best of both worlds.   \n3. We provide a new algorithm for the implementation of the IFT gradient flow. We then empirically demonstrate the use of the IFT gradient flow for the MMD inference task. Compared to the original MMD-energy-flow algorithm of Arbel et al. [2019], IFT flow does not suffer issues such as the collapsing-to-mode issue. Leveraging the first-principled spherical IFT gradient flow, our method does not require a heuristic noise injection that is commonly tuned over the iterations in practice; see [Korba et al., 2021] for a discussion. Our method can also be viewed as addressing a long-standing issue of the kernel-mean embedding methods [Smola et al., 2007, Muandet et al., 2017, Lacoste-Julien et al., 2015] for optimizing the support of distributions. ", "page_idx": 1}, {"type": "text", "text": "Notation We use the notation ${\\mathcal{P}}(X),{\\mathcal{M}}^{+}(X)$ to denote the space of probability and non-negative measures on the closed, bounded, (convex) set $X\\subset\\mathbb{R}^{d}$ . The base space symbol $\\mathbf{\\deltaX}$ is often dropped if there is no ambiguity in the context. We note also that many of our results hold for $X=\\mathbb{R}^{d}$ . In this paper, the first variation of a functional $F$ at $\\mu\\in\\mathcal{M}^{+}$ is defined as a function $\\frac{\\delta F}{\\delta\\mu}[\\mu]$ such that $\\begin{array}{r}{\\frac{\\mathrm{\\boldmath~d~\\,~}}{\\mathrm{\\boldmath~d~}\\epsilon}F(\\mu+\\epsilon\\boldsymbol{\\cdot}\\boldsymbol{\\upsilon})|_{\\epsilon=0}\\,=\\,\\int\\frac{\\delta F}{\\delta\\mu}[\\mu](\\boldsymbol{x})\\ \\mathrm{d}\\boldsymbol{\\upsilon}(\\boldsymbol{x})}\\end{array}$ for any valid perturbation in measure $v$ such that $\\mu+\\epsilon\\cdot v\\in\\mathcal{M}^{+}$ when working with gradient flows over $\\mathcal{M}^{+}$ and $\\mu+\\epsilon\\cdot v\\in\\mathcal P$ over $\\mathcal{P}$ . We often omit the time index $t$ to lessen the notational burden, e.g., the measure at time $t$ , $\\mu(t,\\cdot)$ , is written as $\\mu$ . The infimal convolution (inf-convolution) of two functions $f,g$ on Banach spaces is defined as $(f\\sqcup g)(x)=\\operatorname*{inf}_{y}\\left\\{f(y)+g(x-y)\\right\\}$ . In formal calculation, we often use measures and their density interchangeably, i.e., $\\int\\,f\\cdot\\mu$ means the integral w.r.t. the measure $\\mu$ . For a rigorous generalization of flows over continuous measures to discrete measures, see [Ambrosio et al., 2005]. $\\bar{\\nabla}_{2}k(\\cdot,\\cdot)$ denotes the gradient w.r.t. the second argument of the kernel. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Gradient flows of probability measures for learning and inference ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Gradient flows are powerful tools originated from the field of PDE. The intuition can be easily seen from the perspective of optimization as solving the variational problem ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mu\\in A\\subset\\mathcal{M}^{+}({\\pmb x})}F(\\mu)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "using a \u201ccontinuous-time version\u201d of gradient descent, over a suitable metric space and, in particular, Riemannian manifold. Since the seminal works by Otto [1996] and colleagues, one can view many PDEs as gradient flows over the aforementioned Wasserstein metric space, canonically denoted as $(\\mathcal{P}_{2}(X),\\bar{W}_{2})$ ; see [Villani, 2008, Santambrogio, 2015] for a comprehensive introduction. ", "page_idx": 2}, {"type": "text", "text": "Different from a standard OT problem, a gradient flow solution traverses along the path of the fastest dissipation of the energy $F$ allowed by the corresponding geometry. In this paper, we are only concerned with the geometries with a (pseudo-)Riemannian structure, such as the Wasserstein, (spherical) Hellinger or Fisher-Rao geometries. In such cases, a formal Otto calculus can be developed to greatly simplify the calculations. For example, the Wasserstein Onsager operator (which is the inverse of the Riemannian metric tensor) $\\mathbb{K}_{W}\\!\\big(\\rho):T_{\\rho}^{*}\\mathcal{M}^{+}\\to T_{\\rho}\\mathcal{M}^{+},\\stackrel{\\cdot}{\\xi}\\mapsto-\\mathrm{div}(\\rho\\nabla\\xi).$ , where $T_{\\rho}\\mathcal{M}^{+}$ is the tangent plane of $\\mathcal{M}^{+}$ at $\\rho$ and $T_{\\rho}^{\\ast}\\mathcal{M}_{-}^{+}$ the cotangent plane. Using this notation, a Wasserstein gradient flow equation of some energy $F$ can be written as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\dot{\\mu}=-\\mathbb{K}_{W}(\\mu)\\frac{\\delta F}{\\delta\\mu}=\\mathrm{div}(\\mu\\nabla\\frac{\\delta F}{\\delta\\mu}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In essence, many machine learning applications are about making different choices of the energy $F$ in (2), e.g., the KL, $\\chi^{2}$ -divergence, or MMD. However, Wasserstein and its flow equation (2) are by no means the only meaningful geometry for gradient flows. One major development in the field is the Hellinger-Kantorovich a.k.a. the Wasserstein-Fisher-Rao (WFR) gradient flow. The WFR gradient flow equation is given by the reaction-diffusion equation, for some scaling coefficients $\\alpha,\\beta>0$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\dot{\\boldsymbol{u}}=\\boldsymbol{\\alpha}\\cdot\\mathrm{div}(\\boldsymbol{u}\\nabla\\frac{\\delta\\boldsymbol{F}}{\\delta\\boldsymbol{u}})-\\beta\\boldsymbol{u}\\cdot\\frac{\\delta\\boldsymbol{F}}{\\delta\\boldsymbol{u}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "A few recent works have applied WFR to sampling and inference [Yan et al., 2024, Lu et al., 2019] by choosing the energy functional to be the KL divergence. ", "page_idx": 2}, {"type": "text", "text": "2.2 Reproducing kernel Hilbert space and MMD ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this paper, we refer to a bi-variate function $k:X\\times X\\to\\mathbb{R}$ as a symmetric positive definite kernel if $k$ is symmetric and, for all $n\\,\\in\\,\\mathbb{N},\\alpha_{1},\\dotsc,\\alpha_{n}\\,\\in\\,\\mathbb{R}$ and all $x_{1},\\ldots,x_{n}\\,\\in\\,X$ , we have $\\begin{array}{r}{\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\alpha_{i}\\dot{\\alpha}_{j}k\\left(x_{j},x_{i}\\right)\\geq0}\\end{array}$ . $k$ is a reproducing kernel if it satisfies the reproducing property, i.e., for all $x\\in X$ and all functions in a Hilbert space $f\\in\\mathcal H$ , we have $f(\\boldsymbol{x})=\\langle f,k(\\cdot,\\boldsymbol{x})\\rangle_{\\mathcal{H}}$ . Furthermore, the space $\\mathcal{H}$ is an RKHS if the Dirac functional $\\delta_{x}:\\mathcal{H}\\mapsto\\mathbb{R}$ , $\\delta_{x}(f):=f(x)$ is continuous. It can be shown that there is a one-to-one correspondence between the RKHS $\\mathcal{H}$ and the reproducing kernel $k$ . Suppose the kernel is square-integrable $\\begin{array}{r}{\\|k\\|_{L_{\\rho}^{2}}^{2}:=\\int k(x,x)d\\rho(x)<\\infty}\\end{array}$ w.r.t. $\\rho\\in\\mathcal P$ . The integral operator $\\mathcal{T}_{k,\\rho}:L_{\\rho}^{2}\\to\\mathcal{H}$ is defined by $\\begin{array}{r}{\\mathcal{T}_{k,\\rho}g(x)\\,:=\\,\\int k\\left(x,x^{\\prime}\\right)g\\left(x^{\\prime}\\right)d\\rho\\left(x^{\\prime}\\right)}\\end{array}$ for $g\\in L_{\\rho}^{2}$ . With an abuse of terminology, we refer to the following composition also as the integral operator ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{K}_{\\rho}:=\\mathrm{Id}\\circ\\mathcal{T}_{k,\\rho},\\;L^{2}(\\rho)\\rightarrow L^{2}(\\rho).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "$\\kappa_{\\rho}$ is compact, positive, self-adjoint, and nuclear; cf. [Steinwart and Christmann, 2008]. To simplify the notation, we simply write $\\kappa$ when $\\rho$ is the Lebesgue measure. ", "page_idx": 2}, {"type": "text", "text": "The kernel maximum mean discrepancy (MMD) [Gretton et al., 2012] emerged as an easy-tocompute alternative to optimal transport for computing the distance between probability measures, i.e., $\\mathrm{MMD}^{2}(\\mu,\\nu):=\\|\\bar{K}(\\mu-\\nu)\\|_{\\mathcal{H}}^{2}=\\int\\int k(\\overline{{x}},x^{\\prime})\\overset{\\cdot}{\\mathrm{d}}(\\mu-\\nu)(x)\\,\\mathrm{d}(\\mu-\\nu)(x^{\\prime})$ , where $\\mathcal{H}$ is the RKHS associated with the (positive-definite) kernel $k$ . While the MMD enjoys many favorable properties, such as a closed-form estimator and favorable statistical properties [Tolstikhin et al., 2017, 2016], its mathematical theory is less developed compared to the Wasserstein space especially in the geodesic structure and gradient flow geometries. It has been shown by Zhu and Mielke [2024] that MMD is a (de-)kernelized Hellinger or Fisher-Rao distance by using a dynamic formulation ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{MMD}^{2}(\\mu,\\nu)=\\operatorname*{min}\\left\\{\\int_{0}^{1}\\|\\xi_{t}\\|_{\\mathcal{H}}^{2}\\,\\mathrm{d}t\\ \\Big|\\ i=-K^{-1}\\xi_{t},u(0)=\\mu,u(1)=\\nu,\\,\\xi_{t}\\in\\mathcal{H}\\right\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Mathematically, we can obtain the MMD geodesic structure if we kernelize the Hellinger (Fisher-Rao) Riemannian metric tensor, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{G}_{\\mathrm{MMD}}=K_{\\mu}\\circ\\mathbb{G}_{\\mathsf{H e}}(\\mu),\\quad\\mathbb{K}_{\\mathrm{MMD}}=\\mathbb{K}_{\\mathsf{H e}}(\\mu)\\circ K_{\\mu}^{-1},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "noting that the Onsager operator $\\mathbb{K}$ is the inverse of the Riemannian metric tensor $\\mathbb{K}=\\mathbb{G}^{-1}$ . The MMD suffers from some shortcomings in practice, such as the vanishing gradients and kernel choices that require careful tuning; see e.g., [Feydy et al., 2019]. Furthermore, a theoretical downside of the MMD as a tool for optimizing distributions, and kernel-mean embedding [Smola et al., 2007, Muandet et al., 2017] in general, is that they do not allow transport dynamics. This limitation is manifested in practice, e.g., it is intractable to optimize the location of particle distributions; see e.g. [Lacoste-Julien et al., 2015]. In this paper, we address all those issues. ", "page_idx": 3}, {"type": "text", "text": "3 IFT gradient flows over non-negative and probability measures ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we propose the IFT gradient flows over non-negative and probability measures. Note that our methodology is fundamentally different from a few related works in kernel methods and gradient flows such as [Arbel et al., 2019, Korba et al., 2021, Hertrich et al., 2023, Glaser et al., 2021, Neumayer et al., 2024] in that we are not concerned with the Wasserstein flows of a different energy, but a new gradient flow dissipation geometry. ", "page_idx": 3}, {"type": "text", "text": "3.1 (Spherical) IFT gradient flow equations over non-negative and probability measures ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The construction of the Wasserstein-FisherRao gradient flows crucially relies on the inf-convolution from convex analysis [Liero et al., 2018, Chizat, 2022]. There, the WFR metric tensor is defined using an inf-convolution of the Wasserstein tensor ", "page_idx": 3}, {"type": "image", "img_path": "rPgc5brxmT/tmp/c393c29823bf6f0832cd721a34bc14e880b993c24b98e0d76aa122f0fc8c2cc4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Illustration of the IFT gradient flow. Atoms are subject to both the transport (Kantorovich) potential and the interaction (repulsive) force from other atoms. ", "page_idx": 3}, {"type": "text", "text": "and the Hellinger (Fisher-Rao) tensor $\\mathbb{G}_{\\sf W F R}(\\mu)=\\mathbb{G}_{W}(\\mu)\\sqcup_{\\sf I G}\\!\\!\\!\\!(\\mu)$ . By Legendre transform, its inverse, the Onsager operator, is given by the sum $\\mathbb{K}_{\\mathsf{W F R}}(\\mu)=\\mathbb{K}_{W}(\\mu)+\\mathbb{K}_{\\mathrm{He}}(\\mu)$ . Therefore, we construct the IFT gradient flow by replacing the Hellinger (Fisher-Rao) tensor with the MMD tensor, as in (5). ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{G}_{\\mathrm{IFT}}(\\mu)=\\mathbb{G}_{W}(\\mu)\\Omega\\mathbb{G}_{\\mathrm{MMD}}(\\mu),\\quad\\mathbb{K}_{\\mathrm{IFT}}(\\mu)=\\mathbb{K}_{W}(\\mu)+\\mathbb{K}_{\\mathrm{MMD}}(\\mu).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The MMD gradient flow equation is derived by Zhu and Mielke [2024] using the Onsager operator (5), ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\dot{\\mu}=-\\mathbb{K}_{\\mathrm{MMD}}(\\mu)\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]=-K^{-1}\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Hence, we obtained the desired IFT gradient flow equation using (6). ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\dot{\\mu}=-\\alpha\\mathbb{K}_{W}(\\mu)\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]-\\beta\\mathbb{K}_{\\mathrm{MMD}}(\\mu)\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]=\\alpha\\cdot\\mathrm{div}(\\mu\\nabla\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right])-\\beta\\cdot K^{-1}\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Formally, the IFT gradient flow equation can also be viewed as a kernel-approximation to the Wasserstein-Fisher-Rao gradient flow equation, i.e., the reaction-diffusion equation (3). ", "page_idx": 4}, {"type": "text", "text": "Corollary 3.1. Suppose $\\textstyle\\int k_{\\sigma}(x,\\cdot)\\,\\mathrm{d}\\mu=1$ and the kernel-weighted-measure converges to the Dirac measure $k_{\\sigma}(x,\\cdot)\\;\\mathrm{d}\\mu\\to\\overline{{\\mathrm{~d}\\delta_{x}}}$ as the bandwidth $\\sigma\\rightarrow0$ . Then, the IFT gradient flow equation (8) tends towards the WFR gradient flow equation as $\\sigma\\rightarrow0$ , i.e., the reaction-diffusion equation (3). ", "page_idx": 4}, {"type": "text", "text": "Like the WFR gradient flow over non-negative measures, the gradient flow equation (8) and (7) are not guaranteed to stay within the probability measure space, i.e., total mass 1. This is useful in many applications such as chemical reaction systems. However, probability measures are often required for machine learning applications. We now provide a mass-preserving gradient flow equation that we term the spherical IFT gradient flow. The term spherical is used to emphasize that the flow stays within the probability measure, as in the spherical Hellinger distance [Laschos and Mielke, 2019]. ", "page_idx": 4}, {"type": "text", "text": "To this end, we must first study spherical MMD flows over probability measures. Recall that (7) is a Hilbert space gradient flow (see [Ambrosio et al., 2005]) and does not stay within the probability space. Closely related, many works using kernel-mean embedding [Smola et al., 2007, Muandet et al., 2017] also suffer from this issue of not respecting the probability space. To produce a restricted (or projected) flow in $\\mathcal{P}$ , our starting point is the MMD minimizing-movement scheme restricted to the probability space ", "page_idx": 4}, {"type": "image", "img_path": "rPgc5brxmT/tmp/97f85a21c16aadedbfde2a34c00b9621818cdd1d15467d199e19dbfb17f66ce8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "We now derive the following mass-preserving spherical gradient flows for the MMD and IFT. ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.2 (Spherical MMD and spherical IFT gradient flow equations). The spherical MMD gradient flow equation is given by (where 1 denotes the constant scalar) ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\dot{\\mu}=-K^{-1}\\left(\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]-\\frac{\\int K^{-1}\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]}{\\int K^{-1}1}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Consequently, the spherical IFT gradient flow equation is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\dot{\\mu}=\\alpha\\cdot\\mathrm{div}(\\mu\\nabla\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right])-\\beta\\cdot K^{-1}\\left(\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]-\\frac{\\int K^{-1}\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]}{\\int K^{-1}1}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Furthermore, those equations are mass-preserving, i.e., $\\textstyle\\int{\\dot{\\mu}}=0$ . ", "page_idx": 4}, {"type": "text", "text": "So far, we have identified the gradient flow equations of interest. Now, we are ready to present our main theoretical results on the convergence of the IFT gradient flow via functional inequalities. For example, the logarithmic Sobolev inequality (LSI) ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\|\\nabla\\log\\frac{\\mathrm{d}\\mu}{\\mathrm{d}\\pi}\\right\\|_{L^{2}(\\mu)}^{2}\\geq c_{\\mathrm{LSI}}\\cdot\\operatorname{D}_{\\mathrm{KL}}(\\mu|\\pi)\\;\\mathrm{for}\\;\\mathrm{some}\\;c_{\\mathrm{LSI}}>0\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "is sufficient to guarantee the convergence of the pure Wasserstein gradient flow of the KL divergence energy, which governs the same dynamics as the Langevin equation. The celebrated Bakry-\u00c9mery Theorem [Bakry and \u00c9mery, 1985], is a cornerstone of convergence analysis for dynamical systems as it provides an explicit sufficient condition: suppose the target measure $\\pi$ is $\\lambda$ -log concave for some $\\lambda>0$ , then the global convergence is guaranteed, i.e., ", "page_idx": 4}, {"type": "text", "text": "The question we answer below is whether the IFT gradient flow enjoys such favorable properties.   \nOur starting point is the (Polyak- $)\\mathbf{\\Delta}$ ojasiewicz type functional inequality. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.3. Suppose the following \u0141ojasiewicz type inequality holds for some $c>0$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\alpha\\cdot\\left\\|\\nabla\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]\\right\\|_{L_{\\mu}^{2}}^{2}+\\beta\\cdot\\left\\|\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]\\right\\|_{\\mathcal{H}}^{2}\\geq c\\cdot\\bigg(F(\\mu(t))-\\operatorname*{inf}_{\\mu}F(\\mu)\\bigg)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for the IFT gradient flow, or ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\alpha\\cdot\\left\\Vert\\nabla\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]\\right\\Vert_{L_{\\mu}^{2}}^{2}+\\beta\\cdot\\left\\Vert\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]-\\frac{\\int K^{-1}\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]}{\\int K^{-1}1}\\right\\Vert_{\\mathcal{H}}^{2}\\geq c\\cdot\\left(F(\\mu(t))-\\operatorname*{inf}_{\\mu}F(\\mu)\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for the spherical IFT gradient flow. Then, the energy $F$ decays exponentially along the corresponding gradient flow, i.e., $F(\\dot{\\mu}(t))-\\operatorname*{inf}_{\\mu}F(\\mu)\\leq e^{-c t}\\cdot\\left\\langle F(\\mu(0))-\\operatorname*{inf}_{\\mu}F(\\mu)\\right\\rangle$ . ", "page_idx": 5}, {"type": "text", "text": "To understand a specific gradient flow, one must delve into the detailed analysis of the conditions under which the functional inequalities hold instead of assuming them to hold by default. We provide such analysis for the IFT gradient flows next. ", "page_idx": 5}, {"type": "text", "text": "3.2 Global exponential convergence analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "MMD energy functional As discussed in the introduction, the MMD energy has been proposed as an alternative to the KL divergence energy for sampling by Arbel et al. $\\dot{[2019]}^{2}$ , where they assume the access to samples from the target measure $y_{i}\\sim\\pi$ . However, the theoretical convergence guarantees under the MMD energy is a less-exploited topic. Those authors characterized a local decay behavior under the assumption that the $\\mu_{t}$ must already be close to the target measure $\\pi$ for all $t>0$ . The assumptions they made are not only restrictive, but also difficult to check. There has also been no global convergence analysis. For example, Arbel et al. [2019]\u2019s Proposition 2 states that the MMD is non-increasing, which is not equivalent to convergence and is easily satisfied by other flows. The mathematical limitation is that the MMD is in general not guaranteed to be convex along the Wasserstein geodesics. In addition, our analysis also does not require the heuristic noise injection step as was required in their implementation. Mroueh and Rigotti [2020] also used the MMD energy but with a different gradient flow, which has been shown by Zhu and Mielke [2024] to be a kernel-regularized inf-convolution of the Allen-Cahn and Cahn-Hilliard type of dissipation. However, Mroueh and Rigotti [2020]\u2019s convergence analysis is not sufficient for establishing (global) exponential convergence as no functional inequality has been established there. In contrast, we now provide full global exponential convergence guarantees. ", "page_idx": 5}, {"type": "text", "text": "We first provide an interesting property that will become useful for our analysis. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.4. Suppose the driving energy is the squared MMD, $F(\\mu)={\\textstyle{\\frac{1}{2}}}\\,\\mathrm{MMD}^{2}(\\mu,\\pi)$ and initial datum $\\mu_{0}\\,\\in\\,\\mathcal{P}$ is a probability measure. Then, the spherical MMD gradient flow equation (10) coincides with the MMD gradient flow equation ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\dot{\\mu}=-\\left(\\mu-\\pi\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "(MMD-MMD-GF) ", "page_idx": 5}, {"type": "text", "text": "whose solution is a linear interpolation between the initial measure $\\mu_{0}$ and the target measure $\\pi$ , i.e., ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mu_{t}=e^{-t}\\mu_{0}+(1-e^{-t})\\pi.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Furthermore, same coincidence holds for the spherical IFT and IFT gradient flow equation ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\dot{\\mu}=\\alpha\\cdot\\mathrm{div}\\left(\\mu\\int\\nabla_{2}k(x,\\cdot)\\;\\mathrm{d}\\left(\\mu-\\pi\\right)\\left(x\\right)\\right)-\\beta\\left(\\mu-\\pi\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The explicit solution to the ODE (MMD-MMD-GF) shows an exponential convergence to the target measure $\\pi$ along the (spherical) MMD gradient flow. The (spherical) IFT gradient flow equation (MMD-IFT-GF) differs from the Wasserstein gradient flow equation of [Arbel et al., 2019] by a linear term. This explains the intuition of why we can expect good convergence properties for the IFT gradient flow of the squared MMD energy. We exploit this feature of the IFT gradient flow to show global convergence guarantees for inference with the MMD energy. This has not been possible previously when confined to the pure Wasserstein gradient flow. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.5 (Global exponential convergence of the IFT flow of the MMD energy). Suppose the energy $F$ is the squared MMD energy $\\begin{array}{r}{F(\\bar{\\mu})=\\frac{1}{2}\\,\\mathrm{MMD^{2}}(\\mu,\\nu)}\\end{array}$ . Then, the (IFT-\u0141oj) holds globally with a constant $c\\geq2\\beta>0$ . ", "page_idx": 6}, {"type": "text", "text": "Consequently, for any initialization within the non-negative measure cone $\\mu_{0}\\in\\mathcal{M}^{+}$ , the squared MMD energy decays exponentially along the IFT gradient flow of non-negative measures, i.e., ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\,\\mathrm{MMD}^{2}(\\mu_{t},\\nu)\\leq e^{-2\\beta t}\\cdot\\frac{1}{2}\\,\\mathrm{MMD}^{2}(\\mu_{0},\\nu).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Furthermore, if the initial datum $\\mu_{0}$ and the target measure $\\pi$ are probability measures $\\mu_{0},\\pi\\in\\mathcal P$ , then the squared MMD energy decays exponentially globally along the spherical IFT gradient flow, i.e., the decay estimate (12) holds along the spherical IFT gradient flow of probability measures. ", "page_idx": 6}, {"type": "text", "text": "We emphasize that no Bakry-\u00c9mery type or kernel conditions are required \u2013 the \u0141ojasiewicz inequality holds globally when using the IFT flow. In contrast, the Wasserstein-Fisher-Rao gradient flow ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\dot{\\mu}=\\alpha\\cdot\\mathrm{div}\\left(\\mu\\int\\nabla_{2}k(x,\\cdot)\\operatorname{d}\\left(\\mu-\\pi\\right)\\left(x\\right)\\right)-\\beta\\cdot\\int k(x,\\cdot)\\operatorname{d}\\left(\\mu-\\pi\\right)\\left(x\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "does not enjoy such global convergence guarantees. ", "page_idx": 6}, {"type": "text", "text": "Global exponential convergence under the KL divergence energy For variational inference and MCMC, a common choice of the energy is the KL divergence energy, i.e., $F(\\mu)=\\mathrm{D}_{\\mathrm{KL}}(\\mu|\\pi)$ . This has already been studied by a large body of literature, including the case of Wasserstein-FisherRao [Liero et al., 2023, Lu et al., 2019]. Not surprisingly, (LSI) is still sufficient for the exponential convergence of the WFR type of gradient flows since the dissipation of the Wasserstein part alone is sufficient for driving the system to equilibrium. For the IFT gradient flows under the KL divergence energy functional, the convergence can still be established. This showcases the strength of the IFT geometry \u2013 it enjoys the best of both worlds. The IFT gradient flow equation of the KL divergence energy reads ${\\dot{\\mu}}={\\boldsymbol{\\alpha}}\\cdot\\operatorname{div}(\\mu\\nabla\\log{\\frac{\\mathrm{d}\\mu}{\\mathrm{d}\\pi}})-\\beta\\cdot K^{-1}\\log{\\frac{\\mathrm{d}\\mu}{\\mathrm{d}\\pi}}$ . Unlike the MMD-energy flow case, the spherical IFT gradient flow of the $\\mathrm{KL}$ over probability measures $\\mathcal{P}$ no longer coincides with that of the (non-spherical) IFT and is given by ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\dot{\\mu}}=\\alpha\\cdot\\operatorname{div}(\\mu\\nabla\\log{\\frac{\\mathrm{d}\\mu}{\\mathrm{d}\\pi}})-\\beta\\cdot K^{-1}\\left(\\log{\\frac{\\mathrm{d}\\mu}{\\mathrm{d}\\pi}}-{\\frac{\\int K^{-1}\\log{\\frac{\\mathrm{d}\\mu}{\\mathrm{d}\\pi}}}{\\int K^{-1}1}}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Proposition 3.6 (Exponential convergence of the SIFT gradient flow of the KL divergence energy). Suppose the (LSI) holds with $c_{L S I}=2\\lambda$ or the target measure $\\pi$ is $\\lambda$ -log concave for some $\\lambda>0$ . Then, the KL divergence energy decays exponentially globally along the spherical IFT gradient flow (13) ,i.e., $\\begin{array}{r}{\\mathrm{D}_{\\mathrm{KL}}(\\mu_{t}\\vert\\pi)\\leq e^{-2\\alpha\\lambda t}\\mathrm{D}_{\\mathrm{KL}}(\\mu_{0}\\vert\\pi).}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "The intuition behind the above result is that the SIFT gradient flow converges whenever the pure Wasserstein gradient flow, i.e., its convergence is at least as fast as the Wasserstein gradient flow. However, we emphasize that the decay estimate of the KL divergence energy only holds along the spherical IFT flow over probability measures $\\mathcal{P}$ , but not the full IFT flow over non-negative measures $\\mathcal{M}^{+}$ . ", "page_idx": 6}, {"type": "text", "text": "3.3 Minimizing movement, JKO-splitting, and a practical particle-based algorithm ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In applications to machine learning and computation, continuous-time flow can be discretized via the JKO scheme [Jordan et al., 1998], which is based on the minimizing movement scheme (MMS) [De Giorgi, 1993]. For the reaction-diffusion type gradient flow equation in the WassersteinFisher-Rao setting, the JKO-splitting a.k.a. time-splitting scheme has been studied by Gallou\u00ebt and Monsaingeon [2017], Mielke et al. [2023]. This amounts to splitting the diffusion (Wasserstein) and reaction (MMD) step in (8), i.e., at time step $\\ell\\geq1$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\displaystyle\\mu^{\\ell+\\frac{1}{2}}\\gets\\operatorname*{argmin}_{\\mu\\in\\mathcal{P}}F(\\mu)+\\frac{1}{2\\tau}W_{2}^{2}(\\mu,\\mu^{\\ell}),}}&{{\\mathrm{(Wasserstein~step)}}}\\\\ {{\\displaystyle\\mu^{\\ell+1}\\gets\\operatorname*{argmin}_{\\mu\\in\\mathcal{P}}F(\\mu)+\\frac{1}{2\\eta}\\mathrm{MMD}^{2}(\\mu,\\mu^{\\ell+\\frac{1}{2}}).}}&{{\\mathrm{(MMD~step)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "A similar JKO-splitting scheme can also be constructed via the WFR gradient flow, which amounts to replacing the MMD step in (14) with a proximal step in the KL (as an approximation to the Hellinger), i.e., $\\begin{array}{r}{\\mu^{\\ell+1}\\gets\\mathrm{argmin}_{\\mu\\in\\mathcal{P}}\\,F(\\mu)\\,\\!+\\frac{1}{\\eta}\\mathrm{D}_{\\mathrm{KL}}(\\mu|\\mu^{\\ell+\\frac{1}{2}})}\\end{array}$ , which is well-studied in the optimization literature as the entropic mirror descent [Nemirovskij and Yudin, 1983]. Our MMD step can also be viewed as a mirror descent step with the mirror map $\\frac{1}{2}\\|\\boldsymbol{K}\\cdot\\|_{\\mathcal{H}}^{2}$ . However, for the task of MMD inference of [Arbel et al., 2019], WFR flow does not possess convergence guarantees such as our Theorem 3.4. The MMD step can also be easily implemented as in our simulation. ", "page_idx": 7}, {"type": "text", "text": "We summarize the resulting overall IFT particle gradient descent from the JKO splitting scheme in Algorithm 1 in the appendix. We now look at those two steps respectively. For concreteness, we consider a flexible particle approximation to the probability measures, with possibly non-uniform weights allocated to the particles, i.e., $\\begin{array}{r}{\\mu=\\sum_{i=1}^{n}\\stackrel{\\cdot}{\\alpha_{i}}\\delta_{x_{i}},\\alpha\\in\\Delta^{n}}\\end{array}$ , $x_{i}\\in X$ . ", "page_idx": 7}, {"type": "text", "text": "Wasserstein step: particle position update. (14) is a standard JKO scheme; see, e.g., [Santambrogio, 2015]. The optimality condition of the Wasserstein proximal step can be implemented using a particle gradient descent algorithm ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\boldsymbol{x}_{i}^{\\ell+1}=\\boldsymbol{x}_{i}^{\\ell}-\\boldsymbol{\\tau}\\cdot\\nabla\\frac{\\delta F}{\\delta\\mu}[\\mu^{\\ell}](\\boldsymbol{x}_{i}^{\\ell}),\\;i=1,...,n,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "which is essentially the algorithm proposed by Arbel et al. [2019] when $F(\\mu)={\\textstyle{\\frac{1}{2}}}\\,\\mathrm{MMD}^{2}(\\mu,\\pi)$ . ", "page_idx": 7}, {"type": "text", "text": "MMD step: particle weight update. The MMD step in (14) is a discretization step of the spherical MMD gradient flow, as shown in (9) and Proposition 3.2. We propose to use the updated particle location xi\u2113 $x_{i}^{\\ell+1}$ from the Wasserstein step (15) and update the weights $\\beta_{i}$ by solving ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\beta\\in\\Delta^{n}}F(\\sum_{i=1}^{n}\\beta_{i}\\delta_{x_{i}^{\\ell+1}})+\\frac{1}{2\\eta}\\operatorname{MMD}^{2}(\\sum_{i=1}^{n}\\beta_{i}\\delta_{x_{i}^{\\ell+1}},\\sum_{i=1}^{n}\\alpha_{i}^{\\ell}\\delta_{x_{i}^{\\ell+1}}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "i.e., the MMD step only updates the weights. Alternatively, as in the classical mirror descent optimization, one can use a linear approximation $\\begin{array}{r}{F(\\mu)\\approx F(\\mu^{\\ell})+\\langle\\frac{\\delta F}{\\delta\\mu}\\left[\\mu^{\\ell}\\right],\\mu-\\mu^{\\ell}\\rangle_{L^{2}}}\\end{array}$ . We also provide a specialized discussion on the MMD-energy minimization task of [Arbel et al., 2019]. Let the energy objective be the squared MMD $F(\\mu):={\\textstyle\\frac{1}{2}}\\operatorname{MMD}(\\mu,\\pi)^{2}$ . In this setting, we are given the particles sampled from the target measure $y^{i}\\sim\\pi$ . For the MMD step in (14), the computation is drastically simplified to an MMD barycenter problem, which was also studied in [Cohen et al., 2021]. This amounts to solving a convex quadratic program with a simplex constraint; see the appendix for the detailed expression. ", "page_idx": 7}, {"type": "text", "text": "4 Numerical Example ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The overall goal of the numerical experiments is to approximate the target measure $\\pi$ by minimizing the squared MMD energy, i.e., $\\operatorname*{min}_{\\mu\\in A\\subset{\\mathcal{P}}}\\mathrm{MMD}^{2}(\\mu,\\pi)$ . In all the experiments, we have access to the target measure $\\pi$ in the form of samples $y_{i}\\sim\\pi$ . This setting was studied in [Arbel et al., 2019] as well as in many deep generative model applications. In the following experiments, we compare the performance of our proposed algorithm of IFT gradient flow, which implements the JKO-splitting scheme in (14) and is detailed in Algorithm 1, to that of $(I)$ Arbel et al. [2019]\u2019s the \u201cMMD flow\u201d (see our discussion in 2), we used their algorithm both with and without a heuristic noise injection suggested by those authors; (2) the Wasserstein-Fisher-Rao flow of the MMD (MMD-WFR-GF). The WFR flow was also used by Yan et al. [2024], Lu et al. [2023] but for minimizing the KL divergence function. As discussed in $\\S3.2$ , the MMD flow of [Arbel et al., 2019] does not possess global convergence guarantees while IFT does. Furthermore, in the Gaussian mixture target experiment, the target measure $\\pi$ is not log-concave. We emphasize that our convergence guarantee still holds for the IFT flow while there is no decay guarantee for the WFR flow. We provide the code for the implementation at https://github.com/egorgladin/ift_flow. ", "page_idx": 7}, {"type": "text", "text": "Gaussian target in 2D experiment Figures 3(a) and 4 showcase the performance of the algorithms in a setting where $\\mu^{0}$ and $\\pi$ are both Gaussians in 2D. Specifically, $\\mu^{0}\\,\\sim\\mathcal{N}(5\\cdot{\\bf1},I)$ and $\\pi~\\sim$ $\\mathcal{N}\\left(\\mathbf{0},\\left(\\mathbf{1}_{/2}^{\\mathsf{^{-}}}\\mathbf{\\bar{1}}/2\\right)\\right)$ . The number of samples drawn from $\\mu^{0}$ and $\\pi$ was set to $n=100$ . A Gaussian kernel with bandwidth $\\sigma=10$ was used. For all three algorithms, we chose the largest stepsize that didn\u2019t cause unstable behavior, $\\tau\\,=\\,50$ . The parameter $\\eta$ in (23) was set to 0.1. As can be observed from the trajectories produced by MMD flow (Figure 4(a)), most points collapse into small clusters near the target mode. Some points drift far away from the target distribution and get stuck; the resulting samples represent the target distribution poorly, which is a sign of suboptimal solution. MMD flow with the heuristic noise injection produces much better results. We suspect the noise helps to escape local minima; however, the injection needs to be heuristically tuned. However, it takes a large number iterations for points to get close to locations with high density of the target distribution. Similarly to the previous research on noisy MMD flow, we use a relatively large noise level (10) in the beginning and \u201cturn off\u201d the noise after a sufficient number of iterations (3000 in our case). A drawback of this approach is that the right time for noise deactivation depends on the particular problem instance, which makes the algorithm behavior less predictable. ", "page_idx": 7}, {"type": "image", "img_path": "rPgc5brxmT/tmp/1f39d32ba52366df7ed16b23e65e41ef8305851b11a77177d1876dd6bac725b8.jpg", "img_caption": ["Figure 3: Mean loss and standard deviation computed over 50 runs "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "rPgc5brxmT/tmp/4ae4f9034bf46ab05d398fdb62c878ec923d3e9d14067d9ea255ac7861091683.jpg", "img_caption": ["Figure 4: Trajectory of a randomly selected subsample produced by different algorithms in the Gaussian target experiment. Color intensity indicates points\u2019 weights. The hollow dots indicate the particles that have already vanished. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "rPgc5brxmT/tmp/d38be8a411c3093cd15d147490625380e29b3d66956044f0de83e04b3f647ab2.jpg", "img_caption": ["Figure 5: Trajectory of a randomly selected subsample produced by different algorithms in the Gaussian mixture experiment. Color intensity indicates points\u2019 weights. The hollow dots indicate the particles that have already vanished. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Algorithm 1 achieves a similar accuracy to that of the noise-injected MMD flow, but much faster \u2013 already after 1000 steps \u2013 without any heuristic noise injection. For the few particles that did not make it close to the target Gaussian\u2019s mean, their mass is teleported to those particles that are close to the target. Hence, the resulting performance of the IFT algorithm does not deteriorate. The hollow dots in the trajectory plot indicate the particles whose mass has been teleported and hence their weights are zero. This is a major advantage of unbalanced transport for dealing with local minima. For a faster implementation, in the implementation of the MMD step in (14), we only perform a single step of projected gradient descent instead of computing a solution to the auxiliary optimization problem (23). To ", "page_idx": 9}, {"type": "image", "img_path": "rPgc5brxmT/tmp/573b174396317579bd1f17ffe27b927c5d2187f9d392ec4c25173d2d13856ffd.jpg", "img_caption": ["Figure 6: Comparison with the WFR flow of the MMD in 100 dimensions "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "be fair in comparison, we count each iteration as two steps. Thus, 6000 steps of the algorithm in Figure 3(a) correspond to only 3000 iterations, i.e., the results for IFT algorithm have already been handicapped by a factor of 2. We would also like to note that Algorithm 1 was executed with constant hyperparameters without further tuning over the iterations, in contrast to the noisy MMD flow. In practical implementations, it is possible to further improve the performance by sampling new locations (particle rejuvenation) in the MMD step (14) as done similarly in [Dai et al., 2016]. Since this paper is a theoretical one and not about competitive benchmarking, we leave this for future work. ", "page_idx": 9}, {"type": "text", "text": "Gaussian mixture in 2D experiment The second experiment has a similar setup. However, this time the target is a mixture of equally weighted Gaussian distributions, ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{N}\\left(\\mathbf{0},\\left(\\!\\!1\\!^{{\\scriptscriptstyle1}}\\!\\!^{{\\scriptscriptstyle1}}\\!\\!^{{\\scriptscriptstyle1}/2}\\!\\right)\\!\\right),\\quad\\mathcal{N}\\left(\\left(\\!\\!\\!\\begin{array}{c}{\\!\\!3}\\\\ {\\!\\!-\\!\\!1\\right)\\!,\\!I}\\end{array}\\!\\!\\right),\\quad\\mathcal{N}\\left(\\left(\\!\\!\\!\\begin{array}{c}{\\!\\!1}\\\\ {\\!\\!4}\\end{array}\\!\\!\\right),\\left(\\!\\!\\!1\\!^{{\\scriptscriptstyle3}}\\!\\!^{{\\scriptscriptstyle1}/2}\\!\\!\\right)\\!\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Figures 3(b) and 5 showcase loss curves and trajectories produced by the considered algorithms. ", "page_idx": 9}, {"type": "text", "text": "WFR flow for Gaussian mixture target in 100D We conducted an experiment in dimension $d=100$ , comparing the IFT flow with the WFR flow of the MMD energy. The initial distribution is ${\\mathcal{N}}(0,I)$ , and the target $\\pi$ is a mixture of 3 distributions $\\mathcal{N}(m_{i},\\Sigma_{i})$ , $i={1,2,3}$ , where $m_{i}$ and $\\Sigma_{i}$ are randomly generated such that $\\|m_{i}\\|_{2}=20$ and the smallest eigenvalue of $\\Sigma_{i}$ is greater than 0.5. For fairness, each iteration of IFT particle GD (as well as its version with KL step) counts as two steps, i.e., these methods only performed 4500 iterations. In the noisy MMD flow, the noise is disabled after 4000 steps. All methods are used with equal stepsize. ", "page_idx": 9}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In summary, the (spherical) IFT gradient flows are a suitable choice for energy minimization of both the MMD and KL divergence energies with sound global exponential convergence guarantees. There is also an orthogonal line of works studying the Stein gradient flow and descent [Liu and Wang, 2019, Duncan et al., 2019], which also has the mechanics interpretation of repulsive forces. It can be related to our work in that the IFT gradient flow has a (de-)kernelized reaction part, while the Stein flow has a kernelized diffusion part. Furthermore, there is a work [Manupriya et al., 2024] that proposes a static MMD-regularized Wasserstein distance, which should not be confused with our IFT gradient flow geometry. Another future direction is sampling and inference when we do not have access to the samples from the target distribution $\\pi$ , but can only evaluate its score function $\\nabla\\log\\pi$ . ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Gabriel Peyr\u00e9 for the helpful comments regarding the practical algorithms for the JKOsplitting scheme. This project has received funding from the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany\u2019s Excellence Strategy \u2013 The Berlin Mathematics Research Center MATH $^{+}$ (EXC-2046/1, project ID: 390685689) and from the priority programme \"Theoretical Foundations of Deep Learning\" (SPP 2298, project number: 543963649). During part of the project, the research of E. Gladin was prepared within the framework of the HSE University Basic Research Program. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "L. Ambrosio, N. Gigli, and G. Savare. Gradient Flows: In Metric Spaces and in the Space of Probability Measures. Springer Science & Business Media, 2005.   \nM. Arbel, A. Korba, A. SALIM, and A. Gretton. Maximum mean discrepancy gradient flow. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/ 944a5ae3483ed5c1e10bbccb7942a279-Paper.pdf.   \nM. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In International conference on machine learning, pages 214\u2013223. PMLR, 2017.   \nD. Bakry and M. \u00c9mery. Diffusions hypercontractives. In J. Az\u00e9ma and M. Yor, editors, S\u00e9minaire de Probabilit\u00e9s XIX 1983/84, volume 1123, pages 177\u2013206. Springer Berlin Heidelberg, Berlin, Heidelberg, 1985. ISBN 978-3-540-15230-9 978-3-540-39397-9. doi: 10.1007/BFb0075847.   \nJ. Bigot, R. Gouet, T. Klein, and A. L\u00f3pez. Geodesic PCA in the Wasserstein space by convex PCA. Ann. Inst. H. Poincar\u00e9 Probab. Statist., 53(1):1\u201326, 02 2017. doi: 10.1214/15-AIHP706. URL https://doi.org/10.1214/15-AIHP706.   \nJ. A. Carrillo, K. Craig, and F. S. Patacchini. A blob method for diffusion. Calculus of Variations and Partial Differential Equations, 58:1\u201353, 2019.   \nS. Chewi, T. Le Gouic, C. Lu, T. Maunu, and P. Rigollet. SVGD as a kernelized Wasserstein gradient flow of the chi-squared divergence. Advances in Neural Information Processing Systems, 33: 2098\u20132109, 2020.   \nL. Chizat. Sparse optimization on measures with over-parameterized gradient descent. Mathematical Programming, 194(1-2):487\u2013532, 2022.   \nL. Chizat, G. Peyr\u00e9, B. Schmitzer, and F.-X. Vialard. An interpolating distance between optimal transport and Fisher\u2013Rao metrics. Foundations of Computational Mathematics, 18(1):1\u201344, Feb. 2018. ISSN 1615-3375, 1615-3383. doi: 10.1007/s10208-016-9331-y.   \nL. Chizat, G. Peyr\u00e9, B. Schmitzer, and F.-X. Vialard. Unbalanced optimal transport: Dynamic and Kantorovich formulation. arXiv:1508.05216 [math], Feb. 2019. URL http://arxiv.org/abs/ 1508.05216. arXiv: 1508.05216.   \nS. Cohen, M. Arbel, and M. P. Deisenroth. Estimating barycenters of measures in high dimensions. arXiv:2007.07105 [cs, stat], Feb. 2021.   \nK. Craig, K. Elamvazhuthi, M. Haberland, and O. Turanova. A blob method for inhomogeneous diffusion with applications to multi-agent control and sampling. Mathematics of Computation, 92 (344):2575\u20132654, Nov. 2023. ISSN 0025-5718, 1088-6842. doi: 10.1090/mcom/3841.   \nM. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information processing systems, 26, 2013.   \nB. Dai, N. He, H. Dai, and L. Song. Provable Bayesian inference via particle mirror descent. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, pages 985\u2013994. PMLR, May 2016.   \nE. De Giorgi. New problems on minimizing movements. Ennio de Giorgi: Selected Papers, pages 699\u2013713, 1993.   \nA. Duncan, N. N\u00fcsken, and L. Szpruch. On the geometry of Stein variational gradient descent. arXiv preprint arXiv:1912.00894, 2019.   \nM. A. Efendiev and A. Mielke. On the rate-independent limit of systems with dry friction and small viscosity. Journal of Convex Analysis, 13(1):151, 2006.   \nJ. Feydy, T. S\u00e9journ\u00e9, F.-X. Vialard, S.-i. Amari, A. Trouve, and G. Peyr\u00e9. Interpolating between optimal transport and MMD using Sinkhorn divergences. In Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics, pages 2681\u20132690. PMLR, Apr. 2019.   \nT. O. Gallou\u00ebt and L. Monsaingeon. A JKO splitting scheme for Kantorovich\u2013Fisher\u2013Rao gradient flows. SIAM Journal on Mathematical Analysis, 49(2):1100\u20131130, 2017.   \nP. Glaser, M. Arbel, and A. Gretton. KALE flow: A relaxed KL gradient flow for probabilities with disjoint support. In Neural Information Processing Systems, June 2021.   \nA. Gretton, K. M. Borgwardt, M. J. Rasch, B. Sch\u00f6lkopf, and A. Smola. A kernel two-sample test. The Journal of Machine Learning Research, 13(1):723\u2013773, 2012.   \nJ. Hertrich, C. Wald, F. Altekr\u00fcger, and P. Hagemann. Generative sliced MMD flows with Riesz kernels. arXiv preprint arXiv:2305.11463, 2023.   \nN. Ho, X. Nguyen, M. Yurochkin, H. H. Bui, V. Huynh, and D. Phung. Multilevel clustering via Wasserstein means. In D. Precup and Y. W. Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1501\u20131509, International Convention Centre, Sydney, Australia, 06\u201311 Aug 2017. PMLR. URL http://proceedings.mlr.press/v70/ho17a.html.   \nR. Jordan, D. Kinderlehrer, and F. Otto. The variational formulation of the Fokker\u2013Planck equation. SIAM journal on mathematical analysis, 29(1):1\u201317, 1998. Publisher: SIAM.   \nS. Kondratyev, L. Monsaingeon, and D. Vorotnikov. A new optimal transport distance on the space of finite Radon measures. Advances in Differential Equations, 21(11/12):1117 \u2013 1164, 2016. doi: 10.57262/ade/1476369298. URL https://doi.org/10.57262/ade/1476369298.   \nA. Korba, P.-C. Aubin-Frankowski, S. Majewski, and P. Ablin. Kernel Stein discrepancy descent. In Proceedings of the 38th International Conference on Machine Learning, pages 5719\u20135730. PMLR, July 2021.   \nM. J. Kusner, Y. Sun, N. I. Kolkin, and K. Q. Weinberger. From word embeddings to document distances. In Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37, ICML\u201915, pages 957\u2013966. JMLR.org, 2015. URL http: //dl.acm.org/citation.cfm?id=3045118.3045221.   \nS. Lacoste-Julien, F. Lindsten, and F. Bach. Sequential kernel herding: Frank-Wolfe optimization for particle flitering. In G. Lebanon and S. V. N. Vishwanathan, editors, Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, volume 38 of Proceedings of Machine Learning Research, pages 544\u2013552, San Diego, California, USA, 09\u201312 May 2015. PMLR. URL https://proceedings.mlr.press/v38/lacoste-julien15.html.   \nV. Laschos and A. Mielke. Geometric properties of cones with applications on the Hellinger\u2013 Kantorovich space, and a new distance on the space of probability measures. J. Funct. Analysis, 276(11):3529\u20133576, 2019. doi: 10.1016/j.jfa.2018.12.013.   \nM. Liero, A. Mielke, and G. Savar\u00e9. Optimal entropy-transport problems and a new Hellinger\u2013Kantorovich distance between positive measures. Inventiones mathematicae, 211 (3):969\u20131117, Mar. 2018. ISSN 0020-9910, 1432-1297. doi: 10.1007/s00222-017-0759-8. URL http://link.springer.com/10.1007/s00222-017-0759-8.   \nM. Liero, A. Mielke, and G. Savar\u00e9. Fine properties of geodesics and geodesic $\\lambda$ -convexity for the Hellinger\u2013Kantorovich distance. Arch. Rat. Mech. Analysis, 247(112):1\u201373, 2023. doi: 10.1007/s00205-023-01941-1.   \nQ. Liu and D. Wang. Stein variational gradient descent: A general purpose Bayesian inference algorithm. arXiv:1608.04471 [cs, stat], Sept. 2019. URL http://arxiv.org/abs/1608.04471. arXiv: 1608.04471.   \nY. Lu, J. Lu, and J. Nolen. Accelerating Langevin sampling with birth-death. ArXiv, May 2019.   \nY. Lu, D. Slepc\u02c7ev, and L. Wang. Birth\u2013death dynamics for sampling: global convergence, approximations and their asymptotics. Nonlinearity, 36(11):5731, 2023.   \nP. Manupriya, S. N. Jagarlapudi, and P. Jawanpuria. MMD-regularized unbalanced optimal transport. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id $\\cdot^{=}$ eN9CjU3h1b.   \nA. Mielke, R. Rossi, and A. Stephan. On time-splitting methods for gradient flows with two dissipation mechanisms. arXiv preprint arXiv:2307.16137, 2023.   \nP. Mohajerin Esfahani and D. Kuhn. Data-driven distributionally robust optimization using the Wasserstein metric: Performance guarantees and tractable reformulations. Mathematical Programming, 171(1-2):115\u2013166, Sept. 2018. ISSN 0025-5610, 1436-4646. doi: 10.1007/s10107-017-1172-1.   \nY. Mroueh and M. Rigotti. Unbalanced Sobolev descent, Sept. 2020. URL http://arxiv.org/ abs/2009.14148. arXiv:2009.14148 [cs, stat].   \nK. Muandet, K. Fukumizu, B. Sriperumbudur, and B. Sch\u00f6lkopf. Kernel mean embedding of distributions: A review and beyond. Foundations and Trends\u00ae in Machine Learning, 10(1-2): 1\u2013141, 2017. ISSN 1935-8237, 1935-8245. doi: 10.1561/2200000060.   \nA. S. Nemirovskij and D. B. Yudin. Problem Complexity and Method Efficiency in Optimization. John Wiley, New York, 1983.   \nS. Neumayer, V. Stein, and G. Steidl. Wasserstein gradient flows for Moreau envelopes of fdivergences in reproducing kernel Hilbert spaces. arXiv preprint arXiv:2402.04613, 2024.   \nF. Otto. Double degenerate diffusion equations as steepest descent. Bonn University, 1996. Preprint.   \nY. Rubner, C. Tomasi, and L. J. Guibas. The earth mover\u2019s distance as a metric for image retrieval. International journal of computer vision, 40(2):99\u2013121, 2000.   \nR. Sandler and M. Lindenbaum. Nonnegative matrix factorization with earth mover\u2019s distance metric for image analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(8): 1590\u20131602, 2011. ISSN 0162-8828. doi: 10.1109/TPAMI.2011.18.   \nF. Santambrogio. Optimal Transport for Applied Mathematicians: Calculus of Variations, PDEs, and Modeling. Springer International Publishing, 2015. ISBN 9783319208282. doi: 10.1007/ 978-3-319-20828-2. URL http://dx.doi.org/10.1007/978-3-319-20828-2.   \nA. Sinha, H. Namkoong, R. Volpi, and J. Duchi. Certifying some distributional robustness with principled adversarial training. arXiv:1710.10571 [cs, stat], May 2020.   \nA. Smola, A. Gretton, L. Song, and B. Sch\u00f6lkopf. A Hilbert space embedding for distributions. In M. Hutter, R. A. Servedio, and E. Takimoto, editors, Algorithmic Learning Theory, pages 13\u201331, Berlin, Heidelberg, 2007. Springer. ISBN 978-3-540-75225-7. doi: 10.1007/978-3-540-75225-7_ 5.   \nJ. Solomon, R. M. Rustamov, L. Guibas, and A. Butscher. Wasserstein propagation for semisupervised learning. In Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32, ICML\u201914, pages I\u2013306\u2013I\u2013314. JMLR.org, 2014. URL http://dl.acm.org/citation.cfm?id=3044805.3044841.   \nI. Steinwart and A. Christmann. Support vector machines. Springer Science & Business Media, 2008.   \nI. Tolstikhin, B. Sriperumbudur, and K. Muandet. Minimax estimation of kernel mean embeddings. arXiv:1602.04361 [math, stat], July 2017.   \nI. O. Tolstikhin, B. K. Sriperumbudur, and B. Sch\u00f6lkopf. Minimax estimation of maximum mean discrepancy with radial kernels. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper_files/paper/2016/ file/5055cbf43fac3f7e2336b27310f0b9ef-Paper.pdf.   \nA. B. Tsybakov. Introduction to Nonparametric Estimation. Springer Series in Statistics. Springer, New York, NY, 2009. ISBN 978-0-387-79051-0 978-0-387-79052-7. doi: 10.1007/b13794. URL https://link.springer.com/10.1007/b13794.   \nC. Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008.   \nM. Werman, S. Peleg, and A. Rosenfeld. A distance metric for multidimensional histograms. Computer Vision, Graphics, and Image Processing, 32(3):328 \u2013 336, 1985. ISSN 0734-189X. doi: https://doi.org/10.1016/0734-189X(85)90055-6. URL http://www.sciencedirect.com/ science/article/pii/0734189X85900556.   \nY. Yan, K. Wang, and P. Rigollet. Learning Gaussian mixtures using the Wasserstein-Fisher-Rao gradient flow. The Annals of Statistics, 52(4):1774\u20131795, 2024.   \nJ.-J. Zhu and A. Mielke. Kernel approximation of Fisher-Rao gradient flows, 2024. URL https: //arxiv.org/abs/2410.20622. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Appendix: proofs and additional technical details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof of Proposition 3.2. We first consider the MMS step (9). The Lagrangian of the MMS step is, for $\\lambda\\in\\mathbb{R}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\mu,\\lambda)=F(\\mu)+\\frac{1}{2\\eta}\\mathrm{MMD}^{2}(\\mu,\\mu^{\\ell})+\\lambda(\\int\\mu-1).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The Euler-Lagrange equation gives ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\delta F}{\\delta\\mu}\\left[\\mu^{\\ell}\\right]+\\frac{1}{\\eta}\\kappa(\\mu-\\mu^{\\ell})+\\lambda=0,}\\\\ {\\displaystyle\\int\\mu=1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Rewriting the first equation, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mu=\\mu^{\\ell}-\\eta\\mathcal{K}^{-1}\\frac{\\delta F}{\\delta\\mu}\\left[\\mu^{\\ell}\\right]-\\eta\\lambda\\mathcal{K}^{-1}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Integrating both sides, we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n1=1-\\eta\\int K^{-1}\\frac{\\delta F}{\\delta\\mu}\\left[\\mu^{\\ell}\\right]-\\eta\\lambda\\int K^{-1}1\\implies\\lambda=-\\frac{\\int K^{-1}\\frac{\\delta F}{\\delta\\mu}\\left[\\mu^{\\ell}\\right]}{\\int K^{-1}1}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let the time step $\\eta\\rightarrow0$ in the first equation in the Euler-Lagrange equation (17), we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\dot{\\mu}=-K^{-1}(\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]+\\lambda)=-K^{-1}\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]+\\frac{\\int K^{-1}\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]}{\\int K^{-1}1}\\cdot K^{-1}1,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which is the desired spherical MMD gradient flow equation. ", "page_idx": 14}, {"type": "text", "text": "Spherical IFT gradient flow equation is obtained by an inf-convolution [Gallou\u00ebt and Monsaingeon, 2017, Liero et al., 2018, Chizat et al., 2019] of the above spherical MMD and Wasserstein part. The verification of the mass-preserving property is by a straightforward integration of (18) ", "page_idx": 14}, {"type": "equation", "text": "$$\n0=\\int{\\dot{\\mu}}=-\\int{\\mathcal{K}}^{-1}{\\frac{\\delta F}{\\delta\\mu}}\\left[\\mu\\right]+{\\frac{\\int{\\mathcal{K}}^{-1}{\\frac{\\delta F}{\\delta\\mu}}\\left[\\mu\\right]}{\\int{\\mathcal{K}}^{-1}1}}\\cdot\\int{\\mathcal{K}}^{-1}1=0.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Hence, the theorem is proved. ", "page_idx": 14}, {"type": "text", "text": "Proof of Theorem 3.3. The proof amounts to identifying the correct left-hand side of the \u0141ojasiewicz type inequality. We take the time derivative of the energy ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}F(\\mu)=\\langle\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right],\\alpha\\cdot\\mathrm{div}(\\mu\\nabla\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right])-\\beta\\cdot K^{-1}\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]\\rangle_{L^{2}}\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which is the desired left-hand side of the \u0141ojasiewicz type inequality. ", "page_idx": 14}, {"type": "text", "text": "For the spherical IFT gradient flow, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}F(\\mu)=\\langle\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right],\\alpha\\cdot\\mathrm{div}(\\mu\\nabla\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right])-\\beta\\cdot K^{-1}\\left(\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]-\\frac{\\int K^{-1}\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]}{\\int K^{-1}1}\\right)\\rangle_{L^{2}}}\\\\ {=\\langle\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right],\\alpha\\cdot\\mathrm{div}(\\mu\\nabla\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right])\\rangle_{L^{2}}+\\langle\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]-\\frac{\\int K^{-1}\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]}{\\int K^{-1}1},-\\beta\\cdot K^{-1}\\left(\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]-\\frac{\\int K^{-1}\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]}{\\int K^{-1}1}\\right)\\rangle_{L^{2}}}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad=-\\alpha\\cdot\\left\\Vert\\nabla\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]\\right\\Vert_{L_{\\mu}^{2}}^{2}-\\beta\\cdot\\left\\Vert\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]-\\frac{\\int K^{-1}\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]}{\\int K^{-1}1}\\right\\Vert_{\\mu}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the second equality follows from the fact that the spherical IFT gradient flow is masspreserving. Hence, the left-hand side of the \u0141ojasiewicz type inequality is obtained. ", "page_idx": 14}, {"type": "text", "text": "Proof of Corollary 3.1. The formal proof is by using well-known results for kernel smoothing in non-parametric statistics [Tsybakov, 2009]. Note that the gradient flow equation can be rewritten as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\dot{u}=-\\alpha\\cdot\\mathrm{div}(\\mu\\nabla\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right])+\\beta\\cdot K^{-1}\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]=-\\alpha\\cdot\\mathrm{div}(\\mu\\nabla\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right])+\\beta\\cdot\\mu K_{\\mu}^{-1}\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Recall the definition of the integral operator ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{K}f=\\int k_{\\sigma}(\\cdot,y)f(y)\\;\\mathrm{d}y,\\quad\\mathcal{K}_{\\mu}f=\\int k_{\\sigma}(\\cdot,y)f(y)\\;\\mathrm{d}\\mu(y).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Formally, as $k_{\\sigma}(x,\\cdot)\\;\\mathrm{d}\\mu\\to\\;\\mathrm{d}\\delta_{x}$ , we have $K_{\\mu}\\xi\\rightarrow\\xi$ for any $\\xi\\in L^{2}(\\mu)$ . Then, the IFT gradient flow equation tends towards the PDE ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\dot{\\mu}=-\\alpha\\cdot\\mathrm{div}(\\mu\\nabla\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right])+\\beta\\mu\\cdot\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Furthermore, we also have $\\begin{array}{r}{\\|\\frac{\\delta F}{\\delta\\mu}\\,[\\mu]\\|_{\\mathcal{H}}^{2}\\to\\|\\frac{\\delta F}{\\delta\\mu}\\,[\\mu]\\,\\|_{L^{2}(\\mu)}^{2}}\\end{array}$ . Hence, the conclusion follows. ", "page_idx": 15}, {"type": "text", "text": "Proof of Theorem 3.4. We first recall that the first variation of the squared MMD energy is given by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\delta}{\\delta\\mu}\\left(\\frac{1}{2}\\,\\mathrm{MMD}^{2}(\\mu,\\nu)\\right)[\\mu]=\\int k(x,\\cdot)(\\mu-\\nu)(\\mathrm{\\ensuremath{~d}x}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Plugging the first variation of the squared MMD energy into the gradient flow equation (7), (10), (8), and (11), we obtain the desired flow equations in the theorem. The ODE solution is obtained by an elementary argument. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Proof of Theorem 3.5. We take the time derivative of the energy and apply the chain rule formally and noting the gradient flow equation (8), ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}F(\\mu)=\\langle\\int\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right],\\alpha\\cdot\\mathrm{div}(\\mu\\nabla\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right])-\\beta\\cdot K^{-1}\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]\\rangle_{L^{2}}}\\\\ {\\displaystyle=-\\alpha\\cdot\\|\\nabla\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]\\|_{L_{\\mu}^{2}}^{2}-\\beta\\cdot\\|\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]\\|_{\\mathcal{H}}^{2}\\leq-\\beta\\cdot\\|\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]\\|_{\\mathcal{H}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Plugging in $F(\\mu)={\\textstyle{\\frac{1}{2}}}\\,\\mathrm{MMD}^{2}(\\mu,\\nu)$ , an elementary calculation shows that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\left(\\frac{1}{2}\\,\\mathrm{MMD}^{2}(\\mu,\\nu)\\right)\\leq-\\beta\\cdot\\|\\int k(x,\\cdot)(\\mu-\\nu)(\\,\\mathrm{d}x)\\|_{\\mathcal{H}}^{2}=-2\\beta\\cdot\\frac{1}{2}\\,\\mathrm{MMD}^{2}(\\mu,\\nu),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which establishes the desired \u0141ojasiewicz inequality specialized to the squared MMD energy, which reads ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\alpha\\cdot\\left\\|\\nabla\\int k(x,\\cdot)(\\mu-\\nu)(\\,\\mathrm{d}x)\\right\\|_{L_{\\mu}^{2}}^{2}+\\beta\\cdot\\left\\|\\int k(x,\\cdot)(\\mu-\\nu)(\\,\\mathrm{d}x)\\right\\|_{\\mathcal{H}}^{2}\\geq c\\cdot\\frac{1}{2}\\operatorname{MMD}^{2}(\\mu,\\nu).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By Gr\u00f6nwall\u2019s lemma, exponential decay is established. ", "page_idx": 15}, {"type": "text", "text": "Furthermore, plugging the first variation of the squared MMD energy into the gradient flow equation (10), the extra term in the spherical flow equation becomes ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\int K^{-1}\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]}{\\int K^{-1}1}=\\frac{\\int K^{-1}\\mathcal{K}(\\mu-\\pi)}{\\int K^{-1}1}=0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Hence, the coincidence is proved. ", "page_idx": 15}, {"type": "text", "text": "Proof of Proposition 3.6. By the Bakry-\u00c9mery Theorem, we have the LSI (LSI) hold with $c_{\\mathrm{LSI}}=2\\lambda$ . Taking the time derivative of the KL divergence energy along the SIFT gradient flow, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathrm{D}_{\\mathrm{KL}}(\\mu_{t}|\\pi)=\\left\\langle\\nabla\\log\\frac{\\mathrm{d}\\mu_{t}}{\\mathrm{d}\\pi},\\dot{\\mu}_{t}\\right\\rangle_{L^{2}}}\\\\ {\\displaystyle=-\\alpha\\cdot\\left\\|\\nabla\\log\\frac{\\mathrm{d}\\mu_{t}}{\\mathrm{d}\\pi}\\right\\|_{L_{\\mu_{t}}^{2}}^{2}-\\beta\\cdot\\left\\|\\log\\frac{\\mathrm{d}\\mu_{t}}{\\mathrm{d}\\pi}-\\frac{\\int K^{-1}\\log\\frac{\\mathrm{d}\\mu_{t}}{\\mathrm{d}\\pi}}{\\int K^{-1}1}\\right\\|_{\\mathcal{H}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By Gr\u00f6nwall\u2019s lemma, exponential convergence is established. ", "page_idx": 15}, {"type": "text", "text": "Note that this result does not hold for the full IFT flows over non-negative measures as there exists no LSI globally on $\\mathcal{M}^{+}$ . ", "page_idx": 16}, {"type": "text", "text": "Remark A.1 (Regularized inverse of the integral operator). Strictly speaking, the integral operator $\\kappa$ is compact and hence its inverse is unbounded. Using a viscosity-regularization techniques by iE.fee., $\\begin{array}{r}{\\dot{\\boldsymbol{\\mu}}=\\alpha\\cdot\\mathrm{div}(\\mu\\boldsymbol{\\nabla}\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right])-\\beta\\cdot(\\boldsymbol{K}+\\epsilon\\cdot\\boldsymbol{I})^{-1}\\frac{\\delta F}{\\delta\\mu}\\left[\\mu\\right]}\\end{array}$ . uTathiios nc owrhreersep tohned is ntvoe rasne  aisd dailtiwvaey rs egwuellla-rdizeaftinieodn, of the kernel Gram matrix in practical computation. ", "page_idx": 16}, {"type": "text", "text": "A particle gradient descent algorithm for IFT gradient flows We use the notation $K_{X X}$ to denote the kernel Gram matrix KXX = [k(xi\u2113+1, xj\u2113+1)]in,j=1, KX X\u00af for the cross kernel matrix $K_{X\\bar{X}}=[k(x_{i}^{\\ell+1},x_{j}^{\\ell})]_{i,j=1}^{n}$ , etc. ", "page_idx": 16}, {"type": "text", "text": "Algorithm 1 A JKO-splitting for $\\mathrm{IFT}$ particle gradient descent ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Require: ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1: for $\\ell=1$ to $T-1$ do ", "page_idx": 16}, {"type": "text", "text": "2: Compute the first variation of the energy $F$ at $\\begin{array}{r}{\\mu^{\\ell}\\colon g^{\\ell}=\\frac{\\delta F}{\\delta\\mu}\\left[\\mu^{\\ell}\\right]}\\end{array}$ . Then, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{x_{i}^{\\ell+1}\\gets x_{i}^{\\ell}-\\tau^{\\ell}\\cdot\\nabla g^{\\ell}(x_{i}^{\\ell}),\\quad i=1,...,n\\,}&{\\,(1\\leq N)}\\\\ &{\\alpha^{\\ell+1}\\gets\\underset{\\alpha\\in\\Delta^{n}}{\\operatorname{argmin}}\\,F(\\sum_{i=1}^{n}\\alpha_{i}\\delta_{x_{i}^{\\ell+1}})+\\frac{1}{2\\eta^{\\ell}}\\left[\\alpha^{\\ell}\\right]^{\\top}\\left(\\underset{-K_{X\\bar{X}}}{K_{X\\bar{X}}}\\quad-\\underset{K_{\\bar{X}\\bar{X}}}{K_{\\bar{X}\\bar{X}}}\\right)\\left[\\alpha^{\\ell}\\right]\\quad(1\\leq N)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "3: end for ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "4: Output the particle measure $\\begin{array}{r}{\\widehat{\\mu}^{T}=\\sum_{i=1}^{n}\\alpha_{i}^{T}\\delta_{x_{i}^{T}}}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "Implementing the MMD step The MMD step in the JKO-splitting scheme is a convex quadratic program with a simplex constraint, which can be formulated as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\beta\\in\\Delta^{n}}\\frac{1}{2}\\operatorname{MMD}^{2}(\\sum_{i=1}^{n}\\beta_{i}\\delta_{x_{i}^{\\ell+1}},\\pi)+\\frac{1}{2\\eta}\\operatorname{MMD}^{2}(\\sum_{i=1}^{n}\\beta_{i}\\delta_{x_{i}^{\\ell+1}},\\sum_{i=1}^{n}\\alpha_{i}^{\\ell}\\delta_{x_{i}^{\\ell+1}}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We further expand the optimization objective (multiplied by a factor of $2\\tau$ for convenience) ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\tau\\cdot\\left\\|\\sum_{i=1}^{n}\\beta_{i}\\phi(x_{i}^{k+1})-\\frac{1}{m}\\sum_{j=1}^{m}\\phi(y_{j})\\right\\|^{2}+\\left\\|\\sum_{i=1}^{n}\\beta_{i}\\phi(x_{i}^{k+1})-\\sum_{i=1}^{n}\\alpha_{i}^{k}\\phi(x_{i}^{k})\\right\\|^{2}}\\\\ &{=\\displaystyle\\tau\\cdot\\left(\\beta^{\\top}K_{X X}\\beta-\\frac{2}{m}\\beta^{\\top}K_{X Y}{\\bf1}+\\frac{1}{m^{2}}{\\bf1}^{\\top}K_{Y Y}{\\bf1}\\right)+\\left(\\beta^{\\top}K_{X X}\\beta-2\\beta^{\\top}K_{X\\bar{X}}\\alpha+\\alpha^{\\top}K_{\\bar{X}\\bar{X}}\\alpha\\right)}\\\\ &{\\quad\\qquad=(1+\\tau)\\,\\beta^{\\top}K_{X X}\\beta-\\frac{2\\tau}{m}\\beta^{\\top}K_{X Y}{\\bf1}-2\\beta^{\\top}K_{X\\bar{X}}\\alpha+\\frac{\\tau}{m^{2}}{\\bf1}^{\\top}K_{Y Y}{\\bf1}+\\alpha^{\\top}K_{\\bar{X}\\bar{X}}\\alpha.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, the MMD step in Algorithm 1 can be implemented as a convex quadratic program with a simplex constraint. ", "page_idx": 16}, {"type": "text", "text": "A particle gradient descent algorithm for the WFR flow of the MMD energy We provide the implementation details of the WFR flow of the MMD energy. The goal is to simulate the PDE (MMD-WFR-GF). To the best of our knowledge, there has been no prior implementation of this flow. Nor is there a convergence guarantee. Similar to the JKO-splitting scheme of the IFT flow in (14), (MMD-WFR-GF) can be discretized using the two-step scheme ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\displaystyle\\mu^{\\ell+\\frac{1}{2}}\\gets\\arg\\operatorname*{min}_{\\mu\\in\\mathcal{P}}F(\\mu)+\\frac{1}{2\\tau}W_{2}^{2}(\\mu,\\mu^{\\ell})}}&{{\\mathrm{(Wasserstein~step)}}}\\\\ {{\\displaystyle\\mu^{\\ell+1}\\gets\\operatorname*{argmin}_{\\mu\\in\\mathcal{P}}F(\\mu)+\\frac{1}{\\eta}\\mathrm{KL}(\\mu,\\mu^{\\ell+\\frac{1}{2}})}}&{{\\mathrm{(KL~step)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the energy function $F$ is the squared MMD energy, $F(\\mu)={\\textstyle{\\frac{1}{2}}}\\,\\mathrm{MMD}^{2}(\\mu,\\pi)$ . Use the explicit Euler scheme, the KL step amounts to the entropic mirror descent. In the optimization literature, this ", "page_idx": 16}, {"type": "text", "text": "step can be implemented as multiplicative update of the weights (or density), i.e., suppose \u2113+1is the new particle location after the Wasserstein step, then we update the weights vector $\\alpha$ via ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\alpha_{i}^{\\ell+1}\\gets\\alpha_{i}^{\\ell}\\cdot\\exp\\left(-\\eta\\cdot\\frac{\\delta F}{\\delta\\mu}[\\mu^{\\ell}](x_{i}^{\\ell+1})\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Yes, our claims in the abstract and introduction do accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: In various places in the paper, we discuss the limitations of our work. Especially in the discussion/conclusion section, we discuss the limitations of our approach. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Yes. For example, one focus of our work is precisely to study under what assumptions, functional inequalities such as the \u0141ojasiewicz inequality and the log-Sobolev inequality hold. We do not avoid discussing such assumptions. We provide a complete proof for each theoretical result. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Our implementation is standard and also similar to that of [Arbel et al., 2019] and [Korba et al., 2021]. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Similar to the answer to the previous question. We have submitted the code and added to the text a link to a github repo with the details of the experiments. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Ours is a theory paper with small-scale experiments. We provide all the necessary details in the main text, see Sect. 4. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Yes, we have reported error bars in our experiments. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 20}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our experiments are small-scale and can be reproduced on a standard laptop. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our work is more on the theoretical side and, thus, we do not see any direct societal impact or potential harmful consequences. The research process did not use any personal data and did not involve human subjects or participants. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our work is more on the theoretical side and, thus, we do not see any direct societal impact. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: We do not release any data or models that have a risk for misuse. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: No specific assets are used in our work. We have cited the related work with code and data. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: We do not introduce any new assets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: We did not use crowdsourcing and did not make research with Human Subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: We did not make research with Human Subjects. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]