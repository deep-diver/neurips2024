{"references": [{"fullname_first_author": "Ben Mildenhall", "paper_title": "NeRF: Representing scenes as neural radiance fields for view synthesis", "publication_date": "2020-00-00", "reason": "This paper introduced Neural Radiance Fields (NeRFs), a foundational model for novel view synthesis that heavily influenced the current work."}, {"fullname_first_author": "Bernhard Kerbl", "paper_title": "3D Gaussian splatting for real-time radiance field rendering", "publication_date": "2023-00-00", "reason": "This paper introduced 3D Gaussian Splatting, the core rendering method used in this paper for its speed and efficiency."}, {"fullname_first_author": "Tianye Li", "paper_title": "Neural 3D video synthesis from multi-view video", "publication_date": "2022-00-00", "reason": "This paper introduced a method for 4D neural scene representation that is closely related to the proposed method."}, {"fullname_first_author": "Jiakai Sun", "paper_title": "3DGStream: On-the-fly training of 3D Gaussians for efficient streaming of photo-realistic free-viewpoint videos", "publication_date": "2024-00-00", "reason": "This paper is a direct competitor to the proposed method, providing a strong baseline for comparison and highlighting the state of the art in online free-viewpoint video."}, {"fullname_first_author": "Lingzhi Li", "paper_title": "Streaming radiance fields for 3D video synthesis", "publication_date": "2022-00-00", "reason": "This paper provides a key comparative method that also addresses the online free-viewpoint video problem, allowing for a comparison of different approaches."}]}