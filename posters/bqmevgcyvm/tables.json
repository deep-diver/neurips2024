[{"figure_path": "bQMevGCYVM/tables/tables_6_1.jpg", "caption": "Table 1: The quantitative evaluation results on Refer-Youtube-VOS and Refer-DAVIS-17. In the table, bold denotes the best scores; underline denotes the second place.", "description": "This table presents a quantitative comparison of various video object segmentation methods on two benchmark datasets: Refer-Youtube-VOS and Refer-DAVIS-17.  The methods are categorized into traditional methods (without reasoning ability) and LLM-based methods (with reasoning ability). For each method, the J&F (average of Jaccard and F-measure), J (Jaccard), and F (F-measure) scores are reported.  Bold values indicate the best performance, while underlined values represent the second-best performance on each metric for each dataset.  The table highlights the superior performance of the proposed VideoLISA model, especially when post-optimization is applied, compared to both traditional and other LLM-based methods.", "section": "5.2 Evaluation on Video Tasks"}, {"figure_path": "bQMevGCYVM/tables/tables_7_1.jpg", "caption": "Table 2: Results on MeViS benchmark.", "description": "This table presents the quantitative results of various methods on the MeViS benchmark.  The MeViS benchmark is specifically designed to evaluate the performance of video object segmentation models on videos containing motion expressions. The table shows the performance of several state-of-the-art methods, including VideoLISA, measured by the J&F metric (average of Jaccard index and F-measure), as well as J and F scores separately. This allows for a comparison of VideoLISA's performance to other existing approaches in handling videos with motion expressions.", "section": "5.2 Motion-guided Video Object Segmentation"}, {"figure_path": "bQMevGCYVM/tables/tables_7_2.jpg", "caption": "Table 2: Results on MeViS benchmark.", "description": "This table presents a quantitative comparison of different methods on the MeViS benchmark for motion-guided video object segmentation.  The results are broken down by year, and performance metrics, J&F, J, and F, which measure the average of region similarity and contour accuracy, region similarity, and contour accuracy, respectively.  It shows the performance of VideoLISA compared to other state-of-the-art methods.", "section": "5.2 Motion-guided Video Object Segmentation"}, {"figure_path": "bQMevGCYVM/tables/tables_7_3.jpg", "caption": "Table 4: Reasoning segmentation results among ours and previous related works. \u2018ft\u2019 denotes using 239 reasoning segmentation image-instruction pairs to finetune the model.", "description": "This table compares the performance of VideoLISA with other state-of-the-art models on the image reasoning segmentation task.  The metrics used are gIoU and cIoU for both short and long queries,  and overall performance.  The \u2018ft\u2019 notation indicates models fine-tuned with additional reasoning segmentation data.  It demonstrates VideoLISA's superior performance, especially considering its smaller model size.", "section": "5.3 Reasoning Object Segmentation"}, {"figure_path": "bQMevGCYVM/tables/tables_8_1.jpg", "caption": "Table 5: Ablation study on the temporal modeling architecture.", "description": "This table presents the ablation study results focusing on different temporal modeling architectures within the VideoLISA model.  The baseline is LISA-7B, and several variations are compared, including finetuning LISA-7B on video data (Vid. FT), using the Q-Former architecture from LLaMA-VID-7B, a naive approach of concatenating visual tokens from multiple frames (n-frame), and different pooling strategies (ST Pooling [47], Slow-Fast Pooling [26]).  The key metric for comparison is the performance on the MeViS benchmark and ReasonSeg (val) as measured by J&F, J, F, giou and ciou.  The results demonstrate the effectiveness of the proposed Sparse Dense Sampling strategy in balancing spatial detail and temporal context.", "section": "5.4 Ablation Studies"}, {"figure_path": "bQMevGCYVM/tables/tables_8_2.jpg", "caption": "Table 9: Ablation study on the mask association i.e., tracking architecture. *LISA-7B is reproduced using the released codebase.", "description": "This ablation study compares different methods for temporal association (tracking) within the VideoLISA framework.  The baseline is LISA-7B extended with the One-Token-Seg-All approach.  Other methods include adding XMem (a tracking model) to LISA-7B and using a single <TRK> token for only one frame (One-Token-Seg-One). The results (J&F, J, F scores on the MeViS benchmark and J&F, J, F scores on the Ref-DAVIS-17 benchmark) demonstrate the superior performance of VideoLISA's One-Token-Seg-All approach for temporal consistency in video object segmentation.", "section": "5.2 Evaluation on Video Tasks"}, {"figure_path": "bQMevGCYVM/tables/tables_14_1.jpg", "caption": "Table 7: Referring segmentation results (cIoU) among ours and existing methods.", "description": "This table presents a quantitative comparison of the VideoLISA model's performance against several state-of-the-art methods on three referring image segmentation benchmarks: refCOCO, refCOCO+, and refCOCOg.  The results are evaluated using the cIoU metric (cumulative Intersection over Union).  It showcases VideoLISA's competitive performance, particularly on the refCOCOg benchmark where it achieves state-of-the-art results.", "section": "A.1 Evaluation on Image Segmentation"}, {"figure_path": "bQMevGCYVM/tables/tables_15_1.jpg", "caption": "Table 8: Ablation study on the temporal modeling architecture. *LISA-7B is reproduced using the released codebase.", "description": "This table presents the ablation study on different temporal modeling architectures. It compares the performance of VideoLISA with different temporal strategies, including the baseline LISA-7B, LISA-7B finetuned on video data, LLaMA-VID with Q-Former, VideoLISA with n-frame concatenation, spatial and temporal pooling, slow-fast pooling, and the proposed sparse dense sampling. The performance is evaluated on three benchmarks: ReasonSeg (giou and ciou), MeViS (J&F, J, F), and Ref-DAVIS-17 (J&F, J, F). The results demonstrate the effectiveness of the proposed sparse dense sampling strategy in achieving a balance between temporal context and spatial details for video object segmentation tasks.", "section": "5.4 Ablation Studies"}, {"figure_path": "bQMevGCYVM/tables/tables_15_2.jpg", "caption": "Table 9: Ablation study on the mask association i.e., tracking architecture. *LISA-7B is reproduced using the released codebase.", "description": "This table presents the ablation study on the mask association (tracking) architecture. It compares the performance of different methods on the MeViS and Ref-DAVIS-17 benchmarks. The methods include the baseline LISA-7B, LISA-7B with XMem, and VideoLISA with different training strategies (One-Token-Seg-One, One-Token-Seg-All, and Post optimization). The results show that VideoLISA with One-Token-Seg-All and Post optimization achieves superior performance, indicating that the proposed approaches effectively improve the temporal consistency in video object segmentation.", "section": "5.2 Evaluation on Video Tasks"}, {"figure_path": "bQMevGCYVM/tables/tables_16_1.jpg", "caption": "Table 10: Ablation study on the training data recipe.", "description": "This table presents the ablation study results focusing on different training data recipes for the VideoLISA model.  It compares the model's performance across three benchmarks (ReasonSeg, MeViS, and Ref-DAVIS-17) when trained with various combinations of image segmentation, video segmentation, image question answering, and video question answering data. The results show how the inclusion of different data types affects the model's ability to perform reasoning segmentation and video object segmentation.", "section": "5.4 Ablation Studies"}]